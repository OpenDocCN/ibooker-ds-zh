<html><head></head><body><section data-pdf-bookmark="Chapter 21. Case Study: Detecting Fake News" data-type="chapter" epub:type="chapter"><div class="chapter" id="ch-fake-news">&#13;
<h1><span class="label">Chapter 21. </span>Case Study: Detecting Fake News</h1>&#13;
&#13;
<p><em>Fake news</em>—false information<a contenteditable="false" data-primary="fake news detection case study" data-type="indexterm" id="ix_fake_news_ch21"/> created in order to deceive others—is an important issue because it can harm people. For example, the social media post in <a class="reference internal" data-type="xref" href="#fig-dont-use-sanitizer">Figure 21-1</a> confidently stated that hand sanitizer doesn’t work on coronaviruses. Though factually incorrect, it spread through social media anyway: it was shared nearly 100,000 times and was likely seen by millions of people.</p>&#13;
&#13;
<figure><div class="figure" id="fig-dont-use-sanitizer"><img src="assets/leds_2101.png"/>&#13;
<h6><span class="label">Figure 21-1. </span>A popular post on Twitter from March 2020 falsely claimed that sanitizer doesn’t kill coronaviruses</h6>&#13;
</div></figure>&#13;
&#13;
<p>We might wonder whether we can automatically detect fake news without having to read the stories. For this case study, we go through the steps of the data science lifecycle. We start by refining our research question and obtaining a dataset of news articles and labels. Then we wrangle and transform the data. Next, we explore the data to understand its content and devise features to use for modeling. Finally, we build models using logistic regression to predict whether news articles are real or fake, and evaluate their performance.</p>&#13;
&#13;
<p>We’ve included this case study because it lets us reiterate several important ideas in data science. First, natural language data appear often, and even basic techniques can enable useful analyses. Second, model selection is an important part of data analysis, and in this case study we apply what we’ve learned about cross-validation, the bias-variance trade-off, and regularization. Finally, even models that perform well on the test set might have inherent limitations when we try to use them in practice, as we will soon see.</p>&#13;
&#13;
<p>Let’s start by refining our research question and understanding the scope of our data.</p>&#13;
&#13;
&#13;
<section data-pdf-bookmark="Question and Scope" data-type="sect1"><div class="sect1" id="sec-fake-news-question">&#13;
<h1>Question and Scope</h1>&#13;
&#13;
<p>Our initial research question<a contenteditable="false" data-primary="data scope" data-secondary="fake news detection" data-type="indexterm" id="id1868"/><a contenteditable="false" data-primary="fake news detection case study" data-secondary="question and scope" data-type="indexterm" id="id1869"/> is: can we automatically detect fake news? To refine this question, we consider the kind of information that we might use to build a model for detecting fake news. If we have hand-classified news stories where people have read each story and determined whether it is fake or not, then our question becomes: can we build a model to accurately predict whether a news story is fake based on its <span class="keep-together">content</span>?</p>&#13;
&#13;
<p>To address this question, we can use the FakeNewsNet data repository as described in <a class="reference external" href="https://arxiv.org/abs/1809.01286">Shu et al</a>. This repository contains content from news and social media websites, as well as metadata like user engagement metrics. For simplicity, we only look at the dataset’s political news articles. This subset of the data includes only articles that were fact-checked by <a class="reference external" href="https://www.politifact.com">Politifact</a>, a nonpartisan organization with a good reputation. Each article in the dataset has a “real” or “fake” label based on Politifact’s evaluation, which we use as the ground truth.</p>&#13;
&#13;
<p>Politifact uses a nonrandom sampling method to select articles to fact-check. According to its website, Politifact’s journalists select the “most newsworthy and significant” claims each day. Politifact started in 2007 and the repository was published in 2020, so most of the articles were published between 2007 and 2020.</p>&#13;
&#13;
<p>Summarizing this information, we determine that the target population consists of all political news stories published online in the time period from 2007 to 2020 (we would also want to list the sources of the stories). The access frame is determined by Politifact’s identification of the most newsworthy claims of the day. So the main sources of bias for this data include:</p>&#13;
&#13;
<dl class="simple myst">&#13;
	<dt>Coverage bias</dt>&#13;
	<dd>&#13;
	<p>The news<a contenteditable="false" data-primary="coverage bias" data-type="indexterm" id="id1870"/> outlets are limited to those that Politifact monitored, which may miss arcane or short-lived sites.</p>&#13;
	</dd>&#13;
	<dt>Selection bias</dt>&#13;
	<dd>&#13;
	<p>The data<a contenteditable="false" data-primary="selection bias" data-type="indexterm" id="id1871"/> are limited to articles Politifact decided were interesting enough to fact-check, which means that articles might skew toward ones that are both widely shared and controversial.</p>&#13;
	</dd>&#13;
	<dt>Measurement bias</dt>&#13;
	<dd>&#13;
	<p>Whether a story<a contenteditable="false" data-primary="measurement bias" data-type="indexterm" id="id1872"/> should be labeled “fake” or “real” is determined by one organization (Politifact) and reflects the biases, unintentional or otherwise, that the organization has in its fact-checking methodology.</p>&#13;
	</dd>&#13;
	<dt>Drift</dt>&#13;
	<dd>&#13;
	<p>Since we only have articles published between 2007 and 2020, there is likely to be drift in the content. Topics are popularized and faked in rapidly evolving news trends.</p>&#13;
	</dd>&#13;
</dl>&#13;
&#13;
<p>We will keep these limitations of the data in mind as we begin to wrangle the data into a form that we can analyze.</p>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Obtaining and Wrangling the Data" data-type="sect1"><div class="sect1" id="sec-fake-news-data">&#13;
<h1>Obtaining and Wrangling the Data</h1>&#13;
&#13;
<p>Let’s get the data into Python<a contenteditable="false" data-primary="fake news detection case study" data-secondary="obtaining and wrangling data" data-type="indexterm" id="ix_fake_news_wr"/> using the <a class="reference external" href="https://oreil.ly/0DOHd">GitHub page for FakeNewsNet</a>. Reading over the repository description and code, we find that the repository doesn’t actually store the news articles itself. Instead, running the repository code will scrape news articles from online web pages directly (using techniques we covered in <a class="reference internal" data-type="xref" href="ch14.html#ch-web">Chapter 14</a>). This presents a challenge: if an article is no longer available online, it likely will be missing from our dataset. Noting this, let’s proceed with downloading the data.</p>&#13;
&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>The FakeNewsNet code highlights one challenge in reproducible research—online datasets change over time, but it can be difficult (or even illegal) to store and share copies of this data. For example, other parts of the FakeNewsNet dataset use Twitter posts, but the dataset creators would violate Twitter’s terms and services if they stored copies of the posts in their repository. When working with data gathered from the web, we suggest documenting the date the data were gathered and reading the terms and services of the data sources carefully.</p>&#13;
</div>&#13;
&#13;
<p>Running the script to download the Politifact data takes about an hour. After that, we place the datafiles into the <em>data/politifact</em> folder. The articles that Politifact labeled as fake and real are in <em>data/politifact/fake</em> and <em>data/politifact/real</em>. Let’s take a look at one of the articles labeled “real”:</p>&#13;
&#13;
<div class="cell docutils container">&#13;
<div class="cell_input docutils container">&#13;
<div class="highlight-ipython3 notranslate">&#13;
<div class="highlight">&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<span><code class="err">!</code></span><code class="n">ls</code><code> </code><code class="o">-</code><code class="n">l</code><code> </code><code class="n">data</code><code class="o">/</code><code class="n">politifact</code><code class="o">/</code><code class="n">real</code><code> </code><span><code class="o">|</code></span><code> </code><code class="n">head</code><code> </code><code class="o">-</code><code class="n">n</code><code> </code><span><code class="mi">5</code></span><code>&#13;
</code></pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<div class="cell_output docutils container">&#13;
<div class="output stream highlight-myst-ansi notranslate">&#13;
<div class="highlight">&#13;
<pre data-type="programlisting">&#13;
total 0&#13;
drwxr-xr-x  2 sam  staff  64 Jul 14  2022 <span>politifact100</span>&#13;
drwxr-xr-x  3 sam  staff  96 Jul 14  2022 <span>politifact1013</span>&#13;
drwxr-xr-x  3 sam  staff  96 Jul 14  2022 <span>politifact1014</span>&#13;
drwxr-xr-x  2 sam  staff  64 Jul 14  2022 <span>politifact10185</span>&#13;
ls: stdout: Undefined error: 0&#13;
</pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<div class="cell docutils container">&#13;
<div class="cell_input docutils container">&#13;
<div class="highlight-ipython3 notranslate">&#13;
<div class="highlight">&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<span><code class="err">!</code></span><code class="n">ls</code><code> </code><code class="o">-</code><code class="n">lh</code><code> </code><code class="n">data</code><code class="o">/</code><code class="n">politifact</code><code class="o">/</code><code class="n">real</code><code class="o">/</code><code class="n">politifact1013</code><code class="o">/</code><code>&#13;
</code></pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<div class="cell_output docutils container">&#13;
<div class="output stream highlight-myst-ansi notranslate">&#13;
<div class="highlight">&#13;
<pre data-type="programlisting">&#13;
total 16&#13;
-rw-r--r--  1 sam  staff   5.7K Jul 14  2022 news content.json&#13;
</pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<p>Each article’s data is stored in a JSON file named <em>news content.json</em>. Let’s load the JSON for one article into a Python dictionary (see <a class="reference internal" data-type="xref" href="ch14.html#ch-web">Chapter 14</a>):</p>&#13;
&#13;
<div class="cell docutils container">&#13;
<div class="cell_input docutils container">&#13;
<div class="highlight-ipython3 notranslate">&#13;
<div class="highlight">&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<span><code class="kn">import</code></span><code> </code><span><code class="nn">json</code></span><code>&#13;
</code><span><code class="kn">from</code></span><code> </code><span><code class="nn">pathlib</code></span><code> </code><span><code class="kn">import</code></span><code> </code><span><code class="n">Path</code></span><code>&#13;
</code><code>&#13;
</code><span><code class="n">article_path</code></span><code> </code><span><code class="o">=</code></span><code> </code><span><code class="n">Path</code></span><span><code class="p">(</code></span><span><code class="s1">'</code><code class="s1">data/politifact/real/politifact1013/news content.json</code><code class="s1">'</code></span><span><code class="p">)</code></span><code>&#13;
</code><span><code class="n">article_json</code></span><code> </code><span><code class="o">=</code></span><code> </code><span><code class="n">json</code></span><span><code class="o">.</code></span><span><code class="n">loads</code></span><span><code class="p">(</code></span><span><code class="n">article_path</code></span><span><code class="o">.</code></span><span><code class="n">read_text</code></span><span><code class="p">(</code><code class="p">)</code><code class="p">)</code></span><code>&#13;
</code></pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<p>Here, we’ve displayed the keys and values in <code>article_json</code> as a table:</p>&#13;
&#13;
<div class="cell tag_hide-input docutils container">&#13;
<div class="cell_output docutils container">&#13;
<div class="output text_html">&#13;
<div>&#13;
<table class="dataframe">&#13;
	<thead>&#13;
		<tr>&#13;
			<th> </th>&#13;
			<th>value</th>&#13;
		</tr>&#13;
		<tr>&#13;
			<th>key</th>&#13;
			<th> </th>&#13;
		</tr>&#13;
	</thead>&#13;
	<tbody>&#13;
		<tr>&#13;
			<td><strong>url</strong></td>&#13;
			<td>http://www.senate.gov/legislative/LIS/roll_cal...</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td><strong>text</strong></td>&#13;
			<td>Roll Call Vote 111th Congress - 1st Session\n\...</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td><strong>images</strong></td>&#13;
			<td>[http://statse.webtrendslive.com/dcs222dj3ow9j...</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td><strong>top_img</strong></td>&#13;
			<td>http://www.senate.gov/resources/images/us_sen.ico</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td><strong>keywords</strong></td>&#13;
			<td>[]</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td><strong>authors</strong></td>&#13;
			<td>[]</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td><strong>canonical_link</strong></td>&#13;
			<td> </td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td><strong>title</strong></td>&#13;
			<td>U.S. Senate: U.S. Senate Roll Call Votes 111th...</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td><strong>meta_data</strong></td>&#13;
			<td>{'viewport’: ‘width=device-width, initial-scal...</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td><strong>movies</strong></td>&#13;
			<td>[]</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td><strong>publish_date</strong></td>&#13;
			<td>None</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td><strong>source</strong></td>&#13;
			<td>http://www.senate.gov</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td><strong>summary</strong></td>&#13;
			<td> </td>&#13;
		</tr>&#13;
	</tbody>&#13;
</table>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<p>There are many fields in the JSON file, but for this analysis we only look at a few that are primarily related to the content of the article: the article’s title, text content, URL, and publication date. We create a dataframe where each row represents one article (the granularity in a news story). To do this, we load in each available JSON file as a Python dictionary, and then extract the fields of interest to store as a <code>pandas</code> <code>DataFrame</code> named <code>df_raw</code>:</p>&#13;
&#13;
<div class="cell docutils container">&#13;
<div class="cell_input docutils container">&#13;
<div class="highlight-ipython3 notranslate">&#13;
<div class="highlight">&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<span><code class="kn">from</code></span><code> </code><span><code class="nn">pathlib</code></span><code> </code><span><code class="kn">import</code></span><code> </code><span><code class="n">Path</code></span><code>&#13;
</code><code>&#13;
</code><span><code class="k">def</code></span><code> </code><span><code class="nf">df_row</code></span><span><code class="p">(</code></span><span><code class="n">content_json</code></span><span><code class="p">)</code><code class="p">:</code></span><code>&#13;
</code><code>    </code><span><code class="k">return</code></span><code> </code><span><code class="p">{</code></span><code>&#13;
</code><code>        </code><span><code class="s1">'</code><code class="s1">url</code><code class="s1">'</code></span><span><code class="p">:</code></span><code> </code><span><code class="n">content_json</code></span><span><code class="p">[</code></span><span><code class="s1">'</code><code class="s1">url</code><code class="s1">'</code></span><span><code class="p">]</code><code class="p">,</code></span><code>&#13;
</code><code>        </code><span><code class="s1">'</code><code class="s1">text</code><code class="s1">'</code></span><span><code class="p">:</code></span><code> </code><span><code class="n">content_json</code></span><span><code class="p">[</code></span><span><code class="s1">'</code><code class="s1">text</code><code class="s1">'</code></span><span><code class="p">]</code><code class="p">,</code></span><code>&#13;
</code><code>        </code><span><code class="s1">'</code><code class="s1">title</code><code class="s1">'</code></span><span><code class="p">:</code></span><code> </code><span><code class="n">content_json</code></span><span><code class="p">[</code></span><span><code class="s1">'</code><code class="s1">title</code><code class="s1">'</code></span><span><code class="p">]</code><code class="p">,</code></span><code>&#13;
</code><code>        </code><span><code class="s1">'</code><code class="s1">publish_date</code><code class="s1">'</code></span><span><code class="p">:</code></span><code> </code><span><code class="n">content_json</code></span><span><code class="p">[</code></span><span><code class="s1">'</code><code class="s1">publish_date</code><code class="s1">'</code></span><span><code class="p">]</code><code class="p">,</code></span><code>&#13;
</code><code>    </code><span><code class="p">}</code></span><code>&#13;
</code><code>&#13;
</code><span><code class="k">def</code></span><code> </code><span><code class="nf">load_json</code></span><span><code class="p">(</code></span><span><code class="n">folder</code></span><span><code class="p">,</code></span><code> </code><span><code class="n">label</code></span><span><code class="p">)</code><code class="p">:</code></span><code>&#13;
</code><code>    </code><span><code class="n">filepath</code></span><code> </code><span><code class="o">=</code></span><code> </code><span><code class="n">folder</code></span><code> </code><span><code class="o">/</code></span><code> </code><span><code class="s1">'</code><code class="s1">news content.json</code><code class="s1">'</code></span><code>&#13;
</code><code>    </code><span><code class="n">data</code></span><code> </code><span><code class="o">=</code></span><code> </code><span><code class="n">df_row</code></span><span><code class="p">(</code></span><span><code class="n">json</code></span><span><code class="o">.</code></span><span><code class="n">loads</code></span><span><code class="p">(</code></span><span><code class="n">filepath</code></span><span><code class="o">.</code></span><span><code class="n">read_text</code></span><span><code class="p">(</code><code class="p">)</code><code class="p">)</code><code class="p">)</code></span><code> </code><span><code class="k">if</code></span><code> </code><span><code class="n">filepath</code></span><span><code class="o">.</code></span><span><code class="n">exists</code></span><span><code class="p">(</code><code class="p">)</code></span><code> </code><span><code class="k">else</code></span><code> </code><span><code class="p">{</code><code class="p">}</code></span><code>&#13;
</code><code>    </code><span><code class="k">return</code></span><code> </code><span><code class="p">{</code></span><code>&#13;
</code><code>        </code><span><code class="o">*</code><code class="o">*</code></span><span><code class="n">data</code></span><span><code class="p">,</code></span><code>&#13;
</code><code>        </code><span><code class="s1">'</code><code class="s1">label</code><code class="s1">'</code></span><span><code class="p">:</code></span><code> </code><span><code class="n">label</code></span><span><code class="p">,</code></span><code>&#13;
</code><code>    </code><span><code class="p">}</code></span><code>&#13;
</code><code>&#13;
</code><span><code class="n">fakes</code></span><code> </code><span><code class="o">=</code></span><code> </code><span><code class="n">Path</code></span><span><code class="p">(</code></span><span><code class="s1">'</code><code class="s1">data/politifact/fake</code><code class="s1">'</code></span><span><code class="p">)</code></span><code>&#13;
</code><span><code class="n">reals</code></span><code> </code><span><code class="o">=</code></span><code> </code><span><code class="n">Path</code></span><span><code class="p">(</code></span><span><code class="s1">'</code><code class="s1">data/politifact/real</code><code class="s1">'</code></span><span><code class="p">)</code></span><code>&#13;
</code><code>&#13;
</code><span><code class="n">df_raw</code></span><code> </code><span><code class="o">=</code></span><code> </code><span><code class="n">pd</code></span><span><code class="o">.</code></span><span><code class="n">DataFrame</code></span><span><code class="p">(</code><code class="p">[</code></span><span><code class="n">load_json</code></span><span><code class="p">(</code></span><span><code class="n">path</code></span><span><code class="p">,</code></span><code> </code><span><code class="s1">'</code><code class="s1">fake</code><code class="s1">'</code></span><span><code class="p">)</code></span><code> </code><span><code class="k">for</code></span><code> </code><span><code class="n">path</code></span><code> </code><span><code class="ow">in</code></span><code> </code><span><code class="n">fakes</code></span><span><code class="o">.</code></span><span><code class="n">iterdir</code></span><span><code class="p">(</code><code class="p">)</code><code class="p">]</code></span><code> </code><span><code class="o">+</code></span><code>&#13;
</code><code>                      </code><span><code class="p">[</code></span><span><code class="n">load_json</code></span><span><code class="p">(</code></span><span><code class="n">path</code></span><span><code class="p">,</code></span><code> </code><span><code class="s1">'</code><code class="s1">real</code><code class="s1">'</code></span><span><code class="p">)</code></span><code> </code><span><code class="k">for</code></span><code> </code><span><code class="n">path</code></span><code> </code><span><code class="ow">in</code></span><code> </code><span><code class="n">reals</code></span><span><code class="o">.</code></span><span><code class="n">iterdir</code></span><span><code class="p">(</code><code class="p">)</code><code class="p">]</code><code class="p">)</code></span><code>&#13;
</code></pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<div class="cell docutils container">&#13;
<div class="cell_input docutils container">&#13;
<div class="highlight-ipython3 notranslate">&#13;
<div class="highlight">&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<span><code class="n">df_raw</code></span><span><code class="o">.</code></span><span><code class="n">head</code></span><span><code class="p">(</code></span><span><code class="mi">2</code></span><span><code class="p">)</code></span><code>&#13;
</code></pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<div class="cell_output docutils container">&#13;
<div class="output text_html">&#13;
<div>&#13;
<table class="dataframe">&#13;
	<thead>&#13;
		<tr>&#13;
			<th> </th>&#13;
			<th>url</th>&#13;
			<th>text</th>&#13;
			<th>title</th>&#13;
			<th>publish_date</th>&#13;
			<th>label</th>&#13;
		</tr>&#13;
	</thead>&#13;
	<tbody>&#13;
		<tr>&#13;
			<th>0</th>&#13;
			<td>dailybuzzlive.com/cannibals-arrested-florida/</td>&#13;
			<td>Police in Vernal Heights, Florida, arrested 3-...</td>&#13;
			<td>Cannibals Arrested in Florida Claim Eating Hum...</td>&#13;
			<td>1.62e+09</td>&#13;
			<td>fake</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<th>1</th>&#13;
			<td>https://web.archive.org/web/20171228192703/htt...</td>&#13;
			<td>WASHINGTON — Rod Jay Rosenstein, Deputy Attorn...</td>&#13;
			<td>BREAKING: Trump fires Deputy Attorney General ...</td>&#13;
			<td>1.45e+09</td>&#13;
			<td>fake</td>&#13;
		</tr>&#13;
	</tbody>&#13;
</table>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<p>Exploring this dataframe reveals some issues we’d like to address before we begin the analysis. For example:</p>&#13;
&#13;
<ul>&#13;
	<li>&#13;
	<p>Some articles couldn’t be downloaded. When this happened, the <code>url</code> column contains <code>NaN</code>.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Some articles don’t have text (such as a web page with only video content). We drop these articles from our dataframe.</p></li>&#13;
	<li><p>The <code>publish_date</code> column stores timestamps in Unix format (seconds since the Unix epoch), so we need to convert them to <code>pandas.Timestamp</code> objects.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>We are interested in the base URL of a web page. However, the <code>source</code> field in the JSON file has many missing values compared to the <code>url</code> column, so we must extract the base URL using the full URL in the <code>url</code> column. For example, from <em>dailybuzzlive.com/cannibals-arrested-florida/</em> we get <em>dailybuzzlive.com</em>.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Some articles were downloaded from an archival website (<code>web.archive.org</code>). When this happens, we want to extract the actual base URL from the original by removing the <code>web.archive.org</code> prefix.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>We want to concatenate the <code>title</code> and <code>text</code> columns into a single <code>content</code> column that contains all of the text content of the article.</p>&#13;
	</li>&#13;
</ul>&#13;
&#13;
<p>We can tackle these data issues using a combination of <code>pandas</code> functions and regular expressions:</p>&#13;
&#13;
<div class="cell docutils container">&#13;
<div class="cell_input docutils container">&#13;
<div class="highlight-ipython3 notranslate">&#13;
<div class="highlight">&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<span><code class="kn">import</code></span><code> </code><span><code class="nn">re</code></span><code>&#13;
</code><code>&#13;
</code><span><code class="c1"># [1], [2]</code></span><code>&#13;
</code><span><code class="k">def</code></span><code> </code><span><code class="nf">drop_nans</code></span><span><code class="p">(</code></span><span><code class="n">df</code></span><span><code class="p">)</code><code class="p">:</code></span><code>&#13;
</code><code>    </code><span><code class="k">return</code></span><code> </code><span><code class="n">df</code></span><span><code class="p">[</code></span><span><code class="o">~</code></span><span><code class="p">(</code></span><span><code class="n">df</code></span><span><code class="p">[</code></span><span><code class="s1">'</code><code class="s1">url</code><code class="s1">'</code></span><span><code class="p">]</code></span><span><code class="o">.</code></span><span><code class="n">isna</code></span><span><code class="p">(</code><code class="p">)</code></span><code> </code><span><code class="o">|</code></span><code>&#13;
</code><code>                </code><span><code class="p">(</code></span><span><code class="n">df</code></span><span><code class="p">[</code></span><span><code class="s1">'</code><code class="s1">text</code><code class="s1">'</code></span><span><code class="p">]</code></span><span><code class="o">.</code></span><span><code class="n">str</code></span><span><code class="o">.</code></span><span><code class="n">strip</code></span><span><code class="p">(</code><code class="p">)</code></span><code> </code><span><code class="o">==</code></span><code> </code><span><code class="s1">'</code><code class="s1">'</code></span><span><code class="p">)</code></span><code> </code><span><code class="o">|</code></span><code> </code><code>&#13;
</code><code>                </code><span><code class="p">(</code></span><span><code class="n">df</code></span><span><code class="p">[</code></span><span><code class="s1">'</code><code class="s1">title</code><code class="s1">'</code></span><span><code class="p">]</code></span><span><code class="o">.</code></span><span><code class="n">str</code></span><span><code class="o">.</code></span><span><code class="n">strip</code></span><span><code class="p">(</code><code class="p">)</code></span><code> </code><span><code class="o">==</code></span><code> </code><span><code class="s1">'</code><code class="s1">'</code></span><span><code class="p">)</code><code class="p">)</code><code class="p">]</code></span><code>&#13;
</code><code>&#13;
</code><span><code class="c1"># [3]</code></span><code>&#13;
</code><span><code class="k">def</code></span><code> </code><span><code class="nf">parse_timestamps</code></span><span><code class="p">(</code></span><span><code class="n">df</code></span><span><code class="p">)</code><code class="p">:</code></span><code>&#13;
</code><code>    </code><span><code class="n">timestamp</code></span><code> </code><span><code class="o">=</code></span><code> </code><span><code class="n">pd</code></span><span><code class="o">.</code></span><span><code class="n">to_datetime</code></span><span><code class="p">(</code></span><span><code class="n">df</code></span><span><code class="p">[</code></span><span><code class="s1">'</code><code class="s1">publish_date</code><code class="s1">'</code></span><span><code class="p">]</code><code class="p">,</code></span><code> </code><span><code class="n">unit</code></span><span><code class="o">=</code></span><span><code class="s1">'</code><code class="s1">s</code><code class="s1">'</code></span><span><code class="p">,</code></span><code> </code><span><code class="n">errors</code></span><span><code class="o">=</code></span><span><code class="s1">'</code><code class="s1">coerce</code><code class="s1">'</code></span><span><code class="p">)</code></span><code>&#13;
</code><code>    </code><span><code class="k">return</code></span><code> </code><span><code class="n">df</code></span><span><code class="o">.</code></span><span><code class="n">assign</code></span><span><code class="p">(</code></span><span><code class="n">timestamp</code></span><span><code class="o">=</code></span><span><code class="n">timestamp</code></span><span><code class="p">)</code></span><code>&#13;
</code><code>&#13;
</code><span><code class="c1"># [4], [5]</code></span><code>&#13;
</code><span><code class="n">archive_prefix_re</code></span><code> </code><span><code class="o">=</code></span><code> </code><span><code class="n">re</code></span><span><code class="o">.</code></span><span><code class="n">compile</code></span><span><code class="p">(</code></span><span><code class="sa">r</code></span><span><code class="s1">'</code><code class="s1">https://web.archive.org/web/</code><code class="s1">\</code><code class="s1">d+/</code><code class="s1">'</code></span><span><code class="p">)</code></span><code>&#13;
</code><span><code class="n">site_prefix_re</code></span><code> </code><span><code class="o">=</code></span><code> </code><span><code class="n">re</code></span><span><code class="o">.</code></span><span><code class="n">compile</code></span><span><code class="p">(</code></span><span><code class="sa">r</code></span><span><code class="s1">'</code><code class="s1">(https?://)?(www</code><code class="s1">\</code><code class="s1">.)?</code><code class="s1">'</code></span><span><code class="p">)</code></span><code>&#13;
</code><span><code class="n">port_re</code></span><code> </code><span><code class="o">=</code></span><code> </code><span><code class="n">re</code></span><span><code class="o">.</code></span><span><code class="n">compile</code></span><span><code class="p">(</code></span><span><code class="sa">r</code></span><span><code class="s1">'</code><code class="s1">:</code><code class="s1">\</code><code class="s1">d+</code><code class="s1">'</code></span><span><code class="p">)</code></span><code>&#13;
</code><code>&#13;
</code><span><code class="k">def</code></span><code> </code><span><code class="nf">url_basename</code></span><span><code class="p">(</code></span><span><code class="n">url</code></span><span><code class="p">)</code><code class="p">:</code></span><code>&#13;
</code><code>    </code><span><code class="k">if</code></span><code> </code><span><code class="n">archive_prefix_re</code></span><span><code class="o">.</code></span><span><code class="n">match</code></span><span><code class="p">(</code></span><span><code class="n">url</code></span><span><code class="p">)</code><code class="p">:</code></span><code>&#13;
</code><code>        </code><span><code class="n">url</code></span><code> </code><span><code class="o">=</code></span><code> </code><span><code class="n">archive_prefix_re</code></span><span><code class="o">.</code></span><span><code class="n">sub</code></span><span><code class="p">(</code></span><span><code class="s1">'</code><code class="s1">'</code></span><span><code class="p">,</code></span><code> </code><span><code class="n">url</code></span><span><code class="p">)</code></span><code>&#13;
</code><code>    </code><span><code class="n">site</code></span><code> </code><span><code class="o">=</code></span><code> </code><span><code class="n">site_prefix_re</code></span><span><code class="o">.</code></span><span><code class="n">sub</code></span><span><code class="p">(</code></span><span><code class="s1">'</code><code class="s1">'</code></span><span><code class="p">,</code></span><code> </code><span><code class="n">url</code></span><span><code class="p">)</code></span><span><code class="o">.</code></span><span><code class="n">split</code></span><span><code class="p">(</code></span><span><code class="s1">'</code><code class="s1">/</code><code class="s1">'</code></span><span><code class="p">)</code><code class="p">[</code></span><span><code class="mi">0</code></span><span><code class="p">]</code></span><code>&#13;
</code><code>    </code><span><code class="k">return</code></span><code> </code><span><code class="n">port_re</code></span><span><code class="o">.</code></span><span><code class="n">sub</code></span><span><code class="p">(</code></span><span><code class="s1">'</code><code class="s1">'</code></span><span><code class="p">,</code></span><code> </code><span><code class="n">site</code></span><span><code class="p">)</code></span><code>&#13;
</code><code>&#13;
</code><span><code class="c1"># [6]</code></span><code>&#13;
</code><span><code class="k">def</code></span><code> </code><span><code class="nf">combine_content</code></span><span><code class="p">(</code></span><span><code class="n">df</code></span><span><code class="p">)</code><code class="p">:</code></span><code>&#13;
</code><code>    </code><span><code class="k">return</code></span><code> </code><span><code class="n">df</code></span><span><code class="o">.</code></span><span><code class="n">assign</code></span><span><code class="p">(</code></span><span><code class="n">content</code></span><span><code class="o">=</code></span><span><code class="n">df</code></span><span><code class="p">[</code></span><span><code class="s1">'</code><code class="s1">title</code><code class="s1">'</code></span><span><code class="p">]</code></span><code> </code><span><code class="o">+</code></span><code> </code><span><code class="s1">'</code><code class="s1"> </code><code class="s1">'</code></span><code> </code><span><code class="o">+</code></span><code> </code><span><code class="n">df</code></span><span><code class="p">[</code></span><span><code class="s1">'</code><code class="s1">text</code><code class="s1">'</code></span><span><code class="p">]</code><code class="p">)</code></span><code>&#13;
</code><code>&#13;
</code><span><code class="k">def</code></span><code> </code><span><code class="nf">subset_df</code></span><span><code class="p">(</code></span><span><code class="n">df</code></span><span><code class="p">)</code><code class="p">:</code></span><code>&#13;
</code><code>    </code><span><code class="k">return</code></span><code> </code><span><code class="n">df</code></span><span><code class="p">[</code><code class="p">[</code></span><span><code class="s1">'</code><code class="s1">timestamp</code><code class="s1">'</code></span><span><code class="p">,</code></span><code> </code><span><code class="s1">'</code><code class="s1">baseurl</code><code class="s1">'</code></span><span><code class="p">,</code></span><code> </code><span><code class="s1">'</code><code class="s1">content</code><code class="s1">'</code></span><span><code class="p">,</code></span><code> </code><span><code class="s1">'</code><code class="s1">label</code><code class="s1">'</code></span><span><code class="p">]</code><code class="p">]</code></span><code>&#13;
</code><code>&#13;
</code><span><code class="n">df</code></span><code> </code><span><code class="o">=</code></span><code> </code><span><code class="p">(</code></span><span><code class="n">df_raw</code></span><code>&#13;
</code><code> </code><span><code class="o">.</code></span><span><code class="n">pipe</code></span><span><code class="p">(</code></span><span><code class="n">drop_nans</code></span><span><code class="p">)</code></span><code>&#13;
</code><code> </code><span><code class="o">.</code></span><span><code class="n">reset_index</code></span><span><code class="p">(</code></span><span><code class="n">drop</code></span><span><code class="o">=</code></span><span><code class="kc">True</code></span><span><code class="p">)</code></span><code>&#13;
</code><code> </code><span><code class="o">.</code></span><span><code class="n">assign</code></span><span><code class="p">(</code></span><span><code class="n">baseurl</code></span><span><code class="o">=</code></span><span><code class="k">lambda</code></span><code> </code><span><code class="n">df</code></span><span><code class="p">:</code></span><code> </code><span><code class="n">df</code></span><span><code class="p">[</code></span><span><code class="s1">'</code><code class="s1">url</code><code class="s1">'</code></span><span><code class="p">]</code></span><span><code class="o">.</code></span><span><code class="n">apply</code></span><span><code class="p">(</code></span><span><code class="n">url_basename</code></span><span><code class="p">)</code><code class="p">)</code></span><code>&#13;
</code><code> </code><span><code class="o">.</code></span><span><code class="n">pipe</code></span><span><code class="p">(</code></span><span><code class="n">parse_timestamps</code></span><span><code class="p">)</code></span><code>&#13;
</code><code> </code><span><code class="o">.</code></span><span><code class="n">pipe</code></span><span><code class="p">(</code></span><span><code class="n">combine_content</code></span><span><code class="p">)</code></span><code>&#13;
</code><code> </code><span><code class="o">.</code></span><span><code class="n">pipe</code></span><span><code class="p">(</code></span><span><code class="n">subset_df</code></span><span><code class="p">)</code></span><code>&#13;
</code><span><code class="p">)</code></span><code>&#13;
</code></pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<p>After data wrangling, we end up with the following dataframe named <code>df</code>:</p>&#13;
&#13;
<div class="cell docutils container">&#13;
<div class="cell_input docutils container">&#13;
<div class="highlight-ipython3 notranslate">&#13;
<div class="highlight">&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<span><code class="n">df</code></span><span><code class="o">.</code></span><span><code class="n">head</code></span><span><code class="p">(</code></span><span><code class="mi">2</code></span><span><code class="p">)</code></span><code>&#13;
</code></pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<div class="cell_output docutils container">&#13;
<div class="output text_html">&#13;
<div>&#13;
<table class="dataframe">&#13;
	<thead>&#13;
		<tr>&#13;
			<th> </th>&#13;
			<th>timestamp</th>&#13;
			<th>baseurl</th>&#13;
			<th>content</th>&#13;
			<th>label</th>&#13;
		</tr>&#13;
	</thead>&#13;
	<tbody>&#13;
		<tr>&#13;
			<td><strong>0</strong></td>&#13;
			<td>2021-04-05 16:39:51</td>&#13;
			<td>dailybuzzlive.com</td>&#13;
			<td>Cannibals Arrested in Florida Claim Eating Hum...</td>&#13;
			<td>fake</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td><strong>1</strong></td>&#13;
			<td>2016-01-01 23:17:43</td>&#13;
			<td>houstonchronicle-tv.com</td>&#13;
			<td>BREAKING: Trump fires Deputy Attorney General ...</td>&#13;
			<td>fake</td>&#13;
		</tr>&#13;
	</tbody>&#13;
</table>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<p>Now that we’ve loaded and cleaned the data, we can proceed to exploratory data <span class="keep-together">analysis</span><a contenteditable="false" data-primary="" data-startref="ix_fake_news_wr" data-type="indexterm" id="id1873"/>.</p>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Exploring the Data" data-type="sect1"><div class="sect1" id="sec-fake-news-exploring">&#13;
<h1>Exploring the Data</h1>&#13;
&#13;
<p>The dataset<a contenteditable="false" data-primary="exploratory data analysis (EDA)" data-secondary="fake news detection study" data-type="indexterm" id="ix_eda_fake_news"/><a contenteditable="false" data-primary="fake news detection case study" data-secondary="exploring data" data-type="indexterm" id="ix_fake_news_expl"/> of news articles we’re exploring is just one part of the larger FakeNewsNet dataset. As such, the original paper doesn’t provide detailed information about our subset of data. So, to better understand the data, we must explore it ourselves.</p>&#13;
&#13;
<p>Before starting exploratory data analysis, we apply our standard practice of splitting the data into training and test sets. We perform EDA using only the train set:</p>&#13;
&#13;
<div class="cell docutils container">&#13;
<div class="cell_input docutils container">&#13;
<div class="highlight-ipython3 notranslate">&#13;
<div class="highlight">&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<span><code class="kn">from</code></span><code> </code><span><code class="nn">sklearn</code><code class="nn">.</code><code class="nn">model_selection</code></span><code> </code><span><code class="kn">import</code></span><code> </code><span><code class="n">train_test_split</code></span><code>&#13;
</code><code>&#13;
</code><span><code class="n">df</code></span><span><code class="p">[</code></span><span><code class="s1">'</code><code class="s1">label</code><code class="s1">'</code></span><span><code class="p">]</code></span><code> </code><span><code class="o">=</code></span><code> </code><span><code class="p">(</code></span><span><code class="n">df</code></span><span><code class="p">[</code></span><span><code class="s1">'</code><code class="s1">label</code><code class="s1">'</code></span><span><code class="p">]</code></span><code> </code><span><code class="o">==</code></span><code> </code><span><code class="s1">'</code><code class="s1">fake</code><code class="s1">'</code></span><span><code class="p">)</code></span><span><code class="o">.</code></span><span><code class="n">astype</code></span><span><code class="p">(</code></span><span><code class="nb">int</code></span><span><code class="p">)</code></span><code>&#13;
</code><code>&#13;
</code><span><code class="n">X_train</code></span><span><code class="p">,</code></span><code> </code><span><code class="n">X_test</code></span><span><code class="p">,</code></span><code> </code><span><code class="n">y_train</code></span><span><code class="p">,</code></span><code> </code><span><code class="n">y_test</code></span><code> </code><span><code class="o">=</code></span><code> </code><span><code class="n">train_test_split</code></span><span><code class="p">(</code></span><code>&#13;
</code><code>    </code><span><code class="n">df</code></span><span><code class="p">[</code><code class="p">[</code></span><span><code class="s1">'</code><code class="s1">timestamp</code><code class="s1">'</code></span><span><code class="p">,</code></span><code> </code><span><code class="s1">'</code><code class="s1">baseurl</code><code class="s1">'</code></span><span><code class="p">,</code></span><code> </code><span><code class="s1">'</code><code class="s1">content</code><code class="s1">'</code></span><span><code class="p">]</code><code class="p">]</code><code class="p">,</code></span><code> </code><span><code class="n">df</code></span><span><code class="p">[</code></span><span><code class="s1">'</code><code class="s1">label</code><code class="s1">'</code></span><span><code class="p">]</code><code class="p">,</code></span><code>&#13;
</code><code>    </code><span><code class="n">test_size</code></span><span><code class="o">=</code></span><span><code class="mf">0.25</code></span><span><code class="p">,</code></span><code> </code><span><code class="n">random_state</code></span><span><code class="o">=</code></span><span><code class="mi">42</code></span><span><code class="p">,</code></span><code>&#13;
</code><span><code class="p">)</code></span><code>&#13;
</code></pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<div class="cell docutils container">&#13;
<div class="cell_input docutils container">&#13;
<div class="highlight-ipython3 notranslate">&#13;
<div class="highlight">&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<span><code class="n">X_train</code></span><span><code class="o">.</code></span><span><code class="n">head</code></span><span><code class="p">(</code></span><span><code class="mi">2</code></span><span><code class="p">)</code></span><code>&#13;
</code></pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<div class="cell_output docutils container">&#13;
<div class="output text_html">&#13;
<div>&#13;
<table class="dataframe">&#13;
	<thead>&#13;
		<tr>&#13;
			<th> </th>&#13;
			<th>timestamp</th>&#13;
			<th>baseurl</th>&#13;
			<th>content</th>&#13;
		</tr>&#13;
	</thead>&#13;
	<tbody>&#13;
		<tr>&#13;
			<td><strong>164</strong></td>&#13;
			<td>2019-01-04 19:25:46</td>&#13;
			<td>worldnewsdailyreport.com</td>&#13;
			<td>Chinese lunar rover finds no evidence of Ameri...</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td><strong>28</strong></td>&#13;
			<td>2016-01-12 21:02:28</td>&#13;
			<td>occupydemocrats.com</td>&#13;
			<td>Virginia Republican Wants Schools To Check Chi...</td>&#13;
		</tr>&#13;
	</tbody>&#13;
</table>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<p>Let’s count the number of real and fake articles in the train set:</p>&#13;
&#13;
<div class="cell docutils container">&#13;
<div class="cell_input docutils container">&#13;
<div class="highlight-ipython3 notranslate">&#13;
<div class="highlight">&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<span><code class="n">y_train</code></span><span><code class="o">.</code></span><span><code class="n">value_counts</code></span><span><code class="p">(</code><code class="p">)</code></span><code>&#13;
</code></pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<div class="cell_output docutils container">&#13;
<div class="output text_plain highlight-myst-ansi notranslate">&#13;
<div class="highlight">&#13;
<pre data-type="programlisting">&#13;
label&#13;
0    320&#13;
1    264&#13;
Name: count, dtype: int64&#13;
</pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<p>Our train set has 584 articles, and there are about 60 more articles labeled <code>real</code> than <code>fake</code>. Next, we check for missing values in the three fields:</p>&#13;
&#13;
<div class="cell docutils container">&#13;
<div class="cell_input docutils container">&#13;
<div class="highlight-ipython3 notranslate">&#13;
<div class="highlight">&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<span><code class="n">X_train</code></span><span><code class="o">.</code></span><span><code class="n">info</code></span><span><code class="p">(</code><code class="p">)</code></span><code>&#13;
</code></pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<div class="cell_output docutils container">&#13;
<div class="output stream highlight-myst-ansi notranslate">&#13;
<div class="highlight">&#13;
<pre data-type="programlisting">&#13;
&lt;class 'pandas.core.frame.DataFrame'&gt;&#13;
Index: 584 entries, 164 to 102&#13;
Data columns (total 3 columns):&#13;
 #   Column     Non-Null Count  Dtype         &#13;
---  ------     --------------  -----         &#13;
 0   timestamp  306 non-null    datetime64[ns]&#13;
 1   baseurl    584 non-null    object        &#13;
 2   content    584 non-null    object        &#13;
dtypes: datetime64[ns](1), object(2)&#13;
memory usage: 18.2+ KB&#13;
</pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<p>Nearly half of the timestamps are null. This feature will limit the dataset if we use it in the analysis. Let’s take a closer look at the <code>baseurl</code>, which represents the website that published the original article.</p>&#13;
&#13;
<section data-pdf-bookmark="Exploring the Publishers" data-type="sect2"><div class="sect2" id="exploring-the-publishers">&#13;
<h2>Exploring the Publishers</h2>&#13;
&#13;
<p>To understand the <code>baseurl</code> column, we start by counting the number of articles from each website:</p>&#13;
&#13;
<div class="cell docutils container">&#13;
<div class="cell_input docutils container">&#13;
<div class="highlight-ipython3 notranslate">&#13;
<div class="highlight">&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<span><code class="n">X_train</code></span><span><code class="p">[</code></span><span><code class="s1">'</code><code class="s1">baseurl</code><code class="s1">'</code></span><span><code class="p">]</code></span><span><code class="o">.</code></span><span><code class="n">value_counts</code></span><span><code class="p">(</code><code class="p">)</code></span><code>&#13;
</code></pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<div class="cell_output docutils container">&#13;
<div class="output text_plain highlight-myst-ansi notranslate">&#13;
<div class="highlight">&#13;
<pre data-type="programlisting">&#13;
baseurl&#13;
whitehouse.gov               21&#13;
abcnews.go.com               20&#13;
nytimes.com                  17&#13;
                             ..&#13;
occupydemocrats.com           1&#13;
legis.state.ak.us             1&#13;
dailynewsforamericans.com     1&#13;
Name: count, Length: 337, dtype: int64&#13;
</pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<p>Our train set has 584 rows, and we have found that there are 337 unique publishing websites. This means that the dataset includes many publications with only a few articles. A histogram of the number of articles published by each website confirms this:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<code class="n">fig</code><code> </code><code class="o">=</code><code> </code><code class="n">px</code><code class="o">.</code><code class="n">histogram</code><code class="p">(</code><code class="n">X_train</code><code class="p">[</code><code class="s1">'</code><code class="s1">baseurl</code><code class="s1">'</code><code class="p">]</code><code class="o">.</code><code class="n">value_counts</code><code class="p">(</code><code class="p">)</code><code class="p">,</code><code> </code><code class="n">width</code><code class="o">=</code><code class="mi">450</code><code class="p">,</code><code> </code><code class="n">height</code><code class="o">=</code><code class="mi">250</code><code class="p">,</code><code>&#13;
</code><code>                   </code><code class="n">labels</code><code class="o">=</code><code class="p">{</code><code class="s2">"</code><code class="s2">value</code><code class="s2">"</code><code class="p">:</code><code> </code><code class="s2">"</code><code class="s2">Number of articles published at a URL</code><code class="s2">"</code><code class="p">}</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><span><code class="n">fig</code></span><span><code class="o">.</code></span><span><code class="n">update_layout</code></span><span><code class="p">(</code></span><span><code class="n">showlegend</code></span><span><code class="o">=</code></span><span><code class="kc">False</code></span><span><code class="p">)</code></span><code>&#13;
</code></pre>&#13;
&#13;
<figure class="informal width-75"><div class="figure"><img src="assets/leds_21in01.png"/>&#13;
&#13;
</div></figure>&#13;
&#13;
<p>This histogram shows that the vast majority (261 out of 337) of websites have only one article in the train set, and only a few websites have more than five articles in the train set. Nonetheless, it can be informative to identify the websites that published the most fake or real articles. First, we find the websites that published the most fake <span class="keep-together">articles</span>:</p>&#13;
&#13;
<figure class="informal"><div class="figure"><img src="assets/leds_21in02.png"/>&#13;
&#13;
</div></figure>&#13;
&#13;
<p>Next, we list the websites that published the greatest number of real articles:</p>&#13;
&#13;
<figure class="informal"><div class="figure"><img src="assets/leds_21in03.png"/>&#13;
&#13;
</div></figure>&#13;
&#13;
<p>Only <code>cnn.com</code> and <code>washingtonpost.com</code> appear on both lists. Even without knowing the total number of articles for these sites, we might expect that an article from <code>yournewswire.com</code> is more likely to be labeled as <code>fake</code>, while an article from <code>whitehouse.gov</code> is more likely to be labeled as <code>real</code>. That said, we don’t expect that using the publishing website to predict article truthfulness would work very well; there are simply too few articles from most of the websites in the dataset.</p>&#13;
&#13;
<p>Next, let’s explore the <code>timestamp</code> column, which records the publication date of the news articles.</p>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Exploring Publication Date" data-type="sect2"><div class="sect2" id="exploring-publication-date">&#13;
<h2>Exploring Publication Date</h2>&#13;
&#13;
<p>Plotting the timestamps<a contenteditable="false" data-primary="times and dates" data-secondary="publication dates for fake news study" data-type="indexterm" id="ix_time_fake_news"/> on a histogram shows that most articles were published after 2000, although there seems to be at least one article published before 1940:</p>&#13;
&#13;
<div class="cell docutils container">&#13;
<div class="cell_input docutils container">&#13;
<div class="highlight-ipython3 notranslate">&#13;
<div class="highlight">&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<span><code class="n">fig</code></span><code> </code><span><code class="o">=</code></span><code> </code><span><code class="n">px</code></span><span><code class="o">.</code></span><span><code class="n">histogram</code></span><span><code class="p">(</code></span><code>&#13;
</code><code>    </code><span><code class="n">X_train</code></span><span><code class="p">[</code></span><span><code class="s2">"</code><code class="s2">timestamp</code><code class="s2">"</code></span><span><code class="p">]</code><code class="p">,</code></span><code>&#13;
</code><code>    </code><span><code class="n">labels</code></span><span><code class="o">=</code></span><span><code class="p">{</code></span><span><code class="s2">"</code><code class="s2">value</code><code class="s2">"</code></span><span><code class="p">:</code></span><code> </code><span><code class="s2">"</code><code class="s2">Publication year</code><code class="s2">"</code></span><span><code class="p">}</code><code class="p">,</code></span><code> </code><span><code class="n">width</code></span><span><code class="o">=</code></span><span><code class="mi">550</code></span><span><code class="p">,</code></span><code> </code><span><code class="n">height</code></span><span><code class="o">=</code></span><span><code class="mi">250</code></span><span><code class="p">,</code></span><code>&#13;
</code><span><code class="p">)</code></span><code>&#13;
</code><span><code class="n">fig</code></span><span><code class="o">.</code></span><span><code class="n">update_layout</code></span><span><code class="p">(</code></span><span><code class="n">showlegend</code></span><span><code class="o">=</code></span><span><code class="kc">False</code></span><span><code class="p">)</code></span><code>&#13;
</code></pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<figure class="informal"><div class="figure"><img src="assets/leds_21in04.png"/>&#13;
&#13;
</div></figure>&#13;
&#13;
<p>When we take a closer look at the news articles published prior to 2000, we find that the timestamps don’t match the actual publication date of the article. These date issues are most likely related to the web scraper collecting inaccurate information from the web pages. We can zoom into the region of the histogram after 2000:</p>&#13;
&#13;
<div class="cell docutils container">&#13;
<div class="cell_input docutils container">&#13;
<div class="highlight-ipython3 notranslate">&#13;
<div class="highlight">&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<span><code class="n">fig</code></span><code> </code><span><code class="o">=</code></span><code> </code><span><code class="n">px</code></span><span><code class="o">.</code></span><span><code class="n">histogram</code></span><span><code class="p">(</code></span><code>&#13;
</code><code>    </code><span><code class="n">X_train</code></span><span><code class="o">.</code></span><span><code class="n">loc</code></span><span><code class="p">[</code></span><span><code class="n">X_train</code></span><span><code class="p">[</code></span><span><code class="s2">"</code><code class="s2">timestamp</code><code class="s2">"</code></span><span><code class="p">]</code></span><code> </code><span><code class="o">&gt;</code></span><code> </code><span><code class="s2">"</code><code class="s2">2000</code><code class="s2">"</code></span><span><code class="p">,</code></span><code> </code><span><code class="s2">"</code><code class="s2">timestamp</code><code class="s2">"</code></span><span><code class="p">]</code><code class="p">,</code></span><code>&#13;
</code><code>    </code><span><code class="n">labels</code></span><span><code class="o">=</code></span><span><code class="p">{</code></span><span><code class="s2">"</code><code class="s2">value</code><code class="s2">"</code></span><span><code class="p">:</code></span><code> </code><span><code class="s2">"</code><code class="s2">Publication year</code><code class="s2">"</code></span><span><code class="p">}</code><code class="p">,</code></span><code> </code><span><code class="n">width</code></span><span><code class="o">=</code></span><span><code class="mi">550</code></span><span><code class="p">,</code></span><code> </code><span><code class="n">height</code></span><span><code class="o">=</code></span><span><code class="mi">250</code></span><span><code class="p">,</code></span><code> </code><code>&#13;
</code><span><code class="p">)</code></span><code>&#13;
</code><span><code class="n">fig</code></span><span><code class="o">.</code></span><span><code class="n">update_layout</code></span><span><code class="p">(</code></span><span><code class="n">showlegend</code></span><span><code class="o">=</code></span><span><code class="kc">False</code></span><span><code class="p">)</code></span><code>&#13;
</code></pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<figure class="informal"><div class="figure"><img src="assets/leds_21in05.png"/>&#13;
&#13;
</div></figure>&#13;
&#13;
<p>As expected, most of the articles were published between 2007 (the year Politifact was founded) and 2020 (the year the FakeNewsNet repository was published). But we also find that the timestamps are concentrated on the years 2016 to 2018—the year of the controversial 2016 US presidential election and the two years following. This insight is a further caution on the limitation of our analysis to carry over to nonelection years.</p>&#13;
&#13;
<p>Our main aim is to use the text content for classification. We explore some word frequencies next<a contenteditable="false" data-primary="" data-startref="ix_time_fake_news" data-type="indexterm" id="id1874"/>.</p>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Exploring Words in Articles" data-type="sect2"><div class="sect2" id="exploring-words-in-articles">&#13;
<h2>Exploring Words in Articles</h2>&#13;
&#13;
<p>We’d like to see whether there’s a relationship<a contenteditable="false" data-primary="text" data-secondary="fake news detection analysis" data-type="indexterm" id="ix_text_fake_news"/> between the words used in the articles and whether the article was labeled as <code>fake</code>. One simple way to do this is to look at individual words like <em>military</em>, then count how many articles that mentioned “military” were labeled <code>fake</code>. For <em>military</em> to be useful, the articles that mention it should have a much higher or much lower fraction of fake articles than 45% (the proportion of fake articles in the dataset: 264/584).</p>&#13;
&#13;
<p>We can use our domain knowledge of political topics to pick out a few candidate words to explore:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<code class="n">word_features</code> <code class="o">=</code> <code class="p">[</code>&#13;
    <code class="c1"># names of presidential candidates</code>&#13;
    <code class="s1">'trump'</code><code class="p">,</code> <code class="s1">'clinton'</code><code class="p">,</code>&#13;
    <code class="c1"># congress words</code>&#13;
    <code class="s1">'state'</code><code class="p">,</code> <code class="s1">'vote'</code><code class="p">,</code> <code class="s1">'congress'</code><code class="p">,</code> <code class="s1">'shutdown'</code><code class="p">,</code>&#13;
    &#13;
    <code class="c1"># other possibly useful words</code>&#13;
    <code class="s1">'military'</code><code class="p">,</code> <code class="s1">'princ'</code><code class="p">,</code> <code class="s1">'investig'</code><code class="p">,</code> <code class="s1">'antifa'</code><code class="p">,</code> &#13;
    <code class="s1">'joke'</code><code class="p">,</code> <code class="s1">'homeless'</code><code class="p">,</code> <code class="s1">'swamp'</code><code class="p">,</code> <code class="s1">'cnn'</code><code class="p">,</code> <code class="s1">'the'</code>&#13;
<code class="p">]</code>&#13;
</pre>&#13;
&#13;
<p>Then we define a function that creates a new feature for each word, where the feature contains <code>True</code> if the word appeared in the article and <code>False</code> if not:</p>&#13;
&#13;
<div class="cell docutils container">&#13;
<div class="cell_input docutils container">&#13;
<div class="highlight-ipython3 notranslate">&#13;
<div class="highlight">&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<span><code class="k">def</code></span><code> </code><span><code class="nf">make_word_features</code></span><span><code class="p">(</code></span><span><code class="n">df</code></span><span><code class="p">,</code></span><code> </code><span><code class="n">words</code></span><span><code class="p">)</code><code class="p">:</code></span><code>&#13;
</code><code>    </code><span><code class="n">features</code></span><code> </code><span><code class="o">=</code></span><code> </code><span><code class="p">{</code></span><code> </code><span><code class="n">word</code></span><span><code class="p">:</code></span><code> </code><span><code class="n">df</code></span><span><code class="p">[</code></span><span><code class="s1">'</code><code class="s1">content</code><code class="s1">'</code></span><span><code class="p">]</code></span><span><code class="o">.</code></span><span><code class="n">str</code></span><span><code class="o">.</code></span><span><code class="n">contains</code></span><span><code class="p">(</code></span><span><code class="n">word</code></span><span><code class="p">)</code></span><code> </code><span><code class="k">for</code></span><code> </code><span><code class="n">word</code></span><code> </code><span><code class="ow">in</code></span><code> </code><span><code class="n">words</code></span><code> </code><span><code class="p">}</code></span><code>&#13;
</code><code>    </code><span><code class="k">return</code></span><code> </code><span><code class="n">pd</code></span><span><code class="o">.</code></span><span><code class="n">DataFrame</code></span><span><code class="p">(</code></span><span><code class="n">features</code></span><span><code class="p">)</code></span><code>&#13;
</code></pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<p>This is like one-hot encoding for the presence of a word (see <a class="reference internal" data-type="xref" href="ch15.html#ch-linear">Chapter 15</a>). We can use this function to further wrangle our data and create a new dataframe with a feature for each of our chosen words:</p>&#13;
&#13;
<div class="cell docutils container">&#13;
<div class="cell_input docutils container">&#13;
<div class="highlight-ipython3 notranslate">&#13;
<div class="highlight">&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<span><code class="n">df_words</code></span><code> </code><span><code class="o">=</code></span><code> </code><span><code class="n">make_word_features</code></span><span><code class="p">(</code></span><span><code class="n">X_train</code></span><span><code class="p">,</code></span><code> </code><span><code class="n">word_features</code></span><span><code class="p">)</code></span><code>&#13;
</code><span><code class="n">df_words</code></span><span><code class="p">[</code></span><span><code class="s2">"</code><code class="s2">label</code><code class="s2">"</code></span><span><code class="p">]</code></span><code> </code><span><code class="o">=</code></span><code> </code><span><code class="n">df</code></span><span><code class="p">[</code></span><span><code class="s2">"</code><code class="s2">label</code><code class="s2">"</code></span><span><code class="p">]</code></span><code>&#13;
</code></pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<div class="cell docutils container">&#13;
<div class="cell_input docutils container">&#13;
<div class="highlight-ipython3 notranslate">&#13;
<div class="highlight">&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<span><code class="n">df_words</code></span><span><code class="o">.</code></span><span><code class="n">shape</code></span><code>&#13;
</code></pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<div class="cell_output docutils container">&#13;
<div class="output text_plain highlight-myst-ansi notranslate">&#13;
<div class="highlight">&#13;
<pre data-type="programlisting">&#13;
(584, 16)&#13;
</pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<div class="cell docutils container">&#13;
<div class="cell_input docutils container">&#13;
<div class="highlight-ipython3 notranslate">&#13;
<div class="highlight">&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<span><code class="n">df_words</code></span><span><code class="o">.</code></span><span><code class="n">head</code></span><span><code class="p">(</code></span><span><code class="mi">4</code></span><span><code class="p">)</code></span><code>&#13;
</code></pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<div class="cell_output docutils container">&#13;
<div class="output text_html">&#13;
<div>&#13;
<table class="dataframe pagebreak-before less_space">&#13;
	<thead>&#13;
		<tr>&#13;
			<th> </th>&#13;
			<th>trump</th>&#13;
			<th>clinton</th>&#13;
			<th>state</th>&#13;
			<th>vote</th>&#13;
			<th>...</th>&#13;
			<th>swamp</th>&#13;
			<th>cnn</th>&#13;
			<th>the</th>&#13;
			<th>label</th>&#13;
		</tr>&#13;
	</thead>&#13;
	<tbody>&#13;
		<tr>&#13;
			<td><strong>164</strong></td>&#13;
			<td>False</td>&#13;
			<td>False</td>&#13;
			<td>True</td>&#13;
			<td>False</td>&#13;
			<td>...</td>&#13;
			<td>False</td>&#13;
			<td>False</td>&#13;
			<td>True</td>&#13;
			<td>1</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td><strong>28</strong></td>&#13;
			<td>False</td>&#13;
			<td>False</td>&#13;
			<td>False</td>&#13;
			<td>False</td>&#13;
			<td>...</td>&#13;
			<td>False</td>&#13;
			<td>False</td>&#13;
			<td>True</td>&#13;
			<td>1</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td><strong>708</strong></td>&#13;
			<td>False</td>&#13;
			<td>False</td>&#13;
			<td>True</td>&#13;
			<td>True</td>&#13;
			<td>...</td>&#13;
			<td>False</td>&#13;
			<td>False</td>&#13;
			<td>True</td>&#13;
			<td>0</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td><strong>193</strong></td>&#13;
			<td>False</td>&#13;
			<td>False</td>&#13;
			<td>False</td>&#13;
			<td>False</td>&#13;
			<td>...</td>&#13;
			<td>False</td>&#13;
			<td>False</td>&#13;
			<td>True</td>&#13;
			<td>1</td>&#13;
		</tr>&#13;
	</tbody>&#13;
</table>&#13;
&#13;
<pre>4 rows × 16 columns</pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<p>Now we can find the proportion of these articles that were labeled <code>fake</code>. We visualize these calculations in the following plots. In the left plot, we mark the proportion of <code>fake</code> articles in the entire train set using a dotted line, which helps us understand how informative each word feature is—a highly informative word will have a point that lies far away from the line:</p>&#13;
&#13;
<figure class="informal"><div class="figure"><img src="assets/leds_21in06.png"/>&#13;
&#13;
</div></figure>&#13;
&#13;
<p>This plot reveals a few interesting considerations for modeling. For example, notice that the word <em>antifa</em> is highly predictive—all articles that mention the word <em>antifa</em> are labeled <code>fake</code>. However, <em>antifa</em> only appears in a few articles. On the other hand, the word <em>the</em> appears in nearly every article, but is uninformative for distinguishing between <code>real</code> and <code>fake</code> articles because the proportion of articles with <em>the</em> that are fake matches the proportion of fake articles overall. We might instead do better with a word like <em>vote</em>, which is predictive and appears in many news articles.</p>&#13;
&#13;
<p>This exploratory analysis brought us understanding of the time frame that our news articles were published in, the broad range of publishing websites captured in the data, and candidate words to use for prediction. Next, we fit models for predicting whether articles are fake or real<a contenteditable="false" data-primary="" data-startref="ix_text_fake_news" data-type="indexterm" id="id1875"/><a contenteditable="false" data-startref="ix_fake_news_expl" data-type="indexterm" id="id1876"/><a contenteditable="false" data-startref="ix_eda_fake_news" data-type="indexterm" id="id1877"/>.</p>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Modeling" data-type="sect1"><div class="sect1" id="sec-fake-news-modeling">&#13;
<h1>Modeling</h1>&#13;
&#13;
<p>Now that we’ve obtained, cleaned, and explored our data<a contenteditable="false" data-primary="fake news detection case study" data-secondary="modeling" data-type="indexterm" id="ix_fake_news_mod"/><a contenteditable="false" data-primary="modeling" data-secondary="fake news detection" data-type="indexterm" id="ix_mod_fake_news"/>, let’s fit models to predict whether articles are real or fake. In this section, we use logistic regression because we have a binary classification problem. We fit three different models that increase in complexity. First, we fit a model that just uses the presence of a single handpicked word in the document as an explanatory feature. Then we fit a model that uses multiple handpicked words. Finally, we fit a model that uses all the words in the train set, vectorized using the tf-idf transform (introduced in <a class="reference internal" data-type="xref" href="ch13.html#ch-text">Chapter 13</a>). Let’s start with the simple single-word model.</p>&#13;
&#13;
<section data-pdf-bookmark="A Single-Word Model" data-type="sect2"><div class="sect2" id="a-single-word-model">&#13;
<h2>A Single-Word Model</h2>&#13;
&#13;
<p>Our EDA showed<a contenteditable="false" data-primary="text" data-secondary="fake news detection analysis" data-type="indexterm" id="ix_fake_news_mod1"/><a contenteditable="false" data-primary="text" data-secondary="fake news detection analysis" data-type="indexterm" id="ix_text_fake_news2"/> that the word <em>vote</em> is related to whether an article is labeled <code>real</code> or <code>fake</code>. To test this, we fit a logistic regression model using a single binary feature: <code>1</code> if the word <em>vote</em> appears in the article and <code>0</code> if not. We start by defining a function to lowercase the article content:</p>&#13;
&#13;
<div class="cell docutils container">&#13;
<div class="cell_input docutils container">&#13;
<div class="highlight-ipython3 notranslate">&#13;
<div class="highlight">&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<span><code class="k">def</code></span><code> </code><span><code class="nf">lowercase</code></span><span><code class="p">(</code></span><span><code class="n">df</code></span><span><code class="p">)</code><code class="p">:</code></span><code>&#13;
</code><code>    </code><span><code class="k">return</code></span><code> </code><span><code class="n">df</code></span><span><code class="o">.</code></span><span><code class="n">assign</code></span><span><code class="p">(</code></span><span><code class="n">content</code></span><span><code class="o">=</code></span><span><code class="n">df</code></span><span><code class="p">[</code></span><span><code class="s1">'</code><code class="s1">content</code><code class="s1">'</code></span><span><code class="p">]</code></span><span><code class="o">.</code></span><span><code class="n">str</code></span><span><code class="o">.</code></span><span><code class="n">lower</code></span><span><code class="p">(</code><code class="p">)</code><code class="p">)</code></span><code>&#13;
</code></pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<p>For our first classifier, we only use the word <em>vote</em>:</p>&#13;
&#13;
<div class="cell docutils container">&#13;
<div class="cell_input docutils container">&#13;
<div class="highlight-ipython3 notranslate">&#13;
<div class="highlight">&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<span><code class="n">one_word</code></span><code> </code><span><code class="o">=</code></span><code> </code><span><code class="p">[</code></span><span><code class="s1">'</code><code class="s1">vote</code><code class="s1">'</code></span><span><code class="p">]</code></span><code>&#13;
</code></pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<p>We can chain the <code>lowercase</code> function and the function <code>make_word_features</code> from our EDA into a <code>scikit-learn</code> pipeline. This provides a convenient way to transform and fit data all at once:</p>&#13;
&#13;
<div class="cell docutils container">&#13;
<div class="cell_input docutils container">&#13;
<div class="highlight-ipython3 notranslate">&#13;
<div class="highlight">&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<span><code class="kn">from</code></span><code> </code><span><code class="nn">sklearn</code><code class="nn">.</code><code class="nn">pipeline</code></span><code> </code><span><code class="kn">import</code></span><code> </code><span><code class="n">make_pipeline</code></span><code>&#13;
</code><span><code class="kn">from</code></span><code> </code><span><code class="nn">sklearn</code><code class="nn">.</code><code class="nn">linear_model</code></span><code> </code><span><code class="kn">import</code></span><code> </code><span><code class="n">LogisticRegressionCV</code></span><code>&#13;
</code><span><code class="kn">from</code></span><code> </code><span><code class="nn">sklearn</code><code class="nn">.</code><code class="nn">preprocessing</code></span><code> </code><span><code class="kn">import</code></span><code> </code><span><code class="n">FunctionTransformer</code></span><code>&#13;
</code></pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<div class="cell docutils container">&#13;
<div class="cell_input docutils container">&#13;
<div class="highlight-ipython3 notranslate">&#13;
<div class="highlight">&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<span><code class="n">model1</code></span><code> </code><span><code class="o">=</code></span><code> </code><span><code class="n">make_pipeline</code></span><span><code class="p">(</code></span><code>&#13;
</code><code>    </code><span><code class="n">FunctionTransformer</code></span><span><code class="p">(</code></span><span><code class="n">lowercase</code></span><span><code class="p">)</code><code class="p">,</code></span><code>&#13;
</code><code>    </code><span><code class="n">FunctionTransformer</code></span><span><code class="p">(</code></span><span><code class="n">make_word_features</code></span><span><code class="p">,</code></span><code> </code><span><code class="n">kw_args</code></span><span><code class="o">=</code></span><span><code class="p">{</code></span><span><code class="s1">'</code><code class="s1">words</code><code class="s1">'</code></span><span><code class="p">:</code></span><code> </code><span><code class="n">one_word</code></span><span><code class="p">}</code><code class="p">)</code><code class="p">,</code></span><code>&#13;
</code><code>    </code><span><code class="n">LogisticRegressionCV</code></span><span><code class="p">(</code></span><span><code class="n">Cs</code></span><span><code class="o">=</code></span><span><code class="mi">10</code></span><span><code class="p">,</code></span><code> </code><span><code class="n">solver</code></span><span><code class="o">=</code></span><span><code class="s1">'</code><code class="s1">saga</code><code class="s1">'</code></span><span><code class="p">,</code></span><code> </code><span><code class="n">n_jobs</code></span><span><code class="o">=</code></span><span><code class="mi">4</code></span><span><code class="p">,</code></span><code> </code><span><code class="n">max_iter</code></span><span><code class="o">=</code></span><span><code class="mi">10000</code></span><span><code class="p">)</code><code class="p">,</code></span><code>&#13;
</code><span><code class="p">)</code></span><code>&#13;
</code></pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<p>When used, the preceding pipeline<a contenteditable="false" data-primary="regularization" data-type="indexterm" id="id1878"/><a contenteditable="false" data-primary="L₂ regularization" data-type="indexterm" id="id1879"/><a contenteditable="false" data-primary="LogisticRegressionCV" data-type="indexterm" id="id1880"/> converts the characters in the article content to lowercase, creates a dataframe with a binary feature for each word of interest, and fits a logistic regression model on the data using <span class="math notranslate nohighlight"><math> <msub> <mi>L</mi> <mn>2</mn> </msub> </math></span> regularization. Additionally, the <code>LogisticRegressionCV</code> function uses cross-validation (fivefold by default) to select the best regularization parameter. (See <a class="reference internal" data-type="xref" href="ch16.html#ch-risk">Chapter 16</a> for more on regularization and cross-validation.)</p>&#13;
&#13;
<p class="pagebreak-before less_space">Let’s use the pipeline to fit the training data:</p>&#13;
&#13;
<div class="cell docutils container">&#13;
<div class="cell_input docutils container">&#13;
<div class="highlight-ipython3 notranslate">&#13;
<div class="highlight">&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<span><code class="o">%</code><code class="o">%</code><code class="n">time</code></span><code>&#13;
</code><code>&#13;
</code><span><code class="n">model1</code></span><span><code class="o">.</code></span><span><code class="n">fit</code></span><span><code class="p">(</code></span><span><code class="n">X_train</code></span><span><code class="p">,</code></span><code> </code><span><code class="n">y_train</code></span><span><code class="p">)</code></span><code>&#13;
</code><span><code class="nb">print</code></span><span><code class="p">(</code></span><span><code class="sa">f</code></span><span><code class="s1">'</code></span><span><code class="si">{</code></span><span><code class="n">model1</code></span><span><code class="o">.</code></span><span><code class="n">score</code></span><span><code class="p">(</code></span><span><code class="n">X_train</code></span><span><code class="p">,</code></span><code> </code><span><code class="n">y_train</code></span><span><code class="p">)</code></span><span><code class="si">:</code></span><span><code class="s1">.1%</code></span><span><code class="si">}</code></span><span><code class="s1"> accuracy on training set.</code><code class="s1">'</code></span><span><code class="p">)</code></span><code>&#13;
</code></pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<div class="cell_output docutils container">&#13;
<div class="output stream highlight-myst-ansi notranslate">&#13;
<div class="highlight">&#13;
<pre data-type="programlisting">&#13;
64.9% accuracy on training set.&#13;
CPU times: user 110 ms, sys: 42.7 ms, total: 152 ms&#13;
Wall time: 144 ms&#13;
</pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<p>Overall, the single-word classifier only classifies 65% of articles correctly. <a contenteditable="false" data-primary="confusion matrix" data-type="indexterm" id="ix_conv_matrix_ch21"/>We plot the confusion matrix of the classifier on the train set to see what kinds of mistakes it makes:</p>&#13;
&#13;
<figure class="informal width-40"><div class="figure"><img src="assets/leds_21in07.png"/>&#13;
&#13;
</div></figure>&#13;
&#13;
<p>Our model often misclassifies real articles (0) as fake (1). Since this model is simple, we can take a look at the probabilities for the two cases: the word <em>vote</em> is in the article or is not:</p>&#13;
&#13;
<div class="cell tag_hide-input docutils container">&#13;
<div class="cell_output docutils container">&#13;
<div class="output stream highlight-myst-ansi notranslate">&#13;
<div class="highlight">&#13;
<pre data-type="programlisting">&#13;
"vote" present: [[0.72 0.28]]&#13;
"vote" absent: [[0.48 0.52]]&#13;
</pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<p>When an article contains the word <em>vote</em>, the model gives a high probability of the article being real, and when <em>vote</em> is absent, the probability leans slightly toward the article being fake. We encourage readers to verify this for themselves using the definition of the logistic regression model and the fitted coefficients:</p>&#13;
&#13;
<div class="cell docutils container">&#13;
<div class="cell_input docutils container">&#13;
<div class="highlight-ipython3 notranslate">&#13;
<div class="highlight">&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<span><code class="nb">print</code></span><span><code class="p">(</code></span><span><code class="sa">f</code></span><span><code class="s1">'</code><code class="s1">Intercept: </code></span><span><code class="si">{</code></span><span><code class="n">log_reg</code></span><span><code class="o">.</code></span><span><code class="n">intercept_</code></span><span><code class="p">[</code></span><span><code class="mi">0</code></span><span><code class="p">]</code></span><span><code class="si">:</code></span><span><code class="s1">.2f</code></span><span><code class="si">}</code></span><span><code class="s1">'</code></span><span><code class="p">)</code></span><code>&#13;
</code><span><code class="p">[</code><code class="p">[</code></span><span><code class="n">coef</code></span><span><code class="p">]</code><code class="p">]</code></span><code> </code><span><code class="o">=</code></span><code> </code><span><code class="n">log_reg</code></span><span><code class="o">.</code></span><span><code class="n">coef_</code></span><code>&#13;
</code><span><code class="nb">print</code></span><span><code class="p">(</code></span><span><code class="sa">f</code></span><span><code class="s1">'</code><code class="s1">"</code><code class="s1">vote</code><code class="s1">"</code><code class="s1"> Coefficient: </code></span><span><code class="si">{</code></span><span><code class="n">coef</code></span><span><code class="si">:</code></span><span><code class="s1">.2f</code></span><span><code class="si">}</code></span><span><code class="s1">'</code></span><span><code class="p">)</code></span><code>&#13;
</code></pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<div class="cell_output docutils container">&#13;
<div class="output stream highlight-myst-ansi notranslate">&#13;
<div class="highlight">&#13;
<pre data-type="programlisting">&#13;
Intercept: 0.08&#13;
"vote" Coefficient: -1.00&#13;
</pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<p class="pagebreak-before less_space">As we saw in <a class="reference internal" data-type="xref" href="ch19.html#ch-logistic">Chapter 19</a>, the coefficient indicates the size of the change in the odds with a change in the explanatory variable. With a 0-1 variable like the presence or absence of a word in an article, this has a particularly intuitive meaning. For an article with <em>vote</em> in it, the odds of being fake decrease by a factor of <span class="math notranslate nohighlight"><math> <mi>exp</mi> <mo>⁡</mo> <mo stretchy="false">(</mo> <msub> <mi>θ</mi> <mrow> <mi>v</mi> <mi>o</mi> <mi>t</mi> <mi>e</mi> </mrow> </msub> <mo stretchy="false">)</mo> </math></span>, which is:</p>&#13;
&#13;
<div class="cell docutils container">&#13;
<div class="cell_input docutils container">&#13;
<div class="highlight-ipython3 notranslate">&#13;
<div class="highlight">&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<span><code class="n">np</code></span><span><code class="o">.</code></span><span><code class="n">exp</code></span><span><code class="p">(</code></span><span><code class="n">coef</code></span><span><code class="p">)</code></span><code> </code><code>&#13;
</code></pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<div class="cell_output docutils container">&#13;
<div class="output text_plain highlight-myst-ansi notranslate">&#13;
<div class="highlight">&#13;
<pre data-type="programlisting">&#13;
0.36836305405149367&#13;
</pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>Remember that in this modeling scenario, a label of <code>0</code> corresponds to a real article and a label of <code>1</code> corresponds to a fake article. This might seem a bit counterintuitive—we’re saying that a “true positive” is when a model correctly predicts a fake article as fake. In binary classification, we typically say a “positive” result is the one with the presence of something unusual. For example, a person who tests positive for an illness would expect to have the illness.</p>&#13;
</div>&#13;
&#13;
<p>Let’s make our model a bit more sophisticated by introducing additional word <span class="keep-together">features</span>.</p>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Multiple-Word Model" data-type="sect2"><div class="sect2" id="multiple-word-model">&#13;
<h2>Multiple-Word Model</h2>&#13;
&#13;
<p>We create a model that uses all of the words we examined in our EDA of the train set, except for <em>the</em>. Let’s fit a model using these 15 features:</p>&#13;
&#13;
<div class="cell docutils container">&#13;
<div class="cell_input docutils container">&#13;
<div class="highlight-ipython3 notranslate">&#13;
<div class="highlight">&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<span><code class="n">model2</code></span><code> </code><span><code class="o">=</code></span><code> </code><span><code class="n">make_pipeline</code></span><span><code class="p">(</code></span><code>&#13;
</code><code>    </code><span><code class="n">FunctionTransformer</code></span><span><code class="p">(</code></span><span><code class="n">lowercase</code></span><span><code class="p">)</code><code class="p">,</code></span><code>&#13;
</code><code>    </code><span><code class="n">FunctionTransformer</code></span><span><code class="p">(</code></span><span><code class="n">make_word_features</code></span><span><code class="p">,</code></span><code> </code><span><code class="n">kw_args</code></span><span><code class="o">=</code></span><span><code class="p">{</code></span><span><code class="s1">'</code><code class="s1">words</code><code class="s1">'</code></span><span><code class="p">:</code></span><code> </code><span><code class="n">word_features</code></span><span><code class="p">}</code><code class="p">)</code><code class="p">,</code></span><code>&#13;
</code><code>    </code><span><code class="n">LogisticRegressionCV</code></span><span><code class="p">(</code></span><span><code class="n">Cs</code></span><span><code class="o">=</code></span><span><code class="mi">10</code></span><span><code class="p">,</code></span><code> </code><span><code class="n">solver</code></span><span><code class="o">=</code></span><span><code class="s1">'</code><code class="s1">saga</code><code class="s1">'</code></span><span><code class="p">,</code></span><code> </code><span><code class="n">n_jobs</code></span><span><code class="o">=</code></span><span><code class="mi">4</code></span><span><code class="p">,</code></span><code> </code><span><code class="n">max_iter</code></span><span><code class="o">=</code></span><span><code class="mi">10000</code></span><span><code class="p">)</code><code class="p">,</code></span><code>&#13;
</code><span><code class="p">)</code></span><code>&#13;
</code></pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<div class="cell docutils container">&#13;
<div class="cell_input docutils container">&#13;
<div class="highlight-ipython3 notranslate">&#13;
<div class="highlight">&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<span><code class="o">%</code><code class="o">%</code><code class="n">time</code></span><code>&#13;
</code><code>&#13;
</code><span><code class="n">model2</code></span><span><code class="o">.</code></span><span><code class="n">fit</code></span><span><code class="p">(</code></span><span><code class="n">X_train</code></span><span><code class="p">,</code></span><code> </code><span><code class="n">y_train</code></span><span><code class="p">)</code></span><code>&#13;
</code><span><code class="nb">print</code></span><span><code class="p">(</code></span><span><code class="sa">f</code></span><span><code class="s1">'</code></span><span><code class="si">{</code></span><span><code class="n">model2</code></span><span><code class="o">.</code></span><span><code class="n">score</code></span><span><code class="p">(</code></span><span><code class="n">X_train</code></span><span><code class="p">,</code></span><code> </code><span><code class="n">y_train</code></span><span><code class="p">)</code></span><span><code class="si">:</code></span><span><code class="s1">.1%</code></span><span><code class="si">}</code></span><span><code class="s1"> accuracy on training set.</code><code class="s1">'</code></span><span><code class="p">)</code></span><code>&#13;
</code></pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<div class="cell_output docutils container">&#13;
<div class="output stream highlight-myst-ansi notranslate">&#13;
<div class="highlight">&#13;
<pre data-type="programlisting">&#13;
74.8% accuracy on training set.&#13;
CPU times: user 1.54 s, sys: 59.1 ms, total: 1.6 s&#13;
Wall time: 637 ms&#13;
</pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<p>This model is about 10 percentage points more accurate than the one-word model. It may seem a bit surprising that going from a one-word model to a 15-word model only gains 10 percentage points. The confusion matrix is helpful in teasing out the kinds of errors made:</p>&#13;
&#13;
<figure class="informal width-40"><div class="figure"><img src="assets/leds_21in08.png"/>&#13;
&#13;
</div></figure>&#13;
&#13;
<p>We can see that this classifier does a better job of classifying real articles accurately. However, it makes more mistakes than the simple one-word model when classifying fake article—59 of the fake articles were classified as real. In this scenario, we might be more concerned about misclassifying an article as fake when it is real. So we wish to have a high precision—the ratio of fake articles correctly predicted as fake to articles predicted as fake:</p>&#13;
&#13;
<div class="cell docutils container">&#13;
<div class="cell_input docutils container">&#13;
<div class="highlight-ipython3 notranslate">&#13;
<div class="highlight">&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<span><code class="n">model1_precision</code></span><code> </code><span><code class="o">=</code></span><code> </code><span><code class="mi">238</code></span><code> </code><span><code class="o">/</code></span><code> </code><span><code class="p">(</code></span><span><code class="mi">238</code></span><code> </code><span><code class="o">+</code></span><code> </code><span><code class="mi">179</code></span><span><code class="p">)</code></span><code>&#13;
</code><span><code class="n">model2_precision</code></span><code> </code><span><code class="o">=</code></span><code> </code><span><code class="mi">205</code></span><code> </code><span><code class="o">/</code></span><code> </code><span><code class="p">(</code></span><span><code class="mi">205</code></span><code> </code><span><code class="o">+</code></span><code> </code><span><code class="mi">88</code></span><span><code class="p">)</code></span><code>&#13;
</code><code>&#13;
</code><span><code class="p">[</code></span><span><code class="nb">round</code></span><span><code class="p">(</code></span><span><code class="n">num</code></span><span><code class="p">,</code></span><code> </code><span><code class="mi">2</code></span><span><code class="p">)</code></span><code> </code><span><code class="k">for</code></span><code> </code><span><code class="n">num</code></span><code> </code><span><code class="ow">in</code></span><code> </code><span><code class="p">[</code></span><span><code class="n">model1_precision</code></span><span><code class="p">,</code></span><code> </code><span><code class="n">model2_precision</code></span><span><code class="p">]</code><code class="p">]</code></span><code>&#13;
</code></pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<div class="cell_output docutils container">&#13;
<div class="output text_plain highlight-myst-ansi notranslate">&#13;
<div class="highlight">&#13;
<pre data-type="programlisting">&#13;
[0.57, 0.7]&#13;
</pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<p>The precision in our larger model is improved, but about 30% of the articles labeled as fake are actually real. Let’s take a look at the model’s coefficients:</p>&#13;
&#13;
<figure class="informal width-60"><div class="figure"><img src="assets/leds_21in09.png"/>&#13;
&#13;
</div></figure>&#13;
&#13;
<p>We can make a quick interpretation of the coefficients by looking at their signs. The large positive values on <em>trump</em> and <em>investig</em> indicate that the model predicts that new articles containing these words have a higher probability of being fake. The reverse is true for words like <em>congress</em> and <em>vote</em>, which have negative weights. We can use these coefficients to compare the log odds when an article does or does not contain a particular word<a contenteditable="false" data-primary="" data-startref="ix_conv_matrix_ch21" data-type="indexterm" id="id1881"/>.</p>&#13;
&#13;
<p>Although this larger model performs better than the simple one-word model, we had to handpick the word features using our knowledge of the news. What if we missed the words that are highly predictive? To address this, we can incorporate all the words in the articles using the tf-idf transform<a contenteditable="false" data-primary="" data-startref="ix_fake_news_mod" data-type="indexterm" id="id1882"/>.</p>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Predicting with the tf-idf Transform" data-type="sect2"><div class="sect2" id="predicting-with-the-tf-idf-transform">&#13;
<h2>Predicting with the tf-idf Transform</h2>&#13;
&#13;
<p>For the third and final model<a contenteditable="false" data-primary="feature engineering" data-secondary="tf-idf transform for predicting fake news" data-type="indexterm" id="ix_feat_engin_tfidf"/><a contenteditable="false" data-primary="normalization" data-secondary="and tf-idf transform" data-type="indexterm" id="id1883"/><a contenteditable="false" data-primary="transformations" data-secondary="and normalization" data-secondary-sortas="normalization" data-type="indexterm" id="id1884"/><a contenteditable="false" data-primary="TFidfVectorizer" data-type="indexterm" id="id1885"/><a contenteditable="false" data-primary="features and feature types" data-secondary="feature engineering" data-type="indexterm" id="ix_feat_feat_engin4"/><a contenteditable="false" data-primary="term frequency-inverse document frequency (tf-idf)" data-type="indexterm" id="ix_tfidf_spout_transf"/><a contenteditable="false" data-primary="tf-idf transform" data-type="indexterm" id="ix_tfidf_transf"/>, we use the term frequency-inverse document frequency (tf-idf) transform from <a class="reference internal" data-type="xref" href="ch13.html#ch-text">Chapter 13</a> to vectorize the entire text of all articles in the train set. Recall that with this transform, an article is converted into a vector with one element for each word that appears in any of the 564 articles. The vector consists of normalized counts of the number of times the word appears in the article normalized by the rareness of the word. The tf-idf puts more weight on words that only appear in a few documents. This means that our classifier uses all the words in the train set’s news articles for prediction. As we’ve done previously when we introduced tf-idf, first we remove stopwords, then we tokenize the words, and then we use the <span class="keep-together"><code>TfidfVectorizer</code></span> from <code>scikit-learn</code>:</p>&#13;
&#13;
<div class="cell docutils container">&#13;
<div class="cell_input docutils container">&#13;
<div class="highlight-ipython3 notranslate">&#13;
<div class="highlight">&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<span><code class="n">tfidf</code></span><code> </code><span><code class="o">=</code></span><code> </code><span><code class="n">TfidfVectorizer</code></span><span><code class="p">(</code></span><span><code class="n">tokenizer</code></span><span><code class="o">=</code></span><span><code class="n">stemming_tokenizer</code></span><span><code class="p">,</code></span><code> </code><span><code class="n">token_pattern</code></span><span><code class="o">=</code></span><span><code class="kc">None</code></span><span><code class="p">)</code></span><code>&#13;
</code></pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<div class="cell docutils container">&#13;
<div class="cell_input docutils container">&#13;
<div class="highlight-ipython3 notranslate">&#13;
<div class="highlight">&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<span><code class="kn">from</code></span><code> </code><span><code class="nn">sklearn</code><code class="nn">.</code><code class="nn">compose</code></span><code> </code><span><code class="kn">import</code></span><code> </code><span><code class="n">make_column_transformer</code></span><code>&#13;
</code><code>&#13;
</code><span><code class="n">model3</code></span><code> </code><span><code class="o">=</code></span><code> </code><span><code class="n">make_pipeline</code></span><span><code class="p">(</code></span><code>&#13;
</code><code>    </code><span><code class="n">FunctionTransformer</code></span><span><code class="p">(</code></span><span><code class="n">lowercase</code></span><span><code class="p">)</code><code class="p">,</code></span><code>&#13;
</code><code>    </code><span><code class="n">make_column_transformer</code></span><span><code class="p">(</code><code class="p">(</code></span><span><code class="n">tfidf</code></span><span><code class="p">,</code></span><code> </code><span><code class="s1">'</code><code class="s1">content</code><code class="s1">'</code></span><span><code class="p">)</code><code class="p">)</code><code class="p">,</code></span><code>&#13;
</code><code>    </code><span><code class="n">LogisticRegressionCV</code></span><span><code class="p">(</code></span><span><code class="n">Cs</code></span><span><code class="o">=</code></span><span><code class="mi">10</code></span><span><code class="p">,</code></span><code>&#13;
</code><code>                         </code><span><code class="n">solver</code></span><span><code class="o">=</code></span><span><code class="s1">'</code><code class="s1">saga</code><code class="s1">'</code></span><span><code class="p">,</code></span><code>&#13;
</code><code>                         </code><span><code class="n">n_jobs</code></span><span><code class="o">=</code></span><span><code class="mi">8</code></span><span><code class="p">,</code></span><code>&#13;
</code><code>                         </code><span><code class="n">max_iter</code></span><span><code class="o">=</code></span><span><code class="mi">1000</code></span><span><code class="p">)</code><code class="p">,</code></span><code>&#13;
</code><code>    </code><span><code class="n">verbose</code></span><span><code class="o">=</code></span><span><code class="kc">True</code></span><span><code class="p">,</code></span><code>&#13;
</code><span><code class="p">)</code></span><code>&#13;
</code></pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<div class="cell docutils container">&#13;
<div class="cell_input docutils container">&#13;
<div class="highlight-ipython3 notranslate">&#13;
<div class="highlight">&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<span><code class="o">%</code><code class="o">%</code><code class="n">time</code></span><code>&#13;
</code><code>&#13;
</code><span><code class="n">model3</code></span><span><code class="o">.</code></span><span><code class="n">fit</code></span><span><code class="p">(</code></span><span><code class="n">X_train</code></span><span><code class="p">,</code></span><code> </code><span><code class="n">y_train</code></span><span><code class="p">)</code></span><code>&#13;
</code><span><code class="nb">print</code></span><span><code class="p">(</code></span><span><code class="sa">f</code></span><span><code class="s1">'</code></span><span><code class="si">{</code></span><span><code class="n">model3</code></span><span><code class="o">.</code></span><span><code class="n">score</code></span><span><code class="p">(</code></span><span><code class="n">X_train</code></span><span><code class="p">,</code></span><code> </code><span><code class="n">y_train</code></span><span><code class="p">)</code></span><span><code class="si">:</code></span><span><code class="s1">.1%</code></span><span><code class="si">}</code></span><span><code class="s1"> accuracy on training set.</code><code class="s1">'</code></span><span><code class="p">)</code></span><code>&#13;
</code></pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<div class="cell_output docutils container">&#13;
<div class="output stream highlight-myst-ansi notranslate">&#13;
<div class="highlight">&#13;
<pre data-type="programlisting">&#13;
[Pipeline]  (step 1 of 3) Processing functiontransformer, total=   0.0s&#13;
[Pipeline] . (step 2 of 3) Processing columntransformer, total=  14.5s&#13;
[Pipeline]  (step 3 of 3) Processing logisticregressioncv, total=   6.3s&#13;
100.0% accuracy on training set.&#13;
CPU times: user 50.2 s, sys: 508 ms, total: 50.7 s&#13;
Wall time: 34.2 s&#13;
</pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<p>We find that this model achieves 100% accuracy on the train set. We can take a look at the tf-idf transformer to better understand the model. Let’s start by finding out how many unique tokens the classifier uses:</p>&#13;
&#13;
<div class="cell docutils container">&#13;
<div class="cell_input docutils container">&#13;
<div class="highlight-ipython3 notranslate">&#13;
<div class="highlight">&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<span><code class="n">tfidf</code></span><code> </code><span><code class="o">=</code></span><code> </code><span><code class="n">model3</code></span><span><code class="o">.</code></span><span><code class="n">named_steps</code></span><span><code class="o">.</code></span><span><code class="n">columntransformer</code></span><span><code class="o">.</code></span><span><code class="n">named_transformers_</code></span><span><code class="o">.</code></span><span><code class="n">tfidfvectorizer</code></span><code>&#13;
</code><span><code class="n">n_unique_tokens</code></span><code> </code><span><code class="o">=</code></span><code> </code><span><code class="nb">len</code></span><span><code class="p">(</code></span><span><code class="n">tfidf</code></span><span><code class="o">.</code></span><span><code class="n">vocabulary_</code></span><span><code class="o">.</code></span><span><code class="n">keys</code></span><span><code class="p">(</code><code class="p">)</code><code class="p">)</code></span><code>&#13;
</code><span><code class="nb">print</code></span><span><code class="p">(</code></span><span><code class="sa">f</code></span><span><code class="s1">'</code></span><span><code class="si">{</code></span><span><code class="n">n_unique_tokens</code></span><span><code class="si">}</code></span><span><code class="s1"> tokens appeared across </code></span><span><code class="si">{</code></span><span><code class="nb">len</code></span><span><code class="p">(</code></span><span><code class="n">X_train</code></span><span><code class="p">)</code></span><span><code class="si">}</code></span><span><code class="s1"> examples.</code><code class="s1">'</code></span><span><code class="p">)</code></span><code>&#13;
</code></pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<div class="cell_output docutils container">&#13;
<div class="output stream highlight-myst-ansi notranslate">&#13;
<div class="highlight">&#13;
<pre data-type="programlisting">&#13;
23800 tokens appeared across 584 examples.&#13;
</pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<p>This means that our classifier has 23,812 features, a large increase from our previous model, which only had 15. Since we can’t display that many model weights, we display the 10 most negative and 10 most positive weights:</p>&#13;
&#13;
<figure class="informal width-70"><div class="figure"><img src="assets/leds_21in10.png"/>&#13;
&#13;
</div></figure>&#13;
&#13;
<p>These coefficients show a few quirks about this model. We see that several influential features correspond to punctuation in the original text. It’s unclear whether we should clean out the punctuation in the model. On the one hand, punctuation doesn’t seem to convey as much meaning as words do. On the other, it seems plausible that, for example, lots of exclamation points in an article could help a model decide whether the article is real or fake. In this case, we’ve decided to keep punctuation, but curious readers can repeat this analysis after stripping the punctuation out to see how the resulting model is affected.</p>&#13;
&#13;
<p>We conclude by displaying the test set error for all three models:</p>&#13;
&#13;
<div class="cell tag_hide-input docutils container">&#13;
<div class="cell_output docutils container">&#13;
<div class="output text_html">&#13;
<div>&#13;
<table class="dataframe">&#13;
	<thead>&#13;
		<tr>&#13;
			<th> </th>&#13;
			<th>test set error</th>&#13;
		</tr>&#13;
	</thead>&#13;
	<tbody>&#13;
		<tr>&#13;
			<td><strong>model1</strong></td>&#13;
			<td>0.61</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td><strong>model2</strong></td>&#13;
			<td>0.70</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td><strong>model3</strong></td>&#13;
			<td>0.88</td>&#13;
		</tr>&#13;
	</tbody>&#13;
</table>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<p>As we might expect, the models became more accurate as we introduced more features. The model that used tf-idf performed much better than the models with binary handpicked word features, but it did not meet the 100% accuracy obtained on the train set. This illustrates a common trade-off in modeling: given enough data, more complex models can often outperform simpler ones, especially in situations like this case study where simpler models have too much model bias to perform well. However, complex models can be more difficult to interpret. For example, our tf-idf model had over 20,000 features, which makes it basically impossible to explain how our model makes its decisions. In addition, the tf-idf model takes much longer to make predictions—it’s over 100 times slower compared to model 2. All of these factors need to be considered when deciding which model to use in practice.</p>&#13;
&#13;
<p>In addition, we need to be careful about what our models are useful for. In this case, our models use the content of the news articles for prediction, making them highly dependent on the words that appear in the train set. However, our models will likely not perform as well on future news articles that use words that didn’t appear in the train set. For example, our models use the US election candidates’ names in 2016 for prediction, but they won’t know to incorporate the names of the candidates in 2020 or 2024. To use our models<a contenteditable="false" data-primary="drift in fake news detection data" data-type="indexterm" id="id1886"/> in the longer term, we would need to address this issue of <em>drift</em>.</p>&#13;
&#13;
<p>That said, it’s surprising that a logistic regression model can perform well with a relatively small amount of feature engineering (tf-idf). We’ve addressed our original research question: our tf-idf model appears effective for detecting fake news in our dataset, and it could plausibly generalize to other news published in the same time period covered in the training data<a contenteditable="false" data-primary="" data-startref="ix_feat_feat_engin4" data-type="indexterm" id="id1887"/><a contenteditable="false" data-primary="" data-startref="ix_feat_engin_tfidf" data-type="indexterm" id="id1888"/><a contenteditable="false" data-primary="" data-startref="ix_tfidf_transf" data-type="indexterm" id="id1889"/><a contenteditable="false" data-primary="" data-startref="ix_tfidf_spout_transf" data-type="indexterm" id="id1890"/><a contenteditable="false" data-primary="" data-startref="ix_mod_fake_news" data-type="indexterm" id="id1891"/><a contenteditable="false" data-primary="" data-startref="ix_fake_news_mod" data-type="indexterm" id="id1892"/>.</p>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="sec-fake-news-summary">&#13;
<h1>Summary</h1>&#13;
&#13;
<p>We’re quickly approaching the end of the chapter and thus the end of the book. We started this book by talking about the data science lifecycle. Let’s take another look at the lifecycle, in <a class="reference internal" data-type="xref" href="#fig-ds-lifecycle-conclusion">Figure 21-2</a>, to appreciate everything that you’ve learned.</p>&#13;
&#13;
<figure><div class="figure" id="fig-ds-lifecycle-conclusion"><img src="assets/leds_2102.png"/>&#13;
<h6><span class="label">Figure 21-2. </span>The four high-level steps of the data science lifecycle, each of which we dove into throughout this book</h6>&#13;
</div></figure>&#13;
&#13;
<p>This case study stepped through each stage of the data science lifecycle:</p>&#13;
&#13;
<ol class="arabic simple">&#13;
	<li>&#13;
	<p>Many data analyses begin with a research question. The case study we presented in this chapter started by asking whether we can create models to automatically detect fake news.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>We obtained data by using code found online that scrapes web pages into JSON files. Since the data description was relatively minimal, we needed to clean the data to understand it. This included creating new features to indicate the presence or absence of certain words in the articles.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Our initial explorations identified possible words that might be useful for prediction. After fitting simple models and exploring their precision and accuracy, we further transformed the articles using tf-idf to convert each news article into a normalized word vector.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>We used the vectorized text as features in a logistic model, and we fitted the final model using regularization and cross-validation. Finally, we found the accuracy and precision of the fitted model on the test set.</p>&#13;
	</li>&#13;
</ol>&#13;
&#13;
<p>When we write out the steps in the lifecycle like this, the steps seem to flow smoothly into each other. But reality is messy—as the diagram illustrates, real data analyses jump forward and backward between steps. For example, at the end of our case study, we discovered data cleaning questions that might motivate us to revisit earlier stages of the lifecycle. Although our model was quite accurate, the majority of the training data came from the 2016–2018 time period, so we have to carefully evaluate the model’s performance if we want to use it on articles published outside that time frame.</p>&#13;
&#13;
<p>In essence, it’s important to keep the entire lifecycle in mind at each stage of a data analysis. As a data scientist, you will be asked to justify your decisions, which means that you need to deeply understand your research question and data. To develop this understanding, the principles and techniques in this book equip you with a foundational set of skills. Going forward into your data science journey, we recommend that you continue to expand your skills by:</p>&#13;
&#13;
<ul>&#13;
	<li>&#13;
	<p>Revisiting a case study from this book. Start by replicating our analysis, then dive deeper into questions that you have about the data.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Conducting an independent data analysis. Pose a research question you’re interested in, find relevant data from the web, and analyze the data to see how well the data matched your expectations. Doing this will give you firsthand experience with the entire data science lifecycle.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Taking a deep dive into a topic. We’ve provided many in-depth resources in the <a class="reference internal" data-type="xref" href="bibliography01.html#ax-extra-reading">Additional Material</a> appendix. Take the resource that seems most interesting to you and learn more about<a contenteditable="false" data-primary="" data-startref="ix_fake_news_ch21" data-type="indexterm" id="id1893"/> it.</p>&#13;
	</li>&#13;
</ul>&#13;
&#13;
<p>The world needs people like you who can use data to make conclusions, so we sincerely hope that you’ll use these skills to help others make effective strategies, better products, and informed decisions.</p>&#13;
</div></section>&#13;
</div></section></body></html>