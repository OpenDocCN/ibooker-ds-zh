["```py\nfrom typing import Set\nimport re\n\ndef tokenize(text: str) -> Set[str]:\n    text = text.lower()                         # Convert to lowercase,\n    all_words = re.findall(\"[a-z0-9']+\", text)  # extract the words, and\n    return set(all_words)                       # remove duplicates.\n\nassert tokenize(\"Data Science is science\") == {\"data\", \"science\", \"is\"}\n```", "```py\nfrom typing import NamedTuple\n\nclass Message(NamedTuple):\n    text: str\n    is_spam: bool\n```", "```py\nfrom typing import List, Tuple, Dict, Iterable\nimport math\nfrom collections import defaultdict\n\nclass NaiveBayesClassifier:\n    def __init__(self, k: float = 0.5) -> None:\n        self.k = k  # smoothing factor\n\n        self.tokens: Set[str] = set()\n        self.token_spam_counts: Dict[str, int] = defaultdict(int)\n        self.token_ham_counts: Dict[str, int] = defaultdict(int)\n        self.spam_messages = self.ham_messages = 0\n```", "```py\n    def train(self, messages: Iterable[Message]) -> None:\n        for message in messages:\n            # Increment message counts\n            if message.is_spam:\n                self.spam_messages += 1\n            else:\n                self.ham_messages += 1\n\n            # Increment word counts\n            for token in tokenize(message.text):\n                self.tokens.add(token)\n                if message.is_spam:\n                    self.token_spam_counts[token] += 1\n                else:\n                    self.token_ham_counts[token] += 1\n```", "```py\n    def _probabilities(self, token: str) -> Tuple[float, float]:\n        \"\"\"returns P(token | spam) and P(token | ham)\"\"\"\n        spam = self.token_spam_counts[token]\n        ham = self.token_ham_counts[token]\n\n        p_token_spam = (spam + self.k) / (self.spam_messages + 2 * self.k)\n        p_token_ham = (ham + self.k) / (self.ham_messages + 2 * self.k)\n\n        return p_token_spam, p_token_ham\n```", "```py\n    def predict(self, text: str) -> float:\n        text_tokens = tokenize(text)\n        log_prob_if_spam = log_prob_if_ham = 0.0\n\n        # Iterate through each word in our vocabulary\n        for token in self.tokens:\n            prob_if_spam, prob_if_ham = self._probabilities(token)\n\n            # If *token* appears in the message,\n            # add the log probability of seeing it\n            if token in text_tokens:\n                log_prob_if_spam += math.log(prob_if_spam)\n                log_prob_if_ham += math.log(prob_if_ham)\n\n            # Otherwise add the log probability of _not_ seeing it,\n            # which is log(1 - probability of seeing it)\n            else:\n                log_prob_if_spam += math.log(1.0 - prob_if_spam)\n                log_prob_if_ham += math.log(1.0 - prob_if_ham)\n\n        prob_if_spam = math.exp(log_prob_if_spam)\n        prob_if_ham = math.exp(log_prob_if_ham)\n        return prob_if_spam / (prob_if_spam + prob_if_ham)\n```", "```py\nmessages = [Message(\"spam rules\", is_spam=True),\n            Message(\"ham rules\", is_spam=False),\n            Message(\"hello ham\", is_spam=False)]\n\nmodel = NaiveBayesClassifier(k=0.5)\nmodel.train(messages)\n```", "```py\nassert model.tokens == {\"spam\", \"ham\", \"rules\", \"hello\"}\nassert model.spam_messages == 1\nassert model.ham_messages == 2\nassert model.token_spam_counts == {\"spam\": 1, \"rules\": 1}\nassert model.token_ham_counts == {\"ham\": 2, \"rules\": 1, \"hello\": 1}\n```", "```py\ntext = \"hello spam\"\n\nprobs_if_spam = [\n    (1 + 0.5) / (1 + 2 * 0.5),      # \"spam\"  (present)\n    1 - (0 + 0.5) / (1 + 2 * 0.5),  # \"ham\"   (not present)\n    1 - (1 + 0.5) / (1 + 2 * 0.5),  # \"rules\" (not present)\n    (0 + 0.5) / (1 + 2 * 0.5)       # \"hello\" (present)\n]\n\nprobs_if_ham = [\n    (0 + 0.5) / (2 + 2 * 0.5),      # \"spam\"  (present)\n    1 - (2 + 0.5) / (2 + 2 * 0.5),  # \"ham\"   (not present)\n    1 - (1 + 0.5) / (2 + 2 * 0.5),  # \"rules\" (not present)\n    (1 + 0.5) / (2 + 2 * 0.5),      # \"hello\" (present)\n]\n\np_if_spam = math.exp(sum(math.log(p) for p in probs_if_spam))\np_if_ham = math.exp(sum(math.log(p) for p in probs_if_ham))\n\n# Should be about 0.83\nassert model.predict(text) == p_if_spam / (p_if_spam + p_if_ham)\n```", "```py\nfrom io import BytesIO  # So we can treat bytes as a file.\nimport requests         # To download the files, which\nimport tarfile          # are in .tar.bz format.\n\nBASE_URL = \"https://spamassassin.apache.org/old/publiccorpus\"\nFILES = [\"20021010_easy_ham.tar.bz2\",\n         \"20021010_hard_ham.tar.bz2\",\n         \"20021010_spam.tar.bz2\"]\n\n# This is where the data will end up,\n# in /spam, /easy_ham, and /hard_ham subdirectories.\n# Change this to where you want the data.\nOUTPUT_DIR = 'spam_data'\n\nfor filename in FILES:\n    # Use requests to get the file contents at each URL.\n    content = requests.get(f\"{BASE_URL}/{filename}\").content\n\n    # Wrap the in-memory bytes so we can use them as a \"file.\"\n    fin = BytesIO(content)\n\n    # And extract all the files to the specified output dir.\n    with tarfile.open(fileobj=fin, mode='r:bz2') as tf:\n        tf.extractall(OUTPUT_DIR)\n```", "```py\nimport glob, re\n\n# modify the path to wherever you've put the files\npath = 'spam_data/*/*'\n\ndata: List[Message] = []\n\n# glob.glob returns every filename that matches the wildcarded path\nfor filename in glob.glob(path):\n    is_spam = \"ham\" not in filename\n\n    # There are some garbage characters in the emails; the errors='ignore'\n    # skips them instead of raising an exception.\n    with open(filename, errors='ignore') as email_file:\n        for line in email_file:\n            if line.startswith(\"Subject:\"):\n                subject = line.lstrip(\"Subject: \")\n                data.append(Message(subject, is_spam))\n                break  # done with this file\n```", "```py\nimport random\nfrom scratch.machine_learning import split_data\n\nrandom.seed(0)      # just so you get the same answers as me\ntrain_messages, test_messages = split_data(data, 0.75)\n\nmodel = NaiveBayesClassifier()\nmodel.train(train_messages)\n```", "```py\nfrom collections import Counter\n\npredictions = [(message, model.predict(message.text))\n               for message in test_messages]\n\n# Assume that spam_probability > 0.5 corresponds to spam prediction\n# and count the combinations of (actual is_spam, predicted is_spam)\nconfusion_matrix = Counter((message.is_spam, spam_probability > 0.5)\n                           for message, spam_probability in predictions)\n\nprint(confusion_matrix)\n```", "```py\ndef p_spam_given_token(token: str, model: NaiveBayesClassifier) -> float:\n    # We probably shouldn't call private methods, but it's for a good cause.\n    prob_if_spam, prob_if_ham = model._probabilities(token)\n\n    return prob_if_spam / (prob_if_spam + prob_if_ham)\n\nwords = sorted(model.tokens, key=lambda t: p_spam_given_token(t, model))\n\nprint(\"spammiest_words\", words[-10:])\nprint(\"hammiest_words\", words[:10])\n```", "```py\n    def drop_final_s(word):\n        return re.sub(\"s$\", \"\", word)\n    ```"]