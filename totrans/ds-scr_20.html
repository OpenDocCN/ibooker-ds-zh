<html><head></head><body><section data-pdf-bookmark="Chapter 19. Deep Learning" data-type="chapter" epub:type="chapter"><div class="chapter" id="deep_learning">&#13;
<h1><span class="label">Chapter 19. </span>Deep Learning</h1>&#13;
&#13;
<blockquote data-type="epigraph" epub:type="epigraph">&#13;
    <p>A little learning is a dangerous thing; Drink deep, or taste not the Pierian spring.</p>&#13;
    <p data-type="attribution">Alexander Pope</p>&#13;
</blockquote>&#13;
&#13;
<p><em>Deep learning</em> originally<a data-primary="deep learning" data-secondary="definition of term" data-type="indexterm" id="idm45635730163704"/> referred to the application of “deep” neural networks&#13;
(that is, networks with more than one hidden layer), although in practice the&#13;
term now encompasses&#13;
a wide variety of neural architectures (including the “simple” neural networks&#13;
we developed in <a data-type="xref" href="ch18.html#neural_networks">Chapter 18</a>).</p>&#13;
&#13;
<p>In this chapter we’ll build on our previous work and look at a wider variety of neural networks.&#13;
To do so, we’ll introduce a number of abstractions that allow us to think about neural networks&#13;
in a more general way.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="The Tensor" data-type="sect1"><div class="sect1" id="idm45635730160712">&#13;
<h1>The Tensor</h1>&#13;
&#13;
<p>Previously, we<a data-primary="deep learning" data-secondary="tensors" data-type="indexterm" id="idm45635730158984"/><a data-primary="tensors" data-type="indexterm" id="idm45635730157976"/> made a distinction between vectors (one-dimensional arrays)&#13;
and matrices (two-dimensional arrays). When we start working with more complicated&#13;
neural networks, we’ll need to use higher-dimensional arrays as well.</p>&#13;
&#13;
<p>In many neural network libraries, <em>n</em>-dimensional arrays are referred to as <em>tensors</em>, which is what we’ll call them too. (There are pedantic mathematical reasons not to refer to <em>n</em>-dimensional arrays as tensors; if you are such a pedant, your objection is noted.)</p>&#13;
&#13;
<p>If I were writing an entire book about deep learning, I’d implement&#13;
a full-featured <code>Tensor</code> class that overloaded Python’s arithmetic operators and&#13;
could handle a variety of&#13;
other operations. Such an implementation would take an entire chapter on its own.&#13;
Here we’ll cheat and say that a <code>Tensor</code> is just a <code>list</code>. This is true in one direction—all of our vectors and matrices and higher-dimensional analogues <em>are</em> lists.&#13;
It is certainly not true in the other direction—most Python <code>list</code>s are not <em>n</em>-dimensional arrays in our sense.</p>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>Ideally you’d like to do something like:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="c1"># A Tensor is either a float, or a List of Tensors</code>&#13;
<code class="n">Tensor</code> <code class="o">=</code> <code class="n">Union</code><code class="p">[</code><code class="nb">float</code><code class="p">,</code> <code class="n">List</code><code class="p">[</code><code class="n">Tensor</code><code class="p">]]</code></pre>&#13;
&#13;
<p>However, Python won’t let you define recursive types like that.&#13;
And even if it did that definition is still not right, as it allows for bad “tensors” like:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="p">[[</code><code class="mf">1.0</code><code class="p">,</code> <code class="mf">2.0</code><code class="p">],</code>&#13;
 <code class="p">[</code><code class="mf">3.0</code><code class="p">]]</code></pre>&#13;
&#13;
<p>whose rows have different sizes, which makes it not an <em>n</em>-dimensional array.</p>&#13;
</div>&#13;
&#13;
<p>So, like I said, we’ll just cheat:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="n">Tensor</code> <code class="o">=</code> <code class="nb">list</code></pre>&#13;
&#13;
<p>And we’ll write a helper function to find a tensor’s <em>shape</em>:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="kn">from</code> <code class="nn">typing</code> <code class="kn">import</code> <code class="n">List</code>&#13;
&#13;
<code class="k">def</code> <code class="nf">shape</code><code class="p">(</code><code class="n">tensor</code><code class="p">:</code> <code class="n">Tensor</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="n">List</code><code class="p">[</code><code class="nb">int</code><code class="p">]:</code>&#13;
    <code class="n">sizes</code><code class="p">:</code> <code class="n">List</code><code class="p">[</code><code class="nb">int</code><code class="p">]</code> <code class="o">=</code> <code class="p">[]</code>&#13;
    <code class="k">while</code> <code class="nb">isinstance</code><code class="p">(</code><code class="n">tensor</code><code class="p">,</code> <code class="nb">list</code><code class="p">):</code>&#13;
        <code class="n">sizes</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="nb">len</code><code class="p">(</code><code class="n">tensor</code><code class="p">))</code>&#13;
        <code class="n">tensor</code> <code class="o">=</code> <code class="n">tensor</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code>&#13;
    <code class="k">return</code> <code class="n">sizes</code>&#13;
&#13;
<code class="k">assert</code> <code class="n">shape</code><code class="p">([</code><code class="mi">1</code><code class="p">,</code> <code class="mi">2</code><code class="p">,</code> <code class="mi">3</code><code class="p">])</code> <code class="o">==</code> <code class="p">[</code><code class="mi">3</code><code class="p">]</code>&#13;
<code class="k">assert</code> <code class="n">shape</code><code class="p">([[</code><code class="mi">1</code><code class="p">,</code> <code class="mi">2</code><code class="p">],</code> <code class="p">[</code><code class="mi">3</code><code class="p">,</code> <code class="mi">4</code><code class="p">],</code> <code class="p">[</code><code class="mi">5</code><code class="p">,</code> <code class="mi">6</code><code class="p">]])</code> <code class="o">==</code> <code class="p">[</code><code class="mi">3</code><code class="p">,</code> <code class="mi">2</code><code class="p">]</code></pre>&#13;
&#13;
<p>Because tensors can have any number of dimensions, we’ll typically need to&#13;
work with them recursively. We’ll do one thing in the one-dimensional case&#13;
and recurse in the higher-dimensional case:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="k">def</code> <code class="nf">is_1d</code><code class="p">(</code><code class="n">tensor</code><code class="p">:</code> <code class="n">Tensor</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">bool</code><code class="p">:</code>&#13;
    <code class="sd">"""</code>&#13;
<code class="sd">    If tensor[0] is a list, it's a higher-order tensor.</code>&#13;
<code class="sd">    Otherwise, tensor is 1-dimensional (that is, a vector).</code>&#13;
<code class="sd">    """</code>&#13;
    <code class="k">return</code> <code class="ow">not</code> <code class="nb">isinstance</code><code class="p">(</code><code class="n">tensor</code><code class="p">[</code><code class="mi">0</code><code class="p">],</code> <code class="nb">list</code><code class="p">)</code>&#13;
&#13;
<code class="k">assert</code> <code class="n">is_1d</code><code class="p">([</code><code class="mi">1</code><code class="p">,</code> <code class="mi">2</code><code class="p">,</code> <code class="mi">3</code><code class="p">])</code>&#13;
<code class="k">assert</code> <code class="ow">not</code> <code class="n">is_1d</code><code class="p">([[</code><code class="mi">1</code><code class="p">,</code> <code class="mi">2</code><code class="p">],</code> <code class="p">[</code><code class="mi">3</code><code class="p">,</code> <code class="mi">4</code><code class="p">]])</code></pre>&#13;
&#13;
<p>which we can use to write a recursive <code>tensor_sum</code> function:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="k">def</code> <code class="nf">tensor_sum</code><code class="p">(</code><code class="n">tensor</code><code class="p">:</code> <code class="n">Tensor</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">float</code><code class="p">:</code>&#13;
    <code class="sd">"""Sums up all the values in the tensor"""</code>&#13;
    <code class="k">if</code> <code class="n">is_1d</code><code class="p">(</code><code class="n">tensor</code><code class="p">):</code>&#13;
        <code class="k">return</code> <code class="nb">sum</code><code class="p">(</code><code class="n">tensor</code><code class="p">)</code>  <code class="c1"># just a list of floats, use Python sum</code>&#13;
    <code class="k">else</code><code class="p">:</code>&#13;
        <code class="k">return</code> <code class="nb">sum</code><code class="p">(</code><code class="n">tensor_sum</code><code class="p">(</code><code class="n">tensor_i</code><code class="p">)</code>      <code class="c1"># Call tensor_sum on each row</code>&#13;
                   <code class="k">for</code> <code class="n">tensor_i</code> <code class="ow">in</code> <code class="n">tensor</code><code class="p">)</code>   <code class="c1"># and sum up those results.</code>&#13;
&#13;
<code class="k">assert</code> <code class="n">tensor_sum</code><code class="p">([</code><code class="mi">1</code><code class="p">,</code> <code class="mi">2</code><code class="p">,</code> <code class="mi">3</code><code class="p">])</code> <code class="o">==</code> <code class="mi">6</code>&#13;
<code class="k">assert</code> <code class="n">tensor_sum</code><code class="p">([[</code><code class="mi">1</code><code class="p">,</code> <code class="mi">2</code><code class="p">],</code> <code class="p">[</code><code class="mi">3</code><code class="p">,</code> <code class="mi">4</code><code class="p">]])</code> <code class="o">==</code> <code class="mi">10</code></pre>&#13;
&#13;
<p>If you’re not used to thinking recursively, you should ponder this until it makes sense,&#13;
because we’ll use the same logic throughout this chapter.&#13;
However, we’ll create a couple of helper functions so that we don’t have to rewrite&#13;
this logic everywhere. The first applies a function elementwise to a single tensor:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="kn">from</code> <code class="nn">typing</code> <code class="kn">import</code> <code class="n">Callable</code>&#13;
&#13;
<code class="k">def</code> <code class="nf">tensor_apply</code><code class="p">(</code><code class="n">f</code><code class="p">:</code> <code class="n">Callable</code><code class="p">[[</code><code class="nb">float</code><code class="p">],</code> <code class="nb">float</code><code class="p">],</code> <code class="n">tensor</code><code class="p">:</code> <code class="n">Tensor</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="n">Tensor</code><code class="p">:</code>&#13;
    <code class="sd">"""Applies f elementwise"""</code>&#13;
    <code class="k">if</code> <code class="n">is_1d</code><code class="p">(</code><code class="n">tensor</code><code class="p">):</code>&#13;
        <code class="k">return</code> <code class="p">[</code><code class="n">f</code><code class="p">(</code><code class="n">x</code><code class="p">)</code> <code class="k">for</code> <code class="n">x</code> <code class="ow">in</code> <code class="n">tensor</code><code class="p">]</code>&#13;
    <code class="k">else</code><code class="p">:</code>&#13;
        <code class="k">return</code> <code class="p">[</code><code class="n">tensor_apply</code><code class="p">(</code><code class="n">f</code><code class="p">,</code> <code class="n">tensor_i</code><code class="p">)</code> <code class="k">for</code> <code class="n">tensor_i</code> <code class="ow">in</code> <code class="n">tensor</code><code class="p">]</code>&#13;
&#13;
<code class="k">assert</code> <code class="n">tensor_apply</code><code class="p">(</code><code class="k">lambda</code> <code class="n">x</code><code class="p">:</code> <code class="n">x</code> <code class="o">+</code> <code class="mi">1</code><code class="p">,</code> <code class="p">[</code><code class="mi">1</code><code class="p">,</code> <code class="mi">2</code><code class="p">,</code> <code class="mi">3</code><code class="p">])</code> <code class="o">==</code> <code class="p">[</code><code class="mi">2</code><code class="p">,</code> <code class="mi">3</code><code class="p">,</code> <code class="mi">4</code><code class="p">]</code>&#13;
<code class="k">assert</code> <code class="n">tensor_apply</code><code class="p">(</code><code class="k">lambda</code> <code class="n">x</code><code class="p">:</code> <code class="mi">2</code> <code class="o">*</code> <code class="n">x</code><code class="p">,</code> <code class="p">[[</code><code class="mi">1</code><code class="p">,</code> <code class="mi">2</code><code class="p">],</code> <code class="p">[</code><code class="mi">3</code><code class="p">,</code> <code class="mi">4</code><code class="p">]])</code> <code class="o">==</code> <code class="p">[[</code><code class="mi">2</code><code class="p">,</code> <code class="mi">4</code><code class="p">],</code> <code class="p">[</code><code class="mi">6</code><code class="p">,</code> <code class="mi">8</code><code class="p">]]</code></pre>&#13;
&#13;
<p>We can use this to write a function that creates a zero tensor&#13;
with the same shape as a given tensor:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="k">def</code> <code class="nf">zeros_like</code><code class="p">(</code><code class="n">tensor</code><code class="p">:</code> <code class="n">Tensor</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="n">Tensor</code><code class="p">:</code>&#13;
    <code class="k">return</code> <code class="n">tensor_apply</code><code class="p">(</code><code class="k">lambda</code> <code class="n">_</code><code class="p">:</code> <code class="mf">0.0</code><code class="p">,</code> <code class="n">tensor</code><code class="p">)</code>&#13;
&#13;
<code class="k">assert</code> <code class="n">zeros_like</code><code class="p">([</code><code class="mi">1</code><code class="p">,</code> <code class="mi">2</code><code class="p">,</code> <code class="mi">3</code><code class="p">])</code> <code class="o">==</code> <code class="p">[</code><code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">]</code>&#13;
<code class="k">assert</code> <code class="n">zeros_like</code><code class="p">([[</code><code class="mi">1</code><code class="p">,</code> <code class="mi">2</code><code class="p">],</code> <code class="p">[</code><code class="mi">3</code><code class="p">,</code> <code class="mi">4</code><code class="p">]])</code> <code class="o">==</code> <code class="p">[[</code><code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">],</code> <code class="p">[</code><code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">]]</code></pre>&#13;
&#13;
<p>We’ll also need to apply a function to corresponding elements from two tensors&#13;
(which had better be the exact same shape, although we won’t check that):</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="k">def</code> <code class="nf">tensor_combine</code><code class="p">(</code><code class="n">f</code><code class="p">:</code> <code class="n">Callable</code><code class="p">[[</code><code class="nb">float</code><code class="p">,</code> <code class="nb">float</code><code class="p">],</code> <code class="nb">float</code><code class="p">],</code>&#13;
                   <code class="n">t1</code><code class="p">:</code> <code class="n">Tensor</code><code class="p">,</code>&#13;
                   <code class="n">t2</code><code class="p">:</code> <code class="n">Tensor</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="n">Tensor</code><code class="p">:</code>&#13;
    <code class="sd">"""Applies f to corresponding elements of t1 and t2"""</code>&#13;
    <code class="k">if</code> <code class="n">is_1d</code><code class="p">(</code><code class="n">t1</code><code class="p">):</code>&#13;
        <code class="k">return</code> <code class="p">[</code><code class="n">f</code><code class="p">(</code><code class="n">x</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code> <code class="k">for</code> <code class="n">x</code><code class="p">,</code> <code class="n">y</code> <code class="ow">in</code> <code class="nb">zip</code><code class="p">(</code><code class="n">t1</code><code class="p">,</code> <code class="n">t2</code><code class="p">)]</code>&#13;
    <code class="k">else</code><code class="p">:</code>&#13;
        <code class="k">return</code> <code class="p">[</code><code class="n">tensor_combine</code><code class="p">(</code><code class="n">f</code><code class="p">,</code> <code class="n">t1_i</code><code class="p">,</code> <code class="n">t2_i</code><code class="p">)</code>&#13;
                <code class="k">for</code> <code class="n">t1_i</code><code class="p">,</code> <code class="n">t2_i</code> <code class="ow">in</code> <code class="nb">zip</code><code class="p">(</code><code class="n">t1</code><code class="p">,</code> <code class="n">t2</code><code class="p">)]</code>&#13;
&#13;
<code class="kn">import</code> <code class="nn">operator</code>&#13;
<code class="k">assert</code> <code class="n">tensor_combine</code><code class="p">(</code><code class="n">operator</code><code class="o">.</code><code class="n">add</code><code class="p">,</code> <code class="p">[</code><code class="mi">1</code><code class="p">,</code> <code class="mi">2</code><code class="p">,</code> <code class="mi">3</code><code class="p">],</code> <code class="p">[</code><code class="mi">4</code><code class="p">,</code> <code class="mi">5</code><code class="p">,</code> <code class="mi">6</code><code class="p">])</code> <code class="o">==</code> <code class="p">[</code><code class="mi">5</code><code class="p">,</code> <code class="mi">7</code><code class="p">,</code> <code class="mi">9</code><code class="p">]</code>&#13;
<code class="k">assert</code> <code class="n">tensor_combine</code><code class="p">(</code><code class="n">operator</code><code class="o">.</code><code class="n">mul</code><code class="p">,</code> <code class="p">[</code><code class="mi">1</code><code class="p">,</code> <code class="mi">2</code><code class="p">,</code> <code class="mi">3</code><code class="p">],</code> <code class="p">[</code><code class="mi">4</code><code class="p">,</code> <code class="mi">5</code><code class="p">,</code> <code class="mi">6</code><code class="p">])</code> <code class="o">==</code> <code class="p">[</code><code class="mi">4</code><code class="p">,</code> <code class="mi">10</code><code class="p">,</code> <code class="mi">18</code><code class="p">]</code></pre>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="The Layer Abstraction" data-type="sect1"><div class="sect1" id="idm45635730160088">&#13;
<h1>The Layer Abstraction</h1>&#13;
&#13;
<p>In<a data-primary="deep learning" data-secondary="Layers abstraction" data-type="indexterm" id="idm45635729459960"/><a data-primary="Layers abstraction" data-secondary="basics of" data-type="indexterm" id="idm45635729458952"/> the previous chapter we built a simple neural net&#13;
that allowed us to stack two layers of neurons, each of which computed&#13;
<code>sigmoid(dot(weights, inputs))</code>.</p>&#13;
&#13;
<p>Although that’s perhaps an idealized representation of what an actual&#13;
neuron does, in practice we’d like to allow a wider variety of things.&#13;
Perhaps we’d like the neurons to remember something about their previous inputs.&#13;
Perhaps we’d like to use a different activation function than <code>sigmoid</code>.&#13;
And frequently we’d like to use more than two layers. (Our <code>feed_forward</code>&#13;
function actually handled any number of layers, but our gradient computations&#13;
did not.)</p>&#13;
&#13;
<p>In this chapter we’ll build machinery for implementing such a variety&#13;
of neural networks. Our fundamental abstraction will be the <code>Layer</code>,&#13;
something that knows how to apply some function to its inputs&#13;
and that knows how to backpropagate gradients.</p>&#13;
&#13;
<p>One way of thinking about the neural networks we built in <a data-type="xref" href="ch18.html#neural_networks">Chapter 18</a>&#13;
is as a “linear” layer, followed by a “sigmoid” layer, then another linear layer&#13;
and another sigmoid layer. We didn’t distinguish them in these terms, but&#13;
doing so will allow us to experiment with much more general structures:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="kn">from</code> <code class="nn">typing</code> <code class="kn">import</code> <code class="n">Iterable</code><code class="p">,</code> <code class="n">Tuple</code>&#13;
&#13;
<code class="k">class</code> <code class="nc">Layer</code><code class="p">:</code>&#13;
    <code class="sd">"""</code>&#13;
<code class="sd">    Our neural networks will be composed of Layers, each of which</code>&#13;
<code class="sd">    knows how to do some computation on its inputs in the "forward"</code>&#13;
<code class="sd">    direction and propagate gradients in the "backward" direction.</code>&#13;
<code class="sd">    """</code>&#13;
    <code class="k">def</code> <code class="nf">forward</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="nb">input</code><code class="p">):</code>&#13;
        <code class="sd">"""</code>&#13;
<code class="sd">        Note the lack of types. We're not going to be prescriptive</code>&#13;
<code class="sd">        about what kinds of inputs layers can take and what kinds</code>&#13;
<code class="sd">        of outputs they can return.</code>&#13;
<code class="sd">        """</code>&#13;
        <code class="k">raise</code> <code class="ne">NotImplementedError</code>&#13;
&#13;
    <code class="k">def</code> <code class="nf">backward</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">gradient</code><code class="p">):</code>&#13;
        <code class="sd">"""</code>&#13;
<code class="sd">        Similarly, we're not going to be prescriptive about what the</code>&#13;
<code class="sd">        gradient looks like. It's up to you the user to make sure</code>&#13;
<code class="sd">        that you're doing things sensibly.</code>&#13;
<code class="sd">        """</code>&#13;
        <code class="k">raise</code> <code class="ne">NotImplementedError</code>&#13;
&#13;
    <code class="k">def</code> <code class="nf">params</code><code class="p">(</code><code class="bp">self</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="n">Iterable</code><code class="p">[</code><code class="n">Tensor</code><code class="p">]:</code>&#13;
        <code class="sd">"""</code>&#13;
<code class="sd">        Returns the parameters of this layer. The default implementation</code>&#13;
<code class="sd">        returns nothing, so that if you have a layer with no parameters</code>&#13;
<code class="sd">        you don't have to implement this.</code>&#13;
<code class="sd">        """</code>&#13;
        <code class="k">return</code> <code class="p">()</code>&#13;
&#13;
    <code class="k">def</code> <code class="nf">grads</code><code class="p">(</code><code class="bp">self</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="n">Iterable</code><code class="p">[</code><code class="n">Tensor</code><code class="p">]:</code>&#13;
        <code class="sd">"""</code>&#13;
<code class="sd">        Returns the gradients, in the same order as params().</code>&#13;
<code class="sd">        """</code>&#13;
        <code class="k">return</code> <code class="p">()</code></pre>&#13;
&#13;
<p>The <code>forward</code> and <code>backward</code> methods will have to be implemented in&#13;
our concrete subclasses. Once we build a neural net, we’ll want to train it&#13;
using gradient descent, which means we’ll want to update each parameter in the&#13;
network using its gradient. Accordingly, we insist that each layer be able&#13;
to tell us its parameters and gradients.</p>&#13;
&#13;
<p>Some layers (for example, a layer that applies <code>sigmoid</code> to each of its inputs)&#13;
have no parameters to update, so we provide a default implementation that&#13;
handles that case.</p>&#13;
&#13;
<p>Let’s look at that layer:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="kn">from</code> <code class="nn">scratch.neural_networks</code> <code class="kn">import</code> <code class="n">sigmoid</code>&#13;
&#13;
<code class="k">class</code> <code class="nc">Sigmoid</code><code class="p">(</code><code class="n">Layer</code><code class="p">):</code>&#13;
    <code class="k">def</code> <code class="nf">forward</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="nb">input</code><code class="p">:</code> <code class="n">Tensor</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="n">Tensor</code><code class="p">:</code>&#13;
        <code class="sd">"""</code>&#13;
<code class="sd">        Apply sigmoid to each element of the input tensor,</code>&#13;
<code class="sd">        and save the results to use in backpropagation.</code>&#13;
<code class="sd">        """</code>&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">sigmoids</code> <code class="o">=</code> <code class="n">tensor_apply</code><code class="p">(</code><code class="n">sigmoid</code><code class="p">,</code> <code class="nb">input</code><code class="p">)</code>&#13;
        <code class="k">return</code> <code class="bp">self</code><code class="o">.</code><code class="n">sigmoids</code>&#13;
&#13;
    <code class="k">def</code> <code class="nf">backward</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">gradient</code><code class="p">:</code> <code class="n">Tensor</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="n">Tensor</code><code class="p">:</code>&#13;
        <code class="k">return</code> <code class="n">tensor_combine</code><code class="p">(</code><code class="k">lambda</code> <code class="n">sig</code><code class="p">,</code> <code class="n">grad</code><code class="p">:</code> <code class="n">sig</code> <code class="o">*</code> <code class="p">(</code><code class="mi">1</code> <code class="o">-</code> <code class="n">sig</code><code class="p">)</code> <code class="o">*</code> <code class="n">grad</code><code class="p">,</code>&#13;
                              <code class="bp">self</code><code class="o">.</code><code class="n">sigmoids</code><code class="p">,</code>&#13;
                              <code class="n">gradient</code><code class="p">)</code></pre>&#13;
&#13;
<p>There are a couple of things to notice here. One is that during the forward pass we saved&#13;
the computed sigmoids so that we could use them later in the backward pass.&#13;
Our layers will typically need to do this sort of thing.</p>&#13;
&#13;
<p>Second, you may be wondering where the <code>sig * (1 - sig) * grad</code>&#13;
comes from. This is just the chain rule from calculus and corresponds&#13;
to the <code>output * (1 - output) * (output - target)</code> term in our previous&#13;
neural networks.</p>&#13;
&#13;
<p>Finally, you can see how we were able to make use of the <code>tensor_apply</code>&#13;
and the <code>tensor_combine</code> functions. Most of our layers will use these functions&#13;
similarly.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="The Linear Layer" data-type="sect1"><div class="sect1" id="idm45635729460808">&#13;
<h1>The Linear Layer</h1>&#13;
&#13;
<p>The<a data-primary="deep learning" data-secondary="linear layer" data-type="indexterm" id="idm45635729055208"/><a data-primary="linear layer" data-type="indexterm" id="idm45635729054200"/><a data-primary="Layers abstraction" data-secondary="linear layer" data-type="indexterm" id="idm45635729053528"/> other piece we’ll need to duplicate the neural networks from <a data-type="xref" href="ch18.html#neural_networks">Chapter 18</a>&#13;
is a “linear” layer that represents the <code>dot(weights, inputs)</code> part of the neurons.</p>&#13;
&#13;
<p>This layer will have parameters, which we’d like to initialize with random values.</p>&#13;
&#13;
<p>It turns out that the initial parameter values can make a huge difference&#13;
in how quickly (and sometimes <em>whether</em>) the network trains.&#13;
If weights are too big, they may produce large outputs&#13;
in a range where the activation function has near-zero gradients. And parts of the network&#13;
that have zero gradients necessarily can’t learn anything via gradient descent.</p>&#13;
&#13;
<p>Accordingly, we’ll implement<a data-primary="weight tensors, randomly generating" data-type="indexterm" id="idm45635729048888"/> three different schemes for randomly generating our weight tensors.&#13;
The first is to choose each value from the random uniform distribution on [0, 1]—that is, as a <code>random.random()</code>.&#13;
The second (and default) is to choose each value randomly from a standard normal distribution.&#13;
And<a data-primary="Xavier initialization" data-type="indexterm" id="idm45635729047416"/> the third is to use <em>Xavier initialization</em>,&#13;
where each weight is initialized with a random draw&#13;
from a normal distribution with mean 0 and variance&#13;
2 / (<code>num_inputs</code> + <code>num_outputs</code>). It turns out this often works nicely for neural network weights. We’ll implement these with a <code>random_uniform</code> function and a <code>random_normal</code> function:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="kn">import</code> <code class="nn">random</code>&#13;
&#13;
<code class="kn">from</code> <code class="nn">scratch.probability</code> <code class="kn">import</code> <code class="n">inverse_normal_cdf</code>&#13;
&#13;
<code class="k">def</code> <code class="nf">random_uniform</code><code class="p">(</code><code class="o">*</code><code class="n">dims</code><code class="p">:</code> <code class="nb">int</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="n">Tensor</code><code class="p">:</code>&#13;
    <code class="k">if</code> <code class="nb">len</code><code class="p">(</code><code class="n">dims</code><code class="p">)</code> <code class="o">==</code> <code class="mi">1</code><code class="p">:</code>&#13;
        <code class="k">return</code> <code class="p">[</code><code class="n">random</code><code class="o">.</code><code class="n">random</code><code class="p">()</code> <code class="k">for</code> <code class="n">_</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">dims</code><code class="p">[</code><code class="mi">0</code><code class="p">])]</code>&#13;
    <code class="k">else</code><code class="p">:</code>&#13;
        <code class="k">return</code> <code class="p">[</code><code class="n">random_uniform</code><code class="p">(</code><code class="o">*</code><code class="n">dims</code><code class="p">[</code><code class="mi">1</code><code class="p">:])</code> <code class="k">for</code> <code class="n">_</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">dims</code><code class="p">[</code><code class="mi">0</code><code class="p">])]</code>&#13;
&#13;
<code class="k">def</code> <code class="nf">random_normal</code><code class="p">(</code><code class="o">*</code><code class="n">dims</code><code class="p">:</code> <code class="nb">int</code><code class="p">,</code>&#13;
                  <code class="n">mean</code><code class="p">:</code> <code class="nb">float</code> <code class="o">=</code> <code class="mf">0.0</code><code class="p">,</code>&#13;
                  <code class="n">variance</code><code class="p">:</code> <code class="nb">float</code> <code class="o">=</code> <code class="mf">1.0</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="n">Tensor</code><code class="p">:</code>&#13;
    <code class="k">if</code> <code class="nb">len</code><code class="p">(</code><code class="n">dims</code><code class="p">)</code> <code class="o">==</code> <code class="mi">1</code><code class="p">:</code>&#13;
        <code class="k">return</code> <code class="p">[</code><code class="n">mean</code> <code class="o">+</code> <code class="n">variance</code> <code class="o">*</code> <code class="n">inverse_normal_cdf</code><code class="p">(</code><code class="n">random</code><code class="o">.</code><code class="n">random</code><code class="p">())</code>&#13;
                <code class="k">for</code> <code class="n">_</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">dims</code><code class="p">[</code><code class="mi">0</code><code class="p">])]</code>&#13;
    <code class="k">else</code><code class="p">:</code>&#13;
        <code class="k">return</code> <code class="p">[</code><code class="n">random_normal</code><code class="p">(</code><code class="o">*</code><code class="n">dims</code><code class="p">[</code><code class="mi">1</code><code class="p">:],</code> <code class="n">mean</code><code class="o">=</code><code class="n">mean</code><code class="p">,</code> <code class="n">variance</code><code class="o">=</code><code class="n">variance</code><code class="p">)</code>&#13;
                <code class="k">for</code> <code class="n">_</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">dims</code><code class="p">[</code><code class="mi">0</code><code class="p">])]</code>&#13;
&#13;
<code class="k">assert</code> <code class="n">shape</code><code class="p">(</code><code class="n">random_uniform</code><code class="p">(</code><code class="mi">2</code><code class="p">,</code> <code class="mi">3</code><code class="p">,</code> <code class="mi">4</code><code class="p">))</code> <code class="o">==</code> <code class="p">[</code><code class="mi">2</code><code class="p">,</code> <code class="mi">3</code><code class="p">,</code> <code class="mi">4</code><code class="p">]</code>&#13;
<code class="k">assert</code> <code class="n">shape</code><code class="p">(</code><code class="n">random_normal</code><code class="p">(</code><code class="mi">5</code><code class="p">,</code> <code class="mi">6</code><code class="p">,</code> <code class="n">mean</code><code class="o">=</code><code class="mi">10</code><code class="p">))</code> <code class="o">==</code> <code class="p">[</code><code class="mi">5</code><code class="p">,</code> <code class="mi">6</code><code class="p">]</code></pre>&#13;
&#13;
<p>And then wrap them all in a <code>random_tensor</code> function:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="k">def</code> <code class="nf">random_tensor</code><code class="p">(</code><code class="o">*</code><code class="n">dims</code><code class="p">:</code> <code class="nb">int</code><code class="p">,</code> <code class="n">init</code><code class="p">:</code> <code class="nb">str</code> <code class="o">=</code> <code class="s1">'normal'</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="n">Tensor</code><code class="p">:</code>&#13;
    <code class="k">if</code> <code class="n">init</code> <code class="o">==</code> <code class="s1">'normal'</code><code class="p">:</code>&#13;
        <code class="k">return</code> <code class="n">random_normal</code><code class="p">(</code><code class="o">*</code><code class="n">dims</code><code class="p">)</code>&#13;
    <code class="k">elif</code> <code class="n">init</code> <code class="o">==</code> <code class="s1">'uniform'</code><code class="p">:</code>&#13;
        <code class="k">return</code> <code class="n">random_uniform</code><code class="p">(</code><code class="o">*</code><code class="n">dims</code><code class="p">)</code>&#13;
    <code class="k">elif</code> <code class="n">init</code> <code class="o">==</code> <code class="s1">'xavier'</code><code class="p">:</code>&#13;
        <code class="n">variance</code> <code class="o">=</code> <code class="nb">len</code><code class="p">(</code><code class="n">dims</code><code class="p">)</code> <code class="o">/</code> <code class="nb">sum</code><code class="p">(</code><code class="n">dims</code><code class="p">)</code>&#13;
        <code class="k">return</code> <code class="n">random_normal</code><code class="p">(</code><code class="o">*</code><code class="n">dims</code><code class="p">,</code> <code class="n">variance</code><code class="o">=</code><code class="n">variance</code><code class="p">)</code>&#13;
    <code class="k">else</code><code class="p">:</code>&#13;
        <code class="k">raise</code> <code class="ne">ValueError</code><code class="p">(</code><code class="n">f</code><code class="s2">"unknown init: {init}"</code><code class="p">)</code></pre>&#13;
&#13;
<p>Now we can define our linear layer. We need to initialize it&#13;
with the dimension of the inputs (which tells us how many weights&#13;
each neuron needs), the dimension of the outputs (which tells us&#13;
how many neurons we should have), and the initialization scheme we want:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="kn">from</code> <code class="nn">scratch.linear_algebra</code> <code class="kn">import</code> <code class="n">dot</code>&#13;
&#13;
<code class="k">class</code> <code class="nc">Linear</code><code class="p">(</code><code class="n">Layer</code><code class="p">):</code>&#13;
    <code class="k">def</code> <code class="nf-Magic">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code>&#13;
                 <code class="n">input_dim</code><code class="p">:</code> <code class="nb">int</code><code class="p">,</code>&#13;
                 <code class="n">output_dim</code><code class="p">:</code> <code class="nb">int</code><code class="p">,</code>&#13;
                 <code class="n">init</code><code class="p">:</code> <code class="nb">str</code> <code class="o">=</code> <code class="s1">'xavier'</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="bp">None</code><code class="p">:</code>&#13;
        <code class="sd">"""</code>&#13;
<code class="sd">        A layer of output_dim neurons, each with input_dim weights</code>&#13;
<code class="sd">        (and a bias).</code>&#13;
<code class="sd">        """</code>&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">input_dim</code> <code class="o">=</code> <code class="n">input_dim</code>&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">output_dim</code> <code class="o">=</code> <code class="n">output_dim</code>&#13;
&#13;
        <code class="c1"># self.w[o] is the weights for the oth neuron</code>&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">w</code> <code class="o">=</code> <code class="n">random_tensor</code><code class="p">(</code><code class="n">output_dim</code><code class="p">,</code> <code class="n">input_dim</code><code class="p">,</code> <code class="n">init</code><code class="o">=</code><code class="n">init</code><code class="p">)</code>&#13;
&#13;
        <code class="c1"># self.b[o] is the bias term for the oth neuron</code>&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">b</code> <code class="o">=</code> <code class="n">random_tensor</code><code class="p">(</code><code class="n">output_dim</code><code class="p">,</code> <code class="n">init</code><code class="o">=</code><code class="n">init</code><code class="p">)</code></pre>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>In case you’re wondering how important the initialization&#13;
schemes are, some of the networks in this chapter I couldn’t&#13;
get to train at all with different initializations than the ones I used.</p>&#13;
</div>&#13;
&#13;
<p>The <code>forward</code> method is easy to implement.&#13;
We’ll get one output per neuron, which we stick&#13;
in a vector. And each neuron’s output is just the <code>dot</code> of&#13;
its weights with the input, plus its bias:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting">    <code class="k">def</code> <code class="nf">forward</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="nb">input</code><code class="p">:</code> <code class="n">Tensor</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="n">Tensor</code><code class="p">:</code>&#13;
        <code class="c1"># Save the input to use in the backward pass.</code>&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">input</code> <code class="o">=</code> <code class="nb">input</code>&#13;
&#13;
        <code class="c1"># Return the vector of neuron outputs.</code>&#13;
        <code class="k">return</code> <code class="p">[</code><code class="n">dot</code><code class="p">(</code><code class="nb">input</code><code class="p">,</code> <code class="bp">self</code><code class="o">.</code><code class="n">w</code><code class="p">[</code><code class="n">o</code><code class="p">])</code> <code class="o">+</code> <code class="bp">self</code><code class="o">.</code><code class="n">b</code><code class="p">[</code><code class="n">o</code><code class="p">]</code>&#13;
                <code class="k">for</code> <code class="n">o</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">output_dim</code><code class="p">)]</code></pre>&#13;
&#13;
<p>The <code>backward</code> method is more involved, but if you know calculus&#13;
it’s not difficult:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting">    <code class="k">def</code> <code class="nf">backward</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">gradient</code><code class="p">:</code> <code class="n">Tensor</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="n">Tensor</code><code class="p">:</code>&#13;
        <code class="c1"># Each b[o] gets added to output[o], which means</code>&#13;
        <code class="c1"># the gradient of b is the same as the output gradient.</code>&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">b_grad</code> <code class="o">=</code> <code class="n">gradient</code>&#13;
&#13;
        <code class="c1"># Each w[o][i] multiplies input[i] and gets added to output[o].</code>&#13;
        <code class="c1"># So its gradient is input[i] * gradient[o].</code>&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">w_grad</code> <code class="o">=</code> <code class="p">[[</code><code class="bp">self</code><code class="o">.</code><code class="n">input</code><code class="p">[</code><code class="n">i</code><code class="p">]</code> <code class="o">*</code> <code class="n">gradient</code><code class="p">[</code><code class="n">o</code><code class="p">]</code>&#13;
                        <code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">input_dim</code><code class="p">)]</code>&#13;
                       <code class="k">for</code> <code class="n">o</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">output_dim</code><code class="p">)]</code>&#13;
&#13;
        <code class="c1"># Each input[i] multiplies every w[o][i] and gets added to every</code>&#13;
        <code class="c1"># output[o]. So its gradient is the sum of w[o][i] * gradient[o]</code>&#13;
        <code class="c1"># across all the outputs.</code>&#13;
        <code class="k">return</code> <code class="p">[</code><code class="nb">sum</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">w</code><code class="p">[</code><code class="n">o</code><code class="p">][</code><code class="n">i</code><code class="p">]</code> <code class="o">*</code> <code class="n">gradient</code><code class="p">[</code><code class="n">o</code><code class="p">]</code> <code class="k">for</code> <code class="n">o</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">output_dim</code><code class="p">))</code>&#13;
                <code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">input_dim</code><code class="p">)]</code></pre>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>In a “real” tensor library, these (and many other) operations&#13;
would be represented as matrix or tensor multiplications,&#13;
which those libraries are designed to do very quickly.&#13;
Our library is <em>very</em> slow.</p>&#13;
</div>&#13;
&#13;
<p>Finally, here we do need to implement <code>params</code> and <code>grads</code>.&#13;
We have two parameters and two corresponding gradients:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting">    <code class="k">def</code> <code class="nf">params</code><code class="p">(</code><code class="bp">self</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="n">Iterable</code><code class="p">[</code><code class="n">Tensor</code><code class="p">]:</code>&#13;
        <code class="k">return</code> <code class="p">[</code><code class="bp">self</code><code class="o">.</code><code class="n">w</code><code class="p">,</code> <code class="bp">self</code><code class="o">.</code><code class="n">b</code><code class="p">]</code>&#13;
&#13;
    <code class="k">def</code> <code class="nf">grads</code><code class="p">(</code><code class="bp">self</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="n">Iterable</code><code class="p">[</code><code class="n">Tensor</code><code class="p">]:</code>&#13;
        <code class="k">return</code> <code class="p">[</code><code class="bp">self</code><code class="o">.</code><code class="n">w_grad</code><code class="p">,</code> <code class="bp">self</code><code class="o">.</code><code class="n">b_grad</code><code class="p">]</code></pre>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Neural Networks as a Sequence of Layers" data-type="sect1"><div class="sect1" id="idm45635729056184">&#13;
<h1>Neural Networks as a Sequence of Layers</h1>&#13;
&#13;
<p>We’d<a data-primary="deep learning" data-secondary="neural networks as sequences of layers" data-type="indexterm" id="idm45635728277496"/><a data-primary="neural networks" data-secondary="as sequences of layers" data-type="indexterm" id="idm45635728276552"/> like to think of neural networks as sequences of layers, so let’s&#13;
come up with a way to combine multiple layers into one. The resulting neural&#13;
network is itself a layer, and it implements the <code>Layer</code> methods in the&#13;
obvious ways:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="kn">from</code> <code class="nn">typing</code> <code class="kn">import</code> <code class="n">List</code>&#13;
&#13;
<code class="k">class</code> <code class="nc">Sequential</code><code class="p">(</code><code class="n">Layer</code><code class="p">):</code>&#13;
    <code class="sd">"""</code>&#13;
<code class="sd">    A layer consisting of a sequence of other layers.</code>&#13;
<code class="sd">    It's up to you to make sure that the output of each layer</code>&#13;
<code class="sd">    makes sense as the input to the next layer.</code>&#13;
<code class="sd">    """</code>&#13;
    <code class="k">def</code> <code class="nf-Magic">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">layers</code><code class="p">:</code> <code class="n">List</code><code class="p">[</code><code class="n">Layer</code><code class="p">])</code> <code class="o">-&gt;</code> <code class="bp">None</code><code class="p">:</code>&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">layers</code> <code class="o">=</code> <code class="n">layers</code>&#13;
&#13;
    <code class="k">def</code> <code class="nf">forward</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="nb">input</code><code class="p">):</code>&#13;
        <code class="sd">"""Just forward the input through the layers in order."""</code>&#13;
        <code class="k">for</code> <code class="n">layer</code> <code class="ow">in</code> <code class="bp">self</code><code class="o">.</code><code class="n">layers</code><code class="p">:</code>&#13;
            <code class="nb">input</code> <code class="o">=</code> <code class="n">layer</code><code class="o">.</code><code class="n">forward</code><code class="p">(</code><code class="nb">input</code><code class="p">)</code>&#13;
        <code class="k">return</code> <code class="nb">input</code>&#13;
&#13;
    <code class="k">def</code> <code class="nf">backward</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">gradient</code><code class="p">):</code>&#13;
        <code class="sd">"""Just backpropagate the gradient through the layers in reverse."""</code>&#13;
        <code class="k">for</code> <code class="n">layer</code> <code class="ow">in</code> <code class="nb">reversed</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">layers</code><code class="p">):</code>&#13;
            <code class="n">gradient</code> <code class="o">=</code> <code class="n">layer</code><code class="o">.</code><code class="n">backward</code><code class="p">(</code><code class="n">gradient</code><code class="p">)</code>&#13;
        <code class="k">return</code> <code class="n">gradient</code>&#13;
&#13;
    <code class="k">def</code> <code class="nf">params</code><code class="p">(</code><code class="bp">self</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="n">Iterable</code><code class="p">[</code><code class="n">Tensor</code><code class="p">]:</code>&#13;
        <code class="sd">"""Just return the params from each layer."""</code>&#13;
        <code class="k">return</code> <code class="p">(</code><code class="n">param</code> <code class="k">for</code> <code class="n">layer</code> <code class="ow">in</code> <code class="bp">self</code><code class="o">.</code><code class="n">layers</code> <code class="k">for</code> <code class="n">param</code> <code class="ow">in</code> <code class="n">layer</code><code class="o">.</code><code class="n">params</code><code class="p">())</code>&#13;
&#13;
    <code class="k">def</code> <code class="nf">grads</code><code class="p">(</code><code class="bp">self</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="n">Iterable</code><code class="p">[</code><code class="n">Tensor</code><code class="p">]:</code>&#13;
        <code class="sd">"""Just return the grads from each layer."""</code>&#13;
        <code class="k">return</code> <code class="p">(</code><code class="n">grad</code> <code class="k">for</code> <code class="n">layer</code> <code class="ow">in</code> <code class="bp">self</code><code class="o">.</code><code class="n">layers</code> <code class="k">for</code> <code class="n">grad</code> <code class="ow">in</code> <code class="n">layer</code><code class="o">.</code><code class="n">grads</code><code class="p">())</code></pre>&#13;
&#13;
<p>So we could represent the neural network we used for XOR as:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="n">xor_net</code> <code class="o">=</code> <code class="n">Sequential</code><code class="p">([</code>&#13;
    <code class="n">Linear</code><code class="p">(</code><code class="n">input_dim</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code> <code class="n">output_dim</code><code class="o">=</code><code class="mi">2</code><code class="p">),</code>&#13;
    <code class="n">Sigmoid</code><code class="p">(),</code>&#13;
    <code class="n">Linear</code><code class="p">(</code><code class="n">input_dim</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code> <code class="n">output_dim</code><code class="o">=</code><code class="mi">1</code><code class="p">),</code>&#13;
    <code class="n">Sigmoid</code><code class="p">()</code>&#13;
<code class="p">])</code></pre>&#13;
&#13;
<p>But we still need a little more machinery to train it.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Loss and Optimization" data-type="sect1"><div class="sect1" id="idm45635728026632">&#13;
<h1>Loss and Optimization</h1>&#13;
&#13;
<p>Previously<a data-primary="deep learning" data-secondary="loss and optimization" data-type="indexterm" id="idm45635728024808"/><a data-primary="loss functions" data-type="indexterm" id="idm45635728023800"/> we wrote out individual loss functions and gradient functions for our models.&#13;
Here we’ll want to experiment with different loss functions,&#13;
so (as usual) we’ll introduce a new <code>Loss</code> abstraction that&#13;
encapsulates both the loss computation and the gradient computation:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="k">class</code> <code class="nc">Loss</code><code class="p">:</code>&#13;
    <code class="k">def</code> <code class="nf">loss</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">predicted</code><code class="p">:</code> <code class="n">Tensor</code><code class="p">,</code> <code class="n">actual</code><code class="p">:</code> <code class="n">Tensor</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">float</code><code class="p">:</code>&#13;
        <code class="sd">"""How good are our predictions? (Larger numbers are worse.)"""</code>&#13;
        <code class="k">raise</code> <code class="ne">NotImplementedError</code>&#13;
&#13;
    <code class="k">def</code> <code class="nf">gradient</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">predicted</code><code class="p">:</code> <code class="n">Tensor</code><code class="p">,</code> <code class="n">actual</code><code class="p">:</code> <code class="n">Tensor</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="n">Tensor</code><code class="p">:</code>&#13;
        <code class="sd">"""How does the loss change as the predictions change?"""</code>&#13;
        <code class="k">raise</code> <code class="ne">NotImplementedError</code></pre>&#13;
&#13;
<p>We’ve already worked many times with the loss that’s the sum of the squared errors,&#13;
so we should have an easy time implementing that. The only trick is that we’ll need to use <code>tensor_combine</code>:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="k">class</code> <code class="nc">SSE</code><code class="p">(</code><code class="n">Loss</code><code class="p">):</code>&#13;
    <code class="sd">"""Loss function that computes the sum of the squared errors."""</code>&#13;
    <code class="k">def</code> <code class="nf">loss</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">predicted</code><code class="p">:</code> <code class="n">Tensor</code><code class="p">,</code> <code class="n">actual</code><code class="p">:</code> <code class="n">Tensor</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">float</code><code class="p">:</code>&#13;
        <code class="c1"># Compute the tensor of squared differences</code>&#13;
        <code class="n">squared_errors</code> <code class="o">=</code> <code class="n">tensor_combine</code><code class="p">(</code>&#13;
            <code class="k">lambda</code> <code class="n">predicted</code><code class="p">,</code> <code class="n">actual</code><code class="p">:</code> <code class="p">(</code><code class="n">predicted</code> <code class="o">-</code> <code class="n">actual</code><code class="p">)</code> <code class="o">**</code> <code class="mi">2</code><code class="p">,</code>&#13;
            <code class="n">predicted</code><code class="p">,</code>&#13;
            <code class="n">actual</code><code class="p">)</code>&#13;
&#13;
        <code class="c1"># And just add them up</code>&#13;
        <code class="k">return</code> <code class="n">tensor_sum</code><code class="p">(</code><code class="n">squared_errors</code><code class="p">)</code>&#13;
&#13;
    <code class="k">def</code> <code class="nf">gradient</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">predicted</code><code class="p">:</code> <code class="n">Tensor</code><code class="p">,</code> <code class="n">actual</code><code class="p">:</code> <code class="n">Tensor</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="n">Tensor</code><code class="p">:</code>&#13;
        <code class="k">return</code> <code class="n">tensor_combine</code><code class="p">(</code>&#13;
            <code class="k">lambda</code> <code class="n">predicted</code><code class="p">,</code> <code class="n">actual</code><code class="p">:</code> <code class="mi">2</code> <code class="o">*</code> <code class="p">(</code><code class="n">predicted</code> <code class="o">-</code> <code class="n">actual</code><code class="p">),</code>&#13;
            <code class="n">predicted</code><code class="p">,</code>&#13;
            <code class="n">actual</code><code class="p">)</code></pre>&#13;
&#13;
<p>(We’ll look at a different loss function in a bit.)</p>&#13;
&#13;
<p>The<a data-primary="gradient descent" data-secondary="Optimizer abstraction for" data-type="indexterm" id="idm45635727867576"/><a data-primary="Optimizer abstraction" data-type="indexterm" id="idm45635727801080"/> last piece to figure out&#13;
is gradient descent. Throughout the book we’ve done all of our gradient&#13;
descent manually by having a training loop that involves something like:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="n">theta</code> <code class="o">=</code> <code class="n">gradient_step</code><code class="p">(</code><code class="n">theta</code><code class="p">,</code> <code class="n">grad</code><code class="p">,</code> <code class="o">-</code><code class="n">learning_rate</code><code class="p">)</code></pre>&#13;
&#13;
<p>Here that won’t quite work for us, for a couple reasons. The first is that&#13;
our neural nets will have many parameters, and we’ll need to update all of them.&#13;
The second is that we’d like to be able to use more clever variants of gradient&#13;
descent, and we don’t want to have to rewrite them each time.</p>&#13;
&#13;
<p>Accordingly, we’ll introduce a (you guessed it) <code>Optimizer</code> abstraction,&#13;
of which gradient descent will be a specific instance:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="k">class</code> <code class="nc">Optimizer</code><code class="p">:</code>&#13;
    <code class="sd">"""</code>&#13;
<code class="sd">    An optimizer updates the weights of a layer (in place) using information</code>&#13;
<code class="sd">    known by either the layer or the optimizer (or by both).</code>&#13;
<code class="sd">    """</code>&#13;
    <code class="k">def</code> <code class="nf">step</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">layer</code><code class="p">:</code> <code class="n">Layer</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="bp">None</code><code class="p">:</code>&#13;
        <code class="k">raise</code> <code class="ne">NotImplementedError</code></pre>&#13;
&#13;
<p>After that it’s easy to implement gradient descent, again using <code>tensor_combine</code>:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="k">class</code> <code class="nc">GradientDescent</code><code class="p">(</code><code class="n">Optimizer</code><code class="p">):</code>&#13;
    <code class="k">def</code> <code class="nf-Magic">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">learning_rate</code><code class="p">:</code> <code class="nb">float</code> <code class="o">=</code> <code class="mf">0.1</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="bp">None</code><code class="p">:</code>&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">lr</code> <code class="o">=</code> <code class="n">learning_rate</code>&#13;
&#13;
    <code class="k">def</code> <code class="nf">step</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">layer</code><code class="p">:</code> <code class="n">Layer</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="bp">None</code><code class="p">:</code>&#13;
        <code class="k">for</code> <code class="n">param</code><code class="p">,</code> <code class="n">grad</code> <code class="ow">in</code> <code class="nb">zip</code><code class="p">(</code><code class="n">layer</code><code class="o">.</code><code class="n">params</code><code class="p">(),</code> <code class="n">layer</code><code class="o">.</code><code class="n">grads</code><code class="p">()):</code>&#13;
            <code class="c1"># Update param using a gradient step</code>&#13;
            <code class="n">param</code><code class="p">[:]</code> <code class="o">=</code> <code class="n">tensor_combine</code><code class="p">(</code>&#13;
                <code class="k">lambda</code> <code class="n">param</code><code class="p">,</code> <code class="n">grad</code><code class="p">:</code> <code class="n">param</code> <code class="o">-</code> <code class="n">grad</code> <code class="o">*</code> <code class="bp">self</code><code class="o">.</code><code class="n">lr</code><code class="p">,</code>&#13;
                <code class="n">param</code><code class="p">,</code>&#13;
                <code class="n">grad</code><code class="p">)</code></pre>&#13;
&#13;
<p>The only thing that’s maybe surprising is the “slice assignment,”&#13;
which is a reflection of the fact that reassigning a list doesn’t&#13;
change its original value. That is, if you just did <code>param = tensor_combine(. . .)</code>,&#13;
you would be redefining the local variable <code>param</code>, but you would not be affecting&#13;
the original parameter tensor stored in the layer. If you assign to the slice <code>[:]</code>, however, it actually changes the values inside the list.</p>&#13;
&#13;
<p>Here’s a simple example to demonstrate:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="n">tensor</code> <code class="o">=</code> <code class="p">[[</code><code class="mi">1</code><code class="p">,</code> <code class="mi">2</code><code class="p">],</code> <code class="p">[</code><code class="mi">3</code><code class="p">,</code> <code class="mi">4</code><code class="p">]]</code>&#13;
&#13;
<code class="k">for</code> <code class="n">row</code> <code class="ow">in</code> <code class="n">tensor</code><code class="p">:</code>&#13;
    <code class="n">row</code> <code class="o">=</code> <code class="p">[</code><code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">]</code>&#13;
<code class="k">assert</code> <code class="n">tensor</code> <code class="o">==</code> <code class="p">[[</code><code class="mi">1</code><code class="p">,</code> <code class="mi">2</code><code class="p">],</code> <code class="p">[</code><code class="mi">3</code><code class="p">,</code> <code class="mi">4</code><code class="p">]],</code> <code class="s2">"assignment doesn't update a list"</code>&#13;
&#13;
<code class="k">for</code> <code class="n">row</code> <code class="ow">in</code> <code class="n">tensor</code><code class="p">:</code>&#13;
    <code class="n">row</code><code class="p">[:]</code> <code class="o">=</code> <code class="p">[</code><code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">]</code>&#13;
<code class="k">assert</code> <code class="n">tensor</code> <code class="o">==</code> <code class="p">[[</code><code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">],</code> <code class="p">[</code><code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">]],</code> <code class="s2">"but slice assignment does"</code></pre>&#13;
&#13;
<p>If you are somewhat inexperienced in Python, this behavior may be surprising, so meditate on it and try examples yourself until it makes sense.</p>&#13;
&#13;
<p>To<a data-primary="momentum" data-type="indexterm" id="idm45635727572056"/> demonstrate the value of this abstraction, let’s implement another optimizer that&#13;
uses <em>momentum</em>. The idea is that we don’t want to overreact to each new gradient,&#13;
and so we maintain a running average of the gradients we’ve seen, updating it with&#13;
each new gradient and taking a step in the direction of the average:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="k">class</code> <code class="nc">Momentum</code><code class="p">(</code><code class="n">Optimizer</code><code class="p">):</code>&#13;
    <code class="k">def</code> <code class="nf-Magic">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code>&#13;
                 <code class="n">learning_rate</code><code class="p">:</code> <code class="nb">float</code><code class="p">,</code>&#13;
                 <code class="n">momentum</code><code class="p">:</code> <code class="nb">float</code> <code class="o">=</code> <code class="mf">0.9</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="bp">None</code><code class="p">:</code>&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">lr</code> <code class="o">=</code> <code class="n">learning_rate</code>&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">mo</code> <code class="o">=</code> <code class="n">momentum</code>&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">updates</code><code class="p">:</code> <code class="n">List</code><code class="p">[</code><code class="n">Tensor</code><code class="p">]</code> <code class="o">=</code> <code class="p">[]</code>  <code class="c1"># running average</code>&#13;
&#13;
    <code class="k">def</code> <code class="nf">step</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">layer</code><code class="p">:</code> <code class="n">Layer</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="bp">None</code><code class="p">:</code>&#13;
        <code class="c1"># If we have no previous updates, start with all zeros</code>&#13;
        <code class="k">if</code> <code class="ow">not</code> <code class="bp">self</code><code class="o">.</code><code class="n">updates</code><code class="p">:</code>&#13;
            <code class="bp">self</code><code class="o">.</code><code class="n">updates</code> <code class="o">=</code> <code class="p">[</code><code class="n">zeros_like</code><code class="p">(</code><code class="n">grad</code><code class="p">)</code> <code class="k">for</code> <code class="n">grad</code> <code class="ow">in</code> <code class="n">layer</code><code class="o">.</code><code class="n">grads</code><code class="p">()]</code>&#13;
&#13;
        <code class="k">for</code> <code class="n">update</code><code class="p">,</code> <code class="n">param</code><code class="p">,</code> <code class="n">grad</code> <code class="ow">in</code> <code class="nb">zip</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">updates</code><code class="p">,</code>&#13;
                                       <code class="n">layer</code><code class="o">.</code><code class="n">params</code><code class="p">(),</code>&#13;
                                       <code class="n">layer</code><code class="o">.</code><code class="n">grads</code><code class="p">()):</code>&#13;
            <code class="c1"># Apply momentum</code>&#13;
            <code class="n">update</code><code class="p">[:]</code> <code class="o">=</code> <code class="n">tensor_combine</code><code class="p">(</code>&#13;
                <code class="k">lambda</code> <code class="n">u</code><code class="p">,</code> <code class="n">g</code><code class="p">:</code> <code class="bp">self</code><code class="o">.</code><code class="n">mo</code> <code class="o">*</code> <code class="n">u</code> <code class="o">+</code> <code class="p">(</code><code class="mi">1</code> <code class="o">-</code> <code class="bp">self</code><code class="o">.</code><code class="n">mo</code><code class="p">)</code> <code class="o">*</code> <code class="n">g</code><code class="p">,</code>&#13;
                <code class="n">update</code><code class="p">,</code>&#13;
                <code class="n">grad</code><code class="p">)</code>&#13;
&#13;
            <code class="c1"># Then take a gradient step</code>&#13;
            <code class="n">param</code><code class="p">[:]</code> <code class="o">=</code> <code class="n">tensor_combine</code><code class="p">(</code>&#13;
                <code class="k">lambda</code> <code class="n">p</code><code class="p">,</code> <code class="n">u</code><code class="p">:</code> <code class="n">p</code> <code class="o">-</code> <code class="bp">self</code><code class="o">.</code><code class="n">lr</code> <code class="o">*</code> <code class="n">u</code><code class="p">,</code>&#13;
                <code class="n">param</code><code class="p">,</code>&#13;
                <code class="n">update</code><code class="p">)</code></pre>&#13;
&#13;
<p>Because we used an <code>Optimizer</code> abstraction, we can easily switch between our different optimizers.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Example: XOR Revisited" data-type="sect1"><div class="sect1" id="idm45635728025720">&#13;
<h1>Example: XOR Revisited</h1>&#13;
&#13;
<p>Let’s<a data-primary="deep learning" data-secondary="XOR example" data-type="indexterm" id="idm45635727465336"/><a data-primary="XOR example" data-type="indexterm" id="idm45635727291288"/> see how easy it is to use our new framework to train a network that can compute XOR.&#13;
We start by re-creating the training data:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="c1"># training data</code>&#13;
<code class="n">xs</code> <code class="o">=</code> <code class="p">[[</code><code class="mf">0.</code><code class="p">,</code> <code class="mi">0</code><code class="p">],</code> <code class="p">[</code><code class="mf">0.</code><code class="p">,</code> <code class="mi">1</code><code class="p">],</code> <code class="p">[</code><code class="mf">1.</code><code class="p">,</code> <code class="mi">0</code><code class="p">],</code> <code class="p">[</code><code class="mf">1.</code><code class="p">,</code> <code class="mi">1</code><code class="p">]]</code>&#13;
<code class="n">ys</code> <code class="o">=</code> <code class="p">[[</code><code class="mf">0.</code><code class="p">],</code> <code class="p">[</code><code class="mf">1.</code><code class="p">],</code> <code class="p">[</code><code class="mf">1.</code><code class="p">],</code> <code class="p">[</code><code class="mf">0.</code><code class="p">]]</code></pre>&#13;
&#13;
<p>and then we define the network, although now we can leave off the last sigmoid layer:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="n">random</code><code class="o">.</code><code class="n">seed</code><code class="p">(</code><code class="mi">0</code><code class="p">)</code>&#13;
&#13;
<code class="n">net</code> <code class="o">=</code> <code class="n">Sequential</code><code class="p">([</code>&#13;
    <code class="n">Linear</code><code class="p">(</code><code class="n">input_dim</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code> <code class="n">output_dim</code><code class="o">=</code><code class="mi">2</code><code class="p">),</code>&#13;
    <code class="n">Sigmoid</code><code class="p">(),</code>&#13;
    <code class="n">Linear</code><code class="p">(</code><code class="n">input_dim</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code> <code class="n">output_dim</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>&#13;
<code class="p">])</code></pre>&#13;
&#13;
<p>We can now write a simple training loop, except that now we can use&#13;
the abstractions of <code>Optimizer</code> and <code>Loss</code>. This allows us to easily&#13;
try different ones:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="kn">import</code> <code class="nn">tqdm</code>&#13;
&#13;
<code class="n">optimizer</code> <code class="o">=</code> <code class="n">GradientDescent</code><code class="p">(</code><code class="n">learning_rate</code><code class="o">=</code><code class="mf">0.1</code><code class="p">)</code>&#13;
<code class="n">loss</code> <code class="o">=</code> <code class="n">SSE</code><code class="p">()</code>&#13;
&#13;
<code class="k">with</code> <code class="n">tqdm</code><code class="o">.</code><code class="n">trange</code><code class="p">(</code><code class="mi">3000</code><code class="p">)</code> <code class="k">as</code> <code class="n">t</code><code class="p">:</code>&#13;
    <code class="k">for</code> <code class="n">epoch</code> <code class="ow">in</code> <code class="n">t</code><code class="p">:</code>&#13;
        <code class="n">epoch_loss</code> <code class="o">=</code> <code class="mf">0.0</code>&#13;
&#13;
        <code class="k">for</code> <code class="n">x</code><code class="p">,</code> <code class="n">y</code> <code class="ow">in</code> <code class="nb">zip</code><code class="p">(</code><code class="n">xs</code><code class="p">,</code> <code class="n">ys</code><code class="p">):</code>&#13;
            <code class="n">predicted</code> <code class="o">=</code> <code class="n">net</code><code class="o">.</code><code class="n">forward</code><code class="p">(</code><code class="n">x</code><code class="p">)</code>&#13;
            <code class="n">epoch_loss</code> <code class="o">+=</code> <code class="n">loss</code><code class="o">.</code><code class="n">loss</code><code class="p">(</code><code class="n">predicted</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code>&#13;
            <code class="n">gradient</code> <code class="o">=</code> <code class="n">loss</code><code class="o">.</code><code class="n">gradient</code><code class="p">(</code><code class="n">predicted</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code>&#13;
            <code class="n">net</code><code class="o">.</code><code class="n">backward</code><code class="p">(</code><code class="n">gradient</code><code class="p">)</code>&#13;
&#13;
            <code class="n">optimizer</code><code class="o">.</code><code class="n">step</code><code class="p">(</code><code class="n">net</code><code class="p">)</code>&#13;
&#13;
        <code class="n">t</code><code class="o">.</code><code class="n">set_description</code><code class="p">(</code><code class="n">f</code><code class="s2">"xor loss {epoch_loss:.3f}"</code><code class="p">)</code></pre>&#13;
&#13;
<p>This should train quickly, and you should see the loss go down.&#13;
And now we can inspect the weights:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="k">for</code> <code class="n">param</code> <code class="ow">in</code> <code class="n">net</code><code class="o">.</code><code class="n">params</code><code class="p">():</code>&#13;
    <code class="k">print</code><code class="p">(</code><code class="n">param</code><code class="p">)</code></pre>&#13;
&#13;
<p>For my network I find roughly:</p>&#13;
&#13;
<pre data-type="programlisting">hidden1 = -2.6 * x1 + -2.7 * x2 + 0.2  # NOR&#13;
hidden2 =  2.1 * x1 +  2.1 * x2 - 3.4  # AND&#13;
output =  -3.1 * h1 + -2.6 * h2 + 1.8  # NOR</pre>&#13;
&#13;
<p>So <code>hidden1</code> activates if neither input is 1. <code>hidden2</code> activates if both inputs are 1.&#13;
And <code>output</code> activates if neither hidden output is 1—that is, if it’s not the case&#13;
that neither input is 1 and it’s also not the case&#13;
that both inputs are 1. Indeed, this is exactly the logic of XOR.</p>&#13;
&#13;
<p>Notice that this network learned different features than the one we trained in <a data-type="xref" href="ch18.html#neural_networks">Chapter 18</a>, but it still manages to do the same thing.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Other Activation Functions" data-type="sect1"><div class="sect1" id="idm45635727466280">&#13;
<h1>Other Activation Functions</h1>&#13;
&#13;
<p>The <code>sigmoid</code> function<a data-primary="sigmoid function" data-type="indexterm" id="idm45635726989096"/><a data-primary="deep learning" data-secondary="other activation functions" data-type="indexterm" id="idm45635726988360"/><a data-primary="activation functions" data-type="indexterm" id="idm45635726987448"/> has fallen out of favor for a couple of reasons.&#13;
One reason is that <code>sigmoid(0)</code> equals 1/2, which means that a neuron whose inputs sum to 0 has a positive output. Another is that its gradient is very close to 0 for very large and very small inputs, which means that its gradients can get “saturated” and its weights can get stuck.</p>&#13;
&#13;
<p>One<a data-primary="tanh function" data-type="indexterm" id="idm45635726985576"/> popular replacement is <code>tanh</code> (“hyperbolic tangent”), which is a different sigmoid-shaped&#13;
function that ranges from –1 to 1 and outputs 0 if its input is 0. The derivative of <code>tanh(x)</code> is just <code>1 - tanh(x) ** 2</code>, which makes the layer easy to write:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="kn">import</code> <code class="nn">math</code>&#13;
&#13;
<code class="k">def</code> <code class="nf">tanh</code><code class="p">(</code><code class="n">x</code><code class="p">:</code> <code class="nb">float</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">float</code><code class="p">:</code>&#13;
    <code class="c1"># If x is very large or very small, tanh is (essentially) 1 or -1.</code>&#13;
    <code class="c1"># We check for this because, e.g., math.exp(1000) raises an error.</code>&#13;
    <code class="k">if</code> <code class="n">x</code> <code class="o">&lt;</code> <code class="o">-</code><code class="mi">100</code><code class="p">:</code>  <code class="k">return</code> <code class="o">-</code><code class="mi">1</code>&#13;
    <code class="k">elif</code> <code class="n">x</code> <code class="o">&gt;</code> <code class="mi">100</code><code class="p">:</code> <code class="k">return</code> <code class="mi">1</code>&#13;
&#13;
    <code class="n">em2x</code> <code class="o">=</code> <code class="n">math</code><code class="o">.</code><code class="n">exp</code><code class="p">(</code><code class="o">-</code><code class="mi">2</code> <code class="o">*</code> <code class="n">x</code><code class="p">)</code>&#13;
    <code class="k">return</code> <code class="p">(</code><code class="mi">1</code> <code class="o">-</code> <code class="n">em2x</code><code class="p">)</code> <code class="o">/</code> <code class="p">(</code><code class="mi">1</code> <code class="o">+</code> <code class="n">em2x</code><code class="p">)</code>&#13;
&#13;
<code class="k">class</code> <code class="nc">Tanh</code><code class="p">(</code><code class="n">Layer</code><code class="p">):</code>&#13;
    <code class="k">def</code> <code class="nf">forward</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="nb">input</code><code class="p">:</code> <code class="n">Tensor</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="n">Tensor</code><code class="p">:</code>&#13;
        <code class="c1"># Save tanh output to use in backward pass.</code>&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">tanh</code> <code class="o">=</code> <code class="n">tensor_apply</code><code class="p">(</code><code class="n">tanh</code><code class="p">,</code> <code class="nb">input</code><code class="p">)</code>&#13;
        <code class="k">return</code> <code class="bp">self</code><code class="o">.</code><code class="n">tanh</code>&#13;
&#13;
    <code class="k">def</code> <code class="nf">backward</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">gradient</code><code class="p">:</code> <code class="n">Tensor</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="n">Tensor</code><code class="p">:</code>&#13;
        <code class="k">return</code> <code class="n">tensor_combine</code><code class="p">(</code>&#13;
            <code class="k">lambda</code> <code class="n">tanh</code><code class="p">,</code> <code class="n">grad</code><code class="p">:</code> <code class="p">(</code><code class="mi">1</code> <code class="o">-</code> <code class="n">tanh</code> <code class="o">**</code> <code class="mi">2</code><code class="p">)</code> <code class="o">*</code> <code class="n">grad</code><code class="p">,</code>&#13;
            <code class="bp">self</code><code class="o">.</code><code class="n">tanh</code><code class="p">,</code>&#13;
            <code class="n">gradient</code><code class="p">)</code></pre>&#13;
&#13;
<p>In larger networks another popular replacement is <code>Relu</code>,&#13;
which is 0 for negative inputs and the identity for positive inputs:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="k">class</code> <code class="nc">Relu</code><code class="p">(</code><code class="n">Layer</code><code class="p">):</code>&#13;
    <code class="k">def</code> <code class="nf">forward</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="nb">input</code><code class="p">:</code> <code class="n">Tensor</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="n">Tensor</code><code class="p">:</code>&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">input</code> <code class="o">=</code> <code class="nb">input</code>&#13;
        <code class="k">return</code> <code class="n">tensor_apply</code><code class="p">(</code><code class="k">lambda</code> <code class="n">x</code><code class="p">:</code> <code class="nb">max</code><code class="p">(</code><code class="n">x</code><code class="p">,</code> <code class="mi">0</code><code class="p">),</code> <code class="nb">input</code><code class="p">)</code>&#13;
&#13;
    <code class="k">def</code> <code class="nf">backward</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">gradient</code><code class="p">:</code> <code class="n">Tensor</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="n">Tensor</code><code class="p">:</code>&#13;
        <code class="k">return</code> <code class="n">tensor_combine</code><code class="p">(</code><code class="k">lambda</code> <code class="n">x</code><code class="p">,</code> <code class="n">grad</code><code class="p">:</code> <code class="n">grad</code> <code class="k">if</code> <code class="n">x</code> <code class="o">&gt;</code> <code class="mi">0</code> <code class="k">else</code> <code class="mi">0</code><code class="p">,</code>&#13;
                              <code class="bp">self</code><code class="o">.</code><code class="n">input</code><code class="p">,</code>&#13;
                              <code class="n">gradient</code><code class="p">)</code></pre>&#13;
&#13;
<p>There are many others. I encourage you to play around with them in your networks.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Example: FizzBuzz Revisited" data-type="sect1"><div class="sect1" id="idm45635726760232">&#13;
<h1>Example: FizzBuzz Revisited</h1>&#13;
&#13;
<p>We<a data-primary="deep learning" data-secondary="Fizz Buzz example" data-type="indexterm" id="idm45635726646392"/><a data-primary="Fizz Buzz example" data-type="indexterm" id="idm45635726645384"/> can now use our “deep learning” framework to&#13;
reproduce our solution from <a data-type="xref" href="ch18.html#fizzbuzz">“Example: Fizz Buzz”</a>. Let’s set up the data:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="kn">from</code> <code class="nn">scratch.neural_networks</code> <code class="kn">import</code> <code class="n">binary_encode</code><code class="p">,</code> <code class="n">fizz_buzz_encode</code><code class="p">,</code> <code class="n">argmax</code>&#13;
&#13;
<code class="n">xs</code> <code class="o">=</code> <code class="p">[</code><code class="n">binary_encode</code><code class="p">(</code><code class="n">n</code><code class="p">)</code> <code class="k">for</code> <code class="n">n</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">101</code><code class="p">,</code> <code class="mi">1024</code><code class="p">)]</code>&#13;
<code class="n">ys</code> <code class="o">=</code> <code class="p">[</code><code class="n">fizz_buzz_encode</code><code class="p">(</code><code class="n">n</code><code class="p">)</code> <code class="k">for</code> <code class="n">n</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">101</code><code class="p">,</code> <code class="mi">1024</code><code class="p">)]</code></pre>&#13;
&#13;
<p>and create the network:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="n">NUM_HIDDEN</code> <code class="o">=</code> <code class="mi">25</code>&#13;
&#13;
<code class="n">random</code><code class="o">.</code><code class="n">seed</code><code class="p">(</code><code class="mi">0</code><code class="p">)</code>&#13;
&#13;
<code class="n">net</code> <code class="o">=</code> <code class="n">Sequential</code><code class="p">([</code>&#13;
    <code class="n">Linear</code><code class="p">(</code><code class="n">input_dim</code><code class="o">=</code><code class="mi">10</code><code class="p">,</code> <code class="n">output_dim</code><code class="o">=</code><code class="n">NUM_HIDDEN</code><code class="p">,</code> <code class="n">init</code><code class="o">=</code><code class="s1">'uniform'</code><code class="p">),</code>&#13;
    <code class="n">Tanh</code><code class="p">(),</code>&#13;
    <code class="n">Linear</code><code class="p">(</code><code class="n">input_dim</code><code class="o">=</code><code class="n">NUM_HIDDEN</code><code class="p">,</code> <code class="n">output_dim</code><code class="o">=</code><code class="mi">4</code><code class="p">,</code> <code class="n">init</code><code class="o">=</code><code class="s1">'uniform'</code><code class="p">),</code>&#13;
    <code class="n">Sigmoid</code><code class="p">()</code>&#13;
<code class="p">])</code></pre>&#13;
&#13;
<p>As we’re training, let’s also track our accuracy on the training set:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="k">def</code> <code class="nf">fizzbuzz_accuracy</code><code class="p">(</code><code class="n">low</code><code class="p">:</code> <code class="nb">int</code><code class="p">,</code> <code class="n">hi</code><code class="p">:</code> <code class="nb">int</code><code class="p">,</code> <code class="n">net</code><code class="p">:</code> <code class="n">Layer</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">float</code><code class="p">:</code>&#13;
    <code class="n">num_correct</code> <code class="o">=</code> <code class="mi">0</code>&#13;
    <code class="k">for</code> <code class="n">n</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">low</code><code class="p">,</code> <code class="n">hi</code><code class="p">):</code>&#13;
        <code class="n">x</code> <code class="o">=</code> <code class="n">binary_encode</code><code class="p">(</code><code class="n">n</code><code class="p">)</code>&#13;
        <code class="n">predicted</code> <code class="o">=</code> <code class="n">argmax</code><code class="p">(</code><code class="n">net</code><code class="o">.</code><code class="n">forward</code><code class="p">(</code><code class="n">x</code><code class="p">))</code>&#13;
        <code class="n">actual</code> <code class="o">=</code> <code class="n">argmax</code><code class="p">(</code><code class="n">fizz_buzz_encode</code><code class="p">(</code><code class="n">n</code><code class="p">))</code>&#13;
        <code class="k">if</code> <code class="n">predicted</code> <code class="o">==</code> <code class="n">actual</code><code class="p">:</code>&#13;
            <code class="n">num_correct</code> <code class="o">+=</code> <code class="mi">1</code>&#13;
&#13;
    <code class="k">return</code> <code class="n">num_correct</code> <code class="o">/</code> <code class="p">(</code><code class="n">hi</code> <code class="o">-</code> <code class="n">low</code><code class="p">)</code></pre>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="n">optimizer</code> <code class="o">=</code> <code class="n">Momentum</code><code class="p">(</code><code class="n">learning_rate</code><code class="o">=</code><code class="mf">0.1</code><code class="p">,</code> <code class="n">momentum</code><code class="o">=</code><code class="mf">0.9</code><code class="p">)</code>&#13;
<code class="n">loss</code> <code class="o">=</code> <code class="n">SSE</code><code class="p">()</code>&#13;
&#13;
<code class="k">with</code> <code class="n">tqdm</code><code class="o">.</code><code class="n">trange</code><code class="p">(</code><code class="mi">1000</code><code class="p">)</code> <code class="k">as</code> <code class="n">t</code><code class="p">:</code>&#13;
    <code class="k">for</code> <code class="n">epoch</code> <code class="ow">in</code> <code class="n">t</code><code class="p">:</code>&#13;
        <code class="n">epoch_loss</code> <code class="o">=</code> <code class="mf">0.0</code>&#13;
&#13;
        <code class="k">for</code> <code class="n">x</code><code class="p">,</code> <code class="n">y</code> <code class="ow">in</code> <code class="nb">zip</code><code class="p">(</code><code class="n">xs</code><code class="p">,</code> <code class="n">ys</code><code class="p">):</code>&#13;
            <code class="n">predicted</code> <code class="o">=</code> <code class="n">net</code><code class="o">.</code><code class="n">forward</code><code class="p">(</code><code class="n">x</code><code class="p">)</code>&#13;
            <code class="n">epoch_loss</code> <code class="o">+=</code> <code class="n">loss</code><code class="o">.</code><code class="n">loss</code><code class="p">(</code><code class="n">predicted</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code>&#13;
            <code class="n">gradient</code> <code class="o">=</code> <code class="n">loss</code><code class="o">.</code><code class="n">gradient</code><code class="p">(</code><code class="n">predicted</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code>&#13;
            <code class="n">net</code><code class="o">.</code><code class="n">backward</code><code class="p">(</code><code class="n">gradient</code><code class="p">)</code>&#13;
&#13;
            <code class="n">optimizer</code><code class="o">.</code><code class="n">step</code><code class="p">(</code><code class="n">net</code><code class="p">)</code>&#13;
&#13;
        <code class="n">accuracy</code> <code class="o">=</code> <code class="n">fizzbuzz_accuracy</code><code class="p">(</code><code class="mi">101</code><code class="p">,</code> <code class="mi">1024</code><code class="p">,</code> <code class="n">net</code><code class="p">)</code>&#13;
        <code class="n">t</code><code class="o">.</code><code class="n">set_description</code><code class="p">(</code><code class="n">f</code><code class="s2">"fb loss: {epoch_loss:.2f} acc: {accuracy:.2f}"</code><code class="p">)</code>&#13;
&#13;
<code class="c1"># Now check results on the test set</code>&#13;
<code class="k">print</code><code class="p">(</code><code class="s2">"test results"</code><code class="p">,</code> <code class="n">fizzbuzz_accuracy</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="mi">101</code><code class="p">,</code> <code class="n">net</code><code class="p">))</code></pre>&#13;
&#13;
<p>After 1,000 training iterations, the model gets 90% accuracy&#13;
on the test set; if you keep training it longer, it should do&#13;
even better. (I don’t think it’s possible to train to 100% accuracy&#13;
with only 25 hidden units, but it’s definitely possible if you&#13;
go up to 50 hidden units.)</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Softmaxes and Cross-Entropy" data-type="sect1"><div class="sect1" id="idm45635726390280">&#13;
<h1>Softmaxes and Cross-Entropy</h1>&#13;
&#13;
<p>The<a data-primary="deep learning" data-secondary="softmaxes and cross-entropy" data-type="indexterm" id="idm45635726388712"/><a data-primary="softmax function" data-type="indexterm" id="idm45635726261288"/><a data-primary="cross-entropy loss function" data-type="indexterm" id="idm45635726260616"/><a data-primary="loss functions" data-type="indexterm" id="idm45635726259976"/> neural net we used in the previous section ended in a <code>Sigmoid</code>&#13;
layer, which means that its output was a vector of numbers between 0 and 1.&#13;
In particular, it could output a vector that was entirely 0s,&#13;
or it could output a vector that was entirely 1s.&#13;
Yet when we’re&#13;
doing classification problems, we’d like to output a 1 for the correct class&#13;
and a 0 for all the incorrect classes. Generally our predictions will not be&#13;
so perfect, but we’d at least like to predict an actual probability distribution over the classes.</p>&#13;
&#13;
<p>For example, if we have two classes, and our model outputs <code>[0, 0]</code>, it’s hard&#13;
to make much sense of that. It doesn’t think the output belongs in either class?</p>&#13;
&#13;
<p>But if our model outputs <code>[0.4, 0.6]</code>, we can interpret&#13;
it as a prediction that there’s a probability of 0.4 that our input belongs to the first class&#13;
and 0.6 that our input belongs to the second class.</p>&#13;
&#13;
<p>In order to accomplish this, we typically forgo the final <code>Sigmoid</code>&#13;
layer and instead use the <code>softmax</code> function, which converts a vector&#13;
of real numbers to a vector of probabilities. We compute <code>exp(x)</code> for each number in the vector, which results in a vector of positive numbers.&#13;
After that, we just divide each of those positive numbers by the sum,&#13;
which gives us a bunch of positive numbers that add up to 1—that is,&#13;
a vector of probabilities.</p>&#13;
&#13;
<p>If we ever end up trying to compute, say, <code>exp(1000)</code> we will get a Python&#13;
error, so before taking the <code>exp</code> we subtract off the largest value. This turns&#13;
out to result in the same probabilities; it’s just safer to compute in Python:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="k">def</code> <code class="nf">softmax</code><code class="p">(</code><code class="n">tensor</code><code class="p">:</code> <code class="n">Tensor</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="n">Tensor</code><code class="p">:</code>&#13;
    <code class="sd">"""Softmax along the last dimension"""</code>&#13;
    <code class="k">if</code> <code class="n">is_1d</code><code class="p">(</code><code class="n">tensor</code><code class="p">):</code>&#13;
        <code class="c1"># Subtract largest value for numerical stability.</code>&#13;
        <code class="n">largest</code> <code class="o">=</code> <code class="nb">max</code><code class="p">(</code><code class="n">tensor</code><code class="p">)</code>&#13;
        <code class="n">exps</code> <code class="o">=</code> <code class="p">[</code><code class="n">math</code><code class="o">.</code><code class="n">exp</code><code class="p">(</code><code class="n">x</code> <code class="o">-</code> <code class="n">largest</code><code class="p">)</code> <code class="k">for</code> <code class="n">x</code> <code class="ow">in</code> <code class="n">tensor</code><code class="p">]</code>&#13;
&#13;
        <code class="n">sum_of_exps</code> <code class="o">=</code> <code class="nb">sum</code><code class="p">(</code><code class="n">exps</code><code class="p">)</code>                 <code class="c1"># This is the total "weight."</code>&#13;
        <code class="k">return</code> <code class="p">[</code><code class="n">exp_i</code> <code class="o">/</code> <code class="n">sum_of_exps</code>             <code class="c1"># Probability is the fraction</code>&#13;
                <code class="k">for</code> <code class="n">exp_i</code> <code class="ow">in</code> <code class="n">exps</code><code class="p">]</code>              <code class="c1"># of the total weight.</code>&#13;
    <code class="k">else</code><code class="p">:</code>&#13;
        <code class="k">return</code> <code class="p">[</code><code class="n">softmax</code><code class="p">(</code><code class="n">tensor_i</code><code class="p">)</code> <code class="k">for</code> <code class="n">tensor_i</code> <code class="ow">in</code> <code class="n">tensor</code><code class="p">]</code></pre>&#13;
&#13;
<p>Once our network produces probabilities, we often use a different&#13;
loss function called <em>cross-entropy</em> (or sometimes “negative log likelihood”).</p>&#13;
&#13;
<p>You may recall that in <a data-type="xref" href="ch14.html#maximum_likelihood_estimation">“Maximum Likelihood Estimation”</a>, we justified the use&#13;
of least squares in linear regression by appealing to the fact that&#13;
(under certain assumptions) the least squares coefficients maximized the likelihood&#13;
of the observed data.</p>&#13;
&#13;
<p>Here we can do something similar: if our network outputs are probabilities,&#13;
the cross-entropy loss represents the negative log likelihood of the observed&#13;
data, which means that minimizing that loss is the same as maximizing the&#13;
log likelihood (and hence the likelihood) of the training data.</p>&#13;
&#13;
<p>Typically we won’t include the <code>softmax</code> function as part of the neural network itself.&#13;
This is because it turns out that&#13;
if <code>softmax</code> is part of your loss function but not part of the network itself,&#13;
the gradients of the loss with respect to the network outputs are very easy to compute.</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="k">class</code> <code class="nc">SoftmaxCrossEntropy</code><code class="p">(</code><code class="n">Loss</code><code class="p">):</code>&#13;
    <code class="sd">"""</code>&#13;
<code class="sd">    This is the negative-log-likelihood of the observed values, given the</code>&#13;
<code class="sd">    neural net model. So if we choose weights to minimize it, our model will</code>&#13;
<code class="sd">    be maximizing the likelihood of the observed data.</code>&#13;
<code class="sd">    """</code>&#13;
    <code class="k">def</code> <code class="nf">loss</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">predicted</code><code class="p">:</code> <code class="n">Tensor</code><code class="p">,</code> <code class="n">actual</code><code class="p">:</code> <code class="n">Tensor</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">float</code><code class="p">:</code>&#13;
        <code class="c1"># Apply softmax to get probabilities</code>&#13;
        <code class="n">probabilities</code> <code class="o">=</code> <code class="n">softmax</code><code class="p">(</code><code class="n">predicted</code><code class="p">)</code>&#13;
&#13;
        <code class="c1"># This will be log p_i for the actual class i and 0 for the other</code>&#13;
        <code class="c1"># classes. We add a tiny amount to p to avoid taking log(0).</code>&#13;
        <code class="n">likelihoods</code> <code class="o">=</code> <code class="n">tensor_combine</code><code class="p">(</code><code class="k">lambda</code> <code class="n">p</code><code class="p">,</code> <code class="n">act</code><code class="p">:</code> <code class="n">math</code><code class="o">.</code><code class="n">log</code><code class="p">(</code><code class="n">p</code> <code class="o">+</code> <code class="mf">1e-30</code><code class="p">)</code> <code class="o">*</code> <code class="n">act</code><code class="p">,</code>&#13;
                                     <code class="n">probabilities</code><code class="p">,</code>&#13;
                                     <code class="n">actual</code><code class="p">)</code>&#13;
&#13;
        <code class="c1"># And then we just sum up the negatives.</code>&#13;
        <code class="k">return</code> <code class="o">-</code><code class="n">tensor_sum</code><code class="p">(</code><code class="n">likelihoods</code><code class="p">)</code>&#13;
&#13;
    <code class="k">def</code> <code class="nf">gradient</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">predicted</code><code class="p">:</code> <code class="n">Tensor</code><code class="p">,</code> <code class="n">actual</code><code class="p">:</code> <code class="n">Tensor</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="n">Tensor</code><code class="p">:</code>&#13;
        <code class="n">probabilities</code> <code class="o">=</code> <code class="n">softmax</code><code class="p">(</code><code class="n">predicted</code><code class="p">)</code>&#13;
&#13;
        <code class="c1"># Isn't this a pleasant equation?</code>&#13;
        <code class="k">return</code> <code class="n">tensor_combine</code><code class="p">(</code><code class="k">lambda</code> <code class="n">p</code><code class="p">,</code> <code class="n">actual</code><code class="p">:</code> <code class="n">p</code> <code class="o">-</code> <code class="n">actual</code><code class="p">,</code>&#13;
                              <code class="n">probabilities</code><code class="p">,</code>&#13;
                              <code class="n">actual</code><code class="p">)</code></pre>&#13;
&#13;
<p>If I now train the same Fizz Buzz network&#13;
using <code>SoftmaxCrossEntropy</code> loss, I find that it typically&#13;
trains much faster (that is, in many fewer epochs).&#13;
Presumably this is because it is much easier to find weights&#13;
that <code>softmax</code> to a given distribution than it is to find weights&#13;
that <code>sigmoid</code> to a given distribution.</p>&#13;
&#13;
<p>That is, if I need to predict&#13;
class 0 (a vector with a 1 in the first position and 0s in the remaining positions),&#13;
in the <code>linear</code> + <code>sigmoid</code> case I need the first output to be a large positive number&#13;
and the remaining outputs to be large negative numbers. In the <code>softmax</code> case, however, I just need the first output to be <em>larger than</em> the remaining outputs.&#13;
Clearly there are a lot more ways for the second case to happen, which suggests&#13;
that it should be easier to find weights that make it so:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="n">random</code><code class="o">.</code><code class="n">seed</code><code class="p">(</code><code class="mi">0</code><code class="p">)</code>&#13;
&#13;
<code class="n">net</code> <code class="o">=</code> <code class="n">Sequential</code><code class="p">([</code>&#13;
    <code class="n">Linear</code><code class="p">(</code><code class="n">input_dim</code><code class="o">=</code><code class="mi">10</code><code class="p">,</code> <code class="n">output_dim</code><code class="o">=</code><code class="n">NUM_HIDDEN</code><code class="p">,</code> <code class="n">init</code><code class="o">=</code><code class="s1">'uniform'</code><code class="p">),</code>&#13;
    <code class="n">Tanh</code><code class="p">(),</code>&#13;
    <code class="n">Linear</code><code class="p">(</code><code class="n">input_dim</code><code class="o">=</code><code class="n">NUM_HIDDEN</code><code class="p">,</code> <code class="n">output_dim</code><code class="o">=</code><code class="mi">4</code><code class="p">,</code> <code class="n">init</code><code class="o">=</code><code class="s1">'uniform'</code><code class="p">)</code>&#13;
    <code class="c1"># No final sigmoid layer now</code>&#13;
<code class="p">])</code>&#13;
&#13;
<code class="n">optimizer</code> <code class="o">=</code> <code class="n">Momentum</code><code class="p">(</code><code class="n">learning_rate</code><code class="o">=</code><code class="mf">0.1</code><code class="p">,</code> <code class="n">momentum</code><code class="o">=</code><code class="mf">0.9</code><code class="p">)</code>&#13;
<code class="n">loss</code> <code class="o">=</code> <code class="n">SoftmaxCrossEntropy</code><code class="p">()</code>&#13;
&#13;
<code class="k">with</code> <code class="n">tqdm</code><code class="o">.</code><code class="n">trange</code><code class="p">(</code><code class="mi">100</code><code class="p">)</code> <code class="k">as</code> <code class="n">t</code><code class="p">:</code>&#13;
    <code class="k">for</code> <code class="n">epoch</code> <code class="ow">in</code> <code class="n">t</code><code class="p">:</code>&#13;
        <code class="n">epoch_loss</code> <code class="o">=</code> <code class="mf">0.0</code>&#13;
&#13;
        <code class="k">for</code> <code class="n">x</code><code class="p">,</code> <code class="n">y</code> <code class="ow">in</code> <code class="nb">zip</code><code class="p">(</code><code class="n">xs</code><code class="p">,</code> <code class="n">ys</code><code class="p">):</code>&#13;
            <code class="n">predicted</code> <code class="o">=</code> <code class="n">net</code><code class="o">.</code><code class="n">forward</code><code class="p">(</code><code class="n">x</code><code class="p">)</code>&#13;
            <code class="n">epoch_loss</code> <code class="o">+=</code> <code class="n">loss</code><code class="o">.</code><code class="n">loss</code><code class="p">(</code><code class="n">predicted</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code>&#13;
            <code class="n">gradient</code> <code class="o">=</code> <code class="n">loss</code><code class="o">.</code><code class="n">gradient</code><code class="p">(</code><code class="n">predicted</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code>&#13;
            <code class="n">net</code><code class="o">.</code><code class="n">backward</code><code class="p">(</code><code class="n">gradient</code><code class="p">)</code>&#13;
&#13;
            <code class="n">optimizer</code><code class="o">.</code><code class="n">step</code><code class="p">(</code><code class="n">net</code><code class="p">)</code>&#13;
&#13;
        <code class="n">accuracy</code> <code class="o">=</code> <code class="n">fizzbuzz_accuracy</code><code class="p">(</code><code class="mi">101</code><code class="p">,</code> <code class="mi">1024</code><code class="p">,</code> <code class="n">net</code><code class="p">)</code>&#13;
        <code class="n">t</code><code class="o">.</code><code class="n">set_description</code><code class="p">(</code><code class="n">f</code><code class="s2">"fb loss: {epoch_loss:.3f} acc: {accuracy:.2f}"</code><code class="p">)</code>&#13;
&#13;
<code class="c1"># Again check results on the test set</code>&#13;
<code class="k">print</code><code class="p">(</code><code class="s2">"test results"</code><code class="p">,</code> <code class="n">fizzbuzz_accuracy</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="mi">101</code><code class="p">,</code> <code class="n">net</code><code class="p">))</code></pre>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Dropout" data-type="sect1"><div class="sect1" id="idm45635726389688">&#13;
<h1>Dropout</h1>&#13;
&#13;
<p>Like<a data-primary="deep learning" data-secondary="dropout" data-type="indexterm" id="idm45635725888424"/><a data-primary="Dropout layer" data-type="indexterm" id="idm45635725887416"/><a data-primary="Layers abstraction" data-secondary="Dropout layer" data-type="indexterm" id="idm45635725788840"/> most machine learning models, neural networks are prone to overfitting to&#13;
their training data. We’ve previously seen ways to ameliorate this;&#13;
for example, in <a data-type="xref" href="ch15.html#regularization">“Regularization”</a> we penalized large weights and that helped prevent overfitting.</p>&#13;
&#13;
<p>A common way of regularizing neural networks is using <em>dropout</em>.&#13;
At training time, we randomly turn off each neuron&#13;
(that is, replace its output with 0)&#13;
with some fixed probability.&#13;
This means that the network can’t learn to depend on any individual neuron,&#13;
which seems to help with overfitting.</p>&#13;
&#13;
<p>At evaluation time, we don’t want to dropout any neurons, so a <code>Dropout</code>&#13;
layer will need to know whether it’s training or not. In addition, at&#13;
training time a <code>Dropout</code> layer only passes on some random fraction of&#13;
its input. To make its output comparable during evaluation, we’ll scale&#13;
down the outputs (uniformly) using that same fraction:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="k">class</code> <code class="nc">Dropout</code><code class="p">(</code><code class="n">Layer</code><code class="p">):</code>&#13;
    <code class="k">def</code> <code class="nf-Magic">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">p</code><code class="p">:</code> <code class="nb">float</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="bp">None</code><code class="p">:</code>&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">p</code> <code class="o">=</code> <code class="n">p</code>&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">train</code> <code class="o">=</code> <code class="bp">True</code>&#13;
&#13;
    <code class="k">def</code> <code class="nf">forward</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="nb">input</code><code class="p">:</code> <code class="n">Tensor</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="n">Tensor</code><code class="p">:</code>&#13;
        <code class="k">if</code> <code class="bp">self</code><code class="o">.</code><code class="n">train</code><code class="p">:</code>&#13;
            <code class="c1"># Create a mask of 0s and 1s shaped like the input</code>&#13;
            <code class="c1"># using the specified probability.</code>&#13;
            <code class="bp">self</code><code class="o">.</code><code class="n">mask</code> <code class="o">=</code> <code class="n">tensor_apply</code><code class="p">(</code>&#13;
                <code class="k">lambda</code> <code class="n">_</code><code class="p">:</code> <code class="mi">0</code> <code class="k">if</code> <code class="n">random</code><code class="o">.</code><code class="n">random</code><code class="p">()</code> <code class="o">&lt;</code> <code class="bp">self</code><code class="o">.</code><code class="n">p</code> <code class="k">else</code> <code class="mi">1</code><code class="p">,</code>&#13;
                <code class="nb">input</code><code class="p">)</code>&#13;
            <code class="c1"># Multiply by the mask to dropout inputs.</code>&#13;
            <code class="k">return</code> <code class="n">tensor_combine</code><code class="p">(</code><code class="n">operator</code><code class="o">.</code><code class="n">mul</code><code class="p">,</code> <code class="nb">input</code><code class="p">,</code> <code class="bp">self</code><code class="o">.</code><code class="n">mask</code><code class="p">)</code>&#13;
        <code class="k">else</code><code class="p">:</code>&#13;
            <code class="c1"># During evaluation just scale down the outputs uniformly.</code>&#13;
            <code class="k">return</code> <code class="n">tensor_apply</code><code class="p">(</code><code class="k">lambda</code> <code class="n">x</code><code class="p">:</code> <code class="n">x</code> <code class="o">*</code> <code class="p">(</code><code class="mi">1</code> <code class="o">-</code> <code class="bp">self</code><code class="o">.</code><code class="n">p</code><code class="p">),</code> <code class="nb">input</code><code class="p">)</code>&#13;
&#13;
    <code class="k">def</code> <code class="nf">backward</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">gradient</code><code class="p">:</code> <code class="n">Tensor</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="n">Tensor</code><code class="p">:</code>&#13;
        <code class="k">if</code> <code class="bp">self</code><code class="o">.</code><code class="n">train</code><code class="p">:</code>&#13;
            <code class="c1"># Only propagate the gradients where mask == 1.</code>&#13;
            <code class="k">return</code> <code class="n">tensor_combine</code><code class="p">(</code><code class="n">operator</code><code class="o">.</code><code class="n">mul</code><code class="p">,</code> <code class="n">gradient</code><code class="p">,</code> <code class="bp">self</code><code class="o">.</code><code class="n">mask</code><code class="p">)</code>&#13;
        <code class="k">else</code><code class="p">:</code>&#13;
            <code class="k">raise</code> <code class="ne">RuntimeError</code><code class="p">(</code><code class="s2">"don't call backward when not in train mode"</code><code class="p">)</code></pre>&#13;
&#13;
<p>We’ll use this to help prevent our deep learning models from overfitting.</p>&#13;
&#13;
<p> </p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Example: MNIST" data-type="sect1"><div class="sect1" id="idm45635725780968">&#13;
<h1>Example: MNIST</h1>&#13;
&#13;
<p><a href="http://yann.lecun.com/exdb/mnist/">MNIST</a> is a<a data-primary="deep learning" data-secondary="MNIST example" data-type="indexterm" id="DLmnist19"/><a data-primary="MNIST dataset example" data-type="indexterm" id="mnist19"/> dataset of handwritten digits that everyone uses to learn deep learning.</p>&#13;
&#13;
<p>It is available in a somewhat tricky binary format, so we’ll install the <code>mnist</code> library to work with it.&#13;
(Yes, this part is technically not “from scratch.”)</p>&#13;
&#13;
<pre data-code-language="bash" data-type="programlisting">python -m pip install mnist</pre>&#13;
&#13;
<p>And then we can load the data:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="kn">import</code> <code class="nn">mnist</code>&#13;
&#13;
<code class="c1"># This will download the data; change this to where you want it.</code>&#13;
<code class="c1"># (Yes, it's a 0-argument function, that's what the library expects.)</code>&#13;
<code class="c1"># (Yes, I'm assigning a lambda to a variable, like I said never to do.)</code>&#13;
<code class="n">mnist</code><code class="o">.</code><code class="n">temporary_dir</code> <code class="o">=</code> <code class="k">lambda</code><code class="p">:</code> <code class="s1">'/tmp'</code>&#13;
&#13;
<code class="c1"># Each of these functions first downloads the data and returns a numpy array.</code>&#13;
<code class="c1"># We call .tolist() because our "tensors" are just lists.</code>&#13;
<code class="n">train_images</code> <code class="o">=</code> <code class="n">mnist</code><code class="o">.</code><code class="n">train_images</code><code class="p">()</code><code class="o">.</code><code class="n">tolist</code><code class="p">()</code>&#13;
<code class="n">train_labels</code> <code class="o">=</code> <code class="n">mnist</code><code class="o">.</code><code class="n">train_labels</code><code class="p">()</code><code class="o">.</code><code class="n">tolist</code><code class="p">()</code>&#13;
&#13;
<code class="k">assert</code> <code class="n">shape</code><code class="p">(</code><code class="n">train_images</code><code class="p">)</code> <code class="o">==</code> <code class="p">[</code><code class="mi">60000</code><code class="p">,</code> <code class="mi">28</code><code class="p">,</code> <code class="mi">28</code><code class="p">]</code>&#13;
<code class="k">assert</code> <code class="n">shape</code><code class="p">(</code><code class="n">train_labels</code><code class="p">)</code> <code class="o">==</code> <code class="p">[</code><code class="mi">60000</code><code class="p">]</code></pre>&#13;
&#13;
<p>Let’s plot the first 100 training images to see what they look like (<a data-type="xref" href="#mnist_images">Figure 19-1</a>):</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="kn">import</code> <code class="nn">matplotlib.pyplot</code> <code class="kn">as</code> <code class="nn">plt</code>&#13;
&#13;
<code class="n">fig</code><code class="p">,</code> <code class="n">ax</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">(</code><code class="mi">10</code><code class="p">,</code> <code class="mi">10</code><code class="p">)</code>&#13;
&#13;
<code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">10</code><code class="p">):</code>&#13;
    <code class="k">for</code> <code class="n">j</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">10</code><code class="p">):</code>&#13;
        <code class="c1"># Plot each image in black and white and hide the axes.</code>&#13;
        <code class="n">ax</code><code class="p">[</code><code class="n">i</code><code class="p">][</code><code class="n">j</code><code class="p">]</code><code class="o">.</code><code class="n">imshow</code><code class="p">(</code><code class="n">train_images</code><code class="p">[</code><code class="mi">10</code> <code class="o">*</code> <code class="n">i</code> <code class="o">+</code> <code class="n">j</code><code class="p">],</code> <code class="n">cmap</code><code class="o">=</code><code class="s1">'Greys'</code><code class="p">)</code>&#13;
        <code class="n">ax</code><code class="p">[</code><code class="n">i</code><code class="p">][</code><code class="n">j</code><code class="p">]</code><code class="o">.</code><code class="n">xaxis</code><code class="o">.</code><code class="n">set_visible</code><code class="p">(</code><code class="bp">False</code><code class="p">)</code>&#13;
        <code class="n">ax</code><code class="p">[</code><code class="n">i</code><code class="p">][</code><code class="n">j</code><code class="p">]</code><code class="o">.</code><code class="n">yaxis</code><code class="o">.</code><code class="n">set_visible</code><code class="p">(</code><code class="bp">False</code><code class="p">)</code>&#13;
&#13;
<code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">()</code></pre>&#13;
&#13;
<figure><div class="figure" id="mnist_images">&#13;
<img alt="MNIST images" src="assets/dsf2_1901.png"/>&#13;
<h6><span class="label">Figure 19-1. </span>MNIST images</h6>&#13;
</div></figure>&#13;
&#13;
<p>You can see that indeed they look like handwritten digits.</p>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>My first attempt at showing the images resulted in&#13;
yellow numbers on black backgrounds. I am neither clever&#13;
nor subtle enough to know that I needed to add <code>cmap=<em>Greys</em></code>&#13;
to get black-and-white images; I Googled it and found the&#13;
solution on Stack Overflow. As a data scientist you will&#13;
become quite adept at this workflow.</p>&#13;
</div>&#13;
&#13;
<p>We also need to load the test images:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="n">test_images</code> <code class="o">=</code> <code class="n">mnist</code><code class="o">.</code><code class="n">test_images</code><code class="p">()</code><code class="o">.</code><code class="n">tolist</code><code class="p">()</code>&#13;
<code class="n">test_labels</code> <code class="o">=</code> <code class="n">mnist</code><code class="o">.</code><code class="n">test_labels</code><code class="p">()</code><code class="o">.</code><code class="n">tolist</code><code class="p">()</code>&#13;
&#13;
<code class="k">assert</code> <code class="n">shape</code><code class="p">(</code><code class="n">test_images</code><code class="p">)</code> <code class="o">==</code> <code class="p">[</code><code class="mi">10000</code><code class="p">,</code> <code class="mi">28</code><code class="p">,</code> <code class="mi">28</code><code class="p">]</code>&#13;
<code class="k">assert</code> <code class="n">shape</code><code class="p">(</code><code class="n">test_labels</code><code class="p">)</code> <code class="o">==</code> <code class="p">[</code><code class="mi">10000</code><code class="p">]</code></pre>&#13;
&#13;
<p>Each image is 28 × 28 pixels, but our linear layers can only deal with&#13;
one-dimensional inputs, so we’ll just flatten them (and also divide by 256 to get them between 0 and 1). In addition, our neural net will train better if our inputs are 0 on average, so we’ll subtract out the average value:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="c1"># Compute the average pixel value</code>&#13;
<code class="n">avg</code> <code class="o">=</code> <code class="n">tensor_sum</code><code class="p">(</code><code class="n">train_images</code><code class="p">)</code> <code class="o">/</code> <code class="mi">60000</code> <code class="o">/</code> <code class="mi">28</code> <code class="o">/</code> <code class="mi">28</code>&#13;
&#13;
<code class="c1"># Recenter, rescale, and flatten</code>&#13;
<code class="n">train_images</code> <code class="o">=</code> <code class="p">[[(</code><code class="n">pixel</code> <code class="o">-</code> <code class="n">avg</code><code class="p">)</code> <code class="o">/</code> <code class="mi">256</code> <code class="k">for</code> <code class="n">row</code> <code class="ow">in</code> <code class="n">image</code> <code class="k">for</code> <code class="n">pixel</code> <code class="ow">in</code> <code class="n">row</code><code class="p">]</code>&#13;
                <code class="k">for</code> <code class="n">image</code> <code class="ow">in</code> <code class="n">train_images</code><code class="p">]</code>&#13;
<code class="n">test_images</code> <code class="o">=</code> <code class="p">[[(</code><code class="n">pixel</code> <code class="o">-</code> <code class="n">avg</code><code class="p">)</code> <code class="o">/</code> <code class="mi">256</code> <code class="k">for</code> <code class="n">row</code> <code class="ow">in</code> <code class="n">image</code> <code class="k">for</code> <code class="n">pixel</code> <code class="ow">in</code> <code class="n">row</code><code class="p">]</code>&#13;
               <code class="k">for</code> <code class="n">image</code> <code class="ow">in</code> <code class="n">test_images</code><code class="p">]</code>&#13;
&#13;
<code class="k">assert</code> <code class="n">shape</code><code class="p">(</code><code class="n">train_images</code><code class="p">)</code> <code class="o">==</code> <code class="p">[</code><code class="mi">60000</code><code class="p">,</code> <code class="mi">784</code><code class="p">],</code> <code class="s2">"images should be flattened"</code>&#13;
<code class="k">assert</code> <code class="n">shape</code><code class="p">(</code><code class="n">test_images</code><code class="p">)</code> <code class="o">==</code> <code class="p">[</code><code class="mi">10000</code><code class="p">,</code> <code class="mi">784</code><code class="p">],</code> <code class="s2">"images should be flattened"</code>&#13;
&#13;
<code class="c1"># After centering, average pixel should be very close to 0</code>&#13;
<code class="k">assert</code> <code class="o">-</code><code class="mf">0.0001</code> <code class="o">&lt;</code> <code class="n">tensor_sum</code><code class="p">(</code><code class="n">train_images</code><code class="p">)</code> <code class="o">&lt;</code> <code class="mf">0.0001</code></pre>&#13;
&#13;
<p>We also want to one-hot-encode the targets, since we have 10 outputs.&#13;
First let’s write a <code>one_hot_encode</code> function:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="k">def</code> <code class="nf">one_hot_encode</code><code class="p">(</code><code class="n">i</code><code class="p">:</code> <code class="nb">int</code><code class="p">,</code> <code class="n">num_labels</code><code class="p">:</code> <code class="nb">int</code> <code class="o">=</code> <code class="mi">10</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="n">List</code><code class="p">[</code><code class="nb">float</code><code class="p">]:</code>&#13;
    <code class="k">return</code> <code class="p">[</code><code class="mf">1.0</code> <code class="k">if</code> <code class="n">j</code> <code class="o">==</code> <code class="n">i</code> <code class="k">else</code> <code class="mf">0.0</code> <code class="k">for</code> <code class="n">j</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">num_labels</code><code class="p">)]</code>&#13;
&#13;
<code class="k">assert</code> <code class="n">one_hot_encode</code><code class="p">(</code><code class="mi">3</code><code class="p">)</code> <code class="o">==</code> <code class="p">[</code><code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">1</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">]</code>&#13;
<code class="k">assert</code> <code class="n">one_hot_encode</code><code class="p">(</code><code class="mi">2</code><code class="p">,</code> <code class="n">num_labels</code><code class="o">=</code><code class="mi">5</code><code class="p">)</code> <code class="o">==</code> <code class="p">[</code><code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">1</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">]</code></pre>&#13;
&#13;
<p>and then apply it to our data:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="n">train_labels</code> <code class="o">=</code> <code class="p">[</code><code class="n">one_hot_encode</code><code class="p">(</code><code class="n">label</code><code class="p">)</code> <code class="k">for</code> <code class="n">label</code> <code class="ow">in</code> <code class="n">train_labels</code><code class="p">]</code>&#13;
<code class="n">test_labels</code> <code class="o">=</code> <code class="p">[</code><code class="n">one_hot_encode</code><code class="p">(</code><code class="n">label</code><code class="p">)</code> <code class="k">for</code> <code class="n">label</code> <code class="ow">in</code> <code class="n">test_labels</code><code class="p">]</code>&#13;
&#13;
<code class="k">assert</code> <code class="n">shape</code><code class="p">(</code><code class="n">train_labels</code><code class="p">)</code> <code class="o">==</code> <code class="p">[</code><code class="mi">60000</code><code class="p">,</code> <code class="mi">10</code><code class="p">]</code>&#13;
<code class="k">assert</code> <code class="n">shape</code><code class="p">(</code><code class="n">test_labels</code><code class="p">)</code> <code class="o">==</code> <code class="p">[</code><code class="mi">10000</code><code class="p">,</code> <code class="mi">10</code><code class="p">]</code></pre>&#13;
&#13;
<p>One of the strengths of our abstractions is that we can&#13;
use the same training/evaluation loop with a variety of models.&#13;
So let’s write that first. We’ll pass it our model, the data,&#13;
a loss function, and (if we’re training) an optimizer.</p>&#13;
&#13;
<p>It will make a pass through our data, track performance, and&#13;
(if we passed in an optimizer) update our parameters:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="kn">import</code> <code class="nn">tqdm</code>&#13;
&#13;
<code class="k">def</code> <code class="nf">loop</code><code class="p">(</code><code class="n">model</code><code class="p">:</code> <code class="n">Layer</code><code class="p">,</code>&#13;
         <code class="n">images</code><code class="p">:</code> <code class="n">List</code><code class="p">[</code><code class="n">Tensor</code><code class="p">],</code>&#13;
         <code class="n">labels</code><code class="p">:</code> <code class="n">List</code><code class="p">[</code><code class="n">Tensor</code><code class="p">],</code>&#13;
         <code class="n">loss</code><code class="p">:</code> <code class="n">Loss</code><code class="p">,</code>&#13;
         <code class="n">optimizer</code><code class="p">:</code> <code class="n">Optimizer</code> <code class="o">=</code> <code class="bp">None</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="bp">None</code><code class="p">:</code>&#13;
    <code class="n">correct</code> <code class="o">=</code> <code class="mi">0</code>         <code class="c1"># Track number of correct predictions.</code>&#13;
    <code class="n">total_loss</code> <code class="o">=</code> <code class="mf">0.0</code>    <code class="c1"># Track total loss.</code>&#13;
&#13;
    <code class="k">with</code> <code class="n">tqdm</code><code class="o">.</code><code class="n">trange</code><code class="p">(</code><code class="nb">len</code><code class="p">(</code><code class="n">images</code><code class="p">))</code> <code class="k">as</code> <code class="n">t</code><code class="p">:</code>&#13;
        <code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="n">t</code><code class="p">:</code>&#13;
            <code class="n">predicted</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">forward</code><code class="p">(</code><code class="n">images</code><code class="p">[</code><code class="n">i</code><code class="p">])</code>             <code class="c1"># Predict.</code>&#13;
            <code class="k">if</code> <code class="n">argmax</code><code class="p">(</code><code class="n">predicted</code><code class="p">)</code> <code class="o">==</code> <code class="n">argmax</code><code class="p">(</code><code class="n">labels</code><code class="p">[</code><code class="n">i</code><code class="p">]):</code>       <code class="c1"># Check for</code>&#13;
                <code class="n">correct</code> <code class="o">+=</code> <code class="mi">1</code>                                 <code class="c1"># correctness.</code>&#13;
            <code class="n">total_loss</code> <code class="o">+=</code> <code class="n">loss</code><code class="o">.</code><code class="n">loss</code><code class="p">(</code><code class="n">predicted</code><code class="p">,</code> <code class="n">labels</code><code class="p">[</code><code class="n">i</code><code class="p">])</code>    <code class="c1"># Compute loss.</code>&#13;
&#13;
            <code class="c1"># If we're training, backpropagate gradient and update weights.</code>&#13;
            <code class="k">if</code> <code class="n">optimizer</code> <code class="ow">is</code> <code class="ow">not</code> <code class="bp">None</code><code class="p">:</code>&#13;
                <code class="n">gradient</code> <code class="o">=</code> <code class="n">loss</code><code class="o">.</code><code class="n">gradient</code><code class="p">(</code><code class="n">predicted</code><code class="p">,</code> <code class="n">labels</code><code class="p">[</code><code class="n">i</code><code class="p">])</code>&#13;
                <code class="n">model</code><code class="o">.</code><code class="n">backward</code><code class="p">(</code><code class="n">gradient</code><code class="p">)</code>&#13;
                <code class="n">optimizer</code><code class="o">.</code><code class="n">step</code><code class="p">(</code><code class="n">model</code><code class="p">)</code>&#13;
&#13;
            <code class="c1"># And update our metrics in the progress bar.</code>&#13;
            <code class="n">avg_loss</code> <code class="o">=</code> <code class="n">total_loss</code> <code class="o">/</code> <code class="p">(</code><code class="n">i</code> <code class="o">+</code> <code class="mi">1</code><code class="p">)</code>&#13;
            <code class="n">acc</code> <code class="o">=</code> <code class="n">correct</code> <code class="o">/</code> <code class="p">(</code><code class="n">i</code> <code class="o">+</code> <code class="mi">1</code><code class="p">)</code>&#13;
            <code class="n">t</code><code class="o">.</code><code class="n">set_description</code><code class="p">(</code><code class="n">f</code><code class="s2">"mnist loss: {avg_loss:.3f} acc: {acc:.3f}"</code><code class="p">)</code></pre>&#13;
&#13;
<p>As a baseline, we can use our deep learning library to train a&#13;
(multiclass) logistic regression model, which is just a single&#13;
linear layer followed by a softmax. This model (in essence) just looks&#13;
for 10 linear functions such that if the input represents, say, a 5,&#13;
then the 5th linear function produces the largest output.</p>&#13;
&#13;
<p>One pass through our 60,000 training examples should be&#13;
enough to learn the model:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="n">random</code><code class="o">.</code><code class="n">seed</code><code class="p">(</code><code class="mi">0</code><code class="p">)</code>&#13;
&#13;
<code class="c1"># Logistic regression is just a linear layer followed by softmax</code>&#13;
<code class="n">model</code> <code class="o">=</code> <code class="n">Linear</code><code class="p">(</code><code class="mi">784</code><code class="p">,</code> <code class="mi">10</code><code class="p">)</code>&#13;
<code class="n">loss</code> <code class="o">=</code> <code class="n">SoftmaxCrossEntropy</code><code class="p">()</code>&#13;
&#13;
<code class="c1"># This optimizer seems to work</code>&#13;
<code class="n">optimizer</code> <code class="o">=</code> <code class="n">Momentum</code><code class="p">(</code><code class="n">learning_rate</code><code class="o">=</code><code class="mf">0.01</code><code class="p">,</code> <code class="n">momentum</code><code class="o">=</code><code class="mf">0.99</code><code class="p">)</code>&#13;
&#13;
<code class="c1"># Train on the training data</code>&#13;
<code class="n">loop</code><code class="p">(</code><code class="n">model</code><code class="p">,</code> <code class="n">train_images</code><code class="p">,</code> <code class="n">train_labels</code><code class="p">,</code> <code class="n">loss</code><code class="p">,</code> <code class="n">optimizer</code><code class="p">)</code>&#13;
&#13;
<code class="c1"># Test on the test data (no optimizer means just evaluate)</code>&#13;
<code class="n">loop</code><code class="p">(</code><code class="n">model</code><code class="p">,</code> <code class="n">test_images</code><code class="p">,</code> <code class="n">test_labels</code><code class="p">,</code> <code class="n">loss</code><code class="p">)</code></pre>&#13;
&#13;
<p>This gets about 89% accuracy. Let’s see if we can do better with a deep neural network.&#13;
We’ll use two hidden layers, the first with 30 neurons,&#13;
and the second with 10 neurons. And we’ll use our <code>Tanh</code> activation:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="n">random</code><code class="o">.</code><code class="n">seed</code><code class="p">(</code><code class="mi">0</code><code class="p">)</code>&#13;
&#13;
<code class="c1"># Name them so we can turn train on and off</code>&#13;
<code class="n">dropout1</code> <code class="o">=</code> <code class="n">Dropout</code><code class="p">(</code><code class="mf">0.1</code><code class="p">)</code>&#13;
<code class="n">dropout2</code> <code class="o">=</code> <code class="n">Dropout</code><code class="p">(</code><code class="mf">0.1</code><code class="p">)</code>&#13;
&#13;
<code class="n">model</code> <code class="o">=</code> <code class="n">Sequential</code><code class="p">([</code>&#13;
    <code class="n">Linear</code><code class="p">(</code><code class="mi">784</code><code class="p">,</code> <code class="mi">30</code><code class="p">),</code>  <code class="c1"># Hidden layer 1: size 30</code>&#13;
    <code class="n">dropout1</code><code class="p">,</code>&#13;
    <code class="n">Tanh</code><code class="p">(),</code>&#13;
    <code class="n">Linear</code><code class="p">(</code><code class="mi">30</code><code class="p">,</code> <code class="mi">10</code><code class="p">),</code>   <code class="c1"># Hidden layer 2: size 10</code>&#13;
    <code class="n">dropout2</code><code class="p">,</code>&#13;
    <code class="n">Tanh</code><code class="p">(),</code>&#13;
    <code class="n">Linear</code><code class="p">(</code><code class="mi">10</code><code class="p">,</code> <code class="mi">10</code><code class="p">)</code>    <code class="c1"># Output layer: size 10</code>&#13;
<code class="p">])</code></pre>&#13;
&#13;
<p>And we can just use the same training loop!</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="n">optimizer</code> <code class="o">=</code> <code class="n">Momentum</code><code class="p">(</code><code class="n">learning_rate</code><code class="o">=</code><code class="mf">0.01</code><code class="p">,</code> <code class="n">momentum</code><code class="o">=</code><code class="mf">0.99</code><code class="p">)</code>&#13;
<code class="n">loss</code> <code class="o">=</code> <code class="n">SoftmaxCrossEntropy</code><code class="p">()</code>&#13;
&#13;
<code class="c1"># Enable dropout and train (takes &gt; 20 minutes on my laptop!)</code>&#13;
<code class="n">dropout1</code><code class="o">.</code><code class="n">train</code> <code class="o">=</code> <code class="n">dropout2</code><code class="o">.</code><code class="n">train</code> <code class="o">=</code> <code class="bp">True</code>&#13;
<code class="n">loop</code><code class="p">(</code><code class="n">model</code><code class="p">,</code> <code class="n">train_images</code><code class="p">,</code> <code class="n">train_labels</code><code class="p">,</code> <code class="n">loss</code><code class="p">,</code> <code class="n">optimizer</code><code class="p">)</code>&#13;
&#13;
<code class="c1"># Disable dropout and evaluate</code>&#13;
<code class="n">dropout1</code><code class="o">.</code><code class="n">train</code> <code class="o">=</code> <code class="n">dropout2</code><code class="o">.</code><code class="n">train</code> <code class="o">=</code> <code class="bp">False</code>&#13;
<code class="n">loop</code><code class="p">(</code><code class="n">model</code><code class="p">,</code> <code class="n">test_images</code><code class="p">,</code> <code class="n">test_labels</code><code class="p">,</code> <code class="n">loss</code><code class="p">)</code></pre>&#13;
&#13;
<p>Our deep model gets better than 92% accuracy on the test set, which is a nice improvement from the simple logistic model.</p>&#13;
&#13;
<p>The <a href="http://yann.lecun.com/exdb/mnist/">MNIST website</a> describes a variety of models that outperform these.&#13;
Many of them could be implemented using the machinery we’ve developed&#13;
so far but would take an extremely long time to train in our lists-as-tensors framework.&#13;
Some<a data-primary="convolutional layers" data-type="indexterm" id="idm45635724335944"/><a data-primary="Layers abstraction" data-secondary="convolutional layers" data-type="indexterm" id="idm45635724335304"/> of the best models involve <em>convolutional</em> layers,&#13;
which are important but unfortunately quite out of scope for an introductory book on data science.<a data-primary="" data-startref="DLmnist19" data-type="indexterm" id="idm45635724333720"/><a data-primary="" data-startref="mnist19" data-type="indexterm" id="idm45635724332744"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Saving and Loading Models" data-type="sect1"><div class="sect1" id="idm45635725780056">&#13;
<h1>Saving and Loading Models</h1>&#13;
&#13;
<p>These<a data-primary="deep learning" data-secondary="saving and loading models" data-type="indexterm" id="idm45635724330472"/> models take a long time to train, so it would be nice if we could save&#13;
them so that we don’t have to train them every time. Luckily, we can use the&#13;
<code>json</code> module to easily serialize model weights to a file.</p>&#13;
&#13;
<p>For saving, we can use <code>Layer.params</code> to collect the weights,&#13;
stick them in a list, and use <code>json.dump</code> to save that list to a file:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="kn">import</code> <code class="nn">json</code>&#13;
&#13;
<code class="k">def</code> <code class="nf">save_weights</code><code class="p">(</code><code class="n">model</code><code class="p">:</code> <code class="n">Layer</code><code class="p">,</code> <code class="n">filename</code><code class="p">:</code> <code class="nb">str</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="bp">None</code><code class="p">:</code>&#13;
    <code class="n">weights</code> <code class="o">=</code> <code class="nb">list</code><code class="p">(</code><code class="n">model</code><code class="o">.</code><code class="n">params</code><code class="p">())</code>&#13;
    <code class="k">with</code> <code class="nb">open</code><code class="p">(</code><code class="n">filename</code><code class="p">,</code> <code class="s1">'w'</code><code class="p">)</code> <code class="k">as</code> <code class="n">f</code><code class="p">:</code>&#13;
        <code class="n">json</code><code class="o">.</code><code class="n">dump</code><code class="p">(</code><code class="n">weights</code><code class="p">,</code> <code class="n">f</code><code class="p">)</code></pre>&#13;
&#13;
<p>Loading the weights back is only a little more work.&#13;
We just use <code>json.load</code> to get the list of weights back from the file&#13;
and slice assignment to set the weights of our model.</p>&#13;
&#13;
<p>(In particular, this means that we have to instantiate the model ourselves and <em>then</em> load the weights. An alternative approach would be to also save some representation of the model architecture and use that to instantiate the model. That’s not a terrible idea, but it would require a lot more code and changes to all our <code>Layer</code>s, so we’ll stick with the simpler way.)</p>&#13;
&#13;
<p>Before we load the weights, we’d like to check that they&#13;
have the same shapes as the model params we’re loading them into.&#13;
(This is a safeguard against, for example, trying to load the weights for a saved deep network into a shallow network, or similar issues.)</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="k">def</code> <code class="nf">load_weights</code><code class="p">(</code><code class="n">model</code><code class="p">:</code> <code class="n">Layer</code><code class="p">,</code> <code class="n">filename</code><code class="p">:</code> <code class="nb">str</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="bp">None</code><code class="p">:</code>&#13;
    <code class="k">with</code> <code class="nb">open</code><code class="p">(</code><code class="n">filename</code><code class="p">)</code> <code class="k">as</code> <code class="n">f</code><code class="p">:</code>&#13;
        <code class="n">weights</code> <code class="o">=</code> <code class="n">json</code><code class="o">.</code><code class="n">load</code><code class="p">(</code><code class="n">f</code><code class="p">)</code>&#13;
&#13;
    <code class="c1"># Check for consistency</code>&#13;
    <code class="k">assert</code> <code class="nb">all</code><code class="p">(</code><code class="n">shape</code><code class="p">(</code><code class="n">param</code><code class="p">)</code> <code class="o">==</code> <code class="n">shape</code><code class="p">(</code><code class="n">weight</code><code class="p">)</code>&#13;
               <code class="k">for</code> <code class="n">param</code><code class="p">,</code> <code class="n">weight</code> <code class="ow">in</code> <code class="nb">zip</code><code class="p">(</code><code class="n">model</code><code class="o">.</code><code class="n">params</code><code class="p">(),</code> <code class="n">weights</code><code class="p">))</code>&#13;
&#13;
    <code class="c1"># Then load using slice assignment</code>&#13;
    <code class="k">for</code> <code class="n">param</code><code class="p">,</code> <code class="n">weight</code> <code class="ow">in</code> <code class="nb">zip</code><code class="p">(</code><code class="n">model</code><code class="o">.</code><code class="n">params</code><code class="p">(),</code> <code class="n">weights</code><code class="p">):</code>&#13;
        <code class="n">param</code><code class="p">[:]</code> <code class="o">=</code> <code class="n">weight</code></pre>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>JSON stores your data as text, which makes it&#13;
an extremely inefficient representation. In real applications you’d&#13;
probably use the <code>pickle</code> serialization library, which&#13;
serializes things to a more efficient binary format.&#13;
Here I decided to keep it simple and human-readable.</p>&#13;
</div>&#13;
&#13;
<p>You can download the weights for the various networks we train&#13;
from <a href="https://github.com/joelgrus/data-science-from-scratch">the book’s GitHub repository</a>.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="For Further Exploration" data-type="sect1"><div class="sect1" id="idm45635724331544">&#13;
<h1>For Further Exploration</h1>&#13;
&#13;
<p>Deep learning<a data-primary="deep learning" data-secondary="resources for learning about" data-type="indexterm" id="idm45635724135336"/> is really hot right now, and in this chapter we barely scratched its surface.&#13;
There are many good books and blog posts (and many, many bad blog posts)&#13;
about almost any aspect of deep learning you’d like to know about.</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>The canonical textbook <a href="https://www.deeplearningbook.org/"><em>Deep Learning</em></a>, by Ian Goodfellow, Yoshua Bengio, and Aaron Courville (MIT Press), is freely available online. It is very good, but it involves quite a bit of mathematics.</p>&#13;
</li>&#13;
<li>&#13;
<p>Francois Chollet’s <a href="https://www.manning.com/books/deep-learning-with-python"><em>Deep Learning with Python</em></a> (Manning) is a great introduction to the Keras library, after which our deep learning library is sort of patterned.</p>&#13;
</li>&#13;
<li>&#13;
<p>I myself<a data-primary="deep learning" data-secondary="tools for" data-type="indexterm" id="idm45635724129272"/><a data-primary="PyTorch" data-type="indexterm" id="idm45635724128264"/> mostly use <a href="https://pytorch.org/">PyTorch</a> for deep learning. Its website has lots&#13;
of documentation and tutorials.</p>&#13;
</li>&#13;
</ul>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section></body></html>