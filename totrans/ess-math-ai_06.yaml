- en: 'Chapter 6\. Singular Value Decomposition: Image Processing, Natural Language
    Processing and Social Media'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The singular value decomposition is mathematical operation from Linear Algebra
    that is widely applicable in the fields of Data Science, Machine Learning, and
    Artificial Intelligence. It is the mathematics behind principal component analysis
    (in data analysis) and latent semantic analysis (in natural language processing).
    This operation transforms a dense matrix into a diagonal matrix. In linear algebra,
    diagonal matrices are very special and higly desirable. They behave like scalar
    numbers when we multiply by them, only stretching or squeezing in certain directions.
  prefs: []
  type: TYPE_NORMAL
- en: 'When computing the singular value decomposition of a matrix, we get the extra
    bonus of revealing and quantifying the action of the matrix on space itself: rotating,
    reflecting, stretching, and/or squeezing. There is no warping (bending) of space,
    since this operation is linear (after all, it is called linear algebra). Extreme
    stretching or squeezing in one direction versus the others affects the stability
    of any computations involving our matrix, so having a measure of that allows us
    direct control over the sensitivity of our computations to various perturbations,
    for example, noisy measurements.'
  prefs: []
  type: TYPE_NORMAL
- en: The power of the singular value decomposition lies in the fact that it can be
    applied to *any* matrix. That and its wide use in the field of AI earns it its
    own chapter in this book. In the following sections, we explore the singular value
    decomposition, focusing on the big picture rather than the tiny details, and on
    applications to image processing, natural language processing, and social media.
  prefs: []
  type: TYPE_NORMAL
- en: Given a matrix *C* (an image, a data matrix, *etc.*), we omit the details of
    computing its the singular value decomposition. Most linear algebra books do that,
    presenting a theoretical method based on computing the eigenvectors and eigenvalues
    of the symmetric matrices <math alttext="upper C Superscript t Baseline upper
    C"><mrow><msup><mi>C</mi> <mi>t</mi></msup> <mi>C</mi></mrow></math> and <math
    alttext="upper C upper C Superscript t"><mrow><mi>C</mi> <msup><mi>C</mi> <mi>t</mi></msup></mrow></math>
    , which for us are the covariance matrices of the data (if the data is centered).
    While it is still very important to undertsand the theory, the method it provides
    for computing the singular value decomposition is not useful for efficient computations
    and especially impossible for the large matrices involved in many realistic problems.
    Moreover, we live in an era where software packages help us compute it so easily.
    In Python, all we have to do is call the *numpy.linalg.svd* method from the *numpy*
    library. We peak briefly into the numerical algorithms that go into these software
    packages later in this chapter. However, our main focus is on understanding how
    the singular value decomposition works and why this decomposition is important
    for reducing the storage and computational requirements of a given problem without
    losing its essential information. We will also undertsand the role it plays in
    clustering data.
  prefs: []
  type: TYPE_NORMAL
- en: Matrix Factorization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can factorize a scalar number in multiple ways, for instance, we can write
    the number <math alttext="12 equals 4 times 3"><mrow><mn>12</mn> <mo>=</mo> <mn>4</mn>
    <mo>×</mo> <mn>3</mn></mrow></math> , <math alttext="12 equals 2 times 2 times
    3"><mrow><mn>12</mn> <mo>=</mo> <mn>2</mn> <mo>×</mo> <mn>2</mn> <mo>×</mo> <mn>3</mn></mrow></math>
    or <math alttext="12 equals 0.5 times 24"><mrow><mn>12</mn> <mo>=</mo> <mn>0</mn>
    <mo>.</mo> <mn>5</mn> <mo>×</mo> <mn>24</mn></mrow></math> . Which factorization
    is better depends on our use case. The same can be done for matrices of numbers.
    Linear algebra provides us with a variety of useful matrix factorizations. The
    idea is that we want to break down an object into its smaller components, and
    these components give us insight about the function and the action of the object
    itself. This breakdown also gives us a good idea about which components contain
    the most information, thus are more important than others. In this case, we might
    benefit from throwing away the less important components, and building a smaller
    object with a similar function. The smaller object might not be as detailed as
    the object that we started with, that contained all of its components, however,
    it contains enough significant information from the original object that using
    it with its smaller size provides benefits. The singular value decomposition is
    a matrix factorization that does exactly that. Its formula looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign upper C Subscript m times n Baseline equals upper
    U Subscript m times m Baseline normal upper Sigma Subscript m times n Baseline
    upper V Subscript n times n Superscript t Baseline comma dollar-sign"><mrow><msub><mi>C</mi>
    <mrow><mi>m</mi><mo>×</mo><mi>n</mi></mrow></msub> <mo>=</mo> <msub><mi>U</mi>
    <mrow><mi>m</mi><mo>×</mo><mi>m</mi></mrow></msub> <msub><mi>Σ</mi> <mrow><mi>m</mi><mo>×</mo><mi>n</mi></mrow></msub>
    <msubsup><mi>V</mi> <mrow><mi>n</mi><mo>×</mo><mi>n</mi></mrow> <mi>t</mi></msubsup>
    <mo>,</mo></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'where we break down the matrix *C* into three component matrices: *U*, <math
    alttext="normal upper Sigma"><mi>Σ</mi></math> , and <math alttext="upper V Superscript
    t"><msup><mi>V</mi> <mi>t</mi></msup></math> . *U* and *V* are square matrices
    that have orthonormal rows and columns. <math alttext="normal upper Sigma"><mi>Σ</mi></math>
    is a diagnal matrix that has the same shape as *C* (see [Figure 6-1](#Fig_visualize_svd)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start with matrix multiplication. Suppose *A* is a matrix with *3* rows
    and *3* columns:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign upper A equals Start 3 By 3 Matrix 1st Row 1st Column
    1 2nd Column 2 3rd Column 3 2nd Row 1st Column 4 2nd Column 5 3rd Column 6 3rd
    Row 1st Column 7 2nd Column 8 3rd Column 9 EndMatrix Subscript 3 times 3 Baseline
    comma dollar-sign"><mrow><mi>A</mi> <mo>=</mo> <msub><mfenced close=")" open="("><mtable><mtr><mtd><mn>1</mn></mtd><mtd><mn>2</mn></mtd><mtd><mn>3</mn></mtd></mtr><mtr><mtd><mn>4</mn></mtd><mtd><mn>5</mn></mtd><mtd><mn>6</mn></mtd></mtr><mtr><mtd><mn>7</mn></mtd><mtd><mn>8</mn></mtd><mtd><mn>9</mn></mtd></mtr></mtable></mfenced>
    <mrow><mn>3</mn><mo>×</mo><mn>3</mn></mrow></msub> <mo>,</mo></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'and *B* is a matrix with *3* rows and *2* columns:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign upper B equals Start 3 By 2 Matrix 1st Row 1st Column
    1 2nd Column 3 2nd Row 1st Column 4 2nd Column negative 2 3rd Row 1st Column 0
    2nd Column 1 EndMatrix Subscript 3 times 2 Baseline period dollar-sign"><mrow><mi>B</mi>
    <mo>=</mo> <msub><mfenced close=")" open="("><mtable><mtr><mtd><mn>1</mn></mtd><mtd><mn>3</mn></mtd></mtr><mtr><mtd><mn>4</mn></mtd><mtd><mrow><mo>-</mo><mn>2</mn></mrow></mtd></mtr><mtr><mtd><mn>0</mn></mtd><mtd><mn>1</mn></mtd></mtr></mtable></mfenced>
    <mrow><mn>3</mn><mo>×</mo><mn>2</mn></mrow></msub> <mo>.</mo></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Then *C=AB* is a matrix with *3* rows and *2* columns:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign upper C Subscript 3 times 2 Baseline equals upper
    A Subscript 3 times 3 Baseline upper B Subscript 3 times 2 Baseline equals Start
    3 By 3 Matrix 1st Row 1st Column 1 2nd Column 2 3rd Column 3 2nd Row 1st Column
    4 2nd Column 5 3rd Column 6 3rd Row 1st Column 7 2nd Column 8 3rd Column 9 EndMatrix
    Subscript 3 times 3"><mrow><msub><mi>C</mi> <mrow><mn>3</mn><mo>×</mo><mn>2</mn></mrow></msub>
    <mo>=</mo> <msub><mi>A</mi> <mrow><mn>3</mn><mo>×</mo><mn>3</mn></mrow></msub>
    <msub><mi>B</mi> <mrow><mn>3</mn><mo>×</mo><mn>2</mn></mrow></msub> <mo>=</mo>
    <msub><mfenced close=")" open="("><mtable><mtr><mtd><mn>1</mn></mtd><mtd><mn>2</mn></mtd><mtd><mn>3</mn></mtd></mtr><mtr><mtd><mn>4</mn></mtd><mtd><mn>5</mn></mtd><mtd><mn>6</mn></mtd></mtr><mtr><mtd><mn>7</mn></mtd><mtd><mn>8</mn></mtd><mtd><mn>9</mn></mtd></mtr></mtable></mfenced>
    <mrow><mn>3</mn><mo>×</mo><mn>3</mn></mrow></msub></mrow></math> <math><msub><mfenced
    close=")" open="("><mtable><mtr><mtd><mn>1</mn></mtd><mtd><mn>3</mn></mtd></mtr><mtr><mtd><mn>4</mn></mtd><mtd><mrow><mo>-</mo><mn>2</mn></mrow></mtd></mtr><mtr><mtd><mn>0</mn></mtd><mtd><mn>1</mn></mtd></mtr></mtable></mfenced>
    <mrow><mn>3</mn><mo>×</mo><mn>2</mn></mrow></msub></math> <math><mrow><mo>=</mo>
    <msub><mfenced close=")" open="("><mtable><mtr><mtd><mn>9</mn></mtd><mtd><mn>2</mn></mtd></mtr><mtr><mtd><mn>24</mn></mtd><mtd><mn>8</mn></mtd></mtr><mtr><mtd><mn>39</mn></mtd><mtd><mn>14</mn></mtd></mtr></mtable></mfenced>
    <mrow><mn>3</mn><mo>×</mo><mn>2</mn></mrow></msub> <mo>.</mo></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'We can think of *C* as being factorized into the product of *A* and *B*, in
    the same way the number <math alttext="12 equals 4 times 3"><mrow><mn>12</mn>
    <mo>=</mo> <mn>4</mn> <mo>×</mo> <mn>3</mn></mrow></math> . The above factorization
    of *C* has no significance, since neither *A* nor *B* is a special type of matrix.
    A very significant factorization of *C* is its singular value decomposition. Any
    matrix has a singular value decomposition. We calculate it using Python (see the
    associated Jupyter notebook for the code):'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign StartLayout 1st Row 1st Column Blank 2nd Column upper
    C Subscript 3 times 2 Baseline equals upper U Subscript 3 times 3 Baseline normal
    upper Sigma Subscript 3 times 2 Baseline upper V Subscript 2 times 2 Superscript
    t Baseline equals 2nd Row 1st Column Blank 2nd Column Start 3 By 3 Matrix 1st
    Row 1st Column negative 0.1853757 2nd Column 0.8938507 3rd Column 0.4082482 2nd
    Row 1st Column negative 0.5120459 2nd Column 0.2667251 3rd Column negative 0.8164965
    3rd Row 1st Column negative 0.8387161 2nd Column negative 0.3604005 3rd Column
    0.4082482 EndMatrix Start 3 By 2 Matrix 1st Row 1st Column 49.402266 2nd Column
    0 2nd Row 1st Column 0 2nd Column 1.189980 3rd Row 1st Column 0 2nd Column 0 EndMatrix
    Start 2 By 2 Matrix 1st Row 1st Column negative 0.9446411 2nd Column negative
    0.3281052 2nd Row 1st Column 0.3281052 2nd Column negative 0.9446411 EndMatrix
    period EndLayout dollar-sign"><mtable displaystyle="true"><mtr><mtd columnalign="left"><mrow><msub><mi>C</mi>
    <mrow><mn>3</mn><mo>×</mo><mn>2</mn></mrow></msub> <mo>=</mo> <msub><mi>U</mi>
    <mrow><mn>3</mn><mo>×</mo><mn>3</mn></mrow></msub> <msub><mi>Σ</mi> <mrow><mn>3</mn><mo>×</mo><mn>2</mn></mrow></msub>
    <msubsup><mi>V</mi> <mrow><mn>2</mn><mo>×</mo><mn>2</mn></mrow> <mi>t</mi></msubsup>
    <mo>=</mo></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mrow><mfenced close=")"
    open="("><mtable><mtr><mtd><mrow><mo>-</mo> <mn>0</mn> <mo>.</mo> <mn>1853757</mn></mrow></mtd>
    <mtd><mrow><mn>0</mn> <mo>.</mo> <mn>8938507</mn></mrow></mtd> <mtd><mrow><mn>0</mn>
    <mo>.</mo> <mn>4082482</mn></mrow></mtd></mtr> <mtr><mtd><mrow><mo>-</mo> <mn>0</mn>
    <mo>.</mo> <mn>5120459</mn></mrow></mtd> <mtd><mrow><mn>0</mn> <mo>.</mo> <mn>2667251</mn></mrow></mtd>
    <mtd><mrow><mo>-</mo> <mn>0</mn> <mo>.</mo> <mn>8164965</mn></mrow></mtd></mtr>
    <mtr><mtd><mrow><mo>-</mo> <mn>0</mn> <mo>.</mo> <mn>8387161</mn></mrow></mtd>
    <mtd><mrow><mo>-</mo> <mn>0</mn> <mo>.</mo> <mn>3604005</mn></mrow></mtd> <mtd><mrow><mn>0</mn>
    <mo>.</mo> <mn>4082482</mn></mrow></mtd></mtr></mtable></mfenced> <mfenced close=")"
    open="("><mtable><mtr><mtd><mrow><mn>49</mn> <mo>.</mo> <mn>402266</mn></mrow></mtd>
    <mtd><mn>0</mn></mtd></mtr> <mtr><mtd><mn>0</mn></mtd> <mtd><mrow><mn>1</mn> <mo>.</mo>
    <mn>189980</mn></mrow></mtd></mtr> <mtr><mtd><mn>0</mn></mtd> <mtd><mn>0</mn></mtd></mtr></mtable></mfenced>
    <mfenced close=")" open="("><mtable><mtr><mtd><mrow><mo>-</mo> <mn>0</mn> <mo>.</mo>
    <mn>9446411</mn></mrow></mtd> <mtd><mrow><mo>-</mo> <mn>0</mn> <mo>.</mo> <mn>3281052</mn></mrow></mtd></mtr>
    <mtr><mtd><mrow><mn>0</mn> <mo>.</mo> <mn>3281052</mn></mrow></mtd> <mtd><mrow><mo>-</mo>
    <mn>0</mn> <mo>.</mo> <mn>9446411</mn></mrow></mtd></mtr></mtable></mfenced> <mo>.</mo></mrow></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Observe the following in the above decomposition: The rows of <math alttext="upper
    V Superscript t"><msup><mi>V</mi> <mi>t</mi></msup></math> are the *right singular
    vectors* (these are exactly the columns of *V*), the columns of *U* are the *left
    singular vectors*, and the diagonal entries of <math alttext="normal upper Sigma"><mi>Σ</mi></math>
    are the *singular values*. The singular values are *always* positive and always
    arranged in decreasing order along the diagonal of <math alttext="normal upper
    Sigma"><mi>Σ</mi></math> . The ratio of largest singular value to the smallest
    singular value is the *condition number <math alttext="kappa"><mi>κ</mi></math>*
    of a matrix. In our case there are only two singular values and <math alttext="kappa
    equals StartFraction 49.402266 Over 1.189980 EndFraction equals 41.515207"><mrow><mi>κ</mi>
    <mo>=</mo> <mfrac><mrow><mn>49</mn><mo>.</mo><mn>402266</mn></mrow> <mrow><mn>1</mn><mo>.</mo><mn>189980</mn></mrow></mfrac>
    <mo>=</mo> <mn>41</mn> <mo>.</mo> <mn>515207</mn></mrow></math> . This number
    plays an important role in the stability of computations involving our matrix.
    Well conditioned matrices are those with condition numbers that are not very large.'
  prefs: []
  type: TYPE_NORMAL
- en: The left singular vectors are orthonormal (orthogonal to each other and have
    length 1). Similarly, the right singular vectors are also orthonormal.
  prefs: []
  type: TYPE_NORMAL
- en: 'For qualitative properties, images are faster to assess than endless arrays
    of numbers. It is easy to visualize matrices as images using Python (and vice
    versa, images are stored as matrices of numbers): The value of an entry of the
    matrix corresponds to the intensity of the corresponding pixel. The higher the
    number the brighter the pixel. Smaller numbers in the matrix show up as darker
    pixels and larger numbers show up as brighter pixels. [Figure 6-1](#Fig_visualize_svd)
    shows the above singular value decomposition. We observe that the diagonal matrix
    <math alttext="normal upper Sigma"><mi>Σ</mi></math> has the same shape as *C*,
    and its diagonal entries are arranged in decreasing order, with the brightest
    pixel, corresponding to the largest *singular value*, at the top left corner.'
  prefs: []
  type: TYPE_NORMAL
- en: '![400](assets/emai_0601.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-1\. Visualizing the singular value decomposition. The diagonal matrix
    <math alttext="normal upper Sigma"><mi>Σ</mi></math> has the same shape as *C*,
    and its diagonal entries are arranged in decreasing order, with the brightest
    pixel, corresponding to the largest singular value, at the top left corner.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '[Figure 6-2](#Fig_visualize_svd1) and [Figure 6-3](#Fig_visualize_svd2) visualize
    the singular value decompositions of two rectangular matrices *A* and *B*, where
    *A* is wide and *B* is tall:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign upper A equals Start 3 By 5 Matrix 1st Row 1st Column
    negative 1 2nd Column 3 3rd Column negative 5 4th Column 4 5th Column 18 2nd Row
    1st Column 1 2nd Column negative 2 3rd Column 4 4th Column 0 5th Column negative
    7 3rd Row 1st Column 2 2nd Column 0 3rd Column 4 4th Column negative 3 5th Column
    negative 8 EndMatrix Subscript 3 times 5 Baseline equals upper U Subscript 3 times
    3 Baseline normal upper Sigma Subscript 3 times 5 Baseline upper V Subscript 5
    times 5 Superscript t dollar-sign"><mrow><mi>A</mi> <mo>=</mo> <msub><mfenced
    close=")" open="("><mtable><mtr><mtd><mrow><mo>-</mo><mn>1</mn></mrow></mtd><mtd><mn>3</mn></mtd><mtd><mrow><mo>-</mo><mn>5</mn></mrow></mtd><mtd><mn>4</mn></mtd><mtd><mn>18</mn></mtd></mtr><mtr><mtd><mn>1</mn></mtd><mtd><mrow><mo>-</mo><mn>2</mn></mrow></mtd><mtd><mn>4</mn></mtd><mtd><mn>0</mn></mtd><mtd><mrow><mo>-</mo><mn>7</mn></mrow></mtd></mtr><mtr><mtd><mn>2</mn></mtd><mtd><mn>0</mn></mtd><mtd><mn>4</mn></mtd><mtd><mrow><mo>-</mo><mn>3</mn></mrow></mtd><mtd><mrow><mo>-</mo><mn>8</mn></mrow></mtd></mtr></mtable></mfenced>
    <mrow><mn>3</mn><mo>×</mo><mn>5</mn></mrow></msub> <mo>=</mo> <msub><mi>U</mi>
    <mrow><mn>3</mn><mo>×</mo><mn>3</mn></mrow></msub> <msub><mi>Σ</mi> <mrow><mn>3</mn><mo>×</mo><mn>5</mn></mrow></msub>
    <msubsup><mi>V</mi> <mrow><mn>5</mn><mo>×</mo><mn>5</mn></mrow> <mi>t</mi></msubsup></mrow></math><math
    alttext="dollar-sign upper B equals Start 4 By 2 Matrix 1st Row 1st Column 5 2nd
    Column 4 2nd Row 1st Column 4 2nd Column 0 3rd Row 1st Column 7 2nd Column 10
    4th Row 1st Column negative 1 2nd Column 8 EndMatrix Subscript 4 times 2 Baseline
    equals upper U Subscript 4 times 4 Baseline normal upper Sigma Subscript 4 times
    2 Baseline upper V Subscript 2 times 2 Superscript t dollar-sign"><mrow><mi>B</mi>
    <mo>=</mo> <msub><mfenced close=")" open="("><mtable><mtr><mtd><mn>5</mn></mtd><mtd><mn>4</mn></mtd></mtr><mtr><mtd><mn>4</mn></mtd><mtd><mn>0</mn></mtd></mtr><mtr><mtd><mn>7</mn></mtd><mtd><mn>10</mn></mtd></mtr><mtr><mtd><mrow><mo>-</mo><mn>1</mn></mrow></mtd><mtd><mn>8</mn></mtd></mtr></mtable></mfenced>
    <mrow><mn>4</mn><mo>×</mo><mn>2</mn></mrow></msub> <mo>=</mo> <msub><mi>U</mi>
    <mrow><mn>4</mn><mo>×</mo><mn>4</mn></mrow></msub> <msub><mi>Σ</mi> <mrow><mn>4</mn><mo>×</mo><mn>2</mn></mrow></msub>
    <msubsup><mi>V</mi> <mrow><mn>2</mn><mo>×</mo><mn>2</mn></mrow> <mi>t</mi></msubsup></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: In [Figure 6-2](#Fig_visualize_svd1), we note that the last two columns of late
    <math alttext="normal upper Sigma"><mi>Σ</mi></math> are all zeros (black pixels),
    hence we can economize in storage and throw away these two columns along with
    the last two rows of <math alttext="upper V Superscript t"><msup><mi>V</mi> <mi>t</mi></msup></math>
    (see the next section for multiplying by a diagonal matrix from the left). Similarly
    in [Figure 6-3](#Fig_visualize_svd2), we note that the last two rows of late <math
    alttext="normal upper Sigma"><mi>Σ</mi></math> are all zeros (black pixels), hence
    we can economize in storage and throw away these two rows along with the last
    two columns of <math alttext="upper U"><mi>U</mi></math> (see the next section
    for multiplying by a diagonal matrix from the right). The singular value decomposition
    is already saving us some space (note that we usually only store the diagonal
    entries of <math alttext="normal upper Sigma"><mi>Σ</mi></math> as opposed to
    the whole matrix with all its zeros).
  prefs: []
  type: TYPE_NORMAL
- en: '![400](assets/emai_0602.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6-2\. Visualizing the singular value decomposition of a wide rectangular
    matrix. The last two columns of <math alttext="normal upper Sigma"><mi>Σ</mi></math>
    are all zeros (black pixels), allowing for storage reduction: Throw away the last
    two columns of <math alttext="normal upper Sigma"><mi>Σ</mi></math> along with
    the last two rows of <math alttext="upper V Superscript t"><msup><mi>V</mi> <mi>t</mi></msup></math>
    .'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![400](assets/emai_0603.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6-3\. Visualizing the singular value decomposition of a tall rectangular
    matrix. The last two rows of <math alttext="normal upper Sigma"><mi>Σ</mi></math>
    are all zeros (black pixels), allowing for storage reduction: Throw away the last
    two rows of <math alttext="normal upper Sigma"><mi>Σ</mi></math> along with the
    last two columns of <math alttext="upper U"><mi>U</mi></math> .'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Diagonal Matrices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When we multiply a vector by a scalar number, say 3, we obtain a new vector
    along the same direction, with the same orientation, but whose length is stretched
    three times. When we multiply the same vector by another scalar number, say -0.5,
    we get another vector, again along the same direction, but this time its length
    is halved and its orientation is flipped. Multiplying by a scalar number is such
    a simple operation, and it would be nice if we had matrices that behaved equally
    easily when we applied them to (in other words multiplied them by) vectors. If
    our life was one dimensional then we would only have to deal with scalar numbers,
    but since our life and applications of interest are higher dimensional, then we
    have to satisfy oursleves with diagonal matrices ([Figure 6-4](#Fig_diagonal_matrix)).
    These are the good ones.
  prefs: []
  type: TYPE_NORMAL
- en: '![200](assets/emai_0604.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6-4\. An image of a <math alttext="5 times 4"><mrow><mn>5</mn> <mo>×</mo>
    <mn>4</mn></mrow></math> diagonal matrix with diagonal entries: 10 (brightest
    pixel), 6, 3 and 1 (darkest pixel other than the zeros).'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Multiplying by a diagonal matrix corresponds to stretching or squeezing in certain
    directions in space, with orientation flipping corresponding to any negative numbers
    on the diagonal. As we very well know, most matrices are very far from being diagonal.
    The power of the singular value decomposition is that it provides us with the
    directions in space along which the matrix *behaves like* (albeit in a broad sense)
    a diagonal matrix. A diagonal matrix usually stretches/squeezes in the same directions
    as those for the vector coordinates. If on the other hand the matrix is not diagnal,
    it generally does not stretch/squeeze in the same directions as the coordinates.
    It does so in *other* directions, *after* a change of coordinates. The singular
    value decomposition gives us the required coordinate change (*right singular vectors*),
    the directions along which vectors will be stretched/squeezed (*left singular
    vectors*), as well as the magnitude of the stretch/squeeze (*singular values*).
    We detail this in the next section, but we first clarify multiplication by diagonal
    matrices from the left and from the right.
  prefs: []
  type: TYPE_NORMAL
- en: 'Multiplying by a diagonal matrix <math alttext="normal upper Sigma"><mi>Σ</mi></math>
    :'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If we multiply a matrix <math alttext="upper A"><mi>A</mi></math> by a diagonal
    matrix <math alttext="normal upper Sigma"><mi>Σ</mi></math> from the right, <math
    alttext="upper A normal upper Sigma"><mrow><mi>A</mi> <mi>Σ</mi></mrow></math>
    , then we scale the columns of <math alttext="upper A"><mi>A</mi></math> by the
    <math alttext="sigma"><mi>σ</mi></math> ’s, for example:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign upper A normal upper Sigma equals Start 3 By 2 Matrix
    1st Row 1st Column a 11 2nd Column a 12 2nd Row 1st Column a 21 2nd Column a 22
    3rd Row 1st Column a 31 2nd Column a 32 EndMatrix Start 2 By 2 Matrix 1st Row
    1st Column sigma 1 2nd Column 0 2nd Row 1st Column 0 2nd Column sigma 2 EndMatrix
    equals Start 3 By 2 Matrix 1st Row 1st Column sigma 1 a 11 2nd Column sigma 2
    a 12 2nd Row 1st Column sigma 1 a 21 2nd Column sigma 2 a 22 3rd Row 1st Column
    sigma 1 a 31 2nd Column sigma 2 a 32 EndMatrix dollar-sign"><mrow><mi>A</mi> <mi>Σ</mi>
    <mo>=</mo> <mfenced close=")" open="("><mtable><mtr><mtd><msub><mi>a</mi> <mn>11</mn></msub></mtd>
    <mtd><msub><mi>a</mi> <mn>12</mn></msub></mtd></mtr> <mtr><mtd><msub><mi>a</mi>
    <mn>21</mn></msub></mtd> <mtd><msub><mi>a</mi> <mn>22</mn></msub></mtd></mtr>
    <mtr><mtd><msub><mi>a</mi> <mn>31</mn></msub></mtd> <mtd><msub><mi>a</mi> <mn>32</mn></msub></mtd></mtr></mtable></mfenced>
    <mfenced close=")" open="("><mtable><mtr><mtd><msub><mi>σ</mi> <mn>1</mn></msub></mtd>
    <mtd><mn>0</mn></mtd></mtr> <mtr><mtd><mn>0</mn></mtd> <mtd><msub><mi>σ</mi> <mn>2</mn></msub></mtd></mtr></mtable></mfenced>
    <mo>=</mo> <mfenced close=")" open="("><mtable><mtr><mtd><mrow><msub><mi>σ</mi>
    <mn>1</mn></msub> <msub><mi>a</mi> <mn>11</mn></msub></mrow></mtd> <mtd><mrow><msub><mi>σ</mi>
    <mn>2</mn></msub> <msub><mi>a</mi> <mn>12</mn></msub></mrow></mtd></mtr> <mtr><mtd><mrow><msub><mi>σ</mi>
    <mn>1</mn></msub> <msub><mi>a</mi> <mn>21</mn></msub></mrow></mtd> <mtd><mrow><msub><mi>σ</mi>
    <mn>2</mn></msub> <msub><mi>a</mi> <mn>22</mn></msub></mrow></mtd></mtr> <mtr><mtd><mrow><msub><mi>σ</mi>
    <mn>1</mn></msub> <msub><mi>a</mi> <mn>31</mn></msub></mrow></mtd> <mtd><mrow><msub><mi>σ</mi>
    <mn>2</mn></msub> <msub><mi>a</mi> <mn>32</mn></msub></mrow></mtd></mtr></mtable></mfenced></mrow></math>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'If we multiply <math alttext="upper A"><mi>A</mi></math> by <math alttext="normal
    upper Sigma"><mi>Σ</mi></math> from the left <math alttext="normal upper Sigma
    upper A"><mrow><mi>Σ</mi> <mi>A</mi></mrow></math> then we scale the rows of <math
    alttext="upper A"><mi>A</mi></math> by the <math alttext="sigma"><mi>σ</mi></math>
    ’s, for example:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign normal upper Sigma upper A equals Start 3 By 3 Matrix
    1st Row 1st Column sigma 1 2nd Column 0 3rd Column 0 2nd Row 1st Column 0 2nd
    Column sigma 2 3rd Column 0 3rd Row 1st Column 0 2nd Column 0 3rd Column sigma
    3 EndMatrix Start 3 By 2 Matrix 1st Row 1st Column a 11 2nd Column a 12 2nd Row
    1st Column a 21 2nd Column a 22 3rd Row 1st Column a 31 2nd Column a 32 EndMatrix
    equals Start 3 By 2 Matrix 1st Row 1st Column sigma 1 a 11 2nd Column sigma 1
    a 12 2nd Row 1st Column sigma 2 a 21 2nd Column sigma 2 a 22 3rd Row 1st Column
    sigma 3 a 31 2nd Column sigma 3 a 32 EndMatrix dollar-sign"><mrow><mi>Σ</mi> <mi>A</mi>
    <mo>=</mo> <mfenced close=")" open="("><mtable><mtr><mtd><msub><mi>σ</mi> <mn>1</mn></msub></mtd>
    <mtd><mn>0</mn></mtd> <mtd><mn>0</mn></mtd></mtr> <mtr><mtd><mn>0</mn></mtd> <mtd><msub><mi>σ</mi>
    <mn>2</mn></msub></mtd> <mtd><mn>0</mn></mtd></mtr> <mtr><mtd><mn>0</mn></mtd>
    <mtd><mn>0</mn></mtd> <mtd><msub><mi>σ</mi> <mn>3</mn></msub></mtd></mtr></mtable></mfenced>
    <mfenced close=")" open="("><mtable><mtr><mtd><msub><mi>a</mi> <mn>11</mn></msub></mtd>
    <mtd><msub><mi>a</mi> <mn>12</mn></msub></mtd></mtr> <mtr><mtd><msub><mi>a</mi>
    <mn>21</mn></msub></mtd> <mtd><msub><mi>a</mi> <mn>22</mn></msub></mtd></mtr>
    <mtr><mtd><msub><mi>a</mi> <mn>31</mn></msub></mtd> <mtd><msub><mi>a</mi> <mn>32</mn></msub></mtd></mtr></mtable></mfenced>
    <mo>=</mo> <mfenced close=")" open="("><mtable><mtr><mtd><mrow><msub><mi>σ</mi>
    <mn>1</mn></msub> <msub><mi>a</mi> <mn>11</mn></msub></mrow></mtd> <mtd><mrow><msub><mi>σ</mi>
    <mn>1</mn></msub> <msub><mi>a</mi> <mn>12</mn></msub></mrow></mtd></mtr> <mtr><mtd><mrow><msub><mi>σ</mi>
    <mn>2</mn></msub> <msub><mi>a</mi> <mn>21</mn></msub></mrow></mtd> <mtd><mrow><msub><mi>σ</mi>
    <mn>2</mn></msub> <msub><mi>a</mi> <mn>22</mn></msub></mrow></mtd></mtr> <mtr><mtd><mrow><msub><mi>σ</mi>
    <mn>3</mn></msub> <msub><mi>a</mi> <mn>31</mn></msub></mrow></mtd> <mtd><mrow><msub><mi>σ</mi>
    <mn>3</mn></msub> <msub><mi>a</mi> <mn>32</mn></msub></mrow></mtd></mtr></mtable></mfenced></mrow></math>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Matrices As Linear Transformations Acting on Space
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One way we can view matrices is as linear transformations (no warping) that
    act on vectors in space, and on space itself. If no warping is allowed, because
    that would render an operation nonlinear, then what actions would be allowed?
    The answers are rotation, reflection, stretching, and/or squeezing, which are
    all non-warping operations. The singular value decomposition <math alttext="upper
    A equals upper U normal upper Sigma upper V Superscript t"><mrow><mi>A</mi> <mo>=</mo>
    <mi>U</mi> <mi>Σ</mi> <msup><mi>V</mi> <mi>t</mi></msup></mrow></math> captures
    this concept. When *A* acts on a vector <math alttext="ModifyingAbove v With right-arrow"><mover
    accent="true"><mi>v</mi> <mo>→</mo></mover></math> , let’s go over the multiplication
    <math alttext="upper A ModifyingAbove v With right-arrow equals upper U normal
    upper Sigma upper V Superscript t Baseline ModifyingAbove v With right-arrow"><mrow><mi>A</mi>
    <mover accent="true"><mi>v</mi> <mo>→</mo></mover> <mo>=</mo> <mi>U</mi> <mi>Σ</mi>
    <msup><mi>V</mi> <mi>t</mi></msup> <mover accent="true"><mi>v</mi> <mo>→</mo></mover></mrow></math>
    step by step:'
  prefs: []
  type: TYPE_NORMAL
- en: First <math alttext="ModifyingAbove v With right-arrow"><mover accent="true"><mi>v</mi>
    <mo>→</mo></mover></math> gets rotated/reflected because of the orthogonal matrix
    <math alttext="upper V Superscript t"><msup><mi>V</mi> <mi>t</mi></msup></math>
    .
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then it gets stretched/squeezed along special directions because of the diagonal
    matrix <math alttext="normal upper Sigma"><mi>Σ</mi></math> .
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, it gets rotated/reflected again because of the other orthogonal matrix
    *U*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Reflections and rotations do not really change space as they preserve size and
    symmetries (think of rotating an object or looking at its reflection in a mirror).
    The amount of stretch and/or squeeze encoded in the diagonal matrix <math alttext="normal
    upper Sigma"><mi>Σ</mi></math> (via its singular values on the diagonal) is very
    informative regarding action of *A*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note: Orthogonal Matrix'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'An orthogonal matrix has orthonormal rows and orthonormal columns. It never
    stretches or squeezes, only rotates and/or reflects, meaning that it does not
    change the size and shape of objects when acting on them, only their direction
    and/or orientation. As many things in mathematics, these names are confusing.
    It is called *orthogonal* matrix even though its rows and columns are *orthonormal*,
    which means orthogonal *and* of length equal to one. One more useful fact: If
    *C* is an orthogonal matrix, then <math alttext="upper C upper C Superscript t
    Baseline equals upper C Superscript t Baseline upper C equals upper I"><mrow><mi>C</mi>
    <msup><mi>C</mi> <mi>t</mi></msup> <mo>=</mo> <msup><mi>C</mi> <mi>t</mi></msup>
    <mi>C</mi> <mo>=</mo> <mi>I</mi></mrow></math> , that is, the inverse of this
    matrix is its transpose. Computing the inverse of a matrix is usually a very costly
    operation, but for orthogonal matrices, all we have to do is to exchange its rows
    for its columns.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We illustrate the concepts above using two dimensional matrices since these
    are easy to visualize. In the following subsections, we explore:'
  prefs: []
  type: TYPE_NORMAL
- en: The action of a matrix <math alttext="upper A"><mi>A</mi></math> on the right
    singular vectors, which are the columns <math alttext="ModifyingAbove v With right-arrow
    Subscript 1"><msub><mover accent="true"><mi>v</mi> <mo>→</mo></mover> <mn>1</mn></msub></math>
    and <math alttext="ModifyingAbove v With right-arrow Subscript 2"><msub><mover
    accent="true"><mi>v</mi> <mo>→</mo></mover> <mn>2</mn></msub></math> of the matrix
    *V*. These get sent to multiples of the left singular vectors <math alttext="ModifyingAbove
    u With right-arrow Subscript 1"><msub><mover accent="true"><mi>u</mi> <mo>→</mo></mover>
    <mn>1</mn></msub></math> and <math alttext="ModifyingAbove u With right-arrow
    Subscript 2"><msub><mover accent="true"><mi>u</mi> <mo>→</mo></mover> <mn>2</mn></msub></math>
    , which are the columns of *U*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The action of <math alttext="upper A"><mi>A</mi></math> on the standard unit
    vectors <math alttext="ModifyingAbove e With right-arrow Subscript 1"><msub><mover
    accent="true"><mi>e</mi> <mo>→</mo></mover> <mn>1</mn></msub></math> and <math
    alttext="ModifyingAbove e With right-arrow Subscript 2"><msub><mover accent="true"><mi>e</mi>
    <mo>→</mo></mover> <mn>2</mn></msub></math> . We also notice that the unit square
    gets transformed to a parallegram.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The action of <math alttext="upper A"><mi>A</mi></math> on a general vector
    <math alttext="ModifyingAbove x With right-arrow"><mover accent="true"><mi>x</mi>
    <mo>→</mo></mover></math> . This will help us understand the matrices <math alttext="upper
    U"><mi>U</mi></math> and <math alttext="upper V"><mi>V</mi></math> as rotations
    or reflections in space.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The action of <math alttext="upper A"><mi>A</mi></math> on the unit circle.
    We see that <math alttext="upper A"><mi>A</mi></math> transforms the unit circle
    to an ellipse, with its principal axes along the left singular vectors (the <math
    alttext="ModifyingAbove u With right-arrow"><mover accent="true"><mi>u</mi> <mo>→</mo></mover></math>
    ’s), and the lengths of its principal axes are the singular values (the <math
    alttext="sigma"><mi>σ</mi></math> ’s). Since the singular values are ordered from
    largest to smallest, then <math alttext="ModifyingAbove u With right-arrow Subscript
    1"><msub><mover accent="true"><mi>u</mi> <mo>→</mo></mover> <mn>1</mn></msub></math>
    defines the direction with the most variation, and <math alttext="ModifyingAbove
    u With right-arrow Subscript 2"><msub><mover accent="true"><mi>u</mi> <mo>→</mo></mover>
    <mn>2</mn></msub></math> defines the direction with the second most variation,
    and so on.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Action of A on the right singular vectors
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let *A* be the <math alttext="2 times 2"><mrow><mn>2</mn> <mo>×</mo> <mn>2</mn></mrow></math>
    matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mi>A</mi> <mo>=</mo> <mfenced close=")" open="("><mtable><mtr><mtd><mn>1</mn></mtd>
    <mtd><mn>5</mn></mtd></mtr> <mtr><mtd><mrow><mo>-</mo> <mn>1</mn></mrow></mtd>
    <mtd><mn>2</mn></mtd></mtr></mtable></mfenced></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Its singular value decomposition <math alttext="upper A equals upper U normal
    upper Sigma upper V Superscript t"><mrow><mi>A</mi> <mo>=</mo> <mi>U</mi> <mi>Σ</mi>
    <msup><mi>V</mi> <mi>t</mi></msup></mrow></math> is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mi>A</mi> <mo>=</mo> <mfenced close=")" open="("><mtable><mtr><mtd><mrow><mn>0</mn>
    <mo>.</mo> <mn>93788501</mn></mrow></mtd> <mtd><mrow><mn>0</mn> <mo>.</mo> <mn>34694625</mn></mrow></mtd></mtr>
    <mtr><mtd><mrow><mn>0</mn> <mo>.</mo> <mn>34694625</mn></mrow></mtd> <mtd><mrow><mo>-</mo>
    <mn>0</mn> <mo>.</mo> <mn>93788501</mn></mrow></mtd></mtr></mtable></mfenced>
    <mfenced close=")" open="("><mtable><mtr><mtd><mrow><mn>5</mn> <mo>.</mo> <mn>41565478</mn></mrow></mtd>
    <mtd><mn>0</mn></mtd></mtr> <mtr><mtd><mn>0</mn></mtd> <mtd><mrow><mn>1</mn> <mo>.</mo>
    <mn>29254915</mn></mrow></mtd></mtr></mtable></mfenced> <mfenced close=")" open="("><mtable><mtr><mtd><mrow><mn>0</mn>
    <mo>.</mo> <mn>10911677</mn></mrow></mtd> <mtd><mrow><mn>0</mn> <mo>.</mo> <mn>99402894</mn></mrow></mtd></mtr>
    <mtr><mtd><mrow><mn>0</mn> <mo>.</mo> <mn>99402894</mn></mrow></mtd> <mtd><mrow><mo>-</mo>
    <mn>0</mn> <mo>.</mo> <mn>10911677</mn></mrow></mtd></mtr></mtable></mfenced></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: The expression <math alttext="upper A equals upper U normal upper Sigma upper
    V Superscript t"><mrow><mi>A</mi> <mo>=</mo> <mi>U</mi> <mi>Σ</mi> <msup><mi>V</mi>
    <mi>t</mi></msup></mrow></math> is equivalent to
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mi>A</mi> <mi>V</mi> <mo>=</mo> <mi>U</mi> <mi>Σ</mi>
    <mo>,</mo></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: since all we have to do is multiply <math alttext="upper A equals upper U normal
    upper Sigma upper V Superscript t"><mrow><mi>A</mi> <mo>=</mo> <mi>U</mi> <mi>Σ</mi>
    <msup><mi>V</mi> <mi>t</mi></msup></mrow></math> by *V* from the right and exploit
    the fact that <math alttext="upper V Superscript t Baseline upper V equals upper
    I"><mrow><msup><mi>V</mi> <mi>t</mi></msup> <mi>V</mi> <mo>=</mo> <mi>I</mi></mrow></math>
    due to the orthogonality of *V*.
  prefs: []
  type: TYPE_NORMAL
- en: We can think of *AV* as the matrix *A* acting on each column of the matrix *V*.
    Since <math alttext="upper A upper V equals upper U normal upper Sigma"><mrow><mi>A</mi>
    <mi>V</mi> <mo>=</mo> <mi>U</mi> <mi>Σ</mi></mrow></math> , then the action of
    <math alttext="upper A"><mi>A</mi></math> on the orthonormal columns of <math
    alttext="upper V"><mi>V</mi></math> is the same as stretching/squeezing the columns
    of <math alttext="upper U"><mi>U</mi></math> by the singular values. That is,
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mi>A</mi> <msub><mover accent="true"><mi>v</mi>
    <mo>→</mo></mover> <mn>1</mn></msub> <mo>=</mo> <msub><mi>σ</mi> <mn>1</mn></msub>
    <msub><mover accent="true"><mi>u</mi> <mo>→</mo></mover> <mn>1</mn></msub></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mi>A</mi> <msub><mover accent="true"><mi>v</mi>
    <mo>→</mo></mover> <mn>2</mn></msub> <mo>=</mo> <msub><mi>σ</mi> <mn>2</mn></msub>
    <msub><mover accent="true"><mi>u</mi> <mo>→</mo></mover> <mn>2</mn></msub></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: This is demonstrated in [Figure 6-5](#Fig_SVD_right_to_left).
  prefs: []
  type: TYPE_NORMAL
- en: '![200](assets/emai_0605.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6-5\. The matrix *A* sends the right singular vectors to multiples of
    the left singular vectors: <math alttext="upper A v 1 equals sigma 1 u 1"><mrow><mi>A</mi>
    <msub><mi>v</mi> <mn>1</mn></msub> <mo>=</mo> <msub><mi>σ</mi> <mn>1</mn></msub>
    <msub><mi>u</mi> <mn>1</mn></msub></mrow></math> and <math alttext="upper A v
    2 equals sigma 2 u 2"><mrow><mi>A</mi> <msub><mi>v</mi> <mn>2</mn></msub> <mo>=</mo>
    <msub><mi>σ</mi> <mn>2</mn></msub> <msub><mi>u</mi> <mn>2</mn></msub></mrow></math>
    .'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Action of A on the standard unit vectors and the unit square determined by them
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![200](assets/emai_0606.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-6\. The matrix *A* sends the standard unit vectors to its own columns
    and transforms the unit square into a parallelogram. There is no warping (bending)
    of space.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Action of A on the unit circle
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Figure 6-7](#Fig_transform_circle) shows that the matrix *A* sends the unit
    circle to an ellipse: The principal axes along the <math alttext="u"><mi>u</mi></math>
    ’s and lengths of the principal axes are equal to the <math alttext="sigma"><mi>σ</mi></math>
    ’s. Again, since matrices represent linear transformations, there are reflection/rotation
    and stretching/squeezing of space, but no warping.'
  prefs: []
  type: TYPE_NORMAL
- en: '![300](assets/emai_0607.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-7\. The matrix *A* sends the unit circle to an ellipse with principal
    axes along the left singular vectors and lengths of principal axes equal to the
    singular values.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We can easily see the above action from the Sigular Value Decomposition.
  prefs: []
  type: TYPE_NORMAL
- en: The Polar Decomposition
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mi>A</mi> <mo>=</mo> <mi>Q</mi> <mi>S</mi></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: is a very easy way that geometrically shows how a circle gets transformed into
    an elipse.
  prefs: []
  type: TYPE_NORMAL
- en: Breaking down the circle-to-ellipse transformation according to the singular
    value decomposition
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Figure 6-8](#Fig_svd_steps) shows four subplots that break down the steps
    of the circle to ellipse transformation illustrated above:'
  prefs: []
  type: TYPE_NORMAL
- en: First we multiply the unit circle and the vectors <math alttext="ModifyingAbove
    v With right-arrow Subscript 1"><msub><mover accent="true"><mi>v</mi> <mo>→</mo></mover>
    <mn>1</mn></msub></math> and <math alttext="ModifyingAbove v With right-arrow
    Subscript 2"><msub><mover accent="true"><mi>v</mi> <mo>→</mo></mover> <mn>2</mn></msub></math>
    by <math alttext="upper V Superscript t"><msup><mi>V</mi> <mi>t</mi></msup></math>
    . Since <math alttext="upper V Superscript t Baseline upper V equals upper I"><mrow><msup><mi>V</mi>
    <mi>t</mi></msup> <mi>V</mi> <mo>=</mo> <mi>I</mi></mrow></math> we have <math
    alttext="upper V Superscript t Baseline ModifyingAbove v With right-arrow Subscript
    1 Baseline equals ModifyingAbove e With right-arrow Subscript 1"><mrow><msup><mi>V</mi>
    <mi>t</mi></msup> <msub><mover accent="true"><mi>v</mi> <mo>→</mo></mover> <mn>1</mn></msub>
    <mo>=</mo> <msub><mover accent="true"><mi>e</mi> <mo>→</mo></mover> <mn>1</mn></msub></mrow></math>
    and <math alttext="upper V Superscript t Baseline ModifyingAbove v With right-arrow
    Subscript 2 Baseline equals ModifyingAbove e With right-arrow Subscript 2"><mrow><msup><mi>V</mi>
    <mi>t</mi></msup> <msub><mover accent="true"><mi>v</mi> <mo>→</mo></mover> <mn>2</mn></msub>
    <mo>=</mo> <msub><mover accent="true"><mi>e</mi> <mo>→</mo></mover> <mn>2</mn></msub></mrow></math>
    . So, in the beginning, the right singular vectors get *straightened out*, aligning
    correctly with the standard unit vectors.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Then we multiply by <math alttext="normal upper Sigma"><mi>Σ</mi></math> :
    All that happens here is stretching/squeezing the standard unit vectors by <math
    alttext="sigma 1"><msub><mi>σ</mi> <mn>1</mn></msub></math> and <math alttext="sigma
    2"><msub><mi>σ</mi> <mn>2</mn></msub></math> (the stretch or squeeze depend on
    whether the magnitude of the singular value is greater or smaller than one).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Finally we multiply by <math alttext="upper U"><mi>U</mi></math> : This either
    reflects the elipse across a line or rotates it a certain amount clockwise or
    counter clockwise. The next subsection explains this in detail.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![400](assets/emai_0608.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-8\. Steps of unit circle to ellipse transformation using the singular
    value decomposition.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Rotation And Reflection Matrices
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The matrices *U* and <math alttext="upper V Superscript t"><msup><mi>V</mi>
    <mi>t</mi></msup></math> that appear in the singular value decomposition <math
    alttext="upper A equals upper U normal upper Sigma upper V Superscript t"><mrow><mi>A</mi>
    <mo>=</mo> <mi>U</mi> <mi>Σ</mi> <msup><mi>V</mi> <mi>t</mi></msup></mrow></math>
    are orthogonal matrices. Their rows and columns are orthonormal and their inverse
    is the same as their transpose. In two dimensions, the <math alttext="upper U"><mi>U</mi></math>
    and <math alttext="upper V"><mi>V</mi></math> could either be rotation or reflection
    (about a line) matrices.
  prefs: []
  type: TYPE_NORMAL
- en: Rotation Matrix
  prefs: []
  type: TYPE_NORMAL
- en: 'A matrix that rotates clockwise by an angle <math alttext="theta"><mi>θ</mi></math>
    is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mfenced close=")" open="("><mtable><mtr><mtd><mrow><mo
    form="prefix">cos</mo> <mi>θ</mi></mrow></mtd> <mtd><mrow><mo form="prefix">sin</mo>
    <mi>θ</mi></mrow></mtd></mtr> <mtr><mtd><mrow><mo>-</mo> <mo form="prefix">sin</mo>
    <mi>θ</mi></mrow></mtd> <mtd><mrow><mo form="prefix">cos</mo> <mi>θ</mi></mrow></mtd></mtr></mtable></mfenced></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'The transpose of a rotation matrix is a rotation in the opposite direction.
    So if a matrix rotates clockwise by an angle <math alttext="theta"><mi>θ</mi></math>
    , then its transpose rotates counter clockwise by <math alttext="theta"><mi>θ</mi></math>
    and is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mfenced close=")" open="("><mtable><mtr><mtd><mrow><mo
    form="prefix">cos</mo> <mi>θ</mi></mrow></mtd> <mtd><mrow><mo>-</mo> <mo form="prefix">sin</mo>
    <mi>θ</mi></mrow></mtd></mtr> <mtr><mtd><mrow><mo form="prefix">sin</mo> <mi>θ</mi></mrow></mtd>
    <mtd><mrow><mo form="prefix">cos</mo> <mi>θ</mi></mrow></mtd></mtr></mtable></mfenced></math>
  prefs: []
  type: TYPE_NORMAL
- en: Reflection Matrix
  prefs: []
  type: TYPE_NORMAL
- en: 'A reflection matrix about a line <math alttext="upper L"><mi>L</mi></math>
    making an angle <math alttext="theta"><mi>θ</mi></math> with the <math alttext="x"><mi>x</mi></math>
    -axis is:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mfenced close=")" open="("><mtable><mtr><mtd><mrow><mo
    form="prefix">cos</mo> <mn>2</mn> <mi>θ</mi></mrow></mtd> <mtd><mrow><mo form="prefix">sin</mo>
    <mn>2</mn> <mi>θ</mi></mrow></mtd></mtr> <mtr><mtd><mrow><mo form="prefix">sin</mo>
    <mn>2</mn> <mi>θ</mi></mrow></mtd> <mtd><mrow><mo>-</mo> <mo form="prefix">cos</mo>
    <mn>2</mn> <mi>θ</mi></mrow></mtd></mtr></mtable></mfenced></math>
  prefs: []
  type: TYPE_NORMAL
- en: The slope of the straight line *L* is <math alttext="tangent theta"><mrow><mo
    form="prefix">tan</mo> <mi>θ</mi></mrow></math> and it passes through the origin,
    so its equation is <math alttext="y equals left-parenthesis tangent theta right-parenthesis
    x"><mrow><mi>y</mi> <mo>=</mo> <mo>(</mo> <mo form="prefix">tan</mo> <mi>θ</mi>
    <mo>)</mo> <mi>x</mi></mrow></math> . This line acts like a mirror for the reflection
    operation. [Figure 6-9](#Fig_svd_reflection) shows the two straight lines about
    which the matrices latx <math alttext="upper V Superscript t"><msup><mi>V</mi>
    <mi>t</mi></msup></math> and *U* reflect, together with a vector <math alttext="ModifyingAbove
    x With right-arrow"><mover accent="true"><mi>x</mi> <mo>→</mo></mover></math>
    and its subsequent transformation.
  prefs: []
  type: TYPE_NORMAL
- en: The determinant of a rotation matrix is 1 and the determinant of the reflection
    matrix is <math alttext="negative 1"><mrow><mo>-</mo> <mn>1</mn></mrow></math>
    .
  prefs: []
  type: TYPE_NORMAL
- en: 'In higher dimensions, reflection and rotation matrices look different. Always
    make sure you understand the object you are dealing with: If we have a rotation
    in a three dimensional space, then about what axis? If we have a reflection, then
    about what plane? If you want to dive deeper, then this is a good time to read
    about orthogonal matrices and their properties.'
  prefs: []
  type: TYPE_NORMAL
- en: Action of *A* on a general vector <math alttext="ModifyingAbove x With right-arrow"><mover
    accent="true"><mi>x</mi> <mo>→</mo></mover></math>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We have explored the action of *A* on the right singular vectors (they get mapped
    to the left singular vectors), the standard unit vectors (they get mapped to the
    columns of *A*), the unit square (it gets mapped to a parallelogram), the unit
    circle (it gets mapped to an ellipse with principal axes along the left singular
    vectors and whose lengths equal to the singular values). Finally, we explore the
    action of *A* on a general, nonspecial, vector <math alttext="ModifyingAbove x
    With right-arrow"><mover accent="true"><mi>x</mi> <mo>→</mo></mover></math> .
    This gets mapped to another nonspecial vector <math alttext="upper A ModifyingAbove
    x With right-arrow"><mrow><mi>A</mi> <mover accent="true"><mi>x</mi> <mo>→</mo></mover></mrow></math>
    . However, breaking down this transformation into steps using the singular value
    decomposition is informative.
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall our matrix *A* and its singular value decomposition:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign StartLayout 1st Row 1st Column upper A 2nd Column
    equals Start 2 By 2 Matrix 1st Row 1st Column 1 2nd Column 5 2nd Row 1st Column
    negative 1 2nd Column 2 EndMatrix 2nd Row 1st Column Blank 2nd Column equals upper
    U normal upper Sigma upper V Superscript t Baseline 3rd Row 1st Column Blank 2nd
    Column equals Start 2 By 2 Matrix 1st Row 1st Column 0.93788501 2nd Column 0.34694625
    2nd Row 1st Column 0.34694625 2nd Column negative 0.93788501 EndMatrix Start 2
    By 2 Matrix 1st Row 1st Column 5.41565478 2nd Column 0 2nd Row 1st Column 0 2nd
    Column 1.29254915 EndMatrix Start 2 By 2 Matrix 1st Row 1st Column 0.10911677
    2nd Column 0.99402894 2nd Row 1st Column 0.99402894 2nd Column negative 0.10911677
    EndMatrix EndLayout dollar-sign"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mi>A</mi></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <mfenced close=")" open="("><mtable><mtr><mtd><mn>1</mn></mtd>
    <mtd><mn>5</mn></mtd></mtr> <mtr><mtd><mrow><mo>-</mo> <mn>1</mn></mrow></mtd>
    <mtd><mn>2</mn></mtd></mtr></mtable></mfenced></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mrow><mo>=</mo>
    <mi>U</mi> <mi>Σ</mi> <msup><mi>V</mi> <mi>t</mi></msup></mrow></mtd></mtr> <mtr><mtd
    columnalign="left"><mrow><mo>=</mo> <mfenced close=")" open="("><mtable><mtr><mtd><mrow><mn>0</mn>
    <mo>.</mo> <mn>93788501</mn></mrow></mtd> <mtd><mrow><mn>0</mn> <mo>.</mo> <mn>34694625</mn></mrow></mtd></mtr>
    <mtr><mtd><mrow><mn>0</mn> <mo>.</mo> <mn>34694625</mn></mrow></mtd> <mtd><mrow><mo>-</mo>
    <mn>0</mn> <mo>.</mo> <mn>93788501</mn></mrow></mtd></mtr></mtable></mfenced>
    <mfenced close=")" open="("><mtable><mtr><mtd><mrow><mn>5</mn> <mo>.</mo> <mn>41565478</mn></mrow></mtd>
    <mtd><mn>0</mn></mtd></mtr> <mtr><mtd><mn>0</mn></mtd> <mtd><mrow><mn>1</mn> <mo>.</mo>
    <mn>29254915</mn></mrow></mtd></mtr></mtable></mfenced> <mfenced close=")" open="("><mtable><mtr><mtd><mrow><mn>0</mn>
    <mo>.</mo> <mn>10911677</mn></mrow></mtd> <mtd><mrow><mn>0</mn> <mo>.</mo> <mn>99402894</mn></mrow></mtd></mtr>
    <mtr><mtd><mrow><mn>0</mn> <mo>.</mo> <mn>99402894</mn></mrow></mtd> <mtd><mrow><mo>-</mo>
    <mn>0</mn> <mo>.</mo> <mn>10911677</mn></mrow></mtd></mtr></mtable></mfenced></mrow></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Both *U* and <math alttext="upper V Superscript t"><msup><mi>V</mi> <mi>t</mi></msup></math>
    in the above singular value decomposition happen to be reflection matrices. The
    straight lines <math alttext="upper L Subscript upper U"><msub><mi>L</mi> <mi>U</mi></msub></math>
    and <math alttext="upper L Subscript upper V Sub Superscript t"><msub><mi>L</mi>
    <msup><mi>V</mi> <mi>t</mi></msup></msub></math> that act as mirrors for these
    reflections are plotted in [Figure 6-9](#Fig_svd_reflection) and their equations
    are easy to find from their respective matrices: <math alttext="cosine left-parenthesis
    2 theta right-parenthesis"><mrow><mo form="prefix">cos</mo> <mo>(</mo> <mn>2</mn>
    <mi>θ</mi> <mo>)</mo></mrow></math> and <math alttext="sine left-parenthesis 2
    theta right-parenthesis"><mrow><mo form="prefix">sin</mo> <mo>(</mo> <mn>2</mn>
    <mi>θ</mi> <mo>)</mo></mrow></math> are on the first row so we can use those to
    find the slope <math alttext="tangent left-parenthesis theta right-parenthesis"><mrow><mo
    form="prefix">tan</mo> <mo>(</mo> <mi>θ</mi> <mo>)</mo></mrow></math> . The equation
    of the line along which <math alttext="upper V Superscript t"><msup><mi>V</mi>
    <mi>t</mi></msup></math> reflects is then <math alttext="y equals left-parenthesis
    tangent theta Subscript upper V Sub Superscript t Subscript Baseline right-parenthesis
    x equals 0.8962347008436108 x"><mrow><mi>y</mi> <mo>=</mo> <mo>(</mo> <mo form="prefix">tan</mo>
    <msub><mi>θ</mi> <msup><mi>V</mi> <mi>t</mi></msup></msub> <mo>)</mo> <mi>x</mi>
    <mo>=</mo> <mn>0</mn> <mo>.</mo> <mn>8962347008436108</mn> <mi>x</mi></mrow></math>
    and that of the line along which *U* reflects is <math alttext="y equals left-parenthesis
    tangent theta Subscript upper U Baseline right-parenthesis x equals 0.17903345403184898
    x"><mrow><mi>y</mi> <mo>=</mo> <mo>(</mo> <mo form="prefix">tan</mo> <msub><mi>θ</mi>
    <mi>U</mi></msub> <mo>)</mo> <mi>x</mi> <mo>=</mo> <mn>0</mn> <mo>.</mo> <mn>17903345403184898</mn>
    <mi>x</mi></mrow></math> . Since <math alttext="upper A ModifyingAbove x With
    right-arrow equals upper U normal upper Sigma upper V Superscript t Baseline ModifyingAbove
    x With right-arrow"><mrow><mi>A</mi> <mover accent="true"><mi>x</mi> <mo>→</mo></mover>
    <mo>=</mo> <mi>U</mi> <mi>Σ</mi> <msup><mi>V</mi> <mi>t</mi></msup> <mover accent="true"><mi>x</mi>
    <mo>→</mo></mover></mrow></math> , first <math alttext="ModifyingAbove x With
    right-arrow"><mover accent="true"><mi>x</mi> <mo>→</mo></mover></math> gets reflected
    across the line <math alttext="upper L Subscript upper V Sub Superscript t"><msub><mi>L</mi>
    <msup><mi>V</mi> <mi>t</mi></msup></msub></math> , arriving at <math alttext="upper
    V Superscript t Baseline ModifyingAbove x With right-arrow"><mrow><msup><mi>V</mi>
    <mi>t</mi></msup> <mover accent="true"><mi>x</mi> <mo>→</mo></mover></mrow></math>
    . Then, when we multiply by <math alttext="normal upper Sigma"><mi>Σ</mi></math>
    from the left, the first coordinate of <math alttext="upper V Superscript t Baseline
    ModifyingAbove x With right-arrow"><mrow><msup><mi>V</mi> <mi>t</mi></msup> <mover
    accent="true"><mi>x</mi> <mo>→</mo></mover></mrow></math> gets stretched horizontally
    by the first singular value, and the second coordinate gets stretched by the second
    singular value, obtaining <math alttext="normal upper Sigma upper V Superscript
    t Baseline ModifyingAbove x With right-arrow"><mrow><mi>Σ</mi> <msup><mi>V</mi>
    <mi>t</mi></msup> <mover accent="true"><mi>x</mi> <mo>→</mo></mover></mrow></math>
    . Finally, when we multiply by *U*, the vector <math alttext="normal upper Sigma
    upper V Superscript t Baseline ModifyingAbove x With right-arrow"><mrow><mi>Σ</mi>
    <msup><mi>V</mi> <mi>t</mi></msup> <mover accent="true"><mi>x</mi> <mo>→</mo></mover></mrow></math>
    gets reflected across the line <math alttext="upper L Subscript upper U"><msub><mi>L</mi>
    <mi>U</mi></msub></math> , arriving at <math alttext="upper A ModifyingAbove x
    With right-arrow equals upper U normal upper Sigma upper V Superscript t Baseline
    ModifyingAbove x With right-arrow"><mrow><mi>A</mi> <mover accent="true"><mi>x</mi>
    <mo>→</mo></mover> <mo>=</mo> <mi>U</mi> <mi>Σ</mi> <msup><mi>V</mi> <mi>t</mi></msup>
    <mover accent="true"><mi>x</mi> <mo>→</mo></mover></mrow></math> . [Figure 6-9](#Fig_svd_reflection)
    illustrates this process.'
  prefs: []
  type: TYPE_NORMAL
- en: '![400](assets/emai_0609.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-9\. Action of a matrix A on a general vector <math alttext="ModifyingAbove
    x With right-arrow"><mover accent="true"><mi>x</mi> <mo>→</mo></mover></math>
    . The transformation is done in steps using the singular value decomposition.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Three Ways To Multiply Matrices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Efficient algorithms for matrix multiplication are so desirable in the age
    of big data. In theory, there are three ways to multiply two matrices <math alttext="upper
    A Subscript m times n"><msub><mi>A</mi> <mrow><mi>m</mi><mo>×</mo><mi>n</mi></mrow></msub></math>
    and <math alttext="upper B Subscript n times s"><msub><mi>B</mi> <mrow><mi>n</mi><mo>×</mo><mi>s</mi></mrow></msub></math>
    :'
  prefs: []
  type: TYPE_NORMAL
- en: '**Row-column approach**: Produce one entry <math alttext="left-parenthesis
    a b right-parenthesis Subscript i j"><msub><mrow><mo>(</mo><mi>a</mi><mi>b</mi><mo>)</mo></mrow>
    <mrow><mi>i</mi><mi>j</mi></mrow></msub></math> at a time by taking the dot product
    of the *i*‘th row of *A* with the *j*‘th column of *B*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign left-parenthesis a b right-parenthesis Subscript
    i j Baseline equals upper A Subscript r o w Sub Subscript i Baseline upper B Subscript
    c o l Sub Subscript j Baseline equals sigma-summation Underscript k equals 1 Overscript
    n Endscripts a Subscript i k Baseline b Subscript k j dollar-sign"><mrow><msub><mrow><mo>(</mo><mi>a</mi><mi>b</mi><mo>)</mo></mrow>
    <mrow><mi>i</mi><mi>j</mi></mrow></msub> <mo>=</mo> <msub><mi>A</mi> <mrow><mi>r</mi><mi>o</mi><msub><mi>w</mi>
    <mi>i</mi></msub></mrow></msub> <msub><mi>B</mi> <mrow><mi>c</mi><mi>o</mi><msub><mi>l</mi>
    <mi>j</mi></msub></mrow></msub> <mo>=</mo> <msubsup><mo>∑</mo> <mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>n</mi></msubsup> <msub><mi>a</mi> <mrow><mi>i</mi><mi>k</mi></mrow></msub>
    <msub><mi>b</mi> <mrow><mi>k</mi><mi>j</mi></mrow></msub></mrow></math>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Column-columns approach**: Produce one column <math alttext="left-parenthesis
    upper A upper B right-parenthesis Subscript c o l Sub Subscript i"><msub><mrow><mo>(</mo><mi>A</mi><mi>B</mi><mo>)</mo></mrow>
    <mrow><mi>c</mi><mi>o</mi><msub><mi>l</mi> <mi>i</mi></msub></mrow></msub></math>
    at a time by linearly combining the columns of <math alttext="upper A"><mi>A</mi></math>
    using the entries of the *i*‘th column of <math alttext="upper B"><mi>B</mi></math>
    :'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign left-parenthesis upper A upper B right-parenthesis
    Subscript c o l Sub Subscript i Baseline equals b Subscript 1 i Baseline upper
    A Subscript c o l 1 Baseline plus b Subscript 2 i Baseline upper A Subscript c
    o l 2 Baseline plus ellipsis plus b Subscript n i Baseline upper A Subscript c
    o l Sub Subscript n dollar-sign"><mrow><msub><mrow><mo>(</mo><mi>A</mi><mi>B</mi><mo>)</mo></mrow>
    <mrow><mi>c</mi><mi>o</mi><msub><mi>l</mi> <mi>i</mi></msub></mrow></msub> <mo>=</mo>
    <msub><mi>b</mi> <mrow><mn>1</mn><mi>i</mi></mrow></msub> <msub><mi>A</mi> <mrow><mi>c</mi><mi>o</mi><msub><mi>l</mi>
    <mn>1</mn></msub></mrow></msub> <mo>+</mo> <msub><mi>b</mi> <mrow><mn>2</mn><mi>i</mi></mrow></msub>
    <msub><mi>A</mi> <mrow><mi>c</mi><mi>o</mi><msub><mi>l</mi> <mn>2</mn></msub></mrow></msub>
    <mo>+</mo> <mo>⋯</mo> <mo>+</mo> <msub><mi>b</mi> <mrow><mi>n</mi><mi>i</mi></mrow></msub>
    <msub><mi>A</mi> <mrow><mi>c</mi><mi>o</mi><msub><mi>l</mi> <mi>n</mi></msub></mrow></msub></mrow></math>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Column-row approach**: Produce *rank one pieces* of the product, one piece
    at a time, by multiplying the first column of *A* with the first row of *B*, the
    second column of *A* with the second row of *B*, and so on. Then add all these
    rank one matrices together to get the final product *AB*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mi>A</mi> <mi>B</mi> <mo>=</mo> <msub><mi>A</mi>
    <mrow><mi>c</mi><mi>o</mi><msub><mi>l</mi> <mn>1</mn></msub></mrow></msub> <msub><mi>B</mi>
    <mrow><mi>r</mi><mi>o</mi><msub><mi>w</mi> <mn>1</mn></msub></mrow></msub> <mo>+</mo>
    <msub><mi>A</mi> <mrow><mi>c</mi><mi>o</mi><msub><mi>l</mi> <mn>2</mn></msub></mrow></msub>
    <msub><mi>B</mi> <mrow><mi>r</mi><mi>o</mi><msub><mi>w</mi> <mn>2</mn></msub></mrow></msub>
    <mo>+</mo> <mo>⋯</mo> <mo>+</mo> <msub><mi>A</mi> <mrow><mi>c</mi><mi>o</mi><msub><mi>l</mi>
    <mi>n</mi></msub></mrow></msub> <msub><mi>B</mi> <mrow><mi>r</mi><mi>o</mi><msub><mi>w</mi>
    <mi>n</mi></msub></mrow></msub></mrow></math>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: How does this help us understand the usefulness of the Singular Value Decomposition?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We can expand the product <math alttext="upper A equals upper U normal upper
    Sigma upper V Superscript t"><mrow><mi>A</mi> <mo>=</mo> <mi>U</mi> <mi>Σ</mi>
    <msup><mi>V</mi> <mi>t</mi></msup></mrow></math> of the singular value decomposition
    as a sum of rank one matrices, using the *column-row* approach for matrix multiplication.
    Here, we multiply the matrix <math alttext="upper U normal upper Sigma"><mrow><mi>U</mi>
    <mi>Σ</mi></mrow></math> (which scales each column <math alttext="upper U Subscript
    c o l Sub Subscript i"><msub><mi>U</mi> <mrow><mi>c</mi><mi>o</mi><msub><mi>l</mi>
    <mi>i</mi></msub></mrow></msub></math> of *U* by <math alttext="sigma Subscript
    i"><msub><mi>σ</mi> <mi>i</mi></msub></math> ) with <math alttext="upper V Superscript
    t"><msup><mi>V</mi> <mi>t</mi></msup></math> :'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign upper A equals upper U normal upper Sigma upper V
    Superscript t Baseline equals sigma 1 upper U Subscript c o l 1 Baseline upper
    V Subscript r o w 1 Superscript t Baseline plus sigma 2 upper U Subscript c o
    l 2 Baseline upper V Subscript r o w 2 Superscript t Baseline plus ellipsis plus
    sigma Subscript r Baseline upper U Subscript c o l Sub Subscript r Baseline upper
    V Subscript r o w Sub Subscript r Superscript t dollar-sign"><mrow><mi>A</mi>
    <mo>=</mo> <mi>U</mi> <mi>Σ</mi> <msup><mi>V</mi> <mi>t</mi></msup> <mo>=</mo>
    <msub><mi>σ</mi> <mn>1</mn></msub> <msub><mi>U</mi> <mrow><mi>c</mi><mi>o</mi><msub><mi>l</mi>
    <mn>1</mn></msub></mrow></msub> <msubsup><mi>V</mi> <mrow><mi>r</mi><mi>o</mi><msub><mi>w</mi>
    <mn>1</mn></msub></mrow> <mi>t</mi></msubsup> <mo>+</mo> <msub><mi>σ</mi> <mn>2</mn></msub>
    <msub><mi>U</mi> <mrow><mi>c</mi><mi>o</mi><msub><mi>l</mi> <mn>2</mn></msub></mrow></msub>
    <msubsup><mi>V</mi> <mrow><mi>r</mi><mi>o</mi><msub><mi>w</mi> <mn>2</mn></msub></mrow>
    <mi>t</mi></msubsup> <mo>+</mo> <mo>⋯</mo> <mo>+</mo> <msub><mi>σ</mi> <mi>r</mi></msub>
    <msub><mi>U</mi> <mrow><mi>c</mi><mi>o</mi><msub><mi>l</mi> <mi>r</mi></msub></mrow></msub>
    <msubsup><mi>V</mi> <mrow><mi>r</mi><mi>o</mi><msub><mi>w</mi> <mi>r</mi></msub></mrow>
    <mi>t</mi></msubsup></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: where *r* is the number of nonzero singular values of *A* (also called the *rank
    of A*).
  prefs: []
  type: TYPE_NORMAL
- en: 'The great thing about the above expression is that it splits *A* into a sum
    of rank one matrices arranged according to their order of importance, since the
    <math alttext="sigma"><mi>σ</mi></math> ’s are arranged in decreasing order. Moreover,
    it provides a straightforward way to approximate *A* by lower rank matrices: Throw
    away lower sigular values. The [*Eckart–Young–Mirsky theorem*](https://en.wikipedia.org/wiki/Low-rank_approximation)
    asserts that this is in fact the *best way* to find low rank approximation of
    *A*, when the *closeness* of approximation is measured using the [*Frobenius norm*](https://en.wikipedia.org/wiki/Matrix_norm)
    (which is the square root of the sum of squares of the singular values) for matrices.
    Later in this chapter, we take advantage of this rank one decomposition of *A*
    for digital image compression.'
  prefs: []
  type: TYPE_NORMAL
- en: The Big Picture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far we have focused on the singular value decomposition of a matrix <math
    alttext="upper A equals upper U normal upper Sigma upper V Superscript t"><mrow><mi>A</mi>
    <mo>=</mo> <mi>U</mi> <mi>Σ</mi> <msup><mi>V</mi> <mi>t</mi></msup></mrow></math>
    in terms of *A*’s action on space and in terms of approximating *A* using lower
    rank matrices. Before moving to applications relevant to AI, let’s have an eagle
    eye perspective and address the big picture.
  prefs: []
  type: TYPE_NORMAL
- en: 'Given a matrix of real numbers, we want to understand the following, depending
    on our use case:'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If the matrix represents data that we care for, like images, or tabular data,
    what are the most important components of this matrix (data)?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Along what important directions is the data mostly spread (directions with most
    variation in the data)?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If I think of a matrix <math alttext="upper A Subscript m times n"><msub><mi>A</mi>
    <mrow><mi>m</mi><mo>×</mo><mi>n</mi></mrow></msub></math> as a transformation
    from the initial space <math alttext="double-struck upper R Superscript n"><msup><mi>ℝ</mi>
    <mi>n</mi></msup></math> to the target space <math alttext="double-struck upper
    R Superscript m"><msup><mi>ℝ</mi> <mi>m</mi></msup></math> , what is the effect
    of this matrix on vectors in <math alttext="double-struck upper R Superscript
    n"><msup><mi>ℝ</mi> <mi>n</mi></msup></math> ? To which vectors do they get sent
    to in <math alttext="double-struck upper R Superscript m"><msup><mi>ℝ</mi> <mi>m</mi></msup></math>
    ?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the effect of this matrix on space itself? Since this is a linear transormation
    we know there is no space warping, but there is space stretching, squeezing, rotating,
    reflecting.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Many physical systems can be represented as a system of linear equations <math
    alttext="upper A ModifyingAbove x With right-arrow equals ModifyingAbove b With
    right-arrow"><mrow><mi>A</mi> <mover accent="true"><mi>x</mi> <mo>→</mo></mover>
    <mo>=</mo> <mover accent="true"><mi>b</mi> <mo>→</mo></mover></mrow></math> ?
    How can we solve this system (find <math alttext="ModifyingAbove x With right-arrow"><mover
    accent="true"><mi>x</mi> <mo>→</mo></mover></math> )? What is the most efficient
    way to go about this, depending on the properties of *A*? If there is no solution,
    is there an approximate solution that satisfies our purposes? Note that here we
    are looking for the unknown vector <math alttext="ModifyingAbove x With right-arrow"><mover
    accent="true"><mi>x</mi> <mo>→</mo></mover></math> that gets transformed to <math
    alttext="ModifyingAbove b With right-arrow"><mover accent="true"><mi>b</mi> <mo>→</mo></mover></math>
    when *A* acts on it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The singular value decomposition can be used to answer all the above quesions.
    The first two questions are intrinsic to the matrix itself, while the second two
    questions have to do with the effect of multiplying the matrix with vectors (the
    matrix acts on space and the vectors in this space). The fifth question has to
    do with the very important problem of solving systems of linear equations and
    appears in all kinds of applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, we can investigate a matrix of numbers in two ways:'
  prefs: []
  type: TYPE_NORMAL
- en: What are its intrinsic properties?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are its properties when viewed as a transformation?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: These two are related because the matrix’s intrinsic properties affect how it
    acts on vectors and on space.
  prefs: []
  type: TYPE_NORMAL
- en: Properties To Keep In Mind
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '<math alttext="upper A"><mi>A</mi></math> sends the orthonormal vectors <math
    alttext="v Subscript i"><msub><mi>v</mi> <mi>i</mi></msub></math> (right singular
    vectors) of its initial space to scalar multiples of the orthonormal vectors <math
    alttext="u Subscript i"><msub><mi>u</mi> <mi>i</mi></msub></math> (left singular
    vectors) of its target space:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mi>A</mi> <msub><mi>v</mi> <mi>i</mi></msub> <mo>=</mo>
    <msub><mi>σ</mi> <mi>i</mi></msub> <msub><mi>u</mi> <mi>i</mi></msub></mrow></math>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Determinant of a square matrix**: If our matrix is square, then its determinant
    is equal to the product of all its singular values: <math alttext="sigma 1 sigma
    2 ellipsis sigma Subscript r"><mrow><msub><mi>σ</mi> <mn>1</mn></msub> <msub><mi>σ</mi>
    <mn>2</mn></msub> <mo>⋯</mo> <msub><mi>σ</mi> <mi>r</mi></msub></mrow></math>
    .'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The **condition number** of the matrix, with respect to the <math alttext="l
    squared"><msup><mi>l</mi> <mn>2</mn></msup></math> norm, which is usual distance
    in Euclidean space, is the ratio of the largest singular value to the smallest
    singular value:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mi>κ</mi> <mo>=</mo> <mfrac><msub><mi>σ</mi> <mn>1</mn></msub>
    <msub><mi>σ</mi> <mi>r</mi></msub></mfrac> <mo>.</mo></mrow></math>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The condition number is very important for computational stability
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'It measures how much <math alttext="upper A"><mi>A</mi></math> stretches space.
    If the condition number is too large, then it stretches space too much in one
    direction relative to another direction, and it could be dangerous to do computations
    in such an extremely stretched space: Solving <math alttext="upper A ModifyingAbove
    x With right-arrow equals ModifyingAbove b With right-arrow"><mrow><mi>A</mi>
    <mover accent="true"><mi>x</mi> <mo>→</mo></mover> <mo>=</mo> <mover accent="true"><mi>b</mi>
    <mo>→</mo></mover></mrow></math> when <math alttext="upper A"><mi>A</mi></math>
    has a large condition number makes the solution <math alttext="ModifyingAbove
    x With right-arrow"><mover accent="true"><mi>x</mi> <mo>→</mo></mover></math>
    unstable in the sense that it is extremely sensitive to perturbations in <math
    alttext="b"><mi>b</mi></math> . A small error in <math alttext="ModifyingAbove
    b With right-arrow"><mover accent="true"><mi>b</mi> <mo>→</mo></mover></math>
    will result in a solution <math alttext="ModifyingAbove x With right-arrow"><mover
    accent="true"><mi>x</mi> <mo>→</mo></mover></math> that is wildly different than
    the solution without the error in <math alttext="ModifyingAbove b With right-arrow"><mover
    accent="true"><mi>b</mi> <mo>→</mo></mover></math> . It is easy to envision this
    instability geometrically.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Numerically solving <math alttext="upper A ModifyingAbove x With right-arrow
    equals ModifyingAbove b With right-arrow"><mrow><mi>A</mi> <mover accent="true"><mi>x</mi>
    <mo>→</mo></mover> <mo>=</mo> <mover accent="true"><mi>b</mi> <mo>→</mo></mover></mrow></math>
    (say by Gaussian elimination) and iterative methods work fine when the involved
    matrices have reasonable (not very large) condition numbers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'One thing about a matrix with an especially large condition number: It stretches
    space too much that it almost collapses it into a space of lower dimension. The
    inetresting part is that if we decide to throw away that very small singular value
    and hence work in the collapsed space of lower dimension, our computations become
    perfectly fine. So at the boundaries of extremeness lies normalcy, except that
    this normalcy now lies in a lower dimension.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Many iterative numerical methods, including the very useful gradient descent,
    have matrices involved in their analysis. If the condition number of these matrices
    is too large, then the iterative method might not converge to a solution. The
    condition number controls how fast these iterative methods converge.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Ingredients Of The Singular Value Decomposition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter we have been dissecting only one formula: <math alttext="upper
    A equals upper U normal upper Sigma upper V Superscript t"><mrow><mi>A</mi> <mo>=</mo>
    <mi>U</mi> <mi>Σ</mi> <msup><mi>V</mi> <mi>t</mi></msup></mrow></math> . We used
    Python in order to compute the entries of *U*, <math alttext="normal upper Sigma"><mi>Σ</mi></math>
    , and *V*, but what exactly are these entries? The answer is short, if we happen
    to know what eigenvectors and eigenvalues are, which we clarify in the next section.
    For now , we list the ingredients of *U*, <math alttext="normal upper Sigma"><mi>Σ</mi></math>
    , and *V*:'
  prefs: []
  type: TYPE_NORMAL
- en: The columns of <math alttext="upper V"><mi>V</mi></math> (the right singular
    vectors) are the orthonormal eigenvectors of the symmetric matrix <math alttext="upper
    A Superscript t Baseline upper A"><mrow><msup><mi>A</mi> <mi>t</mi></msup> <mi>A</mi></mrow></math>
    .
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The columns of <math alttext="upper U"><mi>U</mi></math> (the left singular
    vectors) are the orthonormal eigenvectors of the symmetric matrix <math alttext="upper
    A upper A Superscript t"><mrow><mi>A</mi> <msup><mi>A</mi> <mi>t</mi></msup></mrow></math>
    .
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The singular values <math alttext="sigma 1"><msub><mi>σ</mi> <mn>1</mn></msub></math>
    , <math alttext="sigma 2"><msub><mi>σ</mi> <mn>2</mn></msub></math> , <math alttext="ellipsis"><mo>⋯</mo></math>
    <math alttext="sigma Subscript r"><msub><mi>σ</mi> <mi>r</mi></msub></math> are
    the square roots of the eigenvalues of <math alttext="upper A Superscript t Baseline
    upper A"><mrow><msup><mi>A</mi> <mi>t</mi></msup> <mi>A</mi></mrow></math> or
    <math alttext="upper A upper A Superscript t"><mrow><mi>A</mi> <msup><mi>A</mi>
    <mi>t</mi></msup></mrow></math> . The singular values are non-negative and arranged
    in decreasing order. The singular values can be zero.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: <math alttext="upper A v Subscript i Baseline equals sigma Subscript i Baseline
    u Subscript i"><mrow><mi>A</mi> <msub><mi>v</mi> <mi>i</mi></msub> <mo>=</mo>
    <msub><mi>σ</mi> <mi>i</mi></msub> <msub><mi>u</mi> <mi>i</mi></msub></mrow></math>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Note**: Every real symmetric positive semi-definite (non-negative eigenvalues)
    matrix is diagonalizable <math alttext="upper S equals upper P upper D upper P
    Superscript negative 1"><mrow><mi>S</mi> <mo>=</mo> <mi>P</mi> <mi>D</mi> <msup><mi>P</mi>
    <mrow><mo>-</mo><mn>1</mn></mrow></msup></mrow></math> , which means that it is
    similar to a diagonal matrix *D* when viewed in a different set of coordinates
    (the columns of *P*). <math alttext="upper A Superscript t Baseline upper A"><mrow><msup><mi>A</mi>
    <mi>t</mi></msup> <mi>A</mi></mrow></math> and <math alttext="upper A upper A
    Superscript t"><mrow><mi>A</mi> <msup><mi>A</mi> <mi>t</mi></msup></mrow></math>
    both happen to be symmetric positive semi-definite, so they are diagonalizable.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Singular Value Decomposition *vs* The Eigenvalue Decomposition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is important to learn more about symmetric matrices if we want to understand
    the ingredients of the singular value decomposition. This will also help us discern
    the difference between the sigular value decomposition <math alttext="upper A
    equals upper U normal upper Sigma upper V Superscript t"><mrow><mi>A</mi> <mo>=</mo>
    <mi>U</mi> <mi>Σ</mi> <msup><mi>V</mi> <mi>t</mi></msup></mrow></math> and the
    eigenvalue decomposition <math alttext="upper A equals upper P upper D upper P
    Superscript negative 1"><mrow><mi>A</mi> <mo>=</mo> <mi>P</mi> <mi>D</mi> <msup><mi>P</mi>
    <mrow><mo>-</mo><mn>1</mn></mrow></msup></mrow></math> when the latter exists.
  prefs: []
  type: TYPE_NORMAL
- en: '**Note** The singular value decomposition always exists, but the eigenvalue
    decomposition exists only for special matrices, called diagonalizable. Rectangular
    matrices are never diagonalizable. Square matrices may or may not be diagonalizable.
    When the square matrix is diagonalizable, the SVD and the eigenvalue decomposition
    are *not equal*, unless the matrix is symmetric and has non-negative eigenvalues.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The best and easiest matrices are square diagonal matrices with the same number
    along the diagonal.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The second best ones are square diagonal matrices <math alttext="upper D"><mi>D</mi></math>
    that don’t necessarily have the same numbers along the diagonal.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The third best matrices are symmetric matrices. These have real eigenvalues
    and orthogonal eigenvectors. They are the next closest type of matrices to diagonal
    matrices, in the sense that they are diagonalizable <math alttext="upper S equals
    upper P upper D upper P Superscript negative 1"><mrow><mi>S</mi> <mo>=</mo> <mi>P</mi>
    <mi>D</mi> <msup><mi>P</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup></mrow></math>
    , or similar to a diagonal matrix after a change in basis. The columns of <math
    alttext="upper P"><mi>P</mi></math> (eigenvectors) are orthogonal.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The fourth best matrices are square matrices that are diagonalizable <math alttext="upper
    A equals upper P upper D upper P Superscript negative 1"><mrow><mi>A</mi> <mo>=</mo>
    <mi>P</mi> <mi>D</mi> <msup><mi>P</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup></mrow></math>
    . These are similar to a diagonal matrix after a change of basis, however, the
    columns of <math alttext="upper P"><mi>P</mi></math> (eigenvectors) need not be
    orthogonal.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The fifth best matrices are all the rest. These are not diagonalizable, meaning
    there is no change of basis that can turn them diagonal, however, there is the
    next closest approach to making them similar to a diagonal matrix, via the singular
    value decomposition <math alttext="upper A equals upper U normal upper Sigma upper
    V Superscript t"><mrow><mi>A</mi> <mo>=</mo> <mi>U</mi> <mi>Σ</mi> <msup><mi>V</mi>
    <mi>t</mi></msup></mrow></math> . Here <math alttext="upper U"><mi>U</mi></math>
    and <math alttext="upper V"><mi>V</mi></math> are different than each other, and
    they have orthonormal columns and rows. Their inverse is very easy, since it is
    the same as their transpose. The singular value decomposition works for both square
    and non-square matrices.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Given a matrix <math alttext="upper A"><mi>A</mi></math> , both <math alttext="upper
    A Superscript t Baseline upper A"><mrow><msup><mi>A</mi> <mi>t</mi></msup> <mi>A</mi></mrow></math>
    and <math alttext="upper A upper A Superscript t"><mrow><mi>A</mi> <msup><mi>A</mi>
    <mi>t</mi></msup></mrow></math> happen to be symmetric and positive semi-definite
    (meaning their eigenvalues are non-negative), thus, they are diagonalizable with
    two bases of orthogonal eigenvectors. When we divide by the norm of these orthogonal
    eigenvectors they become orthonormal. These are the columns of <math alttext="upper
    V"><mi>V</mi></math> and of <math alttext="upper U"><mi>U</mi></math> respectively.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: <math alttext="upper A Superscript t Baseline upper A"><mrow><msup><mi>A</mi>
    <mi>t</mi></msup> <mi>A</mi></mrow></math> and <math alttext="upper A upper A
    Superscript t"><mrow><mi>A</mi> <msup><mi>A</mi> <mi>t</mi></msup></mrow></math>
    have exactly the same nonnegative eigenvalues, <math alttext="lamda Subscript
    i Baseline equals sigma Subscript i Superscript 2"><mrow><msub><mi>λ</mi> <mi>i</mi></msub>
    <mo>=</mo> <msubsup><mi>σ</mi> <mi>i</mi> <mn>2</mn></msubsup></mrow></math> .
    Arrange the square root of these in decreasing order (keeping the corresponding
    eigenvector order in <math alttext="upper U"><mi>U</mi></math> and <math alttext="upper
    V"><mi>V</mi></math> ), and we get the diagonal matrix <math alttext="normal upper
    Sigma"><mi>Σ</mi></math> in the singular value decomposition.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**What if the matrix we start with is symmetric? How is its singular value
    decomposition <math alttext="upper A equals upper U normal upper Sigma upper V
    Superscript t"><mrow><mi>A</mi> <mo>=</mo> <mi>U</mi> <mi>Σ</mi> <msup><mi>V</mi>
    <mi>t</mi></msup></mrow></math> related to its diagonalization <math alttext="upper
    A equals upper P upper D upper P Superscript negative 1"><mrow><mi>A</mi> <mo>=</mo>
    <mi>P</mi> <mi>D</mi> <msup><mi>P</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup></mrow></math>**
    ? The columns of <math alttext="upper P"><mi>P</mi></math> , which are the eigenvectors
    of symmetric <math alttext="upper A"><mi>A</mi></math> are orthogonal. When we
    divide by their lengths, they become orthonormal. Stack these orthonormal eigenvectors
    in a matrix in the order corresponding to decreasing absolute value of the eigenvalues
    and we get both the <math alttext="upper U"><mi>U</mi></math> and the <math alttext="upper
    V"><mi>V</mi></math> for the singular value decomposition. Now if all the eigenvalues
    of symmetric <math alttext="upper A"><mi>A</mi></math> happen to be nonnegative,
    the singular value decomposition of this positive semi-definite symmetric matrix
    will be the same as its eigenvalue decomposition, provided you normalize the orthogonal
    eigenvectors in <math alttext="upper P"><mi>P</mi></math> , order them with respect
    to the nonnegative eigenvalues in decreasing order. So <math alttext="upper U
    equals upper V"><mrow><mi>U</mi> <mo>=</mo> <mi>V</mi></mrow></math> in this case.
    What if some (or all) of the eigenvalues are negative? Then <math alttext="sigma
    Subscript i Baseline equals StartAbsoluteValue lamda Subscript i Baseline EndAbsoluteValue
    equals minus lamda Subscript i"><mrow><msub><mi>σ</mi> <mi>i</mi></msub> <mo>=</mo>
    <mrow><mo>|</mo> <msub><mi>λ</mi> <mi>i</mi></msub> <mo>|</mo></mrow> <mo>=</mo>
    <mo>-</mo> <msub><mi>λ</mi> <mi>i</mi></msub></mrow></math> , but now we have
    to be careful with the corresponding eigenvectors <math alttext="upper A v Subscript
    i Baseline equals minus lamda Subscript i Baseline v Subscript i Baseline equals
    lamda Subscript i Baseline left-parenthesis minus v Subscript i Baseline right-parenthesis
    equals sigma Subscript i Baseline u Subscript i"><mrow><mi>A</mi> <msub><mi>v</mi>
    <mi>i</mi></msub> <mo>=</mo> <mo>-</mo> <msub><mi>λ</mi> <mi>i</mi></msub> <msub><mi>v</mi>
    <mi>i</mi></msub> <mo>=</mo> <msub><mi>λ</mi> <mi>i</mi></msub> <mrow><mo>(</mo>
    <mo>-</mo> <msub><mi>v</mi> <mi>i</mi></msub> <mo>)</mo></mrow> <mo>=</mo> <msub><mi>σ</mi>
    <mi>i</mi></msub> <msub><mi>u</mi> <mi>i</mi></msub></mrow></math> . This makes
    <math alttext="upper U"><mi>U</mi></math> and <math alttext="upper V"><mi>V</mi></math>
    in the singular value decomposition unequal. So the singular value decomposition
    of a symmetric matrix that has some negative eigenvalues can be easily extracted
    from its eigenvalue decomposition, but it is not exactly the same.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**What if the matrix we start with is not symmteric but diagonalizable? How
    is its singular value decomposition <math alttext="upper A equals upper U normal
    upper Sigma upper V Superscript t"><mrow><mi>A</mi> <mo>=</mo> <mi>U</mi> <mi>Σ</mi>
    <msup><mi>V</mi> <mi>t</mi></msup></mrow></math> related to its diagonalization
    <math alttext="upper A equals upper P upper D upper P Superscript negative 1"><mrow><mi>A</mi>
    <mo>=</mo> <mi>P</mi> <mi>D</mi> <msup><mi>P</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup></mrow></math>**
    ? In this case, the eigenvectors of <math alttext="upper A"><mi>A</mi></math>
    , which are the columns of <math alttext="upper P"><mi>P</mi></math> , are in
    general not orthogonal, so the singular value decomposition and the eigenvalue
    decomposition of such a matrix are not related.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Computation Of The Singular Value Decomposition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'How do Python and others numerically calculate the singular value decomposition
    of a matrix? What numerical algorithms lie under the hood? The fast answer is:
    *QR* decomposition, *Householder reflections*, and *iterative algorithms* for
    eigenvalues and eigenvectors.'
  prefs: []
  type: TYPE_NORMAL
- en: In theory, calculating the singular value decomposition for a general matrix,
    or the eigenvalues and the eigenvectors for a square matrix, requires setting
    a polynomial=0 to solve for the eigenvalues, then setting up a linear system of
    equations to solve for the eigenvectors. This is far from being practical for
    applications. The problem of finding the zeros of a polynomial is very sensitive
    to any variations in the coefficients of the polynomials, so the computational
    problem becomes prone to round off errors that are present in the coefficients.
    We need stable numerical methods that find the eigenvectors and eigenvalues without
    having to numerically compute the zeros of a polynomial. Moreover, we need to
    make sure that the matrices involved in linear systems of equations are well conditioned,
    otherwise popular methods like Gaussian elimination (the <math alttext="upper
    L upper U"><mrow><mi>L</mi> <mi>U</mi></mrow></math> decomposition), do not work.
  prefs: []
  type: TYPE_NORMAL
- en: 'Most numerical implementations of the singular value decomposition try to avoid
    computing <math alttext="upper A upper A Superscript t"><mrow><mi>A</mi> <msup><mi>A</mi>
    <mi>t</mi></msup></mrow></math> and <math alttext="upper A Superscript t Baseline
    upper A"><mrow><msup><mi>A</mi> <mi>t</mi></msup> <mi>A</mi></mrow></math> . This
    is consistent with one of the themes of this book: Avoid multiplying matrices,
    instead, multiply a matrix with vectors. The popular numerical method for the
    singular value decomposition uses an algorithm called *Householder reflections*
    to transform the matrix to a bidiagonal matrix (sometimes preceded by a *QR* decomposition),
    then uses iterative algorithms to find the eigenvalues and eigenvectors. The field
    of numerical linear algebra develops such methods and adapts them to the types
    and sizes of matrices that appear in applications. In the next subsection, we
    present an iterative method to compute one eigenvalue and its corresponding eigenvector
    for a given matrix.'
  prefs: []
  type: TYPE_NORMAL
- en: An easy numerical way to find an eigenvector of a square matrix corresponding
    to its largest eigenvalue
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'An eigenvector of a square matrix <math alttext="upper A"><mi>A</mi></math>
    is a nonzero vector that does not change its direction when multiplied by <math
    alttext="upper A"><mi>A</mi></math> , instead, it only gets scaled by an eigenvalue
    <math alttext="lamda"><mi>λ</mi></math> :'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mi>A</mi> <mi>v</mi> <mo>=</mo> <mi>λ</mi> <mi>v</mi>
    <mo>.</mo></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'The following iterative algorithm is an easy numerical method that finds an
    eigenvector of a matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: Start at a random unit vector (of length 1) <math alttext="v 0"><msub><mi>v</mi>
    <mn>0</mn></msub></math>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Multiply by <math alttext="upper A"><mi>A</mi></math> : <math alttext="v Subscript
    i plus 1 Baseline equals upper A v Subscript i"><mrow><msub><mi>v</mi> <mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub>
    <mo>=</mo> <mi>A</mi> <msub><mi>v</mi> <mi>i</mi></msub></mrow></math>'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Divide by the length of <math alttext="v Subscript i plus 1"><msub><mi>v</mi>
    <mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub></math> in order to avoid the
    size of our vectors growing too large.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Stop when you converge.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The above iterative method is very simple but has a drawback: It only finds
    one eigenvector of the matrix, the eigenvector corresponding to its largest eigenvalue.
    So it finds the direction that gets stretched the most when we apply <math alttext="upper
    A"><mi>A</mi></math> .'
  prefs: []
  type: TYPE_NORMAL
- en: For example, consider the matrix <math alttext="upper A equals Start 2 By 2
    Matrix 1st Row 1st Column 1 2nd Column 2 2nd Row 1st Column 2 2nd Column negative
    3 EndMatrix"><mrow><mi>A</mi> <mo>=</mo> <mfenced close=")" open="("><mtable><mtr><mtd><mn>1</mn></mtd>
    <mtd><mn>2</mn></mtd></mtr> <mtr><mtd><mn>2</mn></mtd> <mtd><mrow><mo>-</mo> <mn>3</mn></mrow></mtd></mtr></mtable></mfenced></mrow></math>
    . We start with the vector <math alttext="ModifyingAbove v With right-arrow Subscript
    0 Baseline equals StartBinomialOrMatrix 1 Choose 0 EndBinomialOrMatrix"><mrow><msub><mover
    accent="true"><mi>v</mi> <mo>→</mo></mover> <mn>0</mn></msub> <mo>=</mo> <mfenced
    close=")" open="("><mtable><mtr><mtd><mn>1</mn></mtd></mtr> <mtr><mtd><mn>0</mn></mtd></mtr></mtable></mfenced></mrow></math>
    and apply the above algorithm. We note that the algorithm after 28 iterations
    to the vector <math alttext="ModifyingAbove v With right-arrow equals StartBinomialOrMatrix
    negative 0.38268343 Choose 0.92387953 EndBinomialOrMatrix"><mrow><mover accent="true"><mi>v</mi>
    <mo>→</mo></mover> <mo>=</mo> <mfenced close=")" open="("><mtable><mtr><mtd><mrow><mo>-</mo>
    <mn>0</mn> <mo>.</mo> <mn>38268343</mn></mrow></mtd></mtr> <mtr><mtd><mrow><mn>0</mn>
    <mo>.</mo> <mn>92387953</mn></mrow></mtd></mtr></mtable></mfenced></mrow></math>
    . The code is in the linked Jupyter notebook and the output is shown below.
  prefs: []
  type: TYPE_NORMAL
- en: '`**Output:**`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[Figure 6-10](#Fig_eigenvector_iteration) shows the above iteration. Note that
    all the vectors have length 1 and that the direction of the vector does not change
    when the algorithm converges, hence capturing an eigenvector of *A*. For the last
    few iterations, the sign keeps oscillating, so the vector keeps flipping orientation,
    so the eigenvalue must be negative. Indeed, we find it to be <math alttext="lamda
    equals negative 3.828427140993716"><mrow><mi>λ</mi> <mo>=</mo> <mo>-</mo> <mn>3</mn>
    <mo>.</mo> <mn>828427140993716</mn></mrow></math> .'
  prefs: []
  type: TYPE_NORMAL
- en: '![400](assets/emai_0610.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-10\. We start at <math alttext="ModifyingAbove v With right-arrow Subscript
    0 Baseline equals StartBinomialOrMatrix 1 Choose 0 EndBinomialOrMatrix"><mrow><msub><mover
    accent="true"><mi>v</mi> <mo>→</mo></mover> <mn>0</mn></msub> <mo>=</mo> <mfenced
    close=")" open="("><mtable><mtr><mtd><mn>1</mn></mtd></mtr> <mtr><mtd><mn>0</mn></mtd></mtr></mtable></mfenced></mrow></math>
    , then we multiply by *A* and normalize until we converge to an eigenvector.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The Pseudo-Inverse
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Many physical systems can be represented by (or approximated by) a linear system
    of equations <math alttext="upper A ModifyingAbove x With right-arrow equals ModifyingAbove
    b With right-arrow"><mrow><mi>A</mi> <mover accent="true"><mi>x</mi> <mo>→</mo></mover>
    <mo>=</mo> <mover accent="true"><mi>b</mi> <mo>→</mo></mover></mrow></math> .
    If <math alttext="ModifyingAbove x With right-arrow"><mover accent="true"><mi>x</mi>
    <mo>→</mo></mover></math> is an unknown vector that we care for, then we need
    to *divide by* the matrix *A* in order to find <math alttext="ModifyingAbove x
    With right-arrow"><mover accent="true"><mi>x</mi> <mo>→</mo></mover></math> .
    The matrix equivalent of division is finding the *inverse* <math alttext="upper
    A Superscript negative 1"><msup><mi>A</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup></math>
    , so that the solution <math alttext="ModifyingAbove x With right-arrow equals
    upper A Superscript negative 1 Baseline ModifyingAbove b With right-arrow"><mrow><mover
    accent="true"><mi>x</mi> <mo>→</mo></mover> <mo>=</mo> <msup><mi>A</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup>
    <mover accent="true"><mi>b</mi> <mo>→</mo></mover></mrow></math> . Matrices that
    have an inverse are called *invertible*. These are square matrices with a nonzero
    determinant (the determinant is the product of the eigenvalues, the product of
    the singular values and the determinant will have the same absolute value). But
    what about all the systems whose matrices are rectangular? How about those with
    non-invertible matrices? And those whose matrices are square and invertible, but
    are *almost* non-invertible (their determinant is very close to zero)? We still
    care about finding *solutions* to such systems. The power of the singular value
    decomposition is that it exists for *any matrix*, including those mentioned above,
    and it can help us *invert* any matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 'Given any matrix and its singular value decomposition <math alttext="upper
    A equals upper U normal upper Sigma upper V Superscript t"><mrow><mi>A</mi> <mo>=</mo>
    <mi>U</mi> <mi>Σ</mi> <msup><mi>V</mi> <mi>t</mi></msup></mrow></math> , we can
    define its *pseudo-inverse* as:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><msup><mi>A</mi> <mo>+</mo></msup> <mo>=</mo> <mi>V</mi>
    <msup><mi>Σ</mi> <mo>+</mo></msup> <msup><mi>U</mi> <mi>t</mi></msup> <mo>,</mo></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: where <math alttext="normal upper Sigma Superscript plus"><msup><mi>Σ</mi> <mo>+</mo></msup></math>
    is obtained from <math alttext="normal upper Sigma"><mi>Σ</mi></math> by inverting
    all its diagonal entries except for the ones which are zero (or very close to
    zero if the matrix happens to be ill-conditioned).
  prefs: []
  type: TYPE_NORMAL
- en: This allows us to find *solutions* to *any* system of linear equation <math
    alttext="upper A x equals b"><mrow><mi>A</mi> <mi>x</mi> <mo>=</mo> <mi>b</mi></mrow></math>
    , namely <math alttext="x equals upper A Superscript plus Baseline b"><mrow><mi>x</mi>
    <mo>=</mo> <msup><mi>A</mi> <mo>+</mo></msup> <mi>b</mi></mrow></math> .
  prefs: []
  type: TYPE_NORMAL
- en: '**Note** The pseudo-inverse of a matrix coincides with its inverse when the
    latter exists.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Applying The Singular Value Decomposition To Images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We are finally ready for real world applications of the singular value decomposition.
    We start with image compression. Digital images are stored as matrices of numbers
    where each number correponds to the intensity of a pixel. We will use the singular
    value decomposition to reduce the storage requirements of an image without losing
    its most essential information. All we have to do is throw away the insignificant
    singular values, along with the corresponding columns of *U* and rows of <math
    alttext="upper V Superscript t"><msup><mi>V</mi> <mi>t</mi></msup></math> . The
    mathematical expression that helps us here is:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign upper A equals upper U normal upper Sigma upper V
    Superscript t Baseline equals sigma 1 upper U Subscript c o l 1 Baseline upper
    V Subscript r o w 1 Superscript t Baseline plus sigma 2 upper U Subscript c o
    l 2 Baseline upper V Subscript r o w 2 Superscript t Baseline plus ellipsis plus
    sigma Subscript r Baseline upper U Subscript c o l Sub Subscript r Baseline upper
    V Subscript r o w Sub Subscript r Superscript t dollar-sign"><mrow><mi>A</mi>
    <mo>=</mo> <mi>U</mi> <mi>Σ</mi> <msup><mi>V</mi> <mi>t</mi></msup> <mo>=</mo>
    <msub><mi>σ</mi> <mn>1</mn></msub> <msub><mi>U</mi> <mrow><mi>c</mi><mi>o</mi><msub><mi>l</mi>
    <mn>1</mn></msub></mrow></msub> <msubsup><mi>V</mi> <mrow><mi>r</mi><mi>o</mi><msub><mi>w</mi>
    <mn>1</mn></msub></mrow> <mi>t</mi></msubsup> <mo>+</mo> <msub><mi>σ</mi> <mn>2</mn></msub>
    <msub><mi>U</mi> <mrow><mi>c</mi><mi>o</mi><msub><mi>l</mi> <mn>2</mn></msub></mrow></msub>
    <msubsup><mi>V</mi> <mrow><mi>r</mi><mi>o</mi><msub><mi>w</mi> <mn>2</mn></msub></mrow>
    <mi>t</mi></msubsup> <mo>+</mo> <mo>⋯</mo> <mo>+</mo> <msub><mi>σ</mi> <mi>r</mi></msub>
    <msub><mi>U</mi> <mrow><mi>c</mi><mi>o</mi><msub><mi>l</mi> <mi>r</mi></msub></mrow></msub>
    <msubsup><mi>V</mi> <mrow><mi>r</mi><mi>o</mi><msub><mi>w</mi> <mi>r</mi></msub></mrow>
    <mi>t</mi></msubsup></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: Recall that the <math alttext="sigma"><mi>σ</mi></math> ’s are arranged from
    the largest value to the smallest value, so the idea is that we can keep the first
    few large <math alttext="sigma"><mi>σ</mi></math> ’s and throw away the rest of
    the <math alttext="sigma"><mi>σ</mi></math> ’s, which are small anyway.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s work with the image in [Figure 6-11](#Fig_image_compression). The code
    and the details are in the linked Jupyter notebook. Each color image has three
    channels, red, green, and blue (see [Figure 6-12](#Fig_image_rgb) and [Figure 6-13](#Fig_image_rgb_tint)).
    Each channel is a matrix of numbers, just like the ones we have been working with
    in this chapter. Each channel of the image in [Figure 6-11](#Fig_image_compression)
    is a <math alttext="s i z e equals 960 times 714"><mrow><mi>s</mi> <mi>i</mi>
    <mi>z</mi> <mi>e</mi> <mo>=</mo> <mn>960</mn> <mo>×</mo> <mn>714</mn></mrow></math>
    matrix, so to store the full image, we need <math alttext="s i z e equals 960
    times 714 times 3 equals 2 comma 056 comma 320"><mrow><mi>s</mi> <mi>i</mi> <mi>z</mi>
    <mi>e</mi> <mo>=</mo> <mn>960</mn> <mo>×</mo> <mn>714</mn> <mo>×</mo> <mn>3</mn>
    <mo>=</mo> <mn>2</mn> <mo>,</mo> <mn>056</mn> <mo>,</mo> <mn>320</mn></mrow></math>
    numbers. Imagine the storage requirements for a streaming video, which contains
    many image frames. We need a compression mechanism, so as not to run out of memory.
    We compute the singular value decomposition for each channel (see [Figure 6-14](#Fig_image_svd_red)
    for an image representation of the singular value decomposition for the red channel).
    We then perform a massive reduction, retaining for each channel only the first
    25 singular values (out of 714), 25 columns of *U* (out of 960), and 25 rows of
    <math alttext="upper V Superscript t"><msup><mi>V</mi> <mi>t</mi></msup></math>
    (out of 714). The storage reduction for each channel is substantial: *U* is now
    <math alttext="960 times 25"><mrow><mn>960</mn> <mo>×</mo> <mn>25</mn></mrow></math>
    , <math alttext="upper V Superscript t"><msup><mi>V</mi> <mi>t</mi></msup></math>
    is <math alttext="25 times 714"><mrow><mn>25</mn> <mo>×</mo> <mn>714</mn></mrow></math>
    and we only need to store 25 singular values (no need to store the zeros of the
    diagonal matrix <math alttext="normal upper Sigma"><mi>Σ</mi></math> ). This adds
    up 41,875 numbers for each channel, so for all three channels, we need to store
    <math alttext="41 comma 875 times 3 equals 125 comma 625"><mrow><mn>41</mn> <mo>,</mo>
    <mn>875</mn> <mo>×</mo> <mn>3</mn> <mo>=</mo> <mn>125</mn> <mo>,</mo> <mn>625</mn></mrow></math>
    numbers, a whopping 93 percent storage requirements reduction.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We put the image back together, one channel at a time, by multiplying the reduced
    *U*, reduced <math alttext="normal upper Sigma"><mi>Σ</mi></math> , and reduced
    <math alttext="upper V Superscript t"><msup><mi>V</mi> <mi>t</mi></msup></math>
    together:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign upper C h a n n e l Subscript r e d u c e d Baseline
    equals upper U Subscript 960 times 25 Baseline normal upper Sigma Subscript 25
    times 25 Baseline left-parenthesis upper V Superscript t Baseline right-parenthesis
    Subscript 25 times 714 dollar-sign"><mrow><mi>C</mi> <mi>h</mi> <mi>a</mi> <mi>n</mi>
    <mi>n</mi> <mi>e</mi> <msub><mi>l</mi> <mrow><mi>r</mi><mi>e</mi><mi>d</mi><mi>u</mi><mi>c</mi><mi>e</mi><mi>d</mi></mrow></msub>
    <mo>=</mo> <msub><mi>U</mi> <mrow><mn>960</mn><mo>×</mo><mn>25</mn></mrow></msub>
    <msub><mi>Σ</mi> <mrow><mn>25</mn><mo>×</mo><mn>25</mn></mrow></msub> <msub><mrow><mo>(</mo><msup><mi>V</mi>
    <mi>t</mi></msup> <mo>)</mo></mrow> <mrow><mn>25</mn><mo>×</mo><mn>714</mn></mrow></msub></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 6-15](#Fig_image_rgb_reduced) shows the result of this multiplication
    for each of the red, green, and blue channels.'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we layer the reduced channels to produce the reduced image ([Figure 6-16](#Fig_image_reduced)).
    It is obvious that we lost a lot of detail in the process, but that is a tradeoff
    we have to live with.
  prefs: []
  type: TYPE_NORMAL
- en: '![400](assets/emai_0611.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-11\. A digital color image of <math alttext="s i z e equals 960 times
    714 times 3"><mrow><mi>s</mi> <mi>i</mi> <mi>z</mi> <mi>e</mi> <mo>=</mo> <mn>960</mn>
    <mo>×</mo> <mn>714</mn> <mo>×</mo> <mn>3</mn></mrow></math> .
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![400](assets/emai_0612.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-12\. The red, green, and blue channels of the digital image. Each has
    size <math alttext="s i z e equals 960 times 714"><mrow><mi>s</mi> <mi>i</mi>
    <mi>z</mi> <mi>e</mi> <mo>=</mo> <mn>960</mn> <mo>×</mo> <mn>714</mn></mrow></math>
    .
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![400](assets/emai_0613.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-13\. Showing the red, green, and blue tints for the three channels
    of the digital image. Each has size <math alttext="s i z e equals 960 times 714
    times 3"><mrow><mi>s</mi> <mi>i</mi> <mi>z</mi> <mi>e</mi> <mo>=</mo> <mn>960</mn>
    <mo>×</mo> <mn>714</mn> <mo>×</mo> <mn>3</mn></mrow></math> .
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![400](assets/emai_0614.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-14\. The singular value decomposition of the red channel. We have 714
    nonzero singular values, but only few significant ones.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![400](assets/emai_0615.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-15\. The red, green, and blue channels after rank reduction. For each
    channel we have retained only the first 25 singular values, the first 25 columns
    of *U*, and the first 25 rows of <math alttext="upper V Superscript t"><msup><mi>V</mi>
    <mi>t</mi></msup></math> .
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![400](assets/emai_0616.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-16\. Original image with 714 singular values *vs* reduced rank image
    with only 25 singular values. Both still have size <math alttext="s i z e equals
    960 times 714 times 3"><mrow><mi>s</mi> <mi>i</mi> <mi>z</mi> <mi>e</mi> <mo>=</mo>
    <mn>960</mn> <mo>×</mo> <mn>714</mn> <mo>×</mo> <mn>3</mn></mrow></math> but require
    different storage space.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'For advanced image compression techniques, check this article: [Image compression
    techniques: A survey in lossless and lossy algorithms](https://www.sciencedirect.com/science/article/abs/pii/S0925231218302935),
    Neurocomputing (Elsevier) Volume 300, 26 July 2018, Pages 44-69.'
  prefs: []
  type: TYPE_NORMAL
- en: Principal Component Analysis And Dimension Reduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Principal component analysis is widely popular for data analysis. It is used
    for dimension reduction and clustering in unsupervised machine learning. In a
    nutshell, it is the singular value decomposition performed on the data matrix
    *X*, after *centering* the data, which means subtracting the average value of
    each feature from each feature column (each column of *X*). The principal components
    are then the right singular vectors, which are the rows of <math alttext="upper
    V Superscript t"><msup><mi>V</mi> <mi>t</mi></msup></math> in the now familiar
    decomposition <math alttext="upper X equals upper U normal upper Sigma upper V
    Superscript t"><mrow><mi>X</mi> <mo>=</mo> <mi>U</mi> <mi>Σ</mi> <msup><mi>V</mi>
    <mi>t</mi></msup></mrow></math> .
  prefs: []
  type: TYPE_NORMAL
- en: 'Statisticians like to describe the principal component analysis using the language
    of variance, or variation in the data, and *uncorrelating* the data. They end
    up working with the eigenvectors of the *covariance matrix* of the data. This
    is a familiar description of principal component analysis in statistics: *It is
    a method that reduces the dimensionality of a dataset, while preserving as much
    variability, or statistical information, as possible. Preserving as much variability
    as possible translates into finding new features that are linear combinations
    of those of the dataset, that successively maximize variance and that are uncorrelated
    with each other*. See for example [Principal component analysis: a review and
    recent developments](https://royalsocietypublishing.org/doi/pdf/10.1098/rsta.2015.0202).'
  prefs: []
  type: TYPE_NORMAL
- en: The two descriptions (the right singular vectors of the centered data, and the
    eigenvectors of the covariance matrix) are exactly the same, since the rows of
    <math alttext="upper V Superscript t"><msup><mi>V</mi> <mi>t</mi></msup></math>
    are the eigenvectors of <math alttext="upper X Subscript c e n t e r e d Superscript
    t Baseline upper X Subscript c e n t e r e d"><mrow><msubsup><mi>X</mi> <mrow><mi>c</mi><mi>e</mi><mi>n</mi><mi>t</mi><mi>e</mi><mi>r</mi><mi>e</mi><mi>d</mi></mrow>
    <mi>t</mi></msubsup> <msub><mi>X</mi> <mrow><mi>c</mi><mi>e</mi><mi>n</mi><mi>t</mi><mi>e</mi><mi>r</mi><mi>e</mi><mi>d</mi></mrow></msub></mrow></math>
    , which in turn is the covariance matrix of the data. Moreover, the term *uncorrelating*
    in statistics corresponds to *diagonalizing* in mathematics and linear algebra,
    and the singular value decomposition says that any matrix *acts like* a diagonal
    matrix, namely <math alttext="normal upper Sigma"><mi>Σ</mi></math> , when written
    in a new set of coordinates, namely the rows of <math alttext="upper V Superscript
    t"><msup><mi>V</mi> <mi>t</mi></msup></math> , which are the columns of *V*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s explain this in detail. Suppose *X* is a centered data matrix and its
    singular value decomposition is <math alttext="upper X equals upper U normal upper
    Sigma upper V Superscript t"><mrow><mi>X</mi> <mo>=</mo> <mi>U</mi> <mi>Σ</mi>
    <msup><mi>V</mi> <mi>t</mi></msup></mrow></math> . This is the same as <math alttext="upper
    X upper V equals upper U normal upper Sigma"><mrow><mi>X</mi> <mi>V</mi> <mo>=</mo>
    <mi>U</mi> <mi>Σ</mi></mrow></math> , or, when we resolve the expression column-wise:
    <math alttext="upper X upper V Subscript c o l Sub Subscript i Baseline equals
    sigma Subscript i Baseline upper U Subscript c o l Sub Subscript i"><mrow><mi>X</mi>
    <msub><mi>V</mi> <mrow><mi>c</mi><mi>o</mi><msub><mi>l</mi> <mi>i</mi></msub></mrow></msub>
    <mo>=</mo> <msub><mi>σ</mi> <mi>i</mi></msub> <msub><mi>U</mi> <mrow><mi>c</mi><mi>o</mi><msub><mi>l</mi>
    <mi>i</mi></msub></mrow></msub></mrow></math> . Note that <math alttext="upper
    X upper V Subscript c o l Sub Subscript i"><mrow><mi>X</mi> <msub><mi>V</mi> <mrow><mi>c</mi><mi>o</mi><msub><mi>l</mi>
    <mi>i</mi></msub></mrow></msub></mrow></math> is just a linear combination of
    the features of the data using the entries of that particular column of *V*. Now,
    faithful to what we have been doing all along in this chapter, we can throw away
    the less significant components, meaning the columns of *V* and *U* corresponding
    to the lower singular values.'
  prefs: []
  type: TYPE_NORMAL
- en: Suppose now that our data has 200 features, but only two singular values are
    significant, so we decide to only keep the first two columns of *V* and the first
    two columns of *U*. Thus, we have reduced the dimension of the features from 200
    to 2\. The first new feature is a linear combination of all the original 200 features
    using the entries of the first column of *V*, but that is exactly <math alttext="sigma
    1 upper U Subscript c o l 1"><mrow><msub><mi>σ</mi> <mn>1</mn></msub> <msub><mi>U</mi>
    <mrow><mi>c</mi><mi>o</mi><msub><mi>l</mi> <mn>1</mn></msub></mrow></msub></mrow></math>
    , and the second new feature is a linear combination of all the original 200 features
    using the entries of the second column of *V*, but that is exactly <math alttext="sigma
    2 upper U Subscript c o l 2"><mrow><msub><mi>σ</mi> <mn>2</mn></msub> <msub><mi>U</mi>
    <mrow><mi>c</mi><mi>o</mi><msub><mi>l</mi> <mn>2</mn></msub></mrow></msub></mrow></math>
    .
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s think of individual data points. A data point in the data matrix *X*
    has 200 features. This means that we need 200 axes to plot this data point. However,
    taking the dimension reduction we performed above using only the first two principal
    components, this data point will now have only two coordinates, which are the
    corresponding entries of <math alttext="sigma 1 upper U Subscript c o l 1"><mrow><msub><mi>σ</mi>
    <mn>1</mn></msub> <msub><mi>U</mi> <mrow><mi>c</mi><mi>o</mi><msub><mi>l</mi>
    <mn>1</mn></msub></mrow></msub></mrow></math> and <math alttext="sigma 2 upper
    U Subscript c o l 2"><mrow><msub><mi>σ</mi> <mn>2</mn></msub> <msub><mi>U</mi>
    <mrow><mi>c</mi><mi>o</mi><msub><mi>l</mi> <mn>2</mn></msub></mrow></msub></mrow></math>
    . So if this was the third data point in the data set, then its new coordinates
    will be the third entry of <math alttext="sigma 1 upper U Subscript c o l 1"><mrow><msub><mi>σ</mi>
    <mn>1</mn></msub> <msub><mi>U</mi> <mrow><mi>c</mi><mi>o</mi><msub><mi>l</mi>
    <mn>1</mn></msub></mrow></msub></mrow></math> and the third entry of <math alttext="sigma
    2 upper U Subscript c o l 2"><mrow><msub><mi>σ</mi> <mn>2</mn></msub> <msub><mi>U</mi>
    <mrow><mi>c</mi><mi>o</mi><msub><mi>l</mi> <mn>2</mn></msub></mrow></msub></mrow></math>
    . Now it is easy to plot this data point in a two dimensional space, as opposed
    to plotting it in the original 200 dimensional space.
  prefs: []
  type: TYPE_NORMAL
- en: It is us who choose how many singular values (and thus principal components)
    to retain. The more we keep, the more faithful to the original data set we would
    be, but of course the dimension will be higher. This *truncation* decision (finding
    optimal threshold for singular value truncation) is the subject of ongoing research.
    The common method is determining the desired rank ahead of time, or keeping a
    certain amount of variance in the original data. Other techniques plot all the
    singular values, observe an obvious change in the graph, and decide to truncate
    at that location, hopefully separating the essential patterns in the data from
    the noise.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is important not to only center the data, but to also standarize it: Subtract
    the mean of each feature and divide by the standard deviation. The reason is that
    the singular value decomposition is sensitive to the scale of the feature measurements.
    When we standarize the data, then we end up working with the correlation matrix
    intead of the covariance matrix. In order not to confuse ourselves, the main point
    to keep in mind is that we perform the singular value decomposition on the standarized
    data set, then the principal components are the columns of *V*, and the new coordinates
    of the data points are the entries of <math alttext="sigma Subscript i Baseline
    upper U Subscript c o l Sub Subscript i"><mrow><msub><mi>σ</mi> <mi>i</mi></msub>
    <msub><mi>U</mi> <mrow><mi>c</mi><mi>o</mi><msub><mi>l</mi> <mi>i</mi></msub></mrow></msub></mrow></math>
    .'
  prefs: []
  type: TYPE_NORMAL
- en: Principal Component Analysis And Clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We saw in the previous section how we can use principal component analysis to
    reduce the number of features of the data, providing a new set of features in
    hierarchical order in terms of variation in the data. This is incredibly useful
    for visualizing data, since we can only vizualize in two or three dimensions.
    It is important to be able to visualize patters and correlations in high dimensional
    data, for example in genetic data. Sometimes, in the reduced dimensional space
    determined by the principal components, there is an inherent clustering of the
    data by its category. For example, if the dataset contains both patients with
    cancer and patients without cancer, along with their genetic expression (usually
    in the thousands), we might notice that plotting the data in the first three principal
    component space, patients with cancer cluster separately from patients without
    cancer.
  prefs: []
  type: TYPE_NORMAL
- en: A Social Media Application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the same essence of principal component analysis and clustering, a recent
    [publication (Dec 2020)](https://link.springer.com/chapter/10.1007/978-3-030-59177-9_2)
    by Dan Vilenchik presents a wonderful application from social media: An unsupervised
    approach to characterizing users in online social media platforms. Here’s the
    abstract from a talk he gave on the subject along with the abstract from his publication:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Making sense of data that is automatically collected from online platforms
    such as online social media or e-learning platforms is a challenging task: the
    data is massive, multidimensional, noisy, and heterogeneous (composed of differently
    behaving individuals). In this talk we focus on a central task common to all on-line
    social platforms and that is the task of user characterization. For example, automatically
    identify a spammer or a bot on Twitter, or a disengaged student in an e-learning
    platform.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Online social media channels play a central role in our lives. Characterizing
    users in social networks is a long-standing question, dating back to the 50’s
    when Katz and Lazarsfeld studied influence in “Mass Communication”. In the era
    of Machine Learning, this task is typically cast as a supervised learning problem,
    where a target variable is to be predicted: age, gender, political incline, income,
    etc. In this talk explore what can be achieved in an unsupervised manner. Specifically,
    we harness principal component analysis to understand what underlying patterns
    and structures are inherent to some social media platforms, but not to others,
    and why. We arrive at a Simpson-like paradox that may give us a deeper understanding
    of the data-driven process of user characterization is such platforms.*'
  prefs: []
  type: TYPE_NORMAL
- en: The idea of principal component analysis for creating clusters with maximal
    variance in the data will appear multiple times througout this book.
  prefs: []
  type: TYPE_NORMAL
- en: Latent Semantic Analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Latent semantic analysis for natural language data (documents) is similar to
    principal component analysis for numerical data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we want to analyze the relationships between a set of documents and the
    words they contain. The distributional hypothesis for latent semantic analysis
    states that words that have similar meaning occur in similar pieces of text, and
    hence in similar documents. Computers only understand numbers so we have to come
    up with a numerical representation of our word documents before doing any analysis
    on them. One such representation is the *word count* matrix *X*: The columns represent
    unique words (such as apple, orange, dog, city, intelligence, *etc.*) and the
    rows represent each document. Such a matrix is very large but very sparse (has
    many zeros). There are too many words (these are the features) so we need to reduce
    the dimension of the features while preserving the similarity structure among
    the documents (the data points). By now we know what to do: Perform the singular
    value decomposition on the word count matrix, <math alttext="upper X equals upper
    U normal upper Sigma upper V Superscript t"><mrow><mi>X</mi> <mo>=</mo> <mi>U</mi>
    <mi>Σ</mi> <msup><mi>V</mi> <mi>t</mi></msup></mrow></math> , then throw away
    the smaller singular values along with the corresponding columns from *U* and
    rows from <math alttext="upper V Superscript t"><msup><mi>V</mi> <mi>t</mi></msup></math>
    . We can now represent each document in the lower dimensional space (of linear
    combinations of words) in exactly the same way principal component analysis allows
    for data representation in lower dimensional feature space.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have reduced the dimension, we can finally compare the documents using
    *cosine similarity*: Compute the cosine of the angle between the two vectors representing
    the documents. If the cosine is close to 1 then the documents point in the same
    direction in *word space* and hence represent very similar documents. If the cosine
    is close to 0 then the vectors representing the documents are orthogonal to each
    other and hence are very different than each other.'
  prefs: []
  type: TYPE_NORMAL
- en: In its early days, Google search was more like an index, then it evolved to
    accept more natural language searches. The same is true for smart phone autocomplete.
    Latent semantic analysis compresses the meaning of a sentence or a document into
    a vector, and when this is integrated into a search engine, it dramatically improves
    the quality of the engine, retreiving the exact documents we are searching for.
  prefs: []
  type: TYPE_NORMAL
- en: Randomized Singular Value Decomposition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have avoided computing the singular value decomposition,
    on purpose, because it is expensive. We did mention, however, that common algorithms
    use a matrix decomposition called *QR* decomposition (that obtains an orthonormal
    basis for the columns of the data matrix), then *Householder reflections* to transform
    to a bidiagonal matrix, and finally iterative methods to compute the required
    eigenvectors and eigenvalues. Sadly, for the ever growing data sets, the matrices
    involved are too large even for these efficient algorithms. Our only salvation
    is through *randomized linear algebra*. This field provides extremely efficient
    methods for matrix decomposition, relying on *the theory of random sampling*.
    Randomized numerical methods work wonders, providing accurate matrix decompositions
    while at the same time much cheaper than deterministic methods. Randomized singular
    value decomposition samples the column space of the large data matrix *X*, computes
    the *QR* decomposition of the sampled (much smaller) matrix, project *X* onto
    the smaller space ( <math alttext="upper Y equals upper Q Superscript t Baseline
    upper X"><mrow><mi>Y</mi> <mo>=</mo> <msup><mi>Q</mi> <mi>t</mi></msup> <mi>X</mi></mrow></math>
    , so <math alttext="upper X almost-equals upper Q upper Y"><mrow><mi>X</mi> <mo>≈</mo>
    <mi>Q</mi> <mi>Y</mi></mrow></math> ), then compute the singular value decomposition
    of *Y* ( <math alttext="upper Y equals upper U normal upper Sigma upper V Superscript
    t"><mrow><mi>Y</mi> <mo>=</mo> <mi>U</mi> <mi>Σ</mi> <msup><mi>V</mi> <mi>t</mi></msup></mrow></math>
    ). The matrix *Q* is orthonormal and approximates the column space of *X*, so
    the matrices <math alttext="normal upper Sigma"><mi>Σ</mi></math> and *V* are
    the same for *X* and *Y*. To find the *U* for *X*, we can compute it from the
    *U* for Y and *Q* <math alttext="upper U Subscript upper X Baseline equals upper
    Q upper U Subscript upper Y"><mrow><msub><mi>U</mi> <mi>X</mi></msub> <mo>=</mo>
    <mi>Q</mi> <msub><mi>U</mi> <mi>Y</mi></msub></mrow></math> .
  prefs: []
  type: TYPE_NORMAL
- en: Like all randomized methods, they have to be accompanied by error bounds, in
    terms of the expectation of how far off the original matrix *X* is from the sampled
    *QY*. We do have such error bounds but we postpone them to the next chapter dedicated
    to large random matrices.
  prefs: []
  type: TYPE_NORMAL
- en: Summary and Looking Ahead
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The star of the show in this chapter was one formula:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign upper X equals upper U normal upper Sigma upper V
    Superscript t Baseline equals sigma 1 upper U Subscript c o l 1 Baseline upper
    V Subscript r o w 1 Superscript t Baseline plus sigma 2 upper U Subscript c o
    l 2 Baseline upper V Subscript r o w 2 Superscript t Baseline plus ellipsis plus
    sigma Subscript r Baseline upper U Subscript c o l Sub Subscript r Baseline upper
    V Subscript r o w Sub Subscript r Superscript t dollar-sign"><mrow><mi>X</mi>
    <mo>=</mo> <mi>U</mi> <mi>Σ</mi> <msup><mi>V</mi> <mi>t</mi></msup> <mo>=</mo>
    <msub><mi>σ</mi> <mn>1</mn></msub> <msub><mi>U</mi> <mrow><mi>c</mi><mi>o</mi><msub><mi>l</mi>
    <mn>1</mn></msub></mrow></msub> <msubsup><mi>V</mi> <mrow><mi>r</mi><mi>o</mi><msub><mi>w</mi>
    <mn>1</mn></msub></mrow> <mi>t</mi></msubsup> <mo>+</mo> <msub><mi>σ</mi> <mn>2</mn></msub>
    <msub><mi>U</mi> <mrow><mi>c</mi><mi>o</mi><msub><mi>l</mi> <mn>2</mn></msub></mrow></msub>
    <msubsup><mi>V</mi> <mrow><mi>r</mi><mi>o</mi><msub><mi>w</mi> <mn>2</mn></msub></mrow>
    <mi>t</mi></msubsup> <mo>+</mo> <mo>⋯</mo> <mo>+</mo> <msub><mi>σ</mi> <mi>r</mi></msub>
    <msub><mi>U</mi> <mrow><mi>c</mi><mi>o</mi><msub><mi>l</mi> <mi>r</mi></msub></mrow></msub>
    <msubsup><mi>V</mi> <mrow><mi>r</mi><mi>o</mi><msub><mi>w</mi> <mi>r</mi></msub></mrow>
    <mi>t</mi></msubsup></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: This is equivalent to <math alttext="upper X upper V equals upper U normal upper
    Sigma"><mrow><mi>X</mi> <mi>V</mi> <mo>=</mo> <mi>U</mi> <mi>Σ</mi></mrow></math>
    and <math alttext="upper X upper V Subscript c o l Sub Subscript i Baseline equals
    sigma Subscript i Baseline upper U Subscript c o l Sub Subscript i"><mrow><mi>X</mi>
    <msub><mi>V</mi> <mrow><mi>c</mi><mi>o</mi><msub><mi>l</mi> <mi>i</mi></msub></mrow></msub>
    <mo>=</mo> <msub><mi>σ</mi> <mi>i</mi></msub> <msub><mi>U</mi> <mrow><mi>c</mi><mi>o</mi><msub><mi>l</mi>
    <mi>i</mi></msub></mrow></msub></mrow></math> .
  prefs: []
  type: TYPE_NORMAL
- en: The power of the singular value decomposition is that it allows for rank reduction
    without losing essential information. This enables us to compress images, reduce
    the dimension of the feature space of a data set, and compute document similarity
    in natural language processing.
  prefs: []
  type: TYPE_NORMAL
- en: We discussed principal component analysis, latent semantic analysis, and the
    clustering structure inherent in the principal component space. We discussed examples
    where principal component analysis was used as an unsupervised clustering technique
    for ovarian cancer patients according to their gene expression, as well as characterizing
    social media users.
  prefs: []
  type: TYPE_NORMAL
- en: 'We ended the chapter with randomized singular value decomposition, highlighting
    a recurring theme for this book: When things are too large, sample them. *Randomness
    often produces reliability*.'
  prefs: []
  type: TYPE_NORMAL
- en: Readers interested in diving deeper can read about tensor decompositions and
    N-way data arrays, and the importance of data alignment for the singular value
    decomposition to work properly. Readers interested popular examples can read about
    eigenfaces from a modern perspective.
  prefs: []
  type: TYPE_NORMAL
