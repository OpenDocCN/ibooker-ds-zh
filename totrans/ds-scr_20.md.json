["```py\n# A Tensor is either a float, or a List of Tensors\nTensor = Union[float, List[Tensor]]\n```", "```py\n[[1.0, 2.0],\n [3.0]]\n```", "```py\nTensor = list\n```", "```py\nfrom typing import List\n\ndef shape(tensor: Tensor) -> List[int]:\n    sizes: List[int] = []\n    while isinstance(tensor, list):\n        sizes.append(len(tensor))\n        tensor = tensor[0]\n    return sizes\n\nassert shape([1, 2, 3]) == [3]\nassert shape([[1, 2], [3, 4], [5, 6]]) == [3, 2]\n```", "```py\ndef is_1d(tensor: Tensor) -> bool:\n    \"\"\"\n If tensor[0] is a list, it's a higher-order tensor.\n Otherwise, tensor is 1-dimensional (that is, a vector).\n \"\"\"\n    return not isinstance(tensor[0], list)\n\nassert is_1d([1, 2, 3])\nassert not is_1d([[1, 2], [3, 4]])\n```", "```py\ndef tensor_sum(tensor: Tensor) -> float:\n    \"\"\"Sums up all the values in the tensor\"\"\"\n    if is_1d(tensor):\n        return sum(tensor)  # just a list of floats, use Python sum\n    else:\n        return sum(tensor_sum(tensor_i)      # Call tensor_sum on each row\n                   for tensor_i in tensor)   # and sum up those results.\n\nassert tensor_sum([1, 2, 3]) == 6\nassert tensor_sum([[1, 2], [3, 4]]) == 10\n```", "```py\nfrom typing import Callable\n\ndef tensor_apply(f: Callable[[float], float], tensor: Tensor) -> Tensor:\n    \"\"\"Applies f elementwise\"\"\"\n    if is_1d(tensor):\n        return [f(x) for x in tensor]\n    else:\n        return [tensor_apply(f, tensor_i) for tensor_i in tensor]\n\nassert tensor_apply(lambda x: x + 1, [1, 2, 3]) == [2, 3, 4]\nassert tensor_apply(lambda x: 2 * x, [[1, 2], [3, 4]]) == [[2, 4], [6, 8]]\n```", "```py\ndef zeros_like(tensor: Tensor) -> Tensor:\n    return tensor_apply(lambda _: 0.0, tensor)\n\nassert zeros_like([1, 2, 3]) == [0, 0, 0]\nassert zeros_like([[1, 2], [3, 4]]) == [[0, 0], [0, 0]]\n```", "```py\ndef tensor_combine(f: Callable[[float, float], float],\n                   t1: Tensor,\n                   t2: Tensor) -> Tensor:\n    \"\"\"Applies f to corresponding elements of t1 and t2\"\"\"\n    if is_1d(t1):\n        return [f(x, y) for x, y in zip(t1, t2)]\n    else:\n        return [tensor_combine(f, t1_i, t2_i)\n                for t1_i, t2_i in zip(t1, t2)]\n\nimport operator\nassert tensor_combine(operator.add, [1, 2, 3], [4, 5, 6]) == [5, 7, 9]\nassert tensor_combine(operator.mul, [1, 2, 3], [4, 5, 6]) == [4, 10, 18]\n```", "```py\nfrom typing import Iterable, Tuple\n\nclass Layer:\n    \"\"\"\n Our neural networks will be composed of Layers, each of which\n knows how to do some computation on its inputs in the \"forward\"\n direction and propagate gradients in the \"backward\" direction.\n \"\"\"\n    def forward(self, input):\n        \"\"\"\n Note the lack of types. We're not going to be prescriptive\n about what kinds of inputs layers can take and what kinds\n of outputs they can return.\n \"\"\"\n        raise NotImplementedError\n\n    def backward(self, gradient):\n        \"\"\"\n Similarly, we're not going to be prescriptive about what the\n gradient looks like. It's up to you the user to make sure\n that you're doing things sensibly.\n \"\"\"\n        raise NotImplementedError\n\n    def params(self) -> Iterable[Tensor]:\n        \"\"\"\n Returns the parameters of this layer. The default implementation\n returns nothing, so that if you have a layer with no parameters\n you don't have to implement this.\n \"\"\"\n        return ()\n\n    def grads(self) -> Iterable[Tensor]:\n        \"\"\"\n Returns the gradients, in the same order as params().\n \"\"\"\n        return ()\n```", "```py\nfrom scratch.neural_networks import sigmoid\n\nclass Sigmoid(Layer):\n    def forward(self, input: Tensor) -> Tensor:\n        \"\"\"\n Apply sigmoid to each element of the input tensor,\n and save the results to use in backpropagation.\n \"\"\"\n        self.sigmoids = tensor_apply(sigmoid, input)\n        return self.sigmoids\n\n    def backward(self, gradient: Tensor) -> Tensor:\n        return tensor_combine(lambda sig, grad: sig * (1 - sig) * grad,\n                              self.sigmoids,\n                              gradient)\n```", "```py\nimport random\n\nfrom scratch.probability import inverse_normal_cdf\n\ndef random_uniform(*dims: int) -> Tensor:\n    if len(dims) == 1:\n        return [random.random() for _ in range(dims[0])]\n    else:\n        return [random_uniform(*dims[1:]) for _ in range(dims[0])]\n\ndef random_normal(*dims: int,\n                  mean: float = 0.0,\n                  variance: float = 1.0) -> Tensor:\n    if len(dims) == 1:\n        return [mean + variance * inverse_normal_cdf(random.random())\n                for _ in range(dims[0])]\n    else:\n        return [random_normal(*dims[1:], mean=mean, variance=variance)\n                for _ in range(dims[0])]\n\nassert shape(random_uniform(2, 3, 4)) == [2, 3, 4]\nassert shape(random_normal(5, 6, mean=10)) == [5, 6]\n```", "```py\ndef random_tensor(*dims: int, init: str = 'normal') -> Tensor:\n    if init == 'normal':\n        return random_normal(*dims)\n    elif init == 'uniform':\n        return random_uniform(*dims)\n    elif init == 'xavier':\n        variance = len(dims) / sum(dims)\n        return random_normal(*dims, variance=variance)\n    else:\n        raise ValueError(f\"unknown init: {init}\")\n```", "```py\nfrom scratch.linear_algebra import dot\n\nclass Linear(Layer):\n    def __init__(self,\n                 input_dim: int,\n                 output_dim: int,\n                 init: str = 'xavier') -> None:\n        \"\"\"\n A layer of output_dim neurons, each with input_dim weights\n (and a bias).\n \"\"\"\n        self.input_dim = input_dim\n        self.output_dim = output_dim\n\n        # self.w[o] is the weights for the oth neuron\n        self.w = random_tensor(output_dim, input_dim, init=init)\n\n        # self.b[o] is the bias term for the oth neuron\n        self.b = random_tensor(output_dim, init=init)\n```", "```py\n    def forward(self, input: Tensor) -> Tensor:\n        # Save the input to use in the backward pass.\n        self.input = input\n\n        # Return the vector of neuron outputs.\n        return [dot(input, self.w[o]) + self.b[o]\n                for o in range(self.output_dim)]\n```", "```py\n    def backward(self, gradient: Tensor) -> Tensor:\n        # Each b[o] gets added to output[o], which means\n        # the gradient of b is the same as the output gradient.\n        self.b_grad = gradient\n\n        # Each w[o][i] multiplies input[i] and gets added to output[o].\n        # So its gradient is input[i] * gradient[o].\n        self.w_grad = [[self.input[i] * gradient[o]\n                        for i in range(self.input_dim)]\n                       for o in range(self.output_dim)]\n\n        # Each input[i] multiplies every w[o][i] and gets added to every\n        # output[o]. So its gradient is the sum of w[o][i] * gradient[o]\n        # across all the outputs.\n        return [sum(self.w[o][i] * gradient[o] for o in range(self.output_dim))\n                for i in range(self.input_dim)]\n```", "```py\n    def params(self) -> Iterable[Tensor]:\n        return [self.w, self.b]\n\n    def grads(self) -> Iterable[Tensor]:\n        return [self.w_grad, self.b_grad]\n```", "```py\nfrom typing import List\n\nclass Sequential(Layer):\n    \"\"\"\n A layer consisting of a sequence of other layers.\n It's up to you to make sure that the output of each layer\n makes sense as the input to the next layer.\n \"\"\"\n    def __init__(self, layers: List[Layer]) -> None:\n        self.layers = layers\n\n    def forward(self, input):\n        \"\"\"Just forward the input through the layers in order.\"\"\"\n        for layer in self.layers:\n            input = layer.forward(input)\n        return input\n\n    def backward(self, gradient):\n        \"\"\"Just backpropagate the gradient through the layers in reverse.\"\"\"\n        for layer in reversed(self.layers):\n            gradient = layer.backward(gradient)\n        return gradient\n\n    def params(self) -> Iterable[Tensor]:\n        \"\"\"Just return the params from each layer.\"\"\"\n        return (param for layer in self.layers for param in layer.params())\n\n    def grads(self) -> Iterable[Tensor]:\n        \"\"\"Just return the grads from each layer.\"\"\"\n        return (grad for layer in self.layers for grad in layer.grads())\n```", "```py\nxor_net = Sequential([\n    Linear(input_dim=2, output_dim=2),\n    Sigmoid(),\n    Linear(input_dim=2, output_dim=1),\n    Sigmoid()\n])\n```", "```py\nclass Loss:\n    def loss(self, predicted: Tensor, actual: Tensor) -> float:\n        \"\"\"How good are our predictions? (Larger numbers are worse.)\"\"\"\n        raise NotImplementedError\n\n    def gradient(self, predicted: Tensor, actual: Tensor) -> Tensor:\n        \"\"\"How does the loss change as the predictions change?\"\"\"\n        raise NotImplementedError\n```", "```py\nclass SSE(Loss):\n    \"\"\"Loss function that computes the sum of the squared errors.\"\"\"\n    def loss(self, predicted: Tensor, actual: Tensor) -> float:\n        # Compute the tensor of squared differences\n        squared_errors = tensor_combine(\n            lambda predicted, actual: (predicted - actual) ** 2,\n            predicted,\n            actual)\n\n        # And just add them up\n        return tensor_sum(squared_errors)\n\n    def gradient(self, predicted: Tensor, actual: Tensor) -> Tensor:\n        return tensor_combine(\n            lambda predicted, actual: 2 * (predicted - actual),\n            predicted,\n            actual)\n```", "```py\ntheta = gradient_step(theta, grad, -learning_rate)\n```", "```py\nclass Optimizer:\n    \"\"\"\n An optimizer updates the weights of a layer (in place) using information\n known by either the layer or the optimizer (or by both).\n \"\"\"\n    def step(self, layer: Layer) -> None:\n        raise NotImplementedError\n```", "```py\nclass GradientDescent(Optimizer):\n    def __init__(self, learning_rate: float = 0.1) -> None:\n        self.lr = learning_rate\n\n    def step(self, layer: Layer) -> None:\n        for param, grad in zip(layer.params(), layer.grads()):\n            # Update param using a gradient step\n            param[:] = tensor_combine(\n                lambda param, grad: param - grad * self.lr,\n                param,\n                grad)\n```", "```py\ntensor = [[1, 2], [3, 4]]\n\nfor row in tensor:\n    row = [0, 0]\nassert tensor == [[1, 2], [3, 4]], \"assignment doesn't update a list\"\n\nfor row in tensor:\n    row[:] = [0, 0]\nassert tensor == [[0, 0], [0, 0]], \"but slice assignment does\"\n```", "```py\nclass Momentum(Optimizer):\n    def __init__(self,\n                 learning_rate: float,\n                 momentum: float = 0.9) -> None:\n        self.lr = learning_rate\n        self.mo = momentum\n        self.updates: List[Tensor] = []  # running average\n\n    def step(self, layer: Layer) -> None:\n        # If we have no previous updates, start with all zeros\n        if not self.updates:\n            self.updates = [zeros_like(grad) for grad in layer.grads()]\n\n        for update, param, grad in zip(self.updates,\n                                       layer.params(),\n                                       layer.grads()):\n            # Apply momentum\n            update[:] = tensor_combine(\n                lambda u, g: self.mo * u + (1 - self.mo) * g,\n                update,\n                grad)\n\n            # Then take a gradient step\n            param[:] = tensor_combine(\n                lambda p, u: p - self.lr * u,\n                param,\n                update)\n```", "```py\n# training data\nxs = [[0., 0], [0., 1], [1., 0], [1., 1]]\nys = [[0.], [1.], [1.], [0.]]\n```", "```py\nrandom.seed(0)\n\nnet = Sequential([\n    Linear(input_dim=2, output_dim=2),\n    Sigmoid(),\n    Linear(input_dim=2, output_dim=1)\n])\n```", "```py\nimport tqdm\n\noptimizer = GradientDescent(learning_rate=0.1)\nloss = SSE()\n\nwith tqdm.trange(3000) as t:\n    for epoch in t:\n        epoch_loss = 0.0\n\n        for x, y in zip(xs, ys):\n            predicted = net.forward(x)\n            epoch_loss += loss.loss(predicted, y)\n            gradient = loss.gradient(predicted, y)\n            net.backward(gradient)\n\n            optimizer.step(net)\n\n        t.set_description(f\"xor loss {epoch_loss:.3f}\")\n```", "```py\nfor param in net.params():\n    print(param)\n```", "```py\nhidden1 = -2.6 * x1 + -2.7 * x2 + 0.2  # NOR\nhidden2 =  2.1 * x1 +  2.1 * x2 - 3.4  # AND\noutput =  -3.1 * h1 + -2.6 * h2 + 1.8  # NOR\n```", "```py\nimport math\n\ndef tanh(x: float) -> float:\n    # If x is very large or very small, tanh is (essentially) 1 or -1.\n    # We check for this because, e.g., math.exp(1000) raises an error.\n    if x < -100:  return -1\n    elif x > 100: return 1\n\n    em2x = math.exp(-2 * x)\n    return (1 - em2x) / (1 + em2x)\n\nclass Tanh(Layer):\n    def forward(self, input: Tensor) -> Tensor:\n        # Save tanh output to use in backward pass.\n        self.tanh = tensor_apply(tanh, input)\n        return self.tanh\n\n    def backward(self, gradient: Tensor) -> Tensor:\n        return tensor_combine(\n            lambda tanh, grad: (1 - tanh ** 2) * grad,\n            self.tanh,\n            gradient)\n```", "```py\nclass Relu(Layer):\n    def forward(self, input: Tensor) -> Tensor:\n        self.input = input\n        return tensor_apply(lambda x: max(x, 0), input)\n\n    def backward(self, gradient: Tensor) -> Tensor:\n        return tensor_combine(lambda x, grad: grad if x > 0 else 0,\n                              self.input,\n                              gradient)\n```", "```py\nfrom scratch.neural_networks import binary_encode, fizz_buzz_encode, argmax\n\nxs = [binary_encode(n) for n in range(101, 1024)]\nys = [fizz_buzz_encode(n) for n in range(101, 1024)]\n```", "```py\nNUM_HIDDEN = 25\n\nrandom.seed(0)\n\nnet = Sequential([\n    Linear(input_dim=10, output_dim=NUM_HIDDEN, init='uniform'),\n    Tanh(),\n    Linear(input_dim=NUM_HIDDEN, output_dim=4, init='uniform'),\n    Sigmoid()\n])\n```", "```py\ndef fizzbuzz_accuracy(low: int, hi: int, net: Layer) -> float:\n    num_correct = 0\n    for n in range(low, hi):\n        x = binary_encode(n)\n        predicted = argmax(net.forward(x))\n        actual = argmax(fizz_buzz_encode(n))\n        if predicted == actual:\n            num_correct += 1\n\n    return num_correct / (hi - low)\n```", "```py\noptimizer = Momentum(learning_rate=0.1, momentum=0.9)\nloss = SSE()\n\nwith tqdm.trange(1000) as t:\n    for epoch in t:\n        epoch_loss = 0.0\n\n        for x, y in zip(xs, ys):\n            predicted = net.forward(x)\n            epoch_loss += loss.loss(predicted, y)\n            gradient = loss.gradient(predicted, y)\n            net.backward(gradient)\n\n            optimizer.step(net)\n\n        accuracy = fizzbuzz_accuracy(101, 1024, net)\n        t.set_description(f\"fb loss: {epoch_loss:.2f} acc: {accuracy:.2f}\")\n\n# Now check results on the test set\nprint(\"test results\", fizzbuzz_accuracy(1, 101, net))\n```", "```py\ndef softmax(tensor: Tensor) -> Tensor:\n    \"\"\"Softmax along the last dimension\"\"\"\n    if is_1d(tensor):\n        # Subtract largest value for numerical stability.\n        largest = max(tensor)\n        exps = [math.exp(x - largest) for x in tensor]\n\n        sum_of_exps = sum(exps)                 # This is the total \"weight.\"\n        return [exp_i / sum_of_exps             # Probability is the fraction\n                for exp_i in exps]              # of the total weight.\n    else:\n        return [softmax(tensor_i) for tensor_i in tensor]\n```", "```py\nclass SoftmaxCrossEntropy(Loss):\n    \"\"\"\n This is the negative-log-likelihood of the observed values, given the\n neural net model. So if we choose weights to minimize it, our model will\n be maximizing the likelihood of the observed data.\n \"\"\"\n    def loss(self, predicted: Tensor, actual: Tensor) -> float:\n        # Apply softmax to get probabilities\n        probabilities = softmax(predicted)\n\n        # This will be log p_i for the actual class i and 0 for the other\n        # classes. We add a tiny amount to p to avoid taking log(0).\n        likelihoods = tensor_combine(lambda p, act: math.log(p + 1e-30) * act,\n                                     probabilities,\n                                     actual)\n\n        # And then we just sum up the negatives.\n        return -tensor_sum(likelihoods)\n\n    def gradient(self, predicted: Tensor, actual: Tensor) -> Tensor:\n        probabilities = softmax(predicted)\n\n        # Isn't this a pleasant equation?\n        return tensor_combine(lambda p, actual: p - actual,\n                              probabilities,\n                              actual)\n```", "```py\nrandom.seed(0)\n\nnet = Sequential([\n    Linear(input_dim=10, output_dim=NUM_HIDDEN, init='uniform'),\n    Tanh(),\n    Linear(input_dim=NUM_HIDDEN, output_dim=4, init='uniform')\n    # No final sigmoid layer now\n])\n\noptimizer = Momentum(learning_rate=0.1, momentum=0.9)\nloss = SoftmaxCrossEntropy()\n\nwith tqdm.trange(100) as t:\n    for epoch in t:\n        epoch_loss = 0.0\n\n        for x, y in zip(xs, ys):\n            predicted = net.forward(x)\n            epoch_loss += loss.loss(predicted, y)\n            gradient = loss.gradient(predicted, y)\n            net.backward(gradient)\n\n            optimizer.step(net)\n\n        accuracy = fizzbuzz_accuracy(101, 1024, net)\n        t.set_description(f\"fb loss: {epoch_loss:.3f} acc: {accuracy:.2f}\")\n\n# Again check results on the test set\nprint(\"test results\", fizzbuzz_accuracy(1, 101, net))\n```", "```py\nclass Dropout(Layer):\n    def __init__(self, p: float) -> None:\n        self.p = p\n        self.train = True\n\n    def forward(self, input: Tensor) -> Tensor:\n        if self.train:\n            # Create a mask of 0s and 1s shaped like the input\n            # using the specified probability.\n            self.mask = tensor_apply(\n                lambda _: 0 if random.random() < self.p else 1,\n                input)\n            # Multiply by the mask to dropout inputs.\n            return tensor_combine(operator.mul, input, self.mask)\n        else:\n            # During evaluation just scale down the outputs uniformly.\n            return tensor_apply(lambda x: x * (1 - self.p), input)\n\n    def backward(self, gradient: Tensor) -> Tensor:\n        if self.train:\n            # Only propagate the gradients where mask == 1.\n            return tensor_combine(operator.mul, gradient, self.mask)\n        else:\n            raise RuntimeError(\"don't call backward when not in train mode\")\n```", "```py\npython -m pip install mnist\n```", "```py\nimport mnist\n\n# This will download the data; change this to where you want it.\n# (Yes, it's a 0-argument function, that's what the library expects.)\n# (Yes, I'm assigning a lambda to a variable, like I said never to do.)\nmnist.temporary_dir = lambda: '/tmp'\n\n# Each of these functions first downloads the data and returns a numpy array.\n# We call .tolist() because our \"tensors\" are just lists.\ntrain_images = mnist.train_images().tolist()\ntrain_labels = mnist.train_labels().tolist()\n\nassert shape(train_images) == [60000, 28, 28]\nassert shape(train_labels) == [60000]\n```", "```py\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(10, 10)\n\nfor i in range(10):\n    for j in range(10):\n        # Plot each image in black and white and hide the axes.\n        ax[i][j].imshow(train_images[10 * i + j], cmap='Greys')\n        ax[i][j].xaxis.set_visible(False)\n        ax[i][j].yaxis.set_visible(False)\n\nplt.show()\n```", "```py\ntest_images = mnist.test_images().tolist()\ntest_labels = mnist.test_labels().tolist()\n\nassert shape(test_images) == [10000, 28, 28]\nassert shape(test_labels) == [10000]\n```", "```py\n# Compute the average pixel value\navg = tensor_sum(train_images) / 60000 / 28 / 28\n\n# Recenter, rescale, and flatten\ntrain_images = [[(pixel - avg) / 256 for row in image for pixel in row]\n                for image in train_images]\ntest_images = [[(pixel - avg) / 256 for row in image for pixel in row]\n               for image in test_images]\n\nassert shape(train_images) == [60000, 784], \"images should be flattened\"\nassert shape(test_images) == [10000, 784], \"images should be flattened\"\n\n# After centering, average pixel should be very close to 0\nassert -0.0001 < tensor_sum(train_images) < 0.0001\n```", "```py\ndef one_hot_encode(i: int, num_labels: int = 10) -> List[float]:\n    return [1.0 if j == i else 0.0 for j in range(num_labels)]\n\nassert one_hot_encode(3) == [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\nassert one_hot_encode(2, num_labels=5) == [0, 0, 1, 0, 0]\n```", "```py\ntrain_labels = [one_hot_encode(label) for label in train_labels]\ntest_labels = [one_hot_encode(label) for label in test_labels]\n\nassert shape(train_labels) == [60000, 10]\nassert shape(test_labels) == [10000, 10]\n```", "```py\nimport tqdm\n\ndef loop(model: Layer,\n         images: List[Tensor],\n         labels: List[Tensor],\n         loss: Loss,\n         optimizer: Optimizer = None) -> None:\n    correct = 0         # Track number of correct predictions.\n    total_loss = 0.0    # Track total loss.\n\n    with tqdm.trange(len(images)) as t:\n        for i in t:\n            predicted = model.forward(images[i])             # Predict.\n            if argmax(predicted) == argmax(labels[i]):       # Check for\n                correct += 1                                 # correctness.\n            total_loss += loss.loss(predicted, labels[i])    # Compute loss.\n\n            # If we're training, backpropagate gradient and update weights.\n            if optimizer is not None:\n                gradient = loss.gradient(predicted, labels[i])\n                model.backward(gradient)\n                optimizer.step(model)\n\n            # And update our metrics in the progress bar.\n            avg_loss = total_loss / (i + 1)\n            acc = correct / (i + 1)\n            t.set_description(f\"mnist loss: {avg_loss:.3f} acc: {acc:.3f}\")\n```", "```py\nrandom.seed(0)\n\n# Logistic regression is just a linear layer followed by softmax\nmodel = Linear(784, 10)\nloss = SoftmaxCrossEntropy()\n\n# This optimizer seems to work\noptimizer = Momentum(learning_rate=0.01, momentum=0.99)\n\n# Train on the training data\nloop(model, train_images, train_labels, loss, optimizer)\n\n# Test on the test data (no optimizer means just evaluate)\nloop(model, test_images, test_labels, loss)\n```", "```py\nrandom.seed(0)\n\n# Name them so we can turn train on and off\ndropout1 = Dropout(0.1)\ndropout2 = Dropout(0.1)\n\nmodel = Sequential([\n    Linear(784, 30),  # Hidden layer 1: size 30\n    dropout1,\n    Tanh(),\n    Linear(30, 10),   # Hidden layer 2: size 10\n    dropout2,\n    Tanh(),\n    Linear(10, 10)    # Output layer: size 10\n])\n```", "```py\noptimizer = Momentum(learning_rate=0.01, momentum=0.99)\nloss = SoftmaxCrossEntropy()\n\n# Enable dropout and train (takes > 20 minutes on my laptop!)\ndropout1.train = dropout2.train = True\nloop(model, train_images, train_labels, loss, optimizer)\n\n# Disable dropout and evaluate\ndropout1.train = dropout2.train = False\nloop(model, test_images, test_labels, loss)\n```", "```py\nimport json\n\ndef save_weights(model: Layer, filename: str) -> None:\n    weights = list(model.params())\n    with open(filename, 'w') as f:\n        json.dump(weights, f)\n```", "```py\ndef load_weights(model: Layer, filename: str) -> None:\n    with open(filename) as f:\n        weights = json.load(f)\n\n    # Check for consistency\n    assert all(shape(param) == shape(weight)\n               for param, weight in zip(model.params(), weights))\n\n    # Then load using slice assignment\n    for param, weight in zip(model.params(), weights):\n        param[:] = weight\n```"]