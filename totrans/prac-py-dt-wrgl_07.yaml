- en: Chapter 7\. Cleaning, Transforming, and Augmenting Data
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第7章。清理、转换和增强数据
- en: Most of the time, the data that we initially find, collect, or acquire doesn’t
    quite suit our needs in one way or another. The format is awkward, the data structure
    is wrong, or its units need to be adjusted. The data itself might contain errors,
    inconsistencies, or gaps. It may contain references we don’t understand or hint
    at additional possibilities that aren’t realized. Whatever the limitation may
    be, in our quest to use data as a source of insight, it is inevitable that we
    will have to clean, transform, and/or augment it in some way in order to get the
    most out of it.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数情况下，我们最初找到、收集或获取的数据在某种方式上都不完全符合我们的需求。格式不合适，数据结构错误，或者其单位需要调整。数据本身可能包含错误、不一致或间断。它可能包含我们不理解的引用，或者暗示着尚未实现的额外可能性。无论限制是什么，在我们使用数据作为洞察力的源头的过程中，不可避免地我们将不得不以某种方式对其进行清理、转换和/或增强，以使其发挥最大的作用。
- en: Up until now, we have put off most of this work because we had more urgent problems
    to solve. In [Chapter 4](ch04.html#chapter4), our focus was on getting data out
    of a tricky file format and into something more accessible; in [Chapter 6](ch06.html#chapter6),
    our priority was thoroughly assessing the quality of our data, so we could make
    an informed decision about whether it was worth the investment of augmentation
    and analysis at all.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们推迟了大部分这项工作，因为我们有更紧急的问题需要解决。在[第4章](ch04.html#chapter4)中，我们的重点是将数据从一个棘手的文件格式中提取出来，并转换为更易访问的形式；在[第6章](ch06.html#chapter6)中，我们的重点是彻底评估我们数据的质量，以便我们可以就是否值得进行增强和分析做出明智的决定。
- en: 'Now, however, it’s time to roll up our sleeves and begin what to me is sort
    of the second phase of data wrangling and quality work: preparing the data we
    have for the analysis we want to perform. Our data is in the table-type format
    we need, and we’ve determined that it’s of high enough quality to yield *some*
    useful insights—even if they are not precisely the ones we first imagined.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，然而，是时候卷起袖子，开始对我来说是数据整理和质量工作的第二阶段：准备我们拥有的数据，以进行我们想要执行的分析。我们的数据处于我们需要的表格类型格式，并且我们已经确定它的质量足够高，可以产生*一些*有用的见解，即使它们不完全是我们最初想象的那些。
- en: Since it’s obviously impossible to identify and address every possible problem
    or technique related to cleaning, transforming, and/or augmenting data, my approach
    here will be to work through the actual examples we’ve already encountered where
    one or more of these tasks is required. For example, we’ll look at different ways
    we might need to transform date-type information using datasets we encountered
    in Chapters [2](ch02.html#chapter2) and [4](ch04.html#chapter4). We’ll also look
    at different ways we can clean up the “cruft” in data files that contain both
    structured data *and* metadata. We’ll even explore *regular expressions*, which
    offer us a powerful way to select only certain parts of a data field or match
    particular terms and patterns irrespective of capitalization and/or punctuation.
    In the process, we’ll manage to cover a decent range of the tools and strategies
    you’re likely to need when cleaning and transforming most datasets. At the very
    least, the approaches outlined in this chapter will give you a useful starting
    place if you encounter a challenge that’s truly gnarly or unique.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 由于显然不可能识别和解决与清理、转换和/或增强数据相关的每一个可能的问题或技术，因此我的方法是通过处理我们已经遇到过的实际示例来进行工作，在这些任务中至少需要一个或多个。例如，我们将看看我们可能需要使用在第[2](ch02.html#chapter2)章和第[4](ch04.html#chapter4)章中遇到的数据集来转换日期类型信息的不同方法。我们还将探讨如何清理同时包含结构化数据*和*元数据的数据文件中的“无用杂质”。我们甚至会探索*正则表达式*，这为我们提供了一种强大的方式，可以选择数据字段的某些部分或匹配特定的术语和模式，无视大小写和/或标点符号。在此过程中，我们将设法涵盖您在清理和转换大多数数据集时可能需要的工具和策略的相当范围。至少，在本章中概述的方法将为您提供一个有用的起点，如果您遇到真正棘手或独特的挑战。
- en: Selecting a Subset of Citi Bike Data
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择 Citi Bike 数据的子集
- en: Way back in [“Hitting the Road with Citi Bike Data”](ch02.html#hitting_the_road_intro),
    we used Citi Bike system data to test out some of our freshly unboxed Python concepts,
    like `for...in` loops and `if/else` conditionals. For the sake of convenience,
    we started with a sample dataset that I had excerpted from [the September 2020
    system data file](https://s3.amazonaws.com/tripdata/index.html).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: 'There are any number of situations where we’ll want to segment large datasets
    for analysis—either because we don’t have the time or computational resources
    to process everything at once or because we’re only interested in a subset of
    the dataset to begin with. If all we wanted to do was select a specific number
    of rows, we could write a `for...in` loop using the `range()` function described
    in [“Adding Iterators: The range Function”](ch04.html#add_iterators). But we might
    also want to excerpt the data based on its values as well. I did this in selecting
    all of the rides from September 1, 2020, but we might also want to do something
    a bit more nuanced, like evaluating weekday Citi Bike rides separately from those
    taken on weekends and holidays.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start with the first task, of excerpting just the September 1, 2020,
    rides from the larger dataset.^([1](ch07.html#idm45143404308272)) Conceptually,
    this is simple enough: we just want to keep every row in our dataset containing
    a ride that started on the first day of September. If we briefly revisit the dataset,
    however, it becomes clear that even this task is not so simple.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: '![Zoomed-in view of the first few lines of Citi Bike trip data.](assets/ppdw_0701.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
- en: Figure 7-1\. First few lines of Citi Bike trip data
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'As you can see in [Figure 7-1](#citibike_sample_screengrab), the `starttime`
    column is not simply a date but some kind of date/time format that includes not
    just the month, day, and year but also the hours, minutes, and seconds (to four
    decimal points!). The first entry in this data file, for example, the value of
    the `starttime`, looks like this:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Obviously, if we want to analyze just the first day of rides—or just rides during
    the morning “rush hour” commute or just weekday rides—we need a way to effectively
    filter our data based on just *part* of the information that’s stored in this
    column. But what options do we have for accomplishing this? In the next few sections,
    we’ll look at each of these tasks—finding just rides on a particular date, in
    a particular time frame, and on a particular “type” of day—in turn. In the process,
    we’ll learn some of the tools Python offers for solving problems like these, as
    well as when and why we might choose one over another.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: A Simple Split
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Solving the first problem—excerpting just the rides that started on September
    1, 2020—is actually relatively easy to do if we combine some of the tools that
    we’ve used already in some previous examples. It starts with recognizing that
    when we read in a basic CSV file with Python, most of our data will be treated
    as strings.^([2](ch07.html#idm45143404288880)) This means that, even though we
    humans clearly know that `2020-09-01 00:00:01.0430` is meant to be *interpreted*
    as a date and time, Python just sees it as a collection of numbers and characters.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: Looking at the `starttime` field this way, the question of how to find all the
    rides that started on September 1, 2020, becomes a bit more straightforward, because
    the part of our data that contains the “date” information is *always* separated
    from the “time” information by a single space. This means that if we can find
    a way to look only at what comes *before* that space, we can easily set up an
    `if/else` conditional to compare that to our target date string—in this case,
    `2020-09-01`—and use that comparison to keep only the rows we want.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: While it may not seem glamorous, the built-in string `split()` is going to be
    our hero here. It’s already played a supporting role in previous exercises when
    we needed to break up filenames or URLs; we actually used it way back in [“Verbs
    ≈ Functions”](ch02.html#verbs_are_functions) to illustrate the difference between
    functions and methods! As a refresher, this method lets us specify a single character
    that should be used to split a string into parts. The output of this method is
    a list, which contains the “leftover” pieces of the string in the order in which
    they appeared, with the character you `split()` on removed. So splitting the string
    `2020-09-01 00:00:01.0430` on a space will yield the list `['2020-09-01', '00:00:01.0430']`.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: To see how simple and effective this is, let’s modify our script from [“Hitting
    the Road with Citi Bike Data”](ch02.html#hitting_the_road_intro). In [Example 7-1](#citibike_september1_rides),
    I’ve edited down some of the comments because these tasks are much more familiar
    now, but it’s still a good idea to outline your script’s objective at the top.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: Example 7-1\. citibike_september1_rides.py
  id: totrans-19
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Pretty simple, right? Of course, you could easily modify this script to capture
    a different date, or even multiple dates if you wanted to. For example, you could
    modify the `if` statement to be something like:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Of course, while this `or` statement works perfectly well if you’re looking
    for two or three specific dates, it starts to get *very* messy if you need to
    look for more than that (you may recall that we ended up with a similarly awkward
    conditional in [Example 6-12](ch06.html#ppp_loan_uses)). In order to filter our
    data with the precision we need without generating extraordinarily complex, awkward,
    and error-prone code, we’ll be better served by a whole different toolkit: *regular
    expressions*.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: 'Regular Expressions: Supercharged String Matching'
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A *regular expression* (often shortened to *regex*), allows you to quickly and
    efficiently search for string patterns within a larger string or piece of text.
    In most cases, if you’re trying to solve a matching or filtering problem and find
    that the solution involves *lots* of `and` or `or` statements, it’s an early sign
    that what you really need is a regular expression.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: Regular expressions are found in most programming languages and are concise,
    powerful, and, at times, *extremely* tricky to work with. While a single regular
    expression can encapsulate even a very complex search pattern, designing regexes
    that work as expected can be extremely time-consuming, often requiring quite a
    bit of trial and error to get right. Since our goal is for our data wrangling
    work to be both efficient *and* comprehensible, we’ll focus here on short regexes
    that offer unique functionality not easily achieved through other means. While
    there are certain tasks where regular expressions are indispensable, they are
    not the tool for solving every problem and usually work best when paired with
    other techniques.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: 'To get started, let’s use a regular expression to tackle the problem of filtering
    out rides that take place within the typical “morning commute” hours, which we’ll
    estimate here as being from 7 a.m. to 9 a.m. Any regex process begins with distinguishing
    for ourselves what we *want* to match from what we *don’t*. Here, we’ll start
    with an example `starttime` entry that is *outside* of our identified time range
    (hence, something we *don’t* want to match):'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now let’s look at a `starttime` entry that falls *within* it:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now, let’s acknowledge first that we *could* address this problem with the
    string-splitting method we saw previously. We could start by splitting on the
    `:` character, which would, in the second instance, give us this:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Then we could take the middle item from the list and use a *compound conditional*—that
    is, an `if` statement that joins two or more tests—to see if it matches the strings
    `'07'`, `'08'`, or `'09'`.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: This approach certainly *works*, but it feels a little awkward. It requires
    multiple steps and a three-part conditional that will quickly get difficult to
    read. A regular expression, meanwhile, will let us narrow in on those hour values
    in a single step while still being fairly readable. Before we dive into writing
    the regex itself, though, let’s do a quick overview of the vocabulary of Python
    regular expressions.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: Because a regular expression has to use characters and strings to describe *patterns*
    of characters and strings, the Python regular expression “language” uses a set
    of *metacharacters* and special sequences to make describing the pattern you’re
    searching for simpler. In [Table 7-1](#regex_sequence_list) I’ve included some
    of the most useful ones, drawn from a more complete list [on W3Schools](https://w3schools.com/python/python_regex.asp).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: Table 7-1\. Common regular expression building blocks
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: '| Expression | Description |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
- en: '| [] | A set of characters |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
- en: '| “\” | Signals a special sequence (can also be used to escape special characters)
    |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
- en: '| . | Any character (except newline character) |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
- en: '| * | Zero or more occurrences |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
- en: '| + | One or more occurrences |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
- en: '| {} | Exactly the specified number of occurrences |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
- en: '| &#124; | Either or |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
- en: '| () | Capture and group |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
- en: '| \d | Returns a match where the string contains digits (numbers from 0–9)
    |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
- en: '| \D | Returns a match where the string DOES NOT contain digits |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
- en: '| \s | Returns a match where the string contains a whitespace character |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
- en: '| \S | Returns a match where the string DOES NOT contain a whitespace character
    |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
- en: '| \w | Returns a match where the string contains any word characters (characters
    from a to Z, digits from 0–9, and the underscore _ character) |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
- en: '| \W | Returns a match where the string DOES NOT contain any word characters
    |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
- en: As always with writing, regular expressions give us more than one way to “capture”
    the pattern we’re looking for. In most cases, our goal is to define a pattern
    that will match what we need to find while avoiding *accidentally* matching on
    anything else. For our “rush hour” problem, we can take advantage of the fact
    that the “hours” digits in the `starttime` column are surrounded by colons (`:`),
    *and nothing else is*. This means that we can use this “surrounded by colons”
    pattern as part of our regular expression and feel confident that we won’t accidentally
    match some other part of the string. To see if this works as we hope, let’s set
    up a few sample regular expressions to test against some (real and constructed)
    sample data to see how they do, as shown in [Example 7-2](#regex_tests).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: Example 7-2\. regex_tests.py
  id: totrans-54
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[![1](assets/1.png)](#co_cleaning__transforming____span_class__keep_together__and_augmenting_data__span__CO1-1)'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: In addition to sample files like this, you can also test out your Python regex
    online using the [W3Schools regex demo](https://w3schools.com/python/trypython.asp?filename=demo_regex).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_cleaning__transforming____span_class__keep_together__and_augmenting_data__span__CO1-2)'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: Even if you only use them once in a script, I *strongly* recommend defining
    your regex at the top of your file using an aptly named variable. It is the simplest,
    most efficient way to keep track of their functionality, especially if you’re
    using more than one!
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: 'When you run the script in [Example 7-2](#regex_tests), your output should
    look something like this:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: As you can see, the “bookended” regex, where we specified both of the colons,
    correctly matches (*and* fails to match) in all three cases; the “one-sided” regex,
    on the other hand, erroneously finds a match on the *seconds* value of `sample3`.
    This is precisely why defining the string you’re looking for as precisely as possible
    is important. If you look at the `Match object` printed out previously, you’ll
    see that it contains information about what was matched (e.g., `match='07:'`)
    and where (e.g., from index positions 14–17 in the string).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: 'So far, this seems pretty straightforward. Things can still get a little tricky,
    however, when the *structure* of the thing we want to match changes. For example,
    what if we wanted to expand the hours we’re interested in to range from 7 a.m.
    to 10 a.m.? Our `bookend_regex` won’t work as written, because it specifies that
    the first character after the colon has to be a `0`. We could try just adding
    the digits `1` and `0` as options to our digit ranges, like so:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'which produces the output:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The problem, as we can see from the output, is that our data uses a 24-hour
    clock and will end up matching on a whole range of times that we don’t want. That’s
    because regular expressions don’t “see” numbers in the way we think of them—all
    they see are sequences of characters. That’s why `18` comes back as a match—our
    regex allows any string that starts with a `0` or a `1` and is followed by a `0`,
    `7`, `8`, or `9`. While we obviously wrote it with the numbers `07`, `08`, `09`,
    and `10` in mind, our code opens the door to many more.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: The solution, in this case, is to use the “either/or” *pipe* character (`|`),
    which we can use to combine to (otherwise) completely distinct regular expressions.
    In this case, that will look something like what’s shown in [Example 7-3](#seven_to_ten).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: Example 7-3\. Capturing 7 to 10
  id: totrans-69
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Try it out yourself with a few sample data points, just to confirm that it captures
    what we’re looking for (and nothing else).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: 'I’m not going to go too much further with regular expressions than this; as
    with the web scraping we explored in [“Web Scraping: The Data Source of Last Resort”](ch05.html#web_scraping),
    no two regular expression problems (or solutions) are alike. However, I hope you
    can see the potential these offer for doing pattern matching that would be very
    awkward with compound conditionals and basic string functions alone.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: Making a Date
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the reasons it’s appealing to treat date-like data as strings is that,
    as we saw in our work with various source formats of unemployment data in [Chapter 4](ch04.html#chapter4),
    the way they are interpreted can vary dramatically across data sources and even
    Python libraries. Still, there are situations and tasks where converting date-like
    data to an actual `datetime` type is very useful. For example, if we want to isolate
    the weekday rides from our Citi Bike data, we *could* try to essentially “brute
    force” it by looking at a calendar, identifying the dates of all the weekdays,
    and then creating a giant string comparison list or writing a regular expression
    to match them. In the case of the September 2020 data, such a regular expression
    object might look something what’s in [Example 7-4](#weekday_regex).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: Example 7-4\. Weekday regex for September 2020
  id: totrans-75
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Ugh. This certainly *works*, but it’s nearly impossible to read and is still
    basically one giant compound conditional—even if it’s captured in fewer characters
    because it’s a regular expression. Moreover, it’s not a solution that scales very
    well. If we wanted to extend our analysis to any *other* month, it would mean
    getting out the calendar all over again.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 唉。这当然*有效*，但几乎不可能阅读，并且仍然基本上是一个巨大的复合条件——即使它因为是正则表达式而被捕获的字符较少。此外，这并不是一个很好扩展的解决方案。如果我们想将分析扩展到*其他*月份，那意味着得重新查看日历。
- en: Fortunately, a well-constructed Python `datetime` object has a number of built-in
    methods that can help with exactly this kind of task. In fact there is a simple
    `weekday()` method that returns a number from 0 to 6 ([with 0 being Monday and
    6 being Sunday](https://docs.python.org/3/library/datetime.html#datetime.date.weekday))
    based on the day of the week on which a certain date falls. This means that if
    we convert the contents of our `starttime` column to a date, as shown in [Example 7-5](#weekday_rides),
    we can use this method to quickly identify the day of the week corresponding to
    *any* date. This will help us apply our code to additional data sources—say, a
    different month or year of ridership data—without having to do a thing!
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，精心构建的 Python `datetime` 对象具有多个内置方法，可以帮助处理这类任务。实际上，有一个简单的 `weekday()` 方法，根据某个日期所在星期的日期返回从
    0 到 6 的数字（[0 表示星期一，6 表示星期日](https://docs.python.org/3/library/datetime.html#datetime.date.weekday)）。这意味着，如果我们将
    `starttime` 列的内容转换为日期格式，就像在[示例 7-5](#weekday_rides)中显示的那样，我们可以使用这个方法快速识别对应于*任何*日期的星期几。这将帮助我们将代码应用于其他数据源——例如不同月份或年份的乘客数据——而无需进行任何操作！
- en: Example 7-5\. weekday_rides.py
  id: totrans-79
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 7-5\. weekday_rides.py
- en: '[PRE12]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[![1](assets/1.png)](#co_cleaning__transforming____span_class__keep_together__and_augmenting_data__span__CO2-1)'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_cleaning__transforming____span_class__keep_together__and_augmenting_data__span__CO2-1)'
- en: As mentioned in [Example 6-1](ch06.html#ppp_date_range), providing [the format
    of our source data](https://docs.python.org/3/library/datetime.html#strftime-strptime-behavior)
    will help our script run faster and more reliably.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 如[示例 6-1](ch06.html#ppp_date_range)所述，提供[源数据的格式](https://docs.python.org/3/library/datetime.html#strftime-strptime-behavior)将有助于我们的脚本运行更快、更可靠。
- en: '[![2](assets/2.png)](#co_cleaning__transforming____span_class__keep_together__and_augmenting_data__span__CO2-2)'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_cleaning__transforming____span_class__keep_together__and_augmenting_data__span__CO2-2)'
- en: The `weekday()` method puts [Monday at position `0`](https://docs.python.org/3/library/datetime.html#datetime.date.weekday),
    so looking for anything up to and including `4` will capture the values for Monday
    through Friday.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '`weekday()` 方法将[星期一置于位置`0`](https://docs.python.org/3/library/datetime.html#datetime.date.weekday)，因此查找包括`4`在内的任何内容将捕获星期一到星期五的值。'
- en: Depending on your device, you may notice that the script in [Example 7-5](#weekday_rides)
    takes a while to run. For example, on my (not very powerful) device, it takes
    more than 85 seconds to complete. Accomplishing the same task using the regular
    expression in [Example 7-4](#weekday_regex), meanwhile, takes only 45 seconds.
    I can also more easily tweak the regular expression to skip days that are officially
    weekdays but are also holidays (like Labor Day).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 根据您的设备，您可能会注意到[示例 7-5](#weekday_rides)中的脚本运行时间较长。例如，在我的（性能不是很强的）设备上，完成所需时间超过
    85 秒。而在[示例 7-4](#weekday_regex)中使用正则表达式完成同样的任务只需 45 秒。我还可以更轻松地调整正则表达式，以跳过官方工作日但同时也是假期的日子（如劳动节）。
- en: So which approach is better? As usual, *it depends*. What will actually work
    best for your particular data wrangling/cleaning/transformation process will be
    specific to your needs and your resources. If answering your question means looking
    for weekday commute patterns in a decade’s worth of Citi Bike data, you’re probably
    better off using the `weekday()` method, because you don’t have to change your
    code to deal with different months or years. On the other hand, if you don’t have
    very many months to work with and execution speed (and absolute precision) is
    your top concern, you might prefer to go the regular expression route. You may
    also just find that regexes make you want to tear your hair out or that using
    multiple steps to get perfect results drives you crazy. As we’ll explore more
    in [Chapter 8](ch08.html#chapter8), all of these can be legitimate reasons for
    a particular design choice—just make sure the choice is *yours*.^([3](ch07.html#idm45143403643280))
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 那么哪种方法更好呢？像往常一样，*这要看情况*。对于你特定的数据整理/清理/转换过程，什么才是最有效的方法将取决于你的需求和资源。如果你要在十年的Citi
    Bike数据中寻找工作日通勤模式，你可能最好使用`weekday()`方法，因为你无需修改代码以处理不同的月份或年份。另一方面，如果你没有太多的月份可供使用，并且执行速度（以及绝对精确度）是你的首要关注点，你可能更喜欢正则表达式的方法。你可能也会发现正则表达式让你想抓狂，或者使用多个步骤以获得完美结果让你发疯。正如我们将在[第8章](ch08.html#chapter8)中进一步探讨的那样，所有这些都可以成为特定设计选择的合理理由——只要确保选择是*你的*选择。
- en: De-crufting Data Files
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据文件的清理
- en: In [Chapter 4](ch04.html#chapter4), we encountered a number of instances where
    we needed to “clean up” a dataset that was otherwise awkward or unintelligible.
    When we went through the process of parsing an old-school-style *.xls* file in
    [Example 4-6](ch04.html#xls_parsing), for example, we encountered a couple of
    distinct issues. First, the spreadsheet contained both table-type data *and* descriptive
    header information that, despite being useful in principle, will inevitably need
    to be relocated in order for us to analyze the rest of it. Second, the *.xls*
    format’s lack of support for “real” dates means that our initial transformation
    from *.xls* to *.csv* left us with a bunch of nonsense numbers where the dates
    should have been. While I chose to put off solving those problems at first, the
    time has come to confront them.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第4章](ch04.html#chapter4)中，我们遇到了许多需要“清理”的数据集实例，否则这些数据集将显得笨拙或难以理解。例如，在我们处理老式
    *.xls* 文件的过程中（参见[示例4-6](ch04.html#xls_parsing)），我们遇到了几个明显的问题。首先，电子表格中包含了表格类型的数据以及描述性的标题信息，尽管原则上很有用，但为了分析其余部分，这些信息显然需要重新安排位置。其次，*.xls*
    格式不支持“真实”的日期，这导致我们从 *.xls* 转换到 *.csv* 的初步转换结果中，日期应有的地方出现了一堆无意义的数字。虽然一开始我选择先搁置这些问题，但现在是解决它们的时候了。
- en: In thinking about the first problem, I want to stress that we *definitely* don’t
    want to just “throw out” the information that’s currently stored at the top of
    the *fredgraph.xls* file. As is hopefully clear from our work in [Chapter 6](ch06.html#chapter6),
    metadata is a precious resource, and we *never* want to discard metadata from
    a [primary source](https://loc.gov/programs/teachers/getting-started-with-primary-sources).
    Rather, my preference in cases like this is to turn one file into two. We’ll strip
    out the metadata and store it in a separate—but congruently named—text file while
    also parsing and saving the table-type data into an analysis-friendly *.csv* format.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在思考第一个问题时，我想强调的是，我们*绝对不*希望简单地“丢弃”当前存储在 *fredgraph.xls* 文件顶部的信息。正如我们在[第6章](ch06.html#chapter6)的工作中所明显的那样，元数据是一种宝贵的资源，我们*永远*不希望从[主要来源](https://loc.gov/programs/teachers/getting-started-with-primary-sources)中丢弃元数据。相反，在这种情况下，我更倾向于将一个文件拆分为两个文件。我们将剥离出元数据，并将其存储在一个独立但同名的文本文件中，同时解析并将表格类型的数据保存为便于分析的
    *.csv* 格式。
- en: 'Looking at our source *.xls* file in a spreadsheet program, it’s easy enough
    to see visually where the metadata ends and the table-type data begins. The real
    question is: how will we detect this transition in our script? As is so often
    the case with data cleaning, the most effective solution is not always an elegant
    one. The metadata ends where the table-type data begins, in the row containing
    its column headers. If we look at the first value in each row as we work our way
    through the file, we can *stop* writing to the metadata file and *start* writing
    to the *.csv* as soon as we encounter the first column header. Since the value
    `observation_date` is the first column header for this dataset, we’ll make that
    transition as soon we find that at the beginning of our current row.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在电子表格程序中查看我们的源*.xls*文件时，可以很容易地直观地看到元数据何时结束并且表格类型的数据从何处开始。真正的问题是：我们在脚本中如何检测这种转换？正如数据清洗经常出现的情况一样，最有效的解决方案并不总是优雅的。元数据在其包含列标题的行中结束，而表格类型的数据开始。如果我们在处理文件时查看每行的第一个值，我们可以在遇到第一个列标题时停止向元数据文件写入并开始向*.csv*写入。由于值`observation_date`是此数据集的第一个列标题，所以我们会在找到它出现在当前行开头时进行这一转换。
- en: Tip
  id: totrans-91
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: Before you begin, check your source file carefully to see *where* the metadata
    appears within it. Especially in cases where the data contains estimates or other
    qualifiers, you’re likely to find metadata both before and *after* the table-type
    data in your source file.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始之前，仔细检查您的源文件，查看元数据在其中的位置。特别是在数据包含估计值或其他限定词的情况下，您可能会发现元数据既在表格类型数据之前，也在其*之后*。
- en: To see what’s involved in creating these two purpose-built files from our single
    source file, take a look at the script in [Example 7-6](#xls_meta_parsing) (if
    you need a refresher on some of the code choices in this example, you may want
    to refer back to [Example 4-6](ch04.html#xls_parsing)).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解如何从我们的单个源文件创建这两个特定用途的文件，请查看[示例 7-6](#xls_meta_parsing)中的脚本（如果您需要复习本示例中某些代码选择，请参考[示例 4-6](ch04.html#xls_parsing)）。
- en: Example 7-6\. xls_meta_parsing.py
  id: totrans-94
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 7-6\. xls_meta_parsing.py
- en: '[PRE13]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[![1](assets/1.png)](#co_cleaning__transforming____span_class__keep_together__and_augmenting_data__span__CO3-1)'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_cleaning__transforming____span_class__keep_together__and_augmenting_data__span__CO3-1)'
- en: While we’re only creating a single metadata file here, we could easily move
    this part of the process inside the `for` loop and create a unique metadata file
    for every worksheet, if necessary.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们在这里只创建一个元数据文件，但如果需要，我们可以轻松将这一过程的这部分移至`for`循环内，并为每个工作表创建一个唯一的元数据文件。
- en: '[![2](assets/2.png)](#co_cleaning__transforming____span_class__keep_together__and_augmenting_data__span__CO3-2)'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_cleaning__transforming____span_class__keep_together__and_augmenting_data__span__CO3-2)'
- en: This type of Boolean (True/False) variable is often described as a *flag* variable.
    The idea is that we set its value *outside* of a loop and then “flip” its value
    when some particular thing has happened—this saves us from having to loop through
    all our data twice. Here, we’ll use it to check when we should start writing to
    our “data” file instead of our “metadata” file.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这种布尔（True/False）变量通常被描述为*标志*变量。其思想是我们在循环之外设置其值，然后在发生特定事件时“翻转”其值——这样我们就不必两次循环遍历所有数据。在这里，我们将使用它来检查何时应该开始向我们的“数据”文件而不是我们的“元数据”文件写入。
- en: 'Before we move on to dealing with the (still inexplicable) dates in this file,
    I want to highlight a new technique introduced in [Example 7-6](#xls_meta_parsing):
    the use of a so-called *flag variable*. This term typically refers to any Boolean
    (True/False) variable that is used to keep track of whether a certain event has
    taken place or condition has been met, especially within a loop. In [Example 7-6](#xls_meta_parsing),
    for example, we are using the `is_table_data` variable as a way to keep track
    of whether we have yet encountered the row of data that marks the beginning of
    our table data. Since a given row of data in our `for...in` loop is essentially
    “forgotten” as soon as the next one is read, we need to create this variable *before*
    our loop. This keeps the `is_table_data` variable available beyond the *scope*
    of our loop—a concept we’ll look at more closely in [Chapter 8](ch08.html#chapter8).'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续处理这个（仍然难以理解的）文件中的日期之前，我想强调在 [示例 7-6](#xls_meta_parsing) 中引入的一种新技术：所谓的 *标志变量*
    的使用。这个术语通常指的是用于跟踪是否发生了某个事件或满足了某个条件的布尔（True/False）变量，特别是在循环内部。例如，在 [示例 7-6](#xls_meta_parsing)
    中，我们使用 `is_table_data` 变量来跟踪是否已经遇到标记我们表格数据开始的数据行。由于我们 `for...in` 循环中的给定数据行在下一个数据行读取后基本上被“遗忘”，因此我们需要在循环之前创建这个变量。这样可以使
    `is_table_data` 变量在我们循环的 *范围* 之外保持可用——这是我们将在 [第 8 章](ch08.html#chapter8) 中更详细地看到的概念。
- en: Decrypting Excel Dates
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解密 Excel 日期
- en: We can avoid the issue of those Excel dates no longer. While hopefully you will
    not encounter this situation often, I’m including it here for completeness and
    because it illustrates a few different ways that code tends to evolve—usually
    getting more complicated and less readable—as we add even seemingly small bits
    of functionality to it. For example, in [Example 7-7](#xls_meta_and_date_parsing),
    we’ll need to check whether a variable contains a number or not, and believe it
    or not, we need a library for this—it is aptly called *numbers*. While that part
    is fundamentally straightforward, you’ll quickly see in [Example 7-7](#xls_meta_and_date_parsing)
    how the need to transform these date values requires adapting our approach to
    writing the table-type data to our output file.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以避免那些 Excel 日期不再的问题。尽管希望你不会经常遇到这种情况，但我在这里包含它是为了完整性，并且因为它展示了代码通常会如何演变——通常在我们添加甚至看似小的功能时变得更加复杂和难以阅读。例如，在
    [示例 7-7](#xls_meta_and_date_parsing) 中，我们需要检查变量是否包含数字，信不信由你，我们需要一个名为 *numbers*
    的库来做到这一点。尽管这部分基本上很简单，但你很快会在 [示例 7-7](#xls_meta_and_date_parsing) 中看到，如何需要转换这些日期值需要调整我们写入输出文件的表格类型数据的方法。
- en: Example 7-7\. xls_meta_and_date_parsing.py
  id: totrans-103
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 7-7\. xls_meta_and_date_parsing.py
- en: '[PRE14]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[![1](assets/1.png)](#co_cleaning__transforming____span_class__keep_together__and_augmenting_data__span__CO4-1)'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_cleaning__transforming____span_class__keep_together__and_augmenting_data__span__CO4-1)'
- en: Converting these *.xls* “dates” using the *xlrd* library’s `xldate_as_datetime()`
    method requires both the number value *and* [the workbook’s `datemode`](https://xlrd.readthedocs.io/en/latest/api.html#xlrd.book.Book.datemode)
    in order to generate the Python datetime object correctly.^([4](ch07.html#idm45143403085296))
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 *xlrd* 库的 `xldate_as_datetime()` 方法转换这些 *.xls* “日期”需要同时使用数字值 *和* [工作簿的 `datemode`](https://xlrd.readthedocs.io/en/latest/api.html#xlrd.book.Book.datemode)，以便正确生成
    Python datetime 对象。^([4](ch07.html#idm45143403085296))
- en: '[![2](assets/2.png)](#co_cleaning__transforming____span_class__keep_together__and_augmenting_data__span__CO4-2)'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_cleaning__transforming____span_class__keep_together__and_augmenting_data__span__CO4-2)'
- en: Here, I’ve decided to write the date to my table-type data file as `MM/DD/YYYY`
    using the appropriate `strftime()` format, but you could use another format if
    you prefer.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我决定使用适当的 `strftime()` 格式将日期写入我的表格类型数据文件，格式为 `MM/DD/YYYY`，但如果你喜欢，也可以使用其他格式。
- en: 'While the *xlrd* library makes the process of converting our strange Excel
    dates to something understandable relatively straightforward, I think the code
    in [Example 7-7](#xls_meta_and_date_parsing) demonstrates how the idiosyncrasies
    of wrangling a particular dataset can quickly add complexity—especially in the
    form of additional, nested `if` statements—to what started as a very simple program.
    This is just one of the reasons why we’ll spend [Chapter 8](ch08.html#chapter8)
    exploring strategies and techniques for effectively and efficiently streamlining
    our code: we want to make sure it does everything we need but *also* that it is
    sufficiently readable and reusable to stand the test of time.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: Generating True CSVs from Fixed-Width Data
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Another instance where we also had only moderate success in transforming our
    data was in [Example 4-7](ch04.html#fixed_width_parsing), where we converted our
    fixed-width source data into a *.csv*. While technically we succeeded in creating
    an output file that was comma separated, the result was honestly pretty unsatisfactory:
    it retained many of the formatting artifacts of the original file that could easily
    stymie our future efforts at data analysis.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: 'Fortunately, the specific problem we encountered—that of “leading” and/or “trailing”
    whitespace—is very well-known, since the data technologies that typically generate
    it have been around for a long time. As a result, fixing this problem is pretty
    simple: the solution exists in the form of the built-in Python `strip()` function,
    as illustrated in [Example 7-8](#fixed_width_strip_parsing).'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: Example 7-8\. fixed_width_strip_parsing.py
  id: totrans-113
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[![1](assets/1.png)](#co_cleaning__transforming____span_class__keep_together__and_augmenting_data__span__CO5-1)'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: If you compare this code with the code in [Example 4-7](ch04.html#fixed_width_parsing),
    you’ll see that it’s identical apart from our having applied the `strip()` method
    to each string before appending it to our data row.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: Once again, we see that modifying our original code to solve a formatting or
    “cleaning” issue isn’t necessarily that hard, but the resulting script isn’t *exactly*
    elegant, either. Stripping the whitespace from our output with the `strip()` method
    is definitely straightforward, but we’ve had to add a whole lot of parentheses
    in the process—leaving us with code that is far less readable than we’d like.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: This illustrates yet another way in which creating good, quality Python code
    mirrors more typical writing processes. If we view our first efforts in [Chapter 4](ch04.html#chapter4)
    as something like the “outline” of our final program—where we solve the high-level
    problem of getting the data format we have into at least the table-type *structure*
    that we’re after—it gives us space to come back later and revise that work, filling
    in the details that allow it to be more nuanced in its handling of the specific
    dataset we’re working with.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, in [Chapter 8](ch08.html#chapter8) we’ll take that revision process
    one step further, refining these programs—which already do everything we need
    them to do—so that they are more concise and easier to understand, just as we
    might do with any piece of writing. This iterative approach to programming not
    only means that we eventually end up with better, more useful code; it also helps
    us break down big, complicated programming problems into a series of less intimidating
    ones that we can solve one step at a time. Equally important, no matter what stage
    of the process we’re in, we have a functioning program we can fall back on if
    needed. This incremental approach is especially useful when we take on more complex
    data-cleaning tasks like the one we’ll look at next: resolving the unintentional
    spelling differences.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，在 [第8章](ch08.html#chapter8) 中，我们将进一步推进这个修订过程，优化这些程序——这些程序已经做到我们需要的一切——以便它们更加简明和易于理解，就像我们对待任何一篇写作一样。这种迭代式的编程方法不仅意味着我们最终得到了更好、更有用的代码；它还帮助我们将复杂的编程问题分解成一系列较少令人畏惧的步骤，我们可以逐步解决。同样重要的是，无论我们处于过程的哪个阶段，我们都有一个可以依赖的功能程序。这种逐步的方法在我们接下来要处理的更复杂的数据清洗任务中特别有用：解决意外的拼写差异。
- en: Correcting for Spelling Inconsistencies
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 纠正拼写不一致
- en: In [Chapter 6](ch06.html#chapter6), we used a “fingerprinting” process to help
    address the possibility that information about banks in our Paycheck Protection
    Program (PPP) data might have spelling inconsistencies—a common issue in *any*
    dataset that relies on human data entry. Of course, the code we wrote in [Example 6-11](ch06.html#ppp_lender_names)
    only estimated the number of genuinely unique entries in the `OriginatingLender`
    column by counting how many of them resulted in a distinct fingerprint. We found
    that our dataset contained 4,337 unique bank names but only 4,242 unique fingerprints—an
    indicator that as many as 95 bank names might actually *be* the same but contain
    typos because they generated the same fingerprint.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [第6章](ch06.html#chapter6) 中，我们使用了“指纹识别”过程来帮助解决我们的支票保护计划（PPP）数据中可能存在的银行信息拼写不一致的可能性——这是任何依赖人工数据输入的数据集中常见的问题。当然，在我们编写的
    [示例6-11](ch06.html#ppp_lender_names) 中，只是通过计算产生不同指纹的条目数量来估计“OriginatingLender”列中真正唯一条目的数量。我们发现我们的数据集包含了4,337个唯一的银行名称，但只有4,242个唯一的指纹——这表明多达95个银行名称实际上
    *是* 相同的，但包含拼写错误，因为它们生成了相同的指纹。
- en: Because those 95 potential typos could affect thousands of rows of data, we
    need a way to transform our dataset so that we can confidently aggregate it by
    lender. At the same time, we also don’t want to *overcorrect* by grouping together
    entries that don’t actually belong together.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 因为这95个潜在的拼写错误可能会影响成千上万行的数据，我们需要一种方法来转换我们的数据集，以便我们可以放心地按贷方机构聚合它。与此同时，我们也不希望 *过度纠正*，将本不相符的条目分组在一起。
- en: 'This is an instance where *transforming* our data is invaluable: we don’t want
    to risk losing any of our original data (retaining it is essential for validation
    and spot-checking), but we *also* need to transform it in order to support our
    future analysis efforts. Because our dataset is large, grouping and filtering
    it to meet our needs is likely to be time-consuming, so we want to preserve the
    results of that work by actually *adding* new columns to the dataset. This lets
    us both preserve our original data *and* the benefits of our transformation work
    in a single file.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个实例，*转换*我们的数据非常宝贵：我们不希望冒失丢失任何原始数据（保留它对验证和抽查至关重要），但我们 *也* 需要转换它以支持我们未来的分析工作。由于我们的数据集很大，分组和过滤以满足我们的需求可能会耗费大量时间，因此我们希望通过实际
    *添加* 新列到数据集中来保留该工作的结果。这样一来，我们既能保留我们的原始数据 *也* 能在单个文件中保留转换工作的好处。
- en: Fortunately, we’ve got some go-to libraries that will make this process pretty
    straightforward. Since we already know how to aggregate names using the fingerprinting
    process from [Example 6-11](ch06.html#ppp_lender_names), the trickier piece may
    be determining when banks that share the same fingerprint should actually be treated
    as distinct organizations. Looking back at the output from [Example 6-7](ch06.html#ppp_columns_review)
    (reproduced in [Example 7-9](#transposed_sample_recent_again) for convenience),
    we see that there are not a lot of fields that contain “originating” lender information,
    so our most likely option for deciding if two originating banks whose names share
    all the same words (and will therefore have the same fingerprint, e.g., “First
    Bank Texas” and “Texas First Bank”) will be to compare the value included in `OriginatingLenderLocationID`.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，我们已经有一些常用库可以使这个过程相当简单。由于我们已经知道如何使用指纹过程聚合名称（来自 [示例 6-11](ch06.html#ppp_lender_names)），更棘手的部分可能是确定具有相同指纹的银行是否应当被视为不同的组织。回顾
    [示例 6-7](ch06.html#ppp_columns_review) 的输出（方便起见在 [示例 7-9](#transposed_sample_recent_again)
    中再现），我们看到并不是很多字段包含“originating”贷款人信息，因此，我们最有可能的选择是通过比较包含在 `OriginatingLenderLocationID`
    中的值来决定两个共享所有相同单词的起始银行是否应当被视为不同的组织，例如，“First Bank Texas” 和 “Texas First Bank”。
- en: Example 7-9\. Recent sample data transposed
  id: totrans-125
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 7-9。最近的样本数据转置
- en: '[PRE16]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Before we proceed, of course, we want to make sure that we understand what the
    data in `OriginatingLenderLocationID` actually *means*. Lucky for us, a web search
    for the words “originating lender location id” brings up [yet another document
    from the SBA website](https://sba.gov/sites/default/files/articles/ETran_Origination_01_2014.pdf)
    as the first result. Searching through this PDF for the term “location” brings
    us to the page shown in [Figure 7-2](#ppp_lender_location_info), which reassures
    us that the “Location ID” value entered should not change from branch to branch
    of the same bank but indicates the *main* branch of a given bank.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续之前，当然，我们要确保理解 `OriginatingLenderLocationID` 中的数据实际上*表示*什么。幸运的是，当我们搜索“originating
    lender location id”时，第一个结果是来自 SBA 网站的[另一份文档](https://sba.gov/sites/default/files/articles/ETran_Origination_01_2014.pdf)。通过这个
    PDF 搜索术语“location”，我们来到了 [图 7-2](#ppp_lender_location_info) 所示的页面，这使我们放心，“Location
    ID” 的值输入不应该因为同一银行的不同分支而改变，而是表示一个给定银行的*主*分支。
- en: '![Information about lender location ID](assets/ppdw_0702.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![贷款人位置 ID 的信息](assets/ppdw_0702.png)'
- en: Figure 7-2\. Information about lender location ID
  id: totrans-129
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-2。关于贷款人位置 ID 的信息
- en: With this additional information, we can go about creating a version of our
    PPP loan data that includes a new column, `OriginatingLenderFingerprint`, that
    contains a combination of the `OriginatingLender` fingerprint and the `OriginatingLenderLocationID`,
    as shown in [Example 7-10](#ppp_add_fingerprints). Later on, we can then use this
    value to quickly aggregate our data by originating lender while being (reasonably)
    confident that we are neither failing to match entries due to typos *nor* treating
    what should be two separate banks as one.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 借助这些额外的信息，我们可以创建一个包含新列 `OriginatingLenderFingerprint` 的 PPP 贷款数据版本，其中包含 `OriginatingLender`
    的指纹和 `OriginatingLenderLocationID` 的组合，如 [示例 7-10](#ppp_add_fingerprints) 所示。稍后，我们可以利用这个值快速地按原始贷款人聚合我们的数据，同时（合理地）确信我们既不因拼写错误而未能匹配条目，*也不*将本应为两家独立银行的机构视为一家。
- en: Example 7-10\. ppp_add_fingerprints.py
  id: totrans-131
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 7-10。ppp_add_fingerprints.py
- en: '[PRE17]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[![1](assets/1.png)](#co_cleaning__transforming____span_class__keep_together__and_augmenting_data__span__CO6-1)'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_cleaning__transforming____span_class__keep_together__and_augmenting_data__span__CO6-1)'
- en: While it may seem excessive, this first loop actually exists *only* to create
    our new header row. As always, we want to avoid introducing typos whenever and
    wherever possible, so in this instance, the whole extra loop is worth it (*way*
    better than typing out this list by hand).
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这看起来可能有些多余，但这第一个循环实际上**只存在**于创建我们的新标题行。与往常一样，我们希望尽可能地避免引入拼写错误，因此在这种情况下，整个额外的循环是值得的（比手动打出这个列表要*好*多了）。
- en: '[![2](assets/2.png)](#co_cleaning__transforming____span_class__keep_together__and_augmenting_data__span__CO6-2)'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_cleaning__transforming____span_class__keep_together__and_augmenting_data__span__CO6-2)'
- en: Since we’re adding a column of data, we need to build the new data row as a
    list, item by item—just as we did with the header row.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们正在添加一列数据，所以我们需要像处理标题行一样，逐项构建新的数据行。
- en: The structure of the resulting file is the same as the original, except that
    a new `OriginatingLenderFingerprint` column has now been added between `OriginatingLender`
    and `OriginatingLenderCity`, as you can see in [Example 7-11](#ppp_data_w_fingerprints).
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 结果文件的结构与原始文件相同，只是在`OriginatingLender`和`OriginatingLenderCity`之间添加了一个新的`OriginatingLenderFingerprint`列，如你可以在[Example 7-11](#ppp_data_w_fingerprints)中看到的那样。
- en: Example 7-11\. PPP data with fingerprints
  id: totrans-138
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Example 7-11\. 带指纹的PPP数据
- en: '[PRE18]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: While this transformation will help us easily aggregate our data by a particular
    “originating” lender, we could quickly duplicate it with the “servicing” lender
    as well. We could *even* write a script that compares the value of these two resulting
    fingerprints to create a “flag” column indicating whether the servicing and originating
    banks for a particular loan are the same.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这种转换将帮助我们通过特定的“发起方”借方轻松聚合数据，但我们也可以迅速地用“服务方”借方进行复制。我们甚至可以编写一个脚本，比较这两个结果指纹的值，创建一个“标志”列，指示特定贷款的服务方和发起方银行是否相同。
- en: The Circuitous Path to “Simple” Solutions
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通向“简单”解决方案的曲折路径
- en: While I hope that you found the [Example 7-10](#ppp_add_fingerprints) exercise
    simple to follow, I want you know that the preceding script was *not* the first
    solution I tried—it wasn’t even the second or third. In fact, I probably spent
    about a dozen hours, all told, thinking, hacking, wrangling, and failing before
    I *finally* realized that my eventual approach to the problem was the fastest,
    simplest, and most effective way to strike a balance between making sure that
    loans from the same bank were grouped together without accidentally conflating
    two different institutions.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望你觉得[Example 7-10](#ppp_add_fingerprints)的练习很简单，但我想告诉你，前面的脚本并不是我尝试的第一个解决方案——甚至不是第二或第三个。事实上，事实上，在我*最终*意识到我的最终方法是在确保来自同一家银行的贷款被分组在一起的同时，不会意外地混淆两个不同机构之间，我可能花了大约十几个小时，思考，尝试，处理失败。
- en: I’m going to describe how I actually worked my way through this process because—as
    is hopefully *also* starting to become clear—data wrangling (and programming in
    general) is not so much about coding as it is about reasoning and problem-solving.
    This means that thinking through the problem in front of you, trying different
    solutions, and, perhaps most importantly, being willing to change course even
    if it feels like “throwing out” a bunch of work are *all* much more important
    to data wrangling than being able to write more than two lines of Python code
    from memory.^([5](ch07.html#idm45143403010368)) So in an effort to illustrate
    what just one of these problem-solving efforts entails, I’m going to (comparatively)
    briefly give you an overview here of the different approaches I tried before settling
    on the solution in [“Correcting for Spelling Inconsistencies”](#correcting_inconsistencies).
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我将描述我实际通过这个过程工作的方式，因为——希望*也*变得清楚——数据整理（以及编程一般来说）不仅仅是编码，更多的是推理和问题解决。这意味着要仔细思考面前的问题，尝试不同的解决方案，而且，也许最重要的是，即使感觉像是“扔掉”了一大堆工作，也要愿意改变方向，这些对于数据整理比能够记住两行
    Python 代码更加重要^([5](ch07.html#idm45143403010368))。因此，为了说明这些问题解决努力的一个方面，我将（相对而言）简要地在这里为你概述我尝试的不同方法，然后确定了在[“纠正拼写不一致性”](#correcting_inconsistencies)中的解决方案。
- en: 'At first, I started out by minimally adapting the script from [Example 6-11](ch06.html#ppp_lender_names),
    creating a new column that contained *just* those fingerprints and writing a new
    CSV that added this new column. But I realized that there was a strong likelihood
    that some banks with similar names would share the same “fingerprint,” so I wrote
    a script that did the following:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 起初，我从[Example 6-11](ch06.html#ppp_lender_names)最小地调整脚本开始，创建了一个只包含那些指纹的新列，并写了一个新的
    CSV，添加了这个新列。但我意识到，一些名字相似的银行很可能会共享相同的“指纹”，所以我编写了一个脚本，做了以下几件事：
- en: Created a list of the unique fingerprints.
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建了一个唯一指纹的列表。
- en: For every unique fingerprint, created a new list (actually a `pandas` DataFrame)
    of all the unique `OriginatingLenderLocationID` values.
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个唯一的指纹，创建了一个新的列表（实际上是一个`pandas` DataFrame），包含所有唯一的`OriginatingLenderLocationID`值。
- en: If there was more than one distinct `OriginatingLenderLocationID` value, I then
    *updated* the “fingerprint” column to incorporate the `OriginatingLenderLocationID`,
    much as we ended up doing for *all* the entries in [Example 7-10](#ppp_add_fingerprints).
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果有多个不同的`OriginatingLenderLocationID`值，我就*更新*“指纹”列以整合`OriginatingLenderLocationID`，就像我们最终为[示例 7-10](#ppp_add_fingerprints)中的*所有*条目所做的那样。
- en: 'Even creating *that* script, however, was much more involved than this numbered
    synopsis would make it seem. The first step was easy, of course—we’d pretty much
    done that already. But when it came time to working with the new file in `pandas`,
    my scrappy little Chromebook didn’t have enough memory, so I moved my work to
    Google Colab. This gave me more memory to work with (sort of), but now every time
    I stepped away for more than a few minutes, I had to authenticate and reload the
    data from my Google Drive file all over again—that took an additional couple of
    minutes every time. Also, while I was *pretty* confident that I had figured out
    how to update the values in my DataFrame correctly, attempting to check my work
    by searching for a new fingerprint that I was sure should exist wasn’t working
    reliably: sometimes I got matches, and sometimes I got an empty DataFrame! Add
    to this that it took about 3 or more minutes to run step 3 each time, and you
    can imagine how many hours (and how much frustration!) it took to be sure my code
    actually worked as intended.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，即使创建*那*个脚本，比这份编号概要看起来更复杂得多。第一步当然很容易 —— 我们几乎已经做过了。但当我开始在`pandas`中使用新文件时，我的小笔记本电脑记忆不足，所以我把工作移到了Google
    Colab。这给了我更多的内存来工作（某种程度上），但现在每次我离开几分钟以上，我都必须重新验证并从我的Google Drive文件中重新加载数据 —— 每次都要额外花几分钟。此外，虽然我*相当*有信心已经正确地更新了我的DataFrame中的值，但尝试通过搜索我确信应该存在的新指纹来检查我的工作并不可靠：有时我会得到匹配，有时我得到一个空的DataFrame！再加上每次运行步骤
    3 大约需要 3 分钟或更多的时间，你可以想象我花了多少小时（以及多么沮丧！）才确信我的代码实际上按预期工作。
- en: 'Of course, once I had managed to code up (and check) that multistep solution,
    I realized that the result wasn’t all that different from what I’d started with.
    In fact, it was a little *less* satisfying because now the format of my new `OriginatingLenderFingerprint`
    column was inconsistent: some had the `OriginatingLenderLocationID` appended,
    some didn’t. But since the actual *value* of the fingerprint didn’t matter—only
    that it could be used accurately to both aggregate and disambiguate banks—why
    was I going to all the trouble of only adding location IDs to the ones that had
    several entries? Couldn’t they *all* just have the location IDs appended?'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，一旦我成功编码（并检查）了这个多步解决方案，我意识到结果与我最初开始时并没有太大的不同。事实上，有点*不*令人满意，因为现在我的新`OriginatingLenderFingerprint`列的格式不一致：有些附加了`OriginatingLenderLocationID`，有些则没有。但由于指纹的实际*值*并不重要
    —— 只需能够准确地用于汇总和消除歧义的意图 —— 我为什么要费事仅将位置 ID 添加到那些有多个条目的指纹？它们*全部*都可以附加位置 ID 吗？
- en: 'Well, of course it was only at *that* point that I bothered to look up the
    documentation shown in [Figure 7-2](#ppp_lender_location_info), which confirmed
    that adding location IDs wouldn’t break up fingerprints that should be the same.^([6](ch07.html#idm45143402992928))
    And that’s how I came full circle: rather than assigning potentially overlapping
    fingerprints and then trying to “weed out” the problems with an awkward and time-consuming
    search process, the best solution was just to make the `OriginatingLenderLocationID`
    part of the new “fingerprint” column right from the start.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，直到*那*一点，我才费事查看了[图 7-2](#ppp_lender_location_info)中显示的文档，确认添加位置 ID 不会破坏应该相同的指纹。^([6](ch07.html#idm45143402992928))
    这就是我全面回到原点的方式：与其分配可能重叠的指纹，然后试图通过一种笨拙而耗时的搜索过程“清理”问题，还不如从一开始就将`OriginatingLenderLocationID`作为新的“指纹”列的一部分。
- en: Having spent hours working out how to “fix” the original fingerprints—and in
    the process, contending with the limits of my device, the vagaries of Google Colab,
    and the tedium of making a small change to a script and then having to wait several
    minutes for it to run—I won’t pretend that it didn’t feel like a bit of a letdown
    to realize that the best solution really just involved a small tweak on my original
    script (though not the one I had started with).
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在花费数小时解决如何“修复”原始指纹——并在此过程中，应对我的设备限制，Google Colab的变数，以及修改脚本后等待几分钟才能运行的乏味过程中——我不会假装没有感到失望，意识到最佳解决方案实际上只涉及对原始脚本进行小调整（虽然不是我最初开始的那个）。
- en: But if there’s one thing that I’ve learned after years of data wrangling, it’s
    that learning when to let go and start over (or go back to the beginning) is one
    of the most important skills you can develop. Sometimes you have to let go of
    a dataset, even if you’ve sunk hours into researching, evaluating, and cleaning
    it. Likewise, sometimes you have to let go of a programming approach, even if
    you’ve spent hours reading documentation and experimenting with new methods just
    to get the result you’re after. Because in the end, the goal is *not* to use a
    particular dataset, or to use a particular library or coding method. *It’s to
    use data to understand something about the world.* And if you can keep your focus
    on that, letting go when you need to will be much easier.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，经过多年的数据整理，如果有一件事我学到的，那就是学会何时放手并重新开始（或回到起点）是你可以培养的最重要的技能之一。有时你必须放弃一个数据集，即使你已经花费了大量时间研究、评估和清理它。同样，有时你必须放弃一个编程方法，即使你已经花了几个小时阅读文档和尝试新方法，只是为了得到你想要的结果。因为最终的目标*不是*使用特定的数据集，或者使用特定的库或编码方法。*而是使用数据来理解世界的某些方面。*如果你能保持这个焦点，那么在需要时放手将会更容易。
- en: You will also probably find it easier to accept this process—whether it involves
    letting go of a dataset or a scripting solution you’ve already spent hours on—when
    you start to experience firsthand that you have learned something valuable even
    from something you eventually “abandon.” Before my detour into “fixing” my original,
    text-only fingerprints, for example, I didn’t really know how to update values
    within a `pandas` DataFrame; now I do (I *really* do). I also now know a bit more
    about Google Colab’s strengths and inconsistencies and was reminded about some
    key “gotchas” to working with diverse datasets (more on that in the next section).
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会发现，在你开始亲身体验之后，接受这个过程变得更容易——无论它涉及放弃一个数据集还是一个你已经花了几个小时去处理的脚本解决方案。例如，在我专注于“修复”原始的纯文本指纹之前，我真的不知道如何在`pandas`
    DataFrame中更新数值；现在我知道了（我*真的*知道）。我现在也对Google Colab的优势和不一致性有了更多了解，并且想起了与处理多样化数据集相关的一些关键“坑”（在下一节中会详述）。
- en: 'The same goes for datasets that might not turn out to be usable for answering
    a particular question: just because they aren’t right for your current project
    doesn’t mean they might not be for another one. But whether or not you ever look
    at them again, working with those datasets will teach you so many things: about
    the subject of the data, about the pitfalls and possibilities of certain data
    types, about experts on the topic, and more. In other words, letting go of a dataset
    or a coding approach is *never* a “waste”: the experience you gain will only make
    your next effort better, if you let it.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 对于可能无法用来回答特定问题的数据集，也是一样的道理：只因为它们不适合当前项目，并不意味着它们对另一个项目也不适用。但是，无论你是否再次查看它们，与这些数据集一起工作将教会你许多东西：关于数据主题，关于某些数据类型的陷阱和可能性，关于该主题的专家等等。换句话说，放弃一个数据集或编码方法*从来不是*“浪费”：你获得的经验只会让你的下一次尝试变得更好，如果你愿意的话。
- en: Gotchas That Will Get Ya!
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 会让你陷入困境的要点！
- en: One of the reasons why it is so important to document your work is that very
    often the person you’re writing that documentation for is really just “future
    you,” who may be returning to a particular dataset or script—or even Python altogether—after
    days, weeks, or months away. In that time, things that were once obvious will
    seem confusing and obscure unless you document them thoroughly, and even common
    “lessons” can get overlooked when you’re in a hurry or focused on something else.
    I had that experience myself as I worked through the exercises in the last few
    chapters, especially as I made an effort to check my own work. For me, that experience
    was just another reminder that when something’s wrong with your script, it’s usually
    something simple ;-)
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some common gotchas to keep in mind:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: Confirm the case
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: Anytime you are checking to see if two strings are the same, remember that capitalization
    matters! When I was working on [Example 6-16](ch06.html#ppp_find_waterford), I
    at first overlooked that all of the business names (but not the bank names!) were
    in all caps. I had a frustrating few minutes thinking that my dataset did *not*
    contain the `WATERFORD RECEPTIONS` example, until I finally looked at the data
    again and realized my error.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: Insist on the data type
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: As I worked my way through the process described in [“The Circuitous Path to
    “Simple” Solutions”](#not_so_fast), I once again had trouble finding matches for
    values that I felt certain should be in the dataset. I had forgotten, however,
    that the *pandas* library (unlike the *csv* library) actually tries to apply data
    types to the columns of data it reads into a DataFrame. In this case, that meant
    that `OriginatingLenderLocationID` became a number (instead of a string), so my
    efforts to find particular values for that column were failing because I was trying
    to match, for example, the number `71453` to the string `"71453"`—which definitely
    doesn’t work!
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: In that instance, I found the simplest solution was simply to add a parameter
    to the `read_csv()` function call, specifying that all the data should be read
    as strings (e.g., `fingerprinted_data1 = pd.read_csv('public_150k_plus_fingerprints.csv',
    dtype='string')`).^([7](ch07.html#idm45143402866320)) This also prevented some
    of the larger dollar amounts in the data from being converted to scientific notation
    (e.g., `1.21068e+06` rather than `1210681`).
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: After basic typos, the sort of data-type “gotchas” described here are probably
    the next most common data wrangling “errors” you’re likely to encounter. So if
    you find you’ve made an oversight like these at some point, try not to be too
    frustrated. It’s really just a sign that your programming logic is good and some
    of your formatting needs to be fixed.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: Augmenting Your Data
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Adding the `OriginatingLenderFingerprint` column in [Example 7-10](#ppp_add_fingerprints)
    was a valuable way to increase the utility and usability of the PPP loan data,
    but another good way to add value to a dataset is to look for *other* datasets
    that you can use to augment it. This is usually easiest when the dataset is *dimensionally
    structured*, in that it already references a widely used standard of some kind.
    In the case of our PPP loan data, we have an example of this in the column called
    `NAICSCode`, which a quick web search^([8](ch07.html#idm45143402846432)) confirms
    is the:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: …North American Industry Classification System. The NAICS System was developed
    for use by Federal Statistical Agencies for the collection, analysis and publication
    of statistical data related to the US Economy.
  id: totrans-166
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Given this, we can probably find a way to augment our data by adding more information
    about the NAICS code for each entry, which might, for example, help us understand
    more about what industries and types of businesses are participating in the PPP
    loan program. While we could probably pull a comprehensive list of NAICS codes
    from the main website, a web search for `naics sba` brings up some interesting
    options. Specifically, the SBA offers a PDF that provides [information about Small
    Business Administration size guidelines for businesses by NAICS code](https://sba.gov/sites/default/files/2019-08/SBA%20Table%20of%20Size%20Standards_Effective%20Aug%2019%2C%202019.pdf),
    in either millions of dollars or number of employees. In addition to providing
    us with more human-readable descriptions of the NAICS codes themselves, augmenting
    our PPP loan data with this additional information can help us answer more general
    questions about what actually qualifies as a “small business.”
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: Our process for this won’t be too much different from data merges we’ve done
    previously, both in the process we’ll follow *and* the issues it introduces. To
    start off with, we’ll look for a non-PDF version of the SBA size guidelines. Clicking
    on the “SBA’s Size Standards Webpage” link on the first page of the PDF brings
    us to a more [general page on the SBA website](https://sba.gov/federal-contracting/contracting-guide/size-standards),
    where in the “Numerical Requirements” section we find a link labeled [“table of
    small business size standards”](https://sba.gov/document/support-object-object-table-size-standards).
    Scrolling down that page turns up a downloadable [XLSX version](https://sba.gov/sites/default/files/2019-08/SBA%20Table%20of%20Size%20Standards_Effective%20Aug%2019%2C%202019.xlsx)
    of the earlier PDF document. From there, we can export the second sheet (which
    contains the actual codes and descriptions) as a CSV file. Now, we can import
    and match this with our PPP loan data.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: As you’ll see in [Example 7-12](#ppp_adding_naics), anytime we integrate a new
    data source, it means we have to evaluate, clean, and transform it just as we
    have our “primary” dataset. In this case, that means that we want to proactively
    update any `<NA>` values in the `NAICSCode` column of our PPP loan data to a flag
    value (I have chosen the string “None”), in order to prevent their being matched
    with essentially random `<NA>` values in our SBA NAICS code file. Similarly, once
    we’ve done our merge, we still want to see what codes from our PPP loan file *didn’t*
    get matched successfully. For now, we’ll leave open the decision about how to
    handle these until we’ve done a bit more digging around in our analysis phase
    to see whether we want to “fill them in” (e.g., with the regular NAICS values/interpretations),
    flag them as being atypical for the SBA, or some combination thereof.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: Example 7-12\. ppp_adding_naics.py
  id: totrans-170
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[![1](assets/1.png)](#co_cleaning__transforming____span_class__keep_together__and_augmenting_data__span__CO7-1)'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: Use the `dtype='string'` parameter to force `pandas` to treat our entire dataset
    as strings; this will make later matching and comparison tasks more predictable.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_cleaning__transforming____span_class__keep_together__and_augmenting_data__span__CO7-2)'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: If we don’t do this replacement, our data will match to unpredictable `NA` values
    from the *SBA-NAICS-data.csv* file.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: Augmenting a dataset as we have in [Example 7-12](#ppp_adding_naics) can help
    us expand the types of questions we can use it to answer, as well as helping support
    faster, more comprehensive data analysis and interpretation. At the same time,
    anytime we introduce new data, we need to complete the same life cycle of evaluation,
    cleaning, transformation, and (maybe even) augmentation that we applied to our
    “primary” dataset. This means that we’ll always need to strike a balance between
    making our primary data more elaborate (and possibly useful) with the time and
    effort involved in finding and wrangling the “secondary” data that we use to augment
    it.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  id: totrans-177
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While the variety of data cleaning, transformation, and augmentation possibilities
    is as varied as both datasets and analysis possibilities, the primary goal of
    this chapter was to illustrate common issues in data cleaning, transformation,
    and augmentation and introduce some key methods for resolving them.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: Before we move on to actually trying to generate insights with our data, however,
    we’re going to take a small “detour” in [Chapter 8](ch08.html#chapter8), which
    will focus on some programming best practices that can help us make sure our code
    is as clear, efficient, and effective as possible. Because while using Python
    to do data wrangling already lets us do things that would be impossible with other
    tools, optimizing our code for both use and reuse is another way to make sure
    we get the most out of each program and piece of code we write, too. In most cases,
    this means structuring our files so that they are more versatile, composable,
    and readable, as we’ll see right now!
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch07.html#idm45143404308272-marker)) While the data format for Citi Bike
    data files changed in early 2021, files from before that date still follow the
    format in these examples.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch07.html#idm45143404288880-marker)) And even if they weren’t, we could
    always convert them to strings.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](ch07.html#idm45143403643280-marker)) Unless, of course, you’re working
    in a team—then you need to consider everyone’s needs. You’ll be glad when it’s
    your turn to work with *their* code.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: ^([4](ch07.html#idm45143403085296-marker)) Of course, Macs and PCs use a different
    “base” date because…*reasons*.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: ^([5](ch07.html#idm45143403010368-marker)) Believe me, most professional programmers
    are looking things up online every five minutes.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: ^([6](ch07.html#idm45143402992928-marker)) At first I was concerned that the
    `OriginatingLenderLocationID` might refer to an individual bank branch, for example.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: ^([7](ch07.html#idm45143402866320-marker)) As a case in point, I didn’t even
    end up using this approach in the final code for [Example 7-10](#ppp_add_fingerprints),
    but I *did* find a use for it in [Example 7-12](#ppp_adding_naics)!
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: ^([8](ch07.html#idm45143402846432-marker)) Which leads us to [*https://naics.com/what-is-a-naics-code-why-do-i-need-one*](https://naics.com/what-is-a-naics-code-why-do-i-need-one).
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
