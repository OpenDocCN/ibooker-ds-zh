- en: Chapter 7\. Cleaning, Transforming, and Augmenting Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most of the time, the data that we initially find, collect, or acquire doesn’t
    quite suit our needs in one way or another. The format is awkward, the data structure
    is wrong, or its units need to be adjusted. The data itself might contain errors,
    inconsistencies, or gaps. It may contain references we don’t understand or hint
    at additional possibilities that aren’t realized. Whatever the limitation may
    be, in our quest to use data as a source of insight, it is inevitable that we
    will have to clean, transform, and/or augment it in some way in order to get the
    most out of it.
  prefs: []
  type: TYPE_NORMAL
- en: Up until now, we have put off most of this work because we had more urgent problems
    to solve. In [Chapter 4](ch04.html#chapter4), our focus was on getting data out
    of a tricky file format and into something more accessible; in [Chapter 6](ch06.html#chapter6),
    our priority was thoroughly assessing the quality of our data, so we could make
    an informed decision about whether it was worth the investment of augmentation
    and analysis at all.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, however, it’s time to roll up our sleeves and begin what to me is sort
    of the second phase of data wrangling and quality work: preparing the data we
    have for the analysis we want to perform. Our data is in the table-type format
    we need, and we’ve determined that it’s of high enough quality to yield *some*
    useful insights—even if they are not precisely the ones we first imagined.'
  prefs: []
  type: TYPE_NORMAL
- en: Since it’s obviously impossible to identify and address every possible problem
    or technique related to cleaning, transforming, and/or augmenting data, my approach
    here will be to work through the actual examples we’ve already encountered where
    one or more of these tasks is required. For example, we’ll look at different ways
    we might need to transform date-type information using datasets we encountered
    in Chapters [2](ch02.html#chapter2) and [4](ch04.html#chapter4). We’ll also look
    at different ways we can clean up the “cruft” in data files that contain both
    structured data *and* metadata. We’ll even explore *regular expressions*, which
    offer us a powerful way to select only certain parts of a data field or match
    particular terms and patterns irrespective of capitalization and/or punctuation.
    In the process, we’ll manage to cover a decent range of the tools and strategies
    you’re likely to need when cleaning and transforming most datasets. At the very
    least, the approaches outlined in this chapter will give you a useful starting
    place if you encounter a challenge that’s truly gnarly or unique.
  prefs: []
  type: TYPE_NORMAL
- en: Selecting a Subset of Citi Bike Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Way back in [“Hitting the Road with Citi Bike Data”](ch02.html#hitting_the_road_intro),
    we used Citi Bike system data to test out some of our freshly unboxed Python concepts,
    like `for...in` loops and `if/else` conditionals. For the sake of convenience,
    we started with a sample dataset that I had excerpted from [the September 2020
    system data file](https://s3.amazonaws.com/tripdata/index.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'There are any number of situations where we’ll want to segment large datasets
    for analysis—either because we don’t have the time or computational resources
    to process everything at once or because we’re only interested in a subset of
    the dataset to begin with. If all we wanted to do was select a specific number
    of rows, we could write a `for...in` loop using the `range()` function described
    in [“Adding Iterators: The range Function”](ch04.html#add_iterators). But we might
    also want to excerpt the data based on its values as well. I did this in selecting
    all of the rides from September 1, 2020, but we might also want to do something
    a bit more nuanced, like evaluating weekday Citi Bike rides separately from those
    taken on weekends and holidays.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start with the first task, of excerpting just the September 1, 2020,
    rides from the larger dataset.^([1](ch07.html#idm45143404308272)) Conceptually,
    this is simple enough: we just want to keep every row in our dataset containing
    a ride that started on the first day of September. If we briefly revisit the dataset,
    however, it becomes clear that even this task is not so simple.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Zoomed-in view of the first few lines of Citi Bike trip data.](assets/ppdw_0701.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-1\. First few lines of Citi Bike trip data
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'As you can see in [Figure 7-1](#citibike_sample_screengrab), the `starttime`
    column is not simply a date but some kind of date/time format that includes not
    just the month, day, and year but also the hours, minutes, and seconds (to four
    decimal points!). The first entry in this data file, for example, the value of
    the `starttime`, looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Obviously, if we want to analyze just the first day of rides—or just rides during
    the morning “rush hour” commute or just weekday rides—we need a way to effectively
    filter our data based on just *part* of the information that’s stored in this
    column. But what options do we have for accomplishing this? In the next few sections,
    we’ll look at each of these tasks—finding just rides on a particular date, in
    a particular time frame, and on a particular “type” of day—in turn. In the process,
    we’ll learn some of the tools Python offers for solving problems like these, as
    well as when and why we might choose one over another.
  prefs: []
  type: TYPE_NORMAL
- en: A Simple Split
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Solving the first problem—excerpting just the rides that started on September
    1, 2020—is actually relatively easy to do if we combine some of the tools that
    we’ve used already in some previous examples. It starts with recognizing that
    when we read in a basic CSV file with Python, most of our data will be treated
    as strings.^([2](ch07.html#idm45143404288880)) This means that, even though we
    humans clearly know that `2020-09-01 00:00:01.0430` is meant to be *interpreted*
    as a date and time, Python just sees it as a collection of numbers and characters.
  prefs: []
  type: TYPE_NORMAL
- en: Looking at the `starttime` field this way, the question of how to find all the
    rides that started on September 1, 2020, becomes a bit more straightforward, because
    the part of our data that contains the “date” information is *always* separated
    from the “time” information by a single space. This means that if we can find
    a way to look only at what comes *before* that space, we can easily set up an
    `if/else` conditional to compare that to our target date string—in this case,
    `2020-09-01`—and use that comparison to keep only the rows we want.
  prefs: []
  type: TYPE_NORMAL
- en: While it may not seem glamorous, the built-in string `split()` is going to be
    our hero here. It’s already played a supporting role in previous exercises when
    we needed to break up filenames or URLs; we actually used it way back in [“Verbs
    ≈ Functions”](ch02.html#verbs_are_functions) to illustrate the difference between
    functions and methods! As a refresher, this method lets us specify a single character
    that should be used to split a string into parts. The output of this method is
    a list, which contains the “leftover” pieces of the string in the order in which
    they appeared, with the character you `split()` on removed. So splitting the string
    `2020-09-01 00:00:01.0430` on a space will yield the list `['2020-09-01', '00:00:01.0430']`.
  prefs: []
  type: TYPE_NORMAL
- en: To see how simple and effective this is, let’s modify our script from [“Hitting
    the Road with Citi Bike Data”](ch02.html#hitting_the_road_intro). In [Example 7-1](#citibike_september1_rides),
    I’ve edited down some of the comments because these tasks are much more familiar
    now, but it’s still a good idea to outline your script’s objective at the top.
  prefs: []
  type: TYPE_NORMAL
- en: Example 7-1\. citibike_september1_rides.py
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Pretty simple, right? Of course, you could easily modify this script to capture
    a different date, or even multiple dates if you wanted to. For example, you could
    modify the `if` statement to be something like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Of course, while this `or` statement works perfectly well if you’re looking
    for two or three specific dates, it starts to get *very* messy if you need to
    look for more than that (you may recall that we ended up with a similarly awkward
    conditional in [Example 6-12](ch06.html#ppp_loan_uses)). In order to filter our
    data with the precision we need without generating extraordinarily complex, awkward,
    and error-prone code, we’ll be better served by a whole different toolkit: *regular
    expressions*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Regular Expressions: Supercharged String Matching'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A *regular expression* (often shortened to *regex*), allows you to quickly and
    efficiently search for string patterns within a larger string or piece of text.
    In most cases, if you’re trying to solve a matching or filtering problem and find
    that the solution involves *lots* of `and` or `or` statements, it’s an early sign
    that what you really need is a regular expression.
  prefs: []
  type: TYPE_NORMAL
- en: Regular expressions are found in most programming languages and are concise,
    powerful, and, at times, *extremely* tricky to work with. While a single regular
    expression can encapsulate even a very complex search pattern, designing regexes
    that work as expected can be extremely time-consuming, often requiring quite a
    bit of trial and error to get right. Since our goal is for our data wrangling
    work to be both efficient *and* comprehensible, we’ll focus here on short regexes
    that offer unique functionality not easily achieved through other means. While
    there are certain tasks where regular expressions are indispensable, they are
    not the tool for solving every problem and usually work best when paired with
    other techniques.
  prefs: []
  type: TYPE_NORMAL
- en: 'To get started, let’s use a regular expression to tackle the problem of filtering
    out rides that take place within the typical “morning commute” hours, which we’ll
    estimate here as being from 7 a.m. to 9 a.m. Any regex process begins with distinguishing
    for ourselves what we *want* to match from what we *don’t*. Here, we’ll start
    with an example `starttime` entry that is *outside* of our identified time range
    (hence, something we *don’t* want to match):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let’s look at a `starttime` entry that falls *within* it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let’s acknowledge first that we *could* address this problem with the
    string-splitting method we saw previously. We could start by splitting on the
    `:` character, which would, in the second instance, give us this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Then we could take the middle item from the list and use a *compound conditional*—that
    is, an `if` statement that joins two or more tests—to see if it matches the strings
    `'07'`, `'08'`, or `'09'`.
  prefs: []
  type: TYPE_NORMAL
- en: This approach certainly *works*, but it feels a little awkward. It requires
    multiple steps and a three-part conditional that will quickly get difficult to
    read. A regular expression, meanwhile, will let us narrow in on those hour values
    in a single step while still being fairly readable. Before we dive into writing
    the regex itself, though, let’s do a quick overview of the vocabulary of Python
    regular expressions.
  prefs: []
  type: TYPE_NORMAL
- en: Because a regular expression has to use characters and strings to describe *patterns*
    of characters and strings, the Python regular expression “language” uses a set
    of *metacharacters* and special sequences to make describing the pattern you’re
    searching for simpler. In [Table 7-1](#regex_sequence_list) I’ve included some
    of the most useful ones, drawn from a more complete list [on W3Schools](https://w3schools.com/python/python_regex.asp).
  prefs: []
  type: TYPE_NORMAL
- en: Table 7-1\. Common regular expression building blocks
  prefs: []
  type: TYPE_NORMAL
- en: '| Expression | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| [] | A set of characters |'
  prefs: []
  type: TYPE_TB
- en: '| “\” | Signals a special sequence (can also be used to escape special characters)
    |'
  prefs: []
  type: TYPE_TB
- en: '| . | Any character (except newline character) |'
  prefs: []
  type: TYPE_TB
- en: '| * | Zero or more occurrences |'
  prefs: []
  type: TYPE_TB
- en: '| + | One or more occurrences |'
  prefs: []
  type: TYPE_TB
- en: '| {} | Exactly the specified number of occurrences |'
  prefs: []
  type: TYPE_TB
- en: '| &#124; | Either or |'
  prefs: []
  type: TYPE_TB
- en: '| () | Capture and group |'
  prefs: []
  type: TYPE_TB
- en: '| \d | Returns a match where the string contains digits (numbers from 0–9)
    |'
  prefs: []
  type: TYPE_TB
- en: '| \D | Returns a match where the string DOES NOT contain digits |'
  prefs: []
  type: TYPE_TB
- en: '| \s | Returns a match where the string contains a whitespace character |'
  prefs: []
  type: TYPE_TB
- en: '| \S | Returns a match where the string DOES NOT contain a whitespace character
    |'
  prefs: []
  type: TYPE_TB
- en: '| \w | Returns a match where the string contains any word characters (characters
    from a to Z, digits from 0–9, and the underscore _ character) |'
  prefs: []
  type: TYPE_TB
- en: '| \W | Returns a match where the string DOES NOT contain any word characters
    |'
  prefs: []
  type: TYPE_TB
- en: As always with writing, regular expressions give us more than one way to “capture”
    the pattern we’re looking for. In most cases, our goal is to define a pattern
    that will match what we need to find while avoiding *accidentally* matching on
    anything else. For our “rush hour” problem, we can take advantage of the fact
    that the “hours” digits in the `starttime` column are surrounded by colons (`:`),
    *and nothing else is*. This means that we can use this “surrounded by colons”
    pattern as part of our regular expression and feel confident that we won’t accidentally
    match some other part of the string. To see if this works as we hope, let’s set
    up a few sample regular expressions to test against some (real and constructed)
    sample data to see how they do, as shown in [Example 7-2](#regex_tests).
  prefs: []
  type: TYPE_NORMAL
- en: Example 7-2\. regex_tests.py
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_cleaning__transforming____span_class__keep_together__and_augmenting_data__span__CO1-1)'
  prefs: []
  type: TYPE_NORMAL
- en: In addition to sample files like this, you can also test out your Python regex
    online using the [W3Schools regex demo](https://w3schools.com/python/trypython.asp?filename=demo_regex).
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_cleaning__transforming____span_class__keep_together__and_augmenting_data__span__CO1-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Even if you only use them once in a script, I *strongly* recommend defining
    your regex at the top of your file using an aptly named variable. It is the simplest,
    most efficient way to keep track of their functionality, especially if you’re
    using more than one!
  prefs: []
  type: TYPE_NORMAL
- en: 'When you run the script in [Example 7-2](#regex_tests), your output should
    look something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the “bookended” regex, where we specified both of the colons,
    correctly matches (*and* fails to match) in all three cases; the “one-sided” regex,
    on the other hand, erroneously finds a match on the *seconds* value of `sample3`.
    This is precisely why defining the string you’re looking for as precisely as possible
    is important. If you look at the `Match object` printed out previously, you’ll
    see that it contains information about what was matched (e.g., `match='07:'`)
    and where (e.g., from index positions 14–17 in the string).
  prefs: []
  type: TYPE_NORMAL
- en: 'So far, this seems pretty straightforward. Things can still get a little tricky,
    however, when the *structure* of the thing we want to match changes. For example,
    what if we wanted to expand the hours we’re interested in to range from 7 a.m.
    to 10 a.m.? Our `bookend_regex` won’t work as written, because it specifies that
    the first character after the colon has to be a `0`. We could try just adding
    the digits `1` and `0` as options to our digit ranges, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'which produces the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The problem, as we can see from the output, is that our data uses a 24-hour
    clock and will end up matching on a whole range of times that we don’t want. That’s
    because regular expressions don’t “see” numbers in the way we think of them—all
    they see are sequences of characters. That’s why `18` comes back as a match—our
    regex allows any string that starts with a `0` or a `1` and is followed by a `0`,
    `7`, `8`, or `9`. While we obviously wrote it with the numbers `07`, `08`, `09`,
    and `10` in mind, our code opens the door to many more.
  prefs: []
  type: TYPE_NORMAL
- en: The solution, in this case, is to use the “either/or” *pipe* character (`|`),
    which we can use to combine to (otherwise) completely distinct regular expressions.
    In this case, that will look something like what’s shown in [Example 7-3](#seven_to_ten).
  prefs: []
  type: TYPE_NORMAL
- en: Example 7-3\. Capturing 7 to 10
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Try it out yourself with a few sample data points, just to confirm that it captures
    what we’re looking for (and nothing else).
  prefs: []
  type: TYPE_NORMAL
- en: 'I’m not going to go too much further with regular expressions than this; as
    with the web scraping we explored in [“Web Scraping: The Data Source of Last Resort”](ch05.html#web_scraping),
    no two regular expression problems (or solutions) are alike. However, I hope you
    can see the potential these offer for doing pattern matching that would be very
    awkward with compound conditionals and basic string functions alone.'
  prefs: []
  type: TYPE_NORMAL
- en: Making a Date
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the reasons it’s appealing to treat date-like data as strings is that,
    as we saw in our work with various source formats of unemployment data in [Chapter 4](ch04.html#chapter4),
    the way they are interpreted can vary dramatically across data sources and even
    Python libraries. Still, there are situations and tasks where converting date-like
    data to an actual `datetime` type is very useful. For example, if we want to isolate
    the weekday rides from our Citi Bike data, we *could* try to essentially “brute
    force” it by looking at a calendar, identifying the dates of all the weekdays,
    and then creating a giant string comparison list or writing a regular expression
    to match them. In the case of the September 2020 data, such a regular expression
    object might look something what’s in [Example 7-4](#weekday_regex).
  prefs: []
  type: TYPE_NORMAL
- en: Example 7-4\. Weekday regex for September 2020
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Ugh. This certainly *works*, but it’s nearly impossible to read and is still
    basically one giant compound conditional—even if it’s captured in fewer characters
    because it’s a regular expression. Moreover, it’s not a solution that scales very
    well. If we wanted to extend our analysis to any *other* month, it would mean
    getting out the calendar all over again.
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, a well-constructed Python `datetime` object has a number of built-in
    methods that can help with exactly this kind of task. In fact there is a simple
    `weekday()` method that returns a number from 0 to 6 ([with 0 being Monday and
    6 being Sunday](https://docs.python.org/3/library/datetime.html#datetime.date.weekday))
    based on the day of the week on which a certain date falls. This means that if
    we convert the contents of our `starttime` column to a date, as shown in [Example 7-5](#weekday_rides),
    we can use this method to quickly identify the day of the week corresponding to
    *any* date. This will help us apply our code to additional data sources—say, a
    different month or year of ridership data—without having to do a thing!
  prefs: []
  type: TYPE_NORMAL
- en: Example 7-5\. weekday_rides.py
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_cleaning__transforming____span_class__keep_together__and_augmenting_data__span__CO2-1)'
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned in [Example 6-1](ch06.html#ppp_date_range), providing [the format
    of our source data](https://docs.python.org/3/library/datetime.html#strftime-strptime-behavior)
    will help our script run faster and more reliably.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_cleaning__transforming____span_class__keep_together__and_augmenting_data__span__CO2-2)'
  prefs: []
  type: TYPE_NORMAL
- en: The `weekday()` method puts [Monday at position `0`](https://docs.python.org/3/library/datetime.html#datetime.date.weekday),
    so looking for anything up to and including `4` will capture the values for Monday
    through Friday.
  prefs: []
  type: TYPE_NORMAL
- en: Depending on your device, you may notice that the script in [Example 7-5](#weekday_rides)
    takes a while to run. For example, on my (not very powerful) device, it takes
    more than 85 seconds to complete. Accomplishing the same task using the regular
    expression in [Example 7-4](#weekday_regex), meanwhile, takes only 45 seconds.
    I can also more easily tweak the regular expression to skip days that are officially
    weekdays but are also holidays (like Labor Day).
  prefs: []
  type: TYPE_NORMAL
- en: So which approach is better? As usual, *it depends*. What will actually work
    best for your particular data wrangling/cleaning/transformation process will be
    specific to your needs and your resources. If answering your question means looking
    for weekday commute patterns in a decade’s worth of Citi Bike data, you’re probably
    better off using the `weekday()` method, because you don’t have to change your
    code to deal with different months or years. On the other hand, if you don’t have
    very many months to work with and execution speed (and absolute precision) is
    your top concern, you might prefer to go the regular expression route. You may
    also just find that regexes make you want to tear your hair out or that using
    multiple steps to get perfect results drives you crazy. As we’ll explore more
    in [Chapter 8](ch08.html#chapter8), all of these can be legitimate reasons for
    a particular design choice—just make sure the choice is *yours*.^([3](ch07.html#idm45143403643280))
  prefs: []
  type: TYPE_NORMAL
- en: De-crufting Data Files
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 4](ch04.html#chapter4), we encountered a number of instances where
    we needed to “clean up” a dataset that was otherwise awkward or unintelligible.
    When we went through the process of parsing an old-school-style *.xls* file in
    [Example 4-6](ch04.html#xls_parsing), for example, we encountered a couple of
    distinct issues. First, the spreadsheet contained both table-type data *and* descriptive
    header information that, despite being useful in principle, will inevitably need
    to be relocated in order for us to analyze the rest of it. Second, the *.xls*
    format’s lack of support for “real” dates means that our initial transformation
    from *.xls* to *.csv* left us with a bunch of nonsense numbers where the dates
    should have been. While I chose to put off solving those problems at first, the
    time has come to confront them.
  prefs: []
  type: TYPE_NORMAL
- en: In thinking about the first problem, I want to stress that we *definitely* don’t
    want to just “throw out” the information that’s currently stored at the top of
    the *fredgraph.xls* file. As is hopefully clear from our work in [Chapter 6](ch06.html#chapter6),
    metadata is a precious resource, and we *never* want to discard metadata from
    a [primary source](https://loc.gov/programs/teachers/getting-started-with-primary-sources).
    Rather, my preference in cases like this is to turn one file into two. We’ll strip
    out the metadata and store it in a separate—but congruently named—text file while
    also parsing and saving the table-type data into an analysis-friendly *.csv* format.
  prefs: []
  type: TYPE_NORMAL
- en: 'Looking at our source *.xls* file in a spreadsheet program, it’s easy enough
    to see visually where the metadata ends and the table-type data begins. The real
    question is: how will we detect this transition in our script? As is so often
    the case with data cleaning, the most effective solution is not always an elegant
    one. The metadata ends where the table-type data begins, in the row containing
    its column headers. If we look at the first value in each row as we work our way
    through the file, we can *stop* writing to the metadata file and *start* writing
    to the *.csv* as soon as we encounter the first column header. Since the value
    `observation_date` is the first column header for this dataset, we’ll make that
    transition as soon we find that at the beginning of our current row.'
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Before you begin, check your source file carefully to see *where* the metadata
    appears within it. Especially in cases where the data contains estimates or other
    qualifiers, you’re likely to find metadata both before and *after* the table-type
    data in your source file.
  prefs: []
  type: TYPE_NORMAL
- en: To see what’s involved in creating these two purpose-built files from our single
    source file, take a look at the script in [Example 7-6](#xls_meta_parsing) (if
    you need a refresher on some of the code choices in this example, you may want
    to refer back to [Example 4-6](ch04.html#xls_parsing)).
  prefs: []
  type: TYPE_NORMAL
- en: Example 7-6\. xls_meta_parsing.py
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_cleaning__transforming____span_class__keep_together__and_augmenting_data__span__CO3-1)'
  prefs: []
  type: TYPE_NORMAL
- en: While we’re only creating a single metadata file here, we could easily move
    this part of the process inside the `for` loop and create a unique metadata file
    for every worksheet, if necessary.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_cleaning__transforming____span_class__keep_together__and_augmenting_data__span__CO3-2)'
  prefs: []
  type: TYPE_NORMAL
- en: This type of Boolean (True/False) variable is often described as a *flag* variable.
    The idea is that we set its value *outside* of a loop and then “flip” its value
    when some particular thing has happened—this saves us from having to loop through
    all our data twice. Here, we’ll use it to check when we should start writing to
    our “data” file instead of our “metadata” file.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we move on to dealing with the (still inexplicable) dates in this file,
    I want to highlight a new technique introduced in [Example 7-6](#xls_meta_parsing):
    the use of a so-called *flag variable*. This term typically refers to any Boolean
    (True/False) variable that is used to keep track of whether a certain event has
    taken place or condition has been met, especially within a loop. In [Example 7-6](#xls_meta_parsing),
    for example, we are using the `is_table_data` variable as a way to keep track
    of whether we have yet encountered the row of data that marks the beginning of
    our table data. Since a given row of data in our `for...in` loop is essentially
    “forgotten” as soon as the next one is read, we need to create this variable *before*
    our loop. This keeps the `is_table_data` variable available beyond the *scope*
    of our loop—a concept we’ll look at more closely in [Chapter 8](ch08.html#chapter8).'
  prefs: []
  type: TYPE_NORMAL
- en: Decrypting Excel Dates
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We can avoid the issue of those Excel dates no longer. While hopefully you will
    not encounter this situation often, I’m including it here for completeness and
    because it illustrates a few different ways that code tends to evolve—usually
    getting more complicated and less readable—as we add even seemingly small bits
    of functionality to it. For example, in [Example 7-7](#xls_meta_and_date_parsing),
    we’ll need to check whether a variable contains a number or not, and believe it
    or not, we need a library for this—it is aptly called *numbers*. While that part
    is fundamentally straightforward, you’ll quickly see in [Example 7-7](#xls_meta_and_date_parsing)
    how the need to transform these date values requires adapting our approach to
    writing the table-type data to our output file.
  prefs: []
  type: TYPE_NORMAL
- en: Example 7-7\. xls_meta_and_date_parsing.py
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_cleaning__transforming____span_class__keep_together__and_augmenting_data__span__CO4-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Converting these *.xls* “dates” using the *xlrd* library’s `xldate_as_datetime()`
    method requires both the number value *and* [the workbook’s `datemode`](https://xlrd.readthedocs.io/en/latest/api.html#xlrd.book.Book.datemode)
    in order to generate the Python datetime object correctly.^([4](ch07.html#idm45143403085296))
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_cleaning__transforming____span_class__keep_together__and_augmenting_data__span__CO4-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Here, I’ve decided to write the date to my table-type data file as `MM/DD/YYYY`
    using the appropriate `strftime()` format, but you could use another format if
    you prefer.
  prefs: []
  type: TYPE_NORMAL
- en: 'While the *xlrd* library makes the process of converting our strange Excel
    dates to something understandable relatively straightforward, I think the code
    in [Example 7-7](#xls_meta_and_date_parsing) demonstrates how the idiosyncrasies
    of wrangling a particular dataset can quickly add complexity—especially in the
    form of additional, nested `if` statements—to what started as a very simple program.
    This is just one of the reasons why we’ll spend [Chapter 8](ch08.html#chapter8)
    exploring strategies and techniques for effectively and efficiently streamlining
    our code: we want to make sure it does everything we need but *also* that it is
    sufficiently readable and reusable to stand the test of time.'
  prefs: []
  type: TYPE_NORMAL
- en: Generating True CSVs from Fixed-Width Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Another instance where we also had only moderate success in transforming our
    data was in [Example 4-7](ch04.html#fixed_width_parsing), where we converted our
    fixed-width source data into a *.csv*. While technically we succeeded in creating
    an output file that was comma separated, the result was honestly pretty unsatisfactory:
    it retained many of the formatting artifacts of the original file that could easily
    stymie our future efforts at data analysis.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Fortunately, the specific problem we encountered—that of “leading” and/or “trailing”
    whitespace—is very well-known, since the data technologies that typically generate
    it have been around for a long time. As a result, fixing this problem is pretty
    simple: the solution exists in the form of the built-in Python `strip()` function,
    as illustrated in [Example 7-8](#fixed_width_strip_parsing).'
  prefs: []
  type: TYPE_NORMAL
- en: Example 7-8\. fixed_width_strip_parsing.py
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_cleaning__transforming____span_class__keep_together__and_augmenting_data__span__CO5-1)'
  prefs: []
  type: TYPE_NORMAL
- en: If you compare this code with the code in [Example 4-7](ch04.html#fixed_width_parsing),
    you’ll see that it’s identical apart from our having applied the `strip()` method
    to each string before appending it to our data row.
  prefs: []
  type: TYPE_NORMAL
- en: Once again, we see that modifying our original code to solve a formatting or
    “cleaning” issue isn’t necessarily that hard, but the resulting script isn’t *exactly*
    elegant, either. Stripping the whitespace from our output with the `strip()` method
    is definitely straightforward, but we’ve had to add a whole lot of parentheses
    in the process—leaving us with code that is far less readable than we’d like.
  prefs: []
  type: TYPE_NORMAL
- en: This illustrates yet another way in which creating good, quality Python code
    mirrors more typical writing processes. If we view our first efforts in [Chapter 4](ch04.html#chapter4)
    as something like the “outline” of our final program—where we solve the high-level
    problem of getting the data format we have into at least the table-type *structure*
    that we’re after—it gives us space to come back later and revise that work, filling
    in the details that allow it to be more nuanced in its handling of the specific
    dataset we’re working with.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, in [Chapter 8](ch08.html#chapter8) we’ll take that revision process
    one step further, refining these programs—which already do everything we need
    them to do—so that they are more concise and easier to understand, just as we
    might do with any piece of writing. This iterative approach to programming not
    only means that we eventually end up with better, more useful code; it also helps
    us break down big, complicated programming problems into a series of less intimidating
    ones that we can solve one step at a time. Equally important, no matter what stage
    of the process we’re in, we have a functioning program we can fall back on if
    needed. This incremental approach is especially useful when we take on more complex
    data-cleaning tasks like the one we’ll look at next: resolving the unintentional
    spelling differences.'
  prefs: []
  type: TYPE_NORMAL
- en: Correcting for Spelling Inconsistencies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 6](ch06.html#chapter6), we used a “fingerprinting” process to help
    address the possibility that information about banks in our Paycheck Protection
    Program (PPP) data might have spelling inconsistencies—a common issue in *any*
    dataset that relies on human data entry. Of course, the code we wrote in [Example 6-11](ch06.html#ppp_lender_names)
    only estimated the number of genuinely unique entries in the `OriginatingLender`
    column by counting how many of them resulted in a distinct fingerprint. We found
    that our dataset contained 4,337 unique bank names but only 4,242 unique fingerprints—an
    indicator that as many as 95 bank names might actually *be* the same but contain
    typos because they generated the same fingerprint.
  prefs: []
  type: TYPE_NORMAL
- en: Because those 95 potential typos could affect thousands of rows of data, we
    need a way to transform our dataset so that we can confidently aggregate it by
    lender. At the same time, we also don’t want to *overcorrect* by grouping together
    entries that don’t actually belong together.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is an instance where *transforming* our data is invaluable: we don’t want
    to risk losing any of our original data (retaining it is essential for validation
    and spot-checking), but we *also* need to transform it in order to support our
    future analysis efforts. Because our dataset is large, grouping and filtering
    it to meet our needs is likely to be time-consuming, so we want to preserve the
    results of that work by actually *adding* new columns to the dataset. This lets
    us both preserve our original data *and* the benefits of our transformation work
    in a single file.'
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, we’ve got some go-to libraries that will make this process pretty
    straightforward. Since we already know how to aggregate names using the fingerprinting
    process from [Example 6-11](ch06.html#ppp_lender_names), the trickier piece may
    be determining when banks that share the same fingerprint should actually be treated
    as distinct organizations. Looking back at the output from [Example 6-7](ch06.html#ppp_columns_review)
    (reproduced in [Example 7-9](#transposed_sample_recent_again) for convenience),
    we see that there are not a lot of fields that contain “originating” lender information,
    so our most likely option for deciding if two originating banks whose names share
    all the same words (and will therefore have the same fingerprint, e.g., “First
    Bank Texas” and “Texas First Bank”) will be to compare the value included in `OriginatingLenderLocationID`.
  prefs: []
  type: TYPE_NORMAL
- en: Example 7-9\. Recent sample data transposed
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Before we proceed, of course, we want to make sure that we understand what the
    data in `OriginatingLenderLocationID` actually *means*. Lucky for us, a web search
    for the words “originating lender location id” brings up [yet another document
    from the SBA website](https://sba.gov/sites/default/files/articles/ETran_Origination_01_2014.pdf)
    as the first result. Searching through this PDF for the term “location” brings
    us to the page shown in [Figure 7-2](#ppp_lender_location_info), which reassures
    us that the “Location ID” value entered should not change from branch to branch
    of the same bank but indicates the *main* branch of a given bank.
  prefs: []
  type: TYPE_NORMAL
- en: '![Information about lender location ID](assets/ppdw_0702.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-2\. Information about lender location ID
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: With this additional information, we can go about creating a version of our
    PPP loan data that includes a new column, `OriginatingLenderFingerprint`, that
    contains a combination of the `OriginatingLender` fingerprint and the `OriginatingLenderLocationID`,
    as shown in [Example 7-10](#ppp_add_fingerprints). Later on, we can then use this
    value to quickly aggregate our data by originating lender while being (reasonably)
    confident that we are neither failing to match entries due to typos *nor* treating
    what should be two separate banks as one.
  prefs: []
  type: TYPE_NORMAL
- en: Example 7-10\. ppp_add_fingerprints.py
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_cleaning__transforming____span_class__keep_together__and_augmenting_data__span__CO6-1)'
  prefs: []
  type: TYPE_NORMAL
- en: While it may seem excessive, this first loop actually exists *only* to create
    our new header row. As always, we want to avoid introducing typos whenever and
    wherever possible, so in this instance, the whole extra loop is worth it (*way*
    better than typing out this list by hand).
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_cleaning__transforming____span_class__keep_together__and_augmenting_data__span__CO6-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Since we’re adding a column of data, we need to build the new data row as a
    list, item by item—just as we did with the header row.
  prefs: []
  type: TYPE_NORMAL
- en: The structure of the resulting file is the same as the original, except that
    a new `OriginatingLenderFingerprint` column has now been added between `OriginatingLender`
    and `OriginatingLenderCity`, as you can see in [Example 7-11](#ppp_data_w_fingerprints).
  prefs: []
  type: TYPE_NORMAL
- en: Example 7-11\. PPP data with fingerprints
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: While this transformation will help us easily aggregate our data by a particular
    “originating” lender, we could quickly duplicate it with the “servicing” lender
    as well. We could *even* write a script that compares the value of these two resulting
    fingerprints to create a “flag” column indicating whether the servicing and originating
    banks for a particular loan are the same.
  prefs: []
  type: TYPE_NORMAL
- en: The Circuitous Path to “Simple” Solutions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While I hope that you found the [Example 7-10](#ppp_add_fingerprints) exercise
    simple to follow, I want you know that the preceding script was *not* the first
    solution I tried—it wasn’t even the second or third. In fact, I probably spent
    about a dozen hours, all told, thinking, hacking, wrangling, and failing before
    I *finally* realized that my eventual approach to the problem was the fastest,
    simplest, and most effective way to strike a balance between making sure that
    loans from the same bank were grouped together without accidentally conflating
    two different institutions.
  prefs: []
  type: TYPE_NORMAL
- en: I’m going to describe how I actually worked my way through this process because—as
    is hopefully *also* starting to become clear—data wrangling (and programming in
    general) is not so much about coding as it is about reasoning and problem-solving.
    This means that thinking through the problem in front of you, trying different
    solutions, and, perhaps most importantly, being willing to change course even
    if it feels like “throwing out” a bunch of work are *all* much more important
    to data wrangling than being able to write more than two lines of Python code
    from memory.^([5](ch07.html#idm45143403010368)) So in an effort to illustrate
    what just one of these problem-solving efforts entails, I’m going to (comparatively)
    briefly give you an overview here of the different approaches I tried before settling
    on the solution in [“Correcting for Spelling Inconsistencies”](#correcting_inconsistencies).
  prefs: []
  type: TYPE_NORMAL
- en: 'At first, I started out by minimally adapting the script from [Example 6-11](ch06.html#ppp_lender_names),
    creating a new column that contained *just* those fingerprints and writing a new
    CSV that added this new column. But I realized that there was a strong likelihood
    that some banks with similar names would share the same “fingerprint,” so I wrote
    a script that did the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Created a list of the unique fingerprints.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For every unique fingerprint, created a new list (actually a `pandas` DataFrame)
    of all the unique `OriginatingLenderLocationID` values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If there was more than one distinct `OriginatingLenderLocationID` value, I then
    *updated* the “fingerprint” column to incorporate the `OriginatingLenderLocationID`,
    much as we ended up doing for *all* the entries in [Example 7-10](#ppp_add_fingerprints).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Even creating *that* script, however, was much more involved than this numbered
    synopsis would make it seem. The first step was easy, of course—we’d pretty much
    done that already. But when it came time to working with the new file in `pandas`,
    my scrappy little Chromebook didn’t have enough memory, so I moved my work to
    Google Colab. This gave me more memory to work with (sort of), but now every time
    I stepped away for more than a few minutes, I had to authenticate and reload the
    data from my Google Drive file all over again—that took an additional couple of
    minutes every time. Also, while I was *pretty* confident that I had figured out
    how to update the values in my DataFrame correctly, attempting to check my work
    by searching for a new fingerprint that I was sure should exist wasn’t working
    reliably: sometimes I got matches, and sometimes I got an empty DataFrame! Add
    to this that it took about 3 or more minutes to run step 3 each time, and you
    can imagine how many hours (and how much frustration!) it took to be sure my code
    actually worked as intended.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Of course, once I had managed to code up (and check) that multistep solution,
    I realized that the result wasn’t all that different from what I’d started with.
    In fact, it was a little *less* satisfying because now the format of my new `OriginatingLenderFingerprint`
    column was inconsistent: some had the `OriginatingLenderLocationID` appended,
    some didn’t. But since the actual *value* of the fingerprint didn’t matter—only
    that it could be used accurately to both aggregate and disambiguate banks—why
    was I going to all the trouble of only adding location IDs to the ones that had
    several entries? Couldn’t they *all* just have the location IDs appended?'
  prefs: []
  type: TYPE_NORMAL
- en: 'Well, of course it was only at *that* point that I bothered to look up the
    documentation shown in [Figure 7-2](#ppp_lender_location_info), which confirmed
    that adding location IDs wouldn’t break up fingerprints that should be the same.^([6](ch07.html#idm45143402992928))
    And that’s how I came full circle: rather than assigning potentially overlapping
    fingerprints and then trying to “weed out” the problems with an awkward and time-consuming
    search process, the best solution was just to make the `OriginatingLenderLocationID`
    part of the new “fingerprint” column right from the start.'
  prefs: []
  type: TYPE_NORMAL
- en: Having spent hours working out how to “fix” the original fingerprints—and in
    the process, contending with the limits of my device, the vagaries of Google Colab,
    and the tedium of making a small change to a script and then having to wait several
    minutes for it to run—I won’t pretend that it didn’t feel like a bit of a letdown
    to realize that the best solution really just involved a small tweak on my original
    script (though not the one I had started with).
  prefs: []
  type: TYPE_NORMAL
- en: But if there’s one thing that I’ve learned after years of data wrangling, it’s
    that learning when to let go and start over (or go back to the beginning) is one
    of the most important skills you can develop. Sometimes you have to let go of
    a dataset, even if you’ve sunk hours into researching, evaluating, and cleaning
    it. Likewise, sometimes you have to let go of a programming approach, even if
    you’ve spent hours reading documentation and experimenting with new methods just
    to get the result you’re after. Because in the end, the goal is *not* to use a
    particular dataset, or to use a particular library or coding method. *It’s to
    use data to understand something about the world.* And if you can keep your focus
    on that, letting go when you need to will be much easier.
  prefs: []
  type: TYPE_NORMAL
- en: You will also probably find it easier to accept this process—whether it involves
    letting go of a dataset or a scripting solution you’ve already spent hours on—when
    you start to experience firsthand that you have learned something valuable even
    from something you eventually “abandon.” Before my detour into “fixing” my original,
    text-only fingerprints, for example, I didn’t really know how to update values
    within a `pandas` DataFrame; now I do (I *really* do). I also now know a bit more
    about Google Colab’s strengths and inconsistencies and was reminded about some
    key “gotchas” to working with diverse datasets (more on that in the next section).
  prefs: []
  type: TYPE_NORMAL
- en: 'The same goes for datasets that might not turn out to be usable for answering
    a particular question: just because they aren’t right for your current project
    doesn’t mean they might not be for another one. But whether or not you ever look
    at them again, working with those datasets will teach you so many things: about
    the subject of the data, about the pitfalls and possibilities of certain data
    types, about experts on the topic, and more. In other words, letting go of a dataset
    or a coding approach is *never* a “waste”: the experience you gain will only make
    your next effort better, if you let it.'
  prefs: []
  type: TYPE_NORMAL
- en: Gotchas That Will Get Ya!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the reasons why it is so important to document your work is that very
    often the person you’re writing that documentation for is really just “future
    you,” who may be returning to a particular dataset or script—or even Python altogether—after
    days, weeks, or months away. In that time, things that were once obvious will
    seem confusing and obscure unless you document them thoroughly, and even common
    “lessons” can get overlooked when you’re in a hurry or focused on something else.
    I had that experience myself as I worked through the exercises in the last few
    chapters, especially as I made an effort to check my own work. For me, that experience
    was just another reminder that when something’s wrong with your script, it’s usually
    something simple ;-)
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some common gotchas to keep in mind:'
  prefs: []
  type: TYPE_NORMAL
- en: Confirm the case
  prefs: []
  type: TYPE_NORMAL
- en: Anytime you are checking to see if two strings are the same, remember that capitalization
    matters! When I was working on [Example 6-16](ch06.html#ppp_find_waterford), I
    at first overlooked that all of the business names (but not the bank names!) were
    in all caps. I had a frustrating few minutes thinking that my dataset did *not*
    contain the `WATERFORD RECEPTIONS` example, until I finally looked at the data
    again and realized my error.
  prefs: []
  type: TYPE_NORMAL
- en: Insist on the data type
  prefs: []
  type: TYPE_NORMAL
- en: As I worked my way through the process described in [“The Circuitous Path to
    “Simple” Solutions”](#not_so_fast), I once again had trouble finding matches for
    values that I felt certain should be in the dataset. I had forgotten, however,
    that the *pandas* library (unlike the *csv* library) actually tries to apply data
    types to the columns of data it reads into a DataFrame. In this case, that meant
    that `OriginatingLenderLocationID` became a number (instead of a string), so my
    efforts to find particular values for that column were failing because I was trying
    to match, for example, the number `71453` to the string `"71453"`—which definitely
    doesn’t work!
  prefs: []
  type: TYPE_NORMAL
- en: In that instance, I found the simplest solution was simply to add a parameter
    to the `read_csv()` function call, specifying that all the data should be read
    as strings (e.g., `fingerprinted_data1 = pd.read_csv('public_150k_plus_fingerprints.csv',
    dtype='string')`).^([7](ch07.html#idm45143402866320)) This also prevented some
    of the larger dollar amounts in the data from being converted to scientific notation
    (e.g., `1.21068e+06` rather than `1210681`).
  prefs: []
  type: TYPE_NORMAL
- en: After basic typos, the sort of data-type “gotchas” described here are probably
    the next most common data wrangling “errors” you’re likely to encounter. So if
    you find you’ve made an oversight like these at some point, try not to be too
    frustrated. It’s really just a sign that your programming logic is good and some
    of your formatting needs to be fixed.
  prefs: []
  type: TYPE_NORMAL
- en: Augmenting Your Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Adding the `OriginatingLenderFingerprint` column in [Example 7-10](#ppp_add_fingerprints)
    was a valuable way to increase the utility and usability of the PPP loan data,
    but another good way to add value to a dataset is to look for *other* datasets
    that you can use to augment it. This is usually easiest when the dataset is *dimensionally
    structured*, in that it already references a widely used standard of some kind.
    In the case of our PPP loan data, we have an example of this in the column called
    `NAICSCode`, which a quick web search^([8](ch07.html#idm45143402846432)) confirms
    is the:'
  prefs: []
  type: TYPE_NORMAL
- en: …North American Industry Classification System. The NAICS System was developed
    for use by Federal Statistical Agencies for the collection, analysis and publication
    of statistical data related to the US Economy.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Given this, we can probably find a way to augment our data by adding more information
    about the NAICS code for each entry, which might, for example, help us understand
    more about what industries and types of businesses are participating in the PPP
    loan program. While we could probably pull a comprehensive list of NAICS codes
    from the main website, a web search for `naics sba` brings up some interesting
    options. Specifically, the SBA offers a PDF that provides [information about Small
    Business Administration size guidelines for businesses by NAICS code](https://sba.gov/sites/default/files/2019-08/SBA%20Table%20of%20Size%20Standards_Effective%20Aug%2019%2C%202019.pdf),
    in either millions of dollars or number of employees. In addition to providing
    us with more human-readable descriptions of the NAICS codes themselves, augmenting
    our PPP loan data with this additional information can help us answer more general
    questions about what actually qualifies as a “small business.”
  prefs: []
  type: TYPE_NORMAL
- en: Our process for this won’t be too much different from data merges we’ve done
    previously, both in the process we’ll follow *and* the issues it introduces. To
    start off with, we’ll look for a non-PDF version of the SBA size guidelines. Clicking
    on the “SBA’s Size Standards Webpage” link on the first page of the PDF brings
    us to a more [general page on the SBA website](https://sba.gov/federal-contracting/contracting-guide/size-standards),
    where in the “Numerical Requirements” section we find a link labeled [“table of
    small business size standards”](https://sba.gov/document/support-object-object-table-size-standards).
    Scrolling down that page turns up a downloadable [XLSX version](https://sba.gov/sites/default/files/2019-08/SBA%20Table%20of%20Size%20Standards_Effective%20Aug%2019%2C%202019.xlsx)
    of the earlier PDF document. From there, we can export the second sheet (which
    contains the actual codes and descriptions) as a CSV file. Now, we can import
    and match this with our PPP loan data.
  prefs: []
  type: TYPE_NORMAL
- en: As you’ll see in [Example 7-12](#ppp_adding_naics), anytime we integrate a new
    data source, it means we have to evaluate, clean, and transform it just as we
    have our “primary” dataset. In this case, that means that we want to proactively
    update any `<NA>` values in the `NAICSCode` column of our PPP loan data to a flag
    value (I have chosen the string “None”), in order to prevent their being matched
    with essentially random `<NA>` values in our SBA NAICS code file. Similarly, once
    we’ve done our merge, we still want to see what codes from our PPP loan file *didn’t*
    get matched successfully. For now, we’ll leave open the decision about how to
    handle these until we’ve done a bit more digging around in our analysis phase
    to see whether we want to “fill them in” (e.g., with the regular NAICS values/interpretations),
    flag them as being atypical for the SBA, or some combination thereof.
  prefs: []
  type: TYPE_NORMAL
- en: Example 7-12\. ppp_adding_naics.py
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_cleaning__transforming____span_class__keep_together__and_augmenting_data__span__CO7-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Use the `dtype='string'` parameter to force `pandas` to treat our entire dataset
    as strings; this will make later matching and comparison tasks more predictable.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_cleaning__transforming____span_class__keep_together__and_augmenting_data__span__CO7-2)'
  prefs: []
  type: TYPE_NORMAL
- en: If we don’t do this replacement, our data will match to unpredictable `NA` values
    from the *SBA-NAICS-data.csv* file.
  prefs: []
  type: TYPE_NORMAL
- en: Augmenting a dataset as we have in [Example 7-12](#ppp_adding_naics) can help
    us expand the types of questions we can use it to answer, as well as helping support
    faster, more comprehensive data analysis and interpretation. At the same time,
    anytime we introduce new data, we need to complete the same life cycle of evaluation,
    cleaning, transformation, and (maybe even) augmentation that we applied to our
    “primary” dataset. This means that we’ll always need to strike a balance between
    making our primary data more elaborate (and possibly useful) with the time and
    effort involved in finding and wrangling the “secondary” data that we use to augment
    it.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While the variety of data cleaning, transformation, and augmentation possibilities
    is as varied as both datasets and analysis possibilities, the primary goal of
    this chapter was to illustrate common issues in data cleaning, transformation,
    and augmentation and introduce some key methods for resolving them.
  prefs: []
  type: TYPE_NORMAL
- en: Before we move on to actually trying to generate insights with our data, however,
    we’re going to take a small “detour” in [Chapter 8](ch08.html#chapter8), which
    will focus on some programming best practices that can help us make sure our code
    is as clear, efficient, and effective as possible. Because while using Python
    to do data wrangling already lets us do things that would be impossible with other
    tools, optimizing our code for both use and reuse is another way to make sure
    we get the most out of each program and piece of code we write, too. In most cases,
    this means structuring our files so that they are more versatile, composable,
    and readable, as we’ll see right now!
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch07.html#idm45143404308272-marker)) While the data format for Citi Bike
    data files changed in early 2021, files from before that date still follow the
    format in these examples.
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch07.html#idm45143404288880-marker)) And even if they weren’t, we could
    always convert them to strings.
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](ch07.html#idm45143403643280-marker)) Unless, of course, you’re working
    in a team—then you need to consider everyone’s needs. You’ll be glad when it’s
    your turn to work with *their* code.
  prefs: []
  type: TYPE_NORMAL
- en: ^([4](ch07.html#idm45143403085296-marker)) Of course, Macs and PCs use a different
    “base” date because…*reasons*.
  prefs: []
  type: TYPE_NORMAL
- en: ^([5](ch07.html#idm45143403010368-marker)) Believe me, most professional programmers
    are looking things up online every five minutes.
  prefs: []
  type: TYPE_NORMAL
- en: ^([6](ch07.html#idm45143402992928-marker)) At first I was concerned that the
    `OriginatingLenderLocationID` might refer to an individual bank branch, for example.
  prefs: []
  type: TYPE_NORMAL
- en: ^([7](ch07.html#idm45143402866320-marker)) As a case in point, I didn’t even
    end up using this approach in the final code for [Example 7-10](#ppp_add_fingerprints),
    but I *did* find a use for it in [Example 7-12](#ppp_adding_naics)!
  prefs: []
  type: TYPE_NORMAL
- en: ^([8](ch07.html#idm45143402846432-marker)) Which leads us to [*https://naics.com/what-is-a-naics-code-why-do-i-need-one*](https://naics.com/what-is-a-naics-code-why-do-i-need-one).
  prefs: []
  type: TYPE_NORMAL
