["```py\nsentences = [\"It was the best of times\",\n             \"it was the worst of times\",\n             \"it was the age of wisdom\",\n             \"it was the age of foolishness\"]\n\n```", "```py\ntokenized_sentences = [[t for t in sentence.split()] for sentence in sentences]\nvocabulary = set([w for s in tokenized_sentences for w in s])\n\nimport pandas as pd\npd.DataFrame([[w, i] for i,w in enumerate(vocabulary)])\n\n```", "```py\ndef onehot_encode(tokenized_sentence):\n    return [1 if w in tokenized_sentence else 0 for w in vocabulary]\n\nonehot = [onehot_encode(tokenized_sentence)\n         for tokenized_sentence in tokenized_sentences]\n\nfor (sentence, oh) in zip(sentences, onehot):\n    print(\"%s: %s\" % (oh, sentence))\n\n```", "```py\n[0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1]: It was the best of times\n[1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0]: it was the worst of times\n[0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0]: it was the age of wisdom\n[0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0]: it was the age of foolishness\n\n```", "```py\nonehot_encode(\"the age of wisdom is the best of times\".split())\n\n```", "```py\n[0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1]\n\n```", "```py\nonehot_encode(\"John likes to watch movies. Mary likes movies too.\".split())\n\n```", "```py\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n\n```", "```py\npd.DataFrame(onehot, columns=vocabulary)\n\n```", "```py\nsim = [onehot[0][i] & onehot[1][i] for i in range(0, len(vocabulary))]\nsum(sim)\n\n```", "```py\n4\n\n```", "```py\nnp.dot(onehot[0], onehot[1])\n\n```", "```py\n4\n\n```", "```py\nnp.dot(onehot, np.transpose(onehot))\n\n```", "```py\narray([[6, 4, 3, 3],       # It was the best of times\n       [4, 6, 4, 4],       # it was the worst of times\n       [3, 4, 6, 5],       # it was the age of wisdom\n       [3, 4, 5, 6]])      # it was the age of foolishness\n\n```", "```py\nfrom sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer()\n\n```", "```py\nmore_sentences = sentences + \\\n                 [\"John likes to watch movies. Mary likes movies too.\",\n                  \"Mary also likes to watch football games.\"]\n\n```", "```py\ncv.fit(more_sentences)\n\n```", "```py\nCountVectorizer(analyzer='word', binary=False, decode_error='strict',\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                tokenizer=None, vocabulary=None)\n\n```", "```py\nprint(cv.get_feature_names())\n\n```", "```py\n['age', 'also', 'best', 'foolishness', 'football', 'games',\n 'it',  'john', 'likes', 'mary', 'movies', 'of', 'the', 'times',\n 'to', 'too', 'was', 'watch', 'wisdom', 'worst']\n\n```", "```py\ndt = cv.transform(more_sentences)\n\n```", "```py\ndt\n\n```", "```py\n<6x20 sparse matrix of type '<class 'numpy.int64'>'\nwith 38 stored elements in Compressed Sparse Row format>\n\n```", "```py\npd.DataFrame(dt.toarray(), columns=cv.get_feature_names())\n```", "```py\nfrom sklearn.metrics.pairwise import cosine_similarity\ncosine_similarity(dt[0], dt[1])\n\n```", "```py\narray([[0.83333333]])\n\n```", "```py\npd.DataFrame(cosine_similarity(dt, dt)))\n```", "```py\nfrom sklearn.feature_extraction.text import TfidfTransformer\ntfidf = TfidfTransformer()\ntfidf_dt = tfidf.fit_transform(dt)\npd.DataFrame(tfidf_dt.toarray(), columns=cv.get_feature_names())\n\n```", "```py\npd.DataFrame(cosine_similarity(tfidf_dt, tfidf_dt))\n\n```", "```py\nheadlines = pd.read_csv(\"abcnews-date-text.csv\", parse_dates=[\"publish_date\"])\nprint(len(headlines))\nheadlines.head()\n\n```", "```py\n1103663\n```", "```py\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ntfidf = TfidfVectorizer()\ndt = tfidf.fit_transform(headlines[\"headline_text\"])\n\n```", "```py\ndt\n\n```", "```py\n<1103663x95878 sparse matrix of type '<class 'numpy.float64'>'\nwith 7001357 stored elements in Compressed Sparse Row format>\n\n```", "```py\n%%time\ncosine_similarity(dt[0:10000], dt[0:10000])\n\n```", "```py\nCPU times: user 154 ms, sys: 261 ms, total: 415 ms\n\nWall time: 414 ms\n\narray([[1.      , 0.      , 0.      , ..., 0.        , 0.        , 0.        ],\n       [0.      , 1.      , 0.      , ..., 0.        , 0.        , 0.        ],\n       [0.      , 0.      , 1.      , ..., 0.        , 0.        , 0.        ],\n       ...,\n       [0.      , 0.      , 0.      , ..., 1.        , 0.16913596, 0.16792138],\n       [0.      , 0.      , 0.      , ..., 0.16913596, 1.        , 0.33258708],\n       [0.      , 0.      , 0.      , ..., 0.16792138, 0.33258708, 1.        ]])\n\n```", "```py\nfrom spacy.lang.en.stop_words import STOP_WORDS as stopwords\nprint(len(stopwords))\ntfidf = TfidfVectorizer(stop_words=stopwords)\ndt = tfidf.fit_transform(headlines[\"headline_text\"])\ndt\n\n```", "```py\n305\n<1103663x95600 sparse matrix of type '<class 'numpy.float64'>'\nwith 5644186 stored elements in Compressed Sparse Row format>\n\n```", "```py\ntfidf = TfidfVectorizer(stop_words=stopwords, min_df=2)\ndt = tfidf.fit_transform(headlines[\"headline_text\"])\ndt\n\n```", "```py\n<1103663x58527 sparse matrix of type '<class 'numpy.float64'>'\nwith 5607113 stored elements in Compressed Sparse Row format>\n\n```", "```py\ntfidf = TfidfVectorizer(stop_words=stopwords, min_df=.0001)\ndt = tfidf.fit_transform(headlines[\"headline_text\"])\ndt\n\n```", "```py\n<1103663x6772 sparse matrix of type '<class 'numpy.float64'>'\nwith 4816381 stored elements in Compressed Sparse Row format>\n\n```", "```py\ntfidf = TfidfVectorizer(stop_words=stopwords, max_df=0.1)\ndt = tfidf.fit_transform(headlines[\"headline_text\"])\ndt\n\n```", "```py\n<1103663x95600 sparse matrix of type '<class 'numpy.float64'>'\nwith 5644186 stored elements in Compressed Sparse Row format>\n\n```", "```py\nimport spacy\n\nnlp = spacy.load(\"en\")\nnouns_adjectives_verbs = [\"NOUN\", \"PROPN\", \"ADJ\", \"ADV\", \"VERB\"]\nfor i, row in headlines.iterrows():\n    doc = nlp(str(row[\"headline_text\"]))\n    headlines.at[i, \"lemmas\"] = \" \".join([token.lemma_ for token in doc])\n    headlines.at[i, \"nav\"] = \" \".join([token.lemma_ for token in doc\n                     if token.pos_ in nouns_adjectives_verbs])\n\n```", "```py\ntfidf = TfidfVectorizer(stop_words=stopwords)\ndt = tfidf.fit_transform(headlines[\"lemmas\"].map(str))\ndt\n\n```", "```py\n<1103663x71921 sparse matrix of type '<class 'numpy.float64'>'\nwith 5053610 stored elements in Compressed Sparse Row format>\n\n```", "```py\ntfidf = TfidfVectorizer(stop_words=stopwords)\ndt = tfidf.fit_transform(headlines[\"nav\"].map(str))\ndt\n\n```", "```py\n<1103663x68426 sparse matrix of type '<class 'numpy.float64'>'\nwith 4889344 stored elements in Compressed Sparse Row format>\n\n```", "```py\ntop_10000 = pd.read_csv(\"https://raw.githubusercontent.com/first20hours/\\\ngoogle-10000-english/master/google-10000-english.txt\", header=None)\ntfidf = TfidfVectorizer(stop_words=set(top_10000.iloc[:,0].values))\ndt = tfidf.fit_transform(headlines[\"nav\"].map(str))\ndt\n\n```", "```py\n<1103663x61630 sparse matrix of type '<class 'numpy.float64'>'\nwith 1298200 stored elements in Compressed Sparse Row format>\n\n```", "```py\ntfidf = TfidfVectorizer(stop_words=stopwords, ngram_range=(1,2), min_df=2)\ndt = tfidf.fit_transform(headlines[\"headline_text\"])\nprint(dt.shape)\nprint(dt.data.nbytes)\ntfidf = TfidfVectorizer(stop_words=stopwords, ngram_range=(1,3), min_df=2)\ndt = tfidf.fit_transform(headlines[\"headline_text\"])\nprint(dt.shape)\nprint(dt.data.nbytes)\n\n```", "```py\n(1103663, 559961)\n67325400\n(1103663, 747988)\n72360104\n\n```", "```py\ntfidf = TfidfVectorizer(ngram_range=(1,2),\n        stop_words=set(top_10000.iloc[:,0].values))\ndt = tfidf.fit_transform(headlines[\"nav\"].map(str))\ndt\n\n```", "```py\n<1103663x385857 sparse matrix of type '<class 'numpy.float64'>'\nwith 1753239 stored elements in Compressed Sparse Row format>\nCompared to the original bigram vectorization with min_df=2 above,\nthere are just 82,370 dimensions left from 67,325,400\n\n```", "```py\n# there are \"test\" headlines in the corpus\nstopwords.add(\"test\")\ntfidf = TfidfVectorizer(stop_words=stopwords, ngram_range=(1,2), min_df=2, \\\n                        norm='l2')\ndt = tfidf.fit_transform(headlines[\"headline_text\"])\n\n```", "```py\nmade_up = tfidf.transform([\"australia and new zealand discuss optimal apple \\\n size\"])\n\n```", "```py\nsim = cosine_similarity(made_up, dt)\n\n```", "```py\nheadlines.iloc[np.argmax(sim)]\n\n```", "```py\npublish_date           2011-08-17 00:00:00\nheadline_text    new zealand apple imports\nName: 633392, dtype: object\n\n```", "```py\n%%time\nnp.dot(dt[0:10000], np.transpose(dt[0:10000]))\n\n```", "```py\nCPU times: user 16.4 ms, sys: 0 ns, total: 16.4 ms\nWall time: 16 ms\n<10000x10000 sparse matrix of type '<class 'numpy.float64'>'\nwith 1818931 stored elements in Compressed Sparse Row format>\n\n```", "```py\n%%time\nbatch = 10000\nmax_sim = 0.0\nmax_a = None\nmax_b = None\nfor a in range(0, dt.shape[0], batch):\n    for b in range(0, a+batch, batch):\n        print(a, b)\n        r = np.dot(dt[a:a+batch], np.transpose(dt[b:b+batch]))\n        # eliminate identical vectors\n        # by setting their similarity to np.nan which gets sorted out\n        r[r > 0.9999] = np.nan\n        sim = r.max()\n        if sim > max_sim:\n            # argmax returns a single value which we have to\n            # map to the two dimensions\n            (max_a, max_b) = np.unravel_index(np.argmax(r), r.shape)\n            # adjust offsets in corpus (this is a submatrix)\n            max_a += a\n            max_b += b\n            max_sim = sim\n\n```", "```py\nCPU times: user 6min 12s, sys: 2.11 s, total: 6min 14s\nWall time: 6min 12s\n\n```", "```py\nprint(headlines.iloc[max_a])\nprint(headlines.iloc[max_b])\n\n```", "```py\npublish_date                                2014-09-18 00:00:00\nheadline_text    vline fails to meet punctuality targets report\nName: 904965, dtype: object\npublish_date                         2008-02-15 00:00:00\nheadline_text    vline fails to meet punctuality targets\nName: 364042, dtype: object\n\n```", "```py\ntfidf_word = TfidfVectorizer(stop_words=stopwords, min_df=1000)\ndt_word = tfidf_word.fit_transform(headlines[\"headline_text\"])\n\n```", "```py\nr = cosine_similarity(dt_word.T, dt_word.T)\nnp.fill_diagonal(r, 0)\n\n```", "```py\nvoc = tfidf_word.get_feature_names()\nsize = r.shape[0] # quadratic\nfor index in np.argsort(r.flatten())[::-1][0:40]:\n    a = int(index/size)\n    b = index%size\n    if a > b:  # avoid repetitions\n        print('\"%s\" related to \"%s\"' % (voc[a], voc[b]))\n\n```", "```py\n\"sri\" related to \"lanka\"\n\"hour\" related to \"country\"\n\"seekers\" related to \"asylum\"\n\"springs\" related to \"alice\"\n\"pleads\" related to \"guilty\"\n\"hill\" related to \"broken\"\n\"trump\" related to \"donald\"\n\"violence\" related to \"domestic\"\n\"climate\" related to \"change\"\n\"driving\" related to \"drink\"\n\"care\" related to \"aged\"\n\"gold\" related to \"coast\"\n\"royal\" related to \"commission\"\n\"mental\" related to \"health\"\n\"wind\" related to \"farm\"\n\"flu\" related to \"bird\"\n\"murray\" related to \"darling\"\n\"world\" related to \"cup\"\n\"hour\" related to \"2014\"\n\"north\" related to \"korea\"\n\n```"]