<html><head></head><body><section data-pdf-bookmark="Chapter 9. Introduction to Data Analysis" data-type="chapter" epub:type="chapter"><div class="chapter" id="chapter9">&#13;
<h1><span class="label">Chapter 9. </span>Introduction to Data Analysis</h1>&#13;
&#13;
&#13;
<p>So far, this book has focused mostly on the logistics of acquiring, assessing, transforming, and augmenting data. We’ve explored how to write code that can retrieve data from the internet, extract it from unfriendly formats, evaluate its completeness, and account for inconsistencies. We’ve even spent some time thinking about how to make sure that the tools we use to do all this—our Python scripts—are optimized to meet our needs, both now and in the future.</p>&#13;
&#13;
<p>At this point, though, it’s time to revisit the <em>why</em> of all this work. Back in <a data-type="xref" href="ch01.html#describing_data_wrangling">“What Is “Data Wrangling”?”</a>, I described the purpose of data wrangling as transforming “raw” data into something that can be used to generate insight and meaning. But unless we follow through with at least <em>some</em> degree of analysis, there’s no way to know if our wrangling efforts were sufficient—or what insights they might produce. In that sense, stopping our data wrangling work at the augmentation/transformation phase would be like setting up your mise en place and then walking out of the kitchen. You don’t spend hours carefully prepping vegetables and measuring ingredients unless you want to <em>cook</em>. And that’s what data analysis is: taking all that beautifully cleaned and prepared data and turning it into new insight and knowledge.</p>&#13;
&#13;
<p>If you fear we’re slipping into abstractions again, don’t worry—the fundamentals of data analysis are simple and concrete enough. Like our data quality assessments, however, they are about one part technical effort to four parts judgment. Yes, the basics of data analysis involve reassuring, 2 + 2 = 4–style math, but the insights depend on <em>interpreting</em> the outputs of those very straightforward formulas. And that’s where you need logic and research—along with human judgment and expertise—to bridge the gap.</p>&#13;
&#13;
<p>Over the course of this chapter, then, we’ll be exploring the basics of data analysis—specifically, the simple measures of <em>central tendency</em> and <em>distribution</em> that help us give data meaningful context. We’ll also go over the rules of thumb for making appropriate inferences about data based on these measures and the role that both numerical and <em>visual</em> analysis play in helping us understand the trends and anomalies within our dataset. Toward the end of the chapter, we’ll address the limits of data analysis and why it <em>always</em> takes more than traditional “data” to get from the “what” to the <em>why</em>. Along the way, of course, we’ll see how Python can help us in all these tasks and why it’s the right tool for everything from quick calculations to essential visualizations.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Context Is Everything" data-type="sect1"><div class="sect1" id="idm45143400265264">&#13;
<h1>Context Is Everything</h1>&#13;
&#13;
<p>If I<a data-primary="data analysis" data-secondary="context" data-tertiary="importance of" data-type="indexterm" id="data-analysis-context-importance"/><a data-primary="context" data-secondary="importance of" data-type="indexterm" id="context-importance"/><a data-primary="relationships" data-see="context" data-type="indexterm" id="idm45143399628752"/> offered to sell you an apple right now for $0.50, would you buy it? For the sake of this example, let’s imagine that you like apples, and you’re feeling like a snack. Also, this is a beautiful apple: shiny, fragrant, and heavy in the hand. Let’s also suppose you’re confident that I’m not trying to harm you with this apple. It’s just a nice, fresh, apple, for $0.50. Would you buy it?</p>&#13;
&#13;
<p>For most people, the answer is: <em>it depends</em>. Depends on what? Lots of things. No matter how much you trust me (and my almost-too-perfect apple), if someone standing next to me was selling also-very-good-looking apples for $0.25 each, you might buy one of those. Why? Well, obviously they’re cheaper, and even my <em>incredibly</em> awesome apple probably isn’t <em>twice</em> as awesome as the next one. At the same time, if your cousin was standing on the other side of me selling tasty apples for $0.60 each, you might well buy one of those instead, just to show support for your cousin’s new apple-selling start-up.</p>&#13;
&#13;
<p>This might seem like a pretty complicated decision-making process for choosing a piece of fruit, but in reality we make these kinds of choices all the time, and the results are sometimes surprising. Economists like <a href="https://danariely.com/all-about-dan">Dan Ariely</a> and <a href="https://timharford.com/articles/undercovereconomist">Tim Harford</a> have conducted research illustrating things like how influential a “free” gift is, even if it creates an added cost, or how our satisfaction with our own pay can go down when we learn what people around us are earning.<sup><a data-type="noteref" href="ch09.html#idm45143400237136" id="idm45143400237136-marker">1</a></sup> Most of our priorities and decisions depend on value judgment, and in order to make those effectively we need to know what our options are. Would I buy a fairy-tale-perfect apple for $0.50? Maybe. Probably not if I could get a really similar one at the next corner for half the price. But I probably would if I was in a hurry and had to walk a mile to get one otherwise. Though we all understand what we mean by it, a more precise way of saying “It depends” would be to say, “It depends on the <em>context</em>.”</p>&#13;
&#13;
<p>The importance of context is why a data point in isolation is meaningless; even if it is factually “true,” a single data point can’t help us make decisions. Generating and acquiring new knowledge, in general, is about connecting new information to information we already know. In other words, the knowledge isn’t “in the data” itself but in its <em>relationship</em> to other things. Since we can’t exhaustively explore the context of every situation where decisions are required (including your concerns about your cousin’s new apple startup and the importance of supporting family efforts), we often have to restrict ourselves to examining those parts of the context that we can (or have chosen to) consistently measure and quantify. In other words, we turn to data.</p>&#13;
&#13;
<p>How do we derive context from data? We do things like investigate its provenance, asking questions about who collected it and when and why—these answers help illuminate both what the data includes and what might be missing. And we look for ways to systematically compare each data point to the rest, to help understand how it conforms to—or breaks—any patterns that might exist across the dataset as a whole. Of course, none of this is likely to offer us definitive “answers,” but it will provide us with insights and ideas that we can share with others and use to inspire our <em>next</em> question about what’s happening in the world <a data-primary="data analysis" data-secondary="context" data-startref="data-analysis-context-importance" data-tertiary="importance of" data-type="indexterm" id="idm45143399603328"/><a data-primary="context" data-secondary="importance of" data-startref="context-importance" data-type="indexterm" id="idm45143399601840"/>around us.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Same but Different" data-type="sect1"><div class="sect1" id="samesame">&#13;
<h1>Same but Different</h1>&#13;
&#13;
<p>While <a data-primary="data analysis" data-secondary="context" data-tertiary="meaningfulness in" data-type="indexterm" id="idm45143400305376"/><a data-primary="context" data-secondary="meaningfulness in" data-type="indexterm" id="idm45143400304096"/>building context is essential for generating insight from data, how do we know <em>which</em> contexts matter? Given that there are infinite relationship <em>types</em> we could identify, even among the data points in a pretty small dataset, how do we decide which ones to pay attention to? Take, for example, the data in <a data-type="xref" href="ch02.html#page_loop">Example 2-9</a>, which was just a simple (fictional) list of page counts:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="n">page_counts</code> <code class="o">=</code> <code class="p">[</code><code class="mi">28</code><code class="p">,</code> <code class="mi">32</code><code class="p">,</code> <code class="mi">44</code><code class="p">,</code> <code class="mi">23</code><code class="p">,</code> <code class="mi">56</code><code class="p">,</code> <code class="mi">32</code><code class="p">,</code> <code class="mi">12</code><code class="p">,</code> <code class="mi">34</code><code class="p">,</code> <code class="mi">30</code><code class="p">]</code></pre>&#13;
&#13;
<p>Even with just this handful of numbers, there are lots of types of “context” we can imagine: we could describe it in terms of even versus odd values, for example, or which ones can be evenly divided by 8. The problem is, most of these relationships are not all that interesting. How can we tell which ones will be?</p>&#13;
&#13;
<p>It turns out, the human brain is pretty well wired to notice—and care about—two types of relationships in particular: sameness and differentness. Trends and anomalies in almost any type of stimulus—from <a href="https://archive.is/20130121151738/http://dbskeptic.com/2007/11/04/apophenia-definition-and-analysis">seeing patterns in clouds or lottery results</a> to <a href="https://csc2.ncsu.edu/faculty/healey/PP">quickly identifying a difference in orientation among similar objects</a>—tend to catch our attention. This means <em>trends</em> and <em>anomalies</em> are<a data-primary="trends" data-secondary="as meaningful" data-type="indexterm" id="idm45143398582480"/><a data-primary="anomalies" data-secondary="as meaningful" data-type="indexterm" id="idm45143398581472"/> interesting, almost by definition. So a pretty good place to start when we want to build meaningful context for our data is with the ways that individual records in a given dataset are similar to—or different from—each other.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section class="pagebreak-before less_space" data-pdf-bookmark="What’s Typical? Evaluating Central Tendency" data-type="sect1"><div class="sect1" id="idm45143398580160">&#13;
<h1>What’s Typical? Evaluating Central Tendency</h1>&#13;
&#13;
<p>What <a data-primary="data analysis" data-secondary="context" data-tertiary="trends" data-type="indexterm" id="data-analysis-context-trends"/><a data-primary="context" data-secondary="trends in" data-type="indexterm" id="context-trends"/><a data-primary="trends" data-secondary="types of" data-type="indexterm" id="trends"/><a data-primary="average" data-type="indexterm" id="average"/><a data-primary="central tendency" data-type="indexterm" id="central-tendency"/>does it mean for something to be “average”? When we use that term in day-to-day life, it’s often a stand-in for “unremarkable,” “expected,” or “typical.” Given its specifically <em>un</em>extraordinary associations, then, in many cases “average” can also be a synonym for “boring.”</p>&#13;
&#13;
<p>When it comes to analyzing data, however, it turns out that what’s “average” is actually what interests us, because it’s a basis for comparison—and comparisons are one thing humans care about a lot. Remember the research on wages? As humans, we want to know how things that affect us compare to what is “typical” for other people. So even if we never hope to <em>be</em> “average,” in general we still want to know both what it is and how our own experience compares.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="What’s That Mean?" data-type="sect2"><div class="sect2" id="idm45143399398816">&#13;
<h2>What’s That Mean?</h2>&#13;
&#13;
<p>You <a data-primary="arithmetic mean" data-type="indexterm" id="arithmetic-mean"/><a data-primary="mean (arithmetic)" data-type="indexterm" id="mean"/><a data-primary="central tendency" data-secondary="mean" data-type="indexterm" id="central-tendency-mean"/>may already be familiar with the process for calculating the “average” of a set of numbers: add them all up and divide by how many you have. That particular measure of <em>central tendency</em> is more precisely known as the <em>arithmetic mean</em>, and it’s calculated just the way you remember. So for our <code>page_counts</code> variable, the math would be:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="n">mean_pgs</code> <code class="o">=</code> <code class="p">(</code><code class="mi">28</code><code class="o">+</code><code class="mi">32</code><code class="o">+</code><code class="mi">44</code><code class="o">+</code><code class="mi">23</code><code class="o">+</code><code class="mi">56</code><code class="o">+</code><code class="mi">32</code><code class="o">+</code><code class="mi">12</code><code class="o">+</code><code class="mi">34</code><code class="o">+</code><code class="mi">30</code><code class="p">)</code><code class="o">/</code><code class="mi">9</code></pre>&#13;
&#13;
<p>And this would give us (roughly) a mean value of:</p>&#13;
&#13;
<pre data-type="programlisting">32.333333333333336</pre>&#13;
&#13;
<p>As a representation of the “typical” chapter length, this seems pretty reasonable: many of our chapters have page counts that are pretty close to 30, and there are even two chapters that have <em>precisely</em> 32 pages. So a mean per-chapter page count of just over 32 seems about right.</p>&#13;
&#13;
<p>While the mean may have served us well in this instance, however, there are many situations where using it as a measure of “typicality” can be deeply misleading. For example, let’s imagine one more, <em>really</em> long chapter (like, 100 pages long) got added to our book. Our method of calculating the mean would be the same:</p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="n">mean_pgs</code> <code class="o">=</code> <code class="p">(</code><code class="mi">28</code><code class="o">+</code><code class="mi">32</code><code class="o">+</code><code class="mi">44</code><code class="o">+</code><code class="mi">23</code><code class="o">+</code><code class="mi">56</code><code class="o">+</code><code class="mi">32</code><code class="o">+</code><code class="mi">12</code><code class="o">+</code><code class="mi">34</code><code class="o">+</code><code class="mi">30</code><code class="o">+</code><code class="mi">100</code><code class="p">)</code><code class="o">/</code><code class="mi">10</code></pre>&#13;
&#13;
<p>But now the mean value would be:</p>&#13;
&#13;
<pre data-type="programlisting">39.1</pre>&#13;
&#13;
<p>All of a sudden, our “average” chapter length has increased by almost 6 pages, even though we only added a single new chapter, and  fully <em>half</em> of our chapters are 28–34 pages long. Is a chapter that’s roughly 39 pages truly “typical” in this case? Not really.</p>&#13;
&#13;
<p>What we’re seeing even in this small example is that while in <em>some</em> cases the mean is a reasonable measure of “typicality,” it’s also heavily influenced by extreme values—and just one of these is enough to make it useless as a shorthand for what is “typical” of a dataset. But what are our alternatives?</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Embrace the Median" data-type="sect2"><div class="sect2" id="idm45143399400432">&#13;
<h2>Embrace the Median</h2>&#13;
&#13;
<p>Another <a data-primary="median" data-type="indexterm" id="median"/><a data-primary="central tendency" data-secondary="median" data-type="indexterm" id="central-tendency-median"/>way to think about “typical” values in a dataset is to figure out what is—quite literally—in the “middle.” In data analysis, the “middle” value in a series of records is known as the <em>median</em>, and we can find it with even <em>less</em> math than we used when calculating the mean: all you need to do is sort and count. For example, in our original set of chapter lengths, we would first sort the values from lowest to highest:<sup><a data-type="noteref" href="ch09.html#idm45143398320448" id="idm45143398320448-marker">2</a></sup></p>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="n">page_counts</code> <code class="o">=</code> <code class="p">[</code><code class="mi">28</code><code class="p">,</code> <code class="mi">32</code><code class="p">,</code> <code class="mi">44</code><code class="p">,</code> <code class="mi">23</code><code class="p">,</code> <code class="mi">56</code><code class="p">,</code> <code class="mi">32</code><code class="p">,</code> <code class="mi">12</code><code class="p">,</code> <code class="mi">34</code><code class="p">,</code> <code class="mi">30</code><code class="p">]</code>&#13;
<code class="n">page_counts</code><code class="o">.</code><code class="n">sort</code><code class="p">()</code>&#13;
<code class="k">print</code><code class="p">(</code><code class="n">page_counts</code><code class="p">)</code></pre>&#13;
&#13;
<p>Which gives us:</p>&#13;
&#13;
<pre data-type="programlisting">[12, 23, 28, 30, 32, 32, 34, 44, 56]</pre>&#13;
&#13;
<p>Now, all we have to do is choose the “middle” value—that is the one that is positioned halfway between the beginning and end of the list. Since this is a nine-item list, that will be the value in the fifth position (leaving four items on either side). Thus, the <em>median</em> value of our <code>page_count</code> dataset is 32.</p>&#13;
&#13;
<p>Now, let’s see what happens to the median when we add that extra-long chapter. Our sorted data will look like this:</p>&#13;
&#13;
<pre data-type="programlisting">[12, 23, 28, 30, 32, 32, 34, 44, 56, 100]</pre>&#13;
&#13;
<p>And what about the median? Since the list now has an <em>even</em> number of items, we can just take the <em>two</em> “middle” values, add them together, and divide by two.<sup><a data-type="noteref" href="ch09.html#idm45143399041152" id="idm45143399041152-marker">3</a></sup> In this case, that will be the values in positions 5 and 6, which are both 32. So our median value is (32 + 32) / 2 = 32. Even when we add our extra-long chapter, the median value is still the same!</p>&#13;
&#13;
<p>Now at first you might be thinking, “Hold on, this feels wrong. A whole new chapter was added—a really <em>long</em> chapter, but the median value didn’t change <em>at all</em>. Shouldn’t it move, at least a little bit?”</p>&#13;
&#13;
<p>The real difference between the mean and the median is that the mean is powerfully affected by the specific <em>values</em> in dataset—as in how high or low they are—while the median is influenced mostly by the <em>frequency</em> with which certain values appear. In a sense, the median is much closer to a “one value, one vote” sort of approach, whereas the mean lets the most extreme values speak “loudest.” Since our current goal is to understand what values are “typical” in our dataset, the median will usually be the <a data-primary="data analysis" data-secondary="context" data-startref="data-analysis-context-trends" data-tertiary="trends" data-type="indexterm" id="idm45143399005168"/><a data-primary="context" data-secondary="trends in" data-startref="context-trends" data-type="indexterm" id="idm45143399003632"/><a data-primary="trends" data-secondary="types of" data-startref="trends" data-type="indexterm" id="idm45143399002416"/><a data-primary="average" data-startref="average" data-type="indexterm" id="idm45143399001200"/><a data-primary="median" data-startref="median" data-type="indexterm" id="idm45143399000256"/><a data-primary="central tendency" data-startref="central-tendency" data-type="indexterm" id="idm45143398999312"/><a data-primary="central tendency" data-secondary="median" data-startref="central-tendency-median" data-type="indexterm" id="idm45143398998368"/>most representative choice.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Think Different: Identifying Outliers" data-type="sect1"><div class="sect1" id="idm45143398870032">&#13;
<h1>Think Different: Identifying Outliers</h1>&#13;
&#13;
<p>In <a data-type="xref" href="#samesame">“Same but Different”</a>, I <a data-primary="data analysis" data-secondary="context" data-tertiary="anomalies" data-type="indexterm" id="idm45143398994800"/><a data-primary="context" data-secondary="anomalies" data-type="indexterm" id="idm45143398993552"/><a data-primary="anomalies" data-type="indexterm" id="idm45143398992608"/><a data-primary="outliers" data-type="indexterm" id="idm45143398991936"/>noted that human beings, in general, are interested in “sameness” and “differentness.” In looking at our two possible measures of central tendency, we were exploring ways in which values in a dataset are similar. But what about the ways in which they are different? If we look again at our original <code>page_count</code> list, we’d probably feel confident that a 32-page chapter is reasonably “typical,” and maybe even a 30-, 28-, or 34-page chapter, too. But what about a 12-page chapter? Or a 56-page chapter? They certainly don’t seem typical, but how do we know which values are different <em>enough</em> to be truly “unusual?”</p>&#13;
&#13;
<p>This is where we have to start mixing math with human judgment. Measures of central tendency can be pretty unequivocally calculated, but determining which values in a dataset are truly unusual—that is, which are <em>anomalies</em> or <em>outliers</em>—cannot be definitively assessed with arithmetic alone. As datasets get larger and more complex, however, it gets harder for humans to interpret them effectively as sets of data points.<sup><a data-type="noteref" href="ch09.html#idm45143398988128" id="idm45143398988128-marker">4</a></sup> So how can we possibly apply human judgment to large datasets? We need to engage our largest and most comprehensive data-processing resource: the human visual system.<sup><a data-type="noteref" href="ch09.html#idm45143398983968" id="idm45143398983968-marker">5</a></sup></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Visualization for Data Analysis" data-type="sect1"><div class="sect1" id="vizforanalysis">&#13;
<h1>Visualization for Data Analysis</h1>&#13;
&#13;
<p>The <a data-primary="data analysis" data-secondary="visualization in" data-type="indexterm" id="data-analysis-visual"/><a data-primary="visualizations" data-secondary="in data analysis" data-secondary-sortas="data analysis" data-type="indexterm" id="visual-data-analysis"/>role of visualization in data work is twofold. On the one hand, visualization can be used to help us <em>analyze</em> and make sense of data; on the other, it can be used to <em>convey</em> the insights that we’ve generated from that analysis. Using data for the latter purpose—as a communication tool to share insights about our data with a broader audience—is something that we’ll explore in-depth in <a data-type="xref" href="ch10.html#chapter10">Chapter 10</a>. Here, we’re going to focus on the ways in which visualization can offer us insight into the data we have.</p>&#13;
&#13;
<p>In order to understand how visualization can help us identify extreme values in our data, we first need to look—quite literally—at the data itself. In this case, I don’t mean that we’re going to open up our CSV file and start reading through data records. Rather, we’re going to create a special kind of bar chart <a data-primary="data analysis" data-secondary="visualization in" data-tertiary="histograms" data-type="indexterm" id="data-analysis-visual-histogram"/><a data-primary="visualizations" data-secondary="in data analysis" data-secondary-sortas="data analysis" data-tertiary="histograms" data-type="indexterm" id="visual-data-analysis-histogram"/><a data-primary="histograms" data-type="indexterm" id="histogram"/>known as a <em>histogram</em>, in which each bar represents the number of times a particular value appears in our dataset. For example, we can see a very simple histogram of our (expanded) <code>page_count</code> data in <a data-type="xref" href="#page_count_histogram">Figure 9-1</a>.</p>&#13;
&#13;
<figure><div class="figure" id="page_count_histogram">&#13;
<img alt="A sample histogram" src="assets/ppdw_0901.png"/>&#13;
<h6><span class="label">Figure 9-1. </span>A basic histogram</h6>&#13;
</div></figure>&#13;
&#13;
<p>For a dataset as small as our <code>page_counts</code> example, a histogram can’t tell us much; in fact, a dataset with only 10 values arguably has too few data points for either the concepts of <em>central tendency</em> (like the <em>mean</em> and the <em>median</em>) or outliers to have much meaning. Even so, in <a data-type="xref" href="#page_count_histogram">Figure 9-1</a> you can see the <em>beginnings</em> of what looks like a pattern: the two chapters of equal length form a double-height spike, with the unique page-lengths of most of the remaining chapters becoming single-height bars clustered reasonably close it. <em>Way</em> out on the right end, meanwhile, is our 100-page chapter, with no other values anywhere near it. While we might have already been tempted to conclude that the 100-page chapter was an anomaly—or <em>outlier</em>—based on the math, this histogram certainly reinforces that interpretation.</p>&#13;
&#13;
<p>Of course, in order to really appreciate the power of visualization for data analysis, we’ll want to look at a dataset with values we couldn’t even hope to “eyeball” the way that we did with our list of page counts. Fortunately, the Paycheck Protection Program (PPP) <a data-primary="PPP (Paycheck Protection Program) example" data-secondary="histograms" data-type="indexterm" id="ppp-histogram"/><a data-primary="histograms" data-secondary="PPP example" data-type="indexterm" id="histogram-ppp"/>data that we worked through in <a data-type="xref" href="ch06.html#chapter6">Chapter 6</a> is <em>definitely</em> not lacking in this regard, since it contains hundreds of <em>thousands</em> of loan records. To see what we can learn about what is and isn’t “typical” in the loan amounts approved through the PPP, we’ll write a quick script to generate a histogram of the currently approved loan values in the PPP. Then, we’ll label that histogram with both the mean and median values in order to see how well each might serve as a potential measure of central tendency. After that, we’ll return to the question of identifying possible outliers in the approved loan amounts, using both our visualization of the data and some math to back it up.</p>&#13;
&#13;
<p>For this to work, we’ll once again be leveraging some powerful Python libraries—specifically, <em>matplotlib</em> and <em>seaborn</em>—both of which have functions for calculating and visualizing data. While <em>matplotlib</em> remains<a data-primary="matplotlib library" data-type="indexterm" id="idm45143398952304"/><a data-primary="seaborn library" data-type="indexterm" id="idm45143398203184"/><a data-primary="Python" data-secondary="histograms, creating" data-type="indexterm" id="python-histogram-create"/> the foundational library for creating charts and graphs in Python, we’re also using <em>seaborn</em> for its helpful support of more advanced calculations and formats. Because the two are highly compatible (<em>seaborn</em> is actually built on <em>top</em> of <em>matplotlib</em>), this combination will offer us the flexibility we need both to quickly create the basic visualizations we need here and also customize them for presenting data effectively in <a data-type="xref" href="ch10.html#chapter10">Chapter 10</a>.</p>&#13;
&#13;
<p>For now, though, let’s focus on the analytical size of our visualization process. We’ll start this by creating a basic histogram of our PPP loan data using the <code>CurrentApprovalAmount</code> values. We’ll also add mean and median lines for more context, as shown in <a data-type="xref" href="#ppp_loan_central_measures">Example 9-1</a>.</p>&#13;
<div data-type="example" id="ppp_loan_central_measures">&#13;
<h5><span class="label">Example 9-1. </span>ppp_loan_central_measures.py</h5>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="c1"># `pandas` for reading and assessing our data</code><code>&#13;
</code><code class="kn">import</code><code> </code><code class="nn">pandas</code><code> </code><code class="kn">as</code><code> </code><code class="nn">pd</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># `seaborn` for its built-in themes and chart types</code><code>&#13;
</code><code class="kn">import</code><code> </code><code class="nn">seaborn</code><code> </code><code class="kn">as</code><code> </code><code class="nn">sns</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># `matplotlib` for customizing visual details</code><code>&#13;
</code><code class="kn">import</code><code> </code><code class="nn">matplotlib.pyplot</code><code> </code><code class="kn">as</code><code> </code><code class="nn">plt</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># read in our data</code><code>&#13;
</code><code class="n">ppp_data</code><code> </code><code class="o">=</code><code> </code><code class="n">pd</code><code class="o">.</code><code class="n">read_csv</code><code class="p">(</code><code class="s1">'</code><code class="s1">public_150k_plus_221.csv</code><code class="s1">'</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># set a basic color theme for our visualization</code><code>&#13;
</code><code class="n">sns</code><code class="o">.</code><code class="n">set_theme</code><code class="p">(</code><code class="n">style</code><code class="o">=</code><code class="s2">"</code><code class="s2">whitegrid</code><code class="s2">"</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># use the built-in `mean()` and `median()` methods in `pandas</code><code>&#13;
</code><code class="n">mean</code><code> </code><code class="o">=</code><code> </code><code class="n">ppp_data</code><code class="p">[</code><code class="s1">'</code><code class="s1">CurrentApprovalAmount</code><code class="s1">'</code><code class="p">]</code><code class="o">.</code><code class="n">mean</code><code class="p">(</code><code class="p">)</code><code> </code><a class="co" href="#callout_introduction_to_data_analysis_CO1-1" id="co_introduction_to_data_analysis_CO1-1"><img alt="1" src="assets/1.png"/></a><code>&#13;
</code><code class="n">median</code><code> </code><code class="o">=</code><code> </code><code class="n">ppp_data</code><code class="p">[</code><code class="s1">'</code><code class="s1">CurrentApprovalAmount</code><code class="s1">'</code><code class="p">]</code><code class="o">.</code><code class="n">median</code><code class="p">(</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># create a histogram of the values in the `CurrentApprovalAmount` column</code><code>&#13;
</code><code class="n">approved_loan_plot</code><code> </code><code class="o">=</code><code> </code><code class="n">sns</code><code class="o">.</code><code class="n">histplot</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="n">ppp_data</code><code class="p">,</code><code> </code><code class="n">x</code><code class="o">=</code><code class="s2">"</code><code class="s2">CurrentApprovalAmount</code><code class="s2">"</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># get the min and max y-values on our histogram</code><code>&#13;
</code><code class="n">y_axis_range</code><code> </code><code class="o">=</code><code> </code><code class="n">approved_loan_plot</code><code class="o">.</code><code class="n">get_ylim</code><code class="p">(</code><code class="p">)</code><code> </code><a class="co" href="#callout_introduction_to_data_analysis_CO1-2" id="co_introduction_to_data_analysis_CO1-2"><img alt="2" src="assets/2.png"/></a><code>&#13;
</code><code>&#13;
</code><code class="c1"># add the vertical lines at the correct locations</code><code>&#13;
</code><code class="n">approved_loan_plot</code><code class="o">.</code><code class="n">vlines</code><code class="p">(</code><code class="n">mean</code><code class="p">,</code><code> </code><code class="mi">0</code><code class="p">,</code><code> </code><code class="n">y_axis_range</code><code class="p">[</code><code class="mi">1</code><code class="p">]</code><code class="p">,</code><code> </code><code class="n">color</code><code class="o">=</code><code class="s1">'</code><code class="s1">crimson</code><code class="s1">'</code><code class="p">,</code><code> </code><code class="n">ls</code><code class="o">=</code><code class="s1">'</code><code class="s1">:</code><code class="s1">'</code><code class="p">)</code><code> </code><a class="co" href="#callout_introduction_to_data_analysis_CO1-3" id="co_introduction_to_data_analysis_CO1-3"><img alt="3" src="assets/3.png"/></a><code>&#13;
</code><code class="n">approved_loan_plot</code><code class="o">.</code><code class="n">vlines</code><code class="p">(</code><code class="n">median</code><code class="p">,</code><code> </code><code class="mi">0</code><code class="p">,</code><code> </code><code class="n">y_axis_range</code><code class="p">[</code><code class="mi">1</code><code class="p">]</code><code class="p">,</code><code> </code><code class="n">color</code><code class="o">=</code><code class="s1">'</code><code class="s1">green</code><code class="s1">'</code><code class="p">,</code><code> </code><code class="n">ls</code><code class="o">=</code><code class="s1">'</code><code class="s1">-</code><code class="s1">'</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># the matplotlib `show()` method actually renders the visualization</code><code>&#13;
</code><code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">(</code><code class="p">)</code><code> </code><a class="co" href="#callout_introduction_to_data_analysis_CO1-4" id="co_introduction_to_data_analysis_CO1-4"><img alt="4" src="assets/4.png"/></a></pre></div>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_introduction_to_data_analysis_CO1-1" id="callout_introduction_to_data_analysis_CO1-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>The <code>CurrentApprovalAmount</code> column in our PPP data tells us the dollar amount of each loan that is currently approved (whether or not it has been disbursed).</p></dd>&#13;
<dt><a class="co" href="#co_introduction_to_data_analysis_CO1-2" id="callout_introduction_to_data_analysis_CO1-2"><img alt="2" src="assets/2.png"/></a></dt>&#13;
<dd><p>The <code>get_ylim()</code> method returns the lowest and highest y-axis value as a list. We’ll mostly be using this to set a legible length for our mean and median lines.</p></dd>&#13;
<dt><a class="co" href="#co_introduction_to_data_analysis_CO1-3" id="callout_introduction_to_data_analysis_CO1-3"><img alt="3" src="assets/3.png"/></a></dt>&#13;
<dd><p>We can add vertical lines anywhere on our histogram (or other visualization) by specifying the “x” position, “y” starting point, “y” ending point, color, and line style. Note that the units for “x” and “y” values are relative to the dataset, <em>not</em> the visual size of the chart.</p></dd>&#13;
<dt><a class="co" href="#co_introduction_to_data_analysis_CO1-4" id="callout_introduction_to_data_analysis_CO1-4"><img alt="4" src="assets/4.png"/></a></dt>&#13;
<dd><p>While calling the <em>matplotlib</em> <code>show()</code> method is not explicitly required in Jupyter notebooks, like <code>print()</code> statements, I prefer to include them for clarity and consistency. By default, the chart will render in “interactive” mode (in Jupyter, you’ll need to include the <code>%matplotlib notebook</code> “magic” command, as I have in the provided files), which allows us to zoom, pan, and otherwise explore our histogram in detail without writing more code.</p></dd>&#13;
</dl>&#13;
&#13;
<p>Most likely, you’re looking at the chart that displayed as a result of running this script (which hopefully looks something like <a data-type="xref" href="#ppp_loan_histogram">Figure 9-2</a>) and thinking, “Now what?” Admittedly, this initial visualization seems a bit lackluster—if not downright confusing. Fear not! If anything, take this as your first case study in why visualization for analysis and visualization for communication are <em>not</em> one and the same. Analytical visualizations like this one, for example, require far more effort to read, understand, and refine than <em>any</em> visualization we would want to use for general communications. For generating insight about our data, however, this is actually just the right place &#13;
<span class="keep-together">to start.</span></p>&#13;
&#13;
<p>Before we forge ahead with our data analysis, though, let’s take a moment to appreciate the distinctly old-school—but incredibly useful—interface that Python has given us for this chart. Rather than just a static image, our Python script has <em>also</em> given us an entire toolbar (shown in <a data-type="xref" href="#ppp_loan_histogram">Figure 9-2</a>) that we can use to interact with it: to zoom, pan, modify, and even save the output. While of course we can (and eventually will) customize the contents and aesthetics of this chart using code, the fact that we can effectively explore our data without having to constantly modify and rerun our code is a huge time-saver. To get a feel for what’s possible, take a few minutes to play with the controls yourself. When you’re ready to move on with our data analysis, just click the “home” icon to return the chart to its initial view and follow along <a data-primary="PPP (Paycheck Protection Program) example" data-secondary="histograms" data-startref="ppp-histogram" data-type="indexterm" id="idm45143397957568"/><a data-primary="histograms" data-secondary="PPP example" data-startref="histogram-ppp" data-type="indexterm" id="idm45143397956480"/><a data-primary="Python" data-secondary="histograms, creating" data-startref="python-histogram-create" data-type="indexterm" id="idm45143397955392"/>in the sections that follow.</p>&#13;
&#13;
<figure><div class="figure" id="ppp_loan_histogram">&#13;
<img alt="PPP loan histogram" src="assets/ppdw_0902.png"/>&#13;
<h6><span class="label">Figure 9-2. </span>PPP loan histogram</h6>&#13;
</div></figure>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="What’s Our Data’s Shape? Understanding Histograms" data-type="sect2"><div class="sect2" id="idm45143397952304">&#13;
<h2>What’s Our Data’s Shape? Understanding Histograms</h2>&#13;
&#13;
<p>When <a data-primary="histograms" data-secondary="shape of" data-type="indexterm" id="histograms-shape"/>we were working with our data in a table-type format, we tended to think of its “shape” in terms of the number of rows and columns it had (this is actually <em>exactly</em> what the <code>pandas.shape</code> property of a DataFrame returns). In this context, the “shape” we’re interested in is the literal shape of the histogram, which will help us identify potentially interesting or important patterns or anomalies. Some of the first things we’ll look for in these instances are:</p>&#13;
<dl>&#13;
<dt>Symmetry</dt>&#13;
<dd>&#13;
<p>Is our data vertically symmetrical? That is, could we draw a vertical line somewhere over our visualized data such that the pattern of bars on one side looks (roughly) like a reflection of those on the other?</p>&#13;
</dd>&#13;
<dt>Density</dt>&#13;
<dd>&#13;
<p>Where <a data-primary="density (of data)" data-type="indexterm" id="idm45143397946048"/><a data-primary="clusters" data-type="indexterm" id="idm45143397945440"/>are most of our data values clustered (if anywhere)? Are there multiple clusters or just one?</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>Unsurprisingly, these questions are about more than aesthetics. The shape of a dataset’s histogram illustrates what is typically described as its <em>distribution</em>. Because <a data-primary="distributions" data-type="indexterm" id="idm45143397943184"/>certain distributions have specific properties, we can use our data’s <em>distribution</em> to help us identify what is typical, what is unusual, and what deserves further <a data-primary="data analysis" data-secondary="visualization in" data-startref="data-analysis-visual-histogram" data-tertiary="histograms" data-type="indexterm" id="idm45143397942064"/><a data-primary="visualizations" data-secondary="in data analysis" data-secondary-sortas="data analysis" data-startref="visual-data-analysis-histogram" data-tertiary="histograms" data-type="indexterm" id="idm45143397940736"/><a data-primary="histograms" data-startref="histogram" data-type="indexterm" id="idm45143397939168"/><a data-primary="histograms" data-secondary="shape of" data-startref="histograms-shape" data-type="indexterm" id="idm45143397938320"/>scrutiny.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="The Significance of Symmetry" data-type="sect2"><div class="sect2" id="idm45143397936976">&#13;
<h2>The Significance of Symmetry</h2>&#13;
&#13;
<p>In<a data-primary="data analysis" data-secondary="visualization in" data-tertiary="symmetry" data-type="indexterm" id="data-analysis-visual-symmetry"/><a data-primary="visualizations" data-secondary="in data analysis" data-secondary-sortas="data analysis" data-tertiary="symmetry" data-type="indexterm" id="visual-data-analysis-symmetry"/><a data-primary="histograms" data-secondary="symmetry" data-type="indexterm" id="histogram-symmetry"/><a data-primary="distributions" data-secondary="symmetry in" data-type="indexterm" id="distribution-symmetry"/><a data-primary="symmetry" data-type="indexterm" id="symmetry"/> the natural world, symmetry is a common occurrence. Plants and animals tend to be symmetrical in many ways; for example, a dog’s face and an oak leaf both exhibit what’s known<a data-primary="bilateral symmetry" data-type="indexterm" id="idm45143397928560"/> as <em>bilateral symmetry</em>—what we might describe as one side being a “mirror image” of the other. Across populations of living things, however, there is also often symmetry in the distribution of certain physical characteristics, like height or wing length. Our histogram lets us observe this symmetry firsthand, by illustrating the <em>frequency</em> of specific heights or wing lengths within a population. A classic example of this is shown in <a data-type="xref" href="#housefly_wings">Figure 9-3</a>, which shows the length of housefly wings as measured by a team of biologists in the mid-20th century.<sup><a data-type="noteref" href="ch09.html#idm45143397925760" id="idm45143397925760-marker">6</a></sup></p>&#13;
&#13;
<p>The symmetrical bell curve shown in <a data-type="xref" href="#housefly_wings">Figure 9-3</a> is also sometimes described<a data-primary="normal (Gaussian) distribution" data-type="indexterm" id="normal-distribution"/><a data-primary="standard (Gaussian) distribution" data-type="indexterm" id="standard-distribution"/><a data-primary="distributions" data-secondary="Gaussian" data-type="indexterm" id="dist-gaussian"/><a data-primary="Gaussian distribution" data-type="indexterm" id="gaussian"/> as the “normal,” “standard,” or “Gaussian” distribution. If you’ve ever had an academic grade “curved,” this was the distribution that the grades in your cohort were being transformed to fit: one with very few scores at the top or bottom and most of them lumped in the middle.</p>&#13;
&#13;
<p>The power of the Gaussian distribution is not just in its pretty shape, however; it’s in what that shape means we can <em>do</em>. Datasets that demonstrate Gaussian distributions can be both described and compared to one another in ways that nonsymmetrical distributions cannot, because we can meaningfully calculate two measures in particular: the <em>standard deviation</em>, which<a data-primary="standard deviation" data-type="indexterm" id="standard-dev"/><a data-primary="z-score" data-type="indexterm" id="z-score"/> quantifies the numerical range of data values within which most of them can be found, and each value’s <em>z-score</em>, which describes its distance from the mean in terms of standard deviations. Because of the fundamental symmetry of Gaussian distributions, we can use the <em>standard deviation</em> and the <em>z-score</em> to compare two sets of functionally similar data <em>even if they use different scales</em>. For example, if student grades demonstrate a Gaussian distribution, we can calculate and compare individual students’ z-scores (that is, their performance relative to their cohort) even across different classes and instructors who may use different grading rubrics. In other words, even if the mean of student grades for one instructor is in the 90s and for another instructor in the 70s, if both sets of students’ grades are truly Gaussian in their distribution, we can still determine which students are doing the best or need the most help across cohorts—something the <em>nominal</em> grades (e.g., 74 or 92) could never tell us.</p>&#13;
&#13;
<figure><div class="figure" id="housefly_wings">&#13;
<img alt="Length of Housefly Wings" src="assets/ppdw_0903.png"/>&#13;
<h6><span class="label">Figure 9-3. </span>Length of housefly wings</h6>&#13;
</div></figure>&#13;
&#13;
<p>These characteristics also inform how we can  think about measuring central tendency and outliers. For example, in a “perfect” Gaussian distribution, the mean and the median will have the same value. What’s more, a value’s z-score gives us a quick way of identifying how typical or unusual that particular value is, because the percentage of data values that we expect to have a given z-score is well-defined. Confused yet? Don’t worry. Just like any other complex data relationship, this all makes much more sense if we visualize it.</p>&#13;
&#13;
<figure><div class="figure" id="gaussian_distro">&#13;
<img alt="The Gaussian distribution, showing what percentage of values exist within 1, 2 and 3 standard deviations (σ) from the mean." src="assets/ppdw_0904.png"/>&#13;
<h6><span class="label">Figure 9-4. </span>The Gaussian distribution, showing what percentage of values exist within 1, 2, and 3 standard deviations (σ) from the mean</h6>&#13;
</div></figure>&#13;
&#13;
<p>As you can see in <a data-type="xref" href="#gaussian_distro">Figure 9-4</a>,<sup><a data-type="noteref" href="ch09.html#idm45143397906944" id="idm45143397906944-marker">7</a></sup> if our data’s distribution is Gaussian, more than two-thirds of the data values (34.1% + 34.1% = 68.2%) can be found within one standard deviation (often designated as it is here, by the Greek letter σ) of the mean. Another 27.2% can be found between one and two standard deviations from the mean, and a final 4.2% can be found between two and three standard deviations from the mean. This means that for a Gaussian distribution, <em>99.7% of all values can be found within 3 standard deviations of the mean</em>.</p>&#13;
&#13;
<p>So what? Well, <a data-primary="central tendency" data-secondary="in Gaussian distribution" data-secondary-sortas="Gaussian distribution" data-type="indexterm" id="idm45143397903264"/><a data-primary="anomalies" data-secondary="in Gaussian distribution" data-secondary-sortas="Gaussian distribution" data-type="indexterm" id="idm45143397902288"/><a data-primary="outliers" data-secondary="in Gaussian distribution" data-secondary-sortas="Gaussian distribution" data-type="indexterm" id="idm45143397901200"/>remember that one of our fundamental objectives in data analysis is to understand what values are typical for our dataset and which ones are truly extreme. While the mean and the median offer a quick shorthand for a dataset’s “typical” value, measures like the standard deviation—and the z-scores we can calculate from it—help us systematically evaluate which values might or might not be truly unusual.</p>&#13;
&#13;
<p>Unsurprisingly, <a data-primary="Python" data-secondary="standard deviation and z-score" data-type="indexterm" id="python-std-dev-zscore"/>calculating these values using Python is quite straightforward. Using either <em>pandas</em> or the <em>statistics</em> library, we can quickly find the value of the standard deviation for our dataset (σ) and then use it to place lines over our histogram where the relevant z-score values are. For this example, we’ll build on the data used to generate <a data-type="xref" href="#housefly_wings">Figure 9-3</a>, as shown in <a data-type="xref" href="#wing_length_with_sd">Example 9-2</a>.</p>&#13;
<div class="pagebreak-before less_space" data-type="example" id="wing_length_with_sd">&#13;
<h5><span class="label">Example 9-2. </span>wing_length_with_sd.py</h5>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="c1"># `pandas` to read in our data</code><code>&#13;
</code><code class="kn">import</code><code> </code><code class="nn">pandas</code><code> </code><code class="kn">as</code><code> </code><code class="nn">pd</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># `seaborn` for built-in themes and chart types</code><code>&#13;
</code><code class="kn">import</code><code> </code><code class="nn">seaborn</code><code> </code><code class="kn">as</code><code> </code><code class="nn">sns</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># `matplotlib` for customizing visual details</code><code>&#13;
</code><code class="kn">import</code><code> </code><code class="nn">matplotlib.pyplot</code><code> </code><code class="kn">as</code><code> </code><code class="nn">plt</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># `statistics` easily calculating statistical measures</code><code>&#13;
</code><code class="kn">import</code><code> </code><code class="nn">statistics</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># read in our data</code><code>&#13;
</code><code class="n">wing_data</code><code> </code><code class="o">=</code><code> </code><code class="n">pd</code><code class="o">.</code><code class="n">read_csv</code><code class="p">(</code><code class="s1">'</code><code class="s1">wing_length - s057.csv</code><code class="s1">'</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># set a basic color theme for our visualization</code><code>&#13;
</code><code class="n">sns</code><code class="o">.</code><code class="n">set_theme</code><code class="p">(</code><code class="n">style</code><code class="o">=</code><code class="s2">"</code><code class="s2">white</code><code class="s2">"</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># create the histogram, allowing `seaborn` to choose default "bin" values</code><code>&#13;
</code><code class="n">wing_plot</code><code> </code><code class="o">=</code><code> </code><code class="n">sns</code><code class="o">.</code><code class="n">histplot</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="n">wing_data</code><code class="p">,</code><code> </code><code class="n">x</code><code class="o">=</code><code class="s2">"</code><code class="s2">wing_length (0.1mm)</code><code class="s2">"</code><code class="p">,</code><code> </code><code class="n">kde</code><code class="o">=</code><code class="s2">"</code><code class="s2">True</code><code class="s2">"</code><code class="p">)</code><code> </code><a class="co" href="#callout_introduction_to_data_analysis_CO2-1" id="co_introduction_to_data_analysis_CO2-1"><img alt="1" src="assets/1.png"/></a><code>&#13;
</code><code>&#13;
</code><code class="c1"># calculate the standard deviation via the `statistics` `stdev()` method</code><code>&#13;
</code><code class="n">sd</code><code> </code><code class="o">=</code><code> </code><code class="n">statistics</code><code class="o">.</code><code class="n">stdev</code><code class="p">(</code><code class="n">wing_data</code><code class="p">[</code><code class="s1">'</code><code class="s1">wing_length (0.1mm)</code><code class="s1">'</code><code class="p">]</code><code class="p">)</code><code> </code><a class="co" href="#callout_introduction_to_data_analysis_CO2-2" id="co_introduction_to_data_analysis_CO2-2"><img alt="2" src="assets/2.png"/></a><code>&#13;
</code><code>&#13;
</code><code class="c1"># get the min and max y-values on our histogram</code><code>&#13;
</code><code class="n">y_axis_range</code><code> </code><code class="o">=</code><code> </code><code class="n">wing_plot</code><code class="o">.</code><code class="n">get_ylim</code><code class="p">(</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># plot the mean as a solid line</code><code>&#13;
</code><code class="n">mean</code><code> </code><code class="o">=</code><code> </code><code class="n">wing_data</code><code class="p">[</code><code class="s1">'</code><code class="s1">wing_length (0.1mm)</code><code class="s1">'</code><code class="p">]</code><code class="o">.</code><code class="n">mean</code><code class="p">(</code><code class="p">)</code><code>&#13;
</code><code class="n">wing_plot</code><code class="o">.</code><code class="n">vlines</code><code class="p">(</code><code class="n">mean</code><code class="p">,</code><code> </code><code class="mi">0</code><code class="p">,</code><code> </code><code class="n">y_axis_range</code><code class="p">[</code><code class="mi">1</code><code class="p">]</code><code class="p">,</code><code> </code><code class="n">color</code><code class="o">=</code><code class="s1">'</code><code class="s1">gray</code><code class="s1">'</code><code class="p">,</code><code> </code><code class="n">ls</code><code class="o">=</code><code class="s1">'</code><code class="s1">-</code><code class="s1">'</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># plot the three standard deviation boundary lines on either side of the mean</code><code>&#13;
</code><code class="k">for</code><code> </code><code class="n">i</code><code> </code><code class="ow">in</code><code> </code><code class="nb">range</code><code class="p">(</code><code class="o">-</code><code class="mi">3</code><code class="p">,</code><code class="mi">4</code><code class="p">)</code><code class="p">:</code><code> </code><a class="co" href="#callout_introduction_to_data_analysis_CO2-3" id="co_introduction_to_data_analysis_CO2-3"><img alt="3" src="assets/3.png"/></a><code>&#13;
</code><code>&#13;
</code><code>    </code><code class="c1"># find the current boundary value</code><code>&#13;
</code><code>    </code><code class="n">z_value</code><code> </code><code class="o">=</code><code> </code><code class="n">mean</code><code> </code><code class="o">+</code><code> </code><code class="p">(</code><code class="n">i</code><code class="o">*</code><code class="n">sd</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code>    </code><code class="c1"># don't draw a second line over the mean line</code><code>&#13;
</code><code>    </code><code class="k">if</code><code> </code><code class="n">z_value</code><code> </code><code class="o">!=</code><code> </code><code class="n">mean</code><code class="p">:</code><code>&#13;
</code><code>&#13;
</code><code>        </code><code class="c1"># plot a dotted gray line at each boundary value</code><code>&#13;
</code><code>        </code><code class="n">wing_plot</code><code class="o">.</code><code class="n">vlines</code><code class="p">(</code><code class="n">z_value</code><code class="p">,</code><code> </code><code class="mi">0</code><code class="p">,</code><code> </code><code class="n">y_axis_range</code><code class="p">[</code><code class="mi">1</code><code class="p">]</code><code class="p">,</code><code> </code><code class="n">color</code><code class="o">=</code><code class="s1">'</code><code class="s1">gray</code><code class="s1">'</code><code class="p">,</code><code> </code><code class="n">ls</code><code class="o">=</code><code class="s1">'</code><code class="s1">:</code><code class="s1">'</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># show the plot!</code><code>&#13;
</code><code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">(</code><code class="p">)</code></pre></div>&#13;
<dl class="calloutlist pagebreak-before less_space">&#13;
<dt><a class="co" href="#co_introduction_to_data_analysis_CO2-1" id="callout_introduction_to_data_analysis_CO2-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>Each “bin” is a range of <em>actual</em> data values that will be lumped together into a single histogram bar; the <code>kde</code> parameter is what adds a smoothed line to our visualization. This line approximates the pattern we would expect if our dataset had infinite data points.</p></dd>&#13;
<dt><a class="co" href="#co_introduction_to_data_analysis_CO2-2" id="callout_introduction_to_data_analysis_CO2-2"><img alt="2" src="assets/2.png"/></a></dt>&#13;
<dd><p>We could also have used the <em>pandas</em> <code>std()</code> method: <code>wing_data['wing_length (0.1mm)'].std()</code>.</p></dd>&#13;
<dt><a class="co" href="#co_introduction_to_data_analysis_CO2-3" id="callout_introduction_to_data_analysis_CO2-3"><img alt="3" src="assets/3.png"/></a></dt>&#13;
<dd><p>Recall that our loop will stop <em>before</em> the second value provided to <code>range()</code>, so to get three positive lines, we set the second value to <code>4</code>. By starting with a negative number, we actually <em>subtract</em> from the mean at first—which we want to do because we want to capture values both above <em>and</em> below the<a data-primary="normal (Gaussian) distribution" data-startref="normal-distribution" data-type="indexterm" id="idm45143397614608"/><a data-primary="standard (Gaussian) distribution" data-startref="standard-distribution" data-type="indexterm" id="idm45143397601376"/><a data-primary="distributions" data-secondary="Gaussian" data-startref="dist-gaussian" data-type="indexterm" id="idm45143397600528"/><a data-primary="Gaussian distribution" data-startref="gaussian" data-type="indexterm" id="idm45143397599440"/><a data-primary="standard deviation" data-startref="standard-dev" data-type="indexterm" id="idm45143397598592"/><a data-primary="z-score" data-startref="z-score" data-type="indexterm" id="idm45143397597744"/><a data-primary="Python" data-secondary="standard deviation and z-score" data-startref="python-std-dev-zscore" data-type="indexterm" id="idm45143397596896"/> mean.</p></dd>&#13;
</dl>&#13;
&#13;
<p>As you review the output of <a data-type="xref" href="#wing_length_with_sd">Example 9-2</a> you might be thinking, “Great, we’ve drawn some lines on data about bugs. How will this help me interpret <em>real</em> data?” After all, the prototypically Gaussian distribution of this housefly wing-length data doesn’t look much like the output we got when we charted our PPP loan data, which was distinctly <em>a</em>symmetrical, as most of your data is likely to be.</p>&#13;
&#13;
<p>So <a data-primary="distributions" data-secondary="asymmetric" data-type="indexterm" id="dist-asymmetric"/><a data-primary="asymmetric distributions" data-type="indexterm" id="asymmetric-dist"/><a data-primary="quartiles" data-type="indexterm" id="quartiles"/><a data-primary="outliers" data-secondary="in asymmetric distributions" data-secondary-sortas="asymmetric distributions" data-type="indexterm" id="outlier-asymmetric"/>what do we do when our data distribution lacks symmetry? We already know how to find the “middle” of an asymmetric distribution like the one in <a data-type="xref" href="#ppp_loan_central_measures">Example 9-1</a>: by calculating the median, rather than the mean. But what about identifying extreme values? Since asymmetric <a data-primary="skewed distributions" data-see="asymmetric distributions" data-type="indexterm" id="idm45143397587712"/>or <em>skewed</em> distributions aren’t, well, symmetric, there is no single “standard” deviation, nor can we use it to calculate z-scores. We can, however, still usefully subdivide an asymmetric dataset in a way that will let us generate insight about possibly unusual or extreme values.</p>&#13;
&#13;
<p>Like finding the median, this subdivision process is really quite simple. First, we find the middle value of our sorted dataset—in other words, the median. Now we look at each half of the data records as if it were a standalone dataset and find <em>their</em> median values. The median of the lower half is traditionally labeled Q1, while the median of the upper half is traditionally labeled Q3. At this point, we’ve split our dataset into four parts, or <em>quartiles</em>, each of which contains an equal number of data values.</p>&#13;
&#13;
<p>What does this do for us? Well, remember that a big part of what z-scores tell us is the <em>percentage of data points that have similar values</em>. Looking at <a data-type="xref" href="#gaussian_distro">Figure 9-4</a>, for example, we can see that a data point with a z-score of 0.75 is (as we would expect) less than one standard deviation from the mean—something we know will be true for roughly 68.2% of all the data values in the set as a whole. By dividing our data into quartiles, we have started along a similar path. For example, any value in our dataset that is <em>numerically</em> less than the value of Q1 is, by definition, smaller than at least 75% of all the data values we have.</p>&#13;
&#13;
<p>Still, what we’re <em>really</em> looking for are ways to identify potentially unusual values. Being smaller—or larger—than 75% of all data values is something, but it’s hardly <em>extreme</em>. Identifying our quartile boundaries alone won’t quite be enough.</p>&#13;
&#13;
<p>Fortunately, we can use our Q1 and Q3 values to calculate what’s known as the <em>lower bound</em> and <em>upper bound</em> of our<a data-primary="lower bound of dataset" data-type="indexterm" id="idm45143397579632"/><a data-primary="upper bound of dataset" data-type="indexterm" id="idm45143397579024"/> dataset. If our data’s distribution was secretly Gaussian, these boundaries would line up almost perfectly with the values found at three standard deviations below and above the mean. While of course we’re using them precisely because our data <em>isn’t</em> Gaussian, I make the comparison to illustrate that we can use them to help identify extreme values in an asymmetrically distributed dataset.</p>&#13;
&#13;
<p>Like finding the median, calculating the upper and lower bounds is actually quite straightforward. We start by finding a value called <a data-primary="IQR (interquartile range)" data-type="indexterm" id="idm45143397576944"/>the <em>interquartile range</em> (IQR)—a fancy-sounding name for the numerical difference between the values of Q3 and Q1. We then multiply that value by 1.5 and subtract it from Q1 to get the lower bound, and add it to Q3 to get the upper bound. That’s it!</p>&#13;
<div data-type="equation">&#13;
<p>IQR (interquartile range) = Q3 – Q1</p>&#13;
<p>Lower bound = Q1 – (1.5 × IQR)</p>&#13;
<p>Upper bound = Q3 + (1.5 × IQR)</p>&#13;
</div>&#13;
&#13;
<p>On a Gaussian distribution, our upper and lower bound values will be about three standard deviations above or below the mean—but does this mean that every value beyond our upper and lower bounds is automatically an <em>outlier</em>? No. But finding these boundaries does let us narrow down where we might start <em>looking</em> for outliers. And just as importantly, these measures help us understand what values are <em>not</em> outliers, even if they might seem, numerically, to be pretty different from the “typical” or “expected” value provided by the median or mean.</p>&#13;
&#13;
<p>As an example, <a data-primary="PPP (Paycheck Protection Program) example" data-secondary="outliers in dataset" data-type="indexterm" id="ppp-outlier"/>let’s return to our PPP loan data. A $1 million loan seems like a lot, even if—as we are—you’re only looking at loans that were over $150,000 to begin with. But is a $1 million loan truly <em>unusual</em>? This is where our measures of central tendency and spread—in this case, the median, quartiles, and lower and upper bound values—can really help us out. Let’s take a look at what our histogram looks like with these values added, as shown in <a data-type="xref" href="#ppp_loan_central_and_dist">Example 9-3</a>, and see what we think.</p>&#13;
<div data-type="example" id="ppp_loan_central_and_dist">&#13;
<h5><span class="label">Example 9-3. </span>ppp_loan_central_and_dist.py</h5>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="c1"># `pandas` for reading and assessing our data</code>&#13;
<code class="kn">import</code> <code class="nn">pandas</code> <code class="kn">as</code> <code class="nn">pd</code>&#13;
&#13;
<code class="c1"># `seaborn` for its built-in themes and chart types</code>&#13;
<code class="kn">import</code> <code class="nn">seaborn</code> <code class="kn">as</code> <code class="nn">sns</code>&#13;
&#13;
<code class="c1"># `matplotlib` for customizing visual details</code>&#13;
<code class="kn">import</code> <code class="nn">matplotlib.pyplot</code> <code class="kn">as</code> <code class="nn">plt</code>&#13;
&#13;
<code class="c1"># read in our data</code>&#13;
<code class="n">ppp_data</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">read_csv</code><code class="p">(</code><code class="s1">'public_150k_plus_221.csv'</code><code class="p">)</code>&#13;
&#13;
<code class="c1"># set a basic color theme for our visualization</code>&#13;
<code class="n">sns</code><code class="o">.</code><code class="n">set_theme</code><code class="p">(</code><code class="n">style</code><code class="o">=</code><code class="s2">"whitegrid"</code><code class="p">)</code>&#13;
&#13;
<code class="c1"># use the built-in `mean()` and `median()` methods in `pandas</code>&#13;
<code class="n">mean</code> <code class="o">=</code> <code class="n">ppp_data</code><code class="p">[</code><code class="s1">'CurrentApprovalAmount'</code><code class="p">]</code><code class="o">.</code><code class="n">mean</code><code class="p">()</code>&#13;
<code class="n">median</code> <code class="o">=</code> <code class="n">ppp_data</code><code class="p">[</code><code class="s1">'CurrentApprovalAmount'</code><code class="p">]</code><code class="o">.</code><code class="n">median</code><code class="p">()</code>&#13;
&#13;
<code class="c1"># Q1 is the value at the position in our dataset</code>&#13;
<code class="c1"># that has 25% of data readings to its left</code>&#13;
<code class="n">Q1</code> <code class="o">=</code> <code class="n">ppp_data</code><code class="p">[</code><code class="s1">'CurrentApprovalAmount'</code><code class="p">]</code><code class="o">.</code><code class="n">quantile</code><code class="p">(</code><code class="mf">0.25</code><code class="p">)</code>&#13;
&#13;
<code class="c1"># Q3 is the value at the position in our dataset</code>&#13;
<code class="c1"># that has 75% of data readings to its left</code>&#13;
<code class="n">Q3</code> <code class="o">=</code> <code class="n">ppp_data</code><code class="p">[</code><code class="s1">'CurrentApprovalAmount'</code><code class="p">]</code><code class="o">.</code><code class="n">quantile</code><code class="p">(</code><code class="mf">0.75</code><code class="p">)</code>&#13;
&#13;
<code class="c1"># IQR is the difference between the Q3 and Q1 values</code>&#13;
<code class="n">IQR</code> <code class="o">=</code> <code class="n">Q3</code><code class="o">-</code><code class="n">Q1</code>&#13;
&#13;
<code class="c1"># and now we calculate our lower and upper bounds</code>&#13;
<code class="n">lower_bound</code> <code class="o">=</code> <code class="n">Q1</code> <code class="o">-</code> <code class="p">(</code><code class="mf">1.5</code><code class="o">*</code><code class="n">IQR</code><code class="p">)</code>&#13;
<code class="n">upper_bound</code> <code class="o">=</code> <code class="n">Q3</code> <code class="o">+</code> <code class="p">(</code><code class="mf">1.5</code><code class="o">*</code><code class="n">IQR</code><code class="p">)</code>&#13;
&#13;
<code class="c1"># use `seaborn` to plot the histogram</code>&#13;
<code class="n">approved_loan_plot</code> <code class="o">=</code> <code class="n">sns</code><code class="o">.</code><code class="n">histplot</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="n">ppp_data</code><code class="p">,</code> <code class="n">x</code><code class="o">=</code><code class="s2">"CurrentApprovalAmount"</code><code class="p">)</code>&#13;
&#13;
<code class="c1"># get the min and max y-values on our histogram</code>&#13;
<code class="n">y_axis_range</code> <code class="o">=</code> <code class="n">approved_loan_plot</code><code class="o">.</code><code class="n">get_ylim</code><code class="p">()</code>&#13;
&#13;
<code class="c1"># add mean line in gray</code>&#13;
<code class="n">approved_loan_plot</code><code class="o">.</code><code class="n">vlines</code><code class="p">(</code><code class="n">mean</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="n">y_axis_range</code><code class="p">[</code><code class="mi">1</code><code class="p">],</code> <code class="n">color</code><code class="o">=</code><code class="s1">'gray'</code><code class="p">,</code> <code class="n">ls</code><code class="o">=</code><code class="s1">'-'</code><code class="p">)</code>&#13;
&#13;
<code class="c1"># other lines in black (median solid, others dotted)</code>&#13;
<code class="n">approved_loan_plot</code><code class="o">.</code><code class="n">vlines</code><code class="p">(</code><code class="n">median</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="n">y_axis_range</code><code class="p">[</code><code class="mi">1</code><code class="p">],</code> <code class="n">color</code><code class="o">=</code><code class="s1">'black'</code><code class="p">,</code> <code class="n">ls</code><code class="o">=</code><code class="s1">'-'</code><code class="p">)</code>&#13;
<code class="n">approved_loan_plot</code><code class="o">.</code><code class="n">vlines</code><code class="p">(</code><code class="n">lower_bound</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="n">y_axis_range</code><code class="p">[</code><code class="mi">1</code><code class="p">],</code> <code class="n">color</code><code class="o">=</code><code class="s1">'black'</code><code class="p">,</code> <code class="n">ls</code><code class="o">=</code><code class="s1">':'</code><code class="p">)</code>&#13;
<code class="n">approved_loan_plot</code><code class="o">.</code><code class="n">vlines</code><code class="p">(</code><code class="n">Q1</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="n">y_axis_range</code><code class="p">[</code><code class="mi">1</code><code class="p">],</code> <code class="n">color</code><code class="o">=</code><code class="s1">'black'</code><code class="p">,</code> <code class="n">ls</code><code class="o">=</code><code class="s1">':'</code><code class="p">)</code>&#13;
<code class="n">approved_loan_plot</code><code class="o">.</code><code class="n">vlines</code><code class="p">(</code><code class="n">Q3</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="n">y_axis_range</code><code class="p">[</code><code class="mi">1</code><code class="p">],</code> <code class="n">color</code><code class="o">=</code><code class="s1">'black'</code><code class="p">,</code> <code class="n">ls</code><code class="o">=</code><code class="s1">':'</code><code class="p">)</code>&#13;
<code class="n">approved_loan_plot</code><code class="o">.</code><code class="n">vlines</code><code class="p">(</code><code class="n">upper_bound</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="n">y_axis_range</code><code class="p">[</code><code class="mi">1</code><code class="p">],</code> <code class="n">color</code><code class="o">=</code><code class="s1">'black'</code><code class="p">,</code> <code class="n">ls</code><code class="o">=</code><code class="s1">':'</code><code class="p">)</code>&#13;
&#13;
<code class="c1"># show the plot!</code>&#13;
<code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">()</code></pre></div>&#13;
&#13;
<p>As you can see from the zoom-in view of the resulting graph (shown in <a data-type="xref" href="#PPP_loan_distribution">Figure 9-5</a>), there’s really no support for the claim that a loan of $1 million is out of the ordinary; that amount falls well below the upper bound we’ve calculated for this dataset. So even though a loan of that amount is larger than three-quarters of all loans approved so far (because the $1 million mark, currently labeled as 1.0 1e6 on the graph’s x-axis, is to the <em>right</em> of our Q3 line), it’s still not <em>so</em> much that any loan of $1 million is likely to be worth investigating further. At least, that’s probably not where we’d want to start.</p>&#13;
&#13;
<figure><div class="figure" id="PPP_loan_distribution">&#13;
<img alt="PPP Current Loan Amount histogram with median, quartiles, and bounds in black, and mean plotted in gray." src="assets/ppdw_0905.png"/>&#13;
<h6><span class="label">Figure 9-5. </span>Detail of PPP Current Loan Amount histogram with median, quartiles, and bounds in black and mean plotted in gray</h6>&#13;
</div></figure>&#13;
&#13;
<p>So where should we look next for potentially interesting patterns in the data? <em>Right in front of us</em>, at the graph we already have. Because while we <em>could</em> start looking for more complex statistical measures to calculate and evaluate, even this basic visualization is showing some intriguing patterns in the data. The first one worth noting—if only to reassure ourselves about our choice of statistical measures—is that the mean of this dataset is at nearly the same position in the distribution as our Q3 value. If we had any concern about selecting the median over the mean as a measure of central tendency for this dataset, that fact should set it to rest. The other thing that we can see—in the data view shown in <a data-type="xref" href="#PPP_loan_distribution">Figure 9-5</a> and if we were to scroll farther to the right—is that there are curious little spikes in our data, indicating that a particular loan amount was approved with relatively high frequency.<sup><a data-type="noteref" href="ch09.html#idm45143397217488" id="idm45143397217488-marker">8</a></sup> Given how clearly these stand out from the data patterns immediately around them, we should probably look at those values <a data-primary="data analysis" data-secondary="visualization in" data-startref="data-analysis-visual-symmetry" data-tertiary="symmetry" data-type="indexterm" id="idm45143397216688"/><a data-primary="visualizations" data-secondary="in data analysis" data-secondary-sortas="data analysis" data-startref="visual-data-analysis-symmetry" data-tertiary="symmetry" data-type="indexterm" id="idm45143397215184"/><a data-primary="histograms" data-secondary="symmetry" data-startref="histogram-symmetry" data-type="indexterm" id="idm45143397213408"/><a data-primary="distributions" data-secondary="symmetry in" data-startref="distribution-symmetry" data-type="indexterm" id="idm45143397212192"/><a data-primary="symmetry" data-startref="symmetry" data-type="indexterm" id="idm45143397210976"/><a data-primary="distributions" data-secondary="asymmetric" data-startref="dist-asymmetric" data-type="indexterm" id="idm45143397210032"/><a data-primary="asymmetric distributions" data-startref="asymmetric-dist" data-type="indexterm" id="idm45143397208816"/><a data-primary="quartiles" data-startref="quartiles" data-type="indexterm" id="idm45143397207856"/><a data-primary="outliers" data-secondary="in asymmetric distributions" data-secondary-sortas="asymmetric distributions" data-startref="outlier-asymmetric" data-type="indexterm" id="idm45143397206912"/><a data-primary="PPP (Paycheck Protection Program) example" data-secondary="outliers in dataset" data-startref="ppp-outlier" data-type="indexterm" id="idm45143397205392"/>next.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Counting “Clusters”" data-type="sect2"><div class="sect2" id="idm45143397936064">&#13;
<h2>Counting “Clusters”</h2>&#13;
&#13;
<p>Imagine <a data-primary="data analysis" data-secondary="visualization in" data-tertiary="clusters" data-type="indexterm" id="data-analysis-visual-clusters"/><a data-primary="visualizations" data-secondary="in data analysis" data-secondary-sortas="data analysis" data-tertiary="clusters" data-type="indexterm" id="visual-data-analysis-clusters"/><a data-primary="histograms" data-secondary="clusters" data-type="indexterm" id="histogram-clusters"/><a data-primary="distributions" data-secondary="clusters" data-type="indexterm" id="distribution-clusters"/><a data-primary="clusters" data-type="indexterm" id="clusters"/><a data-primary="density (of data)" data-type="indexterm" id="density"/>you’re walking down a crowded street and you notice a group of people gathered on the corner across from you. What do you do? On a busy avenue where most pedestrians are concerned with getting from one place to another, more than one or two people stopped in the same place at the same time is enough to signal that <em>something</em> is going on. Whether “something” turns out to be a busker playing music, a vendor of especially popular snacks, or a box full of kittens,<sup><a data-type="noteref" href="ch09.html#idm45143397194032" id="idm45143397194032-marker">9</a></sup> the fact remains that our visual system is drawn to anomalies, and that’s precisely because deviating from a trend indicates that something at least a little bit out of the ordinary is going on.</p>&#13;
&#13;
<p>This is also why visualizing data is such a valuable tool for analyzing it—our eyes and brain are wired to both quickly perceive patterns and to just as quickly notice deviations from them. Sometimes the reason for a pattern is easy to guess, sometimes less so. But in any predictable pattern—whether it’s the flow of people on a street, a bell-shaped data distribution, or one that forms a smoothly sloping curve—something that breaks that pattern is worth investigating.</p>&#13;
&#13;
<p>In the case of <a data-type="xref" href="#PPP_loan_distribution">Figure 9-5</a>, we<a data-primary="PPP (Paycheck Protection Program) example" data-secondary="clusters" data-type="indexterm" id="ppp-cluster"/> can see a range of such pattern violations. The first is the sharp line at the lefthand side of the graph, which serves as a good reminder that our dataset contains <em>only</em> approved loan amounts of $150,000 or more, rather than all of the loans that have been approved within the program. In case we had lost sight of that, the obvious and hard cutoff our data shows at the lefthand edge is a good reminder.</p>&#13;
&#13;
<p>Bu there is also another set of pattern violations: little spikes in our histogram around particular points along the x-axis, at data values like $2 million. Where are these coming from? While we can’t say for sure, scanning our histogram reveals that similar spikes appear at roughly $500,000 intervals, especially as the loan amounts increase. To some degree, these are probably the result of a tendency toward “round” numbers: if you’re going to ask for $1,978,562.34, why not just “round it up” to $2 million? Of course, that would still be $21,437.66 more than maybe you need—and to most of us it is a <em>lot</em> of money. Given that PPP loans are intended to support specific costs, it does seem a little strange that <em>so</em> many loans—nearly 2,000 of them, based on our graph—would happen to work out to precisely $2 million.</p>&#13;
&#13;
<p>So what’s going on? This is where we need to do some additional research to effectively interpret what we’re seeing in the data. Based on my experience, my first step would be to look through the rules for PPP loans to see if I can work out why $2 million might be such a popular amount to request. For example, is $2 million a minimum or maximum allowed amount based on specific attributes of the business or what it’s requesting support for?</p>&#13;
&#13;
<p>A little bit of searching around the Small Business Administration’s (SBA) <a href="https://sba.gov/funding-programs/loans/covid-19-relief-options/paycheck-protection-program/second-draw-ppp-loan">website</a> seems to offer at least part of the answer:</p>&#13;
<blockquote>&#13;
<p>For most borrowers, the maximum loan amount of a Second Draw PPP loan is 2.5x the average monthly 2019 or 2020 payroll costs up to $2 million. For borrowers in the Accommodation and Food Services sector (use NAICS 72 to confirm), the maximum loan amount for a Second Draw PPP loan is 3.5x the average monthly 2019 or 2020 payroll costs up to $2 million.</p></blockquote>&#13;
&#13;
<p>Since $2 million is a ceiling for essentially <em>all</em> types of businesses applying for so-called second-draw (or second-round) PPP loans—including those that might have initially qualified for <em>more</em> money—it makes sense that the cluster of loans approved for <em>precisely</em> $2 million is so large.</p>&#13;
&#13;
<p>This “answer,” of course, just leads to more questions. According to the documentation, $2 million was the upper limit for second-round PPP loans; first-round loans could be up to $10 million. If so many businesses were requesting the upper limit for <em>second</em>-round loans, it indicates that many businesses 1) have already received a first-round loan, and 2) their first-round loan may have been even <em>larger</em> than $2 million, since they would have had to round <em>down</em> to $2 million if they qualified for more than that in the first round. In other words, we might expect those businesses that requested <em>precisely</em> $2 million in second-round loans are among those that were approved for the largest total amounts of PPP loan relief. And of course, if they <em>did</em> get some of the largest pots of money, we (and probably a lot of other people!) certainly want to know about it.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="The $2 Million Question" data-type="sect1"><div class="sect1" id="two_million_question">&#13;
<h1>The $2 Million Question</h1>&#13;
&#13;
<p>In order to understand what (if any) shared characteristics there may be among companies that requested $2 million for their second-round PPP loans, we first have to effectively isolate their records within our dataset. How might we do this? Well, we know that we’re interested in companies approved for more than one loan, which means that their <code>BorrowerName</code> <em>should</em> appear more than once in our data. We <em>also</em> know that no second-round loans were issued before January 13, 2021. By combining these two insights, we can probably use our data wrangling skills to do a decent job of identifying the companies that requested precisely $2 million for their second-round loan.</p>&#13;
&#13;
<p>In order to accomplish this, we’ll do a couple of key transformations on our dataset:</p>&#13;
<ol class="pagebreak-before less_space">&#13;
<li>&#13;
<p>We’ll create a new column for each loan, containing the label <code>first_round</code>, or <code>maybe_second</code>, based on whether it was issued before January 13, 2021. While we can’t be sure that all loans after that date were “second round,” we <em>can</em> be sure that all loans <em>before</em> that date were “first round.”</p>&#13;
</li>&#13;
<li>&#13;
<p>Look for duplicate entries in our dataset. Each approved loan creates a separate record, so if the same business was approved for two loans, that means its information would appear twice in the records.</p>&#13;
</li>&#13;
&#13;
</ol>&#13;
&#13;
<p>The logic here is that if we find a given business name twice in our data <em>and</em> those records have different “round” labels, it probably indicates a business that has, in fact, been approved for two separate loans.</p>&#13;
&#13;
<p>As usual, we’re going to call in the help of some Python libraries to get this work done. We’ll need to <a data-primary="Pandas" data-type="indexterm" id="idm45143397167520"/><a data-primary="numpy library" data-type="indexterm" id="idm45143397166816"/><a data-primary="seaborn library" data-type="indexterm" id="idm45143397166144"/><a data-primary="matplotlib library" data-type="indexterm" id="idm45143397165472"/><a data-primary="Python" data-secondary="clusters of data" data-type="indexterm" id="python-clusters"/>use <em>pandas</em>, as usual, but we’re also going to use another library called <em>numpy</em> that has lots of useful array/list functions (<em>pandas</em> actually relies heavily on <em>numpy</em> under the hood). I’m also going to pull in <em>seaborn</em> and <em>matplotlib</em> again so that we have the option of generating visualizations to help us evaluate our evolving dataset as we go along.</p>&#13;
&#13;
<p>Although what we’re trying to do with this data is conceptually pretty straightforward, the wrangling involved in performing this analysis takes a fair number of steps, as you can see in <a data-type="xref" href="#who_got_2_loans_by_date">Example 9-4</a>.</p>&#13;
<div data-type="example" id="who_got_2_loans_by_date">&#13;
<h5><span class="label">Example 9-4. </span>who_got_2_loans_by_date.py</h5>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="c1"># `pandas` for data loading/transformations</code><code>&#13;
</code><code class="kn">import</code><code> </code><code class="nn">pandas</code><code> </code><code class="kn">as</code><code> </code><code class="nn">pd</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># `seaborn` for visualization</code><code>&#13;
</code><code class="kn">import</code><code> </code><code class="nn">seaborn</code><code> </code><code class="kn">as</code><code> </code><code class="nn">sns</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># `matplotlib` for detailed visualization support</code><code>&#13;
</code><code class="kn">import</code><code> </code><code class="nn">matplotlib.pyplot</code><code> </code><code class="kn">as</code><code> </code><code class="nn">plt</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># `numpy` for manipulating arrays/lists</code><code>&#13;
</code><code class="kn">import</code><code> </code><code class="nn">numpy</code><code> </code><code class="kn">as</code><code> </code><code class="nn">np</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># load our data</code><code>&#13;
</code><code class="n">ppp_data</code><code> </code><code class="o">=</code><code> </code><code class="n">pd</code><code class="o">.</code><code class="n">read_csv</code><code class="p">(</code><code class="s1">'</code><code class="s1">public_150k_plus_borrower_fingerprint_a.csv</code><code class="s1">'</code><code class="p">)</code><code> </code><a class="co" href="#callout_introduction_to_data_analysis_CO3-1" id="co_introduction_to_data_analysis_CO3-1"><img alt="1" src="assets/1.png"/></a><code>&#13;
</code><code>&#13;
</code><code class="c1"># convert the `DateApproved` column to an actual datetime data type</code><code>&#13;
</code><code class="n">ppp_data</code><code class="p">[</code><code class="s1">'</code><code class="s1">DateApproved</code><code class="s1">'</code><code class="p">]</code><code> </code><code class="o">=</code><code> </code><code class="n">pd</code><code class="o">.</code><code class="n">to_datetime</code><code class="p">(</code><code class="n">ppp_data</code><code class="p">[</code><code class="s1">'</code><code class="s1">DateApproved</code><code class="s1">'</code><code class="p">]</code><code class="p">)</code><code> </code><a class="co" href="#callout_introduction_to_data_analysis_CO3-2" id="co_introduction_to_data_analysis_CO3-2"><img alt="2" src="assets/2.png"/></a><code>&#13;
</code><code>&#13;
</code><code class="c1"># create a variable to hold the second-round start date</code><code>&#13;
</code><code class="n">second_round_start</code><code> </code><code class="o">=</code><code>  </code><code class="n">pd</code><code class="o">.</code><code class="n">to_datetime</code><code class="p">(</code><code class="s1">'</code><code class="s1">2021-01-13</code><code class="s1">'</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># treat today's date to use as the "upper" limit on possible second-round loans</code><code>&#13;
</code><code class="n">todays_date</code><code> </code><code class="o">=</code><code> </code><code class="n">pd</code><code class="o">.</code><code class="n">to_datetime</code><code class="p">(</code><code class="s1">'</code><code class="s1">today</code><code class="s1">'</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># use 1/1/2020 as a "lower" limit, since it's before the PPP launched</code><code>&#13;
</code><code class="n">program_start</code><code> </code><code class="o">=</code><code> </code><code class="n">pd</code><code class="o">.</code><code class="n">to_datetime</code><code class="p">(</code><code class="s1">'</code><code class="s1">2020-01-01</code><code class="s1">'</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># pass our boundaries and category labels to the pandas `cut()` function</code><code>&#13;
</code><code class="n">loan_round</code><code> </code><code class="o">=</code><code> </code><code class="n">pd</code><code class="o">.</code><code class="n">cut</code><code class="p">(</code><code class="n">ppp_data</code><code class="o">.</code><code class="n">DateApproved</code><code class="p">,</code><code>&#13;
</code><code>                    </code><code class="n">bins</code><code class="o">=</code><code class="p">[</code><code class="n">program_start</code><code class="p">,</code><code class="n">second_round_start</code><code class="p">,</code><code> </code><code class="n">todays_date</code><code class="p">]</code><code class="p">,</code><code>&#13;
</code><code>                    </code><code class="n">labels</code><code class="o">=</code><code class="p">[</code><code class="s1">'</code><code class="s1">first_round</code><code class="s1">'</code><code class="p">,</code><code> </code><code class="s1">'</code><code class="s1">maybe_second</code><code class="s1">'</code><code class="p">]</code><code class="p">)</code><code> </code><a class="co" href="#callout_introduction_to_data_analysis_CO3-3" id="co_introduction_to_data_analysis_CO3-3"><img alt="3" src="assets/3.png"/></a><code>&#13;
</code><code>&#13;
</code><code class="c1"># insert the new column at the position we specify</code><code>&#13;
</code><code class="n">ppp_data</code><code class="o">.</code><code class="n">insert</code><code class="p">(</code><code class="mi">2</code><code class="p">,</code><code class="s1">'</code><code class="s1">Loan Round</code><code class="s1">'</code><code class="p">,</code><code class="n">loan_round</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># this "pivot table" will return a Series showing the number</code><code>&#13;
</code><code class="c1"># of times a particular 'BorrowerNameFingerprint' appears in the dataset</code><code>&#13;
</code><code class="n">loan_count</code><code> </code><code class="o">=</code><code> </code><code class="n">ppp_data</code><code class="o">.</code><code class="n">pivot_table</code><code class="p">(</code><code class="n">index</code><code class="o">=</code><code class="p">[</code><code class="s1">'</code><code class="s1">BorrowerNameFingerprint</code><code class="s1">'</code><code class="p">]</code><code class="p">,</code><code> </code><code class="n">aggfunc</code><code class="o">=</code><code class="s1">'</code><code class="s1">size</code><code class="s1">'</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># convert our Series to a DataFrame and give it a name</code><code>&#13;
</code><code class="n">loan_count_df</code><code> </code><code class="o">=</code><code> </code><code class="n">loan_count</code><code class="o">.</code><code class="n">to_frame</code><code class="p">(</code><code class="s1">'</code><code class="s1">Loan Count</code><code class="s1">'</code><code class="p">)</code><code> </code><a class="co" href="#callout_introduction_to_data_analysis_CO3-4" id="co_introduction_to_data_analysis_CO3-4"><img alt="4" src="assets/4.png"/></a><code>&#13;
</code><code>&#13;
</code><code class="c1"># use the `describe()` method to print out summary statistics</code><code>&#13;
</code><code class="k">print</code><code class="p">(</code><code class="s2">"</code><code class="s2">Description of duplicate borrower table:</code><code class="s2">"</code><code class="p">)</code><code>&#13;
</code><code class="k">print</code><code class="p">(</code><code class="n">loan_count_df</code><code class="o">.</code><code class="n">describe</code><code class="p">(</code><code class="p">)</code><code class="p">)</code><code> </code><a class="co" href="#callout_introduction_to_data_analysis_CO3-5" id="co_introduction_to_data_analysis_CO3-5"><img alt="5" src="assets/5.png"/></a></pre></div>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_introduction_to_data_analysis_CO3-1" id="callout_introduction_to_data_analysis_CO3-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>This file was generated by running our fingerprinting process on <code>BorrowerName</code>, as described in <a data-type="xref" href="#finding_a_fingerprint">“Finding a Fingerprint”</a>.</p></dd>&#13;
<dt><a class="co" href="#co_introduction_to_data_analysis_CO3-2" id="callout_introduction_to_data_analysis_CO3-2"><img alt="2" src="assets/2.png"/></a></dt>&#13;
<dd><p>We want to know which loans were approved <em>before</em> January 13, 2021. The fastest way to do this will be to convert our <code>DateApproved</code> strings to “real” dates and compare them to that.</p></dd>&#13;
<dt><a class="co" href="#co_introduction_to_data_analysis_CO3-3" id="callout_introduction_to_data_analysis_CO3-3"><img alt="3" src="assets/3.png"/></a></dt>&#13;
<dd><p>The pandas <code>cut()</code> function lets us create a new column by applying boundaries and labels to an existing one. In this case, we label each record according to whether it was approved before or after January 13, 2021.</p></dd>&#13;
<dt><a class="co" href="#co_introduction_to_data_analysis_CO3-4" id="callout_introduction_to_data_analysis_CO3-4"><img alt="4" src="assets/4.png"/></a></dt>&#13;
<dd><p>We do this for convenience so we can use the <code>describe()</code> method.</p></dd>&#13;
<dt><a class="co" href="#co_introduction_to_data_analysis_CO3-5" id="callout_introduction_to_data_analysis_CO3-5"><img alt="5" src="assets/5.png"/></a></dt>&#13;
<dd><p>We expect the maximum value in this table to be <code>2</code>, since no business is allowed to get more than two loans under the PPP.</p></dd>&#13;
</dl>&#13;
&#13;
<p>If you run the code from <a data-type="xref" href="#who_got_2_loans_by_date">Example 9-4</a> and nothing happens for a minute, don’t despair. On my Chromebook, this script takes about 40 to 90 seconds to execute (depending on how many other Linux apps I’m running alongside).<sup><a data-type="noteref" href="ch09.html#idm45143397083520" id="idm45143397083520-marker">10</a></sup> When it’s finished, however, your output will look something like this:</p>&#13;
&#13;
<pre data-type="programlisting">Description of duplicate borrower table:&#13;
          Loan Count&#13;
count  694279.000000&#13;
mean        1.104022&#13;
std         0.306489&#13;
min         1.000000&#13;
25%         1.000000&#13;
50%         1.000000&#13;
75%         1.000000&#13;
max        12.000000</pre>&#13;
&#13;
<p>From this first effort something seems…off. The output from our <code>.describe()</code> command gives us a quick way of getting almost all the summary statistics we’re interested in (the Q1, median, and Q3 are labeled here according to the percentage of values that would appear to their left on a histogram—so 25%, 50%, and 75%, respectively). These values suggest that fewer than 25% of all businesses have received more than one loan (otherwise the 75% value would be greater than 1), which makes sense. But the max value is troubling, since the PPP rules don’t appear to allow a single business to receive more than two loans, much less 12! Let’s take a closer look by adding the code shown in <a data-type="xref" href="#who_got_2_loans_by_date_contd">Example 9-5</a> to what we wrote in <a data-type="xref" href="#who_got_2_loans_by_date">Example 9-4</a> and see what we find.</p>&#13;
<div data-type="example" id="who_got_2_loans_by_date_contd">&#13;
<h5><span class="label">Example 9-5. </span>who_got_2_loans_by_date.py (continued)</h5>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="c1"># start by sorting our DataFrame of loan counts from greatest to least</code>&#13;
<code class="n">sorted_loan_counts</code> <code class="o">=</code> <code class="n">loan_count_df</code><code class="o">.</code><code class="n">sort_values</code><code class="p">(</code><code class="n">by</code><code class="o">=</code><code class="p">[</code><code class="s1">'Loan Count'</code><code class="p">],</code> <code class="n">ascending</code><code class="o">=</code><code class="bp">False</code><code class="p">)</code>&#13;
&#13;
<code class="c1"># create a new DataFrame with *only* those that have more than two loans</code>&#13;
<code class="n">more_than_two</code> <code class="o">=</code> <code class="n">sorted_loan_counts</code><code class="p">[</code><code class="n">sorted_loan_counts</code><code class="p">[</code><code class="s1">'Loan Count'</code><code class="p">]</code> <code class="o">&gt;</code> <code class="mi">2</code><code class="p">]</code>&#13;
&#13;
<code class="c1"># print one instance of each business name that appears in `more_than_two`</code>&#13;
<code class="k">print</code><code class="p">(</code><code class="s2">"Businesses that seem to have gotten more than 2 loans:"</code><code class="p">)</code>&#13;
<code class="k">print</code><code class="p">(</code><code class="n">more_than_two</code><code class="o">.</code><code class="n">shape</code><code class="p">)</code>&#13;
&#13;
<code class="k">print</code><code class="p">(</code><code class="s2">"Number of businesses that appear to have gotten precisely 2 loans:"</code><code class="p">)</code>&#13;
<code class="n">precisely_two</code> <code class="o">=</code> <code class="n">sorted_loan_counts</code><code class="p">[</code><code class="n">sorted_loan_counts</code><code class="p">[</code><code class="s1">'Loan Count'</code><code class="p">]</code> <code class="o">==</code> <code class="mi">2</code><code class="p">]</code>&#13;
<code class="k">print</code><code class="p">(</code><code class="n">precisely_two</code><code class="o">.</code><code class="n">shape</code><code class="p">)</code></pre></div>&#13;
&#13;
<p>Now we get the additional output shown here:</p>&#13;
&#13;
<pre data-type="programlisting">Businesses that seem to have gotten more than 2 loans:&#13;
(58, 1)&#13;
Number of businesses that appear to have gotten precisely 2 loans:&#13;
(72060, 1)</pre>&#13;
&#13;
<p>This suggests that there are only a (relative) handful of businesses that may have been approved for more than two loans, and we can probably attribute those cases to a combination of our chosen fingerprinting approach (a combination of <code>BorrowerName</code>, <code>BorrowerCity</code>, and <code>BorrowerState</code>) along with the possibility that there are multiple &#13;
<span class="keep-together">instances</span> of a single franchise in the same city that applied for PPP funds.<sup><a data-type="noteref" href="ch09.html#idm45143396792864" id="idm45143396792864-marker">11</a></sup> In any case, there are few enough of them that they are unlikely to change the outcome of our analysis considerably, so we won’t focus on tracking down their details right now. At least the second piece of output showing that 72,060 individual businesses got <em>exactly</em> two loans seems reasonable so far, since this is definitely less than 25% of our total dataset, and therefore aligns with the summary statistics we got from our <code>Loan Count</code> DataFrame (because the value of Q3 was still 1, meaning that fewer than 25% of all business names appeared in our dataset more than once).</p>&#13;
&#13;
<p>Of course, this is still just an estimate; it would be much better if we had a more official count of second-round loans to work with. As noted at the end of <a data-type="xref" href="ch06.html#chapter6">Chapter 6</a>, the Small Business Administration <em>did</em> actually release <a href="https://data.sba.gov/dataset/ppp-foia/resource/aab8e9f9-36d1-42e1-b3ba-e59c79f1d7f0">an official data dictionary</a> for the PPP loan data, and while it doesn’t contain all of the information we might hope, it <em>does</em> indicate that the <code>ProcessingMethod</code> field distinguishes between first-round (<code>PPP</code>) and second-round (<code>PPS</code>) loans. Let’s look at our data this way and compare it to our name-matching-based estimate by adding the code in <a data-type="xref" href="#who_got_2_loans_by_date_contd2">Example 9-6</a> further down in our file.</p>&#13;
<div data-type="example" id="who_got_2_loans_by_date_contd2">&#13;
<h5><span class="label">Example 9-6. </span>who_got_2_loans_by_date.py (continued again)</h5>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="c1"># use `ProcessingMethod` value to identify second-round loans</code>&#13;
<code class="n">pps_loans</code> <code class="o">=</code> <code class="n">ppp_data</code><code class="p">[</code><code class="n">ppp_data</code><code class="p">[</code><code class="s1">'ProcessingMethod'</code><code class="p">]</code> <code class="o">==</code> <code class="s1">'PPS'</code><code class="p">]</code>&#13;
&#13;
<code class="c1"># print out the `shape` of this DataFrame to see how many businesses we have</code>&#13;
<code class="k">print</code><code class="p">(</code><code class="s2">"Number of loans labeled as second round:"</code><code class="p">)</code>&#13;
<code class="k">print</code><code class="p">(</code><code class="n">pps_loans</code><code class="o">.</code><code class="n">shape</code><code class="p">)</code></pre></div>&#13;
&#13;
<p>Rerunning our script yields the additional output:</p>&#13;
&#13;
<pre data-type="programlisting">Number of loans labeled as second round:&#13;
(103949, 52)</pre>&#13;
&#13;
<p>Wow! Even with our possibly too-lax fingerprinting method, we still failed to find more than 300,000 businesses with both of their loans. What do we do?</p>&#13;
&#13;
<p>First of all, recognize that this isn’t even an unusual situation. We’re dealing with around 750,000 data records, each one of which is a combination of data entry done by multiple individuals, including the borrower, the lender, and possibly the SBA. The fact that there are still so many discrepancies is not really surprising (I illustrate some of them in the following sidebar), but all is not lost. Remember that our original interest was in those businesses that got precisely $2 million for their second-round loan, which is likely to be just a fraction of all the businesses that got two loans. We can still move ahead with that part of the analysis in order to (1) test how effective our date-based estimate of second-round loans was, and (2) see what we can learn about that specific subset of businesses that got exactly $2 million in the second round.</p>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="finding_a_fingerprint">&#13;
<h5>Finding a Fingerprint</h5>&#13;
<p>Since we<a data-primary="fingerprinting" data-type="indexterm" id="fingerprint2"/><a data-primary="fingerprints library" data-type="indexterm" id="fingerprint-lib2"/><a data-primary="Python" data-secondary="fingerprints library" data-type="indexterm" id="python-fingerprint-lib2"/> have already covered how to use the <em>fingerprints</em> library in previous chapters, I skipped over the exact process used to prepare the data we’re looking at here.<sup><a data-type="noteref" href="ch09.html#idm45143396754128" id="idm45143396754128-marker">12</a></sup> As always, however, the effectiveness of the matching process depends not just on the fingerprinting algorithm itself but also the data it’s applied to. While the combination of <code>BorrowerName</code>, <code>BorrowerCity</code>, and <code>BorrowerState</code> is clearly far from perfect, I settled on it only after having first tried to match up loans based on <code>BorrowerName</code> directly, and then on a combination of the fingerprinted <code>BorrowerName</code> and <code>BorowerZip</code>. In both instances, I could find matches for less than half of the second-round loans.</p>&#13;
&#13;
<p>Wondering what the data discrepancies look like? Here’s one example:</p>&#13;
&#13;
<figure><div class="figure">&#13;
<img alt="ppdw in0901" src="assets/ppdw_in0901.png"/>&#13;
<h6/>&#13;
</div></figure>&#13;
&#13;
<p>As you can see, while the <code>BorrowerName</code> field is <em>similar</em>, it’s not the same, nor is the value in <code>BorrowerAddress</code> or the capitalization of <code>BorrowerCity</code>. The zip codes <em>are</em> the same, but many other columns—such as <code>Gender</code>, <code>Veteran</code>, and even <code>LMIIndicator</code>—don’t match up, either. That’s real-world data for<a data-primary="fingerprinting" data-startref="fingerprint2" data-type="indexterm" id="idm45143396714896"/><a data-primary="fingerprints library" data-startref="fingerprint-lib2" data-type="indexterm" id="idm45143396713840"/><a data-primary="Python" data-secondary="fingerprints library" data-startref="python-fingerprint-lib2" data-type="indexterm" id="idm45143396712896"/> you!</p>&#13;
</div></aside>&#13;
&#13;
<p>At this point, <a data-primary="data validation" data-type="indexterm" id="data-valid"/><a data-primary="validating data" data-type="indexterm" id="valid-data"/><a data-primary="Python" data-secondary="validating data" data-type="indexterm" id="python-valid-data"/>we’re going to use the information from the <code>PaymentProcessingMethod</code> column to <em>validate</em> our earlier work using name-matching and date-based loan round estimates. To do this, we’re going to merge our <code>Loan Count</code> DataFrame back onto our original dataset. Then we’ll select only the $2M loans that we <em>estimate</em>, based on their date, were second-round loans. Finally, we’ll compare that number of loans with the number of $2M loans we <em>know</em> were second draw, based on their <code>ProcessingMethod</code> value of <code>PPS</code>. Obviously, this will mean adding yet more code to our file, as shown in <a data-type="xref" href="#who_got_2_loans_by_date_contd3">Example 9-7</a>.</p>&#13;
<div data-type="example" id="who_got_2_loans_by_date_contd3">&#13;
<h5><span class="label">Example 9-7. </span>who_got_2_loans_by_date.py (continued more)</h5>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="c1"># how many loans in our derived data frame were approved for precisely $2M</code>&#13;
<code class="c1"># during the (possibly) second-round timeframe?</code>&#13;
&#13;
<code class="c1"># merge our `loan_count_df` back to keep track of businesses</code>&#13;
<code class="c1"># we labeled as having precisely two loans</code>&#13;
<code class="n">ppp_data_w_lc</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">merge</code><code class="p">(</code><code class="n">ppp_data</code><code class="p">,</code> <code class="n">loan_count_df</code><code class="p">,</code>&#13;
                         <code class="n">on</code><code class="o">=</code><code class="p">[</code><code class="s1">'BorrowerNameFingerprint'</code><code class="p">],</code> <code class="n">how</code><code class="o">=</code><code class="s1">'left'</code><code class="p">)</code>&#13;
&#13;
<code class="c1"># now get *all* the records of business names we associated with two loans</code>&#13;
<code class="n">matched_two_loans</code> <code class="o">=</code> <code class="n">ppp_data_w_lc</code><code class="p">[(</code><code class="n">ppp_data_w_lc</code><code class="p">[</code><code class="s1">'Loan Count'</code><code class="p">]</code> <code class="o">==</code> <code class="mi">2</code><code class="p">)]</code>&#13;
&#13;
<code class="c1"># select those loans our `maybe_second` loans that have a value of $2M</code>&#13;
<code class="n">maybe_round2_2M</code> <code class="o">=</code> <code class="n">matched_two_loans</code><code class="p">[(</code><code class="n">matched_two_loans</code><code class="p">[</code>&#13;
                                    <code class="s1">'CurrentApprovalAmount'</code><code class="p">]</code> <code class="o">==</code> <code class="mf">2000000.00</code><code class="p">)</code> <code class="o">&amp;</code>&#13;
                                    <code class="p">(</code><code class="n">matched_two_loans</code><code class="p">[</code>&#13;
                                    <code class="s1">'Loan Round'</code><code class="p">]</code> <code class="o">==</code> <code class="s1">'maybe_second'</code><code class="p">)]</code>&#13;
<code class="k">print</code><code class="p">(</code><code class="s2">"Derived $2M second-round loans:"</code><code class="p">)</code>&#13;
<code class="k">print</code><code class="p">(</code><code class="n">maybe_round2_2M</code><code class="o">.</code><code class="n">shape</code><code class="p">)</code>&#13;
&#13;
<code class="c1"># select those loans that we *know* are second round and have a value of $2M</code>&#13;
<code class="n">pps_got_2M</code> <code class="o">=</code> <code class="n">pps_loans</code><code class="p">[</code><code class="n">pps_loans</code><code class="p">[</code><code class="s1">'CurrentApprovalAmount'</code><code class="p">]</code> <code class="o">==</code> <code class="mf">2000000.00</code><code class="p">]</code>&#13;
<code class="k">print</code><code class="p">(</code><code class="s2">"Actual $2M second-round loans:"</code><code class="p">)</code>&#13;
<code class="k">print</code><code class="p">(</code><code class="n">pps_got_2M</code><code class="o">.</code><code class="n">shape</code><code class="p">)</code></pre></div>&#13;
&#13;
<p>Adding this code to our main files give us another few lines of output:</p>&#13;
&#13;
<pre data-type="programlisting">Derived $2M second-round loans:&#13;
(1175, 53)&#13;
Actual $2M second-round loans:&#13;
(1459, 52)</pre>&#13;
&#13;
<p>If we compare these results to previous ones, it looks like we’re doing a bit better. Across <em>all</em> loans, we appear to have matched up 72,060 out of 103,949 actual second-round loans, or about 70%. For those organizations approved for $2M in second-round loans, we’ve found 1,115 out of 1,459, or about 80%.</p>&#13;
&#13;
<p>So what can we say about businesses that got $2M in the second round? We can’t say anything with 100% confidence unless and until we find matches for those 284 companies whose <code>BorrowerNameFingerprint</code> isn’t the same between their first- and second-round loans. But we can still look at our 80% sample and see what we discover. To do this, I’m going to take the following steps:<sup><a data-type="noteref" href="ch09.html#idm45143396564848" id="idm45143396564848-marker">13</a></sup></p>&#13;
<ol>&#13;
<li>&#13;
<p>Find all the unique <code>BorrowerNameFingerprint</code> values for businesses that definitely got $2M second-round loans.</p>&#13;
</li>&#13;
<li>&#13;
<p>Create a DataFrame (<code>biz_names_df</code>) based on this list and fill it out with the flag value <code>2Mil2ndRnd</code>.</p>&#13;
</li>&#13;
<li>&#13;
<p>Merge that DataFrame back onto my dataset and use the flag value to pull <em>all</em> loan records for those businesses (both first and second round).</p>&#13;
</li>&#13;
<li>&#13;
<p>Do some basic analyses of how much money those businesses were approved for across both rounds, and visualize those amounts, comparing the official second-round designation (that is, <code>ProcessingMethod == 'PPS'</code>) with our derived, date-based category.</p>&#13;
</li>&#13;
&#13;
</ol>&#13;
&#13;
<p>And of course, now that I’ve written out in a list the steps my script should take (this is exactly the kind of thing that you’d want to put in your data diary and/or program outline), it’s just a matter of coding it up below our existing work; for clarity I’ve put it into a second script file, the complete code of which is shown in <a data-type="xref" href="#who_got_2M_with_viz">Example 9-8</a>.</p>&#13;
<div data-type="example" id="who_got_2M_with_viz">&#13;
<h5><span class="label">Example 9-8. </span>who_got_2M_with_viz.py</h5>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="c1"># `pandas` for data loading/transformations</code>&#13;
<code class="kn">import</code> <code class="nn">pandas</code> <code class="kn">as</code> <code class="nn">pd</code>&#13;
&#13;
<code class="c1"># `seaborn` for visualization</code>&#13;
<code class="kn">import</code> <code class="nn">seaborn</code> <code class="kn">as</code> <code class="nn">sns</code>&#13;
&#13;
<code class="c1"># `matplotlib` for detailed visualization support</code>&#13;
<code class="kn">import</code> <code class="nn">matplotlib.pyplot</code> <code class="kn">as</code> <code class="nn">plt</code>&#13;
&#13;
<code class="c1"># `numpy` for manipulating arrays/lists</code>&#13;
<code class="kn">import</code> <code class="nn">numpy</code> <code class="kn">as</code> <code class="nn">np</code>&#13;
&#13;
<code class="c1"># load our data</code>&#13;
<code class="n">ppp_data</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">read_csv</code><code class="p">(</code><code class="s1">'public_150k_plus_borrower_fingerprint_a.csv'</code><code class="p">)</code>&#13;
&#13;
<code class="c1"># convert the `DateApproved` column to an actual datetime data type</code>&#13;
<code class="n">ppp_data</code><code class="p">[</code><code class="s1">'DateApproved'</code><code class="p">]</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">to_datetime</code><code class="p">(</code><code class="n">ppp_data</code><code class="p">[</code><code class="s1">'DateApproved'</code><code class="p">])</code>&#13;
&#13;
<code class="c1"># create a variable to hold the second-round start date</code>&#13;
<code class="n">second_round_start</code> <code class="o">=</code>  <code class="n">pd</code><code class="o">.</code><code class="n">to_datetime</code><code class="p">(</code><code class="s1">'2021-01-13'</code><code class="p">)</code>&#13;
&#13;
<code class="c1"># treat today's date to use as the "upper" limit on possible second-round loans</code>&#13;
<code class="n">todays_date</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">to_datetime</code><code class="p">(</code><code class="s1">'today'</code><code class="p">)</code>&#13;
&#13;
<code class="c1"># use 1/1/2020 as a "lower" limit, since it's before the PPP launched</code>&#13;
<code class="n">program_start</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">to_datetime</code><code class="p">(</code><code class="s1">'2020-01-01'</code><code class="p">)</code>&#13;
&#13;
<code class="c1"># pass our boundaries and category labels to the pandas `cut()` function</code>&#13;
<code class="n">loan_round</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">cut</code><code class="p">(</code><code class="n">ppp_data</code><code class="o">.</code><code class="n">DateApproved</code><code class="p">,</code>&#13;
                    <code class="n">bins</code><code class="o">=</code><code class="p">[</code><code class="n">program_start</code><code class="p">,</code><code class="n">second_round_start</code><code class="p">,</code> <code class="n">todays_date</code><code class="p">],</code>&#13;
                    <code class="n">labels</code><code class="o">=</code><code class="p">[</code><code class="s1">'first_round'</code><code class="p">,</code> <code class="s1">'maybe_second'</code><code class="p">])</code>&#13;
&#13;
<code class="c1"># insert the new column at the position we specify</code>&#13;
<code class="n">ppp_data</code><code class="o">.</code><code class="n">insert</code><code class="p">(</code><code class="mi">2</code><code class="p">,</code><code class="s1">'Loan Round'</code><code class="p">,</code><code class="n">loan_round</code><code class="p">)</code>&#13;
&#13;
<code class="c1"># this "pivot table" will return a Series showing the number</code>&#13;
<code class="c1"># of times a particular 'BorrowerNameFingerprint' appears in the dataset</code>&#13;
<code class="n">loan_count</code> <code class="o">=</code> <code class="n">ppp_data</code><code class="o">.</code><code class="n">pivot_table</code><code class="p">(</code><code class="n">index</code><code class="o">=</code><code class="p">[</code><code class="s1">'BorrowerNameFingerprint'</code><code class="p">],</code>&#13;
                                  <code class="n">aggfunc</code><code class="o">=</code><code class="s1">'size'</code><code class="p">)</code>&#13;
&#13;
<code class="c1"># convert our Series to a DataFrame and give it a name</code>&#13;
<code class="n">loan_count_df</code> <code class="o">=</code> <code class="n">loan_count</code><code class="o">.</code><code class="n">to_frame</code><code class="p">(</code><code class="s1">'Loan Count'</code><code class="p">)</code>&#13;
&#13;
<code class="c1"># use the `describe()` method to print out summary statistics</code>&#13;
<code class="k">print</code><code class="p">(</code><code class="s2">"Description of duplicate borrower table:"</code><code class="p">)</code>&#13;
<code class="k">print</code><code class="p">(</code><code class="n">loan_count_df</code><code class="o">.</code><code class="n">describe</code><code class="p">())</code>&#13;
&#13;
<code class="c1"># start by sorting our DataFrame of loan counts from greatest to least</code>&#13;
<code class="n">sorted_loan_counts</code> <code class="o">=</code> <code class="n">loan_count_df</code><code class="o">.</code><code class="n">sort_values</code><code class="p">(</code><code class="n">by</code><code class="o">=</code><code class="p">[</code><code class="s1">'Loan Count'</code><code class="p">],</code>&#13;
                                               <code class="n">ascending</code><code class="o">=</code><code class="bp">False</code><code class="p">)</code>&#13;
&#13;
<code class="c1"># create a new DataFrame with *only* those that have more than two loans</code>&#13;
<code class="n">more_than_two</code> <code class="o">=</code> <code class="n">sorted_loan_counts</code><code class="p">[</code><code class="n">sorted_loan_counts</code><code class="p">[</code><code class="s1">'Loan Count'</code><code class="p">]</code> <code class="o">&gt;</code> <code class="mi">2</code><code class="p">]</code>&#13;
&#13;
<code class="c1"># print one instance of each business name that appears in `more_than_two`</code>&#13;
<code class="k">print</code><code class="p">(</code><code class="s2">"Businesses that seem to have gotten more than 2 loans:"</code><code class="p">)</code>&#13;
<code class="k">print</code><code class="p">(</code><code class="n">more_than_two</code><code class="o">.</code><code class="n">shape</code><code class="p">)</code>&#13;
&#13;
<code class="k">print</code><code class="p">(</code><code class="s2">"Number of businesses that appear to have gotten precisely 2 loans:"</code><code class="p">)</code>&#13;
<code class="n">precisely_two</code> <code class="o">=</code> <code class="n">sorted_loan_counts</code><code class="p">[</code><code class="n">sorted_loan_counts</code><code class="p">[</code><code class="s1">'Loan Count'</code><code class="p">]</code> <code class="o">==</code> <code class="mi">2</code><code class="p">]</code>&#13;
<code class="k">print</code><code class="p">(</code><code class="n">precisely_two</code><code class="o">.</code><code class="n">shape</code><code class="p">)</code>&#13;
&#13;
<code class="c1"># use `ProcessingMethod` value to identify second-round loans</code>&#13;
<code class="n">pps_loans</code> <code class="o">=</code> <code class="n">ppp_data</code><code class="p">[</code><code class="n">ppp_data</code><code class="p">[</code><code class="s1">'ProcessingMethod'</code><code class="p">]</code> <code class="o">==</code> <code class="s1">'PPS'</code><code class="p">]</code>&#13;
&#13;
<code class="c1"># print out the `shape` of this DataFrame to see how many businesses we have</code>&#13;
<code class="k">print</code><code class="p">(</code><code class="s2">"Number of loans labeled as second round:"</code><code class="p">)</code>&#13;
<code class="k">print</code><code class="p">(</code><code class="n">pps_loans</code><code class="o">.</code><code class="n">shape</code><code class="p">)</code>&#13;
&#13;
<code class="c1"># how many loans in our derived data frame were approved for precisely $2M</code>&#13;
<code class="c1"># during the (possibly) second-round timeframe?</code>&#13;
&#13;
<code class="c1"># merge our `loan_count_df` back to keep track of businesses</code>&#13;
<code class="c1"># we labeled as having precisely two loans</code>&#13;
<code class="n">ppp_data_w_lc</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">merge</code><code class="p">(</code><code class="n">ppp_data</code><code class="p">,</code> <code class="n">loan_count_df</code><code class="p">,</code>&#13;
                         <code class="n">on</code><code class="o">=</code><code class="p">[</code><code class="s1">'BorrowerNameFingerprint'</code><code class="p">],</code> <code class="n">how</code><code class="o">=</code><code class="s1">'left'</code><code class="p">)</code>&#13;
&#13;
<code class="c1"># now get *all* the records of business names we associated with two loans</code>&#13;
<code class="n">matched_two_loans</code> <code class="o">=</code> <code class="n">ppp_data_w_lc</code><code class="p">[(</code><code class="n">ppp_data_w_lc</code><code class="p">[</code><code class="s1">'Loan Count'</code><code class="p">]</code> <code class="o">==</code> <code class="mi">2</code><code class="p">)]</code>&#13;
&#13;
<code class="c1"># select those loans our `maybe_second` loans that have a value of $2M</code>&#13;
<code class="n">maybe_round2_2M</code> <code class="o">=</code> <code class="n">matched_two_loans</code><code class="p">[</code>&#13;
                    <code class="p">(</code><code class="n">matched_two_loans</code><code class="p">[</code><code class="s1">'CurrentApprovalAmount'</code><code class="p">]</code> <code class="o">==</code> <code class="mf">2000000.00</code><code class="p">)</code> <code class="o">&amp;</code>&#13;
                    <code class="p">(</code><code class="n">matched_two_loans</code><code class="p">[</code><code class="s1">'Loan Round'</code><code class="p">]</code> <code class="o">==</code> <code class="s1">'maybe_second'</code><code class="p">)]</code>&#13;
<code class="k">print</code><code class="p">(</code><code class="s2">"Derived $2M second-round loans:"</code><code class="p">)</code>&#13;
<code class="k">print</code><code class="p">(</code><code class="n">maybe_round2_2M</code><code class="o">.</code><code class="n">shape</code><code class="p">)</code>&#13;
&#13;
<code class="c1"># select those loans that we *know* are second round and have a value of $2M</code>&#13;
<code class="n">pps_got_2M</code> <code class="o">=</code> <code class="n">pps_loans</code><code class="p">[</code><code class="n">pps_loans</code><code class="p">[</code><code class="s1">'CurrentApprovalAmount'</code><code class="p">]</code> <code class="o">==</code> <code class="mf">2000000.00</code><code class="p">]</code>&#13;
<code class="k">print</code><code class="p">(</code><code class="s2">"Actual $2M second-round loans:"</code><code class="p">)</code>&#13;
<code class="k">print</code><code class="p">(</code><code class="n">pps_got_2M</code><code class="o">.</code><code class="n">shape</code><code class="p">)</code>&#13;
&#13;
<code class="c1"># isolate the fingerprints of businesses that got $2M second-round loans approved</code>&#13;
<code class="n">biz_names</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">unique</code><code class="p">(</code><code class="n">pps_got_2M</code><code class="p">[</code><code class="s1">'BorrowerNameFingerprint'</code><code class="p">])</code>&#13;
&#13;
<code class="c1"># convert that list to a DataFrame</code>&#13;
<code class="n">biz_names_df</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">biz_names</code><code class="p">,</code> <code class="n">columns</code><code class="o">=</code><code class="p">[</code><code class="s1">'BorrowerNameFingerprint'</code><code class="p">])</code>&#13;
&#13;
<code class="c1"># create a new array of the same length as our biz_names_df and fill with</code>&#13;
<code class="c1"># a flag value</code>&#13;
<code class="n">fill_column</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">full</code><code class="p">((</code><code class="nb">len</code><code class="p">(</code><code class="n">biz_names</code><code class="p">),</code><code class="mi">1</code><code class="p">),</code> <code class="s1">'2Mil2ndRnd'</code><code class="p">)</code>&#13;
<code class="n">biz_names_df</code><code class="p">[</code><code class="s1">'GotSecond'</code><code class="p">]</code> <code class="o">=</code> <code class="n">fill_column</code>&#13;
&#13;
<code class="c1"># now merge this new, two-column DataFrame back onto our full_data list,</code>&#13;
<code class="c1"># so that we (hopefully) find their first-round loans as well</code>&#13;
<code class="n">second_round_max</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">merge</code><code class="p">(</code><code class="n">ppp_data_w_lc</code><code class="p">,</code> <code class="n">biz_names_df</code><code class="p">,</code>&#13;
                            <code class="n">on</code><code class="o">=</code><code class="s1">'BorrowerNameFingerprint'</code><code class="p">)</code>&#13;
&#13;
<code class="c1"># now all the loans that share fingerprints with the ones that got the max</code>&#13;
<code class="c1"># amount in the second round should have the flag value '2Mil2ndRnd' in the</code>&#13;
<code class="c1"># 'GotSecond' column</code>&#13;
<code class="n">second_max_all_loans</code> <code class="o">=</code> <code class="n">second_round_max</code><code class="p">[</code>&#13;
                                <code class="n">second_round_max</code><code class="p">[</code><code class="s1">'GotSecond'</code><code class="p">]</code> <code class="o">==</code> <code class="s1">'2Mil2ndRnd'</code><code class="p">]</code>&#13;
&#13;
<code class="c1"># we expect this to be twice the number of businesses that received $2M</code>&#13;
<code class="c1"># second-round loans</code>&#13;
<code class="k">print</code><code class="p">(</code><code class="s1">'Total # of loans approved for most orgs that got $2M for second round:'</code><code class="p">)</code>&#13;
<code class="k">print</code><code class="p">(</code><code class="n">second_max_all_loans</code><code class="o">.</code><code class="n">shape</code><code class="p">)</code>&#13;
&#13;
<code class="c1"># how much money were these businesses approved to get from the PPP, total?</code>&#13;
<code class="n">total_funds</code> <code class="o">=</code> <code class="n">second_max_all_loans</code><code class="p">[</code><code class="s1">'CurrentApprovalAmount'</code><code class="p">]</code><code class="o">.</code><code class="n">sum</code><code class="p">()</code>&#13;
<code class="k">print</code><code class="p">(</code><code class="s2">"Total funds approved for identified orgs that could have "</code> <code class="o">+</code> \&#13;
      <code class="s2">"second-round max:"</code><code class="p">)</code>&#13;
<code class="k">print</code><code class="p">(</code><code class="n">total_funds</code><code class="p">)</code>&#13;
&#13;
<code class="c1"># plot our date-based `Loan Round`-labeled data next to records</code>&#13;
<code class="c1"># separated by `ProcessingMethod`. Do we get the same results?</code>&#13;
&#13;
<code class="c1"># set the seaborn theme</code>&#13;
<code class="n">sns</code><code class="o">.</code><code class="n">set_theme</code><code class="p">(</code><code class="n">style</code><code class="o">=</code><code class="s2">"whitegrid"</code><code class="p">)</code>&#13;
&#13;
<code class="c1"># use `matplotlib` `subplots()` to plot charts next to each other</code>&#13;
<code class="c1"># use `tuples` to access the different subplots later</code>&#13;
<code class="n">fig</code><code class="p">,</code> <code class="p">((</code><code class="n">row1col1</code><code class="p">,</code> <code class="n">row1col2</code><code class="p">))</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">(</code><code class="n">nrows</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code> <code class="n">ncols</code><code class="o">=</code><code class="mi">2</code><code class="p">)</code>&#13;
&#13;
<code class="c1"># plot the histogram of our date-based analysis</code>&#13;
<code class="n">date_based</code> <code class="o">=</code> <code class="n">sns</code><code class="o">.</code><code class="n">histplot</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="n">second_max_all_loans</code><code class="p">,</code> <code class="n">x</code><code class="o">=</code><code class="s1">'CurrentApprovalAmount'</code><code class="p">,</code>&#13;
                          <code class="n">hue</code><code class="o">=</code><code class="s1">'Loan Round'</code><code class="p">,</code> <code class="n">ax</code><code class="o">=</code><code class="n">row1col1</code><code class="p">)</code>&#13;
&#13;
<code class="c1"># plot the histogram of our data-based analysis</code>&#13;
<code class="n">data_based</code> <code class="o">=</code> <code class="n">sns</code><code class="o">.</code><code class="n">histplot</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="n">second_max_all_loans</code><code class="p">,</code> <code class="n">x</code><code class="o">=</code><code class="s1">'CurrentApprovalAmount'</code><code class="p">,</code>&#13;
                          <code class="n">hue</code><code class="o">=</code><code class="s1">'ProcessingMethod'</code><code class="p">,</code> <code class="n">ax</code><code class="o">=</code><code class="n">row1col2</code><code class="p">)</code>&#13;
&#13;
<code class="c1"># show the plot!</code>&#13;
<code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">()</code></pre></div>&#13;
&#13;
<p>Running this script will give us all the output from previous examples, along with yet a few more lines of additional output:</p>&#13;
&#13;
<pre data-type="programlisting">Total # of loans approved for most orgs that got $2M for second round:&#13;
(2634, 54)&#13;
Total funds approved for identified orgs that could have second-round max:&#13;
6250357574.44</pre>&#13;
&#13;
<p>At first, it looks like something’s off, because we might have expected our total number of loans to be 2 × 1,175 = 2,350. But remember that we matched up loans based on whether they got approved for <em>exactly</em> $2M in round two, <em>and</em> we failed to match 284 loans on <code>BorrowerNameFingerprint</code>. This means we have <em>all</em> second-round loans but are missing 284 first-round loans in these numbers. In other words, we’d <em>expect</em> to have (2 × 1,175) + 284 = 2,634—and we do! Good! It’s always nice when <em>something</em> matches up. This means that our “total” figure, while still not 100% accurate, is a somewhat reasonable estimate of the <em>minimum</em> total loan amount this group of businesses were approved for in PPP funds: around $6 billion dollars.</p>&#13;
&#13;
<p>Finally, let’s take a look at the visualization shown in <a data-type="xref" href="#two_loan_business_amounts_by_round">Figure 9-6</a>, which is a view of the graphic generated by the script in which we can compare how our <code>Loan Round</code> classification matches up against the designated <code>PPS</code> loans. This is a rough (but still useful) way to validate our work—and the results look pretty good!<sup><a data-type="noteref" href="ch09.html#idm45143396503936" id="idm45143396503936-marker">14</a></sup></p>&#13;
&#13;
<figure><div class="figure" id="two_loan_business_amounts_by_round">&#13;
<img alt="Dollar amount of most approved loans for businesses that received two PPP loans, by loan round." src="assets/ppdw_0906.png"/>&#13;
<h6><span class="label">Figure 9-6. </span>Dollar amount of most approved loans for businesses that received two PPP loans, by loan round</h6>&#13;
</div></figure>&#13;
&#13;
<p>Interestingly, though, <a data-type="xref" href="#two_loan_business_amounts_by_round">Figure 9-6</a> also illustrates something else: it seems that a fair number of the businesses approved for $2M in second-round loans violate our earlier hypothesis that companies approved for $2M in second-round loans were approved for <em>more</em> than that amount in their first-round loans, when the limits were higher. As usual, in answering one question, we’ve generated another! And of course the work we’ve already done would give us a head start down the road to answering it. Before we let loose on our next round of question-and-answer, though, we need to talk about one more essential component of data analysis and <a data-primary="data validation" data-startref="data-valid" data-type="indexterm" id="idm45143395858544"/><a data-primary="validating data" data-startref="valid-data" data-type="indexterm" id="idm45143395857568"/><a data-primary="Python" data-secondary="validating data" data-startref="python-valid-data" data-type="indexterm" id="idm45143395856624"/><a data-primary="data analysis" data-secondary="visualization in" data-startref="data-analysis-visual" data-type="indexterm" id="idm45143395855408"/><a data-primary="visualizations" data-secondary="in data analysis" data-secondary-sortas="data analysis" data-startref="visual-data-analysis" data-type="indexterm" id="idm45143395854496"/><a data-primary="data analysis" data-secondary="visualization in" data-startref="data-analysis-visual-clusters" data-tertiary="clusters" data-type="indexterm" id="idm45143395853168"/><a data-primary="visualizations" data-secondary="in data analysis" data-secondary-sortas="data analysis" data-startref="visual-data-analysis-clusters" data-tertiary="clusters" data-type="indexterm" id="idm45143395851840"/><a data-primary="histograms" data-secondary="clusters" data-startref="histogram-clusters" data-type="indexterm" id="idm45143395850272"/><a data-primary="distributions" data-secondary="clusters" data-startref="distribution-clusters" data-type="indexterm" id="idm45143395849056"/><a data-primary="clusters" data-startref="clusters" data-type="indexterm" id="idm45143395847840"/><a data-primary="density (of data)" data-startref="density" data-type="indexterm" id="idm45143395846896"/><a data-primary="PPP (Paycheck Protection Program) example" data-secondary="clusters" data-startref="ppp-cluster" data-type="indexterm" id="idm45143395845952"/><a data-primary="Python" data-secondary="clusters of data" data-startref="python-clusters" data-type="indexterm" id="idm45143395844640"/>interpretation: <em>proportionality</em>.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Proportional Response" data-type="sect1"><div class="sect1" id="idm45143397177344">&#13;
<h1>Proportional Response</h1>&#13;
&#13;
<p>Imagine <a data-primary="data analysis" data-secondary="visualization in" data-tertiary="proportionality" data-type="indexterm" id="data-analysis-visual-proportion"/><a data-primary="visualizations" data-secondary="in data analysis" data-secondary-sortas="data analysis" data-tertiary="proportionality" data-type="indexterm" id="visual-data-analysis-proportion"/><a data-primary="proportionality" data-secondary="in data visualization" data-secondary-sortas="data visualization" data-type="indexterm" id="proportionality"/><a data-primary="rationalizing data" data-type="indexterm" id="rationalizing"/><a data-primary="PPP (Paycheck Protection Program) example" data-secondary="proportionality" data-type="indexterm" id="ppp-proportion"/>you go out to eat with some friends. You’ve eaten recently, so you just order a drink, but your three friends are ravenous and each order a full meal. How do you decide who owes what when the check arrives? Most of us would agree that the most sensible thing to do would be to calculate—or at least estimate—what <em>proportion</em> of the total bill each person’s order accounted for, and then have each person pay that, along with that same proportion of, say, the tax and tip.</p>&#13;
&#13;
<p>The same sort of logic applies when we’re analyzing data. In <a data-type="xref" href="#two_million_question">“The $2 Million Question”</a>, we looked at the <em>total</em> funds that had been approved for a certain subset of businesses through the PPP, and while $6B sounds like a lot, we should arguably be more interested in how those businesses <em>used</em> that money, rather than the absolute number of dollars they got. Since the PPP was designed to keep people on the payroll, one thing we might want to know is how much money those businesses received <em>in relation to how many jobs they preserved</em>, a process I think of as <em>rationalizing</em> the data.<sup><a data-type="noteref" href="ch09.html#idm45143395830240" id="idm45143395830240-marker">15</a></sup></p>&#13;
&#13;
<p>Fortunately, the process of rationalizing our data is extremely simple: we calculate the <em>ratio</em> between two data values by dividing one number by the other. For example, if we want to know how many dollars per job the companies identified in <a data-type="xref" href="#two_million_question">“The $2 Million Question”</a> spent, we can (after some sanity checking) divide the value in <code>PAYROLL_PROCEED</code> by the value in <code>JobsReported</code> for each record, as shown in <a data-type="xref" href="#dollars_per_job_2M_rnd2">Example 9-9</a>.</p>&#13;
<div data-type="example" id="dollars_per_job_2M_rnd2">&#13;
<h5><span class="label">Example 9-9. </span>dollars_per_job_2M_rnd2.py</h5>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="c1"># `pandas` for data loading/transformations</code><code>&#13;
</code><code class="kn">import</code><code> </code><code class="nn">pandas</code><code> </code><code class="kn">as</code><code> </code><code class="nn">pd</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># `seaborn` for visualization</code><code>&#13;
</code><code class="kn">import</code><code> </code><code class="nn">seaborn</code><code> </code><code class="kn">as</code><code> </code><code class="nn">sns</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># `matplotlib` for customizing visuals</code><code>&#13;
</code><code class="kn">import</code><code> </code><code class="nn">matplotlib.pyplot</code><code> </code><code class="kn">as</code><code> </code><code class="nn">plt</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># `numpy` for manipulating arrays/lists</code><code>&#13;
</code><code class="kn">import</code><code> </code><code class="nn">numpy</code><code> </code><code class="kn">as</code><code> </code><code class="nn">np</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># load our data</code><code>&#13;
</code><code class="n">ppp_data</code><code> </code><code class="o">=</code><code> </code><code class="n">pd</code><code class="o">.</code><code class="n">read_csv</code><code class="p">(</code><code class="s1">'</code><code class="s1">public_150k_plus_borrower_fingerprint_a.csv</code><code class="s1">'</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># first, sanity check our data</code><code>&#13;
</code><code class="k">print</code><code class="p">(</code><code class="n">ppp_data</code><code class="p">[</code><code class="n">ppp_data</code><code class="p">[</code><code class="s1">'</code><code class="s1">JobsReported</code><code class="s1">'</code><code class="p">]</code><code> </code><code class="o">&lt;</code><code class="o">=</code><code> </code><code class="mi">0</code><code class="p">]</code><code class="p">)</code><code> </code><a class="co" href="#callout_introduction_to_data_analysis_CO4-1" id="co_introduction_to_data_analysis_CO4-1"><img alt="1" src="assets/1.png"/></a><code>&#13;
</code><code>&#13;
</code><code class="c1"># drop the records with no value in `JobsReported`</code><code>&#13;
</code><code class="n">ppp_data</code><code class="o">.</code><code class="n">drop</code><code class="p">(</code><code class="n">labels</code><code class="o">=</code><code class="p">[</code><code class="mi">437083</code><code class="p">,</code><code class="mi">765398</code><code class="p">]</code><code class="p">,</code><code> </code><code class="n">axis</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code><code> </code><a class="co" href="#callout_introduction_to_data_analysis_CO4-1" id="co_introduction_to_data_analysis_CO4-2"><img alt="1" src="assets/1.png"/></a><code>&#13;
</code><code>&#13;
</code><code class="c1"># calculate the dollars per job</code><code>&#13;
</code><code class="n">dollars_per_job</code><code> </code><code class="o">=</code><code> </code><code class="n">ppp_data</code><code class="p">[</code><code class="s1">'</code><code class="s1">CurrentApprovalAmount</code><code class="s1">'</code><code class="p">]</code><code class="o">/</code><code class="n">ppp_data</code><code class="p">[</code><code class="s1">'</code><code class="s1">JobsReported</code><code class="s1">'</code><code class="p">]</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># insert the new column into our original dataset</code><code>&#13;
</code><code class="n">ppp_data</code><code class="o">.</code><code class="n">insert</code><code class="p">(</code><code class="mi">3</code><code class="p">,</code><code> </code><code class="s1">'</code><code class="s1">Dollars per Job</code><code class="s1">'</code><code class="p">,</code><code> </code><code class="n">dollars_per_job</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># use `ProcessingMethod` value to identify second-round loans</code><code>&#13;
</code><code class="n">pps_loans</code><code> </code><code class="o">=</code><code> </code><code class="n">ppp_data</code><code class="p">[</code><code class="n">ppp_data</code><code class="p">[</code><code class="s1">'</code><code class="s1">ProcessingMethod</code><code class="s1">'</code><code class="p">]</code><code> </code><code class="o">==</code><code> </code><code class="s1">'</code><code class="s1">PPS</code><code class="s1">'</code><code class="p">]</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># select all second-round loans that have a value of $2M</code><code>&#13;
</code><code class="n">pps_got_2M</code><code> </code><code class="o">=</code><code> </code><code class="n">pps_loans</code><code class="p">[</code><code class="n">pps_loans</code><code class="p">[</code><code class="s1">'</code><code class="s1">CurrentApprovalAmount</code><code class="s1">'</code><code class="p">]</code><code> </code><code class="o">==</code><code> </code><code class="mf">2000000.00</code><code class="p">]</code><code>&#13;
</code><code class="k">print</code><code class="p">(</code><code class="s2">"</code><code class="s2">Actual $2M second-round loans:</code><code class="s2">"</code><code class="p">)</code><code>&#13;
</code><code class="k">print</code><code class="p">(</code><code class="n">pps_got_2M</code><code class="o">.</code><code class="n">shape</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># pull fingerprints of businesses approved for $2M second-round loans</code><code>&#13;
</code><code class="n">biz_names</code><code> </code><code class="o">=</code><code> </code><code class="n">pd</code><code class="o">.</code><code class="n">unique</code><code class="p">(</code><code class="n">pps_got_2M</code><code class="p">[</code><code class="s1">'</code><code class="s1">BorrowerNameFingerprint</code><code class="s1">'</code><code class="p">]</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># convert that list to a DataFrame</code><code>&#13;
</code><code class="n">biz_names_df</code><code> </code><code class="o">=</code><code> </code><code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">biz_names</code><code class="p">,</code><code> </code><code class="n">columns</code><code class="o">=</code><code class="p">[</code><code class="s1">'</code><code class="s1">BorrowerNameFingerprint</code><code class="s1">'</code><code class="p">]</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># create an array of the same length as `biz_names_df`; fill with flag value</code><code>&#13;
</code><code class="n">fill_column</code><code> </code><code class="o">=</code><code> </code><code class="n">np</code><code class="o">.</code><code class="n">full</code><code class="p">(</code><code class="p">(</code><code class="nb">len</code><code class="p">(</code><code class="n">biz_names</code><code class="p">)</code><code class="p">,</code><code class="mi">1</code><code class="p">)</code><code class="p">,</code><code> </code><code class="s1">'</code><code class="s1">2Mil2ndRnd</code><code class="s1">'</code><code class="p">)</code><code>&#13;
</code><code class="n">biz_names_df</code><code class="p">[</code><code class="s1">'</code><code class="s1">GotSecond</code><code class="s1">'</code><code class="p">]</code><code> </code><code class="o">=</code><code> </code><code class="n">fill_column</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># now merge this new, two-column DataFrame back onto our full_data list</code><code>&#13;
</code><code class="n">second_round_max</code><code> </code><code class="o">=</code><code> </code><code class="n">pd</code><code class="o">.</code><code class="n">merge</code><code class="p">(</code><code class="n">ppp_data</code><code class="p">,</code><code> </code><code class="n">biz_names_df</code><code class="p">,</code><code> </code><code class="n">on</code><code class="o">=</code><code class="s1">'</code><code class="s1">BorrowerNameFingerprint</code><code class="s1">'</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># all loans whose fingerprints match those of businesses that got $2M</code><code>&#13;
</code><code class="c1"># in the second round should have `2Mil2ndRnd` in the `GotSecond` column</code><code>&#13;
</code><code class="n">second_max_all_loans</code><code> </code><code class="o">=</code><code> </code><code class="n">second_round_max</code><code class="p">[</code><code>&#13;
</code><code>                                </code><code class="n">second_round_max</code><code class="p">[</code><code class="s1">'</code><code class="s1">GotSecond</code><code class="s1">'</code><code class="p">]</code><code> </code><code class="o">==</code><code> </code><code class="s1">'</code><code class="s1">2Mil2ndRnd</code><code class="s1">'</code><code class="p">]</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># sbould be 2x the number of businesses approved for $2M second-round</code><code>&#13;
</code><code class="k">print</code><code class="p">(</code><code class="s1">'</code><code class="s1">Total # of loans approved for most orgs that got $2M for second round:</code><code class="s1">'</code><code class="p">)</code><code>&#13;
</code><code class="k">print</code><code class="p">(</code><code class="n">second_max_all_loans</code><code class="o">.</code><code class="n">shape</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># how much money were these businesses approved to get from the PPP, total?</code><code>&#13;
</code><code class="n">total_funds</code><code> </code><code class="o">=</code><code> </code><code class="n">second_max_all_loans</code><code class="p">[</code><code class="s1">'</code><code class="s1">CurrentApprovalAmount</code><code class="s1">'</code><code class="p">]</code><code class="o">.</code><code class="n">sum</code><code class="p">(</code><code class="p">)</code><code>&#13;
</code><code class="k">print</code><code class="p">(</code><code class="s2">"</code><code class="s2">Total funds approved for identified orgs that could have </code><code class="s2">"</code><code> </code><code class="o">+</code><code> </code><code>\&#13;
</code><code>      </code><code class="s2">"</code><code class="s2">second-round max:</code><code class="s2">"</code><code class="p">)</code><code>&#13;
</code><code class="k">print</code><code class="p">(</code><code class="n">total_funds</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># now, let's plot that new column on our selected dataset</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># set the seaborn theme</code><code>&#13;
</code><code class="n">sns</code><code class="o">.</code><code class="n">set_theme</code><code class="p">(</code><code class="n">style</code><code class="o">=</code><code class="s2">"</code><code class="s2">whitegrid</code><code class="s2">"</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># the `matplotlib` `subplots()` to plot charts side by side</code><code>&#13;
</code><code class="n">fig</code><code class="p">,</code><code> </code><code class="p">(</code><code class="p">(</code><code class="n">row1col1</code><code class="p">)</code><code class="p">)</code><code> </code><code class="o">=</code><code> </code><code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">(</code><code class="n">nrows</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code><code> </code><code class="n">ncols</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># plot the histogram of our date-based analysis</code><code>&#13;
</code><code class="n">date_based</code><code> </code><code class="o">=</code><code> </code><code class="n">sns</code><code class="o">.</code><code class="n">histplot</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="n">second_max_all_loans</code><code class="p">,</code><code> </code><code class="n">x</code><code class="o">=</code><code class="s1">'</code><code class="s1">Dollars per Job</code><code class="s1">'</code><code class="p">,</code><code>&#13;
</code><code>                          </code><code class="n">hue</code><code class="o">=</code><code class="s1">'</code><code class="s1">ProcessingMethod</code><code class="s1">'</code><code class="p">,</code><code> </code><code class="n">ax</code><code class="o">=</code><code class="n">row1col1</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># show the plots!</code><code>&#13;
</code><code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">(</code><code class="p">)</code></pre></div>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_introduction_to_data_analysis_CO4-1" id="callout_introduction_to_data_analysis_CO4-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>It turns out that a couple of businesses didn’t report <em>any</em> jobs, which will break our calculation. Since there are only two records guilty of this, we’ll just drop them, using their <em>pandas</em>-assigned row labels.</p></dd>&#13;
</dl>&#13;
&#13;
<p>While the text output here confirms that we’re looking at the same set of loans that we examined in <a data-type="xref" href="#two_million_question">“The $2 Million Question”</a>, our rationalized data highlights some noteworthy anomalies in some of the first-round loans, where a handful of companies appear to have had loans approved that allocated more for payroll than the $100,000-per-job limit allowed, as shown in <a data-type="xref" href="#dollars_per_job">Figure 9-7</a>.</p>&#13;
&#13;
<figure><div class="figure" id="dollars_per_job">&#13;
<img alt="Detail of dollars per job of companies approved for $2M in second-round loans" src="assets/ppdw_0907.png"/>&#13;
<h6><span class="label">Figure 9-7. </span>Detail of dollars per job of companies approved for $2M in second-round loans</h6>&#13;
</div></figure>&#13;
&#13;
<p>What do we make of this? You may notice that by now we’ve veered a rather long way from the question we posed back in <a data-type="xref" href="ch06.html#pandemic_and_ppp">“The Pandemic and the PPP”</a>, where we started out trying to assess whether the PPP had helped “save” American businesses. While that focus helped us work our way through our data quality evaluations, doing some contextual analysis has opened up a number of new questions and directions—which I think you’ll find is a pretty common occurrence when it comes to data wrangling. Hopefully, though, this will just encourage you to keep working with new datasets to see what else you can<a data-primary="data analysis" data-secondary="visualization in" data-startref="data-analysis-visual-proportion" data-tertiary="proportionality" data-type="indexterm" id="idm45143395713072"/><a data-primary="visualizations" data-secondary="in data analysis" data-secondary-sortas="data analysis" data-startref="visual-data-analysis-proportion" data-tertiary="proportionality" data-type="indexterm" id="idm45143395711616"/><a data-primary="proportionality" data-secondary="in data visualization" data-secondary-sortas="data visualization" data-startref="proportionality" data-type="indexterm" id="idm45143395696480"/><a data-primary="rationalizing data" data-startref="rationalizing" data-type="indexterm" id="idm45143395694992"/><a data-primary="PPP (Paycheck Protection Program) example" data-secondary="proportionality" data-startref="ppp-proportion" data-type="indexterm" id="idm45143395694048"/> find!</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Conclusion" data-type="sect1"><div class="sect1" id="idm45143395562784">&#13;
<h1>Conclusion</h1>&#13;
&#13;
<p>After all these analyses, we’ve learned a few new things—some of which are specific to this dataset but many of which are far more generally applicable:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>A relatively small number of companies were approved for the maximum allowable second-round amount in the PPP loan program. While many of them had filed for much more than that for their first-round loan, some did not.</p>&#13;
</li>&#13;
<li>&#13;
<p>A handful of companies that were approved for a $2M second-round loan claimed more than $100,000 per reported job in their first-round loan.</p>&#13;
</li>&#13;
<li>&#13;
<p>Human-entered data is always a mess. That’s why data cleaning is an ongoing, iterative process. Documenting your work is essential to being able to defend your results.</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>So, our introductory data analysis left us with far more questions than answers. At this point, there’s only one way we’re going to find out more: talking to people. Sure, some of the patterns we uncovered  <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3906395">look questionable</a>, but we have far too many unknowns to make any well-supported claims at this point. For example, many of the $2M second-round loans had yet to be disbursed when this data was released, so some companies might have actually gotten or used far less than that. Since the PPP rules only require that a minimum percentage of a loan is spent on payroll in order to be forgivable, companies that appear to have been approved for too much may have simply used the difference on other allowable expenses, like mortgage interest or health care costs. In other words, while we can learn a little bit from this type of numerical data analysis, it will never be enough to tell us the whole story—either the how or the why. That is something for which we need direct human input.</p>&#13;
&#13;
<p>Once we’ve done that work, and are clear about what insights we want to share, we are ready to begin thinking about the most effective way to convey what we’ve learned to others. Just as our data analysis relies on both data <em>and</em> human judgment and input, the most effective data communications almost always involve a balance between words and visualizations. As we’ll see in the next chapter, by crafting both our words <em>and</em> our visualizations with care, we can better ensure that our intended message truly gets heard.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<div data-type="footnotes"><p data-type="footnote" id="idm45143400237136"><sup><a href="ch09.html#idm45143400237136-marker">1</a></sup> See <a class="orm:hideurl" href="https://danariely.com/books/predictably-irrational/"><em>Predictably Irrational</em></a> by Dan Ariely (Harper) for more information.</p><p data-type="footnote" id="idm45143398320448"><sup><a href="ch09.html#idm45143398320448-marker">2</a></sup> You can technically also sort from highest to lowest, but starting with lower values is conventional and will make things easier in the long run.</p><p data-type="footnote" id="idm45143399041152"><sup><a href="ch09.html#idm45143399041152-marker">3</a></sup> There are actually multiple methods of choosing the median value for an even number of data points, but as long as you’re consistent, any of them is fine. Anecdotally, this is the approach that feels most intuitive and that I’ve seen used most often.</p><p data-type="footnote" id="idm45143398988128"><sup><a href="ch09.html#idm45143398988128-marker">4</a></sup> While the precise estimates for the number of items that humans can hold in <em>working memory</em> differ, researchers <em>do</em> agree that this capacity has limits. See <a href="https://ncbi.nlm.nih.gov/pmc/articles/PMC2864034"><em class="hyperlink">https://ncbi.nlm.nih.gov/pmc/articles/PMC2864034</em></a> and <a href="https://pnas.org/content/113/27/7459"><em class="hyperlink">https://pnas.org/content/113/27/7459</em></a>.</p><p data-type="footnote" id="idm45143398983968"><sup><a href="ch09.html#idm45143398983968-marker">5</a></sup> Although there is a long way to go, there is some <a href="https://dl.acm.org/doi/abs/10.1145/3373625.3418027">exciting research being done on tactile graphics</a> to reduce the vision dependency of these approaches, especially for folks who are blind or visually impaired. See <a href="http://shape.stanford.edu/research/constructiveVizAccess/assets20-88.pdf"><em class="hyperlink">http://shape.stanford.edu/research/constructiveVizAccess/assets20-88.pdf</em></a>.</p><p data-type="footnote" id="idm45143397925760"><sup><a href="ch09.html#idm45143397925760-marker">6</a></sup> See “A Morphometric Analysis of Ddt-Resistant and Non-Resistant House Fly Strains” by Robert R. Sokal and Preston E. Hunter, <a href="https://doi.org/10.1093/aesa/48.6.499"><em class="hyperlink">https://doi.org/10.1093/aesa/48.6.499</em></a>; the relevant data is provided there.</p><p data-type="footnote" id="idm45143397906944"><sup><a href="ch09.html#idm45143397906944-marker">7</a></sup> M. W. Toews, “CC BY 2.5,” <a href="https://creativecommons.org/licenses/by/2.5"><em class="hyperlink">https://creativecommons.org/licenses/by/2.5</em></a>, via <a href="https://en.wikipedia.org/wiki/Normal_distribution#/media/File:Standard_deviation_diagram.svg">Wikimedia Commons</a>.</p><p data-type="footnote" id="idm45143397217488"><sup><a href="ch09.html#idm45143397217488-marker">8</a></sup> Specifically relative to the loan amounts just above or below these values.</p><p data-type="footnote" id="idm45143397194032"><sup><a href="ch09.html#idm45143397194032-marker">9</a></sup> It happens.</p><p data-type="footnote" id="idm45143397083520"><sup><a href="ch09.html#idm45143397083520-marker">10</a></sup> If it’s <em>too</em> many, the output will say <code>Killed</code>. This is a sign you either need to close some apps or maybe move into the cloud.</p><p data-type="footnote" id="idm45143396792864"><sup><a href="ch09.html#idm45143396792864-marker">11</a></sup> See <a href="https://sba.gov/document/support-faq-ppp-borrowers-lenders"><em class="hyperlink">https://sba.gov/document/support-faq-ppp-borrowers-lenders</em></a> and <a href="https://sba.gov/document/support-sba-franchise-directory"><em class="hyperlink">https://sba.gov/document/support-sba-franchise-directory</em></a> for more information.</p><p data-type="footnote" id="idm45143396754128"><sup><a href="ch09.html#idm45143396754128-marker">12</a></sup> Though you can find it in the file <em>ppp_fingerprint_borrowers.py</em>.</p><p data-type="footnote" id="idm45143396564848"><sup><a href="ch09.html#idm45143396564848-marker">13</a></sup> Note that I’m intentionally doing this in a <em>slightly</em> roundabout way in order to demonstrate a few more data-wrangling and visualization strategies, but feel free to rework this code to be more efficient as an exercise!</p><p data-type="footnote" id="idm45143396503936"><sup><a href="ch09.html#idm45143396503936-marker">14</a></sup> If we compare the results numerically, we’ll find they’re identical, at least for our subset of companies approved for $2M in the second round.</p><p data-type="footnote" id="idm45143395830240"><sup><a href="ch09.html#idm45143395830240-marker">15</a></sup> This term has more specific meanings in the business and statistics/data-science worlds, but <em>proportionalizing</em> just sounds kind of awkward. Plus, it better matches the actual calculation process!</p></div></div></section></body></html>