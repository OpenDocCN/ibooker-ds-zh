["```py\nIn [1]: %matplotlib inline\n        import numpy as np\n        import matplotlib.pyplot as plt\n        plt.style.use('seaborn-whitegrid')\n```", "```py\nIn [2]: from sklearn.datasets import make_blobs\n\n        X, y = make_blobs(n_samples=300, centers=4,\n                          random_state=0, cluster_std=1.0)\n        plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='rainbow');\n```", "```py\nIn [3]: from sklearn.tree import DecisionTreeClassifier\n        tree = DecisionTreeClassifier().fit(X, y)\n```", "```py\nIn [4]: def visualize_classifier(model, X, y, ax=None, cmap='rainbow'):\n            ax = ax or plt.gca()\n\n            # Plot the training points\n            ax.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=cmap,\n                       clim=(y.min(), y.max()), zorder=3)\n            ax.axis('tight')\n            ax.axis('off')\n            xlim = ax.get_xlim()\n            ylim = ax.get_ylim()\n\n            # fit the estimator\n            model.fit(X, y)\n            xx, yy = np.meshgrid(np.linspace(*xlim, num=200),\n                                 np.linspace(*ylim, num=200))\n            Z = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n\n            # Create a color plot with the results\n            n_classes = len(np.unique(y))\n            contours = ax.contourf(xx, yy, Z, alpha=0.3,\n                                   levels=np.arange(n_classes + 1) - 0.5,\n                                   cmap=cmap, zorder=1)\n\n            ax.set(xlim=xlim, ylim=ylim)\n```", "```py\nIn [5]: visualize_classifier(DecisionTreeClassifier(), X, y)\n```", "```py\nIn [6]: # helpers_05_08 is found in the online appendix\n        import helpers_05_08\n        helpers_05_08.plot_tree_interactive(X, y);\nOut[6]: interactive(children=(Dropdown(description='depth', index=1, options=(1, 5),\n         > value=5), Output()), _dom_classes...\n```", "```py\nIn [7]: # helpers_05_08 is found in the online appendix\n        import helpers_05_08\n        helpers_05_08.randomized_tree_interactive(X, y)\nOut[7]: interactive(children=(Dropdown(description='random_state', options=(0, 100),\n         > value=0), Output()), _dom_classes...\n```", "```py\nIn [8]: from sklearn.tree import DecisionTreeClassifier\n        from sklearn.ensemble import BaggingClassifier\n\n        tree = DecisionTreeClassifier()\n        bag = BaggingClassifier(tree, n_estimators=100, max_samples=0.8,\n                                random_state=1)\n\n        bag.fit(X, y)\n        visualize_classifier(bag, X, y)\n```", "```py\nIn [9]: from sklearn.ensemble import RandomForestClassifier\n\n        model = RandomForestClassifier(n_estimators=100, random_state=0)\n        visualize_classifier(model, X, y);\n```", "```py\nIn [10]: rng = np.random.RandomState(42)\n         x = 10 * rng.rand(200)\n\n         def model(x, sigma=0.3):\n             fast_oscillation = np.sin(5 * x)\n             slow_oscillation = np.sin(0.5 * x)\n             noise = sigma * rng.randn(len(x))\n\n             return slow_oscillation + fast_oscillation + noise\n\n         y = model(x)\n         plt.errorbar(x, y, 0.3, fmt='o');\n```", "```py\nIn [11]: from sklearn.ensemble import RandomForestRegressor\n         forest = RandomForestRegressor(200)\n         forest.fit(x[:, None], y)\n\n         xfit = np.linspace(0, 10, 1000)\n         yfit = forest.predict(xfit[:, None])\n         ytrue = model(xfit, sigma=0)\n\n         plt.errorbar(x, y, 0.3, fmt='o', alpha=0.5)\n         plt.plot(xfit, yfit, '-r');\n         plt.plot(xfit, ytrue, '-k', alpha=0.5);\n```", "```py\nIn [12]: from sklearn.datasets import load_digits\n         digits = load_digits()\n         digits.keys()\nOut[12]: dict_keys(['data', 'target', 'frame', 'feature_names', 'target_names',\n          > 'images', 'DESCR'])\n```", "```py\nIn [13]: # set up the figure\n         fig = plt.figure(figsize=(6, 6))  # figure size in inches\n         fig.subplots_adjust(left=0, right=1, bottom=0, top=1,\n                             hspace=0.05, wspace=0.05)\n\n         # plot the digits: each image is 8x8 pixels\n         for i in range(64):\n             ax = fig.add_subplot(8, 8, i + 1, xticks=[], yticks=[])\n             ax.imshow(digits.images[i], cmap=plt.cm.binary, interpolation='nearest')\n\n             # label the image with the target value\n             ax.text(0, 7, str(digits.target[i]))\n```", "```py\nIn [14]: from sklearn.model_selection import train_test_split\n\n         Xtrain, Xtest, ytrain, ytest = train_test_split(digits.data, digits.target,\n                                                         random_state=0)\n         model = RandomForestClassifier(n_estimators=1000)\n         model.fit(Xtrain, ytrain)\n         ypred = model.predict(Xtest)\n```", "```py\nIn [15]: from sklearn import metrics\n         print(metrics.classification_report(ypred, ytest))\nOut[15]:               precision    recall  f1-score   support\n\n                    0       1.00      0.97      0.99        38\n                    1       0.98      0.98      0.98        43\n                    2       0.95      1.00      0.98        42\n                    3       0.98      0.96      0.97        46\n                    4       0.97      1.00      0.99        37\n                    5       0.98      0.96      0.97        49\n                    6       1.00      1.00      1.00        52\n                    7       1.00      0.96      0.98        50\n                    8       0.94      0.98      0.96        46\n                    9       0.98      0.98      0.98        47\n\n             accuracy                           0.98       450\n            macro avg       0.98      0.98      0.98       450\n         weighted avg       0.98      0.98      0.98       450\n```", "```py\nIn [16]: from sklearn.metrics import confusion_matrix\n         import seaborn as sns\n         mat = confusion_matrix(ytest, ypred)\n         sns.heatmap(mat.T, square=True, annot=True, fmt='d',\n                     cbar=False, cmap='Blues')\n         plt.xlabel('true label')\n         plt.ylabel('predicted label');\n```"]