- en: Chapter 4\. Working with File-Based and Feed-Based Data in Python
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第四章\. 在Python中处理基于文件和基于Feed的数据
- en: In [Chapter 3](ch03.html#chapter3), we focused on the many characteristics that
    contribute to data quality—from the completeness, consistency, and clarity of
    data *integrity* to the reliability, validity, and representativeness of data
    *fit*. We discussed the need to both “clean” and standardize data, as well as
    the need to augment it by combining it with other datasets. But how do we actually
    accomplish these things in practice?
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第三章](ch03.html#chapter3)中，我们着重讨论了对数据质量贡献巨大的许多特征——从数据*完整性*、一致性和清晰度到数据*适应性*的可靠性、有效性和代表性。我们讨论了清理和标准化数据的必要性，以及通过与其他数据集的结合来增强数据的必要性。但在实践中，我们如何实现这些目标呢？
- en: 'Obviously, it’s impossible to begin assessing the *quality* of a dataset without
    first reviewing its contents—but this is sometimes easier said than done. For
    decades, data wrangling was a highly specialized pursuit, leading companies and
    organizations to create a whole range of distinct (and sometimes proprietary)
    digital data formats designed to meet their particular needs. Often, these formats
    came with their own file extensions—some of which you may have seen: *xls*, *csv*,
    *dbf*, and *spss* are all file formats typically associated with “data” files.^([1](ch04.html#idm45143426969488))
    While their specific structures and details vary, all of these formats are what
    I would describe as *file-based*—that is, they contain (more or less) historical
    data in static files that can be downloaded from a database, emailed by a colleague,
    or accessed via file-sharing sites. Most significantly, a file-based dataset will,
    for the most part, contain the same information whether you open it today or a
    week, a month, or a year from now.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 很显然，要评估数据集的*质量*，首先需要审查其内容，但这有时说起来比做起来更容易。几十年来，数据整理一直是一项高度专业化的追求，导致公司和组织创建了各种不同（有时是专有的）数字数据格式，以满足其特定需求。通常，这些格式带有自己的文件扩展名——你可能见过其中的一些：*xls*、*csv*、*dbf*
    和 *spss* 都是通常与“数据”文件相关联的文件格式。^([1](ch04.html#idm45143426969488)) 尽管它们的具体结构和细节各不相同，但所有这些格式都可以被描述为*基于文件*的——也就是说，它们包含（或多或少）静态文件中的历史数据，可以从数据库下载、通过同事的电子邮件发送，或者通过文件共享站点访问。最重要的是，基于文件的数据集在今天打开和一周、一个月或一年后打开时，大部分情况下会包含相同的信息。
- en: Today, these file-based formats stand in contrast to the data formats and interfaces
    that have emerged alongside real-time web services over the past 20 years. Web-based
    data today is available for everything from news to weather monitoring to social
    media sites, and these feed-style data sources have their own unique formats and
    structures. Extensions like *xml*, *json*, and *rss* indicate this type of real-time
    data, which often needs to be accessed via specialized application programming
    interfaces, or APIs. Unlike file-based formats, accessing the same web-based data
    location or “endpoint” via an API will always show you the most *recent* data
    available—and that data may change in days, hours, or even seconds.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 如今，这些基于文件的格式与过去20年中伴随实时网络服务而出现的数据格式和接口形成鲜明对比。网络数据今天涵盖从新闻到天气监测再到社交媒体站点的一切，这些Feed风格的数据源具有其独特的格式和结构。像*xml*、*json*
    和 *rss* 这样的扩展表示这种实时数据类型，通常需要通过专门的应用程序接口或API访问。与基于文件的格式不同，通过API访问同一网络数据位置或“端点”将始终显示出当前可用的最*新*数据——而这些数据可能在几天、几小时甚至几秒钟内发生变化。
- en: These aren’t perfect distinctions, of course. There are many organizations (especially
    government departments) that provide file-based data for download—but then overwrite
    those files with new ones that have the same name whenever the source data is
    updated. At the same time, feed-style data formats *can* be downloaded and saved
    for future reference—but their source location online will not generally provide
    access to older versions. Despite these sometimes unconventional uses for each
    class of data format, however, in most cases you can use the high-level differences
    between file-based and feed-based data formats to help you choose the most appropriate
    sources for a given data wrangling project.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这些并不是完美的区分。许多组织（特别是政府部门）提供可以下载的基于文件的数据，但当源数据更新时，会覆盖这些文件，并使用相同的名称创建新的文件。同时，Feed风格的数据格式*可以*被下载并保存以供将来参考——但在线的源位置通常不提供访问旧版本的权限。尽管每种数据格式类别有时会有非常不寻常的用途，但在大多数情况下，您可以利用基于文件和基于Feed的数据格式之间的高级别区别，来帮助选择适合特定数据整理项目的最合适的数据源。
- en: 'How do you know if you want file-based or feed-based data? In many cases, you
    won’t have a choice. Social media companies, for example, provide ready access
    to their data feeds through their APIs but don’t generally provide retrospective
    data. Other types of data—especially data that is itself synthesized from other
    sources or heavily reviewed before release—are much more likely to be made available
    in file-based formats. If you *do* have a choice between file-based and feed-based
    formats, then which you choose will really depend on the nature of your data wrangling
    question: if it hinges on having the most *recent* data available, then a feed-style
    format will probably be preferable. But if you’re concerned about *trends*, file-based
    data, which is more likely to contain information collected over time, will probably
    be your best bet. That said, even when both formats are available, there’s no
    guarantee they contain the same fields, which once again might make your decision
    for you.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 如何知道你需要基于文件还是基于源的数据？在许多情况下，你没有选择权。例如，社交媒体公司通过其 API 提供对其数据源的即时访问，但通常不提供回顾性数据。其他类型的数据——特别是那些自其他来源综合或在发布前经过大量审查的数据——更有可能以基于文件的格式提供。如果你确实可以在基于文件和基于源的格式之间进行选择，那么你的选择将取决于你的数据整理问题的性质：如果关键在于拥有最近的可用数据，那么可能基于源的格式更可取。但是，如果你关注的是趋势，那么基于文件的数据，更有可能包含随时间收集的信息，可能是你的最佳选择。尽管如此，即使两种格式都可用，也不能保证它们包含相同的字段，这可能再次为你做出决定。
- en: Over the course of this chapter, we’ll work through hands-on examples of wrangling
    data from several of the most common file-based and feed-based data formats, with
    the goal of making them easier to review, clean, augment, and analyze. We’ll also
    take a look at some of the tougher-to-wrangle data formats that you might need
    to work with strictly out of necessity. Throughout these processes, we’ll rely
    heavily on the excellent variety of libraries that the Python community has developed
    for these purposes, including specialty libraries and programs for processing
    everything from spreadsheets to images. By the time we finish, you’ll have the
    skills and sample scripts you need to tackle a huge variety of data wrangling
    projects, paving the way forward for your next data wrangling project!
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将通过实际示例，从几种最常见的基于文件和基于源的数据格式中整理数据，目标是使它们更容易审查、清洗、增强和分析。我们还将查看一些可能需要出于必要性而处理的更难处理的数据格式。在这些过程中，我们将大量依赖
    Python 社区为这些目的开发的出色的各种库，包括用于处理从电子表格到图像等各种内容的专业库和程序。到我们完成时，你将具备处理各种数据整理项目所需的技能和示例脚本，为你的下一个数据整理项目铺平道路！
- en: Structured Versus Unstructured Data
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结构化数据与非结构化数据
- en: Before we dive into writing code and wrangling data, I want to briefly discuss
    one other key attribute of data sources that can impact the direction (and speed)
    of your data wrangling projects—working with *structured* versus *unstructured*
    data.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入编写代码和整理数据之前，我想简要讨论另一个可能影响数据整理项目方向（和速度）的数据源的关键属性——处理*结构化*数据与*非结构化*数据。
- en: 'The goal of most data wrangling projects is to generate insight and, often,
    to use data to make better decisions. But decisions are time sensitive, so our
    work with data also requires balancing trade-offs: instead of waiting for the
    “perfect” dataset, we may combine two or three not-so-perfect ones in order to
    build a valid approximation of the phenomenon we’re investigating, or we may look
    for datasets that share common identifiers (for example, zip codes), even if that
    means we need to later derive the particular dimensional structure (like neighborhood)
    that truly interests us. As long as we can gain these efficiencies without sacrificing
    too much in terms of data quality, improving the timeliness of our data work can
    also increase its impact.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数数据整理项目的目标是产生见解，并且通常是使用数据做出更好的决策。但决策是时间敏感的，因此我们与数据的工作也需要权衡取舍：我们可能不会等待“完美”的数据集，而是可能结合两个或三个不太完美的数据集，以便建立我们正在调查的现象的有效近似，或者我们可能寻找具有共同标识符（例如邮政编码）的数据集，即使这意味着我们需要稍后推导出真正感兴趣的特定维度结构（比如街区）。只要我们能在不牺牲太多数据质量的情况下获得这些效益，提高我们数据工作的及时性也可以增加其影响力。
- en: One of the simplest ways to make our data wrangling more efficient is to seek
    out data formats that are easy for Python and other computational tools to access
    and understand. Although advances in computer vision, natural language processing,
    and machine learning have made it easier for computers to analyze data regardless
    of its underlying structure or format, the fact is that *structured*, *machine-readable*
    data remains—unsurprisingly, perhaps—the most straightforward type of data to
    work with. In truth, while anything from interviews to images to the text of books
    can be used as a data source, when many of us think of “data,” we often think
    of structured, numerical data more than anything else.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 使我们的数据处理更有效的最简单方法之一是寻找易于Python和其他计算工具访问和理解的数据格式。尽管计算机视觉、自然语言处理和机器学习的进步使计算机能够分析几乎不考虑其底层结构或格式的数据变得更加容易，但事实上，*结构化*、*机器可读*的数据仍然——或许不足为奇地——是最直接的数据类型。事实上，虽然从访谈到图像再到书籍文本都可以用作数据源，但当我们讨论“数据”时，我们通常想到的更多是结构化的、数值化的数据。
- en: '*Structured* data is any type of data that has been organized and classified
    in some way, into some version of records and fields. In file-based formats, these
    are usually rows and columns; in feed-based formats they are often (essentially)
    lists of objects or *dict*ionaries.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '*结构化*数据是任何已按某种方式组织和分类的数据类型，通常以记录和字段的形式存在。在基于文件的格式中，通常是行和列；在基于数据源的格式中，它们通常是（本质上是）对象列表或*字典*。'
- en: '*Unstructured* data, by contrast, may consist of a mash-up of different data
    types, combining text, numbers, and even photographs or illustrations. The contents
    of a magazine or a novel, or the waveforms of a song, for example, would typically
    be considered unstructured data.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，*非结构化*数据可能由不同数据类型的混合组成，包括文本、数字，甚至照片或插图。例如，杂志或小说的内容，或歌曲的波形通常被认为是非结构化数据。
- en: 'If right now you’re thinking, “Hang on, novels have structure! What about chapters?”
    then congratulations: you’re already thinking like a data wrangler. We can create
    data about almost anything by collecting information about the world and applying
    structure to it.^([4](ch04.html#idm45143423518064)) And in truth, this is how
    *all* data is created: the datasets that we access via files and feeds are all
    the product of someone’s decisions about how to collect and organize information.
    In other words, there is always more than one way to organize information, but
    the structure chosen influences how it can be analyzed. This is why it’s a *little*
    ridiculous to suggest that data can somehow be “objective”; after all, it’s the
    product of (inherently subjective) human choices.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你现在在想，“等等，小说有结构啊！章节呢？”那么恭喜你：你已经像数据处理者一样思考了。我们可以通过收集关于世界的信息并对其应用结构来创建几乎任何事物的数据。[^4]
    事实上，这就是*所有*数据的生成方式：我们通过文件和数据源访问的数据集都是某人关于如何收集和组织信息的决定的产物。换句话说，有多种方式可以组织信息，但所选择的结构影响了其分析方法。这就是为什么认为数据可以某种方式“客观”有点荒谬；毕竟，它是（固有主观的）人类选择的产物。
- en: 'For example, try conducting this mini-experiment: think about how you organize
    some collection of yours (it could be a collection of music, books, games, or
    varieties of tea—you name it). Now ask a friend how they organize their own collection
    of that item. Do you do it the same way? Which is “better”? Now ask someone else,
    and maybe even a third person. While you may find similarities among the systems
    that you and your friends use for organizing your music collections, for example,
    I would be very surprised if you find that any two of you do it precisely the
    same way. In fact, you’ll probably find that everyone does it a little bit differently
    but *also* feels passionately that their way is “best.” And it is! For *them*.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，尝试进行这个小实验：想象一下如何组织你的某个收藏（可以是音乐、书籍、游戏或茶叶的各种品种——你说了算）。现在问问朋友他们如何组织他们自己的这类收藏物品。你们的方式一样吗？哪种“更好”？再问问其他人，甚至第三个人。虽然你可能会发现你和朋友们用于组织音乐收藏等系统之间的相似之处，但我会非常惊讶，如果你们中有任何两个人做法完全相同的话。事实上，你可能会发现每个人都有点不同的方式，但*也*坚信自己的方式是“最好”的。而且确实如此，对*他们*来说是最好的。
- en: If this is reminding you of our discussion in [“How? And for Whom?”](ch03.html#how_for_whom),
    that’s no coincidence, because the result of your data wrangling question and
    efforts will eventually be—you guessed it!—another dataset, which will reflect
    *your* interests and priorities. It, too, will be structured and organized, which
    makes working with it in certain ways easier than others. But the takeaway here
    is not that any given way is right or wrong, just that every choice involves trade-offs.
    Identifying and acknowledging those trade-offs is a key part of using data honestly
    and responsibly.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这让您想起了我们在[“如何？以及为谁？”](ch03.html#how_for_whom)中的讨论，那并非巧合，因为您的数据整理问题和努力的结果最终将是——你猜对了！——另一个数据集，它将反映*您*的兴趣和优先级。它也将是有结构和组织的，这使得以某些方式处理它比其他方式更容易。但这里的要点不是任何给定的方式是对还是错，而是每个选择都涉及权衡。识别和承认这些权衡是诚实和负责任地使用数据的关键部分。
- en: So a key trade-off when using *structured* data is that it requires depending
    on someone else’s judgments and priorities in organizing the underlying information.
    Obviously, this can be a good—or even great!—thing if that data has been structured
    according to an open, transparent process that involves well-qualified experts.
    Thoughtfully applied data structures like this can give us early insight into
    a subject we may otherwise know little to nothing about. On the other hand, there
    is also the possibility that we will inherit someone else’s biased or poorly designed
    choices.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在使用*结构化*数据时的一个关键权衡是，这要求依赖他人在组织基础信息时的判断和优先考量。显然，如果这些数据是按照一个开放透明的流程，并涉及到合格的专家进行结构化的，那么这可能是一件好事——甚至是一件伟大的事情！像这样经过深思熟虑的数据结构可以让我们对一个我们可能完全不了解的主题有早期的洞察。另一方面，也存在着我们可能会继承他人的偏见或设计不良选择的可能性。
- en: '*Unstructured* data, of course, gives us complete freedom to organize information
    into data structures that suit our needs best. Unsurprisingly, this requires *us*
    to take responsibility for engaging a robust data quality process, which may be
    both complex and time-consuming.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，*非结构化*数据给予了我们完全的自由，可以将信息组织成最适合我们需求的数据结构。不足为奇的是，这要求*我们*负责参与一个强大的数据质量过程，这可能既复杂又耗时。
- en: How do we know if a particular dataset is structured or unstructured up front?
    In this case, file extensions can definitely help us out. Feed-based data formats
    always have at least *some* structure to them, even if they contain chunks of
    “free text,” like social media posts. So if you see the file extensions *.json*,
    *.xml*, *.rss*, or *.atom*, the data has at least some type of record-and-field
    structure, as we’ll explore in [“Feed-Based Data—Web-Driven Live Updates”](#feed_based_data).
    File-based data that ends in *.csv*, *.tsv*, *.txt*, *.xls(x)*, or *.ods* tends
    to follow a table-type, rows-and-columns structure, as we’ll see in the next section.
    Truly unstructured data, meanwhile, is most likely to come to us as *.doc(x)*
    or *.pdf*.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何事先知道特定数据集是结构化的还是非结构化的？在这种情况下，文件扩展名肯定可以帮助我们。基于提要的数据格式始终具有至少*某种*结构，即使它们包含“自由文本”块，如社交媒体帖子。因此，如果您看到文件扩展名*.json*、*.xml*、*.rss*或*.atom*，这些数据至少有某种记录和字段结构，正如我们将在[“基于提要的数据——网络驱动的实时更新”](#feed_based_data)中探讨的那样。以*.csv*、*.tsv*、*.txt*、*.xls(x)*或*.ods*结尾的基于文件的数据往往遵循表格类型、行和列的结构，正如我们将在下一节中看到的那样。而真正的非结构化数据，则最有可能以*.doc(x)*或*.pdf*的形式出现。
- en: Now that we’ve got a good handle on the different types of data sources that
    we’re likely to encounter—and even some sense of how to locate them—let’s get
    wrangling!
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们对可能遇到的不同类型数据源有了很好的掌握，甚至对如何定位它们有了一些了解，让我们开始处理吧！
- en: Working with Structured Data
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用结构化数据
- en: 'Since the early days of digital computing, the *table* has been one of the
    most common ways to structure data. Even today, many of the most common and easy-to-wrangle
    data formats are little more than tables or collections of tables. In fact, we
    already worked with one very common table-type data format in [Chapter 2](ch02.html#chapter2):
    the *.csv* or comma-separated value format.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 自数字计算的早期以来，*表格*一直是结构化数据的最常见方式之一。即使在今天，许多最常见且易于处理的数据格式仍然不过是表格或表格的集合。事实上，在[第二章](ch02.html#chapter2)中，我们已经使用了一种非常常见的表格类型数据格式：*.csv*或逗号分隔值格式。
- en: File-Based, Table-Type Data—Take It to Delimit
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于文件、表格类型数据——从定界到实现
- en: 'In general, all of the table-type data formats you’ll typically encounter are
    examples of what are known as *delimited* files: each data record is on its own
    line or row, and the boundaries between fields or columns of data values are indicated—or
    *delimited*—by a specific text character. Often, an indication of *which* text
    character is being used as the *delimiter* in a file is incorporated into the
    dataset’s file extension. For example, the *.csv* file extension stands for *comma-separated
    value*, because these files use a comma character (`,`) as a delimiter; the *.tsv*
    file extension stands for *tab-separated value*, because the data columns are
    separated by a tab. A list of file extensions commonly associated with delimited
    data follows:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，你通常会遇到的所有表格类型数据格式都是所谓的*分隔*文件的示例：每个数据记录都在自己的行或行上，数据值的字段或列之间的边界由特定的文本字符指示或*分隔*。通常，文件中使用的*分隔符*的指示被纳入到数据集的文件扩展名中。例如，*.csv*文件扩展名代表*逗号分隔值*，因为这些文件使用逗号字符（`,`）作为分隔符；*.tsv*文件扩展名代表*制表符分隔值*，因为数据列是由制表符分隔的。以下是常见与分隔数据相关的文件扩展名列表：
- en: '*.csv*'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '*.csv*'
- en: '*Comma-separated value* files are among the most common form of table-type
    structured data files you’ll encounter. Almost any software system that handles
    tabular data (such as government or corporate data systems, spreadsheet programs,
    and even specialized commercial data programs) can output data as a *.csv*, and,
    as we saw in [Chapter 2](ch02.html#chapter2), there are handy libraries for easily
    working with this data type in Python.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '*逗号分隔值*文件是你可能会遇到的最常见的表格类型结构化数据文件之一。几乎任何处理表格数据的软件系统（例如政府或公司数据系统、电子表格程序，甚至专门的商业数据程序）都可以将数据输出为*.csv*，正如我们在[第二章](ch02.html#chapter2)中看到的，Python中有方便的库可以轻松处理这种数据类型。'
- en: '*.tsv*'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '*.tsv*'
- en: '*Tab-separated value* files have been around for a long time, but the descriptive
    *.tsv* extension has only become common relatively recently. While data providers
    don’t often explain why they choose one delimiter over another, tab-delimited
    files may be more common for datasets whose values need to include commas, such
    as postal addresses.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '*制表符分隔值*文件已经存在很长一段时间，但是描述性的*.tsv*扩展名直到最近才变得普遍。虽然数据提供商通常不解释为什么选择一个分隔符而不是另一个分隔符，但是对于需要包含逗号的值的数据集，例如邮政地址，制表符分隔文件可能更常见。'
- en: '*.txt*'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '*.txt*'
- en: Structured data files with this extension are often *.tsv* files in disguise;
    older data systems often labeled tab-separated data with the *.txt* extension.
    As you’ll see in the worked examples that follow, it’s a good idea to open and
    review *any* data file you want to wrangle with a basic text program (or a code
    editor like Atom) before you write any code, since looking at the contents of
    the file is the only surefire way to know what delimiters you’re working with.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 具有此扩展名的结构化数据文件通常是伪装成*.tsv*文件的文件；旧的数据系统通常使用*.txt*扩展名标记制表符分隔的数据。正如您将在接下来的示例中看到的，最好在编写任何代码之前使用基本文本程序（或类似Atom的代码编辑器）打开和查看*任何*您想要处理的数据文件，因为查看文件内容是了解您正在处理的分隔符的唯一可靠方法。
- en: '*.xls(x)*'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '*.xls(x)*'
- en: This is the file extension of spreadsheets produced with Microsoft Excel. Because
    these files can contain multiple “sheets” in addition to formulas, formatting,
    and other features that simple delimited files cannot replicate, they need more
    memory to store the same amount of data. They also have other limitations (like
    only being able to handle a certain number of rows) that can have implications
    for your dataset’s integrity.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这是使用Microsoft Excel生成的电子表格的文件扩展名。因为这些文件除了公式、格式和其他简单分隔文件无法复制的特性之外，还可以包含多个“工作表”，所以它们需要更多的内存来存储相同数量的数据。它们还具有其他限制（如只能处理一定数量的行），这可能会对数据集的完整性产生影响。
- en: '*.ods*'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '*.ods*'
- en: '*Open-document spreadsheet* files are the default extension for spreadsheets
    produced by a number of open source software suites like [LibreOffice](https://libreoffice.org)
    and [OpenOffice](https://openoffice.org/download/index.html) and have limitations
    and features similar to those of *.xls(x)* files.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '*开放文档电子表格*文件是由许多开源软件套件（如[LibreOffice](https://libreoffice.org)和[OpenOffice](https://openoffice.org/download/index.html)）生成的电子表格的默认扩展名，具有类似于*.xls(x)*文件的限制和功能。'
- en: Before we dive into how to work with each of these file types in Python, it’s
    worth spending just a little time thinking about when we might *want* to work
    with table-type data and where to find it when we do.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入研究如何在Python中处理这些文件类型之前，值得花一点时间考虑一下何时我们可能*想要*处理表格类型数据以及何时找到它。
- en: When to work with table-type data
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 何时处理表格类型数据
- en: Most of the time, we don’t get much of a choice about the format of our source
    data. In fact, much of the reason we need to do data wrangling in the first place
    is because the data we have doesn’t quite meet our needs. That said, it’s still
    valuable to know what data format you would *prefer* to be able to work with so
    that you can use that to inform your initial search for data.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数情况下，我们在源数据格式方面并没有太多选择。事实上，我们需要进行数据整理的主要原因之一是因为我们拥有的数据不完全符合我们的需求。尽管如此，了解您希望能够处理的数据格式仍然是有价值的，这样您可以利用它来指导您对初始数据的搜索。
- en: In [“Structured Versus Unstructured Data”](#structured_vs_unstructured), we
    talked about the benefits and limitations of structured data, and we now know
    that table-type data is one of the oldest and most common forms of machine-readable
    data. This history means, in part, that over the years many forms of source data
    have been crammed into tables, even though they may *not* necessarily be well
    suited to table-like representations. Still, this format can be especially useful
    for answering questions about trends and patterns over time. In our Citi Bike
    exercise from [Chapter 2](ch02.html#chapter2), for example, we examined how many
    “Customers” versus “Subscribers” had taken Citi Bike rides over the course of
    a single month. If we wanted to, we could perform the same calculation for *every*
    available month of Citi Bike rides in order to understand any patterns in this
    ratio over time.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [“结构化数据与非结构化数据”](#structured_vs_unstructured) 中，我们讨论了结构化数据的优缺点，现在我们知道，表格类型数据是最古老和最常见的机器可读数据形式之一。这一历史部分意味着多年来许多形式的源数据已经被塞进表格中，尽管它们可能并不一定适合于类似表格的表示。然而，这种格式对于回答关于时间趋势和模式的问题特别有用。例如，在我们的
    Citi Bike 练习中，来自 [第二章](ch02.html#chapter2) 的“顾客”与“订阅者”在一个月内骑行 Citi Bike 的次数。如果我们愿意，我们可以对每个可用的
    Citi Bike 骑行月份执行相同的计算，以了解该比例随时间的任何模式。
- en: Of course, table-type data is generally not a great format for real-time data,
    or data where not every observation contains the same possible values. These kinds
    of data are often better suited to the feed-based data formats that we discuss
    in [“Feed-Based Data—Web-Driven Live Updates”](#feed_based_data).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，表格类型的数据通常不适合实时数据或者每个观察结果不包含相同可能值的数据。这类数据通常更适合我们在 [“基于Feed的数据—网络驱动的实时更新”](#feed_based_data)
    中讨论的基于feed的数据格式。
- en: Where to find table-type data
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在哪里找到表格类型数据
- en: Since the vast majority of machine-readable data is still in table-type data
    formats, it is among the easiest data formats to locate. Spreadsheets are common
    in every discipline, and a large number of government and commercial information
    systems rely on software that organizes data in this way. Almost any time you
    request data from an expert or organization, a table-type format is what you are
    likely to get. This is also true of almost every open-data portal and data-sharing
    site you’ll find online. As we covered in [“Smart Searching for Specific Data
    Types”](#smart_searching), you can even find table-type data (and other specific
    file formats) via search engines, if you know how to look.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 由于绝大多数机器可读数据仍然以表格类型数据格式存在，因此这是最容易定位的数据格式之一。电子表格在各个学科中很常见，并且许多政府和商业信息系统依赖于能够以此方式组织数据的软件。无论何时你从专家或组织请求数据，你可能都会得到表格类型的格式。这同样适用于几乎所有在线开放数据门户和数据共享站点。正如我们在
    [“智能搜索特定数据类型”](#smart_searching) 中讨论的，即使是通过搜索引擎，你也可以找到表格类型数据（和其他特定文件格式），只要你知道如何查找。
- en: Wrangling Table-Type Data with Python
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用 Python 整理表格类型数据
- en: To help illustrate how simple it is to work with table-type data in Python,
    we’ll walk through examples of how to read in data from all of the file types
    mentioned in this section—plus a few others, just for good measure. While in later
    chapters we’ll look at how to do more with cleaning, transformation, and data
    quality assessments, our focus for the time being will simply be on accessing
    the data within each type of data file and interacting with it using Python.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助说明在 Python 中处理表格类型数据有多简单，我们将逐步介绍如何从本节提到的所有文件类型中读取数据，再加上一些其他类型，只是为了确保。虽然在后面的章节中，我们将看看如何进行更多的数据清理、转换和数据质量评估，但我们目前的重点仅仅是访问每种数据文件中的数据并使用
    Python 与之交互。
- en: Reading data from CSVs
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从 CSV 中读取数据
- en: In case you didn’t follow along in [Chapter 2](ch02.html#chapter2), here’s a
    refresher on how to read data from a *.csv* file, using a sample from the Citi
    Bike dataset ([Example 4-1](#csv_parsing)). As always, I’ve included a description
    of what the program is doing—as well as links to any source files—in the comments
    at the top of my script. Since we’ve worked with this data format before, for
    now we’ll just worry about printing out the first few rows of data to see what
    they look like.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你没有在 [Chapter 2](ch02.html#chapter2) 中跟随进行，这里是一个从 *.csv* 文件中读取数据的复习，使用了来自
    Citi Bike 数据集的示例（[Example 4-1](#csv_parsing)）。和往常一样，在我的脚本顶部的注释中，我包含了程序正在做的描述以及任何源文件的链接。由于我们之前已经使用过这种数据格式，所以现在我们只关心打印出前几行数据以查看它们的样子。
- en: Example 4-1\. csv_parsing.py
  id: totrans-45
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Example 4-1\. csv_parsing.py
- en: '[PRE0]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[![1](assets/1.png)](#co_working_with_file_based_and_feed_based_data_in_python_CO1-1)'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_working_with_file_based_and_feed_based_data_in_python_CO1-1)'
- en: This is our workhorse library when it comes to dealing with table-type data.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们处理表格类型数据时的得力工具库。
- en: '[![2](assets/2.png)](#co_working_with_file_based_and_feed_based_data_in_python_CO1-2)'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_working_with_file_based_and_feed_based_data_in_python_CO1-2)'
- en: '`open()` is a built-in function that takes a filename and a “mode” as parameters.
    In this example, the target file (`202009CitibikeTripdataExample.csv`) should
    be in the same folder as our Python script or notebook. Values for the “mode”
    can be `r` for “read” or `w` for “write.”'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '`open()` 是一个内置函数，接受文件名和“模式”作为参数。在这个例子中，目标文件（`202009CitibikeTripdataExample.csv`）应该与我们的
    Python 脚本或笔记本位于同一个文件夹中。模式的值可以是 `r` 表示“读取”，也可以是 `w` 表示“写入”。'
- en: '[![3](assets/3.png)](#co_working_with_file_based_and_feed_based_data_in_python_CO1-3)'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_working_with_file_based_and_feed_based_data_in_python_CO1-3)'
- en: By printing out the `citibike_reader.fieldnames` values, we can see that the
    exact label for the “User Type” column is `usertype`.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 通过打印出 `citibike_reader.fieldnames` 的值，我们可以看到“User Type”列的确切标签是 `usertype`。
- en: '[![4](assets/4.png)](#co_working_with_file_based_and_feed_based_data_in_python_CO1-4)'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](assets/4.png)](#co_working_with_file_based_and_feed_based_data_in_python_CO1-4)'
- en: 'The `range()` function gives us a way to execute some piece of code a specific
    number of times, starting with the value of the first argument and ending just
    *before* the value of the second argument. For example, the code indented below
    this line will be executed five times, going through the `i` values of `0`, `1`,
    `2`, `3`, and `4`. For more on the `range()` function, see [“Adding Iterators:
    The range Function”](#add_iterators).'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '`range()` 函数提供了一种执行某个代码片段特定次数的方法，从第一个参数的值开始，直到第二个参数的值之前结束。例如，下面缩进的代码将执行五次，遍历
    `i` 的值为 `0`、`1`、`2`、`3` 和 `4`。有关 `range()` 函数的更多信息，请参见 [“添加迭代器：range 函数”](#add_iterators)。'
- en: 'The output from running this should look something like:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 运行后的输出应该是这样的：
- en: '[PRE1]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Reading data from TSV and TXT files
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从 TSV 和 TXT 文件中读取数据
- en: 'Despite its name, the Python *csv* library is basically a one-stop shop for
    wrangling table-type data in Python, thanks to the `DictReader` function’s `delimiter`
    option. Unless you tell it differently, `DictReader` assumes that the comma character
    (`,`) is the separator it should look for. Overriding that assumption is easy,
    however: you can simply specify a different character when you call the function.
    In [Example 4-2](#tsv_parsing), we specify the tab character (`\t`), but we could
    easily substitute any delimiter we prefer (or that appears in a particular source
    file).'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管其名称如此，但 Python 的 *csv* 库基本上是 Python 中处理表格类型数据的一站式解决方案，这要归功于 `DictReader` 函数的
    `delimiter` 选项。除非你另有指示，否则 `DictReader` 会假定逗号字符（`,`）是它应该查找的分隔符。然而，覆盖这一假设很容易：你只需在调用函数时指定不同的字符即可。在
    [Example 4-2](#tsv_parsing) 中，我们指定了制表符 (`\t`)，但我们也可以轻松地替换为我们喜欢的任何分隔符（或者源文件中出现的分隔符）。
- en: Example 4-2\. tsv_parsing.py
  id: totrans-59
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Example 4-2\. tsv_parsing.py
- en: '[PRE2]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[![1](assets/1.png)](#co_working_with_file_based_and_feed_based_data_in_python_CO2-1)'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_working_with_file_based_and_feed_based_data_in_python_CO2-1)'
- en: This dataset was listed in Jeremy Singer-Vine’s (@jsvine) “Data Is Plural” newsletter
    ([*https://data-is-plural.com*](https://data-is-plural.com)).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 此数据集在 Jeremy Singer-Vine 的《Data Is Plural》通讯中列出（[*https://data-is-plural.com*](https://data-is-plural.com)）。
- en: 'This should result in output that looks something like:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该产生类似以下的输出：
- en: '[PRE3]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Though the *.tsv* file extension has become relatively common nowadays, many
    files generated by older databases that are *actually* tab-separated may reach
    you with a *.txt* file extension. Fortunately, as described in the preceding sidebar,
    this changes nothing about how we handle the file as long as we specify the correct
    delimiter—as you can see in [Example 4-3](#txt_parsing).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管*.tsv*文件扩展名如今变得相对常见，但许多由旧数据库生成的*实际上*是制表符分隔的文件可能会以*.txt*文件扩展名的形式传送到您手中。幸运的是，正如前文中所述的那样，在我们指定正确的分隔符的情况下，这并不会影响我们如何处理文件，正如您可以在[示例 4-3](#txt_parsing)中看到的那样。
- en: Example 4-3\. txt_parsing.py
  id: totrans-66
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-3\. txt_parsing.py
- en: '[PRE4]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[![1](assets/1.png)](#co_working_with_file_based_and_feed_based_data_in_python_CO3-1)'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_working_with_file_based_and_feed_based_data_in_python_CO3-1)'
- en: As discussed in [“Don’t Leave Space!”](ch01.html#no_spaces), whitespace characters
    have to be *escaped* when we’re using them in code. Here, we’re using the escaped
    character for a `tab`, which is `\t`. Another common whitespace character code
    is `\n` for `newline` (or `\r` for `return`, depending on your device).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 正如在[“不要留空格！”](ch01.html#no_spaces)中讨论的那样，在我们使用代码时，空白字符必须被*转义*。在这里，我们使用了一个`tab`的转义字符，即`\t`。另一个常见的空白字符代码是`\n`表示`换行`（或者根据您的设备是`\r`表示`回车`）。
- en: If everything has gone well, the output from this script should look exactly
    the same as that from [Example 4-2](#tsv_parsing).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一切顺利，这个脚本的输出应该与[示例 4-2](#tsv_parsing)中的输出完全相同。
- en: 'One question you may be asking yourself at this point is “How do I know what
    delimiter my file has?” While there are programmatic ways to help detect this,
    the simple answer is: Look! Anytime you begin working with (or thinking about
    working with) a new dataset, start by opening it up in the most basic text program
    your device has to offer (any code editor will also be a reliable choice). Especially
    if the file is large, using the simplest program possible will let your device
    devote maximum memory and processing power to actually reading the data—reducing
    the likelihood that the program will hang or your device will crash (closing other
    programs and excess browser tabs will help, too)!'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 此时您可能会问自己一个问题：“我如何知道我的文件有什么分隔符？”虽然有程序化的方法可以帮助检测这一点，但简单的答案是：看！每当您开始处理（或考虑处理）新数据集时，请首先在您的设备上最基本的文本程序中打开它（任何代码编辑器也是一个可靠的选择）。特别是如果文件很大，使用尽可能简单的程序将使您的设备能够将最大的内存和处理能力用于实际读取数据，从而减少程序挂起或设备崩溃的可能性（同时关闭其他程序和多余的浏览器标签页也会有所帮助）！
- en: Though I’ll talk about some ways to inspect small parts of *really* large files
    later on in the book, now is the time to start practicing the skills that are
    essential to assessing data quality—all of which require reviewing your data and
    making judgments about it. So while there *are* ways to “automate away” tasks
    like identifying the correct delimiter for your data, eyeballing it in a text
    editor will often be not just faster and more intuitive, but it will help you
    get more familiar with other important aspects of the data at the same time.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我稍后会谈论一些检查*真正*大型文件中小部分的方法，但现在是开始练习评估数据质量的关键技能的时候——所有这些技能都需要审查您的数据并对其做出评判。因此，虽然确实有“自动化”识别数据正确分隔符等任务的方法，但在文本编辑器中用眼睛查看它通常不仅更快更直观，而且还会帮助您更加熟悉数据的其他重要方面。
- en: 'Real-World Data Wrangling: Understanding Unemployment'
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实际数据整理：理解失业情况
- en: The underlying dataset that we’ll use to explore some of our trickier table-type
    data formats is unemployment data about the United States. Why? In one way or
    another, unemployment affects most of us, and in recent decades the US has experienced
    some particularly high unemployment rates. Unemployment numbers for the US are
    released monthly by the Bureau of Labor Statistics (BLS), and while they are often
    reported by general-interest news sources, they are usually treated as a sort
    of abstract indicator of how “the economy” is doing. What the numbers really represent
    is rarely discussed in-depth.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将用于探索一些复杂表格类型数据格式的基础数据集是关于美国失业情况的数据。为什么选择这个数据集？因为失业以某种方式影响到我们大多数人，近几十年来，美国经历了一些特别高的失业率。美国的失业数据每月由劳工统计局（BLS）发布，尽管它们经常被一般新闻来源报道，但通常被视为对“经济”状况的某种抽象指标。这些数字真正代表的含义很少被深入讨论。
- en: 'When I first joined the *Wall Street Journal* in 2007, building an interactive
    dashboard for exploring monthly economic indicator data—including unemployment—was
    my first major project. One of the more interesting things I learned in the process
    is that there isn’t “an” unemployment rate calculated each month, there are *several*
    (six, to be exact). The one that usually gets reported by news sources is the
    so-called “U3” unemployment rate, which the BLS describes as:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 当我2007年首次加入*华尔街日报*时，为探索月度经济指标数据（包括失业率）构建交互式仪表板是我的第一个重要项目。我在这个过程中学到的更有趣的事情之一是每个月并不只计算“一个”失业率，而是*几个*（确切地说是六个）。通常由新闻来源报告的是所谓的“U3”失业率，这是美国劳工统计局描述的：
- en: Total unemployed, as a percent of the civilian labor force (official unemployment
    rate).
  id: totrans-76
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 总失业人数，作为民用劳动力的百分比（官方失业率）。
- en: 'On its surface, this seems like a straightforward definition of unemployment:
    of all the people who reasonably *could* be working, what percentage are not?'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 表面上看，这似乎是对失业的一个简单定义：所有合理可能在工作的人中，有多少百分比不在工作？
- en: 'Yet the real story is a bit more complex. What does it mean to be “employed”
    or be counted as part of the “labor force”? A look at different unemployment numbers
    makes more clear what the “U3” number does *not* take into account. The “U6” unemployment
    rate is defined as:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，真实情况稍微复杂一些。什么是“就业”或被计算为“劳动力”的一部分意味着什么？查看不同的失业数据更清楚地说明了“U3”数字没有考虑到的内容。 “U6”失业率的定义如下：
- en: Total unemployed, plus all persons marginally attached to the labor force, plus
    total employed part time for economic reasons, as a percent of the civilian labor
    force plus all persons marginally attached to the labor force.
  id: totrans-79
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 总失业人数，加上所有边缘附属于劳动力的人员，再加上因经济原因而只能兼职工作的总人数，作为民用劳动力加上所有边缘附属于劳动力的人员的百分比。
- en: When we read the accompanying note, this longer definition starts to take shape:^([5](ch04.html#idm45143422908112))
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们阅读附带的注释时，这个较长的定义开始清晰起来：^([5](ch04.html#idm45143422908112))
- en: 'NOTE: Persons marginally attached to the labor force are those who currently
    are neither working nor looking for work but indicate that they want and are available
    for a job and have looked for work sometime in the past 12 months. Discouraged
    workers, a subset of the marginally attached, have given a job-market related
    reason for not currently looking for work. Persons employed part time for economic
    reasons are those who want and are available for full-time work but have had to
    settle for a part-time schedule. Updated population controls are introduced annually
    with the release of January data.'
  id: totrans-81
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 注意：边缘附属于劳动力的人员是指目前既不工作也不寻找工作，但表示他们想要工作并且有时间，并且在过去12个月内曾经寻找过工作。较少附属的人员是边缘附属的一个子集，他们没有目前寻找工作的原因。经济原因而只能兼职工作的人员是那些希望全职工作并且有时间但不得不接受兼职安排的人员。每年都会随着一月份的数据发布而引入更新的人口控制。
- en: In other words, if you *want* a job (and have looked for one in the past year)
    but haven’t looked for one very recently—or if you have a part-time job but *want*
    a full-time job—then you don’t officially count as “unemployed” in the U3 definition.
    This means that the economic reality of Americans working multiple jobs (who are
    more likely to be women and have more children)^([6](ch04.html#idm45143422903792)),
    and potentially of “gig” workers (recently estimated as up to 30% of the American
    workforce),^([7](ch04.html#idm45143422901552)) are not necessarily reflected in
    the U3 number. Unsurprisingly, the U6 rate is typically several percentage points
    higher each month than the U3 rate.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，如果你想要一份工作（并且在过去一年内曾经寻找过工作），但最近没有寻找过工作——或者你有一份兼职工作，但希望全职工作——那么在U3的定义中你不被正式计算为“失业”。这意味着美国人工作多份工作的经济现实（更可能是女性并且有更多孩子的人）^([6](ch04.html#idm45143422903792))，以及可能的“零工”工作者（最近估计占美国劳动力的30%），^([7](ch04.html#idm45143422901552))并不一定反映在U3数字中。毫不奇怪，U6率通常比U3率每月高出几个百分点。
- en: To see how these rates compare over time, we can download them from the website
    of the St. Louis Federal Reserve, which provides thousands of economic datasets
    for download in a range of formats, including table-type *.xls(x)* files and,
    as we’ll see later in [Example 4-12](#xml_parsing), feed-type formats as well.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 为了看到这些比率随时间的变化如何，我们可以从圣路易斯联邦储备银行的网站上下载它们，该网站提供了成千上万的经济数据集供下载，包括表格类型的*.xls(x)*文件以及如我们稍后将在[示例 4-12](#xml_parsing)中看到的，还有提供提要型格式。
- en: You can download the data for these exercises from the [Federal Reserve Economic
    Database (FRED) website](https://fred.stlouisfed.org/series/U6RATE). It shows
    the current U6 unemployment rate since the measure was first created in the early
    1990s.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以从 [联邦储备经济数据库 (FRED) 网站](https://fred.stlouisfed.org/series/U6RATE) 下载这些练习的数据。它展示了自上世纪90年代初创建以来的当前
    U6 失业率。
- en: To add the U3 rate to this graph, at the top right choose Edit graph → ADD LINE.
    In the search field, type **`UNRATE`** and then select “Unemployment Rate” when
    it populates below the search bar. Finally, click Add series. Close this side
    window using the X at the top right, and then select Download, being sure to select
    the first option, Excel.^([8](ch04.html#idm45143422892896)) This will be an *.xls*
    file, which we’ll handle last because although still widely available, this is
    a relatively outdated file format (it was replaced by *.xlsx* as the default format
    for [Microsoft Excel spreadsheets in 2007](https://en.wikipedia.org/wiki/Microsoft_Excel#File_formats)).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 要在这张图表上添加 U3 率，请在右上角选择“编辑图表” → “添加线条”。在搜索字段中，输入 **`UNRATE`** 然后在搜索栏下方出现时选择“失业率”。最后，点击“添加系列”。使用右上角的
    X 关闭此侧窗口，然后选择“下载”，确保选择第一个选项，即 Excel。^([8](ch04.html#idm45143422892896)) 这将是一个
    *.xls* 文件，我们将在最后处理它，因为尽管它仍然广泛可用，但这是一个相对过时的文件格式（自 [2007 年起被 *.xlsx* 取代成为 [Microsoft
    Excel 电子表格的默认格式](https://en.wikipedia.org/wiki/Microsoft_Excel#File_formats)）。
- en: 'To get the additional file formats we need, just open the file you downloaded
    with a spreadsheet program like Google Sheets and choose “Save As,” then select
    *.xlsx*, then repeat the process choosing *.ods*. You should now have the following
    three files, all containing the same information: *fredgraph.xlsx*, *fredgraph.ods*,
    and *fredgraph.xls*.^([9](ch04.html#idm45143422886480))'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取我们需要的其他文件格式，只需用电子表格程序如 Google Sheets 打开您下载的文件，选择“另存为”，然后选择 *.xlsx*，然后重复该过程选择
    *.ods*。现在您应该有以下三个包含相同信息的文件：*fredgraph.xlsx*、*fredgraph.ods* 和 *fredgraph.xls*。^([9](ch04.html#idm45143422886480))
- en: Note
  id: totrans-87
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: If you opened the original *fredgraph.xls* file, you probably noticed that it
    contains more than just the unemployment data; it also contains some header information
    about where the data came from and the definitions of U3 and U6 unemployment,
    for example. While doing *analysis* on the unemployment rates these files contain
    would require separating this metadata from the table-type data further down,
    remember that our goal for the moment is simply to convert all of our various
    files to a *.csv* format. We’ll tackle the data cleaning process that would involve
    removing this metadata in [Chapter 7](ch07.html#chapter7).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你打开了原始的 *fredgraph.xls* 文件，你可能注意到它包含的不仅仅是失业数据；它还包含一些关于数据来源以及 U3 和 U6 失业率定义的头部信息。在分析这些文件中的失业率时，需要进一步将这些元数据与表格类型的数据分开。但是请记住，目前我们的目标只是将所有不同格式的文件转换为
    *.csv* 格式。我们将在 [第7章](ch07.html#chapter7) 处理涉及移除这些元数据的数据清洗过程。
- en: XLSX, ODS, and All the Rest
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: XLSX、ODS 和其他所有格式
- en: For the most part, it’s preferable to avoid processing data saved as *.xlsx*,
    *.ods*, and most other nontext table-type data formats directly, if possible.
    If you’re just at the stage of exploring datasets, I suggest you review these
    files simply by opening them with your preferred spreadsheet program and saving
    them as a *.csv* or *.tsv* file format before accessing them in Python. Not only
    will this make them easier to work with, it will give you a chance to actually
    look at the contents of your data file and get a sense of what it contains.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数情况下，最好避免直接处理保存为 *.xlsx*、*.ods* 和大多数其他非文本表格类型数据格式的数据。如果您只是在探索数据集阶段，我建议您简单地使用您喜欢的电子表格程序打开它们，然后将它们保存为
    *.csv* 或 *.tsv* 文件格式，然后再在 Python 中访问它们。这不仅会使它们更容易处理，还可以让您实际查看数据文件的内容并了解其包含的内容。
- en: Resaving and reviewing *.xls(x)* and similar data formats as a *.csv* or equivalent
    text-based file format will both reduce the file size *and* give you a better
    sense of what the “real” data looks like. Because of the formatting options in
    spreadsheet programs, sometimes what you see onscreen is substantially different
    from the raw values that are stored in the actual file. For example, values that
    appear as percentages in a spreadsheet program (e.g., 10%) might actually be decimals
    (.1). This can lead to problems if you try to base aspects of your Python processing
    or analysis on what you saw in the spreadsheet as opposed to a text-based data
    format like *.csv*.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 将 *.xls(x)* 和类似的数据格式重新保存和审查为 *.csv* 或等效的基于文本的文件格式，既能减小文件大小，*又*能让你更好地了解“真实”数据的样子。由于电子表格程序中的格式选项，有时屏幕上看到的内容与实际文件中存储的原始值有很大不同。例如，在电子表格程序中以百分比形式显示的值（例如，10%）实际上可能是小数（.1）。如果你试图基于电子表格中看到的内容而不是像
    *.csv* 这样的基于文本的数据格式进行 Python 处理或分析，这可能会导致问题。
- en: Still, there will definitely be situations where you need to access *.xls(x)*
    and similar file types with Python directly.^([10](ch04.html#idm45143422842640))
    For example, if there’s an *.xls* dataset you need to wrangle on a regular basis
    (say, every month), resaving the file manually each time would become unnecessarily
    time-consuming.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，肯定会有一些情况，你需要直接使用 Python 访问 *.xls(x)* 和类似的文件类型。^([10](ch04.html#idm45143422842640))
    例如，如果有一个 *.xls* 数据集，你需要定期处理（比如，每个月），每次手动重新保存文件都会变得不必要地耗时。
- en: Fortunately, that active Python community we talked about in [“Community”](ch01.html#community)
    has created libraries that can handle an impressive range of data formats with
    ease. To get a thorough feel for how these libraries work with more complex source
    data (and data formats), the following code examples read in the specified file
    format and then create a *new* *.csv* file that contains the same data.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，我们在 [“社区”](ch01.html#community) 中谈到的活跃 Python 社区已经创建了可以轻松处理各种数据格式的库。为了彻底了解这些库如何与更复杂的源数据（以及数据格式）配合工作，以下代码示例读取指定的文件格式，然后创建一个包含相同数据的
    *新* *.csv* 文件。
- en: To make use of these libraries, however, you’ll first need to install them on
    your device by running the following commands one by one in a terminal window:^([11](ch04.html#idm45143422837216))
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，要使用这些库，你首先需要在设备上安装它们，方法是在终端窗口中逐个运行以下命令：^([11](ch04.html#idm45143422837216))
- en: '[PRE5]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: In the following code examples, we’ll be using the *openpyxl* library to access
    (or *parse*) *.xlsx* files, the *pyexcel-ods* library for dealing with *.ods*
    files, and the *xlrd* library for reading from *.xls* files (for more on finding
    and selecting Python libraries, see [“Where to Look for Libraries”](app01.html#where_to_find_libraries)).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码示例中，我们将使用 *openpyxl* 库访问（或*解析*）*.xlsx* 文件，使用 *pyexcel-ods* 库处理 *.ods* 文件，并使用
    *xlrd* 库从 *.xls* 文件中读取数据（有关查找和选择 Python 库的更多信息，请参见 [“查找库的地方”](app01.html#where_to_find_libraries)）。
- en: 'To better illustrate the idiosyncrasies of these different file formats, we’re
    going to do something similar to what we did in [Example 4-3](#txt_parsing): we’ll
    take sample data that is being provided as an *.xls* file and create *.xlsx* and
    *.ods* files containing *the exact same data* by resaving that source file in
    the other formats using a spreadsheet program. Along the way, I think you’ll start
    to get a sense of how these nontext formats make the process of data wrangling
    more (and, I would argue, unnecessarily) complicated.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地说明这些不同文件格式的特殊性，我们将做类似于我们在 [示例 4-3](#txt_parsing) 中所做的事情：我们将获取作为 *.xls*
    文件提供的示例数据，并使用电子表格程序将该源文件重新保存为其他格式，创建包含*完全相同数据*的 *.xlsx* 和 *.ods* 文件。在此过程中，我认为你会开始感受到这些非文本格式如何使数据处理过程变得更加（我会说，是不必要地）复杂。
- en: 'We’ll start by working through an *.xlsx* file in *\ref* ([Example 4-4](#xlsx_parsing)),
    using a version of the unemployment data downloaded from FRED. This example illustrates
    one of the first major differences between dealing with text-based table-type
    data files and nontext formats: because the nontext formats support multiple “sheets,”
    we needed to include a `for` loop at the top of our script, *within* which we
    put the code for creating our individual output files (one for each sheet).'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从 *\ref*（[示例 4-4](#xlsx_parsing)）开始，通过一个 *.xlsx* 文件进行工作，使用从 FRED 下载的失业数据的一个版本。这个示例说明了处理基于文本的表格型数据文件和非文本格式之间的首个主要区别之一：由于非文本格式支持多个“工作表”，我们需要在脚本顶部包含一个
    `for` 循环，在其中放置用于创建各自输出文件的代码（每个工作表一个文件）。
- en: Example 4-4\. xlsx_parsing.py
  id: totrans-99
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-4\. xlsx_parsing.py
- en: '[PRE6]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[![1](assets/1.png)](#co_working_with_file_based_and_feed_based_data_in_python_CO4-1)'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_working_with_file_based_and_feed_based_data_in_python_CO4-1)'
- en: Like the *csv* library’s `DictReader()` function, `openpyxl`’s `load_workbook()`
    function adds properties to our source data, in this case, one that shows us the
    names of all the data sheets in our workbook.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于*csv*库的`DictReader()`函数，`openpyxl`的`load_workbook()`函数会向我们的源数据添加属性，例如显示工作簿中所有数据表名称的属性。
- en: '[![2](assets/2.png)](#co_working_with_file_based_and_feed_based_data_in_python_CO4-2)'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_working_with_file_based_and_feed_based_data_in_python_CO4-2)'
- en: Even though our example workbook only includes one worksheet, we might have
    more in the future. We’ll use the `enumerate()` function so we can access both
    an iterator *and* the sheet name. This will help us create one *.csv* file per
    worksheet.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 即使我们的示例工作簿仅包含一个工作表，未来可能会有更多。我们将使用`enumerate()`函数，这样我们可以访问迭代器*和*工作表名称。这将帮助我们为每个工作表创建一个*.csv*文件。
- en: '[![3](assets/3.png)](#co_working_with_file_based_and_feed_based_data_in_python_CO4-3)'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_working_with_file_based_and_feed_based_data_in_python_CO4-3)'
- en: Each sheet in our `source_workbook` will need its own, uniquely named output
    *.csv* file. To generate these, we’ll “open” a new file with the name `"xlsx_"+sheet_name+".csv"`
    and make it *writable* by passing `w` as the “mode” argument (up until now, we’ve
    used the `r` mode to *read* data from *.csv*s).
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 每个`source_workbook`中的工作表都需要其独特命名的输出*.csv*文件。为了生成这些文件，我们将使用名称为`"xlsx_"+sheet_name+".csv"`的新文件进行“打开”，并通过将`w`作为“mode”参数来使其*可写*（直到现在，我们一直使用`r`模式从*.csv*文件中*读取*数据）。
- en: '[![4](assets/4.png)](#co_working_with_file_based_and_feed_based_data_in_python_CO4-4)'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](assets/4.png)](#co_working_with_file_based_and_feed_based_data_in_python_CO4-4)'
- en: The function `iter_rows()` is specific to the *openpyxl* library. Here, it converts
    the rows of `source_workbook` into a list that can be *iterated*, or looped, over.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 函数`iter_rows()`专用于*openpyxl*库。在这里，它将`source_workbook`的行转换为可以*迭代*或循环的列表。
- en: '[![5](assets/5.png)](#co_working_with_file_based_and_feed_based_data_in_python_CO4-5)'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](assets/5.png)](#co_working_with_file_based_and_feed_based_data_in_python_CO4-5)'
- en: The *openpyxl* library treats each data cell as a Python [`tuple` data type](https://docs.python.org/3/library/stdtypes.html#tuple).
    If we try to just print the rows of `current_sheet` directly, we’ll get sort of
    unhelpful cell locations, rather than the data values they contain. To address
    this, we’ll make *another* loop inside this one to go through every cell in every
    row one at a time and add the actual data values to `row_cells`.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '*openpyxl*库将每个数据单元格视为Python [`tuple`数据类型](https://docs.python.org/3/library/stdtypes.html#tuple)。如果我们尝试直接打印`current_sheet`的行，则会得到不太有用的单元格位置，而不是它们包含的数据值。为了解决这个问题，我们将在此循环内再做*另一个*循环，逐个遍历每行中的每个单元格，并将实际的数据值添加到`row_cells`中。'
- en: '[![6](assets/6.png)](#co_working_with_file_based_and_feed_based_data_in_python_CO4-6)'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '[![6](assets/6.png)](#co_working_with_file_based_and_feed_based_data_in_python_CO4-6)'
- en: Notice that this code is left-aligned with the `for cell in row` code in the
    example. This means that it is *outside* that loop and so will only be run *after*
    all the cells in a given row have been appended to our list.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，此代码与示例中`for cell in row`代码左对齐。这意味着它在该循环之外，因此仅在给定行中的所有单元格都已附加到我们的列表之后才会运行。
- en: This script also begins to demonstrate the way that, just as two chefs may have
    different ways of preparing the same dish, library creators may make different
    choices about how to (re)structure each source file type—with corresponding implications
    for our code. The creators of the *openpyxl* library, for example, chose to store
    each data cell’s location label (e.g., `A6`) and the value it contains in a Python
    `tuple`. That design decision is why we need a second `for` loop to go through
    each row of data—because we actually have to access the data cell by cell in order
    to build the Python list that will become a single row in our output *.csv* file.
    Likewise, if you use a spreadsheet program to open the *xlsx_FRED Graph.csv* created
    by the script in [Example 4-4](#xlsx_parsing), you’ll see that the original *.xls*
    file shows the values in the `observation_date` column in a YYYY-MM-DD format,
    but our output file shows those values in a YYYY-MM-DD HH:MM:SS format. This is
    because the creator(s) of *openpyxl* decided that it would automatically convert
    any “date-like” data strings into the Python `datetime` datatype. Obviously, none
    of these choices are right or wrong; we simply need to account for them in writing
    our code so that we don’t distort or misinterpret the source data.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 此脚本还开始展示了，就像两位厨师可能有不同的准备同一道菜的方式一样，库的创建者们可能会在如何（重新）结构化每种源文件类型上做出不同选择，这对我们的代码也有相应的影响。例如，*openpyxl*
    库的创建者选择将每个数据单元格的位置标签（例如 `A6`）和其包含的值存储在一个 Python `tuple` 中。这个设计决策导致我们需要第二个 `for`
    循环来逐个访问每行数据，因为我们实际上必须逐个访问数据单元格，以构建成为输出 *.csv* 文件中的单行数据的 Python 列表。同样，如果您使用电子表格程序打开由
    [示例 4-4](#xlsx_parsing) 中的脚本创建的 *xlsx_FRED Graph.csv* 输出文件，您会看到原始的 *.xls* 文件在
    `observation_date` 列中显示的值是 YYYY-MM-DD 格式，但我们的输出文件将这些值显示为 YYYY-MM-DD HH:MM:SS 格式。这是因为
    *openpyxl* 的创建者们决定自动将任何类似日期的数据字符串转换为 Python 的 `datetime` 类型。显然，这些选择没有对错之分；我们只需要在编写代码时考虑到它们，以确保不会扭曲或误解源数据。
- en: Now that we’ve wrangled the *.xlsx* version of our data file, let’s see what
    happens when we parse it as an *.ods*, as shown in [Example 4-5](#ods_parsing).
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经处理了数据文件的 *.xlsx* 版本，让我们看看当我们将其解析为 *.ods* 格式时会发生什么，如 [示例 4-5](#ods_parsing)
    所示。
- en: Example 4-5\. ods_parsing.py
  id: totrans-115
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-5\. ods_parsing.py
- en: '[PRE7]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[![1](assets/1.png)](#co_working_with_file_based_and_feed_based_data_in_python_CO5-1)'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_working_with_file_based_and_feed_based_data_in_python_CO5-1)'
- en: The *pyexcel_ods* library converts our source data into Python’s `OrderedDict`
    data type. The associated `items()` method then lets us access each sheet’s name
    and data as a key/value pair that we can loop through. In this case, `sheet_name`
    is the “key” and the entire worksheet’s data is the “value.”
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '*pyexcel_ods* 库将我们的源数据转换为 Python 的 `OrderedDict` 数据类型。然后，相关的 `items()` 方法允许我们访问每个工作表的名称和数据，作为一个可以循环遍历的键/值对。在这种情况下，`sheet_name`
    是“键”，整个工作表的数据是“值”。'
- en: '[![2](assets/2.png)](#co_working_with_file_based_and_feed_based_data_in_python_CO5-2)'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_working_with_file_based_and_feed_based_data_in_python_CO5-2)'
- en: Here, `sheet_data` is already a list, so we can just loop through that list
    with a basic `for` loop.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`sheet_data` 已经是一个列表，因此我们可以使用基本的 `for` 循环来遍历该列表。
- en: '[![3](assets/3.png)](#co_working_with_file_based_and_feed_based_data_in_python_CO5-3)'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_working_with_file_based_and_feed_based_data_in_python_CO5-3)'
- en: This library converts each row in a worksheet to a list, which is why we can
    pass these directly to the `writerow()` method.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 此库将工作表中的每一行转换为一个列表，这就是为什么我们可以直接将它们传递给 `writerow()` 方法的原因。
- en: In the case of the *pyexcel_ods* library, the contents of our output *.csv*
    file *much* more closely resembles what we see visually when we open the original
    *fredgraph.xls* via a spreadsheet program like Google Sheets—the `observation_date`
    field, for example, is in a simple YYYY-MM-DD format. Moreover, the library creator(s)
    decided to treat the values in each row as a list, allowing us to write each record
    directly to our output file without creating any additional loops or lists.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 *pyexcel_ods* 库而言，我们输出的 *.csv* 文件的内容更接近我们通过诸如 Google Sheets 这样的电子表格程序打开原始
    *fredgraph.xls* 文件时所看到的内容 —— 例如，`observation_date` 字段以简单的 YYYY-MM-DD 格式呈现。此外，库的创建者们决定将每行的值视为列表，这使得我们可以直接将每条记录写入输出文件，而无需创建任何额外的循环或列表。
- en: Finally, let’s see what happens when we use the *xlrd* library to parse the
    original *.xls* file directly in [Example 4-6](#xls_parsing).
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们看看当我们使用 *xlrd* 库直接解析原始的 *.xls* 文件时会发生什么，如 [示例 4-6](#xls_parsing) 所示。
- en: Example 4-6\. xls_parsing.py
  id: totrans-125
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-6\. xls_parsing.py
- en: '[PRE8]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[![1](assets/1.png)](#co_working_with_file_based_and_feed_based_data_in_python_CO6-1)'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_working_with_file_based_and_feed_based_data_in_python_CO6-1)'
- en: Notice that this structure is similar to the one we use when working with the
    *csv* library.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这个结构与我们在使用*csv*库时使用的结构类似。
- en: '[![2](assets/2.png)](#co_working_with_file_based_and_feed_based_data_in_python_CO6-2)'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_working_with_file_based_and_feed_based_data_in_python_CO6-2)'
- en: The function `get_rows()` is specific to the *xlrd* library; it converts the
    rows of our current worksheet into a list that can be looped over.^([12](ch04.html#idm45143422318528))
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 函数`get_rows()`是特定于*xlrd*库的；它将我们当前工作表的行转换为一个可以循环遍历的列表。
- en: '[![3](assets/3.png)](#co_working_with_file_based_and_feed_based_data_in_python_CO6-3)'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_working_with_file_based_and_feed_based_data_in_python_CO6-3)'
- en: There will be some funkiness within the “dates” written to our output file.^([13](ch04.html#idm45143422098736))
    We’ll look at how to fix up the dates in [“Decrypting Excel Dates”](ch07.html#decrypting_excel_dates).
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的输出文件中将会有一些关于“日期”的怪异之处。我们将看看如何在[“解密Excel日期”](ch07.html#decrypting_excel_dates)中修复这些日期。
- en: One thing we’ll see in this output file is some *serious* weirdness in the values
    recorded in the `observation_date` field, reflecting the fact that, as the *xlrd*
    library’s creators put it:^([14](ch04.html#idm45143422094400))
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 这个输出文件中我们将看到一些*严重*的奇怪数值记录在`observation_date`字段中，这反映了正如*xlrd*库的创建者所说的那样：
- en: 'Dates in Excel spreadsheets: In reality, there are no such things. What you
    have are floating point numbers and pious hope.'
  id: totrans-134
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Excel电子表格中的日期：实际上，这样的东西是不存在的。你所拥有的是浮点数和虔诚的希望。
- en: As a result, getting a useful, human-readable date out of an *.xls* file requires
    some significant cleanup, which we’ll address in [“Decrypting Excel Dates”](ch07.html#decrypting_excel_dates).
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，从*.xls*文件中获得有用的、人类可读的日期需要一些重要的清理工作，我们将在[“解密Excel日期”](ch07.html#decrypting_excel_dates)中解决这个问题。
- en: As these exercises have hopefully demonstrated, with some clever libraries and
    a few tweaks to our basic code configuration, it’s possible to wrangle data from
    a wide range of table-type data formats with Python quickly and easily. At the
    same time, I hope that these examples have also illustrated why working with text-based
    and/or open source formats is almost always preferable,^([15](ch04.html#idm45143422084224))
    because they often require less “cleaning” and transformation to get them into
    a clear, usable state.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 正如这些练习所希望展示的那样，通过一些聪明的库和对基本代码配置的一些调整，使用Python快速轻松地处理来自各种表格类型数据格式的数据是可能的。同时，我希望这些示例也说明了为什么几乎总是更倾向于使用基于文本和/或开放源代码的格式，因为它们通常需要更少的“清理”和转换以使它们进入清晰、可用的状态。
- en: Finally, Fixed-Width
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最后，固定宽度
- en: Though I didn’t mention it at the top of this section, one of the very oldest
    versions of table-type data is what’s known as “fixed-width.” As the name implies,
    each data column in a fixed-width table contains a specific, predefined number
    of characters—and *always* that number of characters. This means that the meaningful
    data in fixed-width files are often padded with extra characters, such as spaces
    or zeroes.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我在本节的开头没有提到它，但非常古老版本的表格类型数据之一是所谓的“固定宽度”。顾名思义，固定宽度表中的每个数据列包含特定的、预定义的字符数，而且*总是*那么多的字符。这意味着固定宽度文件中的有意义数据通常会被额外的字符填充，例如空格或零。
- en: Though very uncommon in contemporary data systems, you are still likely to encounter
    fixed-width formats if you’re working with government data sources whose infrastructure
    may be decades old.^([16](ch04.html#idm45143422210096)) For example, the US [National
    Oceanic and Atmospheric Administration](https://noaa.gov/our-history) (NOAA),
    whose origins date back to the early 19th century, offers a wide range of detailed,
    up-to-date weather information online for free through its [Global Historical
    Climatology Network](https://ncdc.noaa.gov/data-access/land-based-station-data/land-based-datasets/global-historical-climatology-network-ghcn),
    much of which is published in a fixed-width format. For example, information about
    the stations’ unique identifier, locations, and what network(s) they are a part
    of is stored in the [*ghcnd-stations.txt* file](https://www1.ncdc.noaa.gov/pub/data/ghcn/daily).
    To interpret any actual weather data readings (many of which are *also* released
    as fixed-width files), you’ll need to cross-reference the station data with the
    weather data.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在当代数据系统中非常罕见，但如果你在处理可能存在几十年历史的政府数据源，仍然可能会遇到固定宽度格式。[^16](ch04.html#idm45143422210096)例如，美国国家海洋和大气管理局（[National
    Oceanic and Atmospheric Administration](https://noaa.gov/our-history) ，NOAA）的起源可以追溯到19世纪初，通过其[全球历史气候网络](https://ncdc.noaa.gov/data-access/land-based-station-data/land-based-datasets/global-historical-climatology-network-ghcn)在网上免费提供了大量详细的最新天气信息，其中很多是以固定宽度格式发布的。例如，关于气象站的唯一标识符、位置以及它们属于哪个（些）网络的信息存储在[*ghcnd-stations.txt*
    文件](https://www1.ncdc.noaa.gov/pub/data/ghcn/daily)中。要解释任何实际的天气数据读数（其中许多也是以固定宽度文件发布的），您需要将气象站数据与天气数据进行交叉引用。
- en: Even more than other table-type data files, working with fixed-width data can
    be especially tricky if you don’t have access to the metadata that describes how
    the file and its fields are organized. With delimited files, it’s often possible
    to eyeball the file in a text editor and identify the delimiter used with a reasonable
    level of confidence. At worst, you can simply try parsing the file using different
    delimiters and see which yields the best results. With fixed-width files—especially
    large ones—if there’s no data for a particular field in the sample of the data
    you inspect, it’s easy to end up inadvertently lumping together multiple data
    fields.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他表格类型的数据文件相比，如果没有访问描述文件及其字段组织方式的元数据，使用固定宽度数据可能特别棘手。对于分隔文件，通常可以在文本编辑器中查看文件并以合理的置信水平识别所使用的分隔符。在最坏的情况下，您可以尝试使用不同的分隔符解析文件，看看哪个产生了最佳结果。对于固定宽度文件，特别是对于大文件，如果在您检查的数据样本中某个字段没有数据，很容易意外地将多个数据字段合并在一起。
- en: Fortunately, metadata about the *ghcnd-stations.txt* file that we’re using as
    our data source *is* included in the *readme.txt* file in the same folder on [the
    NOAA site](https://www1.ncdc.noaa.gov/pub/data/ghcn/daily/readme.txt).
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，我们正在使用作为数据源的*ghcnd-stations.txt*文件的元数据*也包含在*NOAA网站的同一文件夹中的*readme.txt*文件中。
- en: 'Looking through that *readme.txt* file, we find the heading `IV. FORMAT OF
    "ghcnd-stations.txt"`, which contains the following table:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在浏览*readme.txt*文件时，我们发现了标题为`IV. FORMAT OF "ghcnd-stations.txt"`的部分，其中包含以下表格：
- en: '[PRE9]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: This is followed by a detailed description of what each field contains or means,
    including information like units. Thanks to this robust *data dictionary*, we
    now know not just how the *ghcnd-stations.txt* file is organized but also how
    to interpret the information it contains. As we’ll see in [Chapter 6](ch06.html#chapter6),
    finding (or building) a data dictionary is an essential part of assessing or improving
    the quality of our data. At the moment, however, we can just focus on transforming
    this fixed-width file into a *.csv*, as detailed in [Example 4-7](#fixed_width_parsing).
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 随后详细描述了每个字段包含或表示的内容，包括单位等信息。由于这个强大的*数据字典*，我们现在不仅知道*ghcnd-stations.txt*文件的组织方式，还知道如何解释它包含的信息。正如我们将在[第6章](ch06.html#chapter6)中看到的，找到（或构建）数据字典是评估或改善数据质量的重要部分。然而，目前，我们可以专注于将这个固定宽度文件转换为*.csv*，如[示例4-7](#fixed_width_parsing)中所详述的那样。
- en: Example 4-7\. fixed_width_parsing.py
  id: totrans-145
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-7\. fixed_width_parsing.py
- en: '[PRE10]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[![1](assets/1.png)](#co_working_with_file_based_and_feed_based_data_in_python_CO7-1)'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_working_with_file_based_and_feed_based_data_in_python_CO7-1)'
- en: Since we don’t have anything *within* the file that we can draw on for column
    headers, we have to “hard code” them based on the information in the [*readme.txt*
    file](https://www1.ncdc.noaa.gov/pub/data/ghcn/daily/readme.txt). Note that I’ve
    eliminated special characters and used underscores in place of spaces to minimize
    hassles when cleaning and analyzing this data later on.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 由于文件内部没有我们可以用作列标题的内容，我们必须根据 [*readme.txt* 文件中的信息](https://www1.ncdc.noaa.gov/pub/data/ghcn/daily/readme.txt)
    “硬编码”它们。请注意，我已经删除了特殊字符，并在空格位置使用下划线，以便在稍后清理和分析数据时最大限度地减少麻烦。
- en: '[![2](assets/2.png)](#co_working_with_file_based_and_feed_based_data_in_python_CO7-2)'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_working_with_file_based_and_feed_based_data_in_python_CO7-2)'
- en: Python actually views lines of text as just lists of characters, so we can just
    tell it to give us the characters between two numbered index positions. Like the
    `range()` function, the character at the first position is included, but the second
    number is not. Also recall that Python starts counting lists of items at zero
    (often called *zero-indexing*). This means that for each entry, the first number
    will be one *less* than whatever the metadata says, but the righthand number will
    be the same.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: Python 实际上将文本行视为字符列表，因此我们只需告诉它给出两个编号位置之间的字符。就像 `range()` 函数一样，包括第一个位置的字符，但第二个数字不包括在内。还要记住，Python
    从零开始计算列表项（通常称为 *零索引*）。这意味着对于每个条目，第一个数字将比元数据所示的少一个，但右手边的数字将相同。
- en: If you run the script in [Example 4-7](#fixed_width_parsing) and open your output
    *.csv* file in a spreadsheet program, you’ll notice that the values in some of
    the columns are not formatted consistently. For example, in the `ELEVATION` column,
    the numbers with decimals are left justified, but those without decimals are right
    justified. What’s going on?
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你运行 [示例 4-7](#fixed_width_parsing) 中的脚本，并在电子表格程序中打开你的输出 *.csv* 文件，你会注意到某些列中的值格式不一致。例如，在
    `ELEVATION` 列中，带小数点的数字左对齐，而没有小数点的数字右对齐。到底是怎么回事？
- en: Once again, opening the file in a text editor is enlightening. Although the
    file we’ve created is *technically* comma separated, the values we put into each
    of our newly “delimited” columns still contain the extra spaces that existed in
    the original file. As a result, our new file still looks pretty “fixed-width.”
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 再次打开文本编辑器查看文件是有启发性的。尽管我们创建的文件 *技术上* 是逗号分隔的，但我们放入每个新“分隔”列的值仍然包含原始文件中存在的额外空格。因此，我们的新文件仍然看起来相当“固定宽度”。
- en: In other words—just as we saw in the case of Excel “dates”—converting our file
    to a *.csv* does not “automagically” generate sensible data types in our output
    file. Determining what data type each field should have—and cleaning them up so
    that they behave appropriately—is part of the data cleaning process that we’ll
    address in [Chapter 7](ch07.html#chapter7).
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说——就像我们在 Excel “日期”案例中看到的那样——将我们的文件转换为 *.csv* 文件并不会“自动”在输出文件中生成合理的数据类型。确定每个字段应该具有的数据类型，并清理它们以使其表现得合适，是数据清理过程的一部分，我们将在
    [第7章](ch07.html#chapter7) 中讨论。
- en: Feed-Based Data—Web-Driven Live Updates
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于 Web 的数据源驱动实时更新
- en: The structure of table-type data formats is well suited to a world where most
    “data” has already been filtered, revised, and processed into a relatively well-organized
    collection of numbers, dates, and short strings. With the rise of the internet,
    however, came the need to transmit large quantities of the type of “free” text
    found in, for example, news stories and social media feeds. Because this type
    of data content typically includes characters like commas, periods, and quotation
    marks that affect its semantic meaning, fitting it into a traditional delimited
    format will be problematic at best. What’s more, the horizontal bias of delimited
    formats (which involves lots of left-right scrolling) runs counter to the vertical-scrolling
    conventions of the web. Feed-based data formats have been designed to address
    both of these limitations.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 表格类型数据格式的结构非常适合一个已经被过滤、修订并转化为相对整理良好的数字、日期和短字符串集合的世界。然而，随着互联网的兴起，传输大量“自由”文本数据的需求也随之而来，比如新闻故事和社交媒体动态。由于这类数据内容通常包括逗号、句点和引号等影响其语义含义的字符，将其放入传统的分隔格式中将会出现问题。此外，分隔格式的水平偏向（涉及大量左右滚动）与
    Web 的垂直滚动约定相矛盾。Feed-based 数据格式已经设计用来解决这些限制。
- en: 'At a high level, there are two main types of feed-based data formats: XML and
    JSON. Both are text-based formats that allow the data provider to define their
    own unique data structure, making them extremely flexible and, consequently, useful
    for the wide variety of content found on internet-connected websites and platforms.
    Whether they’re located online or you save a copy locally, you’ll recognize these
    formats, in part, by their coordinating *.xml* and *.json* file extensions:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在高层次上，基于*feed*的数据格式主要有两种：XML和JSON。它们都是文本格式，允许数据提供者定义自己独特的数据结构，使其极其灵活，因此非常适用于互联网连接的网站和平台上发现的各种内容。无论它们位于在线位置还是您在本地保存了一份副本，您都可以通过它们的*.xml*和*.json*文件扩展名一部分来识别它们：
- en: '*.xml*'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '*.xml*'
- en: Extensible Markup Language encompasses a broad range of file formats, including
    *.rss*, *.atom*, and even *.html*. As the most generic type of markup language,
    XML is extremely flexible and was perhaps the original data format for web-based
    data feeds.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 可扩展标记语言涵盖了广泛的文件格式，包括*.rss*、*.atom*，甚至*.html*。作为最通用的标记语言类型，XML非常灵活，也许是最早用于基于网络的数据feed的数据格式。
- en: '*.json*'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '*.json*'
- en: JavaScript Object Notation files are somewhat more recent than XML files but
    serve a similar purpose. In general, JSON files are less descriptive (and therefore
    shorter and more concise) than XML files. This means that they can encode an almost
    identical amount of data as an XML file while taking up less space, which is especially
    important for speed on the mobile web. Equally important is the fact that JSON
    files are essentially large `object` data types within the JavaScript programming
    language—which is the language that underpins many, if not most, websites and
    mobile apps. This means that parsing JSON-formatted data is very easy for any
    site or program that uses JavaScript, especially when compared with XML. Fortunately,
    JavaScript `object` data types are very similar to Python `dict` data types, which
    also makes working with JSON in Python very straightforward.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: JSON文件比XML文件稍微新一些，但目的类似。总体而言，JSON文件比XML文件描述性较少（因此更短、更简洁）。这意味着它们可以编码几乎与XML文件相同数量的数据，同时占用更少的空间，这对移动网络的速度尤为重要。同样重要的是，JSON文件本质上是JavaScript编程语言中的大型`object`数据类型——这是许多，如果不是大多数，网站和移动应用的基础语言。这意味着解析JSON格式的数据对于使用JavaScript的任何网站或程序来说都非常容易，尤其是与XML相比。幸运的是，JavaScript的`object`数据类型与Python的`dict`数据类型非常相似，这也使得在Python中处理JSON非常简单。
- en: Before we dive into how to work with each of these file types in Python, let’s
    review when we might *want* feed-type data and where to find it when we do.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入讨论如何在Python中处理这些文件类型之前，让我们回顾一下，当我们需要*feed*类型的数据时以及在何处找到它。
- en: When to work with feed-type data
  id: totrans-162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 何时使用*feed*类型数据
- en: 'In a sense, feed-type data is to the 21st century what table-type data was
    to the 20th: the sheer volume of feed-type data generated, stored, and exchanged
    on the web every day is probably millions of times greater than that of all of
    the table-type data in the world put together—in large part because feed-type
    data is what powers social media sites, news apps, and everything in between.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 从某种意义上说，*feed*类型的数据对于21世纪而言就像20世纪的表格类型数据一样：每天在网络上生成、存储和交换的*feed*类型数据的体积可能比全球所有表格类型数据的总和还要大数百万倍——这主要是因为*feed*类型数据是社交媒体网站、新闻应用程序等一切的动力来源。
- en: From a data wrangling perspective, you’ll generally want feed-type data when
    the phenomenon you’re exploring is time sensitive and updated on a frequent and/or
    unpredictable basis. Typically, this type of data is generated in response to
    a human or natural process, such as (once again) posting to social media, publishing
    a news story, or recording an earthquake.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 从数据处理的角度来看，当你探索的现象是时间敏感的并且经常或者说不可预测地更新时，通常你会想要*feed*类型的数据。典型地，这种类型的数据是响应于人类或自然过程生成的，比如（再次）在社交媒体上发布、发布新闻故事或记录地震。
- en: Both file-based, table-type data and web-based, feed-type data can contain historical
    information, but as we discussed at the start of this chapter, the former usually
    reflects the data as it stood at a fixed point in time. The latter, by contrast,
    is typically organized in a “reverse-chronological” (most recent first) order,
    with the first entry being whatever data record was most recently created at the
    time you accessed the data, rather than a predetermined publication date.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 基于文件的表格类型数据和基于Web的提要类型数据都可以包含历史信息，但正如我们在本章开头讨论的那样，前者通常反映了某一固定时间点的数据。相比之下，后者通常以“逆时间顺序”（最新的首先）组织，首个条目是您访问数据时最近创建的数据记录，而不是预定的发布日期。
- en: Where to find feed-type data
  id: totrans-166
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何找到提要类型的数据
- en: 'Feed-type data is found almost exclusively on the web, often at special URLs
    known as application programming interface (API) *endpoints*. We’ll get into the
    details of working with APIs in [Chapter 5](ch05.html#chapter5), but for now all
    you need to know is that API endpoints are really just data-only web pages: you
    can view many of them using a regular web browser, but all you’ll see is the data
    itself. Some API endpoints will even return different data depending on the information
    *you* send to them, and this is part of what makes working with feed-type data
    so flexible: by changing just a few words or values in your code, you can access
    a totally different dataset!'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 提要类型的数据几乎完全可以在网上找到，通常位于称为应用程序编程接口（API）*端点*的特殊网址上。我们将在[第5章](ch05.html#chapter5)详细讨论使用API的细节，但现在你只需要知道API端点实际上只是数据页面：你可以使用常规的Web浏览器查看许多页面，但你所看到的只是数据本身。一些API端点甚至会根据您发送给它们的信息返回不同的数据，这正是处理提要类型数据如此灵活的部分原因：通过仅更改代码中的几个单词或值，您可以访问完全不同的数据集！
- en: 'Finding APIs that offer feed-type data doesn’t require too much in the way
    of special search strategies because usually the sites and services that have
    APIs *want* you to find them. Why? Simply put, when someone writes code that makes
    use of an API, it (usually) returns some benefit to the company that provides
    it—even if that benefit is just more public exposure. In the early days of Twitter,
    for example, many web developers wrote programs using the Twitter API—both making
    the platform more useful *and* saving the company the expense and effort of figuring
    out what users wanted and then building it. By making so much of their platform
    data available for free (at first), the API gave rise to several companies that
    Twitter would eventually purchase—though many more would also be put out of business
    when either the API or its terms of service changed.^([17](ch04.html#idm45143421937168))
    This highlights one of the particular issues that can arise when working with
    any type of data, but especially the feed-type data made available by for-profit
    companies: both the data and your right to access it can change at any time, without
    warning. So while feed-type data sources are indeed valuable, they are also ephemeral
    in more ways than one.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 找到提供提要类型数据的API并不需要太多特殊的搜索策略，因为通常具有API的网站和服务*希望*您找到它们。为什么呢？简单来说，当有人编写使用API的代码时，它（通常）会为提供API的公司带来一些好处，即使这种好处只是更多的公众曝光。例如，在Twitter早期，许多Web开发人员使用Twitter
    API编写程序，这不仅使平台更加有用，*还*节省了公司理解用户需求并构建的费用和工作量。通过最初免费提供其平台数据的API，Twitter促使了几家公司的诞生，这些公司最终被Twitter收购，尽管还有更多公司在API或其服务条款发生变化时被迫停业。^([17](ch04.html#idm45143421937168))
    这突显了处理任何类型数据时可能出现的特定问题之一，尤其是由盈利公司提供的提要类型数据：数据本身及您访问它的权利随时都可能在没有警告的情况下发生变化。因此，尽管提要类型数据源确实很有价值，但它们在更多方面上也是短暂的。
- en: Wrangling Feed-Type Data with Python
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Python整理提要类型数据
- en: As with table-type data, wrangling feed-type data in Python is made possible
    by a combination of helpful libraries and the fact that formats like JSON already
    resemble existing data types in the Python programming language. Moreover, we’ll
    see in the following sections that XML and JSON are often functionally interchangeable
    for our purposes (though many APIs will only offer data in one format or the other).
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 与表格类型的数据类似，使用Python整理提要类型的数据是可能的，这得益于一些有用的库以及像JSON这样的格式已经与Python编程语言中的现有数据类型相似。此外，在接下来的章节中，我们将看到XML和JSON对于我们的目的通常可以互换使用（尽管许多API只会提供其中一种格式的数据）。
- en: 'XML: One markup to rule them all'
  id: totrans-171
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: XML：一种标记语言来统一它们
- en: Markup languages are among the oldest forms of standardized document formats
    in computing, designed with the goal of creating text-based documents that can
    be easily read by both humans and machines. XML became an increasingly important
    part of internet infrastructure in the 1990s as the variety of devices accessing
    and displaying web-based information made the separation of content (e.g., text
    and images) from formatting (e.g., page layout) more of a necessity. Unlike an
    HTML document—in which content and formatting are fully commingled—an XML document
    says pretty much nothing about how its information should be displayed. Instead,
    its tags and attributes act as *metadata* about what kind of information it contains,
    along with the data itself.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 标记语言是计算机中最古老的标准化文档格式之一，旨在创建既能够轻松被人类阅读又能够被机器解析的基于文本的文档。XML在20世纪90年代成为互联网基础设施的越来越重要的一部分，因为各种设备访问和显示基于Web的信息，使得内容（如文本和图像）与格式（如页面布局）的分离变得更加必要。与HTML文档不同——其中内容和格式完全混合——XML文档几乎不指定其信息应如何显示。相反，它的标签和属性充当关于它包含的信息类型以及数据本身的*元数据*。
- en: To get a feel for what XML looks like, take a look at [Example 4-8](#sample_xml_document).
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解XML的外观，可以查看[示例 4-8](https://wiki.example.org/sample_xml_document)。
- en: Example 4-8\. A sample XML document
  id: totrans-174
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-8\. 一个样本XML文档
- en: '[PRE11]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: There are a couple of things going here. The very first line is called the *document
    type* (or `doc-type`) declaration; it’s letting us know that the rest of the document
    should be interpreted as XML (as opposed to any of the other web or markup languages,
    some of which we’ll review later in this chapter).
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有几个要点。第一行称为*文档类型*（或`doc-type`）声明；它告诉我们，文档的其余部分应解释为XML（而不是其他任何网络或标记语言，本章稍后将介绍其中一些）。
- en: 'Starting with the line:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 从以下行开始：
- en: '[PRE12]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'we are into the substance of the document itself. Part of what makes XML so
    flexible is that it only contains two real grammatical structures, both of which
    are included in [Example 4-8](#sample_xml_document):'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进入文档本身的内容。XML如此灵活的部分原因在于它只包含两种真正的语法结构，这两种都包含在[示例 4-8](https://wiki.example.org/sample_xml_document)中：
- en: tags
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 标签
- en: Tags can be either paired (like `element1`, `element2`, `someElement`, or even
    `mainDoc`) or self-closed (like `element3`). The name of a tag is always enclosed
    by *carets* (`<>`). In the case of a closing tag, the opening caret is immediately
    followed by a forward slash (`/`). A matched pair of tags, or a self-closed tag,
    are also described as XML *elements*.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 标签可以是成对的（如`element1`、`element2`、`someElement`或甚至`mainDoc`），也可以是自闭合的（如`element3`）。标签的名称始终用*尖括号*（`<>`）括起来。对于闭合标签，在开放的尖括号后面紧跟着斜杠（`/`）。成对的标签或自闭合标签也被称为XML的*元素*。
- en: attributes
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 属性
- en: Attributes can exist only inside of tags (like `anAttribute`). Attributes are
    a type of *key/value pair* in which the attribute name (or *key*) is immediately
    followed by an equals sign (`=`), followed by the *value* surrounded by double
    quotation marks (`""`).
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 属性只能存在于标签内部（如`anAttribute`）。属性是一种*键/值对*，其中属性名（或*键*）紧跟着等号（`=`），后面是用双引号（`""`）括起来的*值*。
- en: An XML element is whatever is contained between an opening tag and its matching
    closing tag (e.g., `<elements>` and `</elements>`). As such, a given XML element
    may contain many tags, each of which may also contain other tags. Any tags may
    also have any number of attributes (including none). A self-closed tag is also
    considered an element.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: XML元素是包含在开放标签及其匹配闭合标签之间的任何内容（例如，`<elements>`和`</elements>`）。因此，给定的XML元素可能包含许多标签，每个标签也可以包含其他标签。任何标签也可以具有任意数量的属性（包括没有）。自闭合标签也被视为元素之一。
- en: 'The only other meaningful rule for structuring XML documents is that when tags
    appear inside other tags, *the most recently opened tag must be closed first*.
    In other words, while this is a legitimate XML structure:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 当标签出现在其他标签内部时，*最近打开的标签必须首先关闭*。换句话说，虽然以下是一个合法的XML结构：
- en: '[PRE13]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'this is not:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 但这不是：
- en: '[PRE14]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: This principle of *last opened, first closed* is also described as *nesting*,
    similar to the “nested” `for...in` loops from [Figure 2-3](ch02.html#loop_nesting_diagram).^([18](ch04.html#idm45143413262208))
    Nesting is especially important in XML documents because it governs one of the
    primary mechanisms that we use to read or *parse* XML (and other markup language)
    documents with code. In an XML document, the first element after the `doc-type`
    declaration is known as the *root* element. If the XML document has been formatted,
    the root element will always be left justified, and any element that is nested
    directly *within* that element will be indented one level to the right and is
    referred to as a *child* element. In [Example 4-8](#sample_xml_document), then,
    `<mainDoc>` would be considered the *root* element, and `<elements>` would be
    its child. Likewise, `<mainDoc>` is the *parent* element of `<elements>` ([Example 4-9](#sample_xml_annotated)).
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '*先开后闭*原则也被称为*嵌套*，类似于 [图 2-3](ch02.html#loop_nesting_diagram) 中的“嵌套” `for...in`
    循环。^([18](ch04.html#idm45143413262208)) 嵌套在 XML 文档中尤为重要，因为它管理了我们用代码读取或*解析* XML（和其他标记语言）文档的主要机制之一。在
    XML 文档中，`doc-type` 声明之后的第一个元素称为*根*元素。如果 XML 文档已经格式化，根元素将始终左对齐，而直接*在*该元素内嵌套的任何元素将向右缩进一级，并称为*子*元素。因此，在
    [示例 4-8](#sample_xml_document) 中，`<mainDoc>` 将被视为*根*元素，`<elements>` 将被视为其子元素。同样，`<mainDoc>`
    是`<elements>` 的*父*元素（[示例 4-9](#sample_xml_annotated)）。'
- en: Example 4-9\. An annotated XML document
  id: totrans-190
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Example 4-9\. 一个带注释的 XML 文档
- en: '[PRE15]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Given this trend for genealogical jargon, you might be wondering: if `<elements>`
    is the parent of `<element3>`, and `<mainDoc>` is the parent of `<elements>`,
    does that make `<mainDoc>` the *grandparent* of `<element3>`? The answer is: yes,
    but no. While `<mainDoc>` *is* the “parent” of the “parent” of `<element3>`, the
    term “grandparent” is never used in describing an XML structure—that could get
    complicated fast! Instead, we simply describe the relationship as exactly that:
    `<mainDoc>` is the *parent* of the *parent* of `<element3>`.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于这种谱系术语的趋势，您可能会想知道：如果`<elements>`是`<element3>`的父级，`<mainDoc>`是`<elements>`的父级，那么`<mainDoc>`是否是`<element3>`的*祖父*？答案是：是的，但也不是。虽然`<mainDoc>`确实是`<element3>`的“父级”的“父级”，但在描述
    XML 结构时从不使用术语“祖父”—这可能会很快变得复杂！相反，我们简单地描述这种关系正是：`<mainDoc>`是`<element3>`的*父级*的*父级*。
- en: 'Fortunately, there is no such complexity associated with XML attributes: they
    are simply key/value pairs, and they can *only* exist within XML tags, like so:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，与 XML 属性相关的复杂性不存在：它们只是键/值对，并且它们*只能*存在于 XML 标签内部，如下所示：
- en: '[PRE16]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Note that there is no space on either side of the equals sign, just as there
    is no space between the carets and slashes of an element tag.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，等号两侧没有空格，就像元素标签的尖括号和斜杠之间没有空格一样。
- en: Like writing in English (or in Python), the question of when to use tags versus
    attributes for a particular piece of information is largely a matter of preference
    and style. Both Examples [4-10](#sample_book_XML_1) and [4-11](#sample_book_XML_2),
    for example, contain the same information about this book, but each is structured
    slightly differently.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 就像用英语（或 Python）写作一样，何时使用标签而不是属性来表示特定信息，主要取决于个人偏好和风格。例如，[示例 4-10](#sample_book_XML_1)
    和 [示例 4-11](#sample_book_XML_2) 中都包含了关于这本书相同的信息，但每个结构略有不同。
- en: Example 4-10\. Sample XML book data—more attributes
  id: totrans-197
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Example 4-10\. 示例 XML 书籍数据—更多属性
- en: '[PRE17]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Example 4-11\. Sample XML book data—more elements
  id: totrans-199
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Example 4-11\. 示例 XML 书籍数据—更多元素
- en: '[PRE18]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: This degree of flexibility means XML is highly adaptable to a wide variety of
    data sources and formatting preferences. At the same time, it can easily create
    a situation where *every* new data source requires writing custom code. Obviously,
    this would be a pretty inefficient system, especially if many people and organizations
    were publishing pretty similar types of data.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 这种灵活性意味着 XML 非常适应各种数据源和格式化偏好。同时，它可能很容易创建这样的情况：*每一个*新数据源都需要编写定制代码。显然，这将是一个相当低效的系统，特别是如果许多人和组织发布的数据类型非常相似的情况下。
- en: 'It’s not surprising, then, that there are a large number of XML *specifications*
    that define additional rules for formatting XML documents that are intended to
    hold particular types of data. I’m highlighting a few notable examples here, as
    these are formats you may come across in the course of your data wrangling work.
    Despite their various format names and file extensions, however, we can parse
    them all using the same method that we’ll look at shortly in [Example 4-12](#xml_parsing):'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: RSS
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: Really Simple Syndication is an XML specification first introduced in the late
    1990s for news information. The *.atom* XML format is also widely used for these
    purposes.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: KML
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: Keyhole Markup Language is an internationally accepted standard for encoding
    both two-dimensional and three-dimensional geographic data and is compatible with
    tools like Google Earth.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: SVG
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: Scalable Vector Graphics is a commonly used format for graphics on the web,
    thanks to its ability to scale drawings without loss of quality. Many common graphics
    programs can output *.svg* files, which can then be included in web pages and
    other documents that will look good on a wide variety of screen sizes and devices.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: EPUB
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: Electronic publication format (*.epub*) is the widely accepted open standard
    for digital book publishing.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: As you can see from the preceding list, some common XML formats clearly indicate
    their relationship to XML; many others do not.^([19](ch04.html#idm45143410254992))
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a high-level sense of how XML files work, let’s see what it
    takes to parse one with Python. Although Python has some built-in tools for parsing
    XML, we’ll be using a library called *lxml*, which is particularly good at [quickly
    parsing large XML files](https://nickjanetakis.com/blog/how-i-used-the-lxml-library-to-parse-xml-20x-faster-in-python#xmltodict-vs-python-s-standard-library-vs-lxml).
    Even though our example files that follow are quite small, know that we could
    use basically the same code even if our data files got considerably larger.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: 'To begin with, we’ll be using an XML version of the same “U6” unemployment
    data that I’ve already downloaded from the FRED website using its API.^([20](ch04.html#idm45143410249984))
    After downloading a copy of this file from [Google Drive](https://drive.google.com/file/d/1gPGaDTT9Nn6BtlTtVp7gQLSuocMyIaLU),
    you can use the script in [Example 4-12](#xml_parsing) to convert the source XML
    to a *.csv*. Start with the `pip install`:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Example 4-12\. xml_parsing.py
  id: totrans-215
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[![1](assets/1.png)](#co_working_with_file_based_and_feed_based_data_in_python_CO8-1)'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: In this instance, there’s nothing within the data file (like a sheet name) that
    we can pull as a filename, so we’ll just make our own and use it to both load
    our source data and label our output file.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_working_with_file_based_and_feed_based_data_in_python_CO8-2)'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: I lied! The values we’ve been using for the “mode” of the `open()` function
    assume we want to interpret the source file as *text*. But because the *lxml*
    library expects byte data rather than text, we’ll use `rb` (“read bytes”) as the
    “mode.”
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 我撒了谎！我们一直在使用`open()`函数的“模式”值假定我们希望将源文件解释为*文本*。但因为*lxml*库期望字节数据而不是文本，我们将使用`rb`（“读取字节”）作为“模式”。
- en: '[![3](assets/3.png)](#co_working_with_file_based_and_feed_based_data_in_python_CO8-3)'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_working_with_file_based_and_feed_based_data_in_python_CO8-3)'
- en: There is a lot of malformed XML out there! In order to make sure that what looks
    like good XML actually *is*, we’ll retrieve the current XML document’s “root”
    element and make sure that it works.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 有很多格式错误的 XML！为了确保看起来像好的 XML 实际上确实是好的，我们将检索当前 XML 文档的“根”元素，并确保它有效。
- en: '[![4](assets/4.png)](#co_working_with_file_based_and_feed_based_data_in_python_CO8-4)'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](assets/4.png)](#co_working_with_file_based_and_feed_based_data_in_python_CO8-4)'
- en: Because our XML is currently stored as byte data, we need to use the `etree.tostring()`
    method in order to view it as one.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们的 XML 当前存储为字节数据，所以我们需要使用`etree.tostring()`方法来将其视为文本。
- en: '[![5](assets/5.png)](#co_working_with_file_based_and_feed_based_data_in_python_CO8-5)'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](assets/5.png)](#co_working_with_file_based_and_feed_based_data_in_python_CO8-5)'
- en: Thanks to the *lxml*, each XML element (or “node”) in our document has a property
    called `attrib`, whose data type is a Python dictionary (`dict`). Using the [`.keys()`
    method](https://docs.python.org/3/library/stdtypes.html#typesmapping) returns
    all of our XML element’s attribute keys as a list. Since all of the elements in
    our source file are identical, we can use the keys of the first one to create
    a “header” row for our output file.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 多亏了*lxml*，我们文档中的每个 XML 元素（或“节点”）都有一个名为`attrib`的属性，其数据类型是 Python 字典（`dict`）。使用[`.keys()`方法](https://docs.python.org/3/library/stdtypes.html#typesmapping)会返回我们
    XML 元素所有属性键的列表。由于源文件中的所有元素都是相同的，我们可以使用第一个元素的键来创建输出文件的“标题行”。
- en: '[![6](assets/6.png)](#co_working_with_file_based_and_feed_based_data_in_python_CO8-6)'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '[![6](assets/6.png)](#co_working_with_file_based_and_feed_based_data_in_python_CO8-6)'
- en: The *lxml* library [converts XML elements to lists](https://lxml.de/tutorial.html#elements-are-lists),
    so we can use a simple `for...in` loop to go through the elements in our document.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '*lxml*库会将 XML 元素[转换为列表](https://lxml.de/tutorial.html#elements-are-lists)，因此我们可以使用简单的`for...in`循环遍历文档中的元素。'
- en: 'As it happens, the XML version of our unemployment data is structured very
    simply: it’s just a list of elements, and *all* the values we want to access are
    stored as attributes. As a result, we were able to pull the attribute values out
    of each element as a list and write them directly to our *.csv* file with only
    one line of code.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，我们的失业数据的 XML 版本结构非常简单：它只是一个元素列表，*所有*我们想要访问的值都存储为属性。因此，我们能够从每个元素中提取属性值作为列表，并用一行代码直接写入到我们的*.csv*文件中。
- en: Of course, there are many times when we’ll want to pull data from more complex
    XML formats, especially those like RSS or Atom. To see what it takes to handle
    something slightly more complex, in [Example 4-13](#bbc_example) we’ll parse the
    BBC’s RSS feed of science and environment stories, which you can download a copy
    of from my [Google Drive](https://drive.google.com/file/d/1zOaksshLfmXxLTipoOjTTnuO6PsVQgg2).
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，有许多时候我们会想从更复杂的 XML 格式中提取数据，特别是像 RSS 或 Atom 这样的格式。为了看看如何处理稍微复杂一点的东西，在[示例 4-13](#bbc_example)中，我们将解析
    BBC 的科学与环境故事的 RSS 源，你可以从我的[Google Drive](https://drive.google.com/file/d/1zOaksshLfmXxLTipoOjTTnuO6PsVQgg2)下载副本。
- en: Example 4-13\. rss_parsing.py
  id: totrans-231
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-13\. rss_parsing.py
- en: '[PRE21]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[![1](assets/1.png)](#co_working_with_file_based_and_feed_based_data_in_python_CO9-1)'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_working_with_file_based_and_feed_based_data_in_python_CO9-1)'
- en: As always, we’ll want to balance what we handle programmatically and what we
    review visually. In looking at our data, it’s clear that each article’s information
    is stored in a separate `item` element. Since copying over the individual tag
    and attribute names would be time-consuming and error prone, however, we’ll go
    through *one* `item` element and make a list of all the tags (and attributes)
    within it, which we’ll then use as the column headers for our output *.csv* file.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 和往常一样，我们需要在程序处理和视觉审核之间取得平衡。通过查看我们的数据，可以清楚地看到每篇文章的信息都存储在单独的`item`元素中。然而，复制单个标签和属性名称将是耗时且容易出错的，因此我们将遍历*一个*`item`元素，并列出其中所有标签（和属性），然后将其用作输出*.csv*文件的列标题。
- en: '[![2](assets/2.png)](#co_working_with_file_based_and_feed_based_data_in_python_CO9-2)'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_working_with_file_based_and_feed_based_data_in_python_CO9-2)'
- en: The `iterdescendants()` method is particular to the *lxml* library. It returns
    *only* the *descendants* of an XML element, while [the more common `iter()` method
    would return *both* the element itself *and* its children or “descendants.”](https://lxml.de/api.html#iteration)
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '`iterdescendants()` 方法是 *lxml* 库特有的。它仅返回 XML 元素的 *后代*，而 [更常见的 `iter()` 方法则会返回
    *元素本身* 及其子元素或“后代”。](https://lxml.de/api.html#iteration)'
- en: '[![3](assets/3.png)](#co_working_with_file_based_and_feed_based_data_in_python_CO9-3)'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_working_with_file_based_and_feed_based_data_in_python_CO9-3)'
- en: Using `child.tag` will retrieve the text of the tagname of the child element.
    For example, for the `` <pubDate>` `` element it will return `pubDate`.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `child.tag` 将检索子元素标签名的文本。例如，对于 `` <pubDate>` `` 元素，它将返回 `pubDate`。
- en: '[![4](assets/4.png)](#co_working_with_file_based_and_feed_based_data_in_python_CO9-4)'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](assets/4.png)](#co_working_with_file_based_and_feed_based_data_in_python_CO9-4)'
- en: Only one tag in our `<item>` element has an attribute, but we still want to
    include it in our output.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的 `<item>` 元素中只有一个标签具有属性，但我们仍然希望将其包含在输出中。
- en: '[![5](assets/5.png)](#co_working_with_file_based_and_feed_based_data_in_python_CO9-5)'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](assets/5.png)](#co_working_with_file_based_and_feed_based_data_in_python_CO9-5)'
- en: The `keys()` method will give us a list of all the keys in the list of attributes
    that belong to the tag. Be sure to get its name as a string (instead of a one-item
    list).
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '`keys()` 方法将为我们提供属于标签的属性列表中所有键的列表。确保将其名称作为字符串获取（而不是一个单项列表）。'
- en: '[![6](assets/6.png)](#co_working_with_file_based_and_feed_based_data_in_python_CO9-6)'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '[![6](assets/6.png)](#co_working_with_file_based_and_feed_based_data_in_python_CO9-6)'
- en: That whole `article_example` `for` loop was just to build `tag_list`—but it
    was worth it!
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 整个 `article_example` `for` 循环只是为了构建 `tag_list` —— 但这是值得的！
- en: As you can see from [Example 4-13](#bbc_example), with the help of the *lxml*
    library, parsing even slightly more complex XML in Python is still reasonably
    straightforward.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 如您从 [示例 4-13](#bbc_example) 中所见，借助 *lxml* 库，即使在 Python 中解析稍微复杂的 XML 也仍然相对简单。
- en: While XML is still a popular data format for news feeds and a handful of other
    file types, there are a number of features that make it less than ideal for handling
    the high-volume data feeds of the modern web.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 XML 仍然是新闻源等一小部分文件类型的流行数据格式，但有许多功能使其不太适合处理现代网络的高容量数据源。
- en: 'First, there is the simple issue of size. While XML files can be wonderfully
    descriptive—reducing the need for separate data dictionaries—the fact that most
    elements contain both an opening tag and a corresponding closing tag (e.g., `<item>`
    and `</item>`) also makes XML somewhat *verbose*: there is a lot of text in an
    XML document that *isn’t* content. This isn’t a big deal when your document has
    a few dozen or even a few thousand elements, but when you’re trying to handle
    millions or billions of posts on the social web, all that redundant text can really
    slow things down.'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，存在尺寸的简单问题。尽管 XML 文件可以非常描述性地描述，减少了对独立数据字典的需求，但是大多数元素同时包含开标签和相应的闭标签（例如，`<item>`
    和 `</item>`），这也使得 XML 有些 *冗长*：XML 文档中有很多文本 *不是* 内容。当您的文档有几十甚至几千个元素时，这并不是什么大问题，但是当您尝试处理社交网络上的数百万或数十亿篇帖子时，所有这些冗余文本确实会减慢速度。
- en: Second, while XML isn’t exactly *difficult* to transform into other data formats,
    the process also isn’t exactly seamless. The *lxml* library (among others) makes
    parsing XML with Python pretty simple, but doing the same task with web-focused
    languages like JavaScript is convoluted and onerous. Given JavaScript’s prevalence
    on the web, it’s not surprising that a feed-type data format that works seamlessly
    with JavaScript would be developed at some point. As we’ll see in the next section,
    many of XML’s limitations as a data format are addressed by the `object`-like
    nature of the *.json* format, which is at this point the most popular format for
    feed-type data on the internet.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 第二，虽然将 XML 转换为其他数据格式并不 *难*，但这个过程也不是完全无缝的。*lxml* 库（以及其他一些）使得用 Python 解析 XML 变得非常简单，但是使用像
    JavaScript 这样的面向网络的语言执行相同任务则是冗长而繁琐的。考虑到 JavaScript 在网络上的普及程度，一种能够与 JavaScript
    无缝配合的数据格式在某个时候的开发是不奇怪的。正如我们将在下一节中看到的那样，作为一种数据格式，XML 的许多局限性都被 *.json* 格式的 *对象*
    特性所解决，这在当前是互联网上最流行的 feed 类型数据格式。
- en: 'JSON: Web data, the next generation'
  id: totrans-249
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: JSON：Web 数据，下一代
- en: In principle, JSON is similar to XML in that it uses nesting to cluster related
    pieces of information into records and fields. JSON is also fairly human readable,
    though the fact that it doesn’t support comments means that JSON feeds may require
    more robust data dictionaries than XML documents.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: To get started, let’s take a look at the small JSON document in [Example 4-14](#sample_JSON_document).
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: Example 4-14\. Sample JSON document
  id: totrans-252
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Like XML, the grammatical “rules” of JSON are quite simple: there are only
    three distinct data structures in JSON documents, all of which appear in [Example 4-14](#sample_JSON_document):'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: Key/value pairs
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: Technically, everything within a JSON document is a key/value pair, with the
    *key* enclosed in quotes to the left of a colon (`:`) and the *value* being whatever
    appears to the right of the colon. Note that while keys must *always* be strings,
    *values* can be strings (as in `author`), objects (as in `book`), or lists (as
    in `papers`).
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: Objects
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: 'These are opened and closed using pairs of curly braces (`{}`). In [Example 4-14](#sample_JSON_document),
    there are four objects total: the document itself (indicated by the left-justified
    curly braces), the `book` object, and the two unnamed objects in the `papers`
    list.'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: Lists
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: These are enclosed by square brackets (`[]`) and can only contain comma-separated
    objects.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: While XML and JSON can be used to encode the same data, there are some notable
    differences in what each allows. For example, JSON files do not contain a `doc-type`
    specification, nor can they include comments. Also, while XML lists are somewhat
    implicit (any repeated element functions something like a list), in JSON, lists
    must be specified by square brackets (`[]`).
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: Finally, although JSON was designed with JavaScript in mind, you may have noticed
    that its structures are very similar to Python `dict` and `list` types. This is
    part of what makes parsing JSON very straightforward with Python as well as JavaScript
    (and a range of other languages).
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: 'To see just how straightforward this is, in [Example 4-15](#json_parsing) we’ll
    parse the same data as we did in [Example 4-12](#xml_parsing) but in the *.json*
    format also provided by the FRED API. You can download the file from this Google
    Drive link: [*https://drive.google.com/file/d/1Mpb2f5qYgHnKcU1sTxTmhOPHfzIdeBsq/view?usp=sharing*](https://drive.google.com/file/d/1Mpb2f5qYgHnKcU1sTxTmhOPHfzIdeBsq/view?usp=sharing).'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: Example 4-15\. json_parsing.py
  id: totrans-264
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[![1](assets/1.png)](#co_working_with_file_based_and_feed_based_data_in_python_CO10-1)'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: Because the *json* library interprets every object as a [dictionary view object](https://docs.python.org/3/library/stdtypes.html#dict-views),
    we need to tell Python to convert it to a regular list using the `list()` function.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_working_with_file_based_and_feed_based_data_in_python_CO10-2)'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: In most cases, the simplest way to find the name (or “key”) of the main JSON
    object in our document is just to look at it. Because JSON data is often rendered
    on a single line, however, we can get a better sense of its structure by pasting
    it into [JSONLint](https://jsonlint.com). This lets us see that our target data
    is a list whose key is `observations`.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_working_with_file_based_and_feed_based_data_in_python_CO10-3)'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: Because of the way that the *json* library works, if we try to just write the
    rows directly, we’ll get the values labeled with `dict`, rather than the data
    values themselves. So we need to make another loop that goes through every value
    in every `json` object one at a time and appends that value to our `obj_values`
    list.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: Although JSON is not *quite* as human readable as XML, it has other advantages
    that we’ve touched on, like smaller file size and broader code compatibility.
    Likewise, while JSON is not as descriptive as XML, JSON data sources (often APIs)
    are usually reasonably well documented; this reduces the need to simply infer
    what a given key/value pair is describing. Like all work with data, however, the
    first step in wrangling JSON-formatted data is to understand its context as much
    as possible.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: Working with Unstructured Data
  id: totrans-273
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we discussed in [“Structured Versus Unstructured Data”](#structured_vs_unstructured),
    the process of creating data depends on introducing some structure to information;
    otherwise we can’t methodically analyze or derive meaning from it. Even though
    the latter often includes large segments of human-written “free” text, both table-type
    and feed-type data are relatively structured and, most importantly, machine readable.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: 'When we deal with unstructured data, by contrast, our work always involves
    approximations: we cannot be certain that our programmatic wrangling efforts will
    return an accurate interpretation of the underlying information. This is because
    most unstructured data is a representation of content that is designed to be perceived
    and interpreted by humans. And, as we discussed in [Chapter 2](ch02.html#chapter2),
    while they can process large quantities of data much faster and with fewer errors
    than humans can, computers can still be tricked by unstructured data that would
    never fool a human, such as mistaking a [slightly modified stop sign for a speed
    limit sign](https://spectrum.ieee.org/cars-that-think/transportation/sensors/slight-street-sign-modifications-can-fool-machine-learning-algorithms).
    Naturally, this means that when dealing with data that *isn’t* machine readable,
    we always need to do extra proofing and verification—but Python can still help
    us wrangle such data into a more usable format.'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: 'Image-Based Text: Accessing Data in PDFs'
  id: totrans-276
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Portable Document Format (PDF) was created in the early 1990s as a mechanism
    for preserving the visual integrity of electronic documents—whether they were
    created in a text-editing program or captured from printed materials.^([22](ch04.html#idm45143409744752))
    Preserving documents’ visual appearance also meant that, *unlike* machine-readable
    formats (such as word-processing documents), it was difficult to alter or extract
    their contents—an important feature for creating everything from digital versions
    of contracts to formal letters.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 便携式文档格式（PDF）是在1990年代初创建的一种机制，用于保持电子文档的视觉完整性——无论是在文本编辑程序中创建还是从印刷材料中捕获。保持文档的视觉外观也意味着，与可机器读取格式（如文字处理文档）不同，难以更改或提取其内容——这是创建从合同的数字版本到正式信函的重要特性。
- en: In other words, wrangling the data in PDFs was, at first, somewhat difficult
    by design. Because accessing the data in printed documents is a shared problem,
    however, work in optical character recognition (OCR) actually began as early as
    the late *19th* century.^([23](ch04.html#idm45143409740288)) Even digital OCR
    tools have been widely available in software packages and online for decades,
    so while they are far from perfect, the data contained in this type of file is
    also not entirely out of reach.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，最初设计时，在PDF中处理数据确实有些困难。然而，因为访问印刷文档中的数据是一个共同的问题，光学字符识别（OCR）的工作实际上早在19世纪末就已经开始。即使数字化OCR工具几十年来已经在软件包和在线上广泛可用，因此虽然它们远非完美，但这种文件类型中包含的数据也并非完全无法获取。
- en: When to work with text in PDFs
  id: totrans-279
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 处理PDF中的文本的时间
- en: In general, working with PDFs is a last resort (much, as we’ll see in [Chapter 5](ch05.html#chapter5),
    as web scraping should be). In general, if you can avoid relying on PDF information,
    you should. As noted previously, the process of extracting information from PDFs
    will generally yield an *approximation* of the document’s contents, so proofing
    for accuracy is a nonnegotiable part of any *.pdf*-based data wrangling workflow.
    That said, there is an enormous quantity of information that is *only* available
    as images or PDFs of scanned documents, and Python is an efficient way to extract
    a reasonably accurate first version of such document text.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 通常来说，处理PDF文件是最后的选择（正如我们将在[第五章](ch05.html#chapter5)中看到的那样，网页抓取也应如此）。一般来说，如果可以避免依赖PDF信息，那么就应该这样做。正如前面所述，从PDF中提取信息的过程通常会产生文档内容的*近似*值，因此准确性的校对是基于任何基于*.pdf*的数据处理工作流的不可推卸部分。话虽如此，有大量仅以图像或扫描文档PDF形式可用的信息，而Python是从此类文档中提取相对准确的第一版本的有效方法。
- en: Where to find PDFs
  id: totrans-281
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何找到PDF文件
- en: If you’re confident that the data you want can only be found in PDF format,
    then you can (and should) use the tips in [“Smart Searching for Specific Data
    Types”](#smart_searching) to locate this file type using an online search. More
    than likely, you will request information from a person or organization, and they
    will provide it as PDFs, leaving you to deal with the problem of how to extract
    the information you need. As a result, most of the time you will not need to go
    looking for PDFs—more often than not they will, unfortunately, find you.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您确信所需数据只能在PDF格式中找到，那么您可以（而且应该）使用[“智能搜索特定数据类型”](#smart_searching)中的提示，在线搜索中定位此文件类型。很可能，您会向个人或组织请求信息，他们将以PDF形式提供信息，让您自行处理如何提取所需信息的问题。因此，大多数情况下您不需要寻找PDF文件——很不幸，它们通常会找到您。
- en: Wrangling PDFs with Python
  id: totrans-283
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Python处理PDF文件
- en: Because PDFs can be generated both from machine-readable text (such as word-processing
    documents) and from scanned images, it is sometimes possible to extract the document’s
    “live” text programmatically with relatively few errors. While it seems straightforward,
    however, this method can still be unreliable because *.pdf* files can be generated
    with a wide range of encodings that can be difficult to detect accurately. So
    while this *can* be a high-accuracy method of extracting text from a *.pdf*, the
    likelihood of it working for any *given* file is low.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 因为PDF文件可以从可机器阅读的文本（如文字处理文档）和扫描图像生成，有时可以通过程序相对少的错误提取文档的“活”文本。然而，尽管这种方法看似简单，但由于*.pdf*文件可以采用各种难以准确检测的编码生成，因此该方法仍然可能不可靠。因此，虽然这可能是从*.pdf*中提取文本的高准确性方法，但对于任何给定文件而言，它的可行性较低。
- en: 'Because of this, I’m going to focus here on using OCR to recognize and extract
    the text in *.pdf* files. This will require two steps:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: Convert the document pages into individual images.
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run OCR on the page images, extract the text, and write it to individual text
    files.
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Unsurprisingly, we’ll need to install quite a few more Python libraries in
    order to make this all possible. First, we’ll install a couple of libraries for
    converting our *.pdf* pages to images. The first is a general-purpose library
    called `poppler` that is needed to make our Python-specific library `pdf2image`
    work. We’ll be using `pdf2image` to (you guessed it!) convert our *.pdf* file
    to a series of images:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Then:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Next, we need to install the tools for performing the OCR process. The first
    one is a general library called [*tesseract-ocr*](https://github.com/tesseract-ocr/tesseract),
    which uses machine learning to recognize the text in images; the second is a Python
    library that relies on *tesseract-ocr* called *pytesseract*:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Then:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Finally, we need a helper library for Python that can do the computer vision
    needed to bridge the gap between our page images and our OCR library:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Phew! If that seems like a lot of extra libraries, keep in mind that what we’re
    technically using here are is *machine learning*, one of those buzzy data science
    technologies that drives so much of the “artificial intelligence” out there. Fortunately
    for us, Tesseract in particular is relatively robust and inclusive: though it
    was originally developed by Hewlett-Packard as a proprietary system in the early
    1980s, it was open sourced in 2005 and currently supports more than 100 languages—so
    feel free to try the solution in [Example 4-16](#pdf_parsing) out on non-English
    text as well!'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: Example 4-16\. pdf_parsing.py
  id: totrans-299
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[![1](assets/1.png)](#co_working_with_file_based_and_feed_based_data_in_python_CO11-1)'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: Here we pass the path to our source file (in this case, that is just the filename,
    because it is in the same folder as our script) and the desired dots per inch
    (DPI) resolution of the output images to the `convert_from_path()` function. While
    setting a lower DPI will be much faster, the poorer-quality images may yield significantly
    less accurate OCR results. 300 DPI is a standard “print” quality resolution.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_working_with_file_based_and_feed_based_data_in_python_CO11-2)'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: Here we use the *os* library’s `.join` function to save the new files into our
    target folder. We also have to use the `str()` function to convert the page number
    into a string for use in the filename.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_working_with_file_based_and_feed_based_data_in_python_CO11-3)'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: Note that `*.png` can be translated to “any file ending in .png.” The `glob()`
    function creates a list of all the filenames in the folder where our images are
    stored (which in this case has the value `pdf_name`).
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_working_with_file_based_and_feed_based_data_in_python_CO11-4)'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: 'String manipulation is fiddly! To generate unique (but matching!) filenames
    for our OCR’d text files, we need to pull a unique page name out of `img_file`
    whose value starts with `SafetyNet/` and ends in `.png`. So we’ll replace the
    forward slash with a period to get something like `SafetyNet.p1.png`, and then
    if we `split()` *that* on the period, we’ll get a list like: `["SafetyNet", "p1",
    "png"]`. Finally, we can access the “page name” at position 1\. We need to do
    all this because we can’t be sure that `glob()` will, for example, pull `p1.png`
    from the image folder *first*, or that it will pull the images in order at all.'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: 'For the most part, running this script serves our purposes: with a few dozen
    lines of code, it converts a multipage PDF file first into images and then writes
    (most of) the contents to a series of new text files.'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: This all-in-one approach also has its limitations, however. Converting a PDF
    to images—or images to text—is the kind of task that we might need to do quite
    often but not always at the same time. In other words, it would probably be much
    more useful in the long run to have two *separate* scripts for solving this problem,
    and then to run them one after the other. In fact, with a little bit of tweaking,
    we could probably break up the preceding script in such a way that we could convert
    *any* PDF to images or *any* images to text without having to write *any* new
    code at all. Sounds pretty nifty, right?
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: 'This process of rethinking and reorganizing working code is known as *code
    refactoring*. In writing English, we would describe this as revising or editing,
    and the objective in both cases is the same: to make your work simpler, clearer,
    and more effective. And just like documentation, refactoring is actually another
    important way to scale your data wrangling work, because it makes *reusing* your
    code much more straightforward. We’ll look at various strategies for code refactoring
    and script reuse in [Chapter 8](ch08.html#chapter8).'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: Accessing PDF Tables with Tabula
  id: totrans-312
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If you looked at the text files produced in the preceding section, you’ll likely
    have noticed that there are a lot of “extras” in those files: page numbers and
    headers, line breaks, and other [“cruft.”](https://en.wikipedia.org/wiki/Cruft)
    There are also some key elements missing, like images and tables.'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: While our data work won’t extend to analyzing images (that is a much more specialized
    area), it’s not unusual to find tables inside PDFs that hold data we might want
    to work with. In fact, this problem is common enough in my home field of journalism
    that a group of investigative journalists designed and built a tool called [Tabula](https://tabula.technology)
    specifically to deal with this problem.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: 'Tabula isn’t a Python library—it’s actually a standalone piece of software.
    To try it out, [download the installer](https://tabula.technology/#download-install)
    for your system; if you’re on a Chromebook or Linux machine, you’ll need to download
    the *.zip* file and follow the directions in *README.txt*. Whatever system you’re
    using, you’ll probably need to install the Java programming library first, which
    you can do by running the following command in a terminal window:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Like some of the other open source tools we’ll discuss in later chapters (like
    OpenRefine, which I used to prepare some of the sample data in [Chapter 2](ch02.html#chapter2)
    and cover briefly in [Chapter 11](ch11.html#chapter11)), Tabula does its work
    behind the scenes (though some of it is visible in the terminal window), and you
    interact with it in a web browser. This is a way to get access to a more traditional
    graphical interface while still leaving most of your computer’s resources free
    to do the heavy-duty data work.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  id: totrans-318
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Hopefully, the coding examples in this chapter have started to give you an idea
    of the wide variety of data wrangling problems you can solve with relatively little
    Python code, thanks to a combination of carefully selected libraries and those
    few essential Python scripting concepts that were introduced in [Chapter 2](ch02.html#chapter2).
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: You may also have noticed that, with the exception of our PDF text, the output
    of *all* the exercises in this chapter was essentially a *.csv* file. This is
    not by accident. Not only are *.csv* files efficient and versatile, but it turns
    out that to do almost any basic statistical analyses or visualizations, we need
    table-type data to work with. That’s not to say that it isn’t *possible* to analyze
    nontable data; that, in fact, is what much of contemporary computer science research
    (like machine learning) is all about. However, because those systems are often
    both complex and opaque, they’re not really suitable for the type of data wrangling
    work we’re focused on here. As such, we’ll spend our energy on the types of analyses
    that can help us understand, explain, and communicate new insights about the world.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: Finally, while our work in this chapter focused on file-based data and pre-saved
    versions of feed-based data, in [Chapter 5](ch05.html#chapter5) we’ll explore
    how we can use Python with APIs and web scraping to wrangle data out of online
    systems and, where necessary, right off of web pages themselves!
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch04.html#idm45143426969488-marker)) Contrast these with some others you
    might know, like *mp4* or *png*, which are usually associated with music and images,
    respectively.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch04.html#idm45143426940336-marker)) Though you’ll know how to wrangle
    it shortly!
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](ch04.html#idm45143423524592-marker)) Actually, you won’t need that much
    luck—we’ll look at how to do this in [“Wrangling PDFs with Python”](#wrangling_pdfs).
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: '^([4](ch04.html#idm45143423518064-marker)) In computer science, the terms *data*
    and *information* are applied in exactly the opposite way: *data* is the raw facts
    collected about the world, and *information* is the meaningful end product of
    organizing and structuring it. In recent years, however, as talk about “big data”
    has dominated many fields, the interpretation I use here has become more common,
    so I’ll stick with it throughout this book for clarity.'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: ^([5](ch04.html#idm45143422908112-marker)) From the [US Bureau of Labor Statistics](https://bls.gov/news.release/empsit.t15.htm).
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: ^([6](ch04.html#idm45143422903792-marker)) “Multiple Jobholders” by Stéphane
    Auray, David L. Fuller, and Guillaume Vandenbroucke, posted December 21, 2018,
    [*https://research.stlouisfed.org/publications/economic-synopses/2018/12/21/multiple-jobholders*](https://research.stlouisfed.org/publications/economic-synopses/2018/12/21/multiple-jobholders).
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: '^([7](ch04.html#idm45143422901552-marker)) See “New Recommendations on Improving
    Data on Contingent and Alternative Work Arrangements,” [*https://blogs.bls.gov/blog/tag/contingent-workers*](https://blogs.bls.gov/blog/tag/contingent-workers);
    “The Value of Flexible Work: Evidence from Uber Drivers” by M. Keith Chen et al.,
    *Nber Working Paper Series* No. 23296, [*https://nber.org/system/files/working_papers/w23296/w23296.pdf*](https://nber.org/system/files/working_papers/w23296/w23296.pdf).'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: ^([8](ch04.html#idm45143422892896-marker)) You can also find instructions for
    this on the [FRED website](https://fredhelp.stlouisfed.org/fred/graphs/customize-a-fred-graph/data-transformation-add-series-to-existing-line).
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: ^([9](ch04.html#idm45143422886480-marker)) You can also download copies of these
    files [directly from my Google Drive](https://drive.google.com/drive/u/0/folders/1cU5Tdg_fvrCcwvAAyhMOhpbEcI2fF7sb).
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: ^([10](ch04.html#idm45143422842640-marker)) As of this writing, LibreOffice
    can handle the same number of rows as Microsoft Excel (2^(20)), but far fewer
    columns. While Google Sheets can handle *more* columns than Excel, it can only
    handle about 40,000 rows.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: ^([11](ch04.html#idm45143422837216-marker)) As of this writing, all of these
    libraries are already available and ready to use in Google Colab.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: ^([12](ch04.html#idm45143422318528-marker)) For more about `get_rows()`, see
    the [*xlrd* documentation](https://xlrd.readthedocs.io/en/latest/api.html#xlrd-sheet).
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: ^([13](ch04.html#idm45143422098736-marker)) See the [*xlrd* documentation](https://xlrd.readthedocs.io/en/latest/dates.html)
    for more on this issue.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: ^([14](ch04.html#idm45143422094400-marker)) From Stephen John Machin and Chris
    Withers, [“Dates in Excel Spreadsheets”](https://xlrd.readthedocs.io/en/latest/dates.html).
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: ^([15](ch04.html#idm45143422084224-marker)) If you open the output files from
    the three preceding code examples in a text editor, you’ll notice that the open
    source *.ods* format is the simplest and cleanest.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: ^([16](ch04.html#idm45143422210096-marker)) As in, for example, [Pennsylvania](https://spotlightpa.org/news/2021/05/pa-unemployment-claims-overhaul-ibm-gsi-benefits-labor-industry)
    or [Colorado](https://denverpost.com/2021/01/10/colorado-unemployment-benefits-new-claims-system).
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: ^([17](ch04.html#idm45143421937168-marker)) See Vassili van der Mersch’s post,
    [“Twitter’s 10 Year Struggle with Developer Relations” from Nordic APIs](https://nordicapis.com/twitter-10-year-struggle-with-developer-relations).
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: ^([18](ch04.html#idm45143413262208-marker)) Unlike Python code, XML documents
    do *not* have to be properly indented in order to work, though it certainly makes
    them more readable!
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: '^([19](ch04.html#idm45143410254992-marker)) Fun fact: the second *x* in the
    *.xlsx* format actually refers to XML!'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: ^([20](ch04.html#idm45143410249984-marker)) Again, we’ll walk through using
    APIs like this one step by step in [Chapter 5](ch05.html#chapter5), but using
    this document lets us see how different data formats influence our interactions
    with the data.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: ^([21](ch04.html#idm45143409767664-marker)) If a stylesheet has been applied,
    as in the case of the BBC feed we used in [Example 4-13](#bbc_example), you can
    context+click the page and select “View Source” to see the “raw” XML.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: ^([22](ch04.html#idm45143409744752-marker)) See [Adobe’s About PDF page for
    more information](https://acrobat.adobe.com/us/en/acrobat/about-adobe-pdf.html).
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: '^([23](ch04.html#idm45143409740288-marker)) George Nagy, “Disruptive Developments
    in Document Recognition,” *Pattern Recognition Letters* 79 (2016): 106–112, [*https://doi.org/10.1016/j.patrec.2015.11.024*](https://doi.org/10.1016/j.patrec.2015.11.024).
    Available at [*https://ecse.rpi.edu/~nagy/PDF_chrono/2016_PRL_Disruptive_asPublished.pdf*](https://ecse.rpi.edu/~nagy/PDF_chrono/2016_PRL_Disruptive_asPublished.pdf).'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
