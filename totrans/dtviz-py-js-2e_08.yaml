- en: Chapter 5\. Getting Data Off the Web with Python
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第5章。使用Python从网络获取数据
- en: A fundamental part of the data visualizer’s skill set is getting the right dataset
    in as clean a form as possible. Sometimes you will be given a nice, clean dataset
    to analyze but often you will be tasked with either finding the data and/or cleaning
    the data supplied.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 数据可视化工具箱中的一个基本部分是以尽可能干净的形式获取正确的数据集。有时你会被提供一个漂亮干净的数据集来分析，但通常你需要找到数据并/或清理提供的数据。
- en: And more often than not these days, getting data involves getting it off the
    web. There are various ways you can do this, and Python provides some great libraries
    that make sucking up the data easy.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 而如今更多的时候，获取数据意味着从网络上获取数据。你可以通过各种方式实现这一点，Python提供了一些很棒的库来简化数据的获取过程。
- en: 'The main ways to get data off the web are:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 从网络获取数据的主要方法包括：
- en: Get a raw data file in a recognized data format (e.g., JSON or CSV) over HTTP.
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从HTTP获取一个识别的数据格式的原始数据文件（例如JSON或CSV）。
- en: Use a dedicated API to get the data.
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用专用API获取数据。
- en: Scrape the data by getting web pages via HTTP and parsing them locally for the
    required data.
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过HTTP获取网页并在本地解析以获取所需数据。
- en: 'This chapter will deal with these ways in turn, but first let’s get acquainted
    with the best Python HTTP library out there: Requests.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将依次处理这些方法，但首先让我们熟悉一下目前最好的Python HTTP库：Requests。
- en: Getting Web Data with the Requests Library
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Requests库获取网络数据
- en: As we saw in [Chapter 4](ch04.xhtml#chapter_webdev101), the files that are used
    by web browsers to construct web pages are communicated via the Hypertext Transfer
    Protocol (HTTP), first developed by [Tim Berners-Lee](https://oreil.ly/uKF5f).
    Getting web content in order to parse it for data involves making HTTP requests.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[第4章](ch04.xhtml#chapter_webdev101)中看到的，用于构建网页的文件通过超文本传输协议（HTTP）进行通信，最初由[Tim
    Berners-Lee](https://oreil.ly/uKF5f)开发。获取网页内容以解析数据涉及发出HTTP请求。
- en: Negotiating HTTP requests is a vital part of any general-purpose language, but
    getting web pages with Python used to be a rather irksome affair. The venerable
    urllib2 library was hardly user-friendly, with a very clunky API. [*Requests*](https://oreil.ly/6VkKZ),
    courtesy of Kenneth Reitz, changed that, making HTTP a relative breeze and fast
    establishing itself as the go-to Python HTTP library.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 处理HTTP请求是任何通用语言的重要部分，但是以前在Python中获取网页是一件相当讨厌的事情。古老的urllib2库几乎没有用户友好的API，其使用非常笨拙。[*Requests*](https://oreil.ly/6VkKZ)，由Kenneth
    Reitz提供，改变了这一状况，使得HTTP请求变得相对轻松，并迅速确立了作为首选Python HTTP库的地位。
- en: 'Requests is not part of the Python standard library^([1](ch05.xhtml#idm45607789542496))
    but is part of the [Anaconda package](https://oreil.ly/LD0ee) (see [Chapter 1](ch01.xhtml#chapter_install)).
    If you’re not using Anaconda, the following `pip` command should do the job:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: Requests不是Python标准库的一部分^([1](ch05.xhtml#idm45607789542496))，但是是[Anaconda包](https://oreil.ly/LD0ee)的一部分（参见[第1章](ch01.xhtml#chapter_install)）。如果你没有使用Anaconda，以下`pip`命令应该可以胜任：
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: If you’re using a Python version prior to 2.7.9 (I strongly recommend using
    Python 3+ wherever possible), then using Requests may generate some [Secure Sockets
    Layer (SSL) warnings](https://oreil.ly/8D08s). Upgrading to newer SSL libraries
    should fix this:^([2](ch05.xhtml#idm45607789528976))
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用的是Python 2.7.9之前的版本（我强烈建议尽可能使用Python 3+），使用Requests可能会生成一些[安全套接字层（SSL）警告](https://oreil.ly/8D08s)。升级到更新的SSL库应该可以解决这个问题：^([2](ch05.xhtml#idm45607789528976))
- en: '[PRE1]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Now that you have Requests installed, you’re ready to perform the first task
    mentioned at the beginning of this chapter and grab some raw data files off the
    web.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经安装了Requests，可以执行本章开头提到的第一个任务，从网络上获取一些原始数据文件。
- en: Getting Data Files with Requests
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Requests获取数据文件
- en: 'A Python interpreter session is a good way to put Requests through its paces,
    so find a friendly local command line, fire up IPython, and import `requests`:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Python解释器会话是测试Requests的一个好方法，所以找一个友好的本地命令行，启动IPython，并导入`requests`：
- en: '[PRE2]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'To demonstrate, let’s use the library to download a Wikipedia page. We use
    the Requests library’s `get` method to get the page and, by convention, assign
    the result to a `response` object:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示，让我们使用该库下载一个维基百科页面。我们使用Requests库的`get`方法获取页面，并按照惯例将结果分配给一个`response`对象：
- en: '[PRE3]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Let’s use Python’s [`dir`](https://oreil.ly/CrJ8h) method to get a list of
    the `response` object’s attributes:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用Python的[`dir`](https://oreil.ly/CrJ8h)方法来获取`response`对象的属性列表：
- en: '[PRE4]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Most of these attributes are self-explanatory and together provide a lot of
    information about the HTTP response generated. You’ll use a small subset of these
    attributes generally. Firstly, let’s check the status of the response:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数这些属性是不言自明的，一起提供了大量关于生成的HTTP响应的信息。通常情况下，你将只使用这些属性的一个小子集。首先，让我们检查响应的状态：
- en: '[PRE6]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'As all good minimal web developers know, 200 is the [HTTP status code](https://oreil.ly/ucEoo)
    for OK, indicating a successful transaction. Other than 200, the most common codes
    are:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 所有优秀的最小化网络开发者都知道，200是[HTTP状态码](https://oreil.ly/ucEoo)表示OK，指示成功的事务。除了200之外，最常见的代码是：
- en: 401 (Unauthorized)
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 401（未经授权）
- en: Attempting unauthorized access
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试未经授权的访问
- en: 400 (Bad Request)
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 400（错误请求）
- en: Trying to access the web server incorrectly
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试错误地访问Web服务器
- en: 403 (Forbidden)
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 403（禁止）
- en: Similar to 401 but no login opportunity was available
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于401，但没有登录机会可用
- en: 404 (Not Found)
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 404（未找到）
- en: Trying to access a web page that doesn’t exist
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试访问一个不存在的网页
- en: 500 (Internal Server Error)
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 500（内部服务器错误）
- en: A general-purpose, catchall error
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 一个通用的、万能的错误
- en: 'So, for example, if we made a spelling mistake with our request, asking to
    see the `SNoble_Prize` page, we’d get a 404 (Not Found) error:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，例如，如果我们在请求中拼写错误，请求查看`SNoble_Prize`页面，我们将收到404（未找到）错误：
- en: '[PRE7]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'With our 200 OK response, from the correctly spelled request, let’s look at
    some of the info returned. A quick overview can be had with the `headers` property:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 通过我们的200 OK响应，从正确拼写的请求中，让我们看一下返回的一些信息。可以通过`headers`属性快速查看概述：
- en: '[PRE8]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: This shows, among other things, that the page returned was gzip-encoded and
    87 KB in size with `content-type` of `text/html`, encoded with Unicode UTF-8.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这显示，除其他外，返回的页面是经过gzip编码的，大小为87 KB，`content-type`为`text/html`，使用Unicode UTF-8编码。
- en: 'Since we know text has been returned, we can use the `text` property of the
    response to see what it is:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们知道已经返回了文本，我们可以使用响应的`text`属性来查看它是什么：
- en: '[PRE10]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: This shows that we do indeed have our Wikipedia HTML page, with some inline
    JavaScript. As we’ll see in [“Scraping Data”](#get_data_scraping), in order to
    make sense of this content, we’ll need a parser to read the HTML and provide the
    content blocks.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明我们确实有我们的维基百科HTML页面，带有一些内联JavaScript。正如我们将在[“抓取数据”](#get_data_scraping)中看到的那样，为了理解这个内容，我们需要一个解析器来读取HTML并提供内容块。
- en: Now that we’ve grabbed a raw page off the web, let’s see how to use Requests
    to consume a web data API.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经从网络上抓取了一个原始页面，让我们看看如何使用Requests来消耗网络数据API。
- en: Using Python to Consume Data from a Web API
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Python从Web API中获取数据
- en: If the data file you need isn’t on the web, there may well be an Application
    Programming Interface (API) serving the data you need. Using this will involve
    making a request to the appropriate server to retrieve your data in a fixed format
    or one you get to specify in the request.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你需要的数据文件不在网络上，很可能有一个应用程序编程接口（API）提供你所需的数据。使用这个API将涉及向适当的服务器发出请求，以获取你在请求中指定的固定格式或你指定的格式中的数据。
- en: The most popular data formats for web APIs are JSON and XML, though a number
    of esoteric formats exist. For the purposes of the JavaScripting data visualizer,
    JavaScript Object Notation (JSON) is obviously preferred (see [“Data”](ch04.xhtml#sect_data)).
    Lucky for us, it is also starting to predominate.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: Web API的最流行的数据格式是JSON和XML，尽管存在许多奇特的格式。对于JavaScript数据可视化器的目的，JavaScript对象表示法（JSON）显然是首选的（见[“数据”](ch04.xhtml#sect_data)）。幸运的是，它也开始占主导地位。
- en: 'There are different approaches to creating a web API, and for a few years there
    was a little war of the architectures among the three main types of APIs inhabiting
    the web:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 创建Web API有不同的方法，并且在几年内有三种主要类型的API在网络中存在一场小规模的架构之争：
- en: '[REST](https://oreil.ly/ujgdJ)'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '[REST](https://oreil.ly/ujgdJ)'
- en: Short for REpresentational State Transfer, using a combination of HTTP verbs
    (GET, POST, etc.) and Uniform Resource Identifiers (URIs; e.g., */user/kyran*)
    to access, create, and adapt data.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 表示表现状态转移，使用HTTP动词（GET，POST等）和统一资源标识符（URI；例如，*/user/kyran*）来访问，创建和调整数据的组合。
- en: '[XML-RPC](https://oreil.ly/ZMQvW)'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '[XML-RPC](https://oreil.ly/ZMQvW)'
- en: A remote procedure call (RPC) protocol using XML encoding and HTTP transport.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 使用XML编码和HTTP传输的远程过程调用（RPC）协议。
- en: '[SOAP](https://oreil.ly/l5LVL)'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '[SOAP](https://oreil.ly/l5LVL)'
- en: Short for Simple Object Access Protocol, using XML and HTTP.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 表示简单对象访问协议，使用XML和HTTP。
- en: This battle seems to be resolving in a victory for [RESTful APIs](https://oreil.ly/apc1l),
    and this is a very good thing. Quite apart from RESTful APIs being more elegant,
    and easier to use and implement (see [Chapter 13](ch13.xhtml#chapter_delivery_restful)),
    some standardization here makes it much more likely that you will recognize and
    quickly adapt to a new API that comes your way. Ideally, you will be able to reuse
    existing code. There is a new player on the scene in the form of [GraphQL](https://oreil.ly/JUGVS),
    which bills itself as a better REST, but as a datavizzer you’re far more likely
    to be consuming conventional RESTful APIs.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这场战斗似乎正在向[RESTful APIs](https://oreil.ly/apc1l)的胜利方向解决，这是一件非常好的事情。除了RESTful
    API更加优雅、更易于使用和实现（参见[第13章](ch13.xhtml#chapter_delivery_restful)），在这里进行一些标准化使得您更有可能认识并快速适应新的API。理想情况下，您将能够重用现有代码。作为一个数据可视化者，有一个新的参与者出现，名为[GraphQL](https://oreil.ly/JUGVS)，自称为更好的REST，但您更有可能使用传统的RESTful
    API。
- en: Most access and manipulation of remote data can be summed up by the acronym
    CRUD (create, retrieve, update, delete), originally coined to describe all the
    major functions implemented in relational databases. HTTP provides CRUD counterparts
    with the POST, GET, PUT, and DELETE verbs and the REST abstraction builds on this
    use of these verbs, acting on a [Universal Resource Identifier (URI)](https://oreil.ly/xmX1k).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数远程数据的访问和操作可以通过CRUD（创建、检索、更新、删除）这个缩写来概括，最初用来描述在关系数据库中实现的所有主要功能。HTTP提供了CRUD的对应操作，包括POST、GET、PUT和DELETE动词，而REST抽象则基于这些动词的使用，作用于[统一资源标识符（URI）](https://oreil.ly/xmX1k)。
- en: Discussions about what is and isn’t a proper RESTful interface can get quite
    involved, but essentially the URI (e.g., *https://example.com/api/items/2*) should
    contain all the information required in order to perform a CRUD operation. The
    particular operation (e.g., GET or DELETE) is specified by the HTTP verb. This
    excludes architectures such as SOAP, which place stateful information in metadata
    on the requests header. Imagine the URI as the virtual address of the data and
    CRUD as all the operations you can perform on it.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 讨论什么是一个适当的RESTful接口，可以变得非常复杂，但基本上，URI（例如，*https://example.com/api/items/2*）应包含执行CRUD操作所需的所有信息。具体操作（例如GET或DELETE）由HTTP动词指定。这不包括像SOAP这样的体系结构，它将有状态信息放在请求头的元数据中。将URI想象成数据的虚拟地址，CRUD是您可以对其执行的所有操作。
- en: As data visualizers keen to lay our hands on some interesting datasets, we are
    avid consumers here, so our HTTP verb of choice is GET, and the examples that
    follow will focus on the fetching of data with various well-known web APIs. Hopefully,
    some patterns will emerge.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 作为渴望获得一些有趣数据集的数据可视化者，我们在这里是狂热的消费者，因此我们选择的HTTP动词是GET，接下来的示例将专注于使用各种知名的Web API获取数据。希望能找出一些模式。
- en: Although the two constraints of stateless URIs and the use of the CRUD verbs
    is a nice constraint on the shape of RESTful APIs, there still manage to be many
    variants on the theme.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管无状态URI和使用CRUD动词的两个约束是RESTful API形状上的一个不错的约束条件，但这里仍然有很多主题的变体。
- en: Consuming a RESTful Web API with Requests
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Requests消费RESTful Web API
- en: Requests has a fair number of bells and whistles based around the main HTTP
    request verbs. For a good overview, see [the Requests quickstart](https://oreil.ly/Bp8VG).
    For the purposes of getting data, you’ll use GET and POST pretty much exclusively,
    with GET being by a long way the most used verb. POST allows you to emulate web
    forms, including login details, field values, etc. in the request. For those occasions
    where you find yourself driving a web form with, for example, lots of options
    selectors, Requests makes automation with POST easy. GET covers pretty much everything
    else, including the ubiquitous RESTful APIs, which provide an increasing amount
    of the well-formed data available on the web.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: Requests围绕主要的HTTP请求动词有相当多的功能和特性。详细内容请参阅[Requests快速入门](https://oreil.ly/Bp8VG)。为了获取数据，您几乎完全会使用GET和POST动词，其中GET是远远最常用的动词。POST允许您模拟Web表单，包括登录详细信息、字段值等在请求中。对于那些需要使用POST轻松驱动Web表单的情况，例如具有大量选项选择器的情况，Requests使得自动化变得容易。GET几乎涵盖了所有其他内容，包括无处不在的RESTful
    API，这些API提供了越来越多的网上可用的形式良好的数据。
- en: Let’s look at a more complicated use of Requests, getting a URL with arguments.
    The [Organisation for Economic Cooperation and Development (OECD)](https://oreil.ly/QAj3A)
    provides some [useful datasets on its site](https://data.oecd.org). These datasets
    provide mainly economic measures and statistics for the member countries of the
    OECD, and such data can form the basis of many interesting visualizations. The
    OECD provides a few of its own, such as one [allowing you to compare your country](https://oreil.ly/aFmUv)
    with others in the OECD.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个更复杂的请求使用Requests，获取带有参数的URL。[经济合作与发展组织（OECD）](https://oreil.ly/QAj3A)提供了一些[有用的数据集](https://data.oecd.org)。这些数据集主要提供OECD成员国的经济措施和统计数据，这些数据可以成为许多有趣可视化的基础。OECD提供了一些自己的可视化工具，如允许您与OECD中的其他国家进行比较的工具。
- en: 'The OECD web API is described [in this documentation](https://oreil.ly/f5VDc),
    and queries are constructed with the dataset name (dsname) and some dot-separated
    dimensions, each of which can be a number of `+` separated values. The URL can
    also take standard HTTP parameters initiated by a `?` and separated by `&`:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: OECD网络API在[这个文档](https://oreil.ly/f5VDc)中有详细描述，查询由数据集名称（dsname）和一些用点分隔的维度构成，每个维度可以是多个用`+`分隔的值。URL也可以采用标准的HTTP参数，以`?`开头，用`&`分隔：
- en: '[PRE11]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'So the following is a valid URL:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是有效的URL：
- en: '[PRE12]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[![1](assets/1.png)](#co_getting_data_off_the_web__span_class__keep_together__with_python__span__CO1-1)'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_getting_data_off_the_web__span_class__keep_together__with_python__span__CO1-1)'
- en: Specifies the QNA (Quarterly National Accounts) dataset.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 指定季度国民账户（QNA）数据集。
- en: '[![2](assets/2.png)](#co_getting_data_off_the_web__span_class__keep_together__with_python__span__CO1-2)'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_getting_data_off_the_web__span_class__keep_together__with_python__span__CO1-2)'
- en: Four dimensions, by location, subject, measure, and frequency.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 四个维度，按地点、主题、措施和频率。
- en: '[![3](assets/3.png)](#co_getting_data_off_the_web__span_class__keep_together__with_python__span__CO1-3)'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_getting_data_off_the_web__span_class__keep_together__with_python__span__CO1-3)'
- en: Data from the second quarter of 2009 to the fourth quarter of 2011.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 数据从2009年第二季度到2011年第四季度。
- en: Let’s construct a little Python function to query the OECD’s API ([Example 5-1](#oecd_request)).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们构建一个小的Python函数来查询OECD的API（[示例 5-1](#oecd_request)）。
- en: Example 5-1\. Making a URL for the OECD API
  id: totrans-76
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 5-1\. 构建OECD API的URL
- en: '[PRE13]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[![1](assets/1.png)](#co_getting_data_off_the_web__span_class__keep_together__with_python__span__CO2-1)'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_getting_data_off_the_web__span_class__keep_together__with_python__span__CO2-1)'
- en: You shouldn’t use mutable values, such as `{}`, for Python function defaults.
    See [this Python guide](https://oreil.ly/Yv6bX) for an explanation of this gotcha.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Python函数默认值，不应使用可变值，如`{}`。参见[这个Python指南](https://oreil.ly/Yv6bX)以了解此问题的解释。
- en: '[![2](assets/2.png)](#co_getting_data_off_the_web__span_class__keep_together__with_python__span__CO2-2)'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_getting_data_off_the_web__span_class__keep_together__with_python__span__CO2-2)'
- en: We first use a Python list comprehension and the `join` method to create a list
    of dimensions, with members concatenated with plus signs (e.g., [*USA+AUS*, …​
    ]). `join` is then used again to concatenate the members of `dim_str` with periods.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先使用Python的列表推导和`join`方法创建一个维度列表，成员用加号连接（例如，[*USA+AUS*, …​ ]）。然后再次使用`join`将`dim_str`的成员用句点连接。
- en: '[![3](assets/3.png)](#co_getting_data_off_the_web__span_class__keep_together__with_python__span__CO2-3)'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_getting_data_off_the_web__span_class__keep_together__with_python__span__CO2-3)'
- en: Note that `requests`’ `get` can take a parameter dictionary as its second argument,
    using it to make the URL query string.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 注意`requests`的`get`可以将参数字典作为其第二个参数，用于构建URL查询字符串。
- en: 'We can use this function like so, to grab economic data for the USA and Australia
    from 2009 to 2010:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以像这样使用这个函数，从2009年到2010年获取美国和澳大利亚的经济数据：
- en: '[PRE14]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Now, to look at the data, we just check that the response is OK and have a
    look at the dictionary keys:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，查看数据，只需检查响应是否正常，并查看字典键：
- en: '[PRE16]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The resulting JSON data is in the [SDMX format](https://oreil.ly/HeE7G), designed
    to facilitate the communication of statistical data. It’s not the most intuitive
    format around, but it’s often the case that datasets have a less than ideal structure.
    The good news is that Python is a great language for knocking data into shape.
    For Python’s [pandas library](https://pandas.pydata.org) (see [Chapter 8](ch08.xhtml#chapter_intro_to_pandas)),
    there is [pandaSDMX](https://oreil.ly/2PKxZ), which currently handles the XML-based
    format.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 结果JSON数据以[SDMX格式](https://oreil.ly/HeE7G)提供，旨在促进统计数据的通信。这不是最直观的格式，但往往是数据集结构不理想的情况。好消息是，Python非常适合整理数据。对于Python的[pandas库](https://pandas.pydata.org)（参见[第8章](ch08.xhtml#chapter_intro_to_pandas)），有[pandaSDMX](https://oreil.ly/2PKxZ)，目前可以处理基于XML的格式。
- en: The OECD API is essentially RESTful with all of the query being contained in
    the URL and the HTTP verb GET specifying a fetch operation. If a specialized Python
    library isn’t available to use the API (e.g., Tweepy for Twitter), then you’ll
    probably end up writing something like [Example 5-1](#oecd_request). Requests
    is a very friendly, well-designed library and can cope with pretty much all the
    manipulations required to use a web API.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: OECD API基本上是RESTful的，所有查询都包含在URL中，HTTP动词GET指定了获取操作。如果没有专门的Python库可用于使用该API（例如Twitter的Tweepy），那么您可能最终会编写类似于[示例5-1](#oecd_request)的东西。Requests是一个非常友好且设计良好的库，几乎可以处理使用Web
    API所需的所有操作。
- en: Getting Country Data for the Nobel Dataviz
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 获取诺贝尔数据可视化的国家数据
- en: There are some national statistics that will come in handy for the Nobel Prize
    visualization we’re using our toolchain to build. Population sizes, three-letter
    international codes (e.g., GDR, USA), and geographic centers are potentially useful
    when you are visualizing an international prize and its distribution. [REST countries](https://restcountries.com)
    is a handy RESTful web resource with various international stats. Let’s use it
    to grab some data.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们正在使用的工具链中，有一些国家统计数据将对我们构建的诺贝尔奖可视化非常有用。人口规模、三字母国际代码（例如GDR、USA）和地理中心在可视化国际奖项及其分布时可能非常有用。[REST
    countries](https://restcountries.com)是一个非常方便的RESTful网络资源，提供各种国际统计数据。让我们用它来获取一些数据。
- en: 'Requests to REST countries take the following form:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 对REST国家的请求采用以下形式：
- en: '[PRE17]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'As with the OECD API (see [Example 5-1](#oecd_request)), we can make a simple
    calling function to allow easy access to the API’s data, like so:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 就像对OECD API（参见[示例5-1](#oecd_request)）一样，我们可以创建一个简单的调用函数，以便轻松访问API的数据，如下所示：
- en: '[PRE18]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[![1](assets/1.png)](#co_getting_data_off_the_web__span_class__keep_together__with_python__span__CO3-1)'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_getting_data_off_the_web__span_class__keep_together__with_python__span__CO3-1)'
- en: It’s usually a good idea to specify a valid `User-Agent` in the header of your
    request. Some sites will reject the request otherwise.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 通常最好在请求头中指定一个有效的`User-Agent`。否则，一些网站可能会拒绝请求。
- en: '[![2](assets/2.png)](#co_getting_data_off_the_web__span_class__keep_together__with_python__span__CO3-2)'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_getting_data_off_the_web__span_class__keep_together__with_python__span__CO3-2)'
- en: Before returning the response, make sure it has an OK (200) HTTP code; otherwise,
    raise an exception with a helpful message.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在返回响应之前，请确保它具有OK（200）HTTP代码；否则，请提供一个带有有用信息的异常。
- en: 'With the `REST_country_request` function in hand, let’s get a list of all the
    countries using the US dollar as currency:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 现在有了`REST_country_request`函数，让我们获取所有使用美元作为货币的国家列表：
- en: '[PRE19]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The full dataset at REST countries is pretty small, so for convenience we’ll
    store a copy as a JSON file. We’ll be using this in later chapters in both exploratory
    and presentational dataviz:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在REST国家的完整数据集相当小，因此为了方便起见，我们将其存储为JSON文件。我们将在后续章节中使用它，进行探索性和展示性的数据可视化：
- en: '[PRE20]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Now that we’ve rolled a couple of our own API consumers, let’s take a look at
    some dedicated libraries that wrap some of the larger web APIs in an easy-to-use
    form.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经编写了几个自己的API消费者，让我们来看看一些专门的库，它们包装了一些更大的Web API，以便于使用。
- en: Using Libraries to Access Web APIs
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用库访问Web API
- en: Requests is capable of negotiating with pretty much all web APIs, but as the
    APIs start adding authentication and the data structures become more complicated,
    a good wrapper library can save a lot of hassle and reduce the tedious bookkeeping.
    In this section, I’ll cover a couple of the more popular [wrapper libraries](https://oreil.ly/DBrZ8)
    to give you a feel for the workflow and some useful starting points.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: Requests能够处理几乎所有的Web API，但随着API开始添加认证和数据结构变得更复杂，一个好的包装库可以节省大量麻烦，减少繁琐的账务工作。在本节中，我将介绍一些较受欢迎的[包装库](https://oreil.ly/DBrZ8)，帮助你了解工作流程和一些有用的起始点。
- en: Using Google Spreadsheets
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Google电子表格
- en: It’s becoming more common these days to have live datasets *in the cloud*. So,
    for example, you might find yourself required to visualize aspects of a Google
    spreadsheet that is the shared data pool for a group. My preference is to get
    this data out of the Google-plex and into pandas to start exploring it (see [Chapter 11](ch11.xhtml#chapter_pandas_exploring)),
    but a good library will let you access and adapt the data *in place*, negotiating
    the web traffic as required.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 如今，拥有存储在*云端*的实时数据集变得越来越普遍。例如，你可能需要可视化作为一个群组共享数据池的Google电子表格的各个方面。我更喜欢将这些数据从Googleplex导入pandas来开始探索（见[第11章](ch11.xhtml#chapter_pandas_exploring)），但一个好的库可以让你在需要时直接访问和调整数据*原地*，协商网络流量。
- en: '[*gspread*](https://oreil.ly/DNKYT) is the best known Python library for accessing
    Google spreadsheets and makes doing so a relative breeze.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '[*gspread*](https://oreil.ly/DNKYT) 是访问Google电子表格最知名的Python库，使用起来相对轻松。'
- en: You’ll need [OAuth 2.0](https://oreil.ly/z3u6y) credentials to use the API.^([3](ch05.xhtml#idm45607788534000))
    The most up-to-date guide can be found on the [Google Developers site](https://oreil.ly/tnO3b).
    Following those instructions should provide a JSON file containing your private
    key.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要[OAuth 2.0](https://oreil.ly/z3u6y)凭证来使用API。^([3](ch05.xhtml#idm45607788534000))
    可以在[Google Developers网站](https://oreil.ly/tnO3b)上找到最新的指南，按照这些说明应该可以获取包含你私钥的JSON文件。
- en: 'You’ll need to install *gspread* and the latest *google-auth* client library.
    Here’s how to do it with `pip`:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要安装*gspread*和最新的*google-auth*客户端库。以下是使用`pip`安装的方法：
- en: '[PRE21]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Depending on your system, you may also need pyOpenSSL:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 根据你的系统，你可能还需要pyOpenSSL：
- en: '[PRE22]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Read [the docs](https://oreil.ly/1xAPm) for more details and troubleshooting.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读[文档](https://oreil.ly/1xAPm)获取更多详情和故障排除。
- en: Note
  id: totrans-117
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Google’s API assumes that the spreadsheets you are trying to access are owned
    or shared by your API account, not your personal one. The email address to share
    the spreadsheet with is available at your [Google developers console](https://oreil.ly/z5KyM)
    and in the JSON credentials key needed to use the API. It should look something
    like *account-1@My Project…​iam.gserviceaccount.com*.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: Google的API假定你尝试访问的电子表格是由你的API账户拥有或共享的，而不是你的个人账户。分享电子表格的电子邮件地址可以在你的[Google开发者控制台](https://oreil.ly/z5KyM)以及在使用API所需的JSON凭证键中找到，它应该看起来像*account-1@My
    Project…​iam.gserviceaccount.com*。
- en: With those libraries installed, you should be able to access any of your spreadsheets
    with just a few lines. I’m using the [Microbe-scope spreadsheet](https://oreil.ly/AAj9X).
    [Example 5-2](#gspread_access) shows how to load the spreadsheet.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 安装了这些库之后，你应该能够只用几行代码访问任何你的电子表格。我正在使用[Microbe-scope spreadsheet](https://oreil.ly/AAj9X)。示例5-2展示了如何加载电子表格。
- en: Example 5-2\. Opening a Google spreadsheet
  id: totrans-120
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例5-2\. 打开一个Google电子表格
- en: '[PRE23]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[![1](assets/1.png)](#co_getting_data_off_the_web__span_class__keep_together__with_python__span__CO4-1)'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_getting_data_off_the_web__span_class__keep_together__with_python__span__CO4-1)'
- en: The JSON credentials file is the one provided by Google services, usually of
    the form *My Project-b8ab5e38fd68.json*.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: JSON凭证文件是由Google服务提供的，通常格式为*My Project-b8ab5e38fd68.json*。
- en: '[![2](assets/2.png)](#co_getting_data_off_the_web__span_class__keep_together__with_python__span__CO4-2)'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_getting_data_off_the_web__span_class__keep_together__with_python__span__CO4-2)'
- en: Here we’re opening the spreadsheet by name. Alternatives are `open_by_url` or
    `open_by_id`. See [the `gspread` documentation](https://oreil.ly/sa4sa) for details.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们通过名称打开电子表格。也可以使用`open_by_url`或`open_by_id`。详细信息请参阅[`gspread`文档](https://oreil.ly/sa4sa)。
- en: 'Now that we’ve got our spreadsheet, we can see the worksheets it contains:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了我们的电子表格，可以看到它包含的工作表：
- en: '[PRE24]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'With the worksheet `bugs` selected from the spreadsheet, `gspread` allows you
    to access and change column, row, and cell values (assuming the sheet isn’t read-only).
    So we can get the values in the second column with the `col_values` command:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 选择电子表格中的`bugs`工作表后，`gspread`允许您访问和更改列、行和单元格的值（假设表格不是只读的）。因此，我们可以使用`col_values`命令获取第二列中的值：
- en: '[PRE25]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Tip
  id: totrans-130
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: If you get a `BadStatusLine` error while accessing a Google spreadsheet with
    `gspread`, it is probably because the session has expired. Reopening the spreadsheet
    should get things working again. This [outstanding `gspread` issue](https://oreil.ly/xTGg9)
    provides more information.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在使用`gspread`访问Google电子表格时出现`BadStatusLine`错误，则可能是会话已过期。重新打开电子表格应该可以使事情重新运行。这个[未解决的`gspread`问题](https://oreil.ly/xTGg9)提供了更多信息。
- en: 'Although you can use *gspread*’s API to plot directly, using a plot library
    like Matplotlib, I prefer to send the whole sheet to pandas, Python’s powerhouse
    programmatic spreadsheet. This is easily achieved with `gspread`’s `get_all_records`,
    which returns a list of item dictionaries. This list can be used directly to initialize
    a pandas DataFrame (see [“The DataFrame”](ch08.xhtml#pandas_objects)):'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然您可以使用*gspread*的API直接绘制图表，例如使用Matplotlib等绘图库，但我更喜欢将整个表格发送到pandas，Python的强大程序化电子表格。这可以通过`gspread`的`get_all_records`轻松实现，它返回一个项目字典列表。这个列表可以直接用于初始化一个pandas
    DataFrame（参见[“DataFrame”](ch08.xhtml#pandas_objects)）：
- en: '[PRE26]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: In [Chapter 11](ch11.xhtml#chapter_pandas_exploring), we’ll see how to interactively
    explore a DataFrame’s data.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第11章](ch11.xhtml#chapter_pandas_exploring)中，我们将看到如何交互式地探索DataFrame的数据。
- en: Using the Twitter API with Tweepy
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Tweepy的Twitter API
- en: The advent of social media has generated a lot of data and an interest in visualizing
    the social networks, trending hashtags, and media storms contained in them. Twitter’s
    broadcast network is probably the richest source of cool data visualizations,
    and its API provides tweets^([4](ch05.xhtml#idm45607788104048)) filtered by user,
    hashtag, date, and the like.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 社交媒体的出现产生了大量数据，并引起了对可视化其中包含的社交网络、流行标签和媒体风暴的兴趣。Twitter的广播网络可能是最丰富的数据可视化来源，其API提供按用户、标签、日期等过滤的推文^([4](ch05.xhtml#idm45607788104048))。
- en: Python’s Tweepy is an easy-to-use Twitter library that provides a number of
    useful features, such as a `StreamListener` class for streaming live Twitter updates.
    To start using it, you’ll need a Twitter access token, which you can acquire by
    following the instructions [at the Twitter docs](https://oreil.ly/ZkWNf) to create
    your Twitter application. Once this application is created, you can get the keys
    and access tokens for your app by clicking on the link [at your Twitter app page](https://apps.twitter.com).
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: Python的Tweepy是一个易于使用的Twitter库，提供了许多有用的功能，例如用于流式传输实时Twitter更新的`StreamListener`类。要开始使用它，您需要一个Twitter访问令牌，可以通过按照[Twitter文档](https://oreil.ly/ZkWNf)中的说明创建您的Twitter应用程序来获取。一旦创建了此应用程序，您可以通过单击链接在您的Twitter应用程序页面上获取您应用程序的密钥和访问令牌。
- en: 'Tweepy typically requires the four authorization elements shown here:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: Tweepy通常需要这里显示的四个授权元素：
- en: '[PRE27]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'With those defined, accessing tweets could hardly be easier. Here we create
    an OAuth `auth` object using our tokens and keys and use it to start an API session.
    We can then grab the latest tweets from our timeline:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些定义，访问推文变得非常简单。在这里，我们使用我们的令牌和密钥创建一个OAuth `auth`对象，并用它来启动API会话。然后，我们可以从我们的时间线获取最新的推文：
- en: '[PRE28]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Tweepy’s `API` class offers a lot of convenience methods, which you can check
    out [in the Tweepy docs](https://oreil.ly/2FTRw). A common visualization is using
    a network graph to show patterns of friends and followers among Twitter subpopulations.
    The Tweepy method `followers_ids` (get all users following) and `friends_ids`
    (get all users being followed) can be used to construct such a network:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: Tweepy的`API`类提供了许多方便的方法，您可以在[Tweepy文档](https://oreil.ly/2FTRw)中查看。一个常见的可视化方法是使用网络图来展示Twitter子群体中朋友和关注者的模式。Tweepy方法`followers_ids`（获取所有关注用户）和`friends_ids`（获取所有被关注用户）可用于构建这样一个网络：
- en: '[PRE30]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '[![1](assets/1.png)](#co_getting_data_off_the_web__span_class__keep_together__with_python__span__CO5-1)'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_getting_data_off_the_web__span_class__keep_together__with_python__span__CO5-1)'
- en: Gets a list of your followers’ IDs (e.g., `[1191701545, 1554134420, …​]`).
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 获取您关注者的ID列表（例如，`[1191701545, 1554134420, …]`）。
- en: '[![2](assets/2.png)](#co_getting_data_off_the_web__span_class__keep_together__with_python__span__CO5-2)'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_getting_data_off_the_web__span_class__keep_together__with_python__span__CO5-2)'
- en: The first argument to `follower_ids` can be a user ID or screen name.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '`follower_ids`的第一个参数可以是用户ID或屏幕名称。'
- en: Note that you will probably run into rate-limit errors if you try and construct
    a network for anyone with more than a hundred followers (see [this Stack Overflow
    thread](https://oreil.ly/1KDH2) for an explanation). To overcome this you will
    need to implement some basic rate limiting to reduce your request count to 180
    per 15 minutes. Alternatively, you can pay Twitter for a premium account.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，如果你尝试为拥有超过一百个追随者的任何人构建网络，可能会遇到速率限制错误（详见[这个Stack Overflow帖子](https://oreil.ly/1KDH2)的解释）。为了克服这一问题，你需要实施一些基本的速率限制，以将你的请求计数减少到每15分钟180次。或者，你可以支付Twitter以获取高级账户。
- en: By mapping followers of followers, you can create a network of connections that
    might just reveal something interesting about groups and subgroups clustered about
    a particular individual or subject. There’s a nice example of just such a Twitter
    analysis on [Gabe Sawhney’s blog](https://oreil.ly/sWH99).
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 通过映射追随者的追随者，你可以创建一个连接网络，可能会揭示与特定个体或主题相关的群组和子群体中的有趣内容。关于这种Twitter分析的一个很好的例子可以在[Gabe
    Sawhney的博客](https://oreil.ly/sWH99)中找到。
- en: 'One of the coolest features of Tweepy is its `StreamListener` class, which
    makes it easy to collect and process filtered tweets in real time. Live updates
    of Twitter streams have been used by many memorable visualizations (see these
    examples from [FlowingData](https://oreil.ly/mNOYX) and [DensityDesign](https://oreil.ly/ZpmLq)
    for some inspiration). Let’s set up a little stream to record tweets mentioning
    Python, JavaScript, and dataviz. We’ll just print the results to the screen (in
    `on_data`) here, but you would normally cache them in a file or database (or do
    both with SQLite):'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: Tweepy中最酷的功能之一是其`StreamListener`类，它使得在实时收集和处理经过筛选的推文变得很容易。Twitter流的实时更新已经被许多令人难忘的可视化所使用（请参见[FlowingData](https://oreil.ly/mNOYX)和[DensityDesign](https://oreil.ly/ZpmLq)的这些例子，以获得一些灵感）。让我们建立一个小流来记录提到Python、JavaScript和数据可视化的推文。我们只会将结果打印到屏幕上（在`on_data`中），但通常你会将它们缓存到文件或数据库中（或者两者都用SQLite）：
- en: '[PRE31]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Now that we’ve had a taste of the kind of APIs you might run into during your
    search for interesting data, let’s look at the primary technique you’ll use if,
    as is often the case, no one is providing the data you want in a neat, user-friendly
    form: scraping data with Python.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经品尝了你在寻找有趣数据过程中可能遇到的API类型，让我们来看看你将使用的主要技术，如果像通常情况下一样，没有人以整洁、用户友好的形式提供你想要的数据：用Python抓取数据。
- en: Scraping Data
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 抓取数据
- en: Scraping is the chief metaphor used for the practice of getting data that wasn’t
    designed to be programmatically consumed off the web. It is a pretty good metaphor
    because scraping is often about getting the balance right between removing too
    much and too little. Creating procedures that extract just the right data, as
    cleanly as possible, from web pages is a craft skill and often a fairly messy
    one at that. But the payoff is access to visualizable data that often cannot be
    acquired in any other way. Approached in the right way, scraping can even have
    an intrinsic satisfaction.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 抓取是用来获取那些并非被设计为以程序方式消费的数据的实践的首要隐喻。这是一个相当好的隐喻，因为抓取通常涉及在移除过多或过少的数据之间取得平衡。创建能够尽可能干净地从网页中提取恰当数据的程序是一门手艺，而且通常是一门相当混乱的手艺。但回报是可以访问到以其他方式难以获取的可视化数据。以正确的方式接近，抓取甚至可能带来内在的满足感。
- en: Why We Need to Scrape
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么我们需要抓取数据
- en: In an ideal virtual world, online data would be organized in a library, with
    everything cataloged through a sophisticated Dewey decimal system for the web
    page. Unfortunately for the keen data hunter, the web has grown organically, often
    unconstrained by considerations of easy data access for the budding data visualizer.
    So, in reality, the web resembles a big mound of data, some of it clean and usable
    (and thankfully this percentage is increasing) but much of it poorly formed and
    designed for human consumption. And humans are able to parse the kind of messy,
    poorly formed data that our relatively dumb computers have problems with.^([5](ch05.xhtml#idm45607787727008))
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在理想的虚拟世界中，在线数据会像图书馆一样有条理，通过一个复杂的杜威十进制系统对网页进行目录化。然而，对于热衷于数据狩猎的人来说，网络通常是有机地成长的，往往不受潜在的数据可视化者轻松访问的考虑所约束。因此，实际上，网络更像是一大堆数据，其中一些是干净且可用的（幸运的是，这个比例正在增加），但很多是设计不良、难以为人类消费的。而人类能够解析这种混乱、设计不良的数据，而我们相对笨拙的计算机却有些难以应对。^([5](ch05.xhtml#idm45607787727008))
- en: Scraping is about fashioning selection patterns that grab the data we want and
    leave the rest behind. If we’re lucky, the web pages containing the data will
    have helpful pointers, like named tables, specific identities in preference to
    generic classes, and so on. If we’re unlucky, then these pointers will be missing
    and we will have to resort to using other patterns or, in the worst case, ordinal
    specifiers such as *third table in the main div*. These are obviously pretty fragile,
    and will break if somebody adds a table above the third.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 网络爬虫是关于制作选择模式，抓取我们想要的数据并且留下其他内容的过程。如果我们幸运的话，包含数据的网页会有有用的指针，比如具名表格，特定身份优于通用类等等。如果我们不幸的话，这些指针将会丢失，我们将不得不使用其他模式，或者在最坏的情况下，使用顺序指定符号，如*主div中的第三个表格*。显然，这些方法非常脆弱，如果有人在第三个表格之上添加了一个表格，它们就会失效。
- en: In this section, we’ll tackle a little scraping task, to get the same Nobel
    Prize winners data. We’ll use Python’s best-of-breed Beautiful Soup for this lightweight
    scraping foray, saving the heavy guns of Scrapy for the next chapter.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将解决一个小型网络爬虫任务，获取相同的诺贝尔奖获得者数据。我们将使用Python的最佳工具Beautiful Soup进行这次轻量级网络爬虫探险，将Scrapy这个重型武器留给下一章节。
- en: Note
  id: totrans-160
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The fact that data and images are on the web does not mean that they are necessarily
    free to use. For our scraping examples we’ll be using Wikipedia, which allows
    full reuse under the [Creative Commons license](https://oreil.ly/jBTaC). It’s
    a good idea to make sure anything you scrape is available and, if in doubt, contact
    the site maintainer. You may be required to at least cite the original author.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 网页上存在数据和图片并不意味着它们一定是免费使用的。在我们的网络爬虫示例中，我们将使用允许在[创意共享许可证](https://oreil.ly/jBTaC)下完全重用的维基百科。确保您爬取的任何内容可用，并且如有疑问，请联系网站维护者。您可能需要至少引用原始作者。
- en: Beautiful Soup and lxml
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Beautiful Soup和lxml
- en: Python’s key lightweight scraping tools are Beautiful Soup and lxml. Their primary
    selection syntax is different but, confusingly, each can use the other’s parsers.
    The consensus seems to be that lxml’s parser is considerably faster, but Beautiful
    Soup’s might be more robust when dealing with poorly formed HTML. Personally,
    I’ve found lxml to be robust enough and its syntax, based on [xpaths](https://oreil.ly/A43cY),
    more powerful and often more intuitive. I think for someone coming from web development,
    familiar with CSS and jQuery, selection based on CSS selectors is much more natural.
    Depending on your system, lxml is usually the default parser for Beautiful Soup.
    We’ll be using it in the following sections.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: Python的主要轻量级网络爬虫工具是Beautiful Soup和lxml。它们的主要选择语法不同，但令人困惑的是，每个工具都可以使用对方的解析器。一致的观点似乎是lxml的解析器速度要快得多，但Beautiful
    Soup在处理格式不良的HTML时可能更为强大。个人而言，我发现lxml足够强大，其基于[xpaths](https://oreil.ly/A43cY)的语法更加强大且通常更直观。我认为对于从网页开发背景来的人，熟悉CSS和jQuery，基于CSS选择器的选择更加自然。根据您的系统，默认情况下Beautiful
    Soup通常使用lxml作为解析器。我们将在接下来的章节中使用它。
- en: 'Beautiful Soup is part of the Anaconda packages (see [Chapter 1](ch01.xhtml#chapter_install))
    and easily installed with `pip`:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: Beautiful Soup是Anaconda软件包的一部分（见[第1章](ch01.xhtml#chapter_install)），可以通过`pip`轻松安装：
- en: '[PRE32]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: A First Scraping Foray
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第一次网络爬虫探险
- en: Armed with Requests and Beautiful Soup, let’s give ourselves a little task to
    get the names, years, categories, and nationalities of all the Nobel Prize winners.
    We’ll start at the [main Wikipedia Nobel Prize page](https://oreil.ly/cSFFW).
    Scrolling down shows a table with all the laureates by year and category, which
    is a good start to our minimal data requirements.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 装备了Requests和Beautiful Soup，让我们来做一个小任务，获取所有诺贝尔奖获得者的姓名、年份、类别和国籍。我们将从[主维基百科诺贝尔奖页面](https://oreil.ly/cSFFW)开始。向下滚动显示了一个按年份和类别列出所有获奖者的表格，这是我们最低数据需求的良好起点。
- en: Some kind of HTML explorer is pretty much a must for web scraping, and the best
    I know is Chrome’s web developer’s Elements tab (see [“The Elements Tab”](ch04.xhtml#chrome_elements)).
    [Figure 5-1](#wp_nobel) shows the key elements involved in quizzing a web page’s
    structure. We need to know how to select the data of interest, in this case a
    Wikipedia table, while avoiding other elements on the page. Crafting good selector
    patterns is the key to effective scraping, and highlighting the DOM element using
    the element inspector gives us both the CSS pattern and, with a right-click, the
    xpath. The latter is a particularly powerful syntax for DOM element selection
    and the basis of our industrial-strength scraping solution, Scrapy.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 一种HTML浏览器几乎是网页抓取的必备工具，我知道的最好的是Chrome的Web开发者工具中的元素选项卡（参见[“元素选项卡”](ch04.xhtml#chrome_elements)）。[图 5-1](#wp_nobel)展示了涉及查询网页结构的关键元素。我们需要知道如何选择感兴趣的数据，比如维基百科的表格，同时避免页面上的其他元素。制作良好的选择器模式是有效抓取的关键，通过元素检查器突出显示DOM元素可以为我们提供CSS模式和xpath（右键单击）。后者是DOM元素选择的特别强大的语法，也是我们工业强度抓取解决方案Scrapy的基础。
- en: '![dpj2 0501](assets/dpj2_0501.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![dpj2 0501](assets/dpj2_0501.png)'
- en: 'Figure 5-1\. Wikipedia’s main Nobel Prize page: A and B show the wikitable’s
    CSS selector. Right-clicking and selecting C (Copy XPath) gives the table’s xpath
    (`//*[@id="mw-content-text"]/table[1]`). D shows a `thead` tag generated by jQuery.'
  id: totrans-170
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-1. 维基百科的主要诺贝尔奖页面：A和B展示了wikitable的CSS选择器。右键单击并选择C（复制XPath）会得到表格的xpath（`//*[@id="mw-content-text"]/table[1]`）。D显示了由jQuery生成的`thead`标签。
- en: Getting the Soup
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 获取Soup
- en: 'The first thing you need to do before scraping the web page of interest is
    to parse it with Beautiful Soup, converting the HTML into a tag tree hierarchy
    or soup:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在抓取感兴趣的网页之前，您需要使用Beautiful Soup解析它，将HTML转换为标签树层次结构或soup：
- en: '[PRE33]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[![1](assets/1.png)](#co_getting_data_off_the_web__span_class__keep_together__with_python__span__CO6-1)'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_getting_data_off_the_web__span_class__keep_together__with_python__span__CO6-1)'
- en: The second argument specifies the parser we want to use, namely lxml’s.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个参数指定了我们想要使用的解析器，即lxml的。
- en: With our soup in hand, let’s see how to find our target tags.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 有了我们的soup，让我们看看如何找到我们的目标标签。
- en: Selecting Tags
  id: totrans-177
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择标签
- en: 'Beautiful Soup offers a few ways to select tags from the parsed soup, with
    subtle differences that can be confusing. Before demonstrating the selection methods,
    let’s get the soup of our Nobel Prize page:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: Beautiful Soup提供了几种从解析后的soup中选择标签的方法，它们之间有微妙的差异，可能会让人感到困惑。在演示选择方法之前，让我们获取一下诺贝尔奖页面的soup：
- en: '[PRE34]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Our target table (see [Figure 5-1](#wp_nobel)) has two defining classes, `wikitable`
    and `sortable` (there are some unsortable tables on the page). We can use Beautiful
    Soup’s `find` method to find the first table tag with those classes. `find` takes
    a tag name as its first argument and a dictionary with class, ID, and other identifiers
    as its second:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标表格（参见[图 5-1](#wp_nobel)）有两个定义类，`wikitable` 和 `sortable`（页面上还有一些不可排序的表格）。我们可以使用
    Beautiful Soup 的 `find` 方法来查找第一个具有这些类的表格标签。`find` 方法将一个标签名称作为其第一个参数，并将一个包含类、ID
    和其他标识符的字典作为其第二个参数：
- en: '[PRE35]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Although we have successfully found our table by its classes, this method is
    not very robust. Let’s see what happens when we change the order of our CSS classes:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们已经成功地通过类找到了我们的表格，但这种方法并不是很健壮。让我们看看当我们改变我们的CSS类的顺序时会发生什么：
- en: '[PRE36]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: So `find` cares about the order of the classes, using the class string to find
    the tag. If the classes were specified in a different order—​something that might
    well happen during an HTML edit, then the `find` fails. This fragility makes it
    difficult to recommend the Beautiful Soup selectors, such as `find` and `find_all`.
    When doing quick hacking, I find lxml’s [CSS selectors](https://lxml.de/cssselect.xhtml)
    easier and more intuitive.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，`find`关心类的顺序，使用类字符串来找到标签。如果类的顺序不同——这在HTML编辑中可能会发生，那么`find`就会失败。这种脆弱性使得很难推荐Beautiful
    Soup的选择器，比如`find`和`find_all`。在快速进行修改时，我发现lxml的[CSS选择器](https://lxml.de/cssselect.xhtml)更容易和更直观。
- en: Using the soup’s `select` method (available if you specified the lxml parser
    when creating it), you can specify an HTML element using its CSS class, ID, and
    so on. This CSS selector is converted into the xpath syntax lxml uses internally.^([6](ch05.xhtml#idm45607787437712))
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 使用soup的`select`方法（如果在创建时指定了lxml解析器），您可以使用其CSS类、ID等指定HTML元素。此CSS选择器被转换为lxml内部使用的xpath语法。^([6](ch05.xhtml#idm45607787437712))
- en: 'To get our wikitable, we just select a table in the soup, using the dot notation
    to indicate its classes:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取我们的wikitable，我们只需在soup中选择一个表格，使用点符号表示其类：
- en: '[PRE37]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Note that `select` returns an array of results, finding all the matching tags
    in the soup. lxml provides the `select_one` convenience method if you are selecting
    just one HTML element. Let’s grab our Nobel table and see what headers it has:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，`select` 返回一个结果数组，找到 soup 中所有匹配的标签。如果您只选择一个 HTML 元素，则 lxml 提供了 `select_one`
    便捷方法。让我们抓取我们的诺贝尔表格，并看看它有哪些标题：
- en: '[PRE38]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'As a shorthand for `select`, you can call the tag directly on the soup; so
    these two are equivalent:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 作为 `select` 的简写，您可以直接在 soup 上调用标签；所以这两个是等效的：
- en: '[PRE39]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: With lxml’s parser, Beautiful Soup provides a number of different filters for
    finding tags, including the simple string name we’ve just used, searching by [regular
    expression](https://oreil.ly/GeU8Q), using a list of tag names, and more. See
    this [comprehensive list](https://oreil.ly/iBQwc) for more details.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 lxml 解析器，Beautiful Soup 提供了许多不同的过滤器来查找标签，包括我们刚刚使用的简单字符串名称、使用 [正则表达式](https://oreil.ly/GeU8Q)、使用标签名称列表等。更多详情请参阅这个
    [综合列表](https://oreil.ly/iBQwc)。
- en: As well as lxml’s `select` and `select_one`, there are 10 BeautfulSoup convenience
    methods for searching the parsed tree. These are essentially variants on `find`
    and `find_all` that specify which parts of the tree they search. For example,
    `find_parent` and `find_parents`, rather than looking for descendants down the
    tree, look for parent tags of the tag being searched. All 10 methods are available
    in the Beautiful Soup [official docs](https://oreil.ly/oPrQl).
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 lxml 的 `select` 和 `select_one` 外，BeautifulSoup 还提供了 10 个搜索解析树的便捷方法。这些方法本质上是
    `find` 和 `find_all` 的变体，指定了它们搜索的树的哪些部分。例如，`find_parent` 和 `find_parents` 不是在树下查找后代，而是查找被搜索标签的父标签。所有这
    10 个方法都在 Beautiful Soup [官方文档](https://oreil.ly/oPrQl) 中有详细说明。
- en: Now that we know how to select our Wikipedia table and are armed with lxml’s
    selection methods, let’s see how to craft some selection patterns to get the data
    we want.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道如何选择我们的维基百科表格，并且掌握了 lxml 的选择方法，让我们看看如何制定一些选择模式来获取我们想要的数据。
- en: Crafting Selection Patterns
  id: totrans-195
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 制定选择模式
- en: 'Having successfully selected our data table, we now want to craft some selection
    patterns to scrape the required data. Using the HTML explorer, you can see that
    the individual winners are contained in `<td>` cells, with an href `<a>` link
    to Wikipedia’s bio pages (in the case of individuals). Here’s a typical target
    row with CSS classes that we can use as targets to get the data in the `<td>`
    cells:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 成功选择了我们的数据表格后，我们现在想要制定一些选择模式来抓取所需的数据。使用 HTML 浏览器，你可以看到个人得奖者被包含在 `<td>` 单元格中，并带有指向维基百科传记页面的
    href `<a>` 链接（在个人情况下）。这是一个典型的目标行，具有我们可以用作目标以获取 `<td>` 单元格中数据的 CSS 类：
- en: '[PRE40]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: If we loop through these data cells, keeping track of their row (year) and column
    (category), then we should be able to create a list of winners with all the data
    we specified except nationality.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们循环遍历这些数据单元格，并跟踪它们的行（年份）和列（类别），那么我们应该能够创建一个获奖者列表，其中包含我们指定的所有数据，但不包括国籍。
- en: 'The following `get_column_titles` function scrapes our table for the Nobel
    category column headers, ignoring the first Year column. Often the header cell
    in a Wikipedia table contains a web-linked `''a''` tag; all the Nobel categories
    fit this model, pointing to their respective Wikipedia pages. If the header is
    not clickable, we store its text and a null href:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 以下 `get_column_titles` 函数从我们的表格中抓取了诺贝尔奖类别列标题，忽略了第一列 Year。通常，维基百科表格中的标题单元格包含一个带有
    web 链接的 `'a'` 标签；所有诺贝尔奖类别都符合这个模型，指向它们各自的维基百科页面。如果标题不可点击，则我们存储其文本和一个空的 href：
- en: '[PRE41]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '[![1](assets/1.png)](#co_getting_data_off_the_web__span_class__keep_together__with_python__span__CO7-1)'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '![1](assets/1.png)'
- en: We loop through the table head, ignoring the first Year column ([1:]). This
    selects the column headers shown in [Figure 5-2](#scraping_nobel_table).
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 我们循环遍历表头，忽略第一列 Year ([1:])。这选择了 [图 5-2](#scraping_nobel_table) 中显示的列标题。
- en: '![dpj2 0502](assets/dpj2_0502.png)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![dpj2 0502](assets/dpj2_0502.png)'
- en: Figure 5-2\. Wikipedia’s table of Nobel Prize winners
  id: totrans-204
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-2\. 维基百科的诺贝尔奖得主表格
- en: 'Let’s make sure `get_column_titles` is giving us what we want:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们确保 `get_column_titles` 正在给我们想要的东西：
- en: '[PRE42]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '[![1](assets/1.png)](#co_getting_data_off_the_web__span_class__keep_together__with_python__span__CO8-1)'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '![1](assets/1.png)'
- en: Gets all the Year rows, starting from the second, corresponding to the rows
    in [Figure 5-2](#scraping_nobel_table).
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 获取所有 Year 行，从第二行开始，对应于 [图 5-2](#scraping_nobel_table) 中的行。
- en: '[![2](assets/2.png)](#co_getting_data_off_the_web__span_class__keep_together__with_python__span__CO8-2)'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_getting_data_off_the_web__span_class__keep_together__with_python__span__CO8-2)'
- en: Finds the `<td>` data cells shown in [Figure 5-2](#scraping_nobel_table).
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 查找[图 5-2](#scraping_nobel_table)中显示的`<td>`数据单元格。
- en: Iterating through the Year rows, we take the first Year column and then iterate
    over the remaining columns, using `enumerate` to keep track of our index, which
    will map to the category column names. We know that all the winner names are contained
    in an `<a>` tag but that there are occasional extra `<a>` tags beginning with
    `#endnote`, which we filter for. Finally we append a year, category, name, and
    link dictionary to our data array. Note that the winner selector has an `attrs`
    dictionary containing, among other things, the `<a>` tag’s href.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在年份行迭代中，我们获取第一个年份列，然后迭代剩余列，使用`enumerate`来跟踪我们的索引，该索引将映射到类别列名。我们知道所有获奖者的名称都包含在一个`<a>`标签中，但有时会有额外的以`#endnote`开头的`<a>`标签，我们需要过滤掉这些。最后，我们向数据数组中添加一个年份、类别、名称和链接的字典。请注意，获奖者选择器具有包含`<a>`标签的`attrs`字典，其中包括其他内容。
- en: 'Let’s confirm that `get_Nobel_winners` delivers a list of Nobel Prize winner
    dictionaries:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们确认`get_Nobel_winners`是否提供了诺贝尔奖获奖者字典的列表：
- en: '[PRE43]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '[PRE44]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Now that we have the full list of Nobel Prize winners and links to their Wikipedia
    pages, we can use these links to scrape data from the individuals’ biographies.
    This will involve making a largish number of requests, and it’s not something
    we really want to do more than once. The sensible and respectful^([7](ch05.xhtml#idm45607786692608))
    thing is to cache the data we scrape, allowing us to try out various scraping
    experiments without returning to Wikipedia.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们拥有了完整的诺贝尔奖获奖者列表和他们维基百科页面的链接，我们可以利用这些链接从个人传记中抓取数据。这将涉及大量请求，这不是我们真正希望多次执行的操作。明智和尊重的^([7](ch05.xhtml#idm45607786692608))做法是缓存我们抓取的数据，使我们能够尝试各种抓取实验而无需返回维基百科。
- en: Caching the Web Pages
  id: totrans-216
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 缓存网页
- en: It’s easy enough to rustle up a quick cacher in Python, but as often as not
    it’s easier still to find a better solution written by someone else and kindly
    donated to the open source community. Requests has a nice plug-in called `requests-cache`
    that, with a few lines of configuration, will take care of all your basic caching
    needs.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python中轻松地制作一个快速缓存器是很容易的，但往往更容易找到由他人编写并慷慨捐赠给开源社区的更好解决方案。Requests有一个名为`requests-cache`的不错插件，通过几行配置，可以满足所有基本的缓存需求。
- en: 'First, we install the plug-in using `pip`:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们使用`pip`安装插件：
- en: '[PRE45]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '`requests-cache` uses [monkey patching](https://oreil.ly/8IklZ) to dynamically
    replace parts of the `requests` API at runtime. This means it can work transparently.
    You just have to install its cache and then use `requests` as usual, with all
    the caching being taken care of. Here’s the simplest way to use `requests-cache`:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '`requests-cache`使用[猴子补丁技术](https://oreil.ly/8IklZ)，在运行时动态替换`requests` API的部分。这意味着它可以透明地工作。您只需安装其缓存，然后像平常一样使用`requests`，所有缓存都会被处理。这是使用`requests-cache`的最简单方式：'
- en: '[PRE46]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'The `install_cache` method has a number of useful options, including allowing
    you to specify the cache `backend` (`sqlite`, `memory`, `mongdb`, or `redis`)
    or set an expiry time (`expiry_after`) in seconds on the caching. So the following
    creates a cache named `nobel_pages` with an `sqlite` backend and pages that expire
    in two hours (7,200 s):'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '`install_cache`方法有许多有用的选项，包括允许您指定缓存`backend`（`sqlite`、`memory`、`mongdb`或`redis`）或在缓存上设置过期时间（`expiry_after`）（单位为秒）。因此，以下代码将创建一个名为`nobel_pages`的缓存，使用`sqlite`作为后端，并设置页面在两小时（7,200秒）后过期：'
- en: '[PRE47]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '`requests-cache` will serve most of your caching needs and couldn’t be much
    easier to use. For more details, see [the official docs](https://oreil.ly/d67bK)
    where you’ll also find a little example of request throttling, which is a useful
    technique when doing bulk scraping.'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '`requests-cache`将满足大部分缓存需求，并且使用起来非常简单。有关更多详细信息，请参阅[官方文档](https://oreil.ly/d67bK)，您还可以在那里找到请求节流的小例子，这在进行大规模抓取时是一种有用的技术。'
- en: Scraping the Winners’ Nationalities
  id: totrans-225
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 抓取获奖者的国籍
- en: With caching in place, let’s try getting the winners’ nationalities, using the
    first 50 for our experiment. A little `get_winner_nationality()` function will
    use the winner links we stored earlier to scrape their page and then use the info-box
    shown in [Figure 5-3](#country_bio) to get the `Nationality` attribute.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 设置了缓存后，让我们尝试获取获奖者的国籍，使用前50个进行实验。一个小的`get_winner_nationality()`函数将使用我们之前存储的获奖者链接来抓取他们的页面，然后使用[图 5-3](#country_bio)中显示的信息框来获取`Nationality`属性。
- en: '![dpj2 0503](assets/dpj2_0503.png)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![dpj2 0503](assets/dpj2_0503.png)'
- en: Figure 5-3\. Scraping a winner’s nationality
  id: totrans-228
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-3\. 抓取获奖者的国籍
- en: Note
  id: totrans-229
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: When scraping, you are looking for reliable patterns and repeating elements
    with useful data. As we’ll see, the Wikipedia info-boxes for individuals are not
    such a reliable source, but clicking on a few random links certainly gives that
    impression. Depending on the size of the dataset, it’s good to perform a few experimental
    sanity checks. You can do this manually but, as mentioned at the start of the
    chapter, this won’t scale or improve your craft skills.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 在抓取时，您正在寻找可靠的模式和包含有用数据的重复元素。正如我们将看到的那样，个人的维基信息框并不是一个可靠的来源，但是点击几个随机链接确实给人这样的印象。根据数据集的大小，进行一些实验性的健全性检查是很好的。您可以手动执行此操作，但是正如本章开头提到的那样，这不会扩展或提升您的技能。
- en: '[Example 5-3](#get_winner_nationality) takes one of the winner dictionaries
    we scraped earlier and returns a name-labeled dictionary with a `Nationality`
    key if one is found. Let’s run it on the first 50 winners and see how often a
    `Nationality` attribute is missing.'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例 5-3](#get_winner_nationality) 从之前抓取的获奖者字典中获取一个，并在找到`Nationality`键时返回一个带有名称标签的字典。让我们在前50名获奖者上运行它，并看看`Nationality`属性缺失的频率。'
- en: Example 5-3\. Scraping the winner’s country from their biography page
  id: totrans-232
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 5-3\. 从获奖者传记页面抓取获奖者的国家
- en: '[PRE48]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '[![1](assets/1.png)](#co_getting_data_off_the_web__span_class__keep_together__with_python__span__CO9-1)'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_getting_data_off_the_web__span_class__keep_together__with_python__span__CO9-1)'
- en: We use a CSS selector to find all the `<tr>` rows of the table with class `infobox`.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 CSS 选择器来查找所有具有`infobox`类的表格的所有`<tr>`行。
- en: '[![2](assets/2.png)](#co_getting_data_off_the_web__span_class__keep_together__with_python__span__CO9-2)'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_getting_data_off_the_web__span_class__keep_together__with_python__span__CO9-2)'
- en: Cycles through the rows looking for a Nationality field.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 在行中循环查找国籍字段。
- en: '[Example 5-4](#nationality_test) shows that 14 of the 50 first winners failed
    our attempt to scrape their nationality. In the case of the Institut de Droit
    International, national affiliation may well be moot, but Theodore Roosevelt is
    about as American as they come. Clicking on a few of the names shows the problem
    (see [Figure 5-4](#missing_nationality)). The lack of a standardized biography
    format means synonyms for *Nationality* are often employed, as in Marie Curie’s
    *Citizenship*; sometimes no reference is made, as with Niels Finsen; and Randall
    Cremer has nothing but a photograph in his info-box. We can discard the info-boxes
    as a reliable source of winners’ nationalities but, as they appeared to be the
    only regular source of potted data, this sends us back to the drawing board. In
    the next chapter, we’ll see a successful approach using Scrapy and a different
    start page.'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例 5-4](#nationality_test) 表明，在前50名获奖者中，有14人未能成功抓取其国籍信息。在国际法学院的情况下，国籍可能并不重要，但西奥多·罗斯福就如同美国人一般。点击其中几个名字显示了问题（见图 5-4）。“国籍”缺失的标准传记格式意味着通常会使用*国籍*的同义词，例如玛丽·居里的*公民身份*；有时不会有参考文献，例如尼尔斯·芬森；而兰德尔·克雷默的信息框中仅有一张照片。我们可以丢弃信息框作为获奖者国籍的可靠来源，但是因为它们似乎是唯一的常规数据来源，这让我们不得不重新考虑。在下一章中，我们将看到使用
    Scrapy 和不同起始页的成功方法。'
- en: Example 5-4\. Testing for scraped nationalities
  id: totrans-239
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 5-4\. 测试抓取的国籍
- en: '[PRE49]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '![dpj2 0504](assets/dpj2_0504.png)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![dpj2 0504](assets/dpj2_0504.png)'
- en: Figure 5-4\. Winners without a recorded *nationality*
  id: totrans-242
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-4\. 没有记录*国籍*的获奖者
- en: Although Wikipedia is a relative free-for-all, production-wise, where data is
    designed for human consumption, you can expect a lack of rigor. Many sites have
    similar gotchas and as the datasets get bigger, more tests may be needed to find
    the flaws in a collection pattern.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管维基百科相对自由，生产方面的数据设计是为了人类消费，您可能会发现缺乏严谨性。许多网站存在类似的陷阱，随着数据集的增大，可能需要更多的测试来发现集合模式中的缺陷。
- en: Although our first scraping exercise was a little artificial in order to introduce
    the tools, I hope it captured something of the slightly messy spirit of web scraping.
    The ultimately abortive pursuit of a reliable Nationality field for our Nobel
    dataset could have been forestalled by a bit of web browsing and manual HTML-source
    trawling. However, if the dataset were significantly larger and the failure rate
    a bit smaller, then programmatic detection, which gets easier and easier as you
    become acquainted with the scraping modules, really starts to deliver.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们的第一个抓取练习有些人为，是为了介绍工具，我希望它捕捉到了一些网络抓取的稍微混乱的精神。我们最终未能获得可靠的诺贝尔数据集中的国籍字段，这可能可以通过一些网络浏览和手动HTML源代码检索来避免。然而，如果数据集规模更大，失败率稍低，那么编程检测，随着你熟悉抓取模块，确实开始变得更容易。
- en: This little scraping test was designed to introduce Beautiful Soup, and shows
    that collecting the data we seek requires a little more thought, which is often
    the case with scraping. In the next chapter, we’ll wheel out the big gun, Scrapy,
    and, with what we’ve learned in this section, harvest the data we need for our
    Nobel Prize visualization.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 这个小小的抓取测试旨在介绍Beautiful Soup，并显示我们寻找的数据收集需要更多的思考，这在抓取中经常发生。在下一章中，我们将使用我们在本节中学到的知识，推出大杀器Scrapy，并收集我们需要的诺贝尔奖可视化数据。
- en: Summary
  id: totrans-246
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we’ve seen examples of the most common ways in which data can
    be sucked out of the web and into Python containers, databases, or pandas datasets.
    Python’s Requests library is the true workhorse of HTTP negotiation and a fundamental
    tool in our dataviz toolchain. For simpler, RESTful APIs, consuming data with
    Requests is a few lines of Python away. For the more awkward APIs, such as those
    with potentially complicated authorization, a wrapper library like Tweepy (for
    Twitter) can save a lot of hassle. Decent wrappers can also keep track of access
    rates and, where necessary, throttle your requests. This is a key consideration,
    particularly when there is the possibility of blacklisting unfriendly consumers.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们已经看到了从网络中提取数据并存入Python容器、数据库或pandas数据集中的最常见方法的示例。Python的Requests库是HTTP协商的真正工作马和我们数据可视化工具链中的一个基本工具。对于更简单的、符合RESTful标准的API，使用Requests消耗数据只需几行Python代码。对于更为棘手的API，比如那些可能有复杂授权的API，像Tweepy（用于Twitter）这样的包装库可以省去许多麻烦。良好的包装库还可以跟踪访问速率，并在必要时限制你的请求。这是一个关键考虑因素，特别是当有可能会列入黑名单的不友好的消费者时。
- en: We also started our first forays into data scraping, which is often a necessary
    fallback where no API exists and the data is for human consumption. In the next
    chapter, we’ll get all the Nobel Prize data needed for the book’s visualization
    using Python’s Scrapy, an industrial-strength scraping library.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还开始了我们的第一次数据抓取尝试，这通常是一个必要的后备措施，当不存在API并且数据是供人类消费时。在下一章中，我们将使用Python的Scrapy库，一个工业级的抓取库，获取本书可视化所需的所有诺贝尔奖数据。
- en: ^([1](ch05.xhtml#idm45607789542496-marker)) This is actually a [deliberate policy](https://oreil.ly/WOjdB)
    of the developers.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch05.xhtml#idm45607789542496-marker)) 这实际上是开发人员的一个[有意的策略](https://oreil.ly/WOjdB)。
- en: ^([2](ch05.xhtml#idm45607789528976-marker)) There are some platform dependencies
    that might still generate errors. This [Stack Overflow thread](https://oreil.ly/Zm082)
    is a good starting point if you still have problems.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch05.xhtml#idm45607789528976-marker)) 仍可能有一些平台依赖性可能会产生错误。如果仍然遇到问题，这个[Stack
    Overflow线程](https://oreil.ly/Zm082)是一个很好的起点。
- en: ^([3](ch05.xhtml#idm45607788534000-marker)) OAuth1 access has been deprecated
    recently.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch05.xhtml#idm45607788534000-marker)) OAuth1访问最近已经被弃用。
- en: ^([4](ch05.xhtml#idm45607788104048-marker)) The free API is currently limited
    to around [350 requests per hour](https://oreil.ly/LKzJX).
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch05.xhtml#idm45607788104048-marker)) 免费API当前限制为每小时约[350个请求](https://oreil.ly/LKzJX)。
- en: ^([5](ch05.xhtml#idm45607787727008-marker)) Much of modern machine learning
    and artificial intelligence (AI) research is dedicated to creating computer software
    that can cope with messy, noisy, fuzzy, informal data but, as of this book’s publication,
    there’s no off-the-shelf solution I know of.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch05.xhtml#idm45607787727008-marker)) 许多现代机器学习和人工智能（AI）研究致力于创建能够处理杂乱、嘈杂、模糊、非正式数据的计算机软件，但截至本书出版时，我不知道有现成的解决方案。
- en: ^([6](ch05.xhtml#idm45607787437712-marker)) This CSS selection syntax should
    be familiar to anyone who’s used JavaScript’s [jQuery library](https://jquery.com)
    and is also similar to that used by [D3](https://d3js.org).
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: ^([6](ch05.xhtml#idm45607787437712-marker)) 这种 CSS 选择语法对于使用过 JavaScript 的 [jQuery
    library](https://jquery.com) 的人应该很熟悉，它也类似于 [D3](https://d3js.org) 使用的语法。
- en: ^([7](ch05.xhtml#idm45607786692608-marker)) When scraping, you’re using other
    people’s web bandwidth, which ultimately costs them money. It’s just good manners
    to try to limit your number of requests.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: ^([7](ch05.xhtml#idm45607786692608-marker)) 在抓取数据时，你使用了其他人的网络带宽，这最终会花费他们的金钱。试着限制你的请求次数是一种礼貌行为。
