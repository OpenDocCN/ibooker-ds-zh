- en: Chapter 5\. Getting Data Off the Web with Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A fundamental part of the data visualizer’s skill set is getting the right dataset
    in as clean a form as possible. Sometimes you will be given a nice, clean dataset
    to analyze but often you will be tasked with either finding the data and/or cleaning
    the data supplied.
  prefs: []
  type: TYPE_NORMAL
- en: And more often than not these days, getting data involves getting it off the
    web. There are various ways you can do this, and Python provides some great libraries
    that make sucking up the data easy.
  prefs: []
  type: TYPE_NORMAL
- en: 'The main ways to get data off the web are:'
  prefs: []
  type: TYPE_NORMAL
- en: Get a raw data file in a recognized data format (e.g., JSON or CSV) over HTTP.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use a dedicated API to get the data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scrape the data by getting web pages via HTTP and parsing them locally for the
    required data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This chapter will deal with these ways in turn, but first let’s get acquainted
    with the best Python HTTP library out there: Requests.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting Web Data with the Requests Library
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we saw in [Chapter 4](ch04.xhtml#chapter_webdev101), the files that are used
    by web browsers to construct web pages are communicated via the Hypertext Transfer
    Protocol (HTTP), first developed by [Tim Berners-Lee](https://oreil.ly/uKF5f).
    Getting web content in order to parse it for data involves making HTTP requests.
  prefs: []
  type: TYPE_NORMAL
- en: Negotiating HTTP requests is a vital part of any general-purpose language, but
    getting web pages with Python used to be a rather irksome affair. The venerable
    urllib2 library was hardly user-friendly, with a very clunky API. [*Requests*](https://oreil.ly/6VkKZ),
    courtesy of Kenneth Reitz, changed that, making HTTP a relative breeze and fast
    establishing itself as the go-to Python HTTP library.
  prefs: []
  type: TYPE_NORMAL
- en: 'Requests is not part of the Python standard library^([1](ch05.xhtml#idm45607789542496))
    but is part of the [Anaconda package](https://oreil.ly/LD0ee) (see [Chapter 1](ch01.xhtml#chapter_install)).
    If you’re not using Anaconda, the following `pip` command should do the job:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: If you’re using a Python version prior to 2.7.9 (I strongly recommend using
    Python 3+ wherever possible), then using Requests may generate some [Secure Sockets
    Layer (SSL) warnings](https://oreil.ly/8D08s). Upgrading to newer SSL libraries
    should fix this:^([2](ch05.xhtml#idm45607789528976))
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Now that you have Requests installed, you’re ready to perform the first task
    mentioned at the beginning of this chapter and grab some raw data files off the
    web.
  prefs: []
  type: TYPE_NORMAL
- en: Getting Data Files with Requests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A Python interpreter session is a good way to put Requests through its paces,
    so find a friendly local command line, fire up IPython, and import `requests`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'To demonstrate, let’s use the library to download a Wikipedia page. We use
    the Requests library’s `get` method to get the page and, by convention, assign
    the result to a `response` object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s use Python’s [`dir`](https://oreil.ly/CrJ8h) method to get a list of
    the `response` object’s attributes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Most of these attributes are self-explanatory and together provide a lot of
    information about the HTTP response generated. You’ll use a small subset of these
    attributes generally. Firstly, let’s check the status of the response:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'As all good minimal web developers know, 200 is the [HTTP status code](https://oreil.ly/ucEoo)
    for OK, indicating a successful transaction. Other than 200, the most common codes
    are:'
  prefs: []
  type: TYPE_NORMAL
- en: 401 (Unauthorized)
  prefs: []
  type: TYPE_NORMAL
- en: Attempting unauthorized access
  prefs: []
  type: TYPE_NORMAL
- en: 400 (Bad Request)
  prefs: []
  type: TYPE_NORMAL
- en: Trying to access the web server incorrectly
  prefs: []
  type: TYPE_NORMAL
- en: 403 (Forbidden)
  prefs: []
  type: TYPE_NORMAL
- en: Similar to 401 but no login opportunity was available
  prefs: []
  type: TYPE_NORMAL
- en: 404 (Not Found)
  prefs: []
  type: TYPE_NORMAL
- en: Trying to access a web page that doesn’t exist
  prefs: []
  type: TYPE_NORMAL
- en: 500 (Internal Server Error)
  prefs: []
  type: TYPE_NORMAL
- en: A general-purpose, catchall error
  prefs: []
  type: TYPE_NORMAL
- en: 'So, for example, if we made a spelling mistake with our request, asking to
    see the `SNoble_Prize` page, we’d get a 404 (Not Found) error:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'With our 200 OK response, from the correctly spelled request, let’s look at
    some of the info returned. A quick overview can be had with the `headers` property:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: This shows, among other things, that the page returned was gzip-encoded and
    87 KB in size with `content-type` of `text/html`, encoded with Unicode UTF-8.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we know text has been returned, we can use the `text` property of the
    response to see what it is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: This shows that we do indeed have our Wikipedia HTML page, with some inline
    JavaScript. As we’ll see in [“Scraping Data”](#get_data_scraping), in order to
    make sense of this content, we’ll need a parser to read the HTML and provide the
    content blocks.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve grabbed a raw page off the web, let’s see how to use Requests
    to consume a web data API.
  prefs: []
  type: TYPE_NORMAL
- en: Using Python to Consume Data from a Web API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If the data file you need isn’t on the web, there may well be an Application
    Programming Interface (API) serving the data you need. Using this will involve
    making a request to the appropriate server to retrieve your data in a fixed format
    or one you get to specify in the request.
  prefs: []
  type: TYPE_NORMAL
- en: The most popular data formats for web APIs are JSON and XML, though a number
    of esoteric formats exist. For the purposes of the JavaScripting data visualizer,
    JavaScript Object Notation (JSON) is obviously preferred (see [“Data”](ch04.xhtml#sect_data)).
    Lucky for us, it is also starting to predominate.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are different approaches to creating a web API, and for a few years there
    was a little war of the architectures among the three main types of APIs inhabiting
    the web:'
  prefs: []
  type: TYPE_NORMAL
- en: '[REST](https://oreil.ly/ujgdJ)'
  prefs: []
  type: TYPE_NORMAL
- en: Short for REpresentational State Transfer, using a combination of HTTP verbs
    (GET, POST, etc.) and Uniform Resource Identifiers (URIs; e.g., */user/kyran*)
    to access, create, and adapt data.
  prefs: []
  type: TYPE_NORMAL
- en: '[XML-RPC](https://oreil.ly/ZMQvW)'
  prefs: []
  type: TYPE_NORMAL
- en: A remote procedure call (RPC) protocol using XML encoding and HTTP transport.
  prefs: []
  type: TYPE_NORMAL
- en: '[SOAP](https://oreil.ly/l5LVL)'
  prefs: []
  type: TYPE_NORMAL
- en: Short for Simple Object Access Protocol, using XML and HTTP.
  prefs: []
  type: TYPE_NORMAL
- en: This battle seems to be resolving in a victory for [RESTful APIs](https://oreil.ly/apc1l),
    and this is a very good thing. Quite apart from RESTful APIs being more elegant,
    and easier to use and implement (see [Chapter 13](ch13.xhtml#chapter_delivery_restful)),
    some standardization here makes it much more likely that you will recognize and
    quickly adapt to a new API that comes your way. Ideally, you will be able to reuse
    existing code. There is a new player on the scene in the form of [GraphQL](https://oreil.ly/JUGVS),
    which bills itself as a better REST, but as a datavizzer you’re far more likely
    to be consuming conventional RESTful APIs.
  prefs: []
  type: TYPE_NORMAL
- en: Most access and manipulation of remote data can be summed up by the acronym
    CRUD (create, retrieve, update, delete), originally coined to describe all the
    major functions implemented in relational databases. HTTP provides CRUD counterparts
    with the POST, GET, PUT, and DELETE verbs and the REST abstraction builds on this
    use of these verbs, acting on a [Universal Resource Identifier (URI)](https://oreil.ly/xmX1k).
  prefs: []
  type: TYPE_NORMAL
- en: Discussions about what is and isn’t a proper RESTful interface can get quite
    involved, but essentially the URI (e.g., *https://example.com/api/items/2*) should
    contain all the information required in order to perform a CRUD operation. The
    particular operation (e.g., GET or DELETE) is specified by the HTTP verb. This
    excludes architectures such as SOAP, which place stateful information in metadata
    on the requests header. Imagine the URI as the virtual address of the data and
    CRUD as all the operations you can perform on it.
  prefs: []
  type: TYPE_NORMAL
- en: As data visualizers keen to lay our hands on some interesting datasets, we are
    avid consumers here, so our HTTP verb of choice is GET, and the examples that
    follow will focus on the fetching of data with various well-known web APIs. Hopefully,
    some patterns will emerge.
  prefs: []
  type: TYPE_NORMAL
- en: Although the two constraints of stateless URIs and the use of the CRUD verbs
    is a nice constraint on the shape of RESTful APIs, there still manage to be many
    variants on the theme.
  prefs: []
  type: TYPE_NORMAL
- en: Consuming a RESTful Web API with Requests
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Requests has a fair number of bells and whistles based around the main HTTP
    request verbs. For a good overview, see [the Requests quickstart](https://oreil.ly/Bp8VG).
    For the purposes of getting data, you’ll use GET and POST pretty much exclusively,
    with GET being by a long way the most used verb. POST allows you to emulate web
    forms, including login details, field values, etc. in the request. For those occasions
    where you find yourself driving a web form with, for example, lots of options
    selectors, Requests makes automation with POST easy. GET covers pretty much everything
    else, including the ubiquitous RESTful APIs, which provide an increasing amount
    of the well-formed data available on the web.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at a more complicated use of Requests, getting a URL with arguments.
    The [Organisation for Economic Cooperation and Development (OECD)](https://oreil.ly/QAj3A)
    provides some [useful datasets on its site](https://data.oecd.org). These datasets
    provide mainly economic measures and statistics for the member countries of the
    OECD, and such data can form the basis of many interesting visualizations. The
    OECD provides a few of its own, such as one [allowing you to compare your country](https://oreil.ly/aFmUv)
    with others in the OECD.
  prefs: []
  type: TYPE_NORMAL
- en: 'The OECD web API is described [in this documentation](https://oreil.ly/f5VDc),
    and queries are constructed with the dataset name (dsname) and some dot-separated
    dimensions, each of which can be a number of `+` separated values. The URL can
    also take standard HTTP parameters initiated by a `?` and separated by `&`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'So the following is a valid URL:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_getting_data_off_the_web__span_class__keep_together__with_python__span__CO1-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Specifies the QNA (Quarterly National Accounts) dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_getting_data_off_the_web__span_class__keep_together__with_python__span__CO1-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Four dimensions, by location, subject, measure, and frequency.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_getting_data_off_the_web__span_class__keep_together__with_python__span__CO1-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Data from the second quarter of 2009 to the fourth quarter of 2011.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s construct a little Python function to query the OECD’s API ([Example 5-1](#oecd_request)).
  prefs: []
  type: TYPE_NORMAL
- en: Example 5-1\. Making a URL for the OECD API
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_getting_data_off_the_web__span_class__keep_together__with_python__span__CO2-1)'
  prefs: []
  type: TYPE_NORMAL
- en: You shouldn’t use mutable values, such as `{}`, for Python function defaults.
    See [this Python guide](https://oreil.ly/Yv6bX) for an explanation of this gotcha.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_getting_data_off_the_web__span_class__keep_together__with_python__span__CO2-2)'
  prefs: []
  type: TYPE_NORMAL
- en: We first use a Python list comprehension and the `join` method to create a list
    of dimensions, with members concatenated with plus signs (e.g., [*USA+AUS*, …​
    ]). `join` is then used again to concatenate the members of `dim_str` with periods.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_getting_data_off_the_web__span_class__keep_together__with_python__span__CO2-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Note that `requests`’ `get` can take a parameter dictionary as its second argument,
    using it to make the URL query string.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use this function like so, to grab economic data for the USA and Australia
    from 2009 to 2010:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, to look at the data, we just check that the response is OK and have a
    look at the dictionary keys:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The resulting JSON data is in the [SDMX format](https://oreil.ly/HeE7G), designed
    to facilitate the communication of statistical data. It’s not the most intuitive
    format around, but it’s often the case that datasets have a less than ideal structure.
    The good news is that Python is a great language for knocking data into shape.
    For Python’s [pandas library](https://pandas.pydata.org) (see [Chapter 8](ch08.xhtml#chapter_intro_to_pandas)),
    there is [pandaSDMX](https://oreil.ly/2PKxZ), which currently handles the XML-based
    format.
  prefs: []
  type: TYPE_NORMAL
- en: The OECD API is essentially RESTful with all of the query being contained in
    the URL and the HTTP verb GET specifying a fetch operation. If a specialized Python
    library isn’t available to use the API (e.g., Tweepy for Twitter), then you’ll
    probably end up writing something like [Example 5-1](#oecd_request). Requests
    is a very friendly, well-designed library and can cope with pretty much all the
    manipulations required to use a web API.
  prefs: []
  type: TYPE_NORMAL
- en: Getting Country Data for the Nobel Dataviz
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are some national statistics that will come in handy for the Nobel Prize
    visualization we’re using our toolchain to build. Population sizes, three-letter
    international codes (e.g., GDR, USA), and geographic centers are potentially useful
    when you are visualizing an international prize and its distribution. [REST countries](https://restcountries.com)
    is a handy RESTful web resource with various international stats. Let’s use it
    to grab some data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Requests to REST countries take the following form:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'As with the OECD API (see [Example 5-1](#oecd_request)), we can make a simple
    calling function to allow easy access to the API’s data, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_getting_data_off_the_web__span_class__keep_together__with_python__span__CO3-1)'
  prefs: []
  type: TYPE_NORMAL
- en: It’s usually a good idea to specify a valid `User-Agent` in the header of your
    request. Some sites will reject the request otherwise.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_getting_data_off_the_web__span_class__keep_together__with_python__span__CO3-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Before returning the response, make sure it has an OK (200) HTTP code; otherwise,
    raise an exception with a helpful message.
  prefs: []
  type: TYPE_NORMAL
- en: 'With the `REST_country_request` function in hand, let’s get a list of all the
    countries using the US dollar as currency:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The full dataset at REST countries is pretty small, so for convenience we’ll
    store a copy as a JSON file. We’ll be using this in later chapters in both exploratory
    and presentational dataviz:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Now that we’ve rolled a couple of our own API consumers, let’s take a look at
    some dedicated libraries that wrap some of the larger web APIs in an easy-to-use
    form.
  prefs: []
  type: TYPE_NORMAL
- en: Using Libraries to Access Web APIs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Requests is capable of negotiating with pretty much all web APIs, but as the
    APIs start adding authentication and the data structures become more complicated,
    a good wrapper library can save a lot of hassle and reduce the tedious bookkeeping.
    In this section, I’ll cover a couple of the more popular [wrapper libraries](https://oreil.ly/DBrZ8)
    to give you a feel for the workflow and some useful starting points.
  prefs: []
  type: TYPE_NORMAL
- en: Using Google Spreadsheets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It’s becoming more common these days to have live datasets *in the cloud*. So,
    for example, you might find yourself required to visualize aspects of a Google
    spreadsheet that is the shared data pool for a group. My preference is to get
    this data out of the Google-plex and into pandas to start exploring it (see [Chapter 11](ch11.xhtml#chapter_pandas_exploring)),
    but a good library will let you access and adapt the data *in place*, negotiating
    the web traffic as required.
  prefs: []
  type: TYPE_NORMAL
- en: '[*gspread*](https://oreil.ly/DNKYT) is the best known Python library for accessing
    Google spreadsheets and makes doing so a relative breeze.'
  prefs: []
  type: TYPE_NORMAL
- en: You’ll need [OAuth 2.0](https://oreil.ly/z3u6y) credentials to use the API.^([3](ch05.xhtml#idm45607788534000))
    The most up-to-date guide can be found on the [Google Developers site](https://oreil.ly/tnO3b).
    Following those instructions should provide a JSON file containing your private
    key.
  prefs: []
  type: TYPE_NORMAL
- en: 'You’ll need to install *gspread* and the latest *google-auth* client library.
    Here’s how to do it with `pip`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Depending on your system, you may also need pyOpenSSL:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Read [the docs](https://oreil.ly/1xAPm) for more details and troubleshooting.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Google’s API assumes that the spreadsheets you are trying to access are owned
    or shared by your API account, not your personal one. The email address to share
    the spreadsheet with is available at your [Google developers console](https://oreil.ly/z5KyM)
    and in the JSON credentials key needed to use the API. It should look something
    like *account-1@My Project…​iam.gserviceaccount.com*.
  prefs: []
  type: TYPE_NORMAL
- en: With those libraries installed, you should be able to access any of your spreadsheets
    with just a few lines. I’m using the [Microbe-scope spreadsheet](https://oreil.ly/AAj9X).
    [Example 5-2](#gspread_access) shows how to load the spreadsheet.
  prefs: []
  type: TYPE_NORMAL
- en: Example 5-2\. Opening a Google spreadsheet
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_getting_data_off_the_web__span_class__keep_together__with_python__span__CO4-1)'
  prefs: []
  type: TYPE_NORMAL
- en: The JSON credentials file is the one provided by Google services, usually of
    the form *My Project-b8ab5e38fd68.json*.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_getting_data_off_the_web__span_class__keep_together__with_python__span__CO4-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Here we’re opening the spreadsheet by name. Alternatives are `open_by_url` or
    `open_by_id`. See [the `gspread` documentation](https://oreil.ly/sa4sa) for details.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we’ve got our spreadsheet, we can see the worksheets it contains:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'With the worksheet `bugs` selected from the spreadsheet, `gspread` allows you
    to access and change column, row, and cell values (assuming the sheet isn’t read-only).
    So we can get the values in the second column with the `col_values` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If you get a `BadStatusLine` error while accessing a Google spreadsheet with
    `gspread`, it is probably because the session has expired. Reopening the spreadsheet
    should get things working again. This [outstanding `gspread` issue](https://oreil.ly/xTGg9)
    provides more information.
  prefs: []
  type: TYPE_NORMAL
- en: 'Although you can use *gspread*’s API to plot directly, using a plot library
    like Matplotlib, I prefer to send the whole sheet to pandas, Python’s powerhouse
    programmatic spreadsheet. This is easily achieved with `gspread`’s `get_all_records`,
    which returns a list of item dictionaries. This list can be used directly to initialize
    a pandas DataFrame (see [“The DataFrame”](ch08.xhtml#pandas_objects)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: In [Chapter 11](ch11.xhtml#chapter_pandas_exploring), we’ll see how to interactively
    explore a DataFrame’s data.
  prefs: []
  type: TYPE_NORMAL
- en: Using the Twitter API with Tweepy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The advent of social media has generated a lot of data and an interest in visualizing
    the social networks, trending hashtags, and media storms contained in them. Twitter’s
    broadcast network is probably the richest source of cool data visualizations,
    and its API provides tweets^([4](ch05.xhtml#idm45607788104048)) filtered by user,
    hashtag, date, and the like.
  prefs: []
  type: TYPE_NORMAL
- en: Python’s Tweepy is an easy-to-use Twitter library that provides a number of
    useful features, such as a `StreamListener` class for streaming live Twitter updates.
    To start using it, you’ll need a Twitter access token, which you can acquire by
    following the instructions [at the Twitter docs](https://oreil.ly/ZkWNf) to create
    your Twitter application. Once this application is created, you can get the keys
    and access tokens for your app by clicking on the link [at your Twitter app page](https://apps.twitter.com).
  prefs: []
  type: TYPE_NORMAL
- en: 'Tweepy typically requires the four authorization elements shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'With those defined, accessing tweets could hardly be easier. Here we create
    an OAuth `auth` object using our tokens and keys and use it to start an API session.
    We can then grab the latest tweets from our timeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Tweepy’s `API` class offers a lot of convenience methods, which you can check
    out [in the Tweepy docs](https://oreil.ly/2FTRw). A common visualization is using
    a network graph to show patterns of friends and followers among Twitter subpopulations.
    The Tweepy method `followers_ids` (get all users following) and `friends_ids`
    (get all users being followed) can be used to construct such a network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_getting_data_off_the_web__span_class__keep_together__with_python__span__CO5-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Gets a list of your followers’ IDs (e.g., `[1191701545, 1554134420, …​]`).
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_getting_data_off_the_web__span_class__keep_together__with_python__span__CO5-2)'
  prefs: []
  type: TYPE_NORMAL
- en: The first argument to `follower_ids` can be a user ID or screen name.
  prefs: []
  type: TYPE_NORMAL
- en: Note that you will probably run into rate-limit errors if you try and construct
    a network for anyone with more than a hundred followers (see [this Stack Overflow
    thread](https://oreil.ly/1KDH2) for an explanation). To overcome this you will
    need to implement some basic rate limiting to reduce your request count to 180
    per 15 minutes. Alternatively, you can pay Twitter for a premium account.
  prefs: []
  type: TYPE_NORMAL
- en: By mapping followers of followers, you can create a network of connections that
    might just reveal something interesting about groups and subgroups clustered about
    a particular individual or subject. There’s a nice example of just such a Twitter
    analysis on [Gabe Sawhney’s blog](https://oreil.ly/sWH99).
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the coolest features of Tweepy is its `StreamListener` class, which
    makes it easy to collect and process filtered tweets in real time. Live updates
    of Twitter streams have been used by many memorable visualizations (see these
    examples from [FlowingData](https://oreil.ly/mNOYX) and [DensityDesign](https://oreil.ly/ZpmLq)
    for some inspiration). Let’s set up a little stream to record tweets mentioning
    Python, JavaScript, and dataviz. We’ll just print the results to the screen (in
    `on_data`) here, but you would normally cache them in a file or database (or do
    both with SQLite):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we’ve had a taste of the kind of APIs you might run into during your
    search for interesting data, let’s look at the primary technique you’ll use if,
    as is often the case, no one is providing the data you want in a neat, user-friendly
    form: scraping data with Python.'
  prefs: []
  type: TYPE_NORMAL
- en: Scraping Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Scraping is the chief metaphor used for the practice of getting data that wasn’t
    designed to be programmatically consumed off the web. It is a pretty good metaphor
    because scraping is often about getting the balance right between removing too
    much and too little. Creating procedures that extract just the right data, as
    cleanly as possible, from web pages is a craft skill and often a fairly messy
    one at that. But the payoff is access to visualizable data that often cannot be
    acquired in any other way. Approached in the right way, scraping can even have
    an intrinsic satisfaction.
  prefs: []
  type: TYPE_NORMAL
- en: Why We Need to Scrape
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In an ideal virtual world, online data would be organized in a library, with
    everything cataloged through a sophisticated Dewey decimal system for the web
    page. Unfortunately for the keen data hunter, the web has grown organically, often
    unconstrained by considerations of easy data access for the budding data visualizer.
    So, in reality, the web resembles a big mound of data, some of it clean and usable
    (and thankfully this percentage is increasing) but much of it poorly formed and
    designed for human consumption. And humans are able to parse the kind of messy,
    poorly formed data that our relatively dumb computers have problems with.^([5](ch05.xhtml#idm45607787727008))
  prefs: []
  type: TYPE_NORMAL
- en: Scraping is about fashioning selection patterns that grab the data we want and
    leave the rest behind. If we’re lucky, the web pages containing the data will
    have helpful pointers, like named tables, specific identities in preference to
    generic classes, and so on. If we’re unlucky, then these pointers will be missing
    and we will have to resort to using other patterns or, in the worst case, ordinal
    specifiers such as *third table in the main div*. These are obviously pretty fragile,
    and will break if somebody adds a table above the third.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we’ll tackle a little scraping task, to get the same Nobel
    Prize winners data. We’ll use Python’s best-of-breed Beautiful Soup for this lightweight
    scraping foray, saving the heavy guns of Scrapy for the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The fact that data and images are on the web does not mean that they are necessarily
    free to use. For our scraping examples we’ll be using Wikipedia, which allows
    full reuse under the [Creative Commons license](https://oreil.ly/jBTaC). It’s
    a good idea to make sure anything you scrape is available and, if in doubt, contact
    the site maintainer. You may be required to at least cite the original author.
  prefs: []
  type: TYPE_NORMAL
- en: Beautiful Soup and lxml
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Python’s key lightweight scraping tools are Beautiful Soup and lxml. Their primary
    selection syntax is different but, confusingly, each can use the other’s parsers.
    The consensus seems to be that lxml’s parser is considerably faster, but Beautiful
    Soup’s might be more robust when dealing with poorly formed HTML. Personally,
    I’ve found lxml to be robust enough and its syntax, based on [xpaths](https://oreil.ly/A43cY),
    more powerful and often more intuitive. I think for someone coming from web development,
    familiar with CSS and jQuery, selection based on CSS selectors is much more natural.
    Depending on your system, lxml is usually the default parser for Beautiful Soup.
    We’ll be using it in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: 'Beautiful Soup is part of the Anaconda packages (see [Chapter 1](ch01.xhtml#chapter_install))
    and easily installed with `pip`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: A First Scraping Foray
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Armed with Requests and Beautiful Soup, let’s give ourselves a little task to
    get the names, years, categories, and nationalities of all the Nobel Prize winners.
    We’ll start at the [main Wikipedia Nobel Prize page](https://oreil.ly/cSFFW).
    Scrolling down shows a table with all the laureates by year and category, which
    is a good start to our minimal data requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Some kind of HTML explorer is pretty much a must for web scraping, and the best
    I know is Chrome’s web developer’s Elements tab (see [“The Elements Tab”](ch04.xhtml#chrome_elements)).
    [Figure 5-1](#wp_nobel) shows the key elements involved in quizzing a web page’s
    structure. We need to know how to select the data of interest, in this case a
    Wikipedia table, while avoiding other elements on the page. Crafting good selector
    patterns is the key to effective scraping, and highlighting the DOM element using
    the element inspector gives us both the CSS pattern and, with a right-click, the
    xpath. The latter is a particularly powerful syntax for DOM element selection
    and the basis of our industrial-strength scraping solution, Scrapy.
  prefs: []
  type: TYPE_NORMAL
- en: '![dpj2 0501](assets/dpj2_0501.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5-1\. Wikipedia’s main Nobel Prize page: A and B show the wikitable’s
    CSS selector. Right-clicking and selecting C (Copy XPath) gives the table’s xpath
    (`//*[@id="mw-content-text"]/table[1]`). D shows a `thead` tag generated by jQuery.'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Getting the Soup
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The first thing you need to do before scraping the web page of interest is
    to parse it with Beautiful Soup, converting the HTML into a tag tree hierarchy
    or soup:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_getting_data_off_the_web__span_class__keep_together__with_python__span__CO6-1)'
  prefs: []
  type: TYPE_NORMAL
- en: The second argument specifies the parser we want to use, namely lxml’s.
  prefs: []
  type: TYPE_NORMAL
- en: With our soup in hand, let’s see how to find our target tags.
  prefs: []
  type: TYPE_NORMAL
- en: Selecting Tags
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Beautiful Soup offers a few ways to select tags from the parsed soup, with
    subtle differences that can be confusing. Before demonstrating the selection methods,
    let’s get the soup of our Nobel Prize page:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Our target table (see [Figure 5-1](#wp_nobel)) has two defining classes, `wikitable`
    and `sortable` (there are some unsortable tables on the page). We can use Beautiful
    Soup’s `find` method to find the first table tag with those classes. `find` takes
    a tag name as its first argument and a dictionary with class, ID, and other identifiers
    as its second:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Although we have successfully found our table by its classes, this method is
    not very robust. Let’s see what happens when we change the order of our CSS classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: So `find` cares about the order of the classes, using the class string to find
    the tag. If the classes were specified in a different order—​something that might
    well happen during an HTML edit, then the `find` fails. This fragility makes it
    difficult to recommend the Beautiful Soup selectors, such as `find` and `find_all`.
    When doing quick hacking, I find lxml’s [CSS selectors](https://lxml.de/cssselect.xhtml)
    easier and more intuitive.
  prefs: []
  type: TYPE_NORMAL
- en: Using the soup’s `select` method (available if you specified the lxml parser
    when creating it), you can specify an HTML element using its CSS class, ID, and
    so on. This CSS selector is converted into the xpath syntax lxml uses internally.^([6](ch05.xhtml#idm45607787437712))
  prefs: []
  type: TYPE_NORMAL
- en: 'To get our wikitable, we just select a table in the soup, using the dot notation
    to indicate its classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that `select` returns an array of results, finding all the matching tags
    in the soup. lxml provides the `select_one` convenience method if you are selecting
    just one HTML element. Let’s grab our Nobel table and see what headers it has:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'As a shorthand for `select`, you can call the tag directly on the soup; so
    these two are equivalent:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: With lxml’s parser, Beautiful Soup provides a number of different filters for
    finding tags, including the simple string name we’ve just used, searching by [regular
    expression](https://oreil.ly/GeU8Q), using a list of tag names, and more. See
    this [comprehensive list](https://oreil.ly/iBQwc) for more details.
  prefs: []
  type: TYPE_NORMAL
- en: As well as lxml’s `select` and `select_one`, there are 10 BeautfulSoup convenience
    methods for searching the parsed tree. These are essentially variants on `find`
    and `find_all` that specify which parts of the tree they search. For example,
    `find_parent` and `find_parents`, rather than looking for descendants down the
    tree, look for parent tags of the tag being searched. All 10 methods are available
    in the Beautiful Soup [official docs](https://oreil.ly/oPrQl).
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know how to select our Wikipedia table and are armed with lxml’s
    selection methods, let’s see how to craft some selection patterns to get the data
    we want.
  prefs: []
  type: TYPE_NORMAL
- en: Crafting Selection Patterns
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Having successfully selected our data table, we now want to craft some selection
    patterns to scrape the required data. Using the HTML explorer, you can see that
    the individual winners are contained in `<td>` cells, with an href `<a>` link
    to Wikipedia’s bio pages (in the case of individuals). Here’s a typical target
    row with CSS classes that we can use as targets to get the data in the `<td>`
    cells:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: If we loop through these data cells, keeping track of their row (year) and column
    (category), then we should be able to create a list of winners with all the data
    we specified except nationality.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following `get_column_titles` function scrapes our table for the Nobel
    category column headers, ignoring the first Year column. Often the header cell
    in a Wikipedia table contains a web-linked `''a''` tag; all the Nobel categories
    fit this model, pointing to their respective Wikipedia pages. If the header is
    not clickable, we store its text and a null href:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_getting_data_off_the_web__span_class__keep_together__with_python__span__CO7-1)'
  prefs: []
  type: TYPE_NORMAL
- en: We loop through the table head, ignoring the first Year column ([1:]). This
    selects the column headers shown in [Figure 5-2](#scraping_nobel_table).
  prefs: []
  type: TYPE_NORMAL
- en: '![dpj2 0502](assets/dpj2_0502.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-2\. Wikipedia’s table of Nobel Prize winners
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Let’s make sure `get_column_titles` is giving us what we want:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_getting_data_off_the_web__span_class__keep_together__with_python__span__CO8-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Gets all the Year rows, starting from the second, corresponding to the rows
    in [Figure 5-2](#scraping_nobel_table).
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_getting_data_off_the_web__span_class__keep_together__with_python__span__CO8-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Finds the `<td>` data cells shown in [Figure 5-2](#scraping_nobel_table).
  prefs: []
  type: TYPE_NORMAL
- en: Iterating through the Year rows, we take the first Year column and then iterate
    over the remaining columns, using `enumerate` to keep track of our index, which
    will map to the category column names. We know that all the winner names are contained
    in an `<a>` tag but that there are occasional extra `<a>` tags beginning with
    `#endnote`, which we filter for. Finally we append a year, category, name, and
    link dictionary to our data array. Note that the winner selector has an `attrs`
    dictionary containing, among other things, the `<a>` tag’s href.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s confirm that `get_Nobel_winners` delivers a list of Nobel Prize winner
    dictionaries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have the full list of Nobel Prize winners and links to their Wikipedia
    pages, we can use these links to scrape data from the individuals’ biographies.
    This will involve making a largish number of requests, and it’s not something
    we really want to do more than once. The sensible and respectful^([7](ch05.xhtml#idm45607786692608))
    thing is to cache the data we scrape, allowing us to try out various scraping
    experiments without returning to Wikipedia.
  prefs: []
  type: TYPE_NORMAL
- en: Caching the Web Pages
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It’s easy enough to rustle up a quick cacher in Python, but as often as not
    it’s easier still to find a better solution written by someone else and kindly
    donated to the open source community. Requests has a nice plug-in called `requests-cache`
    that, with a few lines of configuration, will take care of all your basic caching
    needs.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we install the plug-in using `pip`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: '`requests-cache` uses [monkey patching](https://oreil.ly/8IklZ) to dynamically
    replace parts of the `requests` API at runtime. This means it can work transparently.
    You just have to install its cache and then use `requests` as usual, with all
    the caching being taken care of. Here’s the simplest way to use `requests-cache`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'The `install_cache` method has a number of useful options, including allowing
    you to specify the cache `backend` (`sqlite`, `memory`, `mongdb`, or `redis`)
    or set an expiry time (`expiry_after`) in seconds on the caching. So the following
    creates a cache named `nobel_pages` with an `sqlite` backend and pages that expire
    in two hours (7,200 s):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: '`requests-cache` will serve most of your caching needs and couldn’t be much
    easier to use. For more details, see [the official docs](https://oreil.ly/d67bK)
    where you’ll also find a little example of request throttling, which is a useful
    technique when doing bulk scraping.'
  prefs: []
  type: TYPE_NORMAL
- en: Scraping the Winners’ Nationalities
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With caching in place, let’s try getting the winners’ nationalities, using the
    first 50 for our experiment. A little `get_winner_nationality()` function will
    use the winner links we stored earlier to scrape their page and then use the info-box
    shown in [Figure 5-3](#country_bio) to get the `Nationality` attribute.
  prefs: []
  type: TYPE_NORMAL
- en: '![dpj2 0503](assets/dpj2_0503.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-3\. Scraping a winner’s nationality
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: When scraping, you are looking for reliable patterns and repeating elements
    with useful data. As we’ll see, the Wikipedia info-boxes for individuals are not
    such a reliable source, but clicking on a few random links certainly gives that
    impression. Depending on the size of the dataset, it’s good to perform a few experimental
    sanity checks. You can do this manually but, as mentioned at the start of the
    chapter, this won’t scale or improve your craft skills.
  prefs: []
  type: TYPE_NORMAL
- en: '[Example 5-3](#get_winner_nationality) takes one of the winner dictionaries
    we scraped earlier and returns a name-labeled dictionary with a `Nationality`
    key if one is found. Let’s run it on the first 50 winners and see how often a
    `Nationality` attribute is missing.'
  prefs: []
  type: TYPE_NORMAL
- en: Example 5-3\. Scraping the winner’s country from their biography page
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_getting_data_off_the_web__span_class__keep_together__with_python__span__CO9-1)'
  prefs: []
  type: TYPE_NORMAL
- en: We use a CSS selector to find all the `<tr>` rows of the table with class `infobox`.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_getting_data_off_the_web__span_class__keep_together__with_python__span__CO9-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Cycles through the rows looking for a Nationality field.
  prefs: []
  type: TYPE_NORMAL
- en: '[Example 5-4](#nationality_test) shows that 14 of the 50 first winners failed
    our attempt to scrape their nationality. In the case of the Institut de Droit
    International, national affiliation may well be moot, but Theodore Roosevelt is
    about as American as they come. Clicking on a few of the names shows the problem
    (see [Figure 5-4](#missing_nationality)). The lack of a standardized biography
    format means synonyms for *Nationality* are often employed, as in Marie Curie’s
    *Citizenship*; sometimes no reference is made, as with Niels Finsen; and Randall
    Cremer has nothing but a photograph in his info-box. We can discard the info-boxes
    as a reliable source of winners’ nationalities but, as they appeared to be the
    only regular source of potted data, this sends us back to the drawing board. In
    the next chapter, we’ll see a successful approach using Scrapy and a different
    start page.'
  prefs: []
  type: TYPE_NORMAL
- en: Example 5-4\. Testing for scraped nationalities
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: '![dpj2 0504](assets/dpj2_0504.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-4\. Winners without a recorded *nationality*
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Although Wikipedia is a relative free-for-all, production-wise, where data is
    designed for human consumption, you can expect a lack of rigor. Many sites have
    similar gotchas and as the datasets get bigger, more tests may be needed to find
    the flaws in a collection pattern.
  prefs: []
  type: TYPE_NORMAL
- en: Although our first scraping exercise was a little artificial in order to introduce
    the tools, I hope it captured something of the slightly messy spirit of web scraping.
    The ultimately abortive pursuit of a reliable Nationality field for our Nobel
    dataset could have been forestalled by a bit of web browsing and manual HTML-source
    trawling. However, if the dataset were significantly larger and the failure rate
    a bit smaller, then programmatic detection, which gets easier and easier as you
    become acquainted with the scraping modules, really starts to deliver.
  prefs: []
  type: TYPE_NORMAL
- en: This little scraping test was designed to introduce Beautiful Soup, and shows
    that collecting the data we seek requires a little more thought, which is often
    the case with scraping. In the next chapter, we’ll wheel out the big gun, Scrapy,
    and, with what we’ve learned in this section, harvest the data we need for our
    Nobel Prize visualization.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we’ve seen examples of the most common ways in which data can
    be sucked out of the web and into Python containers, databases, or pandas datasets.
    Python’s Requests library is the true workhorse of HTTP negotiation and a fundamental
    tool in our dataviz toolchain. For simpler, RESTful APIs, consuming data with
    Requests is a few lines of Python away. For the more awkward APIs, such as those
    with potentially complicated authorization, a wrapper library like Tweepy (for
    Twitter) can save a lot of hassle. Decent wrappers can also keep track of access
    rates and, where necessary, throttle your requests. This is a key consideration,
    particularly when there is the possibility of blacklisting unfriendly consumers.
  prefs: []
  type: TYPE_NORMAL
- en: We also started our first forays into data scraping, which is often a necessary
    fallback where no API exists and the data is for human consumption. In the next
    chapter, we’ll get all the Nobel Prize data needed for the book’s visualization
    using Python’s Scrapy, an industrial-strength scraping library.
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch05.xhtml#idm45607789542496-marker)) This is actually a [deliberate policy](https://oreil.ly/WOjdB)
    of the developers.
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch05.xhtml#idm45607789528976-marker)) There are some platform dependencies
    that might still generate errors. This [Stack Overflow thread](https://oreil.ly/Zm082)
    is a good starting point if you still have problems.
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](ch05.xhtml#idm45607788534000-marker)) OAuth1 access has been deprecated
    recently.
  prefs: []
  type: TYPE_NORMAL
- en: ^([4](ch05.xhtml#idm45607788104048-marker)) The free API is currently limited
    to around [350 requests per hour](https://oreil.ly/LKzJX).
  prefs: []
  type: TYPE_NORMAL
- en: ^([5](ch05.xhtml#idm45607787727008-marker)) Much of modern machine learning
    and artificial intelligence (AI) research is dedicated to creating computer software
    that can cope with messy, noisy, fuzzy, informal data but, as of this book’s publication,
    there’s no off-the-shelf solution I know of.
  prefs: []
  type: TYPE_NORMAL
- en: ^([6](ch05.xhtml#idm45607787437712-marker)) This CSS selection syntax should
    be familiar to anyone who’s used JavaScript’s [jQuery library](https://jquery.com)
    and is also similar to that used by [D3](https://d3js.org).
  prefs: []
  type: TYPE_NORMAL
- en: ^([7](ch05.xhtml#idm45607786692608-marker)) When scraping, you’re using other
    people’s web bandwidth, which ultimately costs them money. It’s just good manners
    to try to limit your number of requests.
  prefs: []
  type: TYPE_NORMAL
