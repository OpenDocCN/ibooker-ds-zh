<html><head></head><body>
<div id="sbo-rt-content"><div class="readable-text" id="p1">
<h1 class="readable-text-h1"><span class="chapter-title-numbering"><span class="num-string">8</span></span> <span class="chapter-title-text">Time series data: Data preparation </span></h1>
</div>
<div class="introduction-summary">
<h3 class="introduction-header sigil_not_in_toc">This chapter covers</h3>
<ul>
<li class="readable-text" id="p2">Preparing time series data for analysis </li>
<li class="readable-text" id="p3">Determining what subset of time series data to use</li>
<li class="readable-text" id="p4">Cleaning time series data by handling gaps and missing values</li>
<li class="readable-text" id="p5">Analyzing patterns in time series data</li>
</ul>
</div>
<div class="readable-text" id="p6">
<p>Most datasets you will come across have a time component. If the process to generate the data involves taking the same measurement at recurring intervals, the data is called <em>time series data</em>. An example is measuring the yearly GDP of a country or the output of machinery in a production line. However, even something seemingly static, such as a customer database, has a time component if we look at the date customer records were created. We might not explicitly think of the data as a time series, but using this time component allows us to unlock additional insights in our data. For example, you could analyze the rate at which new customer records are being created or what times of the day your operations team are inputting data into the database.</p>
</div>
<div class="callout-container sidebar-container">
<div class="readable-text" id="p7">
<h5 class="callout-container-h5 readable-text-h5 sigil_not_in_toc">Real business case: Forecasting</h5>
</div>
<div class="readable-text" id="p8">
<p>The project covered by chapters 8 and 9 was inspired by multiple forecasting projects I have worked on and the fact that most data analysis curricula spend proportionally little time on the topic of working with temporal data.</p>
</div>
<div class="readable-text" id="p9">
<p>In late 2020, I had to provide forecasts for where the used car market was heading after the initial COVID lockdown restrictions were lifted in the United Kingdom. Accurately forecasting an entire market is hard enough, but the added complexity of having patchy data for the lockdown period and having to forecast in a scenario no one had encountered before made this project particularly difficult.</p>
</div>
<div class="readable-text" id="p10">
<p>In the end, we arrived at a prediction using a combination of fundamental forecasting principles, some concrete assumptions about the pandemic, and domain knowledge from our experts. It was a good example of a project where technical skills weren’t enough to solve the business problem.</p>
</div>
</div>
<div class="readable-text" id="p11">
<p>Working with time series data is more than knowing how to work with date formats. It involves extracting time-related components from data, handling time data at different resolutions, handling gaps, forecasting into the future, and working out whether the data can be forecasted at all.</p>
</div>
<div class="readable-text intended-text" id="p12">
<p>Knowing how to extract temporal patterns from your data is a vital skill in the real world and one that we will practice in this chapter through the project.</p>
</div>
<div class="readable-text" id="p13">
<h2 class="readable-text-h2" id="sigil_toc_id_99"><span class="num-string">8.1</span> Working with time series data</h2>
</div>
<div class="readable-text" id="p14">
<p>A time series is a repeated measurement taken at different, ideally uniform, time intervals. A typical tabular dataset, such as a customer dataset, will contain one row per customer, and each column will represent a different property of a customer, such as age, employment status, address, and so forth. A time series, however, typically contains fewer columns: one to represent the date of a measurement and one or more columns to represent the individual measurement value at that time. Each row, therefore, represents the same measurement, and it is the measurement time that makes each row unique.</p>
</div>
<div class="readable-text" id="p15">
<h3 class="readable-text-h3" id="sigil_toc_id_100"><span class="num-string">8.1.1</span> The hidden depth of time series data</h3>
</div>
<div class="readable-text" id="p16">
<p>Let’s take a simple example—customer satisfaction over time. Imagine one of those smiley-face-based satisfaction surveys you can find at airports, supermarket checkouts, or any other public place. As a customer walks by, they can press a smiley face to indicate their level of satisfaction. They see smiley faces, and the database records a simple value from 1 to 5 on a Likert scale to measure the value from most dissatisfied to most satisfied. Table 8.1 shows an example of what this dataset might look like.</p>
</div>
<div class="browsable-container browsable-table-container framemaker-table-container" id="p17">
<h5 class="browsable-container-h5 sigil_not_in_toc"><span class="num-string">Table 8.1</span> An example of a time series dataset</h5>
<table>
<thead>
<tr>
<th>
<div>
         Date 
       </div></th>
<th>
<div>
         Satisfaction score 
       </div></th>
</tr>
</thead>
<tbody>
<tr>
<td>  2023-11-01 11:03:55 <br/></td>
<td>  4 <br/></td>
</tr>
<tr>
<td>  2023-11-01 11:17:02 <br/></td>
<td>  5 <br/></td>
</tr>
<tr>
<td>  2023-11-01 13:41:11 <br/></td>
<td>  3 <br/></td>
</tr>
<tr>
<td>  2023-11-01 14:06:43 <br/></td>
<td>  4 <br/></td>
</tr>
</tbody>
</table>
</div>
<div class="readable-text intended-text" id="p18">
<p>To capture satisfaction over time, you just need a timestamp and the satisfaction value. If you had such a system set up across multiple locations, you might also find a location ID column, but the data wouldn’t be more complex than that.</p>
</div>
<div class="readable-text" id="p19">
<p>What kind of analysis could we do with what is, at first glance, a very simple dataset? We could</p>
</div>
<ul>
<li class="readable-text" id="p20"> Use clusters of rows as a proxy to identify busier periods </li>
<li class="readable-text" id="p21"> Calculate average satisfaction over time at various levels of granularity (daily, weekly, monthly, etc.) </li>
<li class="readable-text" id="p22"> Investigate trends and seasonal patterns in the data (if customers are more satisfied at different points of the day or different days of the week) </li>
<li class="readable-text" id="p23"> Find anomalies where satisfaction rose or dropped to unexpected levels </li>
<li class="readable-text" id="p24"> Cross-reference this with other data to identify how satisfaction relates to external factors, such as special events </li>
<li class="readable-text" id="p25"> Compare satisfaction scores across different locations where that data is available </li>
</ul>
<div class="readable-text" id="p26">
<p>The fact that most of these questions can be answered with just two to three columns of data shows the hidden depth that time series data can have.</p>
</div>
<div class="readable-text" id="p27">
<h3 class="readable-text-h3" id="sigil_toc_id_101"><span class="num-string">8.1.2</span> How to work with time series data</h3>
</div>
<div class="readable-text" id="p28">
<p>What does it mean to work with time series? When exploring time data, we care about a lot of the same things as when exploring tabular data. We want to understand each column, ensure data types are consistent, and check for missing values. However, there are also specific considerations for time data:</p>
</div>
<ul>
<li class="readable-text" id="p29"> What is the granularity of the time series? Is it consistent? </li>
<li class="readable-text" id="p30"> Are there any gaps in the time series? Are these gaps there by design? </li>
<li class="readable-text" id="p31"> Is there a trend in the data? </li>
<li class="readable-text" id="p32"> Are there seasonal patterns? </li>
<li class="readable-text" id="p33"> Are there any outliers worth investigating? </li>
</ul>
<div class="readable-text" id="p34">
<p>Once you have explored your time series dataset and want to proceed to forecasting, there are additional considerations:</p>
</div>
<ul>
<li class="readable-text" id="p35"> What is the right granularity for forecasting? This will depend partly on how much noise there is in the data. Hourly data might be too noisy, and although daily averages might be smoother and easier to forecast, there might not be enough data at a daily level. </li>
<li class="readable-text" id="p36"> Does the time series contain autocorrelation: do past values inform future values? There are statistical tests for this, and time series models will take advantage of this property. </li>
<li class="readable-text" id="p37"> Is the time series stationary? Some time series models require the data to be stationary, which means having a roughly constant mean and variance over time. In reality, a lot of time series have some trend and seasonality, so we either need to handle those directly or use forecasting models that take care of them for us. </li>
<li class="readable-text" id="p38"> Can any outliers be explained by external factors? For example, are certain spikes in your sales data due to one-off special sale days, like Black Friday? These external factors, technically “exogenous variables,” can be used in many forecasting models to improve predictions. </li>
</ul>
<div class="readable-text" id="p39">
<p>A final note on forecasting: the most important question you should ask is, “How will the forecast be used?” This requires answering these questions:</p>
</div>
<ul>
<li class="readable-text" id="p40"> How often are forecasts required? </li>
<li class="readable-text" id="p41"> What is the required granularity of the forecasts? </li>
<li class="readable-text" id="p42"> How far into the future should the forecasts go? </li>
<li class="readable-text" id="p43"> What does an acceptable level of accuracy look like? </li>
<li class="readable-text" id="p44"> What is the value of an accurate forecast in business terms? What is, therefore, the return on investment of additional work to improve existing forecasts? </li>
<li class="readable-text" id="p45"> Will the data required by the forecasting model be available in time? </li>
</ul>
<div class="readable-text" id="p46">
<p>Answers to these questions will inform your analytical decisions at least as much as the technical considerations. When completing this project, and as with any project, you should always focus on the business outcomes you are trying to improve.</p>
</div>
<div class="readable-text" id="p47">
<h2 class="readable-text-h2" id="sigil_toc_id_102"><span class="num-string">8.2</span> Project 6: Analyzing time series to improve cycling infrastructure</h2>
</div>
<div class="readable-text" id="p48">
<p>Let’s look at the project in which we will analyze road traffic data to understand where cycling infrastructure should be improved. In this chapter, we will explore the available data and prepare it for analysis, which will happen in chapter 9.</p>
</div>
<div class="readable-text intended-text" id="p49">
<p>The data is available for you to attempt the project yourself at <a href="https://davidasboth.com/book-code">https://davidasboth.com/book-code</a>. You will find the data you can use for the project, as well as the example solution in the form of a Jupyter notebook.</p>
</div>
<div class="readable-text intended-text" id="p50">
<p>This project is all about using time series data to find answers to our business questions. As usual, we will start by looking at the problem statement, the data dictionary, the outputs we are aiming for, and what tools we need to tackle the problem. We will then formulate our action plan using the results-oriented framework before diving into the example solution.</p>
</div>
<div class="readable-text" id="p51">
<h3 class="readable-text-h3" id="sigil_toc_id_103"><span class="num-string">8.2.1</span> Problem statement</h3>
</div>
<div class="readable-text" id="p52">
<p>You have been hired to work on a new government initiative, Bikes4Britain, which aims to improve cycling infrastructure in the United Kingdom. The aim of the first phase of the project is to identify the most suitable places around the country to improve infrastructure for cyclists. Specifically, your stakeholders are looking for recommendations of places with either substantial existing or increasing cycling traffic.</p>
</div>
<div class="readable-text intended-text" id="p53">
<p>They want to start with open data sources and have identified the Department for Transport’s road traffic statistics (<a href="https://roadtraffic.dft.gov.uk">https://roadtraffic.dft.gov.uk</a>) as a way to measure cycling traffic across the country. This is the dataset we will use in this project to look for patterns and make recommendations.</p>
</div>
<div class="readable-text print-book-callout" id="p54">
<p><span class="print-book-callout-head">NOTE</span>  Data originally taken from <a href="https://roadtraffic.dft.gov.uk/downloads">https://roadtraffic.dft.gov.uk/downloads</a>. Thank you to the Department for Transport for making this data available under the Open Government Licence.</p>
</div>
<div class="readable-text" id="p55">
<p>The data we will use from the Department’s statistics is the raw count data. This is a record of raw counts of vehicles that passed a particular counting location at various times. Some of the datasets are too high-level, such as area-level annual summaries, and some of them are estimates, such as the estimated annual average daily flows data (AADFs). The raw count dataset contains data at the most granular level, and we can always aggregate it to higher levels (e.g., annual values) if needed.</p>
</div>
<div class="readable-text" id="p56">
<h3 class="readable-text-h3" id="sigil_toc_id_104"><span class="num-string">8.2.2</span> Data dictionary</h3>
</div>
<div class="readable-text" id="p57">
<p>Before we think further about the project, we should take a look at what data we have available. The data dictionary document (<a href="https://mng.bz/4ajw">https://mng.bz/4ajw</a>) is included in the project files, and table 8.2 shows the columns in detail. The data dictionary is shown as is, without modification from the original.</p>
</div>
<div class="browsable-container browsable-table-container framemaker-table-container" id="p58">
<h5 class="browsable-container-h5 sigil_not_in_toc"><span class="num-string">Table 8.2</span> The data dictionary, showing all column definitions</h5>
<table>
<thead>
<tr>
<th>
<div>
         Column name 
       </div></th>
<th>
<div>
         Definition 
       </div></th>
</tr>
</thead>
<tbody>
<tr>
<td> <code>Count_point_id</code> <br/></td>
<td>  A unique reference for the road link that links the AADFs to the road network <br/></td>
</tr>
<tr>
<td> <code>Direction_of_travel </code> <br/></td>
<td>  Direction of travel <br/></td>
</tr>
<tr>
<td> <code>Year</code> <br/></td>
<td>  Counts are shown for each year from 2000 onwards <br/></td>
</tr>
<tr>
<td> <code>Count_date</code> <br/></td>
<td>  The date when the actual count took place <br/></td>
</tr>
<tr>
<td> <code>Hour</code> <br/></td>
<td>  The time when the counts in question took place, where 7 represents between 7 a.m. and 8 a.m., and 17 represents between 5 p.m. and 6 p.m. <br/></td>
</tr>
<tr>
<td> <code>Region_id</code> <br/></td>
<td>  Website region identifier <br/></td>
</tr>
<tr>
<td> <code>Region_name</code> <br/></td>
<td>  The name of the region that the count point (CP) sits within <br/></td>
</tr>
<tr>
<td> <code>Region_ons_code</code> <br/></td>
<td>  The Office for National Statistics code identifier for the region <br/></td>
</tr>
<tr>
<td> <code>Local_authority_id</code> <br/></td>
<td>  Website local authority identifier <br/></td>
</tr>
<tr>
<td> <code>Local_authority_name</code> <br/></td>
<td>  The local authority that the CP sits within <br/></td>
</tr>
<tr>
<td> <code>Local_authority_code</code> <br/></td>
<td>  The Office for National Statistics code identifier for the local authority <br/></td>
</tr>
<tr>
<td> <code>Road_name</code> <br/></td>
<td>  The road name (for instance, M25 or A3) <br/></td>
</tr>
<tr>
<td> <code>Road_category</code> <br/></td>
<td>  The classification of the road type (see data definitions for the full list) <br/></td>
</tr>
<tr>
<td> <code>Road_type</code> <br/></td>
<td>  Whether the road is a major or minor road <br/></td>
</tr>
<tr>
<td> <code>Start_junction_road_name</code> <br/></td>
<td>  The road name of the start junction of the link <br/></td>
</tr>
<tr>
<td> <code>End_junction_road_name</code> <br/></td>
<td>  The road name of the end junction of the link <br/></td>
</tr>
<tr>
<td> <code>Easting</code> <br/></td>
<td>  Easting coordinates of the CP location <br/></td>
</tr>
<tr>
<td> <code>Northing</code> <br/></td>
<td>  Northing coordinates of the CP location <br/></td>
</tr>
<tr>
<td> <code>Latitude</code> <br/></td>
<td>  Latitude of the CP location <br/></td>
</tr>
<tr>
<td> <code>Longitude</code> <br/></td>
<td>  Longitude of the CP location <br/></td>
</tr>
<tr>
<td> <code>Link_length_km</code> <br/></td>
<td>  Total length of the network road link for that CP (in kilometers) <br/></td>
</tr>
<tr>
<td> <code>Link_length_miles</code> <br/></td>
<td>  Total length of the network road link for that CP (in miles) <br/></td>
</tr>
<tr>
<td> <code>Pedal_cycles</code> <br/></td>
<td>  Counts for pedal cycles <br/></td>
</tr>
<tr>
<td> <code>Two_wheeled_motor_vehicles</code> <br/></td>
<td>  Counts for two-wheeled motor vehicles <br/></td>
</tr>
<tr>
<td> <code>Cars_and_taxis</code> <br/></td>
<td>  Counts for cars and taxis <br/></td>
</tr>
<tr>
<td> <code>Buses_and_coaches </code> <br/></td>
<td>  Counts for buses and coaches <br/></td>
</tr>
<tr>
<td> <code>LGVs </code> <br/></td>
<td>  Counts for LGVs <br/></td>
</tr>
<tr>
<td> <code>HGVs_2_rigid_axle</code> <br/></td>
<td>  Counts for two-rigid axle HGVs <br/></td>
</tr>
<tr>
<td> <code>HGVs_3_rigid_axle</code> <br/></td>
<td>  Counts for three-rigid axle HGVs <br/></td>
</tr>
<tr>
<td> <code>HGVs_4_or_more_rigid_axle</code> <br/></td>
<td>  Counts for four or more rigid axle HGVs <br/></td>
</tr>
<tr>
<td> <code>HGVs_3_or_4_articulated_axle</code> <br/></td>
<td>  Counts for three- or four-articulated axle HGVs <br/></td>
</tr>
<tr>
<td> <code>HGVs_5_articulated_axle</code> <br/></td>
<td>  Counts for five-articulated axle HGVs <br/></td>
</tr>
<tr>
<td> <code>HGVs_6_articulated_axle</code> <br/></td>
<td>  Counts for six-articulated axle HGVs <br/></td>
</tr>
<tr>
<td> <code>All_HGVs</code> <br/></td>
<td>  Counts for all HGVs <br/></td>
</tr>
<tr>
<td> <code>All_motor_vehicles</code> <br/></td>
<td>  Counts for all motor vehicles <br/></td>
</tr>
</tbody>
</table>
</div>
<div class="readable-text" id="p59">
<p>The data dictionary shows that we have data about the day and time that vehicle counts were recorded. There is a column specifically recording the count of bicycles, as well as plenty of information about the stretch of road where the counting took place. We can already see that we will be able to look at vehicle counts at different locations as separate time series since we satisfy the definition of a time series by having both date and time information, as well as the same measurement taken at different time intervals.</p>
</div>
<div class="readable-text" id="p60">
<h3 class="readable-text-h3" id="sigil_toc_id_105"><span class="num-string">8.2.3</span> Desired outcomes</h3>
</div>
<div class="readable-text" id="p61">
<p>The output of the project is a recommendation of which area, or areas, to concentrate on for further analysis. These might be areas that already have a lot of high bicycle traffic, or they might be areas where cycling is on the rise or forecasted to have high cycling demand in the future. Our recommendation will likely contain suggested additional datasets we could incorporate to continue the analysis. There is likely more that we would like to know about these areas before any infrastructure work is undertaken, and we should outline this additional work to our stakeholders.</p>
</div>
<div class="readable-text intended-text" id="p62">
<p>As this project spans multiple chapters, the desired outcome of this chapter, which is the data preparation part, is a filtered and cleaned version of the raw data, ready to be analyzed. The outcome of chapter 9 will then be the results of the analysis and final recommendations.</p>
</div>
<div class="readable-text" id="p63">
<h3 class="readable-text-h3" id="sigil_toc_id_106"><span class="num-string">8.2.4</span> Required tools</h3>
</div>
<div class="readable-text" id="p64">
<p>As with most projects, your data analysis toolkit needs to read, explore, and visualize data to be suitable for the project. In the example solution, I use Python and the <code>pandas</code> and <code>matplotlib</code> libraries for data exploration and visualization, respectively. In the following chapter, I also introduce some time series functions from the <code>statsmodels</code> library when investigating time-specific aspects of the data and the <code>pmdarima</code> module for automatically choosing the best forecasting model. For this project, your tool should be able to</p>
</div>
<ul>
<li class="readable-text" id="p65"> Load a large dataset from a CSV or Excel file containing millions of rows </li>
<li class="readable-text" id="p66"> Perform basic data manipulation tasks, such as filtering, sorting, grouping, and reshaping data </li>
<li class="readable-text" id="p67"> Create data visualizations </li>
<li class="readable-text" id="p68"> Produce statistical analysis, specifically of time series data </li>
<li class="readable-text" id="p69"> Optionally create forecasts based on time series data </li>
</ul>
<div class="readable-text" id="p70">
<h2 class="readable-text-h2" id="sigil_toc_id_107"><span class="num-string">8.3</span> Applying the results-driven method to analyzing road traffic data</h2>
</div>
<div class="readable-text" id="p71">
<p>Let’s now see how we will address this problem in a results-driven way and formulate our action plan. We will follow the steps of the results-driven process to explore the data with our stakeholders’ requests and areas of interest in mind.</p>
</div>
<div class="browsable-container figure-container" id="p72">
<img alt="figure" height="175" src="../Images/8-unnumb-1.png" width="529"/>
</div>
<div class="readable-text" id="p73">
<p><em><span class="aframe-location"/></em>Do we fully understand the problem statement? Our stakeholders are interested in seeing how the number of cyclists has changed over time and across different areas. We know that’s the part of the data we will focus on. However, their request is not as specific as we might like.</p>
</div>
<div class="readable-text intended-text" id="p74">
<p>We need to define key terms, such as what it means for a place to be suitable for upgrading the cycling infrastructure. Is it somewhere where there are already a lot of cyclists? Or do we want to find places with potentially lower cycling traffic but where cycling is increasing the most over time? In that case, what are our criteria for “increasing”? If we can forecast our time series, we might even be able to make recommendations based on predicted future traffic.</p>
</div>
<div class="browsable-container figure-container" id="p75">
<img alt="figure" height="175" src="../Images/8-unnumb-2.png" width="529"/>
</div>
<div class="readable-text" id="p76">
<p><em><span class="aframe-location"/></em>Let’s now think about the end result. We must focus on patterns in cycling traffic, so there will be parts of the data we can largely ignore for our analysis. Knowing that we are interested in a particular aspect of the data will help us at the start when we are figuring out where to go next after the standard exploratory steps.</p>
</div>
<div class="browsable-container figure-container" id="p77">
<img alt="figure" height="175" src="../Images/8-unnumb-3.png" width="529"/>
</div>
<div class="readable-text" id="p78">
<p><em><span class="aframe-location"/></em>This is the step in which we decided to use the raw count data over the other available datasets. Again, this was driven directly by the problem statement. Area-level annual summaries are too high level to tease out cycling traffic, and using estimated measurements reduces the usefulness of our findings, leaving us with the raw count data to analyze. In this case, we do not have to make further decisions about which dataset to download, as the entire raw dataset comes as one file.</p>
</div>
<div class="browsable-container figure-container" id="p79">
<img alt="figure" height="175" src="../Images/8-unnumb-4.png" width="529"/>
</div>
<div class="readable-text" id="p80">
<p><em><span class="aframe-location"/></em>As with most of the projects, the data has already been downloaded, but no other changes have been made to it to best simulate the experience of exploring it for the first time. In the real world, obtaining the data might be a surprising obstacle, especially if you need permission, and there are privacy and governance concerns. This is not the case here, as we are working with open government data.</p>
</div>
<div class="browsable-container figure-container" id="p81">
<img alt="figure" height="104" src="../Images/8-unnumb-5.png" width="317"/>
</div>
<div class="readable-text" id="p82">
<p><em><span class="aframe-location"/></em>Let’s now think about the steps we will take in our analysis. Before we turn our attention to the recommendation portion, we need to explore the dataset thoroughly. Specifically, we want to</p>
</div>
<ul>
<li class="readable-text" id="p83"> <em>Investigate the granularity of our data</em> —What does one row represent? Is it one row per location per day or something else? The granularity of data is one of the first things to investigate because it informs all other data transformations, like aggregations. </li>
<li class="readable-text" id="p84"> <em>Understand the coverage of the data both geographically and in time</em> —For example, because the dataset is not a single time series but many, we need to know if every available location has the same amount of data. </li>
<li class="readable-text" id="p85"> <em>Identify gaps in the time series</em> —Does every location have measurements at constant intervals? This is important to ensure we have enough of a sample at each location and is also a critical requirement for forecasting. Most forecasting algorithms do not work with gaps in the data or inconsistent intervals. </li>
<li class="readable-text" id="p86"> <em>Investigate the distribution of bicycle counts</em> —What is a typical cycling volume for one row of data? Knowing this will immediately help identify the places with the highest cycling traffic. </li>
<li class="readable-text" id="p87"> <em>Look at temporal patterns</em> —This includes looking at how cycling traffic fluctuates at different times of day, different days of the week, and across multiple years. Are there seasonal patterns we can identify? Which locations are showing a growing trend in cycling traffic? </li>
<li class="readable-text" id="p88"> <em>Reduce the search space</em> —By this I mean we may not be able to analyze every location in equal detail because of gaps. We may have to filter the data down to locations that have more complete records across a longer time horizon, especially if we are interested in looking for temporal patterns and forecasting.<em/> </li>
</ul>
<div class="browsable-container figure-container" id="p89">
<img alt="figure" height="175" src="../Images/8-unnumb-6.png" width="529"/>
</div>
<div class="readable-text" id="p90">
<p><em><span class="aframe-location"/></em>The output of this project is likely to be a combination of line charts and conversations. Line charts are the de facto time series visualization tool because they best represent the temporal component of an analytical result. We will likely end up creating other visualizations, too, but this is a project where the first iteration will spark a lot of conversation with our stakeholders. As we identify the limitations of the available data, we will be able to make recommendations about other datasets, and deciding on which ones to focus on will be done in collaboration with our domain experts.</p>
</div>
<div class="browsable-container figure-container" id="p91">
<img alt="figure" height="104" src="../Images/8-unnumb-7.png" width="317"/>
</div>
<div class="readable-text" id="p92">
<p><em><span class="aframe-location"/></em>Since the dataset focuses narrowly on traffic volume, there will be multiple angles to explore after our initial recommendations. In the example solution, we’ll explore some of these possible directions in which we could take a future iteration.</p>
</div>
<div class="readable-text" id="p93">
<h2 class="readable-text-h2" id="sigil_toc_id_108"><span class="num-string">8.4</span> An example solution: Where should cycling infrastructure improvements be focused?</h2>
</div>
<div class="readable-text" id="p94">
<p>Now, it’s time to look at an example walkthrough of analyzing this data. As always, I strongly recommend attempting the project yourself first. The example solution will be more relevant if you have your own analysis to compare it to. It bears repeating that the solution is not <em>the</em> solution, just one series of decisions you could make and conclusions you could reach along the way. Use it to generate more ideas and gain a different perspective on how you could have approached the same project brief.</p>
</div>
<div class="readable-text intended-text" id="p95">
<p>Knowing what our end goal is and having thought about the various steps we want to take, our action plan will start with investigating the data. Only then will we understand what specific questions we can answer with what’s available. Then, we can focus on looking for patterns and trends in cycling behavior and perhaps even attempt to forecast cycling trends into the future.</p>
</div>
<div class="readable-text" id="p96">
<h3 class="readable-text-h3" id="sigil_toc_id_109"><span class="num-string">8.4.1</span> Investigating available data and extracting time series</h3>
</div>
<div class="readable-text" id="p97">
<p>As with any data problem, our first step is to look at the data itself. We know what columns to expect, but seeing a few sample rows will help us understand. We’ll import the necessary libraries and examine a few rows of data. The output is shown in figure 8.1:</p>
</div>
<div class="browsable-container listing-container" id="p98">
<div class="code-area-container">
<pre class="code-area">import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

traffic = pd.read_csv("./data/dft_traffic_counts_raw_counts.csv.gz")
print(traffic.shape)
traffic.head()<span class="aframe-location"/></pre>
</div>
</div>
<div class="browsable-container figure-container" id="p99">
<img alt="figure" height="568" src="../Images/8-1.png" width="1100"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 8.1</span> A glimpse of the first few rows of traffic count data</h5>
</div>
<div class="readable-text" id="p100">
<p>The shape of the data is <code>(4815504, 35)</code>, meaning we have 35 columns and close to 5 million observations. From the columns, we can tell that this isn’t a single time series. It is, in fact, lots of time series at various “count points,” that is, measurement locations. Measurements at each count point can be treated as a separate time series, but we also have the option to aggregate by region, local authority, or even different time periods. From our data dictionary, we can also tell that we will be interested in the <code>Pedal_cycles</code> column, which measures the number of bicycles observed in a measurement period.</p>
</div>
<div class="readable-text" id="p101">
<h4 class="readable-text-h4 sigil_not_in_toc">Investigating time series completeness</h4>
</div>
<div class="readable-text" id="p102">
<p>Let’s examine the data to see how complete it is. First, we will look at missing data. The following code produces the output in figure 8.2:</p>
</div>
<div class="browsable-container listing-container" id="p103">
<div class="code-area-container">
<pre class="code-area">traffic.isnull().sum()<span class="aframe-location"/></pre>
</div>
</div>
<div class="browsable-container figure-container" id="p104">
<img alt="figure" height="1044" src="../Images/8-2.png" width="480"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 8.2</span> Missing values per column in the traffic data</h5>
</div>
<div class="readable-text" id="p105">
<p>It looks like the records are mostly complete, with only road names and lengths missing a significant amount. This might have something to do with the different road types since not all roads in the country, and therefore, in the dataset, necessarily have a name. We will leave them missing since we have no reason to believe those rows are erroneous. There is a small number of measurements missing, which we will assume can be filled with zeros with the following code:</p>
</div>
<div class="browsable-container listing-container" id="p106">
<div class="code-area-container">
<pre class="code-area">measurement_cols = [
    'Pedal_cycles', 'Two_wheeled_motor_vehicles',
    'Cars_and_taxis', 'Buses_and_coaches',
    'LGVs', 'HGVs_2_rigid_axle', 'HGVs_3_rigid_axle',
    'HGVs_4_or_more_rigid_axle', 'HGVs_3_or_4_articulated_axle',
    'HGVs_5_articulated_axle', 'HGVs_6_articulated_axle',
    'All_HGVs', 'All_motor_vehicles'
]

for col in measurement_cols:
    traffic[col] = traffic[col].fillna(0)</pre>
</div>
</div>
<div class="readable-text" id="p107">
<p>For completeness, we can also investigate what regions are covered by the data by inspecting the <code>Region_name</code> column. The following code produces the output in figure 8.3:</p>
</div>
<div class="browsable-container listing-container" id="p108">
<div class="code-area-container">
<pre class="code-area">traffic["Region_name"].value_counts()<span class="aframe-location"/></pre>
</div>
</div>
<div class="browsable-container figure-container" id="p109">
<img alt="figure" height="398" src="../Images/8-3.png" width="483"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 8.3</span> Distribution of regions in the traffic data</h5>
</div>
<div class="readable-text" id="p110">
<p>It looks like the data covers England, Scotland, and Wales, as well as various regions in England. Before moving on with our investigation, let’s start building the diagram to document the analysis. Figure 8.4 shows the first step, in which we had to make a decision about missing values.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p111">
<img alt="figure" height="480" src="../Images/8-4.png" width="1100"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 8.4</span> The first step in the analysis</h5>
</div>
<div class="readable-text intended-text" id="p112">
<p>Our next question is concerning granularity: What precisely does one row of data represent?</p>
</div>
<div class="readable-text" id="p113">
<h4 class="readable-text-h4 sigil_not_in_toc">Investigating time series granularity</h4>
</div>
<div class="readable-text" id="p114">
<p>It is important to establish what one row of our data represents. It is a measurement at a particular time and location, but what specific combination of columns makes a row unique? We can test this by counting the number of rows for the combination of columns we believe to be unique and verifying if that matches the number of rows in the entire dataset.</p>
</div>
<div class="callout-container sidebar-container">
<div class="readable-text" id="p115">
<h5 class="callout-container-h5 readable-text-h5 sigil_not_in_toc">A note on composite primary keys</h5>
</div>
<div class="readable-text" id="p116">
<p>When multiple columns make a record unique, this is called a <em>composite primary key</em>. It is when uniqueness does not come from a single ID column but a combination of more than one column. For example, customer IDs might not be unique if multiple customer databases are combined. In that case, the customer ID and the source database name might be what makes a record unique.</p>
</div>
<div class="readable-text" id="p117">
<p>This is another way in which foundational training can differ from the real world. In reality, databases often have complex structures, including composite primary keys.</p>
</div>
</div>
<div class="readable-text" id="p118">
<p>The unique key, in this case, must at least contain <code>Count_point_id</code>, the <code>Count_date</code>, and therefore also the <code>Year</code>. There is also an <code>hour</code> column, suggesting the data is at an hourly granularity. If we assume these columns are the composite key, we can count the unique combinations and verify that they match the row count:</p>
</div>
<div class="browsable-container listing-container" id="p119">
<div class="code-area-container code-area-with-html">
<pre class="code-area">len(traffic[["Count_point_id", "Year", "Count_date",
<span class="">↪</span> "hour"]].drop_duplicates())</pre>
</div>
</div>
<div class="readable-text" id="p120">
<p>This gives us <code>2435120</code>, which is too few rows and suggests there is another column we haven’t taken into account. The fact that this number is roughly half of the data suggests the column we are looking for typically has two values, so for every hour at every location, there is also another kind of measurement. Looking at the columns, this could be the <code>Direction_of_travel</code>, meaning traffic is counted separately in both directions at each location. Let’s add that column to the key to see if it matches the number of rows:</p>
</div>
<div class="browsable-container listing-container" id="p121">
<div class="code-area-container code-area-with-html">
<pre class="code-area">len(traffic[["Count_point_id", "Year", "Count_date",
<span class="">↪</span> "hour", "Direction_of_travel"]].drop_duplicates())</pre>
</div>
</div>
<div class="readable-text" id="p122">
<p>This returns <code>4815480</code>, which is much closer to the number of rows, suggesting we have found the right combination of columns, but the data contains duplicates. Let’s investigate these. The following code finds duplicate keys and produces the output in figure 8.5:</p>
</div>
<div class="browsable-container listing-container" id="p123">
<div class="code-area-container code-area-with-html">
<pre class="code-area">duplicate_groups = (
    traffic
    .groupby(["Count_point_id", "Year", "Count_date",
<span class="">↪</span> "hour", "Direction_of_travel"])
    .size()
    .loc[lambda x: x &gt; 1]
)

duplicate_groups<span class="aframe-location"/></pre>
</div>
</div>
<div class="browsable-container figure-container" id="p124">
<img alt="figure" height="823" src="../Images/8-5.png" width="981"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 8.5</span> Duplicate records with the same composite primary key</h5>
</div>
<div class="readable-text" id="p125">
<p>It looks like there are two locations with duplicate measurements on two dates. We want to ascertain whether the measurements are also duplicated or whether the rows are perfect duplicates. We do this by taking one of the keys as an example and looking at which values the duplicate rows differ in. The following code does this and produces the output in figure 8.6:</p>
</div>
<div class="browsable-container listing-container" id="p126">
<div class="code-area-container">
<pre class="code-area">example_dupes = (
  traffic[
    (traffic["Count_point_id"] == 7845)     <span class="aframe-location"/> #1
      &amp; (traffic["Count_date"] == "2014-09-03 00:00:00")
      &amp; (traffic["hour"] == 7)
      &amp; (traffic["Direction_of_travel"] == "W")
  ]
)

(
  example_dupes
  .eq(example_dupes.shift(-1))  <span class="aframe-location"/> #2
  .iloc[0]
  .loc[lambda x: x == False]
)<span class="aframe-location"/></pre>
<div class="code-annotations-overlay-container">
     #1 Finds a specific example of a duplicate
     <br/>#2 Uses the shift method to check whether values are equal in both rows
     <br/>
</div>
</div>
</div>
<div class="browsable-container figure-container" id="p127">
<img alt="figure" height="400" src="../Images/8-6.png" width="486"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 8.6</span> Columns where values don’t match in the example duplicate rows</h5>
</div>
<div class="readable-text" id="p128">
<p>This tells us that for that location and date, the columns that differ are the ones measuring how many vehicles passed by. Let’s look at the specific measurements to determine how different they are. The following code extracts these columns. The returned data contains many columns, and by default, they are not all shown. Even if we could show them all, which is possible to do, they wouldn’t fit horizontally on the screen without the need to scroll. One trick is to take a row or two of data and <em>transpose</em> it to show it as only one or two columns instead. We’ll do this here, and the output is shown in figure 8.7:</p>
</div>
<div class="browsable-container listing-container" id="p129">
<div class="code-area-container">
<pre class="code-area">(
  example_dupes[[
    'Two_wheeled_motor_vehicles', 'Cars_and_taxis', 'Buses_and_coaches',
    'LGVs', 'HGVs_2_rigid_axle', 'HGVs_3_rigid_axle',
    'HGVs_4_or_more_rigid_axle', 'HGVs_3_or_4_articulated_axle',
    'HGVs_5_articulated_axle', 'HGVs_6_articulated_axle', 'All_HGVs',
    'All_motor_vehicles']]
  .transpose()
)<span class="aframe-location"/></pre>
</div>
</div>
<div class="browsable-container figure-container" id="p130">
<img alt="figure" height="642" src="../Images/8-7.png" width="491"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 8.7</span> Side-by-side comparison of duplicate records</h5>
</div>
<div class="readable-text" id="p131">
<p>The values are quite different for the same combination of location and measurement date and time, so we are now confronted with a decision to make. What are our options when handling these duplicates?</p>
</div>
<ul>
<li class="readable-text" id="p132"> Should we combine the values somehow? This would make sense if these were partial measurements, but we have no evidence of this, and the numbers are too similar. </li>
<li class="readable-text" id="p133"> Is one of them newer data, making the other row obsolete, in which case we should drop the first row? This is possible, but if it’s the case, we have no way of knowing which would be the newer measurement apart from assuming the one that appears later is more recent. This feels like a strong assumption to make with no evidence. </li>
<li class="readable-text" id="p134"> We could drop these rows entirely, but this would introduce a gap into some of our time series, which is problematic for time series analysis. </li>
<li class="readable-text" id="p135"> We could average the counts across the two records. This preserves the time series and keeps our numbers in the right ballpark, but we are essentially making up data this way. </li>
</ul>
<div class="readable-text" id="p136">
<p>There is no correct answer here. Each choice has its own assumptions and consequences. We will err on the side of preserving the time series and go with the averaging approach. While this does make up measurements that were not actually recorded, these duplicates are a small enough percentage of the overall data not to make this a big problem.</p>
</div>
<div class="readable-text intended-text" id="p137">
<p>To combine these duplicates, we can group our data by the composite key, creating one group per unique identifier and averaging the measurement rows. In most cases, since the keys are unique, we will be averaging a single row, leaving it unaffected. The only additional trick is to handle missing columns. Our road name and link length columns, which form part of the composite key, contain missing values. The <code>pandas</code> library in particular will not group the records correctly when some grouping columns are missing.</p>
</div>
<div class="readable-text intended-text" id="p138">
<p>To avoid this, we will temporarily fill missing values with a placeholder, do the deduplication, and remove the placeholder values afterward. For the <code>Road_name</code> column, we can use the text “PLACEHOLDER,” but for the numeric columns, we need to find a value that doesn’t already appear in the data. Negative numbers work well here, but we should double-check that there aren’t already negative values for link length for whatever reason:</p>
</div>
<div class="browsable-container listing-container" id="p139">
<div class="code-area-container">
<pre class="code-area">print(traffic["Link_length_km"].min(),
      traffic["Link_length_miles"].min())</pre>
</div>
</div>
<div class="readable-text" id="p140">
<p>The output is 0.1 and 0.06, respectively, telling us that there are no negative values, and we can use one as a placeholder. The process for deduplication is therefore the following:</p>
</div>
<ul>
<li class="readable-text" id="p141"> Replace missing values with placeholders. </li>
<li class="readable-text" id="p142"> Group by all columns except the measurements. </li>
<li class="readable-text" id="p143"> Within each group, which is mostly one row each, average the measurement values. </li>
<li class="readable-text" id="p144"> In the grouped and aggregated dataset, replace placeholders with missing data again. </li>
</ul>
<div class="readable-text" id="p145">
<p>The following code does this and verifies that we have reduced the number of rows to the number of unique groups:</p>
</div>
<div class="browsable-container listing-container" id="p146">
<div class="code-area-container code-area-with-html">
<pre class="code-area">TEXT_PLACEHOLDER = "PLACEHOLDER"
NUMBER_PLACEHOLDER = -9999

group_cols = [
  'Count_point_id', 'Direction_of_travel', 'Year', 'Count_date', 'hour',
  'Region_id', 'Region_name', 'Region_ons_code', 'Local_authority_id',
  'Local_authority_name', 'Local_authority_code', 'Road_name',
  'Road_category', 'Road_type', 'Start_junction_road_name',
  'End_junction_road_name', 'Easting', 'Northing', 'Latitude',
  'Longitude', 'Link_length_km', 'Link_length_miles'
]

traffic_deduped = (
  traffic
  .assign(
    Start_junction_road_name = lambda df_:
<span class="">↪</span> df_["Start_junction_road_name"].fillna(TEXT_PLACEHOLDER),
    End_junction_road_name = lambda df_:
<span class="">↪</span> df_["End_junction_road_name"].fillna(TEXT_PLACEHOLDER),
    Link_length_km = lambda df_:
<span class="">↪</span> df_["Link_length_km"].fillna(NUMBER_PLACEHOLDER),
    Link_length_miles = lambda df_:
<span class="">↪</span> df_["Link_length_miles"].fillna(NUMBER_PLACEHOLDER)
  )
  .groupby(group_cols)
  .mean(numeric_only=True)
  .reset_index()
  .assign(
    Start_junction_road_name = lambda df_:
<span class="">↪</span> df_["Start_junction_road_name"].replace(TEXT_PLACEHOLDER, np.nan),
    End_junction_road_name = lambda df_:
<span class="">↪</span> df_["End_junction_road_name"].replace(TEXT_PLACEHOLDER, np.nan),
    Link_length_km = lambda df_:
<span class="">↪</span> df_["Link_length_km"].replace(NUMBER_PLACEHOLDER, np.nan),
    Link_length_miles = lambda df_:
<span class="">↪</span> df_["Link_length_miles"].replace(NUMBER_PLACEHOLDER, np.nan)
  )
)

print(traffic.shape, traffic_deduped.shape)</pre>
</div>
</div>
<div class="readable-text" id="p147">
<p>The output is <code>(4815504, 35) (4815480, 35)</code>, where the second pair of values shows us that we have reduced the data down to one row per unique combination of columns in the composite primary key. This feels like a lot of work to remove a few duplicates, but the presence of duplicates can cause multiple problems with analysis, so it is best to address them.</p>
</div>
<div class="readable-text intended-text" id="p148">
<p>Figure 8.8 shows the latest version of our process, including the steps we have just taken to merge duplicate records.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p149">
<img alt="figure" height="597" src="../Images/8-8.png" width="647"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 8.8</span> The diagram of our analysis two steps in</h5>
</div>
<div class="readable-text intended-text" id="p150">
<p>So far, we’ve investigated and handled missing data and ensured we understand its granularity. Now, it’s time to look at the coverage.</p>
</div>
<div class="readable-text" id="p151">
<h4 class="readable-text-h4 sigil_not_in_toc">Investigating time series coverage</h4>
</div>
<div class="readable-text" id="p152">
<p>When I say we will look at the coverage, in this instance, I mean the date range of values in the data. We’ve looked briefly at geographic coverage, and now we want to investigate the following:</p>
</div>
<ul>
<li class="readable-text" id="p153"> What is the date range of the data in general? </li>
<li class="readable-text" id="p154"> Does the date range vary across smaller time series (e.g., per location)? </li>
<li class="readable-text" id="p155"> Are there consistent measurement intervals in the data? </li>
<li class="readable-text" id="p156"> Are there gaps in any of the time series? </li>
</ul>
<div class="readable-text" id="p157">
<p>Answers to these questions will determine not only the quality of our final analysis but also whether we need to focus on certain parts of the country purely because of a lack of consistent data coverage everywhere. First, let’s understand the date range of the data after we convert the <code>Count_date</code> column to be the right type. The output is shown in figure 8.9:</p>
</div>
<div class="browsable-container listing-container" id="p158">
<div class="code-area-container code-area-with-html">
<pre class="code-area">traffic["Count_date"] =
<span class="">↪</span> pd.to_datetime(traffic["Count_date"], format="%Y-%m-%d %H:%M:%S")
traffic["Count_date"].agg(["min", "max"])<span class="aframe-location"/></pre>
</div>
</div>
<div class="browsable-container figure-container" id="p159">
<img alt="figure" height="87" src="../Images/8-9.png" width="518"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 8.9</span> The date range of the entire dataset</h5>
</div>
<div class="readable-text" id="p160">
<p>The output of this code is that the first date encountered in the dataset is March 2000, and the latest is November 2022. We have 20-year coverage, though it remains to be seen whether this is consistent across measuring locations. We want to know</p>
</div>
<ul>
<li class="readable-text" id="p161"> Does every location have 20 years of data? </li>
<li class="readable-text" id="p162"> Are there gaps in any of the time series of the different locations? </li>
</ul>
<div class="readable-text" id="p163">
<p>One way to investigate this is to calculate the first and last date per location, calculate the difference, and investigate the distribution of this difference number. This will tell us how long each location-specific time series is at a glance. The following code achieves this, and the output is shown in figure 8.10:</p>
</div>
<div class="browsable-container listing-container" id="p164">
<div class="code-area-container">
<pre class="code-area">coverage_by_point = (
    traffic
    .groupby("Count_point_id")
    ["Count_date"]
    .agg(["min", "max"])
    .assign(coverage_years = lambda x: (x["max"] - x["min"]).dt.days / 365)
    .sort_values("coverage_years", ascending=False)
)

coverage_by_point<span class="aframe-location"/></pre>
</div>
</div>
<div class="browsable-container figure-container" id="p165">
<img alt="figure" height="788" src="../Images/8-10.png" width="696"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 8.10</span> Coverage (in years) by location</h5>
</div>
<div class="readable-text" id="p166">
<p>This table tells us a few important things:</p>
</div>
<ul>
<li class="readable-text" id="p167"> <em>Coverage varies a lot across locations.</em> There are locations with a single day’s worth of measurements and some that have data for the entire 22-year period. </li>
<li class="readable-text" id="p168"> <em>Measurement dates vary.</em> This wasn’t obvious looking at the data initially, but it turns out there is no consistent start or end point for any of the measurement time series. </li>
</ul>
<div class="readable-text" id="p169">
<p>To get a better sense of the distribution of these values, let’s create a histogram. The following code produces the histogram in figure 8.11:</p>
</div>
<div class="browsable-container listing-container" id="p170">
<div class="code-area-container">
<pre class="code-area">fig, axis = plt.subplots()

coverage_by_point["coverage_years"].hist(bins=50, ax=axis)

axis.set(
    title="Distribution of coverage (years) by location",
    xlabel="Date range (number of years)",
    ylabel="Frequency"
)

plt.show()<span class="aframe-location"/></pre>
</div>
</div>
<div class="browsable-container figure-container" id="p171">
<img alt="figure" height="597" src="../Images/8-11.png" width="778"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 8.11</span> Histogram showing coverage in years across different locations</h5>
</div>
<div class="readable-text" id="p172">
<p>It looks like locations overwhelmingly have a coverage of near zero. That is, most locations only have one day of measurement to their name. This presents a problem because the data in those locations does not constitute much of a time series, except for hourly measurements on a single day. That’s not enough data to draw much insight from. Again, we are facing the following choice:</p>
</div>
<ul>
<li class="readable-text" id="p173"> Do we include locations with only one day of measurements? Doing so means we don’t lose a lot of coverage, but we also can’t answer questions about increasing trends in those areas. </li>
<li class="readable-text" id="p174"> Do we focus only on locations with enough data? This will mean we have more robust results but for far fewer locations and areas. </li>
</ul>
<div class="readable-text" id="p175">
<p>This is a good decision point to remind ourselves of our framework. We want to be results driven and have the research question at the forefront of our minds. We will likely need to follow up our recommendations with further work and certainly wouldn’t want to base any infrastructure decisions on sparse data. On that basis, we will seek to trim down the data to keep only the time series that have the most data, the highest coverage, and no gaps.</p>
</div>
<div class="readable-text print-book-callout" id="p176">
<p><span class="print-book-callout-head">Note</span>  This is one of those decisions that will drastically affect how different our solutions will be. If your results don’t match mine, do not assume it is because you have made a mistake. We might have just made different decisions, leading to different results. As long as those decisions and their key assumptions are documented, different results may be equally valuable and useful.</p>
</div>
<div class="readable-text" id="p177">
<p>Whenever we investigate missing data, we want to know whether there are any patterns in the gaps. Is any specific factor causing a part of our data to be missing? In this instance, we are working with low coverage instead of missing data, but the idea holds. Let’s investigate whether there are certain areas of the country that have less coverage. Why could this be?</p>
</div>
<ul>
<li class="readable-text" id="p178"> Some locations may have been added to the “traffic measurement program” later than others. </li>
<li class="readable-text" id="p179"> There might be logistic difficulties with measuring in certain locations. </li>
<li class="readable-text" id="p180"> New roads have been built around newly built housing estates, and measurements could not have started at an earlier date. </li>
</ul>
<div class="readable-text" id="p181">
<p>Whatever the reason, we want to know whether low coverage is randomly distributed across the country or whether there is a pattern we should be aware of. Let’s use the table from figure 8.10 to focus on location points with only one day of data. We will drop duplicate rows for the same location ID because we want to look at how they are distributed and not at their granular measurements. Figure 8.12 shows the number of locations with only one day of coverage, split by region, as obtained by the following code:</p>
</div>
<div class="browsable-container listing-container" id="p182">
<div class="code-area-container code-area-with-html">
<pre class="code-area">zero_location_ids = coverage_by_point                <span class="aframe-location"/> #1
<span class="">↪</span> [coverage_by_point["coverage_years"] == 0].index  

zero_locations = (
    traffic[traffic["Count_point_id"].isin(zero_location_ids)]
    .drop_duplicates("Count_point_id")
)
print(len(zero_locations))
zero_locations["Region_name"].value_counts()<span class="aframe-location"/></pre>
<div class="code-annotations-overlay-container">
     #1 Locations with only one day of measurements are referred to as "zero" because the difference between the first and last measurement dates is zero.
     <br/>
</div>
</div>
</div>
<div class="browsable-container figure-container" id="p183">
<img alt="figure" height="374" src="../Images/8-12.png" width="428"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 8.12</span> Number of locations with one day of data across regions</h5>
</div>
<div class="readable-text" id="p184">
<p>There is variation here, but we must not fall into the trap of using absolute numbers to make a judgment. It might simply be that the South East has more single-day locations than Wales because there are more location points. Let’s calculate these as a percentage of the total number of locations in each region to get a fair comparison.</p>
</div>
<div class="readable-text intended-text" id="p185">
<p>First, we calculate the number of location points by region and then use that number to calculate the numbers in figure 8.12 as a percentage. The following code calculates the locations by region, as shown in figure 8.13:</p>
</div>
<div class="browsable-container listing-container" id="p186">
<div class="code-area-container">
<pre class="code-area">location_sizes = (
    traffic
    .groupby("Region_name")
    ["Count_point_id"]
    .nunique()
)<span class="aframe-location"/></pre>
</div>
</div>
<div class="browsable-container figure-container" id="p187">
<img alt="figure" height="407" src="../Images/8-13.png" width="456"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 8.13</span> Number of count points by region</h5>
</div>
<div class="readable-text" id="p188">
<p>The following code joins the two tables together and produces the output table shown in figure 8.14:</p>
</div>
<div class="browsable-container listing-container" id="p189">
<div class="code-area-container">
<pre class="code-area">(
    location_sizes
    .reset_index()
    .merge(
        zero_locations["Region_name"]
            .value_counts()
            .reset_index(name="count")
            .rename(columns={"index": "Region_name"}),
        on="Region_name"
    )
    .rename(columns={
        "Count_point_id": "total_points",
        "count": "number_of_zeros"
    })
    .assign(pct_zeros = lambda x: x["number_of_zeros"] / x["total_points"])
)<span class="aframe-location"/></pre>
</div>
</div>
<div class="browsable-container figure-container" id="p190">
<img alt="figure" height="658" src="../Images/8-14.png" width="876"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 8.14</span> Number of total locations and single-day locations per region</h5>
</div>
<div class="readable-text" id="p191">
<p>If there was a real problem with single-day locations being limited to only certain regions, we would find considerable variation in this table. As it stands, the percentage of locations that only have data on a single date is consistent across the regions, with only Wales and London being noticeably lower. We could investigate this much deeper, but in the spirit of getting to our result, we will assume we are satisfied that the existence of single-day locations is just something that happens everywhere and is not something to address directly.</p>
</div>
<div class="readable-text intended-text" id="p192">
<p>Now that we have looked at missing data, granularity, and coverage, let’s turn our attention to gaps. Gaps are a problem when it comes to time series, so we want to reduce our data to locations where we can get a longer and complete time series.</p>
</div>
<div class="readable-text" id="p193">
<h4 class="readable-text-h4 sigil_not_in_toc">Investigating gaps in time series</h4>
</div>
<div class="readable-text" id="p194">
<p>We’ve established that different locations have tracked data since a different starting point and for varying lengths of time. To identify gaps, we can’t just count the number of unique dates seen at a location; we need to calculate the difference between each encountered date and the previously encountered date and flag any cases with more than a one-day gap.</p>
</div>
<div class="readable-text intended-text" id="p195">
<p>Let’s first look at the number of data points per location per date to get an idea whether there might be continuity problems. The following code calculates this and produces the table in figure 8.15:</p>
</div>
<div class="browsable-container listing-container" id="p196">
<div class="code-area-container">
<pre class="code-area">points_and_dates = (
    traffic
    .groupby(["Count_point_id", "Count_date"])
    .size()
    .reset_index()
    .sort_values(["Count_point_id", "Count_date"])
)

points_and_dates.head()<span class="aframe-location"/></pre>
</div>
</div>
<div class="browsable-container figure-container" id="p197">
<img alt="figure" height="310" src="../Images/8-15.png" width="442"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 8.15</span> Number of data points per location ID and per date</h5>
</div>
<div class="readable-text" id="p198">
<p>This table shows us something important. We made an incorrect assumption that measurements are taken daily across a year. The data is daily in its granularity, but there is only one day of measurement data for each year. We hadn’t sliced the data in the right way before to find this earlier, but this gives us a clearer picture of what we have.</p>
</div>
<div class="readable-text intended-text" id="p199">
<p>To understand whether there might be gaps, first, we can check how many unique values there are for the <code>Year</code> column in each location. Those that have 23 are the ones that have measurements in every year between 2000 and 2022, inclusive. We will focus only on locations that have at least 10 years of data, but that’s somewhat arbitrary. We could also restrict time series that are complete for the most recent 5 to 10 years. Here, we’ll choose completeness over recency, and the following code calculates this and outputs the result in figure 8.16:</p>
</div>
<div class="browsable-container listing-container" id="p200">
<div class="code-area-container">
<pre class="code-area">num_years_by_point = (
    traffic
    .groupby("Count_point_id")
    ["Year"]
    .nunique()
    .loc[lambda x: x &gt; 10]
    .sort_values(ascending=False)
)

num_years_by_point<span class="aframe-location"/></pre>
</div>
</div>
<div class="browsable-container figure-container" id="p201">
<img alt="figure" height="394" src="../Images/8-16.png" width="487"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 8.16</span> Distribution of the number of unique calendar years per location</h5>
</div>
<div class="readable-text" id="p202">
<p>Let’s also look at the time series for the first of those locations to get a sense of what a complete time series looks like in our data. The following code produces the plot in figure 8.17:</p>
</div>
<div class="browsable-container listing-container" id="p203">
<div class="code-area-container">
<pre class="code-area">fig, axis = plt.subplots()

LOCATION_ID = "26010"

(
    traffic
    .query(f"Count_point_id == {LOCATION_ID}")
    .groupby("Count_date")
    ["All_motor_vehicles"]
    .sum()
    .plot(ax=axis)
)

axis.set(
    title=f"Number of vehicles over time (location {LOCATION_ID})",
    xlabel="Date",
    ylabel="Number of vehicles (total)"
)

plt.show()<span class="aframe-location"/></pre>
</div>
</div>
<div class="browsable-container figure-container" id="p204">
<img alt="figure" height="549" src="../Images/8-17.png" width="783"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 8.17</span> An example time series for location 26010</h5>
</div>
<div class="readable-text" id="p205">
<p>This is the total number of vehicles seen at a particular location on the day counting took place each year. There are already some interesting aspects, such as the dip in 2020 due to the COVID-19 lockdowns.</p>
</div>
<div class="readable-text" id="p206">
<p>The next step is to filter our location list to only the time series that have no gaps. To do this, we will ensure our data is sorted and create a temporary column to capture the year of the previous row so that we can find instances where the gap between a row and the previous one is more than a year. The following code adds these additional columns, and a snapshot of the new <code>gaps</code> DataFrame is shown in figure 8.18:</p>
</div>
<div class="browsable-container listing-container" id="p207">
<div class="code-area-container">
<pre class="code-area">long_count_points = num_years_by_point.index

gaps = (
    traffic
    .query("Count_point_id in @long_count_points")
    [["Count_point_id", "Year"]]
    .drop_duplicates()
    .sort_values(["Count_point_id", "Year"])
    .assign(
        prev_year= lambda x: x["Year"].shift(),
        diff= lambda x: x["Year"] - x["prev_year"]
    )
)

gaps.head()<span class="aframe-location"/></pre>
</div>
</div>
<div class="browsable-container figure-container" id="p208">
<img alt="figure" height="963" src="../Images/8-18.png" width="500"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 8.18</span> A snapshot of the new <code>gaps</code> DataFrame, with important rows highlighted</h5>
</div>
<div class="readable-text" id="p209">
<p>These new columns help us identify places where the previously encountered measurements were from more than one year ago. In the highlighted section of figure 8.18, we can notice that there was no measurement at location ID 501 in the year 2018, so the gap between rows is two years. When the next location ID is encountered, the gap can become negative when the last year of the previous location ID is later than the first year of the next location ID.</p>
</div>
<div class="readable-text intended-text" id="p210">
<p>To identify gaps, we could simply filter this dataset down to where the <code>diff</code> column is greater than 1. However, we might encounter edge cases where the next location ID happens to start two years after the previous one, and we would erroneously mark it as having a gap.</p>
</div>
<div class="readable-text intended-text" id="p211">
<p>To make sure we filter gaps properly, we also need to track the location ID of the previous column so that when we encounter a gap greater than one year, we also check whether the location ID is still the same. The following code does this, and some of the rows with problematic gaps are shown in figure 8.19:</p>
</div>
<div class="browsable-container listing-container" id="p212">
<div class="code-area-container">
<pre class="code-area">gaps = (
    gaps
    .assign(
        prev_id= lambda x: x["Count_point_id"].shift()
    )
    .query("diff &gt; 1 and Count_point_id == prev_id")
)

gaps.head()<span class="aframe-location"/></pre>
</div>
</div>
<div class="browsable-container figure-container" id="p213">
<img alt="figure" height="274" src="../Images/8-19.png" width="613"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 8.19</span> Some of the rows representing problematic gaps in the time series</h5>
</div>
<div class="readable-text" id="p214">
<p>We can now use this <code>gaps</code> DataFrame to find all the unique location IDs that we want to exclude from our final time series data. This is done with the following code. A part of the resulting DataFrame is shown in figure 8.20:</p>
</div>
<div class="browsable-container listing-container" id="p215">
<div class="code-area-container">
<pre class="code-area">gap_ids = gaps["Count_point_id"].unique()

all_time_series_raw = (
    traffic
    .query("Count_point_id in @long_count_points \
    and Count_point_id not in @gap_ids")
)

all_time_series_raw.head()<span class="aframe-location"/></pre>
</div>
</div>
<div class="browsable-container figure-container" id="p216">
<img alt="figure" height="564" src="../Images/8-20.png" width="1100"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 8.20</span> A snapshot of rows from the filtered traffic data</h5>
</div>
<div class="readable-text" id="p217">
<p>Figure 8.20 now shows filtered rows from the original, raw <code>traffic</code> DataFrame. It contains only location IDs that have at least 10 years of continuous, gap-free data. Let’s now aggregate this to actually be a time series summarized at a location level so that we better understand how much data we are left with. The following code performs this aggregation, and figure 8.21 shows the first few rows of our newly aggregated data:</p>
</div>
<div class="browsable-container listing-container" id="p218">
<div class="code-area-container">
<pre class="code-area">all_time_series = (
    all_time_series_raw
    .groupby(["Count_point_id", "Count_date"])
    ["All_motor_vehicles"]
    .sum()
    .reset_index()
)

print(all_time_series["Count_point_id"].nunique())
all_time_series.head()<span class="aframe-location"/></pre>
</div>
</div>
<div class="browsable-container figure-container" id="p219">
<img alt="figure" height="303" src="../Images/8-21.png" width="532"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 8.21</span> A snapshot of filtered traffic data now aggregated as an annual time series</h5>
</div>
<div class="readable-text" id="p220">
<p>This is now one row per location ID and measurement date. As the top of figure 8.21 shows, we still have data for just over 1,400 unique location IDs. These are now all-time series where there are measurements every year, so it is a time series with no gaps.</p>
</div>
<div class="readable-text intended-text" id="p221">
<p>To be precise, we have a time series showing the total number of vehicles that passed a count point in a single day in a particular year. There is only one day where measurements take place each year. This is an important detail because it leads to some caveats:</p>
</div>
<ul>
<li class="readable-text" id="p222"> Figure 8.21 shows that measurements are not taken on the same day each year. If we want to investigate traffic patterns over time, the measurements should at least be taken around the same time of year because, otherwise, we might be comparing summer traffic to winter traffic, for example. One option is to keep only time series where the measurements are consistently taken around the same time of year. </li>
<li class="readable-text" id="p223"> Following this, whichever part of the year our measurements are taken from will introduce bias. Cycling patterns for locations where only winter measurements exist might not be helpful when cycling is likely reduced everywhere around that time of year. </li>
<li class="readable-text" id="p224"> If the date <em>is</em> the same each year, that might actually be a problem because we might be comparing different days of the week, even weekdays to weekends. </li>
</ul>
<div class="readable-text" id="p225">
<p>Let’s check that last point. What days of the week is the data spread across? The following code investigates this, and the output is shown in figure 8.22:</p>
</div>
<div class="browsable-container listing-container" id="p226">
<div class="code-area-container">
<pre class="code-area">(
    all_time_series_raw[["Count_date"]]
    .drop_duplicates()
    ["Count_date"]
    .dt.weekday
    .value_counts(normalize=True)
    .sort_index()
)<span class="aframe-location"/></pre>
</div>
</div>
<div class="browsable-container figure-container" id="p227">
<img alt="figure" height="162" src="../Images/8-22.png" width="381"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 8.22</span> The percentage of rows across different days of the week</h5>
</div>
<div class="readable-text" id="p228">
<p>This tells us that roughly 20% of rows are spread across Tuesday to Friday, with slightly less on Mondays. Because we are counting days of the week starting with Monday as zero, we now also know there are no weekends in our remaining data, so that’s one concern we’ve alleviated.</p>
</div>
<div class="readable-text intended-text" id="p229">
<p>We can still use this data as a proxy for traffic over time to help us hone in on locations that have interesting cycling traffic patterns, but we must be fully aware of the limitations, especially when presenting results to our stakeholders.</p>
</div>
<div class="readable-text intended-text" id="p230">
<p>Before moving on to the analysis of these time series, our final step is to see what would happen if we were to keep only time series where every measurement was made in the same month each year.</p>
</div>
<div class="readable-text" id="p231">
<h4 class="readable-text-h4 sigil_not_in_toc">Finding time series recorded at the same time each year</h4>
</div>
<div class="readable-text" id="p232">
<p>The following code identifies the location points where measurements were only ever made in the same month every year. Figure 8.23 shows the output:</p>
</div>
<div class="browsable-container listing-container" id="p233">
<div class="code-area-container">
<pre class="code-area">same_month_time_series = (
    all_time_series
    .assign(month=lambda x: x["Count_date"].dt.month)
    .groupby("Count_point_id")
    ["month"]
    .nunique()
    .loc[lambda x: x == 1]
)

print(len(same_month_time_series))

same_month_time_series.head()<span class="aframe-location"/></pre>
</div>
</div>
<div class="browsable-container figure-container" id="p234">
<img alt="figure" height="269" src="../Images/8-23.png" width="352"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 8.23</span> Location IDs where only the same month was encountered</h5>
</div>
<div class="readable-text" id="p235">
<p>Keeping this filter would halve our data but still leave us just under 700 time series. We should verify that the time series associated with these location IDs do indeed only contain the same month. We’ll take the first location ID as an example, but in reality, we’d want to spot-check a few cases. The following code examines the time series for the count point with ID 900056, and the output is shown in figure 8.24.</p>
</div>
<div class="browsable-container listing-container" id="p236">
<div class="code-area-container">
<pre class="code-area">all_time_series[all_time_series["Count_point_id"] == 900056]<span class="aframe-location"/></pre>
</div>
</div>
<div class="browsable-container figure-container" id="p237">
<img alt="figure" height="647" src="../Images/8-24.png" width="539"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 8.24</span> Time series data for location ID 900056</h5>
</div>
<div class="readable-text" id="p238">
<p>Figure 8.24 shows that for all 13 years that traffic was counted at location 900056, it was always done in May. This makes the measurements more comparable year on year. Let’s now export this data to an intermediate file to separate the cleaning process from the analysis.</p>
</div>
<div class="readable-text" id="p239">
<h4 class="readable-text-h4 sigil_not_in_toc">Exporting only complete time series data</h4>
</div>
<div class="readable-text" id="p240">
<p>Exporting an intermediate version of your data is a good habit to get into, especially if you have a lot of raw data, and cleaning and transforming it takes a bit of time.</p>
</div>
<div class="readable-text intended-text" id="p241">
<p>We want to keep the raw version of the data filtered down to just the location IDs we have identified. We’ll use the Parquet format as it is compact and preserves data types. This file will be the starting point for the analysis in chapter 9. The following code creates this exported file:</p>
</div>
<div class="browsable-container listing-container" id="p242">
<div class="code-area-container">
<pre class="code-area">ids_to_export = same_month_time_series.index

(
    traffic
    .query("Count_point_id in @ids_to_export")
    .to_parquet("time_series.parquet.gz", compression="gzip")
)</pre>
</div>
</div>
<div class="readable-text" id="p243">
<h3 class="readable-text-h3" id="sigil_toc_id_110"><span class="num-string">8.4.2</span> Project progress so far</h3>
</div>
<div class="readable-text" id="p244">
<p>Before we move on to the analysis portion of the project in chapter 9, let’s recap what we have achieved in this chapter, which was the data preparation part of the project. Here’s what we know about our data:</p>
</div>
<ul>
<li class="readable-text" id="p245"> One row represents measurements taken at a single location, in a single hour on a particular date in a particular direction. A combination of these columns makes a record unique. </li>
<li class="readable-text" id="p246"> For every location, we have a maximum of one unique day of measurements for a given calendar year. </li>
<li class="readable-text" id="p247"> The number of years where measurements were taken varies significantly across location IDs. This means there is both inconsistent coverage and gaps in many of our time series. </li>
<li class="readable-text" id="p248"> Apart from missing roughly half of the road name and length data, there are no significant missing values. </li>
</ul>
<div class="readable-text" id="p249">
<p>To mitigate some of the problems, we have extracted only locations with the longest and most complete time series to focus on in part 2 of our analysis. Figure 8.25 shows the analysis steps we have taken and the decisions we have made so far.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p250">
<img alt="figure" height="784" src="../Images/8-25.png" width="722"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 8.25</span> The latest diagram of our steps, including investigating coverage and handling gaps</h5>
</div>
<div class="readable-text intended-text" id="p251">
<p>This diagram documents our process so far, and we will use the output of this chapter, a filtered version of the raw traffic data, as the starting point for the analysis in chapter 9.</p>
</div>
<div class="readable-text" id="p252">
<h2 class="readable-text-h2" id="sigil_toc_id_111">Summary</h2>
</div>
<ul>
<li class="readable-text" id="p253"> Time series data can seem simple yet contain complexity and hidden value. </li>
<li class="readable-text" id="p254"> Understanding how to manipulate time data broadens your data analysis toolkit. </li>
<li class="readable-text" id="p255"> The granularity of the available time series determines the analysis we can perform. For example, daily patterns cannot be determined from monthly data. </li>
<li class="readable-text" id="p256"> Time series analysis works best if there are no gaps in the data. </li>
<li class="readable-text" id="p257"> If there are gaps, they need to be handled either by smoothing over them or estimating what the values in the gaps should be. </li>
</ul>
</div></body></html>