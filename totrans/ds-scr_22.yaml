- en: Chapter 21\. Natural Language Processing
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 21 章 自然语言处理
- en: They have been at a great feast of languages, and stolen the scraps.
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 他们已经在语言的盛宴中大快朵颐，窃取了残羹剩饭。
- en: ''
  id: totrans-2
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: William Shakespeare
  id: totrans-3
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 威廉·莎士比亚
- en: '*Natural language processing* (NLP) refers to computational techniques involving
    language. It’s a broad field, but we’ll look at a few techniques, both simple
    and not simple.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '*自然语言处理*（NLP）指的是涉及语言的计算技术。这是一个广泛的领域，但我们将看几种简单和复杂的技术。'
- en: Word Clouds
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 词云
- en: In [Chapter 1](ch01.html#introduction), we computed word counts of users’ interests.
    One approach to visualizing words and counts is *word clouds*, which artistically
    depict the words at sizes proportional to their counts.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [第 1 章](ch01.html#introduction)，我们计算了用户兴趣的单词计数。一个可视化单词和计数的方法是 *词云*，它以比例大小艺术化地呈现单词。
- en: Generally, though, data scientists don’t think much of word clouds, in large
    part because the placement of the words doesn’t mean anything other than “here’s
    some space where I was able to fit a word.”
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，数据科学家们对词云并不看重，主要是因为单词的排列除了“这里是我能放置一个词的空间”之外没有其他意义。
- en: 'If you ever are forced to create a word cloud, think about whether you can
    make the axes convey something. For example, imagine that, for each of some collection
    of data science–related buzzwords, you have two numbers between 0 and 100—the
    first representing how frequently it appears in job postings, and the second how
    frequently it appears on résumés:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不得不创建一个词云，考虑一下是否可以让坐标轴传达某种信息。例如，想象一下，对于某些数据科学相关的流行术语，你有两个在 0 到 100 之间的数字——第一个表示它在职位发布中出现的频率，第二个表示它在简历中出现的频率：
- en: '[PRE0]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The word cloud approach is just to arrange the words on a page in a cool-looking
    font ([Figure 21-1](#word_cloud_wordle)).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 词云的方法就是在页面上以酷炫的字体排列这些词（[图 21-1](#word_cloud_wordle)）。
- en: '![Buzzword Cloud.](assets/dsf2_2101.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![术语词云。](assets/dsf2_2101.png)'
- en: Figure 21-1\. Buzzword cloud
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 21-1 术语词云
- en: 'This looks neat but doesn’t really tell us anything. A more interesting approach
    might be to scatter them so that horizontal position indicates posting popularity
    and vertical position indicates résumé popularity, which produces a visualization
    that conveys a few insights ([Figure 21-2](#word_cloud_scatter)):'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这看起来很整齐，但实际上并没有告诉我们什么。一个更有趣的方法可能是将它们散布开来，使得水平位置表示发布的流行度，垂直位置表示简历的流行度，这将产生一个传达几个见解的可视化效果（[图 21-2](#word_cloud_scatter)）：
- en: '[PRE1]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![A more meaningful (if less attractive) word cloud.](assets/dsf2_2102.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![更有意义（虽然不够吸引人）的词云。](assets/dsf2_2102.png)'
- en: Figure 21-2\. A more meaningful (if less attractive) word cloud
  id: totrans-16
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 21-2 更有意义（虽然不够吸引人）的词云
- en: n-Gram Language Models
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: n-Gram 语言模型
- en: The DataSciencester VP of Search Engine Marketing wants to create thousands
    of web pages about data science so that your site will rank higher in search results
    for data science–related terms. (You attempt to explain to her that search engine
    algorithms are clever enough that this won’t actually work, but she refuses to
    listen.)
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: DataSciencester 搜索引擎市场副总裁希望创建成千上万个关于数据科学的网页，以便您的网站在与数据科学相关的搜索结果中排名更高。（你试图向她解释搜索引擎算法已经足够聪明，这实际上不会起作用，但她拒绝听取。）
- en: Of course, she doesn’t want to write thousands of web pages, nor does she want
    to pay a horde of “content strategists” to do so. Instead, she asks you whether
    you can somehow programmatically generate these web pages. To do this, we’ll need
    some way of modeling language.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，她不想写成千上万个网页，也不想支付一大群“内容战略师”来完成。相反，她问你是否可以以某种程序化的方式生成这些网页。为此，我们需要某种语言建模的方法。
- en: One approach is to start with a corpus of documents and learn a statistical
    model of language. In our case, we’ll start with Mike Loukides’s essay [“What
    Is Data Science?”](http://oreil.ly/1Cd6ykN)
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 一种方法是从一组文档语料库开始，并学习语言的统计模型。在我们的案例中，我们将从迈克·劳凯德斯的文章[《什么是数据科学？》](http://oreil.ly/1Cd6ykN)开始。
- en: As in [Chapter 9](ch09.html#getting_data), we’ll use the Requests and Beautiful
    Soup libraries to retrieve the data. There are a couple of issues worth calling
    attention to.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 如同 [第 9 章](ch09.html#getting_data)，我们将使用 Requests 和 Beautiful Soup 库来获取数据。这里有几个值得注意的问题。
- en: 'The first is that the apostrophes in the text are actually the Unicode character
    `u"\u2019"`. We’ll create a helper function to replace them with normal apostrophes:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，文本中的撇号实际上是 Unicode 字符 `u"\u2019"`。我们将创建一个辅助函数来将其替换为正常的撇号：
- en: '[PRE2]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The second issue is that once we get the text of the web page, we’ll want to
    split it into a sequence of words and periods (so that we can tell where sentences
    end). We can do this using `re.findall`:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个问题是，一旦我们获取了网页的文本，我们将希望将其拆分为一系列的单词和句号（以便我们可以知道句子的结束位置）。我们可以使用`re.findall`来实现这一点：
- en: '[PRE3]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We certainly could (and likely should) clean this data further. There is still
    some amount of extraneous text in the document (for example, the first word is
    *Section*), and we’ve split on midsentence periods (for example, in *Web 2.0*),
    and there are a handful of captions and lists sprinkled throughout. Having said
    that, we’ll work with the document as it is.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们当然可以（而且很可能应该）进一步清理这些数据。文档中仍然有一些多余的文本（例如，第一个单词是*Section*），我们已经在中间句点上分割了（例如，在*Web
    2.0*中），还有一些标题和列表散布在其中。话虽如此，我们将按照文档的样子进行处理。
- en: 'Now that we have the text as a sequence of words, we can model language in
    the following way: given some starting word (say, *book*) we look at all the words
    that follow it in the source document. We randomly choose one of these to be the
    next word, and we repeat the process until we get to a period, which signifies
    the end of the sentence. We call this a *bigram model*, as it is determined completely
    by the frequencies of the bigrams (word pairs) in the original data.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将文本作为一系列单词，我们可以按以下方式对语言进行建模：给定一些起始词（比如*book*），我们查看源文档中跟随它的所有单词。我们随机选择其中一个作为下一个单词，并重复这个过程，直到我们遇到一个句号，这表示句子的结束。我们称之为*bigram模型*，因为它完全由原始数据中bigram（单词对）的频率决定。
- en: 'What about a starting word? We can just pick randomly from words that *follow*
    a period. To start, let’s precompute the possible word transitions. Recall that
    `zip` stops when any of its inputs is done, so that `zip(document, document[1:])`
    gives us precisely the pairs of consecutive elements of `document`:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 起始词呢？我们可以随机从跟在句号后面的单词中选择一个。首先，让我们预先计算可能的单词转换。记住`zip`在其输入的任何一个完成时停止，因此`zip(document,
    document[1:])`给出了`document`中连续元素的精确配对：
- en: '[PRE4]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now we’re ready to generate sentences:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备好生成句子了：
- en: '[PRE5]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The sentences it produces are gibberish, but they’re the kind of gibberish
    you might put on your website if you were trying to sound data-sciencey. For example:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 它生成的句子是胡言乱语，但如果你试图听起来像数据科学，它们就是你可能会放在你的网站上的那种胡言乱语。例如：
- en: If you may know which are you want to data sort the data feeds web friend someone
    on trending topics as the data in Hadoop is the data science requires a book demonstrates
    why visualizations are but we do massive correlations across many commercial disk
    drives in Python language and creates more tractable form making connections then
    use and uses it to solve a data.
  id: totrans-33
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果你知道你想要对数据进行排序的数据源网页友好的人在热门话题上作为Hadoop中的数据，那么数据科学需要一本书来演示为什么可视化是数据的可视化是但我们在Python语言中使用许多商业磁盘驱动器上的大量相关性，并创建更可管理的形式进行连接，然后使用它来解决数据的问题。
- en: ''
  id: totrans-34
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Bigram Model
  id: totrans-35
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Bigram模型
- en: 'We can make the sentences less gibberishy by looking at *trigrams*, triplets
    of consecutive words. (More generally, you might look at *n-grams* consisting
    of *n* consecutive words, but three will be plenty for us.) Now the transitions
    will depend on the previous *two* words:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 通过查看三元组，连续三个单词的三元组，我们可以使句子不那么胡言乱语。 （更一般地说，你可以查看由*n*个连续单词组成的*n-gram*，但对于我们来说，三个就足够了。）现在的转换将取决于前两个单词：
- en: '[PRE6]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Notice that now we have to track the starting words separately. We can generate
    sentences in pretty much the same way:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 现在注意，我们现在必须单独跟踪起始词。我们几乎可以以相同的方式生成句子：
- en: '[PRE7]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'This produces better sentences like:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这样产生的句子更好，比如：
- en: In hindsight MapReduce seems like an epidemic and if so does that give us new
    insights into how economies work That’s not a question we could even have asked
    a few years there has been instrumented.
  id: totrans-41
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 事后看来MapReduce看起来像是一场流行病，如果是这样，那么这是否给我们提供了新的见解，即经济如何运作这不是一个我们几年前甚至可以问的问题已经被工具化。
- en: ''
  id: totrans-42
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Trigram Model
  id: totrans-43
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 三元模型
- en: Of course, they sound better because at each step the generation process has
    fewer choices, and at many steps only a single choice. This means that we frequently
    generate sentences (or at least long phrases) that were seen verbatim in the original
    data. Having more data would help; it would also work better if we collected *n*-grams
    from multiple essays about data science.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，它们听起来更好，因为在每一步生成过程中的选择更少，而在许多步骤中只有一个选择。这意味着我们经常生成句子（或至少是长短语），这些句子在原始数据中原封不动地出现过。拥有更多的数据会有所帮助；如果我们收集了关于数据科学的多篇文章中的*n*-gram，它也会更有效。
- en: Grammars
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 语法
- en: A different approach to modeling language is with *grammars*, rules for generating
    acceptable sentences. In elementary school, you probably learned about parts of
    speech and how to combine them. For example, if you had a really bad English teacher,
    you might say that a sentence necessarily consists of a *noun* followed by a *verb*.
    If you then have a list of nouns and verbs, you can generate sentences according
    to the rule.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 一种不同的语言建模方法是使用*语法*，即生成可接受句子的规则。在小学时，你可能学过词性及其如何组合。例如，如果你有一个非常糟糕的英语老师，你可能会说一个句子必然由*名词*后跟一个*动词*。如果你有名词和动词的列表，你可以根据这个规则生成句子。
- en: 'We’ll define a slightly more complicated grammar:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将定义一个稍微复杂的语法：
- en: '[PRE8]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: I made up the convention that names starting with underscores refer to *rules*
    that need further expanding, and that other names are *terminals* that don’t need
    further processing.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我编制了以下约定：以下划线开头的名称指的是*需要进一步扩展的规则*，而其他名称是*不需要进一步处理的终端*。
- en: So, for example, `"_S"` is the “sentence” rule, which produces an `"_NP"` (“noun
    phrase”) rule followed by a `"_VP"` (“verb phrase”) rule.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，例如，`"_S"`是“句子”规则，它生成一个`"_NP"`（“名词短语”）规则后跟一个`"_VP"`（“动词短语”）规则。
- en: The verb phrase rule can produce either the `"_V"` (“verb”) rule, or the verb
    rule followed by the noun phrase rule.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 动词短语规则可以生成`"_V"`（“动词”）规则，或者动词规则后跟名词短语规则。
- en: Notice that the `"_NP"` rule contains itself in one of its productions. Grammars
    can be recursive, which allows even finite grammars like this to generate infinitely
    many different sentences.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 注意`"_NP"`规则包含在其一个产生中。语法可以是递归的，这使得像这样的有限语法可以生成无限多个不同的句子。
- en: How do we generate sentences from this grammar? We’ll start with a list containing
    the sentence rule `["_S"]`. And then we’ll repeatedly expand each rule by replacing
    it with a randomly chosen one of its productions. We stop when we have a list
    consisting solely of terminals.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何从这个语法生成句子？我们将从包含句子规则`["_S"]`的列表开始。然后，我们将通过用其产生之一随机替换每个规则来重复扩展每个规则。当我们有一个完全由终端组成的列表时，我们停止。
- en: 'For example, one such progression might look like:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，这样的进展可能看起来像：
- en: '[PRE9]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'How do we implement this? Well, to start, we’ll create a simple helper function
    to identify terminals:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何实现这一点？嗯，首先，我们将创建一个简单的辅助函数来识别终端：
- en: '[PRE10]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Next we need to write a function to turn a list of tokens into a sentence. We’ll
    look for the first nonterminal token. If we can’t find one, that means we have
    a completed sentence and we’re done.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要编写一个函数，将标记列表转换为句子。我们将寻找第一个非终结符标记。如果找不到一个，那意味着我们有一个完成的句子，我们就完成了。
- en: If we do find a nonterminal, then we randomly choose one of its productions.
    If that production is a terminal (i.e., a word), we simply replace the token with
    it. Otherwise, it’s a sequence of space-separated nonterminal tokens that we need
    to `split` and then splice into the current tokens. Either way, we repeat the
    process on the new set of tokens.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们找到一个非终结符，然后我们随机选择它的一个产生式。如果该产生式是一个终端（即一个词），我们只需用它替换标记。否则，它是一系列以空格分隔的非终结符标记，我们需要`split`然后在当前标记中插入。无论哪种方式，我们都会在新的标记集上重复这个过程。
- en: 'Putting it all together, we get:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有这些放在一起，我们得到：
- en: '[PRE11]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'And now we can start generating sentences:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以开始生成句子了：
- en: '[PRE12]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Try changing the grammar—add more words, add more rules, add your own parts
    of speech—until you’re ready to generate as many web pages as your company needs.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试改变语法——添加更多单词，添加更多规则，添加你自己的词性——直到你准备生成公司所需的多个网页为止。
- en: Grammars are actually more interesting when they’re used in the other direction.
    Given a sentence, we can use a grammar to *parse* the sentence. This then allows
    us to identify subjects and verbs and helps us make sense of the sentence.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 当语法反向使用时，语法实际上更有趣。给定一个句子，我们可以使用语法*解析*句子。这然后允许我们识别主语和动词，并帮助我们理解句子的意义。
- en: Using data science to generate text is a neat trick; using it to *understand*
    text is more magical. (See [“For Further Exploration”](#nlp-further-invest) for
    libraries that you could use for this.)
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 使用数据科学生成文本是一个很棒的技巧；使用它来*理解*文本更加神奇。（参见[“进一步探索”](#nlp-further-invest)可以用于此目的的库。）
- en: 'An Aside: Gibbs Sampling'
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一个旁注：吉布斯采样
- en: 'Generating samples from some distributions is easy. We can get uniform random
    variables with:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 从一些分布生成样本很容易。我们可以得到均匀随机变量：
- en: '[PRE13]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'and normal random variables with:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 和正常的随机变量一样：
- en: '[PRE14]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: But some distributions are harder to sample from. *Gibbs sampling* is a technique
    for generating samples from multidimensional distributions when we only know some
    of the conditional distributions.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 但是一些分布很难进行抽样。*吉布斯抽样* 是一种从多维分布生成样本的技术，当我们只知道一些条件分布时使用。
- en: 'For example, imagine rolling two dice. Let *x* be the value of the first die
    and *y* be the sum of the dice, and imagine you wanted to generate lots of (*x*,
    *y*) pairs. In this case it’s easy to generate the samples directly:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，想象掷两个骰子。让 *x* 是第一个骰子的值，*y* 是两个骰子的和，想象你想生成大量 (*x*, *y*) 对。在这种情况下，直接生成样本是很容易的：
- en: '[PRE15]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'But imagine that you only knew the conditional distributions. The distribution
    of *y* conditional on *x* is easy—if you know the value of *x*, *y* is equally
    likely to be *x* + 1, *x* + 2, *x* + 3, *x* + 4, *x* + 5, or *x* + 6:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 但是假设你只知道条件分布。知道 *x* 的值时，*y* 的分布很简单——如果你知道 *x* 的值，*y* 同样可能是 *x* + 1、*x* + 2、*x*
    + 3、*x* + 4、*x* + 5 或 *x* + 6：
- en: '[PRE16]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The other direction is more complicated. For example, if you know that *y*
    is 2, then necessarily *x* is 1 (since the only way two dice can sum to 2 is if
    both of them are 1). If you know *y* is 3, then *x* is equally likely to be 1
    or 2\. Similarly, if *y* is 11, then *x* has to be either 5 or 6:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个方向更为复杂。例如，如果你知道 *y* 是2，则必然 *x* 是1（因为使两个骰子的和为2的唯一方法是它们都是1）。如果你知道 *y* 是3，则
    *x* 等可能是1或2。同样，如果 *y* 是11，则 *x* 必须是5或6：
- en: '[PRE17]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The way Gibbs sampling works is that we start with any (valid) values for *x*
    and *y* and then repeatedly alternate replacing *x* with a random value picked
    conditional on *y* and replacing *y* with a random value picked conditional on
    *x*. After a number of iterations, the resulting values of *x* and *y* will represent
    a sample from the unconditional joint distribution:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 吉布斯抽样的工作原理是我们从任意（有效的）*x* 和 *y* 的值开始，然后反复替换，用在*y* 条件下随机选择的值替换 *x*，并在 *x* 条件下随机选择的值替换
    *y*。经过若干次迭代，*x* 和 *y* 的结果值将代表无条件联合分布的一个样本：
- en: '[PRE18]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'You can check that this gives similar results to the direct sample:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以检查这是否给出与直接样本相似的结果：
- en: '[PRE19]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: We’ll use this technique in the next section.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在下一节中使用这种技术。
- en: Topic Modeling
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 主题建模
- en: When we built our “Data Scientists You May Know” recommender in [Chapter 1](ch01.html#introduction),
    we simply looked for exact matches in people’s stated interests.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在[第1章](ch01.html#introduction)中构建“您可能认识的数据科学家”推荐系统时，我们简单地查找人们声明的兴趣的完全匹配。
- en: A more sophisticated approach to understanding our users’ interests might try
    to identify the *topics* that underlie those interests. A technique called *latent
    Dirichlet allocation* (LDA) is commonly used to identify common topics in a set
    of documents. We’ll apply it to documents that consist of each user’s interests.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 更复杂的方法是尝试理解用户兴趣的*主题*。一种称为*潜在狄利克雷分配*（LDA）的技术通常用于识别一组文档中的常见主题。我们将其应用于由每个用户兴趣组成的文档。
- en: 'LDA has some similarities to the Naive Bayes classifier we built in [Chapter 13](ch13.html#naive_bayes),
    in that it assumes a probabilistic model for documents. We’ll gloss over the hairier
    mathematical details, but for our purposes the model assumes that:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: LDA与我们在[第13章](ch13.html#naive_bayes)中构建的朴素贝叶斯分类器有一些相似之处，因为它假设文档的概率模型。对于我们的目的，该模型假设以下内容，我们将略过更复杂的数学细节：
- en: There is some fixed number *K* of topics.
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有一些固定数量 *K* 的主题。
- en: There is a random variable that assigns each topic an associated probability
    distribution over words. You should think of this distribution as the probability
    of seeing word *w* given topic *k*.
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有一个随机变量为每个主题分配与之相关联的单词概率分布。你应该将这个分布看作是给定主题 *k* 下看到单词 *w* 的概率。
- en: There is another random variable that assigns each document a probability distribution
    over topics. You should think of this distribution as the mixture of topics in
    document *d*.
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 还有一个随机变量为每个文档分配一个主题的概率分布。你应该将这个分布看作是文档 *d* 中主题的混合。
- en: Each word in a document was generated by first randomly picking a topic (from
    the document’s distribution of topics) and then randomly picking a word (from
    the topic’s distribution of words).
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文档中的每个单词是通过首先随机选择一个主题（从文档的主题分布中）然后随机选择一个单词（从主题的单词分布中）生成的。
- en: In particular, we have a collection of `documents`, each of which is a `list`
    of words. And we have a corresponding collection of `document_topics` that assigns
    a topic (here a number between 0 and *K* – 1) to each word in each document.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是，我们有一个`documents`的集合，每个文档都是一个单词的`list`。并且我们有一个相应的`document_topics`集合，它为每个文档中的每个单词分配一个主题（这里是0到*K*-1之间的数字）。
- en: 'So, the fifth word in the fourth document is:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，第四个文档中的第五个单词是：
- en: '[PRE20]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'and the topic from which that word was chosen is:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 选择该单词的主题是：
- en: '[PRE21]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: This very explicitly defines each document’s distribution over topics, and it
    implicitly defines each topic’s distribution over words.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这非常明确地定义了每个文档在主题上的分布，并且隐含地定义了每个主题在单词上的分布。
- en: We can estimate the likelihood that topic 1 produces a certain word by comparing
    how many times topic 1 produces that word with how many times topic 1 produces
    *any* word. (Similarly, when we built a spam filter in [Chapter 13](ch13.html#naive_bayes),
    we compared how many times each word appeared in spams with the total number of
    words appearing in spams.)
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 通过比较主题1生成该单词的次数与主题1生成*任何*单词的次数，我们可以估计主题1生成某个单词的可能性。（类似地，在[第13章](ch13.html#naive_bayes)中建立垃圾邮件过滤器时，我们比较了每个单词在垃圾邮件中出现的次数与垃圾邮件中出现的总字数。）
- en: Although these topics are just numbers, we can give them descriptive names by
    looking at the words on which they put the heaviest weight. We just have to somehow
    generate the `document_topics`. This is where Gibbs sampling comes into play.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这些主题只是数字，但我们可以通过查看它们赋予最高权重的单词来为它们命名描述性名称。我们只需以某种方式生成`document_topics`。这就是吉布斯抽样发挥作用的地方。
- en: We start by assigning every word in every document a topic completely at random.
    Now we go through each document one word at a time. For that word and document,
    we construct weights for each topic that depend on the (current) distribution
    of topics in that document and the (current) distribution of words for that topic.
    We then use those weights to sample a new topic for that word. If we iterate this
    process many times, we will end up with a joint sample from the topic–word distribution
    and the document–topic distribution.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先随机地为每个文档中的每个单词分配一个主题。现在我们逐个单词地遍历每个文档。对于该单词和文档，我们为每个主题构造依赖于该文档中主题的（当前）分布和该主题中单词的（当前）分布的权重。然后我们使用这些权重来对该单词抽样一个新的主题。如果我们多次迭代这个过程，我们最终会得到从主题-单词分布和文档-主题分布的联合样本。
- en: 'To start with, we’ll need a function to randomly choose an index based on an
    arbitrary set of weights:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 起步，我们需要一个函数根据任意一组权重随机选择一个索引：
- en: '[PRE22]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'For instance, if you give it weights `[1, 1, 3]`, then one-fifth of the time
    it will return 0, one-fifth of the time it will return 1, and three-fifths of
    the time it will return 2\. Let’s write a test:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果给定权重 `[1, 1, 3]`，那么它将返回0的概率为五分之一，返回1的概率为五分之一，返回2的概率为三分之五。让我们编写一个测试：
- en: '[PRE23]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Our documents are our users’ interests, which look like:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的文档是我们用户的兴趣，看起来像：
- en: '[PRE24]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'And we’ll try to find:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将尝试找到：
- en: '[PRE25]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: topics. In order to calculate the sampling weights, we’ll need to keep track
    of several counts. Let’s first create the data structures for them.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 主题。为了计算抽样权重，我们需要跟踪几个计数。让我们首先为它们创建数据结构。
- en: 'How many times each topic is assigned to each document:'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个主题分配给每个文档的次数是：
- en: '[PRE26]'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'How many times each word is assigned to each topic:'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个单词分配到每个主题的次数是：
- en: '[PRE27]'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The total number of words assigned to each topic:'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分配给每个主题的总字数是：
- en: '[PRE28]'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The total number of words contained in each document:'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个文档包含的总字数是：
- en: '[PRE29]'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The number of distinct words:'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不同单词的数量是：
- en: '[PRE30]'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'And the number of documents:'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以及文档的数量：
- en: '[PRE31]'
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Once we populate these, we can find, for example, the number of words in `documents[3]`
    associated with topic 1 as follows:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们填充了这些数据，我们可以如下找到例如与主题1相关联的`documents[3]`中的单词数：
- en: '[PRE32]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'And we can find the number of times *nlp* is associated with topic 2 as follows:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 并且我们可以找到*nlp*与主题2相关联的次数如下：
- en: '[PRE33]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Now we’re ready to define our conditional probability functions. As in [Chapter 13](ch13.html#naive_bayes),
    each has a smoothing term that ensures every topic has a nonzero chance of being
    chosen in any document and that every word has a nonzero chance of being chosen
    for any topic:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备定义我们的条件概率函数。就像[第13章](ch13.html#naive_bayes)中那样，每个函数都有一个平滑项，确保每个主题在任何文档中被选择的概率都不为零，并且每个单词在任何主题中被选择的概率也不为零：
- en: '[PRE34]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'We’ll use these to create the weights for updating topics:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用这些函数来创建更新主题的权重：
- en: '[PRE35]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: There are solid mathematical reasons why `topic_weight` is defined the way it
    is, but their details would lead us too far afield. Hopefully it makes at least
    intuitive sense that—given a word and its document—the likelihood of any topic
    choice depends on both how likely that topic is for the document and how likely
    that word is for the topic.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 有坚实的数学原因解释为什么 `topic_weight` 被定义为它的方式，但是它们的细节会让我们走得太远。希望至少直观地理解，鉴于一个词和它的文档，选择任何主题的可能性取决于该主题对文档的可能性以及该词对该主题的可能性。
- en: 'This is all the machinery we need. We start by assigning every word to a random
    topic and populating our counters appropriately:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们需要的所有机制。我们从将每个单词分配给一个随机主题开始，并相应地填充我们的计数器：
- en: '[PRE36]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Our goal is to get a joint sample of the topics–word distribution and the documents–topic
    distribution. We do this using a form of Gibbs sampling that uses the conditional
    probabilities defined previously:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是获取主题-词分布和文档-主题分布的联合样本。我们使用一种基于吉布斯抽样的形式来完成这一过程，该过程使用之前定义的条件概率：
- en: '[PRE37]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'What are the topics? They’re just numbers 0, 1, 2, and 3\. If we want names
    for them, we have to do that ourselves. Let’s look at the five most heavily weighted
    words for each ([Table 21-1](#table20-1)):'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 主题是什么？它们只是数字 0、1、2 和 3。如果我们想要它们的名称，我们必须自己添加。让我们看看每个主题的五个权重最高的词汇（[表 21-1](#table20-1)）：
- en: '[PRE38]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Table 21-1\. Most common words per topic
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 表 21-1\. 每个主题的最常见词汇
- en: '| Topic 0 | Topic 1 | Topic 2 | Topic 3 |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| 主题 0 | 主题 1 | 主题 2 | 主题 3 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Java | R | HBase | regression |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| Java | R | HBase | 回归分析 |'
- en: '| Big Data | statistics | Postgres | libsvm |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| 大数据 | 统计 | Postgres | libsvm |'
- en: '| Hadoop | Python | MongoDB | scikit-learn |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| Hadoop | Python | MongoDB | scikit-learn |'
- en: '| deep learning | probability | Cassandra | machine learning |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| 深度学习 | 概率 | 卡桑德拉 | 机器学习 |'
- en: '| artificial intelligence | pandas | NoSQL | neural networks |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| 人工智能 | 熊猫 | NoSQL | 神经网络 |'
- en: 'Based on these I’d probably assign topic names:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这些，我可能会分配主题名称：
- en: '[PRE39]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'at which point we can see how the model assigns topics to each user’s interests:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们可以看到模型如何将主题分配给每个用户的兴趣：
- en: '[PRE40]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'which gives:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 这给出了：
- en: '[PRE41]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: and so on. Given the “ands” we needed in some of our topic names, it’s possible
    we should use more topics, although most likely we don’t have enough data to successfully
    learn them.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 等等。考虑到我们在一些主题名称中需要使用的“和”，可能我们应该使用更多的主题，尽管最可能我们没有足够的数据成功学习它们。
- en: Word Vectors
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 单词向量
- en: A lot of recent advances in NLP involve deep learning. In the rest of this chapter
    we’ll look at a couple of them using the machinery we developed in [Chapter 19](ch19.html#deep_learning).
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 最近自然语言处理的许多进展涉及深度学习。在本章的其余部分中，我们将使用我们在[第19章](ch19.html#deep_learning)中开发的机制来看一些这样的进展。
- en: One important innovation involves representing words as low-dimensional vectors.
    These vectors can be compared, added together, fed into machine learning models,
    or anything else you want to do with them. They usually have nice properties;
    for example, similar words tend to have similar vectors. That is, typically the
    word vector for *big* is pretty close to the word vector for *large*, so that
    a model operating on word vectors can (to some degree) handle things like synonymy
    for free.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 一个重要的创新涉及将单词表示为低维向量。这些向量可以进行比较、相加、输入到机器学习模型中，或者任何你想做的事情。它们通常具有良好的特性；例如，相似的单词倾向于有相似的向量。也就是说，通常单词
    *big* 的向量与单词 *large* 的向量非常接近，因此一个操作单词向量的模型可以（在某种程度上）免费处理类似的词语。
- en: Frequently the vectors will exhibit delightful arithmetic properties as well.
    For instance, in some such models if you take the vector for *king*, subtract
    the vector for *man*, and add the vector for *woman*, you will end up with a vector
    that’s very close to the vector for *queen*. It can be interesting to ponder what
    this means about what the word vectors actually “learn,” although we won’t spend
    time on that here.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 经常向量也会展示出令人愉悦的算术特性。例如，在某些模型中，如果你取 *king* 的向量，减去 *man* 的向量，再加上 *woman* 的向量，你将得到一个非常接近
    *queen* 向量的向量。思考这对于单词向量实际上“学到”了什么，可能会很有趣，尽管我们在这里不会花时间讨论这一点。
- en: 'Coming up with such vectors for a large vocabulary of words is a difficult
    undertaking, so typically we’ll *learn* them from a corpus of text. There are
    a couple of different schemes, but at a high level the task typically looks something
    like this:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个庞大的词汇表来说，设计这样的向量是一项困难的任务，所以通常我们会从文本语料库中 *学习* 它们。有几种不同的方案，但在高层次上，任务通常看起来像这样：
- en: Get a bunch of text.
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取一堆文本。
- en: Create a dataset where the goal is to predict a word given nearby words (or
    alternatively, to predict nearby words given a word).
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个数据集，目标是预测给定附近单词的单词（或者，预测给定单词的附近单词）。
- en: Train a neural net to do well on this task.
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练一个神经网络在这个任务上表现良好。
- en: Take the internal states of the trained neural net as the word vectors.
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将训练好的神经网络的内部状态作为单词向量。
- en: In particular, because the task is to predict a word given nearby words, words
    that occur in similar contexts (and hence have similar nearby words) should have
    similar internal states and therefore similar word vectors.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是，由于任务是根据附近的单词预测单词，出现在类似上下文中的单词（因此具有类似的附近单词）应该具有类似的内部状态，因此也应该具有相似的单词向量。
- en: 'Here we’ll measure “similarity” using *cosine similarity*, which is a number
    between –1 and 1 that measures the degree to which two vectors point in the same
    direction:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 *余弦相似度*（cosine similarity）来衡量“相似性”，它是一个介于-1和1之间的数值，用于衡量两个向量指向相同方向的程度：
- en: '[PRE42]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Let’s learn some word vectors to see how this works.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们学习一些词向量，看看它是如何工作的。
- en: 'To start with, we’ll need a toy dataset. The commonly used word vectors are
    typically derived from training on millions or even billions of words. As our
    toy library can’t cope with that much data, we’ll create an artificial dataset
    with some structure to it:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要一个玩具数据集。通常使用的单词向量通常是通过在数百万甚至数十亿个单词上训练而来的。由于我们的玩具库无法处理那么多数据，我们将创建一个具有某些结构的人工数据集：
- en: '[PRE43]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: This will generate lots of sentences with similar structure but different words;
    for example, “The green boat seems quite slow.” Given this setup, the colors will
    mostly appear in “similar” contexts, as will the nouns, and so on. So if we do
    a good job of assigning word vectors, the colors should get similar vectors, and
    so on.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成许多具有类似结构但不同单词的句子；例如，“绿色的船似乎相当慢。” 在这种设置下，颜色将主要出现在“相似”的上下文中，名词也是如此，依此类推。因此，如果我们成功地分配了单词向量，颜色应该会得到相似的向量，依此类推。
- en: Note
  id: totrans-168
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: In practical usage, you’d probably have a corpus of millions of sentences, in
    which case you’d get “enough” context from the sentences as they are. Here, with
    only 50 sentences, we have to make them somewhat artificial.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际使用中，您可能会有数百万个句子的语料库，在这种情况下，您将从句子中获得“足够的”上下文。在这里，我们只有50个句子，我们必须使它们有些人为的。
- en: 'As mentioned earlier, we’ll want to one-hot-encode our words, which means we’ll
    need to convert them to IDs. We’ll introduce a `Vocabulary` class to keep track
    of this mapping:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们将需要对我们的单词进行一位有效编码，这意味着我们需要将它们转换为ID。我们将引入一个`Vocabulary`类来跟踪这个映射：
- en: '[PRE44]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'These are all things we could do manually, but it’s handy to have it in a class.
    We should probably test it:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 这些都是我们可以手动完成的事情，但将它们放在一个类中很方便。我们可能应该测试它：
- en: '[PRE45]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'We should also write simple helper functions to save and load a vocabulary,
    just as we have for our deep learning models:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还应该编写简单的辅助函数来保存和加载词汇表，就像我们为深度学习模型所做的那样：
- en: '[PRE46]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: We’ll be using a word vector model called *skip-gram* that takes as input a
    word and generates probabilities for what words are likely to be seen near it.
    We will feed it training pairs `(word, nearby_word)` and try to minimize the `SoftmaxCrossEntropy`
    loss.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用一个称为 *skip-gram* 的词向量模型，它以单词作为输入，并生成可能性，表明哪些单词可能会在附近出现。我们将向其提供训练对 `(单词,
    附近单词)` 并尝试最小化 `SoftmaxCrossEntropy` 损失。
- en: Note
  id: totrans-177
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Another common model, *continuous bag-of-words* (CBOW), takes the nearby words
    as the inputs and tries to predict the original word.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个常见的模型，*连续词袋*（continuous bag-of-words，CBOW），将附近的单词作为输入，并尝试预测原始单词。
- en: Let’s design our neural network. At its heart will be an *embedding* layer that
    takes as input a word ID and returns a word vector. Under the covers we can just
    use a lookup table for this.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们设计我们的神经网络。在其核心将是一个 *嵌入*（embedding）层，它以单词ID作为输入并返回一个单词向量。在内部，我们可以简单地使用查找表来实现这一点。
- en: We’ll then pass the word vector to a `Linear` layer with the same number of
    outputs as we have words in our vocabulary. As before, we’ll use `softmax` to
    convert these outputs to probabilities over nearby words. As we use gradient descent
    to train the model, we will be updating the vectors in the lookup table. Once
    we’ve finished training, that lookup table gives us our word vectors.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将单词向量传递给一个具有与我们词汇表中单词数量相同的输出的 `Linear` 层。与以前一样，我们将使用 `softmax` 将这些输出转换为附近单词的概率。当我们使用梯度下降训练模型时，我们将更新查找表中的向量。训练完成后，该查找表为我们提供了单词向量。
- en: Let’s create that embedding layer. In practice we might want to embed things
    other than words, so we’ll construct a more general `Embedding` layer. (Later
    we’ll write a `TextEmbedding` subclass that’s specifically for word vectors.)
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建那个嵌入层。实际上，我们可能希望嵌入除单词之外的其他内容，因此我们将构建一个更通用的`Embedding`层。（稍后我们将编写一个`TextEmbedding`子类，专门用于词向量。）
- en: 'In its constructor we’ll provide the number and dimension of our embedding
    vectors, so it can create the embeddings (which will be standard random normals,
    initially):'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在其构造函数中，我们将提供我们嵌入向量的数量和维度，因此它可以创建嵌入（最初将是标准的随机正态分布）：
- en: '[PRE47]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: In our case we’ll only be embedding one word at a time. However, in other models
    we might want to embed a sequence of words and get back a sequence of word vectors.
    (For example, if we wanted to train the CBOW model described earlier.) So an alternative
    design would take sequences of word IDs. We’ll stick with one at a time, to make
    things simpler.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的情况下，我们一次只嵌入一个词。然而，在其他模型中，我们可能希望嵌入一个词序列并返回一个词向量序列。（例如，如果我们想要训练前面描述的CBOW模型。）因此，另一种设计将采用单词ID序列。我们将坚持一次只处理一个，以简化事务。
- en: '[PRE48]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'For the backward pass we’ll get a gradient corresponding to the chosen embedding
    vector, and we’ll need to construct the corresponding gradient for `self.embeddings`,
    which is zero for every embedding other than the chosen one:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 对于反向传播，我们将获得对应于所选嵌入向量的梯度，并且我们需要为`self.embeddings`构建相应的梯度，对于除所选之外的每个嵌入，其梯度都为零：
- en: '[PRE49]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Because we have parameters and gradients, we need to override those methods:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们有参数和梯度，我们需要重写那些方法：
- en: '[PRE50]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'As mentioned earlier, we’ll want a subclass specifically for word vectors.
    In that case our number of embeddings is determined by our vocabulary, so let’s
    just pass that in instead:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们需要一个专门用于词向量的子类。在这种情况下，我们的嵌入数量由我们的词汇决定，所以让我们直接传入它：
- en: '[PRE51]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: The other built-in methods will all work as is, but we’ll add a couple more
    methods specific to working with text. For example, we’d like to be able to retrieve
    the vector for a given word. (This is not part of the `Layer` interface, but we
    are always free to add extra methods to specific layers as we like.)
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 所有其他内置方法都可以原样工作，但我们将添加一些特定于文本处理的方法。例如，我们希望能够检索给定单词的向量。（这不是`Layer`接口的一部分，但我们始终可以根据需要向特定层添加额外的方法。）
- en: '[PRE52]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'This dunder method will allow us to retrieve word vectors using indexing:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 这个dunder方法将允许我们使用索引检索单词向量：
- en: '[PRE53]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'And we’d also like the embedding layer to tell us the closest words to a given
    word:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还希望嵌入层告诉我们给定单词的最接近的单词：
- en: '[PRE54]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: Our embedding layer just outputs vectors, which we can feed into a `Linear`
    layer.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的嵌入层只输出向量，我们可以将其馈送到`Linear`层中。
- en: Now we’re ready to assemble our training data. For each input word, we’ll choose
    as target words the two words to its left and the two words to its right.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备好组装我们的训练数据。对于每个输入单词，我们将选择其左边的两个单词和右边的两个单词作为目标单词。
- en: 'Let’s start by lowercasing the sentences and splitting them into words:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从将句子转换为小写并拆分为单词开始：
- en: '[PRE55]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'at which point we can construct a vocabulary:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 在此之后，我们可以构建一个词汇表：
- en: '[PRE56]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'And now we can create training data:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以创建训练数据：
- en: '[PRE57]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'With the machinery we’ve built up, it’s now easy to create our model:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 利用我们建立的机制，现在很容易创建我们的模型：
- en: '[PRE58]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Using the machinery from [Chapter 19](ch19.html#deep_learning), it’s easy to
    train our model:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 使用来自[第19章](ch19.html#deep_learning)的工具，训练我们的模型非常容易：
- en: '[PRE59]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: As you watch this train, you can see the colors getting closer to each other,
    the adjectives getting closer to each other, and the nouns getting closer to each
    other.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 当你看着这个训练过程时，你会看到颜色变得越来越接近，形容词变得越来越接近，名词也变得越来越接近。
- en: 'Once the model is trained, it’s fun to explore the most similar words:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 模型训练完成后，探索最相似的单词是件有趣的事情：
- en: '[PRE60]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'which (for me) results in:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 这对我来说结果如下：
- en: '[PRE61]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: (Obviously *bed* and *cat* are not really similar, but in our training sentences
    they appear to be, and that’s what the model is capturing.)
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: （显然*bed*和*cat*并不真正相似，但在我们的训练句子中它们似乎相似，并且模型捕捉到了这一点。）
- en: 'We can also extract the first two principal components and plot them:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以提取前两个主成分并将它们绘制出来：
- en: '[PRE62]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'which shows that similar words are indeed clustering together ([Figure 21-3](#word_vectors_plot)):'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明相似的单词确实聚集在一起（参见[图21-3](#word_vectors_plot)）：
- en: '![Word Vectors](assets/dsf2_2103.png)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![单词向量](assets/dsf2_2103.png)'
- en: Figure 21-3\. Word vectors
  id: totrans-220
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图21-3. 单词向量
- en: If you’re interested, it’s not hard to train CBOW word vectors. You’ll have
    to do a little work. First, you’ll need to modify the `Embedding` layer so that
    it takes as input a *list* of IDs and outputs a *list* of embedding vectors. Then
    you’ll have to create a new layer (`Sum`?) that takes a list of vectors and returns
    their sum.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你感兴趣，训练CBOW词向量并不难。你需要做一些工作。首先，你需要修改`Embedding`层，使其接受一个*ID列表*作为输入，并输出一个*嵌入向量列表*。然后你需要创建一个新的层（`Sum`？），它接受一个向量列表并返回它们的总和。
- en: Each word represents a training example where the input is the word IDs for
    the surrounding words, and the target is the one-hot encoding of the word itself.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 每个单词表示一个训练示例，其中输入是周围单词的单词ID，目标是单词本身的独热编码。
- en: The modified `Embedding` layer turns the surrounding words into a list of vectors,
    the new `Sum` layer collapses the list of vectors down to a single vector, and
    then a `Linear` layer can produce scores that can be `softmax`ed to get a distribution
    representing “most likely words, given this context.”
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 修改后的`Embedding`层将周围的单词转换为向量列表，新的`Sum`层将向量列表合并为单个向量，然后`Linear`层可以生成分数，这些分数可以经过`softmax`处理，得到表示“在这个上下文中最可能的单词”的分布。
- en: I found the CBOW model harder to train than the skip-gram one, but I encourage
    you to try it out.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 我发现CBOW模型比跳字模型更难训练，但我鼓励你去尝试一下。
- en: Recurrent Neural Networks
  id: totrans-225
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 循环神经网络
- en: 'The word vectors we developed in the previous section are often used as the
    inputs to neural networks. One challenge to doing this is that sentences have
    varying lengths: you could think of a 3-word sentence as a `[3, embedding_dim]`
    tensor and a 10-word sentence as a `[10, embedding_dim]` tensor. In order to,
    say, pass them to a `Linear` layer, we need to do something about that first variable-length
    dimension.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在前一节中开发的单词向量通常用作神经网络的输入。做这件事的一个挑战是句子的长度不同：你可以将一个三个单词的句子想象为一个`[3, embedding_dim]`张量，而一个十个单词的句子想象为一个`[10,
    embedding_dim]`张量。为了，比如，将它们传递给`Linear`层，我们首先需要处理第一个可变长度维度。
- en: One option is to use a `Sum` layer (or a variant that takes the average); however,
    the *order* of the words in a sentence is usually important to its meaning. To
    take a common example, “dog bites man” and “man bites dog” are two very different
    stories!
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 一个选择是使用`Sum`层（或者一个接受平均值的变体）；然而，句子中单词的*顺序*通常对其含义很重要。以一个常见的例子来说，“狗咬人”和“人咬狗”是两个非常不同的故事！
- en: Another way of handling this is using *recurrent neural networks* (RNNs), which
    have a *hidden state* they maintain between inputs. In the simplest case, each
    input is combined with the current hidden state to produce an output, which is
    then used as the new hidden state. This allows such networks to “remember” (in
    a sense) the inputs they’ve seen, and to build up to a final output that depends
    on all the inputs and their order.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 处理这个问题的另一种方法是使用*循环神经网络*（RNNs），它们具有它们在输入之间保持的*隐藏状态*。在最简单的情况下，每个输入与当前隐藏状态结合以产生输出，然后将其用作新的隐藏状态。这允许这些网络在某种意义上“记住”它们看到的输入，并建立到依赖于所有输入及其顺序的最终输出。
- en: We’ll create pretty much the simplest possible RNN layer, which will accept
    a single input (corresponding to, e.g., a single word in a sentence, or a single
    character in a word), and which will maintain its hidden state between calls.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将创建一个非常简单的RNN层，它将接受单个输入（例如句子中的一个单词或一个单词中的一个字符），并在调用之间保持其隐藏状态。
- en: 'Recall that our `Linear` layer had some weights, `w`, and a bias, `b`. It took
    a vector `input` and produced a different vector as `output` using the logic:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，我们的`Linear`层有一些权重，`w`，和一个偏置，`b`。它接受一个向量`input`并使用逻辑生成不同的向量作为`output`：
- en: '[PRE63]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Here we’ll want to incorporate our hidden state, so we’ll have *two* sets of
    weights—one to apply to the `input` and one to apply to the previous `hidden`
    state:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 这里我们将要整合我们的隐藏状态，因此我们将有*两组*权重——一组用于应用于`input`，另一组用于前一个`hidden`状态：
- en: '[PRE64]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: Next, we’ll use the `output` vector as the new value of `hidden`. This isn’t
    a huge change, but it will allow our networks to do wonderful things.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用`output`向量作为新的`hidden`值。这并不是一个巨大的改变，但它将使我们的网络能够做出奇妙的事情。
- en: '[PRE65]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: You can see that we start out the hidden state as a vector of 0s, and we provide
    a function that people using the network can call to reset the hidden state.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，我们将隐藏状态初始为一个0向量，并提供一个函数，供使用网络的人调用以重置隐藏状态。
- en: 'Given this setup, the `forward` function is reasonably straightforward (at
    least, it is if you remember and understand how our `Linear` layer worked):'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种设置下，`forward`函数相对直接（至少，如果你记得并理解我们的`Linear`层是如何工作的话）：
- en: '[PRE66]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'The `backward` pass is similar to the one in our `Linear` layer, except that
    it needs to compute an additional set of gradients for the `u` weights:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '`backward`传递类似于我们`Linear`层中的传递，只是需要计算额外的`u`权重的梯度：'
- en: '[PRE67]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'And finally we need to override the `params` and `grads` methods:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们需要重写`params`和`grads`方法：
- en: '[PRE68]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: Warning
  id: totrans-243
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: This “simple” RNN is so simple that you probably shouldn’t use it in practice.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 这个“简单”的RNN实在太简单了，你可能不应该在实践中使用它。
- en: Our `SimpleRnn` has a couple of undesirable features. One is that its entire
    hidden state is used to update the input every time you call it. The other is
    that the entire hidden state is overwritten every time you call it. Both of these
    make it difficult to train; in particular, they make it difficult for the model
    to learn long-range dependencies.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的`SimpleRnn`有几个不理想的特性。其中一个是每次调用它时，它的整个隐藏状态都用来更新输入。另一个是每次调用它时，整个隐藏状态都会被覆盖。这两点使得训练变得困难；特别是，它使模型难以学习长期依赖性。
- en: For this reason, almost no one uses this kind of simple RNN. Instead, they use
    more complicated variants like the LSTM (“long short-term memory”) or the GRU
    (“gated recurrent unit”), which have many more parameters and use parameterized
    “gates” that allow only some of the state to be updated (and only some of the
    state to be used) at each timestep.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，几乎没有人使用这种简单的RNN。相反，他们使用更复杂的变体，如LSTM（“长短期记忆”）或GRU（“门控循环单元”），这些变体有更多的参数，并使用参数化的“门”来允许每个时间步只更新一部分状态（并且只使用一部分状态）。
- en: There is nothing particularly *difficult* about these variants; however, they
    involve a great deal more code, which would not be (in my opinion) correspondingly
    more edifying to read. The code for this chapter on [GitHub](https://github.com/joelgrus/data-science-from-scratch)
    includes an LSTM implementation. I encourage you to check it out, but it’s somewhat
    tedious and so we won’t discuss it further here.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 这些变体并没有什么特别的*困难*；然而，它们涉及更多的代码，我认为阅读起来并不会相应更具有教育性。本章的代码可以在[GitHub](https://github.com/joelgrus/data-science-from-scratch)找到，其中包括了LSTM的实现。我鼓励你去看看，但这有点乏味，所以我们在这里不再详细讨论。
- en: One other quirk of our implementation is that it takes only one “step” at a
    time and requires us to manually reset the hidden state. A more practical RNN
    implementation might accept sequences of inputs, set its hidden state to 0s at
    the beginning of each sequence, and produce sequences of outputs. Ours could certainly
    be modified to behave this way; again, this would require more code and complexity
    for little gain in understanding.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实现的另一个怪癖是，它每次只处理一个“步骤”，并且需要我们手动重置隐藏状态。一个更实用的RNN实现可以接受输入序列，将其隐藏状态在每个序列开始时设为0，并生成输出序列。我们的实现肯定可以修改成这种方式；同样地，这将需要更多的代码和复杂性，而对理解的帮助不大。
- en: 'Example: Using a Character-Level RNN'
  id: totrans-249
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 示例：使用字符级别的循环神经网络
- en: The newly hired VP of Branding did not come up with the name *DataSciencester*
    himself, and (accordingly) he suspects that a better name might lead to more success
    for the company. He asks you to use data science to suggest candidates for replacement.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 新任品牌副总裁并不是亲自想出*DataSciencester*这个名字的，因此他怀疑，换一个更好的名字可能会更有利于公司的成功。他请你使用数据科学来提出替换的候选名。
- en: One “cute” application of RNNs involves using *characters* (rather than words)
    as their inputs, training them to learn the subtle language patterns in some dataset,
    and then using them to generate fictional instances from that dataset.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: RNN的一个“可爱”的应用包括使用*字符*（而不是单词）作为它们的输入，训练它们学习某个数据集中微妙的语言模式，然后使用它们生成该数据集的虚构实例。
- en: For example, you could train an RNN on the names of alternative bands, use the
    trained model to generate new names for fake alternative bands, and then hand-select
    the funniest ones and share them on Twitter. Hilarity!
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，你可以训练一个RNN来学习另类乐队的名称，使用训练好的模型来生成新的假另类乐队的名称，然后手动选择最有趣的名称并分享在Twitter上。太有趣了！
- en: Having seen this trick enough times to no longer consider it clever, you decide
    to give it a shot.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 见过这个技巧很多次后，你不再认为它很聪明，但你决定试试看。
- en: 'After some digging, you find that the startup accelerator Y Combinator has
    published [a list of its top 100 (actually 101) most successful startups](https://www.ycombinator.com/topcompanies/),
    which seems like a good starting point. Checking the page, you find that the company
    names all live inside `<b class="h4">` tags, which means it’s easy to use your
    web scraping skills to retrieve them:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 经过一番调查，你发现创业加速器 Y Combinator 发布了其[最成功的100（实际上是101）家初创企业的列表](https://www.ycombinator.com/topcompanies/)，这看起来是一个很好的起点。检查页面后，你发现公司名称都位于`<b
    class="h4">`标签内，这意味着你可以轻松使用你的网络爬虫技能来获取它们：
- en: '[PRE69]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: As always, the page may change (or vanish), in which case this code won’t work.
    If so, you can use your newly learned data science skills to fix it or just get
    the list from the book’s GitHub site.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 和往常一样，页面可能会发生变化（或消失），这种情况下这段代码就不起作用了。如果是这样，你可以使用你新学到的数据科学技能来修复它，或者直接从书的 GitHub
    站点获取列表。
- en: So what is our plan? We’ll train a model to predict the next character of a
    name, given the current character *and* a hidden state representing all the characters
    we’ve seen so far.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 那么我们的计划是什么呢？我们将训练一个模型来预测名称的下一个字符，给定当前字符和表示到目前为止所有字符的隐藏状态。
- en: As usual, we’ll actually predict a probability distribution over characters
    and train our model to minimize the `SoftmaxCrossEntropy` loss.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 和往常一样，我们将预测字符的概率分布，并训练我们的模型以最小化`SoftmaxCrossEntropy`损失。
- en: Once our model is trained, we can use it to generate some probabilities, randomly
    sample a character according to those probabilities, and then feed that character
    as its next input. This will allow us to *generate* company names using the learned
    weights.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们的模型训练好了，我们可以使用它生成一些概率，根据这些概率随机抽取一个字符，然后将该字符作为下一个输入。这将允许我们使用学到的权重*生成*公司名称。
- en: 'To start with, we should build a `Vocabulary` from the characters in the names:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始，我们应该从名称中构建一个`Vocabulary`：
- en: '[PRE70]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: In addition, we’ll use special tokens to signify the start and end of a company
    name. This allows the model to learn which characters should *begin* a company
    name and also to learn when a company name is *finished*.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们将使用特殊的标记来表示公司名称的开始和结束。这允许模型学习哪些字符应该*开始*一个公司名称，以及何时一个公司名称*结束*。
- en: 'We’ll just use the regex characters for start and end, which (luckily) don’t
    appear in our list of companies:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将只使用正则表达式字符来表示开始和结束，这些字符（幸运的是）不会出现在我们的公司名称列表中：
- en: '[PRE71]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'For our model, we’ll one-hot-encode each character, pass it through two `SimpleRnn`s,
    and then use a `Linear` layer to generate the scores for each possible next character:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的模型，我们将对每个字符进行独热编码，通过两个`SimpleRnn`传递它们，然后使用`Linear`层生成每个可能的下一个字符的分数：
- en: '[PRE72]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'Imagine for the moment that we’ve trained this model. Let’s write the function
    that uses it to generate new company names, using the `sample_from` function from
    [“Topic Modeling”](#topic_modeling):'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们已经训练好了这个模型。让我们编写一个函数，使用来自[“主题建模”](#topic_modeling)的`sample_from`函数来生成新的公司名称：
- en: '[PRE73]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: At long last, we’re ready to train our character-level RNN. It will take a while!
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 终于，我们准备好训练我们的字符级 RNN。这会花费一些时间！
- en: '[PRE74]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: After training, the model generates some actual names from the list (which isn’t
    surprising, since the model has a fair amount of capacity and not a lot of training
    data), as well as names that are only slightly different from training names (Scripe,
    Loinbare, Pozium), names that seem genuinely creative (Benuus, Cletpo, Equite,
    Vivest), and names that are garbage-y but still sort of word-like (SFitreasy,
    Sint ocanelp, GliyOx, Doorboronelhav).
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 训练后，模型生成了一些实际的名称（这并不奇怪，因为模型具有相当的容量，但训练数据并不多），以及与训练名称略有不同的名称（Scripe, Loinbare,
    Pozium），看起来确实创意十足的名称（Benuus, Cletpo, Equite, Vivest），以及类似单词但是有点垃圾的名称（SFitreasy,
    Sint ocanelp, GliyOx, Doorboronelhav）。
- en: Unfortunately, like most character-level-RNN outputs, these are only mildly
    clever, and the VP of Branding ends up unable to use them.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，像大多数字符级 RNN 输出一样，这些名称只是略有创意，品牌副总裁最终无法使用它们。
- en: If I up the hidden dimension to 64, I get a lot more names verbatim from the
    list; if I drop it to 8, I get mostly garbage. The vocabulary and final weights
    for all these model sizes are available on [the book’s GitHub site](https://github.com/joelgrus/data-science-from-scratch),
    and you can use `load_weights` and `load_vocab` to use them yourself.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我将隐藏维度提升到64，我将从列表中获得更多名称的原样；如果我将其降至8，我将得到大多数垃圾名称。所有这些模型尺寸的词汇表和最终权重都可以在[书的
    GitHub 站点](https://github.com/joelgrus/data-science-from-scratch)上找到，并且你可以使用`load_weights`和`load_vocab`来自己使用它们。
- en: As mentioned previously, the GitHub code for this chapter also contains an implementation
    for an LSTM, which you should feel free to swap in as a replacement for the `SimpleRnn`s
    in our company name model.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 正如前面提到的，本章的 GitHub 代码还包含了 LSTM 的实现，你可以自由地将其替换为我们公司名称模型中的 `SimpleRnn`。
- en: For Further Exploration
  id: totrans-275
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步探索
- en: '[NLTK](http://www.nltk.org/) is a popular library of NLP tools for Python.
    It has its own entire [book](http://www.nltk.org/book/), which is available to
    read online.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[NLTK](http://www.nltk.org/) 是一个流行的 Python 自然语言处理工具库。它有自己的整本 [书籍](http://www.nltk.org/book/)，可以在线阅读。'
- en: '[gensim](http://radimrehurek.com/gensim/) is a Python library for topic modeling,
    which is a better bet than our from-scratch model.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[gensim](http://radimrehurek.com/gensim/) 是一个用于主题建模的 Python 库，比我们从头开始的模型更可靠。'
- en: '[spaCy](https://spacy.io/) is a library for “Industrial Strength Natural Language
    Processing in Python” and is also quite popular.'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[spaCy](https://spacy.io/) 是一个用于“Python 中的工业级自然语言处理”库，也非常受欢迎。'
- en: Andrej Karpathy has a famous blog post, [“The Unreasonable Effectiveness of
    Recurrent Neural Networks”](http://karpathy.github.io/2015/05/21/rnn-effectiveness/),
    that’s very much worth reading.
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Andrej Karpathy 有一篇著名的博文，[“递归神经网络的非理性有效性”](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)，非常值得一读。
- en: My day job involves building [AllenNLP](https://allennlp.org/), a Python library
    for doing NLP research. (At least, as of the time this book went to press, it
    did.) The library is quite beyond the scope of this book, but you might still
    find it interesting, and it has a cool interactive demo of many state-of-the-art
    NLP models.
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我的日常工作涉及构建 [AllenNLP](https://allennlp.org/)，一个用于进行自然语言处理研究的 Python 库。（至少在本书付印时是这样。）该库超出了本书的范围，但你可能会觉得它很有趣，而且还有一个很酷的交互式演示展示了许多最先进的
    NLP 模型。
