- en: Chapter 21\. Natural Language Processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: They have been at a great feast of languages, and stolen the scraps.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: William Shakespeare
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Natural language processing* (NLP) refers to computational techniques involving
    language. It’s a broad field, but we’ll look at a few techniques, both simple
    and not simple.'
  prefs: []
  type: TYPE_NORMAL
- en: Word Clouds
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 1](ch01.html#introduction), we computed word counts of users’ interests.
    One approach to visualizing words and counts is *word clouds*, which artistically
    depict the words at sizes proportional to their counts.
  prefs: []
  type: TYPE_NORMAL
- en: Generally, though, data scientists don’t think much of word clouds, in large
    part because the placement of the words doesn’t mean anything other than “here’s
    some space where I was able to fit a word.”
  prefs: []
  type: TYPE_NORMAL
- en: 'If you ever are forced to create a word cloud, think about whether you can
    make the axes convey something. For example, imagine that, for each of some collection
    of data science–related buzzwords, you have two numbers between 0 and 100—the
    first representing how frequently it appears in job postings, and the second how
    frequently it appears on résumés:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The word cloud approach is just to arrange the words on a page in a cool-looking
    font ([Figure 21-1](#word_cloud_wordle)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Buzzword Cloud.](assets/dsf2_2101.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 21-1\. Buzzword cloud
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'This looks neat but doesn’t really tell us anything. A more interesting approach
    might be to scatter them so that horizontal position indicates posting popularity
    and vertical position indicates résumé popularity, which produces a visualization
    that conveys a few insights ([Figure 21-2](#word_cloud_scatter)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![A more meaningful (if less attractive) word cloud.](assets/dsf2_2102.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 21-2\. A more meaningful (if less attractive) word cloud
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: n-Gram Language Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The DataSciencester VP of Search Engine Marketing wants to create thousands
    of web pages about data science so that your site will rank higher in search results
    for data science–related terms. (You attempt to explain to her that search engine
    algorithms are clever enough that this won’t actually work, but she refuses to
    listen.)
  prefs: []
  type: TYPE_NORMAL
- en: Of course, she doesn’t want to write thousands of web pages, nor does she want
    to pay a horde of “content strategists” to do so. Instead, she asks you whether
    you can somehow programmatically generate these web pages. To do this, we’ll need
    some way of modeling language.
  prefs: []
  type: TYPE_NORMAL
- en: One approach is to start with a corpus of documents and learn a statistical
    model of language. In our case, we’ll start with Mike Loukides’s essay [“What
    Is Data Science?”](http://oreil.ly/1Cd6ykN)
  prefs: []
  type: TYPE_NORMAL
- en: As in [Chapter 9](ch09.html#getting_data), we’ll use the Requests and Beautiful
    Soup libraries to retrieve the data. There are a couple of issues worth calling
    attention to.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first is that the apostrophes in the text are actually the Unicode character
    `u"\u2019"`. We’ll create a helper function to replace them with normal apostrophes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The second issue is that once we get the text of the web page, we’ll want to
    split it into a sequence of words and periods (so that we can tell where sentences
    end). We can do this using `re.findall`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: We certainly could (and likely should) clean this data further. There is still
    some amount of extraneous text in the document (for example, the first word is
    *Section*), and we’ve split on midsentence periods (for example, in *Web 2.0*),
    and there are a handful of captions and lists sprinkled throughout. Having said
    that, we’ll work with the document as it is.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have the text as a sequence of words, we can model language in
    the following way: given some starting word (say, *book*) we look at all the words
    that follow it in the source document. We randomly choose one of these to be the
    next word, and we repeat the process until we get to a period, which signifies
    the end of the sentence. We call this a *bigram model*, as it is determined completely
    by the frequencies of the bigrams (word pairs) in the original data.'
  prefs: []
  type: TYPE_NORMAL
- en: 'What about a starting word? We can just pick randomly from words that *follow*
    a period. To start, let’s precompute the possible word transitions. Recall that
    `zip` stops when any of its inputs is done, so that `zip(document, document[1:])`
    gives us precisely the pairs of consecutive elements of `document`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we’re ready to generate sentences:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The sentences it produces are gibberish, but they’re the kind of gibberish
    you might put on your website if you were trying to sound data-sciencey. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: If you may know which are you want to data sort the data feeds web friend someone
    on trending topics as the data in Hadoop is the data science requires a book demonstrates
    why visualizations are but we do massive correlations across many commercial disk
    drives in Python language and creates more tractable form making connections then
    use and uses it to solve a data.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Bigram Model
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'We can make the sentences less gibberishy by looking at *trigrams*, triplets
    of consecutive words. (More generally, you might look at *n-grams* consisting
    of *n* consecutive words, but three will be plenty for us.) Now the transitions
    will depend on the previous *two* words:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice that now we have to track the starting words separately. We can generate
    sentences in pretty much the same way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'This produces better sentences like:'
  prefs: []
  type: TYPE_NORMAL
- en: In hindsight MapReduce seems like an epidemic and if so does that give us new
    insights into how economies work That’s not a question we could even have asked
    a few years there has been instrumented.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Trigram Model
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Of course, they sound better because at each step the generation process has
    fewer choices, and at many steps only a single choice. This means that we frequently
    generate sentences (or at least long phrases) that were seen verbatim in the original
    data. Having more data would help; it would also work better if we collected *n*-grams
    from multiple essays about data science.
  prefs: []
  type: TYPE_NORMAL
- en: Grammars
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A different approach to modeling language is with *grammars*, rules for generating
    acceptable sentences. In elementary school, you probably learned about parts of
    speech and how to combine them. For example, if you had a really bad English teacher,
    you might say that a sentence necessarily consists of a *noun* followed by a *verb*.
    If you then have a list of nouns and verbs, you can generate sentences according
    to the rule.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll define a slightly more complicated grammar:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: I made up the convention that names starting with underscores refer to *rules*
    that need further expanding, and that other names are *terminals* that don’t need
    further processing.
  prefs: []
  type: TYPE_NORMAL
- en: So, for example, `"_S"` is the “sentence” rule, which produces an `"_NP"` (“noun
    phrase”) rule followed by a `"_VP"` (“verb phrase”) rule.
  prefs: []
  type: TYPE_NORMAL
- en: The verb phrase rule can produce either the `"_V"` (“verb”) rule, or the verb
    rule followed by the noun phrase rule.
  prefs: []
  type: TYPE_NORMAL
- en: Notice that the `"_NP"` rule contains itself in one of its productions. Grammars
    can be recursive, which allows even finite grammars like this to generate infinitely
    many different sentences.
  prefs: []
  type: TYPE_NORMAL
- en: How do we generate sentences from this grammar? We’ll start with a list containing
    the sentence rule `["_S"]`. And then we’ll repeatedly expand each rule by replacing
    it with a randomly chosen one of its productions. We stop when we have a list
    consisting solely of terminals.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, one such progression might look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'How do we implement this? Well, to start, we’ll create a simple helper function
    to identify terminals:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Next we need to write a function to turn a list of tokens into a sentence. We’ll
    look for the first nonterminal token. If we can’t find one, that means we have
    a completed sentence and we’re done.
  prefs: []
  type: TYPE_NORMAL
- en: If we do find a nonterminal, then we randomly choose one of its productions.
    If that production is a terminal (i.e., a word), we simply replace the token with
    it. Otherwise, it’s a sequence of space-separated nonterminal tokens that we need
    to `split` and then splice into the current tokens. Either way, we repeat the
    process on the new set of tokens.
  prefs: []
  type: TYPE_NORMAL
- en: 'Putting it all together, we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'And now we can start generating sentences:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Try changing the grammar—add more words, add more rules, add your own parts
    of speech—until you’re ready to generate as many web pages as your company needs.
  prefs: []
  type: TYPE_NORMAL
- en: Grammars are actually more interesting when they’re used in the other direction.
    Given a sentence, we can use a grammar to *parse* the sentence. This then allows
    us to identify subjects and verbs and helps us make sense of the sentence.
  prefs: []
  type: TYPE_NORMAL
- en: Using data science to generate text is a neat trick; using it to *understand*
    text is more magical. (See [“For Further Exploration”](#nlp-further-invest) for
    libraries that you could use for this.)
  prefs: []
  type: TYPE_NORMAL
- en: 'An Aside: Gibbs Sampling'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Generating samples from some distributions is easy. We can get uniform random
    variables with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'and normal random variables with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: But some distributions are harder to sample from. *Gibbs sampling* is a technique
    for generating samples from multidimensional distributions when we only know some
    of the conditional distributions.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, imagine rolling two dice. Let *x* be the value of the first die
    and *y* be the sum of the dice, and imagine you wanted to generate lots of (*x*,
    *y*) pairs. In this case it’s easy to generate the samples directly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'But imagine that you only knew the conditional distributions. The distribution
    of *y* conditional on *x* is easy—if you know the value of *x*, *y* is equally
    likely to be *x* + 1, *x* + 2, *x* + 3, *x* + 4, *x* + 5, or *x* + 6:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The other direction is more complicated. For example, if you know that *y*
    is 2, then necessarily *x* is 1 (since the only way two dice can sum to 2 is if
    both of them are 1). If you know *y* is 3, then *x* is equally likely to be 1
    or 2\. Similarly, if *y* is 11, then *x* has to be either 5 or 6:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The way Gibbs sampling works is that we start with any (valid) values for *x*
    and *y* and then repeatedly alternate replacing *x* with a random value picked
    conditional on *y* and replacing *y* with a random value picked conditional on
    *x*. After a number of iterations, the resulting values of *x* and *y* will represent
    a sample from the unconditional joint distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'You can check that this gives similar results to the direct sample:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: We’ll use this technique in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Topic Modeling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When we built our “Data Scientists You May Know” recommender in [Chapter 1](ch01.html#introduction),
    we simply looked for exact matches in people’s stated interests.
  prefs: []
  type: TYPE_NORMAL
- en: A more sophisticated approach to understanding our users’ interests might try
    to identify the *topics* that underlie those interests. A technique called *latent
    Dirichlet allocation* (LDA) is commonly used to identify common topics in a set
    of documents. We’ll apply it to documents that consist of each user’s interests.
  prefs: []
  type: TYPE_NORMAL
- en: 'LDA has some similarities to the Naive Bayes classifier we built in [Chapter 13](ch13.html#naive_bayes),
    in that it assumes a probabilistic model for documents. We’ll gloss over the hairier
    mathematical details, but for our purposes the model assumes that:'
  prefs: []
  type: TYPE_NORMAL
- en: There is some fixed number *K* of topics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is a random variable that assigns each topic an associated probability
    distribution over words. You should think of this distribution as the probability
    of seeing word *w* given topic *k*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is another random variable that assigns each document a probability distribution
    over topics. You should think of this distribution as the mixture of topics in
    document *d*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each word in a document was generated by first randomly picking a topic (from
    the document’s distribution of topics) and then randomly picking a word (from
    the topic’s distribution of words).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In particular, we have a collection of `documents`, each of which is a `list`
    of words. And we have a corresponding collection of `document_topics` that assigns
    a topic (here a number between 0 and *K* – 1) to each word in each document.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, the fifth word in the fourth document is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'and the topic from which that word was chosen is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: This very explicitly defines each document’s distribution over topics, and it
    implicitly defines each topic’s distribution over words.
  prefs: []
  type: TYPE_NORMAL
- en: We can estimate the likelihood that topic 1 produces a certain word by comparing
    how many times topic 1 produces that word with how many times topic 1 produces
    *any* word. (Similarly, when we built a spam filter in [Chapter 13](ch13.html#naive_bayes),
    we compared how many times each word appeared in spams with the total number of
    words appearing in spams.)
  prefs: []
  type: TYPE_NORMAL
- en: Although these topics are just numbers, we can give them descriptive names by
    looking at the words on which they put the heaviest weight. We just have to somehow
    generate the `document_topics`. This is where Gibbs sampling comes into play.
  prefs: []
  type: TYPE_NORMAL
- en: We start by assigning every word in every document a topic completely at random.
    Now we go through each document one word at a time. For that word and document,
    we construct weights for each topic that depend on the (current) distribution
    of topics in that document and the (current) distribution of words for that topic.
    We then use those weights to sample a new topic for that word. If we iterate this
    process many times, we will end up with a joint sample from the topic–word distribution
    and the document–topic distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'To start with, we’ll need a function to randomly choose an index based on an
    arbitrary set of weights:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'For instance, if you give it weights `[1, 1, 3]`, then one-fifth of the time
    it will return 0, one-fifth of the time it will return 1, and three-fifths of
    the time it will return 2\. Let’s write a test:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Our documents are our users’ interests, which look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'And we’ll try to find:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: topics. In order to calculate the sampling weights, we’ll need to keep track
    of several counts. Let’s first create the data structures for them.
  prefs: []
  type: TYPE_NORMAL
- en: 'How many times each topic is assigned to each document:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'How many times each word is assigned to each topic:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The total number of words assigned to each topic:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The total number of words contained in each document:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The number of distinct words:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'And the number of documents:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Once we populate these, we can find, for example, the number of words in `documents[3]`
    associated with topic 1 as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'And we can find the number of times *nlp* is associated with topic 2 as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we’re ready to define our conditional probability functions. As in [Chapter 13](ch13.html#naive_bayes),
    each has a smoothing term that ensures every topic has a nonzero chance of being
    chosen in any document and that every word has a nonzero chance of being chosen
    for any topic:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'We’ll use these to create the weights for updating topics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: There are solid mathematical reasons why `topic_weight` is defined the way it
    is, but their details would lead us too far afield. Hopefully it makes at least
    intuitive sense that—given a word and its document—the likelihood of any topic
    choice depends on both how likely that topic is for the document and how likely
    that word is for the topic.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is all the machinery we need. We start by assigning every word to a random
    topic and populating our counters appropriately:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Our goal is to get a joint sample of the topics–word distribution and the documents–topic
    distribution. We do this using a form of Gibbs sampling that uses the conditional
    probabilities defined previously:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'What are the topics? They’re just numbers 0, 1, 2, and 3\. If we want names
    for them, we have to do that ourselves. Let’s look at the five most heavily weighted
    words for each ([Table 21-1](#table20-1)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Table 21-1\. Most common words per topic
  prefs: []
  type: TYPE_NORMAL
- en: '| Topic 0 | Topic 1 | Topic 2 | Topic 3 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Java | R | HBase | regression |'
  prefs: []
  type: TYPE_TB
- en: '| Big Data | statistics | Postgres | libsvm |'
  prefs: []
  type: TYPE_TB
- en: '| Hadoop | Python | MongoDB | scikit-learn |'
  prefs: []
  type: TYPE_TB
- en: '| deep learning | probability | Cassandra | machine learning |'
  prefs: []
  type: TYPE_TB
- en: '| artificial intelligence | pandas | NoSQL | neural networks |'
  prefs: []
  type: TYPE_TB
- en: 'Based on these I’d probably assign topic names:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'at which point we can see how the model assigns topics to each user’s interests:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'which gives:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: and so on. Given the “ands” we needed in some of our topic names, it’s possible
    we should use more topics, although most likely we don’t have enough data to successfully
    learn them.
  prefs: []
  type: TYPE_NORMAL
- en: Word Vectors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A lot of recent advances in NLP involve deep learning. In the rest of this chapter
    we’ll look at a couple of them using the machinery we developed in [Chapter 19](ch19.html#deep_learning).
  prefs: []
  type: TYPE_NORMAL
- en: One important innovation involves representing words as low-dimensional vectors.
    These vectors can be compared, added together, fed into machine learning models,
    or anything else you want to do with them. They usually have nice properties;
    for example, similar words tend to have similar vectors. That is, typically the
    word vector for *big* is pretty close to the word vector for *large*, so that
    a model operating on word vectors can (to some degree) handle things like synonymy
    for free.
  prefs: []
  type: TYPE_NORMAL
- en: Frequently the vectors will exhibit delightful arithmetic properties as well.
    For instance, in some such models if you take the vector for *king*, subtract
    the vector for *man*, and add the vector for *woman*, you will end up with a vector
    that’s very close to the vector for *queen*. It can be interesting to ponder what
    this means about what the word vectors actually “learn,” although we won’t spend
    time on that here.
  prefs: []
  type: TYPE_NORMAL
- en: 'Coming up with such vectors for a large vocabulary of words is a difficult
    undertaking, so typically we’ll *learn* them from a corpus of text. There are
    a couple of different schemes, but at a high level the task typically looks something
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: Get a bunch of text.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a dataset where the goal is to predict a word given nearby words (or
    alternatively, to predict nearby words given a word).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train a neural net to do well on this task.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Take the internal states of the trained neural net as the word vectors.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In particular, because the task is to predict a word given nearby words, words
    that occur in similar contexts (and hence have similar nearby words) should have
    similar internal states and therefore similar word vectors.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here we’ll measure “similarity” using *cosine similarity*, which is a number
    between –1 and 1 that measures the degree to which two vectors point in the same
    direction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Let’s learn some word vectors to see how this works.
  prefs: []
  type: TYPE_NORMAL
- en: 'To start with, we’ll need a toy dataset. The commonly used word vectors are
    typically derived from training on millions or even billions of words. As our
    toy library can’t cope with that much data, we’ll create an artificial dataset
    with some structure to it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: This will generate lots of sentences with similar structure but different words;
    for example, “The green boat seems quite slow.” Given this setup, the colors will
    mostly appear in “similar” contexts, as will the nouns, and so on. So if we do
    a good job of assigning word vectors, the colors should get similar vectors, and
    so on.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In practical usage, you’d probably have a corpus of millions of sentences, in
    which case you’d get “enough” context from the sentences as they are. Here, with
    only 50 sentences, we have to make them somewhat artificial.
  prefs: []
  type: TYPE_NORMAL
- en: 'As mentioned earlier, we’ll want to one-hot-encode our words, which means we’ll
    need to convert them to IDs. We’ll introduce a `Vocabulary` class to keep track
    of this mapping:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'These are all things we could do manually, but it’s handy to have it in a class.
    We should probably test it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'We should also write simple helper functions to save and load a vocabulary,
    just as we have for our deep learning models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: We’ll be using a word vector model called *skip-gram* that takes as input a
    word and generates probabilities for what words are likely to be seen near it.
    We will feed it training pairs `(word, nearby_word)` and try to minimize the `SoftmaxCrossEntropy`
    loss.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Another common model, *continuous bag-of-words* (CBOW), takes the nearby words
    as the inputs and tries to predict the original word.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s design our neural network. At its heart will be an *embedding* layer that
    takes as input a word ID and returns a word vector. Under the covers we can just
    use a lookup table for this.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll then pass the word vector to a `Linear` layer with the same number of
    outputs as we have words in our vocabulary. As before, we’ll use `softmax` to
    convert these outputs to probabilities over nearby words. As we use gradient descent
    to train the model, we will be updating the vectors in the lookup table. Once
    we’ve finished training, that lookup table gives us our word vectors.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s create that embedding layer. In practice we might want to embed things
    other than words, so we’ll construct a more general `Embedding` layer. (Later
    we’ll write a `TextEmbedding` subclass that’s specifically for word vectors.)
  prefs: []
  type: TYPE_NORMAL
- en: 'In its constructor we’ll provide the number and dimension of our embedding
    vectors, so it can create the embeddings (which will be standard random normals,
    initially):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: In our case we’ll only be embedding one word at a time. However, in other models
    we might want to embed a sequence of words and get back a sequence of word vectors.
    (For example, if we wanted to train the CBOW model described earlier.) So an alternative
    design would take sequences of word IDs. We’ll stick with one at a time, to make
    things simpler.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'For the backward pass we’ll get a gradient corresponding to the chosen embedding
    vector, and we’ll need to construct the corresponding gradient for `self.embeddings`,
    which is zero for every embedding other than the chosen one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Because we have parameters and gradients, we need to override those methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'As mentioned earlier, we’ll want a subclass specifically for word vectors.
    In that case our number of embeddings is determined by our vocabulary, so let’s
    just pass that in instead:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: The other built-in methods will all work as is, but we’ll add a couple more
    methods specific to working with text. For example, we’d like to be able to retrieve
    the vector for a given word. (This is not part of the `Layer` interface, but we
    are always free to add extra methods to specific layers as we like.)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'This dunder method will allow us to retrieve word vectors using indexing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'And we’d also like the embedding layer to tell us the closest words to a given
    word:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: Our embedding layer just outputs vectors, which we can feed into a `Linear`
    layer.
  prefs: []
  type: TYPE_NORMAL
- en: Now we’re ready to assemble our training data. For each input word, we’ll choose
    as target words the two words to its left and the two words to its right.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start by lowercasing the sentences and splitting them into words:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'at which point we can construct a vocabulary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'And now we can create training data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'With the machinery we’ve built up, it’s now easy to create our model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the machinery from [Chapter 19](ch19.html#deep_learning), it’s easy to
    train our model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: As you watch this train, you can see the colors getting closer to each other,
    the adjectives getting closer to each other, and the nouns getting closer to each
    other.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the model is trained, it’s fun to explore the most similar words:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'which (for me) results in:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: (Obviously *bed* and *cat* are not really similar, but in our training sentences
    they appear to be, and that’s what the model is capturing.)
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also extract the first two principal components and plot them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'which shows that similar words are indeed clustering together ([Figure 21-3](#word_vectors_plot)):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Word Vectors](assets/dsf2_2103.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 21-3\. Word vectors
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If you’re interested, it’s not hard to train CBOW word vectors. You’ll have
    to do a little work. First, you’ll need to modify the `Embedding` layer so that
    it takes as input a *list* of IDs and outputs a *list* of embedding vectors. Then
    you’ll have to create a new layer (`Sum`?) that takes a list of vectors and returns
    their sum.
  prefs: []
  type: TYPE_NORMAL
- en: Each word represents a training example where the input is the word IDs for
    the surrounding words, and the target is the one-hot encoding of the word itself.
  prefs: []
  type: TYPE_NORMAL
- en: The modified `Embedding` layer turns the surrounding words into a list of vectors,
    the new `Sum` layer collapses the list of vectors down to a single vector, and
    then a `Linear` layer can produce scores that can be `softmax`ed to get a distribution
    representing “most likely words, given this context.”
  prefs: []
  type: TYPE_NORMAL
- en: I found the CBOW model harder to train than the skip-gram one, but I encourage
    you to try it out.
  prefs: []
  type: TYPE_NORMAL
- en: Recurrent Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The word vectors we developed in the previous section are often used as the
    inputs to neural networks. One challenge to doing this is that sentences have
    varying lengths: you could think of a 3-word sentence as a `[3, embedding_dim]`
    tensor and a 10-word sentence as a `[10, embedding_dim]` tensor. In order to,
    say, pass them to a `Linear` layer, we need to do something about that first variable-length
    dimension.'
  prefs: []
  type: TYPE_NORMAL
- en: One option is to use a `Sum` layer (or a variant that takes the average); however,
    the *order* of the words in a sentence is usually important to its meaning. To
    take a common example, “dog bites man” and “man bites dog” are two very different
    stories!
  prefs: []
  type: TYPE_NORMAL
- en: Another way of handling this is using *recurrent neural networks* (RNNs), which
    have a *hidden state* they maintain between inputs. In the simplest case, each
    input is combined with the current hidden state to produce an output, which is
    then used as the new hidden state. This allows such networks to “remember” (in
    a sense) the inputs they’ve seen, and to build up to a final output that depends
    on all the inputs and their order.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll create pretty much the simplest possible RNN layer, which will accept
    a single input (corresponding to, e.g., a single word in a sentence, or a single
    character in a word), and which will maintain its hidden state between calls.
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall that our `Linear` layer had some weights, `w`, and a bias, `b`. It took
    a vector `input` and produced a different vector as `output` using the logic:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'Here we’ll want to incorporate our hidden state, so we’ll have *two* sets of
    weights—one to apply to the `input` and one to apply to the previous `hidden`
    state:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: Next, we’ll use the `output` vector as the new value of `hidden`. This isn’t
    a huge change, but it will allow our networks to do wonderful things.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: You can see that we start out the hidden state as a vector of 0s, and we provide
    a function that people using the network can call to reset the hidden state.
  prefs: []
  type: TYPE_NORMAL
- en: 'Given this setup, the `forward` function is reasonably straightforward (at
    least, it is if you remember and understand how our `Linear` layer worked):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'The `backward` pass is similar to the one in our `Linear` layer, except that
    it needs to compute an additional set of gradients for the `u` weights:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'And finally we need to override the `params` and `grads` methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This “simple” RNN is so simple that you probably shouldn’t use it in practice.
  prefs: []
  type: TYPE_NORMAL
- en: Our `SimpleRnn` has a couple of undesirable features. One is that its entire
    hidden state is used to update the input every time you call it. The other is
    that the entire hidden state is overwritten every time you call it. Both of these
    make it difficult to train; in particular, they make it difficult for the model
    to learn long-range dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: For this reason, almost no one uses this kind of simple RNN. Instead, they use
    more complicated variants like the LSTM (“long short-term memory”) or the GRU
    (“gated recurrent unit”), which have many more parameters and use parameterized
    “gates” that allow only some of the state to be updated (and only some of the
    state to be used) at each timestep.
  prefs: []
  type: TYPE_NORMAL
- en: There is nothing particularly *difficult* about these variants; however, they
    involve a great deal more code, which would not be (in my opinion) correspondingly
    more edifying to read. The code for this chapter on [GitHub](https://github.com/joelgrus/data-science-from-scratch)
    includes an LSTM implementation. I encourage you to check it out, but it’s somewhat
    tedious and so we won’t discuss it further here.
  prefs: []
  type: TYPE_NORMAL
- en: One other quirk of our implementation is that it takes only one “step” at a
    time and requires us to manually reset the hidden state. A more practical RNN
    implementation might accept sequences of inputs, set its hidden state to 0s at
    the beginning of each sequence, and produce sequences of outputs. Ours could certainly
    be modified to behave this way; again, this would require more code and complexity
    for little gain in understanding.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: Using a Character-Level RNN'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The newly hired VP of Branding did not come up with the name *DataSciencester*
    himself, and (accordingly) he suspects that a better name might lead to more success
    for the company. He asks you to use data science to suggest candidates for replacement.
  prefs: []
  type: TYPE_NORMAL
- en: One “cute” application of RNNs involves using *characters* (rather than words)
    as their inputs, training them to learn the subtle language patterns in some dataset,
    and then using them to generate fictional instances from that dataset.
  prefs: []
  type: TYPE_NORMAL
- en: For example, you could train an RNN on the names of alternative bands, use the
    trained model to generate new names for fake alternative bands, and then hand-select
    the funniest ones and share them on Twitter. Hilarity!
  prefs: []
  type: TYPE_NORMAL
- en: Having seen this trick enough times to no longer consider it clever, you decide
    to give it a shot.
  prefs: []
  type: TYPE_NORMAL
- en: 'After some digging, you find that the startup accelerator Y Combinator has
    published [a list of its top 100 (actually 101) most successful startups](https://www.ycombinator.com/topcompanies/),
    which seems like a good starting point. Checking the page, you find that the company
    names all live inside `<b class="h4">` tags, which means it’s easy to use your
    web scraping skills to retrieve them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: As always, the page may change (or vanish), in which case this code won’t work.
    If so, you can use your newly learned data science skills to fix it or just get
    the list from the book’s GitHub site.
  prefs: []
  type: TYPE_NORMAL
- en: So what is our plan? We’ll train a model to predict the next character of a
    name, given the current character *and* a hidden state representing all the characters
    we’ve seen so far.
  prefs: []
  type: TYPE_NORMAL
- en: As usual, we’ll actually predict a probability distribution over characters
    and train our model to minimize the `SoftmaxCrossEntropy` loss.
  prefs: []
  type: TYPE_NORMAL
- en: Once our model is trained, we can use it to generate some probabilities, randomly
    sample a character according to those probabilities, and then feed that character
    as its next input. This will allow us to *generate* company names using the learned
    weights.
  prefs: []
  type: TYPE_NORMAL
- en: 'To start with, we should build a `Vocabulary` from the characters in the names:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: In addition, we’ll use special tokens to signify the start and end of a company
    name. This allows the model to learn which characters should *begin* a company
    name and also to learn when a company name is *finished*.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll just use the regex characters for start and end, which (luckily) don’t
    appear in our list of companies:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'For our model, we’ll one-hot-encode each character, pass it through two `SimpleRnn`s,
    and then use a `Linear` layer to generate the scores for each possible next character:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'Imagine for the moment that we’ve trained this model. Let’s write the function
    that uses it to generate new company names, using the `sample_from` function from
    [“Topic Modeling”](#topic_modeling):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: At long last, we’re ready to train our character-level RNN. It will take a while!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: After training, the model generates some actual names from the list (which isn’t
    surprising, since the model has a fair amount of capacity and not a lot of training
    data), as well as names that are only slightly different from training names (Scripe,
    Loinbare, Pozium), names that seem genuinely creative (Benuus, Cletpo, Equite,
    Vivest), and names that are garbage-y but still sort of word-like (SFitreasy,
    Sint ocanelp, GliyOx, Doorboronelhav).
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, like most character-level-RNN outputs, these are only mildly
    clever, and the VP of Branding ends up unable to use them.
  prefs: []
  type: TYPE_NORMAL
- en: If I up the hidden dimension to 64, I get a lot more names verbatim from the
    list; if I drop it to 8, I get mostly garbage. The vocabulary and final weights
    for all these model sizes are available on [the book’s GitHub site](https://github.com/joelgrus/data-science-from-scratch),
    and you can use `load_weights` and `load_vocab` to use them yourself.
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned previously, the GitHub code for this chapter also contains an implementation
    for an LSTM, which you should feel free to swap in as a replacement for the `SimpleRnn`s
    in our company name model.
  prefs: []
  type: TYPE_NORMAL
- en: For Further Exploration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[NLTK](http://www.nltk.org/) is a popular library of NLP tools for Python.
    It has its own entire [book](http://www.nltk.org/book/), which is available to
    read online.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[gensim](http://radimrehurek.com/gensim/) is a Python library for topic modeling,
    which is a better bet than our from-scratch model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[spaCy](https://spacy.io/) is a library for “Industrial Strength Natural Language
    Processing in Python” and is also quite popular.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Andrej Karpathy has a famous blog post, [“The Unreasonable Effectiveness of
    Recurrent Neural Networks”](http://karpathy.github.io/2015/05/21/rnn-effectiveness/),
    that’s very much worth reading.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: My day job involves building [AllenNLP](https://allennlp.org/), a Python library
    for doing NLP research. (At least, as of the time this book went to press, it
    did.) The library is quite beyond the scope of this book, but you might still
    find it interesting, and it has a cool interactive demo of many state-of-the-art
    NLP models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
