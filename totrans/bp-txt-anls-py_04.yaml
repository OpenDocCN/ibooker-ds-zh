- en: Chapter 4\. Preparing Textual Data for Statistics and Machine Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第4章。为统计和机器学习准备文本数据
- en: Technically, any text document is just a sequence of characters. To build models
    on the content, we need to transform a text into a sequence of words or, more
    generally, meaningful sequences of characters called *tokens*. But that alone
    is not sufficient. Think of the word sequence *New York*, which should be treated
    as a single named-entity. Correctly identifying such word sequences as compound
    structures requires sophisticated linguistic processing.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 从技术上讲，任何文本文档都只是一系列字符。为了在内容上构建模型，我们需要将文本转换为一系列单词或更一般地说，被称为*标记*的有意义的字符序列。但仅仅这样是不够的。想象一下单词序列*New
    York*，它应该被视为一个单一的命名实体。正确地识别这样的单词序列作为复合结构需要复杂的语言处理。
- en: Data preparation or data preprocessing in general involves not only the transformation
    of data into a form that can serve as the basis for analysis but also the removal
    of disturbing noise. What’s noise and what isn’t always depends on the analysis
    you are going to perform. When working with text, noise comes in different flavors.
    The raw data may include HTML tags or special characters that should be removed
    in most cases. But frequent words carrying little meaning, the so-called *stop
    words*, introduce noise into machine learning and data analysis because they make
    it harder to detect patterns.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 数据准备或一般的数据预处理不仅涉及将数据转换为可以用于分析的形式，还涉及消除干扰噪声。什么是噪声，什么不是，这取决于您将要执行的分析。在处理文本时，噪声呈现不同的形式。原始数据可能包括应在大多数情况下移除的HTML标记或特殊字符。但是，频繁出现的具有很少含义的单词，所谓的*停用词*，会给机器学习和数据分析引入噪声，因为它们使得检测模式变得更加困难。
- en: What You’ll Learn and What We’ll Build
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 您将学到什么以及我们将要构建什么
- en: In this chapter, we will develop blueprints for a text preprocessing pipeline.
    The pipeline will take the raw text as input, clean it, transform it, and extract
    the basic features of textual content. We start with regular expressions for data
    cleaning and tokenization and then focus on linguistic processing with *spaCy*.
    spaCy is a powerful NLP library with a modern API and state-of-the-art models.
    For some operations we will make use of *textacy*, a library that provides some
    nice add-on functionality especially for data preprocessing. We will also point
    to NLTK and other libraries whenever it appears helpful.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将为文本预处理流水线开发蓝图。该流水线将接受原始文本作为输入，对其进行清理、转换，并提取文本内容的基本特征。我们首先使用正则表达式进行数据清理和标记化，然后专注于使用*spaCy*进行语言处理。spaCy是一个功能强大的自然语言处理库，具有现代API和最先进的模型。对于一些操作，我们将利用*textacy*，这是一个提供一些很好的附加功能，特别是用于数据预处理的库。我们还会在有需要时指向NLTK和其他库。
- en: After studying this chapter, you will know the required and optional steps of
    data preparation. You will know how to use regular expressions for data cleaning
    and how to use spaCy for feature extraction. With the provided blueprints you
    will be able to quickly set up a data preparation pipeline for your own project.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在学习本章之后，您将了解数据准备的必需和可选步骤。您将学会如何使用正则表达式进行数据清理，以及如何使用spaCy进行特征提取。通过提供的蓝图，您将能够快速为自己的项目建立数据准备流水线。
- en: A Data Preprocessing Pipeline
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据预处理流水线
- en: Data preprocessing usually involves a sequence of steps. Often, this sequence
    is called a *pipeline* because you feed raw data into the pipeline and get the
    transformed and preprocessed data out of it. In [Chapter 1](ch01.xhtml#ch-exploration)
    we already built a simple data processing pipeline including tokenization and
    stop word removal. We will use the term *pipeline* in this chapter as a general
    term for a sequence of processing steps. [Figure 4-1](#fig-pipeline) gives an
    overview of the blueprints we are going to build for the preprocessing pipeline
    in this chapter.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 数据预处理通常涉及一系列步骤。通常，这个序列被称为*流水线*，因为您将原始数据输入到流水线中，并从中获得转换和预处理后的数据。在[第1章](ch01.xhtml#ch-exploration)中，我们已经构建了一个简单的数据处理流水线，包括标记化和停用词去除。我们将在本章中使用术语*流水线*作为处理步骤序列的通用术语。[图 4-1](#fig-pipeline)概述了我们将在本章中为预处理流水线构建的蓝图。
- en: '![](Images/btap_0401.jpg)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/btap_0401.jpg)'
- en: Figure 4-1\. A pipeline with typical preprocessing steps for textual data.
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-1。文本数据预处理的典型预处理步骤流水线。
- en: The first major block of operations in our pipeline is *data cleaning*. We start
    by identifying and removing noise in text like HTML tags and nonprintable characters.
    During *character normalization*, special characters such as accents and hyphens
    are transformed into a standard representation. Finally, we can mask or remove
    identifiers like URLs or email addresses if they are not relevant for the analysis
    or if there are privacy issues. Now the text is clean enough to start linguistic
    processing.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: Here, *tokenization* splits a document into a list of separate tokens like words
    and punctuation characters. *Part-of-speech (POS) tagging* is the process of determining
    the word class, whether it’s a noun, a verb, an article, etc. *Lemmatization*
    maps inflected words to their uninflected root, the lemma (e.g., “are” → “be”).
    The target of *named-entity recognition* is the identification of references to
    people, organizations, locations, etc., in the text.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: In the end, we want to create a database with preprared data ready for analysis
    and machine learning. Thus, the required preparation steps vary from project to
    project. It’s up to you to decide which of the following blueprints you need to
    include in your problem-specific pipeline.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: 'Introducing the Dataset: Reddit Self-Posts'
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The preparation of textual data is particularly challenging when you work with
    user-generated content (UGC). In contrast to well-redacted text from professional
    reports, news, and blogs, user contributions in social media usually are short
    and contain lots of abbreviations, hashtags, emojis, and typos. Thus, we will
    use the [Reddit Self-Posts](https://oreil.ly/0pnqO) dataset, which is hosted on
    Kaggle. The complete dataset contains roughly 1 million user posts with title
    and content, arranged in 1,013 different subreddits, each of which has 1,000 records.
    We will use a subset of only 20,000 posts contained in the autos category. The
    dataset we prepare in this chapter is the basis for the analysis of word embeddings
    in [Chapter 10](ch10.xhtml#ch-embeddings).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: Loading Data Into Pandas
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The original dataset consists of two separate CSV files, one with the posts
    and the other one with some metadata for the subreddits, including category information.
    Both files are loaded into a Pandas `DataFrame` by `pd.read_csv()` and then joined
    into a single `DataFrame`.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Blueprint: Standardizing Attribute Names'
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before we start working with the data, we will change the dataset-specific column
    names to more generic names. We recommend always naming the main `DataFrame` `df`,
    and naming the column with the text to analyze `text`. Such naming conventions
    for common variables and attribute names make it easier to reuse the code of the
    blueprints in different projects.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take a look at the columns list of this dataset:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '`Out:`'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: For column renaming and selection, we define a dictionary `column_mapping` where
    each entry defines a mapping from the current column name to a new name. Columns
    mapped to `None` and unmentioned columns are dropped. A dictionary is perfect
    documentation for such a transformation and easy to reuse. This dictionary is
    then used to select and rename the columns that we want to keep.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 对于列重命名和选择，我们定义一个名为 `column_mapping` 的字典，其中每个条目定义了当前列名到新名称的映射。映射到 `None` 的列和未提及的列将被丢弃。这种转换的字典非常适合文档化并易于重复使用。然后使用这个字典来选择和重命名我们想要保留的列。
- en: '[PRE3]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'As already mentioned, we limit the data to the autos category:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们将数据限制在汽车类别中：
- en: '[PRE4]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Let’s take a brief look at a sample record to get a first impression of the
    data:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们简要看一下样本记录，以对数据有个初步印象：
- en: '[PRE5]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '|  | 14356 |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '|  | 14356 |'
- en: '| --- | --- |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| id | 7jc2k4 |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| id | 7jc2k4 |'
- en: '| subreddit | volt |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| subreddit | volt |'
- en: '| title | Dashcam for 2017 volt |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| title | 2017 伏特行车记录仪 |'
- en: '| text | Hello.<lb>I’m looking into getting a dashcam. <lb>Does anyone have
    any recommendations? <lb><lb>I’m generally looking for a rechargeable one so that
    I don’t have to route wires down to the cigarette lighter. <lb>Unless there are
    instructions on how to wire it properly without wires showing. <lb><lb><lb>Thanks!
    |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| text | Hello.<lb>我正在考虑购买一款行车记录仪。<lb>有人有推荐吗？<lb><lb>我一般寻找一款可充电的，这样我就不必把电线引到点烟器里去了。<lb>除非有关于如何正确安装不露电线的说明。<lb><lb><lb>谢谢！
    |'
- en: '| category | autos |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| category | autos |'
- en: '| subcategory | chevrolet |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| subcategory | chevrolet |'
- en: Saving and Loading a DataFrame
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 保存和加载 DataFrame
- en: 'After each step of data preparation, it is helpful to write the respective
    `DataFrame` to disk as a checkpoint. Pandas directly supports a number of [serialization
    options](https://oreil.ly/VaXTx). Text-based formats like CSV or JSON can be imported
    into most other tools easily. However, information about data types is lost (CSV)
    or only saved rudimentarily (JSON). The standard serialization format of Python,
    `pickle`, is supported by Pandas and therefore a viable option. It is fast and
    preserves all information but can only be processed by Python. “Pickling” a data
    frame is easy; you just need to specify the filename:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个数据准备步骤之后，将相应的 `DataFrame` 写入磁盘作为检查点非常有帮助。Pandas 直接支持多种[序列化选项](https://oreil.ly/VaXTx)。像
    CSV 或 JSON 这样的文本格式可以轻松导入到大多数其他工具中。然而，数据类型信息丢失（CSV）或仅保存基本信息（JSON）。Python 的标准序列化格式
    `pickle` 受到 Pandas 支持，因此是一个可行的选择。它速度快且保留所有信息，但只能由 Python 处理。“Pickling” 一个数据帧很简单；您只需指定文件名：
- en: '[PRE6]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: We prefer, however, storing dataframes in SQL databases because they give you
    all the advantages of SQL, including filters, joins, and easy access from many
    tools. But in contrast to `pickle`, only SQL data types are supported. Columns
    containing objects or lists, for example, cannot simply be saved this way and
    need to be serialized manually.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们更倾向于将数据帧存储在 SQL 数据库中，因为它们为您提供 SQL 的所有优势，包括过滤、连接和从许多工具轻松访问。但与 `pickle` 不同，只支持
    SQL 数据类型。例如，包含对象或列表的列不能简单地以这种方式保存，需要手动序列化。
- en: In our examples, we will use SQLite to persist data frames. SQLite is well integrated
    with Python. Moreover, it’s just a library and does not require a server, so the
    files are self-contained and can be exchanged between different team members easily.
    For more power and safety, we recommend a server-based SQL database.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例中，我们将使用 SQLite 来持久化数据帧。SQLite 与 Python 集成良好。此外，它只是一个库，不需要服务器，因此文件是自包含的，并且可以在不同团队成员之间轻松交换。为了更大的功能和安全性，我们建议使用基于服务器的
    SQL 数据库。
- en: 'We use `pd.to_sql()` to save our `DataFrame` as table `posts` into an SQLite
    database. The `DataFrame` index is not stored, and any existing data is overwritten:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 `pd.to_sql()` 将我们的 `DataFrame` 保存为 SQLite 数据库中的 `posts` 表。`DataFrame` 索引不会被保存，任何现有数据都会被覆盖：
- en: '[PRE7]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The `DataFrame` can be easily restored with `pd.read_sql()`:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用 `pd.read_sql()` 轻松恢复 `DataFrame`：
- en: '[PRE8]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Cleaning Text Data
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 清洁文本数据
- en: 'When working with user requests or comments as opposed to well-edited articles,
    you usually have to deal with a number of quality issues:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 当处理用户请求或评论时，相对于精心编辑的文章，通常需要处理一些质量问题：
- en: Special formatting and program code
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 特殊格式和程序代码
- en: The text may still contain special characters, HTML entities, Markdown tags,
    and things like that. These artifacts should be cleaned in advance because they
    complicate tokenization and introduce noise.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 文本可能仍然包含特殊字符、HTML 实体、Markdown 标记等。这些残留物应提前清理，因为它们会复杂化标记化并引入噪音。
- en: Salutations, signatures, addresses, etc.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 问候语、签名、地址等。
- en: Personal communication often contains meaningless polite phrases and salutations
    by name that are usually irrelevant for the analysis.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: Replies
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: If your text contains answers repeating the question text, you need to delete
    the duplicate questions. Keeping them will distort any model and statistics.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will demonstrate how to use regular expressions to identify
    and remove unwanted patterns in the data. Check out the following sidebar for
    some more details on regular expressions in Python.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: 'Take a look at the following text example from the Reddit dataset:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: It will definitely improve the results if this text gets some cleaning and polishing.
    Some tags are just artifacts from web scraping, so we will get rid of them. And
    as we are not interested in the URLs and other links, we will discard them as
    well.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: 'Blueprint: Identify Noise with Regular Expressions'
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The identification of quality problems in a big dataset can be tricky. Of course,
    you can and should take a look at a sample of the data. But the probability is
    high that you won’t find all the issues. It is better to define rough patterns
    indicating likely problems and check the complete dataset programmatically.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: The following function can help you to identify noise in textual data. By *noise*
    we mean everything that’s not plain text and may therefore disturb further analysis.
    The function uses a regular expression to search for a number of suspicious characters
    and returns their share of all characters as a score for impurity. Very short
    texts (less than `min_len` characters) are ignored because here a single special
    character would lead to a significant impurity and distort the result.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '`Out:`'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: You almost never find these characters in well-redacted text, so the scores
    in general should be very small. For the previous example text, about 9% of the
    characters are “suspicious” according to our definition. The search pattern may
    of course need adaption for corpora containing hashtags or similar tokens containing
    special characters. However, it doesn’t need to be perfect; it just needs to be
    good enough to indicate potential quality issues.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: For the Reddit data, we can get the most “impure” records with the following
    two statements. Note that we use Pandas `apply()` instead of the similar `map()`
    because it allows us to forward additional parameters like `min_len` to the applied
    function.^([1](ch04.xhtml#idm45634202714920))
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '|  | text | impurity |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
- en: '| 19682 | Looking at buying a 335i with 39k miles and 11 months left on the
    CPO warranty. I asked the deal... | 0.21 |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
- en: '| 12357 | I’m looking to lease an a4 premium plus automatic with the nav package.<lb><lb>Vehicle
    Price:<ta... | 0.17 |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
- en: '| 2730 | Breakdown below:<lb><lb>Elantra GT<lb><lb>2.0L 4-cylinder<lb><lb>6-speed
    Manual Transmission<lb>... | 0.14 |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
- en: 'Obviously, there are many tags like `<lb>` (linebreak) and `<tab>` included.
    Let’s check if there are others by utilizing our word count blueprint from [Chapter 1](ch01.xhtml#ch-exploration)
    in combination with a simple regex tokenizer for such tags:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，有许多像`<lb>`（换行符）和`<tab>`（制表符）这样的标签包含在内。让我们利用我们在[第1章](ch01.xhtml#ch-exploration)中的单词计数蓝图，结合简单的正则表达式分词器，来检查是否还有其他标签：
- en: '[PRE13]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '| freq | token |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| 频率 | 词元 |'
- en: '| --- | --- |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| <lb> | 100729 |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| <lb> | 100729 |'
- en: '| <tab> | 642 |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| <tab> | 642 |'
- en: Now we know that although these two tags are common, they are the only ones.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道，尽管这两个标签很常见，但它们是唯一的标签。
- en: 'Blueprint: Removing Noise with Regular Expressions'
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 蓝图：使用正则表达式去除噪音
- en: 'Our approach to data cleaning consists of defining a set of regular expressions
    and identifying problematic patterns and corresponding substitution rules.^([2](ch04.xhtml#idm45634202565272))
    The blueprint function first substitutes all HTML escapes (e.g., `&amp;`) by their
    plain-text representation and then replaces certain patterns by spaces. Finally,
    sequences of whitespaces are pruned:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的数据清理方法包括定义一组正则表达式，并识别问题模式及相应的替换规则。^([2](ch04.xhtml#idm45634202565272)) 蓝图函数首先用它们的纯文本表示替换所有HTML转义符（例如`&amp;`），然后用空格替换特定模式。最后，修剪空白序列：
- en: '[PRE14]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Warning
  id: totrans-83
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: 'Be careful: if your regular expressions are not defined precisely enough, you
    can accidentally delete valuable information during this process without noticing
    it! The repeaters `+` and `*` can be especially dangerous because they match unbounded
    sequences of characters and can remove large portions of the text.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 要小心：如果您的正则表达式定义不够精确，您可能会在此过程中意外删除有价值的信息而未注意到！重复符号`+`和`*`尤其危险，因为它们匹配无界字符序列，可能会删除大部分文本。
- en: 'Let’s apply the `clean` function to the earlier sample text and check the result:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们对前面的示例文本应用`clean`函数并检查结果：
- en: '[PRE15]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '`Out:`'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出：`'
- en: '[PRE16]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'That looks pretty good. Once you have treated the first patterns, you should
    check the impurity of the cleaned text again and add further cleaning steps if
    necessary:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来不错。一旦处理了第一个模式，您应该再次检查清理后文本的杂质，并在必要时添加进一步的清理步骤：
- en: '[PRE17]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '|  | clean_text | impurity |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '|  | 清理文本 | 杂质 |'
- en: '| --- | --- | --- |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 14058 | Mustang 2018, 2019, or 2020? Must Haves!! 1\. Have a Credit score
    of 780\+ for the best low interest rates! 2\. Join a Credit Union to finance the
    vehicle! 3\. Or Find a Lender to finance the vehicle... | 0.03 |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| 14058 | Mustang 2018、2019年还是2020年？必备条件！！1\. 信用评分达到780分以上，以获得最低的利率！2\. 加入信用社来为车辆融资！3\.
    或者找一个贷款人来为车辆融资... | 0.03 |'
- en: '| 18934 | At the dealership, they offered an option for foot-well illumination,
    but I cannot find any reference to this online. Has anyone gotten it? How does
    it look? Anyone have pictures. Not sure if this... | 0.03 |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| 18934 | 在经销商那里，他们提供了一个车内照明的选项，但我在网上找不到任何相关信息。有人得到了吗？它看起来怎么样？有人有照片。不确定这是什么意思...
    | 0.03 |'
- en: '| 16505 | I am looking at four Caymans, all are in a similar price range. The
    major differences are the miles, the years, and one isn’t a S. https://www.cargurus.com/Cars/inventorylisting/viewDetailsFilterV...
    | 0.02 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| 16505 | 我正在看四辆凯曼，价格都差不多。主要区别在于里程、年限，还有一款不是S型。https://www.cargurus.com/Cars/inventorylisting/viewDetailsFilterV...
    | 0.02 |'
- en: Even the dirtiest records, according to our regular expression, look pretty
    clean now. But besides those rough patterns we were searching for, there are also
    more subtle variations of characters that can cause problems.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 即使根据我们的正则表达式，最肮脏的记录现在看起来也非常干净。但除了我们搜索的粗糙模式之外，还有更微妙的字符变体可能会引起问题。
- en: 'Blueprint: Character Normalization with textacy'
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 蓝图：使用textacy进行字符归一化
- en: 'Take a look at the following sentence, which contains typical issues related
    to variants of letters and quote characters:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 注意以下句子，其中包含与字母变体和引号字符相关的典型问题：
- en: '[PRE19]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Accented characters can be a problem because people do not consistently use
    them. For example, the tokens `Saint-Raphaël` and `Saint-Raphael` will not be
    recognized as identical. In addition, texts often contain words separated by a
    hyphen due to the automatic line breaks. Fancy Unicode hyphens and apostrophes
    like the ones used in the text can be a problem for tokenization. For all of these
    issues it makes sense to normalize the text and replace accents and fancy characters
    with ASCII equivalents.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 重音字符可能会带来问题，因为人们并不一致地使用它们。例如，tokens `Saint-Raphaël` 和 `Saint-Raphael` 将不会被识别为相同的。此外，文本经常包含由于自动换行而分隔的单词。像文本中使用的花哨Unicode连字符和撇号这样的字符对于标记化来说可能是个问题。针对所有这些问题，规范化文本并用ASCII等效物替换重音字符和花哨字符是有意义的。
- en: We will use [*textacy*](https://textacy.readthedocs.io) for that purpose. textacy
    is an NLP library built to work with spaCy. It leaves the linguistic part to spaCy
    and focuses on pre- and postprocessing. Thus, its preprocessing module comprises
    a nice collection of functions to normalize characters and to treat common patterns
    such as URLs, email addresses, telephone numbers, and so on, which we will use
    next. [Table 4-1](#tab-textacy-prep) shows a selection of textacy’s preprocessing
    functions. All of these functions work on plain text, completely independent from
    spaCy.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 [*textacy*](https://textacy.readthedocs.io) 来实现这一目的。textacy是一个与spaCy配套使用的NLP库，它将语言学部分交给spaCy，专注于预处理和后处理。因此，其预处理模块包括了一系列用于规范化字符以及处理常见模式（如URLs、电子邮件地址、电话号码等）的函数，我们将在下面使用它们。[表 4-1](#tab-textacy-prep)
    展示了textacy预处理函数的一部分。所有这些函数都可以独立于spaCy在纯文本上工作。
- en: Table 4-1\. Subset of textacy’s preprocessing functions
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4-1\. textacy预处理函数子集
- en: '| Function | Description |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| 功能 | 描述 |'
- en: '| --- | --- |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| `normalize_hyphenated_words` | Reassembles words that were separated by a
    line break |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| `normalize_hyphenated_words` | 重新组合被连字符分隔的单词 |'
- en: '| `normalize_quotation_marks` | Replaces all kind of fancy quotation marks
    with an ASCII equivalent |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| `normalize_quotation_marks` | 用ASCII等效物替换所有类型的花哨引号 |'
- en: '| `normalize_unicode` | Unifies different codes of accented characters in Unicode
    |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| `normalize_unicode` | 统一Unicode中不同格式的重音字符 |'
- en: '| `remove_accents` | Replaces accented characters with ASCII, if possible,
    or drops them |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| `remove_accents` | 尽可能用ASCII替换重音字符，否则删除它们 |'
- en: '| `replace_urls` | Similar for URLs like https://xyz.com |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| `replace_urls` | 类似于URLs，比如 https://xyz.com |'
- en: '| `replace_emails` | Replaces emails with _EMAIL_ |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| `replace_emails` | 将电子邮件替换为_EMAIL_ |'
- en: '| `replace_hashtags` | Similar for tags like #sunshine |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| `replace_hashtags` | 类似于标签 #sunshine |'
- en: '| `replace_numbers` | Similar for numbers like 1235 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| `replace_numbers` | 类似于数字 1235 |'
- en: '| `replace_phone_numbers` | Similar for telephone numbers +1 800 456-6553 |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| `replace_phone_numbers` | 类似于电话号码 +1 800 456-6553 |'
- en: '| `replace_user_handles` | Similar for user handles like @pete |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| `replace_user_handles` | 类似于用户句柄 @pete |'
- en: '| `replace_emojis` | Replaces smileys etc. with _EMOJI_ |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| `replace_emojis` | 用 _EMOJI_ 替换表情符号等 |'
- en: 'Our blueprint function shown here standardizes fancy hyphens and quotes and
    removes accents with the help of textacy:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们这里展示的蓝图函数，使用textacy标准化了花哨的连字符和引号，并通过它来去除重音：
- en: '[PRE20]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'When this is applied to the earlier example sentence, we get the following
    result:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 当这个应用到之前的例句时，我们得到以下结果：
- en: '[PRE21]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '`Out:`'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出:`'
- en: '[PRE22]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Note
  id: totrans-123
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: As Unicode normalization has many facets, you can check out other libraries.
    [*unidecode*](https://oreil.ly/eqKI-), for example, does an excellent job here.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于Unicode规范化有许多方面，您可以查看其他库。例如，[*unidecode*](https://oreil.ly/eqKI-) 在这方面表现出色。
- en: 'Blueprint: Pattern-Based Data Masking with textacy'
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 蓝图：基于模式的数据屏蔽与textacy
- en: Text, in particular content written by users, often contains not only ordinary
    words but also several kinds of identifiers, such as URLs, email addresses, or
    phone numbers. Sometimes we are interested especially in those items, for example,
    to analyze the most frequently mentioned URLs. In many cases, though, it may be
    better to remove or mask this information, either because it is not relevant or
    for privacy reasons.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 文本，尤其是用户写的内容，经常包含不仅是普通单词，还有多种标识符，比如URLs、电子邮件地址或电话号码等。有时我们特别对这些项感兴趣，例如分析提及最频繁的URLs。然而，在许多情况下，出于隐私或无关紧要的原因，删除或屏蔽这些信息可能更为合适。
- en: 'textacy has some convenient `replace` functions for data masking (see [Table 4-1](#tab-textacy-prep)).
    Most of the functions are based on regular expressions, which are easily accessible
    via the [open source code](https://oreil.ly/Ly6Ce). Thus, whenever you need to
    treat any of these items, textacy has a regular expression for it that you can
    directly use or adapt to your needs. Let’s illustrate this with a simple call
    to find the most frequently used URLs in the corpus:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: textacy 提供了一些方便的 `replace` 函数用于数据屏蔽（参见 [表 4-1](#tab-textacy-prep)）。大部分函数基于正则表达式，可以通过
    [开源代码](https://oreil.ly/Ly6Ce) 轻松访问。因此，每当需要处理这些项中的任何一个时，textacy 都有一个相应的正则表达式可以直接使用或根据需要进行调整。让我们通过一个简单的调用来说明这一点，以找到语料库中最常用的
    URL：
- en: '[PRE23]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '| token | freq |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| token | freq |'
- en: '| --- | --- |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| www.getlowered.com | 3 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| www.getlowered.com | 3 |'
- en: '| http://www.ecolamautomotive.com/#!2/kv7fq | 2 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| http://www.ecolamautomotive.com/#!2/kv7fq | 2 |'
- en: '| https://www.reddit.com/r/Jeep/comments/4ux232/just_ordered_an_android_head_unit_joying_jeep/
    | 2 |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| https://www.reddit.com/r/Jeep/comments/4ux232/just_ordered_an_android_head_unit_joying_jeep/
    | 2 |'
- en: 'For the analysis we want to perform with this dataset (in [Chapter 10](ch10.xhtml#ch-embeddings)),
    we are not interested in those URLs. They rather represent a disturbing artifact.
    Thus, we will substitute all URLs in our text with `replace_urls`, which is in
    fact just a call to `RE_URL.sub`. The default substitution for all of textacy’s
    replace functions is a generic tag enclosed by underscores like `_URL_`. You can
    choose your own substitution by specifying the `replace_with` parameter. Often
    it makes sense to not completely remove those items because it leaves the structure
    of the sentences intact. The following call illustrates the functionality:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们在本数据集中要执行的分析（见 [第 10 章](ch10.xhtml#ch-embeddings)），我们对这些 URL 不感兴趣。它们更多地代表了一个干扰因素。因此，我们将用
    `replace_urls` 替换文本中的所有 URL，实际上这只是调用了 `RE_URL.sub`。textacy 所有替换函数的默认替换是用下划线括起来的通用标记，如
    `_URL_`。您可以通过指定 `replace_with` 参数选择自己的替换。通常情况下，不完全移除这些项是有意义的，因为这样可以保持句子的结构不变。下面的调用演示了这一功能：
- en: '[PRE24]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '`Out:`'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE25]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'To finalize data cleaning, we apply the normalization and data masking functions
    to our data:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 为了最终完成数据清理，我们对数据应用标准化和数据屏蔽函数：
- en: '[PRE26]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Data cleaning is like cleaning your house. You’ll always find some dirty corners,
    and you won’t ever get your house totally clean. So you stop cleaning when it
    is *sufficiently* clean. That’s what we assume for our data at the moment. Later
    in the process, if analysis results are suffering from remaining noise, we may
    need to get back to data cleaning.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 数据清理就像打扫你的房子一样。你总会发现一些脏角落，而且你永远也不可能把房子完全打扫干净。所以当它足够干净时你就停止清理了。这就是我们目前对数据的假设。在后续的过程中，如果分析结果受到剩余噪音的影响，我们可能需要重新进行数据清理。
- en: We finally rename the text columns so that `clean_text` becomes `text`, drop
    the impurity column, and store the new version of the `DataFrame` in the database.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将文本列重命名为 `clean_text` 变成 `text`，删除杂质列，并将 `DataFrame` 的新版本存储在数据库中。
- en: '[PRE27]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Tokenization
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分词
- en: 'We already introduced a regex tokenizer in [Chapter 1](ch01.xhtml#ch-exploration),
    which used a simple rule. In practice, however, tokenization can be quite complex
    if we want to treat everything correctly. Consider the following piece of text
    as an example:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在 [第 1 章](ch01.xhtml#ch-exploration) 中引入了一个正则表达式分词器，它使用了一个简单的规则。然而，在实践中，如果我们希望正确处理一切，分词可能会相当复杂。考虑下面的文本片段作为例子：
- en: '[PRE28]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Obviously, the rules to define word and sentence boundaries are not that simple.
    So what exactly is a token? Unfortunately, there is no clear definition. We could
    say that a token is a linguistic unit that is semantically useful for analysis.
    This definition implies that tokenization is application dependent to some degree.
    For example, in many cases we can simply discard punctuation characters, but not
    if we want to keep emoticons like `:-)` for sentiment analysis. The same is true
    for tokens containing numbers or hashtags. Even though most tokenizers, including
    those used in NLTK and spaCy, are based on regular expressions, they apply quite
    complex and sometimes language-specific rules.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，定义词和句边界的规则并不是那么简单。那么什么是一个标记？不幸的是，没有明确的定义。我们可以说，一个标记是一个在语义分析中有用的语言单元。这个定义意味着分词在某种程度上依赖于应用程序。例如，在许多情况下，我们可以简单地丢弃标点符号，但如果我们想要保留表情符号像
    `:-)` 用于情感分析，则不行。对于包含数字或标签的标记也是如此。尽管大多数分词器，包括 NLTK 和 spaCy 中使用的分词器，都基于正则表达式，但它们应用的规则相当复杂，有时是语言特定的。
- en: We will first develop our own blueprint for tokenization-based regular expressions
    before we briefly introduce NLTK’s tokenizers. Tokenization in spaCy will be covered
    in the next section of this chapter as part of spaCy’s integrated process.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先为基于分词的正则表达式开发我们自己的蓝图，然后简要介绍 NLTK 的分词器。spaCy 中的分词将在本章的下一节作为 spaCy 综合处理的一部分进行讨论。
- en: 'Blueprint: Tokenization with Regular Expressions'
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 蓝图：正则表达式分词
- en: Useful functions for tokenization are `re.split()` and `re.findall()`. The first
    one splits a string at matching expressions, while the latter extracts all character
    sequences matching a certain pattern. For example, in [Chapter 1](ch01.xhtml#ch-exploration)
    we used the `regex` library with the POSIX pattern `[\w-]*\p{L}[\w-]*` to find
    sequences of alphanumeric characters with at least one letter. The scikit-learn
    `CountVectorizer` uses the pattern `\w\w+` for its default tokenization. It matches
    all sequences of two or more alphanumeric characters. Applied to our sample sentence,
    this yields the following result:^([3](ch04.xhtml#idm45634201737000))
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 分词的有用函数包括`re.split()`和`re.findall()`。前者在匹配表达式时将字符串分割，而后者提取所有匹配特定模式的字符序列。例如，在[第一章](ch01.xhtml#ch-exploration)中，我们使用
    POSIX 模式`\[\w-]*\p{L}\[\w-]*`和`regex`库来查找至少包含一个字母的字母数字字符序列。scikit-learn 的`CountVectorizer`默认使用模式`\w\w+`进行分词。它匹配所有由两个或更多字母数字字符组成的序列。应用于我们的示例句子时，它产生以下结果：^([3](ch04.xhtml#idm45634201737000))
- en: '[PRE29]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '`Out:`'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出：`'
- en: '[PRE30]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Unfortunately, all special characters and the emojis are lost. To improve the
    result, we add some additional expressions for the emojis and create a reusable
    regular expression `RE_TOKEN`. The `VERBOSE` option allows readable formatting
    of complex expressions. The following `tokenize` function and the example illustrate
    the use:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 遗憾的是，所有特殊字符和表情符号都丢失了。为了改善结果，我们添加了一些表情符号的额外表达式，并创建了可重用的正则表达式`RE_TOKEN`。`VERBOSE`选项允许对复杂表达式进行可读格式化。以下是`tokenize`函数和示例说明其用法：
- en: '[PRE31]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '`Out:`'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出：`'
- en: '[PRE32]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: This expression should yield reasonably good results on most user-generated
    content. It can be used to quickly tokenize text for data exploration, as explained
    in [Chapter 1](ch01.xhtml#ch-exploration). It’s also a good alternative for the
    default tokenization of the scikit-learn vectorizers, which will be introduced
    in the next chapter.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 这个表达式应该能够在大多数用户生成内容上产生合理的结果。它可用于快速为数据探索分词，正如在[第一章](ch01.xhtml#ch-exploration)中所解释的那样。对于
    scikit-learn 向量化器的默认分词，它也是一个很好的替代选择，该向量化器将在下一章中介绍。
- en: Tokenization with NLTK
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 NLTK 进行分词
- en: 'Let’s take a brief look at NLTK’s tokenizers, as NLTK is frequently used for
    tokenization. The standard NLTK tokenizer can be called by the shortcut `word_tokenize`.
    It produces the following result on our sample text:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们简要了解一下 NLTK 的分词器，因为 NLTK 经常用于分词。标准的 NLTK 分词器可以通过快捷方式`word_tokenize`调用。它在我们的示例文本上产生以下结果：
- en: '[PRE33]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '`Out:`'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出：`'
- en: '[PRE34]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: The function internally uses the `TreebankWordTokenizer` in combination with
    the `PunktSentenceTokenizer`. It works well for standard text but has its flaws
    with hashtags or text emojis. NLTK also provides a `RegexpTokenizer`, which is
    basically a wrapper for `re.findall()` with some added convenience functionality.
    Besides that, there are other regular-expression-based tokenizers in NLTK, like
    the `TweetTokenizer` or the multilingual `ToktokTokenizer`, which you can check
    out in the notebook on [GitHub](https://oreil.ly/zLTEl) for this chapter.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数在内部结合了`TreebankWordTokenizer`和`PunktSentenceTokenizer`。它适用于标准文本，但在处理标签或文本表情符号时存在缺陷。NLTK
    还提供了`RegexpTokenizer`，它基本上是`re.findall()`的包装器，具有一些附加的便利功能。此外，NLTK 中还有其他基于正则表达式的分词器，如`TweetTokenizer`或多语言`ToktokTokenizer`，您可以在本章的[GitHub](https://oreil.ly/zLTEl)笔记本中查看。
- en: Recommendations for Tokenization
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分词建议
- en: You will probably need to use custom regular expressions if you aim for high
    precision on domain-specific token patterns. Fortunately, you can find regular
    expressions for many common patterns in open source libraries and adapt them to
    your needs.^([4](ch04.xhtml#idm45634201564856))
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的目标是在特定领域的标记模式上达到高精度，您可能需要使用自定义正则表达式。幸运的是，您可以在开源库中找到许多常见模式的正则表达式，并根据自己的需求进行调整。^([4](ch04.xhtml#idm45634201564856))
- en: In general, you should be aware of the following problematic cases in your application
    and define how to treat them:^([5](ch04.xhtml#idm45634201561544))
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 一般情况下，您应该注意应用程序中的以下问题案例，并定义如何处理它们：^([5](ch04.xhtml#idm45634201561544))
- en: Tokens containing periods, such as `Dr.`, `Mrs.`, `U.`, `xyz.com`
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包含句点的标记，比如`Dr.`、`Mrs.`、`U.`、`xyz.com`
- en: Hyphens, like in `rule-based`
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 连字符，比如`rule-based`
- en: Clitics (connected word abbreviations), like in `couldn't`, `we've` or `je t'aime`
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缩写词（连接词缩写），如 `couldn't`、`we've` 或 `je t'aime`
- en: Numerical expressions, such as telephone numbers (`(123) 456-7890`) or dates
    (`August 7th, 2019`)
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数字表达式，如电话号码（`(123) 456-7890`）或日期（`2019年8月7日`）
- en: Emojis, hashtags, email addresses, or URLs
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 表情符号，主题标签，电子邮件地址或网址
- en: The tokenizers in common libraries differ especially with regard to those tokens.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 常见库中的分词器特别在这些标记方面有所不同。
- en: Linguistic Processing with spaCy
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 spaCy 进行语言处理
- en: spaCy is a powerful library for linguistic data processing. It provides an integrated
    pipeline of processing components, by default a tokenizer, a part-of-speech tagger,
    a dependency parser, and a named-entity recognizer (see [Figure 4-2](#fig-spacy-pipe)).
    Tokenization is based on complex language-dependent rules and regular expressions,
    while all subsequent steps use pretrained neural models.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy 是一个强大的语言数据处理库。它提供了一个集成的处理组件流水线，默认包括分词器、词性标注器、依存解析器和命名实体识别器（详见 [图 4-2](#fig-spacy-pipe)）。分词基于复杂的语言相关规则和正则表达式，而所有后续步骤都使用预训练的神经模型。
- en: '![](Images/btap_0402.jpg)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/btap_0402.jpg)'
- en: Figure 4-2\. spaCy’s NLP pipeline.
  id: totrans-176
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-2\. spaCy 的自然语言处理流水线。
- en: The philosophy of spaCy is that the original text is retained throughout the
    process. Instead of transforming it, spaCy adds layers of information. The main
    object to represent the processed text is a `Doc` object, which itself contains
    a list of `Token` objects. Any range selection of tokens creates a `Span`. Each
    of these object types has properties that are determined step-by-step.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy 的哲学是在整个处理过程中保留原始文本。而不是转换它，spaCy 添加了信息层。用于表示处理过的文本的主要对象是 `Doc` 对象，它本身包含了
    `Token` 对象的列表。任何一组标记的选择都会创建一个 `Span`。每种这些对象类型都有逐步确定的属性。
- en: In this section, we explain how to process a document with spaCy, how to work
    with tokens and their attributes, how to use part-of-speech tags, and how to extract
    named entities. We will dive even deeper into spaCy’s more advanced concepts in
    [Chapter 12](ch12.xhtml#ch-knowledge), where we write our own pipeline components,
    create custom attributes, and work with the dependency tree generated by the parser
    for knowledge extraction.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将解释如何使用 spaCy 处理文档，如何处理标记及其属性，如何使用词性标签以及如何提取命名实体。我们将在 [第 12 章](ch12.xhtml#ch-knowledge)
    深入探讨 spaCy 更高级的概念，其中我们编写自己的管道组件，创建自定义属性，并使用解析器生成的依存树进行知识提取。
- en: Warning
  id: totrans-179
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: For the development of the examples in this book, we used spaCy version 2.3.2\.
    If you already use spaCy 3.0, which is still under development at the time of
    writing, your results may look slightly different.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 本书示例的开发使用的是 spaCy 版本 2.3.2\. 如果您已经在使用仍在开发中的 spaCy 3.0，那么您的结果可能会略有不同。
- en: Instantiating a Pipeline
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实例化管道
- en: 'Let’s get started with spaCy. As a first step we need to instantiate an object
    of spaCy’s `Language` class by calling `spacy.load()` along with the name of the
    model file to use.^([6](ch04.xhtml#idm45634201500904)) We will use the small English
    language model `en_core_web_sm` in this chapter. The variable for the `Language`
    object is usually called `nlp`:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始使用 spaCy。作为第一步，我们需要通过调用 `spacy.load()` 并指定要使用的模型文件来实例化 spaCy 的 `Language`
    类的对象。^([6](ch04.xhtml#idm45634201500904)) 在本章中，我们将使用小型英语语言模型 `en_core_web_sm`。通常用于
    `Language` 对象的变量是 `nlp`：
- en: '[PRE35]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'This `Language` object now contains the shared vocabulary, the model, and the
    processing pipeline. You can check the pipeline components via this property of
    the object:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 现在这个 `Language` 对象包含了共享的词汇表、模型和处理管道。您可以通过该对象的属性检查管道组件：
- en: '[PRE36]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '`Out:`'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出:`'
- en: '[PRE37]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: The default pipeline consists of a tagger, parser, and named-entity recognizer
    (`ner`), all of which are language-dependent. The tokenizer is not explicitly
    listed because this step is always necessary.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 默认的处理流程包括标注器、解析器和命名实体识别器（`ner`），所有这些都是依赖于语言的。分词器没有明确列出，因为这一步骤总是必要的。
- en: spaCy’s tokenizer is pretty fast, but all other steps are based on neural models
    and consume a significant amount of time. Compared to other libraries, though,
    spaCy’s models are among the fastest. Processing the whole pipeline takes about
    10–20 times as long as just tokenization, where each step is taking a similar
    share of the total time. If tokenization of 1,000 documents takes, for example,
    one second, tagging, parsing, and NER may each take an additional five seconds.
    This may become a problem if you process big datasets. So, it’s better to switch
    off the parts that you don’t need.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy 的分词速度相当快，但所有其他步骤都基于神经模型，消耗大量时间。与其他库相比，spaCy 的模型速度是最快的。处理整个流程大约需要 10 到
    20 倍于仅仅进行分词的时间，每个步骤所占的时间相似。例如，对 1,000 个文档进行分词如果需要一秒钟，标记、解析和命名实体识别可能会额外花费五秒钟。如果处理大型数据集，这可能成为问题。因此，最好关闭你不需要的部分。
- en: 'Often you will only need the tokenizer and the part-of-speech tagger. In this
    case, you should disable the parser and named-entity recognition like this:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 通常你只需要分词器和词性标注器。在这种情况下，你应该像这样禁用解析器和命名实体识别：
- en: '[PRE38]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: If you just want the tokenizer and nothing else, you can also simply call `nlp.make_doc`
    on a text.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你只需要分词器而不需要其他东西，你也可以简单地在文本上调用 `nlp.make_doc`。
- en: Processing Text
  id: totrans-193
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 文本处理
- en: The pipeline is executed by calling the `nlp` object. The call returns an object
    of type `spacy.tokens.doc.Doc`, a container to access the tokens, spans (ranges
    of tokens), and their linguistic annotations.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 通过调用 `nlp` 对象执行流水线。调用返回一个 `spacy.tokens.doc.Doc` 类型的对象，一个访问 token、span（token
    范围）及其语言标注的容器。
- en: '[PRE39]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'spaCy is object-oriented as well as nondestructive. The original text is always
    retained. When you print the `doc` object, it uses `doc.text`, the property containing
    the original text. But `doc` is also a container object for the tokens, and you
    can use it as an iterator for them:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy 是面向对象的同时也是非破坏性的。原始文本始终保留。当你打印 `doc` 对象时，它使用 `doc.text`，这个属性包含原始文本。但 `doc`
    也是一个 token 的容器，你可以像对待迭代器一样使用它们：
- en: '[PRE40]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '`Out:`'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE41]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Each token is actually an object of spaCy’s class `Token`. Tokens, as well as
    docs, have a number of interesting properties for language processing. [Table 4-2](#tab-spacy-attributes)
    shows which of these properties are created by each pipeline component.^([7](ch04.xhtml#idm45634201354856))
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 每个 token 实际上是 spaCy 类 `Token` 的对象。Token 和 doc 都有许多用于语言处理的有趣属性。[Table 4-2](#tab-spacy-attributes)
    显示了每个流水线组件创建的这些属性。^([7](ch04.xhtml#idm45634201354856))
- en: Table 4-2\. Selection of attributes created by spaCy’s built-in pipeline
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4-2\. spaCy 内置流水线创建的属性选择
- en: '| Component | Creates |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| 组件 | 创建 |'
- en: '| --- | --- |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Tokenizer | `Token.is_punct`, `Token.is_alpha`, `Token.like_email`, `Token.like_url`
    |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| 分词器 | `Token.is_punct`, `Token.is_alpha`, `Token.like_email`, `Token.like_url`
    |'
- en: '| Part-of-speech tagger | `Token.pos_` |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| 词性标注器 | `Token.pos_` |'
- en: '| Dependency parser | `Token.dep_`, `Token.head`, `Doc.sents`, `Doc.noun_chunks`
    |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| 依赖解析器 | `Token.dep_`, `Token.head`, `Doc.sents`, `Doc.noun_chunks` |'
- en: '| Named-entity recognizer | `Doc.ents`, `Token.ent_iob_`, `Token.ent_type_`
    |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| 命名实体识别器 | `Doc.ents`, `Token.ent_iob_`, `Token.ent_type_` |'
- en: 'We provide a small utility function, `display_nlp`, to generate a table containing
    the tokens and their attributes. Internally, we create a `DataFrame` for this
    and use the token position in the document as an index. Punctuation characters
    are skipped by default in this function. [Table 4-3](#tab-nlp-result) shows the
    output of this function for our example sentence:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提供了一个小型实用函数 `display_nlp`，生成包含 token 及其属性的表格。在内部，我们为此创建了一个 `DataFrame`，并将文档中的
    token 位置用作索引。默认情况下，此函数跳过标点符号。[Table 4-3](#tab-nlp-result) 显示了我们示例句子的输出：
- en: '[PRE42]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Table 4-3\. Result of spaCy’s document processing as generated by display_nlp
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4-3\. spaCy 文档处理的结果，由 `display_nlp` 生成
- en: '|  | text | lemma_ | is_stop | is_alpha | pos_ | dep_ | ent_type_ | ent_iob_
    |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '|  | text | lemma_ | is_stop | is_alpha | pos_ | dep_ | ent_type_ | ent_iob_
    |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| 0 | My | -PRON- | True | True | DET | poss |  | O |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| 0 | My | -PRON- | True | True | DET | poss |  | O |'
- en: '| 1 | best | good | False | True | ADJ | amod |  | O |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| 1 | best | good | False | True | ADJ | amod |  | O |'
- en: '| 2 | friend | friend | False | True | NOUN | nsubj |  | O |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| 2 | friend | friend | False | True | NOUN | nsubj |  | O |'
- en: '| 3 | Ryan | Ryan | False | True | PROPN | compound | PERSON | B |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| 3 | Ryan | Ryan | False | True | PROPN | compound | PERSON | B |'
- en: '| 4 | Peters | Peters | False | True | PROPN | appos | PERSON | I |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| 4 | Peters | Peters | False | True | PROPN | appos | PERSON | I |'
- en: '| 5 | likes | like | False | True | VERB | ROOT |  | O |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| 5 | likes | like | False | True | VERB | ROOT |  | O |'
- en: '| 6 | fancy | fancy | False | True | ADJ | amod |  | O |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| 6 | fancy | fancy | False | True | ADJ | amod |  | O |'
- en: '| 7 | adventure | adventure | False | True | NOUN | compound |  | O |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| 7 | adventure | adventure | False | True | NOUN | compound |  | O |'
- en: '| 8 | games | game | False | True | NOUN | dobj |  | O |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| 8 | games | game | False | True | NOUN | dobj |  | O |'
- en: For each token, you find the lemma, some descriptive flags, the part-of-speech
    tag, the dependency tag (not used here, but in [Chapter 12](ch12.xhtml#ch-knowledge)),
    and possibly some information about the entity type. The `is_<something>` flags
    are created based on rules, but all part-of-speech, dependency, and named-entity
    attributes are based on neural network models. So, there is always some degree
    of uncertainty in this information. The corpora used for training contain a mixture
    of news articles and online articles. The predictions of the model are fairly
    accurate if your data has similar linguistic characteristics. But if your data
    is very different—if you are working with Twitter data or IT service desk tickets,
    for example—you should be aware that this information is unreliable.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个标记，您可以找到词元、一些描述性标志、词性标签、依赖标签（这里未使用，但在[第12章](ch12.xhtml#ch-knowledge)中使用），以及可能有关实体类型的信息。`is_<something>`
    标志是基于规则创建的，但所有词性、依赖和命名实体属性都基于神经网络模型。因此，这些信息总是存在一定的不确定性。用于训练的语料库包含新闻文章和在线文章的混合体。如果您的数据具有相似的语言特征，则模型的预测非常准确。但是，如果您的数据非常不同——例如，您正在处理
    Twitter 数据或 IT 服务台工单——您应该意识到这些信息是不可靠的。
- en: Warning
  id: totrans-223
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: spaCy uses the convention that token attributes with an underscore like `pos_`
    yield the readable textual representation. `pos` without an underscore returns
    spaCy’s numeric identifier of a part-of-speech tag.^([8](ch04.xhtml#idm45634201147656))
    The numeric identifiers can be imported as constants, e.g., `spacy.symbols.VERB`.
    Make sure not to mix them up!
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy 使用带有下划线的标记属性约定，例如 `pos_` 返回可读的文本表示。不带下划线的 `pos` 返回 spaCy 的词性标签的数值标识符。^([8](ch04.xhtml#idm45634201147656))
    这些数值标识符可以作为常量导入，例如 `spacy.symbols.VERB`。请确保不要混淆它们！
- en: 'Blueprint: Customizing Tokenization'
  id: totrans-225
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 蓝图：自定义标记化
- en: 'Tokenization is the first step in the pipeline, and everything depends on the
    correct tokens. spaCy’s tokenizer does a good job in most cases, but it splits
    on hash signs, hyphens, and underscores, which is sometimes not what you want.
    Therefore, it may be necessary to adjust its behavior. Let’s look at the following
    text as an example:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 标记化是管道中的第一步，一切都依赖于正确的标记。在大多数情况下，spaCy 的标记器表现良好，但有时会在井号、连字符和下划线上分割，这并不总是您想要的。因此，可能需要调整其行为。让我们以以下文本作为例子：
- en: '[PRE43]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '`Out:`'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE44]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: spaCy’s tokenizer is completely rule-based. First, it splits the text on whitespace
    characters. Then it uses prefix, suffix, and infix splitting rules defined by
    regular expressions to further split the remaining tokens. Exception rules are
    used to handle language-specific exceptions like *can’t*, which should be split
    into *ca* and *n’t* with lemmas *can* and *not*.^([9](ch04.xhtml#idm45634201023736))
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy 的标记器完全基于规则。首先，它在空格字符上分割文本。然后，它使用由正则表达式定义的前缀、后缀和中缀分割规则来进一步分割剩余的标记。异常规则用于处理语言特定的异常情况，如
    *can’t*，应该分割为 *ca* 和 *n’t*，词元为 *can* 和 *not*。^([9](ch04.xhtml#idm45634201023736))
- en: As you can see in the example, spaCy’s English tokenizer contains an infix rule
    for splits at hyphens. In addition, it has a prefix rule to split off characters
    like `#` or `_`. It works well for tokens prefixed with `@` and emojis, though.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 如您在示例中所见，spaCy 的英文标记器包含一个中缀规则，用于在连字符处拆分。此外，它还有一个前缀规则，用于拆分类似 `#` 或 `_` 的字符。它对以
    `@` 开头的标记和表情符号也适用。
- en: One option is to merge tokens in a postprocessing step using `doc.retokenize`.
    However, that will not fix any miscalculated part-of-speech tags and syntactical
    dependencies because these rely on tokenization. So it may be better to change
    the tokenization rules and create correct tokens in the first place.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 一种选项是在后处理步骤中使用 `doc.retokenize` 合并标记。然而，这并不能修复任何计算错误的词性标签和句法依赖，因为这些依赖于标记化。因此，更改标记化规则并在一开始创建正确的标记可能会更好。
- en: 'The best approach for this is to create your own variant of the tokenizer with
    individual rules for infix, prefix, and suffix splitting.^([10](ch04.xhtml#idm45634201018552))
    The following function creates a tokenizer object with individual rules in a “minimally
    invasive” way: we just drop the respective patterns from spaCy’s default rules
    but retain the major part of the logic:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个问题，最好的方法是创建自己的分词器变体，具有单独的中缀、前缀和后缀分割规则。^([10](ch04.xhtml#idm45634201018552))
    下面的函数以“最小侵入”方式创建了一个具有单独规则的分词器对象：我们只是从spaCy的默认规则中删除了相应的模式，但保留了主要部分的逻辑：
- en: '[PRE45]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '`Out:`'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE46]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Warning
  id: totrans-237
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Be careful with tokenization modifications because their effects can be subtle,
    and fixing a set of cases can break another set of cases. For example, with our
    modification, tokens like `Chicago-based` won’t be split anymore. In addition,
    there are several Unicode characters for hyphens and dashes that could cause problems
    if they have not been normalized.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 在修改分词的过程中要小心，因为它们的影响可能会很微妙，修复一组案例可能会破坏另一组案例。例如，通过我们的修改，像`Chicago-based`这样的标记将不再被分割。此外，如果Unicode字符的连字符和破折号没有被规范化，可能会出现问题。
- en: 'Blueprint: Working with Stop Words'
  id: totrans-239
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 蓝图：处理停用词
- en: 'spaCy uses language-specific stop word lists to set the `is_stop` property
    for each token directly after tokenization. Thus, filtering stop words (and similarly
    punctuation tokens) is easy:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy使用语言特定的停用词列表直接在分词后为每个标记设置`is_stop`属性。因此，过滤停用词（以及类似的标点符号标记）非常容易：
- en: '[PRE47]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '`Out:`'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE48]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: The list of English stop words with more than 300 entries can be accessed by
    importing `spacy.lang.en.STOP_WORDS`. When an `nlp` object is created, this list
    is loaded and stored under `nlp.Defaults.stop_words`. We can modify spaCy’s default
    behavior by setting the `is_stop` property of the respective words in spaCy’s
    vocabulary:^([11](ch04.xhtml#idm45634200775880))
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过导入`spacy.lang.en.STOP_WORDS`来访问包含超过300个条目的英文停用词列表。当创建一个`nlp`对象时，该列表被加载并存储在`nlp.Defaults.stop_words`下。我们可以通过设置spaCy词汇表中相应单词的`is_stop`属性来修改spaCy的默认行为:^([11](ch04.xhtml#idm45634200775880))
- en: '[PRE49]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'If we rerun the previous example, we get the following result:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们重新运行上一个示例，我们将得到以下结果：
- en: '[PRE50]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Blueprint: Extracting Lemmas Based on Part of Speech'
  id: totrans-248
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 蓝图：基于词性提取词元
- en: '*Lemmatization* is the mapping of a word to its uninflected root. Treating
    words like *housing*, *housed*, and *house* as the same has many advantages for
    statistics, machine learning, and information retrieval. It can not only improve
    the quality of the models but also decrease training time and model size because
    the vocabulary is much smaller if only uninflected forms are kept. In addition,
    it is often helpful to restrict the types of the words used to certain categories,
    such as nouns, verbs, and adjectives. Those word types are called *part-of-speech
    tags*.'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '*词形还原*是将单词映射到其未屈折根的过程。像*housing*、*housed*和*house*这样的词被视为相同，对于统计、机器学习和信息检索具有许多优势。它不仅可以改善模型的质量，还可以减少训练时间和模型大小，因为词汇量如果只保留未屈折形式会更小。此外，将单词类型限制为特定类别，如名词、动词和形容词，通常也是有帮助的。这些词类型称为*词性标签*。'
- en: 'Let’s first take a closer look at lemmatization. The lemma of a token or span
    can be accessed by the `lemma_` property, as illustrated in the following example:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先深入了解词形还原。可以通过`lemma_`属性访问标记或跨度的词元，如下例所示：
- en: '[PRE51]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '`Out:`'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE52]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: The correct assignment of the lemma requires a lookup dictionary and knowledge
    about the part of speech of a word. For example, the lemma of the noun *meeting*
    is *meeting*, while the lemma of the verb is *meet*. In English, spaCy is able
    to make this distinction. In most other languages, however, lemmatization is purely
    dictionary-based, ignoring the part-of-speech dependency. Note that personal pronouns
    like *I*, *me*, *you*, and *her* always get the lemma `-PRON-` in spaCy.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 正确地分配词元需要查找字典和对单词的词性的知识。例如，名词*meeting*的词元是*meeting*，而动词*meet*的词元是*meet*。在英语中，spaCy能够做到这种区分。然而，在大多数其他语言中，词形还原纯粹基于字典，忽略了词性依赖。请注意，像*I*、*me*、*you*和*her*这样的人称代词在spaCy中总是得到词元`-PRON-`。
- en: 'The other token attribute we will use in this blueprint is the part-of-speech
    tag. [Table 4-3](#tab-nlp-result) shows that each token in a spaCy doc has two
    part-of-speech attributes: `pos_` and `tag_`. `tag_` is the tag from the tagset
    used to train the model. For spaCy’s English models, which have been trained on
    the OntoNotes 5 corpus, this is the Penn Treebank tagset. For a German model,
    this would be the Stuttgart-Tübingen tagset. The `pos_` attribute contains the
    simplified tag of the universal part-of-speech tagset.^([12](ch04.xhtml#idm45634200544984))
    We recommend using this attribute as the values will remain stable across different
    models. [Table 4-4](#tab-pos-tags) shows the complete tag set descriptions.'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 在这份蓝图中我们将使用的另一个标记属性是词性标记。[表 4-3](#tab-nlp-result) 显示 spaCy 文档中的每个标记都有两个词性属性：`pos_`
    和 `tag_`。 `tag_` 是从用于训练模型的标记集中提取的标记。对于 spaCy 的英语模型，它们是在 OntoNotes 5 语料库上训练的，这是宾夕法尼亚树库标记集。对于德语模型，这将是斯图加特-图宾根标记集。
    `pos_` 属性包含通用词性标记集的简化标记。^([12](ch04.xhtml#idm45634200544984)) 我们建议使用此属性，因为其值将在不同模型之间保持稳定。[表
    4-4](#tab-pos-tags) 显示了完整的标记集描述。
- en: Table 4-4\. Universal part-of-speech tags
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4-4\. 通用词性标记
- en: '| Tag | Description | Examples |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| Tag | 描述 | 例子 |'
- en: '| --- | --- | --- |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| ADJ | Adjectives (describe nouns) | big, green, African |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| ADJ | 形容词（描述名词） | 大的，绿色的，非洲的 |'
- en: '| ADP | Adpositions (prepositions and postpositions) | in, on |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| ADP | 介词（前置词和后置词） | 在，上 |'
- en: '| ADV | Adverbs (modify verbs or adjectives) | very, exactly, always |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| ADV | 副词（修改动词或形容词） | 非常，确切地，总是 |'
- en: '| AUX | Auxiliary (accompanies verb) | can (do), is (doing) |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| AUX | 助动词（伴随动词） | 能（做），是（在做） |'
- en: '| CCONJ | Connecting conjunction | and, or, but |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| CCONJ | 连接连词 | 和，或，但是 |'
- en: '| DET | Determiner (with regard to nouns) | the, a, all (things), your (idea)
    |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| DET | 限定词（关于名词） | 这个，一个，所有（事物），你的（想法） |'
- en: '| INTJ | Interjection (independent word, exclamation, expression of emotion)
    | hi, yeah |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| INTJ | 感叹词（独立词，感叹词，表达情感） | 嗨，是的 |'
- en: '| NOUN | Nouns (common and proper) | house, computer |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| NOUN | 名词（普通名词和专有名词） | 房子，电脑 |'
- en: '| NUM | Cardinal numbers | nine, 9, IX |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| NUM | 基数 | 九，9，IX |'
- en: '| PROPN | Proper noun, name, or part of a name | Peter, Berlin |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| PROPN | 专有名词，名字或名字的一部分 | 彼得，柏林 |'
- en: '| PRON | Pronoun, substitute for noun | I, you, myself, who |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| PRON | 代词，代替名词 | 我，你，我自己，谁 |'
- en: '| PART | Particle (makes sense only with other word) |  |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| PART | 粒子（只有与其他单词一起才有意义） |  |'
- en: '| PUNCT | Punctuation characters | , . ; |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| PUNCT | 标点符号字符 | ，。； |'
- en: '| SCONJ | Subordinating conjunction | before, since, if |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| SCONJ | 从属连词 | 在…之前，因为，如果 |'
- en: '| SYM | Symbols (word-like) | $, © |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| SYM | 符号（类似单词） | $，© |'
- en: '| VERB | Verbs (all tenses and modes) | go, went, thinking |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| VERB | 动词（所有时态和方式） | 去，去过，思考 |'
- en: '| X | Anything that cannot be assigned | grlmpf |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| X | 任何无法分配的东西 | grlmpf |'
- en: Part-of-speech tags are an excellent alternative to stop words as word filters.
    In linguistics, pronouns, prepositions, conjunctions, and determiners are called
    *function words* because their main function is to create grammatical relationships
    within a sentence. Nouns, verbs, adjectives, and adverbs are content words, and
    the meaning of a sentence depends mainly on them.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 词性标记是作为单词过滤器的出色选择。在语言学中，代词、介词、连词和限定词被称为*功能词*，因为它们的主要功能是在句子内创建语法关系。名词、动词、形容词和副词是内容词，句子的意义主要取决于它们。
- en: 'Often, we are interested only in content words. Thus, instead of using a stop
    word list, we can use part-of-speech tags to select the word types we are interested
    in and discard the rest. For example, a list containing only the nouns and proper
    nouns in a `doc` can be generated like this:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们只对内容词感兴趣。因此，我们可以使用词性标记来选择我们感兴趣的单词类型，并且丢弃其余部分。例如，可以生成一个仅包含文档中名词和专有名词的列表：
- en: '[PRE53]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '`Out:`'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出：`'
- en: '[PRE54]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: We could easily define a more general filter function for this purpose, but
    textacy’s `extract.words` function conveniently provides this functionality. It
    also allows us to filter on part of speech and additional token properties such
    as `is_punct` or `is_stop`. Thus, the filter function allows both part-of-speech
    selection and stop word filtering. Internally it works just like we illustrated
    for the noun filter shown previously.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以很容易地为此目的定义一个更通用的过滤器函数，但是 textacy 的 `extract.words` 函数方便地提供了此功能。它还允许我们根据词性和其他标记属性（如
    `is_punct` 或 `is_stop`）进行过滤。因此，过滤函数允许同时进行词性选择和停用词过滤。在内部，它的工作原理与我们之前展示的名词过滤器所示的方式相同。
- en: 'The following example shows how to extract tokens for adjectives and nouns
    from the sample sentence:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例展示了如何从样本句子中提取形容词和名词的标记：
- en: '[PRE55]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '`Out:`'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE56]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: Our blueprint function to extract a filtered list of word lemmas is finally
    just a tiny wrapper around that function. By forwarding the keyword arguments
    (`**kwargs`), this function accepts the same parameters as textacy’s `extract.words`.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，我们提取过滤后的词元列表的蓝图函数只是这个函数的一个小包装。通过转发关键字参数（`**kwargs`），这个函数接受与textacy的`extract.words`相同的参数。
- en: '[PRE57]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: '`Out:`'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE58]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: Note
  id: totrans-290
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Using lemmas instead of inflected words is often a good idea, but not always.
    For example, it can have a negative effect on sentiment analysis where “good”
    and “best” make a difference.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 使用词元而不是屈折词通常是个好主意，但并非总是如此。例如，在情感分析中，“好”和“最好”会产生不同的效果。
- en: 'Blueprint: Extracting Noun Phrases'
  id: totrans-292
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 蓝图：提取名词短语
- en: 'In [Chapter 1](ch01.xhtml#ch-exploration) we illustrated how to use n-grams
    for analysis. n-grams are simple enumerations of subsequences of *n* words in
    a sentence. For example, the sentence we used earlier contains the following bigrams:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第1章](ch01.xhtml#ch-exploration)中，我们说明了如何使用n-gram进行分析。n-gram是句子中*n*个连续词的简单枚举。例如，我们之前使用的句子包含以下二元组：
- en: '[PRE59]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Many of those bigrams are not very useful for analysis, for example, `likes_fancy`
    or `my_best`. It would be even worse for trigrams. But how can we detect word
    sequences that have real meaning? One way is to apply pattern-matching on the
    part-of-speech tags. spaCy has a quite powerful [rule-based matcher](https://oreil.ly/VjOJK),
    and textacy has a convenient wrapper for [pattern-based phrase extraction](https://oreil.ly/Xz70U).
    The following pattern extracts sequences of nouns with a preceding adjective:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 许多这些二元组对于分析并不十分有用，例如，`likes_fancy`或`my_best`。对于三元组而言情况可能会更糟。但是我们如何检测具有实际含义的词序列呢？一种方法是对词性标记应用模式匹配。spaCy具有一个相当强大的[基于规则的匹配器](https://oreil.ly/VjOJK)，而textacy则提供了一个便捷的[基于模式的短语提取包装器](https://oreil.ly/Xz70U)。以下模式提取带有前置形容词的名词序列：
- en: '[PRE60]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: '`Out:`'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE61]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Alternatively, you could use spaCy’s `doc.noun_chunks` for noun phrase extraction.
    However, as the returned chunks can also include pronouns and determiners, this
    function is less suited for feature extraction:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，您可以使用spaCy的`doc.noun_chunks`进行名词短语提取。但是，由于返回的块还可能包括代词和限定词，因此此功能不太适合用于特征提取：
- en: '[PRE62]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: '`Out:`'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE63]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Thus, we define our blueprint for noun phrase extraction based on part-of-speech
    patterns. The function takes a `doc`, a list of part-of-speech tags, and a separator
    character to join the words of the noun phrase. The constructed pattern searches
    for sequences of nouns that are preceded by a token with one of the specified
    part-of-speech tags. Returned are the lemmas. Our example extracts all phrases
    consisting of an adjective or a noun followed by a sequence of nouns:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们根据词性模式定义了我们的名词短语提取蓝图。该函数接受一个`doc`，一组词性标记以及一个分隔字符，用于连接名词短语中的单词。构造的模式搜索由形容词或名词后跟名词序列组成的短语。返回的是词元。我们的例子提取所有由形容词或名词后跟名词序列组成的短语：
- en: '[PRE64]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: '`Out:`'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE65]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Blueprint: Extracting Named Entities'
  id: totrans-307
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 蓝图：提取命名实体
- en: '*Named-entity* *recognition* refers to the process of detecting entities such
    as people, locations, or organizations in text. Each entity can consist of one
    or more tokens, like *San Francisco*. Therefore, named entities are represented
    by `Span` objects. As with noun phrases, it can be helpful to retrieve a list
    of named entities for further analysis.'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: '*命名实体识别*指的是在文本中检测人物、地点或组织等实体的过程。每个实体可以由一个或多个标记组成，例如*旧金山*。因此，命名实体由`Span`对象表示。与名词短语类似，检索命名实体的列表以供进一步分析也很有帮助。'
- en: 'If you look again at [Table 4-3](#tab-nlp-result), you see the token attributes
    for named-entity recognition, `ent_type_` and `ent_iob_`. `ent_iob_` contains
    the information if a token begins an entity (`B`), is inside an entity (`I`),
    or is outside (`O`). Instead of iterating through the tokens, we can also access
    the named entities directly with `doc.ents`. Here, the property for the entity
    type is called `label_`. Let’s illustrate this with an example:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你再次查看[表 4-3](#tab-nlp-result)，你会看到用于命名实体识别的标记属性，`ent_type_`和`ent_iob_`。`ent_iob_`包含了一个标记是否开始一个实体（`B`）、是否在一个实体内部（`I`）或是否在外部（`O`）的信息。与遍历标记不同，我们还可以直接通过`doc.ents`访问命名实体。在这里，实体类型的属性被称为`label_`。让我们通过一个例子来说明这一点：
- en: '[PRE66]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: '`Out:`'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE67]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'spaCy’s `displacy` module also provides visualization for the named-entity
    recognition, which makes the result much more readable and visually supports the
    identification of misclassified entities:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy的`displacy`模块还提供命名实体识别的可视化，这大大增强了结果的可读性，并在视觉上支持误分类实体的识别：
- en: '[PRE68]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: '![](Images/btap_04in01.jpg)'
  id: totrans-315
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/btap_04in01.jpg)'
- en: The named entities were identified correctly as a person, an organization, and
    a geo-political entity (GPE). But be aware that the accuracy for named-entity
    recognition may not be very good if your corpus is missing a clear grammatical
    structure. Check out [“Named-Entity Recognition”](ch12.xhtml#ch12-ner) for a detailed
    discussion.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 命名实体被正确识别为一个人物、一个组织和一个地缘政治实体（GPE）。但请注意，如果您的语料库缺乏明确的语法结构，则命名实体识别的准确性可能不会很高。详细讨论请参阅[“命名实体识别”](ch12.xhtml#ch12-ner)。
- en: 'For the extraction of named entities of certain types, we again make use of
    one of textacy’s convenient functions:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 对于特定类型的命名实体提取，我们再次利用textacy的一个便利函数：
- en: '[PRE69]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'With this function we can, for example, retrieve the named entities of types
    `PERSON` and `GPE` (geo-political entity) like this:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，使用此函数我们可以检索`PERSON`和`GPE`（地缘政治实体）类型的命名实体：
- en: '[PRE70]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: '`Out:`'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE71]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: Feature Extraction on a Large Dataset
  id: totrans-323
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大型数据集上的特征提取
- en: Now that we know the tools spaCy provides, we can finally build our linguistic
    feature extractor. [Figure 4-3](#fig-feature-extraction) illustrates what we are
    going to do. In the end, we want to create a dataset that can be used as input
    to statistical analysis and various machine learning algorithms. Once extracted,
    we will persist the preprocessed data “ready to use” in a database.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们了解了spaCy提供的工具，我们最终可以构建我们的语言特征提取器了。[图4-3](#fig-feature-extraction)说明了我们要做的事情。最终，我们希望创建一个可用作统计分析和各种机器学习算法输入的数据集。一旦提取完成，我们将在数据库中持久化预处理好的“即用”数据。
- en: '![](Images/btap_0403.jpg)'
  id: totrans-325
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/btap_0403.jpg)'
- en: Figure 4-3\. Feature extraction from text with spaCy.
  id: totrans-326
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-3\. 使用spaCy从文本中提取特征。
- en: 'Blueprint: Creating One Function to Get It All'
  id: totrans-327
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 蓝图：创建一个函数来获取所有内容
- en: 'This blueprint function combines all the extraction functions from the previous
    section. It neatly puts everything we want to extract in one place in the code
    so that the subsequent steps do not need to be adjusted if you add or change something
    here:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 此蓝图函数将前面章节中的所有提取功能结合起来。它将我们想要提取的所有内容整齐地放在代码中的一个位置，这样如果您在此处添加或更改内容，后续步骤无需调整：
- en: '[PRE72]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'The function returns a dictionary with everything we want to extract, as shown
    in this example:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数返回一个包含我们想要提取的所有内容的字典，如本例所示：
- en: '[PRE73]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: '`Out:`'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE74]'
  id: totrans-333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'The list of returned column names is needed for the next steps. Instead of
    hard-coding it, we just call `extract_nlp` with an empty document to retrieve
    the list:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 返回的列名列表将在接下来的步骤中需要。我们不是硬编码它，而是简单地调用`extract_nlp`并传入一个空文档来检索列表：
- en: '[PRE75]'
  id: totrans-335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: '`Out:`'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE76]'
  id: totrans-337
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'Blueprint: Using spaCy on a Large Dataset'
  id: totrans-338
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 蓝图：在大型数据集上使用spaCy
- en: 'Now we can use this function to extract features from all the records of a
    dataset. We take the cleaned texts that we created and saved at the beginning
    of this chapter and add the titles:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以使用此函数从数据集的所有记录中提取特征。我们获取并添加在本章开头创建和保存的清理文本及其标题：
- en: '[PRE77]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'Before we start NLP processing, we initialize the new `DataFrame` columns we
    want to fill with values:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始自然语言处理处理之前，我们初始化要填充值的新`DataFrame`列：
- en: '[PRE78]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'spaCy’s neural models benefit from running on GPU. Thus, we try to load the
    model on the GPU before we start:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy的神经模型受益于在GPU上运行。因此，在开始之前，我们尝试在GPU上加载模型：
- en: '[PRE79]'
  id: totrans-344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'Now we have to decide which model and which of the pipeline components to use.
    Remember to disable unneccesary components to improve runtime! We stick to the
    small English model with the default pipeline and use our custom tokenizer that
    splits on hyphens:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要决定使用哪个模型和流水线组件。记住要禁用不必要的组件以提高运行时效率！我们坚持使用默认流水线的小型英语模型，并使用我们自定义的分词器，在连字符上进行分割：
- en: '[PRE80]'
  id: totrans-346
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: When processing larger datasets, it is recommended to use spaCy’s batch processing
    for a significant performance gain (roughly factor 2 on our dataset). The function
    `nlp.pipe` takes an iterable of texts, processes them internally as a batch, and
    yields a list of processed `Doc` objects in the same order as the input data.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理较大数据集时，建议使用spaCy的批处理来获得显著的性能提升（在我们的数据集上大约提升了2倍）。函数`nlp.pipe`接受一个文本的可迭代对象，在内部作为一个批次处理它们，并按照输入数据的顺序生成处理过的`Doc`对象列表。
- en: To use it, we first have to define a batch size. Then we can loop over the batches
    and call `nlp.pipe`.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用它，我们首先必须定义一个批处理大小。然后我们可以循环处理这些批次并调用`nlp.pipe`。
- en: '[PRE81]'
  id: totrans-349
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: In the inner loop we extract the features from the processed `doc` and write
    the values back into the `DataFrame`. The whole process takes about six to eight
    minutes on the dataset without using a GPU and about three to four minutes with
    the GPU on Colab.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 在内部循环中，我们从处理过的`doc`中提取特征，并将这些值写回到`DataFrame`中。在没有使用GPU的数据集上，整个过程大约需要六到八分钟，在Colab上使用GPU时大约需要三到四分钟。
- en: 'The newly created columns are perfectly suited for frequency analysis with
    the functions from [Chapter 1](ch01.xhtml#ch-exploration). Let’s check for the
    most frequently mentioned noun phrases in the autos category:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 新创建的列非常适合使用来自[第1章](ch01.xhtml#ch-exploration)的函数进行频率分析。让我们来检查汽车类别中提到最频繁的名词短语：
- en: '[PRE82]'
  id: totrans-352
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: '`Out:`'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '![](Images/btap_04in02.jpg)'
  id: totrans-354
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/btap_04in02.jpg)'
- en: Persisting the Result
  id: totrans-355
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 持久化结果
- en: 'Finally, we save the complete `DataFrame` to SQLite. To do so, we need to serialize
    the extracted lists to space-separated strings, as lists are not supported by
    most databases:'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将完整的`DataFrame`保存到SQLite中。为此，我们需要将提取的列表序列化为以空格分隔的字符串，因为大多数数据库不支持列表：
- en: '[PRE83]'
  id: totrans-357
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: The resulting table provides a solid and ready-to-use basis for further analyses.
    In fact, we will use this data again in [Chapter 10](ch10.xhtml#ch-embeddings)
    to train word embeddings on the extracted lemmas. Of course, the preprocessing
    steps depend on what you are going to do with the data. Working with sets of words
    like those produced by our blueprint is perfect for any kind of statistical analysis
    on word frequencies and machine learning based on a bag-of-words vectorization.
    You will need to adapt the steps for algorithms that rely on knowledge about word
    sequences.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表提供了一个坚实且可以直接使用的基础，用于进一步的分析。实际上，我们将在[第10章](ch10.xhtml#ch-embeddings)再次使用这些数据来训练从提取的词形中得到的词向量。当然，预处理步骤取决于您要对数据执行什么操作。像我们的蓝图生成的单词集合这样的工作非常适合进行基于词袋向量化的任何类型的统计分析和机器学习。您将需要根据依赖于单词序列知识的算法来调整这些步骤。
- en: A Note on Execution Time
  id: totrans-359
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关于执行时间的注意事项
- en: Complete linguistic processing is *really* time-consuming. In fact, processing
    just the 20,000 Reddit posts with spaCy takes several minutes. A simple regexp
    tokenizer, in contrast, takes only a few seconds to tokenize all records on the
    same machine. It’s the tagging, parsing, and named-entity recognition that’s expensive,
    even though spaCy is really fast compared to other libraries. So if you don’t
    need named entities, you should definitely disable the parser and name-entity
    recognition to save more than 60% of the runtime.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的语言处理确实*非常*耗时。事实上，仅处理20,000个Reddit帖子就需要几分钟的时间。相比其他库，虽然spaCy处理速度非常快，但标记、解析和命名实体识别却是代价昂贵的。因此，如果您不需要命名实体，您应该明确地禁用解析器和命名实体识别，以节省超过60%的运行时间。
- en: Processing the data in batches with `nlp.pipe` and using GPUs is one way to
    speed up data processing for spaCy. But data preparation in general is also a
    perfect candidate for parallelization. One option to parallelize tasks in Python
    is using the library [`multiprocessing`](https://oreil.ly/hoqxv). Especially for
    the parallelization of operations on dataframes, there are some scalable alternatives
    to Pandas worth checking, namely [Dask](https://dask.org), [Modin](https://oreil.ly/BPMLh),
    and [Vaex](https://oreil.ly/hb66b). [pandarallel](https://oreil.ly/-PCJa) is a
    library that adds parallel apply operators directly to Pandas.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`nlp.pipe`批处理数据并使用GPU是加快spaCy数据处理速度的一种方法。但是，一般来说，数据准备也是并行化的一个完美候选项。在Python中并行任务的一个选项是使用[`multiprocessing`](https://oreil.ly/hoqxv)库。特别是对于数据框操作的并行化，还有一些可伸缩的替代方案值得一试，即[Dask](https://dask.org)、[Modin](https://oreil.ly/BPMLh)和[Vaex](https://oreil.ly/hb66b)。[pandarallel](https://oreil.ly/-PCJa)是一个直接向Pandas添加并行应用运算符的库。
- en: In any case, it is helpful to watch the progress and get a runtime estimate.
    As already mentioned in [Chapter 1](ch01.xhtml#ch-exploration), the *tqdm* library
    is a great tool for that purpose because it provides [progress bars](https://oreil.ly/Rbh_-)
    for iterators and dataframe operations. Our notebooks on GitHub use tqdm whenever
    possible.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 无论如何，观察进展并获取运行时估计都是有帮助的。正如在[第1章](ch01.xhtml#ch-exploration)中已经提到的那样，*tqdm*库是一个非常好的工具，因为它为迭代器和数据框操作提供了[进度条](https://oreil.ly/Rbh_-)。我们的GitHub笔记本在可能的情况下都使用了tqdm。
- en: There Is More
  id: totrans-363
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多
- en: We started out with data cleaning and went through a whole pipeline of linguistic
    processing. Still, there some aspects that we didn’t cover in detail but that
    may be helpful or even necessary in your projects.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
- en: Language Detection
  id: totrans-365
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Many corpora contain text in different languages. Whenever you are working
    with a multilingual corpus, you have to decide on one of these options:'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
- en: Ignore other languages if they represent a negligible minority, and treat every
    text as if it were of the corpus’s major language, e.g., English.
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Translate all texts to the main language, for example, by using Google Translate.
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identify the language and do language-dependent preprocessing in the next steps.
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are good libraries for language detection. Our recommendation is Facebook’s
    [fastText library](https://oreil.ly/6QhAj). fastText provides a pretrained model
    that identifies 176 languages really fast and accurately. We provide an additional
    blueprint for language detection with fastText in the [GitHub repository](https://oreil.ly/c3dsK)
    for this chapter.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
- en: textacy’s `make_spacy_doc` function allows you to automatically load the respective
    language model for linguistic processing if available. By default, it uses a language
    detection model based on [Google’s Compact Language Detector v3](https://oreil.ly/mJLfx),
    but you could also hook in any language detection function (for example, fastText).
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
- en: Spell-Checking
  id: totrans-372
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: User-generated content suffers from a lot of misspellings. It would be great
    if a spell-checker could automatically correct these errors. [SymSpell](https://oreil.ly/puo2S)
    is a popular spell-checker with a [Python port](https://oreil.ly/yNs_k). However,
    as you know from your smartphone, automatic spelling correction may itself introduce
    funny artifacts. So, you should definitely check whether the quality really improves.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
- en: Token Normalization
  id: totrans-374
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Often, there are different spellings for identical terms or variations of terms
    that you want to treat and especially count identically. In this case, it is useful
    to normalize these terms and map them to a common standard. Here are some examples:'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
- en: U.S.A. or U.S. → USA
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: dot-com bubble → dotcom bubble
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: München → Munich
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You could use spaCy’s phrase matcher to integrate this kind of normalization
    as a post-processing step into its pipeline. If you don’t use spaCy, you can use
    a simple Python dictionary to map different spellings to their normalized forms.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
- en: Closing Remarks and Recommendations
  id: totrans-380
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: “Garbage in, garbage out” is a frequently cited problem in data projects. This
    is especially true for textual data, which is inherently noisy. Therefore, data
    cleaning is one of the most important tasks in any text analysis project. Spend
    enough effort to ensure high data quality and check it systematically. We have
    shown many solutions to identify and resolve quality issues in this section.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
- en: The second prerequisite for reliable analyses and robust models is normalization.
    Many machine learning algorithms for text are based on the bag-of-words model,
    which generates a notion of similarity between documents based on word frequencies.
    In general, you are better off with lemmatized text when you do text classification,
    topic modeling, or clustering based on TF-IDF. You should avoid or use only sparingly
    those kinds of normalization or stop word removal for more complex machine learning
    tasks such as text summarization, machine translation, or question answering where
    the model needs to reflect the variety of the language.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
- en: '^([1](ch04.xhtml#idm45634202714920-marker)) The Pandas operations `map` and
    `apply` were explained in [“Blueprint: Building a Simple Text Preprocessing Pipeline”](ch01.xhtml#ch1-pipeline).'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch04.xhtml#idm45634202565272-marker)) Libraries specialized in HTML data
    cleaning such as Beautiful Soup were introduced in [Chapter 3](ch03.xhtml#ch-scraping).
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](ch04.xhtml#idm45634201737000-marker)) The asterisk operator (*) unpacks
    the list into separate arguments for `print`.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
- en: ^([4](ch04.xhtml#idm45634201564856-marker)) For example, check out [NLTK’s tweet
    tokenizer](https://oreil.ly/R45_t) for regular expressions for text emoticons
    and URLs, or see textacy’s [compile regexes](https://oreil.ly/i0HhJ).
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
- en: ^([5](ch04.xhtml#idm45634201561544-marker)) A good overview is [“The Art of
    Tokenization” by Craig Trim](https://oreil.ly/LyGvt).
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
- en: ^([6](ch04.xhtml#idm45634201500904-marker)) See [spaCy’s website](https://oreil.ly/spaCy)
    for a list of available models.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
- en: ^([7](ch04.xhtml#idm45634201354856-marker)) See [spaCy’s API](https://oreil.ly/cvNhV)
    for a complete list.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
- en: ^([8](ch04.xhtml#idm45634201147656-marker)) See [spaCy’s API](https://oreil.ly/EpmEI)
    for a complete list of attributes.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
- en: ^([9](ch04.xhtml#idm45634201023736-marker)) See [spaCy’s tokenization usage
    docs](https://oreil.ly/HMWja) for details and an illustrative example.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
- en: ^([10](ch04.xhtml#idm45634201018552-marker)) See [spaCy’s tokenizer usage docs](https://oreil.ly/45yU4)
    for details.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
- en: ^([11](ch04.xhtml#idm45634200775880-marker)) Modifying the stop word list this
    way will probably become deprecated with spaCy 3.0\. Instead, it is recommended
    to create a modified subclass of the respective language class. See the [GitHub
    notebook](https://oreil.ly/CV2Cz) for this chapter for details.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
- en: ^([12](ch04.xhtml#idm45634200544984-marker)) See [Universal Part-of-speech tags](https://oreil.ly/lAKtm)
    for more.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
