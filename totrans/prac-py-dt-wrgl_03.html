<html><head></head><body><section data-pdf-bookmark="Chapter 3. Understanding Data Quality" data-type="chapter" epub:type="chapter"><div class="chapter" id="chapter3">&#13;
<h1><span class="label">Chapter 3. </span>Understanding Data Quality</h1>&#13;
&#13;
&#13;
<p>Data is everywhere. It’s automatically generated by our mobile devices, our shopping activities, and our physical movements. It’s captured by our electric meters, public transportation systems, and communications infrastructure. And it’s used to estimate our health outcomes, our earning potential, and our credit worthiness.<sup><a data-type="noteref" href="ch03.html#idm45143427340352" id="idm45143427340352-marker">1</a></sup> Economists have even declared that data is the “new oil,”<sup><a data-type="noteref" href="ch03.html#idm45143427337760" id="idm45143427337760-marker">2</a></sup> given its potential to transform so many aspects of human life.</p>&#13;
&#13;
<p>While data <a data-primary="data quality" data-secondary="importance of" data-type="indexterm" id="idm45143427335648"/>may be plentiful, however, the truth is that <em>good</em> data is scarce. The claim of “the data revolution” is that, with enough data, we can better understand the present and improve—or even predict—the future. For any of that to even be possible, however, the data underlying those insights has to be high quality. Without good-quality data, all of our efforts to wrangle, analyze, visualize, and communicate it will, at best, leave us with no more insight about the world than when we started. While that would be an unfortunate waste of effort, the consequences of failing to recognize that we have poor-quality data is even worse, because it can lead us to develop a seemingly rational but dangerously distorted view of reality. What’s more, because data-driven systems are used to make decisions at scale, the harms caused by even a small amount of bad data can be significant. Sure, data about hundreds or even thousands of people may be used to “train” a machine learning model. But if that data is not representative of the population to which the model will be applied, the repercussions of that system can affect hundreds or thousands of times the number of people in the original dataset. Because the stakes are so high, ensuring data quality is an essential part of data wrangling. But what does it mean for data to be “high quality”? My view is that data is high quality only if it is both <em>fit</em> for purpose and has high internal <em>integrity</em>.</p>&#13;
&#13;
<p>What does each of those terms actually mean? That is exactly what we’ll explore, in depth, in this chapter. We’ll begin by discussing the concept of data <em>fit</em>, which relates to the appropriateness of data for use in a particular context, or to answer a particular question. We’ll then break down the many aspects of data <em>integrity</em>: the characteristics of a dataset that influence both its fitness for purpose and the types of analyses we can responsibly use it for. Finally, we’ll discuss some tools and strategies for finding and working with data that can help you maximize its overall quality, lending confidence and credibility to the work that you produce with it.</p>&#13;
&#13;
<p>In case you start to find any of this tedious, let me repeat my exhortations from <a data-type="xref" href="ch01.html#describing_data_wrangling">“What Is “Data Wrangling”?”</a>: trying to “skip over” the work of assessing data quality can only undermine your data wrangling efforts. At best, you’ll go to share your work and encounter questions about your process that you don’t have answers for. At worst, you’ll end up promoting “insights” that are both wrong and do active harm. Along the way, you’ll <em>also</em> be cheating yourself out of good technical skills, because solving data quality problems is where you’ll expand your programming knowledge the most. If you truly want to be good at the work of data wrangling, assessing data quality <em>has</em> to be part of your practice.</p>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45143427327456">&#13;
<h5>Don’t Forget to Document</h5>&#13;
<p>While<a data-primary="documentation" data-secondary="in data quality assessments" data-secondary-sortas="data quality assessments" data-type="indexterm" id="documentation-data-quality"/><a data-primary="data quality" data-secondary="documentation of" data-type="indexterm" id="data-quality-documentation"/> thoroughly commenting your code is a sound time investment if you want to improve your coding skills <em>and</em> get more value out of that code in the future, with enough time and effort it’s almost always possible to retranslate computer code later on if you need to.</p>&#13;
&#13;
<p>The same is <em>not</em> true for your work on data quality: the documentation in your data diary is actually irreplaceable. The conclusion that you reach about a dataset being representative or valid will, in most cases, be informed by everything from your own reading and research to conversations with experts to additional datasets you’ve located. But without good documentation of who said what or how you came across the information, any attempt to repeat or confirm your previous work will almost certainly fail.</p>&#13;
&#13;
<p>Why? Because information sources—especially on the internet<sup><a data-type="noteref" href="ch03.html#idm45143427320640" id="idm45143427320640-marker">3</a></sup>—move, change, and disappear. Even in the course of just a few months, many of the links I originally included in this book moved or stopped working. The expert you spoke to six months ago may no longer be available, or the search you conducted may return different results. While the way you approach it is up to you, I cannot state emphatically enough how important it is to document your data wrangling work, especially as it pertains to data quality (which is pretty much all of it). Without a detailed description of your process, you may find you’re on uncertain footing when it’s time to share the results of your work—forcing you to start from the beginning all over <a data-primary="documentation" data-secondary="in data quality assessments" data-secondary-sortas="data quality assessments" data-startref="documentation-data-quality" data-type="indexterm" id="idm45143427318160"/><a data-primary="data quality" data-secondary="documentation of" data-startref="data-quality-documentation" data-type="indexterm" id="idm45143427316624"/>again.</p>&#13;
</div></aside>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Assessing Data Fit" data-type="sect1"><div class="sect1" id="idm45143427315008">&#13;
<h1>Assessing Data Fit</h1>&#13;
&#13;
<p>Perhaps<a data-primary="data quality" data-secondary="data fit assessments" data-type="indexterm" id="data-quality-fit-assessment"/><a data-primary="data fit" data-secondary="assessing" data-type="indexterm" id="data-fit-assessment"/><a data-primary="assessing" data-secondary="data fit" data-type="indexterm" id="assessing-data-fit"/><a data-primary="fit" data-see="data fit" data-type="indexterm" id="idm45143427309536"/> one of the most common misconceptions about data wrangling is that it is a predominantly <em>quantitative</em> process, that is, that data wrangling is mostly about working with numbers, formulas, and code. In fact, irrespective of the type of data you’re dealing with—it could be anything from temperature readings to social media posts—the core work of data wrangling involves making judgment calls: from whether your data accurately represents the phenomenon you’re investigating, to what to do about missing data points and whether you have enough data to generate any real insight at all. That first concept—the extent to which a given dataset accurately represents the phenomenon you’re investigating—is broadly what I mean by its <em>fit</em>, and assessing your dataset’s <em>fit</em>ness for purpose is much more about applying informed judgment than it is about applying mathematical formulas. The reason for this is quite simple: the world is a messy place, and what may seem like even the simplest data about it is always filtered through some kind of human lens. Take something as straightforward as measuring the temperature in your workspace over the course of a week. In theory, all you need to do is get a thermometer, put it in the space, and note down the reading every day. Done, right?</p>&#13;
&#13;
<p>Or are you? Let’s start with your equipment. Did you use a digital thermometer or a mercury thermometer? Where in the space did you place it? Is it near a door, a window, or a heating or cooling source? Did you take the reading at the same time every day? Is the thermometer ever in direct sunlight? What is the typical humidity level?</p>&#13;
&#13;
<p>You may think I’m introducing a contrived level of complexity here, but if you’ve ever lived in a shared space (like an apartment building), you’ve probably been through the experience of <em>feeling</em> like it’s much warmer or colder than what some thermometer said. Likewise, if you’ve ever looked after a child who’s ill, you’re likely all too familiar with the different body temperature readings that you’ll get with different types of thermometers—or even with the same one, just minutes apart.</p>&#13;
&#13;
<p>In other words, there are a huge number of factors contributing to that two- or three-digit temperature you record—and the number itself doesn’t provide information about any of them. That’s why when you begin the process of trying to answer a question with data, it’s not enough to know just what is in the dataset; you need to know about the processes and mechanisms used to <em>collect</em> it. Then, given everything you know about how the data was gathered, you need to determine if it can really be used to answer your specific question in a meaningful way.</p>&#13;
&#13;
<p>Of course, this problem is neither new nor unique; it’s the same challenge that all scientific fields face in their efforts to discover new information about the world. Cancer research could hardly advance if every single researcher had to conduct every single study themselves; without the ability to build on the work of others, scientific and technological progress would grind to a halt (if not go off the rails entirely). Because of this, over time the scientific community has developed three key metrics for determining the appropriateness or fit of a dataset for answering a given question: <em>validity</em>, <em>reliability</em>, and <em>representativeness</em>.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Validity" data-type="sect2"><div class="sect2" id="validity">&#13;
<h2>Validity</h2>&#13;
&#13;
<p>At its <a data-primary="data quality" data-secondary="data fit assessments" data-tertiary="validity" data-type="indexterm" id="data-quality-fit-assessment-validity"/><a data-primary="data fit" data-secondary="assessing" data-tertiary="validity" data-type="indexterm" id="data-fit-assessment-validity"/><a data-primary="assessing" data-secondary="data fit" data-tertiary="validity" data-type="indexterm" id="assessing-data-fit-validity"/><a data-primary="validity, in data fit" data-type="indexterm" id="validity-data-fit"/>most basic, <em>validity</em> describes the extent to which something measures what it is supposed to. In our room temperature example, this would mean ensuring that the type of thermometer you’ve chosen will actually measure the air temperature rather than something else. For example, while traditional liquid-in-glass thermometers will probably capture air temperature well, infrared thermometers will tend to capture the temperature of whatever surface they’re pointed at. So even with something as seemingly basic as room temperature, you need to understand the tools and methods used to collect your data readings in order to ensure their <em>validity</em> with respect to your question.</p>&#13;
&#13;
<p>Unsurprisingly, things only get more involved when we’re not collecting data about common physical phenomena. <em>Construct validity</em> describes<a data-primary="construct validity" data-type="indexterm" id="construct-validity"/> the extent to which your data measurements effectively capture the (usually abstract) <em>construct</em>, or idea, you’re trying to understand. For example, let’s say you want to know which are the “best” schools in your area. What data can help you answer that question? First we have &#13;
<span class="keep-together">to recognize</span> that the term <em>best</em> is imprecise. Best in what way? Are you interested &#13;
<span class="keep-together">in which</span> school has the highest graduation rate? Standardized test scores? &#13;
<span class="keep-together">School-assigned</span> grades? Teacher evaluations? Student satisfaction? Extracurricular &#13;
<span class="keep-together">participation</span>?</p>&#13;
&#13;
<p>In order to use data to begin to answer this question, you first need to articulate two things. First, “best” <em>for whom</em>? Are you trying to answer this question for your own child? A friend’s? Having answered that, you’ll be better able to complete the second task, which is <em>operationalizing</em> your specific idea of “best.” If your friend’s child &#13;
<span class="keep-together">loves sports</span>, for example, extracurricular activities might be more important than &#13;
<span class="keep-together">academics</span>.</p>&#13;
&#13;
<p>In data analysis, this process of selecting measures<a data-primary="operationalizing a construct" data-type="indexterm" id="idm45143427281200"/> is known as <em>operationalizing a construct</em>, and it inevitably requires choosing among—and balancing—proxies for the idea or concept you are trying to understand. These proxies—like graduation rates, test scores, extracurricular activities, and so on—are things <em>about which you can</em> &#13;
<span class="keep-together"><em>collect</em></span> <em>data</em> that you are choosing to use to represent an abstract concept (“best” school) <em>that cannot be measured directly</em>. Good-quality data, to say the least, must have good <em>construct validity</em> with respect to your question, otherwise your data wrangling results will be meaningless.</p>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="how_for_whom">&#13;
<h5>How? And for Whom?</h5>&#13;
<p>Once you begin thinking about the <em>construct validity</em> of your own data wrangling questions, you’ll likely find yourself much more curious about it anywhere you encounter data being used to make decisions or “predictions” about the world. More than likely, you’ll find many situations where imprecise claims (like “best”) are being made, with little or no explanation being offered about <em>how</em> the concept of “best” was defined. Similarly, the designers of data-driven systems may initially answer the question of “for whom?” with an enthusiastic “for everyone!” The real answer lies in their choice of proxies, which they may well have selected heuristically based only on their own tastes or preferences. In those instances, the <em>real</em> answer to the question “for whom?” is “for people like me.”</p>&#13;
&#13;
<p>Of course, sometimes you may be told that there’s simply no way to know how “best” is being defined, because the system being used to make the decisions or predictions is a so-called “black box”—which is how unsupervised machine learning systems are often described. As I mentioned in <a data-type="xref" href="ch01.html#what_is_data_quality">“What Is “Data Quality”?”</a>, however, this is somewhat misleading. Though it’s true that we can’t currently say with confidence precisely how such a machine learning system has weighted or prioritized certain things when making predictions, we <em>do</em> know that it will <em>always</em> replicate and amplify the patterns that exist within the data on which it was “trained.” In those instances, then, it’s the composition of the training data that will tell you both “how?” and “for whom?”—if the people who made the system are confident enough to <a data-primary="construct validity" data-startref="construct-validity" data-type="indexterm" id="idm45143427270464"/>&#13;
<span class="keep-together">share it.</span></p>&#13;
</div></aside>&#13;
&#13;
<p>The other type of validity that is important for data fit<a data-primary="content validity" data-type="indexterm" id="idm45143427268032"/> is <em>content validity</em>. This type of validity has to do with how complete your data is for a given proxy measurement. In the “best” school example, let’s say you have determined that grades are relevant for determining what school is best, but you only have grades for history and physical education courses available. Though for many people grade data might, in principle, have <em>construct validity</em> for identifying the best school, having grade data for only two types of courses wouldn’t be sufficient to satisfy the requirement for <em>content validity</em>—and for high-quality data, you need to have <em>both</em>.</p>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45143427264848">&#13;
<h5>“Let’s See What the Data Says”</h5>&#13;
<p>Data-driven decision making can be a powerful tool for understanding what’s happening in the world. But sometimes it is attractive because it appears to transcend (or at least avoid) the need to wade through difficult discussions about norms, values, and ethics.</p>&#13;
&#13;
<p>In reality, though, relying on “the data” this way just means that you are deferring to the values, assumptions, and biases of the data’s creators—whoever designed the data collection and carried it out. This is why data itself can never be objective—only the data wrangling processes we use to <a data-primary="data quality" data-secondary="data fit assessments" data-startref="data-quality-fit-assessment-validity" data-tertiary="validity" data-type="indexterm" id="idm45143427262448"/><a data-primary="data fit" data-secondary="assessing" data-startref="data-fit-assessment-validity" data-tertiary="validity" data-type="indexterm" id="idm45143427260864"/><a data-primary="assessing" data-secondary="data fit" data-startref="assessing-data-fit-validity" data-tertiary="validity" data-type="indexterm" id="idm45143427259360"/><a data-primary="validity, in data fit" data-startref="validity-data-fit" data-type="indexterm" id="idm45143427257856"/>handle it.</p>&#13;
</div></aside>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Reliability" data-type="sect2"><div class="sect2" id="idm45143427256528">&#13;
<h2>Reliability</h2>&#13;
&#13;
<p>Within<a data-primary="data quality" data-secondary="data fit assessments" data-tertiary="reliability" data-type="indexterm" id="idm45143427254976"/><a data-primary="data fit" data-secondary="assessing" data-tertiary="reliability" data-type="indexterm" id="idm45143427253696"/><a data-primary="assessing" data-secondary="data fit" data-tertiary="reliability" data-type="indexterm" id="idm45143427252480"/><a data-primary="reliability, in data fit" data-type="indexterm" id="idm45143427251264"/> a dataset, the <em>reliability</em> of a given measure describes its <em>accuracy</em> and <em>stability</em>. Together, these help us assess whether the same measure taken twice in the same circumstances will give us the same—or at least very similar—results. To revisit our temperature example: taking a child’s temperature with an oral thermometer is not likely to be very <em>reliable</em>, because the process requires that the child keep their mouth closed for a relatively long time (which, in my experience, they’re not great at). By contrast, taking a child’s temperature under their arm might be more reliable—because you can hug them to keep the thermometer in place—but it may not provide as <em>accurate</em> a reading of the child’s true internal body temperature as some other methods. This is why most medical advice lists different temperature thresholds for a fever in children, depending on which method you use to take their temperature.</p>&#13;
&#13;
<p>With abstract concepts and real-world data, determining the reliability of a data measure is especially tricky, because it is never really possible to collect the &#13;
<span class="keep-together">data more</span> than once—whether because the cost is prohibitive, the circumstances can’t be replicated, or both. In those cases, we typically estimate reliability by comparing one similar group to another, using either previously or newly collected data. So even though a dramatic fluctuation in a school’s standardized test scores from one year to the next indicates that those scores may not be a <em>reliable</em> measure of school quality, this inconsistency itself is only part of the story. After all, those test scores <em>might</em> reflect the quality of teaching, but they might also reflect a change in the test being administered, how it was scored, or some other disruption to the learning or test-taking environment. In order to determine whether the standardized test data is <em>reliable</em> enough to be part of your “best” school assessment, you would need to look at comparison data from other years or other schools, in addition to learning more about the broader circumstances that may have led to the fluctuation. In the end, you may conclude that <em>most</em> of the test score information is reliable enough to be included but that a few particular data points should be removed, or you may conclude that the data is too unreliable to be part of a high-quality data process.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Representativeness" data-type="sect2"><div class="sect2" id="idm45143427242992">&#13;
<h2>Representativeness</h2>&#13;
&#13;
<p>The<a data-primary="data quality" data-secondary="data fit assessments" data-tertiary="representativeness" data-type="indexterm" id="data-quality-fit-assessment-represent"/><a data-primary="data fit" data-secondary="assessing" data-tertiary="representativeness" data-type="indexterm" id="data-fit-assessment-represent"/><a data-primary="assessing" data-secondary="data fit" data-tertiary="representativeness" data-type="indexterm" id="assessing-data-fit-represent"/><a data-primary="representativeness, in data fit" data-type="indexterm" id="represent-data-fit"/> key value proposition for data-driven systems is that they allow us to generate insights—or even predictions—about people and phenomena that are too massive or too complex for humans to reason about effectively. By wrangling and analyzing data, the logic goes, we can make decisions faster and more fairly. Given the powerful computational tools that even individuals—to say nothing of companies—have access to these days, there’s no doubt that data-driven systems can generate “decisions” more quickly than humans can. Whether those insights are an accurate portrait of a particular population or situation, however, depends directly on the <em>representativeness</em> of the data being used.</p>&#13;
&#13;
<p>Whether a dataset is sufficiently representative depends on a few things, the most significant of which goes back to the “for whom?” question we discussed in <a data-type="xref" href="#validity">“Validity”</a>. If you’re trying to design a new course schedule for a specific grade school, you may be able to collect data about its entire population. If all the other criteria for data fitness have been met, then you already know that your data is representative, because you have collected data directly from or about the entire population to which it will apply.</p>&#13;
&#13;
<p>But what if you’re trying to complete the same task for an entire city’s worth of schools? It’s deeply unlikely that you’ll succeed in collecting data about every single student in every single school, which means that you’ll be relying on input from only a subset of the students when you try to design the new schedule.</p>&#13;
&#13;
<p>Anytime you’re working with a subset or <em>sample</em> in this way, it’s crucial to make sure that it is <em>representative</em> of the broader population to which you plan to apply your findings. While proper sampling methodology is beyond the scope of this book,<sup><a data-type="noteref" href="ch03.html#idm45143427230496" id="idm45143427230496-marker">4</a></sup> the basic idea is that in order for your insights to accurately generalize to a particular community of people, the data sample you use must proportionally reflect that community’s makeup. That means that you need to invest the time and resources to understand a number of things about that community <em>as a whole</em> before you can even know if your sample is representative.</p>&#13;
&#13;
<p>At this point you may be thinking: wait, if we could already get information about the whole population, we wouldn’t need a sample in the first place! And that’s true—sort of. In many cases, it’s possible to get <em>some</em> information about an entire community—just not precisely the information we need. In our school-scheduling scenario, for example, we would ideally get information about how—and how long—students travel to and from school each day, as well as some sense of their caretakers’ schedules. But the information we might <em>have</em> about the entire school population (if we are working cooperatively with the school system) will probably include only things like home address, school address, and perhaps the <a href="https://schools.nyc.gov/school-life/transportation/bus-eligibility">type of transportation support</a>. Using this information, likely in conjunction with some additional administrative information, we could begin to create estimates for the proportion of certain types of student commuters that exist in the <em>entire</em> population, and then seek to replicate those proportions in selecting a <em>representative sample</em> from our survey results. Only at that point would we be ready to move on to the next step of the data wrangling process.</p>&#13;
&#13;
<p>As you can see, ensuring representativeness demands that we carefully consider which characteristics of a population are relevant to our data wrangling question <em>and</em> that we seek out enough additional information to ensure that our dataset proportionally represents those characteristics. Perhaps unsurprisingly, this is the data fitness test that many data-driven goods and services fail on, again and again. While companies and researchers may tout the <em>quantity</em> of data they use to develop in their systems, the reality is that most datasets that are readily available to companies and researchers tend to <em>not</em> be representative of, say, the US or global population. For example, data about search engine trends, social media activity, public transit usage, or smartphone ownership, for example, are all extremely <em>unlikely</em> to be representative of the broader population, since they are inevitably influenced by things like internet access and income level. This means that communities are <em>over</em>represented in these datasets while others are (sometimes severely) <em>under</em>represented. The result is systems that don’t <em>generalize</em>—like facial recognition systems <a href="http://proceedings.mlr.press/v81/buolamwini18a/buolamwini18a.pdf">that cannot “see” Black faces</a>.</p>&#13;
&#13;
<p>If you are faced with nonrepresentative data, what do you do? At the very least, you will need to revise (and clearly communicate) your “for whom?” assessment to reflect whatever population it <em>does</em> represent; this is the only community for whom your data wrangling insights will be valid. Also keep in mind that representativeness can only ensure that the outcome of your data wrangling efforts accurately reflects reality; it is not a value judgment on whether that reality should be <em>perpetuated</em>. If the outcome of your data wrangling effort will be used to make changes to a system or organization, the end of your data wrangling process is really just the starting point for thinking about what should be done with your insights, especially with respect to complex issues like <a href="https://youtube.com/watch?v=jIXIuYdnyyk">fairness</a>.</p>&#13;
&#13;
<p>This is true even if you have data about an entire population. For example, if you want to know which organizations have received a certain type of grant, then the “population” or community you’re interested in is just those grant recipients. At the same time, checking your data for representativeness against the population at large still has value: if one or more communities are over- or underrepresented in your population of grant recipients, it may hint at hidden factors influencing who receives that money.</p>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45143427215008">&#13;
<h5>The Problem of Context Collapse</h5>&#13;
<p>A huge <a data-primary="context collapse" data-type="indexterm" id="idm45143427213488"/>source of “bad” data and data analyses can be traced to a kind of <a href="https://zephoria.org/thoughts/archives/2013/12/08/coining-context-collapse.html"><em>context collapse</em></a>, in which data generated for one purpose is used—typically without much critical reflection—for another. For example, online advertising networks often use our web browsing or buying history to determine what ads to display to us or products to recommend. But in doing so, these systems frequently conflate what we <em>have</em> bought with what we <em>will</em> buy, and the results are often pointedly tone deaf, as illustrated in the following <a href="https://twitter.com/GirlFromBlupo/status/982156453396996096">tweet</a>:</p>&#13;
&#13;
<figure class="informal-width-80"><div class="figure">&#13;
<img alt="Jacqui Rayner via Twitter: Dear Amazon, I bought a toilet seat because I needed one. Necessity, not desire. I do not collect them. I am not a toilet seat addict. No matter how temptingly you email me, I'm not going to think, oh go on then, just one more toilet seat, I'll treat myself." src="assets/ppdw_in0301.png"/>&#13;
<h6/>&#13;
</div></figure>&#13;
&#13;
<p>When data generated in one context (browsing) is used to make decisions about another (advertising), the results can become nonsensical to the point of absurdity. And while situations like this are among the familiar oddities of online life, as we saw in <a data-type="xref" href="ch01.html#compas">“Unpacking COMPAS”</a>, similar instances of context collapse can—and do—cause serious<a data-primary="data quality" data-secondary="data fit assessments" data-startref="data-quality-fit-assessment" data-type="indexterm" id="idm45143427206048"/><a data-primary="data fit" data-secondary="assessing" data-startref="data-fit-assessment" data-type="indexterm" id="idm45143427204784"/><a data-primary="assessing" data-secondary="data fit" data-startref="assessing-data-fit" data-type="indexterm" id="idm45143427203568"/><a data-primary="data quality" data-secondary="data fit assessments" data-startref="data-quality-fit-assessment-represent" data-tertiary="representativeness" data-type="indexterm" id="idm45143427202352"/><a data-primary="data fit" data-secondary="assessing" data-startref="data-fit-assessment-represent" data-tertiary="representativeness" data-type="indexterm" id="idm45143427200848"/><a data-primary="assessing" data-secondary="data fit" data-startref="assessing-data-fit-represent" data-tertiary="representativeness" data-type="indexterm" id="idm45143427199344"/><a data-primary="representativeness, in data fit" data-startref="represent-data-fit" data-type="indexterm" id="idm45143427197840"/> harm.</p>&#13;
</div></aside>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Assessing Data Integrity" data-type="sect1"><div class="sect1" id="idm45143427242368">&#13;
<h1>Assessing Data Integrity</h1>&#13;
&#13;
<p>Data <em>fit</em> is<a data-primary="data quality" data-secondary="data integrity assessments" data-type="indexterm" id="data-quality-integrity-assessment"/><a data-primary="data integrity" data-secondary="assessing" data-type="indexterm" id="data-integrity-assessment"/><a data-primary="assessing" data-secondary="data integrity" data-type="indexterm" id="assessing-data-integrity"/><a data-primary="integrity" data-see="data integrity" data-type="indexterm" id="idm45143427190784"/> essentially about whether you have the right data for answering your data wrangling question. Data <em>integrity</em>, on the other hand, is largely about whether the data you have can support the analyses you’ll need to perform in order to answer that question.&#13;
As you’ll see throughout this section,&#13;
there are a <em>lot</em> of different aspects of the data to consider when performing an integrity assessment. But does a given dataset need to have <em>all</em> of them in order to be high integrity, and therefore high quality? Not necessarily. While some are essential, the importance of others depends on your specific question and the methods you’ll need to answer it. And with rare exceptions, many are characteristics of the data that you will enhance and develop as part of your data wrangling process.</p>&#13;
&#13;
<p>In other words, while ensuring data <em>fit</em> is nonoptional, the types and degree of <em>integrity</em> that your particular data wrangling project requires will vary. Of course, the more of these requirements your data meets, the more useful it will be—both to you and others.</p>&#13;
&#13;
<p>In general, a high-integrity dataset will, to one degree or another, be:<sup><a data-type="noteref" href="ch03.html#idm45143427185920" id="idm45143427185920-marker">5</a></sup></p>&#13;
<dl>&#13;
<dt>Necessary, but not sufficient</dt>&#13;
<dd>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Of known provenance</p>&#13;
</li>&#13;
<li>&#13;
<p>Well-annotated</p>&#13;
</li>&#13;
</ul>&#13;
</dd>&#13;
<dt>Important</dt>&#13;
<dd>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Timely</p>&#13;
</li>&#13;
<li>&#13;
<p>Complete</p>&#13;
</li>&#13;
<li>&#13;
<p>High volume</p>&#13;
</li>&#13;
<li>&#13;
<p>Multivariate</p>&#13;
</li>&#13;
<li>&#13;
<p>Atomic</p>&#13;
</li>&#13;
</ul>&#13;
</dd>&#13;
<dt>Achievable</dt>&#13;
<dd>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Consistent</p>&#13;
</li>&#13;
<li>&#13;
<p>Clear</p>&#13;
</li>&#13;
<li>&#13;
<p>Dimensionally structured</p>&#13;
</li>&#13;
</ul>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>As I have already mentioned, however, not all of these data integrity characteristics are equally important. Some of them are nonnegotiable, while others are almost always the <em>result</em> of certain steps in the data wrangling process, not precursors to it. And while the goal is for your data to have as many of these characteristics as possible before you begin your analysis, there is always a balance to be struck between more fully elaborating your data and completing your work in time for your insights to be useful.</p>&#13;
&#13;
<p>In this sense, assessing data integrity can be a useful way to prioritize your data wrangling efforts. For example, if a dataset is missing either of the “necessary, but not sufficient” characteristics, you may as well move on from it entirely. If it’s missing one or two of the characteristics in <a data-type="xref" href="#important">“Important”</a>, it may still be possible to salvage the data you’re working with by combining it with others or limiting the scope of your analyses—and claims. Meanwhile, developing the characteristics in <a data-type="xref" href="#achievable">“Achievable”</a> is often the <em>goal</em> of your data wrangling process’s “cleaning” step, rather than something you can expect most real-world data to have when you first encounter it.</p>&#13;
&#13;
<p>In the end, the degree to which your dataset embodies many of the characteristics that follow will depend on the time you have to invest in your data wrangling project, but without a good solid majority of them, your insights will be limited. As will be illustrated (in detail!) in <a data-type="xref" href="ch06.html#chapter6">Chapter 6</a>, this variability is precisely why data wrangling and data quality are so thoroughly intertwined.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Necessary, but Not Sufficient" data-type="sect2"><div class="sect2" id="necessary">&#13;
<h2>Necessary, but Not Sufficient</h2>&#13;
&#13;
<p>Most of the time, the data we’re wrangling was compiled by someone else—or a whole collection of people and processes that we don’t have direct access to. At the same time, we need to be able to stand behind both our data wrangling process and any insights we derive from it. This means that there are a couple of data characteristics that are really <em>essential</em> to a successful data wrangling process.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Of known provenance" data-type="sect3"><div class="sect3" id="idm45143427161968">&#13;
<h3>Of known provenance</h3>&#13;
&#13;
<p>As<a data-primary="data quality" data-secondary="data integrity assessments" data-tertiary="of known provenance" data-tertiary-sortas="known provenance" data-type="indexterm" id="idm45143427160416"/><a data-primary="data integrity" data-secondary="assessing" data-tertiary="of known provenance" data-tertiary-sortas="known provenance" data-type="indexterm" id="idm45143427158848"/><a data-primary="assessing" data-secondary="data integrity" data-tertiary="of known provenance" data-tertiary-sortas="known provenance" data-type="indexterm" id="idm45143427157360"/><a data-primary="known provenance, in data integrity" data-type="indexterm" id="idm45143427155872"/> discussed in <a data-type="xref" href="ch01.html#what_is_data_quality">“What Is “Data Quality”?”</a>, data is the output of human decisions about what to measure and how. This means that using a dataset collected by others requires putting a significant amount of trust in them, especially because independently verifying <em>every single data point</em> is rarely possible; if it were, you would probably just collect your own data instead. This is why knowing the <em>provenance</em> of a dataset is so important: if you don’t know who compiled the data, the methods that they used, and/or the purpose for which they collected it, you will have a very hard time judging whether it is <em>fit</em> for your data wrangling purpose, or how to correctly interpret it.</p>&#13;
&#13;
<p>Of course, this doesn’t mean you need to know the birthdays and favorite colors of everyone who helped build a given dataset. But you <em>should</em> try to find out enough about their professional backgrounds, motivations for collecting the data (is it legally mandated, for example?), and the methods they employed so that you have some sense of which measures you’ll want to corroborate versus those that might be okay to take at face value. Ideally, both information about the data authors and sufficient documentation about these processes&#13;
will be readily enough available that you can answer all of these questions about the data’s <em>provenance</em> fairly quickly. If they prove hard to locate, however, you may wish to move on; since you need this information to assess data <em>fit</em>, your time may be better spent looking for a different dataset or even collecting the data yourself.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Well-annotated" data-type="sect3"><div class="sect3" id="well_annotated">&#13;
<h3>Well-annotated</h3>&#13;
&#13;
<p>A <a data-primary="data quality" data-secondary="data integrity assessments" data-tertiary="well-annotated" data-type="indexterm" id="idm45143427147840"/><a data-primary="data integrity" data-secondary="assessing" data-tertiary="well-annotated" data-type="indexterm" id="idm45143427146592"/><a data-primary="assessing" data-secondary="data integrity" data-tertiary="well-annotated" data-type="indexterm" id="idm45143427145376"/><a data-primary="well-annotated, in data integrity" data-type="indexterm" id="idm45143427144160"/>well-annotated dataset has enough surrounding information, or <em>metadata</em>, <a data-primary="metadata" data-secondary="in data integrity assessments" data-secondary-sortas="data integrity" data-type="indexterm" id="idm45143427142880"/>to make interpretation possible. This will include everything from high-level explanations of the data collection methodology to the “data dictionaries” that describe each data measure right down to its units. While this may seem straightforward, there are not always well-accepted standards for how such annotation information should be provided: sometimes it appears directly in data files or data entries themselves, and sometimes the information might be contained in separate files or documents in a location totally separate from where you retrieved the data.</p>&#13;
&#13;
<p>However they are structured or provided, robust data annotation documents are an essential component of high-integrity data because without them, it’s impossible to apply any analyses or draw any inferences from it. For example, imagine trying to interpret a budget without knowing if the figures provided refer to dollars, thousands of dollars, or millions of dollars—it’s clearly impossible. Or for a very American expression of the importance of annotation documents like data dictionaries, know that they sometimes <a href="https://trac.syr.edu/foia/ice/20210805">require a lawsuit</a> to get.</p>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="usda_data">&#13;
<h5>USDA Data Sleuthing</h5>&#13;
<p>The <a data-primary="Agricultural Marketing Service (USDA)" data-type="indexterm" id="idm45143427137472"/><a data-primary="USDA Agricultural Marketing Service" data-type="indexterm" id="idm45143427136720"/>USDA’s Agricultural Marketing Service <a href="https://www.ams.usda.gov/market-news/custom-reports">provides daily reports</a> about the food moving through 16 terminal markets around the world. Each day, USDA experts walk through the markets, noting the size, price, condition, origin, and other aspects of the goods for sale, such as&#13;
<a class="orm:hideurl" href="https://oreil.ly/DzjBP">apples</a>.&#13;
While the data has many high-integrity features—it is clean, richly segmented, and high volume—the meaning of some of the data values is unclear. What do “FINEAPPEAR” and “FRAPPEAR” mean? Fortunately, knowing that this data is compiled by the USDA about the terminal markets makes answering that question much simpler. Halfway down the page of web search results for “USDA Terminal Market” is a link for a <a href="https://www.ams.usda.gov/mnreports/hc_fv020.txt">Los Angeles terminal market report</a> that includes a phone number. One quick phone call, and the mystery is solved!</p>&#13;
</div></aside>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Important" data-type="sect2"><div class="sect2" id="important">&#13;
<h2>Important</h2>&#13;
&#13;
<p>While it’s hard to get <em>anywhere</em> with data wrangling unless you have sufficient provenance and metadata information about a dataset, you’re still not going to get very far unless you have enough data, from the right time period(s), and at the right level of detail. That said, the following data characteristics are ones that we may often assess—and sometimes improve—with our own data wrangling work. So while it will certainly be easier to work with datasets that have these characteristics already, even those that fall short on a few of these may still be worth exploring.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Timely" data-type="sect3"><div class="sect3" id="idm45143427129568">&#13;
<h3>Timely</h3>&#13;
&#13;
<p>How<a data-primary="data quality" data-secondary="data integrity assessments" data-tertiary="timely" data-type="indexterm" id="idm45143427128240"/><a data-primary="data integrity" data-secondary="assessing" data-tertiary="timely" data-type="indexterm" id="idm45143427126944"/><a data-primary="assessing" data-secondary="data integrity" data-tertiary="timely" data-type="indexterm" id="idm45143427125728"/><a data-primary="timely, in data integrity" data-type="indexterm" id="idm45143427124512"/> up to date is the data that you’re using? Unless you’re studying a historical period, ensuring that your data is recent enough to meaningfully describe the <em>current</em> state of the world is important—though how old is “too old” will depend on both the phenomenon you’re exploring and the frequency with which data about it is collected and released.</p>&#13;
&#13;
<p>For example, if you’re interested in neighborhood demographics but your most recent data is several years old, there’s a strong likelihood that things may have changed considerably since. For unemployment data, data older than a month will no longer be timely, while for stock market data, it takes just a few seconds for information to be considered too old to inform trades. Unless it’s about a field you’re already familiar with, assessing whether your data is timely will likely require some research with experts as well as the data publisher.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Complete" data-type="sect3"><div class="sect3" id="idm45143427121680">&#13;
<h3>Complete</h3>&#13;
&#13;
<p>Does<a data-primary="data quality" data-secondary="data integrity assessments" data-tertiary="complete" data-type="indexterm" id="data-quality-integrity-assessment-complete"/><a data-primary="data integrity" data-secondary="assessing" data-tertiary="complete" data-type="indexterm" id="data-integrity-assessment-complete"/><a data-primary="assessing" data-secondary="data integrity" data-tertiary="complete" data-type="indexterm" id="assessing-data-integrity-complete"/><a data-primary="complete, in data integrity" data-type="indexterm" id="complete-data-integrity"/> the dataset contain all of the data values it should? In the USDA apple data in <a data-type="xref" href="#usda_data">“USDA Data Sleuthing”</a>, for example, only a few of the rows contain appearance descriptions, while most are left blank. Can we still generate useful data insights when parts of the data are so incomplete? Addressing this question means first answering two others. First, <em>why</em> is the data missing? Second, do you <em>need</em> that data measure in order to perform a specific analysis needed for your data wrangling process?</p>&#13;
&#13;
<p>For example, your data may be incomplete because individual values are missing, or because the data has been reported irregularly—perhaps there is a half-year gap in data that is usually recorded every month. The dataset may also have been truncated, which is a common problem when large datasets are opened in a spreadsheet program. Whatever the reason, discovering <em>why</em> some part of the data is missing is essential in order to know how to proceed. In <a data-type="xref" href="#usda_data">“USDA Data Sleuthing”</a>, we could potentially ignore the “appearance” category if our primary interest is in the way that different varietals of apples are priced, or we could contact the terminal market again to clarify if blank values in the appearance column actually correspond to some default value that they don’t bother to indicate explicitly. Even truncated data may not be a problem if what we have available covers a sufficient time period for our purposes, but it is still useful to learn the true number of records and date range of the data for context. And while there are statistical tricks that can sometimes make gaps in data collection less problematic, learning that the recording gap is due to some disruptive event may change the type of analysis you do, or even the question you’re pursuing altogether.</p>&#13;
&#13;
<p>In other words, while having <em>complete</em> data is always preferable, once you know <em>why</em> that data is missing, you may be able to proceed with your data wrangling process, regardless. But you should always find out—and be sure to document what you learn!</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="High volume" data-type="sect3"><div class="sect3" id="idm45143427108048">&#13;
<h3>High volume</h3>&#13;
&#13;
<p>How <a data-primary="data quality" data-secondary="data integrity assessments" data-tertiary="high volume" data-type="indexterm" id="data-quality-integrity-assessment-volume"/><a data-primary="data integrity" data-secondary="assessing" data-tertiary="high volume" data-type="indexterm" id="data-integrity-assessment-volume"/><a data-primary="assessing" data-secondary="data integrity" data-tertiary="high volume" data-type="indexterm" id="assessing-data-integrity-volume"/><a data-primary="high volume, in data integrity" data-type="indexterm" id="high-volume-data-integrity"/>many data points are “enough”? At minimum, a dataset will need to have sufficient records to support the type of analysis needed to answer your particular question. If what you need is a <em>count</em>—for example, the number of 311 calls that involved noise complaints in a particular year—then having “enough” data means having records of <em>all</em> of the 311 calls for that particular year.</p>&#13;
&#13;
<p>If your question is about general or generalizable patterns or trends, however, what counts as “enough” is a little less clear. For example, if you wanted to determine which Citi Bike station is the “busiest,” how long a time period should you consider? A week? A month? A year? Should only weekdays be considered, or all days? The correct answer will partly depend on more thoroughly specifying your question. Are you interested in the experiences of commuters or visitors? Are you trying to generate insights to drive transit planning, retail placement, or service quality? Also, are you <em>really</em> interested in what station is “busiest,” or is it really more about a particular rate of turnover? As is so often the case, the correct answer is largely about specifying the question correctly—and that requires being, in most cases, <em>very</em> specific.</p>&#13;
&#13;
<p>One of the trickiest parts of assessing data “completeness,” however, is that accounting for factors that may influence the trend or pattern you’re investigating is difficult without knowing the subject area pretty well already. For example, while we might easily expect that Citi Bike usage might vary across seasons, what about a reduction in public transit service? An increase in fares? These changes <em>might</em> have implications for our analysis, but how can we know that when we’re still starting out?</p>&#13;
&#13;
<p>The answer—as it is so often—is (human) experts. Maybe fare increases temporarily increase bike share ridership, but only for a few months. Maybe bicycle commuters are prepared for bad weather and stick with their patterns even when it’s snowing. With infinite time and infinite data, we might be able to answer these questions for ourselves; talking to <em>humans</em> is just so much faster and so much more informative. And believe it or not, there is an expert in almost everything. For example, a quick search on <a class="orm:hideurl" href="https://oreil.ly/eGJzD">Google Scholar</a> for “seasonal ridership bike sharing” returns everything from blog posts to peer-reviewed research on the topic.</p>&#13;
&#13;
<p>What does this mean for data completeness? Existing research or—even better—a real-time conversation with a subject matter expert (see <a data-type="xref" href="app03.html#smes">“Subject Matter Experts”</a> for more on this) will help you decide which factors you’re going to include in your analysis, and as a result, how much data you need for your chosen analysis in order for it to be complete.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Multivariate" data-type="sect3"><div class="sect3" id="idm45143427091824">&#13;
<h3>Multivariate</h3>&#13;
&#13;
<p>Wrangling<a data-primary="data quality" data-secondary="data integrity assessments" data-tertiary="multivariate" data-type="indexterm" id="idm45143427090288"/><a data-primary="data integrity" data-secondary="assessing" data-tertiary="multivariate" data-type="indexterm" id="idm45143427089040"/><a data-primary="assessing" data-secondary="data integrity" data-tertiary="multivariate" data-type="indexterm" id="idm45143427087824"/><a data-primary="multivariate data (data integrity)" data-type="indexterm" id="idm45143427086608"/> real-world data inevitably means encountering real-world complexity, where a huge range of factors may influence the phenomenon we’re trying to investigate. Our “busiest” Citi Bike station is one example: beyond seasonality and transit service, the <a href="https://www.sciencedirect.com/science/article/abs/pii/S136192091731057X">surrounding terrain</a> or the density of stations could play a role. If our Citi Bike data contained only information about how many trips started and ended at a particular station, then it would be very difficult to create an analysis that could say more than “this station had the most bikes removed and returned” in a given time period.</p>&#13;
&#13;
<p>When data is multivariate, though, it means that it has multiple <em>attributes</em> or <em>features</em> associated with each record. For the <a href="https://citibikenyc.com/system-data">historical Citi Bike data</a>, for example, we know we have all of the following, thanks to our little bit of wrangling in <a data-type="xref" href="ch02.html#hitting_the_road_intro">“Hitting the Road with Citi Bike Data”</a>:</p>&#13;
&#13;
<pre data-type="programlisting">['tripduration', 'starttime', 'stoptime', 'start station id', 'start station&#13;
 name', 'start station latitude', 'start station longitude', 'end station id',&#13;
 'end station name', 'end station latitude', 'end station longitude', 'bikeid',&#13;
 'usertype', 'birth year', 'gender']</pre>&#13;
&#13;
<p>That’s 15 different features about each recorded ride, any number of which might be able to leverage toward a more meaningful or nuanced way to understand which Citi Bike station is the “busiest.”</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Atomic" data-type="sect3"><div class="sect3" id="idm45143427079712">&#13;
<h3>Atomic</h3>&#13;
&#13;
<p>Atomic data<a data-primary="data quality" data-secondary="data integrity assessments" data-tertiary="atomic" data-type="indexterm" id="idm45143427078496"/><a data-primary="data integrity" data-secondary="assessing" data-tertiary="atomic" data-type="indexterm" id="idm45143427077152"/><a data-primary="assessing" data-secondary="data integrity" data-tertiary="atomic" data-type="indexterm" id="idm45143427075936"/><a data-primary="atomic data (data integrity)" data-type="indexterm" id="idm45143427074720"/> is highly granular; it is both measured precisely and not aggregated into summary statistics or measures. In general, summary measures like rates and averages aren’t great candidates for further analysis, because so much of the underlying data’s detail has already been lost. For example, the <em>arithmetic average</em> or <em>mean</em> of both of the following sets of numbers is 30: 20, 25, 30, 45 and 15, 20, 40, 45. While summary statistics are often helpful when making comparisons across different datasets, they offer too little insight into the data’s underlying structure to support further analysis.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Achievable" data-type="sect2"><div class="sect2" id="achievable">&#13;
<h2>Achievable</h2>&#13;
&#13;
<p>No matter how a dataset appears at first, the reality is that truly clean, well-organized, and error-free data is a near impossibility—in part just because, as time goes by, some things (like dollar values) simply <em>become</em> inconsistent. As a result, there are data quality characteristics that, as data wranglers, we should just always expect we’ll need to review and improve. Fortunately, this is where the flexibility and reusability of Python really shines—meaning these tasks will get easier and faster over time as we build up our programming skills.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section class="pagebreak-before less_space" data-pdf-bookmark="Consistent" data-type="sect3"><div class="sect3" id="idm45143427069232">&#13;
<h3>Consistent</h3>&#13;
&#13;
<p>High-integrity <a data-primary="data quality" data-secondary="data integrity assessments" data-tertiary="consistent" data-type="indexterm" id="data-quality-integrity-assessment-consistent"/><a data-primary="data integrity" data-secondary="assessing" data-tertiary="consistent" data-type="indexterm" id="data-integrity-assessment-consistent"/><a data-primary="assessing" data-secondary="data integrity" data-tertiary="consistent" data-type="indexterm" id="assessing-data-integrity-consistent"/><a data-primary="consistent, in data integrity" data-type="indexterm" id="consistent-data-integrity"/>data needs to be consistent in a number of different ways. Often the most obvious type of consistency in a dataset has to do with the frequency of data that has been collected over time. Are the time intervals between individual records <em>consistent</em>? Are there gaps? Irregular intervals between data records are important to investigate (if not resolve) in part because they can be a first indicator of disruptions or disparities within the data collection process itself. Another source of pervasively <em>in</em>consistent data tends to turn up anytime a data field has text involved: fields that contain names or descriptors will almost inevitably contain multiple spellings of what is supposed to be the same term. Even fields that might seem straightforward to standardize may contain varying degrees of detail. For example, a “zip code” field might contain both five-digit zip code entries and more precise “Zip+4” values.</p>&#13;
&#13;
<p>Other types of consistency may be less obvious but no less important. Units of measure, for example, need to be consistent across the dataset. While this might seem obvious, it’s actually easier than you might first imagine for this <em>not</em> to be the case. Let’s say you’re looking at the cost of an apple over a period of a decade. Sure, your data may record all of the prices in dollars, but inflation will be constantly changing what a dollar is <em>worth</em>. And while most of us are aware that Celsius and Fahrenheit “degrees” are of different sizes, it doesn’t stop there: an imperial pint is about 568 ml, whereas an American pint is about 473 ml. In fact, even accounts of Napoleon Bonaparte’s famously short stature is likely the result of an <a href="https://britannica.com/story/was-napoleon-short">inconsistency</a> between the size of the 19th-century French inch (about 2.71cm) and today’s version (about &#13;
<span class="keep-together">2.54 cm).</span></p>&#13;
&#13;
<p>The solution to such inconsistencies is to <em>normalize</em> your data before doing any comparisons or analyses. In most cases, this is a matter of simple arithmetic: you simply have to choose which interpretation of the unit to which you will convert all others (another important moment for documentation!). This is true even of currency, which analysts often begin by converting to “real” (read: inflation-controlled) dollars, using some chosen year as a benchmark. For example, if we wanted to compare the real dollar value of the US federal minimum wage in 2009 (when it was last increased) to 2021, we could use an inflation calculator <a href="https://bls.gov/data/inflation_calculator.htm">like the one maintained by the Bureau of Labor Statistics (BLS)</a> to see that the real dollar value of $7.25 per hour in 2009 is equivalent to only $5.72 per hour in 2021.</p>&#13;
<aside class="pagebreak-before less_space" data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45143427054096">&#13;
<h5>Who Tops the “Hit” List?</h5>&#13;
<p>Data providers don’t always make it clear what units they’re using—or whether or not their numbers have been normalized. For example, Box Office Mojo provides a running tally of the world’s <a href="https://boxofficemojo.com/chart/ww_top_lifetime_gross/?area=XWW">highest-grossing films</a>, but it’s not clear whether the dollar amounts provided have been normalized to current dollars. If they haven’t, it’s pretty clear that there would be some definite reshuffling at the top of that<a data-primary="data quality" data-secondary="data integrity assessments" data-startref="data-quality-integrity-assessment-consistent" data-tertiary="consistent" data-type="indexterm" id="idm45143427051296"/><a data-primary="data integrity" data-secondary="assessing" data-startref="data-integrity-assessment-consistent" data-tertiary="consistent" data-type="indexterm" id="idm45143427049680"/><a data-primary="assessing" data-secondary="data integrity" data-startref="assessing-data-integrity-consistent" data-tertiary="consistent" data-type="indexterm" id="idm45143427048176"/><a data-primary="consistent, in data integrity" data-startref="consistent-data-integrity" data-type="indexterm" id="idm45143427046672"/> list!</p>&#13;
</div></aside>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Clear" data-type="sect3"><div class="sect3" id="idm45143427045312">&#13;
<h3>Clear</h3>&#13;
&#13;
<p>Like our<a data-primary="data quality" data-secondary="data integrity assessments" data-tertiary="clear" data-type="indexterm" id="idm45143427043984"/><a data-primary="data integrity" data-secondary="assessing" data-tertiary="clear" data-type="indexterm" id="idm45143427042736"/><a data-primary="assessing" data-secondary="data integrity" data-tertiary="clear" data-type="indexterm" id="idm45143427041520"/><a data-primary="clear, in data integrity" data-type="indexterm" id="idm45143427040304"/> Python code, our data and its labels will ideally be easy to read and interpret. Realistically, field (and even dataset) descriptors can sometimes be little more than cryptic codes that require constant cross-referencing with a data dictionary or other resources. This is part of why this type of data integrity is almost always the <em>product of</em> rather than the <em>precursor to</em> some degree of data wrangling.</p>&#13;
&#13;
<p>For example, is there some logic to the fact that the table code for the US Census Bureau’s American Community Survey Demographic and Housing Estimates is <a class="orm:hideurl" href="https://oreil.ly/kD48L"><code>DP05</code></a>? Perhaps. But it’s hardly obvious to an occasional user of Census data, any more than the column label <code>DP05_0001E</code> is likely to be.<sup><a data-type="noteref" href="ch03.html#idm45143427035984" id="idm45143427035984-marker">6</a></sup> While a download of this Census table does include multiple files that can help you piece together the meaning of the filenames and column headers, developing a clear, high-integrity dataset may well require a fair amount of relabeling, reformatting, and reindexing—especially where government-produced data is involved. As always, however, documenting your information sources and renaming processes as you go is crucial.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Dimensionally structured" data-type="sect3"><div class="sect3" id="idm45143427034560">&#13;
<h3>Dimensionally structured</h3>&#13;
&#13;
<p>Dimensionally structured <a data-primary="data quality" data-secondary="data integrity assessments" data-tertiary="dimensionally structured" data-type="indexterm" id="idm45143427033200"/><a data-primary="data integrity" data-secondary="assessing" data-tertiary="dimensionally structured" data-type="indexterm" id="idm45143427031872"/><a data-primary="assessing" data-secondary="data integrity" data-tertiary="dimensionally structured" data-type="indexterm" id="idm45143427030640"/><a data-primary="dimensionally structured, in data integrity" data-type="indexterm" id="idm45143427029408"/>data contains fields that have been grouped into or additionally labeled with useful categories, such as geographic region, year, or language. Such features often provide quick entry points for both data augmentation (which we’ll address in <a data-type="xref" href="#data_augmentation">“Data Augmentation”</a>) and data analysis, because they reduce the correlation and aggregation effort we have to do ourselves. These features can also serve as an indicator of what the data creator thought was important to record.</p>&#13;
&#13;
<p>For example, the dimensions of our Citi Bike data include whether a bike was checked out to the account of a “Subscriber” or a “Customer,” as well as the account holder’s birth year and gender—suggesting that the data’s designers believed these features might yield useful insights about Citi Bike trips and usage. As we’ll see in <a data-type="xref" href="ch07.html#chapter7">Chapter 7</a>, however, they did <em>not</em> choose to include any sort of “weekday/holiday” indicator—meaning that is a <em>dimension</em> of the data we’ll have to derive ourselves if we <a data-primary="data quality" data-secondary="data integrity assessments" data-startref="data-quality-integrity-assessment" data-type="indexterm" id="idm45143427024592"/><a data-primary="data integrity" data-secondary="assessing" data-startref="data-integrity-assessment" data-type="indexterm" id="idm45143427023360"/><a data-primary="assessing" data-secondary="data integrity" data-startref="assessing-data-integrity" data-type="indexterm" id="idm45143427022128"/>need it.</p>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="data_decoding">&#13;
<h5>Data Decoding</h5>&#13;
<p>Although <a data-primary="data decoding" data-type="indexterm" id="idm45143427019168"/><a data-primary="decoding data" data-type="indexterm" id="idm45143427018432"/>easily readable filenames and column headers are useful, coded data descriptors aren’t necessarily all bad. As we’ll explore later on, building robust, high-quality datasets sometimes requires combining two or more data sources. In these instances, the use of standardized—if occasionally cryptic—codes can be useful. For example, the <a href="https://data.nysed.gov">New York State Department of Education website</a> provides individual school profiles, along with multiple identifiers for each one. While these ID numbers aren’t very informative in and of themselves, plugging one into a search engine can help turn up other documents and datasets, making it easier to collate multiple data sources with high confidence.</p>&#13;
</div></aside>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Improving Data Quality" data-type="sect1"><div class="sect1" id="improving">&#13;
<h1>Improving Data Quality</h1>&#13;
&#13;
<p>As <a data-primary="data quality" data-secondary="improving" data-type="indexterm" id="data-quality-improve"/><a data-primary="improving data quality" data-type="indexterm" id="improve-data-quality"/>previously noted, many aspects of data quality are the product of a data wrangling process—whether that involves reconciling and normalizing units, clarifying the meaning of obscure data labels, or seeking out background information about the representativeness of your dataset. Part of what this illustrates is that, in the real world, ensuring data <em>quality</em> is at least partly the result of multiple, iterative data wrangling processes. While the terms for these phases of the data wrangling process vary, I usually describe them as data <em>cleaning</em> and data <em>augmentation</em>.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Data Cleaning" data-type="sect2"><div class="sect2" id="idm45143427009856">&#13;
<h2>Data Cleaning</h2>&#13;
&#13;
<p>In<a data-primary="data quality" data-secondary="improving" data-tertiary="cleaning" data-type="indexterm" id="idm45143427008272"/><a data-primary="improving data quality" data-secondary="cleaning" data-type="indexterm" id="idm45143427006992"/><a data-primary="data cleaning" data-type="indexterm" id="idm45143427006048"/><a data-primary="cleaning data" data-type="indexterm" id="idm45143427005376"/> reality, data “cleaning” is not so much its own step in the data wrangling process as it is a constant activity that accompanies every <em>other</em> step, both because most data is <em>not</em> clean when we encounter it and because <em>how</em> a dataset (or part of it) needs &#13;
<span class="keep-together">to be</span> “cleaned” is often revealed progressively as we work. At a high level, clean &#13;
<span class="keep-together">data might</span> be summarized as being free from errors or typos—such as mismatched units, multiple spellings of the same word or term, and fields that are not well separated—and missing or impossible values. While many of these are at least somewhat &#13;
<span class="keep-together">straightforward</span> to recognize (though not always to correct), deeper data problems may still persist. Measurement changes, calculation errors, and other oversights—especially in system-generated data—often don’t reveal themselves until some level of analysis has been done and the data has been “reality-checked” with folks who have significant expertise and/or firsthand experience with the subject.</p>&#13;
&#13;
<p>The iterative nature of data cleaning is an example of why data wrangling is a cycle rather than a linear series of steps: as your work reveals more about the data and your understanding of its relationship to the world deepens, you may find that you need to revisit earlier work that you’ve done and repeat or adjust certain aspects of the data. Of course, this is just another reason why documenting your data wrangling work is so crucial: you can use your documentation to quickly identify where and how you made any changes or updates, and to reconcile them with what you now know. Without robust documentation to guide you, you may quickly find yourself needing to start from scratch.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Data Augmentation" data-type="sect2"><div class="sect2" id="data_augmentation">&#13;
<h2>Data Augmentation</h2>&#13;
&#13;
<p><em>Augmenting</em> a <a data-primary="data quality" data-secondary="improving" data-tertiary="augmenting" data-type="indexterm" id="idm45143426997040"/><a data-primary="improving data quality" data-secondary="augmenting" data-type="indexterm" id="idm45143426995760"/><a data-primary="data augmentation" data-type="indexterm" id="idm45143426994816"/><a data-primary="augmenting data" data-type="indexterm" id="idm45143426994144"/>dataset is the process of expanding or elaborating it, usually by connecting it with other datasets—this is really the nature of “big data” in the 21st century.<sup><a data-type="noteref" href="ch03.html#idm45143426993168" id="idm45143426993168-marker">7</a></sup> By using features shared among datasets, it’s possible to bring together data from multiple sources in order to get a more complete portrait of what is happening in the world by filling in gaps, providing corroborating measures, or adding contextual data that helps us better assess data fit. In our “best” school example, this might mean using school codes to bring together the several types of data collected by different entities, such as the state-level standardized test scores and local building information mentioned in <a data-type="xref" href="#data_decoding">“Data Decoding”</a>. Through a combination of effective research and data wrangling, data augmentation can help us build data quality and answer questions far too nuanced to address through any single dataset.</p>&#13;
&#13;
<p>Like data cleaning, opportunities for data augmentation may arise at almost any point in the data wrangling process. At the same time, each new dataset we introduce will spawn a data wrangling process of its own. This means that <em>unlike</em> data cleaning, data augmentation has no definitive “end state”—there will <em>always</em> be another dataset we can add. This is yet another reason why specifically and succinctly stating our data wrangling question up front is so essential to the process: without a clearly articulated statement about what you’re trying to investigate, it’s all too easy to run out of time or other resources for your data wrangling effort. The good news is that if you’ve been keeping up your data diary, you’ll never lose track of a promising dataset for use in a future <a data-primary="data quality" data-secondary="improving" data-startref="data-quality-improve" data-type="indexterm" id="idm45143426988464"/><a data-primary="improving data quality" data-startref="improve-data-quality" data-type="indexterm" id="idm45143426987216"/>project.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section class="pagebreak-before less_space" data-pdf-bookmark="Conclusion" data-type="sect1"><div class="sect1" id="idm45143426985888">&#13;
<h1>Conclusion</h1>&#13;
&#13;
<p>Since our hands-on work with actual data has been limited up to this point, many of the concepts discussed in this chapter may seem a little bit abstract right now. Don’t worry! Things are about to get <em>very</em> hands on. In the coming chapters, we’ll start wrangling data that comes in various formats and from different sources, offering an inside look at how various characteristics of data quality play into the decisions we make as we access, evaluate, clean, analyze, and present our data. And you can be confident that by the end of this volume, you’ll be able to create meaningful, accurate, and compelling data analyses and visualizations to share your insights with the world!</p>&#13;
&#13;
<p>In the next chapter, we’ll start this process by working through how to wrangle data from a wide variety of formats into a structure that will let us do the cleaning, augmentation, and analyses we need. Let’s dive in!</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<div data-type="footnotes"><p data-type="footnote" id="idm45143427340352"><sup><a href="ch03.html#idm45143427340352-marker">1</a></sup> For example, see <a href="https://themarkup.org/denied/2021/08/25/the-secret-bias-hidden-in-mortgage-approval-algorithms">“The Secret Bias Hidden in Mortgage-Approval Algorithms”</a> by Emmanuel Martinez and  <span class="keep-together">Lauren</span> Kirschner (<em>The Markup</em>) .</p><p data-type="footnote" id="idm45143427337760"><sup><a href="ch03.html#idm45143427337760-marker">2</a></sup> “The World’s Most Valuable Resource Is No Longer Oil, but Data,” in <a href="https://economist.com/leaders/2017/05/06/the-worlds-most-valuable-resource-is-no-longer-oil-but-data"><em>The Economist</em></a>.</p><p data-type="footnote" id="idm45143427320640"><sup><a href="ch03.html#idm45143427320640-marker">3</a></sup> For example, see “Raiders of the Lost Web” by Adrienne LaFrance in <a href="https://theatlantic.com/technology/archive/2015/10/raiders-of-the-lost-web/409210"><em>The Atlantic</em></a>.</p><p data-type="footnote" id="idm45143427230496"><sup><a href="ch03.html#idm45143427230496-marker">4</a></sup> For a highly readable overview, see <a href="https://ncbi.nlm.nih.gov/pmc/articles/PMC3105563">“Statistics Without Tears” by Amitav Banerjee and Suprakash Chaudhury</a>.</p><p data-type="footnote" id="idm45143427185920"><sup><a href="ch03.html#idm45143427185920-marker">5</a></sup> This list is adapted from Stephen Few’s excellent book <em>Now You See It: Simple Visualization Techniques for Quantitative Analysis</em>  (Analytics Press), with adjustments based on my own data wrangling experience.</p><p data-type="footnote" id="idm45143427035984"><sup><a href="ch03.html#idm45143427035984-marker">6</a></sup> It indicates the total population estimate, by the way.</p><p data-type="footnote" id="idm45143426993168"><sup><a href="ch03.html#idm45143426993168-marker">7</a></sup> See danah boyd and Kate Crawford’s <a href="https://tandfonline.com/doi/full/10.1080/1369118X.2012.678878">“Critical Questions for Big Data”</a> for a discussion on big data.</p></div></div></section></body></html>