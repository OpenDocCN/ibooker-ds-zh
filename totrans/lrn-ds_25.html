<html><head></head><body><section data-pdf-bookmark="Chapter 19. Classification" data-type="chapter" epub:type="chapter"><div class="chapter" id="ch-logistic">&#13;
<h1><span class="label">Chapter 19. </span>Classification</h1>&#13;
&#13;
<p>This chapter continues<a contenteditable="false" data-primary="classification" data-type="indexterm" id="ix_classif_ch19"/> our foray into the fourth stage of the data science lifecycle: fitting and evaluating models to understand the world. So far, we’ve described how to fit a constant model using absolute error (<a class="reference internal" data-type="xref" href="ch04.html#ch-modeling">Chapter 4</a>) and simple and multiple linear models using squared error (<a class="reference internal" data-type="xref" href="ch15.html#ch-linear">Chapter 15</a>). We’ve also fit linear models with an asymmetric loss function (<a class="reference internal" data-type="xref" href="ch18.html#ch-donkey">Chapter 18</a>) and with regularized loss (<a class="reference internal" data-type="xref" href="ch16.html#ch-risk">Chapter 16</a>). In all of these cases, we aimed to predict or explain the behavior of a numeric outcome—bus wait times, smoke particles in the air, and donkey weights are all numeric variables.</p>&#13;
&#13;
<p>In this chapter we expand our view of modeling. Instead of predicting numeric outcomes, we build models to predict nominal outcomes. These sorts of models enable banks to predict whether a credit card transaction is fraudulent or not, doctors to classify tumors as benign or malignant, and your email service to identify spam and set it aside from your usual emails. This type of modeling is called <em>classification</em> and occurs widely in data science.</p>&#13;
&#13;
<p>Just as with linear regression, we formulate a model, choose a loss function, fit the model by minimizing average loss for our data, and assess the fitted model. But unlike linear regression, our model is not linear, the loss function is not squared error, and our assessment compares different kinds of classification errors. Despite these differences, the overall structure of model fitting carries over to this setting. Together, regression<a contenteditable="false" data-primary="supervised learning" data-type="indexterm" id="id1805"/> and classification compose the primary approaches for <em>supervised learning</em>, the general task of fitting models based on observed outcomes and covariates.</p>&#13;
&#13;
<p>We begin by introducing an example that we use throughout this chapter.</p>&#13;
&#13;
&#13;
<section data-pdf-bookmark="Example: Wind-Damaged Trees" data-type="sect1"><div class="sect1" id="example-wind-damaged-trees">&#13;
<h1>Example: Wind-Damaged Trees</h1>&#13;
&#13;
<p>In 1999, a huge storm<a contenteditable="false" data-primary="classification" data-secondary="wind-damaged trees" data-type="indexterm" id="ix_class_wind"/><a contenteditable="false" data-primary="wind-damaged trees dataset" data-type="indexterm" id="ix_wind_tree"/><a contenteditable="false" data-primary="Rich, Roy Lawrence" data-type="indexterm" id="id1806"/> with winds over 90 mph damaged millions of trees in the <a class="reference external" href="https://oreil.ly/O2qOL">Boundary Waters Canoe Area Wilderness</a> (BWCAW), which has the largest tract of virgin forest in the eastern US. In an effort to understand the susceptibility of trees to wind damage, a researcher named <a class="reference external" href="https://oreil.ly/plX02">Roy Lawrence Rich</a> carried out a ground survey of the BWCAW. In the years<a contenteditable="false" data-primary="windthrow" data-type="indexterm" id="id1807"/> following this study, other researchers have used this dataset to model <em>windthrow</em>, or the uprooting of trees in strong winds.</p>&#13;
&#13;
<p>The population under study are the trees in the BWCAW. The access frame are <em>transects</em>: straight lines that cut through the natural landscape. These particular transects begin close to a lake and travel orthogonally to the gradient of the land for 250–400 meters. Along these transects, surveyors stop every 25 meters and examine a 5-by-5-meter plot. At each plot, trees are counted, categorized as blown down or standing, measured in diameter at 6 ft from the ground, and their species recorded.</p>&#13;
&#13;
<p>Sampling protocols like this are common for studying natural resources. In the BWCAW, over 80% of the land in the region is within 500 meters of a lake, so the access frame nearly covers the population. The study took place over the summers of 2000 and 2001, and no other natural disasters happened between the 1999 storm and when the data were collected.</p>&#13;
&#13;
<p>Measurements were collected on over 3,600 trees, but in this example, we examine just the black spruce. There are over 650 of them. We read in these data:</p>&#13;
&#13;
<div class="cell docutils container">&#13;
<div class="cell_input docutils container">&#13;
<div class="highlight-ipython3 notranslate">&#13;
<div class="highlight">&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<span><code class="n">trees</code></span><code> </code><span><code class="o">=</code></span><code> </code><span><code class="n">pd</code></span><span><code class="o">.</code></span><span><code class="n">read_csv</code></span><span><code class="p">(</code></span><span><code class="s1">'</code><code class="s1">data/black_spruce.csv</code><code class="s1">'</code></span><span><code class="p">)</code></span><code>&#13;
</code><span><code class="n">trees</code></span><code>&#13;
</code></pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<div class="cell_output docutils container">&#13;
<div class="output text_html">&#13;
<div>&#13;
<table class="dataframe">&#13;
	<thead>&#13;
		<tr>&#13;
			<th> </th>&#13;
			<th>diameter</th>&#13;
			<th>storm</th>&#13;
			<th>status</th>&#13;
		</tr>&#13;
	</thead>&#13;
	<tbody>&#13;
		<tr>&#13;
			<td><strong>0</strong></td>&#13;
			<td class="right">9.0</td>&#13;
			<td>0.02</td>&#13;
			<td>standing</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td><strong>1</strong></td>&#13;
			<td class="right">11.0</td>&#13;
			<td>0.03</td>&#13;
			<td>standing</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td><strong>2</strong></td>&#13;
			<td class="right">9.0</td>&#13;
			<td>0.03</td>&#13;
			<td>standing</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td><strong>...</strong></td>&#13;
			<td>...</td>&#13;
			<td>...</td>&#13;
			<td>...</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td><strong>656</strong></td>&#13;
			<td class="right">9.0</td>&#13;
			<td>0.94</td>&#13;
			<td>fallen</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td><strong>657</strong></td>&#13;
			<td class="right">17.0</td>&#13;
			<td>0.94</td>&#13;
			<td>fallen</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td><strong>658</strong></td>&#13;
			<td class="right">8.0</td>&#13;
			<td>0.98</td>&#13;
			<td>fallen</td>&#13;
		</tr>&#13;
	</tbody>&#13;
</table>&#13;
&#13;
<pre>659 rows × 3 columns</pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<p>Each row corresponds to a single tree and has the following attributes:</p>&#13;
&#13;
<dl class="simple myst">&#13;
	<dt><code>diameter</code></dt>&#13;
	<dd>&#13;
	<p>Diameter of the tree in cm, measured at 6 ft above the ground </p>&#13;
	</dd>&#13;
<dt><code>storm</code></dt>&#13;
	<dd>&#13;
	<p>Severity of the storm (fraction of trees that fell in a 25-meter-wide area containing the tree) </p>&#13;
	</dd>&#13;
<dt><code>status</code></dt>&#13;
	<dd>&#13;
	<p>Tree has “fallen” or is “standing”</p>&#13;
	</dd>&#13;
</dl>&#13;
&#13;
<p>Let’s begin with some exploratory analysis before we turn to modeling. First, we calculate some simple summary statistics:</p>&#13;
&#13;
<div class="cell docutils container">&#13;
<div class="cell_input docutils container">&#13;
<div class="highlight-ipython3 notranslate">&#13;
<div class="highlight">&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<span><code class="n">trees</code></span><span><code class="o">.</code></span><span><code class="n">describe</code></span><span><code class="p">(</code><code class="p">)</code><code class="p">[</code></span><span><code class="mi">3</code></span><span><code class="p">:</code><code class="p">]</code></span><code>&#13;
</code></pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<div class="cell_output docutils container">&#13;
<div class="output text_html">&#13;
<div>&#13;
<table class="dataframe">&#13;
	<thead>&#13;
		<tr>&#13;
			<th> </th>&#13;
			<th>diameter</th>&#13;
			<th>storm</th>&#13;
		</tr>&#13;
	</thead>&#13;
	<tbody>&#13;
		<tr>&#13;
			<td><strong>min</strong></td>&#13;
			<td class="right">5.0</td>&#13;
			<td>0.02</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td><strong>25%</strong></td>&#13;
			<td class="right">6.0</td>&#13;
			<td>0.21</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td><strong>50%</strong></td>&#13;
			<td class="right">8.0</td>&#13;
			<td>0.36</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td><strong>75%</strong></td>&#13;
			<td class="right">12.0</td>&#13;
			<td>0.55</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td><strong>max</strong></td>&#13;
			<td class="right">32.0</td>&#13;
			<td>0.98</td>&#13;
		</tr>&#13;
	</tbody>&#13;
</table>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<p>Based on the quartiles, the distribution of tree diameter seems skewed right. Let’s compare the distribution of diameters for the standing and fallen trees with <span class="keep-together">histograms</span>:</p>&#13;
&#13;
<figure class="informal"><div class="figure"><img src="assets/leds_19in01.png"/>&#13;
&#13;
</div></figure>&#13;
&#13;
<p>The distribution of the diameter of the trees that fell in the storm is centered at 12 cm with a right skew. In comparison, the standing trees were nearly all under 10 cm in diameter with a mode at about 6 cm (only trees with a diameter of at least 5 cm are included in the study).</p>&#13;
&#13;
<p>Another feature<a contenteditable="false" data-primary="overplotting" data-type="indexterm" id="id1808"/> to investigate is the strength of the storm. We plot the storm strength against the tree diameter using the symbol and marker color to distinguish the standing trees from the fallen ones. Since the diameter is essentially measured to the nearest cm, many trees have the same diameter, so we jitter the values by adding a bit of noise to the diameter values to help reduce overplotting (see <a class="reference internal" data-type="xref" href="ch11.html#ch-viz">Chapter 11</a>). We also adjust the opacity of the marker colors to reveal the denser regions on the plot:</p>&#13;
&#13;
<figure class="informal"><div class="figure"><img src="assets/leds_19in02.png"/>&#13;
&#13;
</div></figure>&#13;
&#13;
<p>From this plot, it looks like both the tree diameter and the strength of the storm are related to windthrow: whether the tree was uprooted or left standing. Notice that windthrow, the feature we want to predict, is a nominal variable. In the next section, we consider how this impacts the prediction problem<a contenteditable="false" data-primary="" data-startref="ix_wind_tree" data-type="indexterm" id="id1809"/><a contenteditable="false" data-primary="" data-startref="ix_class_wind" data-type="indexterm" id="id1810"/>.</p>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Modeling and Classification" data-type="sect1"><div class="sect1" id="modeling-and-classification">&#13;
<h1>Modeling and Classification</h1>&#13;
&#13;
<p>We’d like to create<a contenteditable="false" data-primary="modeling" data-secondary="and classification" data-secondary-sortas="classification" data-type="indexterm" id="ix_mod_class_ch19"/><a contenteditable="false" data-primary="classification" data-secondary="and modeling" data-secondary-sortas="modeling" data-type="indexterm" id="ix_class_mod_ch19"/> a model that explains the susceptibility of trees to windthrow. In other words, we need to build a model for a two-level nominal feature: fallen or standing. When the response variable is nominal, this modeling task is called <em>classification</em>. In this case<a contenteditable="false" data-primary="binary classification" data-type="indexterm" id="id1811"/> there are only two levels, so this task is more specifically called <em>binary classification</em>.</p>&#13;
&#13;
<section data-pdf-bookmark="A Constant Model" data-type="sect2"><div class="sect2" id="a-constant-model">&#13;
<h2>A Constant Model</h2>&#13;
&#13;
<p>Let’s start by considering<a contenteditable="false" data-primary="constant model" data-type="indexterm" id="id1812"/> the simplest model: a constant model that always predicts one class. We use <span class="math notranslate nohighlight"><math> <mi>C</mi> </math></span> to denote the constant model’s prediction. For our windthrow dataset, this model will predict either <span class="math notranslate nohighlight"><math> <mi>C</mi> <mo>=</mo> <mtext>standing</mtext> </math></span> or <span class="math notranslate nohighlight"><math> <mi>C</mi> <mo>=</mo> <mtext>fallen</mtext> </math></span> for every input.</p>&#13;
&#13;
<p>In classification<a contenteditable="false" data-primary="zero-one error" data-type="indexterm" id="id1813"/>, we want to track how often our model predicts the correct category. For now, we simply use a count of the correct predictions. This is sometimes called the <em>zero-one error</em> because the loss function takes on one of two possible values: 1 when an incorrect prediction is made and 0 for a correct prediction. For a given observed outcome <span class="math notranslate nohighlight"><math> <msub> <mi>y</mi> <mi>i</mi> </msub> </math></span> and prediction <span class="math notranslate nohighlight"><math> <mi>C</mi> </math></span>, we can express this loss function as follows:</p>&#13;
&#13;
<div data-type="equation">&#13;
<div class="math notranslate nohighlight"><math display="block"> <mtable columnalign="right" columnspacing="0em" displaystyle="true" rowspacing="3pt"> <mtr> <mtd> <mtable displaystyle="true" rowspacing="3pt"> <mtr> <mtd> <mrow> <mi>ℓ</mi> </mrow> <mo stretchy="false">(</mo> <mi>C</mi> <mo>,</mo> <mi>y</mi> <mo stretchy="false">)</mo> <mo>=</mo> <mrow> <mo>{</mo> <mtable columnalign="left left" columnspacing="1em" rowspacing=".2em"> <mtr> <mtd> <mn>0</mn> </mtd> <mtd/> <mtd> <mtext>when </mtext> <mi>C</mi> <mtext> matches </mtext> <mi>y</mi> </mtd> </mtr> <mtr> <mtd> <mn>1</mn> </mtd> <mtd/> <mtd> <mtext>when </mtext> <mi>C</mi> <mtext> is a mismatch for </mtext> <mi>y</mi> </mtd> </mtr> </mtable> <mo fence="true" stretchy="true" symmetric="true"/> </mrow> </mtd> </mtr> </mtable> </mtd> </mtr> </mtable> </math></div>&#13;
</div>&#13;
&#13;
<p>When we have collected data, <span class="math notranslate nohighlight"><math> <mrow> <mi mathvariant="bold">y</mi> </mrow> <mo>=</mo> <mo stretchy="false">[</mo> <msub> <mi>y</mi> <mn>1</mn> </msub> <mo>,</mo> <mo>…</mo> <mo>,</mo> <msub> <mi>y</mi> <mi>n</mi> </msub> <mo stretchy="false">]</mo> </math></span>, then the average loss is:</p>&#13;
&#13;
<div data-type="equation">&#13;
<div class="math notranslate nohighlight"><math display="block"> <mtable columnalign="right" columnspacing="0em" displaystyle="true" rowspacing="3pt"> <mtr> <mtd> <mtable columnalign="right left" columnspacing="0em" displaystyle="true" rowspacing="3pt"> <mtr> <mtd> <mi>L</mi> <mo stretchy="false">(</mo> <mi>C</mi> <mo>,</mo> <mrow> <mi mathvariant="bold">y</mi> </mrow> <mo stretchy="false">)</mo> </mtd> <mtd> <mi/> <mo>=</mo> <mfrac> <mn>1</mn> <mi>n</mi> </mfrac> <munderover> <mo>∑</mo> <mrow> <mi>i</mi> <mo>=</mo> <mn>1</mn> </mrow> <mi>n</mi> </munderover> <mrow> <mi>ℓ</mi> </mrow> <mo stretchy="false">(</mo> <mi>C</mi> <mo>,</mo> <mi>y</mi> <mo stretchy="false">)</mo> </mtd> </mtr> <mtr> <mtd/> <mtd> <mi/> <mo>=</mo> <mfrac> <mrow> <mi mathvariant="normal">#</mi> <mtext> mismatches</mtext> </mrow> <mi>n</mi> </mfrac> </mtd> </mtr> </mtable> </mtd> </mtr> </mtable> </math></div>&#13;
</div>&#13;
&#13;
<p>For the constant model (see <a class="reference internal" data-type="xref" href="ch04.html#ch-modeling">Chapter 4</a>), the model minimizes the loss when <span class="math notranslate nohighlight"><math> <mi>C</mi> </math></span> is set to the most prevalent category.</p>&#13;
&#13;
<p>In the case of the black spruce, we have the following proportions of standing and fallen trees:</p>&#13;
&#13;
<div class="cell docutils container">&#13;
<div class="cell_input docutils container">&#13;
<div class="highlight-ipython3 notranslate">&#13;
<div class="highlight">&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<span><code class="n">trees</code></span><span><code class="p">[</code></span><span><code class="s1">'</code><code class="s1">status</code><code class="s1">'</code></span><span><code class="p">]</code></span><span><code class="o">.</code></span><span><code class="n">value_counts</code></span><span><code class="p">(</code><code class="p">)</code></span><code> </code><span><code class="o">/</code></span><code> </code><span><code class="nb">len</code></span><span><code class="p">(</code></span><span><code class="n">trees</code></span><span><code class="p">)</code></span><code>&#13;
</code></pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<div class="cell_output docutils container">&#13;
<div class="output text_plain highlight-myst-ansi notranslate">&#13;
<div class="highlight">&#13;
<pre data-type="programlisting">&#13;
status&#13;
standing    0.65&#13;
fallen      0.35&#13;
Name: count, dtype: float64&#13;
</pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<p>So our prediction is that a tree stands, and the average loss for our dataset is <span class="math notranslate nohighlight"><math> <mn>0.35</mn> </math></span>.</p>&#13;
&#13;
<p>That said, this prediction is not particularly helpful or insightful. For example, in our EDA of the trees dataset, we saw that the size of the tree is correlated with whether the tree stands or falls. Ideally, we could incorporate this information into the model, but the constant model doesn’t let us do this. Let’s build some intuition for how we can incorporate predictors into our model.</p>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Examining the Relationship Between Size and Windthrow" data-type="sect2"><div class="sect2" id="examining-the-relationship-between-size-and-windthrow">&#13;
<h2>Examining the Relationship Between Size and Windthrow</h2>&#13;
&#13;
<p>We want to take a closer look at how tree size is related to windthrow. For convenience, we transform the nominal windthrow feature into a 0-1 numeric feature where 1 stands for a fallen tree and 0 for standing:</p>&#13;
&#13;
<div class="cell docutils container">&#13;
<div class="cell_input docutils container">&#13;
<div class="highlight-ipython3 notranslate">&#13;
<div class="highlight">&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<span><code class="n">trees</code></span><span><code class="p">[</code></span><span><code class="s1">'</code><code class="s1">status_0_1</code><code class="s1">'</code></span><span><code class="p">]</code></span><code> </code><span><code class="o">=</code></span><code> </code><span><code class="p">(</code></span><span><code class="n">trees</code></span><span><code class="p">[</code></span><span><code class="s1">'</code><code class="s1">status</code><code class="s1">'</code></span><span><code class="p">]</code></span><code> </code><span><code class="o">==</code></span><code> </code><span><code class="s1">'</code><code class="s1">fallen</code><code class="s1">'</code></span><span><code class="p">)</code></span><span><code class="o">.</code></span><span><code class="n">astype</code></span><span><code class="p">(</code></span><span><code class="nb">int</code></span><span><code class="p">)</code></span><code>&#13;
</code><span><code class="n">trees</code></span><code>&#13;
</code></pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<div class="cell_output docutils container">&#13;
<div class="output text_html">&#13;
<div>&#13;
<table class="dataframe">&#13;
	<thead>&#13;
		<tr>&#13;
			<th> </th>&#13;
			<th>diameter</th>&#13;
			<th>storm</th>&#13;
			<th>status</th>&#13;
			<th>status_0_1</th>&#13;
		</tr>&#13;
	</thead>&#13;
	<tbody>&#13;
		<tr>&#13;
			<td><strong>0</strong></td>&#13;
			<td class="right">9.0</td>&#13;
			<td>0.02</td>&#13;
			<td>standing</td>&#13;
			<td>0</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td><strong>1</strong></td>&#13;
			<td class="right">11.0</td>&#13;
			<td>0.03</td>&#13;
			<td>standing</td>&#13;
			<td>0</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td><strong>2</strong></td>&#13;
			<td class="right">9.0</td>&#13;
			<td>0.03</td>&#13;
			<td>standing</td>&#13;
			<td>0</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td><strong>...</strong></td>&#13;
			<td>...</td>&#13;
			<td>...</td>&#13;
			<td>...</td>&#13;
			<td>...</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td><strong>656</strong></td>&#13;
			<td class="right">9.0</td>&#13;
			<td>0.94</td>&#13;
			<td>fallen</td>&#13;
			<td>1</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td><strong>657</strong></td>&#13;
			<td class="right">17.0</td>&#13;
			<td>0.94</td>&#13;
			<td>fallen</td>&#13;
			<td>1</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td><strong>658</strong></td>&#13;
			<td class="right">8.0</td>&#13;
			<td>0.98</td>&#13;
			<td>fallen</td>&#13;
			<td>1</td>&#13;
		</tr>&#13;
	</tbody>&#13;
</table>&#13;
&#13;
<pre>659 rows × 4 columns</pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<p>This representation is useful in many ways. For example, the average of <code>status_0_1</code> is the proportion of fallen trees in the dataset:</p>&#13;
&#13;
<div class="cell docutils container">&#13;
<div class="cell_input docutils container">&#13;
<div class="highlight-ipython3 notranslate">&#13;
<div class="highlight">&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<span><code class="n">pr_fallen</code></span><code> </code><span><code class="o">=</code></span><code> </code><span><code class="n">np</code></span><span><code class="o">.</code></span><span><code class="n">mean</code></span><span><code class="p">(</code></span><span><code class="n">trees</code></span><span><code class="p">[</code></span><span><code class="s1">'</code><code class="s1">status_0_1</code><code class="s1">'</code></span><span><code class="p">]</code><code class="p">)</code></span><code>&#13;
</code><span><code class="nb">print</code></span><span><code class="p">(</code></span><span><code class="sa">f</code></span><span><code class="s2">"</code><code class="s2">Proportion of fallen black spruce: </code></span><span><code class="si">{</code></span><span><code class="n">pr_fallen</code></span><span><code class="si">:</code></span><span><code class="s2">0.2f</code></span><span><code class="si">}</code></span><span><code class="s2">"</code></span><span><code class="p">)</code></span><code>&#13;
</code></pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<div class="cell_output docutils container">&#13;
<div class="output stream highlight-myst-ansi notranslate">&#13;
<div class="highlight">&#13;
<pre data-type="programlisting">&#13;
Proportion of fallen black spruce: 0.35&#13;
</pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<p>Having this 0-1 feature also lets us make a plot to show the relationship between tree diameter and windthrow. This is analogous to our process for linear regression, where we make scatterplots of the outcome variable against explanatory variable(s) (see <a class="reference internal" data-type="xref" href="ch15.html#ch-linear">Chapter 15</a>).</p>&#13;
&#13;
<p>Here we plot the tree status against the diameter, but we add a small amount of random noise to the status to help us see the density of 0 and 1 values at each diameter. As before, we jitter the diameter values too and adjust the opacity of the markers to reduce overplotting. We also add a horizontal line at the proportion of fallen trees:</p>&#13;
&#13;
<figure class="informal"><div class="figure"><img src="assets/leds_19in03.png"/>&#13;
&#13;
</div></figure>&#13;
&#13;
<p>This scatterplot shows that the smaller trees are more likely to be standing than the larger trees. Notice that the average status for trees (0.35) essentially fits a constant model to the response variable. If we consider tree diameter as an explanatory feature, we should be able to improve the model.</p>&#13;
&#13;
<p>A starting place might be to compute the proportion of fallen trees for different diameters. The following block of code divides tree diameter into intervals and computes the proportion of fallen trees in each bin:</p>&#13;
&#13;
<div class="cell docutils container">&#13;
<div class="cell_input docutils container">&#13;
<div class="highlight-ipython3 notranslate">&#13;
<div class="highlight">&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<span><code class="n">splits</code></span><code> </code><span><code class="o">=</code></span><code> </code><span><code class="p">[</code></span><span><code class="mi">4</code></span><span><code class="p">,</code></span><code> </code><span><code class="mi">5</code></span><span><code class="p">,</code></span><code> </code><span><code class="mi">6</code></span><span><code class="p">,</code></span><code> </code><span><code class="mi">7</code></span><span><code class="p">,</code></span><code> </code><span><code class="mi">8</code></span><span><code class="p">,</code></span><code> </code><span><code class="mi">9</code></span><span><code class="p">,</code></span><code> </code><span><code class="mi">10</code></span><span><code class="p">,</code></span><code> </code><span><code class="mi">12</code></span><span><code class="p">,</code></span><code> </code><span><code class="mi">14</code></span><span><code class="p">,</code></span><code> </code><span><code class="mi">17</code></span><span><code class="p">,</code></span><code> </code><span><code class="mi">20</code></span><span><code class="p">,</code></span><code> </code><span><code class="mi">25</code></span><span><code class="p">,</code></span><code> </code><span><code class="mi">32</code></span><span><code class="p">]</code></span><code>&#13;
</code><span><code class="n">tree_bins</code></span><code> </code><span><code class="o">=</code></span><code> </code><span><code class="p">(</code></span><code>&#13;
</code><code>    </code><span><code class="n">trees</code></span><span><code class="p">[</code></span><span><code class="s2">"</code><code class="s2">status_0_1</code><code class="s2">"</code></span><span><code class="p">]</code></span><code>&#13;
</code><code>    </code><span><code class="o">.</code></span><span><code class="n">groupby</code></span><span><code class="p">(</code></span><span><code class="n">pd</code></span><span><code class="o">.</code></span><span><code class="n">cut</code></span><span><code class="p">(</code></span><span><code class="n">trees</code></span><span><code class="p">[</code></span><span><code class="s2">"</code><code class="s2">diameter</code><code class="s2">"</code></span><span><code class="p">]</code><code class="p">,</code></span><code> </code><span><code class="n">splits</code></span><span><code class="p">)</code><code class="p">)</code></span><code>&#13;
</code><code>    </code><span><code class="o">.</code></span><span><code class="n">agg</code></span><span><code class="p">(</code><code class="p">[</code></span><span><code class="s2">"</code><code class="s2">mean</code><code class="s2">"</code></span><span><code class="p">,</code></span><code> </code><span><code class="s2">"</code><code class="s2">count</code><code class="s2">"</code></span><span><code class="p">]</code><code class="p">)</code></span><code>&#13;
</code><code>    </code><span><code class="o">.</code></span><span><code class="n">rename</code></span><span><code class="p">(</code></span><span><code class="n">columns</code></span><span><code class="o">=</code></span><span><code class="p">{</code></span><span><code class="s2">"</code><code class="s2">mean</code><code class="s2">"</code></span><span><code class="p">:</code></span><code> </code><span><code class="s2">"</code><code class="s2">proportion</code><code class="s2">"</code></span><span><code class="p">}</code><code class="p">)</code></span><code>&#13;
</code><code>    </code><span><code class="o">.</code></span><span><code class="n">assign</code></span><span><code class="p">(</code></span><span><code class="n">diameter</code></span><span><code class="o">=</code></span><span><code class="k">lambda</code></span><code> </code><span><code class="n">df</code></span><span><code class="p">:</code></span><code> </code><span><code class="p">[</code></span><span><code class="n">i</code></span><span><code class="o">.</code></span><span><code class="n">right</code></span><code> </code><span><code class="k">for</code></span><code> </code><span><code class="n">i</code></span><code> </code><span><code class="ow">in</code></span><code> </code><span><code class="n">df</code></span><span><code class="o">.</code></span><span><code class="n">index</code></span><span><code class="p">]</code><code class="p">)</code></span><code>&#13;
</code><span><code class="p">)</code></span><code>&#13;
</code></pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<p>We can plot these proportions against tree diameter:</p>&#13;
&#13;
<figure class="informal"><div class="figure"><img src="assets/leds_19in04.png"/>&#13;
&#13;
</div></figure>&#13;
&#13;
<p>The size of the markers reflects the number of trees in the diameter bin. We can use these proportions to improve our model. For example, for a tree that is 6 cm in diameter, we would classify it as standing, whereas for a 20 cm tree, our classification would be fallen. A natural starting place for binary classification is to model the observed proportions and then use these proportions to classify. Next, we develop a model for these proportions<a contenteditable="false" data-primary="" data-startref="ix_class_mod_ch19" data-type="indexterm" id="id1814"/><a contenteditable="false" data-primary="" data-startref="ix_mod_class_ch19" data-type="indexterm" id="id1815"/>.</p>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Modeling Proportions (and Probabilities)" data-type="sect1"><div class="sect1" id="modeling-proportions-and-probabilities">&#13;
<h1>Modeling Proportions (and Probabilities)</h1>&#13;
&#13;
<p>Recall that when we model<a contenteditable="false" data-primary="probability" data-secondary="modeling" data-type="indexterm" id="ix_prob_mod"/><a contenteditable="false" data-primary="proportions, modeling" data-type="indexterm" id="ix_proport_mod"/><a contenteditable="false" data-primary="classification" data-secondary="proportions modeling" data-type="indexterm" id="ix_class_prop_mod"/><a contenteditable="false" data-primary="probability" data-secondary="and classification" data-secondary-sortas="classification" data-type="indexterm" id="ix_prob_class"/><a contenteditable="false" data-primary="classification" data-secondary="and probabilities" data-secondary-sortas="probabilities" data-type="indexterm" id="ix_class_prob"/>, we need to choose three things: a model, a loss function, and a method to minimize the average loss on our train set. In the previous section, we chose a constant model, the 0-1 loss, and a proof to fit the model. However, the constant model doesn’t incorporate predictor variables. In this section, we address this issue by introducing a new model called the <em>logistic</em> model.</p>&#13;
&#13;
<p>To motivate these models, notice that the relationship between tree diameter and the proportion of downed trees does not appear linear. For demonstration, let’s fit a simple linear model to these data to show that it has several undesirable features. Using the techniques from <a class="reference internal" data-type="xref" href="ch15.html#ch-linear">Chapter 15</a>, we fit a linear model of tree status to diameter:</p>&#13;
&#13;
<div class="cell docutils container">&#13;
<div class="cell_input docutils container">&#13;
<div class="highlight-ipython3 notranslate">&#13;
<div class="highlight">&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<span><code class="kn">from</code></span><code> </code><span><code class="nn">sklearn</code><code class="nn">.</code><code class="nn">linear_model</code></span><code> </code><span><code class="kn">import</code></span><code> </code><span><code class="n">LinearRegression</code></span><code>&#13;
</code><span><code class="n">X</code></span><code> </code><span><code class="o">=</code></span><code> </code><span><code class="n">trees</code></span><span><code class="p">[</code><code class="p">[</code></span><span><code class="s1">'</code><code class="s1">diameter</code><code class="s1">'</code></span><span><code class="p">]</code><code class="p">]</code></span><code>&#13;
</code><span><code class="n">y</code></span><code> </code><span><code class="o">=</code></span><code> </code><span><code class="n">trees</code></span><span><code class="p">[</code></span><span><code class="s1">'</code><code class="s1">status_0_1</code><code class="s1">'</code></span><span><code class="p">]</code></span><code>&#13;
</code><code>&#13;
</code><span><code class="n">lin_reg</code></span><code> </code><span><code class="o">=</code></span><code> </code><span><code class="n">LinearRegression</code></span><span><code class="p">(</code><code class="p">)</code></span><span><code class="o">.</code></span><span><code class="n">fit</code></span><span><code class="p">(</code></span><span><code class="n">X</code></span><span><code class="p">,</code></span><code> </code><span><code class="n">y</code></span><span><code class="p">)</code></span><code>&#13;
</code></pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<p>Then we add this fitted line to our scatterplot of proportions:</p>&#13;
&#13;
<figure class="informal"><div class="figure"><img src="assets/leds_19in05.png"/>&#13;
&#13;
</div></figure>&#13;
&#13;
<p>Clearly, the model doesn’t fit the proportions well at all. There are several problems:</p>&#13;
&#13;
<ul>&#13;
	<li>&#13;
	<p>The model gives proportions greater than 1 for large trees.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>The model doesn’t pick up the curvature in the proportions.</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>An extreme point (such as a tree that’s 30 cm across) shifts the fitted line to the right, away from the bulk of the data.</p>&#13;
	</li>&#13;
</ul>&#13;
&#13;
<p>To address these issues, we introduce the <em>logistic model</em>.</p>&#13;
&#13;
<section data-pdf-bookmark="A Logistic Model" data-type="sect2"><div class="sect2" id="a-logistic-model">&#13;
<h2>A Logistic Model</h2>&#13;
&#13;
<p>The logistic model<a contenteditable="false" data-primary="logistic function" data-type="indexterm" id="id1816"/><a contenteditable="false" data-primary="logistic model" data-type="indexterm" id="ix_log_model_ch19"/> is one of the most widely used basic models for classification and a simple extension of the linear model. The <em>logistic function</em>, often called the <em>sigmoid function</em>, is defined as:</p>&#13;
&#13;
<div data-type="equation">&#13;
<div class="math notranslate nohighlight"><math display="block"> <mtext mathvariant="bold">logistic</mtext> <mo stretchy="false">(</mo> <mi>t</mi> <mo stretchy="false">)</mo> <mo>=</mo> <mfrac> <mn>1</mn> <mrow> <mn>1</mn> <mo>+</mo> <mi>exp</mi> <mo>⁡</mo> <mo stretchy="false">(</mo> <mo>−</mo> <mi>t</mi> <mo stretchy="false">)</mo> </mrow> </mfrac> </math></div>&#13;
</div>&#13;
&#13;
<div data-type="warning" epub:type="warning"><h6>Warning</h6>&#13;
<p>The <em>sigmoid</em> function<a contenteditable="false" data-primary="sigmoid (σ) function" data-type="indexterm" id="id1817"/><a contenteditable="false" data-primary="σ (sigmoid function)" data-type="indexterm" id="id1818"/> is typically denoted by <span class="math notranslate nohighlight"><math> <mi>σ</mi> <mo stretchy="false">(</mo> <mi>t</mi> <mo stretchy="false">)</mo> </math></span>. Sadly, the Greek letter <span class="math notranslate nohighlight"><math> <mi>σ</mi> </math></span> is widely used to mean a lot of things in data science and statistics, like the standard deviation, logistic function, and a permutation. You’ll have to be careful when seeing <span class="math notranslate nohighlight"><math> <mi>σ</mi> </math></span> and use context to understand its meaning.</p>&#13;
</div>&#13;
&#13;
<p>We can plot the logistic function to reveal its s-shape (sigmoid-shape) and confirm that it outputs numbers between 0 and 1. The function monotonically increases with <span class="math notranslate nohighlight"><math> <mi>t</mi> </math></span>, and large values of <span class="math notranslate nohighlight"><math> <mi>t</mi> </math></span> get close to 1:</p>&#13;
&#13;
<div class="cell docutils container">&#13;
<div class="cell_input docutils container">&#13;
<div class="highlight-ipython3 notranslate">&#13;
<div class="highlight">&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<span><code class="k">def</code></span><code> </code><span><code class="nf">logistic</code></span><span><code class="p">(</code></span><span><code class="n">t</code></span><span><code class="p">)</code><code class="p">:</code></span><code>&#13;
</code><code>    </code><span><code class="k">return</code></span><code> </code><span><code class="mf">1.</code></span><code> </code><span><code class="o">/</code></span><code> </code><span><code class="p">(</code></span><span><code class="mf">1.</code></span><code> </code><span><code class="o">+</code></span><code> </code><span><code class="n">np</code></span><span><code class="o">.</code></span><span><code class="n">exp</code></span><span><code class="p">(</code></span><span><code class="o">-</code></span><span><code class="n">t</code></span><span><code class="p">)</code><code class="p">)</code></span><code>&#13;
</code></pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<p>Since the logistic function maps to the interval between 0 and 1, it is commonly used when modeling proportions and probabilities. Also, we can write the logistic as a function of a line, <span class="math notranslate nohighlight"><math> <msub> <mi>θ</mi> <mn>0</mn> </msub> <mo>+</mo> <msub> <mi>θ</mi> <mn>1</mn> </msub> <mi>x</mi> </math></span>:</p>&#13;
&#13;
<div data-type="equation">&#13;
<div class="math notranslate nohighlight"><math display="block"> <mi>σ</mi> <mrow> <mo>(</mo> <msub> <mi>θ</mi> <mn>0</mn> </msub> <mo>+</mo> <msub> <mi>θ</mi> <mn>1</mn> </msub> <mi>x</mi> <mo>)</mo> </mrow> <mo>=</mo> <mfrac> <mn>1</mn> <mrow> <mn>1</mn> <mo>+</mo> <mi>exp</mi> <mo>⁡</mo> <mo stretchy="false">(</mo> <mo>−</mo> <msub> <mi>θ</mi> <mn>0</mn> </msub> <mo>−</mo> <msub> <mi>θ</mi> <mn>1</mn> </msub> <mi>x</mi> <mo stretchy="false">)</mo> </mrow> </mfrac> </math></div>&#13;
</div>&#13;
&#13;
<p>To help build your intuition for the shape of this function, the following plot shows the logistic function as we vary <span class="math notranslate nohighlight"><math> <msub> <mi>θ</mi> <mn>0</mn> </msub> </math></span> and <span class="math notranslate nohighlight"><math> <msub> <mi>θ</mi> <mn>1</mn> </msub> </math></span>:</p>&#13;
&#13;
<figure class="informal"><div class="figure"><img src="assets/leds_19in06.png"/>&#13;
&#13;
</div></figure>&#13;
&#13;
<p>We can see that changing the magnitude of <span class="math notranslate nohighlight"><math> <msub> <mrow> <mi>θ</mi> </mrow> <mn>1</mn> </msub> </math></span> changes the sharpness of the curve; the farther away from 0, the steeper the curve. Flipping the sign of <span class="math notranslate nohighlight"><math> <msub> <mrow> <mi>θ</mi> </mrow> <mn>1</mn> </msub> </math></span> reflects the curve about the vertical line <span class="math notranslate nohighlight"><math> <mi>x</mi> <mo>=</mo> <mn>0</mn> </math></span>. Changing <span class="math notranslate nohighlight"><math> <msub> <mi>θ</mi> <mn>0</mn> </msub> </math></span> shifts the curve left and right.</p>&#13;
&#13;
<p>The logistic function can be seen as a transformation: it transforms a linear function into a nonlinear smooth curve, and the output always lies between 0 and 1. In fact, the output of a logistic function has a deeper probabilistic interpretation, which we describe next<a contenteditable="false" data-primary="" data-startref="ix_log_model_ch19" data-type="indexterm" id="id1819"/>.</p>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Log Odds" data-type="sect2"><div class="sect2" id="log-odds">&#13;
<h2>Log Odds</h2>&#13;
&#13;
<p>Recall that the odds<a contenteditable="false" data-primary="log odds model" data-type="indexterm" id="id1820"/> are the ratio <span class="math notranslate nohighlight"><math> <mi>p</mi> <mrow> <mo>/</mo> </mrow> <mo stretchy="false">(</mo> <mn>1</mn> <mo>−</mo> <mi>p</mi> <mo stretchy="false">)</mo> </math></span> for a probability <span class="math notranslate nohighlight"><math> <mi>p</mi> </math></span>. For example, when we toss a fair coin, the odds of getting heads are 1; for a coin that’s twice as likely to land heads as tails (<span class="math notranslate nohighlight"><math> <mi>p</mi> <mo>=</mo> <mn>2</mn> <mrow> <mo>/</mo> </mrow> <mn>3</mn> </math></span>), the odds of getting heads are 2. The logistic model is also called the <em>log odds</em> model because the logistic function coincides with a linear function of the log odds.</p>&#13;
&#13;
<p>We can see this in the following equations. To show this, we multiply the numerator and denominator of the sigmoid function by <span class="math notranslate nohighlight"><math> <mi>exp</mi> <mo>⁡</mo> <mo stretchy="false">(</mo> <mi>t</mi> <mo stretchy="false">)</mo> </math></span>:</p>&#13;
&#13;
<div data-type="equation">&#13;
<div class="math notranslate nohighlight"><math display="block"> <mtable columnalign="right" columnspacing="0em" displaystyle="true" rowspacing="3pt"> <mtr> <mtd> <mtable columnalign="right left" columnspacing="0em" displaystyle="true" rowspacing="3pt"> <mtr> <mtd> <mi>σ</mi> <mo stretchy="false">(</mo> <mi>t</mi> <mo stretchy="false">)</mo> </mtd> <mtd> <mi/> <mo>=</mo> <mfrac> <mn>1</mn> <mrow> <mn>1</mn> <mo>+</mo> <mi>exp</mi> <mo>⁡</mo> <mo stretchy="false">(</mo> <mo>−</mo> <mi>t</mi> <mo stretchy="false">)</mo> </mrow> </mfrac> <mo>=</mo> <mfrac> <mrow> <mi>exp</mi> <mo>⁡</mo> <mo stretchy="false">(</mo> <mi>t</mi> <mo stretchy="false">)</mo> </mrow> <mrow> <mn>1</mn> <mo>+</mo> <mi>exp</mi> <mo>⁡</mo> <mo stretchy="false">(</mo> <mi>t</mi> <mo stretchy="false">)</mo> </mrow> </mfrac> </mtd> </mtr> <mtr> <mtd/> </mtr> <mtr> <mtd> <mo stretchy="false">(</mo> <mn>1</mn> <mo>−</mo> <mi>σ</mi> <mo stretchy="false">(</mo> <mi>t</mi> <mo stretchy="false">)</mo> <mo stretchy="false">)</mo> </mtd> <mtd> <mi/> <mo>=</mo> <mn>1</mn> <mo>−</mo> <mfrac> <mrow> <mi>exp</mi> <mo>⁡</mo> <mo stretchy="false">(</mo> <mi>t</mi> <mo stretchy="false">)</mo> </mrow> <mrow> <mn>1</mn> <mo>+</mo> <mi>exp</mi> <mo>⁡</mo> <mo stretchy="false">(</mo> <mi>t</mi> <mo stretchy="false">)</mo> </mrow> </mfrac> <mo>=</mo> <mfrac> <mn>1</mn> <mrow> <mn>1</mn> <mo>+</mo> <mi>exp</mi> <mo>⁡</mo> <mo stretchy="false">(</mo> <mi>t</mi> <mo stretchy="false">)</mo> </mrow> </mfrac> </mtd> </mtr> </mtable> </mtd> </mtr> </mtable> </math></div>&#13;
</div>&#13;
&#13;
<p>Then we take the logarithm of the odds and simplify:</p>&#13;
&#13;
<div data-type="equation">&#13;
<div class="math notranslate nohighlight"><math display="block"> <mtable columnalign="right" columnspacing="0em" displaystyle="true" rowspacing="3pt"> <mtr> <mtd> <mtable columnalign="right left" columnspacing="0em" displaystyle="true" rowspacing="3pt"> <mtr> <mtd> <mi>log</mi> <mo>⁡</mo> <mrow> <mo>(</mo> <mfrac> <mrow> <mi>σ</mi> <mo stretchy="false">(</mo> <mi>t</mi> <mo stretchy="false">)</mo> </mrow> <mrow> <mn>1</mn> <mo>−</mo> <mi>σ</mi> <mo stretchy="false">(</mo> <mi>t</mi> <mo stretchy="false">)</mo> </mrow> </mfrac> <mo>)</mo> </mrow> </mtd> <mtd> <mi/> <mo>=</mo> <mi>log</mi> <mo>⁡</mo> <mo stretchy="false">(</mo> <mi>exp</mi> <mo>⁡</mo> <mrow> <mo stretchy="false">(</mo> <mi>t</mi> <mo stretchy="false">)</mo> </mrow> <mo stretchy="false">)</mo> <mo>=</mo> <mi>t</mi> </mtd> </mtr> </mtable> </mtd> </mtr> </mtable> </math></div>&#13;
</div>&#13;
&#13;
<p>So, for <span class="math notranslate nohighlight"><math> <mi>σ</mi> <mo stretchy="false">(</mo> <msub> <mi>θ</mi> <mn>0</mn> </msub> <mo>+</mo> <msub> <mi>θ</mi> <mn>1</mn> </msub> <mi>x</mi> <mo stretchy="false">)</mo> </math></span>, we find the log odds are a linear function of <span class="math notranslate nohighlight"><math> <mi>x</mi> </math></span>:</p>&#13;
&#13;
<div data-type="equation">&#13;
<div class="math notranslate nohighlight"><math display="block"> <mtable columnalign="right" columnspacing="0em" displaystyle="true" rowspacing="3pt"> <mtr> <mtd> <mtable columnalign="right left" columnspacing="0em" displaystyle="true" rowspacing="3pt"> <mtr> <mtd> <mi>log</mi> <mo>⁡</mo> <mrow> <mo>(</mo> <mfrac> <mrow> <mi>σ</mi> <mo stretchy="false">(</mo> <msub> <mi>θ</mi> <mn>0</mn> </msub> <mo>+</mo> <msub> <mi>θ</mi> <mn>1</mn> </msub> <mi>x</mi> <mo stretchy="false">)</mo> </mrow> <mrow> <mn>1</mn> <mo>−</mo> <mi>σ</mi> <mo stretchy="false">(</mo> <msub> <mi>θ</mi> <mn>0</mn> </msub> <mo>+</mo> <msub> <mi>θ</mi> <mn>1</mn> </msub> <mi>x</mi> <mo stretchy="false">)</mo> </mrow> </mfrac> <mo>)</mo> </mrow> </mtd> <mtd> <mi/> <mo>=</mo> <mi>log</mi> <mo>⁡</mo> <mo stretchy="false">(</mo> <mi>exp</mi> <mo>⁡</mo> <mrow> <mo stretchy="false">(</mo> <msub> <mi>θ</mi> <mn>0</mn> </msub> <mo>+</mo> <msub> <mi>θ</mi> <mn>1</mn> </msub> <mi>x</mi> <mo stretchy="false">)</mo> </mrow> <mo stretchy="false">)</mo> <mo>=</mo> <msub> <mi>θ</mi> <mn>0</mn> </msub> <mo>+</mo> <msub> <mi>θ</mi> <mn>1</mn> </msub> <mi>x</mi> </mtd> </mtr> </mtable> </mtd> </mtr> </mtable> </math></div>&#13;
</div>&#13;
&#13;
<p>This representation of the logistic in terms of log odds gives a useful interpretation for the coefficient <span class="math notranslate nohighlight"><math> <msub> <mi>θ</mi> <mn>1</mn> </msub> </math></span>. Suppose the explanatory variable increases by 1. Then the odds change as follows:</p>&#13;
&#13;
<div data-type="equation">&#13;
<div class="math notranslate nohighlight"><math display="block"> <mtable columnalign="right" columnspacing="0em" displaystyle="true" rowspacing="3pt"> <mtr> <mtd> <mtable columnalign="right left" columnspacing="0em" displaystyle="true" rowspacing="3pt"> <mtr> <mtd> <mtext> odds </mtext> <mo>=</mo> </mtd> <mtd> <mtext> </mtext> <mi>exp</mi> <mo>⁡</mo> <mrow> <mo>(</mo> <msub> <mi>θ</mi> <mn>0</mn> </msub> <mo>+</mo> <msub> <mi>θ</mi> <mn>1</mn> </msub> <mo stretchy="false">(</mo> <mi>x</mi> <mo>+</mo> <mn>1</mn> <mo stretchy="false">)</mo> <mo>)</mo> </mrow> </mtd> </mtr> <mtr> <mtd> <mo>=</mo> </mtd> <mtd> <mtext> </mtext> <mi>exp</mi> <mo>⁡</mo> <mo stretchy="false">(</mo> <msub> <mi>θ</mi> <mn>1</mn> </msub> <mo stretchy="false">)</mo> <mo>×</mo> <mi>exp</mi> <mo>⁡</mo> <mrow> <mo stretchy="false">(</mo> <msub> <mi>θ</mi> <mn>0</mn> </msub> <mo>+</mo> <msub> <mi>θ</mi> <mn>1</mn> </msub> <mi>x</mi> <mo stretchy="false">)</mo> </mrow> </mtd> </mtr> </mtable> </mtd> </mtr> </mtable> </math></div>&#13;
</div>&#13;
&#13;
<p>We see that the odds increase or decrease by a factor of <span class="math notranslate nohighlight"><math> <mi>exp</mi> <mo>⁡</mo> <mo stretchy="false">(</mo> <msub> <mi>θ</mi> <mn>1</mn> </msub> <mo stretchy="false">)</mo> </math></span>.</p>&#13;
&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>Here, the <span class="math notranslate nohighlight"><math> <mi>log</mi> </math></span> function is the natural logarithm. Since the natural log is the default in data science, we typically don’t bother to write it as <span class="math notranslate nohighlight"><math> <mi>ln</mi> </math></span>.</p>&#13;
</div>&#13;
&#13;
<p>Next, let’s add a logistic curve to our plot of proportions to get a sense of how well it might fit the data.</p>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Using a Logistic Curve" data-type="sect2"><div class="sect2" id="using-a-logistic-curve">&#13;
<h2>Using a Logistic Curve</h2>&#13;
&#13;
<p>In the following plot, we’ve added a logistic<a contenteditable="false" data-primary="logistic curve" data-type="indexterm" id="id1821"/> curve on top of the plot of proportions of fallen trees:</p>&#13;
&#13;
<figure class="informal"><div class="figure"><img src="assets/leds_19in07.png"/>&#13;
&#13;
</div></figure>&#13;
&#13;
<p>We can see that the curve follows the proportions reasonably well. In fact, we selected this particular logistic by fitting it to the data. The fitted logistic regression is:</p>&#13;
&#13;
<div class="cell tag_hide-input docutils container">&#13;
<div class="cell_output docutils container">&#13;
<div class="output stream highlight-myst-ansi notranslate">&#13;
<div class="highlight">&#13;
<pre data-type="programlisting">&#13;
σ(-7.4 + 3.0x)&#13;
</pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<p>Now that we’ve seen that logistic curves can model probabilities well, we turn to the process of fitting logistic curves to data. In the next section, we proceed to our second step in modeling: selecting an appropriate loss function<a contenteditable="false" data-primary="" data-startref="ix_proport_mod" data-type="indexterm" id="id1822"/><a contenteditable="false" data-primary="" data-startref="ix_prob_class" data-type="indexterm" id="id1823"/><a contenteditable="false" data-primary="" data-startref="ix_prob_mod" data-type="indexterm" id="id1824"/><a contenteditable="false" data-primary="" data-startref="ix_class_prob" data-type="indexterm" id="id1825"/><a contenteditable="false" data-primary="" data-startref="ix_class_prop_mod" data-type="indexterm" id="id1826"/>.</p>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="A Loss Function for the Logistic Model" data-type="sect1"><div class="sect1" id="a-loss-function-for-the-logistic-model">&#13;
<h1>A Loss Function for the Logistic Model</h1>&#13;
&#13;
<p>The logistic<a contenteditable="false" data-primary="logistic model" data-type="indexterm" id="ix_loss_func_log_mod"/><a contenteditable="false" data-primary="classification" data-secondary="loss function for logistic model" data-type="indexterm" id="ix_class_loss_func"/><a contenteditable="false" data-primary="loss functions" data-secondary="for logistic model" data-secondary-sortas="logistic model" data-type="indexterm" id="ix_loss_func_log_mod2"/> model gives us probabilities (or empirical proportions), so we write our loss function as <span class="math notranslate nohighlight"><math> <mi>ℓ</mi> <mo stretchy="false">(</mo> <mi>p</mi> <mo>,</mo> <mi>y</mi> <mo stretchy="false">)</mo> </math></span>, where <span class="math notranslate nohighlight"><math> <mi>p</mi> </math></span> is between 0 and 1. The response takes on one of two values because our outcome feature is a binary classification. Thus, any loss function reduces to:</p>&#13;
&#13;
<div data-type="equation">&#13;
<div class="math notranslate nohighlight"><math display="block"> <mtable columnalign="right" columnspacing="0em" displaystyle="true" rowspacing="3pt"> <mtr> <mtd> <mtable displaystyle="true" rowspacing="3pt"> <mtr> <mtd> <mrow> <mi>ℓ</mi> </mrow> <mo stretchy="false">(</mo> <mi>p</mi> <mo>,</mo> <mi>y</mi> <mo stretchy="false">)</mo> <mo>=</mo> <mrow> <mo>{</mo> <mtable columnalign="left left" columnspacing="1em" rowspacing=".2em"> <mtr> <mtd> <mi>ℓ</mi> <mo stretchy="false">(</mo> <mi>p</mi> <mo>,</mo> <mn>0</mn> <mo stretchy="false">)</mo> </mtd> <mtd> <mrow> <mtext>if </mtext> <mrow> <mi>y</mi> </mrow> <mtext> is 0</mtext> </mrow> </mtd> </mtr> <mtr> <mtd> <mi>ℓ</mi> <mo stretchy="false">(</mo> <mi>p</mi> <mo>,</mo> <mn>1</mn> <mo stretchy="false">)</mo> </mtd> <mtd> <mrow> <mtext>if </mtext> <mrow> <mi>y</mi> </mrow> <mtext> is 1</mtext> </mrow> </mtd> </mtr> </mtable> <mo fence="true" stretchy="true" symmetric="true"/> </mrow> </mtd> </mtr> </mtable> </mtd> </mtr> </mtable> </math></div>&#13;
</div>&#13;
&#13;
<p>Once again, using 0 and 1 to represent the categories has an advantage because we can conveniently write the loss as:</p>&#13;
&#13;
<div data-type="equation">&#13;
<div class="math notranslate nohighlight"><math display="block"> <mi>ℓ</mi> <mo stretchy="false">(</mo> <mi>p</mi> <mo>,</mo> <mi>y</mi> <mo stretchy="false">)</mo> <mo>=</mo> <mtext> </mtext> <mi>y</mi> <mi>ℓ</mi> <mo stretchy="false">(</mo> <mi>p</mi> <mo>,</mo> <mi>y</mi> <mo stretchy="false">)</mo> <mo>+</mo> <mo stretchy="false">(</mo> <mn>1</mn> <mo>−</mo> <mi>y</mi> <mo stretchy="false">)</mo> <mi>ℓ</mi> <mo stretchy="false">(</mo> <mi>p</mi> <mo>,</mo> <mn>1</mn> <mo>−</mo> <mi>y</mi> <mo stretchy="false">)</mo> </math></div>&#13;
</div>&#13;
&#13;
<p>We encourage you to confirm this equivalence by considering the two cases <span class="math notranslate nohighlight"><math> <mi>y</mi> <mo>=</mo> <mn>1</mn> </math></span> and <span class="math notranslate nohighlight"><math> <mi>y</mi> <mo>=</mo> <mn>0</mn> </math></span>.</p>&#13;
&#13;
<p>The logistic model pairs well with <em>log loss</em>:</p>&#13;
&#13;
<div data-type="equation">&#13;
<div class="math notranslate nohighlight"><math display="block"> <mtable columnalign="right" columnspacing="0em" displaystyle="true" rowspacing="3pt"> <mtr> <mtd> <mtable columnalign="right left" columnspacing="0em" displaystyle="true" rowspacing="3pt"> <mtr> <mtd> <mrow> <mi>ℓ</mi> </mrow> <mo stretchy="false">(</mo> <mi>p</mi> <mo>,</mo> <mi>y</mi> <mo stretchy="false">)</mo> <mo>=</mo> </mtd> <mtd> <mrow> <mo>{</mo> <mtable columnalign="left left" columnspacing="1em" rowspacing=".2em"> <mtr> <mtd> <mo>−</mo> <mi>log</mi> <mo>⁡</mo> <mo stretchy="false">(</mo> <mi>p</mi> <mo stretchy="false">)</mo> </mtd> <mtd> <mrow> <mtext>if </mtext> <mrow> <mi>y</mi> </mrow> <mtext> is 1</mtext> </mrow> </mtd> </mtr> <mtr> <mtd> <mo>−</mo> <mi>log</mi> <mo>⁡</mo> <mo stretchy="false">(</mo> <mn>1</mn> <mo>−</mo> <mi>p</mi> <mo stretchy="false">)</mo> </mtd> <mtd> <mrow> <mtext>if </mtext> <mrow> <mi>y</mi> </mrow> <mtext> is 0</mtext> </mrow> </mtd> </mtr> </mtable> <mo fence="true" stretchy="true" symmetric="true"/> </mrow> </mtd> </mtr> <mtr> <mtd> <mo>=</mo> </mtd> <mtd> <mi/> <mo>−</mo> <mi>y</mi> <mi>log</mi> <mo>⁡</mo> <mo stretchy="false">(</mo> <mi>p</mi> <mo stretchy="false">)</mo> <mo>−</mo> <mo stretchy="false">(</mo> <mn>1</mn> <mo>−</mo> <mi>y</mi> <mo stretchy="false">)</mo> <mi>log</mi> <mo>⁡</mo> <mo stretchy="false">(</mo> <mn>1</mn> <mo>−</mo> <mi>p</mi> <mo stretchy="false">)</mo> </mtd> </mtr> </mtable> </mtd> </mtr> </mtable> </math></div>&#13;
</div>&#13;
&#13;
<p>Note that the log loss is not defined at 0 and 1 because <span class="math notranslate nohighlight"><math> <mo>−</mo> <mi>log</mi> <mo>⁡</mo> <mo stretchy="false">(</mo> <mi>p</mi> <mo stretchy="false">)</mo> </math></span> tends to <span class="math notranslate nohighlight"><math> <mi mathvariant="normal">∞</mi> </math></span> as <span class="math notranslate nohighlight"><math> <mi>p</mi> </math></span> approaches 0, and similarly for <span class="math notranslate nohighlight"><math> <mo>−</mo> <mi>log</mi> <mo>⁡</mo> <mo stretchy="false">(</mo> <mn>1</mn> <mo>−</mo> <mi>p</mi> <mo stretchy="false">)</mo> </math></span> as <span class="math notranslate nohighlight"><math> <mi>p</mi> </math></span> tends to 1. We need to be careful to avoid the end points in our minimization. We can see this in the following plot of the two forms of the loss function:</p>&#13;
&#13;
<figure class="informal"><div class="figure"><img src="assets/leds_19in08.png"/>&#13;
&#13;
</div></figure>&#13;
&#13;
<p>When <span class="math notranslate nohighlight"><math> <mi>y</mi> </math></span> is 1 (solid line), the loss is small for <span class="math notranslate nohighlight"><math> <mi>p</mi> </math></span> near 1, and when <span class="math notranslate nohighlight"><math> <mi>y</mi> </math></span> is 0 (dotted line), the loss is small near 0.</p>&#13;
&#13;
<p>If our goal is to fit a constant to the data using log loss, then the average loss is:</p>&#13;
&#13;
<div data-type="equation">&#13;
<div class="math notranslate nohighlight"><math display="block"> <mtable columnalign="right" columnspacing="0em" displaystyle="true" rowspacing="3pt"> <mtr> <mtd> <mtable columnalign="right left" columnspacing="0em" displaystyle="true" rowspacing="3pt"> <mtr> <mtd> <mi>L</mi> <mo stretchy="false">(</mo> <mi>p</mi> <mo>,</mo> <mtext mathvariant="bold">y</mtext> <mo stretchy="false">)</mo> <mo>=</mo> </mtd> <mtd> <mfrac> <mn>1</mn> <mi>n</mi> </mfrac> <munder> <mo>∑</mo> <mi>i</mi> </munder> <mo stretchy="false">[</mo> <mo>−</mo> <msub> <mi>y</mi> <mi>i</mi> </msub> <mi>log</mi> <mo>⁡</mo> <mo stretchy="false">(</mo> <mi>p</mi> <mo stretchy="false">)</mo> <mo>−</mo> <mo stretchy="false">(</mo> <mn>1</mn> <mo>−</mo> <msub> <mi>y</mi> <mi>i</mi> </msub> <mo stretchy="false">)</mo> <mi>log</mi> <mo>⁡</mo> <mo stretchy="false">(</mo> <mn>1</mn> <mo>−</mo> <mi>p</mi> <mo stretchy="false">)</mo> <mo stretchy="false">]</mo> </mtd> </mtr> <mtr> <mtd> <mo>=</mo> </mtd> <mtd> <mi/> <mo>−</mo> <mfrac> <msub> <mi>n</mi> <mn>1</mn> </msub> <mi>n</mi> </mfrac> <mi>log</mi> <mo>⁡</mo> <mo stretchy="false">(</mo> <mi>p</mi> <mo stretchy="false">)</mo> <mo>−</mo> <mfrac> <msub> <mi>n</mi> <mn>0</mn> </msub> <mi>n</mi> </mfrac> <mi>log</mi> <mo>⁡</mo> <mo stretchy="false">(</mo> <mn>1</mn> <mo>−</mo> <mi>p</mi> <mo stretchy="false">)</mo> <mo stretchy="false">)</mo> </mtd> </mtr> </mtable> </mtd> </mtr> </mtable> </math></div>&#13;
</div>&#13;
&#13;
<p>Here <span class="math notranslate nohighlight"><math> <msub> <mi>n</mi> <mn>0</mn> </msub> </math></span> and <span class="math notranslate nohighlight"><math> <msub> <mi>n</mi> <mn>1</mn> </msub> </math></span> are the number of <span class="math notranslate nohighlight"><math> <msub> <mi>y</mi> <mi>i</mi> </msub> </math></span> that are 0 and 1, respectively. We can differentiate with respect to <span class="math notranslate nohighlight"><math> <mi>p</mi> </math></span> to find the minimizer:</p>&#13;
&#13;
<div data-type="equation">&#13;
<div class="math notranslate nohighlight"><math display="block"> <mfrac> <mrow> <mi>∂</mi> <mi>L</mi> <mo stretchy="false">(</mo> <mi>p</mi> <mo>,</mo> <mtext mathvariant="bold">y</mtext> <mo stretchy="false">)</mo> </mrow> <mrow> <mi>∂</mi> <mi>p</mi> </mrow> </mfrac> <mo>=</mo> <mo>−</mo> <mfrac> <msub> <mi>n</mi> <mn>1</mn> </msub> <mrow> <mi>n</mi> <mi>p</mi> </mrow> </mfrac> <mo>+</mo> <mfrac> <msub> <mi>n</mi> <mn>0</mn> </msub> <mrow> <mi>n</mi> <mo stretchy="false">(</mo> <mn>1</mn> <mo>−</mo> <mi>p</mi> <mo stretchy="false">)</mo> </mrow> </mfrac> </math></div>&#13;
</div>&#13;
&#13;
<p>Then we set the derivative to 0 and solve for the minimizing value <span class="math notranslate nohighlight"><math> <mrow> <mover> <mi>p</mi> <mo stretchy="false">^</mo> </mover> </mrow> </math></span>:</p>&#13;
&#13;
<div data-type="equation">&#13;
<div class="math notranslate nohighlight"><math display="block"> <mtable columnalign="right" columnspacing="0em" displaystyle="true" rowspacing="3pt"> <mtr> <mtd> <mtable columnalign="right left" columnspacing="0em" displaystyle="true" rowspacing="3pt"> <mtr> <mtd> <mn>0</mn> </mtd> <mtd> <mi/> <mo>=</mo> <mo>−</mo> <mfrac> <msub> <mi>n</mi> <mn>1</mn> </msub> <mrow> <mi>n</mi> <mrow> <mrow> <mover> <mi>p</mi> <mo stretchy="false">^</mo> </mover> </mrow> </mrow> </mrow> </mfrac> <mo>+</mo> <mfrac> <msub> <mi>n</mi> <mn>0</mn> </msub> <mrow> <mi>n</mi> <mo stretchy="false">(</mo> <mn>1</mn> <mo>−</mo> <mrow> <mrow> <mover> <mi>p</mi> <mo stretchy="false">^</mo> </mover> </mrow> </mrow> <mo stretchy="false">)</mo> </mrow> </mfrac> </mtd> </mtr> <mtr> <mtd> <mn>0</mn> </mtd> <mtd> <mi/> <mo>=</mo> <mo>−</mo> <mrow> <mover> <mi>p</mi> <mo stretchy="false">^</mo> </mover> </mrow> <mo stretchy="false">(</mo> <mn>1</mn> <mo>−</mo> <mrow> <mover> <mi>p</mi> <mo stretchy="false">^</mo> </mover> </mrow> <mo stretchy="false">)</mo> <mfrac> <msub> <mi>n</mi> <mn>1</mn> </msub> <mrow> <mover> <mi>p</mi> <mo stretchy="false">^</mo> </mover> </mrow> </mfrac> <mo>+</mo> <mrow> <mover> <mi>p</mi> <mo stretchy="false">^</mo> </mover> </mrow> <mo stretchy="false">(</mo> <mn>1</mn> <mo>−</mo> <mrow> <mover> <mi>p</mi> <mo stretchy="false">^</mo> </mover> </mrow> <mo stretchy="false">)</mo> <mfrac> <msub> <mi>n</mi> <mn>0</mn> </msub> <mrow> <mo stretchy="false">(</mo> <mn>1</mn> <mo>−</mo> <mrow> <mrow> <mover> <mi>p</mi> <mo stretchy="false">^</mo> </mover> </mrow> </mrow> <mo stretchy="false">)</mo> </mrow> </mfrac> </mtd> </mtr> <mtr> <mtd> <mrow> <msub> <mi>n</mi> <mn>1</mn> </msub> </mrow> <mo stretchy="false">(</mo> <mn>1</mn> <mo>−</mo> <mrow> <mover> <mi>p</mi> <mo stretchy="false">^</mo> </mover> </mrow> <mo stretchy="false">)</mo> </mtd> <mtd> <mi/> <mo>=</mo> <mrow> <msub> <mi>n</mi> <mn>0</mn> </msub> </mrow> <mrow> <mover> <mi>p</mi> <mo stretchy="false">^</mo> </mover> </mrow> </mtd> </mtr> <mtr> <mtd> <mrow> <mover> <mi>p</mi> <mo stretchy="false">^</mo> </mover> </mrow> </mtd> <mtd> <mi/> <mo>=</mo> <mfrac> <msub> <mi>n</mi> <mn>1</mn> </msub> <mi>n</mi> </mfrac> </mtd> </mtr> </mtable> </mtd> </mtr> </mtable> </math></div>&#13;
</div>&#13;
&#13;
<p>(The final equation results from noting that <span class="math notranslate nohighlight"><math> <msub> <mi>n</mi> <mn>0</mn> </msub> <mo>+</mo> <msub> <mi>n</mi> <mn>1</mn> </msub> <mo>=</mo> <mi>n</mi> </math></span>.)</p>&#13;
&#13;
<p>To fit a more complex model based on the logistic function, we can substitute <span class="math notranslate nohighlight"><math> <mi>σ</mi> <mo stretchy="false">(</mo> <msub> <mi>θ</mi> <mn>0</mn> </msub> <mo>+</mo> <msub> <mi>θ</mi> <mn>1</mn> </msub> <mi>x</mi> <mo stretchy="false">)</mo> </math></span> for <span class="math notranslate nohighlight"><math> <mi>p</mi> </math></span>. And the loss for the logistic model becomes:</p>&#13;
&#13;
<div data-type="equation">&#13;
<div class="math notranslate nohighlight"><math display="block"> <mtable columnalign="right" columnspacing="0em" displaystyle="true" rowspacing="3pt"> <mtr> <mtd> <mtable columnalign="right left" columnspacing="0em" displaystyle="true" rowspacing="3pt"> <mtr> <mtd> <mrow> <mi>ℓ</mi> </mrow> <mo stretchy="false">(</mo> <mi>σ</mi> <mo stretchy="false">(</mo> <msub> <mi>θ</mi> <mn>0</mn> </msub> <mo>+</mo> <msub> <mi>θ</mi> <mn>1</mn> </msub> <mi>x</mi> <mo stretchy="false">)</mo> <mo>,</mo> <mi>y</mi> <mo stretchy="false">)</mo> </mtd> <mtd> <mi/> <mo>=</mo> <mtext> </mtext> <mi>y</mi> <mi>ℓ</mi> <mo stretchy="false">(</mo> <mi>σ</mi> <mo stretchy="false">(</mo> <msub> <mi>θ</mi> <mn>0</mn> </msub> <mo>+</mo> <msub> <mi>θ</mi> <mn>1</mn> </msub> <mi>x</mi> <mo stretchy="false">)</mo> <mo>,</mo> <mi>y</mi> <mo stretchy="false">)</mo> <mo>+</mo> <mo stretchy="false">(</mo> <mn>1</mn> <mo>−</mo> <mi>y</mi> <mo stretchy="false">)</mo> <mi>ℓ</mi> <mo stretchy="false">(</mo> <mi>σ</mi> <mo stretchy="false">(</mo> <msub> <mi>θ</mi> <mn>0</mn> </msub> <mo>+</mo> <msub> <mi>θ</mi> <mn>1</mn> </msub> <mi>x</mi> <mo stretchy="false">)</mo> <mo>,</mo> <mn>1</mn> <mo>−</mo> <mi>y</mi> <mo stretchy="false">)</mo> </mtd> </mtr> <mtr> <mtd/> <mtd> <mi/> <mo>=</mo> <mi>y</mi> <mi>log</mi> <mo>⁡</mo> <mo stretchy="false">(</mo> <mi>σ</mi> <mo stretchy="false">(</mo> <msub> <mi>θ</mi> <mn>0</mn> </msub> <mo>+</mo> <msub> <mi>θ</mi> <mn>1</mn> </msub> <mi>x</mi> <mo stretchy="false">)</mo> <mo stretchy="false">)</mo> <mo>+</mo> <mo stretchy="false">(</mo> <mn>1</mn> <mo>−</mo> <mi>y</mi> <mo stretchy="false">)</mo> <mi>log</mi> <mo>⁡</mo> <mo stretchy="false">(</mo> <mi>σ</mi> <mo stretchy="false">(</mo> <msub> <mi>θ</mi> <mn>0</mn> </msub> <mo>+</mo> <msub> <mi>θ</mi> <mn>1</mn> </msub> <mi>x</mi> <mo stretchy="false">)</mo> <mo stretchy="false">)</mo> </mtd> </mtr> </mtable> </mtd> </mtr> </mtable> </math></div>&#13;
</div>&#13;
&#13;
<p>Averaging the loss over the data, we arrive at:</p>&#13;
&#13;
<div data-type="equation">&#13;
<div class="math notranslate nohighlight"><math display="block"> <mtable columnalign="right" columnspacing="0em" displaystyle="true" rowspacing="3pt"> <mtr> <mtd> <mtable columnalign="right left" columnspacing="0em" displaystyle="true" rowspacing="3pt"> <mtr> <mtd> <mi>L</mi> <mo stretchy="false">(</mo> <msub> <mi>θ</mi> <mn>0</mn> </msub> <mo>,</mo> <msub> <mi>θ</mi> <mn>1</mn> </msub> <mo>,</mo> <mtext mathvariant="bold">x</mtext> <mo>,</mo> <mtext mathvariant="bold">y</mtext> <mo stretchy="false">)</mo> <mo>=</mo> <mfrac> <mn>1</mn> <mi>n</mi> </mfrac> <munder> <mo>∑</mo> <mi>i</mi> </munder> </mtd> <mtd> <mi/> <mo>−</mo> <msub> <mi>y</mi> <mi>i</mi> </msub> <mi>log</mi> <mo>⁡</mo> <mo stretchy="false">(</mo> <mi>σ</mi> <mo stretchy="false">(</mo> <msub> <mi>θ</mi> <mn>0</mn> </msub> <mo>+</mo> <msub> <mi>θ</mi> <mn>1</mn> </msub> <msub> <mi>x</mi> <mi>i</mi> </msub> <mo stretchy="false">)</mo> <mo stretchy="false">)</mo> </mtd> </mtr> <mtr> <mtd/> <mtd> <mi/> <mo>−</mo> <mo stretchy="false">(</mo> <mn>1</mn> <mo>−</mo> <msub> <mi>y</mi> <mi>i</mi> </msub> <mo stretchy="false">)</mo> <mi>log</mi> <mo>⁡</mo> <mo stretchy="false">(</mo> <mn>1</mn> <mo>−</mo> <mi>σ</mi> <mo stretchy="false">(</mo> <msub> <mi>θ</mi> <mn>0</mn> </msub> <mo>+</mo> <msub> <mi>θ</mi> <mn>1</mn> </msub> <msub> <mi>x</mi> <mi>i</mi> </msub> <mo stretchy="false">)</mo> <mo stretchy="false">)</mo> </mtd> </mtr> </mtable> </mtd> </mtr> </mtable> </math></div>&#13;
</div>&#13;
&#13;
<p>Unlike with squared loss, there is no closed-form solution to this loss function. Instead, we use iterative methods like gradient descent (see <a class="reference internal" data-type="xref" href="ch20.html#ch-gd">Chapter 20</a>) to minimize the average loss. This is also one of the reasons we don’t use squared error loss for logistic models—the average squared error is nonconvex, which makes it hard to optimize. The notion of convexity is covered in greater detail in <a class="reference internal" data-type="xref" href="ch20.html#ch-gd">Chapter 20</a>, and <a class="reference internal" data-type="xref" href="ch20.html#gd-convex">Figure 20-4</a> gives a picture for intuition.</p>&#13;
&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>Log loss<a contenteditable="false" data-primary="cross-entropy loss" data-type="indexterm" id="id1827"/><a contenteditable="false" data-primary="negative log likelihood" data-type="indexterm" id="id1828"/> is also called <em>logistic loss</em> and <em>cross-entropy loss</em>. Another name for it is the <em>negative log likelihood</em>. This name refers to the technique of fitting models using the likelihood that a probability distribution produced our data. We do not go any further into the background of these alternative approaches here.</p>&#13;
</div>&#13;
&#13;
<p>Fitting<a contenteditable="false" data-primary="logistic regression" data-type="indexterm" id="id1829"/> the logistic model (with the log loss) is called <em>logistic regression</em>. Logistic regression is an example of a generalized linear model, a linear model with a nonlinear transformation.</p>&#13;
&#13;
<p>We can fit logistic<a contenteditable="false" data-primary="scikit-learn library" data-secondary="logistic models" data-type="indexterm" id="id1830"/> models with <code>scikit-learn</code>. The package designers made the API very similar to fitting linear models by least squares (see <a class="reference internal" data-type="xref" href="ch15.html#ch-linear">Chapter 15</a>). First, we import the logistic regression module:</p>&#13;
&#13;
<div class="cell docutils container">&#13;
<div class="cell_input docutils container">&#13;
<div class="highlight-ipython3 notranslate">&#13;
<div class="highlight">&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<span><code class="kn">from</code></span><code> </code><span><code class="nn">sklearn</code><code class="nn">.</code><code class="nn">linear_model</code></span><code> </code><span><code class="kn">import</code></span><code> </code><span><code class="n">LogisticRegression</code></span><code>&#13;
</code></pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<p>Then we set up the regression problem with outcome <code>y</code>, the status of the tree, and covariate <code>X</code>, the diameter (which we have log-transformed):</p>&#13;
&#13;
<div class="cell docutils container">&#13;
<div class="cell_input docutils container">&#13;
<div class="highlight-ipython3 notranslate">&#13;
<div class="highlight">&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<span><code class="n">trees</code></span><span><code class="p">[</code></span><span><code class="s1">'</code><code class="s1">log_diam</code><code class="s1">'</code></span><span><code class="p">]</code></span><code> </code><span><code class="o">=</code></span><code> </code><span><code class="n">np</code></span><span><code class="o">.</code></span><span><code class="n">log</code></span><span><code class="p">(</code></span><span><code class="n">trees</code></span><span><code class="p">[</code></span><span><code class="s1">'</code><code class="s1">diameter</code><code class="s1">'</code></span><span><code class="p">]</code><code class="p">)</code></span><code> </code><code>&#13;
</code><span><code class="n">X</code></span><code> </code><span><code class="o">=</code></span><code> </code><span><code class="n">trees</code></span><span><code class="p">[</code><code class="p">[</code></span><span><code class="s1">'</code><code class="s1">log_diam</code><code class="s1">'</code></span><span><code class="p">]</code><code class="p">]</code></span><code>&#13;
</code><span><code class="n">y</code></span><code> </code><span><code class="o">=</code></span><code> </code><span><code class="n">trees</code></span><span><code class="p">[</code></span><span><code class="s1">'</code><code class="s1">status_0_1</code><code class="s1">'</code></span><span><code class="p">]</code></span><code>&#13;
</code></pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<p>Then we fit the logistic regression and examine the intercept and coefficient for diameter:</p>&#13;
&#13;
<div class="cell docutils container">&#13;
<div class="cell_input docutils container">&#13;
<div class="highlight-ipython3 notranslate">&#13;
<div class="highlight">&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<span><code class="n">lr_model</code></span><code> </code><span><code class="o">=</code></span><code> </code><span><code class="n">LogisticRegression</code></span><span><code class="p">(</code><code class="p">)</code></span><code>&#13;
</code><span><code class="n">lr_model</code></span><span><code class="o">.</code></span><span><code class="n">fit</code></span><span><code class="p">(</code></span><span><code class="n">X</code></span><span><code class="p">,</code></span><code> </code><span><code class="n">y</code></span><span><code class="p">)</code></span><code>&#13;
</code><code>&#13;
</code><span><code class="p">[</code></span><span><code class="n">intercept</code></span><span><code class="p">]</code></span><code> </code><span><code class="o">=</code></span><code> </code><span><code class="n">lr_model</code></span><span><code class="o">.</code></span><span><code class="n">intercept_</code></span><code>&#13;
</code><span><code class="p">[</code><code class="p">[</code></span><span><code class="n">coef</code></span><span><code class="p">]</code><code class="p">]</code></span><code> </code><span><code class="o">=</code></span><code> </code><span><code class="n">lr_model</code></span><span><code class="o">.</code></span><span><code class="n">coef_</code></span><code>&#13;
</code><span><code class="nb">print</code></span><span><code class="p">(</code></span><span><code class="sa">f</code></span><span><code class="s1">'</code><code class="s1">Intercept:           </code></span><span><code class="si">{</code></span><span><code class="n">intercept</code></span><span><code class="si">:</code></span><span><code class="s1">.1f</code></span><span><code class="si">}</code></span><span><code class="s1">'</code></span><span><code class="p">)</code></span><code>&#13;
</code><span><code class="nb">print</code></span><span><code class="p">(</code></span><span><code class="sa">f</code></span><span><code class="s1">'</code><code class="s1">Diameter coefficient: </code></span><span><code class="si">{</code></span><span><code class="n">coef</code></span><span><code class="si">:</code></span><span><code class="s1">.1f</code></span><span><code class="si">}</code></span><span><code class="s1">'</code></span><span><code class="p">)</code></span><code>&#13;
</code></pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<div class="cell_output docutils container">&#13;
<div class="output stream highlight-myst-ansi notranslate">&#13;
<div class="highlight">&#13;
<pre data-type="programlisting">&#13;
Intercept:           -7.4&#13;
Diameter coefficient: 3.0&#13;
</pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<p>When making a prediction<a contenteditable="false" data-primary="predict_proba() function" data-type="indexterm" id="id1831"/><a contenteditable="false" data-primary="predict() function" data-type="indexterm" id="id1832"/>, the <code>predict</code> function returns the predicted (most likely) class, and <code>predict_proba</code> returns the predicted probability. For a tree with diameter 6, we expect the prediction to be 0 (meaning <code>standing</code>) with a high probability. Let’s check:</p>&#13;
&#13;
<div class="cell docutils container">&#13;
<div class="cell_input docutils container">&#13;
<div class="highlight-ipython3 notranslate">&#13;
<div class="highlight">&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<span><code class="n">diameter6</code></span><code> </code><span><code class="o">=</code></span><code> </code><span><code class="n">pd</code></span><span><code class="o">.</code></span><span><code class="n">DataFrame</code></span><span><code class="p">(</code><code class="p">{</code></span><span><code class="s1">'</code><code class="s1">log_diam</code><code class="s1">'</code></span><span><code class="p">:</code></span><code> </code><span><code class="p">[</code></span><span><code class="n">np</code></span><span><code class="o">.</code></span><span><code class="n">log</code></span><span><code class="p">(</code></span><span><code class="mi">6</code></span><span><code class="p">)</code><code class="p">]</code><code class="p">}</code><code class="p">)</code></span><code>&#13;
</code><span><code class="p">[</code></span><span><code class="n">pred_prof</code></span><span><code class="p">]</code></span><code> </code><span><code class="o">=</code></span><code> </code><span><code class="n">lr_model</code></span><span><code class="o">.</code></span><span><code class="n">predict_proba</code></span><span><code class="p">(</code></span><span><code class="n">diameter6</code></span><span><code class="p">)</code></span><code>&#13;
</code><span><code class="nb">print</code></span><span><code class="p">(</code></span><span><code class="sa">f</code></span><span><code class="s1">'</code><code class="s1">Predicted probabilities: </code></span><span><code class="si">{</code></span><span><code class="n">pred_prof</code></span><span><code class="si">}</code></span><span><code class="s1">'</code></span><span><code class="p">)</code></span><code>&#13;
</code></pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<div class="cell_output docutils container">&#13;
<div class="output stream highlight-myst-ansi notranslate">&#13;
<div class="highlight">&#13;
<pre data-type="programlisting">&#13;
Predicted probabilities: [0.87 0.13]&#13;
</pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<p>Thus, the model predicts that a tree with a diameter of 6 has a 0.87 probability for the class <code>standing</code> and a 0.13 probability for <code>fallen</code>.</p>&#13;
&#13;
<p>Now that we’ve fit a model with one feature, we might want to see if including another feature like the strength of the storm can improve the model. To do this, we can fit a multiple logistic regression by adding a feature to <code>X</code> and fitting the model again.</p>&#13;
&#13;
<p>Notice that the logistic regression fits a model to predict probabilities—the model predicts that a tree with diameter 6 has a 0.87 probability of class <code>standing</code> and a 0.13 probability of class <code>fallen</code>. Since probabilities can be any number between 0 and 1, we need to convert the probabilities back to categories to perform classification. We address this classification problem in the next section<a contenteditable="false" data-primary="" data-startref="ix_loss_func_log_mod2" data-type="indexterm" id="id1833"/><a contenteditable="false" data-primary="" data-startref="ix_class_loss_func" data-type="indexterm" id="id1834"/><a contenteditable="false" data-primary="" data-startref="ix_loss_func_log_mod" data-type="indexterm" id="id1835"/>.</p>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="From Probabilities to Classification" data-type="sect1"><div class="sect1" id="from-probabilities-to-classification">&#13;
<h1>From Probabilities to Classification</h1>&#13;
&#13;
<p>We started this chapter<a contenteditable="false" data-primary="probability" data-secondary="and classification" data-secondary-sortas="classification" data-type="indexterm" id="ix_prob_class_ch19"/><a contenteditable="false" data-primary="classification" data-secondary="and probabilities" data-secondary-sortas="probabilities" data-type="indexterm" id="ix_class_prob_ch19"/> by presenting a binary classification problem where we want to model a nominal response variable. At this point, we have used logistic regression to model proportions or probabilities, and we’re now ready to return to the original problem: we use the predicted probabilities to classify records. For our example, this means that for a tree of a particular diameter, we use the fitted coefficients from the logistic regression to estimate the chance it is fallen. If the chance is high, we classify a tree as fallen; otherwise, we classify it as standing. But we need to choose a threshold for making this <em>decision rule</em>.</p>&#13;
&#13;
<p>The <code>sklearn</code> logistic regression model’s <code>predict</code> function implements the basic decision rule: predict <code>1</code> if the predicted probability <span class="math notranslate nohighlight"><math> <mi>p</mi> <mo>&gt;</mo> <mn>0.5</mn> </math></span>. Otherwise, predict 0. We’ve overlaid this decision rule on top of the model predictions as a dotted line:</p>&#13;
&#13;
<figure class="informal"><div class="figure"><img src="assets/leds_19in09.png"/>&#13;
&#13;
</div></figure>&#13;
&#13;
<p>In this section, we consider a more general decision rule. For some choice of <span class="math notranslate nohighlight"><math> <mi>τ</mi> </math></span>, predict 1 if the model’s predicted probability <span class="math notranslate nohighlight"><math> <mi>p</mi> <mo>&gt;</mo> <mi>τ</mi> </math></span>, otherwise predict 0. By default, <code>sklearn</code> sets <span class="math notranslate nohighlight"><math> <mi>τ</mi> <mo>=</mo> <mn>0.5</mn> </math></span>. Let’s explore what happens when <span class="math notranslate nohighlight"><math> <mi>τ</mi> </math></span> is set to other values.</p>&#13;
&#13;
<p>Choosing an appropriate value for <span class="math notranslate nohighlight"><math> <mi>τ</mi> </math></span> depends on our goals. Suppose we want to maximize accuracy. The <em>accuracy</em> of a classifier is the fraction of correct predictions. We can compute the accuracy for different thresholds, meaning different <span class="math notranslate nohighlight"><math> <mi>τ</mi> </math></span> values:</p>&#13;
&#13;
<div class="cell docutils container">&#13;
<div class="cell_input docutils container">&#13;
<div class="highlight-ipython3 notranslate">&#13;
<div class="highlight">&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<span><code class="k">def</code></span><code> </code><span><code class="nf">threshold_predict</code></span><span><code class="p">(</code></span><span><code class="n">model</code></span><span><code class="p">,</code></span><code> </code><span><code class="n">X</code></span><span><code class="p">,</code></span><code> </code><span><code class="n">threshold</code></span><span><code class="p">)</code><code class="p">:</code></span><code>&#13;
</code><code>    </code><span><code class="k">return</code></span><code> </code><span><code class="n">np</code></span><span><code class="o">.</code></span><span><code class="n">where</code></span><span><code class="p">(</code></span><span><code class="n">model</code></span><span><code class="o">.</code></span><span><code class="n">predict_proba</code></span><span><code class="p">(</code></span><span><code class="n">X</code></span><span><code class="p">)</code><code class="p">[</code><code class="p">:</code><code class="p">,</code></span><code> </code><span><code class="mi">1</code></span><span><code class="p">]</code></span><code> </code><span><code class="o">&gt;</code></span><code> </code><span><code class="n">threshold</code></span><span><code class="p">,</code></span><code> </code><span><code class="mf">1.0</code></span><span><code class="p">,</code></span><code> </code><span><code class="mf">0.0</code></span><span><code class="p">)</code></span><code>&#13;
</code><code>&#13;
</code><span><code class="k">def</code></span><code> </code><span><code class="nf">accuracy</code></span><span><code class="p">(</code></span><span><code class="n">threshold</code></span><span><code class="p">,</code></span><code> </code><span><code class="n">X</code></span><span><code class="p">,</code></span><code> </code><span><code class="n">y</code></span><span><code class="p">)</code><code class="p">:</code></span><code>&#13;
</code><code>    </code><span><code class="k">return</code></span><code> </code><span><code class="n">np</code></span><span><code class="o">.</code></span><span><code class="n">mean</code></span><span><code class="p">(</code></span><span><code class="n">threshold_predict</code></span><span><code class="p">(</code></span><span><code class="n">lr_model</code></span><span><code class="p">,</code></span><code> </code><span><code class="n">X</code></span><span><code class="p">,</code></span><code> </code><span><code class="n">threshold</code></span><span><code class="p">)</code></span><code> </code><span><code class="o">==</code></span><code> </code><span><code class="n">y</code></span><span><code class="p">)</code></span><code>&#13;
</code><code>&#13;
</code><span><code class="n">thresholds</code></span><code> </code><span><code class="o">=</code></span><code> </code><span><code class="n">np</code></span><span><code class="o">.</code></span><span><code class="n">linspace</code></span><span><code class="p">(</code></span><span><code class="mi">0</code></span><span><code class="p">,</code></span><code> </code><span><code class="mi">1</code></span><span><code class="p">,</code></span><code> </code><span><code class="mi">200</code></span><span><code class="p">)</code></span><code>&#13;
</code><span><code class="n">accs</code></span><code> </code><span><code class="o">=</code></span><code> </code><span><code class="p">[</code></span><span><code class="n">accuracy</code></span><span><code class="p">(</code></span><span><code class="n">t</code></span><span><code class="p">,</code></span><code> </code><span><code class="n">X</code></span><span><code class="p">,</code></span><code> </code><span><code class="n">y</code></span><span><code class="p">)</code></span><code> </code><span><code class="k">for</code></span><code> </code><span><code class="n">t</code></span><code> </code><span><code class="ow">in</code></span><code> </code><span><code class="n">thresholds</code></span><span><code class="p">]</code></span><code>&#13;
</code></pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<p>To understand how accuracy changes with respect to <span class="math notranslate nohighlight"><math> <mi>τ</mi> </math></span>, we make a plot:</p>&#13;
&#13;
<figure class="informal width-70"><div class="figure"><img src="assets/leds_19in10.png"/>&#13;
&#13;
</div></figure>&#13;
&#13;
<p>Notice that the threshold with the highest accuracy isn’t exactly at 0.5. In practice, we should use cross-validation to select the threshold (see <a class="reference internal" data-type="xref" href="ch16.html#ch-risk">Chapter 16</a>).</p>&#13;
&#13;
<p>The threshold<a contenteditable="false" data-startref="class imbalance" data-type="indexterm" id="id1836"/><a contenteditable="false" data-primary="imbalanced classes, handling" data-type="indexterm" id="id1837"/> that maximizes accuracy could be a value other than 0.5 for many reasons, but a common one is <em>class imbalance</em>, where one category is more frequent than another. Class imbalance can lead to a model that classifies a record as belonging to the more common category. In extreme cases (like fraud detection) when only a tiny fraction of the data contain a particular class, our models can achieve high accuracy by simply always predicting the frequent class without learning what makes a good classifier for the rare class. There are techniques for managing class imbalance, such as:</p>&#13;
&#13;
<ul>&#13;
	<li>&#13;
	<p class="pagebreak-before less_space">Resampling the data to reduce or eliminate the class imbalance</p>&#13;
	</li>&#13;
	<li>&#13;
	<p>Adjusting the loss function to put a larger penalty on the smaller class</p>&#13;
	</li>&#13;
</ul>&#13;
&#13;
<p>In our example, the class imbalance is not that extreme, so we continue without these adjustments.</p>&#13;
&#13;
<p>The problem of class imbalance explains why accuracy alone is often not how we want to judge a model. Instead, we want to differentiate between the types of correct and incorrect classifications. We describe these next.</p>&#13;
&#13;
<section data-pdf-bookmark="The Confusion Matrix" data-type="sect2"><div class="sect2" id="the-confusion-matrix">&#13;
<h2>The Confusion Matrix</h2>&#13;
&#13;
<p>A convenient<a contenteditable="false" data-primary="confusion matrix" data-type="indexterm" id="id1838"/> way to visualize errors in a binary classification is to look at the confusion matrix. The confusion matrix compares what the model predicts with the actual outcomes. There are two types of error in this situation:</p>&#13;
&#13;
<dl class="simple myst">&#13;
	<dt><em>False positives</em></dt>&#13;
	<dd>&#13;
	<p>When the actual class is 0 (false) but the model predicts 1 (true)</p>&#13;
	</dd>&#13;
	<dt><em>False negatives</em></dt>&#13;
	<dd>&#13;
	<p>When the actual class is 1 (true) but the model predicts 0 (false)</p>&#13;
	</dd>&#13;
</dl>&#13;
&#13;
<p>Ideally, we would like to minimize both kinds of errors, but we often need to manage the balance between these two sources.</p>&#13;
&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>The terms <em>positive</em> and <em>negative</em> come from disease testing, where a test indicating the presence of a disease is called a positive result. This can be a bit confusing because having a disease doesn’t seem like something positive at all. And <span class="math notranslate nohighlight"><math> <mi>y</mi> <mo>=</mo> <mn>1</mn> </math></span> denotes the “positive” case. To keep things straight, it’s a good idea to confirm your understanding of what <span class="math notranslate nohighlight"><math> <mi>y</mi> <mo>=</mo> <mn>1</mn> </math></span> stands for in the context of your data.</p>&#13;
</div>&#13;
&#13;
<p><code>scikit-learn</code> has a function to compute and plot the confusion matrix:</p>&#13;
&#13;
<div class="cell docutils container">&#13;
<div class="cell_input docutils container">&#13;
<div class="highlight-ipython3 notranslate">&#13;
<div class="highlight">&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<span><code class="kn">from</code></span><code> </code><span><code class="nn">sklearn</code><code class="nn">.</code><code class="nn">metrics</code></span><code> </code><span><code class="kn">import</code></span><code> </code><span><code class="n">confusion_matrix</code></span><code>&#13;
</code><span><code class="n">mat</code></span><code> </code><span><code class="o">=</code></span><code> </code><span><code class="n">confusion_matrix</code></span><span><code class="p">(</code></span><span><code class="n">y</code></span><span><code class="p">,</code></span><code> </code><span><code class="n">lr_model</code></span><span><code class="o">.</code></span><span><code class="n">predict</code></span><span><code class="p">(</code></span><span><code class="n">X</code></span><span><code class="p">)</code><code class="p">)</code></span><code>&#13;
</code><span><code class="n">mat</code></span><code>&#13;
</code></pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<div class="cell_output docutils container">&#13;
<div class="output text_plain highlight-myst-ansi notranslate">&#13;
<div class="highlight">&#13;
<pre data-type="programlisting">&#13;
array([[377,  49],&#13;
       [104, 129]])&#13;
</pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<figure class="informal"><div class="figure"><img src="assets/leds_19in11.png"/>&#13;
&#13;
</div></figure>&#13;
&#13;
<p>Ideally, we want to see all of the counts in the diagonal squares True negative and True positive. That means we have correctly classified everything. But this is rarely the case, and we need to assess the size of the errors. For this, it’s easier to compare rates than counts. Next, we describe different rates and when we might prefer to prioritize one or the other.</p>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Precision Versus Recall" data-type="sect2"><div class="sect2" id="precision-versus-recall">&#13;
<h2>Precision Versus Recall</h2>&#13;
&#13;
<p>In some settings<a contenteditable="false" data-primary="visualization, data" data-secondary="classifier performance" data-type="indexterm" id="ix_vis_classif"/><a contenteditable="false" data-primary="sensitivity (recall)" data-type="indexterm" id="id1839"/><a contenteditable="false" data-primary="precision versus recall" data-type="indexterm" id="ix_precision_recall"/>, there might be a much higher cost to missing positive cases. For example, if we are building a classifier to identify tumors, we want to make sure that we don’t miss any malignant tumors. Conversely, we’re less concerned about classifying a benign tumor as malignant because a pathologist would still need to take a closer look to verify the malignant classification. In this case, we want to have a high true positive rate among the records that are actually positive. The rate is called <em>sensitivity</em>, or <em>recall</em>:</p>&#13;
&#13;
<div data-type="equation">&#13;
<div class="math notranslate nohighlight"><math display="block"> <mtext>Recall</mtext> <mo>=</mo> <mfrac> <mtext>True Positives</mtext> <mrow> <mtext>True Positives</mtext> <mo>+</mo> <mtext>False Negatives</mtext> </mrow> </mfrac> <mo>=</mo> <mfrac> <mtext>True Positives</mtext> <mtext>Actually True</mtext> </mfrac> </math></div>&#13;
</div>&#13;
&#13;
<p>Higher recall runs the risk of predicting true on false records (false positives).</p>&#13;
&#13;
<p>On the other hand, when classifying email as spam (positive) or ham (negative), we might be annoyed if an important email gets thrown into our spam folder. In this setting, we want high <em>precision</em>, the accuracy of the model for positive predictions:</p>&#13;
&#13;
<div data-type="equation">&#13;
<div class="math notranslate nohighlight"><math display="block"> <mtext>Precision</mtext> <mo>=</mo> <mfrac> <mtext>True Positives</mtext> <mrow> <mtext>True Positives</mtext> <mo>+</mo> <mtext>False Positives</mtext> </mrow> </mfrac> <mo>=</mo> <mfrac> <mtext>True Positives</mtext> <mtext>Predicted True</mtext> </mfrac> </math></div>&#13;
</div>&#13;
&#13;
<p>Higher-precision models are often more likely to predict that true observations are negative (higher false-negative rate).</p>&#13;
&#13;
<p>A common analysis compares the precision and recall at different thresholds:</p>&#13;
&#13;
<div class="cell docutils container">&#13;
<div class="cell_input docutils container">&#13;
<div class="highlight-ipython3 notranslate">&#13;
<div class="highlight">&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<span><code class="kn">from</code></span><code> </code><span><code class="nn">sklearn</code></span><code> </code><span><code class="kn">import</code></span><code> </code><span><code class="n">metrics</code></span><code>&#13;
</code><span><code class="n">precision</code></span><span><code class="p">,</code></span><code> </code><span><code class="n">recall</code></span><span><code class="p">,</code></span><code> </code><span><code class="n">threshold</code></span><code> </code><span><code class="o">=</code></span><code> </code><span><code class="p">(</code></span><code>&#13;
</code><code>    </code><span><code class="n">metrics</code></span><span><code class="o">.</code></span><span><code class="n">precision_recall_curve</code></span><span><code class="p">(</code></span><span><code class="n">y</code></span><span><code class="p">,</code></span><code> </code><span><code class="n">lr_model</code></span><span><code class="o">.</code></span><span><code class="n">predict_proba</code></span><span><code class="p">(</code></span><span><code class="n">X</code></span><span><code class="p">)</code><code class="p">[</code><code class="p">:</code><code class="p">,</code></span><code> </code><span><code class="mi">1</code></span><span><code class="p">]</code><code class="p">)</code><code class="p">)</code></span><code>&#13;
</code><code>&#13;
</code><span><code class="n">tpr_df</code></span><code> </code><span><code class="o">=</code></span><code> </code><span><code class="n">pd</code></span><span><code class="o">.</code></span><span><code class="n">DataFrame</code></span><span><code class="p">(</code><code class="p">{</code></span><span><code class="s2">"</code><code class="s2">threshold</code><code class="s2">"</code></span><span><code class="p">:</code></span><span><code class="n">threshold</code></span><span><code class="p">,</code></span><code> </code><code>&#13;
</code><code>                       </code><span><code class="s2">"</code><code class="s2">precision</code><code class="s2">"</code></span><span><code class="p">:</code></span><span><code class="n">precision</code></span><span><code class="p">[</code><code class="p">:</code></span><span><code class="o">-</code></span><span><code class="mi">1</code></span><span><code class="p">]</code><code class="p">,</code></span><code> </code><span><code class="s2">"</code><code class="s2">recall</code><code class="s2">"</code></span><span><code class="p">:</code></span><code> </code><span><code class="n">recall</code></span><span><code class="p">[</code><code class="p">:</code></span><span><code class="o">-</code></span><span><code class="mi">1</code></span><span><code class="p">]</code><code class="p">,</code></span><code> </code><span><code class="p">}</code><code class="p">)</code></span><code>&#13;
</code></pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<p>To see how precision and recall relate, we plot them both against the threshold <span class="math notranslate nohighlight"><math> <mi>τ</mi> </math></span>:</p>&#13;
&#13;
<figure class="informal"><div class="figure"><img src="assets/leds_19in12.png"/>&#13;
&#13;
</div></figure>&#13;
&#13;
<p>Another common<a contenteditable="false" data-primary="precision-recall (PR) curve" data-type="indexterm" id="id1840"/> plot used to evaluate the performance of a classifier is the <em>precision-recall curve</em>, or PR curve for short. It plots the precision-recall pairs for each threshold:</p>&#13;
&#13;
<div class="cell docutils container">&#13;
<div class="cell_input docutils container">&#13;
<div class="highlight-ipython3 notranslate">&#13;
<div class="highlight">&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<span><code class="n">fig</code></span><code> </code><span><code class="o">=</code></span><code> </code><span><code class="n">px</code></span><span><code class="o">.</code></span><span><code class="n">line</code></span><span><code class="p">(</code></span><span><code class="n">tpr_df</code></span><span><code class="p">,</code></span><code> </code><span><code class="n">x</code></span><span><code class="o">=</code></span><span><code class="s2">"</code><code class="s2">recall</code><code class="s2">"</code></span><span><code class="p">,</code></span><code> </code><span><code class="n">y</code></span><span><code class="o">=</code></span><span><code class="s2">"</code><code class="s2">precision</code><code class="s2">"</code></span><span><code class="p">,</code></span><code>&#13;
</code><code>              </code><span><code class="n">labels</code></span><span><code class="o">=</code></span><span><code class="p">{</code></span><span><code class="s2">"</code><code class="s2">recall</code><code class="s2">"</code></span><span><code class="p">:</code></span><span><code class="s2">"</code><code class="s2">Recall</code><code class="s2">"</code></span><span><code class="p">,</code></span><span><code class="s2">"</code><code class="s2">precision</code><code class="s2">"</code></span><span><code class="p">:</code></span><span><code class="s2">"</code><code class="s2">Precision</code><code class="s2">"</code></span><span><code class="p">}</code><code class="p">)</code></span><code>&#13;
</code><span><code class="n">fig</code></span><span><code class="o">.</code></span><span><code class="n">update_layout</code></span><span><code class="p">(</code></span><span><code class="n">width</code></span><span><code class="o">=</code></span><span><code class="mi">450</code></span><span><code class="p">,</code></span><code> </code><span><code class="n">height</code></span><span><code class="o">=</code></span><span><code class="mi">250</code></span><span><code class="p">,</code></span><code> </code><span><code class="n">yaxis_range</code></span><span><code class="o">=</code></span><span><code class="p">[</code></span><span><code class="mi">0</code></span><span><code class="p">,</code></span><code> </code><span><code class="mi">1</code></span><span><code class="p">]</code><code class="p">)</code></span><code>&#13;
</code><span><code class="n">fig</code></span><code>&#13;
</code></pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<figure class="informal width-75"><div class="figure"><img src="assets/leds_19in13.png"/>&#13;
&#13;
</div></figure>&#13;
&#13;
<p>Notice that the righthand end of the curve reflects the imbalance in the sample. The precision matches the fraction of fallen trees in the sample, 0.35. Plotting multiple PR curves for different models can be particularly useful for comparing models.</p>&#13;
&#13;
<p>Using precision and recall gives us more control over what kinds of errors matter. As an example, let’s suppose we want to ensure that at least 75% of the fallen trees are classified as fallen. We can find the threshold where this occurs:</p>&#13;
&#13;
<div class="cell docutils container">&#13;
<div class="cell_input docutils container">&#13;
<div class="highlight-ipython3 notranslate">&#13;
<div class="highlight">&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<span><code class="n">fall75_ind</code></span><code> </code><span><code class="o">=</code></span><code> </code><span><code class="n">np</code></span><span><code class="o">.</code></span><span><code class="n">argmin</code></span><span><code class="p">(</code></span><span><code class="n">recall</code></span><code> </code><span><code class="o">&gt;</code><code class="o">=</code></span><code> </code><span><code class="mf">0.75</code></span><span><code class="p">)</code></span><code> </code><span><code class="o">-</code></span><code> </code><span><code class="mi">1</code></span><code>&#13;
</code><code>&#13;
</code><span><code class="n">fall75_threshold</code></span><code> </code><span><code class="o">=</code></span><code> </code><span><code class="n">threshold</code></span><span><code class="p">[</code></span><span><code class="n">fall75_ind</code></span><span><code class="p">]</code></span><code>&#13;
</code><span><code class="n">fall75_precision</code></span><code> </code><span><code class="o">=</code></span><code> </code><span><code class="n">precision</code></span><span><code class="p">[</code></span><span><code class="n">fall75_ind</code></span><span><code class="p">]</code></span><code>&#13;
</code><span><code class="n">fall75_recall</code></span><code> </code><span><code class="o">=</code></span><code> </code><span><code class="n">recall</code></span><span><code class="p">[</code></span><span><code class="n">fall75_ind</code></span><span><code class="p">]</code></span><code>&#13;
</code></pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<div class="cell tag_hide-input docutils container">&#13;
<div class="cell_output docutils container">&#13;
<div class="output stream highlight-myst-ansi notranslate">&#13;
<div class="highlight">&#13;
<pre data-type="programlisting">&#13;
Threshold: 0.33&#13;
Precision: 0.59&#13;
Recall:    0.81&#13;
</pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<p>We find that about 41% (1 – precision) of the trees that we classify as fallen are actually standing. In addition, we find the fraction of trees below this threshold to be:</p>&#13;
&#13;
<div class="cell docutils container">&#13;
<div class="cell_input docutils container">&#13;
<div class="highlight-ipython3 notranslate">&#13;
<div class="highlight">&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<span><code class="nb">print</code></span><span><code class="p">(</code></span><span><code class="s2">"</code><code class="s2">Proportion of samples below threshold:</code><code class="s2">"</code></span><span><code class="p">,</code></span><code> </code><code>&#13;
</code><code>      </code><span><code class="sa">f</code></span><span><code class="s2">"</code></span><span><code class="si">{</code></span><span><code class="n">np</code></span><span><code class="o">.</code></span><span><code class="n">mean</code></span><span><code class="p">(</code></span><span><code class="n">lr_model</code></span><span><code class="o">.</code></span><span><code class="n">predict_proba</code></span><span><code class="p">(</code></span><span><code class="n">X</code></span><span><code class="p">)</code><code class="p">[</code><code class="p">:</code><code class="p">,</code></span><span><code class="mi">1</code></span><span><code class="p">]</code></span><code> </code><span><code class="o">&lt;</code></span><code> </code><span><code class="n">fall75_threshold</code></span><span><code class="p">)</code></span><span><code class="si">:</code></span><span><code class="s2">0.2f</code></span><span><code class="si">}</code></span><span><code class="s2">"</code></span><span><code class="p">)</code></span><code>&#13;
</code></pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<div class="cell_output docutils container">&#13;
<div class="output stream highlight-myst-ansi notranslate">&#13;
<div class="highlight">&#13;
<pre data-type="programlisting">&#13;
Proportion of samples below threshold: 0.52&#13;
</pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<p>So, we have classified<a contenteditable="false" data-primary="specificity (true negative rate)" data-type="indexterm" id="id1841"/><a contenteditable="false" data-primary="true negative rate" data-type="indexterm" id="id1842"/> 52% of the samples as standing (negative). <em>Specificity</em> (also called <em>true negative rate</em>) measures the proportion of data belonging to the negative class that the classifier labels as negative:</p>&#13;
&#13;
<div data-type="equation">&#13;
<div class="math notranslate nohighlight"><math display="block"> <mtext>Specificity</mtext> <mo>=</mo> <mfrac> <mtext>True Negatives</mtext> <mrow> <mtext>True Negatives</mtext> <mo>+</mo> <mtext>False Positives</mtext> </mrow> </mfrac> <mo>=</mo> <mfrac> <mtext>True Negatives</mtext> <mtext>Predicted False</mtext> </mfrac> </math></div>&#13;
</div>&#13;
&#13;
<p>The specificity for our threshold is:</p>&#13;
&#13;
<div class="cell docutils container">&#13;
<div class="cell_input docutils container">&#13;
<div class="highlight-ipython3 notranslate">&#13;
<div class="highlight">&#13;
<pre data-code-language="python" data-type="programlisting">&#13;
<span><code class="n">act_neg</code></span><code> </code><span><code class="o">=</code></span><code> </code><span><code class="p">(</code></span><span><code class="n">y</code></span><code> </code><span><code class="o">==</code></span><code> </code><span><code class="mi">0</code></span><span><code class="p">)</code></span><code>&#13;
</code><span><code class="n">true_neg</code></span><code> </code><span><code class="o">=</code></span><code> </code><span><code class="p">(</code></span><span><code class="n">lr_model</code></span><span><code class="o">.</code></span><span><code class="n">predict_proba</code></span><span><code class="p">(</code></span><span><code class="n">X</code></span><span><code class="p">)</code><code class="p">[</code><code class="p">:</code><code class="p">,</code></span><span><code class="mi">1</code></span><span><code class="p">]</code></span><code> </code><span><code class="o">&lt;</code></span><code> </code><span><code class="n">fall75_threshold</code></span><span><code class="p">)</code></span><code> </code><span><code class="o">&amp;</code></span><code> </code><span><code class="n">act_neg</code></span><code>&#13;
</code></pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<div class="cell tag_hide-input docutils container">&#13;
<div class="cell_output docutils container">&#13;
<div class="output stream highlight-myst-ansi notranslate">&#13;
<div class="highlight">&#13;
<pre data-type="programlisting">&#13;
Specificity: 0.70&#13;
</pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
&#13;
<p>In other words, 70% of the trees classified as standing are actually standing.</p>&#13;
&#13;
<p>As we’ve seen, there are several ways to use the 2-by-2 confusion matrix<a contenteditable="false" data-primary="confusion matrix" data-type="indexterm" id="id1843"/>. Ideally, we want accuracy, precision, and recall to all be high. This happens when most predictions fall along the diagonal for the table, so our predictions are nearly all correct–true negatives and true positives. Unfortunately, in most scenarios our models will have some amount of error. In our example, trees of the same diameter include a mix of fallen and standing, so we can’t perfectly classify trees based on their diameter. In practice, when data scientists choose a threshold, they need to consider their context to decide whether to prioritize precision, recall, or specificity<a contenteditable="false" data-primary="" data-startref="ix_precision_recall" data-type="indexterm" id="id1844"/><a contenteditable="false" data-primary="" data-startref="ix_vis_classif" data-type="indexterm" id="id1845"/><a contenteditable="false" data-primary="" data-startref="ix_class_prob_ch19" data-type="indexterm" id="id1846"/><a contenteditable="false" data-primary="" data-startref="ix_prob_class_ch19" data-type="indexterm" id="id1847"/>.</p>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="sec-class-summary">&#13;
<h1>Summary</h1>&#13;
&#13;
<p>In this chapter, we fit simple logistic regressions with one explanatory variable, but we can easily include other variables in the model by adding more features to our design matrix. For example, if some predictors are categorical, we can include them as one-hot encoded features. These ideas carry over directly from <a class="reference internal" data-type="xref" href="ch15.html#ch-linear">Chapter 15</a>. The technique of regularization (<a class="reference internal" data-type="xref" href="ch16.html#ch-risk">Chapter 16</a>) also applies to logistic regression. We will integrate all of these modeling techniques—including using a train-test split to assess the model and cross-validation to choose the threshold—in the case study in <a class="reference internal" data-type="xref" href="ch21.html#ch-fake-news">Chapter 21</a> that develops a model to classify fake news<a contenteditable="false" data-primary="" data-startref="ix_classif_ch19" data-type="indexterm" id="id1848"/>.</p>&#13;
&#13;
<p>Logistic regression is a cornerstone in machine learning since it naturally extends to more complex models. For example, logistic regression is one of the basic components of a neural network. When the response variable has more than two categories, logistic regression can be extended to multinomial logistic regression. Another extension of logistic regression for modeling counts is called Poisson regression. These different forms of regression are related to maximum likelihood, where the underlying model for the response is binomial, multinomial, or Poisson, respectively, and the goal is to optimize the likelihood of the data over the parameters of the respective distribution. This family of models is also known as generalized linear models. In all of these scenarios, closed-form solutions for minimizing loss don’t exist, so optimization of the average loss relies on numerical methods, which we cover in the next chapter.</p>&#13;
</div></section>&#13;
</div></section></body></html>