<html><head></head><body><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 7. A Case Study in Bilingual Data Science"><div class="chapter" id="ch08">
<h1><span class="label">Chapter 7. </span>A Case Study in Bilingual Data Science</h1>


<p class="byline">Rick J. Scavetta</p>

<p class="byline">Boyan Angelov</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45127447560648">
<h5>A note for Early Release readers</h5>
<p>With Early Release ebooks, you get books in their earliest form—the author’s raw and unedited content as they write—so you can take advantage of these technologies long before the official release of these titles.</p>
</div></aside>

<p>In this final chapter, our goal is to present a case study that demonstrates a sample of all the concepts and tools we’ve shown throughout this book. Although data science provides a practically overwhelming diversity of methods and applications, we typically rely on a core toolkit in our daily work. Thus, it’s unlikely that you’ll make use of <em>all</em> the tools presented in this book (or this case study, for that matter). But that’s alright! We hope that you’ll focus on those parts of the case study that are most relevant to your work and that you’ll be inspired to be a modern, bilingual data scientist.</p>






<section data-type="sect1" data-pdf-bookmark="24 years and 1.88 million wildfires"><div class="sect1" id="idm45127447557352">
<h1>24 years and 1.88 million wildfires</h1>

<p>Our case study will focus on the <em>US Wildfires dataset</em> <sup><a data-type="noteref" id="idm45127447555192-marker" href="ch07.xhtml#idm45127447555192">1</a></sup>. This dataset, released by the US Department of Agriculture (USDA), contains 1.88 million geo-referenced wildfire records. Collectively, these fires have resulted in the loss of 140 million acres of forest over 24 years. If you want to execute the code in this chapter, download the SQLite data set from the <a href="https://doi.org/10.2737/RDS-2013-0009.4">USDA website</a> directly or from <a href="https://www.kaggle.com/rtatman/188-million-us-wildfires">Kaggle</a>. Some preprocessing has already been performed, e.g., duplicate removal.</p>

<p>There are 39 features, plus another shape variable in raw format. Many of these are unique identifiers or redundant categorical and continuous representations. Thus, to simplify our case study, we’ll focus on a few features listed in <a data-type="xref" href="#csFeatures">Table 7-1</a>.</p>
<table id="csFeatures">
<caption><span class="label">Table 7-1. </span>The <code>fires</code> table contains 39 features describing over 1.88 million wildfires in the US from 1992-2015</caption>
<thead>
<tr>
<th>Variable</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><p>STAT_CAUSE_DESCR</p></td>
<td><p>Cause of the fire (The target variable)</p></td>
</tr>
<tr>
<td><p>OWNER_CODE</p></td>
<td><p>Code for primary owner of the land</p></td>
</tr>
<tr>
<td><p>DISCOVERY_DOY</p></td>
<td><p>Day of year of fire discovery or confirmation</p></td>
</tr>
<tr>
<td><p>FIRE_SIZE</p></td>
<td><p>Estimate of the final fire size (acres)</p></td>
</tr>
<tr>
<td><p>LATITUDE</p></td>
<td><p>Latitude (NAD83) of the fire</p></td>
</tr>
<tr>
<td><p>LONGITUDE</p></td>
<td><p>Longitude (NAD83) of the fire</p></td>
</tr>
</tbody>
</table>

<p>We’ll develop a classification model to predict the cause of a fire (<code>STAT_CAUSE_CODE</code>) using the five other features as features. The target and the model are secondary; this is not an ML case study. Thus, we’re not going to focus on details such as cross-validation or hyperparameter tuning<sup><a data-type="noteref" id="idm45127447535416-marker" href="ch07.xhtml#idm45127447535416">2</a></sup>. We’ll also limit ourselves to observations from 2015 and exclude Hawaii and Alaska to reduce the data set to a more manageable size. The end product of our case study will be to produce an interactive document that will allow us to input new predictor values, as depicted in <a data-type="xref" href="#case_arch">Figure 7-1</a><sup><a data-type="noteref" id="idm45127447533464-marker" href="ch07.xhtml#idm45127447533464">3</a></sup>.</p>

<p>Before we dig in, it’s worth taking a moment to consider data lineage - from raw to product. Answering the following questions will help orientate us.</p>
<ol>
<li>
<p>What is the end product?</p>
</li>
<li>
<p>How will it be used, and by whom?</p>
</li>
<li>
<p>Can we break down the project into component pieces?</p>
</li>
<li>
<p>How will each component be built? i.e., Python or R? Which additional packages may be necessary?</p>
</li>
<li>
<p>How will these component pieces work together?</p>
</li>

</ol>

<p>Answering these questions allows us to draw a path from the raw data to the end product, hopefully avoiding bottlenecks along the way. For question 1, we’ve already stated that we want to build an interactive document. For the second question, to keep things simple, let’s assume it’s for us to easily input new feature values and see the model’s prediction.</p>

<p>Questions 3-5 are what we’ve considered in this book. In question 3, we imagine the parts as a series of steps for our overall workflow. Question 4 was addressed in <a data-type="xref" href="ch04.xhtml#ch05">Chapter 4</a> and <a data-type="xref" href="ch05.xhtml#ch06">Chapter 5</a>. We summarize those steps in <a data-type="xref" href="#csOverview">Table 7-2</a>.</p>
<table id="csOverview">
<caption><span class="label">Table 7-2. </span>The steps in our case study and their respective languages.</caption>
<thead>
<tr>
<th>Component/Step</th>
<th>Language</th>
<th>Additional packages?</th>
</tr>
</thead>
<tbody>
<tr>
<td><p>1. Data Importing</p></td>
<td><p>R</p></td>
<td><p><code>RSQLite</code>, <code>DBI</code></p></td>
</tr>
<tr>
<td><p>2. EDA &amp; Data Visualization</p></td>
<td><p>R</p></td>
<td><p><code>ggplot2</code>, <code>GGally</code>, <code>visdat</code>, <code>nanair</code></p></td>
</tr>
<tr>
<td><p>4. Feature Engineering</p></td>
<td><p>Python</p></td>
<td><p><code>scikit-learn</code></p></td>
</tr>
<tr>
<td><p>5. Machine Learning</p></td>
<td><p>Python</p></td>
<td><p><code>scikit-learn</code></p></td>
</tr>
<tr>
<td><p>6. Mapping</p></td>
<td><p>R</p></td>
<td><p><code>leaflet</code></p></td>
</tr>
<tr>
<td><p>7. Interactive web interface</p></td>
<td><p>R</p></td>
<td><p><code>shiny</code> runtime in an RMarkdown</p></td>
</tr>
</tbody>
</table>

<p>Finally, question 5 asks us to consider the project architecture. The diagram presented in <a data-type="xref" href="#case_arch">Figure 7-1</a> shows how each of the steps in <a data-type="xref" href="#csOverview">Table 7-2</a> will be linked together.</p>

<figure><div id="case_arch" class="figure">
<img src="Images/prds_0701.png" alt="" width="5367" height="2981"/>
<h6><span class="label">Figure 7-1. </span>Architecture for our case study project.</h6>
</div></figure>

<p>Alright, now that we know where we’re going, let’s choose our tools with care and assemble all the components into a unified whole.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>We prepared this case study exclusively using the RStudio IDE. As we discussed in the <a data-type="xref" href="ch06.xhtml#ch07">Chapter 6</a>, if we’re writing in R and accessing Python functions, this would be the way to go. The reason is the built-in capabilities in executing Python code chunks within RMarkdown, the features of the Environment and Plot panes, and finally, the tooling around <code>shiny</code>.</p>
</div>
</div></section>













<section data-type="sect1" data-pdf-bookmark="Setup and data import"><div class="sect1" id="idm45127447556728">
<h1>Setup and data import</h1>

<p>We can see from our diagram that our end product will be an interactive RMarkdown document. So let’s begin as we have done in <a data-type="xref" href="ch05.xhtml#ch06">Chapter 5</a>. Our YAML<sup><a data-type="noteref" id="idm45127447463752-marker" href="ch07.xhtml#idm45127447463752">4</a></sup> header will consist of at least:</p>

<pre data-type="programlisting" data-code-language="yaml"><code class="nn">---</code>
<code class="nt">title</code><code class="p">:</code> <code class="s">"R</code><code class="nv"> </code><code class="s">&amp;</code><code class="nv"> </code><code class="s">Python</code><code class="nv"> </code><code class="s">Case</code><code class="nv"> </code><code class="s">Study"</code>
<code class="nt">author</code><code class="p">:</code> <code class="s">"Python</code><code class="nv"> </code><code class="s">&amp;</code><code class="nv"> </code><code class="s">R</code><code class="nv"> </code><code class="s">for</code><code class="nv"> </code><code class="s">the</code><code class="nv"> </code><code class="s">modern</code><code class="nv"> </code><code class="s">data</code><code class="nv"> </code><code class="s">scientist"</code>
<code class="nt">runtime</code><code class="p">:</code> <code class="l-Scalar-Plain">shiny</code>
<code class="nn">---</code></pre>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>To have nicer formatting, we’ll exclude the characters specifying an RMarkdown chunk from the following examples. Naturally, if you are following along, you need to add them.</p>
</div>

<p>Since the data is stored in an SQLite database, we need to use some additional packages in addition to ones we’ve already seen. Our first code chunk is:</p>

<pre data-type="programlisting" data-code-language="r"><code class="nf">library</code><code class="p">(</code><code class="n">tidyverse</code><code class="p">)</code>
<code class="nf">library</code><code class="p">(</code><code class="n">RSQLite</code><code class="p">)</code> <code class="c1"># SQLite</code>
<code class="nf">library</code><code class="p">(</code><code class="n">DBI</code><code class="p">)</code> <code class="c1"># R Database Interface</code></pre>

<p>In our second code chunk, we’ll connect to the database and list all of the 33 available tables.</p>

<pre data-type="programlisting" data-code-language="r"><code class="c1"># Connect to an in-memory RSQLite database</code>
<code class="n">con</code> <code class="o">&lt;-</code> <code class="nf">dbConnect</code><code class="p">(</code><code class="nf">SQLite</code><code class="p">(),</code> <code class="s">"ch07/data/FPA_FOD_20170508.sqlite"</code><code class="p">)</code>

<code class="c1"># Show all tables</code>
<code class="nf">dbListTables</code><code class="p">(</code><code class="n">con</code><code class="p">)</code></pre>

<p>Creating a connection (<code>con</code>) object is a standard practice in establishing programmatic access to databases. In contrast to R, Python has built-in support for opening such files with the <code>sqlite3</code> package. This is preferable to R since we don’t need to install and load two additional packages. Nonetheless, R is a core language for the initial steps, so we might as well just import the data in R from the outset.</p>

<p>Our data is stored in the <code>Fires</code> table. As we know the columns we want to access, we can specify that while importing.</p>

<p>It’s also important to remember to close the connections when working with remote or shared databases, since that might prevent other users from accessing the database and cause issues<sup><a data-type="noteref" id="idm45127447406008-marker" href="ch07.xhtml#idm45127447406008">5</a></sup>.</p>

<pre data-type="programlisting" data-code-language="r"><code class="n">fires</code> <code class="o">&lt;-</code> <code class="nf">dbGetQuery</code><code class="p">(</code><code class="n">con</code><code class="p">,</code> <code class="s">"</code>
<code class="s">                        SELECT</code>
<code class="s">                        STAT_CAUSE_DESCR, OWNER_CODE, DISCOVERY_DOY,</code>
<code class="s">                        FIRE_SIZE, LATITUDE, LONGITUDE</code>
<code class="s">                        FROM Fires</code>
<code class="s">                        WHERE (FIRE_YEAR=2015 AND STATE != 'AK' AND STATE != 'HI');"</code><code class="p">)</code>
<code class="nf">dbDisconnect</code><code class="p">(</code><code class="n">con</code><code class="p">)</code>

<code class="nf">dim</code><code class="p">(</code><code class="n">fires</code><code class="p">)</code></pre>

<p>We limit our dataset size already at this very first importing step. It’s a shame to throw out so much data. Still, we do this since older data, especially in climate applications, tends to be less representative of the current or near-future situation. Predictions based on old data can be inherently biased. By limiting the size of the data set, we also reduce the amount of memory used, improving performance.</p>
<div data-type="tip"><h1>Performance tip</h1>
<p>Often in the case of enormous datasets (those barely or not fitting into the memory of your machine), you can use such an ingestion command to select just a sample, such as <code>LIMIT 1000</code>.</p>
</div>

<p>We can get a quick preview of the data using the <code>tidyverse</code> function <code>dplyr::glimpse()</code>:</p>

<pre data-type="programlisting" data-code-language="markdown">glimpse(fires)</pre>

<pre data-type="programlisting" data-code-language="markdown">Rows: 73,688
Columns: 6
$ STAT_CAUSE_DESCR &lt;chr&gt; "Lightning", "Lightning", "Lightning", "Lightning", "Misc…
$ OWNER_CODE       &lt;dbl&gt; 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 8, 5, 8, 5, 5, 5, …
$ DISCOVERY_DOY    &lt;int&gt; 226, 232, 195, 226, 272, 181, 146, 219, 191, 192, 191, 19…
$ FIRE_SIZE        &lt;dbl&gt; 0.10, 6313.00, 0.25, 0.10, 0.10, 0.25, 0.10, 0.10, 0.50, …
$ LATITUDE         &lt;dbl&gt; 45.93417, 45.51528, 45.72722, 45.45556, 44.41667, 46.0522…
$ LONGITUDE        &lt;dbl&gt; -113.0208, -113.2453, -112.9439, -113.7497, -112.8433, -1…</pre>
</div></section>













<section data-type="sect1" data-pdf-bookmark="EDA &amp; Data Visualization"><div class="sect1" id="idm45127447298312">
<h1>EDA &amp; Data Visualization</h1>

<p>Since the dataset is still relatively large, we should think carefully about the best data visualization strategy. Our first instinct may be to plot a map since we have latitude and longitude coordinates. This can be fed into <code>ggplot2</code> directly as x and y-axis coordinates as such:</p>

<pre data-type="programlisting" data-code-language="r"><code class="n">g</code> <code class="o">&lt;-</code> <code class="nf">ggplot</code><code class="p">(</code><code class="n">fires</code><code class="p">,</code> <code class="nf">aes</code><code class="p">(</code><code class="n">x</code> <code class="o">=</code> <code class="n">LONGITUDE</code><code class="p">,</code>
                  <code class="n">y</code> <code class="o">=</code> <code class="n">LATITUDE</code><code class="p">,</code>
                  <code class="n">size</code> <code class="o">=</code> <code class="n">FIRE_SIZE</code><code class="p">,</code>
                  <code class="n">color</code> <code class="o">=</code> <code class="nf">factor</code><code class="p">(</code><code class="n">OWNER_CODE</code><code class="p">)))</code> <code class="o">+</code>
  <code class="nf">geom_point</code><code class="p">(</code><code class="n">alpha</code> <code class="o">=</code> <code class="m">0.15</code><code class="p">,</code> <code class="n">shape</code> <code class="o">=</code> <code class="m">16</code><code class="p">)</code> <code class="o">+</code>
  <code class="nf">scale_size</code><code class="p">(</code><code class="n">range</code> <code class="o">=</code> <code class="nf">c</code><code class="p">(</code><code class="m">0.5</code><code class="p">,</code> <code class="m">10</code><code class="p">))</code> <code class="o">+</code>
  <code class="nf">theme_classic</code><code class="p">()</code> <code class="o">+</code>
  <code class="nf">theme</code><code class="p">(</code><code class="n">legend.position</code> <code class="o">=</code> <code class="s">"bottom"</code><code class="p">,</code>
        <code class="n">panel.background</code> <code class="o">=</code> <code class="nf">element_rect</code><code class="p">(</code><code class="n">fill</code> <code class="o">=</code> <code class="s">"grey10"</code><code class="p">))</code>

<code class="n">g</code></pre>

<figure><div id="bubble_1" class="figure">
<img src="Images/prds_0702.png" alt="" width="3600" height="2400"/>
<h6><span class="label">Figure 7-2. </span>Plotting the sizes of individual fires.</h6>
</div></figure>

<p>By mapping <code>OWNER_CODE</code> onto the color aesthetic, we can see a strong correlation in some states. We can predict that this will have a substantial effect on our model’s performance. In the above code snippet, we assigned the plot to the object <code>g</code>. This is not strictly necessary, but we did it in this case to showcase the strength of the <code>ggplot2</code> layering method. We can add a <code>facet_wrap()</code> layer to this plot and separate it into 13 facets, or <em>small multiples</em>, one for each type of <code>STAT_CAUSE_DESCR</code>.</p>

<pre data-type="programlisting" data-code-language="r"><code class="n">g</code> <code class="o">+</code>
  <code class="nf">facet_wrap</code><code class="p">(</code><code class="n">facets</code> <code class="o">=</code> <code class="nf">vars</code><code class="p">(</code><code class="n">STAT_CAUSE_DESCR</code><code class="p">),</code> <code class="n">nrow</code> <code class="o">=</code> <code class="m">4</code><code class="p">)</code></pre>

<figure><div id="bubble_2" class="figure">
<img src="Images/prds_0703.png" alt="" width="4800" height="3600"/>
<h6><span class="label">Figure 7-3. </span>Faceting the fires plot, based on the fire cause.</h6>
</div></figure>

<p>This allows us to appreciate that some causes are abundant while others are rare, an observation we’ll see again shortly in a different way. We can also begin to assess any strong associations between, e.g., region, owner code, and cause of a fire.</p>

<p>Returning to the entirety of the data set, an easy way to get a comprehensive overview is to use a pairs plot, sometimes called a splom (or “scatter plot matrix” if it consists of purely numeric data). The <code>GGally</code> package<sup><a data-type="noteref" id="idm45127447143208-marker" href="ch07.xhtml#idm45127447143208">6</a></sup> provides an exceptional function, <code>ggpairs()</code> that produces a matrix of plots. Each pair-wise bi-variate plot is shown as univariate density plots or histograms on the diagonal. In the upper triangle, the correlation between continuous features is available.</p>

<pre data-type="programlisting" data-code-language="r"><code class="nf">library</code><code class="p">(</code><code class="n">GGally</code><code class="p">)</code>
<code class="n">fires</code> <code class="o">%&gt;%</code>
  <code class="nf">ggpairs</code><code class="p">()</code></pre>

<figure><div id="pairsplot" class="figure">
<img src="Images/prds_0704.png" alt="" width="3600" height="3600"/>
<h6><span class="label">Figure 7-4. </span>A pairs plot.</h6>
</div></figure>

<p>This information-rich visualization demands some time to process. It’s handy as an <em>exploratory</em> plot, in EDA, but not necessarily as an <em>explanatory</em> in reporting our results. Can you spot any unusual patterns? First, <code>STAT_CAISE_DESCR</code> looks imbalanced<sup><a data-type="noteref" id="idm45127447128744-marker" href="ch07.xhtml#idm45127447128744">7</a></sup>, meaning there is a significant difference between the number of observations per class. Additionally, <code>OWNER_CODE</code> appears to be bimodal (having two maxima). Those properties can negatively affect our analysis depending on which model we choose. Second, all correlations seem to be relatively low, making our job easier (since correlated data is not good for ML). Still, we already know there is a strong association between location (<code>LATITUDE</code> &amp; <code>LONGITUDE</code>) and OWNER CODE from our previous plot. So we should take these correlations with a grain of salt. We would expect to detect this issue in feature engineering. Third, <code>FIRE_SIZE</code> has a very unusual distribution. It looks like that plot is empty, with just the x and y axes present. We see a density plot with a very high and narrow peak at the very low range and an extremely long positive skew. We can quickly generate a <code>log10</code> transformed density plot:</p>

<pre data-type="programlisting" data-code-language="r"><code class="nf">ggplot</code><code class="p">(</code><code class="n">fires</code><code class="p">,</code> <code class="nf">aes</code><code class="p">(</code><code class="n">FIRE_SIZE</code><code class="p">))</code> <code class="o">+</code>
  <code class="nf">geom_density</code><code class="p">()</code> <code class="o">+</code>
  <code class="nf">scale_x_log10</code><code class="p">()</code></pre>

<figure><div id="log10_size" class="figure">
<img src="Images/prds_0705.png" alt="" width="3600" height="3600"/>
<h6><span class="label">Figure 7-5. </span>Density plot of the log-transformed <code>FIRE_SIZE</code> feature.</h6>
</div></figure>
<div data-type="tip"><h1>Additional visualizations</h1>
<p>For the case study, we’ll keep the tasks to a minimum, but there might be a few other interesting things to visualize that can help tell a story for the end-user. For example, note that the dataset has a temporal dimension. It would be interesting how forest fires’ quantity (and quality) has been changing over time. We’ll leave this to the motivated user to explore with the excellent <code>gganimate</code> package.</p>
</div>

<p>Interactive data visualization is often overused, without a special purpose in mind. Even for the most popular packages, the documentation shows just basic usage. In our case, since we have so many data points in a spatial setting, and we want to have a final deliverable that is accessible, creating an interactive map is an obvious choice. As in <a data-type="xref" href="ch05.xhtml#ch06">Chapter 5</a> we use <code>leaflet</code>:</p>

<pre data-type="programlisting" data-code-language="r"><code class="nf">library</code><code class="p">(</code><code class="n">leaflet</code><code class="p">)</code>

<code class="nf">leaflet</code><code class="p">()</code> <code class="o">%&gt;%</code>
  <code class="nf">addTiles</code><code class="p">()</code> <code class="o">%&gt;%</code>
  <code class="nf">addMarkers</code><code class="p">(</code><code class="n">lng</code> <code class="o">=</code> <code class="n">df</code><code class="o">$</code><code class="n">LONGITUDE</code><code class="p">,</code> <code class="n">lat</code> <code class="o">=</code> <code class="n">df</code><code class="o">$</code><code class="n">LATITUDE</code><code class="p">,</code>
  <code class="n">clusterOptions</code> <code class="o">=</code> <code class="nf">markerClusterOptions</code><code class="p">()</code>
<code class="p">)</code></pre>

<figure><div id="case_leaflet" class="figure">
<img src="Images/prds_0706.png" alt="" width="656" height="490"/>
<h6><span class="label">Figure 7-6. </span>Interactive map showing the locations of forest fires.</h6>
</div></figure>

<p>Note how using <code>clusterOptions</code> allows us to simultaneously present all of the data without overwhelming the user or reducing visibility. For our purposes, this satisfies our curiosity using some great visualizations in EDA. There are plenty of other statistics we can apply, but let’s move machine learning in Python.</p>
</div></section>













<section data-type="sect1" data-pdf-bookmark="Machine Learning"><div class="sect1" id="idm45127447301272">
<h1>Machine Learning</h1>

<p>By now, we have some idea about the factors that may influence the cause of a fire. Let’s dive into building a machine learning model using <code>scikit-learn</code> in Python<sup><a data-type="noteref" id="idm45127446950648-marker" href="ch07.xhtml#idm45127446950648">8</a></sup>.</p>

<p>We argued that ML is best done in Python as we saw in <a data-type="xref" href="ch05.xhtml#ch06">Chapter 5</a>. We’ll use a Random Forest algorithm. There are several reasons for this choice:</p>
<ol>
<li>
<p>It’s a well-established algorithm</p>
</li>
<li>
<p>It’s relatively easy to understand</p>
</li>
<li>
<p>It does not require feature scaling before training</p>
</li>

</ol>

<p>There are other reasons why it’s good, such as working well with missing data and having out-of-the-box explainability.</p>








<section data-type="sect2" data-pdf-bookmark="Setting up our Python Environment"><div class="sect2" id="idm45127446943832">
<h2>Setting up our Python Environment</h2>

<p>As discussed in <a data-type="xref" href="ch06.xhtml#ch07">Chapter 6</a>, there are a few ways to access Python using the <code>reticulate</code> package. The choice depends on the circumstances, which we laid out in our project architecture. Here, we’ll pass our R <code>data.frame</code> to a Python virtual environment. If you followed the steps in <a data-type="xref" href="ch06.xhtml#ch07">Chapter 6</a>, you’d already have the <code>modern_data</code> virtual environment set up. We already installed some packages into this environment. To recap, we executed the following commands:</p>

<pre data-type="programlisting" data-code-language="r"><code class="nf">library</code><code class="p">(</code><code class="n">reticulate</code><code class="p">)</code>

<code class="c1"># Create a new virtualenv</code>
<code class="nf">virtualenv_create</code><code class="p">(</code><code class="s">"modern_data"</code><code class="p">)</code>

<code class="c1"># Install Python packages into this virtualenv</code>
<code class="nf">library</code><code class="p">(</code><code class="n">tidyverse</code><code class="p">)</code>
<code class="nf">c</code><code class="p">(</code><code class="s">"scikit-learn"</code><code class="p">,</code> <code class="s">"pandas"</code><code class="p">,</code> <code class="s">"seaborn"</code><code class="p">)</code> <code class="o">%&gt;%</code>
  <code class="n">purrr</code><code class="o">::</code><code class="nf">map</code><code class="p">(</code><code class="o">~</code> <code class="nf">virtualenv_install</code><code class="p">(</code><code class="s">"modern_data"</code><code class="p">,</code> <code class="n">.)</code><code class="p">)</code></pre>

<p>If you don’t have the <code>modern_data</code> virtualenv or you’re using Windows, please refer to the steps in the files <code>0 - setup.R</code> and <code>1 - activate.R</code> and discussed in <a data-type="xref" href="ch06.xhtml#ch07">Chapter 6</a>. You may want to restart R at this point to make sure that you’ll be able to activate your virtual environment using the following command:</p>

<pre data-type="programlisting" data-code-language="r"><code class="c1"># Activate virtual environment</code>
<code class="nf">use_virtualenv</code><code class="p">(</code><code class="s">"modern_data"</code><code class="p">,</code> <code class="n">required</code> <code class="o">=</code> <code class="kc">TRUE</code><code class="p">)</code>

<code class="c1"># If using miniconda (windows)</code>
<code class="c1"># use_condaenv("modern_data")</code></pre>

<p>We’ll include all the Python steps into a single script; you can find this script in the book <a href="https://github.com/moderndatadesign/PyR4MDS">repository</a> under <code>ml.py</code>. First, we’ll import the necessary modules.</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.ensemble</code> <code class="kn">import</code> <code class="n">RandomForestClassifier</code>
<code class="kn">from</code> <code class="nn">sklearn.</code> <code class="nn">preprocessing</code> <code class="kn">import</code> <code class="n">LabelEncoder</code>
<code class="kn">from</code> <code class="nn">sklearn.model_selection</code> <code class="kn">import</code> <code class="n">train_test_split</code>
<code class="kn">from</code> <code class="nn">sklearn</code> <code class="kn">import</code> <code class="n">metrics</code></pre>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Feature engineering"><div class="sect2" id="idm45127446855448">
<h2>Feature engineering</h2>

<p>There are features in the dataset that might be informative to a data analyst but are at best useless for training the model, and at worst - can reduce its accuracy. This is called “adding noise” to the dataset, and we want to avoid it at all costs. This is the purpose behind feature engineering. Let’s select just the features we need, as specified in <a data-type="xref" href="#csFeatures">Table 7-1</a>. We also use standard ML convention in storing them in <code>X</code>, and our target in ‘y’.</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">features</code> <code class="o">=</code> <code class="p">[</code><code class="s2">"OWNER_CODE"</code><code class="p">,</code> <code class="s2">"DISCOVERY_DOY"</code><code class="p">,</code> <code class="s2">"FIRE_SIZE"</code><code class="p">,</code> <code class="s2">"LATITUDE"</code><code class="p">,</code> <code class="s2">"LONGITUDE"</code><code class="p">]</code>
<code class="n">X</code> <code class="o">=</code> <code class="n">df</code><code class="p">[</code><code class="n">features</code><code class="p">]</code>
<code class="n">y</code> <code class="o">=</code> <code class="n">df</code><code class="p">[</code><code class="s2">"STAT_CAUSE_DESCR"</code><code class="p">]</code></pre>

<p>Here, we create an instance of the <code>LaberEncoder</code>. We use this to encode a categorical feature to numeric. In our case, we apply it to our target:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">le</code> <code class="o">=</code> <code class="n">LabelEncoder</code><code class="p">()</code>
<code class="n">y</code> <code class="o">=</code> <code class="n">le</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">y</code><code class="p">)</code></pre>

<p>Here, we split the dataset into a training and a test set (note that we are also using the handy <code>stratify</code> parameter to make sure the splitting function samples our imbalanced classes fairly):</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">X_train</code><code class="p">,</code> <code class="n">X_test</code><code class="p">,</code> <code class="n">y_train</code><code class="p">,</code> <code class="n">y_test</code> <code class="o">=</code> <code class="n">train_test_split</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">,</code> <code class="n">test_size</code><code class="o">=</code><code class="mf">0.33</code><code class="p">,</code>
                                                    <code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">,</code> <code class="n">stratify</code><code class="o">=</code><code class="n">y</code><code class="p">)</code></pre>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Model training"><div class="sect2" id="idm45127446753816">
<h2>Model training</h2>

<p>To apply the Random Forest classifier, we’ll make an instance of <code>RandomForestClassifier</code>. As in <a data-type="xref" href="ch05.xhtml#ch06">Chapter 5</a> we use the <code>fit/predict</code> paradigm and store the predicted values in <code>preds</code>:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">clf</code> <code class="o">=</code> <code class="n">RandomForestClassifier</code><code class="p">()</code>

<code class="n">clf</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code>

<code class="n">preds</code> <code class="o">=</code> <code class="n">clf</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X_test</code><code class="p">)</code></pre>

<p>In the final step, we’ll assign the confusion matrix and the accuracy score to objects.</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">conmat</code> <code class="o">=</code> <code class="n">metrics</code><code class="o">.</code><code class="n">confusion_matrix</code><code class="p">(</code><code class="n">y_test</code><code class="p">,</code> <code class="n">preds</code><code class="p">)</code>
<code class="n">acc</code> <code class="o">=</code> <code class="n">metrics</code><code class="o">.</code><code class="n">accuracy_score</code><code class="p">(</code><code class="n">y_test</code><code class="p">,</code> <code class="n">preds</code><code class="p">)</code></pre>

<p>After we have complete our script, we can source it into R:</p>

<pre data-type="programlisting" data-code-language="r"><code class="nf">source_python</code><code class="p">(</code><code class="s">"ml.py"</code><code class="p">)</code></pre>

<p>After running this command, we’ll have access to all the Python objects directly in our environment. The accuracy is <code>0.58</code>, which is not phenomenal, but certainly much better than random!</p>
<div data-type="tip"><h1>The power of sourcing Python scripts</h1>
<p>When we use the <code>source_python</code> function from <code>reticulate</code> we can significantly increase our productivity, especially if we are working in a bilingual team. Imagine the scenario when a coworker of yours builds the ML part in Python and you need to include their work in yours. It would be as easy as sourcing without worrying about re-coding everything. This scenario is also plausible when joining a new company or project and inheriting Python code that you need to use straight away.</p>
</div>

<p>If we want to take advantage of <code>ggplot</code> to examine the confusion matrix, we first need to convert to an R <code>data.frame</code>. The <code>value</code> is then the number of observations of each case, which we map onto <code>size</code>, and change the <code>shape</code> to 1 (a circle). The result is shown on <a data-type="xref" href="#conf_mat_plot">Figure 7-7</a>.</p>

<pre data-type="programlisting" data-code-language="r"><code class="nf">library</code><code class="p">(</code><code class="n">ggplot2</code><code class="p">)</code>
<code class="n">py</code><code class="o">$</code><code class="n">conmat</code> <code class="o">%&gt;%</code>
  <code class="nf">as.data.frame.table</code><code class="p">(</code><code class="n">responseName</code> <code class="o">=</code> <code class="s">"value"</code><code class="p">)</code> <code class="o">%&gt;%</code>
  <code class="nf">ggplot</code><code class="p">(</code><code class="nf">aes</code><code class="p">(</code><code class="n">Var1</code><code class="p">,</code> <code class="n">Var2</code><code class="p">,</code> <code class="n">size</code> <code class="o">=</code> <code class="n">value</code><code class="p">))</code> <code class="o">+</code>
  <code class="nf">geom_point</code><code class="p">(</code><code class="n">shape</code> <code class="o">=</code> <code class="m">1</code><code class="p">)</code></pre>

<figure><div id="conf_mat_plot" class="figure">
<img src="Images/prds_0707.png" alt="" width="852" height="528"/>
<h6><span class="label">Figure 7-7. </span>Plot of the classifier confusion matrix.</h6>
</div></figure>

<p>It’s not surprising that we have some groups with a very high match since we already knew that our data was imbalanced to begin with. Now, what do we do with this nice Python code and output? At the end of <a data-type="xref" href="ch06.xhtml#ch07">Chapter 6</a>, we saw a simple and effective way to create an interactive document (remember what you learned in <a data-type="xref" href="ch05.xhtml#ch06">Chapter 5</a>) using an RMarkdown with a <code>shiny</code> runtime. Let’s implement the same concept here.</p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Prediction and UI"><div class="sect1" id="shiny_case">
<h1>Prediction and UI</h1>

<p>Once we have established a Python model, it’s general practice to test it with mock input. This allows us to ensure our model can handle the correct input data and is standard practice in ML engineering before connecting it with real user input. To this end, we’ll create five <code>sliderInputs</code> for the five features of our model. Here, we’ve hard-coded the min and max values for the sake of simplicity, but these can, of course, be dynamic.</p>

<pre data-type="programlisting" data-code-language="r"><code class="nf">sliderInput</code><code class="p">(</code><code class="s">"OWNER_CODE"</code><code class="p">,</code> <code class="s">"Owner code:"</code><code class="p">,</code>
            <code class="n">min</code> <code class="o">=</code> <code class="m">1</code><code class="p">,</code> <code class="n">max</code> <code class="o">=</code> <code class="m">15</code><code class="p">,</code> <code class="n">value</code> <code class="o">=</code> <code class="m">1</code><code class="p">)</code>
<code class="nf">sliderInput</code><code class="p">(</code><code class="s">"DISCOVERY_DOY"</code><code class="p">,</code> <code class="s">"Day of the year:"</code><code class="p">,</code>
            <code class="n">min</code> <code class="o">=</code> <code class="m">1</code><code class="p">,</code> <code class="n">max</code> <code class="o">=</code> <code class="m">365</code><code class="p">,</code> <code class="n">value</code> <code class="o">=</code> <code class="m">36</code><code class="p">)</code>
<code class="nf">sliderInput</code><code class="p">(</code><code class="s">"FIRE_SIZE"</code><code class="p">,</code> <code class="s">"Number of bins (log10):"</code><code class="p">,</code>
            <code class="n">min</code> <code class="o">=</code> <code class="m">-4</code><code class="p">,</code> <code class="n">max</code> <code class="o">=</code> <code class="m">6</code><code class="p">,</code> <code class="n">value</code> <code class="o">=</code> <code class="m">1</code><code class="p">)</code>
<code class="nf">sliderInput</code><code class="p">(</code><code class="s">"LATITUDE"</code><code class="p">,</code> <code class="s">"latitude:"</code><code class="p">,</code>
            <code class="n">min</code> <code class="o">=</code> <code class="m">17.965571</code><code class="p">,</code> <code class="n">max</code> <code class="o">=</code> <code class="m">48.9992</code><code class="p">,</code> <code class="n">value</code> <code class="o">=</code> <code class="m">30</code><code class="p">)</code>
<code class="nf">sliderInput</code><code class="p">(</code><code class="s">"LONGITUDE"</code><code class="p">,</code> <code class="s">"Longitude:"</code><code class="p">,</code>
            <code class="n">min</code> <code class="o">=</code> <code class="m">-124.6615</code><code class="p">,</code> <code class="n">max</code> <code class="o">=</code> <code class="m">-65.321389</code><code class="p">,</code> <code class="n">value</code> <code class="o">=</code> <code class="m">30</code><code class="p">)</code></pre>

<p>Similar to what we did at the end of <a data-type="xref" href="ch06.xhtml#ch07">Chapter 6</a>, we’ll access these values in the internal <code>input</code> list and use a <code>shiny</code> package function to render the appropriate output.</p>

<pre data-type="programlisting" data-code-language="r"><code class="n">prediction</code> <code class="o">&lt;-</code> <code class="nf">renderText</code><code class="p">({</code>
  <code class="n">input_df</code> <code class="o">&lt;-</code> <code class="nf">data.frame</code><code class="p">(</code><code class="n">OWNER_CODE</code> <code class="o">=</code> <code class="n">input</code><code class="o">$</code><code class="n">OWNER_CODE</code><code class="p">,</code>
                         <code class="n">DISCOVERY_DOY</code> <code class="o">=</code> <code class="n">input</code><code class="o">$</code><code class="n">DISCOVERY_DOY</code><code class="p">,</code>
                         <code class="n">FIRE_SIZE</code> <code class="o">=</code> <code class="n">input</code><code class="o">$</code><code class="n">FIRE_SIZE</code><code class="p">,</code>
                         <code class="n">LATITUDE</code> <code class="o">=</code> <code class="n">input</code><code class="o">$</code><code class="n">LATITUDE</code><code class="p">,</code>
                         <code class="n">LONGITUDE</code> <code class="o">=</code> <code class="n">input</code><code class="o">$</code><code class="n">LONGITUDE</code><code class="p">)</code>

  <code class="n">clf</code><code class="o">$</code><code class="nf">predict</code><code class="p">(</code><code class="nf">r_to_py</code><code class="p">(</code><code class="n">input_df</code><code class="p">))</code>
<code class="p">})</code></pre>

<figure><div id="case_study_shiny" class="figure">
<img src="Images/prds_0708.png" alt="" width="409" height="788"/>
<h6><span class="label">Figure 7-8. </span>The result of our case study.</h6>
</div></figure>

<p>Those elements will respond dynamically to changes in user input. This is precisely what we need for our work since this is an interactive product and not a static one. You can see all of the different code blocks that we used in preparation for this project. They should require little change, with the most notable one being the ability to capture the user input in the inference part. This can be done by accessing the <code>input</code> object.</p>
</div></section>













<section data-type="sect1" data-pdf-bookmark="Final thoughts"><div class="sect1" id="idm45127446208424">
<h1>Final thoughts</h1>

<p>In this case study, we demonstrated how one could take the best of both worlds and combine such excellent tools that modern data scientists have at our disposal to create remarkable user experiences, which delight visually and inform decision-making. This is but a basic example of such an elegant system, and we are confident that by showing you what’s possible, you - our readers - will create the data science products of the future!</p>
</div></section>







<div data-type="footnotes"><p data-type="footnote" id="idm45127447555192"><sup><a href="ch07.xhtml#idm45127447555192-marker">1</a></sup> Short, Karen C. 2017. Spatial wildfire occurrence data for the United States, 1992-2015, FPA_FOD_20170508. 4th Edition. Fort Collins, CO: Forest Service Research Data Archive. <a href="https://doi.org/10.2737/RDS-2013-0009.4"><em class="hyperlink">https://doi.org/10.2737/RDS-2013-0009.4</em></a></p><p data-type="footnote" id="idm45127447535416"><sup><a href="ch07.xhtml#idm45127447535416-marker">2</a></sup> We’ll leave a thorough development of a robust classification model to our motivated readers. Indeed you may also be interested in a regression that predicts the final fire size in acres.  Curious readers will note that a few interesting notebooks are available on Kaggle to get you started.</p><p data-type="footnote" id="idm45127447533464"><sup><a href="ch07.xhtml#idm45127447533464-marker">3</a></sup> This is a far cry from developing, hosting, and deploying robust ML models, which, in any case, is not the focus of this book.</p><p data-type="footnote" id="idm45127447463752"><sup><a href="ch07.xhtml#idm45127447463752-marker">4</a></sup> Some readers might not be familiar with this language. It is commonly used to specify configuration options as code, such as in this case.</p><p data-type="footnote" id="idm45127447406008"><sup><a href="ch07.xhtml#idm45127447406008-marker">5</a></sup> This part can also be done very well within R, by using packages such as <code>dbplyr</code> or the using the Connections panel in RStudio.</p><p data-type="footnote" id="idm45127447143208"><sup><a href="ch07.xhtml#idm45127447143208-marker">6</a></sup> This package is used to extend the <code>ggplot2</code> functionality for transformed datasets.</p><p data-type="footnote" id="idm45127447128744"><sup><a href="ch07.xhtml#idm45127447128744-marker">7</a></sup> As another example of the modularity of the Python ML ecosystem have a look at the <code>imbalanced-learn</code> package <a href="https://imbalanced-learn.org/">here</a> if you are looking for a solution to this.</p><p data-type="footnote" id="idm45127446950648"><sup><a href="ch07.xhtml#idm45127446950648-marker">8</a></sup> This is not a thorough exposition of all possible methods or optimizations since our focus is on building a bilingual workflow, not exploring machine learning techniques in detail. Readers may choose to refer to the official <code>scikit-learn</code> documentation for further guidance, in the aptly named <a href="https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html">“Choosing the right estimator”</a></p></div></div></section></div></body></html>