<html><head></head><body>
<div id="sbo-rt-content"><section data-pdf-bookmark="Chapter 42. In Depth: Linear Regression" data-type="chapter" epub:type="chapter"><div class="chapter" id="section-0506-linear-regression">
<h1><span class="label">Chapter 42. </span>In Depth: Linear Regression</h1>
<p><a data-primary="linear regression (in machine learning)" data-type="indexterm" id="ix_ch42-asciidoc0"/><a data-primary="machine learning" data-secondary="linear regression" data-type="indexterm" id="ix_ch42-asciidoc1"/>Just as naive Bayes (discussed in <a data-type="xref" href="ch41.xhtml#section-0505-naive-bayes">Chapter 41</a>) is a good starting point for classification
tasks, linear regression models are a good starting point for regression
tasks. Such models are popular because they can be fit quickly and are
straightforward to interpret. You are already familiar with the simplest
form of linear regression model (i.e., fitting a straight line to
two-dimensional data), but such models can be extended to model more
complicated data behavior.</p>
<p>In this chapter we will start with a quick walkthrough of the
mathematics behind this well-known problem, before moving on to see how
linear models can be generalized to account for more complicated
patterns in data.</p>
<p>We begin with the standard imports:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">1</code><code class="p">]:</code> <code class="o">%</code><code class="k">matplotlib</code> inline
        <code class="kn">import</code> <code class="nn">matplotlib.pyplot</code> <code class="k">as</code> <code class="nn">plt</code>
        <code class="n">plt</code><code class="o">.</code><code class="n">style</code><code class="o">.</code><code class="n">use</code><code class="p">(</code><code class="s1">'seaborn-whitegrid'</code><code class="p">)</code>
        <code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code></pre>
<section data-pdf-bookmark="Simple Linear Regression" data-type="sect1"><div class="sect1" id="ch_0506-linear-regression_simple-linear-regression">
<h1>Simple Linear Regression</h1>
<p><a data-primary="linear regression (in machine learning)" data-secondary="simple" data-type="indexterm" id="ix_ch42-asciidoc2"/><a data-primary="simple linear regression" data-type="indexterm" id="ix_ch42-asciidoc3"/>We will start with the most familiar linear regression, a straight-line
fit to data. A straight-line fit is a model of the form:</p>
<div data-type="equation">
<math alttext="y equals a x plus b" display="block">
<mrow>
<mi>y</mi>
<mo>=</mo>
<mi>a</mi>
<mi>x</mi>
<mo>+</mo>
<mi>b</mi>
</mrow>
</math>
</div>
<p>where <math alttext="a">
<mi>a</mi>
</math> is commonly known as the <em>slope</em>, and
<math alttext="b">
<mi>b</mi>
</math> is commonly known as the <em>intercept</em>.</p>
<p>Consider the following data, which is scattered about a line with a
slope of 2 and an intercept of –5 (see <a data-type="xref" href="#fig_0506-linear-regression_files_in_output_4_0">Figure 42-1</a>).</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">2</code><code class="p">]:</code> <code class="n">rng</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">RandomState</code><code class="p">(</code><code class="mi">1</code><code class="p">)</code>
        <code class="n">x</code> <code class="o">=</code> <code class="mi">10</code> <code class="o">*</code> <code class="n">rng</code><code class="o">.</code><code class="n">rand</code><code class="p">(</code><code class="mi">50</code><code class="p">)</code>
        <code class="n">y</code> <code class="o">=</code> <code class="mi">2</code> <code class="o">*</code> <code class="n">x</code> <code class="o">-</code> <code class="mi">5</code> <code class="o">+</code> <code class="n">rng</code><code class="o">.</code><code class="n">randn</code><code class="p">(</code><code class="mi">50</code><code class="p">)</code>
        <code class="n">plt</code><code class="o">.</code><code class="n">scatter</code><code class="p">(</code><code class="n">x</code><code class="p">,</code> <code class="n">y</code><code class="p">);</code></pre>
<figure class="width-75"><div class="figure" id="fig_0506-linear-regression_files_in_output_4_0">
<img alt="output 4 0" height="194" src="assets/output_4_0.png" width="600"/>
<h6><span class="label">Figure 42-1. </span>Data for linear regression</h6>
</div></figure>
<p>We can use Scikit-Learn’s <code>LinearRegression</code> estimator to
fit this data and construct the best-fit line, as shown in <a data-type="xref" href="#fig_0506-linear-regression_files_in_output_6_0">Figure 42-2</a>.</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">3</code><code class="p">]:</code> <code class="kn">from</code> <code class="nn">sklearn.linear_model</code> <code class="kn">import</code> <code class="n">LinearRegression</code>
        <code class="n">model</code> <code class="o">=</code> <code class="n">LinearRegression</code><code class="p">(</code><code class="n">fit_intercept</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>

        <code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">x</code><code class="p">[:,</code> <code class="n">np</code><code class="o">.</code><code class="n">newaxis</code><code class="p">],</code> <code class="n">y</code><code class="p">)</code>

        <code class="n">xfit</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">linspace</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="mi">10</code><code class="p">,</code> <code class="mi">1000</code><code class="p">)</code>
        <code class="n">yfit</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">xfit</code><code class="p">[:,</code> <code class="n">np</code><code class="o">.</code><code class="n">newaxis</code><code class="p">])</code>

        <code class="n">plt</code><code class="o">.</code><code class="n">scatter</code><code class="p">(</code><code class="n">x</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code>
        <code class="n">plt</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">xfit</code><code class="p">,</code> <code class="n">yfit</code><code class="p">);</code></pre>
<figure class="width-75"><div class="figure" id="fig_0506-linear-regression_files_in_output_6_0">
<img alt="output 6 0" height="383" src="assets/output_6_0.png" width="600"/>
<h6><span class="label">Figure 42-2. </span>A simple linear regression model</h6>
</div></figure>
<p>The slope and intercept of the data are contained in the
model’s fit parameters, which in Scikit-Learn are always
marked by a trailing underscore. Here the relevant parameters are
<code>coef_</code> and <code>intercept_</code>:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">4</code><code class="p">]:</code> <code class="nb">print</code><code class="p">(</code><code class="s2">"Model slope:    "</code><code class="p">,</code> <code class="n">model</code><code class="o">.</code><code class="n">coef_</code><code class="p">[</code><code class="mi">0</code><code class="p">])</code>
        <code class="nb">print</code><code class="p">(</code><code class="s2">"Model intercept:"</code><code class="p">,</code> <code class="n">model</code><code class="o">.</code><code class="n">intercept_</code><code class="p">)</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">4</code><code class="p">]:</code> <code class="n">Model</code> <code class="n">slope</code><code class="p">:</code>     <code class="mf">2.0272088103606953</code>
        <code class="n">Model</code> <code class="n">intercept</code><code class="p">:</code> <code class="o">-</code><code class="mf">4.998577085553204</code></pre>
<p>We see that the results are very close to the values used to generate
the data, as we might hope.</p>
<p>The <code>LinearRegression</code> estimator is much more capable than this,
however—in addition to simple straight-line fits, it can also handle
multidimensional linear models of the form:</p>
<div data-type="equation">
<math alttext="y equals a 0 plus a 1 x 1 plus a 2 x 2 plus ellipsis" display="block">
<mrow>
<mi>y</mi>
<mo>=</mo>
<msub><mi>a</mi> <mn>0</mn> </msub>
<mo>+</mo>
<msub><mi>a</mi> <mn>1</mn> </msub>
<msub><mi>x</mi> <mn>1</mn> </msub>
<mo>+</mo>
<msub><mi>a</mi> <mn>2</mn> </msub>
<msub><mi>x</mi> <mn>2</mn> </msub>
<mo>+</mo>
<mo>⋯</mo>
</mrow>
</math>
</div>
<p>where there are multiple <math alttext="x">
<mi>x</mi>
</math> values. Geometrically, this
is akin to fitting a plane to points in three dimensions, or fitting a
hyperplane to points in higher dimensions.</p>
<p>The multidimensional nature of such regressions makes them more
difficult to visualize, but we can see one of these fits in action by
building some example data, using NumPy’s matrix
multiplication operator:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">5</code><code class="p">]:</code> <code class="n">rng</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">RandomState</code><code class="p">(</code><code class="mi">1</code><code class="p">)</code>
        <code class="n">X</code> <code class="o">=</code> <code class="mi">10</code> <code class="o">*</code> <code class="n">rng</code><code class="o">.</code><code class="n">rand</code><code class="p">(</code><code class="mi">100</code><code class="p">,</code> <code class="mi">3</code><code class="p">)</code>
        <code class="n">y</code> <code class="o">=</code> <code class="mf">0.5</code> <code class="o">+</code> <code class="n">np</code><code class="o">.</code><code class="n">dot</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="p">[</code><code class="mf">1.5</code><code class="p">,</code> <code class="o">-</code><code class="mf">2.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">])</code>

        <code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code>
        <code class="nb">print</code><code class="p">(</code><code class="n">model</code><code class="o">.</code><code class="n">intercept_</code><code class="p">)</code>
        <code class="nb">print</code><code class="p">(</code><code class="n">model</code><code class="o">.</code><code class="n">coef_</code><code class="p">)</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">5</code><code class="p">]:</code> <code class="mf">0.50000000000001</code>
        <code class="p">[</code> <code class="mf">1.5</code> <code class="o">-</code><code class="mf">2.</code>   <code class="mf">1.</code> <code class="p">]</code></pre>
<p>Here the <math alttext="y">
<mi>y</mi>
</math> data is constructed from a linear combination
of three random <math alttext="x">
<mi>x</mi>
</math> values, and the linear regression
recovers the coefficients used to construct the data.</p>
<p>In this way, we can use the single <code>LinearRegression</code> estimator to fit
lines, planes, or hyperplanes to our data. It still appears that this
approach would be limited to strictly linear relationships between
variables, but it turns out we can relax this as well.<a data-startref="ix_ch42-asciidoc3" data-type="indexterm" id="idm45858736554112"/><a data-startref="ix_ch42-asciidoc2" data-type="indexterm" id="idm45858736553408"/></p>
</div></section>
<section data-pdf-bookmark="Basis Function Regression" data-type="sect1"><div class="sect1" id="ch_0506-linear-regression_basis-function-regression">
<h1>Basis Function Regression</h1>
<p><a data-primary="basis function regression" data-type="indexterm" id="ix_ch42-asciidoc4"/><a data-primary="linear regression (in machine learning)" data-secondary="basis function regression" data-type="indexterm" id="ix_ch42-asciidoc5"/>One trick you can use to adapt linear regression to nonlinear
relationships between variables is to transform the data according to
<em>basis functions</em>. We have seen one version of this before, in the
<code>PolynomialRegression</code> pipeline used in Chapters
<a href="ch39.xhtml#section-0503-hyperparameters-and-model-validation">39</a> and <a href="ch40.xhtml#section-0504-feature-engineering">40</a>. The idea is to take our multidimensional linear model:</p>
<div data-type="equation">
<math alttext="y equals a 0 plus a 1 x 1 plus a 2 x 2 plus a 3 x 3 plus ellipsis" display="block">
<mrow>
<mi>y</mi>
<mo>=</mo>
<msub><mi>a</mi> <mn>0</mn> </msub>
<mo>+</mo>
<msub><mi>a</mi> <mn>1</mn> </msub>
<msub><mi>x</mi> <mn>1</mn> </msub>
<mo>+</mo>
<msub><mi>a</mi> <mn>2</mn> </msub>
<msub><mi>x</mi> <mn>2</mn> </msub>
<mo>+</mo>
<msub><mi>a</mi> <mn>3</mn> </msub>
<msub><mi>x</mi> <mn>3</mn> </msub>
<mo>+</mo>
<mo>⋯</mo>
</mrow>
</math>
</div>
<p>and build the <math alttext="x 1 comma x 2 comma x 3 comma">
<mrow>
<msub><mi>x</mi> <mn>1</mn> </msub>
<mo>,</mo>
<msub><mi>x</mi> <mn>2</mn> </msub>
<mo>,</mo>
<msub><mi>x</mi> <mn>3</mn> </msub>
<mo>,</mo>
</mrow>
</math> and so on from our
single-dimensional input <math alttext="x">
<mi>x</mi>
</math>. That is, we let
<math alttext="x Subscript n Baseline equals f Subscript n Baseline left-parenthesis x right-parenthesis">
<mrow>
<msub><mi>x</mi> <mi>n</mi> </msub>
<mo>=</mo>
<msub><mi>f</mi> <mi>n</mi> </msub>
<mrow>
<mo>(</mo>
<mi>x</mi>
<mo>)</mo>
</mrow>
</mrow>
</math>, where <math alttext="f Subscript n Baseline left-parenthesis right-parenthesis">
<mrow>
<msub><mi>f</mi> <mi>n</mi> </msub>
<mrow>
<mo>(</mo>
<mo>)</mo>
</mrow>
</mrow>
</math> is some function
that transforms our data.</p>
<p>For example, if <math alttext="f Subscript n Baseline left-parenthesis x right-parenthesis equals x Superscript n">
<mrow>
<msub><mi>f</mi> <mi>n</mi> </msub>
<mrow>
<mo>(</mo>
<mi>x</mi>
<mo>)</mo>
</mrow>
<mo>=</mo>
<msup><mi>x</mi> <mi>n</mi> </msup>
</mrow>
</math>, our model becomes a
polynomial regression:</p>
<div data-type="equation">
<math alttext="y equals a 0 plus a 1 x plus a 2 x squared plus a 3 x cubed plus ellipsis" display="block">
<mrow>
<mi>y</mi>
<mo>=</mo>
<msub><mi>a</mi> <mn>0</mn> </msub>
<mo>+</mo>
<msub><mi>a</mi> <mn>1</mn> </msub>
<mi>x</mi>
<mo>+</mo>
<msub><mi>a</mi> <mn>2</mn> </msub>
<msup><mi>x</mi> <mn>2</mn> </msup>
<mo>+</mo>
<msub><mi>a</mi> <mn>3</mn> </msub>
<msup><mi>x</mi> <mn>3</mn> </msup>
<mo>+</mo>
<mo>⋯</mo>
</mrow>
</math>
</div>
<p>Notice that this is <em>still a linear model</em>—the linearity refers to
the fact that the coefficients <math alttext="a Subscript n">
<msub><mi>a</mi> <mi>n</mi> </msub>
</math> never multiply or
divide each other. What we have effectively done is taken our
one-dimensional <math alttext="x">
<mi>x</mi>
</math> values and projected them into a higher
dimension, so that a linear fit can fit more complicated relationships
between <math alttext="x">
<mi>x</mi>
</math> and <math alttext="y">
<mi>y</mi>
</math>.</p>
<section data-pdf-bookmark="Polynomial Basis Functions" data-type="sect2"><div class="sect2" id="ch_0506-linear-regression_polynomial-basis-functions">
<h2>Polynomial Basis Functions</h2>
<p><a data-primary="basis function regression" data-secondary="polynomial basis functions" data-type="indexterm" id="idm45858736417904"/><a data-primary="polynomial basis functions" data-type="indexterm" id="idm45858736416848"/>This polynomial projection is useful enough that it is built into
Scikit-Learn, using the <code>PolynomialFeatures</code> transformer:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">6</code><code class="p">]:</code> <code class="kn">from</code> <code class="nn">sklearn.preprocessing</code> <code class="kn">import</code> <code class="n">PolynomialFeatures</code>
        <code class="n">x</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">([</code><code class="mi">2</code><code class="p">,</code> <code class="mi">3</code><code class="p">,</code> <code class="mi">4</code><code class="p">])</code>
        <code class="n">poly</code> <code class="o">=</code> <code class="n">PolynomialFeatures</code><code class="p">(</code><code class="mi">3</code><code class="p">,</code> <code class="n">include_bias</code><code class="o">=</code><code class="kc">False</code><code class="p">)</code>
        <code class="n">poly</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">x</code><code class="p">[:,</code> <code class="kc">None</code><code class="p">])</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">6</code><code class="p">]:</code> <code class="n">array</code><code class="p">([[</code> <code class="mf">2.</code><code class="p">,</code>  <code class="mf">4.</code><code class="p">,</code>  <code class="mf">8.</code><code class="p">],</code>
               <code class="p">[</code> <code class="mf">3.</code><code class="p">,</code>  <code class="mf">9.</code><code class="p">,</code> <code class="mf">27.</code><code class="p">],</code>
               <code class="p">[</code> <code class="mf">4.</code><code class="p">,</code> <code class="mf">16.</code><code class="p">,</code> <code class="mf">64.</code><code class="p">]])</code></pre>
<p>We see here that the transformer has converted our one-dimensional array
into a three-dimensional array, where each column contains the
exponentiated value. This new, higher-dimensional data representation
can then be plugged into a linear 
<span class="keep-together">regression</span>.</p>
<p>As we saw in <a data-type="xref" href="ch40.xhtml#section-0504-feature-engineering">Chapter 40</a>,
the cleanest way to accomplish this is to use a pipeline.
Let’s make a 7th-degree polynomial model in this way:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">7</code><code class="p">]:</code> <code class="kn">from</code> <code class="nn">sklearn.pipeline</code> <code class="kn">import</code> <code class="n">make_pipeline</code>
        <code class="n">poly_model</code> <code class="o">=</code> <code class="n">make_pipeline</code><code class="p">(</code><code class="n">PolynomialFeatures</code><code class="p">(</code><code class="mi">7</code><code class="p">),</code>
                                   <code class="n">LinearRegression</code><code class="p">())</code></pre>
<p>With this transform in place, we can use the linear model to fit much
more complicated relationships between <math alttext="x">
<mi>x</mi>
</math> and
<math alttext="y">
<mi>y</mi>
</math>. For example, here is a sine wave with noise (see <a data-type="xref" href="#fig_0506-linear-regression_files_in_output_19_0">Figure 42-3</a>).</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">8</code><code class="p">]:</code> <code class="n">rng</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">RandomState</code><code class="p">(</code><code class="mi">1</code><code class="p">)</code>
        <code class="n">x</code> <code class="o">=</code> <code class="mi">10</code> <code class="o">*</code> <code class="n">rng</code><code class="o">.</code><code class="n">rand</code><code class="p">(</code><code class="mi">50</code><code class="p">)</code>
        <code class="n">y</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">sin</code><code class="p">(</code><code class="n">x</code><code class="p">)</code> <code class="o">+</code> <code class="mf">0.1</code> <code class="o">*</code> <code class="n">rng</code><code class="o">.</code><code class="n">randn</code><code class="p">(</code><code class="mi">50</code><code class="p">)</code>

        <code class="n">poly_model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">x</code><code class="p">[:,</code> <code class="n">np</code><code class="o">.</code><code class="n">newaxis</code><code class="p">],</code> <code class="n">y</code><code class="p">)</code>
        <code class="n">yfit</code> <code class="o">=</code> <code class="n">poly_model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">xfit</code><code class="p">[:,</code> <code class="n">np</code><code class="o">.</code><code class="n">newaxis</code><code class="p">])</code>

        <code class="n">plt</code><code class="o">.</code><code class="n">scatter</code><code class="p">(</code><code class="n">x</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code>
        <code class="n">plt</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">xfit</code><code class="p">,</code> <code class="n">yfit</code><code class="p">);</code></pre>
<figure class="width-75"><div class="figure" id="fig_0506-linear-regression_files_in_output_19_0">
<img alt="output 19 0" height="398" src="assets/output_19_0.png" width="600"/>
<h6><span class="label">Figure 42-3. </span>A linear polynomial fit to nonlinear training data</h6>
</div></figure>
<p>Our linear model, through the use of seventh-order polynomial basis
functions, can provide an excellent fit to this nonlinear data!</p>
</div></section>
<section data-pdf-bookmark="Gaussian Basis Functions" data-type="sect2"><div class="sect2" id="ch_0506-linear-regression_gaussian-basis-functions">
<h2>Gaussian Basis Functions</h2>
<p><a data-primary="basis function regression" data-secondary="Gaussian basis functions" data-type="indexterm" id="ix_ch42-asciidoc6"/><a data-primary="Gaussian basis functions" data-type="indexterm" id="ix_ch42-asciidoc7"/>Of course, other basis functions are possible. For example, one useful
pattern is to fit a model that is not a sum of polynomial bases, but a
sum of Gaussian bases. The result might look something like <a data-type="xref" href="#fig_images_in_0506-gaussian-basis">Figure 42-4</a>.</p>
<figure class="width-75"><div class="figure" id="fig_images_in_0506-gaussian-basis">
<img alt="05.06 gaussian basis" height="381" src="assets/05.06-gaussian-basis.png" width="600"/>
<h6><span class="label">Figure 42-4. </span>A Gaussian basis function fit to nonlinear data<sup><a data-type="noteref" href="ch42.xhtml#idm45858736130864" id="idm45858736130864-marker">1</a></sup></h6>
</div></figure>
<p>The shaded regions in the plot are the scaled basis functions, and when
added together they reproduce the smooth curve through the data. These
Gaussian basis functions are not built into Scikit-Learn, but we can
write a custom transformer that will create them, as shown here and
illustrated in <a data-type="xref" href="#fig_0506-linear-regression_files_in_output_24_0">Figure 42-5</a> (Scikit-Learn transformers are
implemented as Python classes; reading Scikit-Learn’s source
is a good way to see how they can be created):</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">9</code><code class="p">]:</code> <code class="kn">from</code> <code class="nn">sklearn.base</code> <code class="kn">import</code> <code class="n">BaseEstimator</code><code class="p">,</code> <code class="n">TransformerMixin</code>

        <code class="k">class</code> <code class="nc">GaussianFeatures</code><code class="p">(</code><code class="n">BaseEstimator</code><code class="p">,</code> <code class="n">TransformerMixin</code><code class="p">):</code>
            <code class="sd">"""Uniformly spaced Gaussian features for one-dimensional input"""</code>

            <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">N</code><code class="p">,</code> <code class="n">width_factor</code><code class="o">=</code><code class="mf">2.0</code><code class="p">):</code>
                <code class="bp">self</code><code class="o">.</code><code class="n">N</code> <code class="o">=</code> <code class="n">N</code>
                <code class="bp">self</code><code class="o">.</code><code class="n">width_factor</code> <code class="o">=</code> <code class="n">width_factor</code>

            <code class="nd">@staticmethod</code>
            <code class="k">def</code> <code class="nf">_gauss_basis</code><code class="p">(</code><code class="n">x</code><code class="p">,</code> <code class="n">y</code><code class="p">,</code> <code class="n">width</code><code class="p">,</code> <code class="n">axis</code><code class="o">=</code><code class="kc">None</code><code class="p">):</code>
                <code class="n">arg</code> <code class="o">=</code> <code class="p">(</code><code class="n">x</code> <code class="o">-</code> <code class="n">y</code><code class="p">)</code> <code class="o">/</code> <code class="n">width</code>
                <code class="k">return</code> <code class="n">np</code><code class="o">.</code><code class="n">exp</code><code class="p">(</code><code class="o">-</code><code class="mf">0.5</code> <code class="o">*</code> <code class="n">np</code><code class="o">.</code><code class="n">sum</code><code class="p">(</code><code class="n">arg</code> <code class="o">**</code> <code class="mi">2</code><code class="p">,</code> <code class="n">axis</code><code class="p">))</code>

            <code class="k">def</code> <code class="nf">fit</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="o">=</code><code class="kc">None</code><code class="p">):</code>
                <code class="c1"># create N centers spread along the data range</code>
                <code class="bp">self</code><code class="o">.</code><code class="n">centers_</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">linspace</code><code class="p">(</code><code class="n">X</code><code class="o">.</code><code class="n">min</code><code class="p">(),</code> <code class="n">X</code><code class="o">.</code><code class="n">max</code><code class="p">(),</code> <code class="bp">self</code><code class="o">.</code><code class="n">N</code><code class="p">)</code>
                <code class="bp">self</code><code class="o">.</code><code class="n">width_</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">width_factor</code><code class="o">*</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">centers_</code><code class="p">[</code><code class="mi">1</code><code class="p">]</code><code class="o">-</code><code class="bp">self</code><code class="o">.</code><code class="n">centers_</code><code class="p">[</code><code class="mi">0</code><code class="p">])</code>
                <code class="k">return</code> <code class="bp">self</code>

            <code class="k">def</code> <code class="nf">transform</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">X</code><code class="p">):</code>
                <code class="k">return</code> <code class="bp">self</code><code class="o">.</code><code class="n">_gauss_basis</code><code class="p">(</code><code class="n">X</code><code class="p">[:,</code> <code class="p">:,</code> <code class="n">np</code><code class="o">.</code><code class="n">newaxis</code><code class="p">],</code> <code class="bp">self</code><code class="o">.</code><code class="n">centers_</code><code class="p">,</code>
                                         <code class="bp">self</code><code class="o">.</code><code class="n">width_</code><code class="p">,</code> <code class="n">axis</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>

        <code class="n">gauss_model</code> <code class="o">=</code> <code class="n">make_pipeline</code><code class="p">(</code><code class="n">GaussianFeatures</code><code class="p">(</code><code class="mi">20</code><code class="p">),</code>
                                    <code class="n">LinearRegression</code><code class="p">())</code>
        <code class="n">gauss_model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">x</code><code class="p">[:,</code> <code class="n">np</code><code class="o">.</code><code class="n">newaxis</code><code class="p">],</code> <code class="n">y</code><code class="p">)</code>
        <code class="n">yfit</code> <code class="o">=</code> <code class="n">gauss_model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">xfit</code><code class="p">[:,</code> <code class="n">np</code><code class="o">.</code><code class="n">newaxis</code><code class="p">])</code>

        <code class="n">plt</code><code class="o">.</code><code class="n">scatter</code><code class="p">(</code><code class="n">x</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code>
        <code class="n">plt</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">xfit</code><code class="p">,</code> <code class="n">yfit</code><code class="p">)</code>
        <code class="n">plt</code><code class="o">.</code><code class="n">xlim</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="mi">10</code><code class="p">);</code></pre>
<figure class="width-75"><div class="figure" id="fig_0506-linear-regression_files_in_output_24_0">
<img alt="output 24 0" height="383" src="assets/output_24_0.png" width="600"/>
<h6><span class="label">Figure 42-5. </span>A Gaussian basis function fit computed with a custom transformer</h6>
</div></figure>
<p>I’ve included this example just to make clear that there is
nothing magic about polynomial basis functions: if you have some sort of
intuition into the generating process of your data that makes you think
one basis or another might be appropriate, you can use that instead<a data-startref="ix_ch42-asciidoc7" data-type="indexterm" id="idm45858735978336"/><a data-startref="ix_ch42-asciidoc6" data-type="indexterm" id="idm45858735851216"/>.<a data-startref="ix_ch42-asciidoc5" data-type="indexterm" id="idm45858735850416"/><a data-startref="ix_ch42-asciidoc4" data-type="indexterm" id="idm45858735849712"/></p>
</div></section>
</div></section>
<section data-pdf-bookmark="Regularization" data-type="sect1"><div class="sect1" id="ch_0506-linear-regression_regularization">
<h1>Regularization</h1>
<p><a data-primary="linear regression (in machine learning)" data-secondary="regularization" data-type="indexterm" id="ix_ch42-asciidoc8"/><a data-primary="regularization" data-type="indexterm" id="ix_ch42-asciidoc9"/>The introduction of basis functions into our linear regression makes the
model much more flexible, but it also can very quickly lead to
overfitting (refer back to
<a data-type="xref" href="ch39.xhtml#section-0503-hyperparameters-and-model-validation">Chapter 39</a> for a discussion of this). For example, <a data-type="xref" href="#fig_0506-linear-regression_files_in_output_27_0">Figure 42-6</a> shows what happens if we use a large number of Gaussian
basis functions:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">10</code><code class="p">]:</code> <code class="n">model</code> <code class="o">=</code> <code class="n">make_pipeline</code><code class="p">(</code><code class="n">GaussianFeatures</code><code class="p">(</code><code class="mi">30</code><code class="p">),</code>
                               <code class="n">LinearRegression</code><code class="p">())</code>
         <code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">x</code><code class="p">[:,</code> <code class="n">np</code><code class="o">.</code><code class="n">newaxis</code><code class="p">],</code> <code class="n">y</code><code class="p">)</code>

         <code class="n">plt</code><code class="o">.</code><code class="n">scatter</code><code class="p">(</code><code class="n">x</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code>
         <code class="n">plt</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">xfit</code><code class="p">,</code> <code class="n">model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">xfit</code><code class="p">[:,</code> <code class="n">np</code><code class="o">.</code><code class="n">newaxis</code><code class="p">]))</code>

         <code class="n">plt</code><code class="o">.</code><code class="n">xlim</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="mi">10</code><code class="p">)</code>
         <code class="n">plt</code><code class="o">.</code><code class="n">ylim</code><code class="p">(</code><code class="o">-</code><code class="mf">1.5</code><code class="p">,</code> <code class="mf">1.5</code><code class="p">);</code></pre>
<figure class="width-75"><div class="figure" id="fig_0506-linear-regression_files_in_output_27_0">
<img alt="output 27 0" height="383" src="assets/output_27_0.png" width="600"/>
<h6><span class="label">Figure 42-6. </span>An overly complex basis function model that overfits the data</h6>
</div></figure>
<p>With the data projected to the 30-dimensional basis, the model has far
too much flexibility and goes to extreme values between locations where
it is constrained by data. We can see the reason for this if we plot the
coefficients of the Gaussian bases with respect to their locations, as
shown in <a data-type="xref" href="#fig_0506-linear-regression_files_in_output_29_0">Figure 42-7</a>.</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">11</code><code class="p">]:</code> <code class="k">def</code> <code class="nf">basis_plot</code><code class="p">(</code><code class="n">model</code><code class="p">,</code> <code class="n">title</code><code class="o">=</code><code class="kc">None</code><code class="p">):</code>
             <code class="n">fig</code><code class="p">,</code> <code class="n">ax</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">(</code><code class="mi">2</code><code class="p">,</code> <code class="n">sharex</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>
             <code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">x</code><code class="p">[:,</code> <code class="n">np</code><code class="o">.</code><code class="n">newaxis</code><code class="p">],</code> <code class="n">y</code><code class="p">)</code>
             <code class="n">ax</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="o">.</code><code class="n">scatter</code><code class="p">(</code><code class="n">x</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code>
             <code class="n">ax</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">xfit</code><code class="p">,</code> <code class="n">model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">xfit</code><code class="p">[:,</code> <code class="n">np</code><code class="o">.</code><code class="n">newaxis</code><code class="p">]))</code>
             <code class="n">ax</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="o">.</code><code class="n">set</code><code class="p">(</code><code class="n">xlabel</code><code class="o">=</code><code class="s1">'x'</code><code class="p">,</code> <code class="n">ylabel</code><code class="o">=</code><code class="s1">'y'</code><code class="p">,</code> <code class="n">ylim</code><code class="o">=</code><code class="p">(</code><code class="o">-</code><code class="mf">1.5</code><code class="p">,</code> <code class="mf">1.5</code><code class="p">))</code>

             <code class="k">if</code> <code class="n">title</code><code class="p">:</code>
                 <code class="n">ax</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="o">.</code><code class="n">set_title</code><code class="p">(</code><code class="n">title</code><code class="p">)</code>

             <code class="n">ax</code><code class="p">[</code><code class="mi">1</code><code class="p">]</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">model</code><code class="o">.</code><code class="n">steps</code><code class="p">[</code><code class="mi">0</code><code class="p">][</code><code class="mi">1</code><code class="p">]</code><code class="o">.</code><code class="n">centers_</code><code class="p">,</code>
                        <code class="n">model</code><code class="o">.</code><code class="n">steps</code><code class="p">[</code><code class="mi">1</code><code class="p">][</code><code class="mi">1</code><code class="p">]</code><code class="o">.</code><code class="n">coef_</code><code class="p">)</code>
             <code class="n">ax</code><code class="p">[</code><code class="mi">1</code><code class="p">]</code><code class="o">.</code><code class="n">set</code><code class="p">(</code><code class="n">xlabel</code><code class="o">=</code><code class="s1">'basis location'</code><code class="p">,</code>
                       <code class="n">ylabel</code><code class="o">=</code><code class="s1">'coefficient'</code><code class="p">,</code>
                       <code class="n">xlim</code><code class="o">=</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="mi">10</code><code class="p">))</code>

         <code class="n">model</code> <code class="o">=</code> <code class="n">make_pipeline</code><code class="p">(</code><code class="n">GaussianFeatures</code><code class="p">(</code><code class="mi">30</code><code class="p">),</code> <code class="n">LinearRegression</code><code class="p">())</code>
         <code class="n">basis_plot</code><code class="p">(</code><code class="n">model</code><code class="p">)</code></pre>
<figure><div class="figure" id="fig_0506-linear-regression_files_in_output_29_0">
<img alt="output 29 0" height="565" src="assets/output_29_0.png" width="600"/>
<h6><span class="label">Figure 42-7. </span>The coefficients of the Gaussian bases in the overly complex model</h6>
</div></figure>
<p>The lower panel of this figure shows the amplitude of the basis function
at each location. This is typical overfitting behavior when basis
functions overlap: the coefficients of adjacent basis functions blow up
and cancel each other out. We know that such behavior is problematic,
and it would be nice if we could limit such spikes explicitly in the
model by penalizing large values of the model parameters. Such a penalty
is known as <em>regularization</em>, and comes in several forms.</p>
<section data-pdf-bookmark="Ridge Regression (L2 Regularization)" data-type="sect2"><div class="sect2" id="ch_0506-linear-regression_ridge-regression">
<h2>Ridge Regression (L<sub>2</sub> Regularization)</h2>
<p><a data-primary="regularization" data-secondary="ridge regression" data-type="indexterm" id="idm45858735523440"/><a data-primary="ridge regression (L2 regularization)" data-type="indexterm" id="idm45858735522240"/><a data-primary="Tikhonov regularization" data-type="indexterm" id="idm45858735521600"/>Perhaps the most common form of regularization is known as <em>ridge
regression</em> or <math alttext="upper L 2">
<msub><mi>L</mi> <mn>2</mn> </msub>
</math> <em>regularization</em> (sometimes also called
<em>Tikhonov regularization</em>). This proceeds by penalizing the sum of
squares (2-norms) of the model coefficients <math alttext="theta Subscript n">
<msub><mi>θ</mi> <mi>n</mi> </msub>
</math>. In
this case, the penalty on the model fit would be:</p>
<div data-type="equation">
<math alttext="upper P equals alpha sigma-summation Underscript n equals 1 Overscript upper N Endscripts theta Subscript n Superscript 2" display="block">
<mrow>
<mi>P</mi>
<mo>=</mo>
<mi>α</mi>
<munderover><mo>∑</mo> <mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow> <mi>N</mi> </munderover>
<msubsup><mi>θ</mi> <mi>n</mi> <mn>2</mn> </msubsup>
</mrow>
</math>
</div>
<p>where <math alttext="alpha">
<mi>α</mi>
</math> is a free parameter that controls the
strength of the penalty. This type of penalized model is built into
Scikit-Learn with the <code>Ridge</code> estimator (see <a data-type="xref" href="#fig_0506-linear-regression_files_in_output_32_0">Figure 42-8</a>).</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">12</code><code class="p">]:</code> <code class="kn">from</code> <code class="nn">sklearn.linear_model</code> <code class="kn">import</code> <code class="n">Ridge</code>
         <code class="n">model</code> <code class="o">=</code> <code class="n">make_pipeline</code><code class="p">(</code><code class="n">GaussianFeatures</code><code class="p">(</code><code class="mi">30</code><code class="p">),</code> <code class="n">Ridge</code><code class="p">(</code><code class="n">alpha</code><code class="o">=</code><code class="mf">0.1</code><code class="p">))</code>
         <code class="n">basis_plot</code><code class="p">(</code><code class="n">model</code><code class="p">,</code> <code class="n">title</code><code class="o">=</code><code class="s1">'Ridge Regression'</code><code class="p">)</code></pre>
<figure><div class="figure" id="fig_0506-linear-regression_files_in_output_32_0">
<img alt="output 32 0" height="377" src="assets/output_32_0.png" width="600"/>
<h6><span class="label">Figure 42-8. </span>Ridge (L<sub>2</sub>) regularization applied to the overly complex model (compare to <a data-type="xref" href="#fig_0506-linear-regression_files_in_output_29_0">Figure 42-7</a>)</h6>
</div></figure>
<p>The <math alttext="alpha">
<mi>α</mi>
</math> parameter is essentially a knob controlling the
complexity of the resulting model. In the limit
<math alttext="alpha right-arrow 0">
<mrow>
<mi>α</mi>
<mo>→</mo>
<mn>0</mn>
</mrow>
</math>, we recover the standard linear regression
result; in the limit <math alttext="alpha right-arrow normal infinity">
<mrow>
<mi>α</mi>
<mo>→</mo>
<mi>∞</mi>
</mrow>
</math>, all model
responses will be suppressed. One advantage of ridge regression in
particular is that it can be computed very efficiently—at hardly more
computational cost than the original linear regression model.</p>
</div></section>
<section data-pdf-bookmark="Lasso Regression (L1 Regularization)" data-type="sect2"><div class="sect2" id="ch_0506-linear-regression_lasso-regression_L_1">
<h2>Lasso Regression (L<sub>1</sub> Regularization)</h2>
<p><a data-primary="lasso regularization (L1 regularization)" data-type="indexterm" id="idm45858735415152"/><a data-primary="regularization" data-secondary="lasso regularization" data-type="indexterm" id="idm45858735414416"/>Another common type of regularization is known as <em>lasso regression</em> or
<em>L<sub>1</sub> regularization</em> and involves penalizing the sum of absolute values
(1-norms) of regression 
<span class="keep-together">coefficients</span>:</p>
<div data-type="equation">
<math alttext="upper P equals alpha sigma-summation Underscript n equals 1 Overscript upper N Endscripts StartAbsoluteValue theta Subscript n Baseline EndAbsoluteValue" display="block">
<mrow>
<mi>P</mi>
<mo>=</mo>
<mi>α</mi>
<munderover><mo>∑</mo> <mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow> <mi>N</mi> </munderover>
<mrow>
<mo>|</mo>
<msub><mi>θ</mi> <mi>n</mi> </msub>
<mo>|</mo>
</mrow>
</mrow>
</math>
</div>
<p>Though this is conceptually very similar to ridge regression, the
results can differ surprisingly. For example, due to its construction,
lasso regression tends to favor <em>sparse models</em> where possible: that is,
it preferentially sets many model coefficients to exactly zero.</p>
<p>We can see this behavior if we duplicate the previous example using
<math alttext="upper L 1">
<msub><mi>L</mi> <mn>1</mn> </msub>
</math>-normalized coefficients (see <a data-type="xref" href="#fig_0506-linear-regression_files_in_output_35_0">Figure 42-9</a>).</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">13</code><code class="p">]:</code> <code class="kn">from</code> <code class="nn">sklearn.linear_model</code> <code class="kn">import</code> <code class="n">Lasso</code>
         <code class="n">model</code> <code class="o">=</code> <code class="n">make_pipeline</code><code class="p">(</code><code class="n">GaussianFeatures</code><code class="p">(</code><code class="mi">30</code><code class="p">),</code>
                               <code class="n">Lasso</code><code class="p">(</code><code class="n">alpha</code><code class="o">=</code><code class="mf">0.001</code><code class="p">,</code> <code class="n">max_iter</code><code class="o">=</code><code class="mi">2000</code><code class="p">))</code>
         <code class="n">basis_plot</code><code class="p">(</code><code class="n">model</code><code class="p">,</code> <code class="n">title</code><code class="o">=</code><code class="s1">'Lasso Regression'</code><code class="p">)</code></pre>
<figure><div class="figure" id="fig_0506-linear-regression_files_in_output_35_0">
<img alt="output 35 0" height="392" src="assets/output_35_0.png" width="600"/>
<h6><span class="label">Figure 42-9. </span>Lasso (L<sub>1</sub>) regularization applied to the overly complex model (compare to <a data-type="xref" href="#fig_0506-linear-regression_files_in_output_32_0">Figure 42-8</a>)</h6>
</div></figure>
<p>With the lasso regression penalty, the majority of the coefficients are
exactly zero, with the functional behavior being modeled by a small
subset of the available basis functions. As with ridge regularization,
the <math alttext="alpha">
<mi>α</mi>
</math> parameter tunes the strength of the penalty and
should be determined via, for example, cross-validation (refer back to
<a data-type="xref" href="ch39.xhtml#section-0503-hyperparameters-and-model-validation">Chapter 39</a> for a discussion of this).<a data-startref="ix_ch42-asciidoc9" data-type="indexterm" id="idm45858735346880"/><a data-startref="ix_ch42-asciidoc8" data-type="indexterm" id="idm45858735346208"/></p>
</div></section>
</div></section>
<section data-pdf-bookmark="Example: Predicting Bicycle Traffic" data-type="sect1"><div class="sect1" id="ch_0506-linear-regression_example-predicting-bicycle-traffic">
<h1>Example: Predicting Bicycle Traffic</h1>
<p><a data-primary="bicycle traffic prediction" data-secondary="linear regression" data-type="indexterm" id="ix_ch42-asciidoc10"/><a data-primary="linear regression (in machine learning)" data-secondary="Seattle bicycle traffic prediction example" data-type="indexterm" id="ix_ch42-asciidoc11"/><a data-primary="Seattle, bicycle traffic prediction in" data-secondary="linear regression" data-type="indexterm" id="ix_ch42-asciidoc12"/>As an example, let’s take a look at whether we can predict
the number of bicycle trips across Seattle’s Fremont Bridge
based on weather, season, and other factors. We already saw this data in
<a data-type="xref" href="ch23.xhtml#section-0311-working-with-time-series">Chapter 23</a>, but
here we will join the bike data with another dataset and try to
determine the extent to which weather and seasonal factors—temperature,
precipitation, and daylight hours—affect the volume of bicycle traffic
through this corridor. Fortunately, the National Oceanic and Atmospheric
Administration (NOAA) makes its daily
<a href="https://oreil.ly/sE5zO">weather station
data</a> available—I used station ID USW00024233—and we can easily use
Pandas to join the two data sources. We will perform a simple linear
regression to relate weather and other information to bicycle counts, in
order to estimate how a change in any one of these parameters affects
the number of riders on a given day.</p>
<p>In particular, this is an example of how the tools of Scikit-Learn can
be used in a statistical modeling framework, in which the parameters of
the model are assumed to have interpretable meaning. As discussed
previously, this is not a standard approach within machine learning, but
such interpretation is possible for some models.</p>
<p>Let’s start by loading the two datasets, indexing by date:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">14</code><code class="p">]:</code> <code class="c1"># url = 'https://raw.githubusercontent.com/jakevdp/bicycle-data/main'</code>
         <code class="c1"># !curl -O {url}/FremontBridge.csv</code>
         <code class="c1"># !curl -O {url}/SeattleWeather.csv</code></pre>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">15</code><code class="p">]:</code> <code class="kn">import</code> <code class="nn">pandas</code> <code class="k">as</code> <code class="nn">pd</code>
         <code class="n">counts</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">read_csv</code><code class="p">(</code><code class="s1">'FremontBridge.csv'</code><code class="p">,</code>
                              <code class="n">index_col</code><code class="o">=</code><code class="s1">'Date'</code><code class="p">,</code> <code class="n">parse_dates</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>
         <code class="n">weather</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">read_csv</code><code class="p">(</code><code class="s1">'SeattleWeather.csv'</code><code class="p">,</code>
                               <code class="n">index_col</code><code class="o">=</code><code class="s1">'DATE'</code><code class="p">,</code> <code class="n">parse_dates</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code></pre>
<p>For simplicity, let’s look at data prior to 2020 in order to
avoid the effects of the COVID-19 pandemic, which significantly affected
commuting patterns in Seattle:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">16</code><code class="p">]:</code> <code class="n">counts</code> <code class="o">=</code> <code class="n">counts</code><code class="p">[</code><code class="n">counts</code><code class="o">.</code><code class="n">index</code> <code class="o">&lt;</code> <code class="s2">"2020-01-01"</code><code class="p">]</code>
         <code class="n">weather</code> <code class="o">=</code> <code class="n">weather</code><code class="p">[</code><code class="n">weather</code><code class="o">.</code><code class="n">index</code> <code class="o">&lt;</code> <code class="s2">"2020-01-01"</code><code class="p">]</code></pre>
<p>Next we will compute the total daily bicycle traffic, and put this in
its own <code>DataFrame</code>:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">17</code><code class="p">]:</code> <code class="n">daily</code> <code class="o">=</code> <code class="n">counts</code><code class="o">.</code><code class="n">resample</code><code class="p">(</code><code class="s1">'d'</code><code class="p">)</code><code class="o">.</code><code class="n">sum</code><code class="p">()</code>
         <code class="n">daily</code><code class="p">[</code><code class="s1">'Total'</code><code class="p">]</code> <code class="o">=</code> <code class="n">daily</code><code class="o">.</code><code class="n">sum</code><code class="p">(</code><code class="n">axis</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>
         <code class="n">daily</code> <code class="o">=</code> <code class="n">daily</code><code class="p">[[</code><code class="s1">'Total'</code><code class="p">]]</code> <code class="c1"># remove other columns</code></pre>
<p>We saw previously that the patterns of use generally vary from day to
day. Let’s account for this in our data by adding binary
columns that indicate the day of the week:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">18</code><code class="p">]:</code> <code class="n">days</code> <code class="o">=</code> <code class="p">[</code><code class="s1">'Mon'</code><code class="p">,</code> <code class="s1">'Tue'</code><code class="p">,</code> <code class="s1">'Wed'</code><code class="p">,</code> <code class="s1">'Thu'</code><code class="p">,</code> <code class="s1">'Fri'</code><code class="p">,</code> <code class="s1">'Sat'</code><code class="p">,</code> <code class="s1">'Sun'</code><code class="p">]</code>
         <code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">7</code><code class="p">):</code>
             <code class="n">daily</code><code class="p">[</code><code class="n">days</code><code class="p">[</code><code class="n">i</code><code class="p">]]</code> <code class="o">=</code> <code class="p">(</code><code class="n">daily</code><code class="o">.</code><code class="n">index</code><code class="o">.</code><code class="n">dayofweek</code> <code class="o">==</code> <code class="n">i</code><code class="p">)</code><code class="o">.</code><code class="n">astype</code><code class="p">(</code><code class="nb">float</code><code class="p">)</code></pre>
<p>Similarly, we might expect riders to behave differently on holidays;
let’s add an indicator of this as well:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">19</code><code class="p">]:</code> <code class="kn">from</code> <code class="nn">pandas.tseries.holiday</code> <code class="kn">import</code> <code class="n">USFederalHolidayCalendar</code>
         <code class="n">cal</code> <code class="o">=</code> <code class="n">USFederalHolidayCalendar</code><code class="p">()</code>
         <code class="n">holidays</code> <code class="o">=</code> <code class="n">cal</code><code class="o">.</code><code class="n">holidays</code><code class="p">(</code><code class="s1">'2012'</code><code class="p">,</code> <code class="s1">'2020'</code><code class="p">)</code>
         <code class="n">daily</code> <code class="o">=</code> <code class="n">daily</code><code class="o">.</code><code class="n">join</code><code class="p">(</code><code class="n">pd</code><code class="o">.</code><code class="n">Series</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="n">index</code><code class="o">=</code><code class="n">holidays</code><code class="p">,</code> <code class="n">name</code><code class="o">=</code><code class="s1">'holiday'</code><code class="p">))</code>
         <code class="n">daily</code><code class="p">[</code><code class="s1">'holiday'</code><code class="p">]</code><code class="o">.</code><code class="n">fillna</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="n">inplace</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code></pre>
<p>We also might suspect that the hours of daylight would affect how many
people ride. Let’s use the standard astronomical calculation
to add this information (see <a data-type="xref" href="#fig_0506-linear-regression_files_in_output_50_1">Figure 42-10</a>).</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">20</code><code class="p">]:</code> <code class="k">def</code> <code class="nf">hours_of_daylight</code><code class="p">(</code><code class="n">date</code><code class="p">,</code> <code class="n">axis</code><code class="o">=</code><code class="mf">23.44</code><code class="p">,</code> <code class="n">latitude</code><code class="o">=</code><code class="mf">47.61</code><code class="p">):</code>
             <code class="sd">"""Compute the hours of daylight for the given date"""</code>
             <code class="n">days</code> <code class="o">=</code> <code class="p">(</code><code class="n">date</code> <code class="o">-</code> <code class="n">pd</code><code class="o">.</code><code class="n">datetime</code><code class="p">(</code><code class="mi">2000</code><code class="p">,</code> <code class="mi">12</code><code class="p">,</code> <code class="mi">21</code><code class="p">))</code><code class="o">.</code><code class="n">days</code>
             <code class="n">m</code> <code class="o">=</code> <code class="p">(</code><code class="mf">1.</code> <code class="o">-</code> <code class="n">np</code><code class="o">.</code><code class="n">tan</code><code class="p">(</code><code class="n">np</code><code class="o">.</code><code class="n">radians</code><code class="p">(</code><code class="n">latitude</code><code class="p">))</code>
                  <code class="o">*</code> <code class="n">np</code><code class="o">.</code><code class="n">tan</code><code class="p">(</code><code class="n">np</code><code class="o">.</code><code class="n">radians</code><code class="p">(</code><code class="n">axis</code><code class="p">)</code> <code class="o">*</code> <code class="n">np</code><code class="o">.</code><code class="n">cos</code><code class="p">(</code><code class="n">days</code> <code class="o">*</code> <code class="mi">2</code> <code class="o">*</code> <code class="n">np</code><code class="o">.</code><code class="n">pi</code> <code class="o">/</code> <code class="mf">365.25</code><code class="p">)))</code>
             <code class="k">return</code> <code class="mf">24.</code> <code class="o">*</code> <code class="n">np</code><code class="o">.</code><code class="n">degrees</code><code class="p">(</code><code class="n">np</code><code class="o">.</code><code class="n">arccos</code><code class="p">(</code><code class="mi">1</code> <code class="o">-</code> <code class="n">np</code><code class="o">.</code><code class="n">clip</code><code class="p">(</code><code class="n">m</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">2</code><code class="p">)))</code> <code class="o">/</code> <code class="mf">180.</code>

         <code class="n">daily</code><code class="p">[</code><code class="s1">'daylight_hrs'</code><code class="p">]</code> <code class="o">=</code> <code class="nb">list</code><code class="p">(</code><code class="nb">map</code><code class="p">(</code><code class="n">hours_of_daylight</code><code class="p">,</code> <code class="n">daily</code><code class="o">.</code><code class="n">index</code><code class="p">))</code>
         <code class="n">daily</code><code class="p">[[</code><code class="s1">'daylight_hrs'</code><code class="p">]]</code><code class="o">.</code><code class="n">plot</code><code class="p">()</code>
         <code class="n">plt</code><code class="o">.</code><code class="n">ylim</code><code class="p">(</code><code class="mi">8</code><code class="p">,</code> <code class="mi">17</code><code class="p">)</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">20</code><code class="p">]:</code> <code class="p">(</code><code class="mf">8.0</code><code class="p">,</code> <code class="mf">17.0</code><code class="p">)</code></pre>
<figure><div class="figure" id="fig_0506-linear-regression_files_in_output_50_1">
<img alt="output 50 1" height="420" src="assets/output_50_1.png" width="600"/>
<h6><span class="label">Figure 42-10. </span>Visualization of hours of daylight in Seattle</h6>
</div></figure>
<p>We can also add the average temperature and total precipitation to the
data. In addition to the inches of precipitation, let’s add
a flag that indicates whether a day is dry (has zero precipitation):</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">21</code><code class="p">]:</code> <code class="n">weather</code><code class="p">[</code><code class="s1">'Temp (F)'</code><code class="p">]</code> <code class="o">=</code> <code class="mf">0.5</code> <code class="o">*</code> <code class="p">(</code><code class="n">weather</code><code class="p">[</code><code class="s1">'TMIN'</code><code class="p">]</code> <code class="o">+</code> <code class="n">weather</code><code class="p">[</code><code class="s1">'TMAX'</code><code class="p">])</code>
         <code class="n">weather</code><code class="p">[</code><code class="s1">'Rainfall (in)'</code><code class="p">]</code> <code class="o">=</code> <code class="n">weather</code><code class="p">[</code><code class="s1">'PRCP'</code><code class="p">]</code>
         <code class="n">weather</code><code class="p">[</code><code class="s1">'dry day'</code><code class="p">]</code> <code class="o">=</code> <code class="p">(</code><code class="n">weather</code><code class="p">[</code><code class="s1">'PRCP'</code><code class="p">]</code> <code class="o">==</code> <code class="mi">0</code><code class="p">)</code><code class="o">.</code><code class="n">astype</code><code class="p">(</code><code class="nb">int</code><code class="p">)</code>

         <code class="n">daily</code> <code class="o">=</code> <code class="n">daily</code><code class="o">.</code><code class="n">join</code><code class="p">(</code><code class="n">weather</code><code class="p">[[</code><code class="s1">'Rainfall (in)'</code><code class="p">,</code> <code class="s1">'Temp (F)'</code><code class="p">,</code> <code class="s1">'dry day'</code><code class="p">]])</code></pre>
<p>Finally, let’s add a counter that increases from day 1, and
measures how many years have passed. This will let us measure any
observed annual increase or decrease in daily crossings:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">22</code><code class="p">]:</code> <code class="n">daily</code><code class="p">[</code><code class="s1">'annual'</code><code class="p">]</code> <code class="o">=</code> <code class="p">(</code><code class="n">daily</code><code class="o">.</code><code class="n">index</code> <code class="o">-</code> <code class="n">daily</code><code class="o">.</code><code class="n">index</code><code class="p">[</code><code class="mi">0</code><code class="p">])</code><code class="o">.</code><code class="n">days</code> <code class="o">/</code> <code class="mf">365.</code></pre>
<p class="pagebreak-before less_space">Now that our data is in order, and we can take a look at it:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">23</code><code class="p">]:</code> <code class="n">daily</code><code class="o">.</code><code class="n">head</code><code class="p">()</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">23</code><code class="p">]:</code>               <code class="n">Total</code>  <code class="n">Mon</code>  <code class="n">Tue</code>  <code class="n">Wed</code>  <code class="n">Thu</code>  <code class="n">Fri</code>  <code class="n">Sat</code>  <code class="n">Sun</code>  <code class="n">holiday</code> \
         <code class="n">Date</code>
         <code class="mi">2012</code><code class="o">-</code><code class="mi">10</code><code class="o">-</code><code class="mi">03</code>  <code class="mf">14084.0</code>  <code class="mf">0.0</code>  <code class="mf">0.0</code>  <code class="mf">1.0</code>  <code class="mf">0.0</code>  <code class="mf">0.0</code>  <code class="mf">0.0</code>  <code class="mf">0.0</code>      <code class="mf">0.0</code>
         <code class="mi">2012</code><code class="o">-</code><code class="mi">10</code><code class="o">-</code><code class="mi">04</code>  <code class="mf">13900.0</code>  <code class="mf">0.0</code>  <code class="mf">0.0</code>  <code class="mf">0.0</code>  <code class="mf">1.0</code>  <code class="mf">0.0</code>  <code class="mf">0.0</code>  <code class="mf">0.0</code>      <code class="mf">0.0</code>
         <code class="mi">2012</code><code class="o">-</code><code class="mi">10</code><code class="o">-</code><code class="mi">05</code>  <code class="mf">12592.0</code>  <code class="mf">0.0</code>  <code class="mf">0.0</code>  <code class="mf">0.0</code>  <code class="mf">0.0</code>  <code class="mf">1.0</code>  <code class="mf">0.0</code>  <code class="mf">0.0</code>      <code class="mf">0.0</code>
         <code class="mi">2012</code><code class="o">-</code><code class="mi">10</code><code class="o">-</code><code class="mi">06</code>   <code class="mf">8024.0</code>  <code class="mf">0.0</code>  <code class="mf">0.0</code>  <code class="mf">0.0</code>  <code class="mf">0.0</code>  <code class="mf">0.0</code>  <code class="mf">1.0</code>  <code class="mf">0.0</code>      <code class="mf">0.0</code>
         <code class="mi">2012</code><code class="o">-</code><code class="mi">10</code><code class="o">-</code><code class="mi">07</code>   <code class="mf">8568.0</code>  <code class="mf">0.0</code>  <code class="mf">0.0</code>  <code class="mf">0.0</code>  <code class="mf">0.0</code>  <code class="mf">0.0</code>  <code class="mf">0.0</code>  <code class="mf">1.0</code>      <code class="mf">0.0</code>

                     <code class="n">daylight_hrs</code> <code class="n">Rainfall</code> <code class="p">(</code><code class="ow">in</code><code class="p">)</code>  <code class="n">Temp</code> <code class="p">(</code><code class="n">F</code><code class="p">)</code>  <code class="n">dry</code> <code class="n">day</code>    <code class="n">annual</code>
         <code class="n">Date</code>
         <code class="mi">2012</code><code class="o">-</code><code class="mi">10</code><code class="o">-</code><code class="mi">03</code>     <code class="mf">11.277359</code>           <code class="mf">0.0</code>      <code class="mf">56.0</code>        <code class="mi">1</code>  <code class="mf">0.000000</code>
         <code class="mi">2012</code><code class="o">-</code><code class="mi">10</code><code class="o">-</code><code class="mi">04</code>     <code class="mf">11.219142</code>           <code class="mf">0.0</code>      <code class="mf">56.5</code>        <code class="mi">1</code>  <code class="mf">0.002740</code>
         <code class="mi">2012</code><code class="o">-</code><code class="mi">10</code><code class="o">-</code><code class="mi">05</code>     <code class="mf">11.161038</code>           <code class="mf">0.0</code>      <code class="mf">59.5</code>        <code class="mi">1</code>  <code class="mf">0.005479</code>
         <code class="mi">2012</code><code class="o">-</code><code class="mi">10</code><code class="o">-</code><code class="mi">06</code>     <code class="mf">11.103056</code>           <code class="mf">0.0</code>      <code class="mf">60.5</code>        <code class="mi">1</code>  <code class="mf">0.008219</code>
         <code class="mi">2012</code><code class="o">-</code><code class="mi">10</code><code class="o">-</code><code class="mi">07</code>     <code class="mf">11.045208</code>           <code class="mf">0.0</code>      <code class="mf">60.5</code>        <code class="mi">1</code>  <code class="mf">0.010959</code></pre>
<p>With this in place, we can choose the columns to use, and fit a linear
regression model to our data. We will set <code>fit_intercept=False</code>, because
the daily flags essentially operate as their own day-specific
intercepts:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">24</code><code class="p">]:</code> <code class="c1"># Drop any rows with null values</code>
         <code class="n">daily</code><code class="o">.</code><code class="n">dropna</code><code class="p">(</code><code class="n">axis</code><code class="o">=</code><code class="mi">0</code><code class="p">,</code> <code class="n">how</code><code class="o">=</code><code class="s1">'any'</code><code class="p">,</code> <code class="n">inplace</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>

         <code class="n">column_names</code> <code class="o">=</code> <code class="p">[</code><code class="s1">'Mon'</code><code class="p">,</code> <code class="s1">'Tue'</code><code class="p">,</code> <code class="s1">'Wed'</code><code class="p">,</code> <code class="s1">'Thu'</code><code class="p">,</code> <code class="s1">'Fri'</code><code class="p">,</code> <code class="s1">'Sat'</code><code class="p">,</code> <code class="s1">'Sun'</code><code class="p">,</code>
                         <code class="s1">'holiday'</code><code class="p">,</code> <code class="s1">'daylight_hrs'</code><code class="p">,</code> <code class="s1">'Rainfall (in)'</code><code class="p">,</code>
                         <code class="s1">'dry day'</code><code class="p">,</code> <code class="s1">'Temp (F)'</code><code class="p">,</code> <code class="s1">'annual'</code><code class="p">]</code>
         <code class="n">X</code> <code class="o">=</code> <code class="n">daily</code><code class="p">[</code><code class="n">column_names</code><code class="p">]</code>
         <code class="n">y</code> <code class="o">=</code> <code class="n">daily</code><code class="p">[</code><code class="s1">'Total'</code><code class="p">]</code>

         <code class="n">model</code> <code class="o">=</code> <code class="n">LinearRegression</code><code class="p">(</code><code class="n">fit_intercept</code><code class="o">=</code><code class="kc">False</code><code class="p">)</code>
         <code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code>
         <code class="n">daily</code><code class="p">[</code><code class="s1">'predicted'</code><code class="p">]</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X</code><code class="p">)</code></pre>
<p>Finally, we can compare the total and predicted bicycle traffic visually
(see <a data-type="xref" href="#fig_0506-linear-regression_files_in_output_60_0">Figure 42-11</a>).</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">25</code><code class="p">]:</code> <code class="n">daily</code><code class="p">[[</code><code class="s1">'Total'</code><code class="p">,</code> <code class="s1">'predicted'</code><code class="p">]]</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">alpha</code><code class="o">=</code><code class="mf">0.5</code><code class="p">);</code></pre>
<figure><div class="figure" id="fig_0506-linear-regression_files_in_output_60_0">
<img alt="output 60 0" height="564" src="assets/output_60_0.png" width="600"/>
<h6><span class="label">Figure 42-11. </span>Our model’s prediction of bicycle traffic</h6>
</div></figure>
<p>From the fact that the data and model predictions don’t line
up exactly, it is evident that we have missed some key features. Either
our features are not complete (i.e., people decide whether to ride to
work based on more than just these features), or there are some
nonlinear relationships that we have failed to take into account (e.g.,
perhaps people ride less at both high and low temperatures).
Nevertheless, our rough approximation is enough to give us some
insights, and we can take a look at the coefficients of the linear model
to estimate how much each feature contributes to the daily bicycle
count:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">26</code><code class="p">]:</code> <code class="n">params</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">Series</code><code class="p">(</code><code class="n">model</code><code class="o">.</code><code class="n">coef_</code><code class="p">,</code> <code class="n">index</code><code class="o">=</code><code class="n">X</code><code class="o">.</code><code class="n">columns</code><code class="p">)</code>
         <code class="n">params</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">26</code><code class="p">]:</code> <code class="n">Mon</code>              <code class="o">-</code><code class="mf">3309.953439</code>
         <code class="n">Tue</code>              <code class="o">-</code><code class="mf">2860.625060</code>
         <code class="n">Wed</code>              <code class="o">-</code><code class="mf">2962.889892</code>
         <code class="n">Thu</code>              <code class="o">-</code><code class="mf">3480.656444</code>
         <code class="n">Fri</code>              <code class="o">-</code><code class="mf">4836.064503</code>
         <code class="n">Sat</code>             <code class="o">-</code><code class="mf">10436.802843</code>
         <code class="n">Sun</code>             <code class="o">-</code><code class="mf">10795.195718</code>
         <code class="n">holiday</code>          <code class="o">-</code><code class="mf">5006.995232</code>
         <code class="n">daylight_hrs</code>       <code class="mf">409.146368</code>
         <code class="n">Rainfall</code> <code class="p">(</code><code class="ow">in</code><code class="p">)</code>    <code class="o">-</code><code class="mf">2789.860745</code>
         <code class="n">dry</code> <code class="n">day</code>           <code class="mf">2111.069565</code>
         <code class="n">Temp</code> <code class="p">(</code><code class="n">F</code><code class="p">)</code>           <code class="mf">179.026296</code>
         <code class="n">annual</code>             <code class="mf">324.437749</code>
         <code class="n">dtype</code><code class="p">:</code> <code class="n">float64</code></pre>
<p class="pagebreak-before less_space">These numbers are difficult to interpret without some measure of their
uncertainty. We can compute these uncertainties quickly using bootstrap
resamplings of the data:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">27</code><code class="p">]:</code> <code class="kn">from</code> <code class="nn">sklearn.utils</code> <code class="kn">import</code> <code class="n">resample</code>
         <code class="n">np</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">seed</code><code class="p">(</code><code class="mi">1</code><code class="p">)</code>
         <code class="n">err</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">std</code><code class="p">([</code><code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="o">*</code><code class="n">resample</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">))</code><code class="o">.</code><code class="n">coef_</code>
                       <code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">1000</code><code class="p">)],</code> <code class="mi">0</code><code class="p">)</code></pre>
<p>With these errors estimated, let’s again look at the
results:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">28</code><code class="p">]:</code> <code class="nb">print</code><code class="p">(</code><code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">({</code><code class="s1">'effect'</code><code class="p">:</code> <code class="n">params</code><code class="o">.</code><code class="n">round</code><code class="p">(</code><code class="mi">0</code><code class="p">),</code>
                             <code class="s1">'uncertainty'</code><code class="p">:</code> <code class="n">err</code><code class="o">.</code><code class="n">round</code><code class="p">(</code><code class="mi">0</code><code class="p">)}))</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">28</code><code class="p">]:</code>                 <code class="n">effect</code>  <code class="n">uncertainty</code>
         <code class="n">Mon</code>            <code class="o">-</code><code class="mf">3310.0</code>        <code class="mf">265.0</code>
         <code class="n">Tue</code>            <code class="o">-</code><code class="mf">2861.0</code>        <code class="mf">274.0</code>
         <code class="n">Wed</code>            <code class="o">-</code><code class="mf">2963.0</code>        <code class="mf">268.0</code>
         <code class="n">Thu</code>            <code class="o">-</code><code class="mf">3481.0</code>        <code class="mf">268.0</code>
         <code class="n">Fri</code>            <code class="o">-</code><code class="mf">4836.0</code>        <code class="mf">261.0</code>
         <code class="n">Sat</code>           <code class="o">-</code><code class="mf">10437.0</code>        <code class="mf">259.0</code>
         <code class="n">Sun</code>           <code class="o">-</code><code class="mf">10795.0</code>        <code class="mf">267.0</code>
         <code class="n">holiday</code>        <code class="o">-</code><code class="mf">5007.0</code>        <code class="mf">401.0</code>
         <code class="n">daylight_hrs</code>     <code class="mf">409.0</code>         <code class="mf">26.0</code>
         <code class="n">Rainfall</code> <code class="p">(</code><code class="ow">in</code><code class="p">)</code>  <code class="o">-</code><code class="mf">2790.0</code>        <code class="mf">186.0</code>
         <code class="n">dry</code> <code class="n">day</code>         <code class="mf">2111.0</code>        <code class="mf">101.0</code>
         <code class="n">Temp</code> <code class="p">(</code><code class="n">F</code><code class="p">)</code>         <code class="mf">179.0</code>          <code class="mf">7.0</code>
         <code class="n">annual</code>           <code class="mf">324.0</code>         <code class="mf">22.0</code></pre>
<p>The <code>effect</code> column here, roughly speaking, shows how the number of
riders is affected by a change of the feature in question. For example,
there is a clear divide when it comes to the day of the week: there are
thousands fewer riders on weekends than on weekdays. We also see that
for each additional hour of daylight, 409 ± 26 more people choose to
ride; a temperature increase of one degree Fahrenheit encourages 179 ± 7
people to grab their bicycle; a dry day means an average of 2,111 ± 101
more riders, and every inch of rainfall leads 2,790 ± 186 riders to
choose another mode of transport. Once all these effects are accounted
for, we see a modest increase of 324 ± 22 new daily riders each year.</p>
<p>Our simple model is almost certainly missing some relevant information.
For example, as mentioned earlier, nonlinear effects (such as effects of
precipitation <em>and</em> cold temperature) and nonlinear trends within each
variable (such as disinclination to ride at very cold and very hot
temperatures) cannot be accounted for in a simple linear model.
Additionally, we have thrown away some of the finer-grained information
(such as the difference between a rainy morning and a rainy afternoon),
and we have ignored correlations between days (such as the possible
effect of a rainy Tuesday on Wednesday’s numbers, or the
effect of an unexpected sunny day after a streak of rainy days). These
are all potentially interesting effects, and you now have the tools to
begin exploring them if you wish<a data-startref="ix_ch42-asciidoc12" data-type="indexterm" id="idm45858733832864"/><a data-startref="ix_ch42-asciidoc11" data-type="indexterm" id="idm45858733832192"/><a data-startref="ix_ch42-asciidoc10" data-type="indexterm" id="idm45858733831520"/>!<a data-startref="ix_ch42-asciidoc1" data-type="indexterm" id="idm45858733830720"/><a data-startref="ix_ch42-asciidoc0" data-type="indexterm" id="idm45858733830016"/></p>
</div></section>
<div data-type="footnotes"><p data-type="footnote" id="idm45858736130864"><sup><a href="ch42.xhtml#idm45858736130864-marker">1</a></sup> Code to produce this figure can be found in the <a href="https://oreil.ly/o1Zya">online appendix</a>.</p></div></div></section></div></body></html>