- en: Chapter 9\. Cloud Entity Resolution Services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the last chapter, we saw how to scale up our entity resolution process to
    run on a Google Cloud–managed Spark cluster. This approach allowed us to match
    larger datasets in a reasonable time but it required us to do quite a bit of setup
    and management ourselves.
  prefs: []
  type: TYPE_NORMAL
- en: An alternative approach is to use entity resolution API provided by a cloud
    provider to perform the hard work for us. Google, Amazon, and Microsoft all offer
    these services.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will use the entity reconciliation service, provided as
    part of Google’s Enterprise Knowledge Graph API, to resolve the MCA and Companies
    House datasets we examined in Chapters [6](ch06.html#chapter_6) and [8](ch08.html#chapter_8).
    We will:'
  prefs: []
  type: TYPE_NORMAL
- en: Upload our standardized datasets to Google’s data warehouse, BigQuery.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Provide a mapping of our data schema to a standard ontology.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Invoke the API from the console (we will also invoke the API using a Python
    script).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use some basic SQL to process the results.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ​​To complete the chapter we will examine how well the service performs.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to BigQuery
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: BigQuery is Google’s fully managed, serverless data warehouse that enables scalable
    analysis over petabytes of data. It is a platform as a service that supports data
    querying and analysis using a dialect of SQL.
  prefs: []
  type: TYPE_NORMAL
- en: To begin, we select the BigQuery product from the Google Cloud console. Under
    ANALYSIS we select “SQL workspace.”
  prefs: []
  type: TYPE_NORMAL
- en: Our first step is to select “Create dataset” from the ellipsis menu alongside
    your project name, as shown in [Figure 9-1](#fig-9-1).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/hoer_0901.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-1\. BigQuery Create dataset
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In the pop-up window, as shown in [Figure 9-2](#fig-9-2), we need to name the
    Dataset ID as Chapter9, and then select a Location Type. You can then select a
    specific Region if you prefer or simply accept the Multi-region default. Optionally,
    you can add a number of days after which the table expires automatically.
  prefs: []
  type: TYPE_NORMAL
- en: Once we have an empty dataset created, our next task is to upload our MCA and
    Companies House tables. We can upload these tables from the data we saved in the
    Google Cloud Storage bucket in [Chapter 8](ch08.html#chapter_8).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/hoer_0902.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-2\. BigQuery Create dataset config
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: With the dataset selected, we can click “+ Add,” or Add Data and then select
    Google Cloud Storage as the source (as shown in [Figure 9-3](#fig-9-3)). You can
    then browse to your Cloud Storage bucket and select the *mari_clean.csv* file.
    Select the Chapter9 dataset as the destination and name the table *mari*. Under
    Schema, click the “Auto detect” checkbox. You can accept the remainder of the
    default settings.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/hoer_0903.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-3\. BigQuery Create table
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Repeat this procedure for the *basic_clean.csv* file, naming it *basic*. You
    can then select the table from the dataset to examine the schema. Selecting Preview
    will give you a view of the first few rows, as shown in [Figure 9-4](#fig-9-4).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/hoer_0904.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-4\. BigQuery table schema
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Now that we have successfully loaded our data, we need to tell the Enterprise
    Knowledge Graph API how to map our schema and then run a reconciliation job.
  prefs: []
  type: TYPE_NORMAL
- en: Enterprise Knowledge Graph API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Google Enterprise Knowledge Graph API provides a lightweight entity resolution
    service that they call Entity Reconciliation. The service uses an AI model trained
    on Google data. It uses a parallel version of *hierarchical agglomerative clustering*.
  prefs: []
  type: TYPE_NORMAL
- en: Hierarchical Agglomerative Clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is a “bottom-up” approach to clustering entities. Each entity starts in
    its own cluster and then they are aggregated depending upon their similarity.
  prefs: []
  type: TYPE_NORMAL
- en: At the time of writing, the Entity Reconciliation service is at Preview status
    and is made available on Pre-GA terms, details of which are available on the [Google
    Cloud website](https://oreil.ly/dThBk).
  prefs: []
  type: TYPE_NORMAL
- en: To enable the API, select Enterprise KG under Artificial Intelligence from the
    console navigation menu. From here you can click “Enable the Enterprise Knowledge
    Graph API” for your project.
  prefs: []
  type: TYPE_NORMAL
- en: Schema Mapping
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To set up our entity resolution job, we first need to map our data schema onto
    the schema that the Google Entity Reconciliation API understands. We do this by
    creating a mapping file for each data source we are going to use. The API uses
    a human-readable simple format language called YARRRML to define the mappings
    between source schema and a target ontology from [schema.org](https://schema.org).
    It supports three different entity types: Organization, Person, and Local Business.
    For our example, we will use the Organization schema.'
  prefs: []
  type: TYPE_NORMAL
- en: To begin, we click on Schema Mapping and then select “Create a Mapping” in the
    Organization box. This brings us to an editor where we can modify and save a template
    mapping file. The mapping file is divided into a prefix section that tells the
    API which model and schema reference we are going to use. The mapping section
    then lists each entity type contained in the dataset. For each entity type, we
    specify the sources, a subject key (`s`) that uniquely refers to an entity in
    the dataset, and then the predicate list (`po`) which specifies the attributes
    of the entity we wish to match on.
  prefs: []
  type: TYPE_NORMAL
- en: 'The default template is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Starting with the mapping file for the MCA dataset, edit the default template
    as follows, remembering to insert your project name in the source line. This file
    is also available in the repository as *Chapter9SchemaMari*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Note here that we are pointing the API to the *mari* BigQuery table we created
    earlier in the Chapter9 dataset. We are using the `unique_id` column as our subject
    key, and we are mapping our `Postcode` field to the `postalCode` property in the
    schema and our `CompanyName` field to the `name` property.
  prefs: []
  type: TYPE_NORMAL
- en: 'Save this edited file into your Google Storage bucket under the *handsonentityresolution*
    directory as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Repeat this process to create a mapping file for the Companies House dataset,
    saving in the same location as *Chapter9SchemaBasic*. Remember to substitute *basic*
    for *mari* in the relevant lines and reference these entities as *company2*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: We now have our datasets and our mapping files, so we can run an entity resolution
    (or reconciliation) job.
  prefs: []
  type: TYPE_NORMAL
- en: Reconciliation Job
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To start a reconciliation job, select Jobs from the Enterprise KG section in
    the console navigation menu, as shown in [Figure 9-5](#fig-9-5).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/hoer_0905.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-5\. Start a reconciliation job
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Select the RUN A JOB tab, as shown in [Figure 9-6](#fig-9-6).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/hoer_0906.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-6\. Run an API Job for Entity Reconciliation
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'From the pop-up menu:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 1: Click “Select entity type”'
  prefs: []
  type: TYPE_NORMAL
- en: Select Organization.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 2: Add BigQuery data sources'
  prefs: []
  type: TYPE_NORMAL
- en: Browse to the BigQuery path and select the *mari* table. Then select the matching
    mapping table by browsing to the *handsonentityresolution* directory in your bucket
    and selecting the *Chapter9SchemaMari* file we created earlier.
  prefs: []
  type: TYPE_NORMAL
- en: Click Add Another BigQuery Datasource and repeat the process for the *basic*
    table and mapping file.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 3: Set BigQuery data destination'
  prefs: []
  type: TYPE_NORMAL
- en: Browse and select the Chapter9 BigQuery dataset to tell the API where to write
    its results.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 4: Advanced settings (optional)'
  prefs: []
  type: TYPE_NORMAL
- en: For the final step, we can specify a previous result table so that the entity
    reconciliation service assigns consistent IDs to entities across different jobs,
    as shown in [Figure 9-7](#fig-9-7). This can be particularly useful to update
    existing entity records as new data is added.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/hoer_0907.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-7\. Entity Reconciliation API advanced settings
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The number of clustering rounds (iterations of the entity resolution model)
    can be specified; the higher the number, the more loosely entities are merged
    into the same cluster. The default is fine for our use case.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we can click Done and start our job. Assuming all is well, we should
    then see a new job created under Job History, as shown in [Figure 9-8](#fig-9-8).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/hoer_0908.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-8\. Entity Reconciliation Job History
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We can watch the Job Display Status column to monitor the progress of our job
    as it moves sequentially through the display states shown in [Table 9-1](#table-9-1),
    and then finally displays Finished when complete.
  prefs: []
  type: TYPE_NORMAL
- en: Table 9-1\. Job display state
  prefs: []
  type: TYPE_NORMAL
- en: '| Job display state | Code state | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Running | `JOB_<wbr>STATE_<wbr>RUNNING` | The job is in progress. |'
  prefs: []
  type: TYPE_TB
- en: '| Knowledge extraction | `JOB_<wbr>STATE_<wbr>KNOWLEDGE_<wbr>EXTRACTION` |
    Enterprise Knowledge Graph is pulling data out from BigQuery and creating features.
    |'
  prefs: []
  type: TYPE_TB
- en: '| Reconciliation preprocessing | `JOB_<wbr>STATE_<wbr>RECON_<wbr>​PRE⁠PROCESSING`
    | The job is at the reconciliation preprocessing step. |'
  prefs: []
  type: TYPE_TB
- en: '| Clustering | `JOB_<wbr>STATE_<wbr>CLUSTERING` | The job is at the clustering
    step. |'
  prefs: []
  type: TYPE_TB
- en: '| Exporting clusters | `JOB_<wbr>STATE_<wbr>EXPORTING_<wbr>CLUSTERS` | The
    job is writing output into the BigQuery destination dataset. |'
  prefs: []
  type: TYPE_TB
- en: This job should take approximately 1 hour 20 minutes but the duration varies
    widely at this Preview stage of the product.
  prefs: []
  type: TYPE_NORMAL
- en: When the job is finished, if we look in the BigQuery SQL workspace we should
    see a new table in our Chapter9 dataset called something like clusters_15719257497877843494,
    as shown in [Figure 9-9](#fig-9-9).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/hoer_0909.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-9\. BigQuery clusters results table
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Selecting the clusters_15719257497877843494 table and then selecting the Preview
    tab gives us a view of the results. [Figure 9-10](#fig-9-10) shows the first few
    rows.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/hoer_0910.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-10\. BigQuery cluster results preview
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Let’s consider the columns in the output:'
  prefs: []
  type: TYPE_NORMAL
- en: The *cluster_id* gives the unique reference of the cluster to which the Entity
    Reconciliation API has assigned the source entity.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *source_name* column gives us the name of the source table, in our case
    either *mari* or *basic*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *source_key* column contains the `unique_id` of the row in the source table.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *confidence* score, between 0 and 1, indicates how strongly a record is
    associated with a given cluster.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *assignment_age* column is an internal API reference.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *cloud_kg_mid* column contains an MID value link to the entity in the Google
    Cloud Enterprise Knowledge Graph if the API can resolve a match. This can be used
    to look up additional details that Google has on the entity using the Cloud Enterprise
    Knowledge Graph API.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As every entity in both the *mari* and *basic* tables is assigned to a cluster,
    the row count for this table is the sum of the row counts for the source tables.
    In our case, this is over 5 million rows. At a glance, it’s not easy to identify
    which entities the API has matched, so we need to refine this data a little.
  prefs: []
  type: TYPE_NORMAL
- en: Result Processing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With our entity reconciliation results we can then use BigQuery SQL to process
    this raw information into an easier form for us to examine the resolved entities.
  prefs: []
  type: TYPE_NORMAL
- en: To start, we click “Compose a New Query”, which takes us to a SQL editor. You
    can cut and paste the SQL template from the *Chapter9.sql* file.
  prefs: []
  type: TYPE_NORMAL
- en: First we need to create a temporary table containing only rows whose `cluster_id`
    has at least one MCA match. We do this by building a subset of the cluster table
    whose rows have “mari” as the `source_name`. Then we find the intersection between
    the rows of this subset and the rows of the full cluster table using an `INNER
    JOIN` on matching `cluster_id`s.
  prefs: []
  type: TYPE_NORMAL
- en: 'Make sure to replace the cluster table name with the name of your results table,
    which will be in the format `clusters_*<job reference>*`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The resulting temporary table now has only 151 rows. Next we create a second
    temporary table, this time with the subset of clusters that have both an MCA match
    and at least one Companies House match; i.e., we remove clusters with only an
    MCA match.
  prefs: []
  type: TYPE_NORMAL
- en: To do this we select those `cluster_id`s with a count of greater than 1 and
    again find the intersection of this subset with the first temporary table using
    an `INNER JOIN` on the matching `cluster_id`s.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we have a table of clusters containing only rows where the entity is found
    in both the Companies House and MCA datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: This table now has 106 rows. We have the population we are looking for, so we
    can create a persistent results table picking up the `CompanyName` and `Postcode`
    from the source tables so that we can examine the results.
  prefs: []
  type: TYPE_NORMAL
- en: 'We need to build this table in two parts. First, for the rows that refer to
    the Companies House data we need to look up the identifier in the `source_key`
    column and use that to retrieve the corresponding name and postcode. Then we need
    to do the same for rows that refer to the MCA data. We use the `UNION ALL` statement
    to join these two datasets and then `ORDER BY` `confidence` first and then `cluster_id`.
    This means that entities assigned to the same cluster are adjacent in the table
    for easy viewing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: This gives us our results table, which looks like that given in [Figure 9-11](#fig-9-11).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/hoer_0911.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-11\. Processed results table
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In the first row, we can see that both the MCA entity with `CompanyName` CREW
    AND CONCIERGE, `Postcode` BS31 1TP, and `unique_id` 18 has been assigned to cluster
    r-03fxqun0t2rjxn. In the second row, the Companies House entity with `CompanyName`
    CREW and CONCIERGE, the same `Postcode`, and `unique_id` 1182534 has been assigned
    to the same cluster.
  prefs: []
  type: TYPE_NORMAL
- en: This means the Google Entity Reconciliation API has grouped these records into
    the same cluster, i.e., resolved these rows as referring to the same real-world
    entity, with a confidence rating of 0.7.
  prefs: []
  type: TYPE_NORMAL
- en: Before we examine these results in detail, we’ll take a quick detour to see
    how to invoke the API from Python instead of the cloud console.
  prefs: []
  type: TYPE_NORMAL
- en: Entity Reconciliation Python Client
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Google Enterprise Knowledge Graph API also supports a Python client to create,
    cancel, and delete entity reconciliation jobs. We can use the Cloud Shell virtual
    machine to run these Python scripts and launch these jobs.
  prefs: []
  type: TYPE_NORMAL
- en: To activate Google Cloud Shell, click on the terminal symbol in the top right
    of the console. This will open a window with a command-line prompt.
  prefs: []
  type: TYPE_NORMAL
- en: 'A Python script to invoke the entity reconciliation job is included in the
    repository. To transfer a copy to your Cloud Shell machine we can use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: A pop-up window will ask you to authorize the Cloud Shell to connect to your
    bucket.
  prefs: []
  type: TYPE_NORMAL
- en: 'The script, *Chapter9.py*, is reproduced here. You can use the Cloud Shell
    editor to edit this file to reference your project and bucket:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The Cloud Shell has Python installed so we can simply run this script from
    the command prompt with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'To process the results, we can use the SQL script we examined previously. To
    copy this from your Cloud Storage bucket:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we run this BigQuery script using:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that if the results table has already been created by running this query
    from the SQL workspace, this command will fail because the table already exists.
    You can delete the table using:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Now we can examine how well the API performed on our example.
  prefs: []
  type: TYPE_NORMAL
- en: Measuring Performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Recall from our Preview of the BigQuery results table that we have 106 rows.
    The distribution of match confidence is shown in [Table 9-2](#table-9-2).
  prefs: []
  type: TYPE_NORMAL
- en: Table 9-2\. Match confidence
  prefs: []
  type: TYPE_NORMAL
- en: '| Number of matches | Confidence |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | 0.7 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 0.8 |'
  prefs: []
  type: TYPE_TB
- en: '| 45 | 0.99 |'
  prefs: []
  type: TYPE_TB
- en: '| 44 | No match found |'
  prefs: []
  type: TYPE_TB
- en: Two of the MCA entities matched to two of the Companies House entities.
  prefs: []
  type: TYPE_NORMAL
- en: Looking back to [Figure 9-11](#fig-9-11), we can see the first seven matches
    in ascending order of confidence. You can see that the entity reconciliation service
    has been able to match these entities in spite of minor spelling differences or
    postcode variations. The remainder are exact matches on both `CompanyName` and
    `Postcode` with the exception of a mismatched hyphen between INDIE PEARL and INDIE-PEARL,
    which has not affected the confidence score.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we assume that the unique matches are true positive matches and that the
    two additional matches are false positives, then we can evaluate our performance
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="upper T r u e p o s i t i v e m a t c h e s left-parenthesis
    upper T upper P right-parenthesis equals 52"><mrow><mi>T</mi> <mi>r</mi> <mi>u</mi>
    <mi>e</mi> <mi>p</mi> <mi>o</mi> <mi>s</mi> <mi>i</mi> <mi>t</mi> <mi>i</mi> <mi>v</mi>
    <mi>e</mi> <mi>m</mi> <mi>a</mi> <mi>t</mi> <mi>c</mi> <mi>h</mi> <mi>e</mi> <mi>s</mi>
    <mo>(</mo> <mi>T</mi> <mi>P</mi> <mo>)</mo> <mo>=</mo> <mn>52</mn></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="upper F a l s e p o s i t i v e m a t c h e s left-parenthesis
    upper F upper P right-parenthesis equals 2"><mrow><mi>F</mi> <mi>a</mi> <mi>l</mi>
    <mi>s</mi> <mi>e</mi> <mi>p</mi> <mi>o</mi> <mi>s</mi> <mi>i</mi> <mi>t</mi> <mi>i</mi>
    <mi>v</mi> <mi>e</mi> <mi>m</mi> <mi>a</mi> <mi>t</mi> <mi>c</mi> <mi>h</mi> <mi>e</mi>
    <mi>s</mi> <mo>(</mo> <mi>F</mi> <mi>P</mi> <mo>)</mo> <mo>=</mo> <mn>2</mn></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="upper F a l s e n e g a t i v e m a t c h e s left-parenthesis
    upper F upper N right-parenthesis equals 44"><mrow><mi>F</mi> <mi>a</mi> <mi>l</mi>
    <mi>s</mi> <mi>e</mi> <mi>n</mi> <mi>e</mi> <mi>g</mi> <mi>a</mi> <mi>t</mi> <mi>i</mi>
    <mi>v</mi> <mi>e</mi> <mi>m</mi> <mi>a</mi> <mi>t</mi> <mi>c</mi> <mi>h</mi> <mi>e</mi>
    <mi>s</mi> <mo>(</mo> <mi>F</mi> <mi>N</mi> <mo>)</mo> <mo>=</mo> <mn>44</mn></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="upper P r e c i s i o n equals StartFraction upper T upper P
    Over left-parenthesis upper T upper P plus upper F upper P right-parenthesis EndFraction
    equals StartFraction 52 Over left-parenthesis 52 plus 2 right-parenthesis EndFraction
    almost-equals 96 percent-sign"><mrow><mi>P</mi> <mi>r</mi> <mi>e</mi> <mi>c</mi>
    <mi>i</mi> <mi>s</mi> <mi>i</mi> <mi>o</mi> <mi>n</mi> <mo>=</mo> <mfrac><mrow><mi>T</mi><mi>P</mi></mrow>
    <mrow><mo>(</mo><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>P</mi><mo>)</mo></mrow></mfrac>
    <mo>=</mo> <mfrac><mn>52</mn> <mrow><mo>(</mo><mn>52</mn><mo>+</mo><mn>2</mn><mo>)</mo></mrow></mfrac>
    <mo>≈</mo> <mn>96</mn> <mo>%</mo></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="upper R e c a l l equals StartFraction upper T upper P Over left-parenthesis
    upper T upper P plus upper F upper N right-parenthesis EndFraction equals StartFraction
    52 Over left-parenthesis 52 plus 44 right-parenthesis EndFraction almost-equals
    54.2 percent-sign"><mrow><mi>R</mi> <mi>e</mi> <mi>c</mi> <mi>a</mi> <mi>l</mi>
    <mi>l</mi> <mo>=</mo> <mfrac><mrow><mi>T</mi><mi>P</mi></mrow> <mrow><mo>(</mo><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>N</mi><mo>)</mo></mrow></mfrac>
    <mo>=</mo> <mfrac><mn>52</mn> <mrow><mo>(</mo><mn>52</mn><mo>+</mo><mn>44</mn><mo>)</mo></mrow></mfrac>
    <mo>≈</mo> <mn>54</mn> <mo>.</mo> <mn>2</mn> <mo>%</mo></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: So the entity reconciliation gives us excellent precision but with relatively
    poor recall.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter we have seen how to use the Google Cloud Entity Reconciliation
    API to resolve our organization entities. We have seen how to configure and run
    matching jobs from both the cloud console and via the Python client.
  prefs: []
  type: TYPE_NORMAL
- en: Using the API abstracts us away from much of the complexity of configuring our
    own matching process. It is also inherently scalable to very large datasets (hundreds
    of millions of rows). However, we are constrained to using a set of predefined
    schemas and we don’t have the freedom to tune the matching algorithm to optimize
    the recall/precision trade-off for our use case.
  prefs: []
  type: TYPE_NORMAL
