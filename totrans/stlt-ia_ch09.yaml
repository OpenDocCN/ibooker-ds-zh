- en: 10 A customer support chatbot with LangGraph and Streamlit
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Developing a chatbot frontend with Streamlit's chat elements
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using LangGraph and LangChain to streamline an advanced AI app
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How embeddings and vector databases work
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Augmenting an LLM's pre-trained knowledge using Retrieval Augmented Generation
    (RAG)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enabling an LLM to access and execute real-world actions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a fun and engaging experience—like the trivia game we built in chapter
    nine—is exciting, but the true power of AI lies in its ability to drive real business
    value. AI isn't just about answering questions or generating text; it's about
    transforming industries, streamlining operations, and enabling entirely new business
    models.
  prefs: []
  type: TYPE_NORMAL
- en: However, building AI applications that deliver economic value requires more
    than just calling a pre-trained model. For AI to be useful in real-world scenarios,
    it needs to be aware of the context in which it operates, connect to external
    data sources, and take meaningful actions. Companies need AI to understand and
    respond to domain-specific queries, interact with business systems, and provide
    personalized assistance.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we''ll build such an application: a customer service chatbot
    that will retrieve real company data, help customers track and cancel orders,
    and intelligently decide when to escalate issues to a human agent. By the end
    of this chapter, you''ll understand how to integrate LLMs with private knowledge
    bases, implement retrieval-augmented generation (RAG), and enable an AI agent
    to take action in the real world. Let''s dive in.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The GitHub repo for this book is [https://github.com/aneevdavis/streamlit-in-action](https://github.com/aneevdavis/streamlit-in-action).
    The chapter_10 folder has this chapter's code and a requirements.txt file with
    exact versions of all the required Python libraries.
  prefs: []
  type: TYPE_NORMAL
- en: '10.1 Nibby: A customer service bot'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Under the leadership of Note n' Nib's new CEO—renowned for his legendary decision-making
    prowess, aided by a certain dashboard revered across the company—the brand has
    flourished into a stationery powerhouse with rocketing sales.
  prefs: []
  type: TYPE_NORMAL
- en: But success brings its own challenges. The customer support department is swamped
    with calls from buyers who are impatient for their orders to arrive or seeking
    advice about fountain pen maintenance. After a month of complaints about long
    wait times, the CEO summons the one person known company-wide for reliable innovation.
  prefs: []
  type: TYPE_NORMAL
- en: 'And so it is that *you* are tasked with solving the support crisis. When you''re
    not delivering seminars on Streamlit, you''re reading up on the latest advances
    in AI; it is not long before an intriguing possibility hits you: might it be possible
    to *automate* customer support?'
  prefs: []
  type: TYPE_NORMAL
- en: 'Over the course of a sleepless night, you sketch out plans for a Streamlit
    support bot named Nibby. Whispers of your project spread across the company. "We
    are saved!" some declare. "Nibby will not fail us!" Skeptics scoff: "''Tis folly!
    No *robot* can fix this."'
  prefs: []
  type: TYPE_NORMAL
- en: Who will prove right? Let's find out.
  prefs: []
  type: TYPE_NORMAL
- en: 10.1.1 Stating the concept and requirements
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As always, we start with a distilled one-line description of what we intend
    to build.
  prefs: []
  type: TYPE_NORMAL
- en: Concept
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Nibby, a customer support chatbot that can help Note n' Nib's customers with
    information and basic service requests
  prefs: []
  type: TYPE_NORMAL
- en: '"Customer support" obviously spans a lot of territory, so let''s define the
    exact requirements more clearly.'
  prefs: []
  type: TYPE_NORMAL
- en: Requirements
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In our vision of automating customer support, Nibby will be able to:'
  prefs: []
  type: TYPE_NORMAL
- en: hold a human-like conversation with a customer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: answer relevant questions about Note n' Nib and its products based on a custom
    knowledge base
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'handle the following requests from the customer:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: tracking an order
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: canceling an order
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: redirect to a human customer support agent if it cannot fulfill the request
    on its own
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In essence, we want Nibby to take as much load off Note n' Nib's overworked
    human support agents as possible. Nibby should act as a "frontline" agent who
    can take care of most basic requests, such as providing product information or
    canceling orders, and redirect to a human only when necessary.
  prefs: []
  type: TYPE_NORMAL
- en: What's out of scope
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To keep this project manageable and small enough to fit into this chapter,
    we''ll decide *not* to implement the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Storing or remembering prior conversations with a user
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Any "actions" other than the two provided above, i.e., tracking and canceling
    orders
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Actual working logic for the two actions discussed, e.g., building an order
    tracking or cancellation system
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: From a learning perspective, we truly want to focus on building a relatively
    complex AI system that can converse with users, understand a custom knowledge
    base, and take real-world actions.
  prefs: []
  type: TYPE_NORMAL
- en: The specific actions we enable the app to perform don't matter. For instance,
    the fact that our app can cancel an order—as opposed to replacing an item—is not
    of any particular significance. Indeed, as implied by the third point above, the
    order cancellation we will implement is dummy "toy" logic. What *is* significant
    is that our bot should be able to intelligently choose to run that logic based
    on the free-form conversation the user is having with it.
  prefs: []
  type: TYPE_NORMAL
- en: 10.1.2 Visualizing the user experience
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The user interface for Nibby might be the most straightforward among all the
    apps in this book. Figure 10.1 shows a sketch of what we'll build.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../Images/10__image001.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.1 UI sketch for Nibby, our customer support chatbot.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Nibby's UI isn't significantly different from any chat or instant messaging
    app you may have used—from WhatsApp on your phone to Slack on your corporate laptop.
    You'll notice a familiar-looking text box at the bottom for users to type messages.
    Each user message triggers an AI response, which is appended to the conversation
    view above.
  prefs: []
  type: TYPE_NORMAL
- en: 10.1.3 Brainstorming the implementation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The difficult part of building this app will be the backend—specifically, getting
    the bot to answer questions correctly and connect to outside tools. Figure 10.2
    shows our overall design.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../Images/10__image002.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.2 Overall design for Nibby
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: While our trivia app from chapter 9 had an interesting design when it came to
    state management, its "intelligent" aspect was fairly simple—feed a prompt to
    an LLM and have it respond.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other hand, a customer support app with the capabilities we''re envisioning
    has a more involved design in at least two respects:'
  prefs: []
  type: TYPE_NORMAL
- en: It needs a way to augment a customer's query with private knowledge about the
    company that a human agent would possess.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It needs to be able to execute code in the real world.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Figure 10.2 gives a basic overview of how we achieve these. When a user message
    comes in through our frontend, we retrieve context relevant to the message from
    a private knowledge store—a vector database, as we'll see later—and send that
    to the LLM.
  prefs: []
  type: TYPE_NORMAL
- en: We also organize the actions we want our bot to be able to take into so-called
    *tools* and make the LLM aware of their existence, what each tool does, and how
    to call them. For any given input, the LLM can either issue a call to a tool or
    respond to the user directly.
  prefs: []
  type: TYPE_NORMAL
- en: In the former case, we execute the tool as specified by the LLM and send the
    results for further processing, while in the latter case, we append the response
    to a conversation view on the frontend and await the user's next message.
  prefs: []
  type: TYPE_NORMAL
- en: 10.1.4 Installing dependencies
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We''ll be using several Python libraries in this chapter. To get everything
    ready in advance, install them all at once by running the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 10.2 Creating a basic chatbot
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Chapter nine introduced LLMs and demonstrated how to use the OpenAI API for
    simple applications. While OpenAI's API is easy to integrate, developing more
    sophisticated AI-driven apps—such as those leveraging Retrieval-Augmented Generation
    (RAG) or agent-based workflows, which we'll encounter soon—adds complexity.
  prefs: []
  type: TYPE_NORMAL
- en: A new ecosystem of libraries and tools has emerged to make creating complex
    AI apps as easy as possible. In this chapter, we'll explore LangGraph and LangChain,
    two libraries that work together to smooth the application creation process.
  prefs: []
  type: TYPE_NORMAL
- en: 10.2.1 Intro to LangGraph and LangChain
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: LLMs have undoubtedly been the most influential technological advance of the
    last decade. At their core, interacting with an LLM consists of providing a prompt
    that the LLM can "complete." That's what everything else is built around.
  prefs: []
  type: TYPE_NORMAL
- en: 'Contrast this with the complexities that modern AI applications have to deal
    with:'
  prefs: []
  type: TYPE_NORMAL
- en: Handling multi-step workflows (e.g., retrieving information before responding)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integrating with external tools
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Retaining conversation context across multiple turns
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Managing this complexity manually is difficult, which is where LangChain and
    LangGraph—both Python libraries—come in. LangChain provides building blocks for
    working with LLMs, including prompt management, memory, and tool integration.
    LangGraph—developed by the same company—extends LangChain by structuring AI workflows
    as *graphs*, allowing for decision-making, branching logic, and multi-step processing.
    By combining these, we can design structured, intelligent AI applications that
    go beyond simple chat responses—enabling Nibby to retrieve knowledge, call APIs,
    and make decisions dynamically.
  prefs: []
  type: TYPE_NORMAL
- en: In the rest of this chapter, we'll use these libraries extensively to achieve
    the functionality we want.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Since we will model our chatbot as a *graph* in LangGraph, we'll primarily speak
    about and refer to LangGraph rather than LangChain. However, you'll notice that
    many of the underlying classes and functions we'll use are imported from LangChain.
  prefs: []
  type: TYPE_NORMAL
- en: 10.2.2 Graphs, nodes, edges, and state
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In LangGraph, you construct an AI application by building a *graph* of *nodes*
    that transform the application's *state*. If you don't have a background in computer
    science, that statement might trip you up, so let's break it down.
  prefs: []
  type: TYPE_NORMAL
- en: What is a graph, exactly?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In graph theory, a graph is a network of interconnected vertices (also called
    nodes) and edges, which connect the vertices with each other. Software developers
    often use graphs to create conceptual models of real-world objects and their relationships.
    For instance, figure 10.3 shows a graph of people you might expect to find on
    a social media website.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../Images/10__image003.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.3 Using a graph to model friend relationships in a social network
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Here, each person is a vertex or node (shown by a circle), and the "friend"-relationship
    between any two people is an edge (the lines between the circles).
  prefs: []
  type: TYPE_NORMAL
- en: By modeling relationships in this way, the social media website can apply various
    algorithms developed for graphs to do useful real-world things. For instance,
    there's an algorithm called breadth-first search (BFS) that finds the shortest
    path from one node to any other node. In this case, we could use it to find the
    fewest common friends required to connect two people.
  prefs: []
  type: TYPE_NORMAL
- en: In LangGraph, we model an application as a graph of *actions*, where a *node*
    signifies a single action that the application performs. A graph has a *state*,
    simply a collection of named attributes with values (similar to Streamit's concept
    of "session state"). Each node takes the current state of the graph as input,
    does something to modify the state, and returns the new state as its output.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../Images/10__image004.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.4 In LangGraph, nodes take in the graph state and modify it.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: An edge in the graph represents a connection between two nodes, or in other
    words, the fact that the output of one node may be the input to another. Unlike
    in the case of a social media graph where the edges had no direction (i.e., if
    two people are friends, each is a friend of the other), edges in LangGraph are
    *directed* because one node in the edge is executed *before* the other. Visually,
    we represent the direction as an arrow on the edge (figure 10.4).
  prefs: []
  type: TYPE_NORMAL
- en: The input to the graph is its initial state that is passed to the *first* node
    that's executed, while the output is the final state returned by the *last* node
    that's executed.
  prefs: []
  type: TYPE_NORMAL
- en: That was a fair bit of theory; let's now consider a toy example (figure 10.5)
    to make this all real.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../Images/10__image005.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.5 A graph in LangGraph that computes the sum of squares of numbers
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The application shown in figure 10.5 is a very simple one. There's no AI involved;
    it's just a program that takes a list of numbers and returns the sum of their
    squares—e.g., for the input `[3, 4]`, the graph would calculate the output `25`
    (`3^2 + 4^2 = 25`)
  prefs: []
  type: TYPE_NORMAL
- en: 'The graph''s state contains three values: `numbers`, `squares`, and `answer`.
    `numbers` holds the list of input values (e.g. `[3, 4]`), while `squares` and
    `answer` don''t have a value to start with.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Each LangGraph graph has dummy nodes called `START` and `END`, which represent
    the start and end of execution. There are two other "real" nodes: `squaring_node`
    and `summing_node`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s how the graph is executed:'
  prefs: []
  type: TYPE_NORMAL
- en: The `START` node receives the initial state.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since there's a directed edge from `START` to `squaring_node`, `squaring_node`
    is executed first.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`squaring_node` takes in the starting state, squares the numbers in the `numbers`
    list and saves the new list (`[9, 16]`) under the variable `squares` in the state.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As there's an edge from `squaring_node` to `summing_node`, `summing_node` takes
    as input this modified state returned by `squaring_node`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`summing_node` adds up the numbers in `squares`, and saves the result as `answer`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`summing_node` has an edge to `END`, which means the end of execution. The
    final state returned will contain `25` under `answer`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Of course, this is a simple graph with only one path the execution can take.
    In a later part of this chapter, you'll encounter a graph with multiple paths—where
    a single node may branch into multiple nodes based on the state at that point.
  prefs: []
  type: TYPE_NORMAL
- en: I hope this helped crystallize the concept of graphs and how LangGraph uses
    them to perform a task. It's now time to use what we've learned to start building
    our app.
  prefs: []
  type: TYPE_NORMAL
- en: 10.2.3 A one-node LLM graph
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The basic graph we built in the previous section had nothing to do with AI or
    LLMs. Indeed, you can use LangGraph to build anything you like, whether or not
    AI is involved, but in practice, the point of LangGraph is to make building AI
    applications easier.
  prefs: []
  type: TYPE_NORMAL
- en: Creating and running your first graph
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In chapter nine, we encountered the OpenAI API's chat completions endpoint.
    In this endpoint, we pass a list of messages to the API, which predicts the next
    message in the conversation. In LangGraph, such an application could be represented
    by a simple one-node graph, as shown in figure 10.6.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../Images/10__image006.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.6 A basic single-node (apart from START and END) graph
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The state of the graph consists of a single variable, `messages`, which is—as
    you might expect—a list of messages.
  prefs: []
  type: TYPE_NORMAL
- en: The only node in the graph, `assistant_node`, passes `messages` to an LLM, and
    returns the same list with an AI response message appended.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.1 shows this graph translated to real (non-Streamlit) Python code.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.1 graph_example.py
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: (`chapter_10/graph_example.py` in the GitHub repo)
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we initialize the LLM in this line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This is similar to what we did in chapter 9 when we created an OpenAI API client.
    LangChain—a library closely related to LangGraph—provides a class called `ChatOpenAI`
    that does essentially the same thing but is slightly easier to use. As before,
    don't forget to replace `sk-proj...` with your actual OpenAI API key.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the next part:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: As we've discussed, a graph has a state. For each graph you define, you specify
    what fields exist in the state by creating a class that contains those fields.
  prefs: []
  type: TYPE_NORMAL
- en: In the above two lines, we're creating `MyGraphState` to represent the state
    of the graph we're about to define. In line with the example in figure 10.6, `MyGraphState`
    contains one field—`messages`—which is a list of objects of the type `AnyMessage`.
  prefs: []
  type: TYPE_NORMAL
- en: In chapter nine, we saw that each message in an (OpenAI) LLM conversation has
    a *role*—one of `"user"`, `"assistant"`, or `"system"`. In LangChain, the same
    concept is represented by the `AnyMessage` superclass. `HumanMessage`, `AIMessage`,
    and `SystemMessage` are subclasses that inherit functionality from `AnyMessage`,
    and correspond to the `"user"`, `"assistant"`, and `"system"` roles, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: '`MyGraphState` itself is a subclass of `TypedDict`, a specialized dictionary
    type from Python''s typing module that allows us to define a dictionary with a
    fixed set of keys and associated value types. Since it inherits all of TypedDict''s
    behaviors, we can treat instances of TypedDict—and therefore, MyGraphState— as
    regular dictionaries, using the same syntax for accessing its keys (i.e. the fields
    in the class) and values.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We don't technically *have* to use a `TypedDict` to represent the state of the
    graph. We could also have used a regular class, a dataclass, or a Pydantic `BaseModel,`
    which we used in chapter nine. We've introduced and used TypedDict here because
    it plays well with `MessagesState`, a built-in LangGraph class that we'll discuss
    shortly.
  prefs: []
  type: TYPE_NORMAL
- en: The next line, `builder = StateGraph(MyGraphState)`, initializes the construction
    of our graph. Here we're telling LangGraph that we're building a `StateGraph`—the
    type of graph we've been talking about where the nodes read from and write into
    a shared state—whose state is represented by a `MyGraphState` instance (which,
    as we've seen, will have a `messages` list).
  prefs: []
  type: TYPE_NORMAL
- en: 'We now define the only node in our graph thus:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Each node in a LangGraph graph is a regular Python function that accepts the
    current state of the graph—a `MyGraphState` instance—as input, and returns the
    parts of the state it wants to modify.
  prefs: []
  type: TYPE_NORMAL
- en: The `assistant_node` we've defined above is quite minimal; it simply passes
    the `messages` list—accessed using square brackets as `state["messages"]` just
    like in a regular dictionary—to the `invoke` method of `llm`, obtaining the AI's
    response message. It then modifies the `"messages"` key of the state, adding `ai_response_message`
    to the end, and returns the result.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In the above code, since `MyGraphState` only has a single key, `messages`, it
    seems that `assistant_node` is simply returning the entirety of the modified state.
    That's not strictly true—it's actually only returning the keys it wants to modify,
    leaving any other keys untouched. This will become clear in later sections.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we''ve created our only node, it''s time to put it in our graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The first line adds a node called `assistant` to the graph, pointing to the
    `assistant_node` function we just developed as the logic for the node.
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned earlier, each graph has dummy `START` and `END` nodes. The remaining
    two lines create directed edges from `START` to our `assistant` node, and from
    our `assistant` node to `END`, thus completing the graph.
  prefs: []
  type: TYPE_NORMAL
- en: The immediately following line, `graph = builder.compile()`, *compiles* the
    graph, readying it for execution.
  prefs: []
  type: TYPE_NORMAL
- en: 'The last few lines of code in the file show how a graph can be invoked:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: We first use the `input()` function—which prompts the user to enter something
    in the terminal—to collect the user's input message.
  prefs: []
  type: TYPE_NORMAL
- en: We then construct the starting state of the graph as a dictionary with the key
    `"messages."` The message itself is an instance of `HumanMessage` with its content
    attribute set to the `input_message` we just collected.
  prefs: []
  type: TYPE_NORMAL
- en: Passing `initial_state` to the graph's `invoke` method finally causes the graph
    to execute, effectively passing our user input to the LLM through `assistant_node`,
    returning the final state.
  prefs: []
  type: TYPE_NORMAL
- en: '`final_state` contains all of the messages in the conversation so far (our
    user message and the LLM''s response message), so we access the response message
    using `final_state["messages"][-1]` and print its content to the screen.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To see this in action, copy all the code to a new file called `graph_example.py`,
    and run your code in the terminal using the `python` command like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Enter a message when you see the `"Talk to the bot"` prompt. As an example
    output, here''s what I got:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: It looks like AI stole my dream haiku gig. Maybe I'll pivot to the performing
    arts—everyone loves a good mime.
  prefs: []
  type: TYPE_NORMAL
- en: Turning our graph into a class
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We've run our first graph in the terminal, but what we really want is to use
    it to power our customer support bot. We'll organize our code using object-oriented
    principles as we did in the last two chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Let's start by converting the code we wrote in the prior section into a `SupportAgentGraph`
    class in `graph.py`, shown in listing 10.2.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.2 graph.py
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: (`chapter_10/in_progress_01/graph.py` in the GitHub repo)
  prefs: []
  type: TYPE_NORMAL
- en: The code here is very similar to that in `graph_example.py`, but I want to highlight
    a few differences.
  prefs: []
  type: TYPE_NORMAL
- en: Most obviously, we're encapsulating our graph in a class—`SupportAgentGraph`—which
    has a method for building the actual graph (`build_graph`) and another (`invoke`)
    for invoking it by passing a human (user) message.
  prefs: []
  type: TYPE_NORMAL
- en: Rather than creating the LLM object within the class, we accept it as a parameter
    in `SupportAgentGraph`'s `__init__`, which builds the graph by calling `self.build_graph()`
    and saves it under `self.graph` for future invocations.
  prefs: []
  type: TYPE_NORMAL
- en: You'll notice that our `MyGraphState` class, which we defined earlier, is nowhere
    to be found. We've swapped it out for `MessagesState`, a built-in LangGraph class
    that does more or less the same thing. `MessagesState`, like `MyGraphState`, has
    a `messages` field, which is a list of `AnyMessage` objects. The big difference
    between `MyGraphState` and `MessagesState` is how the `messages` field can be
    modified in a node—more on that in a second.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, consider the `get_assistant_node` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: This method has a function definition—for `assistant_node`, which we encountered
    in the previous section—nested under it. It seems to do nothing other than return
    the function. What's that about?
  prefs: []
  type: TYPE_NORMAL
- en: Well, since `assistant_node` needs to access the LLM object (`self.llm`), its
    code must live inside a method of the `SupportAgentGraph` class. But `assistant_node`
    can't *itself* be a method of the class, because the first argument passed to
    a method is `self`—the current instance of the class—while the first (and only)
    argument passed to a valid LangGraph node must be the graph state.
  prefs: []
  type: TYPE_NORMAL
- en: So instead, we define `assistant_node` as an inner function within an outer
    method called `get_assistant_node`—taking advantage of the outer method's scope
    to access `self.llm` within the inner function—and have the outer method *return*
    the inner function so we can plug it into the graph. This programming pattern
    is called a *closure* since the inner function retains access to variables from
    its *enclosing* scope, even after the outer function has returned.
  prefs: []
  type: TYPE_NORMAL
- en: 'The aforementioned plugging-in of the node happens in the `build_graph` method
    in this line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Since `get_assistant_node()` *returns* the `assistant_node` function (as opposed
    to calling it), we can use the call to `get_assistant_node` to refer to the inner
    function.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `assistant_node` function differs from the one of the same name we defined
    in the prior section in one important way. Consider the return statement, which
    has changed from:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'to:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Why do we not return the other items in the `messages` list anymore? The answer
    has to do with our having replaced `MyGraphState` with `MessagesState`. You see,
    each node in LangGraph's `StateGraph` receives the complete state as input, but
    the value it returns is treated as a set of *updates* to each key in the state.
    How exactly these updates are merged with the existing values depends on how we've
    specified it in our state type.
  prefs: []
  type: TYPE_NORMAL
- en: In `MyGraphState`, we didn't mention any particular way of handling this, so
    the value associated with the key `messages` is simply replaced by whatever the
    node returns for that key. This is why we needed to return the entire list—because
    we would have lost the earlier messages otherwise.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, `MessagesState` internally specifies that the value returned
    by a node should be appended to the existing list. So, `ai_response_message` is
    simply tacked on to the existing messages, and we don't have to return the older
    messages separately.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '`MessagesState` implements this append feature through a function called `add_messages`.
    In fact, the only difference between `MyGraphState` and `MessagesState` is that
    the `messages` field in `MessagesState` is defined (internally) like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: I won't go into this in detail, but this is essentially saying that when updates
    occur, they should be handled by the `add_messages` function rather than a simple
    replacement.
  prefs: []
  type: TYPE_NORMAL
- en: Whew! That was a lot of explanation, but hopefully, you now understand how graphs
    are modeled in LangGraph.
  prefs: []
  type: TYPE_NORMAL
- en: The Bot class
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Let's set aside `SupportAgentGraph` now and pivot to our main backend class,
    which we'll call `Bot`. `Bot` will be the single point of entry to the backend
    for our Streamlit frontend, similar to the `Game` and `Hub` classes in earlier
    chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Importantly, `Bot` will supply the LLM object that `SupportAgentGraph` needs
    and provide a user-friendly method that our frontend can call to chat with the
    bot.
  prefs: []
  type: TYPE_NORMAL
- en: To create it, copy the code in listing 10.3 to a new file, `bot.py`.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.3 bot.py
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: (`chapter_10/in_progress_01/bot.py` in the GitHub repo)
  prefs: []
  type: TYPE_NORMAL
- en: Luckily, the `Bot` class is a lot more straightforward than `SupportAgentGraph`.
    `__init__` accepts a dictionary of API keys—which, spoiler alert, we'll supply
    through `st.secrets` again—before setting up the LLM object with a call to the
    `get_llm` method, and passing it to the `SupportAgentGraph` instance, saved to
    `self.graph`.
  prefs: []
  type: TYPE_NORMAL
- en: '`get_llm` simply uses LangChain''s `ChatOpenAI` class to create the LLM object
    as discussed earlier. Notice that we''ve added a new parameter called `max_tokens`.
    As you may remember from the previous chapter, tokens are the basic units of text
    that language models process. By setting `max_tokens=2000`, we''re telling OpenAI''s
    API to limit responses to a maximum of 2000 tokens (about 1,500 words), which
    helps in both cost reduction and keeping responses (relatively) concise.'
  prefs: []
  type: TYPE_NORMAL
- en: The `chat` method abstracts away the complexity of dealing with graphs and states.
    It has a simple contract—put a human message string in, and get an AI response
    string out. It fulfills this promise by calling the `invoke` method of our `SupportAgentGraph`'s
    instance, and returning the content of the last message—which happens to be the
    AI message, as we saw earlier.
  prefs: []
  type: TYPE_NORMAL
- en: A chatbot frontend in Streamlit
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Our app's backend is now ready, so let's focus on the frontend next. Streamlit
    really shines when it comes to chatbot interfaces because of its native support
    for them.
  prefs: []
  type: TYPE_NORMAL
- en: This is evident in the fact that our first iteration of `frontend.py`—shown
    in listing 10.4—is only 12 lines long.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.4 frontend.py
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: (`chapter_10/in_progress_01/frontend.py` in the GitHub repo)
  prefs: []
  type: TYPE_NORMAL
- en: We start by putting a reference to our `Bot` instance—`bot`—in `st.session_state`,
    which is essentially the same pattern we've used in the last two chapters for
    the `Hub` and `Game` classes. To do so, we pass in the `api_keys` object from
    `st.secrets` to do so. We'll create `secrets.toml` in a bit.
  prefs: []
  type: TYPE_NORMAL
- en: 'The interesting part is the last four lines. The first of these introduces
    a new Streamlit widget called `st.chat_input`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '`st.chat_input` creates a text input box with a "Send" icon, similar to what
    you''re probably used to in various messaging apps. Besides the "Send" icon, it''s
    different from `st.text_input` in a few noticeable ways:'
  prefs: []
  type: TYPE_NORMAL
- en: It's pinned to the *bottom* of the screen or the containing widget you put it
    in
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unlike `st.text_input,` which returns a value once a user clicks out of the
    textbox, `st.chat_input` only returns a value once the user has clicked "Send"
    or pressed Enter.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apart from `st.chat_input`, the code above may look unfamiliar for another reason;
    we're using the character sequence `:=`, which is called a *walrus operator* in
    Python (because if you tilt your head to the side, it kind of looks like a walrus).
  prefs: []
  type: TYPE_NORMAL
- en: 'The walrus operator is just a trick to make your code slightly more concise.
    It allows you to assign values to variables as part of a larger expression rather
    than requiring a separate line for assignment. In other words, instead of the
    line we''re discussing, we could have written the following to obtain the same
    effect:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Python developers are divided on whether the walrus operator increases or decreases
    the readability of your code. Regardless of whether you choose to use it, it's
    a good idea to know what it is.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have an input message from the user, we can display the conversation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '`st.chat_message` is a Streamlit display widget that accepts either of two
    strings—`"human"` or `"ai"`—and styles the container accordingly. This includes
    showing an avatar corresponding to a user or a robot.'
  prefs: []
  type: TYPE_NORMAL
- en: In this case, we display `human_message_text` using `st.chat_message("human")`,
    call the `chat` method of our Bot instance, and display the response AI text with
    `st.chat_message("ai")`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Just to be clear, `st.chat_message` is similar to other Streamlit elements
    like `st.column` in that we could also have written:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: To complete the first version of Nibby, we need to create a `secrets.toml` file
    for our OpenAI API key, in a new `.streamlit` folder. The contents of this file
    are in listing 10.5.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.5 .streamlit/secrets.toml
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '#A Replace sk-proj-... with your actual OpenAI API key.'
  prefs: []
  type: TYPE_NORMAL
- en: Go ahead and run your app with `streamlit run frontend.py` to test it out. Figure
    10.7 shows our chatbot in action.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../Images/10__image007.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.7 A single-prompt-single-response chatbot in Streamlit (see chapter_10/in_progress_01
    in the GitHub repo for the full code).
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Sweet! Notice the human and bot avatars, as well as the subtle background shading
    to distinguish between the two kinds of displayed messages.
  prefs: []
  type: TYPE_NORMAL
- en: If you play around with the app, you'll realize that Nibby can't hold a conversation
    at this point, only respond to single messages. Next up, let's fix that!
  prefs: []
  type: TYPE_NORMAL
- en: 10.3 Multi-turn conversations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It took some effort to get there, but we've built an initial version of Nibby.
    Unfortunately, at the moment, Nibby's idea of a conversation is a single response
    to a single message.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, consider the exchange in figure 10.8
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../Images/10__image008.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.8 Our chatbot doesn't remember the information we gave it.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'There are two things wrong here:'
  prefs: []
  type: TYPE_NORMAL
- en: The bot didn't remember the information I gave it in the prior message.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Our frontend treats the second message-response pair as a completely new conversation,
    removing all traces of the first.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this section, we'll iterate on Nibby, solving both issues.
  prefs: []
  type: TYPE_NORMAL
- en: 10.3.1 Adding memory to our graph
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Recall the simple one-node graph we created: it starts with a state containing
    a human message, passes this message to an LLM, appends the AI''s response to
    the state, and then returns the updated state.'
  prefs: []
  type: TYPE_NORMAL
- en: What happens if you invoke the graph again with a follow-up message? Well, the
    process repeats—a *new* state containing *only* the follow-up message is created
    and passed to the graph, which treats this as a brand-new independent execution.
  prefs: []
  type: TYPE_NORMAL
- en: This is a clear problem since conversations very rarely consist exclusively
    of a single message and response. The user *will* want to follow up, and the chatbot
    needs to remember what came before.
  prefs: []
  type: TYPE_NORMAL
- en: To enable our graph to remember prior executions, we need to *persist* the state,
    rather than starting from scratch every time. Luckily, LangGraph makes this a
    snap through the concept of *checkpointers*, which can save the state of the graph
    at each step.
  prefs: []
  type: TYPE_NORMAL
- en: Specifically, we're going to use a checkpointer to allow our graph state to
    be stored in memory. We can then assign a *thread ID* to each invocation of the
    graph. Whenever we pass the same thread ID while invoking the graph, the graph
    will recall the state previously stored in memory for that thread ID and start
    from *there* rather than from a clean slate.
  prefs: []
  type: TYPE_NORMAL
- en: 'To implement this, make the changes shown below to `graph.py`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: (`chapter_10/in_progress_02/graph.py` in the GitHub repo)
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start our discussion of the code above with `build_graph`. We''ve added
    a line at the top of this method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '`MemorySaver` is a checkpointer built into LangGraph that can store graph states
    in memory. Various other kinds of checkpointers are available, depending on where
    you want to save your graph state. For instance, you could use different checkpointers
    to store conversations in a database like PostgreSQL or SQLite.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We pass this to our graph when we compile it at the end of the method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: This allows our graph to save its state, but that's not enough. If we don't
    make any more changes, each graph invocation would still be a new, independent
    one. We need a way to tell the graph that a particular invocation belongs to a
    *thread* it has seen before.
  prefs: []
  type: TYPE_NORMAL
- en: 'Direct your attention to `__init__`, where we''ve assigned a strange-looking
    value to a field called `self.config`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The important part to notice here is `{"thread_id": "1"}`. Further down, in
    the `invoke` method, we pass this to the graph while invoking it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: We're essentially passing the thread ID `1` to the graph here so it knows that
    every time we invoke it, we're always in the same conversation thread which has
    an ID of `1`.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a result of this change, the first invocation of the graph (`"Hi, my name''s
    Bob"` in the example that prompted these changes) will be saved under the thread
    ID `1`. At this point, the state will have two messages: the original human message
    and the AI response.'
  prefs: []
  type: TYPE_NORMAL
- en: When the follow-up message (`"What's my name"`) arrives, since we already have
    an existing thread with ID `1`, it will be *appended* to the existing state. The
    state that's passed to `assistant_node` (and therefore to the LLM) will have *three*
    messages, enabling it to respond correctly.
  prefs: []
  type: TYPE_NORMAL
- en: Additional questions you may have
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Two natural questions may arise at this juncture:'
  prefs: []
  type: TYPE_NORMAL
- en: Why is the thread ID always 1?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recall that our Streamlit app session doesn't persist beyond a single browser
    refresh. So each time the user accesses the app by opening it in a new tab or
    refreshing the browser, the `SupportAgentGraph` instance is rebuilt, and the graph
    is re-compiled with a new `MemorySaver` object. Since `MemorySaver` stores graph
    states in memory instead of persisting it to an external data store like PostgreSQL,
    any thread from a different browser session—whatever the thread ID—is inaccessible,
    so we can safely use the same thread ID `1` for the new session.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Long story short, how we've set things up guarantees that a single graph instance
    will see at most one conversation in its lifetime, so we only need to specify
    one thread ID.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why is the value of self.config so convoluted?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Looking at our explanation of checkpointers and memory, it seems that all we
    need to pass the graph when we invoke it is the value `1`. So why do we have this
    monstrosity: `{"configurable": {"thread_id": "1"}}`?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Though they are beyond the scope of this book, LangGraph offers many options
    when you invoke a graph, such as the ability to specify metadata or the number
    of parallel calls it can make. The thread ID is the only configuration we're using
    here, but it's far from the only one available. The convoluted-seeming structure
    of self.config reflects this.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Try running Nibby again and entering the same messages as before. This time
    you should see something similar to figure 10.9.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../Images/10__image009.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.9 Nibby now remembers information we told it earlier in the conversation
    (see chapter_10/in_progress_02 in the GitHub repo for the full code).
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: As you can see, the app does remember the previous information we gave it this
    time, but we still need to update the frontend to show the entire conversation.
  prefs: []
  type: TYPE_NORMAL
- en: 10.3.2 Displaying the conversation history
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Our Streamlit frontend is currently only set up to show the latest user-entered
    message and the AI's response.
  prefs: []
  type: TYPE_NORMAL
- en: 'To display the full history, we need to first expose it in the backend. Let''s
    start by adding a method to `graph.py` to get the entire conversation so far at
    any point:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: (`chapter_10/in_progress_03/graph.py` in the GitHub repo)
  prefs: []
  type: TYPE_NORMAL
- en: The `get_conversation` method in `SupportAgentGraph` simply returns the `messages`
    list in the graph's current state.
  prefs: []
  type: TYPE_NORMAL
- en: To do this, it first gets a reference to the state (`self.graph.get_state(self.config)`),
    and then accesses the `"messages"` key using `state.values["messages"]`. Passing
    `self.config` to `get_state` is required to get us the correct conversation thread,
    though—as the sidebar in the previous section discusses—there's only one.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let''s expose the full `messages` list in `bot.py`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: (`chapter_10/in_progress_03/bot.py` in the GitHub repo)
  prefs: []
  type: TYPE_NORMAL
- en: All the `get_history` method does is to pass the result of the `get_conversation`
    method we just defined faithfully through to its caller.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: (`chapter_10/in_progress_03/frontend.py` in the GitHub repo)
  prefs: []
  type: TYPE_NORMAL
- en: We call `bot.get_history()` to get the list of messages and iterate through
    it, displaying each in its own `st.chat_message` container.
  prefs: []
  type: TYPE_NORMAL
- en: Recall that each message in the `messages` list is an instance of either `HumanMessage`
    or `AIMessage`. Either way, it also has a type field with a value of `"human"`
    in the case of `HumanMessage` and `"ai"` for an `AIMessage`. This therefore works
    perfectly as the type indicator argument in `st.chat_message`.
  prefs: []
  type: TYPE_NORMAL
- en: '`message.content` has the message''s text, so we display that using `st.markdown`.'
  prefs: []
  type: TYPE_NORMAL
- en: Rerun the app to see figure 10.10.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../Images/10__image010.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.10 Our frontend now displays the full conversation history (see chapter_10/in_progress_03
    in the GitHub repo for the full code)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: As expected, Nibby now shows the full conversation so we can keep track of what's
    going on.
  prefs: []
  type: TYPE_NORMAL
- en: 10.4 Restricting our bot to customer support
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Thus far, we've focused on getting Nibby's basic functionality right—including
    calling an LLM and handling a full conversation. The result is a *general* chatbot
    you can ask for pretty much anything.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, consider what happens if we ask Nibby to sing a song (figure 10.11):'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../Images/10__image011.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.11 Nibby entertains frivolous requests, potentially costing us money.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Nibby can sing a song all right. It can also help you solve math problems or
    write an essay about the fall of the Roman Empire. Unfortunately, it does all
    of that on the company's dime. Remember, interacting with a cloud-based LLM costs
    *money*.
  prefs: []
  type: TYPE_NORMAL
- en: Each time someone makes a frivolous request to your customer support bot, and
    the bot indulges the request with a long-winded response, it spends precious LLM
    tokens and costs you something. Sure, each message is only a fraction of a cent
    but add up all the pleas for coding assistance or role-playing as a character
    from Battlestar Galactica, and suddenly your boss wants to know why there's a
    Nibby-shaped hole in the company's quarterly earnings report.
  prefs: []
  type: TYPE_NORMAL
- en: 'Of course, I''m being hyperbolic, but the point stands: we want Nibby to be
    strictly business.'
  prefs: []
  type: TYPE_NORMAL
- en: The best thing about an LLM is that you can simply tell it what you want it
    to do or not do, so this turns out to be an easy fix; we'll simply add an appropriate
    *prompt*.
  prefs: []
  type: TYPE_NORMAL
- en: 10.4.1 Creating a base prompt
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As we did in chapter nine, we want to give the LLM some context about the use
    case we want it to serve. In that chapter, we did this by creating a message with
    the role "`system`." That's essentially what we're going to do here, too, though
    the abstractions we'll use are slightly different.
  prefs: []
  type: TYPE_NORMAL
- en: Create a file called `prompts.py` with the content shown in listing 10.6.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.6 prompts.py
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: (`chapter_10/in_progress_04/prompts.py` in the GitHub repo)
  prefs: []
  type: TYPE_NORMAL
- en: The prompt gives Nibby its first indication that Note n' Nib exists and that
    it's supposed to be providing customer support for the company.
  prefs: []
  type: TYPE_NORMAL
- en: Importantly, `BASE_SYS_MSG` also has an instruction to refuse any requests that
    are unrelated to Note n' Nib. Next, let's incorporate this into our graph.
  prefs: []
  type: TYPE_NORMAL
- en: 10.4.2 Inserting a base context node in our graph
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As we learned in chapter nine, using OpenAI's chat completions endpoint involves
    passing a sequence of messages to the LLM.
  prefs: []
  type: TYPE_NORMAL
- en: In our current graph, the list starts with the user's first instruction and
    contains only user messages and AI responses. To prevent Nibby from responding
    to frivolous requests, we just need to insert the system prompt we just created
    as the first message in the list we send to the LLM.
  prefs: []
  type: TYPE_NORMAL
- en: We'll do this by inserting a new node in the graph to add the system message
    to the graph state and modifying the existing `assistant_node` to pass this message
    to the LLM before anything else.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.12 shows a visual representation of the new graph.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../Images/10__image012.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.12 Adding a base context node in our graph
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The changes required to `graph.py` are shown in listing 10.7:'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.7 graph.py (modified)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: (`chapter_10/in_progress_04/graph.py` in the GitHub repo)
  prefs: []
  type: TYPE_NORMAL
- en: Starting from the top, we've added a couple of imports; we need the `SystemMessage`
    class in addition to `HumanMessage`, so that's one.
  prefs: []
  type: TYPE_NORMAL
- en: The statement `from prompts import *` allows us to access any prompt we may
    add to `prompts.py` using only its variable name—without a prefix like `prompt.`.
    Since we're using the `*` wildcard here rather than importing specific objects,
    every object in `prompt.py`'s global scope becomes part of `graph.py`'s scope.
    In this case, it means we can refer to `BASE_SYS_MSG` directly, as we do later
    in the code.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ve defined a new `AgentState` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '`AgentState` inherits from `MessagesState`, so it also contains the `messages`
    field we''ve been using thus far. What we''re effectively doing here is adding
    a new field to the state—called `sys_msg_text`—meant to hold the text of the system
    message.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, within the class itself, we''ve added a new static method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'This function represents the new node we''re adding to the graph, called `base_context`.
    All this node does is to populate the `sys_msg_text` field we''ve added to the
    state. By returning `{"sys_msg_text": BASE_SYS_MSG}`, this node sets `sys_msg_text`
    to `BASE_SYS_MSG`—the context prompt we created a few minutes ago—in the graph''s
    current state.'
  prefs: []
  type: TYPE_NORMAL
- en: To understand how this works, it's helpful to remember that a graph node does
    not return the entirety of the state; rather it only returns the keys in the state
    that need to be modified. Therefore, even though there's no mention of the `messages`
    field here, once this node has been executed, the state will continue to have
    that field—unmodified—in addition to `sys_msg_text`.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Unlike in the case of `messages`, when we return a dictionary with a `sys_msg_text`
    key it *replaces* the value of `sys_msg_text` in the state. This is because `sys_msg_text`
    uses the default update behavior, as opposed to the *append* behavior (enabled
    internally by the `add_messages` function) that `messages` uses.
  prefs: []
  type: TYPE_NORMAL
- en: Why have we made `base_context_node` a static method? Well, recall once again
    that each node in the graph needs to accept the graph state as its first argument.
    We would like to put `base_context_node` inside `SupportAgentGraph` for logical
    code organization purposes, but if we make it a regular method, it'll need to
    accept the class instance (`self`) as its first argument. Making it a static method
    removes that requirement, and frees us to add a state argument.
  prefs: []
  type: TYPE_NORMAL
- en: Some of you might be asking, "Wait a minute, didn't we structure `assistant_node`
    as a nested function for the same reason? Why didn't we do *that* here?"
  prefs: []
  type: TYPE_NORMAL
- en: We could indeed have used a closure-based solution for `base_context_node` too,
    but we don't need to; unlike `assistant_node` which references `self.llm`, `base_context_node`
    doesn't need to access `self` at all. We therefore employ the more straightforward
    technique of applying the `@staticmethod` decorator to `base_context_node`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Speaking of `assistant_node`, consider the changes we''ve made to its code
    above:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Rather than invoking the LLM directly with `state["messages"]`, we now create
    a `SystemMessage` object with the `sys_msg_text` field we populated in `base_context_node`
    as the content and prepend it to `state["messages"]` to form the list we pass
    the LLM.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, note our updates to `build_graph`. Since we''ve extended `MessagesState`
    to include a `sys_msg_text` field, we use that to initialize the `StateGraph`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'We add the base context node like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Notice how we're passing a reference to the `self.base_context_node` method
    itself here, as opposed to calling it with the double parentheses.
  prefs: []
  type: TYPE_NORMAL
- en: 'We also reorder the edges in the graph to insert the `base_context` node between
    `START` and `assistant`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: That should be all we need. Go ahead and re-run your app. Try requesting the
    bot to sing a song again to get a response similar to figure 10.13.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../Images/10__image013.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.13 Nibby now refuses to entertain frivolous requests (see chapter_10/in_progress_04
    in the GitHub repo for the full code).
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'It looks like Nibby got the memo! It won''t help the user with irrelevant requests
    anymore. In the next section, we''ll solve the opposite problem: getting it to
    help with *relevant* questions.'
  prefs: []
  type: TYPE_NORMAL
- en: 10.5 Retrieval Augmented Generation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Models like GPT-4o are so effective because they have been pre-trained on a
    huge corpus of publicly available information, such as books, magazines, and websites.
    It's why Fact Frenzy, our trivia app from chapter nine, was able to ask and answer
    questions on such a wide range of topics.
  prefs: []
  type: TYPE_NORMAL
- en: However, many of the more economically valuable use cases of generative AI require
    more than information in the public domain. Truly molding AI into something that
    fits your specific use case usually requires providing it with private information
    that only you possess.
  prefs: []
  type: TYPE_NORMAL
- en: Take Nibby, for instance, who is ultimately meant to assist customers of Note
    n' Nib with their queries. What happens if we pose a valid question about a stationery
    product to Nibby? Figure 10.14 shows such a conversation.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../Images/10__image014.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.14 Nibby makes up information when it doesn't know the answer.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Seems like Nibby knocked that one out of the park, right? Not quite. We never
    told our bot what kinds of pens Note n' Nib carries, so where is it getting its
    information from? Additionally, where are the fictional brands—InkStream and RoyalQuill—we
    encountered in chapter six? As it turns out, Nibby had no information available
    about fountain pens and, therefore, simply hallucinated this response!
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we'll discover a way to augment Nibby's existing store of worldly
    knowledge with custom information that we provide.
  prefs: []
  type: TYPE_NORMAL
- en: 10.5.1 What is Retrieval Augmented Generation?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: So, how do we supplement all of the information an LLM has been trained on with
    our own? For relatively small pieces of information, it's actually trivially easy—in
    fact, we already know how! All we need to do is provide the info to the LLM as
    part of our prompt!
  prefs: []
  type: TYPE_NORMAL
- en: We could get Nibby to get the question we posed in figure 10.14 right simply
    by listing the products Note n' Nib sells directly in the system message we send
    to the LLM.
  prefs: []
  type: TYPE_NORMAL
- en: What about other questions, though? Technically, we could give the model all
    of the contextual information it might realistically need to answer any question
    directly in the prompt. The maximum amount of such information we can provide
    is measured in tokens, called the model's *context window length.*
  prefs: []
  type: TYPE_NORMAL
- en: Relatively recent models have a huge context window. For instance, gpt-4o-mini
    can take up to 128,000 tokens (about 96,000 words, since—on average—a token is
    roughly three-quarters of a word), while o3-mini—a newer reasoning model from
    OpenAI—has a context window of 200,000 tokens. Models from other providers can
    take even more tokens in a single prompt. Google's Gemini 2.0 Pro has a context
    window that is a whopping *2 million* tokens long—enough to fit the entire Harry
    Potter series of books, with space left over for almost all of the Lord of the
    Rings trilogy.
  prefs: []
  type: TYPE_NORMAL
- en: Surely our problem is solved then? We can simply assemble all the information
    we possess about Note n' Nib and feed it to the LLM in each prompt, correct?
  prefs: []
  type: TYPE_NORMAL
- en: 'Certainly, we could, but we probably don''t want to for a couple of reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: LLMs are prone to information overload; we generally see degraded performance
    with extremely large prompts.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Even if there were no such degradation, LLM providers usually charge by the
    token, so if we had to pass our entire custom knowledge base in every LLM call,
    the costs would go through the roof.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: No, we need a different solution. If only we could **read in a user's question
    and feed the LLM just the** ***relevant*** **parts of our knowledge base required
    to answer it**.
  prefs: []
  type: TYPE_NORMAL
- en: And that—in case you've somehow failed to realize where this spiel is going—is
    exactly what Retrieval Augmented Generation (RAG) is.
  prefs: []
  type: TYPE_NORMAL
- en: 'RAG has the following essential steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Read the user's question
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Retrieve** the context *relevant* to the question from the knowledge base'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Augment** the question with the context required to answer it'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Generate** the answer to the question by feeding the question and context
    to the LLM'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The hard part of RAG is the *retrieve* step. Specifically, given a user question
    and a large custom knowledge base, how do you identify the parts of the knowledge
    base that are relevant to the question and extract only those parts from the base?
  prefs: []
  type: TYPE_NORMAL
- en: The answer lies in the concept of *embeddings* and a piece of software known
    as a *vector database*.
  prefs: []
  type: TYPE_NORMAL
- en: Embeddings and Vector databases
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: While we don't—strictly speaking—need to learn how embeddings—or even vector
    databases—work under the hood to implement RAG, it would be a good idea to gain
    a basic understanding of these concepts.
  prefs: []
  type: TYPE_NORMAL
- en: Let's start with a simplified example to achieve this. Say you're known as something
    of a movie buff in your friend circle. Your buddy approaches you and says, "Hey,
    I watched *The Dark Knight* yesterday and loved it! Could you recommend another
    movie like it?"
  prefs: []
  type: TYPE_NORMAL
- en: You're in a fix because—though you have an encyclopedic knowledge of movies—you're
    not sure how exactly to measure the *similarity* between two movies, so you can
    recommend the one that's the *most* similar to *The Dark Knight*. Refusing to
    accept defeat, you flee to your underground lair and try to work it out in solitude.
  prefs: []
  type: TYPE_NORMAL
- en: 'Eventually, you come up with a system. You reckon that when people express
    their preference for various movies, they''re subconsciously talking about two
    attributes: *comedic value* and *explosions per hour*. Therefore, you rate your
    entire catalog of movies against those two scales and plot the results in a chart
    (partially reproduced in figure 10.15).'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../Images/10__image015.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.15 Converting movies into vectors and plotting them on a chart
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'As you can see, *The Dark Knight* has a comedic value of `1.2` but a relatively
    high explosions-per-hour of `6`. We can represent it as a list of numbers: `[1.2,
    6]`, called a *vector*. We can call the vector `[1.2, 6]` the *embedding* of the
    movie *The Dark Knight* in the two-dimensional comedic-value/explosions-per-hour
    space.'
  prefs: []
  type: TYPE_NORMAL
- en: Converting movies into numbers in this way makes it possible to measure their
    similarity. For instance, *Office Space* is represented as `[7.2, 0.4]` in the
    same space. The similarity (or, rather, lack thereof) between *Office Space* and
    *The Dark Knight* can be calculated mathematically by considering their *geometric*
    *distance*. The closer the embeddings of two movies are geometrically—as measured
    by the length of a straight line drawn between them—the more similar the underlying
    movies are.
  prefs: []
  type: TYPE_NORMAL
- en: After several such calculations, you find that *The Bourne Ultimatum*, which
    has the vector `[1.1, 5.9]` is the closest to *The Dark Knight*. Having concluded
    your research, you get back to your friend and let them know (to which your friend
    responds, "Thank goodness you're alive! It's been two years, where have you *been*?").
  prefs: []
  type: TYPE_NORMAL
- en: The question we're facing with Nibby is analogous to the movie recommendation
    problem above. Given a user's message (the movie your friend liked), and a knowledge
    base (your catalog of movies), we have to find the paragraphs/chunks (movies)
    that are most relevant (most "similar") to the user's message.
  prefs: []
  type: TYPE_NORMAL
- en: 'To answer this efficiently, we need two things:'
  prefs: []
  type: TYPE_NORMAL
- en: A way to convert a given piece of text into embeddings that capture its meaning
    (or *semantics*)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A way to store these embeddings and quickly calculate distances between them
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Obviously, the movie example above is overly simplistic. Our "space" only had
    two dimensions: comedic value and explosions per hour. Encoding the meaning of
    a piece of text requires a lot more dimensions (like hundreds or thousands), and
    the dimensions themselves would not be human-understandable concepts like "comedic
    value." We''ll use a text embedding model provided by OpenAI for our use case.'
  prefs: []
  type: TYPE_NORMAL
- en: To store the embeddings, we'll use a program called a *vector database*. Vector
    databases make it easy to calculate the distance between embeddings or find the
    entries closest to a specific one. Rather than the "straight line" (or *Euclidean*)
    distance between vectors, we'll use a score called *cosine similarity*, which
    measures the *angle* between two vectors to determine their similarity. The vector
    database we'll use is Pinecone, a cloud-hosted service.
  prefs: []
  type: TYPE_NORMAL
- en: 10.5.2 Implementing RAG in our app
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Armed with a conceptual understanding of RAG, let's now focus on implementing
    it to assist Nibby in answering customer questions.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the knowledge base
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The GitHub folder for this chapter ([https://github.com/aneevdavis/streamlit-in-action/tree/main/chapter_10](https://github.com/aneevdavis/streamlit-in-action/tree/main/chapter_10))
    has a subdirectory called `articles` with a series of customer support articles
    for Note n' Nib. Each article is a text file containing information about Note
    n' Nib's products or how it conducts business. For instance, `our_products.txt`
    includes a list of product descriptions, while `fountain_pen_maintenance.txt`
    is about maintaining RoyalQuill and InkStream pens.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s an excerpt from the article that we''ll reference later:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Copy the `articles` folder into your working directory now. This is the knowledge
    base that Nibby will have access to.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up a vector database
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As mentioned briefly in the previous section, we will be using Pinecone, a managed
    vector database optimized for fast and scalable similarity search that's popular
    for AI applications. Pinecone's free "Starter" plan is more than enough for this
    chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Go to [https://www.pinecone.io/](https://www.pinecone.io/) and sign up for an
    account now. As soon as you're done with the setup, you'll be presented with an
    API key, which you should save immediately. Once that's done, create a new *index*.
    An index is analogous to a table in a regular non-vector database like PostgreSQL.
    The index will store our support articles broken into parts, along with their
    embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: 'You''ll be asked to supply values for various options during the creation process:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Index name:** You can use whatever you like, but remember to save it since
    we''ll use it in our code.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model configuration:** Choose `text-embedding-ada-002`, the OpenAI embedding
    model we''ll use; the value of dimensions should automatically be set to 1,536.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Metric:** Pick `cosine` to use the cosine similarity score we briefly discussed
    earlier.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Capacity mode** should be `Serverless`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cloud provider**: `AWS` is fine for this.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Region:** At the time of writing, only `us-east-1` is available in the free
    plan, so pick that.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ingesting the knowledge base
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To incorporate the vector store into our chatbot app, we''ll create a `VectorStore`
    class. Before doing so, add your Pinecone API key and the index name you just
    created to `secrets.toml` so it now looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '#A Replace with your actual OpenAI API key.'
  prefs: []
  type: TYPE_NORMAL
- en: '#B Replace with your Pinecone API key.'
  prefs: []
  type: TYPE_NORMAL
- en: '#C Replace sk-proj-... with the actual index name.'
  prefs: []
  type: TYPE_NORMAL
- en: We've added a new key to `api_keys` and a new section called `config` to hold
    the index name.
  prefs: []
  type: TYPE_NORMAL
- en: Now on to the `VectorStore` class. Create a file named `vector_store.py` with
    the contents shown in listing 10.8.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.8 vector_store.py
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: (`chapter_10/in_progress_05/vector_store.py` in the GitHub repo)
  prefs: []
  type: TYPE_NORMAL
- en: There's a fair bit going on here, so let's go through it step-by-step.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `__init__` has the boilerplate code required to set up the vector store
    connection. It accepts two arguments: `api_keys` (the dictionary of API keys)
    and `index_name` (the name of the Pinecone index).'
  prefs: []
  type: TYPE_NORMAL
- en: '`__init__` first creates the `Pinecone` object—`pc`—using the Pinecone API
    key we noted a minute ago, and then an `OpenAIEmbeddings` object by passing it
    the OpenAI key. `pc.Index(index_name)` refers to the index we created earlier.
    Finally, we obtain a vector store object from the index and embeddings and assign
    it to `self.store` so we can use it in other methods.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `ingest_folder` method accepts the path to a folder and saves its contents
    to the Pinecone index. Consider the first part of this method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: LangChain provides various *document loaders* to help ingest and parse various
    types of data (text files, PDFs, web pages, databases, etc.) into a structured
    format suitable for processing. `DirectoryLoader` makes it easy to load files
    from a specified directory.
  prefs: []
  type: TYPE_NORMAL
- en: The `glob="**/*.txt"` ensures that all text files (`.txt`) in the folder (including
    subfolders) are included.
  prefs: []
  type: TYPE_NORMAL
- en: '`loader_cls=TextLoader` tells `DirectoryLoader` to use another loader class
    called `TextLoader`—also provided by LangChain—for loading individual text files.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the `DirectoryLoader` is created, the next step is to load the documents:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: This reads all `.txt` files in the directory and loads them into a list of `Document`
    objects, which LangChain uses to store the raw text and metadata.
  prefs: []
  type: TYPE_NORMAL
- en: At the moment, each `Document` consists of an entire article. While the articles
    in our folder are relatively small, one can easily imagine a support article being
    thousands of words long. The point of fetching just the relevant text from our
    knowledge base is to reduce the overall size of the prompt; simply fetching entire
    articles defeats this purpose.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, we want to divvy up the articles into manageable *chunks* of text
    that are roughly of equal size. That''s what the next part of our code does:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: '`RecursiveCharacterTextSplitter` is a text-splitting utility from LangChain
    that can break documents into chunks while preserving meaningful context.'
  prefs: []
  type: TYPE_NORMAL
- en: '`chunk_size=500` sets the length of each chunk to 500 characters.'
  prefs: []
  type: TYPE_NORMAL
- en: '`chunk_overlap=200` means that chunks will have a 200-character overlap to
    maintain context.'
  prefs: []
  type: TYPE_NORMAL
- en: 'After splitting the documents, the chunks—available in `texts`—are ready to
    be stored:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: '`add_documents` takes the split text chunks and adds them to the Pinecone index,
    storing both the text and embeddings generated using the `text-embedding-ada-002`
    model, making them searchable.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `retrieve` method allows callers to query the vector store:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: The `similarity_search` method of our `PineconeVectorStore` instance (`self.store`)
    searches the vector store for documents similar to `query`—the user's message—using
    the generated embeddings, returning a list of relevant `Document` objects based
    on the query.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, we've coded up the vector store functionality into a handy little
    class; next, let's use the class to ingest our `articles/` folder.
  prefs: []
  type: TYPE_NORMAL
- en: This is an *offline* step that you only need to perform one time; once you've
    stored your articles in Pinecone, they'll remain there until you remove them.
  prefs: []
  type: TYPE_NORMAL
- en: Go ahead and create a file called `ingest_to_vector_store.py`, copying the contents
    from listing 10.9.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.9 ingest_to_vector_store.py
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: (`chapter_10/in_progress_05/ingest_to_vector_store.py` in the GitHub repo)
  prefs: []
  type: TYPE_NORMAL
- en: 'Since this isn''t meant to be run using Streamlit, we use the `toml` module
    directly to read our `secrets.toml`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: At the end of this, `secrets` should be a dictionary that contains the `api_keys`
    and `config` keys we organized `secrets.toml` into earlier.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next we grab the values we need:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now instantiate our `VectorStore` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we trigger the `ingest_folder` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'To perform the actual ingestion, run this file in your terminal with the `python`
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: Once it completes, you can go to the page corresponding to your index on the
    Pinecone website to see the newly ingested chunks, as seen in figure 10.16.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../Images/10__image016.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.16 You can see the chunks in your index on the Pinecone website.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Notice the source field which holds the file name each chunk came from. You
    can also click the edit button on a record to add more metadata or to see its
    numeric vector values. If you were to count them, you'd find that there are 1536
    of them, corresponding to the number of dimensions in the embedding model.
  prefs: []
  type: TYPE_NORMAL
- en: Adding RAG to the graph
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The Pinecone index we need for RAG is ready to go, but we still need to incorporate
    the functionality in our chatbot. For this, let''s first lay out the additional
    instructions Nibby needs to use the knowledge base. Append the following to `prompts.py`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: '{docs_content}'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: (`chapter_10/in_progress_05/prompts.py` in the GitHub repo)
  prefs: []
  type: TYPE_NORMAL
- en: In a moment, we'll write the logic to replace `{docs_content}` with the document
    chunks we retrieve from Pinecone. The idea here is to give Nibby the context it
    needs and to get it to stop fabricating an answer out of thin air if it's not
    confident.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let's modify `graph.py` as shown in listing 10.10 so that it implements
    RAG.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.10 graph.py (with RAG nodes)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: (`chapter_10/in_progress_05/graph.py` in the GitHub repo)
  prefs: []
  type: TYPE_NORMAL
- en: 'The first change is to `AgentState`, which now looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: We'll now store the list of chunks retrieved from Pinecone in the graph state
    in a variable called `retrieved_docs`.
  prefs: []
  type: TYPE_NORMAL
- en: '`__init__` now accepts `vector_store`—an instance of our `VectorStore` class—as
    an argument and saves it to `self.vector_store`.'
  prefs: []
  type: TYPE_NORMAL
- en: Note that rather than create the `VectorStore` instance *within* `graph.py`,
    we've chosen to have it be created elsewhere (`bot.py`, as we'll soon find out)
    and simply pass it to the `SupportAgentGraph` class. This is because we want `graph.py`
    to only contain the core logic of the graph. Objects that the graph *depends*
    on such as `llm` and `vector_store` should be passed to it. This coding pattern
    is called *dependency injection*, and is helpful while writing automated tests.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we need to introduce the process of retrieval-augmented generation to
    our graph. Figure 10.17 shows what the graph should look like by the end of this.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../Images/10__image017.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.17 Our graph now has retrieve and augment nodes for RAG.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'We''ve inserted two nodes in between the `base_context` and `assistant` nodes:
    a `retrieve` node that grabs context from the knowledge base that''s relevant
    to the user''s query and an `augment` node that adds this info to the prompt.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s the code for the `retrieve` node:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: As in the case of `assistant_node`, `retrieve_node` is structured as a nested
    function within a class method. It simply extracts the content of all the messages
    in the conversation and puts them in a single string to form the "query" that
    we'll pass to the vector store.
  prefs: []
  type: TYPE_NORMAL
- en: The idea here is to find the chunks most relevant to the conversation. Since
    we're measuring relevance against the query, it makes sense that this is simply
    the text of the conversation.
  prefs: []
  type: TYPE_NORMAL
- en: Once we have the query, we can call the `retrieve` method we defined earlier
    and return the list of retrieved documents in a dictionary, thereby updating the
    `retrieved_docs` key of the graph state.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: While we've kept things simple here by including the text of the entire conversation
    in the retrieval query, you'll likely run into challenges as the conversation
    gets longer and longer. For any particular AI response, the most *recent* messages
    in the conversation are probably more contextually relevant—so it might be a good
    idea to form the retrieval query from just the last, say, five or six messages
    in the conversation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The augmentation node is shown below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: This one doesn't need to access anything from `SupportAgentGraph`, so we structure
    it as a static method as we did for `base_context_node`.
  prefs: []
  type: TYPE_NORMAL
- en: '`augment_node` mostly does the tedious work of wrangling and inserting the
    retrieved chunks into the system message. Once it has formed a string by concatenating
    the contents of the retrieved `Document` chunks, it simply plugs it into the text
    of the `SYS_MSG_AUGMENTATION` value we added to `prompts.py`, replacing `{docs_content}`.'
  prefs: []
  type: TYPE_NORMAL
- en: At the end of this node, `sys_msg_text` has the full system message—the earlier
    base message warning Nibby not to entertain frivolous questions, as well as the
    retrieved context.
  prefs: []
  type: TYPE_NORMAL
- en: We already have a node for the "generate" step in RAG—`assistant_node`—so there's
    no need to add another.
  prefs: []
  type: TYPE_NORMAL
- en: The edits to `build_graph` should be fairly obvious given figure 10.17; we officially
    add the `retrieve` and `augment` nodes, and attach them to the right edges.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next file to edit is `bot.py`. Make the changes below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: (`chapter_10/in_progress_05/bot.py` in the GitHub repo)
  prefs: []
  type: TYPE_NORMAL
- en: '`__init__` now accepts a `config` parameter, saved to `self.config`. It also
    creates the `vector_store` object by calling the `get_vector_store` method we''ll
    discuss below, and passes it to the `SupportAgentGraph` constructor.'
  prefs: []
  type: TYPE_NORMAL
- en: '`get_vector_store` has the code required to complete the loop. It obtains the
    Pinecone index name from `self.config` and passes both `self.api_keys` and `index_name`
    to create an instance of `VectorStore` before returning it.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The last change we need to make is a pretty small one in `frontend.py`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: (`chapter_10/in_progress_05/frontend.py` in the GitHub repo)
  prefs: []
  type: TYPE_NORMAL
- en: Since the `Bot` class now accepts a `config` parameter, we get its value from
    `st.secrets` and pass it in while instantiating the class.
  prefs: []
  type: TYPE_NORMAL
- en: Let's try asking Nibby what products Note n' Nib sells again. Re-run the app
    and talk to it. Figure 10.18 shows an example of the new interaction.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../Images/10__image018.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.18 Nibby can now access and use information from our knowledge base
    (see chapter_10/in_progress_05 in the GitHub repo for the full code).
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Notice how Nibby answers our questions using information from our knowledge
    base and redirects to a 1-800 number when it encounters a question it doesn't
    know how to answer.
  prefs: []
  type: TYPE_NORMAL
- en: 10.6 Turning our bot into an agent
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Giving Nibby access to Note n' Nib's customer support knowledge base has turned
    Nibby into a capable assistant, but it's still a purely informational bot. When
    customers call a support number or chat with a service representative, they're
    more often than not looking for help with their specific situation or order.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, a customer might wonder why it's taking so long for their placed
    order to arrive and want to check on the status, or they might want to simply
    cancel it. To resolve such issues, Nibby can't just rely on static textual articles;
    it needs to connect to Note n' Nib's systems and retrieve the right information
    or take the appropriate action.
  prefs: []
  type: TYPE_NORMAL
- en: 'AI applications that can interact with the real world in this way have a special
    name: agents.'
  prefs: []
  type: TYPE_NORMAL
- en: 10.6.1 What are agents?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Traditional AI chatbots follow a question-answer pattern, providing helpful
    but static responses. However, when users need personalized assistance—such as
    checking an order status, updating an address, or canceling an order—an informational
    bot falls short.
  prefs: []
  type: TYPE_NORMAL
- en: This is where *agents* come in. Unlike passive chatbots, AI agents—also called
    *agentic apps*—can reason, plan, and interact with external systems to complete
    tasks. They don't just retrieve knowledge; they take actions based on it. These
    agents often rely on *tool use*, meaning they can call APIs, run database queries,
    and even trigger workflows in real-world applications.
  prefs: []
  type: TYPE_NORMAL
- en: For example, instead of telling a customer to reach out to a customer care number
    for an order tracking number, an agent can fetch the tracking details and provide
    an update directly. Instead of directing a user to a cancellation policy page,
    it can process a cancellation request on their behalf.
  prefs: []
  type: TYPE_NORMAL
- en: In short, agents make AI practical by allowing it to interface with the systems
    people already use. The key to making an agent work effectively is a framework
    that enables it to reason and decide the next steps dynamically. One such popular
    framework is called *ReAct*, short for *Reasoning + Acting*.
  prefs: []
  type: TYPE_NORMAL
- en: 10.6.2 The ReAct framework
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To function as a true agent, an AI system must do more than just retrieve facts—it
    needs to reason about a situation, determine the right action, execute that action,
    and then incorporate the result into its next steps. The ReAct framework is designed
    to facilitate this process.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The ReAct AI framework is not to be confused with React, a Javascript toolkit
    for building web apps. It just so happens that we'll encounter the latter in chapter
    13.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.19 is a visual representation of the ReAct framework.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../Images/10__image019.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.19 The ReAct framework
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'ReAct structures an AI agent''s behavior as an interleaving of reasoning steps
    and actions:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Reason:** The agent analyzes the user''s query, breaks it down into logical
    steps, and determines what needs to be done.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Act:** The agent takes a concrete action, such as calling an API or querying
    a database, to retrieve relevant data or execute a task.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Reason:** The agent incorporates the action''s results into its reasoning
    and decides whether further steps are necessary.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Repeat:** If further steps are needed, the cycle repeats.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Respond:** If no further steps are needed, respond to the user.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For example, consider a customer asking Nibby: "What''s the status of my order?"
    Using the ReAct framework, the bot might follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Reason:** "The customer wants to check their order status. I need to fetch
    order details from the order database."'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Act:** Call Note & Nib''s order management system to retrieve the order status.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Reason:** "The system shows the order has shipped and is in transit. I should
    provide the expected delivery date."'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Respond:** Respond to the customer with the latest tracking details and estimated
    delivery date.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Creating tools our agent can use
  prefs: []
  type: TYPE_NORMAL
- en: The concept of *tools* is central to the process of developing a ReAct AI agent.
    In AI parlance, a tool is a function or API that an AI agent can call to perform
    real-world actions.
  prefs: []
  type: TYPE_NORMAL
- en: Referring back to the requirements we drafted, we want our bot to be able to
    track and cancel orders on behalf of Note n' Nib's customers. Of course, Note
    n' Nib is a fictional company with no real orders or customers.
  prefs: []
  type: TYPE_NORMAL
- en: This means we need some example data to work with. You can find this in the
    database.py file in the GitHub repository ([https://github.com/aneevdavis/streamlit-in-action/tree/main/chapter_10/in_progress_06/](https://github.com/aneevdavis/streamlit-in-action/tree/main/chapter_10/in_progress_06/)).
    Copy the file to your working directory. Listing 10.11 shows a couple of excerpts
    from it.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.11 database.py
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: (`chapter_10/in_progress_06/database.py` in the GitHub repo)
  prefs: []
  type: TYPE_NORMAL
- en: The file has two dictionaries—`users` and `orders`—keyed on user ID and order
    ID, respectively. Highlighted in the listing is user ID `1` (`Alice Johnson`)
    and a corresponding order the user placed on `2025-02-10`.
  prefs: []
  type: TYPE_NORMAL
- en: In a real application, this information would be stored in a database like PostgreSQL.
    Still, since our focus is on the mechanics of building an AI agent, the static
    data in `database.py` will suffice for our purposes.
  prefs: []
  type: TYPE_NORMAL
- en: There's another file in the repo called `tools.py`. Copy that one over too.
  prefs: []
  type: TYPE_NORMAL
- en: '`tools.py` contains all the functions or tools that Nibby will be able to call.
    The file defines four such functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '`retrieve_user_id` looks up a user''s user ID, given their email address and
    date of birth.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`get_order_id` returns the ID of a particular order, given the user ID of the
    user that placed the order, and the date that they placed it. For simplicity,
    we assume that a user can only place one order on a particular day.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`get_order_status` accepts an order ID and returns its order status, tracking
    number, and expected delivery date.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cancel_order` cancels an order, given an order ID.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We're not interested in the actual implementations of these functions, though
    you can read through them in `tools.py`. What matters is that Nibby needs to be
    able to call and use them correctly.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this, we''ll rely on type hints and docstrings. For instance, consider
    the definition of one of these functions in `tools.py`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: (`chapter_10/in_progress_06/tools.py` in the GitHub repo)
  prefs: []
  type: TYPE_NORMAL
- en: 'Notice how we''re using type hints in the function signature (`(email: str,
    dob: str) -> str`), and explaining in detail what the function does in a docstring
    (the multi-line string after the signature), along with details about the arguments
    and return value.'
  prefs: []
  type: TYPE_NORMAL
- en: These auxiliary items are what the AI model will use to figure out what tool
    to call when.
  prefs: []
  type: TYPE_NORMAL
- en: 'At the bottom of `tools.py`, there''s this line that exports all the functions
    in the file as a list named tools:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: We'll use the `tools` variable in our graph in a bit.
  prefs: []
  type: TYPE_NORMAL
- en: 10.6.3 Making our graph agentic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Consider an example how Nibby might use the four tools we''ve made available
    to help a customer. Let''s say the customer in the example data, Alice Johnson,
    wants to know the status of their order (order 101 shown in the excerpt from `database.py`).
    Here''s how the interaction might proceed:'
  prefs: []
  type: TYPE_NORMAL
- en: '**User:** "Hi, my name''s Alice! I placed an order on Feb 10, 2025\. Could
    you check on its status, please?"'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Nibby (reasoning to itself):** "I have a tool called `get_order_status` that
    will give me the status of an order. To call it, I need an `order_id`. I don''t
    have one, but I can get one by calling the `get_order_id` tool, which needs a
    `user_id` and an `order_placed_date`. The user says the `order_placed_date` is
    Feb 10, 2025, so I have that. I still need a `user_id`. To obtain *that*, I can
    call the `retrieve_user_id` tool, which requires an email and a date of birth.
    Therefore, I should ask the customer for this info."'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Nibby:** "Could you give me your email address and date of birth?"'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**User:** "Sure, my email is alice.johnson@example.com and my date of birth
    is May 10, 1990."'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Nibby (tool call):** Call the tool `retrieve_user_id` with the parameters
    `email=`alice.johnson@example.com and `dob=1990-05-10`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`retrieve_user_id(''`alice.johnson@example.com`'', ''1990-05-10'')`: <returns
    the value `1`, which is Alice''s `user_id`>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Nibby (tool call):** Call the tool `get_order_id` with the parameters `user_id=1`
    and `order_placed_date=2025-02-10`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`get_order_id(1, 1990-05-10)`: <returns the value `101`, the correct order
    ID>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Nibby (tool call):** Call the tool `get_order_status` with the parameter
    `order_id=101`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**get_order_status(101)**: <returns a dictionary: {''order_status'': ''Shipped'',
    ''tracking_number'': ''TRK123456789'', ''expected_delivery_date'': ''2025-02-18''}>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Nibby:** "Your order has been shipped and should arrive on Feb 18, 2025\.
    You can use the tracking number TRK123456789 to track it."'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note how the bot needs to alternate between conversing with the user, reasoning
    about what needs to be done next, and issuing a call to a tool. How do we code
    all of this up? LangGraph actually makes this surprisingly easy.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s modify `bot.py` to make our LLM aware of the tools available
    to it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: Here we're *binding* the tools we defined earlier—imported from `tools.py`—to
    our LLM object so that it knows they exist. As a result, if the LLM thinks it's
    appropriate, it can respond with a *tool call*.
  prefs: []
  type: TYPE_NORMAL
- en: In the example above, the tool calls are marked with "Nibby (tool call)". In
    practical terms, these are `AIMessage`s produced by the LLM that have a property
    called `tool_calls` which contains information about any tools that the LLM wants
    us to call on its behalf and the parameters to call them with.
  prefs: []
  type: TYPE_NORMAL
- en: The binding logic uses the docstrings and type hints we specified in `tools.py`
    to explain to the LLM what each tool does and how to use it.
  prefs: []
  type: TYPE_NORMAL
- en: What about the graph itself? Listing 10.12 shows the changes required to `graph.py`
    to turn our bot into an agent.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.12 graph.py (for an agentic app)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: (`chapter_10/in_progress_06/graph.py` in the GitHub repo)
  prefs: []
  type: TYPE_NORMAL
- en: Incredibly, all we needed to do was to add three lines to the `build_graph`
    method, and import a few extra items!
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s go through the line additions, starting with the first one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'This adds a node named tools to our graph. `ToolNode` is a node already built
    into LangGraph, so we don''t have to define it ourselves. It essentially does
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: It takes the last message in the messages list (from the graph state) and executes
    any tool calls in it based on the list of tools we pass it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appends the value returned by the tool(s) as a `ToolMessage`—like `HumanMessage`
    and `AIMessage`—to the messages list.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At the end of a `ToolNode`, the last message in the messages variable is a `ToolMessage`
    representing the output of calling a tool.
  prefs: []
  type: TYPE_NORMAL
- en: We now have all the *pieces* we need, but how do we orchestrate the kind of
    thinking process outlined in the example interaction at the beginning of this
    section?
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we get into that, turn your attention to figure 10.20, which is what
    our graph will look like at the end of these changes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../Images/10__image020.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.20 Our graph now has a tools node and a conditional edge
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Notice that there are now *two* lines flowing out from the assistant node—one
    goes to `END` as before, while another goes to our newly added `tools` node.
  prefs: []
  type: TYPE_NORMAL
- en: 'In LangGraph, this is known as a *conditional edge*. We can choose the next
    node to execute from multiple options depending on a specified condition. A conditional
    edge is implemented as a function that takes the following form:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'In our case, we don''t actually need to build our own conditional edge because
    LangGraph already has exactly what we want:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: '`tools_condition`—imported from `langgraph.prebuilt`—simply routes to our `ToolNode`
    (named `tools`) if the last message in the conversation—i.e. the LLM''s response—contains
    any tool calls, or to END otherwise.'
  prefs: []
  type: TYPE_NORMAL
- en: Since `tools_condition` already has the logic to route to `END`, we can remove
    the earlier line that created a direct (non-conditional) edge between `assistant`
    and `END`.
  prefs: []
  type: TYPE_NORMAL
- en: Once the `ToolNode` executes, we need the LLM to read the return value and decide
    what to do with it—whether it's calling *another* tool, or responding to the user.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, we create a *loop* in the graph by connecting the tools node *back*
    to `assistant`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: And that's all it takes! Now when a request comes in, the LLM will reason about
    what to do. If it decides to call a tool, it'll put a tool call in its response,
    causing `tools_condition` to route to the `ToolNode` which executes the call.
    Since `ToolNode` has a direct edge to assistant, the LLM will get the updated
    list of messages with the appended `ToolMessage` and can again reason about what
    to do with the response.
  prefs: []
  type: TYPE_NORMAL
- en: If the LLM decides it doesn't need to call any more tools—or that the next step
    in the process is to get some information from the user—it won't include any tool
    calls in its response, which means that `tools_condition` will route to `END`,
    and the final message will be displayed to the user.
  prefs: []
  type: TYPE_NORMAL
- en: While that's all that's required to get the bot to work correctly, there's a
    last change we need to make in `graph.py` that's related to what gets shown to
    the customer in the frontend.
  prefs: []
  type: TYPE_NORMAL
- en: 'As the above paragraphs hopefully make clear, the communication between the
    `assistant` and `tools` nodes happens through the `messages` variable in the graph
    state, and consists of internal messages of two types: `AIMessages` containing
    tool calls, and `ToolMessages` containing tool return values.'
  prefs: []
  type: TYPE_NORMAL
- en: Since we don't want to expose these internal messages to the user of our Streamlit
    app, we need to hide them when we pass the conversation history back. Recall that
    this history is relayed through the `get_history` method in `bot.py`, which calls
    the `get_conversation` method in `graph.py`
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s make the appropriate changes in `graph.py` to remove these messages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: (`chapter_10/in_progress_06/graph.py` in the GitHub repo)
  prefs: []
  type: TYPE_NORMAL
- en: First, we define `is_internal_message`, a static method that determines whether
    a message that we pass it is an "internal" one, i.e., one that's not fit to show
    the user. Above, we're defining an internal message as one that either has the
    type "`tool"`—which is true of `ToolMessages`—or has a `"tool_calls"` property
    (within `additional_kwargs`, a property that the LLM will use to set metadata
    within a message).
  prefs: []
  type: TYPE_NORMAL
- en: Then, rather than returning all the messages from the state in `get_conversation`,
    we now filter for the non-internal messages and only return those.
  prefs: []
  type: TYPE_NORMAL
- en: With that out of the way, re-run the app and test out its new capabilities!
    Figure 10.21 shows an example.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../Images/10__image021.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.21 Nibby can handle real-world actions like tracking and canceling
    orders (see chapter_10/in_progress_06 in the GitHub repo for the full code).
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: By allowing Nibby to access external tools, we've given it superpowers and saved
    Note n' Nib's customer support division some serious time!
  prefs: []
  type: TYPE_NORMAL
- en: This has been our most advanced app yet. If you've been following along and
    working on these projects yourself, you probably appreciate that the more complex
    an app becomes, the more things can go wrong in the real world when users actually
    start interacting with it. In the next chapter, we'll explore how to catch these
    proble
  prefs: []
  type: TYPE_NORMAL
- en: ms beforehand and test your app to make it as robust as possible.
  prefs: []
  type: TYPE_NORMAL
- en: 10.7 Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Real-world AI applications require advanced capabilities, such as knowledge
    retrieval and action execution.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LangGraph structures AI workflows as graphs; nodes represent steps in the AI
    process, and edges define the flow between them.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each node in LangGraph takes in the graph state and modifies it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`st.chat_input` renders a textbox where users can type messages.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`st.chat_message` displays human and AI messages appropriately.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LangGraph uses checkpointers to persist information across multiple graph executions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It's important to instruct the LLM explicitly to stay on-topic and ignore irrelevant
    requests; the system message is a good place for this.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Embeddings involve converting an object—such as a piece of text—into lists of
    numbers called vectors to find relevant content using similarity search.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Retrieval Augmented Generation (RAG) is a technique for providing custom knowledge
    to a pre-trained LLM by retrieving chunks of context relevant to the user query
    from a vector database like Pinecone.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A *tool* is simply a well-documented function that an LLM can choose to call
    in response to a user query.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An agentic app—or simply an *agent*—is one that can use tools to interact with
    the real world.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LangGraph makes it extremely easy to write agentic apps through its `ToolNode`
    and `tools_condition` abstractions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
