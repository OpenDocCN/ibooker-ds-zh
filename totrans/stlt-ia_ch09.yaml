- en: 10 A customer support chatbot with LangGraph and Streamlit
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 10 使用LangGraph和Streamlit的客户支持聊天机器人
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Developing a chatbot frontend with Streamlit's chat elements
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Streamlit的聊天元素开发聊天机器人前端
- en: Using LangGraph and LangChain to streamline an advanced AI app
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用LangGraph和LangChain简化高级AI应用
- en: How embeddings and vector databases work
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 嵌入和向量数据库是如何工作的
- en: Augmenting an LLM's pre-trained knowledge using Retrieval Augmented Generation
    (RAG)
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用检索增强生成（RAG）增强LLM的预训练知识
- en: Enabling an LLM to access and execute real-world actions
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使LLM能够访问和执行现实世界的操作
- en: Creating a fun and engaging experience—like the trivia game we built in chapter
    nine—is exciting, but the true power of AI lies in its ability to drive real business
    value. AI isn't just about answering questions or generating text; it's about
    transforming industries, streamlining operations, and enabling entirely new business
    models.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个有趣且引人入胜的体验——就像我们在第九章中构建的问答游戏一样——令人兴奋，但AI的真正力量在于其推动真实商业价值的能力。AI不仅仅是回答问题或生成文本；它是关于改变行业，简化运营，并使全新的商业模式成为可能。
- en: However, building AI applications that deliver economic value requires more
    than just calling a pre-trained model. For AI to be useful in real-world scenarios,
    it needs to be aware of the context in which it operates, connect to external
    data sources, and take meaningful actions. Companies need AI to understand and
    respond to domain-specific queries, interact with business systems, and provide
    personalized assistance.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，构建能够带来经济价值的AI应用，仅仅调用预训练模型是不够的。为了使AI在现实场景中发挥作用，它需要了解其操作的环境，连接到外部数据源，并采取有意义的行动。公司需要AI理解并响应特定领域的查询，与业务系统交互，并提供个性化帮助。
- en: 'In this chapter, we''ll build such an application: a customer service chatbot
    that will retrieve real company data, help customers track and cancel orders,
    and intelligently decide when to escalate issues to a human agent. By the end
    of this chapter, you''ll understand how to integrate LLMs with private knowledge
    bases, implement retrieval-augmented generation (RAG), and enable an AI agent
    to take action in the real world. Let''s dive in.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将构建这样一个应用：一个客户服务聊天机器人，它将检索真实公司的数据，帮助客户跟踪和取消订单，并智能地决定何时将问题升级给人工客服。到本章结束时，你将了解如何将LLM与私有知识库集成，实现检索增强生成（RAG），并使AI代理能够在现实世界中采取行动。让我们深入探讨。
- en: Note
  id: totrans-10
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: The GitHub repo for this book is [https://github.com/aneevdavis/streamlit-in-action](https://github.com/aneevdavis/streamlit-in-action).
    The chapter_10 folder has this chapter's code and a requirements.txt file with
    exact versions of all the required Python libraries.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本书GitHub仓库的链接是[https://github.com/aneevdavis/streamlit-in-action](https://github.com/aneevdavis/streamlit-in-action)。chapter_10文件夹包含本章的代码和一个requirements.txt文件，其中包含所有必需Python库的确切版本。
- en: '10.1 Nibby: A customer service bot'
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.1 Nibby：一个客户服务机器人
- en: Under the leadership of Note n' Nib's new CEO—renowned for his legendary decision-making
    prowess, aided by a certain dashboard revered across the company—the brand has
    flourished into a stationery powerhouse with rocketing sales.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在Note n' Nib的新任CEO的领导下——以其传奇般的决策能力而闻名，得益于公司内备受推崇的某个仪表板——品牌已经发展成为一家销售激增的文具巨头。
- en: But success brings its own challenges. The customer support department is swamped
    with calls from buyers who are impatient for their orders to arrive or seeking
    advice about fountain pen maintenance. After a month of complaints about long
    wait times, the CEO summons the one person known company-wide for reliable innovation.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 但成功也带来了自己的挑战。客户支持部门被急于收到订单或寻求钢笔维护建议的买家电话淹没。在一个月的长时间等待投诉之后，CEO召集了公司公认的可靠创新者。
- en: 'And so it is that *you* are tasked with solving the support crisis. When you''re
    not delivering seminars on Streamlit, you''re reading up on the latest advances
    in AI; it is not long before an intriguing possibility hits you: might it be possible
    to *automate* customer support?'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，*你*被委以解决支持危机的重任。当你不在进行Streamlit研讨会时，你正在研究AI的最新进展；不久，一个引人入胜的可能性击中了你：自动化客户支持是否可能？
- en: 'Over the course of a sleepless night, you sketch out plans for a Streamlit
    support bot named Nibby. Whispers of your project spread across the company. "We
    are saved!" some declare. "Nibby will not fail us!" Skeptics scoff: "''Tis folly!
    No *robot* can fix this."'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个不眠之夜中，你草拟了Streamlit支持机器人Nibby的计划。你项目的消息在公司中传播。“我们得救了！”有些人宣称。“Nibby不会让我们失望！”怀疑者嘲笑：“这是愚蠢！没有*机器人*能解决这个问题。”
- en: Who will prove right? Let's find out.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 谁将证明是正确的？让我们拭目以待。
- en: 10.1.1 Stating the concept and requirements
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.1.1 陈述概念和需求
- en: As always, we start with a distilled one-line description of what we intend
    to build.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 与往常一样，我们从一个简短的描述开始，说明我们打算构建什么。
- en: Concept
  id: totrans-20
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 概念
- en: Nibby, a customer support chatbot that can help Note n' Nib's customers with
    information and basic service requests
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: Nibby，一个客户支持聊天机器人，可以帮助Note n' Nib的客户处理信息和基本服务请求
- en: '"Customer support" obviously spans a lot of territory, so let''s define the
    exact requirements more clearly.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: “客户支持”显然覆盖了很大范围，因此让我们更清楚地定义具体要求。
- en: Requirements
  id: totrans-23
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 需求
- en: 'In our vision of automating customer support, Nibby will be able to:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们自动化的客户支持愿景中，Nibby将能够：
- en: hold a human-like conversation with a customer
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与客户进行类似人类的对话
- en: answer relevant questions about Note n' Nib and its products based on a custom
    knowledge base
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据自定义知识库回答关于Note n' Nib及其产品的相关问题
- en: 'handle the following requests from the customer:'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理客户提出的以下请求：
- en: tracking an order
  id: totrans-28
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 跟踪订单
- en: canceling an order
  id: totrans-29
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 取消订单
- en: redirect to a human customer support agent if it cannot fulfill the request
    on its own
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果无法自行满足请求，则将请求重定向到人工客户支持代理
- en: In essence, we want Nibby to take as much load off Note n' Nib's overworked
    human support agents as possible. Nibby should act as a "frontline" agent who
    can take care of most basic requests, such as providing product information or
    canceling orders, and redirect to a human only when necessary.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 从本质上讲，我们希望Nibby尽可能多地减轻Note n' Nib过度劳累的人工支持代理的负担。Nibby应作为“前线”代理，能够处理大多数基本请求，例如提供产品信息或取消订单，并在必要时才将请求转接到人工代理。
- en: What's out of scope
  id: totrans-32
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 范围之外的内容
- en: 'To keep this project manageable and small enough to fit into this chapter,
    we''ll decide *not* to implement the following:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使这个项目可管理，并且足够小，可以放入本章，我们将决定**不**实现以下功能：
- en: Storing or remembering prior conversations with a user
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存储或记住与用户的先前对话
- en: Any "actions" other than the two provided above, i.e., tracking and canceling
    orders
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 上述两个提供的“操作”之外的任何“操作”，即跟踪和取消订单
- en: Actual working logic for the two actions discussed, e.g., building an order
    tracking or cancellation system
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两个讨论的操作的实际工作逻辑，例如，构建订单跟踪或取消系统
- en: From a learning perspective, we truly want to focus on building a relatively
    complex AI system that can converse with users, understand a custom knowledge
    base, and take real-world actions.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 从学习角度来看，我们真正希望专注于构建一个相对复杂的AI系统，该系统能够与用户进行对话，理解自定义知识库，并执行现实世界的操作。
- en: The specific actions we enable the app to perform don't matter. For instance,
    the fact that our app can cancel an order—as opposed to replacing an item—is not
    of any particular significance. Indeed, as implied by the third point above, the
    order cancellation we will implement is dummy "toy" logic. What *is* significant
    is that our bot should be able to intelligently choose to run that logic based
    on the free-form conversation the user is having with it.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使应用程序能够执行的具体操作并不重要。例如，我们的应用程序可以取消订单——而不是替换物品——这并不具有特别的意义。实际上，如上述第三点所暗示的，我们将实现的订单取消是虚拟的“玩具”逻辑。真正重要的是，我们的机器人应该能够根据用户与其进行的自由形式对话智能地选择运行该逻辑。
- en: 10.1.2 Visualizing the user experience
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.1.2 可视化用户体验
- en: The user interface for Nibby might be the most straightforward among all the
    apps in this book. Figure 10.1 shows a sketch of what we'll build.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: Nibby的用户界面可能是本书中所有应用程序中最直接的。图10.1显示了我们将构建的内容草图。
- en: '![image](../Images/10__image001.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![image](../Images/10__image001.png)'
- en: Figure 10.1 UI sketch for Nibby, our customer support chatbot.
  id: totrans-42
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.1 Nibby，我们的客户支持聊天机器人的UI草图。
- en: Nibby's UI isn't significantly different from any chat or instant messaging
    app you may have used—from WhatsApp on your phone to Slack on your corporate laptop.
    You'll notice a familiar-looking text box at the bottom for users to type messages.
    Each user message triggers an AI response, which is appended to the conversation
    view above.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: Nibby的用户界面与您可能使用过的任何聊天或即时通讯应用没有显著区别——从手机上的WhatsApp到公司笔记本电脑上的Slack。您会注意到底部有一个熟悉的文本框，供用户输入消息。每个用户消息都会触发一个AI响应，并将其附加到上面的对话视图中。
- en: 10.1.3 Brainstorming the implementation
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.1.3 实施方案头脑风暴
- en: The difficult part of building this app will be the backend—specifically, getting
    the bot to answer questions correctly and connect to outside tools. Figure 10.2
    shows our overall design.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 构建这个应用程序的难点在于后端——特别是让机器人正确回答问题并连接到外部工具。图10.2显示了我们的总体设计。
- en: '![image](../Images/10__image002.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/10__image002.png)'
- en: Figure 10.2 Overall design for Nibby
  id: totrans-47
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.2 Nibby的整体设计
- en: While our trivia app from chapter 9 had an interesting design when it came to
    state management, its "intelligent" aspect was fairly simple—feed a prompt to
    an LLM and have it respond.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然第九章中的Trivia应用在状态管理方面有一个有趣的设计，但其“智能”方面相当简单——向LLM提供一个提示并让它做出回应。
- en: 'On the other hand, a customer support app with the capabilities we''re envisioning
    has a more involved design in at least two respects:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，我们构想的客户支持应用在至少两个方面有更复杂的设计：
- en: It needs a way to augment a customer's query with private knowledge about the
    company that a human agent would possess.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它需要一种方法来增强客户的查询，加入公司内部代理人员所拥有的私人知识。
- en: It needs to be able to execute code in the real world.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它需要能够在现实世界中执行代码。
- en: Figure 10.2 gives a basic overview of how we achieve these. When a user message
    comes in through our frontend, we retrieve context relevant to the message from
    a private knowledge store—a vector database, as we'll see later—and send that
    to the LLM.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.2展示了我们如何实现这些功能的基本概述。当用户消息通过我们的前端传入时，我们从私有知识库中检索与消息相关的上下文——正如我们稍后将要看到的，这是一个向量数据库——并将其发送给LLM。
- en: We also organize the actions we want our bot to be able to take into so-called
    *tools* and make the LLM aware of their existence, what each tool does, and how
    to call them. For any given input, the LLM can either issue a call to a tool or
    respond to the user directly.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将把我们的机器人能够执行的操作组织成所谓的“工具”，并让LLM了解它们的存在、每个工具的功能以及如何调用它们。对于任何给定的输入，LLM可以发出对工具的调用或直接对用户做出回应。
- en: In the former case, we execute the tool as specified by the LLM and send the
    results for further processing, while in the latter case, we append the response
    to a conversation view on the frontend and await the user's next message.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一种情况下，我们按照LLM指定的方式执行工具，并将结果发送进行进一步处理；而在后一种情况下，我们将回复附加到前端上的对话视图中，并等待用户的下一条消息。
- en: 10.1.4 Installing dependencies
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.1.4 安装依赖项
- en: 'We''ll be using several Python libraries in this chapter. To get everything
    ready in advance, install them all at once by running the following command:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用几个Python库。为了提前做好准备，可以通过运行以下命令一次性安装它们：
- en: '[PRE0]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 10.2 Creating a basic chatbot
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.2 创建基本聊天机器人
- en: Chapter nine introduced LLMs and demonstrated how to use the OpenAI API for
    simple applications. While OpenAI's API is easy to integrate, developing more
    sophisticated AI-driven apps—such as those leveraging Retrieval-Augmented Generation
    (RAG) or agent-based workflows, which we'll encounter soon—adds complexity.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 第九章介绍了LLM，并演示了如何使用OpenAI API进行简单应用。虽然OpenAI的API易于集成，但开发更复杂的AI驱动应用——例如利用检索增强生成（RAG）或基于代理的工作流程，我们很快就会遇到——增加了复杂性。
- en: A new ecosystem of libraries and tools has emerged to make creating complex
    AI apps as easy as possible. In this chapter, we'll explore LangGraph and LangChain,
    two libraries that work together to smooth the application creation process.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 一个新的生态系统已经出现，提供了库和工具，使得创建复杂的AI应用尽可能容易。在本章中，我们将探讨LangGraph和LangChain这两个库，它们协同工作以简化应用创建过程。
- en: 10.2.1 Intro to LangGraph and LangChain
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.2.1 LangGraph和LangChain简介
- en: LLMs have undoubtedly been the most influential technological advance of the
    last decade. At their core, interacting with an LLM consists of providing a prompt
    that the LLM can "complete." That's what everything else is built around.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs无疑是过去十年中最有影响力的技术进步。其核心在于与LLM的交互，包括提供LLM可以“完成”的提示。所有其他功能都是围绕这一点构建的。
- en: 'Contrast this with the complexities that modern AI applications have to deal
    with:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 与现代AI应用必须处理的复杂性相比：
- en: Handling multi-step workflows (e.g., retrieving information before responding)
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理多步骤工作流程（例如，在回复之前检索信息）
- en: Integrating with external tools
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与外部工具集成
- en: Retaining conversation context across multiple turns
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在多个回合中保留对话上下文
- en: Managing this complexity manually is difficult, which is where LangChain and
    LangGraph—both Python libraries—come in. LangChain provides building blocks for
    working with LLMs, including prompt management, memory, and tool integration.
    LangGraph—developed by the same company—extends LangChain by structuring AI workflows
    as *graphs*, allowing for decision-making, branching logic, and multi-step processing.
    By combining these, we can design structured, intelligent AI applications that
    go beyond simple chat responses—enabling Nibby to retrieve knowledge, call APIs,
    and make decisions dynamically.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 手动管理这种复杂性是困难的，这就是LangChain和LangGraph——都是Python库——发挥作用的地方。LangChain提供了与LLMs一起工作的构建块，包括提示管理、记忆和工具集成。LangGraph——由同一家公司开发——通过将AI工作流程结构化为*图*来扩展LangChain，允许决策、分支逻辑和多步处理。通过结合这些，我们可以设计结构化、智能的AI应用，这些应用超越了简单的聊天响应——使Nibby能够检索知识、调用API并动态做出决策。
- en: In the rest of this chapter, we'll use these libraries extensively to achieve
    the functionality we want.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的其余部分，我们将广泛使用这些库来实现我们想要的功能。
- en: Note
  id: totrans-69
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: Since we will model our chatbot as a *graph* in LangGraph, we'll primarily speak
    about and refer to LangGraph rather than LangChain. However, you'll notice that
    many of the underlying classes and functions we'll use are imported from LangChain.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们将在LangGraph中将我们的聊天机器人建模为一个*图*，所以我们主要会讨论和引用LangGraph而不是LangChain。然而，你会注意到我们将使用的许多底层类和函数都是从LangChain导入的。
- en: 10.2.2 Graphs, nodes, edges, and state
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.2.2 图、节点、边和状态
- en: In LangGraph, you construct an AI application by building a *graph* of *nodes*
    that transform the application's *state*. If you don't have a background in computer
    science, that statement might trip you up, so let's break it down.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在LangGraph中，你通过构建一个*图*来构建一个AI应用，这个*图*由*节点*组成，这些节点会转换应用的状态。如果你没有计算机科学背景，这个声明可能会让你感到困惑，所以让我们来分解它。
- en: What is a graph, exactly?
  id: totrans-73
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 图究竟是什么？
- en: In graph theory, a graph is a network of interconnected vertices (also called
    nodes) and edges, which connect the vertices with each other. Software developers
    often use graphs to create conceptual models of real-world objects and their relationships.
    For instance, figure 10.3 shows a graph of people you might expect to find on
    a social media website.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在图论中，图是由相互连接的顶点（也称为节点）和边组成的网络，这些边将顶点相互连接。软件开发者经常使用图来创建现实世界对象及其关系的概念模型。例如，图10.3显示了可能在社会媒体网站上找到的人的图。
- en: '![image](../Images/10__image003.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![image](../Images/10__image003.png)'
- en: Figure 10.3 Using a graph to model friend relationships in a social network
  id: totrans-76
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.3 使用图来模拟社交网络中的朋友关系
- en: Here, each person is a vertex or node (shown by a circle), and the "friend"-relationship
    between any two people is an edge (the lines between the circles).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，每个人都是一个顶点或节点（用圆圈表示），任何两个人之间的“朋友”关系是一条边（圆圈之间的线条）。
- en: By modeling relationships in this way, the social media website can apply various
    algorithms developed for graphs to do useful real-world things. For instance,
    there's an algorithm called breadth-first search (BFS) that finds the shortest
    path from one node to any other node. In this case, we could use it to find the
    fewest common friends required to connect two people.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式模拟关系，社交媒体网站可以应用为图开发的算法来执行有用的现实世界任务。例如，有一个称为广度优先搜索（BFS）的算法，它可以从一个节点找到到任何其他节点的最短路径。在这种情况下，我们可以用它来找到连接两个人的最少共同朋友数量。
- en: In LangGraph, we model an application as a graph of *actions*, where a *node*
    signifies a single action that the application performs. A graph has a *state*,
    simply a collection of named attributes with values (similar to Streamit's concept
    of "session state"). Each node takes the current state of the graph as input,
    does something to modify the state, and returns the new state as its output.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在LangGraph中，我们将应用建模为一个*动作*的图，其中*节点*表示应用执行的单个动作。图有一个*状态*，简单地说，是一组具有值的命名属性（类似于Streamit的“会话状态”概念）。每个节点将图的当前状态作为输入，对状态进行一些修改，并将新的状态作为其输出返回。
- en: '![image](../Images/10__image004.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![image](../Images/10__image004.png)'
- en: Figure 10.4 In LangGraph, nodes take in the graph state and modify it.
  id: totrans-81
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.4 在LangGraph中，节点接收图状态并对其进行修改。
- en: An edge in the graph represents a connection between two nodes, or in other
    words, the fact that the output of one node may be the input to another. Unlike
    in the case of a social media graph where the edges had no direction (i.e., if
    two people are friends, each is a friend of the other), edges in LangGraph are
    *directed* because one node in the edge is executed *before* the other. Visually,
    we represent the direction as an arrow on the edge (figure 10.4).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图中的边表示两个节点之间的连接，或者换句话说，一个节点的输出可能是另一个节点的输入。与社交媒体图中的边没有方向（即如果两个人是朋友，那么每个人都是对方的朋友）的情况不同，LangGraph中的边是*有向的*，因为边中的一个节点在另一个节点之前执行。从视觉上看，我们用边上的箭头来表示方向（图10.4）。
- en: The input to the graph is its initial state that is passed to the *first* node
    that's executed, while the output is the final state returned by the *last* node
    that's executed.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图的输入是其初始状态，该状态传递给*第一个*执行的节点，而输出是最后一个执行的节点返回的最终状态。
- en: That was a fair bit of theory; let's now consider a toy example (figure 10.5)
    to make this all real.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 那是相当多的理论；现在让我们考虑一个玩具示例（图10.5）来使这一切变得真实。
- en: '![image](../Images/10__image005.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![image](../Images/10__image005.png)'
- en: Figure 10.5 A graph in LangGraph that computes the sum of squares of numbers
  id: totrans-86
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.5 LangGraph中计算数字平方和的图
- en: The application shown in figure 10.5 is a very simple one. There's no AI involved;
    it's just a program that takes a list of numbers and returns the sum of their
    squares—e.g., for the input `[3, 4]`, the graph would calculate the output `25`
    (`3^2 + 4^2 = 25`)
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.5中所示的应用非常简单。没有涉及AI；它只是一个程序，它接受一个数字列表并返回它们的平方和——例如，对于输入`[3, 4]`，图将计算输出`25`（`3^2
    + 4^2 = 25`）。
- en: 'The graph''s state contains three values: `numbers`, `squares`, and `answer`.
    `numbers` holds the list of input values (e.g. `[3, 4]`), while `squares` and
    `answer` don''t have a value to start with.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图的状态包含三个值：`numbers`、`squares`和`answer`。`numbers`保存输入值列表（例如`[3, 4]`），而`squares`和`answer`最初没有值。
- en: 'Each LangGraph graph has dummy nodes called `START` and `END`, which represent
    the start and end of execution. There are two other "real" nodes: `squaring_node`
    and `summing_node`.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 每个LangGraph图都有一个名为`START`和`END`的虚拟节点，它们代表执行的开始和结束。还有两个其他“真实”节点：`squaring_node`和`summing_node`。
- en: 'Here''s how the graph is executed:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是如何执行这个图的：
- en: The `START` node receives the initial state.
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`START`节点接收初始状态。'
- en: Since there's a directed edge from `START` to `squaring_node`, `squaring_node`
    is executed first.
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于从`START`到`squaring_node`存在有向边，因此`squaring_node`首先执行。
- en: '`squaring_node` takes in the starting state, squares the numbers in the `numbers`
    list and saves the new list (`[9, 16]`) under the variable `squares` in the state.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`squaring_node`接收初始状态，将`numbers`列表中的数字平方，并将新的列表（`[9, 16]`）保存到状态中的变量`squares`下。'
- en: As there's an edge from `squaring_node` to `summing_node`, `summing_node` takes
    as input this modified state returned by `squaring_node`.
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于从`squaring_node`到`summing_node`存在边，`summing_node`以`squaring_node`返回的修改后的状态作为输入。
- en: '`summing_node` adds up the numbers in `squares`, and saves the result as `answer`.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`summing_node`将`squares`中的数字相加，并将结果保存为`answer`。'
- en: '`summing_node` has an edge to `END`, which means the end of execution. The
    final state returned will contain `25` under `answer`.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`summing_node`有一个指向`END`的边，这意味着执行的结束。返回的最终状态将在`answer`下包含`25`。'
- en: Of course, this is a simple graph with only one path the execution can take.
    In a later part of this chapter, you'll encounter a graph with multiple paths—where
    a single node may branch into multiple nodes based on the state at that point.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这是一个只有一个执行路径的简单图。在本章的稍后部分，你将遇到一个有多个路径的图——在这个点上，一个节点可能根据该点的状态分支到多个节点。
- en: I hope this helped crystallize the concept of graphs and how LangGraph uses
    them to perform a task. It's now time to use what we've learned to start building
    our app.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 希望这有助于澄清图的概念以及LangGraph如何使用它们来执行任务。现在是时候利用我们所学的内容开始构建我们的应用了。
- en: 10.2.3 A one-node LLM graph
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.2.3 一个单节点LLM图
- en: The basic graph we built in the previous section had nothing to do with AI or
    LLMs. Indeed, you can use LangGraph to build anything you like, whether or not
    AI is involved, but in practice, the point of LangGraph is to make building AI
    applications easier.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在上一节中构建的基本图与AI或LLM无关。实际上，你可以使用LangGraph构建任何你想要的东西，无论是否涉及AI，但实践中，LangGraph的目的是使构建AI应用变得更加容易。
- en: Creating and running your first graph
  id: totrans-101
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 创建和运行你的第一个图
- en: In chapter nine, we encountered the OpenAI API's chat completions endpoint.
    In this endpoint, we pass a list of messages to the API, which predicts the next
    message in the conversation. In LangGraph, such an application could be represented
    by a simple one-node graph, as shown in figure 10.6.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在第九章中，我们遇到了 OpenAI API 的聊天完成端点。在这个端点中，我们向 API 传递一个消息列表，API 预测对话中的下一个消息。在 LangGraph
    中，这样的应用可以由一个简单的单节点图表示，如图 10.6 所示。
- en: '![image](../Images/10__image006.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![image](../Images/10__image006.png)'
- en: Figure 10.6 A basic single-node (apart from START and END) graph
  id: totrans-104
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 10.6 一个基本的单节点（除 START 和 END 外）图
- en: The state of the graph consists of a single variable, `messages`, which is—as
    you might expect—a list of messages.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图的状态由一个变量 `messages` 组成，正如你所预期的那样，它是一个消息列表。
- en: The only node in the graph, `assistant_node`, passes `messages` to an LLM, and
    returns the same list with an AI response message appended.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 图中的唯一节点 `assistant_node` 将 `messages` 传递给 LLM，并返回添加了 AI 响应消息的相同列表。
- en: Listing 10.1 shows this graph translated to real (non-Streamlit) Python code.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 10.1 显示了这个图被转换成实际的（非 Streamlit）Python 代码。
- en: Listing 10.1 graph_example.py
  id: totrans-108
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 10.1 graph_example.py
- en: '[PRE1]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: (`chapter_10/graph_example.py` in the GitHub repo)
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: （GitHub 仓库中的 `chapter_10/graph_example.py`）
- en: 'First, we initialize the LLM in this line:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们在这行初始化 LLM：
- en: '[PRE2]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This is similar to what we did in chapter 9 when we created an OpenAI API client.
    LangChain—a library closely related to LangGraph—provides a class called `ChatOpenAI`
    that does essentially the same thing but is slightly easier to use. As before,
    don't forget to replace `sk-proj...` with your actual OpenAI API key.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这与我们第九章创建 OpenAI API 客户端时所做的类似。LangChain——一个与 LangGraph 密切相关的库——提供了一个名为 `ChatOpenAI`
    的类，它基本上做的是同样的事情，但使用起来稍微简单一些。像之前一样，别忘了将 `sk-proj...` 替换为你的实际 OpenAI API 密钥。
- en: 'Consider the next part:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑下一部分：
- en: '[PRE3]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: As we've discussed, a graph has a state. For each graph you define, you specify
    what fields exist in the state by creating a class that contains those fields.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前讨论的，一个图有一个状态。对于你定义的每个图，你通过创建一个包含那些字段的类来指定状态中存在的字段。
- en: In the above two lines, we're creating `MyGraphState` to represent the state
    of the graph we're about to define. In line with the example in figure 10.6, `MyGraphState`
    contains one field—`messages`—which is a list of objects of the type `AnyMessage`.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述两行中，我们正在创建 `MyGraphState` 来表示我们即将定义的图的状态。按照图 10.6 中的示例，`MyGraphState` 包含一个字段——`messages`——它是一个
    `AnyMessage` 类型的对象列表。
- en: In chapter nine, we saw that each message in an (OpenAI) LLM conversation has
    a *role*—one of `"user"`, `"assistant"`, or `"system"`. In LangChain, the same
    concept is represented by the `AnyMessage` superclass. `HumanMessage`, `AIMessage`,
    and `SystemMessage` are subclasses that inherit functionality from `AnyMessage`,
    and correspond to the `"user"`, `"assistant"`, and `"system"` roles, respectively.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在第九章中，我们了解到在（OpenAI）LLM 对话中，每条消息都有一个*角色*——其中一个是 `"user"`、`"assistant"` 或 `"system"`。在
    LangChain 中，这个概念由 `AnyMessage` 超类表示。`HumanMessage`、`AIMessage` 和 `SystemMessage`
    是继承自 `AnyMessage` 的子类，分别对应 `"user"`、`"assistant"` 和 `"system"` 角色。
- en: '`MyGraphState` itself is a subclass of `TypedDict`, a specialized dictionary
    type from Python''s typing module that allows us to define a dictionary with a
    fixed set of keys and associated value types. Since it inherits all of TypedDict''s
    behaviors, we can treat instances of TypedDict—and therefore, MyGraphState— as
    regular dictionaries, using the same syntax for accessing its keys (i.e. the fields
    in the class) and values.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '`MyGraphState` 本身是 `TypedDict` 的子类，它是 Python 的 typing 模块中的一个专用字典类型，允许我们定义一个具有固定键集及其相关值类型的字典。由于它继承了
    TypedDict 的所有行为，我们可以将 TypedDict 的实例——以及因此 MyGraphState——视为常规字典，使用相同的语法来访问其键（即类中的字段）和值。'
- en: Note
  id: totrans-120
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: We don't technically *have* to use a `TypedDict` to represent the state of the
    graph. We could also have used a regular class, a dataclass, or a Pydantic `BaseModel,`
    which we used in chapter nine. We've introduced and used TypedDict here because
    it plays well with `MessagesState`, a built-in LangGraph class that we'll discuss
    shortly.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实际上并不*必须*使用 `TypedDict` 来表示图的状态。我们也可以使用常规类、dataclass 或 Pydantic 的 `BaseModel`，这是我们第九章中使用过的。我们在这里引入并使用
    TypedDict 是因为它与 `MessagesState`（我们将很快讨论的内置 LangGraph 类）配合得很好。
- en: The next line, `builder = StateGraph(MyGraphState)`, initializes the construction
    of our graph. Here we're telling LangGraph that we're building a `StateGraph`—the
    type of graph we've been talking about where the nodes read from and write into
    a shared state—whose state is represented by a `MyGraphState` instance (which,
    as we've seen, will have a `messages` list).
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个语句`builder = StateGraph(MyGraphState)`初始化了图的构建。在这里，我们告诉LangGraph我们正在构建一个`StateGraph`——我们一直在谈论的类型，其中节点从共享状态中读取和写入——其状态由一个`MyGraphState`实例表示（正如我们所见，它将有一个`messages`列表）。
- en: 'We now define the only node in our graph thus:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在这样定义我们图中的唯一节点：
- en: '[PRE4]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Each node in a LangGraph graph is a regular Python function that accepts the
    current state of the graph—a `MyGraphState` instance—as input, and returns the
    parts of the state it wants to modify.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: LangGraph图中的每个节点都是一个常规的Python函数，它接受图的当前状态——一个`MyGraphState`实例——作为输入，并返回它想要修改的状态部分。
- en: The `assistant_node` we've defined above is quite minimal; it simply passes
    the `messages` list—accessed using square brackets as `state["messages"]` just
    like in a regular dictionary—to the `invoke` method of `llm`, obtaining the AI's
    response message. It then modifies the `"messages"` key of the state, adding `ai_response_message`
    to the end, and returns the result.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义的`assistant_node`相当简单；它只是将`messages`列表（使用方括号作为`state["messages"]`访问，就像在常规字典中一样）传递给`llm`的`invoke`方法，以获取AI的响应消息。然后它修改状态中的`"messages"`键，将`ai_response_message`添加到末尾，并返回结果。
- en: Note
  id: totrans-127
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: In the above code, since `MyGraphState` only has a single key, `messages`, it
    seems that `assistant_node` is simply returning the entirety of the modified state.
    That's not strictly true—it's actually only returning the keys it wants to modify,
    leaving any other keys untouched. This will become clear in later sections.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码中，由于`MyGraphState`只有一个键，即`messages`，所以看起来`assistant_node`只是返回了整个修改后的状态。这并不完全正确——实际上，它只返回它想要修改的键，而其他键保持不变。这一点将在后面的章节中变得清晰。
- en: 'Now that we''ve created our only node, it''s time to put it in our graph:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经创建了唯一的节点，是时候将其放入我们的图中：
- en: '[PRE5]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The first line adds a node called `assistant` to the graph, pointing to the
    `assistant_node` function we just developed as the logic for the node.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 第一行向图中添加了一个名为`assistant`的节点，指向我们刚刚开发的`assistant_node`函数作为节点的逻辑。
- en: As mentioned earlier, each graph has dummy `START` and `END` nodes. The remaining
    two lines create directed edges from `START` to our `assistant` node, and from
    our `assistant` node to `END`, thus completing the graph.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，每个图都有一个虚拟的`START`和`END`节点。剩下的两行创建了从`START`到我们的`assistant`节点，以及从我们的`assistant`节点到`END`的有向边，从而完成了图的构建。
- en: The immediately following line, `graph = builder.compile()`, *compiles* the
    graph, readying it for execution.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 紧接着的行，`graph = builder.compile()`，*编译*了图，使其准备执行。
- en: 'The last few lines of code in the file show how a graph can be invoked:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 文件中的最后几行代码展示了如何调用一个图：
- en: '[PRE6]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: We first use the `input()` function—which prompts the user to enter something
    in the terminal—to collect the user's input message.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先使用`input()`函数——在终端中提示用户输入一些内容——来收集用户的输入消息。
- en: We then construct the starting state of the graph as a dictionary with the key
    `"messages."` The message itself is an instance of `HumanMessage` with its content
    attribute set to the `input_message` we just collected.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们然后以字典的形式构建图的起始状态，键为`"messages"`。消息本身是一个`HumanMessage`实例，其内容属性设置为刚刚收集到的`input_message`。
- en: Passing `initial_state` to the graph's `invoke` method finally causes the graph
    to execute, effectively passing our user input to the LLM through `assistant_node`,
    returning the final state.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 将`initial_state`传递给图的`invoke`方法最终导致图执行，有效地通过`assistant_node`将我们的用户输入传递给LLM，并返回最终状态。
- en: '`final_state` contains all of the messages in the conversation so far (our
    user message and the LLM''s response message), so we access the response message
    using `final_state["messages"][-1]` and print its content to the screen.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '`final_state`包含到目前为止对话中的所有消息（我们的用户消息和LLM的响应消息），因此我们使用`final_state["messages"][-1]`访问响应消息并将其内容打印到屏幕上。'
- en: 'To see this in action, copy all the code to a new file called `graph_example.py`,
    and run your code in the terminal using the `python` command like this:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 要看到这个功能如何工作，将所有代码复制到一个名为`graph_example.py`的新文件中，然后在终端中使用`python`命令运行你的代码，如下所示：
- en: '[PRE7]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Enter a message when you see the `"Talk to the bot"` prompt. As an example
    output, here''s what I got:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 当你看到`"Talk to the bot"`提示时，输入一条消息。以下是一个示例输出：
- en: '[PRE8]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: It looks like AI stole my dream haiku gig. Maybe I'll pivot to the performing
    arts—everyone loves a good mime.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来AI偷走了我的俳句梦想。也许我会转向表演艺术——每个人都喜欢一个好的默剧演员。
- en: Turning our graph into a class
  id: totrans-145
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 将我们的图转换为类
- en: We've run our first graph in the terminal, but what we really want is to use
    it to power our customer support bot. We'll organize our code using object-oriented
    principles as we did in the last two chapters.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在终端运行了我们的第一个图，但我们真正想要的是用它来为我们的客户支持机器人提供动力。我们将像在前两章中做的那样，使用面向对象的原则来组织我们的代码。
- en: Let's start by converting the code we wrote in the prior section into a `SupportAgentGraph`
    class in `graph.py`, shown in listing 10.2.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先从将上一节中编写的代码转换为`graph.py`中的`SupportAgentGraph`类开始，如列表10.2所示。
- en: Listing 10.2 graph.py
  id: totrans-148
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表10.2 graph.py
- en: '[PRE9]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: (`chapter_10/in_progress_01/graph.py` in the GitHub repo)
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: （GitHub仓库中的`chapter_10/in_progress_01/graph.py`）
- en: The code here is very similar to that in `graph_example.py`, but I want to highlight
    a few differences.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的代码与`graph_example.py`中的代码非常相似，但我想要强调一些差异。
- en: Most obviously, we're encapsulating our graph in a class—`SupportAgentGraph`—which
    has a method for building the actual graph (`build_graph`) and another (`invoke`)
    for invoking it by passing a human (user) message.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 最明显的是，我们将我们的图封装在一个类中——`SupportAgentGraph`——它有一个用于构建实际图的方法（`build_graph`）和另一个方法（`invoke`），用于通过传递人类（用户）消息来调用它。
- en: Rather than creating the LLM object within the class, we accept it as a parameter
    in `SupportAgentGraph`'s `__init__`, which builds the graph by calling `self.build_graph()`
    and saves it under `self.graph` for future invocations.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不是在类中创建LLM对象，而是在`SupportAgentGraph`的`__init__`中将其作为参数接受，通过调用`self.build_graph()`构建图，并将其保存为`self.graph`以供将来调用。
- en: You'll notice that our `MyGraphState` class, which we defined earlier, is nowhere
    to be found. We've swapped it out for `MessagesState`, a built-in LangGraph class
    that does more or less the same thing. `MessagesState`, like `MyGraphState`, has
    a `messages` field, which is a list of `AnyMessage` objects. The big difference
    between `MyGraphState` and `MessagesState` is how the `messages` field can be
    modified in a node—more on that in a second.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到我们之前定义的`MyGraphState`类，现在无处可寻。我们已经将其替换为`MessagesState`，这是一个内置的LangGraph类，它或多或少能做同样的事情。`MessagesState`，就像`MyGraphState`一样，有一个`messages`字段，它是一个`AnyMessage`对象的列表。`MyGraphState`和`MessagesState`之间最大的区别在于如何在节点中修改`messages`字段——关于这一点，我们稍后再谈。
- en: 'Next, consider the `get_assistant_node` method:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，考虑`get_assistant_node`方法：
- en: '[PRE10]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: This method has a function definition—for `assistant_node`, which we encountered
    in the previous section—nested under it. It seems to do nothing other than return
    the function. What's that about?
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方法有一个函数定义——为`assistant_node`，我们在上一节中遇到了它——嵌套在其中。它似乎除了返回函数之外什么也没做。这是怎么回事？
- en: Well, since `assistant_node` needs to access the LLM object (`self.llm`), its
    code must live inside a method of the `SupportAgentGraph` class. But `assistant_node`
    can't *itself* be a method of the class, because the first argument passed to
    a method is `self`—the current instance of the class—while the first (and only)
    argument passed to a valid LangGraph node must be the graph state.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，既然`assistant_node`需要访问LLM对象（`self.llm`），其代码必须位于`SupportAgentGraph`类的某个方法内部。但是`assistant_node`不能*自身*成为类的一个方法，因为传递给方法的第一参数是`self`——类的当前实例——而传递给有效LangGraph节点的第一个（也是唯一一个）参数必须是图状态。
- en: So instead, we define `assistant_node` as an inner function within an outer
    method called `get_assistant_node`—taking advantage of the outer method's scope
    to access `self.llm` within the inner function—and have the outer method *return*
    the inner function so we can plug it into the graph. This programming pattern
    is called a *closure* since the inner function retains access to variables from
    its *enclosing* scope, even after the outer function has returned.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们定义`assistant_node`为外层方法`get_assistant_node`内部的内联函数——利用外层方法的范围在内部函数中访问`self.llm`——并且让外层方法*返回*内部函数，这样我们就可以将其插入到图中。这种编程模式被称为*闭包*，因为内部函数保留了对其*封装*范围中变量的访问，即使外层函数已经返回。
- en: 'The aforementioned plugging-in of the node happens in the `build_graph` method
    in this line:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 上述节点的插入发生在`build_graph`方法的这一行：
- en: '[PRE11]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Since `get_assistant_node()` *returns* the `assistant_node` function (as opposed
    to calling it), we can use the call to `get_assistant_node` to refer to the inner
    function.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 由于`get_assistant_node()`*返回*`assistant_node`函数（而不是调用它），我们可以使用对`get_assistant_node`的调用来引用内部函数。
- en: 'The `assistant_node` function differs from the one of the same name we defined
    in the prior section in one important way. Consider the return statement, which
    has changed from:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '`assistant_node`函数与我们在上一节中定义的同名函数在一点上有所不同。考虑返回语句，它已经从：'
- en: '[PRE12]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'to:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 到：
- en: '[PRE13]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Why do we not return the other items in the `messages` list anymore? The answer
    has to do with our having replaced `MyGraphState` with `MessagesState`. You see,
    each node in LangGraph's `StateGraph` receives the complete state as input, but
    the value it returns is treated as a set of *updates* to each key in the state.
    How exactly these updates are merged with the existing values depends on how we've
    specified it in our state type.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么我们不再返回`messages`列表中的其他项了？答案与我们将`MyGraphState`替换为`MessagesState`有关。您可以看到，LangGraph的`StateGraph`中的每个节点都接收完整的状态作为输入，但返回的值被视为状态中每个键的*更新*集合。这些更新如何与现有值合并取决于我们在状态类型中如何指定。
- en: In `MyGraphState`, we didn't mention any particular way of handling this, so
    the value associated with the key `messages` is simply replaced by whatever the
    node returns for that key. This is why we needed to return the entire list—because
    we would have lost the earlier messages otherwise.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在`MyGraphState`中，我们没有提到任何特定的处理方式，因此与键`messages`关联的值简单地被节点返回的该键的值所替换。这就是为什么我们需要返回整个列表——否则我们就会丢失早期的消息。
- en: On the other hand, `MessagesState` internally specifies that the value returned
    by a node should be appended to the existing list. So, `ai_response_message` is
    simply tacked on to the existing messages, and we don't have to return the older
    messages separately.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，`MessagesState`内部指定节点返回的值应该追加到现有列表中。因此，`ai_response_message`简单地附加到现有消息上，我们不需要单独返回旧消息。
- en: Note
  id: totrans-170
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: '`MessagesState` implements this append feature through a function called `add_messages`.
    In fact, the only difference between `MyGraphState` and `MessagesState` is that
    the `messages` field in `MessagesState` is defined (internally) like this:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '`MessagesState`通过一个名为`add_messages`的函数实现了这个追加功能。实际上，`MyGraphState`和`MessagesState`之间的唯一区别在于`MessagesState`中的`messages`字段（内部）定义如下：'
- en: '[PRE14]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: I won't go into this in detail, but this is essentially saying that when updates
    occur, they should be handled by the `add_messages` function rather than a simple
    replacement.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 我不会详细说明这一点，但本质上这是说，当发生更新时，应该由`add_messages`函数而不是简单的替换来处理。
- en: Whew! That was a lot of explanation, but hopefully, you now understand how graphs
    are modeled in LangGraph.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 呼吁！这有很多解释，但希望你现在已经理解了在LangGraph中如何建模图。
- en: The Bot class
  id: totrans-175
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Bot类
- en: Let's set aside `SupportAgentGraph` now and pivot to our main backend class,
    which we'll call `Bot`. `Bot` will be the single point of entry to the backend
    for our Streamlit frontend, similar to the `Game` and `Hub` classes in earlier
    chapters.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们暂时放下`SupportAgentGraph`，转向我们的主要后端类，我们将称之为`Bot`。`Bot`将是Streamlit前端进入后端的单一入口点，类似于前几章中的`Game`和`Hub`类。
- en: Importantly, `Bot` will supply the LLM object that `SupportAgentGraph` needs
    and provide a user-friendly method that our frontend can call to chat with the
    bot.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是，`Bot`将提供`SupportAgentGraph`需要的LLM对象，并提供一个用户友好的方法，我们的前端可以调用该方法与机器人聊天。
- en: To create it, copy the code in listing 10.3 to a new file, `bot.py`.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建它，将列表10.3中的代码复制到一个新文件中，命名为`bot.py`。
- en: Listing 10.3 bot.py
  id: totrans-179
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表10.3 bot.py
- en: '[PRE15]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: (`chapter_10/in_progress_01/bot.py` in the GitHub repo)
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: （位于GitHub仓库的`chapter_10/in_progress_01/bot.py`）
- en: Luckily, the `Bot` class is a lot more straightforward than `SupportAgentGraph`.
    `__init__` accepts a dictionary of API keys—which, spoiler alert, we'll supply
    through `st.secrets` again—before setting up the LLM object with a call to the
    `get_llm` method, and passing it to the `SupportAgentGraph` instance, saved to
    `self.graph`.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，`Bot`类比`SupportAgentGraph`简单得多。`__init__`接受一个API密钥字典——提示一下，我们还将通过`st.secrets`再次提供——在通过调用`get_llm`方法设置LLM对象并将它传递给保存到`self.graph`的`SupportAgentGraph`实例之前。
- en: '`get_llm` simply uses LangChain''s `ChatOpenAI` class to create the LLM object
    as discussed earlier. Notice that we''ve added a new parameter called `max_tokens`.
    As you may remember from the previous chapter, tokens are the basic units of text
    that language models process. By setting `max_tokens=2000`, we''re telling OpenAI''s
    API to limit responses to a maximum of 2000 tokens (about 1,500 words), which
    helps in both cost reduction and keeping responses (relatively) concise.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '`get_llm`简单地使用LangChain的`ChatOpenAI`类来创建LLM对象，正如之前讨论的那样。注意，我们添加了一个名为`max_tokens`的新参数。你可能还记得，从前一章中，token是语言模型处理文本的基本单位。通过设置`max_tokens=2000`，我们告诉OpenAI的API将响应限制在最多2000个token（大约1,500个单词），这有助于成本降低并保持响应（相对）简洁。'
- en: The `chat` method abstracts away the complexity of dealing with graphs and states.
    It has a simple contract—put a human message string in, and get an AI response
    string out. It fulfills this promise by calling the `invoke` method of our `SupportAgentGraph`'s
    instance, and returning the content of the last message—which happens to be the
    AI message, as we saw earlier.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '`chat`方法抽象掉了处理图和状态复杂性的问题。它有一个简单的契约——放入一个人类消息字符串，并得到一个AI响应字符串。它通过调用我们的`SupportAgentGraph`实例的`invoke`方法，并返回最后一条消息的内容——碰巧是AI消息，正如我们之前看到的。'
- en: A chatbot frontend in Streamlit
  id: totrans-185
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Streamlit中的聊天机器人前端
- en: Our app's backend is now ready, so let's focus on the frontend next. Streamlit
    really shines when it comes to chatbot interfaces because of its native support
    for them.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的应用程序后端现在已准备就绪，所以让我们接下来关注前端。Streamlit在聊天机器人界面方面非常出色，因为它对其有原生支持。
- en: This is evident in the fact that our first iteration of `frontend.py`—shown
    in listing 10.4—is only 12 lines long.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 这一点在事实中很明显，即我们的`frontend.py`的第一个迭代版本——如列表10.4所示——只有12行长。
- en: Listing 10.4 frontend.py
  id: totrans-188
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表10.4 frontend.py
- en: '[PRE16]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: (`chapter_10/in_progress_01/frontend.py` in the GitHub repo)
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: （GitHub仓库中的`chapter_10/in_progress_01/frontend.py`）
- en: We start by putting a reference to our `Bot` instance—`bot`—in `st.session_state`,
    which is essentially the same pattern we've used in the last two chapters for
    the `Hub` and `Game` classes. To do so, we pass in the `api_keys` object from
    `st.secrets` to do so. We'll create `secrets.toml` in a bit.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先在`st.session_state`中放入对`Bot`实例的引用——`bot`——这基本上是我们在前两章中用于`Hub`和`Game`类的相同模式。为此，我们传递`st.secrets`中的`api_keys`对象。我们稍后会创建`secrets.toml`。
- en: 'The interesting part is the last four lines. The first of these introduces
    a new Streamlit widget called `st.chat_input`:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 最有趣的部分是最后四行。其中第一行引入了一个新的Streamlit小部件，称为`st.chat_input`：
- en: '[PRE17]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '`st.chat_input` creates a text input box with a "Send" icon, similar to what
    you''re probably used to in various messaging apps. Besides the "Send" icon, it''s
    different from `st.text_input` in a few noticeable ways:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '`st.chat_input`创建了一个带有“发送”图标的文本输入框，这可能与你在各种消息应用中可能习惯的类似。除了“发送”图标之外，它在几个明显的方面与`st.text_input`不同：'
- en: It's pinned to the *bottom* of the screen or the containing widget you put it
    in
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它被固定在屏幕的**底部**或者你放入其中的包含小部件里
- en: Unlike `st.text_input,` which returns a value once a user clicks out of the
    textbox, `st.chat_input` only returns a value once the user has clicked "Send"
    or pressed Enter.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与`st.text_input`不同，后者在用户点击出文本框时返回一个值，`st.chat_input`只有在用户点击“发送”或按Enter键时才返回一个值。
- en: Apart from `st.chat_input`, the code above may look unfamiliar for another reason;
    we're using the character sequence `:=`, which is called a *walrus operator* in
    Python (because if you tilt your head to the side, it kind of looks like a walrus).
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 除了`st.chat_input`之外，上面的代码可能还因为另一个原因看起来不熟悉；我们使用了字符序列`:=`，这在Python中被称为*walrus运算符*（因为如果你把头歪向一边，它看起来有点像海象）。
- en: 'The walrus operator is just a trick to make your code slightly more concise.
    It allows you to assign values to variables as part of a larger expression rather
    than requiring a separate line for assignment. In other words, instead of the
    line we''re discussing, we could have written the following to obtain the same
    effect:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: Walrus运算符只是一个使你的代码稍微更简洁的技巧。它允许你将值分配给变量作为更大表达式的部分，而不是需要单独一行进行赋值。换句话说，我们不需要讨论的行，我们可以写出以下内容来获得相同的效果：
- en: '[PRE18]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Note
  id: totrans-200
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: Python developers are divided on whether the walrus operator increases or decreases
    the readability of your code. Regardless of whether you choose to use it, it's
    a good idea to know what it is.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: Python开发者对于walrus运算符是否增加了或减少了代码的可读性意见不一。无论你是否选择使用它，了解它是什么都是一个好主意。
- en: 'Once we have an input message from the user, we can display the conversation:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们收到用户的输入消息，我们就可以显示对话：
- en: '[PRE19]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '`st.chat_message` is a Streamlit display widget that accepts either of two
    strings—`"human"` or `"ai"`—and styles the container accordingly. This includes
    showing an avatar corresponding to a user or a robot.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '`st.chat_message` 是一个 Streamlit 显示小部件，它接受两个字符串之一——“human”或“ai”，并相应地设置容器样式。这包括显示与用户或机器人对应的头像。'
- en: In this case, we display `human_message_text` using `st.chat_message("human")`,
    call the `chat` method of our Bot instance, and display the response AI text with
    `st.chat_message("ai")`.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们使用 `st.chat_message("human")` 显示 `human_message_text`，调用我们的 Bot 实例的
    `chat` 方法，并使用 `st.chat_message("ai")` 显示 AI 响应文本。
- en: 'Just to be clear, `st.chat_message` is similar to other Streamlit elements
    like `st.column` in that we could also have written:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 为了清楚起见，`st.chat_message` 与其他 Streamlit 元素（如 `st.column`）类似，我们也可以这样写：
- en: '[PRE20]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: To complete the first version of Nibby, we need to create a `secrets.toml` file
    for our OpenAI API key, in a new `.streamlit` folder. The contents of this file
    are in listing 10.5.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 要完成 Nibby 的第一个版本，我们需要在新的 `.streamlit` 文件夹中创建一个 `secrets.toml` 文件来存储我们的 OpenAI
    API 密钥。此文件的 内容如列表 10.5 所示。
- en: Listing 10.5 .streamlit/secrets.toml
  id: totrans-209
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 10.5 .streamlit/secrets.toml
- en: '[PRE21]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '#A Replace sk-proj-... with your actual OpenAI API key.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '#A 将 sk-proj-... 替换为您的实际 OpenAI API 密钥。'
- en: Go ahead and run your app with `streamlit run frontend.py` to test it out. Figure
    10.7 shows our chatbot in action.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 运行您的应用程序以 `streamlit run frontend.py` 测试它。图 10.7 展示了我们的聊天机器人正在运行。
- en: '![image](../Images/10__image007.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![image](../Images/10__image007.png)'
- en: Figure 10.7 A single-prompt-single-response chatbot in Streamlit (see chapter_10/in_progress_01
    in the GitHub repo for the full code).
  id: totrans-214
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 10.7 Streamlit 中的单提示单响应聊天机器人（完整的代码在 GitHub 仓库的 chapter_10/in_progress_01 中）。
- en: Sweet! Notice the human and bot avatars, as well as the subtle background shading
    to distinguish between the two kinds of displayed messages.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了！注意人类和机器人的头像，以及微妙的背景阴影，以区分两种显示的消息类型。
- en: If you play around with the app, you'll realize that Nibby can't hold a conversation
    at this point, only respond to single messages. Next up, let's fix that!
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你玩弄这个应用程序，你会意识到 Nibby 目前还不能进行对话，只能对单条消息做出响应。接下来，让我们修复这个问题！
- en: 10.3 Multi-turn conversations
  id: totrans-217
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.3 多轮对话
- en: It took some effort to get there, but we've built an initial version of Nibby.
    Unfortunately, at the moment, Nibby's idea of a conversation is a single response
    to a single message.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然有些努力，但我们已经构建了 Nibby 的初始版本。不幸的是，目前 Nibby 对对话的理解是单条消息的单个响应。
- en: For instance, consider the exchange in figure 10.8
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑图 10.8 中的交换。
- en: '![image](../Images/10__image008.png)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![image](../Images/10__image008.png)'
- en: Figure 10.8 Our chatbot doesn't remember the information we gave it.
  id: totrans-221
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 10.8 我们的聊天机器人没有记住我们给它提供的信息。
- en: 'There are two things wrong here:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有两个问题：
- en: The bot didn't remember the information I gave it in the prior message.
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器人没有记住我在上一条消息中给它提供的信息。
- en: Our frontend treats the second message-response pair as a completely new conversation,
    removing all traces of the first.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们的前端将第二个消息-响应对视为一个全新的对话，删除了第一个的所有痕迹。
- en: In this section, we'll iterate on Nibby, solving both issues.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将对 Nibby 进行迭代，解决这两个问题。
- en: 10.3.1 Adding memory to our graph
  id: totrans-226
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.3.1 为我们的图添加内存
- en: 'Recall the simple one-node graph we created: it starts with a state containing
    a human message, passes this message to an LLM, appends the AI''s response to
    the state, and then returns the updated state.'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下我们创建的简单单节点图：它从一个包含人类消息的状态开始，将此消息传递给 LLM，将 AI 的响应追加到状态中，然后返回更新后的状态。
- en: What happens if you invoke the graph again with a follow-up message? Well, the
    process repeats—a *new* state containing *only* the follow-up message is created
    and passed to the graph, which treats this as a brand-new independent execution.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你再次调用图并带有后续消息会发生什么？嗯，过程会重复——创建一个新的状态，只包含后续消息，并将其传递给图，图将其视为全新的独立执行。
- en: This is a clear problem since conversations very rarely consist exclusively
    of a single message and response. The user *will* want to follow up, and the chatbot
    needs to remember what came before.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个明显的问题，因为对话很少只由一条消息和响应组成。用户*会*想要跟进，聊天机器人需要记住之前的内容。
- en: To enable our graph to remember prior executions, we need to *persist* the state,
    rather than starting from scratch every time. Luckily, LangGraph makes this a
    snap through the concept of *checkpointers*, which can save the state of the graph
    at each step.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使我们的图能够记住之前的执行，我们需要*持久化*状态，而不是每次都从头开始。幸运的是，LangGraph 通过*检查点器*的概念使这一点变得简单，它可以在每个步骤中保存图的状态。
- en: Specifically, we're going to use a checkpointer to allow our graph state to
    be stored in memory. We can then assign a *thread ID* to each invocation of the
    graph. Whenever we pass the same thread ID while invoking the graph, the graph
    will recall the state previously stored in memory for that thread ID and start
    from *there* rather than from a clean slate.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，我们将使用检查点器来允许我们的图状态存储在内存中。然后我们可以为图的每次调用分配一个*thread ID*。每次我们调用图时传递相同的线程ID，图将回忆之前为该线程ID存储在内存中的状态，并从*那里*开始，而不是从一张白纸开始。
- en: 'To implement this, make the changes shown below to `graph.py`:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一点，对`graph.py`进行以下更改：
- en: '[PRE22]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: (`chapter_10/in_progress_02/graph.py` in the GitHub repo)
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: （位于GitHub仓库的`chapter_10/in_progress_02/graph.py`）
- en: 'Let''s start our discussion of the code above with `build_graph`. We''ve added
    a line at the top of this method:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从上面的代码开始讨论`build_graph`。我们在该方法的顶部添加了一行：
- en: '[PRE23]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '`MemorySaver` is a checkpointer built into LangGraph that can store graph states
    in memory. Various other kinds of checkpointers are available, depending on where
    you want to save your graph state. For instance, you could use different checkpointers
    to store conversations in a database like PostgreSQL or SQLite.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '`MemorySaver`是LangGraph内置的检查点器，可以存储图的状态。根据您希望将图状态保存的位置，还有各种其他类型的检查点器可用。例如，您可以使用不同的检查点器将对话存储在数据库中，如PostgreSQL或SQLite。'
- en: 'We pass this to our graph when we compile it at the end of the method:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在方法末尾编译我们的图时，我们将这个值传递给它：
- en: '[PRE24]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: This allows our graph to save its state, but that's not enough. If we don't
    make any more changes, each graph invocation would still be a new, independent
    one. We need a way to tell the graph that a particular invocation belongs to a
    *thread* it has seen before.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 这允许我们的图保存其状态，但这还不够。如果我们不再进行任何更改，每次图调用仍然是一个新的、独立的调用。我们需要一种方法来告诉图，特定的调用属于它之前见过的某个`*thread*`。
- en: 'Direct your attention to `__init__`, where we''ve assigned a strange-looking
    value to a field called `self.config`:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 将你的注意力转向`__init__`方法，我们在一个名为`self.config`的字段中分配了一个看起来很奇怪的值：
- en: '[PRE25]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The important part to notice here is `{"thread_id": "1"}`. Further down, in
    the `invoke` method, we pass this to the graph while invoking it:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '这里需要注意的重要部分是`{"thread_id": "1"}`。在下面的`invoke`方法中，我们在调用图时传递这个值：'
- en: '[PRE26]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: We're essentially passing the thread ID `1` to the graph here so it knows that
    every time we invoke it, we're always in the same conversation thread which has
    an ID of `1`.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实际上是将线程ID `1`传递给图，这样它就知道每次我们调用它时，我们总是在同一个会话线程中，该线程的ID为`1`。
- en: 'As a result of this change, the first invocation of the graph (`"Hi, my name''s
    Bob"` in the example that prompted these changes) will be saved under the thread
    ID `1`. At this point, the state will have two messages: the original human message
    and the AI response.'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这个更改，图的第一次调用（例如，在引发这些更改的示例中`"Hi, my name's Bob"`）将保存在线程ID `1`下。此时，状态将包含两条消息：原始的人类消息和AI的回复。
- en: When the follow-up message (`"What's my name"`) arrives, since we already have
    an existing thread with ID `1`, it will be *appended* to the existing state. The
    state that's passed to `assistant_node` (and therefore to the LLM) will have *three*
    messages, enabling it to respond correctly.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 当后续消息（`"What's my name"`）到达时，因为我们已经有一个ID为`1`的现有线程，它将被*附加*到现有状态。传递给`assistant_node`（因此是LLM）的状态将包含*三条*消息，使其能够正确响应。
- en: Additional questions you may have
  id: totrans-248
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 您可能还有其他问题
- en: 'Two natural questions may arise at this juncture:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段可能会出现两个自然的问题：
- en: Why is the thread ID always 1?
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么线程ID总是1？
- en: Recall that our Streamlit app session doesn't persist beyond a single browser
    refresh. So each time the user accesses the app by opening it in a new tab or
    refreshing the browser, the `SupportAgentGraph` instance is rebuilt, and the graph
    is re-compiled with a new `MemorySaver` object. Since `MemorySaver` stores graph
    states in memory instead of persisting it to an external data store like PostgreSQL,
    any thread from a different browser session—whatever the thread ID—is inaccessible,
    so we can safely use the same thread ID `1` for the new session.
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回想一下，我们的Streamlit应用会话不会在单个浏览器刷新后持久化。因此，每次用户通过在新标签页中打开它或刷新浏览器来访问应用时，`SupportAgentGraph`实例都会重新构建，并且图会使用新的`MemorySaver`对象重新编译。由于`MemorySaver`在内存中存储图状态而不是将其持久化到外部数据存储（如PostgreSQL），因此来自不同浏览器会话的任何线程（无论线程ID是多少）都是不可访问的，因此我们可以安全地使用相同的线程ID
    `1`来处理新会话。
- en: Long story short, how we've set things up guarantees that a single graph instance
    will see at most one conversation in its lifetime, so we only need to specify
    one thread ID.
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 简而言之，我们设置的方式保证了单个图实例在其生命周期中最多只能看到一次对话，因此我们只需要指定一个线程ID。
- en: Why is the value of self.config so convoluted?
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么self.config的值如此复杂？
- en: 'Looking at our explanation of checkpointers and memory, it seems that all we
    need to pass the graph when we invoke it is the value `1`. So why do we have this
    monstrosity: `{"configurable": {"thread_id": "1"}}`?'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '看看我们对检查点和内存的解释，似乎我们在调用图时只需要传递值`1`。那么为什么我们有这个怪物：`{"configurable": {"thread_id":
    "1"}}`？'
- en: Though they are beyond the scope of this book, LangGraph offers many options
    when you invoke a graph, such as the ability to specify metadata or the number
    of parallel calls it can make. The thread ID is the only configuration we're using
    here, but it's far from the only one available. The convoluted-seeming structure
    of self.config reflects this.
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 虽然它们超出了本书的范围，但LangGraph在调用图时提供了许多选项，例如指定元数据或它能够进行的并行调用数量。线程ID是我们在这里使用的唯一配置，但它远非唯一可用的配置。self.config看似复杂的结构反映了这一点。
- en: Try running Nibby again and entering the same messages as before. This time
    you should see something similar to figure 10.9.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 再次运行Nibby并输入之前相同的消息。这次你应该会看到类似于图10.9的内容。
- en: '![image](../Images/10__image009.png)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
  zh: '![image](../Images/10__image009.png)'
- en: Figure 10.9 Nibby now remembers information we told it earlier in the conversation
    (see chapter_10/in_progress_02 in the GitHub repo for the full code).
  id: totrans-258
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.9 Nibby现在记得我们在对话中之前告诉它的信息（请参阅GitHub仓库中的chapter_10/in_progress_02以获取完整代码）。
- en: As you can see, the app does remember the previous information we gave it this
    time, but we still need to update the frontend to show the entire conversation.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，应用程序这次确实记得我们之前给出的信息，但我们仍然需要更新前端以显示整个对话。
- en: 10.3.2 Displaying the conversation history
  id: totrans-260
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.3.2 显示对话历史
- en: Our Streamlit frontend is currently only set up to show the latest user-entered
    message and the AI's response.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 我们当前的Streamlit前端仅设置用来显示最新的用户输入的消息和AI的响应。
- en: 'To display the full history, we need to first expose it in the backend. Let''s
    start by adding a method to `graph.py` to get the entire conversation so far at
    any point:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 要显示完整的历史记录，我们首先需要在后端公开它。让我们首先在`graph.py`中添加一个方法来获取到目前为止的整个对话：
- en: '[PRE27]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: (`chapter_10/in_progress_03/graph.py` in the GitHub repo)
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: （GitHub仓库中的`chapter_10/in_progress_03/graph.py`）
- en: The `get_conversation` method in `SupportAgentGraph` simply returns the `messages`
    list in the graph's current state.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '`SupportAgentGraph`中的`get_conversation`方法简单地返回图中当前状态的`messages`列表。'
- en: To do this, it first gets a reference to the state (`self.graph.get_state(self.config)`),
    and then accesses the `"messages"` key using `state.values["messages"]`. Passing
    `self.config` to `get_state` is required to get us the correct conversation thread,
    though—as the sidebar in the previous section discusses—there's only one.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 要做到这一点，它首先获取对状态的引用（`self.graph.get_state(self.config)`），然后使用`state.values["messages"]`访问`"messages"`键。将`self.config`传递给`get_state`是必要的，以获取正确的对话线程，尽管——如前一小节中的侧边栏所讨论的——只有一个。
- en: 'Next, let''s expose the full `messages` list in `bot.py`:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们在`bot.py`中公开完整的`messages`列表：
- en: '[PRE28]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: (`chapter_10/in_progress_03/bot.py` in the GitHub repo)
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: （GitHub仓库中的`chapter_10/in_progress_03/bot.py`）
- en: All the `get_history` method does is to pass the result of the `get_conversation`
    method we just defined faithfully through to its caller.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '`get_history`方法所做的只是将我们刚刚定义的`get_conversation`方法的输出忠实地传递给其调用者。'
- en: '[PRE29]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: (`chapter_10/in_progress_03/frontend.py` in the GitHub repo)
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: （GitHub仓库中的`chapter_10/in_progress_03/frontend.py`）
- en: We call `bot.get_history()` to get the list of messages and iterate through
    it, displaying each in its own `st.chat_message` container.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 我们调用`bot.get_history()`来获取消息列表，并遍历它，在每个自己的`st.chat_message`容器中显示每条消息。
- en: Recall that each message in the `messages` list is an instance of either `HumanMessage`
    or `AIMessage`. Either way, it also has a type field with a value of `"human"`
    in the case of `HumanMessage` and `"ai"` for an `AIMessage`. This therefore works
    perfectly as the type indicator argument in `st.chat_message`.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，`messages`列表中的每个消息都是`HumanMessage`或`AIMessage`的实例。无论如何，它都有一个类型字段，对于`HumanMessage`，其值为`"human"`，对于`AIMessage`，其值为`"ai"`。因此，它完美地作为`st.chat_message`中的类型指示符参数使用。
- en: '`message.content` has the message''s text, so we display that using `st.markdown`.'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '`message.content`包含消息的文本，所以我们使用`st.markdown`来显示它。'
- en: Rerun the app to see figure 10.10.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 重新运行应用程序以查看图10.10。
- en: '![image](../Images/10__image010.png)'
  id: totrans-277
  prefs: []
  type: TYPE_IMG
  zh: '![image](../Images/10__image010.png)'
- en: Figure 10.10 Our frontend now displays the full conversation history (see chapter_10/in_progress_03
    in the GitHub repo for the full code)
  id: totrans-278
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.10 我们的前端现在显示完整的对话历史（完整代码请参阅GitHub仓库中的 chapter_10/in_progress_03）
- en: As expected, Nibby now shows the full conversation so we can keep track of what's
    going on.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期，Nibby现在显示了完整的对话，这样我们就可以跟踪正在发生的事情。
- en: 10.4 Restricting our bot to customer support
  id: totrans-280
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.4 限制我们的机器人仅用于客户支持
- en: Thus far, we've focused on getting Nibby's basic functionality right—including
    calling an LLM and handling a full conversation. The result is a *general* chatbot
    you can ask for pretty much anything.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直专注于确保Nibby的基本功能正确——包括调用LLM和处理完整的对话。结果是你可以询问几乎任何问题的*通用聊天机器人*。
- en: 'For example, consider what happens if we ask Nibby to sing a song (figure 10.11):'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑如果我们要求Nibby唱一首歌会发生什么（图10.11）：
- en: '![image](../Images/10__image011.png)'
  id: totrans-283
  prefs: []
  type: TYPE_IMG
  zh: '![image](../Images/10__image011.png)'
- en: Figure 10.11 Nibby entertains frivolous requests, potentially costing us money.
  id: totrans-284
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.11 Nibby迎合无聊的请求，可能会给我们带来费用。
- en: Nibby can sing a song all right. It can also help you solve math problems or
    write an essay about the fall of the Roman Empire. Unfortunately, it does all
    of that on the company's dime. Remember, interacting with a cloud-based LLM costs
    *money*.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: Nibby确实可以唱歌。它还可以帮助你解决数学问题或撰写关于罗马帝国衰亡的论文。不幸的是，它所有这些都由公司承担费用。记住，与基于云的LLM交互是*需要付费的*。
- en: Each time someone makes a frivolous request to your customer support bot, and
    the bot indulges the request with a long-winded response, it spends precious LLM
    tokens and costs you something. Sure, each message is only a fraction of a cent
    but add up all the pleas for coding assistance or role-playing as a character
    from Battlestar Galactica, and suddenly your boss wants to know why there's a
    Nibby-shaped hole in the company's quarterly earnings report.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 每当有人对你的客户支持机器人提出无聊的请求，并且机器人用冗长的回复来迎合这个请求时，它就会消耗宝贵的LLM令牌，并给你带来一些费用。当然，每条消息只占几分之一的美分，但如果你把所有请求编码帮助或扮演《星际迷航》中角色的请求加起来，突然间你的老板想知道为什么季度收益报告中有一个像Nibby一样的洞。
- en: 'Of course, I''m being hyperbolic, but the point stands: we want Nibby to be
    strictly business.'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我在这里是在夸张，但观点是：我们希望Nibby严格专注于业务。
- en: The best thing about an LLM is that you can simply tell it what you want it
    to do or not do, so this turns out to be an easy fix; we'll simply add an appropriate
    *prompt*.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: LLM最好的地方在于你可以简单地告诉它它应该做什么或不做什么，所以这实际上是一个简单的修复；我们将简单地添加一个合适的*提示*。
- en: 10.4.1 Creating a base prompt
  id: totrans-289
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.4.1 创建基本提示
- en: As we did in chapter nine, we want to give the LLM some context about the use
    case we want it to serve. In that chapter, we did this by creating a message with
    the role "`system`." That's essentially what we're going to do here, too, though
    the abstractions we'll use are slightly different.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在第九章所做的那样，我们希望给LLM一些关于我们希望它服务的用例的背景信息。在那个章节中，我们通过创建一个带有角色"`system`"的消息来实现这一点。这里我们也将做同样的事情，尽管我们将使用的抽象略有不同。
- en: Create a file called `prompts.py` with the content shown in listing 10.6.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个名为`prompts.py`的文件，内容如列表10.6所示。
- en: Listing 10.6 prompts.py
  id: totrans-292
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表10.6 prompts.py
- en: '[PRE30]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: (`chapter_10/in_progress_04/prompts.py` in the GitHub repo)
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: (`chapter_10/in_progress_04/prompts.py` 在GitHub仓库中)
- en: The prompt gives Nibby its first indication that Note n' Nib exists and that
    it's supposed to be providing customer support for the company.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 这个提示给Nibby第一个暗示，Note n' Nib存在，并且它应该为公司提供客户支持。
- en: Importantly, `BASE_SYS_MSG` also has an instruction to refuse any requests that
    are unrelated to Note n' Nib. Next, let's incorporate this into our graph.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是，`BASE_SYS_MSG`还有一个指令拒绝与Note n' Nib无关的任何请求。接下来，让我们将这个指令整合到我们的图中。
- en: 10.4.2 Inserting a base context node in our graph
  id: totrans-297
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.4.2 在我们的图中插入一个基本上下文节点
- en: As we learned in chapter nine, using OpenAI's chat completions endpoint involves
    passing a sequence of messages to the LLM.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在第九章所学到的，使用OpenAI的聊天完成端点涉及向LLM传递一系列消息。
- en: In our current graph, the list starts with the user's first instruction and
    contains only user messages and AI responses. To prevent Nibby from responding
    to frivolous requests, we just need to insert the system prompt we just created
    as the first message in the list we send to the LLM.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们当前的图中，列表从用户的第一个指令开始，只包含用户消息和AI响应。为了防止Nibby对无聊的请求做出回应，我们只需要将我们刚刚创建的系统提示作为列表中的第一条消息插入到我们发送给LLM的列表中。
- en: We'll do this by inserting a new node in the graph to add the system message
    to the graph state and modifying the existing `assistant_node` to pass this message
    to the LLM before anything else.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过在图中插入一个新节点来添加系统消息到图状态，并修改现有的`assistant_node`在传递任何其他信息之前将此消息传递给LLM。
- en: Figure 10.12 shows a visual representation of the new graph.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.12显示了新图的视觉表示。
- en: '![image](../Images/10__image012.png)'
  id: totrans-302
  prefs: []
  type: TYPE_IMG
  zh: '![image](../Images/10__image012.png)'
- en: Figure 10.12 Adding a base context node in our graph
  id: totrans-303
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.12 在我们的图中添加基本上下文节点
- en: 'The changes required to `graph.py` are shown in listing 10.7:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 对`graph.py`所需的更改在列表10.7中显示：
- en: Listing 10.7 graph.py (modified)
  id: totrans-305
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表10.7 graph.py（已修改）
- en: '[PRE31]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: (`chapter_10/in_progress_04/graph.py` in the GitHub repo)
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: （GitHub仓库中的`chapter_10/in_progress_04/graph.py`）
- en: Starting from the top, we've added a couple of imports; we need the `SystemMessage`
    class in addition to `HumanMessage`, so that's one.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 从顶部开始，我们添加了一些导入；我们需要`SystemMessage`类以及`HumanMessage`，所以这是第一个。
- en: The statement `from prompts import *` allows us to access any prompt we may
    add to `prompts.py` using only its variable name—without a prefix like `prompt.`.
    Since we're using the `*` wildcard here rather than importing specific objects,
    every object in `prompt.py`'s global scope becomes part of `graph.py`'s scope.
    In this case, it means we can refer to `BASE_SYS_MSG` directly, as we do later
    in the code.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 语句`from prompts import *`允许我们仅使用变量名——而不是像`prompt.`这样的前缀——访问我们可能添加到`prompts.py`中的任何提示。由于我们在这里使用的是`*`通配符而不是导入特定对象，`prompt.py`的全局作用域中的每个对象都成为`graph.py`作用域的一部分。在这种情况下，这意味着我们可以直接引用`BASE_SYS_MSG`，就像我们在代码中稍后所做的那样。
- en: 'We''ve defined a new `AgentState` class:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义了一个新的`AgentState`类：
- en: '[PRE32]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '`AgentState` inherits from `MessagesState`, so it also contains the `messages`
    field we''ve been using thus far. What we''re effectively doing here is adding
    a new field to the state—called `sys_msg_text`—meant to hold the text of the system
    message.'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: '`AgentState`从`MessagesState`继承，因此它也包含我们迄今为止一直在使用的`messages`字段。我们在这里实际上是在状态中添加一个新的字段——称为`sys_msg_text`——用来存储系统消息的文本。'
- en: 'Next, within the class itself, we''ve added a new static method:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，在类内部，我们添加了一个新的静态方法：
- en: '[PRE33]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'This function represents the new node we''re adding to the graph, called `base_context`.
    All this node does is to populate the `sys_msg_text` field we''ve added to the
    state. By returning `{"sys_msg_text": BASE_SYS_MSG}`, this node sets `sys_msg_text`
    to `BASE_SYS_MSG`—the context prompt we created a few minutes ago—in the graph''s
    current state.'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: '这个函数代表我们在图中添加的新节点，称为`base_context`。这个节点所做的只是填充我们添加到状态中的`sys_msg_text`字段。通过返回`{"sys_msg_text":
    BASE_SYS_MSG}`，这个节点将`sys_msg_text`设置为`BASE_SYS_MSG`——我们几分钟前创建的上下文提示——在图当前状态中。'
- en: To understand how this works, it's helpful to remember that a graph node does
    not return the entirety of the state; rather it only returns the keys in the state
    that need to be modified. Therefore, even though there's no mention of the `messages`
    field here, once this node has been executed, the state will continue to have
    that field—unmodified—in addition to `sys_msg_text`.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解这是如何工作的，记住一个图节点并不返回整个状态；相反，它只返回需要修改的状态中的键。因此，尽管这里没有提到`messages`字段，一旦这个节点被执行，状态将继续保留那个字段——未修改的——除了`sys_msg_text`。
- en: Note
  id: totrans-317
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: Unlike in the case of `messages`, when we return a dictionary with a `sys_msg_text`
    key it *replaces* the value of `sys_msg_text` in the state. This is because `sys_msg_text`
    uses the default update behavior, as opposed to the *append* behavior (enabled
    internally by the `add_messages` function) that `messages` uses.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 与`messages`的情况不同，当我们返回一个包含`sys_msg_text`键的字典时，它会*替换*状态中`sys_msg_text`的值。这是因为`sys_msg_text`使用默认的更新行为，而不是`messages`使用的（由`add_messages`函数内部启用的）*追加*行为。
- en: Why have we made `base_context_node` a static method? Well, recall once again
    that each node in the graph needs to accept the graph state as its first argument.
    We would like to put `base_context_node` inside `SupportAgentGraph` for logical
    code organization purposes, but if we make it a regular method, it'll need to
    accept the class instance (`self`) as its first argument. Making it a static method
    removes that requirement, and frees us to add a state argument.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么我们将`base_context_node`做成静态方法？好吧，再次回想一下，图中的每个节点都需要接受图状态作为其第一个参数。我们希望将`base_context_node`放在`SupportAgentGraph`中，以便于逻辑代码组织，但如果我们将其做成普通方法，它将需要接受类实例（`self`）作为第一个参数。将其做成静态方法消除了这个要求，并使我们能够添加一个状态参数。
- en: Some of you might be asking, "Wait a minute, didn't we structure `assistant_node`
    as a nested function for the same reason? Why didn't we do *that* here?"
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 一些读者可能会问，“等等，我们不是也为了同样的原因将 `assistant_node` 结构化为嵌套函数吗？为什么这里没有这样做？”
- en: We could indeed have used a closure-based solution for `base_context_node` too,
    but we don't need to; unlike `assistant_node` which references `self.llm`, `base_context_node`
    doesn't need to access `self` at all. We therefore employ the more straightforward
    technique of applying the `@staticmethod` decorator to `base_context_node`.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 我们确实可以使用基于闭包的解决方案来处理 `base_context_node`，但我们不需要；与引用 `self.llm` 的 `assistant_node`
    不同，`base_context_node` 完全不需要访问 `self`。因此，我们采用了更直接的技术，即应用 `@staticmethod` 装饰器到
    `base_context_node`。
- en: 'Speaking of `assistant_node`, consider the changes we''ve made to its code
    above:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 说到 `assistant_node`，考虑一下我们对它代码所做的更改：
- en: '[PRE34]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Rather than invoking the LLM directly with `state["messages"]`, we now create
    a `SystemMessage` object with the `sys_msg_text` field we populated in `base_context_node`
    as the content and prepend it to `state["messages"]` to form the list we pass
    the LLM.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在不是直接用 `state["messages"]` 调用 LLM，而是创建一个 `SystemMessage` 对象，其中 `sys_msg_text`
    字段是我们填充在 `base_context_node` 中的内容，并将其添加到 `state["messages"]` 的前面，以形成传递给 LLM 的列表。
- en: 'Finally, note our updates to `build_graph`. Since we''ve extended `MessagesState`
    to include a `sys_msg_text` field, we use that to initialize the `StateGraph`:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，请注意我们对 `build_graph` 的更新。由于我们已经扩展了 `MessagesState` 以包括 `sys_msg_text` 字段，我们使用它来初始化
    `StateGraph`：
- en: '[PRE35]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'We add the base context node like this:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 我们像这样添加基础上下文节点：
- en: '[PRE36]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Notice how we're passing a reference to the `self.base_context_node` method
    itself here, as opposed to calling it with the double parentheses.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 注意我们在这里是如何传递对 `self.base_context_node` 方法的引用，而不是用双括号调用它。
- en: 'We also reorder the edges in the graph to insert the `base_context` node between
    `START` and `assistant`:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还重新排列了图中的边，在 `START` 和 `assistant` 之间插入 `base_context` 节点：
- en: '[PRE37]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: That should be all we need. Go ahead and re-run your app. Try requesting the
    bot to sing a song again to get a response similar to figure 10.13.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该就是我们所需要的。继续运行您的应用程序。尝试再次请求机器人唱歌，以获得类似于图 10.13 的响应。
- en: '![image](../Images/10__image013.png)'
  id: totrans-333
  prefs: []
  type: TYPE_IMG
  zh: '![image](../Images/10__image013.png)'
- en: Figure 10.13 Nibby now refuses to entertain frivolous requests (see chapter_10/in_progress_04
    in the GitHub repo for the full code).
  id: totrans-334
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 10.13 Nibby 现在拒绝处理无聊的请求（完整的代码请参阅 GitHub 仓库中的 chapter_10/in_progress_04）。
- en: 'It looks like Nibby got the memo! It won''t help the user with irrelevant requests
    anymore. In the next section, we''ll solve the opposite problem: getting it to
    help with *relevant* questions.'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来 Nibby 收到了通知！它不会再帮助用户处理不相关的请求了。在下一节中，我们将解决相反的问题：让它帮助回答*相关*的问题。
- en: 10.5 Retrieval Augmented Generation
  id: totrans-336
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.5 检索增强生成
- en: Models like GPT-4o are so effective because they have been pre-trained on a
    huge corpus of publicly available information, such as books, magazines, and websites.
    It's why Fact Frenzy, our trivia app from chapter nine, was able to ask and answer
    questions on such a wide range of topics.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 模型如 GPT-4o 非常有效，因为它们已经在大量公开可用的信息语料库上进行了预训练，例如书籍、杂志和网站。这就是为什么我们的第九章中的知识问答应用“Fact
    Frenzy”能够就如此广泛的主题提问和回答问题。
- en: However, many of the more economically valuable use cases of generative AI require
    more than information in the public domain. Truly molding AI into something that
    fits your specific use case usually requires providing it with private information
    that only you possess.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，许多更具经济价值的生成式 AI 用例需要的信息不仅限于公共领域。真正将 AI 调整为适合您特定用例通常需要提供只有您拥有的私有信息。
- en: Take Nibby, for instance, who is ultimately meant to assist customers of Note
    n' Nib with their queries. What happens if we pose a valid question about a stationery
    product to Nibby? Figure 10.14 shows such a conversation.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 以 Nibby 为例，它最终旨在帮助 Note n' Nib 的客户查询。如果我们向 Nibby 提出一个关于文具产品的有效问题会发生什么？图 10.14
    展示了这样的对话。
- en: '![image](../Images/10__image014.png)'
  id: totrans-340
  prefs: []
  type: TYPE_IMG
  zh: '![image](../Images/10__image014.png)'
- en: Figure 10.14 Nibby makes up information when it doesn't know the answer.
  id: totrans-341
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 10.14 Nibby 在不知道答案时编造信息。
- en: Seems like Nibby knocked that one out of the park, right? Not quite. We never
    told our bot what kinds of pens Note n' Nib carries, so where is it getting its
    information from? Additionally, where are the fictional brands—InkStream and RoyalQuill—we
    encountered in chapter six? As it turns out, Nibby had no information available
    about fountain pens and, therefore, simply hallucinated this response!
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来Nibby已经做得很好了，对吧？并不完全是这样。我们从未告诉我们的机器人Note n' Nib携带哪些类型的笔，所以它从哪里得到这些信息呢？此外，我们在第六章中遇到的虚构品牌——InkStream和RoyalQuill——在哪里？实际上，Nibby没有关于钢笔的信息，因此只是简单地虚构了这个回答！
- en: In this section, we'll discover a way to augment Nibby's existing store of worldly
    knowledge with custom information that we provide.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将发现一种方法，通过我们提供的自定义信息来增强Nibby现有的世界知识库。
- en: 10.5.1 What is Retrieval Augmented Generation?
  id: totrans-344
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.5.1 什么是检索增强生成？
- en: So, how do we supplement all of the information an LLM has been trained on with
    our own? For relatively small pieces of information, it's actually trivially easy—in
    fact, we already know how! All we need to do is provide the info to the LLM as
    part of our prompt!
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们如何补充一个LLM训练的所有信息以及我们自己的信息呢？对于相对较小的信息片段，实际上是非常容易的——事实上，我们已经有了解决方法！我们只需要将信息作为我们提示的一部分提供给LLM！
- en: We could get Nibby to get the question we posed in figure 10.14 right simply
    by listing the products Note n' Nib sells directly in the system message we send
    to the LLM.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过在发送给LLM的系统消息中直接列出Note n' Nib销售的产品，简单地让Nibby正确回答我们在图10.14中提出的问题。
- en: What about other questions, though? Technically, we could give the model all
    of the contextual information it might realistically need to answer any question
    directly in the prompt. The maximum amount of such information we can provide
    is measured in tokens, called the model's *context window length.*
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，其他问题呢？技术上，我们可以直接在提示中给出模型可能需要回答任何问题的所有上下文信息。我们可以提供此类信息的最大量是以标记计的，称为模型的*上下文窗口长度*。
- en: Relatively recent models have a huge context window. For instance, gpt-4o-mini
    can take up to 128,000 tokens (about 96,000 words, since—on average—a token is
    roughly three-quarters of a word), while o3-mini—a newer reasoning model from
    OpenAI—has a context window of 200,000 tokens. Models from other providers can
    take even more tokens in a single prompt. Google's Gemini 2.0 Pro has a context
    window that is a whopping *2 million* tokens long—enough to fit the entire Harry
    Potter series of books, with space left over for almost all of the Lord of the
    Rings trilogy.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 相对较新的模型拥有巨大的上下文窗口。例如，gpt-4o-mini可以处理高达128,000个标记（大约96,000个单词，因为——平均而言——一个标记大约是三分之二的单词），而o3-mini——来自OpenAI的一个较新的推理模型——上下文窗口为200,000个标记。其他提供商的模型在一个提示中可以处理更多的标记。Google的Gemini
    2.0 Pro的上下文窗口长达惊人的*2百万*个标记——足以容纳整个《哈利·波特》系列书籍，还剩下足够的空间来容纳几乎所有的《指环王》三部曲。
- en: Surely our problem is solved then? We can simply assemble all the information
    we possess about Note n' Nib and feed it to the LLM in each prompt, correct?
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 难道我们的问题就解决了？我们可以简单地组装我们关于Note n' Nib的所有信息，并在每个提示中将其提供给LLM，对吗？
- en: 'Certainly, we could, but we probably don''t want to for a couple of reasons:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我们可以这样做，但可能出于几个原因我们不想这么做：
- en: LLMs are prone to information overload; we generally see degraded performance
    with extremely large prompts.
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLM容易受到信息过载的影响；我们通常看到在极其大的提示下性能下降。
- en: Even if there were no such degradation, LLM providers usually charge by the
    token, so if we had to pass our entire custom knowledge base in every LLM call,
    the costs would go through the roof.
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 即使没有这种退化，LLM提供商通常按标记收费，所以如果我们必须在每次LLM调用中传递我们的整个自定义知识库，成本将会飙升。
- en: No, we need a different solution. If only we could **read in a user's question
    and feed the LLM just the** ***relevant*** **parts of our knowledge base required
    to answer it**.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 不，我们需要一个不同的解决方案。如果我们能够**读取用户的问题，并仅向LLM提供** ***回答问题所需的我们知识库中的相关部分***。
- en: And that—in case you've somehow failed to realize where this spiel is going—is
    exactly what Retrieval Augmented Generation (RAG) is.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 而且——如果你还没有意识到这个讨论的方向——这正是检索增强生成（RAG）的含义。
- en: 'RAG has the following essential steps:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: RAG具有以下基本步骤：
- en: Read the user's question
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 读取用户的问题
- en: '**Retrieve** the context *relevant* to the question from the knowledge base'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**检索**与问题相关的上下文信息从知识库中'
- en: '**Augment** the question with the context required to answer it'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**增强**问题，添加回答它所需的上下文'
- en: '**Generate** the answer to the question by feeding the question and context
    to the LLM'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**生成**问题的答案，通过将问题和上下文输入到LLM中'
- en: The hard part of RAG is the *retrieve* step. Specifically, given a user question
    and a large custom knowledge base, how do you identify the parts of the knowledge
    base that are relevant to the question and extract only those parts from the base?
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: RAG的难点在于*检索*步骤。具体来说，给定一个用户问题和大型自定义知识库，你如何识别与问题相关的知识库部分，并仅从库中提取这些部分？
- en: The answer lies in the concept of *embeddings* and a piece of software known
    as a *vector database*.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 答案在于*嵌入*的概念以及一个名为*向量数据库*的软件。
- en: Embeddings and Vector databases
  id: totrans-362
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 嵌入和向量数据库
- en: While we don't—strictly speaking—need to learn how embeddings—or even vector
    databases—work under the hood to implement RAG, it would be a good idea to gain
    a basic understanding of these concepts.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然从严格意义上讲，我们不需要学习嵌入或向量数据库的工作原理来实现RAG，但了解这些概念的基本知识会是一个好主意。
- en: Let's start with a simplified example to achieve this. Say you're known as something
    of a movie buff in your friend circle. Your buddy approaches you and says, "Hey,
    I watched *The Dark Knight* yesterday and loved it! Could you recommend another
    movie like it?"
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从简化示例开始，以实现这一点。假设你在朋友圈中以电影爱好者而闻名。你的朋友走到你面前说：“嘿，我昨天看了*黑暗骑士*，非常喜欢！你能推荐一部类似的电影吗？”
- en: You're in a fix because—though you have an encyclopedic knowledge of movies—you're
    not sure how exactly to measure the *similarity* between two movies, so you can
    recommend the one that's the *most* similar to *The Dark Knight*. Refusing to
    accept defeat, you flee to your underground lair and try to work it out in solitude.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 你陷入了困境，因为你虽然对电影有百科全书式的了解，但你不确定如何确切地衡量两部电影之间的*相似度*，以便推荐与*黑暗骑士*最相似的电影。拒绝接受失败，你逃到你的地下巢穴，独自尝试解决这个问题。
- en: 'Eventually, you come up with a system. You reckon that when people express
    their preference for various movies, they''re subconsciously talking about two
    attributes: *comedic value* and *explosions per hour*. Therefore, you rate your
    entire catalog of movies against those two scales and plot the results in a chart
    (partially reproduced in figure 10.15).'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，你提出了一套系统。你认为当人们表达对各种电影的偏好时，他们潜意识中在谈论两个属性：*喜剧值*和*每小时爆炸次数*。因此，你将你的整个电影目录与这两个尺度进行比较，并在图表上绘制结果（部分如图10.15所示）。
- en: '![image](../Images/10__image015.png)'
  id: totrans-367
  prefs: []
  type: TYPE_IMG
  zh: '![image](../Images/10__image015.png)'
- en: Figure 10.15 Converting movies into vectors and plotting them on a chart
  id: totrans-368
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.15 将电影转换为向量并在图表上绘制
- en: 'As you can see, *The Dark Knight* has a comedic value of `1.2` but a relatively
    high explosions-per-hour of `6`. We can represent it as a list of numbers: `[1.2,
    6]`, called a *vector*. We can call the vector `[1.2, 6]` the *embedding* of the
    movie *The Dark Knight* in the two-dimensional comedic-value/explosions-per-hour
    space.'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，*黑暗骑士*的喜剧值为`1.2`，但每小时爆炸次数相对较高，为`6`。我们可以将其表示为一个数字列表：`[1.2, 6]`，称为一个*向量*。我们可以称这个向量`[1.2,
    6]`为电影*黑暗骑士*在二维喜剧值/每小时爆炸次数空间中的*嵌入*。
- en: Converting movies into numbers in this way makes it possible to measure their
    similarity. For instance, *Office Space* is represented as `[7.2, 0.4]` in the
    same space. The similarity (or, rather, lack thereof) between *Office Space* and
    *The Dark Knight* can be calculated mathematically by considering their *geometric*
    *distance*. The closer the embeddings of two movies are geometrically—as measured
    by the length of a straight line drawn between them—the more similar the underlying
    movies are.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 以这种方式将电影转换为数字，使得可以衡量它们的相似度。例如，*办公室空间*在同一空间中表示为`[7.2, 0.4]`。通过考虑它们的*几何*距离，可以数学地计算出*办公室空间*和*黑暗骑士*之间的相似度（或者更确切地说，是缺乏相似度）。两部电影嵌入的几何距离越近——通过它们之间画出的直线长度来衡量——它们背后的电影就越相似。
- en: After several such calculations, you find that *The Bourne Ultimatum*, which
    has the vector `[1.1, 5.9]` is the closest to *The Dark Knight*. Having concluded
    your research, you get back to your friend and let them know (to which your friend
    responds, "Thank goodness you're alive! It's been two years, where have you *been*?").
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 经过几次这样的计算后，你发现具有向量`[1.1, 5.9]`的*终极越狱*与*黑暗骑士*最接近。在完成你的研究后，你回到朋友那里告诉他们（你的朋友回答说：“谢天谢地你还在！已经两年了，你去哪了*了*？”）。
- en: The question we're facing with Nibby is analogous to the movie recommendation
    problem above. Given a user's message (the movie your friend liked), and a knowledge
    base (your catalog of movies), we have to find the paragraphs/chunks (movies)
    that are most relevant (most "similar") to the user's message.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 我们面临的 Nibby 问题与上述电影推荐问题类似。给定一个用户的消息（你朋友喜欢的电影），以及一个知识库（你的电影目录），我们必须找到与用户消息最相关（最“相似”）的段落/片段（电影）。
- en: 'To answer this efficiently, we need two things:'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 为了高效地回答这个问题，我们需要两样东西：
- en: A way to convert a given piece of text into embeddings that capture its meaning
    (or *semantics*)
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一种将给定文本转换为能够捕捉其意义（或*语义*）的嵌入的方法
- en: A way to store these embeddings and quickly calculate distances between them
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一种存储这些嵌入并快速计算它们之间距离的方法
- en: 'Obviously, the movie example above is overly simplistic. Our "space" only had
    two dimensions: comedic value and explosions per hour. Encoding the meaning of
    a piece of text requires a lot more dimensions (like hundreds or thousands), and
    the dimensions themselves would not be human-understandable concepts like "comedic
    value." We''ll use a text embedding model provided by OpenAI for our use case.'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，上面提到的电影例子过于简单。我们的“空间”只有两个维度：喜剧价值和每小时爆炸次数。编码文本的意义需要更多的维度（如数百或数千），而这些维度本身也不会是像“喜剧价值”这样人类可理解的概念。我们将使用
    OpenAI 提供的文本嵌入模型来满足我们的用例。
- en: To store the embeddings, we'll use a program called a *vector database*. Vector
    databases make it easy to calculate the distance between embeddings or find the
    entries closest to a specific one. Rather than the "straight line" (or *Euclidean*)
    distance between vectors, we'll use a score called *cosine similarity*, which
    measures the *angle* between two vectors to determine their similarity. The vector
    database we'll use is Pinecone, a cloud-hosted service.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 为了存储嵌入，我们将使用一个名为*向量数据库*的程序。向量数据库使得计算嵌入之间的距离或找到与特定嵌入最接近的条目变得容易。我们不会使用向量之间的“直线”（或*欧几里得*）距离，而是使用一个称为*余弦相似度*的分数，它通过测量两个向量之间的*角度*来确定它们的相似度。我们将使用的向量数据库是
    Pinecone，这是一个云托管服务。
- en: 10.5.2 Implementing RAG in our app
  id: totrans-378
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.5.2 在我们的应用中实现 RAG
- en: Armed with a conceptual understanding of RAG, let's now focus on implementing
    it to assist Nibby in answering customer questions.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 在掌握了 RAG 的概念理解之后，我们现在将专注于将其实现，以帮助 Nibby 回答客户问题。
- en: Preparing the knowledge base
  id: totrans-380
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 准备知识库
- en: The GitHub folder for this chapter ([https://github.com/aneevdavis/streamlit-in-action/tree/main/chapter_10](https://github.com/aneevdavis/streamlit-in-action/tree/main/chapter_10))
    has a subdirectory called `articles` with a series of customer support articles
    for Note n' Nib. Each article is a text file containing information about Note
    n' Nib's products or how it conducts business. For instance, `our_products.txt`
    includes a list of product descriptions, while `fountain_pen_maintenance.txt`
    is about maintaining RoyalQuill and InkStream pens.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 本章节的 GitHub 文件夹（[https://github.com/aneevdavis/streamlit-in-action/tree/main/chapter_10](https://github.com/aneevdavis/streamlit-in-action/tree/main/chapter_10)）中有一个名为
    `articles` 的子目录，其中包含一系列针对 Note n' Nib 的客户支持文章。每篇文章都是一个包含有关 Note n' Nib 产品信息或其业务运营方式的文本文件。例如，`our_products.txt`
    包含产品描述列表，而 `fountain_pen_maintenance.txt` 则是关于维护 RoyalQuill 和 InkStream 笔的说明。
- en: 'Here''s an excerpt from the article that we''ll reference later:'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是我们将要引用的文章摘录：
- en: '[PRE38]'
  id: totrans-383
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Copy the `articles` folder into your working directory now. This is the knowledge
    base that Nibby will have access to.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 现在将 `articles` 文件夹复制到您的当前工作目录中。这是 Nibby 将能够访问的知识库。
- en: Setting up a vector database
  id: totrans-385
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 设置向量数据库
- en: As mentioned briefly in the previous section, we will be using Pinecone, a managed
    vector database optimized for fast and scalable similarity search that's popular
    for AI applications. Pinecone's free "Starter" plan is more than enough for this
    chapter.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 如前一小节简要提到的，我们将使用 Pinecone，这是一个针对快速和可扩展的相似性搜索进行优化的托管向量数据库，它因适用于人工智能应用而受到欢迎。Pinecone
    的免费“入门”计划对于本章来说已经足够了。
- en: Go to [https://www.pinecone.io/](https://www.pinecone.io/) and sign up for an
    account now. As soon as you're done with the setup, you'll be presented with an
    API key, which you should save immediately. Once that's done, create a new *index*.
    An index is analogous to a table in a regular non-vector database like PostgreSQL.
    The index will store our support articles broken into parts, along with their
    embeddings.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 前往 [https://www.pinecone.io/](https://www.pinecone.io/) 现在注册一个账户。一旦设置完成，你将获得一个
    API 密钥，你应该立即保存。完成之后，创建一个新的 *索引*。索引类似于常规的非向量数据库（如 PostgreSQL）中的表。索引将存储我们的支持文章的各个部分及其嵌入。
- en: 'You''ll be asked to supply values for various options during the creation process:'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建过程中，你将需要提供各种选项的值：
- en: '**Index name:** You can use whatever you like, but remember to save it since
    we''ll use it in our code.'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**索引名称**：你可以使用任何你喜欢的名称，但请记住保存它，因为我们将在代码中使用它。'
- en: '**Model configuration:** Choose `text-embedding-ada-002`, the OpenAI embedding
    model we''ll use; the value of dimensions should automatically be set to 1,536.'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型配置**：选择 `text-embedding-ada-002`，这是我们将要使用的 OpenAI 嵌入模型；维度的值应自动设置为 1,536。'
- en: '**Metric:** Pick `cosine` to use the cosine similarity score we briefly discussed
    earlier.'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**度量标准**：选择 `cosine` 以使用我们之前简要讨论过的余弦相似度分数。'
- en: '**Capacity mode** should be `Serverless`'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**容量模式** 应该是 `Serverless`'
- en: '**Cloud provider**: `AWS` is fine for this.'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**云提供商**：`AWS` 对此是合适的。'
- en: '**Region:** At the time of writing, only `us-east-1` is available in the free
    plan, so pick that.'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**区域**：在撰写本文时，免费计划中只有 `us-east-1` 可用，所以请选择它。'
- en: Ingesting the knowledge base
  id: totrans-395
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 知识库的导入
- en: 'To incorporate the vector store into our chatbot app, we''ll create a `VectorStore`
    class. Before doing so, add your Pinecone API key and the index name you just
    created to `secrets.toml` so it now looks like this:'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 要将向量存储集成到我们的聊天机器人应用程序中，我们将创建一个 `VectorStore` 类。在这样做之前，将你的 Pinecone API 密钥和刚刚创建的索引名称添加到
    `secrets.toml` 中，使其看起来像这样：
- en: '[PRE39]'
  id: totrans-397
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '#A Replace with your actual OpenAI API key.'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: '#A 用你的实际 OpenAI API 密钥替换。'
- en: '#B Replace with your Pinecone API key.'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: '#B 用你的 Pinecone API 密钥替换。'
- en: '#C Replace sk-proj-... with the actual index name.'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: '#C 用实际的索引名称替换 sk-proj-...。'
- en: We've added a new key to `api_keys` and a new section called `config` to hold
    the index name.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 `api_keys` 中添加了一个新键，并在 `config` 部分添加了一个新部分来保存索引名称。
- en: Now on to the `VectorStore` class. Create a file named `vector_store.py` with
    the contents shown in listing 10.8.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是 `VectorStore` 类。创建一个名为 `vector_store.py` 的文件，其内容如列表 10.8 所示。
- en: Listing 10.8 vector_store.py
  id: totrans-403
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 10.8 vector_store.py
- en: '[PRE40]'
  id: totrans-404
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: (`chapter_10/in_progress_05/vector_store.py` in the GitHub repo)
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: （GitHub 仓库中的 `chapter_10/in_progress_05/vector_store.py`）
- en: There's a fair bit going on here, so let's go through it step-by-step.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有很多事情要做，所以让我们一步一步地来看。
- en: 'The `__init__` has the boilerplate code required to set up the vector store
    connection. It accepts two arguments: `api_keys` (the dictionary of API keys)
    and `index_name` (the name of the Pinecone index).'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: '`__init__` 包含设置向量存储连接所需的样板代码。它接受两个参数：`api_keys`（API 密钥的字典）和 `index_name`（Pinecone
    索引的名称）。'
- en: '`__init__` first creates the `Pinecone` object—`pc`—using the Pinecone API
    key we noted a minute ago, and then an `OpenAIEmbeddings` object by passing it
    the OpenAI key. `pc.Index(index_name)` refers to the index we created earlier.
    Finally, we obtain a vector store object from the index and embeddings and assign
    it to `self.store` so we can use it in other methods.'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: '`__init__` 首先使用我们一分钟前记录的 Pinecone API 密钥创建 `Pinecone` 对象——`pc`，然后通过传递 OpenAI
    密钥创建一个 `OpenAIEmbeddings` 对象。`pc.Index(index_name)` 指的是我们之前创建的索引。最后，我们从索引和嵌入中获取一个向量存储对象，并将其分配给
    `self.store`，以便我们可以在其他方法中使用它。'
- en: 'The `ingest_folder` method accepts the path to a folder and saves its contents
    to the Pinecone index. Consider the first part of this method:'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: '`ingest_folder` 方法接受文件夹的路径并将内容保存到 Pinecone 索引中。考虑这个方法的第一个部分：'
- en: '[PRE41]'
  id: totrans-410
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: LangChain provides various *document loaders* to help ingest and parse various
    types of data (text files, PDFs, web pages, databases, etc.) into a structured
    format suitable for processing. `DirectoryLoader` makes it easy to load files
    from a specified directory.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: LangChain 提供了各种 *文档加载器*，以帮助将各种类型的数据（文本文件、PDF、网页、数据库等）导入并解析成适合处理的结构化格式。`DirectoryLoader`
    使从指定目录加载文件变得容易。
- en: The `glob="**/*.txt"` ensures that all text files (`.txt`) in the folder (including
    subfolders) are included.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: '`glob="**/*.txt"` 确保包含文件夹（包括子文件夹）中的所有文本文件（`.txt`）。'
- en: '`loader_cls=TextLoader` tells `DirectoryLoader` to use another loader class
    called `TextLoader`—also provided by LangChain—for loading individual text files.'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: '`loader_cls=TextLoader` 告诉 `DirectoryLoader` 使用另一个名为 `TextLoader` 的加载器类（由 LangChain
    提供）来加载单个文本文件。'
- en: 'Once the `DirectoryLoader` is created, the next step is to load the documents:'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦创建 `DirectoryLoader`，下一步是加载文档：
- en: '[PRE42]'
  id: totrans-415
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: This reads all `.txt` files in the directory and loads them into a list of `Document`
    objects, which LangChain uses to store the raw text and metadata.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 这将读取目录中的所有 `.txt` 文件并将它们加载到一个 `Document` 对象列表中，LangChain 使用它来存储原始文本和元数据。
- en: At the moment, each `Document` consists of an entire article. While the articles
    in our folder are relatively small, one can easily imagine a support article being
    thousands of words long. The point of fetching just the relevant text from our
    knowledge base is to reduce the overall size of the prompt; simply fetching entire
    articles defeats this purpose.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，每个 `Document` 都包含一整篇文章。虽然我们文件夹中的文章相对较小，但可以很容易地想象一个支持文章可能长达数千个单词。从我们的知识库中获取相关文本的目的就是为了减少整个提示的大小；简单地获取整个文章是达不到这个目的的。
- en: 'Therefore, we want to divvy up the articles into manageable *chunks* of text
    that are roughly of equal size. That''s what the next part of our code does:'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们希望将文章分成可管理的、大致相等大小的文本块。这正是我们代码的下一部分所做的事情：
- en: '[PRE43]'
  id: totrans-419
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '`RecursiveCharacterTextSplitter` is a text-splitting utility from LangChain
    that can break documents into chunks while preserving meaningful context.'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: '`RecursiveCharacterTextSplitter` 是 LangChain 中的一个文本分割实用工具，可以在保留有意义上下文的同时将文档分割成块。'
- en: '`chunk_size=500` sets the length of each chunk to 500 characters.'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: '`chunk_size=500` 设置每个块的长度为 500 个字符。'
- en: '`chunk_overlap=200` means that chunks will have a 200-character overlap to
    maintain context.'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: '`chunk_overlap=200` 表示块之间将有一个 200 个字符的重叠，以保持上下文。'
- en: 'After splitting the documents, the chunks—available in `texts`—are ready to
    be stored:'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 在分割文档后，`texts` 中可用的块准备存储：
- en: '[PRE44]'
  id: totrans-424
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '`add_documents` takes the split text chunks and adds them to the Pinecone index,
    storing both the text and embeddings generated using the `text-embedding-ada-002`
    model, making them searchable.'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: '`add_documents` 函数将分割后的文本块添加到 Pinecone 索引中，存储使用 `text-embedding-ada-002` 模型生成的文本和嵌入，使其可搜索。'
- en: 'The `retrieve` method allows callers to query the vector store:'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: '`retrieve` 方法允许调用者查询向量存储：'
- en: '[PRE45]'
  id: totrans-427
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: The `similarity_search` method of our `PineconeVectorStore` instance (`self.store`)
    searches the vector store for documents similar to `query`—the user's message—using
    the generated embeddings, returning a list of relevant `Document` objects based
    on the query.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 我们 `PineconeVectorStore` 实例（`self.store`）的 `similarity_search` 方法使用生成的嵌入在向量存储中搜索与
    `query`（用户的消息）相似的文档，根据查询返回相关 `Document` 对象的列表。
- en: At this point, we've coded up the vector store functionality into a handy little
    class; next, let's use the class to ingest our `articles/` folder.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经将向量存储功能编码到一个方便的小类中；接下来，让我们使用这个类来导入我们的 `articles/` 文件夹。
- en: This is an *offline* step that you only need to perform one time; once you've
    stored your articles in Pinecone, they'll remain there until you remove them.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个 *离线* 步骤，您只需要执行一次；一旦您将文章存储在 Pinecone 中，它们将保留在那里，直到您删除它们。
- en: Go ahead and create a file called `ingest_to_vector_store.py`, copying the contents
    from listing 10.9.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 现在创建一个名为 `ingest_to_vector_store.py` 的文件，并将列表 10.9 的内容复制进去。
- en: Listing 10.9 ingest_to_vector_store.py
  id: totrans-432
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 10.9 ingest_to_vector_store.py
- en: '[PRE46]'
  id: totrans-433
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: (`chapter_10/in_progress_05/ingest_to_vector_store.py` in the GitHub repo)
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: (`chapter_10/in_progress_05/ingest_to_vector_store.py` 在 GitHub 仓库中)
- en: 'Since this isn''t meant to be run using Streamlit, we use the `toml` module
    directly to read our `secrets.toml`:'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这不是使用 Streamlit 运行的，我们直接使用 `toml` 模块来读取我们的 `secrets.toml`：
- en: '[PRE47]'
  id: totrans-436
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: At the end of this, `secrets` should be a dictionary that contains the `api_keys`
    and `config` keys we organized `secrets.toml` into earlier.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 在此之后，`secrets` 应该是一个包含我们之前组织到 `secrets.toml` 中的 `api_keys` 和 `config` 键的字典。
- en: 'Next we grab the values we need:'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们获取所需的值：
- en: '[PRE48]'
  id: totrans-439
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'We can now instantiate our `VectorStore` class:'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以实例化我们的 `VectorStore` 类：
- en: '[PRE49]'
  id: totrans-441
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Finally, we trigger the `ingest_folder` method:'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们触发 `ingest_folder` 方法：
- en: '[PRE50]'
  id: totrans-443
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'To perform the actual ingestion, run this file in your terminal with the `python`
    command:'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行实际的导入，请在您的终端中使用 `python` 命令运行此文件：
- en: '[PRE51]'
  id: totrans-445
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Once it completes, you can go to the page corresponding to your index on the
    Pinecone website to see the newly ingested chunks, as seen in figure 10.16.
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦完成，您可以去 Pinecone 网站上与您的索引对应的页面查看新导入的块，如图 10.16 所示。
- en: '![image](../Images/10__image016.png)'
  id: totrans-447
  prefs: []
  type: TYPE_IMG
  zh: '![image](../Images/10__image016.png)'
- en: Figure 10.16 You can see the chunks in your index on the Pinecone website.
  id: totrans-448
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.16 你可以在Pinecone网站上看到你的索引中的块。
- en: Notice the source field which holds the file name each chunk came from. You
    can also click the edit button on a record to add more metadata or to see its
    numeric vector values. If you were to count them, you'd find that there are 1536
    of them, corresponding to the number of dimensions in the embedding model.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 注意源字段，它包含每个块来自的文件名。你还可以点击记录上的编辑按钮来添加更多元数据或查看其数值向量值。如果你要计数，你会找到1536个，这对应于嵌入模型的维度数。
- en: Adding RAG to the graph
  id: totrans-450
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 向图中添加RAG
- en: 'The Pinecone index we need for RAG is ready to go, but we still need to incorporate
    the functionality in our chatbot. For this, let''s first lay out the additional
    instructions Nibby needs to use the knowledge base. Append the following to `prompts.py`:'
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要的用于RAG的Pinecone索引已经准备好了，但我们还需要将其功能整合到我们的聊天机器人中。为此，让我们首先列出Nibby需要使用知识库的附加说明。将以下内容追加到`prompts.py`：
- en: '[PRE52]'
  id: totrans-452
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '{docs_content}'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: '{docs_content}'
- en: '[PRE53]'
  id: totrans-454
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: (`chapter_10/in_progress_05/prompts.py` in the GitHub repo)
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: （GitHub仓库中的`chapter_10/in_progress_05/prompts.py`）
- en: In a moment, we'll write the logic to replace `{docs_content}` with the document
    chunks we retrieve from Pinecone. The idea here is to give Nibby the context it
    needs and to get it to stop fabricating an answer out of thin air if it's not
    confident.
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 在不久的将来，我们将编写逻辑来用我们从Pinecone检索到的文档块替换`{docs_content}`。这里的想法是给Nibby提供所需的上下文，并让它停止在没有信心的情况下凭空编造答案。
- en: Next, let's modify `graph.py` as shown in listing 10.10 so that it implements
    RAG.
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们修改`graph.py`，如列表10.10所示，以实现RAG。
- en: Listing 10.10 graph.py (with RAG nodes)
  id: totrans-458
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表10.10 graph.py（带有RAG节点）
- en: '[PRE54]'
  id: totrans-459
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: (`chapter_10/in_progress_05/graph.py` in the GitHub repo)
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: （GitHub仓库中的`chapter_10/in_progress_05/graph.py`）
- en: 'The first change is to `AgentState`, which now looks like this:'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个更改是`AgentState`，现在看起来是这样的：
- en: '[PRE55]'
  id: totrans-462
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: We'll now store the list of chunks retrieved from Pinecone in the graph state
    in a variable called `retrieved_docs`.
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将从Pinecone检索到的块列表存储在图的`retrieved_docs`变量中。
- en: '`__init__` now accepts `vector_store`—an instance of our `VectorStore` class—as
    an argument and saves it to `self.vector_store`.'
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: '`__init__`现在接受`vector_store`——我们的`VectorStore`类的一个实例——作为参数，并将其保存到`self.vector_store`。'
- en: Note that rather than create the `VectorStore` instance *within* `graph.py`,
    we've chosen to have it be created elsewhere (`bot.py`, as we'll soon find out)
    and simply pass it to the `SupportAgentGraph` class. This is because we want `graph.py`
    to only contain the core logic of the graph. Objects that the graph *depends*
    on such as `llm` and `vector_store` should be passed to it. This coding pattern
    is called *dependency injection*, and is helpful while writing automated tests.
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们不是在`graph.py`中创建`VectorStore`实例，而是选择在别处（`bot.py`，我们很快就会知道）创建它，并将其简单地传递给`SupportAgentGraph`类。这是因为我们希望`graph.py`只包含图的逻辑核心。图所依赖的对象，如`llm`和`vector_store`，应该传递给它。这种编码模式被称为*依赖注入*，在编写自动化测试时很有帮助。
- en: Next, we need to introduce the process of retrieval-augmented generation to
    our graph. Figure 10.17 shows what the graph should look like by the end of this.
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要将检索增强生成过程引入我们的图中。图10.17显示了本节结束时图应该的样子。
- en: '![image](../Images/10__image017.png)'
  id: totrans-467
  prefs: []
  type: TYPE_IMG
  zh: '![image](../Images/10__image017.png)'
- en: Figure 10.17 Our graph now has retrieve and augment nodes for RAG.
  id: totrans-468
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.17 我们现在在图中有了用于RAG的检索和增强节点。
- en: 'We''ve inserted two nodes in between the `base_context` and `assistant` nodes:
    a `retrieve` node that grabs context from the knowledge base that''s relevant
    to the user''s query and an `augment` node that adds this info to the prompt.'
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在`base_context`和`assistant`节点之间插入了两个节点：一个用于从与用户查询相关的知识库中检索上下文的`retrieve`节点，以及一个用于将此信息添加到提示中的`augment`节点。
- en: 'Here''s the code for the `retrieve` node:'
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是`retrieve`节点的代码：
- en: '[PRE56]'
  id: totrans-471
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: As in the case of `assistant_node`, `retrieve_node` is structured as a nested
    function within a class method. It simply extracts the content of all the messages
    in the conversation and puts them in a single string to form the "query" that
    we'll pass to the vector store.
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 就像`assistant_node`的情况一样，`retrieve_node`被结构化为一个类方法内的嵌套函数。它只是提取对话中所有消息的内容，并将它们放入一个单独的字符串中，形成我们将传递给向量存储的“查询”。
- en: The idea here is to find the chunks most relevant to the conversation. Since
    we're measuring relevance against the query, it makes sense that this is simply
    the text of the conversation.
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的想法是找到与对话最相关的块。由于我们是在与查询测量相关性，所以这仅仅是对话的文本是有意义的。
- en: Once we have the query, we can call the `retrieve` method we defined earlier
    and return the list of retrieved documents in a dictionary, thereby updating the
    `retrieved_docs` key of the graph state.
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了查询，我们可以调用我们之前定义的`retrieve`方法，并以字典的形式返回检索到的文档列表，从而更新图状态中的`retrieved_docs`键。
- en: Note
  id: totrans-475
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: While we've kept things simple here by including the text of the entire conversation
    in the retrieval query, you'll likely run into challenges as the conversation
    gets longer and longer. For any particular AI response, the most *recent* messages
    in the conversation are probably more contextually relevant—so it might be a good
    idea to form the retrieval query from just the last, say, five or six messages
    in the conversation.
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们在这里通过包含整个对话的文本在检索查询中保持了简单，但随着对话变得越来越长，你可能会遇到挑战。对于任何特定的AI响应，对话中最**最近**的消息可能更有语境相关性——因此，从对话的最后几条，比如最后五或六条消息中形成检索查询可能是个好主意。
- en: 'The augmentation node is shown below:'
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图显示了增强节点：
- en: '[PRE57]'
  id: totrans-478
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: This one doesn't need to access anything from `SupportAgentGraph`, so we structure
    it as a static method as we did for `base_context_node`.
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: 这个不需要从`SupportAgentGraph`访问任何内容，所以我们将其结构化为一个静态方法，就像我们对`base_context_node`所做的那样。
- en: '`augment_node` mostly does the tedious work of wrangling and inserting the
    retrieved chunks into the system message. Once it has formed a string by concatenating
    the contents of the retrieved `Document` chunks, it simply plugs it into the text
    of the `SYS_MSG_AUGMENTATION` value we added to `prompts.py`, replacing `{docs_content}`.'
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: '`augment_node`主要做的是繁琐的工作，将检索到的块整理并插入到系统消息中。一旦它通过连接检索到的`Document`块的 内容形成了一个字符串，它就简单地将其插入到我们添加到`prompts.py`中的`SYS_MSG_AUGMENTATION`值的文本中，替换`{docs_content}`。'
- en: At the end of this node, `sys_msg_text` has the full system message—the earlier
    base message warning Nibby not to entertain frivolous questions, as well as the
    retrieved context.
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: 在此节点的末尾，`sys_msg_text`包含完整的系统消息——早些时候的基本消息警告Nibby不要回答无聊的问题，以及检索到的上下文。
- en: We already have a node for the "generate" step in RAG—`assistant_node`—so there's
    no need to add another.
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在RAG的“生成”步骤中有一个节点了——`assistant_node`——因此不需要再添加一个。
- en: The edits to `build_graph` should be fairly obvious given figure 10.17; we officially
    add the `retrieve` and `augment` nodes, and attach them to the right edges.
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: 根据10.17图，对`build_graph`的修改应该是相当明显的；我们正式添加了`retrieve`和`augment`节点，并将它们连接到正确的边上。
- en: 'The next file to edit is `bot.py`. Make the changes below:'
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个要编辑的文件是`bot.py`。进行以下更改：
- en: '[PRE58]'
  id: totrans-485
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: (`chapter_10/in_progress_05/bot.py` in the GitHub repo)
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: （GitHub仓库中的`chapter_10/in_progress_05/bot.py`）
- en: '`__init__` now accepts a `config` parameter, saved to `self.config`. It also
    creates the `vector_store` object by calling the `get_vector_store` method we''ll
    discuss below, and passes it to the `SupportAgentGraph` constructor.'
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: '`__init__`现在接受一个`config`参数，保存到`self.config`。它还通过调用我们下面将要讨论的`get_vector_store`方法创建`vector_store`对象，并将其传递给`SupportAgentGraph`构造函数。'
- en: '`get_vector_store` has the code required to complete the loop. It obtains the
    Pinecone index name from `self.config` and passes both `self.api_keys` and `index_name`
    to create an instance of `VectorStore` before returning it.'
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: '`get_vector_store`包含完成循环所需的代码。它从`self.config`获取Pinecone索引名称，并在返回之前将`self.api_keys`和`index_name`传递给创建`VectorStore`实例。'
- en: 'The last change we need to make is a pretty small one in `frontend.py`:'
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要在`frontend.py`中进行的最后一个更改相当小：
- en: '[PRE59]'
  id: totrans-490
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: (`chapter_10/in_progress_05/frontend.py` in the GitHub repo)
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: （GitHub仓库中的`chapter_10/in_progress_05/frontend.py`）
- en: Since the `Bot` class now accepts a `config` parameter, we get its value from
    `st.secrets` and pass it in while instantiating the class.
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 由于`Bot`类现在接受一个`config`参数，我们从`st.secrets`获取其值，并在实例化类时传递它。
- en: Let's try asking Nibby what products Note n' Nib sells again. Re-run the app
    and talk to it. Figure 10.18 shows an example of the new interaction.
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次尝试询问Nibby Note n' Nib销售什么产品。重新运行应用程序并与它交谈。图10.18显示了新的交互示例。
- en: '![image](../Images/10__image018.png)'
  id: totrans-494
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/10__image018.png)'
- en: Figure 10.18 Nibby can now access and use information from our knowledge base
    (see chapter_10/in_progress_05 in the GitHub repo for the full code).
  id: totrans-495
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.18 现在Nibby可以访问并使用我们的知识库中的信息（有关完整代码，请参阅GitHub仓库中的`chapter_10/in_progress_05`）。
- en: Notice how Nibby answers our questions using information from our knowledge
    base and redirects to a 1-800 number when it encounters a question it doesn't
    know how to answer.
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 注意Nibby如何使用我们的知识库中的信息回答我们的问题，并在遇到不知道如何回答的问题时转向1-800号码。
- en: 10.6 Turning our bot into an agent
  id: totrans-497
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.6 将我们的机器人转换为代理
- en: Giving Nibby access to Note n' Nib's customer support knowledge base has turned
    Nibby into a capable assistant, but it's still a purely informational bot. When
    customers call a support number or chat with a service representative, they're
    more often than not looking for help with their specific situation or order.
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 给Nibby访问Note n' Nib的客户支持知识库已经使Nibby成为一个有能力的助手，但它仍然是一个纯粹的信息型机器人。当客户拨打支持电话或与服务代表聊天时，他们往往更希望得到针对他们特定情况或订单的帮助。
- en: For instance, a customer might wonder why it's taking so long for their placed
    order to arrive and want to check on the status, or they might want to simply
    cancel it. To resolve such issues, Nibby can't just rely on static textual articles;
    it needs to connect to Note n' Nib's systems and retrieve the right information
    or take the appropriate action.
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，一个客户可能会想知道为什么他们下订单的到达时间这么长，并想检查状态，或者他们可能只想取消订单。为了解决这类问题，Nibby不能仅仅依赖于静态的文本文章；它需要连接到Note
    n' Nib的系统并检索正确的信息或采取适当的行动。
- en: 'AI applications that can interact with the real world in this way have a special
    name: agents.'
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: 能够以这种方式与真实世界互动的AI应用有一个特殊的名称：代理。
- en: 10.6.1 What are agents?
  id: totrans-501
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.6.1 什么是代理？
- en: Traditional AI chatbots follow a question-answer pattern, providing helpful
    but static responses. However, when users need personalized assistance—such as
    checking an order status, updating an address, or canceling an order—an informational
    bot falls short.
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的AI聊天机器人遵循问答模式，提供有帮助但静态的响应。然而，当用户需要个性化帮助——例如检查订单状态、更新地址或取消订单时——信息型机器人就不够用了。
- en: This is where *agents* come in. Unlike passive chatbots, AI agents—also called
    *agentic apps*—can reason, plan, and interact with external systems to complete
    tasks. They don't just retrieve knowledge; they take actions based on it. These
    agents often rely on *tool use*, meaning they can call APIs, run database queries,
    and even trigger workflows in real-world applications.
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是*代理*的用武之地。与被动的聊天机器人不同，AI代理——也称为*代理应用*——可以进行推理、规划和与外部系统交互以完成任务。它们不仅检索知识；它们根据知识采取行动。这些代理通常依赖于*工具使用*，这意味着它们可以调用API、运行数据库查询，甚至在实际应用中触发工作流程。
- en: For example, instead of telling a customer to reach out to a customer care number
    for an order tracking number, an agent can fetch the tracking details and provide
    an update directly. Instead of directing a user to a cancellation policy page,
    it can process a cancellation request on their behalf.
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，与其告诉客户联系客户服务电话以获取订单跟踪号，代理可以获取跟踪详情并提供直接更新。而不是将用户引导到取消政策页面，它可以代表他们处理取消请求。
- en: In short, agents make AI practical by allowing it to interface with the systems
    people already use. The key to making an agent work effectively is a framework
    that enables it to reason and decide the next steps dynamically. One such popular
    framework is called *ReAct*, short for *Reasoning + Acting*.
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，代理通过允许AI与人们已经使用的系统接口，使AI变得实用。使代理有效工作的关键是能够使其动态推理和决定下一步行动的框架。这样一个流行的框架被称为*ReAct*，即*推理+行动*。
- en: 10.6.2 The ReAct framework
  id: totrans-506
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.6.2 ReAct框架
- en: To function as a true agent, an AI system must do more than just retrieve facts—it
    needs to reason about a situation, determine the right action, execute that action,
    and then incorporate the result into its next steps. The ReAct framework is designed
    to facilitate this process.
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: 要作为一个真正的代理，一个AI系统必须做的不仅仅是检索事实——它需要对该情况进行推理，确定正确的行动，执行该行动，然后将结果纳入其下一步行动。ReAct框架旨在促进这一过程。
- en: Note
  id: totrans-508
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: The ReAct AI framework is not to be confused with React, a Javascript toolkit
    for building web apps. It just so happens that we'll encounter the latter in chapter
    13.
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: ReAct AI框架与用于构建Web应用的JavaScript工具包React不可混淆。碰巧的是，我们将在第13章遇到后者。
- en: Figure 10.19 is a visual representation of the ReAct framework.
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.19是ReAct框架的视觉表示。
- en: '![image](../Images/10__image019.png)'
  id: totrans-511
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/10__image019.png)'
- en: Figure 10.19 The ReAct framework
  id: totrans-512
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.19 ReAct框架
- en: 'ReAct structures an AI agent''s behavior as an interleaving of reasoning steps
    and actions:'
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: ReAct将AI代理的行为结构化为推理步骤和行动的交织：
- en: '**Reason:** The agent analyzes the user''s query, breaks it down into logical
    steps, and determines what needs to be done.'
  id: totrans-514
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**原因**：代理分析用户的查询，将其分解为逻辑步骤，并确定需要做什么。'
- en: '**Act:** The agent takes a concrete action, such as calling an API or querying
    a database, to retrieve relevant data or execute a task.'
  id: totrans-515
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**行动**：代理采取具体行动，例如调用API或查询数据库，以检索相关数据或执行任务。'
- en: '**Reason:** The agent incorporates the action''s results into its reasoning
    and decides whether further steps are necessary.'
  id: totrans-516
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**原因：** 代理将动作的结果纳入其推理中，并决定是否需要进一步的操作。'
- en: '**Repeat:** If further steps are needed, the cycle repeats.'
  id: totrans-517
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**重复：** 如果需要进一步的操作，循环将重复。'
- en: '**Respond:** If no further steps are needed, respond to the user.'
  id: totrans-518
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**响应：** 如果不需要进一步的操作，向用户响应。'
- en: 'For example, consider a customer asking Nibby: "What''s the status of my order?"
    Using the ReAct framework, the bot might follow these steps:'
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑一个客户询问Nibby：“我的订单状态是什么？”使用ReAct框架，机器人可能会遵循以下步骤：
- en: '**Reason:** "The customer wants to check their order status. I need to fetch
    order details from the order database."'
  id: totrans-520
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**原因：** "客户想要检查他们的订单状态。我需要从订单数据库中获取订单详情。"'
- en: '**Act:** Call Note & Nib''s order management system to retrieve the order status.'
  id: totrans-521
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**行动：** 调用Note & Nib的订单管理系统以检索订单状态。'
- en: '**Reason:** "The system shows the order has shipped and is in transit. I should
    provide the expected delivery date."'
  id: totrans-522
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**原因：** "系统显示订单已发货并正在运输中。我应该提供预计的交货日期。"'
- en: '**Respond:** Respond to the customer with the latest tracking details and estimated
    delivery date.'
  id: totrans-523
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**响应：** 向客户提供最新的跟踪细节和预计交货日期。'
- en: Creating tools our agent can use
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: 创建代理可以使用的工具
- en: The concept of *tools* is central to the process of developing a ReAct AI agent.
    In AI parlance, a tool is a function or API that an AI agent can call to perform
    real-world actions.
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: '*工具*的概念对于开发ReAct AI代理的过程至关重要。在人工智能的术语中，工具是一个AI代理可以调用来执行现实世界动作的函数或API。'
- en: Referring back to the requirements we drafted, we want our bot to be able to
    track and cancel orders on behalf of Note n' Nib's customers. Of course, Note
    n' Nib is a fictional company with no real orders or customers.
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾我们起草的要求，我们希望我们的机器人能够代表Note n' Nib的客户跟踪和取消订单。当然，Note n' Nib是一个虚构的公司，没有真实的订单或客户。
- en: This means we need some example data to work with. You can find this in the
    database.py file in the GitHub repository ([https://github.com/aneevdavis/streamlit-in-action/tree/main/chapter_10/in_progress_06/](https://github.com/aneevdavis/streamlit-in-action/tree/main/chapter_10/in_progress_06/)).
    Copy the file to your working directory. Listing 10.11 shows a couple of excerpts
    from it.
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着我们需要一些示例数据来工作。你可以在GitHub仓库中的database.py文件中找到这些数据（[https://github.com/aneevdavis/streamlit-in-action/tree/main/chapter_10/in_progress_06/](https://github.com/aneevdavis/streamlit-in-action/tree/main/chapter_10/in_progress_06/)).
    将文件复制到你的工作目录。列表10.11展示了其中的几个摘录。
- en: Listing 10.11 database.py
  id: totrans-528
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表10.11 database.py
- en: '[PRE60]'
  id: totrans-529
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: (`chapter_10/in_progress_06/database.py` in the GitHub repo)
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: （GitHub仓库中的`chapter_10/in_progress_06/database.py`）
- en: The file has two dictionaries—`users` and `orders`—keyed on user ID and order
    ID, respectively. Highlighted in the listing is user ID `1` (`Alice Johnson`)
    and a corresponding order the user placed on `2025-02-10`.
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: 文件中有两个字典——`users`和`orders`，分别以用户ID和订单ID为键。列表中突出显示了用户ID `1`（`Alice Johnson`）和用户在`2025-02-10`放置的相应订单。
- en: In a real application, this information would be stored in a database like PostgreSQL.
    Still, since our focus is on the mechanics of building an AI agent, the static
    data in `database.py` will suffice for our purposes.
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际应用中，这些信息会被存储在像PostgreSQL这样的数据库中。然而，由于我们的重点是构建AI代理的机制，`database.py`中的静态数据将满足我们的需求。
- en: There's another file in the repo called `tools.py`. Copy that one over too.
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: 仓库中还有一个名为`tools.py`的文件。也要将这个文件复制过来。
- en: '`tools.py` contains all the functions or tools that Nibby will be able to call.
    The file defines four such functions:'
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: '`tools.py`包含Nibby将能够调用的所有函数或工具。该文件定义了四个这样的函数：'
- en: '`retrieve_user_id` looks up a user''s user ID, given their email address and
    date of birth.'
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`retrieve_user_id`根据用户的电子邮件地址和出生日期查找用户的用户ID。'
- en: '`get_order_id` returns the ID of a particular order, given the user ID of the
    user that placed the order, and the date that they placed it. For simplicity,
    we assume that a user can only place one order on a particular day.'
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`get_order_id`根据放置订单的用户ID和放置日期返回特定订单的ID。为了简单起见，我们假设用户在特定的一天只能放置一个订单。'
- en: '`get_order_status` accepts an order ID and returns its order status, tracking
    number, and expected delivery date.'
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`get_order_status`接受一个订单ID，并返回其订单状态、跟踪号和预计交货日期。'
- en: '`cancel_order` cancels an order, given an order ID.'
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cancel_order`根据订单ID取消订单。'
- en: We're not interested in the actual implementations of these functions, though
    you can read through them in `tools.py`. What matters is that Nibby needs to be
    able to call and use them correctly.
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对这些函数的实际实现不感兴趣，尽管你可以在 `tools.py` 中阅读它们。重要的是 Nibby 需要能够正确地调用并使用它们。
- en: 'For this, we''ll rely on type hints and docstrings. For instance, consider
    the definition of one of these functions in `tools.py`:'
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个，我们将依赖于类型提示和文档字符串。例如，考虑 `tools.py` 中这些函数定义中的一个：
- en: '[PRE61]'
  id: totrans-541
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: (`chapter_10/in_progress_06/tools.py` in the GitHub repo)
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: (`chapter_10/in_progress_06/tools.py` 在 GitHub 仓库中)
- en: 'Notice how we''re using type hints in the function signature (`(email: str,
    dob: str) -> str`), and explaining in detail what the function does in a docstring
    (the multi-line string after the signature), along with details about the arguments
    and return value.'
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: '注意我们如何在函数签名中使用类型提示（`(email: str, dob: str) -> str`），并在文档字符串中详细解释函数的功能（签名后的多行字符串），包括参数和返回值的详细信息。'
- en: These auxiliary items are what the AI model will use to figure out what tool
    to call when.
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: 这些辅助项是 AI 模型将用来确定在何时调用哪个工具的。
- en: 'At the bottom of `tools.py`, there''s this line that exports all the functions
    in the file as a list named tools:'
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `tools.py` 文件的底部，有一行代码将文件中的所有函数导出为一个名为 `tools` 的列表：
- en: '[PRE62]'
  id: totrans-546
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: We'll use the `tools` variable in our graph in a bit.
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在我们的图中稍后使用 `tools` 变量。
- en: 10.6.3 Making our graph agentic
  id: totrans-548
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.6.3 使我们的图具有代理性
- en: 'Consider an example how Nibby might use the four tools we''ve made available
    to help a customer. Let''s say the customer in the example data, Alice Johnson,
    wants to know the status of their order (order 101 shown in the excerpt from `database.py`).
    Here''s how the interaction might proceed:'
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个例子，看看 Nibby 如何使用我们提供的四个工具来帮助客户。假设示例数据中的客户 Alice Johnson 想要知道她订单的状态（如 `database.py`
    中摘录的订单 101）。以下是可能的交互过程：
- en: '**User:** "Hi, my name''s Alice! I placed an order on Feb 10, 2025\. Could
    you check on its status, please?"'
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**用户:** "嗨，我叫 Alice！我在 2025 年 2 月 10 日下了一单。你能检查一下它的状态吗？"'
- en: '**Nibby (reasoning to itself):** "I have a tool called `get_order_status` that
    will give me the status of an order. To call it, I need an `order_id`. I don''t
    have one, but I can get one by calling the `get_order_id` tool, which needs a
    `user_id` and an `order_placed_date`. The user says the `order_placed_date` is
    Feb 10, 2025, so I have that. I still need a `user_id`. To obtain *that*, I can
    call the `retrieve_user_id` tool, which requires an email and a date of birth.
    Therefore, I should ask the customer for this info."'
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Nibby (自我推理):** "我有一个名为 `get_order_status` 的工具，它可以给我提供订单的状态。要调用它，我需要一个 `order_id`。我没有一个，但我可以通过调用
    `get_order_id` 工具来获取一个，这个工具需要一个 `user_id` 和一个 `order_placed_date`。用户说 `order_placed_date`
    是 2025 年 2 月 10 日，所以我有了这个信息。我还需要一个 `user_id`。为了获得 *这个*，我可以调用 `retrieve_user_id`
    工具，它需要一个电子邮件和一个出生日期。因此，我应该向客户询问这些信息。"'
- en: '**Nibby:** "Could you give me your email address and date of birth?"'
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Nibby:** "你能给我你的电子邮件地址和出生日期吗？"'
- en: '**User:** "Sure, my email is alice.johnson@example.com and my date of birth
    is May 10, 1990."'
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**用户:** "当然，我的电子邮件是 alice.johnson@example.com，我的出生日期是 1990 年 5 月 10 日。"'
- en: '**Nibby (tool call):** Call the tool `retrieve_user_id` with the parameters
    `email=`alice.johnson@example.com and `dob=1990-05-10`'
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Nibby (工具调用):** 使用参数 `email=alice.johnson@example.com` 和 `dob=1990-05-10`
    调用工具 `retrieve_user_id`'
- en: '`retrieve_user_id(''`alice.johnson@example.com`'', ''1990-05-10'')`: <returns
    the value `1`, which is Alice''s `user_id`>'
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`retrieve_user_id(''`alice.johnson@example.com`'', ''1990-05-10'')`: <返回值 `1`，这是
    Alice 的 `user_id`>'
- en: '**Nibby (tool call):** Call the tool `get_order_id` with the parameters `user_id=1`
    and `order_placed_date=2025-02-10`'
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Nibby (工具调用):** 使用参数 `user_id=1` 和 `order_placed_date=2025-02-10` 调用工具 `get_order_id`'
- en: '`get_order_id(1, 1990-05-10)`: <returns the value `101`, the correct order
    ID>'
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`get_order_id(1, 1990-05-10)`: <返回值 `101`，正确的订单 ID>'
- en: '**Nibby (tool call):** Call the tool `get_order_status` with the parameter
    `order_id=101`'
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Nibby (工具调用):** 使用参数 `order_id=101` 调用工具 `get_order_status`'
- en: '**get_order_status(101)**: <returns a dictionary: {''order_status'': ''Shipped'',
    ''tracking_number'': ''TRK123456789'', ''expected_delivery_date'': ''2025-02-18''}>'
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**get_order_status(101)**: <返回一个字典：{''order_status'': ''Shipped'', ''tracking_number'':
    ''TRK123456789'', ''expected_delivery_date'': ''2025-02-18''>}'
- en: '**Nibby:** "Your order has been shipped and should arrive on Feb 18, 2025\.
    You can use the tracking number TRK123456789 to track it."'
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Nibby:** "你的订单已经发货，预计将于 2025 年 2 月 18 日到达。你可以使用跟踪号码 TRK123456789 来跟踪它。"'
- en: Note how the bot needs to alternate between conversing with the user, reasoning
    about what needs to be done next, and issuing a call to a tool. How do we code
    all of this up? LangGraph actually makes this surprisingly easy.
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: 注意机器人需要交替与用户交谈、推理下一步需要做什么，以及发出对工具的调用。我们如何将所有这些编码起来？LangGraph实际上使这一切出奇地简单。
- en: 'First, let''s modify `bot.py` to make our LLM aware of the tools available
    to it:'
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们修改`bot.py`，让我们的LLM了解它可用的工具：
- en: '[PRE63]'
  id: totrans-563
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: Here we're *binding* the tools we defined earlier—imported from `tools.py`—to
    our LLM object so that it knows they exist. As a result, if the LLM thinks it's
    appropriate, it can respond with a *tool call*.
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将之前定义的工具（从`tools.py`导入）绑定到我们的LLM对象上，以便它知道它们的存在。因此，如果LLM认为合适，它可以以*工具调用*的形式进行响应。
- en: In the example above, the tool calls are marked with "Nibby (tool call)". In
    practical terms, these are `AIMessage`s produced by the LLM that have a property
    called `tool_calls` which contains information about any tools that the LLM wants
    us to call on its behalf and the parameters to call them with.
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的示例中，工具调用被标记为“Nibby（工具调用）”。在实际情况中，这些是由LLM产生的`AIMessage`，它们有一个名为`tool_calls`的属性，其中包含LLM希望我们代表它调用的任何工具及其参数的信息。
- en: The binding logic uses the docstrings and type hints we specified in `tools.py`
    to explain to the LLM what each tool does and how to use it.
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: 绑定逻辑使用我们在`tools.py`中指定的文档字符串和类型提示来向LLM解释每个工具的作用以及如何使用它。
- en: What about the graph itself? Listing 10.12 shows the changes required to `graph.py`
    to turn our bot into an agent.
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，图本身呢？列表10.12显示了将我们的机器人变成代理所需的对`graph.py`的更改。
- en: Listing 10.12 graph.py (for an agentic app)
  id: totrans-568
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表10.12 graph.py（用于代理应用程序）
- en: '[PRE64]'
  id: totrans-569
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: (`chapter_10/in_progress_06/graph.py` in the GitHub repo)
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
  zh: （GitHub仓库中的`chapter_10/in_progress_06/graph.py`）
- en: Incredibly, all we needed to do was to add three lines to the `build_graph`
    method, and import a few extra items!
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: 令人难以置信的是，我们只需要在`build_graph`方法中添加三行，并导入一些额外的项目！
- en: 'Let''s go through the line additions, starting with the first one:'
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐一查看行添加，从第一个开始：
- en: '[PRE65]'
  id: totrans-573
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'This adds a node named tools to our graph. `ToolNode` is a node already built
    into LangGraph, so we don''t have to define it ourselves. It essentially does
    the following:'
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
  zh: 这在我们的图中添加了一个名为工具的节点。`ToolNode`是LangGraph中已经构建好的节点，所以我们不需要自己定义它。它本质上执行以下操作：
- en: It takes the last message in the messages list (from the graph state) and executes
    any tool calls in it based on the list of tools we pass it.
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它取消息列表（来自图状态）中的最后一条消息，并根据我们传递给它的工具列表执行其中的任何工具调用。
- en: Appends the value returned by the tool(s) as a `ToolMessage`—like `HumanMessage`
    and `AIMessage`—to the messages list.
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将工具（的）返回值作为`ToolMessage`——就像`HumanMessage`和`AIMessage`一样——追加到消息列表中。
- en: At the end of a `ToolNode`, the last message in the messages variable is a `ToolMessage`
    representing the output of calling a tool.
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
  zh: 在`ToolNode`的末尾，消息变量中的最后一条消息是一个表示调用工具输出的`ToolMessage`。
- en: We now have all the *pieces* we need, but how do we orchestrate the kind of
    thinking process outlined in the example interaction at the beginning of this
    section?
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在有了所有需要的*部分*，但我们如何协调本节开头示例交互中概述的那种思维过程？
- en: 'Before we get into that, turn your attention to figure 10.20, which is what
    our graph will look like at the end of these changes:'
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入探讨之前，请将注意力转向图10.20，这是我们进行这些更改后图将看起来的样子：
- en: '![image](../Images/10__image020.png)'
  id: totrans-580
  prefs: []
  type: TYPE_IMG
  zh: '![image](../Images/10__image020.png)'
- en: Figure 10.20 Our graph now has a tools node and a conditional edge
  id: totrans-581
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.20 我们现在的图已经有一个工具节点和一个条件边
- en: Notice that there are now *two* lines flowing out from the assistant node—one
    goes to `END` as before, while another goes to our newly added `tools` node.
  id: totrans-582
  prefs: []
  type: TYPE_NORMAL
  zh: 注意现在从助手节点流出*两条*线——一条像以前一样流向`END`，而另一条流向我们新添加的`tools`节点。
- en: 'In LangGraph, this is known as a *conditional edge*. We can choose the next
    node to execute from multiple options depending on a specified condition. A conditional
    edge is implemented as a function that takes the following form:'
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
  zh: 在LangGraph中，这被称为*条件边*。我们可以根据指定的条件从多个选项中选择下一个要执行的节点。条件边被实现为一个具有以下形式的函数：
- en: '[PRE66]'
  id: totrans-584
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'In our case, we don''t actually need to build our own conditional edge because
    LangGraph already has exactly what we want:'
  id: totrans-585
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的情况下，我们实际上不需要构建自己的条件边，因为LangGraph已经有了我们想要的：
- en: '[PRE67]'
  id: totrans-586
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: '`tools_condition`—imported from `langgraph.prebuilt`—simply routes to our `ToolNode`
    (named `tools`) if the last message in the conversation—i.e. the LLM''s response—contains
    any tool calls, or to END otherwise.'
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
  zh: '`tools_condition`——从`langgraph.prebuilt`导入——如果会话中的最后一条消息（即LLM的响应）包含任何工具调用，则简单地路由到我们的`ToolNode`（命名为`tools`），否则路由到END。'
- en: Since `tools_condition` already has the logic to route to `END`, we can remove
    the earlier line that created a direct (non-conditional) edge between `assistant`
    and `END`.
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
  zh: 由于`tools_condition`已经具有路由到`END`的逻辑，我们可以删除创建`assistant`和`END`之间直接（非条件）边的早期行。
- en: Once the `ToolNode` executes, we need the LLM to read the return value and decide
    what to do with it—whether it's calling *another* tool, or responding to the user.
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦`ToolNode`执行完毕，我们需要LLM读取返回值并决定如何处理它——无论是调用*另一个*工具，还是响应用户。
- en: 'Therefore, we create a *loop* in the graph by connecting the tools node *back*
    to `assistant`:'
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们在图中创建了一个*循环*，通过将工具节点*回连*到`assistant`：
- en: '[PRE68]'
  id: totrans-591
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: And that's all it takes! Now when a request comes in, the LLM will reason about
    what to do. If it decides to call a tool, it'll put a tool call in its response,
    causing `tools_condition` to route to the `ToolNode` which executes the call.
    Since `ToolNode` has a direct edge to assistant, the LLM will get the updated
    list of messages with the appended `ToolMessage` and can again reason about what
    to do with the response.
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
  zh: 那就是全部！现在当收到请求时，LLM将推理出要做什么。如果它决定调用工具，它将在其响应中放置一个工具调用，导致`tools_condition`路由到执行调用的`ToolNode`。由于`ToolNode`有一个直接连接到`assistant`的边，LLM将获得带有附加`ToolMessage`的更新后的消息列表，并可以再次推理出如何处理响应。
- en: If the LLM decides it doesn't need to call any more tools—or that the next step
    in the process is to get some information from the user—it won't include any tool
    calls in its response, which means that `tools_condition` will route to `END`,
    and the final message will be displayed to the user.
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
  zh: 如果LLM决定不需要再调用任何工具，或者流程的下一步是从用户那里获取一些信息，它将不会在其响应中包含任何工具调用，这意味着`tools_condition`将路由到`END`，并将最终消息显示给用户。
- en: While that's all that's required to get the bot to work correctly, there's a
    last change we need to make in `graph.py` that's related to what gets shown to
    the customer in the frontend.
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这已经足够让机器人正确工作，但我们还需要在`graph.py`中做出最后一个更改，这与前端显示给客户的内容相关。
- en: 'As the above paragraphs hopefully make clear, the communication between the
    `assistant` and `tools` nodes happens through the `messages` variable in the graph
    state, and consists of internal messages of two types: `AIMessages` containing
    tool calls, and `ToolMessages` containing tool return values.'
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所述的段落所希望清楚地表明，`assistant`和`tools`节点之间的通信是通过图状态中的`messages`变量进行的，并包括两种类型的内部消息：包含工具调用的`AIMessages`和包含工具返回值的`ToolMessages`。
- en: Since we don't want to expose these internal messages to the user of our Streamlit
    app, we need to hide them when we pass the conversation history back. Recall that
    this history is relayed through the `get_history` method in `bot.py`, which calls
    the `get_conversation` method in `graph.py`
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们不希望将这些内部消息暴露给我们的Streamlit应用程序的用户，我们需要在将会话历史记录返回时隐藏它们。回想一下，这个历史记录是通过`bot.py`中的`get_history`方法传递的，该方法调用`graph.py`中的`get_conversation`方法。
- en: 'Let''s make the appropriate changes in `graph.py` to remove these messages:'
  id: totrans-597
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在`graph.py`中做出适当的更改以删除这些消息：
- en: '[PRE69]'
  id: totrans-598
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: (`chapter_10/in_progress_06/graph.py` in the GitHub repo)
  id: totrans-599
  prefs: []
  type: TYPE_NORMAL
  zh: （GitHub仓库中的`chapter_10/in_progress_06/graph.py`）
- en: First, we define `is_internal_message`, a static method that determines whether
    a message that we pass it is an "internal" one, i.e., one that's not fit to show
    the user. Above, we're defining an internal message as one that either has the
    type "`tool"`—which is true of `ToolMessages`—or has a `"tool_calls"` property
    (within `additional_kwargs`, a property that the LLM will use to set metadata
    within a message).
  id: totrans-600
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们定义`is_internal_message`，这是一个静态方法，用于确定我们传递给它的消息是否是“内部”的，即不适合显示给用户的消息。在上文中，我们将内部消息定义为具有类型"`tool"`——这是`ToolMessages`的情况——或者具有`"tool_calls"`属性（在`additional_kwargs`中，这是LLM将用于在消息中设置元数据的属性）。
- en: Then, rather than returning all the messages from the state in `get_conversation`,
    we now filter for the non-internal messages and only return those.
  id: totrans-601
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们不再从`get_conversation`状态中返回所有消息，而是现在只过滤非内部消息并只返回这些消息。
- en: With that out of the way, re-run the app and test out its new capabilities!
    Figure 10.21 shows an example.
  id: totrans-602
  prefs: []
  type: TYPE_NORMAL
  zh: 在完成这些之后，重新运行应用程序并测试其新功能！图10.21显示了示例。
- en: '![image](../Images/10__image021.png)'
  id: totrans-603
  prefs: []
  type: TYPE_IMG
  zh: '![image](../Images/10__image021.png)'
- en: Figure 10.21 Nibby can handle real-world actions like tracking and canceling
    orders (see chapter_10/in_progress_06 in the GitHub repo for the full code).
  id: totrans-604
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图10.21 Nibby可以处理现实世界的操作，如跟踪和取消订单（完整代码请见GitHub仓库中的chapter_10/in_progress_06）。
- en: By allowing Nibby to access external tools, we've given it superpowers and saved
    Note n' Nib's customer support division some serious time!
  id: totrans-605
  prefs: []
  type: TYPE_NORMAL
  zh: 通过允许Nibby访问外部工具，我们赋予了它超能力，并为Note n' Nib的客户支持部门节省了大量时间！
- en: This has been our most advanced app yet. If you've been following along and
    working on these projects yourself, you probably appreciate that the more complex
    an app becomes, the more things can go wrong in the real world when users actually
    start interacting with it. In the next chapter, we'll explore how to catch these
    proble
  id: totrans-606
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们迄今为止最先进的程序。如果你一直在跟进并自己从事这些项目，你可能很欣赏这样一个事实：应用越复杂，当用户真正开始与之互动时，在现实世界中可能出错的事情就越多。在下一章中，我们将探讨如何捕捉这些问题。
- en: ms beforehand and test your app to make it as robust as possible.
  id: totrans-607
  prefs: []
  type: TYPE_NORMAL
  zh: 提前`ms`并测试你的应用，使其尽可能健壮。
- en: 10.7 Summary
  id: totrans-608
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.7 摘要
- en: Real-world AI applications require advanced capabilities, such as knowledge
    retrieval and action execution.
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现实世界的AI应用需要高级功能，例如知识检索和动作执行。
- en: LangGraph structures AI workflows as graphs; nodes represent steps in the AI
    process, and edges define the flow between them.
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LangGraph将AI工作流程结构化为图；节点代表AI过程中的步骤，边定义它们之间的流动。
- en: Each node in LangGraph takes in the graph state and modifies it.
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LangGraph中的每个节点都接收图状态并对其进行修改。
- en: '`st.chat_input` renders a textbox where users can type messages.'
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`st.chat_input`渲染一个文本框，用户可以在其中输入消息。'
- en: '`st.chat_message` displays human and AI messages appropriately.'
  id: totrans-613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`st.chat_message`适当地显示人类和AI的消息。'
- en: LangGraph uses checkpointers to persist information across multiple graph executions.
  id: totrans-614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LangGraph使用检查点器来在多个图执行之间持久化信息。
- en: It's important to instruct the LLM explicitly to stay on-topic and ignore irrelevant
    requests; the system message is a good place for this.
  id: totrans-615
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 明确指导LLM保持主题并忽略不相关的请求是很重要的；系统消息是这样一个好地方。
- en: Embeddings involve converting an object—such as a piece of text—into lists of
    numbers called vectors to find relevant content using similarity search.
  id: totrans-616
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 嵌入涉及将对象（如文本）转换为称为向量的数字列表，以使用相似性搜索找到相关内容。
- en: Retrieval Augmented Generation (RAG) is a technique for providing custom knowledge
    to a pre-trained LLM by retrieving chunks of context relevant to the user query
    from a vector database like Pinecone.
  id: totrans-617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检索增强生成（RAG）是一种技术，通过从Pinecone这样的向量数据库中检索与用户查询相关的上下文块，为预训练的LLM提供定制知识。
- en: A *tool* is simply a well-documented function that an LLM can choose to call
    in response to a user query.
  id: totrans-618
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*工具*是一个经过良好文档化的函数，LLM可以选择在响应用户查询时调用。'
- en: An agentic app—or simply an *agent*—is one that can use tools to interact with
    the real world.
  id: totrans-619
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个代理应用——或者简单地称为*代理*——是那种可以使用工具与真实世界互动的应用。
- en: LangGraph makes it extremely easy to write agentic apps through its `ToolNode`
    and `tools_condition` abstractions.
  id: totrans-620
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LangGraph通过其`ToolNode`和`tools_condition`抽象，使得编写代理应用变得极其简单。
