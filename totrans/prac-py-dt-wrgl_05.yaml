- en: Chapter 5\. Accessing Web-Based Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The internet is an incredible source of data; it is, arguably, the reason that
    data has become such a dominant part of our social, economic, political, and even
    creative lives. In [Chapter 4](ch04.html#chapter4), we focused our data wrangling
    efforts on the process of accessing and reformatting file-based data that had
    already been saved to our devices or to the cloud. At the same time, much of it
    came from the internet originally—whether it was downloaded from a website, like
    the unemployment data, or retrieved from a URL, like the Citi Bike data. Now that
    we have a handle on how to use Python to parse and transform a variety of file-based
    data formats, however, it’s time to look at what’s involved in collecting those
    files in the first place—especially when the data they contain is of the real-time,
    feed-based variety. To do this, we’re going to spend the bulk of this chapter
    learning how to get ahold of data made available through APIs—those *a*pplication
    *p*rogramming *i*nterfaces I mentioned early in [Chapter 4](ch04.html#chapter4).
    APIs are the primary (and sometimes only) way that we can access the data generated
    by real-time or on-demand services like social media platforms, streaming music,
    and search services—as well as many other private and public (e.g., government-generated)
    data sources.
  prefs: []
  type: TYPE_NORMAL
- en: While the many benefits of APIs (see [“Why APIs?”](#why_apis) for a refresher)
    make them a popular resource for data-collecting companies to offer, there are
    significant costs and risks to doing so. For advertising-driven businesses like
    social media platforms, an outside product or project that is too comprehensive
    in its data collection is a profit risk. The ready availability of so much data
    about individuals has also significantly [increased privacy risks](https://dataprivacylab.org/projects/kanonymity/kanonymity.pdf).
    As a result, accessing data via many APIs requires registering with the data collector
    in advance, and even completing a code-based login or *authentication* process
    anytime you request data. At the same time, the data accessibility that APIs offer
    is a powerful tool for improving the transparency of government systems^([1](ch05.html#idm45143408861008))
    and accountability for private companies,^([2](ch05.html#idm45143408859632)) so
    the up-front work of creating an account and protecting any Python scripts you
    make that access API-based data is well worth the effort.
  prefs: []
  type: TYPE_NORMAL
- en: 'Over the course of this chapter, we’ll cover how to access a range of web-based,
    feed-type datasets via APIs, addressing everything from basic, no-login-required
    resources all the way to the multistep, highly protected APIs of social media
    platforms like Twitter. As we’ll see in [“Accessing Online XML and JSON”](#online_xml_and_json),
    the simpler end of this spectrum just involves using the Python *requests* library
    to download a web page already formatted as JSON or XML—all we need is the URL.
    In [“Specialized APIs: Adding Basic Authentication”](#basic_authentication), we’ll
    move on to the process of accessing data made available through the [Federal Reserve
    Economic Database (FRED)](https://fred.stlouisfed.org/docs/api/fred) API. This
    is the same data we looked at in Examples [4-12](ch04.html#xml_parsing) and [4-15](ch04.html#json_parsing),
    but rather than working with example files that I’ve provided, you’ll be programmatically
    downloading whatever data is most recent *whenever you run the script*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This will require both creating a login on the FRED website as well as creating—and
    protecting—your own basic API “key” in order to retrieve data. Finally, in [“Specialized
    APIs: Working With OAuth”](#oauth_apis) we’ll cover the more complex API authentication
    process required for social media platforms like Twitter. Despite the degree of
    up-front work involved, learning how to programmatically interact with APIs like
    this has big payoffs—for the most part, you’ll be able to rerun these scripts
    at any time to retrieve the most up-to-date data these services offer.^([3](ch05.html#idm45143408848400))
    Of course, since not every data source we need offers an API, we’ll wrap up the
    chapter with [“Web Scraping: The Data Source of Last Resort”](#web_scraping) by
    explaining how we can use code to *responsibly* “scrape” data from websites with
    the *Beautiful Soup* Python library. Though in many cases these data-access tasks
    *could* be accomplished with a browser and mouse, you’ll quickly see how using
    Python helps lets us scale our data-retrieval efforts by making the process faster
    and more repeatable.'
  prefs: []
  type: TYPE_NORMAL
- en: Accessing Online XML and JSON
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In [“Wrangling Feed-Type Data with Python”](ch04.html#wrangling_feed_data),
    we explored the process of accessing and transforming two common forms of web-based
    data: XML and JSON. What we didn’t address, however, was how to actually get those
    data files from the internet onto your computer. With the help of the versatile
    Python *requests* library, however, it only take a few lines of code to access
    and download that data without ever having to open a web browser.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For the sake of comparison, let’s start by “manually” downloading two of the
    files we’ve used in previous examples: the BBC’s RSS feed of articles from [Example 4-13](ch04.html#bbc_example)
    and the Citi Bike JSON data mentioned in [“One Data Source, Two Ways”](ch04.html#one_data_two_ways).'
  prefs: []
  type: TYPE_NORMAL
- en: 'For both of these data sources, the process is basically the same:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Visit the target URL; in this case, one of the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[*http://feeds.bbci.co.uk/news/science_and_environment/rss.xml*](http://feeds.bbci.co.uk/news/science_and_environment/rss.xml)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*https://gbfs.citibikenyc.com/gbfs/en/station_status.json*](https://gbfs.citibikenyc.com/gbfs/en/station_status.json)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Context-click (also known as “right-click” or sometime “Ctrl+click,” depending
    on your system). From the menu that appears, simply choose “Save As” and save
    the file to the same folder where your Jupyter notebook or Python script is located.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: That’s it! Now you can run the scripts from [Example 4-13](ch04.html#bbc_example)
    on that updated XML file or paste the Citi Bike JSON data into [*https://jsonlint.com*](https://jsonlint.com)
    to see what it looks like when it’s properly formatted. Note that even though
    the BBC page looks almost like a “normal” website in your browser, true to its
    *.xml* file extension, it downloads as well-formatted XML.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve seen how to do this part of the process by hand, let’s see what
    it takes to do the same thing in Python. To keep this short, the code in [Example 5-1](#data_download)
    will download and save *both* files, one after the other.
  prefs: []
  type: TYPE_NORMAL
- en: Example 5-1\. data_download.py
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Pretty simple, right? Apart from different filenames, the *.xml* and *.json*
    files produced by [Example 5-1](#data_download) are exactly the same as the ones
    we saved manually from the web. And once we have this script set up, of course,
    all we have to do to get the latest data is run it again and the new data will
    overwrite the earlier files.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing APIs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Up until this point, most of our data wrangling work has focused on data sources
    whose contents are almost entirely controlled by the data provider. In fact, while
    the contents of spreadsheet files, and documents—and even the web pages containing
    XML and JSON that we accessed just now in [Example 5-1](#data_download)—may change
    based on *when* we access them, we don’t really have any influence on *what* data
    they contain.
  prefs: []
  type: TYPE_NORMAL
- en: At the same time, most of us are used to using the internet to get information
    that’s much more tailored to our needs. Often our first step when looking for
    information is to enter keywords or phrases into a search engine, and we expect
    to receive a list of highly customized “results” based (at least in part) on our
    chosen combination of terms. Sure, we can’t control what web pages are actually
    out there for our search to retrieve, but this process is so common—and so useful—for
    most of us that we rarely stop to think about what is happening behind the scenes.
  prefs: []
  type: TYPE_NORMAL
- en: Despite their visually oriented interfaces, search engines are actually just
    a special instance of APIs. They are essentially just web pages that let you *interface*
    with a database containing information about websites on the internet, such as
    their URLs, titles, text, images, videos, and more. When you enter your search
    terms and hit Enter or Return, the search engine *queries* its database for web
    content that “matches” your search in some respect and then updates the web page
    you’re looking at to display those results in a list. Though the specialized APIs
    made available by social media platforms and other online services require us
    to authenticate *and* structure our searches in a very particular way, there are
    enough features shared between search engines and more specialized APIs that we
    can learn something useful about APIs by deconstructing a basic Google search.
  prefs: []
  type: TYPE_NORMAL
- en: 'Basic APIs: A Search Engine Example'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Though an internet search engine is probably the most straightforward form of
    API around, that’s not always obvious from the way we see them behave onscreen.
    For example, if you were to visit Google and search for “weather sebastopol,”
    you would probably see a page that looks something like [Figure 5-1](#google_search_results).
  prefs: []
  type: TYPE_NORMAL
- en: '![Sebastopol weather search results](assets/ppdw_0501.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-1\. Sample search results
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'While the format of the search results is probably pretty familiar, right now
    let’s take a closer look at what’s happening in the URL bar. What you see will
    definitely be different from the [Figure 5-1](#google_search_results) screenshot,
    but it should contain at least some of the same information. Specifically, look
    through the text that now appears in *your* URL bar to find the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Found it? Great. Now without refreshing the page, change the text in the search
    box to “weather san francisco” and hit Enter. Once again look through the text
    in the URL to find:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, copy and paste the following into your URL bar and hit Enter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Notice anything? Hopefully, you’re seeing the same (or *almost* the same) search
    results when you type “weather san francisco” into Google’s search bar and hit
    Enter as when you directly visit the Google search URL with the key/value pair
    of `q=weather+san+francisco` appended (e.g., `https://www.google.com/search?q=weather+san+francisco`).
    That’s because `q=weather+san+francisco` is the part of the *query string* that
    delivers your actual search terms to Google’s database; everything else is just
    additional information Google tacks on for customization or tracking purposes.
  prefs: []
  type: TYPE_NORMAL
- en: 'While Google can (and will!) add whatever it wants to our search URL, *we*
    can also add other useful key/value pairs. For example, in [“Smart Searching for
    Specific Data Types”](ch04.html#smart_searching), we looked at searching for specific
    file types, such as *.xml*, by adding `filetype: .xml` to our search box query;
    we can do the same thing directly in the URL bar by adding the corresponding key/value
    pair of `as_filetype=xml` to our query string:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Not only will this return results in the correct format, but notice that it
    updates the contents of the search box as well!
  prefs: []
  type: TYPE_NORMAL
- en: The behavior of the Google search engine in this situation is almost identical
    to what we’ll see with more specialized APIs in the remainder of this chapter.
    Most APIs follow the general structure we’re seeing in this search example, where
    an *endpoint* (in this case `https://www.google.com/search`) is combined with
    one or more *query parameters* or *key/value pairs* (such as `as_filetype=xml`
    or `q=weather+san+francisco`), which comprise the *query string* that is appended
    after the question mark (`?`). A general overview of API endpoint and query string
    structure is shown in [Figure 5-2](#query_string_structure).
  prefs: []
  type: TYPE_NORMAL
- en: '![Sebastopol weather search query, limited to 5 results](assets/ppdw_0502.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-2\. Basic query string structure
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'While this structure is pretty universal, here are a couple of other useful
    tips about query string-based APIs:'
  prefs: []
  type: TYPE_NORMAL
- en: Key/value pairs (such as `as_filetype=xml`, `num=5`, or even `q=weather+san+francisco`)
    can appear *in any order*, as long as they are added after the question mark (`?`)
    that indicates the start of the query string.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The particular keys and values that are meaningful for a given API are determined
    by the API provider and can only be identified by reading the API documentation,
    or through experimentation (though this can present problems of its own). Anything
    appended to the query string that is not a recognized key or valid parameter value
    will probably be ignored.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While these characteristics are common to almost all APIs, the vast majority
    of them will not allow you to access any data at all without first identifying
    (or *authenticating*) yourself by creating a login and providing unique, specialized
    “keys” to the API along with your queries. This part of the API process is what
    we’ll turn to next.
  prefs: []
  type: TYPE_NORMAL
- en: 'Specialized APIs: Adding Basic Authentication'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first step in using most APIs is creating some kind of account with the
    API provider. Although many APIs allow *you* to use them for free, the process
    of compiling, storing, searching for, and returning data to you over the internet
    still presents risks and costs money, so providers want to track who is using
    their APIs and be able to cut off your access if they want to.^([4](ch05.html#idm45143408324000))
    This first part of the *authentication* process usually consists of creating an
    account and requesting an API “key” for yourself and/or each project, program,
    or “app” that you plan to have interact with the API. In a “basic” API authentication
    process, like the one we’ll go through now, once you’ve created your API key on
    the service provider’s website, all you need to do to retrieve data successfully
    is append your key to your data request just like any other query parameter, and
    you’re all set.
  prefs: []
  type: TYPE_NORMAL
- en: As an example, let’s get set up to programmatically access the unemployment
    data we worked with in [Example 4-15](ch04.html#json_parsing). We’ll start by
    making an account on the FRED website and requesting an API key. Once we have
    that, we can just append it to our query string and start downloading data!
  prefs: []
  type: TYPE_NORMAL
- en: Getting a FRED API Key
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To create an account with the Federal Reserve Economic Database (FRED), visit
    [*https://fred.stlouisfed.org*](https://fred.stlouisfed.org) and click on My Account
    in the upper-righthand corner, as shown in [Figure 5-3](#fred_login).
  prefs: []
  type: TYPE_NORMAL
- en: '![Federal Reserve Economic Database (FRED) homepage](assets/ppdw_0503.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-3\. FRED login link
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Follow the directions in the pop-up, either creating an account with a new username
    and password or using your Google account to log in. Once your registration/login
    process is complete, clicking on the My Account link will open a drop-down menu
    that includes an option titled API Keys, as shown in [Figure 5-4](#fred_account_option).
  prefs: []
  type: TYPE_NORMAL
- en: '![FRED account actions](assets/ppdw_0504.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-4\. FRED account actions
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Clicking that link will take you to a page where you can request one or more
    API keys using the Request API Key button. On the next page, you’ll be asked to
    provide a brief description of the application with which the API key will be
    used; this can just be a sentence or two. You’ll also need to read and agree to
    the Terms of Service by checking the provided box. Complete the process by clicking
    the Request API Key button.
  prefs: []
  type: TYPE_NORMAL
- en: If your request is successful (and it should be), you’ll be taken to an interim
    page that will display the key that’s been generated. If you leave that page,
    you can always just log in and visit [*https://research.stlouisfed.org/useraccount/apikeys*](https://research.stlouisfed.org/useraccount/apikeys)
    to see all of your available API keys.
  prefs: []
  type: TYPE_NORMAL
- en: Using Your API key to Request Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that you have an API key, let’s explore how to request the data we used
    in [Example 4-15](ch04.html#json_parsing). Start by trying to load the following
    URL in a browser:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Even if you’re already logged in to FRED on that browser, you’ll see something
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'This is a pretty descriptive error message: it not only tells you that something
    went wrong, but it gives you some idea of how to fix it. Since you just created
    an API key, all you have to do is add it to your request as an additional parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'replacing `YOUR_API_KEY_HERE` with, of course, your API key. Loading that page
    in a browser should return something that looks something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Pretty nifty, right? Now that you know how to use your API key to make data
    requests, it’s time to review how to both customize those requests *and* protect
    your API key when you use it in Python scripts.
  prefs: []
  type: TYPE_NORMAL
- en: Reading API Documentation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As you can see from the preceding example, once we have an API key, we can load
    the latest data from the FRED database whenever we want. All we need to do is
    construct our query string and add our API key.
  prefs: []
  type: TYPE_NORMAL
- en: But how do we know what key/value pairs the FRED API will accept and what type
    of information they’ll return? The only really reliable way to do this is to read
    the API *documentation*, which should offer guidance and (hopefully) examples
    of how the API can be used.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, there’s no widely adopted standard for API documentation, which
    means that using a new API is almost always something of a trial-and-error process,
    especially if the documentation quality is poor or the provided examples don’t
    include the information you’re looking for. In fact, even *finding* the documentation
    for a particular API isn’t always straightforward, and often a web search is the
    simplest route.
  prefs: []
  type: TYPE_NORMAL
- en: For example, getting to the FRED API documentation from the [FRED homepage](https://fred.stlouisfed.org)
    requires clicking on the Tools tab about halfway down the page, then selecting
    the Developer API link at the bottom right, which takes you to [*https://fred.stlouisfed.org/docs/api/fred*](https://fred.stlouisfed.org/docs/api/fred).
    By contrast, a web search for “fred api documentation” will take you to the same
    page, shown in [Figure 5-5](#fred_api_docs_main), directly.
  prefs: []
  type: TYPE_NORMAL
- en: '![FRED API documentation homepage](assets/ppdw_0505.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-5\. FRED API documentation page
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Unfortunately, the list of links on this page is actually a list of *endpoints*—different
    base URLs that you can use to request more specific information (recall that the
    *endpoint* is everything before the question mark (`?`), which separates it from
    the *query string*). In the preceding example, you used the endpoint `https://api.stlouisfed.org/fred/series/observations`
    and then paired it with the key/value pairs of `series_id=U6RATE`, `file_type=json`,
    and, of course, your API key in order to generate a response.
  prefs: []
  type: TYPE_NORMAL
- en: Scrolling down the page in [Figure 5-5](#fred_api_docs_main) and clicking on
    the documentation link labeled “fred/series/observations” will take you [*https://fred.stlouisfed.org/docs/api/fred/series_observations.html*](https://fred.stlouisfed.org/docs/api/fred/series_observations.html),
    which outlines all of the valid query keys (or *parameters*) for that particular
    endpoint, as well as the valid values those keys can have and some sample query
    URLs, as shown in [Figure 5-6](#fred_api_observations_docs).
  prefs: []
  type: TYPE_NORMAL
- en: '![FRED API ''observations'' endpoint documentation](assets/ppdw_0506.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-6\. FRED API *observations* endpoint documentation
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: For example, you could limit the number of observations returned by using the
    `limit` parameter, or reverse the sort order of the returned results by adding
    `sort_order=desc`. You can also specify particular data formats (such as `file_type=xml`
    for XML output) or units (such as `units=pc1` to see the output as percent change
    from a year ago).
  prefs: []
  type: TYPE_NORMAL
- en: Protecting Your API Key When Using Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As you may have already guessed, downloading data from FRED (or other, similar
    APIs) is as simple as replacing one of the URLs in [Example 5-1](#data_download)
    with your complete query, because the web page it generates is just another JSON
    file on the internet.
  prefs: []
  type: TYPE_NORMAL
- en: 'At the same time, that query contains some especially sensitive information:
    your API key. Remember that as far as FRED (or any other API owner) is concerned,
    you are responsible for any activity on their platform that uses your API key.
    This means that while you *always* want to be documenting, saving, and versioning
    your code with something like Git, you *never* want your API keys or other credentials
    to end up in a file that others can access.'
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Properly protecting your API key takes some effort, and if you’re new to working
    with data, Python, or APIs (or all three), you may be tempted to skip the next
    couple of sections and just leave your API credentials inside files that could
    get uploaded to the internet.^([5](ch05.html#idm45143408221168)) Don’t! While
    right now you may be thinking, “Who’s ever going to bother looking at *my* work?”
    or “I’m just playing around anyway—what difference does it make?” there are two
    things you should know.
  prefs: []
  type: TYPE_NORMAL
- en: First, as with documentation, if you don’t deal with protecting your credentials
    correctly now, it will be *much* more difficult and time-consuming to do so later,
    in part because by then you’ll have forgotten what exactly is involved, and in
    part because *it may already be too late*. Second, while few of us feel that what
    we’re doing is “important” or visible enough that anyone *else* would bother looking
    at it, the reality is that bad actors don’t mind who their scapegoat is—and if
    you make it easy, they might choose you. The fallout, moreover, might not be limited
    to getting you kicked off of a data platform. In 2021, the former SolarWinds CEO
    claimed that the massive breach of thousands of high-security systems through
    compromises to the company’s software was made possible, in part, because of a
    weak password that was uploaded to a file on an intern’s personal GitHub account.^([6](ch05.html#idm45143408216624))
    In other words, even if you’re “just practicing,” you’re better off practicing
    good security in the first place.
  prefs: []
  type: TYPE_NORMAL
- en: 'Protecting your API credentials is a two-part process:'
  prefs: []
  type: TYPE_NORMAL
- en: You need to separate your API key or other sensitive information from the rest
    of your code. We’ll do this by storing these credentials in a separate file that
    our main code only loads when the script is actually run.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You need a reliable way to ensure that as you are backing up your code using
    Git, for example, those credential files are *never* backed up to any online location.
    We’ll accomplish this by putting the word `credentials` in the name of any file
    that includes them and then using a `gitignore` file to make sure they don’t get
    uploaded to GitHub.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The simplest way to achieve both of these things consistently is to define a
    naming convention for any file that contains API keys or other sensitive login-related
    information. In this case, we’ll make sure that any such file has the word `credentials`
    somewhere in the filename. We’ll then make sure to create or update a special
    type of Git file known as a *.gitignore*, which stores rules for telling Git which
    files in our repo folder should *never* be committed to our repository and/or
    uploaded to GitHub. By including a rule for our “credentials” file to *.gitignore*,
    we guarantee that no files containing sensitive login information get uploaded
    to GitHub by accident.
  prefs: []
  type: TYPE_NORMAL
- en: Creating Your “Credentials” File
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Up until now, we’ve been putting all our code for a particular task—such as
    downloading or transforming a data file—into a single Python file or notebook.
    For the purposes of both security and reuse, however, when we’re working with
    APIs, it makes much more sense to separate our functional code from our credentials.
    Luckily, this process is very straightforward.
  prefs: []
  type: TYPE_NORMAL
- en: First, create and save a new, empty Python file called *FRED_credentials.py*.
    For simplicity’s sake, go ahead and put this file in the same folder where you
    plan to put the Python code you’ll use to download data from FRED.
  prefs: []
  type: TYPE_NORMAL
- en: Then, simply create a new variable and set its value to your own API key, as
    shown in [Example 5-2](#FRED_credentials_example).
  prefs: []
  type: TYPE_NORMAL
- en: Example 5-2\. Example FRED credentials file
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Now just save your file!
  prefs: []
  type: TYPE_NORMAL
- en: Using Your Credentials in a Separate Script
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that your API key exists as a variable in another file, you can import it
    into any file where you want to use it, using the same method we’ve used previously
    to import libraries created by others. [Example 5-3](#FRED_download_api_key_import)
    is a sample script for downloading the U6 unemployment data from FRED using the
    API key stored in my *FRED_credentials.py* file.
  prefs: []
  type: TYPE_NORMAL
- en: Example 5-3\. FRED_API_example.py
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_accessing_web_based_data_CO1-1)'
  prefs: []
  type: TYPE_NORMAL
- en: We can make our API available to this script by using the `from` keyword with
    the name of our credentials file (notice that we *don’t* include the *.py* extension
    here) and then telling it to `import` the variable that contains our API key.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve succeeded in separating our API credentials from the main part
    of our code, we need to make sure that our credentials file doesn’t accidentally
    get backed up when we `git commit` our work and/or `git push` it to the internet.
    To do this simply and systematically, we’ll make use of a special type of file
    known as *.gitignore*.
  prefs: []
  type: TYPE_NORMAL
- en: Getting Started with .gitignore
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As the name suggests, a *.gitignore* file lets you specify certain types of
    files that—surprise, surprise!—you want Git to “ignore,” rather than track or
    back up. By creating (or modifying) the pattern-matching rules in the *.gitignore*
    file for a repository, we can predefine which types of files our repo will track
    or upload. While we could *theoretically* accomplish the same thing manually—by
    never using `git add` on files we don’t want to track—using a *.gitignore* file
    enforces this behavior^([8](ch05.html#idm45143408008560)) *and* prevents Git from
    “warning” us that we have untracked files every time we run `git status`. *Without*
    a *.gitignore* file, we would have to confirm which files we want to ignore every
    time we commit—which would quickly get tedious and easily lead to mistakes. All
    it would take is one hasty `git add -A` command to accidentally begin tracking
    our sensitive credentials file(s)—and getting things *out* of your Git history
    is much trickier than getting them in. Much better to avoid the whole problem
    with a little preparation.
  prefs: []
  type: TYPE_NORMAL
- en: In other words, *.gitignore* files are our friend, letting us create general
    rules that prevent us from accidentally tracking files we don’t want to, and by
    making sure that Git only reports the status of files that we genuinely care about.
  prefs: []
  type: TYPE_NORMAL
- en: For the time being, we’ll create a new *.gitignore* file in the same folder/repository
    where our *FRED_credentials.py* file is, just to get a feel for how they work.^([9](ch05.html#idm45143407997552))
    To do this, we’ll start by opening up a new file in Atom (or you can add a new
    file directly in your GitHub repo) and saving it in the same folder as your *FRED_credentials.py*
    with the name *.gitignore* (be sure to start the filename with a dot (`.`)—that’s
    important!).
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, add the following lines to your file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: As in Python, comments in *.gitignore* files are started with a hash (`#`) symbol,
    so the first line of this file is just descriptive. The contents of the second
    line (`**credentials*`) is a sort of *regular expression*—a special kind of pattern-matching
    system that lets us describe strings (including filenames) in the sort of generic
    way we might explain them to another person.^([10](ch05.html#idm45143407977504))
    In this case, the expression `**credentials*` translates to “a file anywhere in
    this repository that contains the word *credentials*.” By adding this line to
    our *.gitignore* file, we ensure that any file in this repository whose filename
    includes the word *credentials* will never be tracked or uploaded to GitHub.
  prefs: []
  type: TYPE_NORMAL
- en: 'To see your *.gitignore* in action, save the file, and then in the command
    line, run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'While you should see the new file you created for the code in [Example 5-3](#FRED_download_api_key_import),
    you should *not* see your *FRED_credentials.py* file listed as “untracked.” If
    you want to be really sure that the files you intend to be ignored are, in fact,
    being ignored, you can also run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: which will show you *only* the files in your repository that are currently being
    ignored. Among them you’ll probably also see the *__pycache__* folder, which we
    also don’t need to back up.
  prefs: []
  type: TYPE_NORMAL
- en: 'Specialized APIs: Working With OAuth'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So far, we have everything we need to work with APIs that have what’s often
    described as a “basic” authentication process: we create an account with the API
    provider and are given a key that we append to our data request, just as we did
    in [Example 5-3](#FRED_download_api_key_import).'
  prefs: []
  type: TYPE_NORMAL
- en: 'While this process is very straightforward, it has some drawbacks. These days,
    APIs can do much more than just return data: they are also the way that apps post
    updates to a social media account or add items to your online calendar. To make
    that possible, they obviously need some type of access to your account—but of
    course you don’t want to be sharing your login credentials with apps and programs
    willy-nilly. If you did just give apps that information, the only way to later
    *stop* an app from accessing your account would be to change your username and
    password, and then you’d have to give your updated credentials to all the apps
    you still *want* to use in order for them to continue working…it gets messy and
    complicated, fast.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The OAuth authentication workflow was designed to address these problems by
    providing a way to provide API access without passing around a bunch of usernames
    and passwords. In general, this is achieved by scripting a so-called *authorization
    loop*, which includes three basic steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Obtaining and encoding your API key and “secret” (each of which is just a string
    that you get from the API provider—just as we did with the FRED API key).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sending an encoded combination of those two strings as (yet another) “key” to
    a special “authorization” endpoint/URL.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Receiving an *access token* (yet another string) from the authorization endpoint.
    The access token is what you actually then send along to the API’s data endpoint
    along with your query information.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: While this probably sounds convoluted, rest assured that, in practice, even
    this complex-sounding process is mostly about passing strings back and forth to
    and from certain URLs in a certain order. Yes, in the process we’ll need to do
    some “encoding” on them, but as you may have guessed, that part will be handled
    for us by a handy Python library.
  prefs: []
  type: TYPE_NORMAL
- en: 'Despite needing to complete an authorization loop and retrieve an access token,
    the process of interacting with even these more specialized APIs via Python is
    essentially the same as what we saw in [“Specialized APIs: Adding Basic Authentication”](#basic_authentication).
    We’ll create an account, request API credentials, and then create a file that
    both contains those credentials and does a little bit of preparatory work to them
    so we can use them in our main script. Then our main script will pull in those
    credentials and use them to request data from our target platform and write it
    to an output file.'
  prefs: []
  type: TYPE_NORMAL
- en: For this example, we’ll be working with the Twitter API, but you’ll be able
    to use roughly the same process for other platforms (like Facebook) that use an
    OAuth approach. One thing we *won’t* do here is spend much time discussing how
    to structure specific queries, since the ins and outs of any given API could easily
    fill a book in and of itself! That said, once you have this authentication process
    down, you’ll have what you need to start experimenting with a whole range of APIs
    and can start practicing with them in order to access the data you want. Let’s
    get started!
  prefs: []
  type: TYPE_NORMAL
- en: Applying for a Twitter Developer Account
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As it will be with (almost) every new API we want to use, our first step will
    be to request an API key from Twitter. Even if you already have a Twitter account,
    you’ll need to apply for “developer access,” which will take about 15 minutes
    (not counting the time for Twitter to review and/or approve), all told. Start
    by visiting [the Twitter Developer API “Apply for Access” page](https://developer.twitter.com/en/apply-for-access),
    and click the “Apply for a developer account” button. Once you’ve logged in, you’ll
    be prompted for more information about how you plan to use the API. For this exercise,
    you can select “Hobbyist” and “Exploring the API,” as shown in [Figure 5-7](#twitter_api_use_case).
  prefs: []
  type: TYPE_NORMAL
- en: '![Use case selection for the Twitter developer API](assets/ppdw_0507.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-7\. Twitter developer API use case selection
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'In the next step, you’ll be asked to provide a 200+ character explanation of
    what you plan to use the API for; here you can enter something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: Using the Twitter API to learn more about doing data wrangling with Python.
    Interested in experimenting with OAuth loops and pulling different kinds of information
    from public Twitter feeds and conversations.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Since our goal here is just to practice downloading data from Twitter using
    a Python script and an OAuth loop, you can toggle the answer to the four subsequent
    questions to “No,” as shown in [Figure 5-8](#twitter_api_explanation), though
    if you begin using the API in some other way, you will need to update these answers.
  prefs: []
  type: TYPE_NORMAL
- en: '![Intended use selection for data from the Twitter developer API](assets/ppdw_0508.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-8\. Twitter developer API intended uses
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: On the next two screens, you’ll review your previous selections and click a
    checkbox to acknowledge the Developer Agreement. You then click “Submit application,”
    which will trigger a verification email. If the email doesn’t arrive in your inbox
    within a few minutes, be sure to check your Spam and Trash. Once you locate it,
    click on the link in the email for your access to be confirmed!
  prefs: []
  type: TYPE_NORMAL
- en: Creating Your Twitter “App” and Credentials
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once your developer access has been approved by Twitter, you can create a new
    “app” by logging in to your Twitter account and visiting [*https://developer.twitter.com/en/portal/projects-and-apps*](https://developer.twitter.com/en/portal/projects-and-apps).
    In the center of the page, click the Create Project button.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here you’ll be taken through a mini version of the process to apply for developer
    access: you’ll need to provide a name for your project, indicate how you intend
    to use the Twitter API, describe that purpose in words, and provide a name for
    the first app associated with that project, as shown in Figures [5-9](#twitter_project_name)
    through [5-12](#twitter_app_name).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Twitter project name screen](assets/ppdw_0509.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5-9\. Twitter project creation: project name'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![Twitter project purpose screen](assets/ppdw_0510.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5-10\. Twitter project creation: project purpose'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![Twitter project description screen](assets/ppdw_0511.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5-11\. Twitter project creation: project description'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![Twitter app name](assets/ppdw_0512.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5-12\. Twitter project creation: app name'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Once you’ve added your app name, you’ll see a screen that shows your API key,
    API secret key, and Bearer token, as shown in [Figure 5-13](#twitter_keys_and_tokens).^([11](ch05.html#idm45143407870720))
  prefs: []
  type: TYPE_NORMAL
- en: '![API Keys and tokens screen](assets/ppdw_0513.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-13\. Twitter API keys and tokens screen
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: For security reasons, you’ll only be able to view your API key and API secret
    key *on this screen*, so we’re going to put them into a file for our Twitter credentials
    right away (note that in other places in the developer dashboard these are referred
    to as the “API Key” and “API Key Secret”—even big tech companies can have trouble
    with consistency!). Don’t worry, though! If you accidentally click away from this
    screen too soon, miscopy a value, or anything else, you can always go back to
    [your dashboard](https://developer.twitter.com/en/portal/dashboard) and click
    on the key icon next to your app, as shown in [Figure 5-14](#twitter_project_app_list).
  prefs: []
  type: TYPE_NORMAL
- en: '![Twitter Dashboard with Apps](assets/ppdw_0514.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-14\. Twitter dashboard with apps
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Then, just click “Regenerate” under “Consumer Keys” to get a new API Key and
    API Key Secret, as shown in [Figure 5-15](#twitter_regenerate_keys).
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know how to access the API Key and API Key Secret for our app, we
    need to put these into a new “credentials” file, similar to the one we created
    for our FRED API key. To do this, create a new file called *Twitter_credentials.py*
    and save it in the folder where you want to put your Python script for accessing
    Twitter data, as shown in [Example 5-4](#Twitter_credentials_example).
  prefs: []
  type: TYPE_NORMAL
- en: Example 5-4\. Twitter_credentials.py
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '![Twitter Regenerate Keys](assets/ppdw_0515.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-15\. Twitter regenerate keys
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Be sure to include the word `credentials` in the name of the file where you
    store your Twitter API Key and API Key Secret! Recall that in [“Getting Started
    with .gitignore”](#using_gitignore), we created a rule that ignores any file whose
    name includes the word `credentials` to make sure our API keys never get uploaded
    to GitHub by accident. So make sure to double-check the spelling in your filename!
    Just to be extra sure, you can always run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: in the command line to confirm that all of your credentials files are indeed
    being ignored.
  prefs: []
  type: TYPE_NORMAL
- en: Encoding Your API Key and Secret
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far, we haven’t needed to do things *too* differently than we did for the
    FRED API; we’ve just had to create two variables in our credentials file (`my_Twitter_key`
    and `my_Twitter_secret`) instead of one.
  prefs: []
  type: TYPE_NORMAL
- en: Now, however, we need to do a little bit of work on these values to get them
    in the right format for the next step of the authentication process. While I won’t
    go too far into the details of what’s happening “under the hood” in these next
    few steps, just know that these coding and decoding steps are necessary for protecting
    the raw string values of your API Key and API Key Secret so that they can be safely
    sent over the internet.
  prefs: []
  type: TYPE_NORMAL
- en: So, to our *Twitter_credentials.py* file we’re now going to add several lines
    of code so that the completed file looks like [Example 5-5](#complete_Twitter_credentials).
  prefs: []
  type: TYPE_NORMAL
- en: Example 5-5\. Twitter_credentials.py
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_accessing_web_based_data_CO2-1)'
  prefs: []
  type: TYPE_NORMAL
- en: This library will let us transform our raw API Key and API Key Secret into the
    correct format for sending to the Twitter authorization endpoint.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve got our API Key and API Key Secret properly encoded, we can import
    *just* the `auth_ready_key` into the script we’ll use to actually specify and
    pull our data. After doing one request to the authorization endpoint to get our
    *access token*, we’ll finally(!) be ready retrieve some tweets!
  prefs: []
  type: TYPE_NORMAL
- en: Requesting an Access Token and Data from the Twitter API
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we did in [Example 5-3](#FRED_download_api_key_import), we’ll now create
    a Python file (or notebook) where we’ll do the next two steps of our Twitter data
    loading process:'
  prefs: []
  type: TYPE_NORMAL
- en: Requesting (and receiving) an access token or *bearer token* from Twitter
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Including that bearer token in a data request to Twitter and receiving the results
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Requesting an access token: get versus post'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Requesting an access or bearer token from Twitter is really just a matter of
    sending a well-formatted request to the *authorization endpoint*, which is [*https://api.twitter.com/oauth2/token*](https://api.twitter.com/oauth2/token).
    Instead of appending our `auth_ready_key` to the endpoint URL, however, we’ll
    use something called a *post* request (recall that in Examples [5-1](#data_download)
    and [5-3](#FRED_download_api_key_import), the `requests` recipe we used was called
    `get`).
  prefs: []
  type: TYPE_NORMAL
- en: A `post` request is important here in part because it offers some additional
    security over `get` requests^([12](ch05.html#idm45143407651104)) but mostly because
    `post` requests are effectively the standard when we’re asking an API to do something
    *beyond* simply returning data. So while in [Example 5-3](#FRED_download_api_key_import)
    the data we got back was unique to our query, the API would return that same data
    to *anyone* who submitted that same request. By contrast, when we use `post` to
    submit our `auth_ready_key`, the Twitter API will process our unique key and return
    a unique bearer token—so we use a `post` request.
  prefs: []
  type: TYPE_NORMAL
- en: 'When building our `post` request in Python, we’ll need to create two `dict`
    objects: one that contains the request’s *headers*, which will contain both our
    `auth_ready_key` and some other information, and another that contains the request’s
    *data*, which in this case will specify that we’re asking for credentials. Then
    we’ll just pass these as parameters to the *requests* library’s `post` recipe,
    rather than sticking them on the end of the URL string, as shown in [Example 5-6](#twitter_api_access_token_request).'
  prefs: []
  type: TYPE_NORMAL
- en: Example 5-6\. Twitter_data_download.py
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_accessing_web_based_data_CO3-1)'
  prefs: []
  type: TYPE_NORMAL
- en: This `dict` contains the information the authorization endpoint wants in order
    to return an access token to us. This includes our encoded key and its data format.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_accessing_web_based_data_CO3-2)'
  prefs: []
  type: TYPE_NORMAL
- en: The format of both the `auth_headers` and the `auth_data` objects was defined
    by the API provider.
  prefs: []
  type: TYPE_NORMAL
- en: Actually pretty straightforward, right? If everything went well (which we’ll
    confirm momentarily), we’ll just need to add a few lines of code to this script
    in order to use our access token to request some actual data about what’s happening
    on Twitter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have an access token (or *bearer token*) handy, we can go ahead
    and make a request for some tweets. For demonstration purposes, we’re going to
    keep our request simple: we’ll do a basic search request for recent tweets that
    contain the word *Python*, and ask to have a maximum of four tweets returned.
    In [Example 5-7](#twitter_api_data_request), we’re building on the script we started
    in [Example 5-6](#twitter_api_access_token_request), and we’ll structure and make
    this request, including our newly acquired bearer token in the header. Once the
    response arrives, we’ll write the data to a file. Since there is a *lot* in the
    JSON data we get back besides the text of the tweets, however, we’re also going
    to print out *just* the text of each tweet, just to give us confidence that we
    got correct (if sometimes unexpected) results ;-)'
  prefs: []
  type: TYPE_NORMAL
- en: Example 5-7\. Twitter_data_download.py, continued
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_accessing_web_based_data_CO4-1)'
  prefs: []
  type: TYPE_NORMAL
- en: In this case, our query (`q`) is `Python`, we’re looking for `recent` results,
    and we want a maximum of `4` tweets back. Remember that the keys and values we
    can include in this object are defined by the data provider.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_accessing_web_based_data_CO4-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Because the response from Twitter is a JSON object, we have to use the built-in
    Python `str()` function to convert it to a string before we can write it to our
    file.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_accessing_web_based_data_CO4-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Because there is a *lot* of information in each result, we’re going to print
    out the text of each tweet returned, just to get a sense of what’s there. The
    `statuses` is the list of tweets in the JSON object, and the actual text of the
    tweets can be accessed with the key `text`.
  prefs: []
  type: TYPE_NORMAL
- en: Depending on how actively Twitter users have been posting about Python recently,
    you may see different results even if you run this script again in only a few
    minutes.^([13](ch05.html#idm45143407307360)) Of course you can change this search
    to contain any query term you want; just modify the value of the `search_params`
    variable as you like. To see all the possible parameters and their valid values,
    you can look through [the API documentation for this particular Twitter API endpoint](https://developer.twitter.com/en/docs/twitter-api/v1/tweets/search/api-reference/get-search-tweets).
  prefs: []
  type: TYPE_NORMAL
- en: And that’s it! While there are a number of different APIs that Twitter makes
    available (others allow you to actually post to your own timeline or even someone
    else’s), for the purposes of accessing and wrangling data, what we’ve covered
    here should be enough to get you started with this and other similar APIs.
  prefs: []
  type: TYPE_NORMAL
- en: API Ethics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that you know how to make API requests from services like Twitter (and others
    that use an OAuth process), you may be imagining all the cool things you can do
    with the data you can collect. Before you start writing dozens of scripts to track
    the conversations happening around your favorite topics online, however, it’s
    time to do both some practical and ethical reflection.
  prefs: []
  type: TYPE_NORMAL
- en: First, know that almost every API uses *rate-limiting* to restrict how many
    data requests you can make within a given time interval. On the particular API
    endpoint we used in [Example 5-7](#twitter_api_data_request), for example, you
    can make a maximum of 450 requests in a 15-minute time period, and each request
    can return a maximum of 100 tweets. If you exceed this, your data requests will
    probably fail to return data until Twitter determines that the next 15-minute
    window has begun.
  prefs: []
  type: TYPE_NORMAL
- en: Second, while you probably didn’t read the Developer Agreement in detail (don’t
    worry, you’re not alone;^([14](ch05.html#idm45143407245520)) you can always find
    a [reference copy online](https://developer.twitter.com/en/developer-terms/agreement-and-policy)),
    it includes provisions that have important practical and ethical implications.
    For example, Twitter’s Developer Agreement *specifically* prohibits the practice
    of “Off-Twitter matching”—that is, combining Twitter data with other information
    about a user, unless that user has provided you the information directly or expressly
    provided their consent. It also contains rules about how you can store and display
    Twitter content that you may get from the API, and a whole host of other rules
    and restrictions.
  prefs: []
  type: TYPE_NORMAL
- en: Whether or not those terms of service are legally binding,^([15](ch05.html#idm45143407239440))
    or truly ethical in and of themselves, remember that it is ultimately *your* responsibility
    to make sure that you are gathering, analyzing, storing, and sharing *any* data
    you use in an ethical way. That means taking into account the privacy and security
    of the people you may be using data about, as well as thinking about the implications
    of aggregating and sharing it.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, if ethical issues were easy to identify and agree upon, we would
    live in a very different world. Because they aren’t, many organizations have well-defined
    review processes and oversight committees designed to help explore and (if possible)
    resolve ethical issues before, for example, research and data collection impacting
    humans ever begins. For my own part, I still find that a good place to start when
    trying to think through ethical issues is [the “Society of Professional Journalists’
    Code of Ethics”](https://spj.org/ethicscode.asp). While that document doesn’t
    cover every possible ethics situation in detail, it articulates some core principles
    that I think all data users would do well to consider when they are collecting,
    analyzing, and sharing information.
  prefs: []
  type: TYPE_NORMAL
- en: In the end, however, the most important thing is that *whatever* choices you
    make, you’re ready to stand behind them. One of the great possibilities of data
    access and wrangling is the ability to uncover new information and generate new
    insights. Just as the skills for doing that are now entirely in your control,
    so is the responsibility for how you use them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Web Scraping: The Data Source of Last Resort'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While APIs are designed by companies and organizations with the specific intent
    of providing access to often rich, diverse datasets via the internet, there is
    still a whole lot of information online that only exists on form-based or highly
    formatted web pages as opposed to an API or even as a CSV or PDF. For situations
    like these, the only real solution is *web scraping*, which is the process of
    using code to programmatically retrieve the contents of a web page and systematically
    extract some amount of (usually) structured data from it.
  prefs: []
  type: TYPE_NORMAL
- en: The reason why I refer to web scraping as “the data source of last resort” is
    that it’s both a technically and ethically complex process. Creating web scraping
    Python scripts almost always requires manually wading through a jumble of HTML
    code to locate the data you’re looking for, typically followed by a significant
    amount of trial and error to successfully separate the information you want from
    everything you don’t. It’s time-consuming, fiddly, and often frustrating. And
    if the web page changes even a little, you may have to start from scratch in order
    to make your script work with the updated page.
  prefs: []
  type: TYPE_NORMAL
- en: Web scraping is also ethically complicated because, for a variety of reasons,
    many website owners don’t *want* you to scrape their pages. Poorly coded scraping
    programs can overwhelm the website, making it inaccessible to other users. Making
    lots of scripted data requests in quick succession can also drive up the website
    owner’s costs, because they have to pay their own service provider to return all
    that data in a brief period. As a result, many websites explicitly prohibit web
    scraping in their Terms of Service.
  prefs: []
  type: TYPE_NORMAL
- en: At the same time, if important information—especially about powerful organizations
    or government agencies—is *only* available via a web page, then scraping may be
    your *only* option even if it goes against the Terms of Service. While it is *far*
    beyond the scope of this book to provide even pseudolegal advice on the subject,
    keep in mind that even if your scripts are written responsibly and there is a
    good, public-interest reason for your scraping activity, you may face sanction
    from the website owner (such as a “cease and desist” letter) or even legal action.
  prefs: []
  type: TYPE_NORMAL
- en: Because of this, I strongly recommend that before you start down the road of
    writing *any* web scraping script you work through the excellent decision tree
    by Sophie Chou shown in [Figure 5-16](#scraping_decision_tree).^([16](ch05.html#idm45143407214288))
  prefs: []
  type: TYPE_NORMAL
- en: '![Web scraping decision tree by Sophie Chou, for Storybench.org, 2016](assets/ppdw_0516.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-16\. Web scraping decision tree (Sophie Chou for Storybench.org, 2016)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Once you’ve determined that scraping is your only/best option, it’s time to
    get to work.
  prefs: []
  type: TYPE_NORMAL
- en: Carefully Scraping the MTA
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For this example, we’re going to use web scraping to download and extract data
    from a web page provided by New York City’s Metropolitan Transit Authority (MTA),
    which provides links to all of the turnstile data for the city’s subway stations,
    going back to 2010\. To ensure that we’re doing this as responsibly as possible,
    we’re going to make sure that *any* Python script we write:'
  prefs: []
  type: TYPE_NORMAL
- en: Identifies who we are and how to get in touch with us (in the example, we’ll
    include an email address).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Pauses between web page requests to make sure that we don’t overwhelm the server.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In addition, we’ll structure and separate the parts of our script to make sure
    that we never download a particular web page more than absolutely necessary. To
    accomplish this, we’ll start by downloading and saving a copy of the web page
    that contains the links to the individual turnstile data files. Then we’ll write
    a separate script that goes through the *saved* version of the initial page and
    extracts the data we need. That way, any trial and error we go through in extracting
    data from the web page happens on our saved version, meaning it adds no additional
    load to the MTA’s server. Finally, we’ll write a third script that parses our
    file of extracted links and downloads the last four weeks of data.
  prefs: []
  type: TYPE_NORMAL
- en: Before we begin writing any code, let’s take a look at [the page we’re planning
    to scrape](http://web.mta.info/developers/turnstile.html). As you can see, the
    page is little more than a header and a long list of links, each of which will
    take you to a comma-separated *.txt* file. To load the last several weeks of these
    data files, our first step is to download a copy of this index-style page ([Example 5-8](#downloading_turnstile_index)).
  prefs: []
  type: TYPE_NORMAL
- en: Example 5-8\. MTA_turnstiles_index.py
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_accessing_web_based_data_CO5-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Since we’re *not* using an API here, we want to proactively provide the website
    owner with information about who we are and how to contact us. In this case, we’re
    describing the browser it should treat our traffic as being from, along with our
    name and contact information. This is data that the website owner will be able
    to see in their server logs.
  prefs: []
  type: TYPE_NORMAL
- en: Now you’ll have a file called *MTA_turnstiles_index.html* in the same folder
    where your Python script was located. To see what it contains, you can just double-click
    on it, and it should open in your default web browser. Of course, because we only
    downloaded the raw code on the page and none of the extra files, images, and other
    materials that it would normally have access to on the web, it’s going to look
    a little wonky, probably something like what’s shown in [Figure 5-17](#local_copy_mta_turnstile_index).
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, that doesn’t matter at all, since what we’re after here is the
    list of links that’s stored with the page’s HTML. Before we worry about how to
    pull that data programmatically, however, we first need to find it within the
    page’s HTML code. To do this, we’re going to use our web browser’s *inspection
    tools*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Viewing our local copy of the MTA web page in a browser](assets/ppdw_0517.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-17\. Viewing our local copy of the MTA web page in a browser
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Using Browser Inspection Tools
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With the local copy of the MTA turnstile data page open in a browser in front
    of you, scroll down until you can see the “Data Files” header, as shown in [Figure 5-17](#local_copy_mta_turnstile_index).
    To better target *just* the information we want on this web page with our Python
    script, we need to try to identify something unique about HTML code that surrounds
    it—this will make it easier for us to have our script zero in quickly on just
    the content we want. The easiest way to do this is by “inspecting” the code alongside
    the regular browser interface.
  prefs: []
  type: TYPE_NORMAL
- en: To get started, put your mouse cursor over the “Data Files” text and context-click
    (also known as “right-click” or sometimes “Ctrl+click,” depending on your system).
    At the bottom of the menu that pops up, shown in [Figure 5-18](#MTA_context_click),
    choose “Inspect.”
  prefs: []
  type: TYPE_NORMAL
- en: '![The context menu on our local copy of the MTA web page](assets/ppdw_0518.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-18\. The context menu on our local copy of the MTA web page
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: While the precise location and shape of your particular browser’s inspection
    tools window will vary (this screenshot is from the Chrome browser), its contents
    will hopefully look at least somewhat similar to the image in [Figure 5-19](#inspection_window_highlights).
  prefs: []
  type: TYPE_NORMAL
- en: '![The inspection tools window with the Data Files header highlighted.](assets/ppdw_0519.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-19\. Inspection tools example
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Wherever your own window appears (sometimes it is anchored to the side or bottom
    of your browser window, and, as in [Figure 5-19](#inspection_window_highlights),
    there are often multiple panes of information), the main thing we want to locate
    in the inspection tools window are the words “Data Files.” If you lost them (or
    never saw them in the first place!) once the window appeared, just move your mouse
    over those words on the web page and context-click to open the inspection tools
    window again.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, if you use your mouse to hover over the code in the inspection
    tools window that says:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: you should see the “Data Files” section of the web page highlighted in your
    browser window. Based on the area highlighted, it *appears* that this bit of code
    includes the entire list of links we’re interested in, which we can confirm by
    scrolling down in the inspection tools window. There we’ll see that all of the
    data links we want (which end in *.txt*) are indeed inside this `div` (notice
    how they are indented beneath it? That’s another instance of nesting at work!).
    Now, if we can confirm that class `span-84 last` only exists in one place on the
    web page, we have a good starting point for writing our Python script to extract
    the list of links.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Python Web Scraping Solution: Beautiful Soup'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before we begin writing our next Python script, let’s confirm that the `span-84
    last` class really is unique on the page we downloaded. The simplest way to do
    this is to first open the page in Atom (context-click on the filename instead
    of double-clicking, and choose Atom from the “Open with” menu option), which will
    show us the page’s code. Then do a regular “find” command (Ctrl+F or command+F)
    and search for `span-84 last`. As it turns out, even the `span-84` part only appears
    once in our file, so we can confine our Python script to looking for link information
    nested within that HTML tag.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we’re ready to start writing the Python script that will extract the links
    from the web page. For this we’ll install and use the *Beautiful Soup* library,
    which is widely used for parsing the often messy markup we find on web pages.
    While *Beautiful Soup* has some functionality overlap with the *lxml* library
    that we used in [Example 4-12](ch04.html#xml_parsing), the main difference between
    them is that *Beautiful Soup* can handle parsing even less-than-perfectly-structured
    HTML and XML—which is what we invariably have to deal with on the web. In addition,
    *Beautiful Soup* lets us “grab” bits of markup by almost any feature we want—class
    name, tag type, or even attribute value—so it’s pretty much the go-to library
    for pulling data out of the “soup” of markup we often find online. You can read
    the [full documentation for the library](https://crummy.com/software/BeautifulSoup/bs4/doc),
    but the easiest way to install it will be the same process we’ve used for other
    libraries, by using `pip` on the command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Now, we’re ready to use Python to open the local copy of our web page and use
    the *Beautiful Soup* library to quickly grab all the links we need and write them
    to a simple *.csv* file, as shown in [Example 5-9](#parsing_the_soup).
  prefs: []
  type: TYPE_NORMAL
- en: Example 5-9\. MTA_turnstiles_parsing.py
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_accessing_web_based_data_CO6-1)'
  prefs: []
  type: TYPE_NORMAL
- en: If we click one of the data links on the live copy of the web page, we see that
    the first part of the URL where the actual data file lives is `http://web.mta.info/developers/`,
    but each link only contains the latter half of the URL (in the format `data/nyct/turnstile/turnstile_YYMMDD.txt`).
    So that our download script has working links, we need to specify the “base” URL.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_accessing_web_based_data_CO6-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Thanks to our work with the inspection tools, we can go straight to a `div`
    with the class `span-84 last` to start looking for the links we want. Note that
    because the word `class` has a special meaning in Python, Beautiful Soup appends
    an underscore to the end when we’re using it here (e.g., `class_`).
  prefs: []
  type: TYPE_NORMAL
- en: OK! What we should have now is a new file that contains a list of all the data
    links we’re interested in. Next we need to read that list in using a new script
    and download the files at those URLs. However, because we want to be careful not
    to overload the MTA website by downloading the files too quickly, we’re going
    to use the built-in Python *time* library to space out our requests by a second
    or two each. Also, we’ll be sure to only download the four files that we really
    want, rather than downloading everything just for the sake of it. To see how this
    second script is organized, take a look at [Example 5-10](#downloading_the_data).
  prefs: []
  type: TYPE_NORMAL
- en: Example 5-10\. MTA_turnstiles_data_download.py
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_accessing_web_based_data_CO7-1)'
  prefs: []
  type: TYPE_NORMAL
- en: This library will let us “pause” our downloading script between data requests
    so that we don’t overload the MTA server with too many requests in too short a
    time period (and possibly get in trouble).
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_accessing_web_based_data_CO7-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Here, we’re splitting the link URL on slashes, then taking the last item from
    the resulting list using [negative indexing](https://w3schools.com/python/gloss_python_string_negative_indexing.asp),
    which counts backward from the end of the string. This means that the item at
    position `-1` is the last item, which here is the *.txt* filename. This is the
    filename we’ll use for the local copy of the data that we save.
  prefs: []
  type: TYPE_NORMAL
- en: If everything has gone well, you should now have a new folder called *turnstile_data*,
    with the four most recent turnstile data files saved inside it. Pretty neat, right?
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we have explored the many ways to actually *get* the data we’re after
    and convert it into formats we can use, the next question is: what do we *do*
    with it all? Since the goal of all this data wrangling is to be able to answer
    questions and generate some insight about the world, we now need to move on from
    the process of *acquiring* data and start the process of assessing, improving,
    and analyzing it. To this end, in the next chapter we’ll work through a data quality
    evaluation of a public dataset, with an eye toward understanding both its possibilities
    and limitations and how our data wrangling work can help us make the most of it.'
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch05.html#idm45143408861008-marker)) Like the [US Treasurey, for example](https://fiscaldata.treasury.gov/api-documentation).
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch05.html#idm45143408859632-marker)) For two examples, see articles in
    [*The Markup*](https://themarkup.org/google-the-giant/2021/04/09/how-we-discovered-googles-social-justice-blocklist-for-youtube-ad-placements)
    and [*NPR*](https://npr.org/2021/08/04/1024791053/facebook-boots-nyu-disinformation-researchers-off-its-platform-and-critics-cry-f).
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](ch05.html#idm45143408848400-marker)) This is also the first step to building
    your own “apps”!
  prefs: []
  type: TYPE_NORMAL
- en: ^([4](ch05.html#idm45143408324000-marker)) While this is probably most likely
    to happen if you make too many data requests in too short a time frame, most API
    providers can terminate your access to their API pretty much whenever they want,
    and for whatever reason they want.
  prefs: []
  type: TYPE_NORMAL
- en: ^([5](ch05.html#idm45143408221168-marker)) For example, when you `git push`
    your code.
  prefs: []
  type: TYPE_NORMAL
- en: ^([6](ch05.html#idm45143408216624-marker)) “Hackers Used SolarWinds’ Dominance
    against It in Sprawling Spy Campaign” by Raphael Satter, Christopher Bing, and
    Joseph Menn, [*https://reuters.com/article/global-cyber-solarwinds/hackers-at-center-of-sprawling-spy-campaign-turned-solarwinds-dominance-against-it-idUSKBN28P2N8*](https://reuters.com/article/global-cyber-solarwinds/hackers-at-center-of-sprawling-spy-campaign-turned-solarwinds-dominance-against-it-idUSKBN28P2N8);
    “Former SolarWinds CEO Blames Intern for *solarwinds123* Password Leak” by Brian
    Fung and Geneva Sands, [*https://www.cnn.com/2021/02/26/politics/solarwinds123-password-intern*](https://www.cnn.com/2021/02/26/politics/solarwinds123-password-intern).
  prefs: []
  type: TYPE_NORMAL
- en: ^([7](ch05.html#idm45143408103984-marker)) That’s why the program that makes
    Python run on your device is often called a Python *interpreter*—because it translates
    the code we humans write into bytecode that your device can actually understand.
  prefs: []
  type: TYPE_NORMAL
- en: ^([8](ch05.html#idm45143408008560-marker)) Even if we run, say, `git add -A`
    or `git commit -a`.
  prefs: []
  type: TYPE_NORMAL
- en: ^([9](ch05.html#idm45143407997552-marker)) A repository can have different *.gitignore*
    files in different folders; for complete details, you can take a look at the [documentation](https://git-scm.com/docs/gitignore).
  prefs: []
  type: TYPE_NORMAL
- en: '^([10](ch05.html#idm45143407977504-marker)) We used this approach with the
    *glob* library in [Example 4-16](ch04.html#pdf_parsing) and will examine it in
    more detail in [“Regular Expressions: Supercharged String Matching”](ch07.html#regex_example).'
  prefs: []
  type: TYPE_NORMAL
- en: ^([11](ch05.html#idm45143407870720-marker)) These keys have since been replaced
    and will not work!
  prefs: []
  type: TYPE_NORMAL
- en: ^([12](ch05.html#idm45143407651104-marker)) For example, the contents of a `post`
    request are not saved in the browser history the way that `get` requests are.
  prefs: []
  type: TYPE_NORMAL
- en: ^([13](ch05.html#idm45143407307360-marker)) Remember that each time you run
    the script, you will also overwrite your output file, so it will only ever contain
    the most recent results.
  prefs: []
  type: TYPE_NORMAL
- en: '^([14](ch05.html#idm45143407245520-marker)) Aleecia M. McDonald and Lorrie
    Faith Cranor, “The Cost of Reading Privacy Policies,” *I/S: A Journal of Law and
    Policy for the Information Society* 4 (2008): 543, [*https://kb.osu.edu/bitstream/handle/1811/72839/ISJLP_V4N3_543.pdf*](https://kb.osu.edu/bitstream/handle/1811/72839/ISJLP_V4N3_543.pdf),
    and Jonathan A. Obar and Anne Oeldorf-Hirsch, “The Biggest Lie on the Internet:
    Ignoring the Privacy Policies and Terms of Service Policies of Social Networking
    Services,” *Information, Communication & Society* 23 no. 1 (2020): 128–147, [*https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2757465*](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2757465).'
  prefs: []
  type: TYPE_NORMAL
- en: ^([15](ch05.html#idm45143407239440-marker)) Victoria D. Baranetsky, “Data Journalism
    and the Law,” *Tow Center for Digital Journalism* (2018) [*https://doi.org/10.7916/d8-15sw-fy51*](https://doi.org/10.7916/d8-15sw-fy51).
  prefs: []
  type: TYPE_NORMAL
- en: '^([16](ch05.html#idm45143407214288-marker)) The accompanying blog post is also
    excellent: [*https://storybench.org/to-scrape-or-not-to-scrape-the-technical-and-ethical-challenges-of-collecting-data-off-the-web*](https://storybench.org/to-scrape-or-not-to-scrape-the-technical-and-ethical-challenges-of-collecting-data-off-the-web).'
  prefs: []
  type: TYPE_NORMAL
