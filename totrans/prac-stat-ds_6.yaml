- en: Chapter 6\. Statistical Machine Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第6章 统计机器学习
- en: 'Recent advances in statistics have been devoted to developing more powerful
    automated techniques for predictive modeling—both regression and classification.
    These methods, like those discussed in the previous chapter, are *supervised methods*—they
    are trained on data where outcomes are known and learn to predict outcomes in
    new data. They fall under the umbrella of *statistical machine learning* and are
    distinguished from classical statistical methods in that they are data-driven
    and do not seek to impose linear or other overall structure on the data. The *K*-Nearest
    Neighbors method, for example, is quite simple: classify a record in accordance
    with how similar records are classified. The most successful and widely used techniques
    are based on *ensemble learning* applied to *decision trees*. The basic idea of
    ensemble learning is to use many models to form a prediction, as opposed to using
    just a single model. Decision trees are a flexible and automatic technique to
    learn rules about the relationships between predictor variables and outcome variables.
    It turns out that the combination of ensemble learning with decision trees leads
    to some of the best performing off-the-shelf predictive modeling techniques.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 最近统计学的进展致力于开发更强大的自动化预测建模技术，包括回归和分类。这些方法，如前一章讨论的那些，是*监督方法*——它们在已知结果的数据上训练，并学会在新数据中预测结果。它们属于*统计机器学习*的范畴，与经典统计方法不同，它们是数据驱动的，不寻求对数据施加线性或其他整体结构。例如，*K*-最近邻方法相当简单：根据相似的记录进行分类。最成功和广泛使用的技术基于应用于*决策树*的*集成学习*。集成学习的基本思想是使用多个模型来形成预测，而不是仅使用单个模型。决策树是一种灵活和自动的技术，用于学习关于预测变量和结果变量之间关系的规则。事实证明，集成学习与决策树的结合导致了一些性能最佳的现成预测建模技术。
- en: The development of many of the techniques in statistical machine learning can
    be traced back to the statisticians Leo Breiman (see [Figure 6-1](#LeoBreiman))
    at the University of California at Berkeley and Jerry Friedman at Stanford University.
    Their work, along with that of other researchers at Berkeley and Stanford, started
    with the development of tree models in 1984. The subsequent development of ensemble
    methods of bagging and boosting in the 1990s established the foundation of statistical
    machine learning.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 许多统计机器学习技术的发展可以追溯到加州大学伯克利分校的统计学家列奥·布雷曼（见[图6-1](#LeoBreiman)）和斯坦福大学的杰瑞·弗里德曼。他们的工作与伯克利和斯坦福的其他研究人员一起，始于1984年的树模型的发展。随后在1990年代开发的装袋（bagging）和提升（boosting）等集成方法奠定了统计机器学习的基础。
- en: '![Leo Breiman, who was a professor of statistics at UC Berkeley, was at the
    forefront of the development of many techniques in a data scientist''s toolkit
    today](Images/psd2_0601.png)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
  zh: '![列奥·布雷曼，加州大学伯克利分校的统计学教授，是今天数据科学家工具包中许多技术发展的前沿人物](Images/psd2_0601.png)'
- en: Figure 6-1\. Leo Breiman, who was a professor of statistics at UC Berkeley,
    was at the forefront of the development of many techniques in a data scientist’s
    toolkit today
  id: totrans-4
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-1 列奥·布雷曼，加州大学伯克利分校的统计学教授，是今天数据科学家工具包中许多技术发展的前沿人物
- en: Machine Learning Versus Statistics
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习与统计学的对比
- en: In the context of predictive modeling, what is the difference between machine
    learning and statistics? There is not a bright line dividing the two disciplines.
    Machine learning tends to be focused more on developing efficient algorithms that
    scale to large data in order to optimize the predictive model. Statistics generally
    pays more attention to the probabilistic theory and underlying structure of the
    model. Bagging, and the random forest (see [“Bagging and the Random Forest”](#Bagging)),
    grew up firmly in the statistics camp. Boosting (see [“Boosting”](#Boosting)),
    on the other hand, has been developed in both disciplines but receives more attention
    on the machine learning side of the divide. Regardless of the history, the promise
    of boosting ensures that it will thrive as a technique in both statistics and
    machine learning.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在预测建模的背景下，机器学习与统计学的区别是什么？两个学科之间没有明显的分界线。机器学习更倾向于开发能够扩展到大数据的高效算法，以优化预测模型。统计学通常更关注模型的概率理论和基本结构。装袋法和随机森林（参见[“装袋法和随机森林”](#Bagging)）最初坚定地发展在统计学阵营中。增强（参见[“增强”](#Boosting)），另一方面，在两个学科中都有发展，但更多地受到机器学习一侧的关注。无论历史如何，增强的前景确保它将作为一种技术在统计学和机器学习中蓬勃发展。
- en: K-Nearest Neighbors
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: K-最近邻
- en: 'The idea behind *K*-Nearest Neighbors (KNN) is very simple.^([1](ch06.xhtml#idm46522845224392))
    For each record to be classified or predicted:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '*K*-最近邻（KNN）的思想非常简单。^([1](ch06.xhtml#idm46522845224392)) 对于每个待分类或预测的记录：'
- en: Find *K* records that have similar features (i.e., similar predictor values).
  id: totrans-9
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找到*K*个具有相似特征的记录（即类似的预测值）。
- en: For classification, find out what the majority class is among those similar
    records and assign that class to the new record.
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了分类，找出类似记录中的主要类别，并将该类别分配给新记录。
- en: For prediction (also called *KNN regression*), find the average among those
    similar records, and predict that average for the new record.
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于预测（也称为*KNN回归*），找到类似记录中的平均值，并预测新记录的该平均值。
- en: 'KNN is one of the simpler prediction/classification techniques: there is no
    model to be fit (as in regression). This doesn’t mean that using KNN is an automatic
    procedure. The prediction results depend on how the features are scaled, how similarity
    is measured, and how big *K* is set. Also, all predictors must be in numeric form.
    We will illustrate how to use the KNN method with a classification example.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: KNN是较简单的预测/分类技术之一：不需要拟合模型（如回归）。这并不意味着使用KNN是一个自动化过程。预测结果取决于特征如何缩放、相似性如何衡量以及*K*的设置大小。此外，所有预测变量必须为数值形式。我们将通过分类示例说明如何使用KNN方法。
- en: 'A Small Example: Predicting Loan Default'
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 小例子：预测贷款违约
- en: '[Table 6-1](#loan_data) shows a few records of personal loan data from LendingClub.
    LendingClub is a leader in peer-to-peer lending in which pools of investors make
    personal loans to individuals. The goal of an analysis would be to predict the
    outcome of a new potential loan: paid off versus default.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 6-1](#loan_data)显示了来自LendingClub的个人贷款数据的几个记录。LendingClub是P2P借贷领域的领先者，投资者汇集资金向个人提供贷款。分析的目标是预测新潜在贷款的结果：偿还还是违约。'
- en: Table 6-1\. A few records and columns for LendingClub loan data
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6-1\. LendingClub贷款数据的几个记录和列
- en: '| Outcome | Loan amount | Income | Purpose | Years employed | Home ownership
    | State |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| 结果 | 贷款金额 | 收入 | 目的 | 工作年限 | 房屋所有权 | 州 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| Paid off | 10000 | 79100 | debt_consolidation | 11 | MORTGAGE | NV |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| 偿还 | 10000 | 79100 | 债务合并 | 11 | 按揭 | NV |'
- en: '| Paid off | 9600 | 48000 | moving | 5 | MORTGAGE | TN |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| 偿还 | 9600 | 48000 | 搬迁 | 5 | 按揭 | TN |'
- en: '| Paid off | 18800 | 120036 | debt_consolidation | 11 | MORTGAGE | MD |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| 偿还 | 18800 | 120036 | 债务合并 | 11 | 按揭 | MD |'
- en: '| Default | 15250 | 232000 | small_business | 9 | MORTGAGE | CA |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| 违约 | 15250 | 232000 | 小企业 | 9 | 按揭 | CA |'
- en: '| Paid off | 17050 | 35000 | debt_consolidation | 4 | RENT | MD |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| 偿还 | 17050 | 35000 | 债务合并 | 4 | 租房 | MD |'
- en: '| Paid off | 5500 | 43000 | debt_consolidation | 4 | RENT | KS |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| 偿还 | 5500 | 43000 | 债务合并 | 4 | 租房 | KS |'
- en: 'Consider a very simple model with just two predictor variables: `dti`, which
    is the ratio of debt payments (excluding mortgage) to income, and `payment_inc_ratio`,
    which is the ratio of the loan payment to income. Both ratios are multiplied by
    100. Using a small set of 200 loans, `loan200`, with known binary outcomes (default
    or no-default, specified in the predictor `outcome200`), and with *K* set to 20,
    the KNN estimate for a new loan to be predicted, `newloan`, with `dti=22.5` and
    `payment_inc_ratio=9` can be calculated in *R* as follows:^([2](ch06.xhtml#idm46522845155448))'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个非常简单的模型，只有两个预测变量：`dti`，即债务支付（不包括抵押贷款）与收入的比率，以及 `payment_inc_ratio`，即贷款支付与收入的比率。这两个比率均乘以
    100。使用一个小集合的 200 笔贷款，`loan200`，具有已知的二元结果（违约或未违约，由预测变量 `outcome200` 指定），并且将 *K*
    设为 20，可以在 *R* 中如下计算要预测的新贷款 `newloan`，其 `dti=22.5` 和 `payment_inc_ratio=9`：^([2](ch06.xhtml#idm46522845155448))
- en: '[PRE0]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The KNN prediction is for the loan to default.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: KNN 预测是贷款违约。
- en: While *R* has a native `knn` function, the contributed *R* package [`FNN`, for
    Fast Nearest Neighbor](https://oreil.ly/RMQFG), scales more effectively to big
    data and provides more flexibility.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 *R* 有本地的 `knn` 函数，但贡献的 *R* 软件包 [`FNN`, for Fast Nearest Neighbor](https://oreil.ly/RMQFG)
    在大数据方面效果更好，提供了更多的灵活性。
- en: 'The `scikit-learn` package provides a fast and efficient implementation of
    KNN in *Python*:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '`scikit-learn` 软件包在 *Python* 中提供了 KNN 的快速高效实现：'
- en: '[PRE1]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[Figure 6-2](#LoanKNN) gives a visual display of this example. The new loan
    to be predicted is the cross in the middle. The squares (paid off) and circles
    (default) are the training data. The large black circle shows the boundary of
    the nearest 20 points. In this case, 9 defaulted loans lie within the circle,
    as compared with 11 paid-off loans. Hence the predicted outcome of the loan is
    paid off. Note that if we consider only three nearest neighbors, the prediction
    would be that the loan defaults.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 6-2](#LoanKNN) 提供了此示例的视觉展示。要预测的新贷款是中间的交叉点。方块（已还清）和圆圈（违约）表示训练数据。大黑色圆圈显示了最近的
    20 个点的边界。在这种情况下，圆圈内有 9 笔违约贷款，相比之下有 11 笔已还清贷款。因此，贷款的预测结果是已还清。请注意，如果仅考虑三个最近的邻居，则预测将是贷款违约。'
- en: '![KNN prediction of loan default using two variables: debt-to-income ratio
    and loan-payment-to-income ratio](Images/psd2_0602.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![KNN 预测贷款违约使用两个变量：债务收入比和贷款支付收入比](Images/psd2_0602.png)'
- en: 'Figure 6-2\. KNN prediction of loan default using two variables: debt-to-income
    ratio and loan-payment-to-income ratio'
  id: totrans-32
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-2\. KNN 预测贷款违约使用两个变量：债务收入比和贷款支付收入比
- en: Note
  id: totrans-33
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注
- en: While the output of KNN for classification is typically a binary decision, such
    as default or paid off in the loan data, KNN routines usually offer the opportunity
    to output a probability (propensity) between 0 and 1. The probability is based
    on the fraction of one class in the *K* nearest neighbors. In the preceding example,
    this probability of default would have been estimated at <math alttext="nine-twenty-ths"><mfrac><mn>9</mn>
    <mn>20</mn></mfrac></math> , or 0.45. Using a probability score lets you use classification
    rules other than simple majority votes (probability of 0.5). This is especially
    important in problems with imbalanced classes; see [“Strategies for Imbalanced
    Data”](ch05.xhtml#ImbalancedData). For example, if the goal is to identify members
    of a rare class, the cutoff would typically be set below 50%. One common approach
    is to set the cutoff at the probability of the rare event.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 KNN 在分类的输出通常是二元决策，例如贷款数据中的违约或已还清，但 KNN 程序通常提供了输出介于 0 到 1 之间的概率（倾向性）。概率基于
    *K* 个最近邻居中某一类的分数。在前述示例中，此违约的概率将被估计为 <math alttext="nine-twenty-ths"><mfrac><mn>9</mn>
    <mn>20</mn></mfrac></math>，即 0.45。使用概率分数可以让您使用除简单多数投票（0.5 的概率）之外的分类规则。这在存在类别不平衡问题时尤为重要；参见
    [“不平衡数据的策略”](ch05.xhtml#ImbalancedData)。例如，如果目标是识别罕见类别的成员，则截止点通常设置在低于 50% 的概率上。一种常见的方法是将截止点设置为罕见事件的概率。
- en: Distance Metrics
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 距离度量
- en: 'Similarity (nearness) is determined using a *distance metric*, which is a function
    that measures how far two records (*x[1]*, *x[2]*, …, *x[p]*) and (*u[1]*, *u[2]*,
    …, *u[p]*) are from one another. The most popular distance metric between two
    vectors is *Euclidean distance*. To measure the Euclidean distance between two
    vectors, subtract one from the other, square the differences, sum them, and take
    the square root:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 使用*距离度量*来确定相似性（接近度），这是一个衡量两条记录（*x[1]*, *x[2]*, …, *x[p]*）和（*u[1]*, *u[2]*, …,
    *u[p]*）之间距离的函数。两个向量之间最流行的距离度量是*欧氏距离*。要测量两个向量之间的欧氏距离，将一个向量减去另一个向量，平方差值，求和，然后取平方根：
- en: <math display="block"><mrow><msqrt><mrow><msup><mrow><mo>(</mo><msub><mi>x</mi>
    <mn>1</mn></msub> <mo>-</mo><msub><mi>u</mi> <mn>1</mn></msub> <mo>)</mo></mrow>
    <mn>2</mn></msup> <mo>+</mo> <msup><mrow><mo>(</mo><msub><mi>x</mi> <mn>2</mn></msub>
    <mo>-</mo><msub><mi>u</mi> <mn>2</mn></msub> <mo>)</mo></mrow> <mn>2</mn></msup>
    <mo>+</mo> <mo>⋯</mo> <mo>+</mo> <msup><mrow><mo>(</mo><msub><mi>x</mi> <mi>p</mi></msub>
    <mo>-</mo><msub><mi>u</mi> <mi>p</mi></msub> <mo>)</mo></mrow> <mn>2</mn></msup></mrow></msqrt>
    <mo>.</mo></mrow></math>
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><msqrt><mrow><msup><mrow><mo>(</mo><msub><mi>x</mi>
    <mn>1</mn></msub> <mo>-</mo><msub><mi>u</mi> <mn>1</mn></msub> <mo>)</mo></mrow>
    <mn>2</mn></msup> <mo>+</mo> <msup><mrow><mo>(</mo><msub><mi>x</mi> <mn>2</mn></msub>
    <mo>-</mo><msub><mi>u</mi> <mn>2</mn></msub> <mo>)</mo></mrow> <mn>2</mn></msup>
    <mo>+</mo> <mo>⋯</mo> <mo>+</mo> <msup><mrow><mo>(</mo><msub><mi>x</mi> <mi>p</mi></msub>
    <mo>-</mo><msub><mi>u</mi> <mi>p</mi></msub> <mo>)</mo></mrow> <mn>2</mn></msup></mrow></msqrt>
    <mo>.</mo></mrow></math>
- en: 'Another common distance metric for numeric data is *Manhattan distance*:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个常见的数值数据距离度量是*曼哈顿距离*：
- en: <math display="block"><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow>
    <msub><mi>x</mi> <mn>1</mn></msub> <mo>−</mo> <msub><mi>u</mi> <mn>1</mn></msub>
    <mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow> <mo>+</mo> <mrow
    class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow> <msub><mi>x</mi> <mn>2</mn></msub>
    <mo>−</mo> <msub><mi>u</mi> <mn>2</mn></msub> <mrow class="MJX-TeXAtom-ORD"><mo
    stretchy="false">|</mo></mrow> <mo>+</mo> <mo>⋯</mo> <mo>+</mo> <mrow class="MJX-TeXAtom-ORD"><mo
    stretchy="false">|</mo></mrow> <msub><mi>x</mi> <mi>p</mi></msub> <mo>−</mo> <msub><mi>u</mi>
    <mi>p</mi></msub> <mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow></math>
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow>
    <msub><mi>x</mi> <mn>1</mn></msub> <mo>−</mo> <msub><mi>u</mi> <mn>1</mn></msub>
    <mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow> <mo>+</mo> <mrow
    class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow> <msub><mi>x</mi> <mn>2</mn></msub>
    <mo>−</mo> <msub><mi>u</mi> <mn>2</mn></msub> <mrow class="MJX-TeXAtom-ORD"><mo
    stretchy="false">|</mo></mrow> <mo>+</mo> <mo>⋯</mo> <mo>+</mo> <mrow class="MJX-TeXAtom-ORD"><mo
    stretchy="false">|</mo></mrow> <msub><mi>x</mi> <mi>p</mi></msub> <mo>−</mo> <msub><mi>u</mi>
    <mi>p</mi></msub> <mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow></math>
- en: Euclidean distance corresponds to the straight-line distance between two points
    (e.g., as the crow flies). Manhattan distance is the distance between two points
    traversed in a single direction at a time (e.g., traveling along rectangular city
    blocks). For this reason, Manhattan distance is a useful approximation if similarity
    is defined as point-to-point travel time.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 欧氏距离对应于两点之间的直线距离（例如，鸟儿飞行的距离）。曼哈顿距离是两点在单一方向上的距离（例如，沿着矩形城市街区行驶）。因此，如果相似性定义为点对点的旅行时间，则曼哈顿距离是一个有用的近似值。
- en: In measuring distance between two vectors, variables (features) that are measured
    with comparatively large scale will dominate the measure. For example, for the
    loan data, the distance would be almost solely a function of the income and loan
    amount variables, which are measured in tens or hundreds of thousands. Ratio variables
    would count for practically nothing in comparison. We address this problem by
    standardizing the data; see [“Standardization (Normalization, z-Scores)”](#Standardization).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在测量两个向量之间的距离时，具有相对较大规模的变量（特征）将主导测量结果。例如，在贷款数据中，距离几乎完全取决于收入和贷款金额这两个变量，这些变量的度量单位是数十或数百万。比率变量与之相比几乎不计。我们通过标准化数据来解决这个问题；参见[“标准化（归一化，z分数）”](#Standardization)。
- en: Other Distance Metrics
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 其他距离度量
- en: There are numerous other metrics for measuring distance between vectors. For
    numeric data, *Mahalanobis distance* is attractive since it accounts for the correlation
    between two variables. This is useful since if two variables are highly correlated,
    Mahalanobis will essentially treat these as a single variable in terms of distance.
    Euclidean and Manhattan distance do not account for the correlation, effectively
    placing greater weight on the attribute that underlies those features. Mahalanobis
    distance is the Euclidean distance between the principal components (see [“Principal
    Components Analysis”](ch07.xhtml#PCA)). The downside of using Mahalanobis distance
    is increased computational effort and complexity; it is computed using the *covariance
    matrix* (see [“Covariance Matrix”](ch05.xhtml#Covariance)).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 还有许多其他衡量向量之间距离的度量标准。对于数值数据，*马氏距离*很有吸引力，因为它考虑了两个变量之间的相关性。这是有用的，因为如果两个变量高度相关，马氏距离本质上将这些变量视为距离上的单一变量。欧氏距离和曼哈顿距离不考虑相关性，实际上更加关注支持这些特征的属性。马氏距离是主成分之间的欧氏距离（参见[“主成分分析”](ch07.xhtml#PCA)）。使用马氏距离的缺点是增加了计算工作量和复杂性；它是通过*协方差矩阵*计算的（参见[“协方差矩阵”](ch05.xhtml#Covariance)）。
- en: One Hot Encoder
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 独热编码器
- en: The loan data in [Table 6-1](#loan_data) includes several factor (string) variables.
    Most statistical and machine learning models require this type of variable to
    be converted to a series of binary dummy variables conveying the same information,
    as in [Table 6-2](#home_ownership_dummy). Instead of a single variable denoting
    the home occupant status as “owns with a mortgage,” “owns with no mortgage,” “rents,”
    or “other,” we end up with four binary variables. The first would be “owns with
    a mortgage—Y/N,” the second would be “owns with no mortgage—Y/N,” and so on. This
    one predictor, home occupant status, thus yields a vector with one 1 and three
    0s that can be used in statistical and machine learning algorithms. The phrase
    *one hot encoding* comes from digital circuit terminology, where it describes
    circuit settings in which only one bit is allowed to be positive (hot).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 6-1](#loan_data)中的贷款数据包含几个因子（字符串）变量。大多数统计和机器学习模型要求将这种类型的变量转换为一系列二进制虚拟变量，传达相同的信息，如[表 6-2](#home_ownership_dummy)所示。与仅仅表示家庭占有状态为“有抵押贷款”，“无抵押贷款”，“租房”或“其他”的单一变量不同，我们最终得到四个二进制变量。第一个变量将是“有抵押贷款—是/否”，第二个变量将是“无抵押贷款—是/否”，依此类推。因此，这一个预测因子，家庭占有状态，产生一个向量，其中有一个1和三个0，可用于统计和机器学习算法中。短语*独热编码*来源于数字电路术语，用来描述电路设置中只允许一个位为正（热）的情况。'
- en: Table 6-2\. Representing home ownership factor data in [Table 6-1](#loan_data)
    as a numeric dummy variable
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6-2\. 将[表 6-1](#loan_data)中的家庭占有因子数据表示为数字虚拟变量
- en: '| OWNS_WITH_MORTGAGE | OWNS_WITHOUT_MORTGAGE | OTHER | RENT |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| 有抵押贷款 | 无抵押贷款 | 其他 | 租房 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| 1 | 0 | 0 | 0 |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0 | 0 | 0 |'
- en: '| 1 | 0 | 0 | 0 |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0 | 0 | 0 |'
- en: '| 1 | 0 | 0 | 0 |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0 | 0 | 0 |'
- en: '| 1 | 0 | 0 | 0 |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0 | 0 | 0 |'
- en: '| 0 | 0 | 0 | 1 |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 0 | 0 | 1 |'
- en: '| 0 | 0 | 0 | 1 |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 0 | 0 | 1 |'
- en: Note
  id: totrans-55
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: In linear and logistic regression, one hot encoding causes problems with multicollinearity;
    see [“Multicollinearity”](ch04.xhtml#Multicollinearity). In such cases, one dummy
    is omitted (its value can be inferred from the other values). This is not an issue
    with KNN and other methods discussed in this book.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在线性和逻辑回归中，独热编码会导致多重共线性问题；参见[“多重共线性”](ch04.xhtml#Multicollinearity)。在这种情况下，会省略一个虚拟变量（其值可以从其他值推断出）。但在KNN和本书中讨论的其他方法中，这不是一个问题。
- en: Standardization (Normalization, z-Scores)
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 标准化（归一化，z-分数）
- en: 'In measurement, we are often not so much interested in “how much” but in “how
    different from the average.” Standardization, also called *normalization*, puts
    all variables on similar scales by subtracting the mean and dividing by the standard
    deviation; in this way, we ensure that a variable does not overly influence a
    model simply due to the scale of its original measurement:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在测量中，我们通常更关心的是“与平均值相比有多大差异”，而不是“具体数值是多少”。标准化，也称为*归一化*，通过减去均值并除以标准差，将所有变量放在类似的尺度上；这样做可以确保一个变量不会仅仅因为其原始测量的尺度而对模型产生过大影响：
- en: <math display="block"><mrow><mi>z</mi> <mo>=</mo> <mfrac><mrow><mi>x</mi><mo>-</mo><mover
    accent="true"><mi>x</mi> <mo>¯</mo></mover></mrow> <mi>s</mi></mfrac></mrow></math>
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mi>z</mi> <mo>=</mo> <mfrac><mrow><mi>x</mi><mo>-</mo><mover
    accent="true"><mi>x</mi> <mo>¯</mo></mover></mrow> <mi>s</mi></mfrac></mrow></math>
- en: The result of this transformation is commonly referred to as a *z-score*. Measurements
    are then stated in terms of “standard deviations away from the mean.”
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这种转换的结果通常被称为*z分数*。然后，测量结果以“距离平均值的标准差数”表示。
- en: Caution
  id: totrans-61
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: '*Normalization* in this statistical context is not to be confused with *database
    normalization*, which is the removal of redundant data and the verification of
    data dependencies.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个统计上下文中的*归一化*与*数据库归一化*不要混淆，后者是消除冗余数据并验证数据依赖性。
- en: 'For KNN and a few other procedures (e.g., principal components analysis and
    clustering), it is essential to consider standardizing the data prior to applying
    the procedure. To illustrate this idea, KNN is applied to the loan data using
    `dti` and `payment_inc_ratio` (see [“A Small Example: Predicting Loan Default”](#LoanExampleKNN))
    plus two other variables: `revol_bal`, the total revolving credit available to
    the applicant in dollars, and `revol_util`, the percent of the credit being used.
    The new record to be predicted is shown here:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 对于KNN和其他一些过程（例如主成分分析和聚类），在应用程序之前考虑对数据进行标准化是至关重要的。为了说明这个想法，KNN应用于贷款数据，使用`dti`和`payment_inc_ratio`（参见[“一个小例子：预测贷款违约”](#LoanExampleKNN)）以及另外两个变量：`revol_bal`，申请人的总可循环信贷额度（以美元计），和`revol_util`，使用的信贷百分比。待预测的新记录如下所示：
- en: '[PRE2]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The magnitude of `revol_bal`, which is in dollars, is much bigger than that
    of the other variables. The `knn` function returns the index of the nearest neighbors
    as an attribute `nn.index`, and this can be used to show the top-five closest
    rows in `loan_df`:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '`revol_bal`的大小（以美元计）比其他变量大得多。`knn`函数将最近邻的索引作为属性`nn.index`返回，可以用来显示`loan_df`中最接近的五行：'
- en: '[PRE3]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Following the model fit, we can use the `kneighbors` method to identify the
    five closest rows in the training set with `scikit-learn`:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型拟合后，我们可以使用`scikit-learn`的`kneighbors`方法来识别训练集中与`loan_df`中最接近的五行：
- en: '[PRE4]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The value of `revol_bal` in these neighbors is very close to its value in the
    new record, but the other predictor variables are all over the map and essentially
    play no role in determining neighbors.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这些邻居中`revol_bal`的值非常接近新记录中的值，但其他预测变量却大相径庭，基本上不起作用。
- en: 'Compare this to KNN applied to the standardized data using the *R* function
    `scale`, which computes the *z*-score for each variable:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 将此与使用*R*函数`scale`对标准化数据应用KNN进行比较，该函数计算每个变量的*z*-分数：
- en: '[PRE5]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[![1](Images/1.png)](#co_statistical_machine_learning_CO1-1)'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_statistical_machine_learning_CO1-1)'
- en: We need to remove the first row from `loan_df` as well, so that the row numbers
    correspond to each other.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要从`loan_df`中删除第一行，以便行号相互对应。
- en: 'The `sklearn.preprocessing.StandardScaler` method is first trained with the
    predictors and is subsequently used to transform the data set prior to training
    the KNN model:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 首先使用预测变量训练`sklearn.preprocessing.StandardScaler`方法，然后在训练KNN模型之前对数据集进行转换：
- en: '[PRE6]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The five nearest neighbors are much more alike in all the variables, providing
    a more sensible result. Note that the results are displayed on the original scale,
    but KNN was applied to the scaled data and the new loan to be predicted.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 最接近的五个邻居在所有变量上都更相似，从而提供了更合理的结果。请注意，结果显示在原始比例上，但是KNN是应用于经过缩放的数据和新贷款预测的。
- en: Tip
  id: totrans-77
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: Using the *z*-score is just one way to rescale variables. Instead of the mean,
    a more robust estimate of location could be used, such as the median. Likewise,
    a different estimate of scale such as the interquartile range could be used instead
    of the standard deviation. Sometimes, variables are “squashed” into the 0–1 range.
    It’s also important to realize that scaling each variable to have unit variance
    is somewhat arbitrary. This implies that each variable is thought to have the
    same importance in predictive power. If you have subjective knowledge that some
    variables are more important than others, then these could be scaled up. For example,
    with the loan data, it is reasonable to expect that the payment-to-income ratio
    is very important.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 使用*z*-分数只是重新缩放变量的一种方法。可以使用更健壮的位置估计，例如中位数，而不是均值。同样，可以使用不同的尺度估计，例如四分位距，而不是标准偏差。有时，变量被“压缩”到0–1范围内也很重要。还要意识到，将每个变量缩放为单位方差在某种程度上是任意的。这意味着每个变量在预测能力中被认为具有相同的重要性。如果您有主观知识表明某些变量比其他变量更重要，那么可以将它们放大。例如，对于贷款数据，可以合理地期望支付收入比非常重要。
- en: Note
  id: totrans-79
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Normalization (standardization) does not change the distributional shape of
    the data; it does not make it normally shaped if it was not already normally shaped
    (see [“Normal Distribution”](ch02.xhtml#NormalDist)).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 标准化（标准化）不会改变数据的分布形状；如果数据不是正态分布的，则不会使其成为正态分布（参见[“正态分布”](ch02.xhtml#NormalDist)）。
- en: Choosing K
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择K
- en: 'The choice of *K* is very important to the performance of KNN. The simplest
    choice is to set <math alttext="upper K equals 1"><mrow><mi>K</mi> <mo>=</mo>
    <mn>1</mn></mrow></math> , known as the 1-nearest neighbor classifier. The prediction
    is intuitive: it is based on finding the data record in the training set most
    similar to the new record to be predicted. Setting <math alttext="upper K equals
    1"><mrow><mi>K</mi> <mo>=</mo> <mn>1</mn></mrow></math> is rarely the best choice;
    you’ll almost always obtain superior performance by using *K* > 1-nearest neighbors.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 选择*K*对KNN的性能非常重要。最简单的选择是设置<math alttext="upper K equals 1"><mrow><mi>K</mi>
    <mo>=</mo> <mn>1</mn></mrow></math>，即1-最近邻分类器。预测直观：它基于找到训练集中与待预测的新记录最相似的数据记录。设置<math
    alttext="upper K equals 1"><mrow><mi>K</mi> <mo>=</mo> <mn>1</mn></mrow></math>很少是最佳选择；使用*K*
    > 1-最近邻几乎总能获得更好的性能。
- en: 'Generally speaking, if *K* is too low, we may be overfitting: including the
    noise in the data. Higher values of *K* provide smoothing that reduces the risk
    of overfitting in the training data. On the other hand, if *K* is too high, we
    may oversmooth the data and miss out on KNN’s ability to capture the local structure
    in the data, one of its main advantages.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 一般而言，如果*K*太低，我们可能会过度拟合：包括数据中的噪音。较高的*K*值提供了平滑处理，从而降低了在训练数据中过度拟合的风险。另一方面，如果*K*太高，我们可能会过度平滑数据，错过KNN捕获数据中的局部结构的能力，这是其主要优势之一。
- en: The *K* that best balances between overfitting and oversmoothing is typically
    determined by accuracy metrics and, in particular, accuracy with holdout or validation
    data. There is no general rule about the best *K*—it depends greatly on the nature
    of the data. For highly structured data with little noise, smaller values of *K*
    work best. Borrowing a term from the signal processing community, this type of
    data is sometimes referred to as having a high *signal-to-noise ratio* (*SNR*).
    Examples of data with a typically high SNR are data sets for handwriting and speech
    recognition. For noisy data with less structure (data with a low SNR), such as
    the loan data, larger values of *K* are appropriate. Typically, values of *K*
    fall in the range 1 to 20. Often, an odd number is chosen to avoid ties.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 最好平衡过拟合和过度平滑之间的*K*通常由准确度指标来确定，特别是在使用留出或验证数据进行准确度评估时。关于最佳*K*没有通用规则——它在很大程度上取决于数据的性质。对于结构化程度高且噪音少的数据，较小的*K*值效果最佳。从信号处理社区借来一个术语，这种类型的数据有时被称为具有高*信噪比*（*SNR*）的数据。具有典型高SNR的数据示例包括手写和语音识别数据集。对于噪音较多、结构较少的数据（信噪比低的数据），例如贷款数据，适合使用较大的*K*值。通常，*K*值落在1到20的范围内。通常选择奇数以避免平局。
- en: Bias-Variance Trade-off
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 偏差-方差权衡
- en: The tension between oversmoothing and overfitting is an instance of the *bias-variance
    trade-off*, a ubiquitous problem in statistical model fitting. Variance refers
    to the modeling error that occurs because of the choice of training data; that
    is, if you were to choose a different set of training data, the resulting model
    would be different. Bias refers to the modeling error that occurs because you
    have not properly identified the underlying real-world scenario; this error would
    not disappear if you simply added more training data. When a flexible model is
    overfit, the variance increases. You can reduce this by using a simpler model,
    but the bias may increase due to the loss of flexibility in modeling the real
    underlying situation. A general approach to handling this trade-off is through
    *cross-validation*. See [“Cross-Validation”](ch04.xhtml#CrossValidation) for more
    details.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 过度平滑和过度拟合之间的张力是*偏差-方差权衡*的一个例子，这是统计模型拟合中普遍存在的问题。方差是指由于选择训练数据而产生的建模误差；也就是说，如果选择不同的训练数据集，得到的模型会不同。偏差是指由于未能正确识别出真实世界情况而产生的建模误差；如果简单地添加更多的训练数据，这种误差不会消失。当一个灵活的模型过度拟合时，方差会增加。您可以通过使用更简单的模型来减少这种情况，但由于失去了对真实情况建模的灵活性，偏差可能会增加。处理这种权衡的一般方法是通过*交叉验证*。详细信息请参见[“交叉验证”](ch04.xhtml#CrossValidation)。
- en: KNN as a Feature Engine
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 作为特征引擎的KNN
- en: 'KNN gained its popularity due to its simplicity and intuitive nature. In terms
    of performance, KNN by itself is usually not competitive with more sophisticated
    classification techniques. In practical model fitting, however, KNN can be used
    to add “local knowledge” in a staged process with other classification techniques:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: KNN 之所以受欢迎，是因为其简单直观的特性。就性能而言，KNN 本身通常无法与更复杂的分类技术竞争。然而，在实际模型拟合中，KNN 可以与其他分类技术一起以分阶段的方式使用，以添加“局部知识”：
- en: KNN is run on the data, and for each record, a classification (or quasi-probability
    of a class) is derived.
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: KNN 对数据进行运算，对于每条记录，都会得出一个分类（或类的准概率）。
- en: That result is added as a new feature to the record, and another classification
    method is then run on the data. The original predictor variables are thus used
    twice.
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 结果被添加为记录的新特征，然后再对数据运行另一种分类方法。因此，原始预测变量被使用了两次。
- en: At first you might wonder whether this process, since it uses some predictors
    twice, causes a problem with multicollinearity (see [“Multicollinearity”](ch04.xhtml#Multicollinearity)).
    This is not an issue, since the information being incorporated into the second-stage
    model is highly local, derived only from a few nearby records, and is therefore
    additional information and not redundant.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 起初，您可能会想知道，由于它两次使用了一些预测变量，这个过程是否会导致多重共线性问题（参见[“多重共线性”](ch04.xhtml#Multicollinearity)）。这不是一个问题，因为被纳入第二阶段模型的信息是高度局部的，仅来自几个附近的记录，因此是额外信息而不是冗余信息。
- en: Note
  id: totrans-92
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: You can think of this staged use of KNN as a form of ensemble learning, in which
    multiple predictive modeling methods are used in conjunction with one another.
    It can also be considered as a form of feature engineering in which the aim is
    to derive features (predictor variables) that have predictive power. Often this
    involves some manual review of the data; KNN gives a fairly automatic way to do
    this.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以将KNN的这种分阶段使用视为集成学习的一种形式，其中多个预测建模方法与彼此结合使用。它也可以被视为一种特征工程形式，其目的是提取具有预测能力的特征（预测变量）。通常这涉及对数据的一些手动审查；KNN提供了一种相对自动化的方法来实现这一点。
- en: 'For example, consider the King County housing data. In pricing a home for sale,
    a realtor will base the price on similar homes recently sold, known as “comps.”
    In essence, realtors are doing a manual version of KNN: by looking at the sale
    prices of similar homes, they can estimate what a home will sell for. We can create
    a new feature for a statistical model to mimic the real estate professional by
    applying KNN to recent sales. The predicted value is the sales price, and the
    existing predictor variables could include location, total square feet, type of
    structure, lot size, and number of bedrooms and bathrooms. The new predictor variable
    (feature) that we add via KNN is the KNN predictor for each record (analogous
    to the realtors’ comps). Since we are predicting a numerical value, the average
    of the *K*-Nearest Neighbors is used instead of a majority vote (known as *KNN
    regression*).'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑金县房屋数据。在定价待售住宅时，房地产经纪人将根据最近售出的类似房屋——称为“comps”——确定价格。本质上，房地产经纪人正在执行KNN的手动版本：通过查看类似房屋的销售价格，他们可以估计一处住宅的销售价格。我们可以为统计模型创建一个新的特征，以模仿房地产专业人员通过KNN应用到最近销售中的做法。预测值是销售价格，现有的预测变量可以包括位置、总面积、建筑类型、土地面积以及卧室和浴室数量。我们通过KNN添加的新预测变量（特征）是每条记录的KNN预测器（类似于房地产经纪人的comps）。由于我们在预测数值值，所以使用K最近邻居的平均值，而不是多数投票（称为*KNN回归*）。
- en: 'Similarly, for the loan data, we can create features that represent different
    aspects of the loan process. For example, the following *R* code would build a
    feature that represents a borrower’s creditworthiness:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，对于贷款数据，我们可以创建代表贷款流程不同方面的特征。例如，以下*R*代码将构建一个代表借款人信用价值的特征：
- en: '[PRE7]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'With `scikit-learn`, we use the `predict_proba` method of the trained model
    to get the probabilities:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`scikit-learn`，我们使用训练模型的`predict_proba`方法来获取概率：
- en: '[PRE8]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The result is a feature that predicts the likelihood a borrower will default
    based on his credit history.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是基于借款人的信用历史预测借款人违约可能性的特征。
- en: Tree Models
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 树模型
- en: Tree models, also called *Classification and Regression Trees* (*CART*),^([3](ch06.xhtml#idm46522843869576))
    *decision trees*, or just *trees*, are an effective and popular classification
    (and regression) method initially developed by Leo Breiman and others in 1984.
    Tree models, and their more powerful descendants *random forests* and *boosted
    trees* (see [“Bagging and the Random Forest”](#Bagging) and [“Boosting”](#Boosting)),
    form the basis for the most widely used and powerful predictive modeling tools
    in data science for regression and classification.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 树模型，也称为*分类和回归树*（*CART*），^([3](ch06.xhtml#idm46522843869576)) *决策树*或简称*树*，是一种由Leo
    Breiman等人在1984年最初开发的有效和流行的分类（和回归）方法。树模型及其更强大的后继者*随机森林*和*提升树*（见[“Bagging和随机森林”](#Bagging)和[“提升”](#Boosting)）构成了数据科学中用于回归和分类的最广泛使用和强大的预测建模工具的基础。
- en: A tree model is a set of “if-then-else” rules that are easy to understand and
    to implement. In contrast to linear and logistic regression, trees have the ability
    to discover hidden patterns corresponding to complex interactions in the data.
    However, unlike KNN or naive Bayes, simple tree models can be expressed in terms
    of predictor relationships that are easily interpretable.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 树模型是一组易于理解和实现的“如果-那么-否则”规则。与线性和逻辑回归相比，树具有发现数据中复杂交互作用的隐藏模式的能力。然而，与KNN或朴素贝叶斯不同，简单的树模型可以用易于解释的预测者关系来表达。
- en: Decision Trees in Operations Research
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运筹学中的决策树
- en: The term *decision trees* has a different (and older) meaning in decision science
    and operations research, where it refers to a human decision analysis process.
    In this meaning, decision points, possible outcomes, and their estimated probabilities
    are laid out in a branching diagram, and the decision path with the maximum expected
    value is chosen.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '*决策树*一词在决策科学和运筹学中有着不同（也更老的）含义，它指的是一种人类决策分析过程。在这个含义下，决策点、可能的结果以及它们的估计概率被列在一个分支图表中，选择具有最大期望值的决策路径。'
- en: A Simple Example
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一个简单的例子
- en: 'The two main packages to fit tree models in *R* are `rpart` and `tree`. Using
    the `rpart` package, a model is fit to a sample of 3,000 records of the loan data
    using the variables `payment_inc_ratio` and `borrower_score` (see [“K-Nearest
    Neighbors”](#KNN) for a description of the data):'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *R* 中适配树模型的两个主要包是 `rpart` 和 `tree`。使用 `rpart` 包，将模型拟合到 3,000 条贷款数据记录样本，使用变量
    `payment_inc_ratio` 和 `borrower_score`（见[“K-最近邻”](#KNN)以获取数据描述）：
- en: '[PRE9]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The `sklearn.tree.DecisionTreeClassifier` provides an implementation of a decision
    tree. The `dmba` package provides a convenience function to create a visualization
    inside a Jupyter notebook:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '`sklearn.tree.DecisionTreeClassifier` 提供了一个决策树的实现。`dmba` 包提供了一个方便的函数，在 Jupyter
    笔记本内创建可视化：'
- en: '[PRE10]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The resulting tree is shown in [Figure 6-3](#LoanTree). Due to the different
    implementations, you will find that the results from *R* and *Python* are not
    identical; this is expected. These classification rules are determined by traversing
    through a hierarchical tree, starting at the root and moving left if the node
    is true and right if not, until a leaf is reached.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 结果树显示在 [图 6-3](#LoanTree) 中。由于不同的实现，您会发现 *R* 和 *Python* 的结果并不相同；这是预期的。这些分类规则是通过遍历层次树来确定的，从根开始，如果节点为真则向左移动，否则向右移动，直到达到叶子节点。
- en: Typically, the tree is plotted upside-down, so the root is at the top and the
    leaves are at the bottom. For example, if we get a loan with `borrower_score`
    of 0.6 and a `payment_inc_ratio` of 8.0, we end up at the leftmost leaf and predict
    the loan will be paid off.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，树被倒置绘制，所以根在顶部，叶在底部。例如，如果我们获得一个 `borrower_score` 为 0.6 和 `payment_inc_ratio`
    为 8.0 的贷款，我们将在最左边的叶子处结束，并预测该贷款将被偿还。
- en: '![The rules for a simple tree model fit to the loan data.](Images/psd2_0603.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![一个简单的树模型拟合到贷款数据的规则。](Images/psd2_0603.png)'
- en: Figure 6-3\. The rules for a simple tree model fit to the loan data
  id: totrans-113
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-3\. 一个简单的树模型拟合到贷款数据的规则
- en: 'A nicely printed version of the tree is also easily produced in *R*:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *R* 中也很容易产生树的漂亮打印版本：
- en: '[PRE11]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The depth of the tree is shown by the indent. Each node corresponds to a provisional
    classification determined by the prevalent outcome in that partition. The “loss”
    is the number of misclassifications yielded by the provisional classification
    in a partition. For example, in node 2, there were 261 misclassifications out
    of a total of 878 total records. The values in the parentheses correspond to the
    proportion of records that are paid off or in default, respectively. For example,
    in node 13, which predicts default, over 60 percent of the records are loans that
    are in default.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 树的深度由缩进显示。每个节点对应于由该分区中普遍结果确定的临时分类。 “损失”是由分区中的临时分类产生的错误分类数。例如，在节点 2 中，总共有 878
    条记录中的 261 条错误分类。括号中的值分别对应于已偿还或违约的记录比例。例如，在预测违约的节点 13 中，超过 60% 的记录是违约贷款。
- en: 'The `scikit-learn` documentation describes how to create a text representation
    of a decision tree model. We included a convenience function in our `dmba` package:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '`scikit-learn` 文档描述了如何创建决策树模型的文本表示。我们在我们的 `dmba` 包中包含了一个方便的函数：'
- en: '[PRE12]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The Recursive Partitioning Algorithm
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 递归分区算法
- en: The algorithm to construct a decision tree, called *recursive partitioning*,
    is straightforward and intuitive. The data is repeatedly partitioned using predictor
    values that do the best job of separating the data into relatively homogeneous
    partitions. [Figure 6-4](#LoanRecursivePartitioning) shows the partitions created
    for the tree in [Figure 6-3](#LoanTree). The first rule, depicted by rule 1, is
    `borrower_score >= 0.575` and segments the right portion of the plot. The second
    rule is `borrower_score < 0.375` and segments the left portion.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 构建决策树的算法称为*递归分区*，非常直观和简单。数据使用能够最好地将数据分隔成相对同质分区的预测器值反复分区。[图 6-4](#LoanRecursivePartitioning)显示了[图 6-3](#LoanTree)中树创建的分区。第一个规则由规则1描述，即`borrower_score
    >= 0.575`并分割绘图的右侧部分。第二个规则是`borrower_score < 0.375`并分割左侧部分。
- en: '![The first five rules for a simple tree model fit to the loan data.](Images/psd2_0604.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![适用于贷款数据的简单树模型的前五条规则。](Images/psd2_0604.png)'
- en: Figure 6-4\. The first three rules for a simple tree model fit to the loan data
  id: totrans-122
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-4. 适用于贷款数据的简单树模型的前三条规则
- en: 'Suppose we have a response variable *Y* and a set of *P* predictor variables
    *X[j]* for <math alttext="j equals 1 comma ellipsis comma upper P"><mrow><mi>j</mi>
    <mo>=</mo> <mn>1</mn> <mo>,</mo> <mo>⋯</mo> <mo>,</mo> <mi>P</mi></mrow></math>
    . For a partition *A* of records, recursive partitioning will find the best way
    to partition *A* into two subpartitions:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有响应变量*Y*和一组*P*个预测变量*X[j]*，对于<math alttext="j equals 1 comma ellipsis comma
    upper P"><mrow><mi>j</mi> <mo>=</mo> <mn>1</mn> <mo>,</mo> <mo>⋯</mo> <mo>,</mo>
    <mi>P</mi></mrow></math>的分区*A*，递归分区将找到将*A*分割为两个子分区的最佳方法：
- en: 'For each predictor variable *X[j]*:'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个预测变量*X[j]*：
- en: 'For each value *s[j]* of *X[j]*:'
  id: totrans-125
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个*X[j]*的值*s[j]*：
- en: Split the records in *A* with *X[j]* values < *s[j]* as one partition, and the
    remaining records where *X[j]* ≥ *s[j]* as another partition.
  id: totrans-126
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将具有*X[j]*值小于*s[j]*的记录在*A*中拆分为一个分区，将其余*X[j]*值大于或等于*s[j]*的记录拆分为另一个分区。
- en: Measure the homogeneity of classes within each subpartition of *A*.
  id: totrans-127
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 测量*A*每个子分区内类的同质性。
- en: Select the value of *s[j]* that produces maximum within-partition homogeneity
    of class.
  id: totrans-128
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择产生类内最大同质性的*s[j]*的值。
- en: Select the variable *X[j]* and the split value *s[j]* that produces maximum
    within-partition homogeneity of class.
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择变量*X[j]*和分割值*s[j]*，以产生类内最大同质性。
- en: 'Now comes the recursive part:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 现在进入递归部分：
- en: Initialize *A* with the entire data set.
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用整个数据集初始化*A*。
- en: Apply the partitioning algorithm to split *A* into two subpartitions, *A[1]*
    and *A[2]*.
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应用分区算法将*A*分割成两个子分区，*A[1]*和*A[2]*。
- en: Repeat step 2 on subpartitions *A[1]* and *A[2]*.
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在子分区*A[1]*和*A[2]*上重复步骤2。
- en: The algorithm terminates when no further partition can be made that sufficiently
    improves the homogeneity of the partitions.
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当不能再进行足够改进分区同质性的分区时，算法终止。
- en: The end result is a partitioning of the data, as in [Figure 6-4](#LoanRecursivePartitioning),
    except in *P*-dimensions, with each partition predicting an outcome of 0 or 1
    depending on the majority vote of the response in that partition.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 最终结果是数据的分区，如[图 6-4](#LoanRecursivePartitioning)中所示，除了在*P*维度中，每个分区根据该分区中响应的多数投票预测为0或1的结果。
- en: Note
  id: totrans-136
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'In addition to a binary 0/1 prediction, tree models can produce a probability
    estimate based on the number of 0s and 1s in the partition. The estimate is simply
    the sum of 0s or 1s in the partition divided by the number of observations in
    the partition:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 除了进行二元0/1预测外，树模型还可以根据分区中0和1的数量生成概率估计。估计值简单地是分区中0或1的总和除以分区中的观察次数：
- en: <math display="block"><mrow><mtext>Prob</mtext> <mrow><mo>(</mo> <mi>Y</mi>
    <mo>=</mo> <mn>1</mn> <mo>)</mo></mrow> <mo>=</mo> <mfrac><mrow><mtext>Number</mtext><mtext>of</mtext><mtext>1s</mtext><mtext>in</mtext><mtext>the</mtext><mtext>partition</mtext></mrow>
    <mrow><mtext>Size</mtext><mtext>of</mtext><mtext>the</mtext><mtext>partition</mtext></mrow></mfrac></mrow></math>
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mtext>Prob</mtext> <mrow><mo>(</mo> <mi>Y</mi>
    <mo>=</mo> <mn>1</mn> <mo>)</mo></mrow> <mo>=</mo> <mfrac><mrow><mtext>Number</mtext><mtext>of</mtext><mtext>1s</mtext><mtext>in</mtext><mtext>the</mtext><mtext>partition</mtext></mrow>
    <mrow><mtext>Size</mtext><mtext>of</mtext><mtext>the</mtext><mtext>partition</mtext></mrow></mfrac></mrow></math>
- en: The estimated <math alttext="Prob left-parenthesis upper Y equals 1 right-parenthesis"><mrow><mtext>Prob</mtext>
    <mo>(</mo> <mi>Y</mi> <mo>=</mo> <mn>1</mn> <mo>)</mo></mrow></math> can then
    be converted to a binary decision; for example, set the estimate to 1 if Prob(*Y*
    = 1) > 0.5.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 估计值<math alttext="Prob left-parenthesis upper Y equals 1 right-parenthesis"><mrow><mtext>Prob</mtext>
    <mo>(</mo> <mi>Y</mi> <mo>=</mo> <mn>1</mn> <mo>)</mo></mrow></math>然后可以转换为二进制决策；例如，如果Prob(*Y*
    = 1) > 0.5，则将估计值设置为1。
- en: Measuring Homogeneity or Impurity
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 测量同质性或不纯度
- en: Tree models recursively create partitions (sets of records), *A*, that predict
    an outcome of *Y* = 0 or *Y* = 1. You can see from the preceding algorithm that
    we need a way to measure homogeneity, also called *class purity*, within a partition.
    Or equivalently, we need to measure the impurity of a partition. The accuracy
    of the predictions is the proportion *p* of misclassified records within that
    partition, which ranges from 0 (perfect) to 0.5 (purely random guessing).
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 树模型递归地创建预测*Y* = 0或*Y* = 1的分区（记录集）*A*。从前面的算法中可以看出，我们需要一种方法来衡量分区内的同质性，也称为*类纯度*。或者等效地，我们需要衡量分区的不纯度。预测的准确性是分区内错误分类的记录比例*p*，其范围从0（完美）到0.5（纯粹的随机猜测）。
- en: 'It turns out that accuracy is not a good measure for impurity. Instead, two
    common measures for impurity are the *Gini impurity* and *entropy* of *information*.
    While these (and other) impurity measures apply to classification problems with
    more than two classes, we focus on the binary case. The Gini impurity for a set
    of records *A* is:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，准确率不是衡量纯度的好指标。相反，衡量纯度的两个常见方法是*基尼不纯度*和*信息熵*。虽然这些（以及其他）纯度度量适用于具有两个以上类别的分类问题，但我们关注二元情况。一组记录*A*的基尼不纯度为：
- en: <math display="block"><mrow><mi>I</mi> <mo>(</mo> <mi>A</mi> <mo>)</mo> <mo>=</mo>
    <mi>p</mi> <mo>(</mo> <mn>1</mn> <mo>-</mo> <mi>p</mi> <mo>)</mo></mrow></math>
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mi>I</mi> <mo>(</mo> <mi>A</mi> <mo>)</mo> <mo>=</mo>
    <mi>p</mi> <mo>(</mo> <mn>1</mn> <mo>-</mo> <mi>p</mi> <mo>)</mo></mrow></math>
- en: 'The entropy measure is given by:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 熵测量由以下公式给出：
- en: <math display="block"><mrow><mi>I</mi> <mrow><mo>(</mo> <mi>A</mi> <mo>)</mo></mrow>
    <mo>=</mo> <mo>-</mo> <mi>p</mi> <msub><mo form="prefix">log</mo> <mn>2</mn></msub>
    <mrow><mo>(</mo> <mi>p</mi> <mo>)</mo></mrow> <mo>-</mo> <mrow><mo>(</mo> <mn>1</mn>
    <mo>-</mo> <mi>p</mi> <mo>)</mo></mrow> <msub><mo form="prefix">log</mo> <mn>2</mn></msub>
    <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <mi>p</mi> <mo>)</mo></mrow></mrow></math>
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mi>I</mi> <mrow><mo>(</mo> <mi>A</mi> <mo>)</mo></mrow>
    <mo>=</mo> <mo>-</mo> <mi>p</mi> <msub><mo form="prefix">log</mo> <mn>2</mn></msub>
    <mrow><mo>(</mo> <mi>p</mi> <mo>)</mo></mrow> <mo>-</mo> <mrow><mo>(</mo> <mn>1</mn>
    <mo>-</mo> <mi>p</mi> <mo>)</mo></mrow> <msub><mo form="prefix">log</mo> <mn>2</mn></msub>
    <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <mi>p</mi> <mo>)</mo></mrow></mrow></math>
- en: '[Figure 6-5](#Impurity) shows that Gini impurity (rescaled) and entropy measures
    are similar, with entropy giving higher impurity scores for moderate and high
    accuracy rates.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 6-5](#Impurity)显示基尼不纯度（重新缩放）和熵测量类似，熵对中等和高准确率得分给出较高的不纯度评分。'
- en: '![Gini impurity and entropy measures.](Images/psd2_0605.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![基尼不纯度和熵测量。](Images/psd2_0605.png)'
- en: Figure 6-5\. Gini impurity and entropy measures
  id: totrans-148
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-5。基尼不纯度和熵测量
- en: Gini Coefficient
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基尼系数
- en: Gini impurity is not to be confused with the *Gini coefficient*. They represent
    similar concepts, but the Gini coefficient is limited to the binary classification
    problem and is related to the AUC metric (see [“AUC”](ch05.xhtml#AUC)).
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 不要将基尼不纯度与*基尼系数*混淆。它们代表类似的概念，但基尼系数仅适用于二元分类问题，并且与AUC度量相关（参见[“AUC”](ch05.xhtml#AUC)）。
- en: The impurity metric is used in the splitting algorithm described earlier. For
    each proposed partition of the data, impurity is measured for each of the partitions
    that result from the split. A weighted average is then calculated, and whichever
    partition (at each stage) yields the lowest weighted average is selected.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 不纯度度量在前面描述的分裂算法中使用。对于数据的每个拟议分区，都会测量每个分裂产生的分区的不纯度。然后计算加权平均值，并选择（在每个阶段）产生最低加权平均值的分区。
- en: Stopping the Tree from Growing
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 阻止树继续生长
- en: As the tree grows bigger, the splitting rules become more detailed, and the
    tree gradually shifts from identifying “big” rules that identify real and reliable
    relationships in the data to “tiny” rules that reflect only noise. A fully grown
    tree results in completely pure leaves and, hence, 100% accuracy in classifying
    the data that it is trained on. This accuracy is, of course, illusory—we have
    overfit (see [“Bias-Variance Trade-off”](#bvt_note)) the data, fitting the noise
    in the training data, not the signal that we want to identify in new data.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 随着树变得越来越大，分裂规则变得更加详细，树逐渐从识别数据中真实可靠的关系的“大”规则转变为反映了只有噪音的“微小”规则。完全生长的树导致完全纯净的叶子，因此在对其进行训练的数据上对数据进行分类的准确率为100%。当然，这种准确率是虚假的——我们已经过度拟合（见[“偏差-方差权衡”](#bvt_note)）数据，拟合了训练数据中的噪音，而不是我们想要在新数据中识别的信号。
- en: 'We need some way to determine when to stop growing a tree at a stage that will
    generalize to new data. There are various ways to stop splitting in *R* and *Python*:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要一种方法来确定何时停止树的生长，以便在一个阶段上对新数据进行泛化。在*R*和*Python*中，有各种方法来停止分裂：
- en: Avoid splitting a partition if a resulting subpartition is too small, or if
    a terminal leaf is too small. In `rpart` (*R*), these constraints are controlled
    separately by the parameters `minsplit` and `minbucket`, respectively, with defaults
    of `20` and `7`. In *Python*’s `DecisionTreeClassifier`, we can control this using
    the parameters `min_samples_split` (default `2`) and `min_samples_leaf` (default
    `1`).
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 避免分裂分区，如果结果子分区太小，或者终端叶子太小。在`rpart`（*R*）中，这些约束分别由参数`minsplit`和`minbucket`控制，默认值分别为`20`和`7`。在*Python*的`DecisionTreeClassifier`中，我们可以使用参数`min_samples_split`（默认为`2`）和`min_samples_leaf`（默认为`1`）来控制这一点。
- en: Don’t split a partition if the new partition does not “significantly” reduce
    the impurity. In `rpart`, this is controlled by the *complexity parameter* `cp`,
    which is a measure of how complex a tree is—the more complex, the greater the
    value of `cp`. In practice, `cp` is used to limit tree growth by attaching a penalty
    to additional complexity (splits) in a tree. `DecisionTreeClassifier` (*Python*)
    has the parameter `min_impurity_decrease`, which limits splitting based on a weighted
    impurity decrease value. Here, smaller values will lead to more complex trees.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果新的分区不能“显著”减少不纯度，则不要分割分区。在`rpart`中，这由*复杂度参数* `cp` 控制，它衡量树的复杂程度——越复杂，`cp`值越大。在实践中，`cp`用于通过对树中额外复杂度（分割）附加惩罚来限制树的增长。`DecisionTreeClassifier`（*Python*）具有参数`min_impurity_decrease`，它限制基于加权不纯度减少值的分割。在这里，较小的值将导致更复杂的树。
- en: These methods involve arbitrary rules and can be useful for exploratory work,
    but we can’t easily determine optimum values (i.e., values that maximize predictive
    accuracy with new data). We need to combine cross-validation with either systematically
    changing the model parameters or modifying the tree through pruning.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法涉及任意规则，对探索性工作可能有用，但我们无法轻易确定最优值（即最大化使用新数据预测准确性的值）。我们需要结合交叉验证和系统地更改模型参数或通过修剪修改树。
- en: Controlling tree complexity in *R*
  id: totrans-158
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在*R*中控制树的复杂度。
- en: With the complexity parameter, `cp`, we can estimate what size tree will perform
    best with new data. If `cp` is too small, then the tree will overfit the data,
    fitting noise and not signal. On the other hand, if `cp` is too large, then the
    tree will be too small and have little predictive power. The default in `rpart`
    is 0.01, although for larger data sets, you are likely to find this is too large.
    In the previous example, `cp` was set to `0.005` since the default led to a tree
    with a single split. In exploratory analysis, it is sufficient to simply try a
    few values.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 使用复杂度参数`cp`，我们可以估计什么样的树在新数据中表现最佳。如果`cp`太小，则树将过度拟合数据，适应噪声而不是信号。另一方面，如果`cp`太大，则树将过于简单且具有较少的预测能力。在`rpart`中，默认值为0.01，尽管对于更大的数据集，你可能会发现这个值太大了。在前面的例子中，`cp`设置为`0.005`，因为默认值导致树只有一个分割。在探索性分析中，仅需尝试几个值即可。
- en: 'Determining the optimum `cp` is an instance of the bias-variance trade-off.
    The most common way to estimate a good value of `cp` is via cross-validation (see
    [“Cross-Validation”](ch04.xhtml#CrossValidation)):'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 确定最佳的`cp`值是偏差-方差权衡的一个实例。估计一个好的`cp`值的最常见方法是通过交叉验证（参见[“交叉验证”](ch04.xhtml#CrossValidation)）：
- en: Partition the data into training and validation (holdout) sets.
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据分割为训练集和验证（留置）集。
- en: Grow the tree with the training data.
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用训练数据生长树。
- en: Prune it successively, step by step, recording `cp` (using the *training* data)
    at each step.
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 逐步修剪树，在每一步中使用*训练*数据记录`cp`。
- en: Note the `cp` that corresponds to the minimum error (loss) on the *validation*
    data.
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 注意对应于*验证*数据上最小错误（损失）的`cp`值。
- en: Repartition the data into training and validation, and repeat the growing, pruning,
    and `cp` recording process.
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据重新分割为训练和验证集，并重复生成、修剪和记录`cp`的过程。
- en: Do this again and again, and average the `cp`s that reflect minimum error for
    each tree.
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一次又一次地执行此操作，并计算反映每棵树最小错误的`cp`的平均值。
- en: Go back to the original data, or future data, and grow a tree, stopping at this
    optimum `cp` value.
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 回到原始数据或未来数据，并以此最佳的`cp`值停止生成树。
- en: In `rpart`, you can use the argument `cptable` to produce a table of the `cp`
    values and their associated cross-validation error (`xerror` in *R*), from which
    you can determine the `cp` value that has the lowest cross-validation error.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在`rpart`中，可以使用参数`cptable`生成`cp`值及其相关的交叉验证错误（在*R*中为`xerror`）的表格，从中可以确定具有最低交叉验证错误的`cp`值。
- en: Controlling tree complexity in *Python*
  id: totrans-169
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在*Python*中控制树的复杂度。
- en: Neither the complexity parameter nor pruning is available in `scikit-learn`’s
    decision tree implementation. The solution is to use grid search over combinations
    of different parameter values. For example, we can vary `max_depth` in the range
    5 to 30 and `min_samples_split` between 20 and 100\. The `GridSearchCV` method
    in `scikit-learn` is a convenient way to combine the exhaustive search through
    all combinations with cross-validation. An optimal parameter set is then selected
    using the cross-validated model performance.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '`scikit-learn` 的决策树实现中既没有复杂度参数，也没有剪枝功能。解决方法是对不同参数值的组合进行网格搜索。例如，我们可以将 `max_depth`
    变化范围设置为 5 到 30，将 `min_samples_split` 设置为 20 到 100。`scikit-learn` 中的 `GridSearchCV`
    方法是通过交叉验证组合进行穷举搜索的便捷方式。然后，通过交叉验证的模型性能选择最优参数集。'
- en: Predicting a Continuous Value
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预测连续值
- en: Predicting a continuous value (also termed *regression*) with a tree follows
    the same logic and procedure, except that impurity is measured by squared deviations
    from the mean (squared errors) in each subpartition, and predictive performance
    is judged by the square root of the mean squared error (RMSE) (see [“Assessing
    the Model”](ch04.xhtml#RMSE)) in each partition.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 使用树进行连续值预测（也称为*回归*）遵循相同的逻辑和步骤，只是在每个子分区中，不纯度是通过与平均值的平方偏差（平方误差）来衡量的，预测性能是通过每个分区中的均方根误差（RMSE）（参见[“评估模型”](ch04.xhtml#RMSE)）来判断的。
- en: '`scikit-learn` has the `sklearn.tree.DecisionTreeRegressor` method to train
    a decision tree regression model.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '`scikit-learn` 有 `sklearn.tree.DecisionTreeRegressor` 方法来训练决策树回归模型。'
- en: How Trees Are Used
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何使用树
- en: 'One of the big obstacles faced by predictive modelers in organizations is the
    perceived “black box” nature of the methods they use, which gives rise to opposition
    from other elements of the organization. In this regard, the tree model has two
    appealing aspects:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 组织中的预测建模者面临的一个重大障碍是所使用方法的被视为“黑盒子”的特性，这导致组织的其他部门反对使用这些方法。在这方面，树模型具有两个吸引人的方面：
- en: Tree models provide a visual tool for exploring the data, to gain an idea of
    what variables are important and how they relate to one another. Trees can capture
    nonlinear relationships among predictor variables.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 树模型提供了一个可视化工具，用于探索数据，以获得哪些变量是重要的，以及它们如何相互关联的想法。树可以捕捉预测变量之间的非线性关系。
- en: Tree models provide a set of rules that can be effectively communicated to nonspecialists,
    either for implementation or to “sell” a data mining project.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 树模型提供了一组规则，可以有效地传达给非专业人员，用于实施或“销售”数据挖掘项目。
- en: When it comes to prediction, however, harnessing the results from multiple trees
    is typically more powerful than using just a single tree. In particular, the random
    forest and boosted tree algorithms almost always provide superior predictive accuracy
    and performance (see [“Bagging and the Random Forest”](#Bagging) and [“Boosting”](#Boosting)),
    but the aforementioned advantages of a single tree are lost.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在预测方面，利用多个树的结果通常比仅使用单个树更强大。特别是，随机森林和提升树算法几乎总是提供更优越的预测准确性和性能（参见[“装袋和随机森林”](#Bagging)
    和 [“提升”](#Boosting)），但单棵树的上述优势会丧失。
- en: Further Reading
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'Analytics Vidhya Content Team, [“Tree Based Algorithms: A Complete Tutorial
    from Scratch (in *R* & *Python*)”](https://oreil.ly/zOr4B), April 12, 2016.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Analytics Vidhya 内容团队，[“基于树的算法：从头开始的完整教程（*R* & *Python*）”](https://oreil.ly/zOr4B)，2016年4月12日。
- en: Terry M. Therneau, Elizabeth J. Atkinson, and the Mayo Foundation, [“An Introduction
    to Recursive Partitioning Using the RPART Routines”](https://oreil.ly/6rLGk),
    April 11, 2019.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Terry M. Therneau、Elizabeth J. Atkinson 和 Mayo Foundation，[“使用 RPART 程序介绍递归分区”](https://oreil.ly/6rLGk)，2019年4月11日。
- en: Bagging and the Random Forest
  id: totrans-182
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 装袋和随机森林
- en: 'In 1906, the statistician Sir Francis Galton was visiting a county fair in
    England, at which a contest was being held to guess the dressed weight of an ox
    that was on exhibit. There were 800 guesses, and while the individual guesses
    varied widely, both the mean and the median came out within 1% of the ox’s true
    weight. James Surowiecki has explored this phenomenon in his book *The Wisdom
    of Crowds* (Doubleday, 2004). This principle applies to predictive models as well:
    averaging (or taking majority votes) of multiple models—an *ensemble* of models—turns
    out to be more accurate than just selecting one model.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 1906年，统计学家弗朗西斯·高尔顿（Francis Galton）在英格兰一个乡村展会上参与了一个竞猜展出牛的装重比赛。共有800个猜测，虽然个别猜测相差很大，但平均值和中位数都在牛的真实重量范围内误差不超过1%。詹姆斯·苏罗维埃基在他的著作《众智》（The
    Wisdom of Crowds，Doubleday出版，2004年）中探讨了这一现象。这一原则同样适用于预测模型：多模型的平均值（或多数票制）——即模型的集成——比单一模型更为精确。
- en: 'The ensemble approach has been applied to and across many different modeling
    methods, most publicly in the Netflix Prize, in which Netflix offered a $1 million
    prize to any contestant who came up with a model that produced a 10% improvement
    in predicting the rating that a Netflix customer would award a movie. The simple
    version of ensembles is as follows:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 集成方法已应用于多种不同的建模方法，最为公众熟知的是在Netflix Prize中的应用，该竞赛由Netflix提供100万美元奖金，以奖励那些能提高10%以上准确预测用户评分的模型。集成模型的简化版本如下：
- en: Develop a predictive model and record the predictions for a given data set.
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 开发一个预测模型，并记录给定数据集的预测结果。
- en: Repeat for multiple models on the same data.
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在相同数据上重复多个模型的拟合。
- en: For each record to be predicted, take an average (or a weighted average, or
    a majority vote) of the predictions.
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个要预测的记录，取预测结果的平均值（或加权平均值，或多数投票）。
- en: Ensemble methods have been applied most systematically and effectively to decision
    trees. Ensemble tree models are so powerful that they provide a way to build good
    predictive models with relatively little effort.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 集成方法在决策树中应用最为系统和有效。集成树模型非常强大，能够以相对较少的努力构建出良好的预测模型。
- en: 'Going beyond the simple ensemble algorithm, there are two main variants of
    ensemble models: *bagging* and *boosting*. In the case of ensemble tree models,
    these are referred to as *random forest* models and *boosted tree* models. This
    section focuses on bagging; boosting is covered in [“Boosting”](#Boosting).'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在简单的集成算法基础上，有两个主要的集成模型变体：*bagging* 和 *boosting*。在集成树模型中，它们被称为 *random forest*
    模型和 *boosted tree* 模型。本节重点介绍 *bagging*；*boosting* 见 [“Boosting”](#Boosting)。
- en: Bagging
  id: totrans-190
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Bagging
- en: Bagging, which stands for “bootstrap aggregating,” was introduced by Leo Breiman
    in 1994. Suppose we have a response *Y* and *P* predictor variables <math alttext="bold
    upper X equals upper X 1 comma upper X 2 comma ellipsis comma upper X Subscript
    upper P Baseline"><mrow><mi>𝐗</mi> <mo>=</mo> <msub><mi>X</mi> <mn>1</mn></msub>
    <mo>,</mo> <msub><mi>X</mi> <mn>2</mn></msub> <mo>,</mo> <mo>⋯</mo> <mo>,</mo>
    <msub><mi>X</mi> <mi>P</mi></msub></mrow></math> with *N* records.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: Bagging，即“bootstrap aggregating”，由Leo Breiman于1994年提出。假设我们有一个响应 *Y* 和 *P* 个预测变量
    <math alttext="bold upper X equals upper X 1 comma upper X 2 comma ellipsis comma
    upper X Subscript upper P Baseline"><mrow><mi>𝐗</mi> <mo>=</mo> <msub><mi>X</mi>
    <mn>1</mn></msub> <mo>,</mo> <msub><mi>X</mi> <mn>2</mn></msub> <mo>,</mo> <mo>⋯</mo>
    <mo>,</mo> <msub><mi>X</mi> <mi>P</mi></msub></mrow></math>，具有 *N* 条记录。
- en: 'Bagging is like the basic algorithm for ensembles, except that, instead of
    fitting the various models to the same data, each new model is fitted to a bootstrap
    resample. Here is the algorithm presented more formally:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: Bagging与集成的基本算法类似，不同之处在于，每个新模型是基于一个自助采样重新拟合的。以下更正式地呈现了该算法：
- en: Initialize *M*, the number of models to be fit, and *n*, the number of records
    to choose (*n* < *N*). Set the iteration <math alttext="m equals 1"><mrow><mi>m</mi>
    <mo>=</mo> <mn>1</mn></mrow></math> .
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化 *M*，要拟合的模型数量，和 *n*，要选择的记录数（*n* < *N*）。设置迭代 <math alttext="m equals 1"><mrow><mi>m</mi>
    <mo>=</mo> <mn>1</mn></mrow></math> 。
- en: Take a bootstrap resample (i.e., with replacement) of *n* records from the training
    data to form a subsample <math alttext="upper Y Subscript m"><msub><mi>Y</mi>
    <mi>m</mi></msub></math> and <math alttext="bold upper X Subscript m"><msub><mi>𝐗</mi>
    <mi>m</mi></msub></math> (the bag).
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从训练数据中获取一个自助采样（即带有替换的样本）以形成子样本 <math alttext="upper Y Subscript m"><msub><mi>Y</mi>
    <mi>m</mi></msub></math> 和 <math alttext="bold upper X Subscript m"><msub><mi>𝐗</mi>
    <mi>m</mi></msub></math>（称为“包”）。
- en: Train a model using <math alttext="upper Y Subscript m"><msub><mi>Y</mi> <mi>m</mi></msub></math>
    and <math alttext="bold upper X Subscript m"><msub><mi>𝐗</mi> <mi>m</mi></msub></math>
    to create a set of decision rules <math alttext="ModifyingAbove f With caret Subscript
    m Baseline left-parenthesis bold upper X right-parenthesis"><mrow><msub><mover
    accent="true"><mi>f</mi> <mo>^</mo></mover> <mi>m</mi></msub> <mrow><mo>(</mo>
    <mi>𝐗</mi> <mo>)</mo></mrow></mrow></math> .
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 <math alttext="upper Y Subscript m"><msub><mi>Y</mi> <mi>m</mi></msub></math>
    和 <math alttext="bold upper X Subscript m"><msub><mi>𝐗</mi> <mi>m</mi></msub></math>
    训练模型，创建一组决策规则 <math alttext="ModifyingAbove f With caret Subscript m Baseline
    left-parenthesis bold upper X right-parenthesis"><mrow><msub><mover accent="true"><mi>f</mi>
    <mo>^</mo></mover> <mi>m</mi></msub> <mrow><mo>(</mo> <mi>𝐗</mi> <mo>)</mo></mrow></mrow></math>。
- en: Increment the model counter <math alttext="m equals m plus 1"><mrow><mi>m</mi>
    <mo>=</mo> <mi>m</mi> <mo>+</mo> <mn>1</mn></mrow></math> . If *m* <= *M*, go
    to step 2.
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将模型计数器递增 <math alttext="m equals m plus 1"><mrow><mi>m</mi> <mo>=</mo> <mi>m</mi>
    <mo>+</mo> <mn>1</mn></mrow></math>。如果 *m* <= *M*，则转到步骤 2。
- en: 'In the case where <math alttext="ModifyingAbove f With caret Subscript m"><msub><mover
    accent="true"><mi>f</mi> <mo>^</mo></mover> <mi>m</mi></msub></math> predicts
    the probability <math alttext="upper Y equals 1"><mrow><mi>Y</mi> <mo>=</mo> <mn>1</mn></mrow></math>
    , the bagged estimate is given by:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在 <math alttext="ModifyingAbove f With caret Subscript m"><msub><mover accent="true"><mi>f</mi>
    <mo>^</mo></mover> <mi>m</mi></msub></math> 预测概率 <math alttext="upper Y equals
    1"><mrow><mi>Y</mi> <mo>=</mo> <mn>1</mn></mrow></math> 的情况下，袋装估计如下：
- en: <math alttext="ModifyingAbove f With caret equals StartFraction 1 Over upper
    M EndFraction left-parenthesis ModifyingAbove f With caret Subscript 1 Baseline
    left-parenthesis bold upper X right-parenthesis plus ModifyingAbove f With caret
    Subscript 2 Baseline left-parenthesis bold upper X right-parenthesis plus ellipsis
    plus ModifyingAbove f With caret Subscript upper M Baseline left-parenthesis bold
    upper X right-parenthesis right-parenthesis" display="block"><mrow><mover accent="true"><mi>f</mi>
    <mo>^</mo></mover> <mo>=</mo> <mfrac><mn>1</mn> <mi>M</mi></mfrac> <mfenced separators=""
    open="(" close=")"><msub><mover accent="true"><mi>f</mi> <mo>^</mo></mover> <mn>1</mn></msub>
    <mrow><mo>(</mo> <mi>𝐗</mi> <mo>)</mo></mrow> <mo>+</mo> <msub><mover accent="true"><mi>f</mi>
    <mo>^</mo></mover> <mn>2</mn></msub> <mrow><mo>(</mo> <mi>𝐗</mi> <mo>)</mo></mrow>
    <mo>+</mo> <mo>⋯</mo> <mo>+</mo> <msub><mover accent="true"><mi>f</mi> <mo>^</mo></mover>
    <mi>M</mi></msub> <mrow><mo>(</mo> <mi>𝐗</mi> <mo>)</mo></mrow></mfenced></mrow></math>
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="ModifyingAbove f With caret equals StartFraction 1 Over upper
    M EndFraction left-parenthesis ModifyingAbove f With caret Subscript 1 Baseline
    left-parenthesis bold upper X right-parenthesis plus ModifyingAbove f With caret
    Subscript 2 Baseline left-parenthesis bold upper X right-parenthesis plus ellipsis
    plus ModifyingAbove f With caret Subscript upper M Baseline left-parenthesis bold
    upper X right-parenthesis right-parenthesis" display="block"><mrow><mover accent="true"><mi>f</mi>
    <mo>^</mo></mover> <mo>=</mo> <mfrac><mn>1</mn> <mi>M</mi></mfrac> <mfenced separators=""
    open="(" close=")"><msub><mover accent="true"><mi>f</mi> <mo>^</mo></mover> <mn>1</mn></msub>
    <mrow><mo>(</mo> <mi>𝐗</mi> <mo>)</mo></mrow> <mo>+</mo> <msub><mover accent="true"><mi>f</mi>
    <mo>^</mo></mover> <mn>2</mn></msub> <mrow><mo>(</mo> <mi>𝐗</mi> <mo>)</mo></mrow>
    <mo>+</mo> <mo>⋯</mo> <mo>+</mo> <msub><mover accent="true"><mi>f</mi> <mo>^</mo></mover>
    <mi>M</mi></msub> <mrow><mo>(</mo> <mi>𝐗</mi> <mo>)</mo></mrow></mfenced></mrow></math>
- en: Random Forest
  id: totrans-199
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 随机森林
- en: 'The *random forest* is based on applying bagging to decision trees, with one
    important extension: in addition to sampling the records, the algorithm also samples
    the variables.^([4](ch06.xhtml#idm46522842847944)) In traditional decision trees,
    to determine how to create a subpartition of a partition *A*, the algorithm makes
    the choice of variable and split point by minimizing a criterion such as Gini
    impurity (see [“Measuring Homogeneity or Impurity”](#Gini)). With random forests,
    at each stage of the algorithm, the choice of variable is limited to a *random
    subset of variables*. Compared to the basic tree algorithm (see [“The Recursive
    Partitioning Algorithm”](#RecursivePartitioning)), the random forest algorithm
    adds two more steps: the bagging discussed earlier (see [“Bagging and the Random
    Forest”](#Bagging)), and the bootstrap sampling of variables at each split:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '*随机森林* 基于对决策树应用装袋算法，但有一个重要扩展：除了对记录进行采样外，该算法还对变量进行采样。^([4](ch06.xhtml#idm46522842847944))
    在传统决策树中，为了确定如何创建分区 *A* 的子分区，算法通过最小化 Gini 不纯度等准则来选择变量和分割点（请参阅[“测量同质性或不纯度”](#Gini)）。随机森林中，在算法的每个阶段，变量的选择限于*变量的随机子集*。与基本树算法（请参阅[“递归分区算法”](#RecursivePartitioning)）相比，随机森林算法增加了另外两个步骤：前面讨论过的装袋（请参阅[“装袋和随机森林”](#Bagging)），以及在每次分割时对变量进行自助采样：'
- en: Take a bootstrap (with replacement) subsample from the *records*.
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 *记录* 中获取一个自助（有替换地）子样本。
- en: For the first split, sample *p* < *P* *variables* at random without replacement.
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于第一个分割，随机无重复地采样 *p* < *P* *变量*。
- en: 'For each of the sampled variables <math alttext="upper X Subscript j left-parenthesis
    1 right-parenthesis Baseline comma upper X Subscript j left-parenthesis 2 right-parenthesis
    Baseline comma ellipsis comma upper X Subscript j left-parenthesis p right-parenthesis
    Baseline"><mrow><msub><mi>X</mi> <mrow><mi>j</mi><mo>(</mo><mn>1</mn><mo>)</mo></mrow></msub>
    <mo>,</mo> <msub><mi>X</mi> <mrow><mi>j</mi><mo>(</mo><mn>2</mn><mo>)</mo></mrow></msub>
    <mo>,</mo> <mo>...</mo> <mo>,</mo> <msub><mi>X</mi> <mrow><mi>j</mi><mo>(</mo><mi>p</mi><mo>)</mo></mrow></msub></mrow></math>
    , apply the splitting algorithm:'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个采样变量 <math alttext="upper X Subscript j left-parenthesis 1 right-parenthesis
    Baseline comma upper X Subscript j left-parenthesis 2 right-parenthesis Baseline
    comma ellipsis comma upper X Subscript j left-parenthesis p right-parenthesis
    Baseline"><mrow><msub><mi>X</mi> <mrow><mi>j</mi><mo>(</mo><mn>1</mn><mo>)</mo></mrow></msub>
    <mo>,</mo> <msub><mi>X</mi> <mrow><mi>j</mi><mo>(</mo><mn>2</mn><mo>)</mo></mrow></msub>
    <mo>,</mo> <mo>...</mo> <mo>,</mo> <msub><mi>X</mi> <mrow><mi>j</mi><mo>(</mo><mi>p</mi><mo>)</mo></mrow></msub></mrow></math>，应用分割算法：
- en: 'For each value <math alttext="s Subscript j left-parenthesis k right-parenthesis"><msub><mi>s</mi>
    <mrow><mi>j</mi><mo>(</mo><mi>k</mi><mo>)</mo></mrow></msub></math> of <math alttext="upper
    X Subscript j left-parenthesis k right-parenthesis"><msub><mi>X</mi> <mrow><mi>j</mi><mo>(</mo><mi>k</mi><mo>)</mo></mrow></msub></math>
    :'
  id: totrans-204
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个值 <math alttext="s Subscript j left-parenthesis k right-parenthesis"><msub><mi>s</mi>
    <mrow><mi>j</mi><mo>(</mo><mi>k</mi><mo>)</mo></mrow></msub></math> 的 <math alttext="upper
    X Subscript j left-parenthesis k right-parenthesis"><msub><mi>X</mi> <mrow><mi>j</mi><mo>(</mo><mi>k</mi><mo>)</mo></mrow></msub></math>：
- en: Split the records in partition *A*, with *X*[*j*(*k*)] < *s*[*j*(*k*)] as one
    partition and the remaining records where <math alttext="upper X Subscript j left-parenthesis
    k right-parenthesis Baseline greater-than-or-equal-to s Subscript j left-parenthesis
    k right-parenthesis"><mrow><msub><mi>X</mi> <mrow><mi>j</mi><mo>(</mo><mi>k</mi><mo>)</mo></mrow></msub>
    <mo>≥</mo> <msub><mi>s</mi> <mrow><mi>j</mi><mo>(</mo><mi>k</mi><mo>)</mo></mrow></msub></mrow></math>
    as another partition.
  id: totrans-205
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将记录分割为分区 *A*，其中 *X*[*j*(*k*)] < *s*[*j*(*k*)] 作为一个分区，剩余记录，其中 <math alttext="upper
    X Subscript j left-parenthesis k right-parenthesis Baseline greater-than-or-equal-to
    s Subscript j left-parenthesis k right-parenthesis"><mrow><msub><mi>X</mi> <mrow><mi>j</mi><mo>(</mo><mi>k</mi><mo>)</mo></mrow></msub>
    <mo>≥</mo> <msub><mi>s</mi> <mrow><mi>j</mi><mo>(</mo><mi>k</mi><mo>)</mo></mrow></msub></mrow></math>
    作为另一个分区。
- en: Measure the homogeneity of classes within each subpartition of *A*.
  id: totrans-206
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 测量 *A* 的每个子分区内类的同质性。
- en: Select the value of <math alttext="s Subscript j left-parenthesis k right-parenthesis"><msub><mi>s</mi>
    <mrow><mi>j</mi><mo>(</mo><mi>k</mi><mo>)</mo></mrow></msub></math> that produces
    maximum within-partition homogeneity of class.
  id: totrans-207
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择值 <math alttext="s Subscript j left-parenthesis k right-parenthesis"><msub><mi>s</mi>
    <mrow><mi>j</mi><mo>(</mo><mi>k</mi><mo>)</mo></mrow></msub></math>，以产生类内最大的分区同质性。
- en: Select the variable <math alttext="upper X Subscript j left-parenthesis k right-parenthesis"><msub><mi>X</mi>
    <mrow><mi>j</mi><mo>(</mo><mi>k</mi><mo>)</mo></mrow></msub></math> and the split
    value <math alttext="s Subscript j left-parenthesis k right-parenthesis"><msub><mi>s</mi>
    <mrow><mi>j</mi><mo>(</mo><mi>k</mi><mo>)</mo></mrow></msub></math> that produces
    maximum within-partition homogeneity of class.
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择变量 <math alttext="upper X Subscript j left-parenthesis k right-parenthesis"><msub><mi>X</mi>
    <mrow><mi>j</mi><mo>(</mo><mi>k</mi><mo>)</mo></mrow></msub></math> 和分割值 <math
    alttext="s Subscript j left-parenthesis k right-parenthesis"><msub><mi>s</mi>
    <mrow><mi>j</mi><mo>(</mo><mi>k</mi><mo>)</mo></mrow></msub></math>，以产生类内最大的分区同质性。
- en: Proceed to the next split and repeat the previous steps, starting with step
    2.
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 继续下一个拆分，并重复前面的步骤，从第二步开始。
- en: Continue with additional splits, following the same procedure until the tree
    is grown.
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照相同的步骤继续进行额外的拆分，直到树生长完毕。
- en: Go back to step 1, take another bootstrap subsample, and start the process over
    again.
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 回到第 1 步，获取另一个自举子样本，并重新开始过程。
- en: 'How many variables to sample at each step? A rule of thumb is to choose <math
    alttext="StartRoot upper P EndRoot"><msqrt><mi>P</mi></msqrt></math> where *P*
    is the number of predictor variables. The package `randomForest` implements the
    random forest in *R*. The following applies this package to the loan data (see
    [“K-Nearest Neighbors”](#KNN) for a description of the data):'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 每一步抽样多少变量？一个经验法则是选择 <math alttext="StartRoot upper P EndRoot"><msqrt><mi>P</mi></msqrt></math>，其中
    *P* 是预测变量的数量。包 `randomForest` 在 *R* 中实现了随机森林。以下应用此包到贷款数据（参见 [“K-最近邻”](#KNN) 对数据的描述）：
- en: '[PRE13]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'In *Python*, we use the method `sklearn.ensemble.RandomForestClassifier`:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *Python* 中，我们使用 `sklearn.ensemble.RandomForestClassifier` 方法：
- en: '[PRE14]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: By default, 500 trees are trained. Since there are only two variables in the
    predictor set, the algorithm randomly selects the variable on which to split at
    each stage (i.e., a bootstrap subsample of size 1).
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，训练了 500 棵树。由于预测器集中只有两个变量，算法在每个阶段随机选择变量进行拆分（即每个阶段的自举子样本大小为 1）。
- en: 'The *out-of-bag* (*OOB*) estimate of error is the error rate for the trained
    models, applied to the data left out of the training set for that tree. Using
    the output from the model, the OOB error can be plotted versus the number of trees
    in the random forest in *R*:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '*袋外* (*OOB*) 错误估计是对留在该树训练集之外的数据应用于训练模型的错误率。使用模型输出，可以将 OOB 错误绘制成在随机森林中树的数量的图表化在
    *R*：'
- en: '[PRE15]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The `RandomForestClassifier` implementation has no easy way to get out-of-bag
    estimates as a function of number of trees in the random forest. We can train
    a sequence of classifiers with an increasing number of trees and keep track of
    the `oob_score_` values. This method is, however, not efficient:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '`RandomForestClassifier` 的实现没有简单的方法来获取随机森林中树的数量作为袋外估计。我们可以训练一系列分类器，树的数量逐渐增加，并跟踪
    `oob_score_` 值。然而，这种方法并不高效：'
- en: '[PRE16]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The result is shown in [Figure 6-6](#RFAccuracy). The error rate rapidly decreases
    from over 0.44 before stabilizing around 0.385. The predicted values can be obtained
    from the `predict` function and plotted as follows in *R*:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 结果显示在 [图 6-6](#RFAccuracy) 中。错误率从超过 0.44 迅速下降，稳定在约 0.385。预测值可以从 `predict` 函数获取，并在
    *R* 中按以下方式绘制：
- en: '[PRE17]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'In *Python*, we can create a similar plot as follows:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *Python* 中，我们可以创建一个类似的图表如下所示：
- en: '[PRE18]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '![The improvement in accuracy of the random forest with the addition of more
    trees.](Images/psd2_0606.png)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![随着更多树的添加，随机森林准确性的改善。](Images/psd2_0606.png)'
- en: Figure 6-6\. An example of the improvement in accuracy of the random forest
    with the addition of more trees
  id: totrans-226
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-6\. 随着更多树的添加，随机森林准确性的改善示例
- en: The plot, shown in [Figure 6-7](#LoanRF), is quite revealing about the nature
    of the random forest.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 图6-7显示的图表对随机森林的性质非常揭示。
- en: 'The random forest method is a “black box” method. It produces more accurate
    predictions than a simple tree, but the simple tree’s intuitive decision rules
    are lost. The random forest predictions are also somewhat noisy: note that some
    borrowers with a very high score, indicating high creditworthiness, still end
    up with a prediction of default. This is a result of some unusual records in the
    data and demonstrates the danger of overfitting by the random forest (see [“Bias-Variance
    Trade-off”](#bvt_note)).'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林方法是一种“黑箱”方法。它产生比简单树更准确的预测，但失去了简单树直观的决策规则。随机森林的预测也有些噪音：请注意，一些信用评分非常高（表示高信用度）的借款人最终仍然会有违约预测。这是数据中一些异常记录的结果，显示了随机森林过拟合的危险（见[“偏差-方差权衡”](#bvt_note)）。
- en: '![The predicted outcomes from the random forest applied to the loan default
    data.](Images/psd2_0607.png)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
  zh: '![随机森林应用于贷款违约数据的预测结果。](Images/psd2_0607.png)'
- en: Figure 6-7\. The predicted outcomes from the random forest applied to the loan
    default data
  id: totrans-230
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-7\. 随机森林应用于贷款违约数据的预测结果
- en: Variable Importance
  id: totrans-231
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 变量重要性
- en: 'The power of the random forest algorithm shows itself when you build predictive
    models for data with many features and records. It has the ability to automatically
    determine which predictors are important and discover complex relationships between
    predictors corresponding to interaction terms (see [“Interactions and Main Effects”](ch04.xhtml#Interactions)).
    For example, fit a model to the loan default data with all columns included. The
    following shows this in *R*:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 当您为具有许多特征和记录的数据构建预测模型时，随机森林算法展现了其能力。它能够自动确定哪些预测变量是重要的，并发现与交互项相对应的预测变量之间的复杂关系（见[“交互作用和主效应”](ch04.xhtml#Interactions)）。例如，使用所有列拟合贷款违约数据的模型。以下是*R*中的示例：
- en: '[PRE19]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'And in *Python*:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 以及在*Python*中：
- en: '[PRE20]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The argument `importance=TRUE` requests that the `randomForest` store additional
    information about the importance of different variables. The function `varImpPlot`
    will plot the relative performance of the variables (relative to permuting that
    variable):'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 参数`importance=TRUE`要求`randomForest`存储有关不同变量重要性的额外信息。函数`varImpPlot`将绘制变量相对性能的图表（相对于置换该变量）：
- en: '[PRE21]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[![1](Images/1.png)](#co_statistical_machine_learning_CO2-1)'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_statistical_machine_learning_CO2-1)'
- en: mean decrease in accuracy
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 准确性的平均降低
- en: '[![2](Images/2.png)](#co_statistical_machine_learning_CO2-2)'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_statistical_machine_learning_CO2-2)'
- en: mean decrease in node impurity
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 节点纯度的平均降低
- en: 'In *Python*, the `RandomForestClassifier` collects information about feature
    importance during training and makes it available with the field `feature_importances_`:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 在*Python*中，`RandomForestClassifier`在训练过程中收集关于特征重要性的信息，并在`feature_importances_`字段中提供：
- en: '[PRE22]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The “Gini decrease” is available as the `feature_importance_` property of the
    fitted classifier. Accuracy decrease, however, is not available out of the box
    for *Python*. We can calculate it (`scores`) using the following code:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: “Gini减少”作为拟合分类器的`feature_importance_`属性可用。然而，准确性的降低并不是*Python*的开箱即用功能。我们可以使用以下代码计算它（`scores`）：
- en: '[PRE23]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The result is shown in [Figure 6-8](#LoanVarImp). A similar graph can be created
    with this *Python* code:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 结果显示在[图6-8](#LoanVarImp)中。可以使用此*Python*代码创建类似的图表：
- en: '[PRE24]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'There are two ways to measure variable importance:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种衡量变量重要性的方法：
- en: By the decrease in accuracy of the model if the values of a variable are randomly
    permuted (`type=1`). Randomly permuting the values has the effect of removing
    all predictive power for that variable. The accuracy is computed from the out-of-bag
    data (so this measure is effectively a cross-validated estimate).
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当变量的值被随机置换（`type=1`）时，模型的准确性会下降。随机置换值的效果是移除该变量的所有预测能力。准确性是从袋外数据计算的（因此这个度量实际上是一个交叉验证的估计）。
- en: By the mean decrease in the Gini impurity score (see [“Measuring Homogeneity
    or Impurity”](#Gini)) for all of the nodes that were split on a variable (`type=2`).
    This measures how much including that variable improves the purity of the nodes.
    This measure is based on the training set and is therefore less reliable than
    a measure calculated on out-of-bag data.
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过Gini不纯度得分的平均减少（参见[“测量同质性或不纯度”](#Gini)）对所有分裂在变量上的节点（`type=2`）进行计算。这度量了包含该变量后节点纯度的提高程度。此度量基于训练集，因此不如在袋外数据上计算的度量可靠。
- en: '![The importance of variables for the full model fit to the loan data.](Images/psd2_0608.png)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: '![适用于贷款数据的完整模型拟合的变量重要性。](Images/psd2_0608.png)'
- en: Figure 6-8\. The importance of variables for the full model fit to the loan
    data
  id: totrans-252
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-8。适用于贷款数据的完整模型拟合的变量重要性
- en: The top and bottom panels of [Figure 6-8](#LoanVarImp) show variable importance
    according to the decrease in accuracy and in Gini impurity, respectively. The
    variables in both panels are ranked by the decrease in accuracy. The variable
    importance scores produced by these two measures are quite different.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '[图6-8](#LoanVarImp)的顶部和底部面板显示了根据准确度和Gini不纯度减少的变量重要性。两个面板中的变量都按准确度减少进行排序。这两种度量产生的变量重要性得分相当不同。'
- en: 'Since the accuracy decrease is a more reliable metric, why should we use the
    Gini impurity decrease measure? By default, `randomForest` computes only this
    Gini impurity: Gini impurity is a byproduct of the algorithm, whereas model accuracy
    by variable requires extra computations (randomly permuting the data and predicting
    this data). In cases where computational complexity is important, such as in a
    production setting where thousands of models are being fit, it may not be worth
    the extra computational effort. In addition, the Gini decrease sheds light on
    which variables the random forest is using to make its splitting rules (recall
    that this information, readily visible in a simple tree, is effectively lost in
    a random forest).'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 由于准确度减少是一个更可靠的度量标准，我们为什么要使用Gini不纯度减少度量？默认情况下，`randomForest`只计算这个Gini不纯度：Gini不纯度是算法的副产品，而通过变量的模型准确度需要额外的计算（随机置换数据并预测这些数据）。在计算复杂度很重要的情况下，比如在拟合数千个模型的生产环境中，可能不值得额外的计算工作。此外，Gini减少揭示了随机森林用于制定其分割规则的哪些变量（回想一下，在简单树中很容易看到的信息，在随机森林中实际上丢失了）。
- en: Hyperparameters
  id: totrans-255
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 超参数
- en: 'The random forest, as with many statistical machine learning algorithms, can
    be considered a black-box algorithm with knobs to adjust how the box works. These
    knobs are called *hyperparameters*, which are parameters that you need to set
    before fitting a model; they are not optimized as part of the training process.
    While traditional statistical models require choices (e.g., the choice of predictors
    to use in a regression model), the hyperparameters for random forest are more
    critical, especially to avoid overfitting. In particular, the two most important
    hyperparameters for the random forest are:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 与许多统计机器学习算法一样，随机森林可以被视为一个黑盒算法，其中有一些调节盒子如何工作的旋钮。这些旋钮称为*超参数*，它们是在拟合模型之前需要设置的参数；它们不会作为训练过程的一部分进行优化。虽然传统的统计模型需要选择（例如，在回归模型中使用的预测变量的选择），但是随机森林的超参数更为关键，特别是为了避免过度拟合。特别是，随机森林的两个最重要的超参数是：
- en: '`nodesize`/`min_samples_leaf`'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '`nodesize`/`min_samples_leaf`'
- en: The minimum size for terminal nodes (leaves in the tree). The default is 1 for
    classification and 5 for regression in *R*. The `scikit-learn` implementation
    in *Python* uses a default of 1 for both.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 终端节点（树中的叶子）的最小大小。对于分类，默认值为1，在*R*中回归为5。*Python*中的`scikit-learn`实现都默认为1。
- en: '`maxnodes`/`max_leaf_nodes`'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '`maxnodes`/`max_leaf_nodes`'
- en: 'The maximum number of nodes in each decision tree. By default, there is no
    limit and the largest tree will be fit subject to the constraints of `nodesize`.
    Note that in *Python*, you specify the maximum number of terminal nodes. The two
    parameters are related:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 每个决策树中的最大节点数。默认情况下，没有限制，树的大小将根据`nodesize`的约束进行调整。请注意，在*Python*中，您指定的是最大终端节点数。这两个参数有关联：
- en: <math alttext="maxnodes equals 2 max reverse-solidus bar leaf reverse-solidus
    bar nodes negative 1" display="block"><mrow><mi>maxnodes</mi> <mo>=</mo> <mn>2</mn>
    <mi>max</mi> <mo>_</mo> <mi>leaf</mi> <mo>_</mo> <mi>nodes</mi> <mo>-</mo> <mn>1</mn></mrow></math>
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="maxnodes equals 2 max reverse-solidus bar leaf reverse-solidus
    bar nodes negative 1" display="block"><mrow><mi>maxnodes</mi> <mo>=</mo> <mn>2</mn>
    <mi>max</mi> <mo>_</mo> <mi>leaf</mi> <mo>_</mo> <mi>nodes</mi> <mo>-</mo> <mn>1</mn></mrow></math>
- en: It may be tempting to ignore these parameters and simply go with the default
    values. However, using the defaults may lead to overfitting when you apply the
    random forest to noisy data. When you increase `nodesize`/`min_samples_leaf` or
    set `maxnodes`/`max_leaf_nodes`, the algorithm will fit smaller trees and is less
    likely to create spurious predictive rules. Cross-validation (see [“Cross-Validation”](ch04.xhtml#CrossValidation))
    can be used to test the effects of setting different values for hyperparameters.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 或许会忽略这些参数，仅使用默认值。然而，当将随机森林应用于嘈杂的数据时，使用默认值可能会导致过拟合。当您增加 `nodesize`/`min_samples_leaf`
    或设置 `maxnodes`/`max_leaf_nodes` 时，算法将拟合较小的树，并且不太可能创建虚假的预测规则。可以使用交叉验证（参见 [“Cross-Validation”](ch04.xhtml#CrossValidation)）来测试设置不同超参数值的效果。
- en: Boosting
  id: totrans-263
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Boosting
- en: Ensemble models have become a standard tool for predictive modeling. *Boosting*
    is a general technique to create an ensemble of models. It was developed around
    the same time as *bagging* (see [“Bagging and the Random Forest”](#Bagging)).
    Like bagging, boosting is most commonly used with decision trees. Despite their
    similarities, boosting takes a very different approach—one that comes with many
    more bells and whistles. As a result, while bagging can be done with relatively
    little tuning, boosting requires much greater care in its application. If these
    two methods were cars, bagging could be considered a Honda Accord (reliable and
    steady), whereas boosting could be considered a Porsche (powerful but requires
    more care).
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 集成模型已成为预测建模的标准工具。*Boosting* 是创建模型集成的一般技术。它与 *bagging* 大致同时发展（参见 [“Bagging and
    the Random Forest”](#Bagging)）。像 bagging 一样，boosting 最常用于决策树。尽管它们有相似之处，但 boosting
    采用的是一种截然不同的方法，带有更多的花里胡哨。因此，虽然 bagging 可以相对轻松地完成，但 boosting 在应用时需要更多的注意。如果将这两种方法比作汽车，那么
    bagging 可以被视为本田雅阁（可靠而稳定），而 boosting 则可以被视为保时捷（强大但需要更多关注）。
- en: 'In linear regression models, the residuals are often examined to see if the
    fit can be improved (see [“Partial Residual Plots and Nonlinearity”](ch04.xhtml#PartialResidualPlots)).
    Boosting takes this concept much further and fits a series of models, in which
    each successive model seeks to minimize the error of the previous model. Several
    variants of the algorithm are commonly used: *Adaboost*, *gradient boosting*,
    and *stochastic gradient boosting*. The latter, stochastic gradient boosting,
    is the most general and widely used. Indeed, with the right choice of parameters,
    the algorithm can emulate the random forest.'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 在线性回归模型中，经常会检查残差，以查看是否可以改进拟合（参见 [“Partial Residual Plots and Nonlinearity”](ch04.xhtml#PartialResidualPlots)）。Boosting
    将这个概念推广到更深层次，并拟合一系列模型，其中每个后续模型旨在减少前一个模型的误差。通常使用几种算法变体：*Adaboost*、*gradient boosting*
    和 *stochastic gradient boosting*。后者，即随机梯度 boosting，是最通用且广泛使用的。事实上，通过正确选择参数，该算法可以模拟随机森林。
- en: The Boosting Algorithm
  id: totrans-266
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Boosting 算法
- en: 'There are various boosting algorithms, and the basic idea behind all of them
    is essentially the same. The easiest to understand is Adaboost, which proceeds
    as follows:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 存在各种 boosting 算法，它们的基本思想本质上是相同的。最容易理解的是 Adaboost，其过程如下：
- en: Initialize *M*, the maximum number of models to be fit, and set the iteration
    counter <math alttext="m equals 1"><mrow><mi>m</mi> <mo>=</mo> <mn>1</mn></mrow></math>
    . Initialize the observation weights <math alttext="w Subscript i Baseline equals
    1 slash upper N"><mrow><msub><mi>w</mi> <mi>i</mi></msub> <mo>=</mo> <mn>1</mn>
    <mo>/</mo> <mi>N</mi></mrow></math> for <math alttext="i equals 1 comma 2 comma
    ellipsis comma upper N"><mrow><mi>i</mi> <mo>=</mo> <mn>1</mn> <mo>,</mo> <mn>2</mn>
    <mo>,</mo> <mo>...</mo> <mo>,</mo> <mi>N</mi></mrow></math> . Initialize the ensemble
    model <math alttext="ModifyingAbove upper F With caret Subscript 0 Baseline equals
    0"><mrow><msub><mover accent="true"><mi>F</mi> <mo>^</mo></mover> <mn>0</mn></msub>
    <mo>=</mo> <mn>0</mn></mrow></math> .
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化 *M*，要拟合的模型的最大数量，并设置迭代计数器 <math alttext="m equals 1"><mrow><mi>m</mi> <mo>=</mo>
    <mn>1</mn></mrow></math> 。为观测权重 <math alttext="w Subscript i Baseline equals 1
    slash upper N"><mrow><msub><mi>w</mi> <mi>i</mi></msub> <mo>=</mo> <mn>1</mn>
    <mo>/</mo> <mi>N</mi></mrow></math> 初始化，其中 <math alttext="i equals 1 comma 2 comma
    ellipsis comma upper N"><mrow><mi>i</mi> <mo>=</mo> <mn>1</mn> <mo>,</mo> <mn>2</mn>
    <mo>,</mo> <mo>...</mo> <mo>,</mo> <mi>N</mi></mrow></math> 。初始化集成模型 <math alttext="ModifyingAbove
    upper F With caret Subscript 0 Baseline equals 0"><mrow><msub><mover accent="true"><mi>F</mi>
    <mo>^</mo></mover> <mn>0</mn></msub> <mo>=</mo> <mn>0</mn></mrow></math> 。
- en: Using the observation weights <math alttext="w 1 comma w 2 comma ellipsis comma
    w Subscript upper N Baseline"><mrow><msub><mi>w</mi> <mn>1</mn></msub> <mo>,</mo>
    <msub><mi>w</mi> <mn>2</mn></msub> <mo>,</mo> <mo>...</mo> <mo>,</mo> <msub><mi>w</mi>
    <mi>N</mi></msub></mrow></math> , train a model <math alttext="ModifyingAbove
    f With caret Subscript m"><msub><mover accent="true"><mi>f</mi> <mo>^</mo></mover>
    <mi>m</mi></msub></math> that minimizes the weighted error <math alttext="e Subscript
    m"><msub><mi>e</mi> <mi>m</mi></msub></math> defined by summing the weights for
    the misclassified observations.
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用观测权重<math alttext="w 1 comma w 2 comma ellipsis comma w Subscript upper N
    Baseline"><mrow><msub><mi>w</mi> <mn>1</mn></msub> <mo>,</mo> <msub><mi>w</mi>
    <mn>2</mn></msub> <mo>,</mo> <mo>...</mo> <mo>,</mo> <msub><mi>w</mi> <mi>N</mi></msub></mrow></math>
    ，训练一个模型<math alttext="ModifyingAbove f With caret Subscript m"><msub><mover accent="true"><mi>f</mi>
    <mo>^</mo></mover> <mi>m</mi></msub></math>，最小化由误分类观测权重定义的加权误差<math alttext="e
    Subscript m"><msub><mi>e</mi> <mi>m</mi></msub></math>。
- en: 'Add the model to the ensemble: <math alttext="ModifyingAbove upper F With caret
    Subscript m Baseline equals ModifyingAbove upper F With caret Subscript m minus
    1 Baseline plus alpha Subscript m Baseline ModifyingAbove f With caret Subscript
    m"><mrow><msub><mover accent="true"><mi>F</mi> <mo>^</mo></mover> <mi>m</mi></msub>
    <mo>=</mo> <msub><mover accent="true"><mi>F</mi> <mo>^</mo></mover> <mrow><mi>m</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mo>+</mo> <msub><mi>α</mi> <mi>m</mi></msub> <msub><mover accent="true"><mi>f</mi>
    <mo>^</mo></mover> <mi>m</mi></msub></mrow></math> where <math alttext="alpha
    Subscript m Baseline equals StartFraction log 1 minus e Subscript m Baseline Over
    e Subscript m Baseline EndFraction"><mrow><msub><mi>α</mi> <mi>m</mi></msub> <mo>=</mo>
    <mfrac><mrow><mo form="prefix">log</mo><mn>1</mn><mo>-</mo><msub><mi>e</mi> <mi>m</mi></msub></mrow>
    <msub><mi>e</mi> <mi>m</mi></msub></mfrac></mrow></math> .'
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将模型添加到集成模型中：<math alttext="ModifyingAbove upper F With caret Subscript m Baseline
    equals ModifyingAbove upper F With caret Subscript m minus 1 Baseline plus alpha
    Subscript m Baseline ModifyingAbove f With caret Subscript m"><mrow><msub><mover
    accent="true"><mi>F</mi> <mo>^</mo></mover> <mi>m</mi></msub> <mo>=</mo> <msub><mover
    accent="true"><mi>F</mi> <mo>^</mo></mover> <mrow><mi>m</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mo>+</mo> <msub><mi>α</mi> <mi>m</mi></msub> <msub><mover accent="true"><mi>f</mi>
    <mo>^</mo></mover> <mi>m</mi></msub></mrow></math>，其中<math alttext="alpha Subscript
    m Baseline equals StartFraction log 1 minus e Subscript m Baseline Over e Subscript
    m Baseline EndFraction"><mrow><msub><mi>α</mi> <mi>m</mi></msub> <mo>=</mo> <mfrac><mrow><mo
    form="prefix">log</mo><mn>1</mn><mo>-</mo><msub><mi>e</mi> <mi>m</mi></msub></mrow>
    <msub><mi>e</mi> <mi>m</mi></msub></mfrac></mrow></math>。
- en: Update the weights <math alttext="w 1 comma w 2 comma ellipsis comma w Subscript
    upper N Baseline"><mrow><msub><mi>w</mi> <mn>1</mn></msub> <mo>,</mo> <msub><mi>w</mi>
    <mn>2</mn></msub> <mo>,</mo> <mo>...</mo> <mo>,</mo> <msub><mi>w</mi> <mi>N</mi></msub></mrow></math>
    so that the weights are increased for the observations that were misclassified.
    The size of the increase depends on <math alttext="alpha Subscript m"><msub><mi>α</mi>
    <mi>m</mi></msub></math> , with larger values of <math alttext="alpha Subscript
    m"><msub><mi>α</mi> <mi>m</mi></msub></math> leading to bigger weights.
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新权重<math alttext="w 1 comma w 2 comma ellipsis comma w Subscript upper N Baseline"><mrow><msub><mi>w</mi>
    <mn>1</mn></msub> <mo>,</mo> <msub><mi>w</mi> <mn>2</mn></msub> <mo>,</mo> <mo>...</mo>
    <mo>,</mo> <msub><mi>w</mi> <mi>N</mi></msub></mrow></math>，使误分类的观测权重增加。增加的大小取决于<math
    alttext="alpha Subscript m"><msub><mi>α</mi> <mi>m</mi></msub></math>，较大的<math
    alttext="alpha Subscript m"><msub><mi>α</mi> <mi>m</mi></msub></math>值会导致权重增加。
- en: Increment the model counter <math alttext="m equals m plus 1"><mrow><mi>m</mi>
    <mo>=</mo> <mi>m</mi> <mo>+</mo> <mn>1</mn></mrow></math> . If <math alttext="m
    less-than-or-equal-to upper M"><mrow><mi>m</mi> <mo>≤</mo> <mi>M</mi></mrow></math>
    , go to step 2.
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 增加模型计数器<math alttext="m equals m plus 1"><mrow><mi>m</mi> <mo>=</mo> <mi>m</mi>
    <mo>+</mo> <mn>1</mn></mrow></math>。如果<math alttext="m less-than-or-equal-to upper
    M"><mrow><mi>m</mi> <mo>≤</mo> <mi>M</mi></mrow></math>，则转到步骤2。
- en: 'The boosted estimate is given by:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 提升后的估计如下：
- en: <math alttext="ModifyingAbove upper F With caret equals alpha 1 ModifyingAbove
    f With caret Subscript 1 Baseline plus alpha 2 ModifyingAbove f With caret Subscript
    2 Baseline plus ellipsis plus alpha Subscript upper M Baseline ModifyingAbove
    f With caret Subscript upper M" display="block"><mrow><mover accent="true"><mi>F</mi>
    <mo>^</mo></mover> <mo>=</mo> <msub><mi>α</mi> <mn>1</mn></msub> <msub><mover
    accent="true"><mi>f</mi> <mo>^</mo></mover> <mn>1</mn></msub> <mo>+</mo> <msub><mi>α</mi>
    <mn>2</mn></msub> <msub><mover accent="true"><mi>f</mi> <mo>^</mo></mover> <mn>2</mn></msub>
    <mo>+</mo> <mo>⋯</mo> <mo>+</mo> <msub><mi>α</mi> <mi>M</mi></msub> <msub><mover
    accent="true"><mi>f</mi> <mo>^</mo></mover> <mi>M</mi></msub></mrow></math>
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="ModifyingAbove upper F With caret equals alpha 1 ModifyingAbove
    f With caret Subscript 1 Baseline plus alpha 2 ModifyingAbove f With caret Subscript
    2 Baseline plus ellipsis plus alpha Subscript upper M Baseline ModifyingAbove
    f With caret Subscript upper M" display="block"><mrow><mover accent="true"><mi>F</mi>
    <mo>^</mo></mover> <mo>=</mo> <msub><mi>α</mi> <mn>1</mn></msub> <msub><mover
    accent="true"><mi>f</mi> <mo>^</mo></mover> <mn>1</mn></msub> <mo>+</mo> <msub><mi>α</mi>
    <mn>2</mn></msub> <msub><mover accent="true"><mi>f</mi> <mo>^</mo></mover> <mn>2</mn></msub>
    <mo>+</mo> <mo>⋯</mo> <mo>+</mo> <msub><mi>α</mi> <mi>M</mi></msub> <msub><mover
    accent="true"><mi>f</mi> <mo>^</mo></mover> <mi>M</mi></msub></mrow></math>
- en: By increasing the weights for the observations that were misclassified, the
    algorithm forces the models to train more heavily on the data for which it performed
    poorly. The factor <math alttext="alpha Subscript m"><msub><mi>α</mi> <mi>m</mi></msub></math>
    ensures that models with lower error have a bigger weight.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 通过增加误分类观测的权重，该算法迫使模型更加深入地训练表现不佳的数据。因子<math alttext="alpha Subscript m"><msub><mi>α</mi>
    <mi>m</mi></msub></math>确保误差较低的模型具有较大的权重。
- en: Gradient boosting is similar to Adaboost but casts the problem as an optimization
    of a cost function. Instead of adjusting weights, gradient boosting fits models
    to a *pseudo-residual*, which has the effect of training more heavily on the larger
    residuals. In the spirit of the random forest, stochastic gradient boosting adds
    randomness to the algorithm by sampling observations and predictor variables at
    each stage.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度提升类似于Adaboost，但将问题表述为成本函数的优化。梯度提升不是调整权重，而是对*伪残差*进行模型拟合，这样更重视较大残差的训练效果。与随机森林类似，随机梯度提升通过在每个阶段对观测和预测变量进行抽样，为算法引入随机性。
- en: XGBoost
  id: totrans-277
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: XGBoost
- en: The most widely used public domain software for boosting is XGBoost, an implementation
    of stochastic gradient boosting originally developed by Tianqi Chen and Carlos
    Guestrin at the University of Washington. A computationally efficient implementation
    with many options, it is available as a package for most major data science software
    languages. In *R*, XGBoost is available as [the package `xgboost`](https://xgboost.readthedocs.io)
    and with the same name also for *Python*.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 提升的最广泛使用的公共领域软件是XGBoost，这是由华盛顿大学的Tianqi Chen和Carlos Guestrin最初开发的随机梯度提升的实现。作为一个计算效率高的实现，它在大多数主要数据科学软件语言中作为一个包提供。在*R*中，XGBoost可作为[包`xgboost`](https://xgboost.readthedocs.io)和同名包在*Python*中使用。
- en: 'The method `xgboost` has many parameters that can, and should, be adjusted
    (see [“Hyperparameters and Cross-Validation”](#HyperparametersCV)). Two very important
    parameters are `subsample`, which controls the fraction of observations that should
    be sampled at each iteration, and `eta`, a shrinkage factor applied to <math alttext="alpha
    Subscript m"><msub><mi>α</mi> <mi>m</mi></msub></math> in the boosting algorithm
    (see [“The Boosting Algorithm”](#BoostingAlgorithm)). Using `subsample` makes
    boosting act like the random forest except that the sampling is done without replacement.
    The shrinkage parameter `eta` is helpful to prevent overfitting by reducing the
    change in the weights (a smaller change in the weights means the algorithm is
    less likely to overfit to the training set). The following applies `xgboost` in
    *R* to the loan data with just two predictor variables:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '`xgboost`方法有许多参数可以调整（参见[“超参数和交叉验证”](#HyperparametersCV)）。两个非常重要的参数是`subsample`，控制每次迭代应抽样的观测部分，以及`eta`，是应用于提升算法中的权重缩减因子（参见[“提升算法”](#BoostingAlgorithm)）。使用`subsample`使提升算法的行为类似于随机森林，不同之处在于抽样是无替换的。缩减参数`eta`有助于通过减小权重的变化来防止过拟合（权重变化较小意味着算法对训练集的过拟合可能性较小）。以下示例在*R*中应用`xgboost`到贷款数据中，仅使用两个预测变量：'
- en: '[PRE25]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Note that `xgboost` does not support the formula syntax, so the predictors need
    to be converted to a `data.matrix` and the response needs to be converted to 0/1
    variables. The `objective` argument tells `xgboost` what type of problem this
    is; based on this, `xgboost` will choose a metric to optimize.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 注意`xgboost`不支持公式语法，因此预测变量需要转换为`data.matrix`，响应变量需要转换为0/1变量。`objective`参数告诉`xgboost`这是什么类型的问题；基于此，`xgboost`将选择一个优化度量。
- en: 'In *Python*, `xgboost` has two different interfaces: a `scikit-learn` API and
    a more functional interface like in *R*. To be consistent with other `scikit-learn`
    methods, some parameters were renamed. For example, `eta` is renamed to `learning_rate`;
    using `eta` will not fail, but it will not have the desired effect:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 在*Python*中，`xgboost`有两种不同的接口：`scikit-learn` API和类似*R*中的更功能化接口。为了与其他`scikit-learn`方法保持一致，一些参数已经重命名。例如，`eta`被重命名为`learning_rate`；使用`eta`不会失败，但不会产生预期的效果：
- en: '[PRE26]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The predicted values can be obtained from the `predict` function in *R* and,
    since there are only two variables, plotted versus the predictors:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 预测值可以从*R*中的`predict`函数中获得，并且由于只有两个变量，可以与预测变量绘制对比图：
- en: '[PRE27]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The same figure can be created in *Python* using the following code:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 在*Python*中可以使用以下代码创建相同的图形：
- en: '[PRE28]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The result is shown in [Figure 6-9](#LoanXGB). Qualitatively, this is similar
    to the predictions from the random forest; see [Figure 6-7](#LoanRF). The predictions
    are somewhat noisy in that some borrowers with a very high borrower score still
    end up with a prediction of default.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 结果显示在[图6-9](#LoanXGB)中。从质量上讲，这与随机森林的预测类似；请参见[图6-7](#LoanRF)。预测结果有些嘈杂，即一些借款人即使借款者评分非常高，最终也会被预测为违约。
- en: '![The predicted outcomes from XGBoost applied to the loan default data.](Images/psd2_0609.png)'
  id: totrans-289
  prefs: []
  type: TYPE_IMG
  zh: '![应用于贷款违约数据的XGBoost预测结果。](Images/psd2_0609.png)'
- en: Figure 6-9\. The predicted outcomes from XGBoost applied to the loan default
    data
  id: totrans-290
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-9\. XGBoost 应用于贷款违约数据的预测结果
- en: 'Regularization: Avoiding Overfitting'
  id: totrans-291
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 正则化：避免过拟合
- en: 'Blind application of `xgboost` can lead to unstable models as a result of *overfitting*
    to the training data. The problem with overfitting is twofold:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 盲目应用 `xgboost` 可能导致由于对训练数据的 *过度拟合* 而导致的不稳定模型。过拟合问题有两个方面：
- en: The accuracy of the model on new data not in the training set will be degraded.
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型在训练集之外的新数据上的准确性将下降。
- en: The predictions from the model are highly variable, leading to unstable results.
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型的预测结果变化很大，导致结果不稳定。
- en: Any modeling technique is potentially prone to overfitting. For example, if
    too many variables are included in a regression equation, the model may end up
    with spurious predictions. However, for most statistical techniques, overfitting
    can be avoided by a judicious selection of predictor variables. Even the random
    forest generally produces a reasonable model without tuning the parameters.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 任何建模技术都可能存在过拟合的风险。例如，如果回归方程中包含了太多变量，模型可能会产生虚假预测。然而，对于大多数统计技术，可以通过谨慎选择预测变量来避免过拟合。即使是随机森林通常在不调整参数的情况下也能产生合理的模型。
- en: 'This, however, is not the case for `xgboost`. Fit `xgboost` to the loan data
    for a training set with all of the variables included in the model. In *R*, you
    can do this as follows:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，对于 `xgboost` 来说并非如此。在贷款数据集上使用 `xgboost` 进行训练集时，包括模型中的所有变量。在 *R* 中，可以这样做：
- en: '[PRE29]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'We use the function `train_test_split` in *Python* to split the data set into
    training and test sets:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 `train_test_split` 函数在 *Python* 中将数据集分割为训练集和测试集：
- en: '[PRE30]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The test set consists of 10,000 randomly sampled records from the full data,
    and the training set consists of the remaining records. Boosting leads to an error
    rate of only 13.3% for the training set. The test set, however, has a much higher
    error rate of 35.3%. This is a result of overfitting: while boosting can explain
    the variability in the training set very well, the prediction rules do not apply
    to new data.'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 测试集包含从完整数据随机抽取的 10,000 条记录，训练集包含其余记录。增强学习导致训练集的错误率仅为 13.3%。然而，测试集的错误率要高得多，为
    35.3%。这是由于过拟合造成的：虽然增强学习可以很好地解释训练集中的变异性，但预测规则不适用于新数据。
- en: Boosting provides several parameters to avoid overfitting, including the parameters
    `eta` (or `learning_rate`) and `subsample` (see [“XGBoost”](#XGBoost)). Another
    approach is *regularization*, a technique that modifies the cost function in order
    to *penalize* the complexity of the model. Decision trees are fit by minimizing
    cost criteria such as Gini’s impurity score (see [“Measuring Homogeneity or Impurity”](#Gini)).
    In `xgboost`, it is possible to modify the cost function by adding a term that
    measures the complexity of the model.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 增强学习提供了几个参数来避免过拟合，包括参数 `eta`（或 `learning_rate`）和 `subsample`（参见 [“XGBoost”](#XGBoost)）。另一种方法是
    *正则化*，这是一种修改成本函数以 *惩罚* 模型复杂性的技术。决策树通过最小化诸如基尼不纯度分数之类的成本标准来拟合（参见 [“测量同质性或不纯度”](#Gini)）。在
    `xgboost` 中，可以通过添加一个衡量模型复杂性的项来修改成本函数。
- en: 'There are two parameters in `xgboost` to regularize the model: `alpha` and
    `lambda`, which correspond to Manhattan distance (L1-regularization) and squared
    Euclidean distance (L2-regularization), respectively (see [“Distance Metrics”](#DistanceMetrics)).
    Increasing these parameters will penalize more complex models and reduce the size
    of the trees that are fit. For example, see what happens if we set `lambda` to
    1,000 in *R*:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: '`xgboost` 中有两个正则化模型的参数：`alpha` 和 `lambda`，分别对应曼哈顿距离（L1 正则化）和平方欧几里得距离（L2 正则化）（参见
    [“距离度量”](#DistanceMetrics)）。增加这些参数将惩罚更复杂的模型，并减少拟合的树的大小。例如，看看如果在 *R* 中将 `lambda`
    设置为 1,000 会发生什么：'
- en: '[PRE31]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'In the `scikit-learn` API, the parameters are called `reg_alpha` and `reg_lambda`:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `scikit-learn` API 中，这些参数称为 `reg_alpha` 和 `reg_lambda`：
- en: '[PRE32]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Now the training error is only slightly lower than the error on the test set.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 现在训练误差仅比测试集上的误差略低。
- en: 'The `predict` method in *R* offers a convenient argument, `ntreelimit`, that
    forces only the first *i* trees to be used in the prediction. This lets us directly
    compare the in-sample versus out-of-sample error rates as more models are included:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *R* 中，`predict` 方法提供了一个方便的参数 `ntreelimit`，强制只使用前 *i* 棵树进行预测。这使我们可以直接比较样本内与样本外的错误率随着模型增加的变化：
- en: '[PRE33]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'In *Python*, we can call the `predict_proba` method with the `ntree_limit`
    argument:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *Python* 中，我们可以使用 `predict_proba` 方法并带有 `ntree_limit` 参数：
- en: '[PRE34]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The output from the model returns the error for the training set in the component
    `xgb_default$evaluation_log`. By combining this with the out-of-sample errors,
    we can plot the errors versus the number of iterations:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的输出返回训练集中`xgb_default$evaluation_log`组件的错误信息。通过将其与样本外的错误结合起来，我们可以绘制错误与迭代次数的关系图：
- en: '[PRE35]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'We can use the `pandas` plot method to create the line graph. The axis returned
    from the first plot allows us to overlay additional lines onto the same graph.
    This is a pattern that many of *Python*’s graph packages support:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`pandas`的绘图方法创建折线图。第一个图返回的轴允许我们在同一图上叠加额外的线条。这是许多*Python*图形包支持的模式：
- en: '[PRE36]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: The result, displayed in [Figure 6-10](#XGBoostError), shows how the default
    model steadily improves the accuracy for the training set but actually gets worse
    for the test set. The penalized model does not exhibit this behavior.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 结果显示在[图 6-10](#XGBoostError)中， 默认模型持续改善训练集的准确度，但实际上对于测试集来说却变得更糟。惩罚模型则没有这种行为。
- en: '![The error rate of the default XGBoost versus a penalized version of XGBoost.](Images/psd2_0610.png)'
  id: totrans-316
  prefs: []
  type: TYPE_IMG
  zh: '![默认XGBoost与惩罚版本XGBoost的错误率。](Images/psd2_0610.png)'
- en: Figure 6-10\. The error rate of the default XGBoost versus a penalized version
    of XGBoost
  id: totrans-317
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-10\. 默认的XGBoost错误率与惩罚版本的对比
- en: Hyperparameters and Cross-Validation
  id: totrans-318
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 超参数和交叉验证
- en: '`xgboost` has a daunting array of hyperparameters; see [“XGBoost Hyperparameters”](#BoostingParameters)
    for a discussion. As seen in [“Regularization: Avoiding Overfitting”](#Regularization),
    the specific choice can dramatically change the model fit. Given a huge combination
    of hyperparameters to choose from, how should we be guided in our choice? A standard
    solution to this problem is to use *cross-validation*; see [“Cross-Validation”](ch04.xhtml#CrossValidation).
    Cross-validation randomly splits up the data into *K* different groups, also called
    *folds*. For each fold, a model is trained on the data not in the fold and then
    evaluated on the data in the fold. This yields a measure of accuracy of the model
    on out-of-sample data. The best set of hyperparameters is the one given by the
    model with the lowest overall error as computed by averaging the errors from each
    of the folds.'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: '`xgboost`具有让人望而却步的一系列超参数；有关讨论，请参见[“XGBoost超参数”](#BoostingParameters)。正如在[“正则化：避免过拟合”](#Regularization)中所看到的，具体的选择可以显著改变模型的拟合效果。面对庞大的超参数组合选择，我们该如何做出指导性的选择？解决这个问题的一种标准方法是使用*交叉验证*；请参见[“交叉验证”](ch04.xhtml#CrossValidation)。交叉验证将数据随机分成*K*个不同的组，也称为*折叠*。对于每个折叠，模型在不在折叠中的数据上进行训练，然后在折叠中的数据上进行评估。这给出了模型在样本外数据上准确度的衡量。最佳的超参数组合是由具有最低整体错误的模型给出的，该错误通过计算每个折叠的平均错误得出。'
- en: 'To illustrate the technique, we apply it to parameter selection for `xgboost`.
    In this example, we explore two parameters: the shrinkage parameter `eta` (`learning_rate`—see
    [“XGBoost”](#XGBoost)) and the maximum depth of trees `max_depth`. The parameter
    `max_depth` is the maximum depth of a leaf node to the root of the tree with a
    default value of six. This gives us another way to control overfitting: deep trees
    tend to be more complex and may overfit the data. First we set up the folds and
    parameter list. In *R*, this is done as follows:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明这种技术，我们将其应用于`xgboost`的参数选择。在这个例子中，我们探索了两个参数：收缩参数`eta`（即`learning_rate`—参见[“XGBoost”](#XGBoost)）和树的最大深度`max_depth`。参数`max_depth`是叶节点到树根的最大深度，默认值为六。这为我们提供了另一种控制过拟合的方式：深树往往更复杂，可能会导致数据过拟合。首先我们设置了折叠和参数列表。在*R*中，可以这样做：
- en: '[PRE37]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Now we apply the preceding algorithm to compute the error for each model and
    each fold using five folds:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将上述算法应用于使用五折交叉验证计算每个模型和每个折叠的错误。
- en: '[PRE38]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'In the following *Python* code, we create all possible combinations of hyperparameters
    and fit and evaluate models with each combination:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下*Python*代码中，我们创建了所有可能的超参数组合，并使用每个组合拟合和评估模型：
- en: '[PRE39]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '[![1](Images/1.png)](#co_statistical_machine_learning_CO3-1)'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_statistical_machine_learning_CO3-1)'
- en: We use the function `itertools.product` from the *Python* standard library to
    create all possible combinations of the two hyperparameters.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用*Python*标准库中的`itertools.product`函数来创建两个超参数的所有可能组合。
- en: 'Since we are fitting 45 total models, this can take a while. The errors are
    stored as a matrix with the models along the rows and folds along the columns.
    Using the function `rowMeans`, we can compare the error rate for the different
    parameter sets:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们正在拟合总共45个模型，这可能需要一些时间。错误被存储为一个矩阵，模型沿行排列，而折叠沿列排列。使用函数`rowMeans`，我们可以比较不同参数设置的错误率。
- en: '[PRE40]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Cross-validation suggests that using shallower trees with a smaller value of
    `eta`/`learning_rate` yields more accurate results. Since these models are also
    more stable, the best parameters to use are `eta=0.1` and `max_depth=3` (or possibly
    `max_depth=6`).
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉验证表明，使用较小的`eta`/`learning_rate`值和更浅的树会产生更准确的结果。由于这些模型也更加稳定，所以最佳参数是`eta=0.1`和`max_depth=3`（或可能是`max_depth=6`）。
- en: Summary
  id: totrans-331
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: This chapter has described two classification and prediction methods that “learn”
    flexibly and locally from data, rather than starting with a structural model (e.g.,
    a linear regression) that is fit to the entire data set. *K*-Nearest Neighbors
    is a simple process that looks around at similar records and assigns their majority
    class (or average value) to the record being predicted. Trying various cutoff
    (split) values of predictor variables, tree models iteratively divide the data
    into sections and subsections that are increasingly homogeneous with respect to
    class. The most effective split values form a path, and also a “rule,” to a classification
    or prediction. Tree models are a very powerful and popular predictive tool, often
    outperforming other methods. They have given rise to various ensemble methods
    (random forests, boosting, bagging) that sharpen the predictive power of trees.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 本章已经描述了两种分类和预测方法，这些方法灵活地和局部地从数据中“学习”，而不是从适用于整个数据集的结构模型（例如线性回归）开始。*K*-最近邻居是一个简单的过程，它查看类似记录并将它们的主要类别（或平均值）分配给要预测的记录。尝试不同的预测变量的截断（分割）值，树模型迭代地将数据划分为越来越同类的部分和子部分。最有效的分割值形成一条路径，同时也是通向分类或预测的“规则”。树模型是一个非常强大且受欢迎的预测工具，通常优于其他方法。它们已经衍生出各种集成方法（随机森林、提升、装袋），以提高树的预测能力。
- en: ^([1](ch06.xhtml#idm46522845224392-marker)) This and subsequent sections in
    this chapter © 2020 Datastats, LLC, Peter Bruce, Andrew Bruce, and Peter Gedeck;
    used with permission.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch06.xhtml#idm46522845224392-marker)) 本章及后续章节版权所有 © 2020 Datastats, LLC,
    Peter Bruce, Andrew Bruce, 和 Peter Gedeck；已获授权使用。
- en: ^([2](ch06.xhtml#idm46522845155448-marker)) For this example, we take the first
    row in the `loan200` data set as the `newloan` and exclude it from the data set
    for training.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch06.xhtml#idm46522845155448-marker)) 对于这个例子，我们将`loan200`数据集中的第一行作为`newloan`，并将其从训练数据集中排除。
- en: ^([3](ch06.xhtml#idm46522843869576-marker)) The term CART is a registered trademark
    of Salford Systems related to their specific implementation of tree models.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch06.xhtml#idm46522843869576-marker)) CART一词是Salford Systems注册商标，与他们的树模型特定实现相关。
- en: ^([4](ch06.xhtml#idm46522842847944-marker)) The term *random forest* is a trademark
    of Leo Breiman and Adele Cutler and licensed to Salford Systems. There is no standard
    nontrademark name, and the term random forest is as synonymous with the algorithm
    as Kleenex is with facial tissues.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch06.xhtml#idm46522842847944-marker)) *随机森林*一词是Leo Breiman和Adele Cutler的商标，并许可给Salford
    Systems。没有标准的非商标名称，而随机森林这个术语就像Kleenex与面巾纸一样与该算法同义。
