- en: Chapter 6\. Statistical Machine Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Recent advances in statistics have been devoted to developing more powerful
    automated techniques for predictive modeling‚Äîboth regression and classification.
    These methods, like those discussed in the previous chapter, are *supervised methods*‚Äîthey
    are trained on data where outcomes are known and learn to predict outcomes in
    new data. They fall under the umbrella of *statistical machine learning* and are
    distinguished from classical statistical methods in that they are data-driven
    and do not seek to impose linear or other overall structure on the data. The *K*-Nearest
    Neighbors method, for example, is quite simple: classify a record in accordance
    with how similar records are classified. The most successful and widely used techniques
    are based on *ensemble learning* applied to *decision trees*. The basic idea of
    ensemble learning is to use many models to form a prediction, as opposed to using
    just a single model. Decision trees are a flexible and automatic technique to
    learn rules about the relationships between predictor variables and outcome variables.
    It turns out that the combination of ensemble learning with decision trees leads
    to some of the best performing off-the-shelf predictive modeling techniques.'
  prefs: []
  type: TYPE_NORMAL
- en: The development of many of the techniques in statistical machine learning can
    be traced back to the statisticians Leo Breiman (see [Figure¬†6-1](#LeoBreiman))
    at the University of California at Berkeley and Jerry Friedman at Stanford University.
    Their work, along with that of other researchers at Berkeley and Stanford, started
    with the development of tree models in 1984. The subsequent development of ensemble
    methods of bagging and boosting in the 1990s established the foundation of statistical
    machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: '![Leo Breiman, who was a professor of statistics at UC Berkeley, was at the
    forefront of the development of many techniques in a data scientist''s toolkit
    today](Images/psd2_0601.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-1\. Leo Breiman, who was a professor of statistics at UC Berkeley,
    was at the forefront of the development of many techniques in a data scientist‚Äôs
    toolkit today
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Machine Learning Versus Statistics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the context of predictive modeling, what is the difference between machine
    learning and statistics? There is not a bright line dividing the two disciplines.
    Machine learning tends to be focused more on developing efficient algorithms that
    scale to large data in order to optimize the predictive model. Statistics generally
    pays more attention to the probabilistic theory and underlying structure of the
    model. Bagging, and the random forest (see [‚ÄúBagging and the Random Forest‚Äù](#Bagging)),
    grew up firmly in the statistics camp. Boosting (see [‚ÄúBoosting‚Äù](#Boosting)),
    on the other hand, has been developed in both disciplines but receives more attention
    on the machine learning side of the divide. Regardless of the history, the promise
    of boosting ensures that it will thrive as a technique in both statistics and
    machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: K-Nearest Neighbors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The idea behind *K*-Nearest Neighbors (KNN) is very simple.^([1](ch06.xhtml#idm46522845224392))
    For each record to be classified or predicted:'
  prefs: []
  type: TYPE_NORMAL
- en: Find *K* records that have similar features (i.e., similar predictor values).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For classification, find out what the majority class is among those similar
    records and assign that class to the new record.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For prediction (also called *KNN regression*), find the average among those
    similar records, and predict that average for the new record.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'KNN is one of the simpler prediction/classification techniques: there is no
    model to be fit (as in regression). This doesn‚Äôt mean that using KNN is an automatic
    procedure. The prediction results depend on how the features are scaled, how similarity
    is measured, and how big *K* is set. Also, all predictors must be in numeric form.
    We will illustrate how to use the KNN method with a classification example.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A Small Example: Predicting Loan Default'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Table¬†6-1](#loan_data) shows a few records of personal loan data from LendingClub.
    LendingClub is a leader in peer-to-peer lending in which pools of investors make
    personal loans to individuals. The goal of an analysis would be to predict the
    outcome of a new potential loan: paid off versus default.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 6-1\. A few records and columns for LendingClub loan data
  prefs: []
  type: TYPE_NORMAL
- en: '| Outcome | Loan amount | Income | Purpose | Years employed | Home ownership
    | State |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Paid off | 10000 | 79100 | debt_consolidation | 11 | MORTGAGE | NV |'
  prefs: []
  type: TYPE_TB
- en: '| Paid off | 9600 | 48000 | moving | 5 | MORTGAGE | TN |'
  prefs: []
  type: TYPE_TB
- en: '| Paid off | 18800 | 120036 | debt_consolidation | 11 | MORTGAGE | MD |'
  prefs: []
  type: TYPE_TB
- en: '| Default | 15250 | 232000 | small_business | 9 | MORTGAGE | CA |'
  prefs: []
  type: TYPE_TB
- en: '| Paid off | 17050 | 35000 | debt_consolidation | 4 | RENT | MD |'
  prefs: []
  type: TYPE_TB
- en: '| Paid off | 5500 | 43000 | debt_consolidation | 4 | RENT | KS |'
  prefs: []
  type: TYPE_TB
- en: 'Consider a very simple model with just two predictor variables: `dti`, which
    is the ratio of debt payments (excluding mortgage) to income, and `payment_inc_ratio`,
    which is the ratio of the loan payment to income. Both ratios are multiplied by
    100. Using a small set of 200 loans, `loan200`, with known binary outcomes (default
    or no-default, specified in the predictor `outcome200`), and with *K* set to 20,
    the KNN estimate for a new loan to be predicted, `newloan`, with `dti=22.5` and
    `payment_inc_ratio=9` can be calculated in *R* as follows:^([2](ch06.xhtml#idm46522845155448))'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The KNN prediction is for the loan to default.
  prefs: []
  type: TYPE_NORMAL
- en: While *R* has a native `knn` function, the contributed *R* package [`FNN`, for
    Fast Nearest Neighbor](https://oreil.ly/RMQFG), scales more effectively to big
    data and provides more flexibility.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `scikit-learn` package provides a fast and efficient implementation of
    KNN in *Python*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[Figure¬†6-2](#LoanKNN) gives a visual display of this example. The new loan
    to be predicted is the cross in the middle. The squares (paid off) and circles
    (default) are the training data. The large black circle shows the boundary of
    the nearest 20 points. In this case, 9 defaulted loans lie within the circle,
    as compared with 11 paid-off loans. Hence the predicted outcome of the loan is
    paid off. Note that if we consider only three nearest neighbors, the prediction
    would be that the loan defaults.'
  prefs: []
  type: TYPE_NORMAL
- en: '![KNN prediction of loan default using two variables: debt-to-income ratio
    and loan-payment-to-income ratio](Images/psd2_0602.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6-2\. KNN prediction of loan default using two variables: debt-to-income
    ratio and loan-payment-to-income ratio'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: While the output of KNN for classification is typically a binary decision, such
    as default or paid off in the loan data, KNN routines usually offer the opportunity
    to output a probability (propensity) between 0 and 1. The probability is based
    on the fraction of one class in the *K* nearest neighbors. In the preceding example,
    this probability of default would have been estimated at <math alttext="nine-twenty-ths"><mfrac><mn>9</mn>
    <mn>20</mn></mfrac></math> , or 0.45. Using a probability score lets you use classification
    rules other than simple majority votes (probability of 0.5). This is especially
    important in problems with imbalanced classes; see [‚ÄúStrategies for Imbalanced
    Data‚Äù](ch05.xhtml#ImbalancedData). For example, if the goal is to identify members
    of a rare class, the cutoff would typically be set below 50%. One common approach
    is to set the cutoff at the probability of the rare event.
  prefs: []
  type: TYPE_NORMAL
- en: Distance Metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Similarity (nearness) is determined using a *distance metric*, which is a function
    that measures how far two records (*x[1]*, *x[2]*, ‚Ä¶, *x[p]*) and (*u[1]*, *u[2]*,
    ‚Ä¶, *u[p]*) are from one another. The most popular distance metric between two
    vectors is *Euclidean distance*. To measure the Euclidean distance between two
    vectors, subtract one from the other, square the differences, sum them, and take
    the square root:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><msqrt><mrow><msup><mrow><mo>(</mo><msub><mi>x</mi>
    <mn>1</mn></msub> <mo>-</mo><msub><mi>u</mi> <mn>1</mn></msub> <mo>)</mo></mrow>
    <mn>2</mn></msup> <mo>+</mo> <msup><mrow><mo>(</mo><msub><mi>x</mi> <mn>2</mn></msub>
    <mo>-</mo><msub><mi>u</mi> <mn>2</mn></msub> <mo>)</mo></mrow> <mn>2</mn></msup>
    <mo>+</mo> <mo>‚ãØ</mo> <mo>+</mo> <msup><mrow><mo>(</mo><msub><mi>x</mi> <mi>p</mi></msub>
    <mo>-</mo><msub><mi>u</mi> <mi>p</mi></msub> <mo>)</mo></mrow> <mn>2</mn></msup></mrow></msqrt>
    <mo>.</mo></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Another common distance metric for numeric data is *Manhattan distance*:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow>
    <msub><mi>x</mi> <mn>1</mn></msub> <mo>‚àí</mo> <msub><mi>u</mi> <mn>1</mn></msub>
    <mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow> <mo>+</mo> <mrow
    class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow> <msub><mi>x</mi> <mn>2</mn></msub>
    <mo>‚àí</mo> <msub><mi>u</mi> <mn>2</mn></msub> <mrow class="MJX-TeXAtom-ORD"><mo
    stretchy="false">|</mo></mrow> <mo>+</mo> <mo>‚ãØ</mo> <mo>+</mo> <mrow class="MJX-TeXAtom-ORD"><mo
    stretchy="false">|</mo></mrow> <msub><mi>x</mi> <mi>p</mi></msub> <mo>‚àí</mo> <msub><mi>u</mi>
    <mi>p</mi></msub> <mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: Euclidean distance corresponds to the straight-line distance between two points
    (e.g., as the crow flies). Manhattan distance is the distance between two points
    traversed in a single direction at a time (e.g., traveling along rectangular city
    blocks). For this reason, Manhattan distance is a useful approximation if similarity
    is defined as point-to-point travel time.
  prefs: []
  type: TYPE_NORMAL
- en: In measuring distance between two vectors, variables (features) that are measured
    with comparatively large scale will dominate the measure. For example, for the
    loan data, the distance would be almost solely a function of the income and loan
    amount variables, which are measured in tens or hundreds of thousands. Ratio variables
    would count for practically nothing in comparison. We address this problem by
    standardizing the data; see [‚ÄúStandardization (Normalization, z-Scores)‚Äù](#Standardization).
  prefs: []
  type: TYPE_NORMAL
- en: Other Distance Metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are numerous other metrics for measuring distance between vectors. For
    numeric data, *Mahalanobis distance* is attractive since it accounts for the correlation
    between two variables. This is useful since if two variables are highly correlated,
    Mahalanobis will essentially treat these as a single variable in terms of distance.
    Euclidean and Manhattan distance do not account for the correlation, effectively
    placing greater weight on the attribute that underlies those features. Mahalanobis
    distance is the Euclidean distance between the principal components (see [‚ÄúPrincipal
    Components Analysis‚Äù](ch07.xhtml#PCA)). The downside of using Mahalanobis distance
    is increased computational effort and complexity; it is computed using the *covariance
    matrix* (see [‚ÄúCovariance Matrix‚Äù](ch05.xhtml#Covariance)).
  prefs: []
  type: TYPE_NORMAL
- en: One Hot Encoder
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The loan data in [Table¬†6-1](#loan_data) includes several factor (string) variables.
    Most statistical and machine learning models require this type of variable to
    be converted to a series of binary dummy variables conveying the same information,
    as in [Table¬†6-2](#home_ownership_dummy). Instead of a single variable denoting
    the home occupant status as ‚Äúowns with a mortgage,‚Äù ‚Äúowns with no mortgage,‚Äù ‚Äúrents,‚Äù
    or ‚Äúother,‚Äù we end up with four binary variables. The first would be ‚Äúowns with
    a mortgage‚ÄîY/N,‚Äù the second would be ‚Äúowns with no mortgage‚ÄîY/N,‚Äù and so on. This
    one predictor, home occupant status, thus yields a vector with one 1 and three
    0s that can be used in statistical and machine learning algorithms. The phrase
    *one hot encoding* comes from digital circuit terminology, where it describes
    circuit settings in which only one bit is allowed to be positive (hot).
  prefs: []
  type: TYPE_NORMAL
- en: Table 6-2\. Representing home ownership factor data in [Table¬†6-1](#loan_data)
    as a numeric dummy variable
  prefs: []
  type: TYPE_NORMAL
- en: '| OWNS_WITH_MORTGAGE | OWNS_WITHOUT_MORTGAGE | OTHER | RENT |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 0 | 0 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 0 | 0 | 1 |'
  prefs: []
  type: TYPE_TB
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In linear and logistic regression, one hot encoding causes problems with multicollinearity;
    see [‚ÄúMulticollinearity‚Äù](ch04.xhtml#Multicollinearity). In such cases, one dummy
    is omitted (its value can be inferred from the other values). This is not an issue
    with KNN and other methods discussed in this book.
  prefs: []
  type: TYPE_NORMAL
- en: Standardization (Normalization, z-Scores)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In measurement, we are often not so much interested in ‚Äúhow much‚Äù but in ‚Äúhow
    different from the average.‚Äù Standardization, also called *normalization*, puts
    all variables on similar scales by subtracting the mean and dividing by the standard
    deviation; in this way, we ensure that a variable does not overly influence a
    model simply due to the scale of its original measurement:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mi>z</mi> <mo>=</mo> <mfrac><mrow><mi>x</mi><mo>-</mo><mover
    accent="true"><mi>x</mi> <mo>¬Ø</mo></mover></mrow> <mi>s</mi></mfrac></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: The result of this transformation is commonly referred to as a *z-score*. Measurements
    are then stated in terms of ‚Äústandard deviations away from the mean.‚Äù
  prefs: []
  type: TYPE_NORMAL
- en: Caution
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '*Normalization* in this statistical context is not to be confused with *database
    normalization*, which is the removal of redundant data and the verification of
    data dependencies.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For KNN and a few other procedures (e.g., principal components analysis and
    clustering), it is essential to consider standardizing the data prior to applying
    the procedure. To illustrate this idea, KNN is applied to the loan data using
    `dti` and `payment_inc_ratio` (see [‚ÄúA Small Example: Predicting Loan Default‚Äù](#LoanExampleKNN))
    plus two other variables: `revol_bal`, the total revolving credit available to
    the applicant in dollars, and `revol_util`, the percent of the credit being used.
    The new record to be predicted is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The magnitude of `revol_bal`, which is in dollars, is much bigger than that
    of the other variables. The `knn` function returns the index of the nearest neighbors
    as an attribute `nn.index`, and this can be used to show the top-five closest
    rows in `loan_df`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Following the model fit, we can use the `kneighbors` method to identify the
    five closest rows in the training set with `scikit-learn`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The value of `revol_bal` in these neighbors is very close to its value in the
    new record, but the other predictor variables are all over the map and essentially
    play no role in determining neighbors.
  prefs: []
  type: TYPE_NORMAL
- en: 'Compare this to KNN applied to the standardized data using the *R* function
    `scale`, which computes the *z*-score for each variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_statistical_machine_learning_CO1-1)'
  prefs: []
  type: TYPE_NORMAL
- en: We need to remove the first row from `loan_df` as well, so that the row numbers
    correspond to each other.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `sklearn.preprocessing.StandardScaler` method is first trained with the
    predictors and is subsequently used to transform the data set prior to training
    the KNN model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The five nearest neighbors are much more alike in all the variables, providing
    a more sensible result. Note that the results are displayed on the original scale,
    but KNN was applied to the scaled data and the new loan to be predicted.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Using the *z*-score is just one way to rescale variables. Instead of the mean,
    a more robust estimate of location could be used, such as the median. Likewise,
    a different estimate of scale such as the interquartile range could be used instead
    of the standard deviation. Sometimes, variables are ‚Äúsquashed‚Äù into the 0‚Äì1 range.
    It‚Äôs also important to realize that scaling each variable to have unit variance
    is somewhat arbitrary. This implies that each variable is thought to have the
    same importance in predictive power. If you have subjective knowledge that some
    variables are more important than others, then these could be scaled up. For example,
    with the loan data, it is reasonable to expect that the payment-to-income ratio
    is very important.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Normalization (standardization) does not change the distributional shape of
    the data; it does not make it normally shaped if it was not already normally shaped
    (see [‚ÄúNormal Distribution‚Äù](ch02.xhtml#NormalDist)).
  prefs: []
  type: TYPE_NORMAL
- en: Choosing K
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The choice of *K* is very important to the performance of KNN. The simplest
    choice is to set <math alttext="upper K equals 1"><mrow><mi>K</mi> <mo>=</mo>
    <mn>1</mn></mrow></math> , known as the 1-nearest neighbor classifier. The prediction
    is intuitive: it is based on finding the data record in the training set most
    similar to the new record to be predicted. Setting <math alttext="upper K equals
    1"><mrow><mi>K</mi> <mo>=</mo> <mn>1</mn></mrow></math> is rarely the best choice;
    you‚Äôll almost always obtain superior performance by using *K* > 1-nearest neighbors.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Generally speaking, if *K* is too low, we may be overfitting: including the
    noise in the data. Higher values of *K* provide smoothing that reduces the risk
    of overfitting in the training data. On the other hand, if *K* is too high, we
    may oversmooth the data and miss out on KNN‚Äôs ability to capture the local structure
    in the data, one of its main advantages.'
  prefs: []
  type: TYPE_NORMAL
- en: The *K* that best balances between overfitting and oversmoothing is typically
    determined by accuracy metrics and, in particular, accuracy with holdout or validation
    data. There is no general rule about the best *K*‚Äîit depends greatly on the nature
    of the data. For highly structured data with little noise, smaller values of *K*
    work best. Borrowing a term from the signal processing community, this type of
    data is sometimes referred to as having a high *signal-to-noise ratio* (*SNR*).
    Examples of data with a typically high SNR are data sets for handwriting and speech
    recognition. For noisy data with less structure (data with a low SNR), such as
    the loan data, larger values of *K* are appropriate. Typically, values of *K*
    fall in the range 1 to 20. Often, an odd number is chosen to avoid ties.
  prefs: []
  type: TYPE_NORMAL
- en: Bias-Variance Trade-off
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The tension between oversmoothing and overfitting is an instance of the *bias-variance
    trade-off*, a ubiquitous problem in statistical model fitting. Variance refers
    to the modeling error that occurs because of the choice of training data; that
    is, if you were to choose a different set of training data, the resulting model
    would be different. Bias refers to the modeling error that occurs because you
    have not properly identified the underlying real-world scenario; this error would
    not disappear if you simply added more training data. When a flexible model is
    overfit, the variance increases. You can reduce this by using a simpler model,
    but the bias may increase due to the loss of flexibility in modeling the real
    underlying situation. A general approach to handling this trade-off is through
    *cross-validation*. See [‚ÄúCross-Validation‚Äù](ch04.xhtml#CrossValidation) for more
    details.
  prefs: []
  type: TYPE_NORMAL
- en: KNN as a Feature Engine
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'KNN gained its popularity due to its simplicity and intuitive nature. In terms
    of performance, KNN by itself is usually not competitive with more sophisticated
    classification techniques. In practical model fitting, however, KNN can be used
    to add ‚Äúlocal knowledge‚Äù in a staged process with other classification techniques:'
  prefs: []
  type: TYPE_NORMAL
- en: KNN is run on the data, and for each record, a classification (or quasi-probability
    of a class) is derived.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: That result is added as a new feature to the record, and another classification
    method is then run on the data. The original predictor variables are thus used
    twice.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: At first you might wonder whether this process, since it uses some predictors
    twice, causes a problem with multicollinearity (see [‚ÄúMulticollinearity‚Äù](ch04.xhtml#Multicollinearity)).
    This is not an issue, since the information being incorporated into the second-stage
    model is highly local, derived only from a few nearby records, and is therefore
    additional information and not redundant.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You can think of this staged use of KNN as a form of ensemble learning, in which
    multiple predictive modeling methods are used in conjunction with one another.
    It can also be considered as a form of feature engineering in which the aim is
    to derive features (predictor variables) that have predictive power. Often this
    involves some manual review of the data; KNN gives a fairly automatic way to do
    this.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, consider the King County housing data. In pricing a home for sale,
    a realtor will base the price on similar homes recently sold, known as ‚Äúcomps.‚Äù
    In essence, realtors are doing a manual version of KNN: by looking at the sale
    prices of similar homes, they can estimate what a home will sell for. We can create
    a new feature for a statistical model to mimic the real estate professional by
    applying KNN to recent sales. The predicted value is the sales price, and the
    existing predictor variables could include location, total square feet, type of
    structure, lot size, and number of bedrooms and bathrooms. The new predictor variable
    (feature) that we add via KNN is the KNN predictor for each record (analogous
    to the realtors‚Äô comps). Since we are predicting a numerical value, the average
    of the *K*-Nearest Neighbors is used instead of a majority vote (known as *KNN
    regression*).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, for the loan data, we can create features that represent different
    aspects of the loan process. For example, the following *R* code would build a
    feature that represents a borrower‚Äôs creditworthiness:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'With `scikit-learn`, we use the `predict_proba` method of the trained model
    to get the probabilities:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The result is a feature that predicts the likelihood a borrower will default
    based on his credit history.
  prefs: []
  type: TYPE_NORMAL
- en: Tree Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Tree models, also called *Classification and Regression Trees* (*CART*),^([3](ch06.xhtml#idm46522843869576))
    *decision trees*, or just *trees*, are an effective and popular classification
    (and regression) method initially developed by Leo Breiman and others in 1984.
    Tree models, and their more powerful descendants *random forests* and *boosted
    trees* (see [‚ÄúBagging and the Random Forest‚Äù](#Bagging) and [‚ÄúBoosting‚Äù](#Boosting)),
    form the basis for the most widely used and powerful predictive modeling tools
    in data science for regression and classification.
  prefs: []
  type: TYPE_NORMAL
- en: A tree model is a set of ‚Äúif-then-else‚Äù rules that are easy to understand and
    to implement. In contrast to linear and logistic regression, trees have the ability
    to discover hidden patterns corresponding to complex interactions in the data.
    However, unlike KNN or naive Bayes, simple tree models can be expressed in terms
    of predictor relationships that are easily interpretable.
  prefs: []
  type: TYPE_NORMAL
- en: Decision Trees in Operations Research
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The term *decision trees* has a different (and older) meaning in decision science
    and operations research, where it refers to a human decision analysis process.
    In this meaning, decision points, possible outcomes, and their estimated probabilities
    are laid out in a branching diagram, and the decision path with the maximum expected
    value is chosen.
  prefs: []
  type: TYPE_NORMAL
- en: A Simple Example
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The two main packages to fit tree models in *R* are `rpart` and `tree`. Using
    the `rpart` package, a model is fit to a sample of 3,000 records of the loan data
    using the variables `payment_inc_ratio` and `borrower_score` (see [‚ÄúK-Nearest
    Neighbors‚Äù](#KNN) for a description of the data):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The `sklearn.tree.DecisionTreeClassifier` provides an implementation of a decision
    tree. The `dmba` package provides a convenience function to create a visualization
    inside a Jupyter notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The resulting tree is shown in [Figure¬†6-3](#LoanTree). Due to the different
    implementations, you will find that the results from *R* and *Python* are not
    identical; this is expected. These classification rules are determined by traversing
    through a hierarchical tree, starting at the root and moving left if the node
    is true and right if not, until a leaf is reached.
  prefs: []
  type: TYPE_NORMAL
- en: Typically, the tree is plotted upside-down, so the root is at the top and the
    leaves are at the bottom. For example, if we get a loan with `borrower_score`
    of 0.6 and a `payment_inc_ratio` of 8.0, we end up at the leftmost leaf and predict
    the loan will be paid off.
  prefs: []
  type: TYPE_NORMAL
- en: '![The rules for a simple tree model fit to the loan data.](Images/psd2_0603.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-3\. The rules for a simple tree model fit to the loan data
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'A nicely printed version of the tree is also easily produced in *R*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The depth of the tree is shown by the indent. Each node corresponds to a provisional
    classification determined by the prevalent outcome in that partition. The ‚Äúloss‚Äù
    is the number of misclassifications yielded by the provisional classification
    in a partition. For example, in node 2, there were 261 misclassifications out
    of a total of 878 total records. The values in the parentheses correspond to the
    proportion of records that are paid off or in default, respectively. For example,
    in node 13, which predicts default, over 60 percent of the records are loans that
    are in default.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `scikit-learn` documentation describes how to create a text representation
    of a decision tree model. We included a convenience function in our `dmba` package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The Recursive Partitioning Algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The algorithm to construct a decision tree, called *recursive partitioning*,
    is straightforward and intuitive. The data is repeatedly partitioned using predictor
    values that do the best job of separating the data into relatively homogeneous
    partitions. [Figure¬†6-4](#LoanRecursivePartitioning) shows the partitions created
    for the tree in [Figure¬†6-3](#LoanTree). The first rule, depicted by rule 1, is
    `borrower_score >= 0.575` and segments the right portion of the plot. The second
    rule is `borrower_score < 0.375` and segments the left portion.
  prefs: []
  type: TYPE_NORMAL
- en: '![The first five rules for a simple tree model fit to the loan data.](Images/psd2_0604.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-4\. The first three rules for a simple tree model fit to the loan data
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Suppose we have a response variable *Y* and a set of *P* predictor variables
    *X[j]* for <math alttext="j equals 1 comma ellipsis comma upper P"><mrow><mi>j</mi>
    <mo>=</mo> <mn>1</mn> <mo>,</mo> <mo>‚ãØ</mo> <mo>,</mo> <mi>P</mi></mrow></math>
    . For a partition *A* of records, recursive partitioning will find the best way
    to partition *A* into two subpartitions:'
  prefs: []
  type: TYPE_NORMAL
- en: 'For each predictor variable *X[j]*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For each value *s[j]* of *X[j]*:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Split the records in *A* with *X[j]* values < *s[j]* as one partition, and the
    remaining records where *X[j]* ‚â• *s[j]* as another partition.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Measure the homogeneity of classes within each subpartition of *A*.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the value of *s[j]* that produces maximum within-partition homogeneity
    of class.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the variable *X[j]* and the split value *s[j]* that produces maximum
    within-partition homogeneity of class.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now comes the recursive part:'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize *A* with the entire data set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Apply the partitioning algorithm to split *A* into two subpartitions, *A[1]*
    and *A[2]*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat step 2 on subpartitions *A[1]* and *A[2]*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The algorithm terminates when no further partition can be made that sufficiently
    improves the homogeneity of the partitions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The end result is a partitioning of the data, as in [Figure¬†6-4](#LoanRecursivePartitioning),
    except in *P*-dimensions, with each partition predicting an outcome of 0 or 1
    depending on the majority vote of the response in that partition.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'In addition to a binary 0/1 prediction, tree models can produce a probability
    estimate based on the number of 0s and 1s in the partition. The estimate is simply
    the sum of 0s or 1s in the partition divided by the number of observations in
    the partition:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mtext>Prob</mtext> <mrow><mo>(</mo> <mi>Y</mi>
    <mo>=</mo> <mn>1</mn> <mo>)</mo></mrow> <mo>=</mo> <mfrac><mrow><mtext>Number</mtext><mtext>of</mtext><mtext>1s</mtext><mtext>in</mtext><mtext>the</mtext><mtext>partition</mtext></mrow>
    <mrow><mtext>Size</mtext><mtext>of</mtext><mtext>the</mtext><mtext>partition</mtext></mrow></mfrac></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: The estimated <math alttext="Prob left-parenthesis upper Y equals 1 right-parenthesis"><mrow><mtext>Prob</mtext>
    <mo>(</mo> <mi>Y</mi> <mo>=</mo> <mn>1</mn> <mo>)</mo></mrow></math> can then
    be converted to a binary decision; for example, set the estimate to 1 if Prob(*Y*
    = 1) > 0.5.
  prefs: []
  type: TYPE_NORMAL
- en: Measuring Homogeneity or Impurity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Tree models recursively create partitions (sets of records), *A*, that predict
    an outcome of *Y* = 0 or *Y* = 1. You can see from the preceding algorithm that
    we need a way to measure homogeneity, also called *class purity*, within a partition.
    Or equivalently, we need to measure the impurity of a partition. The accuracy
    of the predictions is the proportion *p* of misclassified records within that
    partition, which ranges from 0 (perfect) to 0.5 (purely random guessing).
  prefs: []
  type: TYPE_NORMAL
- en: 'It turns out that accuracy is not a good measure for impurity. Instead, two
    common measures for impurity are the *Gini impurity* and *entropy* of *information*.
    While these (and other) impurity measures apply to classification problems with
    more than two classes, we focus on the binary case. The Gini impurity for a set
    of records *A* is:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mi>I</mi> <mo>(</mo> <mi>A</mi> <mo>)</mo> <mo>=</mo>
    <mi>p</mi> <mo>(</mo> <mn>1</mn> <mo>-</mo> <mi>p</mi> <mo>)</mo></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'The entropy measure is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mi>I</mi> <mrow><mo>(</mo> <mi>A</mi> <mo>)</mo></mrow>
    <mo>=</mo> <mo>-</mo> <mi>p</mi> <msub><mo form="prefix">log</mo> <mn>2</mn></msub>
    <mrow><mo>(</mo> <mi>p</mi> <mo>)</mo></mrow> <mo>-</mo> <mrow><mo>(</mo> <mn>1</mn>
    <mo>-</mo> <mi>p</mi> <mo>)</mo></mrow> <msub><mo form="prefix">log</mo> <mn>2</mn></msub>
    <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <mi>p</mi> <mo>)</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure¬†6-5](#Impurity) shows that Gini impurity (rescaled) and entropy measures
    are similar, with entropy giving higher impurity scores for moderate and high
    accuracy rates.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Gini impurity and entropy measures.](Images/psd2_0605.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-5\. Gini impurity and entropy measures
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Gini Coefficient
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Gini impurity is not to be confused with the *Gini coefficient*. They represent
    similar concepts, but the Gini coefficient is limited to the binary classification
    problem and is related to the AUC metric (see [‚ÄúAUC‚Äù](ch05.xhtml#AUC)).
  prefs: []
  type: TYPE_NORMAL
- en: The impurity metric is used in the splitting algorithm described earlier. For
    each proposed partition of the data, impurity is measured for each of the partitions
    that result from the split. A weighted average is then calculated, and whichever
    partition (at each stage) yields the lowest weighted average is selected.
  prefs: []
  type: TYPE_NORMAL
- en: Stopping the Tree from Growing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As the tree grows bigger, the splitting rules become more detailed, and the
    tree gradually shifts from identifying ‚Äúbig‚Äù rules that identify real and reliable
    relationships in the data to ‚Äútiny‚Äù rules that reflect only noise. A fully grown
    tree results in completely pure leaves and, hence, 100% accuracy in classifying
    the data that it is trained on. This accuracy is, of course, illusory‚Äîwe have
    overfit (see [‚ÄúBias-Variance Trade-off‚Äù](#bvt_note)) the data, fitting the noise
    in the training data, not the signal that we want to identify in new data.
  prefs: []
  type: TYPE_NORMAL
- en: 'We need some way to determine when to stop growing a tree at a stage that will
    generalize to new data. There are various ways to stop splitting in *R* and *Python*:'
  prefs: []
  type: TYPE_NORMAL
- en: Avoid splitting a partition if a resulting subpartition is too small, or if
    a terminal leaf is too small. In `rpart` (*R*), these constraints are controlled
    separately by the parameters `minsplit` and `minbucket`, respectively, with defaults
    of `20` and `7`. In *Python*‚Äôs `DecisionTreeClassifier`, we can control this using
    the parameters `min_samples_split` (default `2`) and `min_samples_leaf` (default
    `1`).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Don‚Äôt split a partition if the new partition does not ‚Äúsignificantly‚Äù reduce
    the impurity. In `rpart`, this is controlled by the *complexity parameter* `cp`,
    which is a measure of how complex a tree is‚Äîthe more complex, the greater the
    value of `cp`. In practice, `cp` is used to limit tree growth by attaching a penalty
    to additional complexity (splits) in a tree. `DecisionTreeClassifier` (*Python*)
    has the parameter `min_impurity_decrease`, which limits splitting based on a weighted
    impurity decrease value. Here, smaller values will lead to more complex trees.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These methods involve arbitrary rules and can be useful for exploratory work,
    but we can‚Äôt easily determine optimum values (i.e., values that maximize predictive
    accuracy with new data). We need to combine cross-validation with either systematically
    changing the model parameters or modifying the tree through pruning.
  prefs: []
  type: TYPE_NORMAL
- en: Controlling tree complexity in *R*
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With the complexity parameter, `cp`, we can estimate what size tree will perform
    best with new data. If `cp` is too small, then the tree will overfit the data,
    fitting noise and not signal. On the other hand, if `cp` is too large, then the
    tree will be too small and have little predictive power. The default in `rpart`
    is 0.01, although for larger data sets, you are likely to find this is too large.
    In the previous example, `cp` was set to `0.005` since the default led to a tree
    with a single split. In exploratory analysis, it is sufficient to simply try a
    few values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Determining the optimum `cp` is an instance of the bias-variance trade-off.
    The most common way to estimate a good value of `cp` is via cross-validation (see
    [‚ÄúCross-Validation‚Äù](ch04.xhtml#CrossValidation)):'
  prefs: []
  type: TYPE_NORMAL
- en: Partition the data into training and validation (holdout) sets.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Grow the tree with the training data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Prune it successively, step by step, recording `cp` (using the *training* data)
    at each step.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note the `cp` that corresponds to the minimum error (loss) on the *validation*
    data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repartition the data into training and validation, and repeat the growing, pruning,
    and `cp` recording process.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Do this again and again, and average the `cp`s that reflect minimum error for
    each tree.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Go back to the original data, or future data, and grow a tree, stopping at this
    optimum `cp` value.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In `rpart`, you can use the argument `cptable` to produce a table of the `cp`
    values and their associated cross-validation error (`xerror` in *R*), from which
    you can determine the `cp` value that has the lowest cross-validation error.
  prefs: []
  type: TYPE_NORMAL
- en: Controlling tree complexity in *Python*
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Neither the complexity parameter nor pruning is available in `scikit-learn`‚Äôs
    decision tree implementation. The solution is to use grid search over combinations
    of different parameter values. For example, we can vary `max_depth` in the range
    5 to 30 and `min_samples_split` between 20 and 100\. The `GridSearchCV` method
    in `scikit-learn` is a convenient way to combine the exhaustive search through
    all combinations with cross-validation. An optimal parameter set is then selected
    using the cross-validated model performance.
  prefs: []
  type: TYPE_NORMAL
- en: Predicting a Continuous Value
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Predicting a continuous value (also termed *regression*) with a tree follows
    the same logic and procedure, except that impurity is measured by squared deviations
    from the mean (squared errors) in each subpartition, and predictive performance
    is judged by the square root of the mean squared error (RMSE) (see [‚ÄúAssessing
    the Model‚Äù](ch04.xhtml#RMSE)) in each partition.
  prefs: []
  type: TYPE_NORMAL
- en: '`scikit-learn` has the `sklearn.tree.DecisionTreeRegressor` method to train
    a decision tree regression model.'
  prefs: []
  type: TYPE_NORMAL
- en: How Trees Are Used
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'One of the big obstacles faced by predictive modelers in organizations is the
    perceived ‚Äúblack box‚Äù nature of the methods they use, which gives rise to opposition
    from other elements of the organization. In this regard, the tree model has two
    appealing aspects:'
  prefs: []
  type: TYPE_NORMAL
- en: Tree models provide a visual tool for exploring the data, to gain an idea of
    what variables are important and how they relate to one another. Trees can capture
    nonlinear relationships among predictor variables.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tree models provide a set of rules that can be effectively communicated to nonspecialists,
    either for implementation or to ‚Äúsell‚Äù a data mining project.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When it comes to prediction, however, harnessing the results from multiple trees
    is typically more powerful than using just a single tree. In particular, the random
    forest and boosted tree algorithms almost always provide superior predictive accuracy
    and performance (see [‚ÄúBagging and the Random Forest‚Äù](#Bagging) and [‚ÄúBoosting‚Äù](#Boosting)),
    but the aforementioned advantages of a single tree are lost.
  prefs: []
  type: TYPE_NORMAL
- en: Further Reading
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Analytics Vidhya Content Team, [‚ÄúTree Based Algorithms: A Complete Tutorial
    from Scratch (in *R* & *Python*)‚Äù](https://oreil.ly/zOr4B), April 12, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Terry M. Therneau, Elizabeth J. Atkinson, and the Mayo Foundation, [‚ÄúAn Introduction
    to Recursive Partitioning Using the RPART Routines‚Äù](https://oreil.ly/6rLGk),
    April 11, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bagging and the Random Forest
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In 1906, the statistician Sir Francis Galton was visiting a county fair in
    England, at which a contest was being held to guess the dressed weight of an ox
    that was on exhibit. There were 800 guesses, and while the individual guesses
    varied widely, both the mean and the median came out within 1% of the ox‚Äôs true
    weight. James Surowiecki has explored this phenomenon in his book *The Wisdom
    of Crowds* (Doubleday, 2004). This principle applies to predictive models as well:
    averaging (or taking majority votes) of multiple models‚Äîan *ensemble* of models‚Äîturns
    out to be more accurate than just selecting one model.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The ensemble approach has been applied to and across many different modeling
    methods, most publicly in the Netflix Prize, in which Netflix offered a $1 million
    prize to any contestant who came up with a model that produced a 10% improvement
    in predicting the rating that a Netflix customer would award a movie. The simple
    version of ensembles is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Develop a predictive model and record the predictions for a given data set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat for multiple models on the same data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each record to be predicted, take an average (or a weighted average, or
    a majority vote) of the predictions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Ensemble methods have been applied most systematically and effectively to decision
    trees. Ensemble tree models are so powerful that they provide a way to build good
    predictive models with relatively little effort.
  prefs: []
  type: TYPE_NORMAL
- en: 'Going beyond the simple ensemble algorithm, there are two main variants of
    ensemble models: *bagging* and *boosting*. In the case of ensemble tree models,
    these are referred to as *random forest* models and *boosted tree* models. This
    section focuses on bagging; boosting is covered in [‚ÄúBoosting‚Äù](#Boosting).'
  prefs: []
  type: TYPE_NORMAL
- en: Bagging
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Bagging, which stands for ‚Äúbootstrap aggregating,‚Äù was introduced by Leo Breiman
    in 1994. Suppose we have a response *Y* and *P* predictor variables <math alttext="bold
    upper X equals upper X 1 comma upper X 2 comma ellipsis comma upper X Subscript
    upper P Baseline"><mrow><mi>ùêó</mi> <mo>=</mo> <msub><mi>X</mi> <mn>1</mn></msub>
    <mo>,</mo> <msub><mi>X</mi> <mn>2</mn></msub> <mo>,</mo> <mo>‚ãØ</mo> <mo>,</mo>
    <msub><mi>X</mi> <mi>P</mi></msub></mrow></math> with *N* records.
  prefs: []
  type: TYPE_NORMAL
- en: 'Bagging is like the basic algorithm for ensembles, except that, instead of
    fitting the various models to the same data, each new model is fitted to a bootstrap
    resample. Here is the algorithm presented more formally:'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize *M*, the number of models to be fit, and *n*, the number of records
    to choose (*n* < *N*). Set the iteration <math alttext="m equals 1"><mrow><mi>m</mi>
    <mo>=</mo> <mn>1</mn></mrow></math> .
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Take a bootstrap resample (i.e., with replacement) of *n* records from the training
    data to form a subsample <math alttext="upper Y Subscript m"><msub><mi>Y</mi>
    <mi>m</mi></msub></math> and <math alttext="bold upper X Subscript m"><msub><mi>ùêó</mi>
    <mi>m</mi></msub></math> (the bag).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train a model using <math alttext="upper Y Subscript m"><msub><mi>Y</mi> <mi>m</mi></msub></math>
    and <math alttext="bold upper X Subscript m"><msub><mi>ùêó</mi> <mi>m</mi></msub></math>
    to create a set of decision rules <math alttext="ModifyingAbove f With caret Subscript
    m Baseline left-parenthesis bold upper X right-parenthesis"><mrow><msub><mover
    accent="true"><mi>f</mi> <mo>^</mo></mover> <mi>m</mi></msub> <mrow><mo>(</mo>
    <mi>ùêó</mi> <mo>)</mo></mrow></mrow></math> .
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Increment the model counter <math alttext="m equals m plus 1"><mrow><mi>m</mi>
    <mo>=</mo> <mi>m</mi> <mo>+</mo> <mn>1</mn></mrow></math> . If *m* <= *M*, go
    to step 2.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the case where <math alttext="ModifyingAbove f With caret Subscript m"><msub><mover
    accent="true"><mi>f</mi> <mo>^</mo></mover> <mi>m</mi></msub></math> predicts
    the probability <math alttext="upper Y equals 1"><mrow><mi>Y</mi> <mo>=</mo> <mn>1</mn></mrow></math>
    , the bagged estimate is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="ModifyingAbove f With caret equals StartFraction 1 Over upper
    M EndFraction left-parenthesis ModifyingAbove f With caret Subscript 1 Baseline
    left-parenthesis bold upper X right-parenthesis plus ModifyingAbove f With caret
    Subscript 2 Baseline left-parenthesis bold upper X right-parenthesis plus ellipsis
    plus ModifyingAbove f With caret Subscript upper M Baseline left-parenthesis bold
    upper X right-parenthesis right-parenthesis" display="block"><mrow><mover accent="true"><mi>f</mi>
    <mo>^</mo></mover> <mo>=</mo> <mfrac><mn>1</mn> <mi>M</mi></mfrac> <mfenced separators=""
    open="(" close=")"><msub><mover accent="true"><mi>f</mi> <mo>^</mo></mover> <mn>1</mn></msub>
    <mrow><mo>(</mo> <mi>ùêó</mi> <mo>)</mo></mrow> <mo>+</mo> <msub><mover accent="true"><mi>f</mi>
    <mo>^</mo></mover> <mn>2</mn></msub> <mrow><mo>(</mo> <mi>ùêó</mi> <mo>)</mo></mrow>
    <mo>+</mo> <mo>‚ãØ</mo> <mo>+</mo> <msub><mover accent="true"><mi>f</mi> <mo>^</mo></mover>
    <mi>M</mi></msub> <mrow><mo>(</mo> <mi>ùêó</mi> <mo>)</mo></mrow></mfenced></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: Random Forest
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The *random forest* is based on applying bagging to decision trees, with one
    important extension: in addition to sampling the records, the algorithm also samples
    the variables.^([4](ch06.xhtml#idm46522842847944)) In traditional decision trees,
    to determine how to create a subpartition of a partition *A*, the algorithm makes
    the choice of variable and split point by minimizing a criterion such as Gini
    impurity (see [‚ÄúMeasuring Homogeneity or Impurity‚Äù](#Gini)). With random forests,
    at each stage of the algorithm, the choice of variable is limited to a *random
    subset of variables*. Compared to the basic tree algorithm (see [‚ÄúThe Recursive
    Partitioning Algorithm‚Äù](#RecursivePartitioning)), the random forest algorithm
    adds two more steps: the bagging discussed earlier (see [‚ÄúBagging and the Random
    Forest‚Äù](#Bagging)), and the bootstrap sampling of variables at each split:'
  prefs: []
  type: TYPE_NORMAL
- en: Take a bootstrap (with replacement) subsample from the *records*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For the first split, sample *p* < *P* *variables* at random without replacement.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For each of the sampled variables <math alttext="upper X Subscript j left-parenthesis
    1 right-parenthesis Baseline comma upper X Subscript j left-parenthesis 2 right-parenthesis
    Baseline comma ellipsis comma upper X Subscript j left-parenthesis p right-parenthesis
    Baseline"><mrow><msub><mi>X</mi> <mrow><mi>j</mi><mo>(</mo><mn>1</mn><mo>)</mo></mrow></msub>
    <mo>,</mo> <msub><mi>X</mi> <mrow><mi>j</mi><mo>(</mo><mn>2</mn><mo>)</mo></mrow></msub>
    <mo>,</mo> <mo>...</mo> <mo>,</mo> <msub><mi>X</mi> <mrow><mi>j</mi><mo>(</mo><mi>p</mi><mo>)</mo></mrow></msub></mrow></math>
    , apply the splitting algorithm:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For each value <math alttext="s Subscript j left-parenthesis k right-parenthesis"><msub><mi>s</mi>
    <mrow><mi>j</mi><mo>(</mo><mi>k</mi><mo>)</mo></mrow></msub></math> of <math alttext="upper
    X Subscript j left-parenthesis k right-parenthesis"><msub><mi>X</mi> <mrow><mi>j</mi><mo>(</mo><mi>k</mi><mo>)</mo></mrow></msub></math>
    :'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Split the records in partition *A*, with *X*[*j*(*k*)] < *s*[*j*(*k*)] as one
    partition and the remaining records where <math alttext="upper X Subscript j left-parenthesis
    k right-parenthesis Baseline greater-than-or-equal-to s Subscript j left-parenthesis
    k right-parenthesis"><mrow><msub><mi>X</mi> <mrow><mi>j</mi><mo>(</mo><mi>k</mi><mo>)</mo></mrow></msub>
    <mo>‚â•</mo> <msub><mi>s</mi> <mrow><mi>j</mi><mo>(</mo><mi>k</mi><mo>)</mo></mrow></msub></mrow></math>
    as another partition.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Measure the homogeneity of classes within each subpartition of *A*.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the value of <math alttext="s Subscript j left-parenthesis k right-parenthesis"><msub><mi>s</mi>
    <mrow><mi>j</mi><mo>(</mo><mi>k</mi><mo>)</mo></mrow></msub></math> that produces
    maximum within-partition homogeneity of class.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the variable <math alttext="upper X Subscript j left-parenthesis k right-parenthesis"><msub><mi>X</mi>
    <mrow><mi>j</mi><mo>(</mo><mi>k</mi><mo>)</mo></mrow></msub></math> and the split
    value <math alttext="s Subscript j left-parenthesis k right-parenthesis"><msub><mi>s</mi>
    <mrow><mi>j</mi><mo>(</mo><mi>k</mi><mo>)</mo></mrow></msub></math> that produces
    maximum within-partition homogeneity of class.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Proceed to the next split and repeat the previous steps, starting with step
    2.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Continue with additional splits, following the same procedure until the tree
    is grown.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Go back to step 1, take another bootstrap subsample, and start the process over
    again.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'How many variables to sample at each step? A rule of thumb is to choose <math
    alttext="StartRoot upper P EndRoot"><msqrt><mi>P</mi></msqrt></math> where *P*
    is the number of predictor variables. The package `randomForest` implements the
    random forest in *R*. The following applies this package to the loan data (see
    [‚ÄúK-Nearest Neighbors‚Äù](#KNN) for a description of the data):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'In *Python*, we use the method `sklearn.ensemble.RandomForestClassifier`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: By default, 500 trees are trained. Since there are only two variables in the
    predictor set, the algorithm randomly selects the variable on which to split at
    each stage (i.e., a bootstrap subsample of size 1).
  prefs: []
  type: TYPE_NORMAL
- en: 'The *out-of-bag* (*OOB*) estimate of error is the error rate for the trained
    models, applied to the data left out of the training set for that tree. Using
    the output from the model, the OOB error can be plotted versus the number of trees
    in the random forest in *R*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The `RandomForestClassifier` implementation has no easy way to get out-of-bag
    estimates as a function of number of trees in the random forest. We can train
    a sequence of classifiers with an increasing number of trees and keep track of
    the `oob_score_` values. This method is, however, not efficient:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is shown in [Figure¬†6-6](#RFAccuracy). The error rate rapidly decreases
    from over 0.44 before stabilizing around 0.385. The predicted values can be obtained
    from the `predict` function and plotted as follows in *R*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'In *Python*, we can create a similar plot as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '![The improvement in accuracy of the random forest with the addition of more
    trees.](Images/psd2_0606.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-6\. An example of the improvement in accuracy of the random forest
    with the addition of more trees
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The plot, shown in [Figure¬†6-7](#LoanRF), is quite revealing about the nature
    of the random forest.
  prefs: []
  type: TYPE_NORMAL
- en: 'The random forest method is a ‚Äúblack box‚Äù method. It produces more accurate
    predictions than a simple tree, but the simple tree‚Äôs intuitive decision rules
    are lost. The random forest predictions are also somewhat noisy: note that some
    borrowers with a very high score, indicating high creditworthiness, still end
    up with a prediction of default. This is a result of some unusual records in the
    data and demonstrates the danger of overfitting by the random forest (see [‚ÄúBias-Variance
    Trade-off‚Äù](#bvt_note)).'
  prefs: []
  type: TYPE_NORMAL
- en: '![The predicted outcomes from the random forest applied to the loan default
    data.](Images/psd2_0607.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-7\. The predicted outcomes from the random forest applied to the loan
    default data
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Variable Importance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The power of the random forest algorithm shows itself when you build predictive
    models for data with many features and records. It has the ability to automatically
    determine which predictors are important and discover complex relationships between
    predictors corresponding to interaction terms (see [‚ÄúInteractions and Main Effects‚Äù](ch04.xhtml#Interactions)).
    For example, fit a model to the loan default data with all columns included. The
    following shows this in *R*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'And in *Python*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The argument `importance=TRUE` requests that the `randomForest` store additional
    information about the importance of different variables. The function `varImpPlot`
    will plot the relative performance of the variables (relative to permuting that
    variable):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_statistical_machine_learning_CO2-1)'
  prefs: []
  type: TYPE_NORMAL
- en: mean decrease in accuracy
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_statistical_machine_learning_CO2-2)'
  prefs: []
  type: TYPE_NORMAL
- en: mean decrease in node impurity
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Python*, the `RandomForestClassifier` collects information about feature
    importance during training and makes it available with the field `feature_importances_`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The ‚ÄúGini decrease‚Äù is available as the `feature_importance_` property of the
    fitted classifier. Accuracy decrease, however, is not available out of the box
    for *Python*. We can calculate it (`scores`) using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is shown in [Figure¬†6-8](#LoanVarImp). A similar graph can be created
    with this *Python* code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'There are two ways to measure variable importance:'
  prefs: []
  type: TYPE_NORMAL
- en: By the decrease in accuracy of the model if the values of a variable are randomly
    permuted (`type=1`). Randomly permuting the values has the effect of removing
    all predictive power for that variable. The accuracy is computed from the out-of-bag
    data (so this measure is effectively a cross-validated estimate).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the mean decrease in the Gini impurity score (see [‚ÄúMeasuring Homogeneity
    or Impurity‚Äù](#Gini)) for all of the nodes that were split on a variable (`type=2`).
    This measures how much including that variable improves the purity of the nodes.
    This measure is based on the training set and is therefore less reliable than
    a measure calculated on out-of-bag data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![The importance of variables for the full model fit to the loan data.](Images/psd2_0608.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-8\. The importance of variables for the full model fit to the loan
    data
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The top and bottom panels of [Figure¬†6-8](#LoanVarImp) show variable importance
    according to the decrease in accuracy and in Gini impurity, respectively. The
    variables in both panels are ranked by the decrease in accuracy. The variable
    importance scores produced by these two measures are quite different.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since the accuracy decrease is a more reliable metric, why should we use the
    Gini impurity decrease measure? By default, `randomForest` computes only this
    Gini impurity: Gini impurity is a byproduct of the algorithm, whereas model accuracy
    by variable requires extra computations (randomly permuting the data and predicting
    this data). In cases where computational complexity is important, such as in a
    production setting where thousands of models are being fit, it may not be worth
    the extra computational effort. In addition, the Gini decrease sheds light on
    which variables the random forest is using to make its splitting rules (recall
    that this information, readily visible in a simple tree, is effectively lost in
    a random forest).'
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The random forest, as with many statistical machine learning algorithms, can
    be considered a black-box algorithm with knobs to adjust how the box works. These
    knobs are called *hyperparameters*, which are parameters that you need to set
    before fitting a model; they are not optimized as part of the training process.
    While traditional statistical models require choices (e.g., the choice of predictors
    to use in a regression model), the hyperparameters for random forest are more
    critical, especially to avoid overfitting. In particular, the two most important
    hyperparameters for the random forest are:'
  prefs: []
  type: TYPE_NORMAL
- en: '`nodesize`/`min_samples_leaf`'
  prefs: []
  type: TYPE_NORMAL
- en: The minimum size for terminal nodes (leaves in the tree). The default is 1 for
    classification and 5 for regression in *R*. The `scikit-learn` implementation
    in *Python* uses a default of 1 for both.
  prefs: []
  type: TYPE_NORMAL
- en: '`maxnodes`/`max_leaf_nodes`'
  prefs: []
  type: TYPE_NORMAL
- en: 'The maximum number of nodes in each decision tree. By default, there is no
    limit and the largest tree will be fit subject to the constraints of `nodesize`.
    Note that in *Python*, you specify the maximum number of terminal nodes. The two
    parameters are related:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="maxnodes equals 2 max reverse-solidus bar leaf reverse-solidus
    bar nodes negative 1" display="block"><mrow><mi>maxnodes</mi> <mo>=</mo> <mn>2</mn>
    <mi>max</mi> <mo>_</mo> <mi>leaf</mi> <mo>_</mo> <mi>nodes</mi> <mo>-</mo> <mn>1</mn></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: It may be tempting to ignore these parameters and simply go with the default
    values. However, using the defaults may lead to overfitting when you apply the
    random forest to noisy data. When you increase `nodesize`/`min_samples_leaf` or
    set `maxnodes`/`max_leaf_nodes`, the algorithm will fit smaller trees and is less
    likely to create spurious predictive rules. Cross-validation (see [‚ÄúCross-Validation‚Äù](ch04.xhtml#CrossValidation))
    can be used to test the effects of setting different values for hyperparameters.
  prefs: []
  type: TYPE_NORMAL
- en: Boosting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Ensemble models have become a standard tool for predictive modeling. *Boosting*
    is a general technique to create an ensemble of models. It was developed around
    the same time as *bagging* (see [‚ÄúBagging and the Random Forest‚Äù](#Bagging)).
    Like bagging, boosting is most commonly used with decision trees. Despite their
    similarities, boosting takes a very different approach‚Äîone that comes with many
    more bells and whistles. As a result, while bagging can be done with relatively
    little tuning, boosting requires much greater care in its application. If these
    two methods were cars, bagging could be considered a Honda Accord (reliable and
    steady), whereas boosting could be considered a Porsche (powerful but requires
    more care).
  prefs: []
  type: TYPE_NORMAL
- en: 'In linear regression models, the residuals are often examined to see if the
    fit can be improved (see [‚ÄúPartial Residual Plots and Nonlinearity‚Äù](ch04.xhtml#PartialResidualPlots)).
    Boosting takes this concept much further and fits a series of models, in which
    each successive model seeks to minimize the error of the previous model. Several
    variants of the algorithm are commonly used: *Adaboost*, *gradient boosting*,
    and *stochastic gradient boosting*. The latter, stochastic gradient boosting,
    is the most general and widely used. Indeed, with the right choice of parameters,
    the algorithm can emulate the random forest.'
  prefs: []
  type: TYPE_NORMAL
- en: The Boosting Algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are various boosting algorithms, and the basic idea behind all of them
    is essentially the same. The easiest to understand is Adaboost, which proceeds
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize *M*, the maximum number of models to be fit, and set the iteration
    counter <math alttext="m equals 1"><mrow><mi>m</mi> <mo>=</mo> <mn>1</mn></mrow></math>
    . Initialize the observation weights <math alttext="w Subscript i Baseline equals
    1 slash upper N"><mrow><msub><mi>w</mi> <mi>i</mi></msub> <mo>=</mo> <mn>1</mn>
    <mo>/</mo> <mi>N</mi></mrow></math> for <math alttext="i equals 1 comma 2 comma
    ellipsis comma upper N"><mrow><mi>i</mi> <mo>=</mo> <mn>1</mn> <mo>,</mo> <mn>2</mn>
    <mo>,</mo> <mo>...</mo> <mo>,</mo> <mi>N</mi></mrow></math> . Initialize the ensemble
    model <math alttext="ModifyingAbove upper F With caret Subscript 0 Baseline equals
    0"><mrow><msub><mover accent="true"><mi>F</mi> <mo>^</mo></mover> <mn>0</mn></msub>
    <mo>=</mo> <mn>0</mn></mrow></math> .
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using the observation weights <math alttext="w 1 comma w 2 comma ellipsis comma
    w Subscript upper N Baseline"><mrow><msub><mi>w</mi> <mn>1</mn></msub> <mo>,</mo>
    <msub><mi>w</mi> <mn>2</mn></msub> <mo>,</mo> <mo>...</mo> <mo>,</mo> <msub><mi>w</mi>
    <mi>N</mi></msub></mrow></math> , train a model <math alttext="ModifyingAbove
    f With caret Subscript m"><msub><mover accent="true"><mi>f</mi> <mo>^</mo></mover>
    <mi>m</mi></msub></math> that minimizes the weighted error <math alttext="e Subscript
    m"><msub><mi>e</mi> <mi>m</mi></msub></math> defined by summing the weights for
    the misclassified observations.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Add the model to the ensemble: <math alttext="ModifyingAbove upper F With caret
    Subscript m Baseline equals ModifyingAbove upper F With caret Subscript m minus
    1 Baseline plus alpha Subscript m Baseline ModifyingAbove f With caret Subscript
    m"><mrow><msub><mover accent="true"><mi>F</mi> <mo>^</mo></mover> <mi>m</mi></msub>
    <mo>=</mo> <msub><mover accent="true"><mi>F</mi> <mo>^</mo></mover> <mrow><mi>m</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mo>+</mo> <msub><mi>Œ±</mi> <mi>m</mi></msub> <msub><mover accent="true"><mi>f</mi>
    <mo>^</mo></mover> <mi>m</mi></msub></mrow></math> where <math alttext="alpha
    Subscript m Baseline equals StartFraction log 1 minus e Subscript m Baseline Over
    e Subscript m Baseline EndFraction"><mrow><msub><mi>Œ±</mi> <mi>m</mi></msub> <mo>=</mo>
    <mfrac><mrow><mo form="prefix">log</mo><mn>1</mn><mo>-</mo><msub><mi>e</mi> <mi>m</mi></msub></mrow>
    <msub><mi>e</mi> <mi>m</mi></msub></mfrac></mrow></math> .'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the weights <math alttext="w 1 comma w 2 comma ellipsis comma w Subscript
    upper N Baseline"><mrow><msub><mi>w</mi> <mn>1</mn></msub> <mo>,</mo> <msub><mi>w</mi>
    <mn>2</mn></msub> <mo>,</mo> <mo>...</mo> <mo>,</mo> <msub><mi>w</mi> <mi>N</mi></msub></mrow></math>
    so that the weights are increased for the observations that were misclassified.
    The size of the increase depends on <math alttext="alpha Subscript m"><msub><mi>Œ±</mi>
    <mi>m</mi></msub></math> , with larger values of <math alttext="alpha Subscript
    m"><msub><mi>Œ±</mi> <mi>m</mi></msub></math> leading to bigger weights.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Increment the model counter <math alttext="m equals m plus 1"><mrow><mi>m</mi>
    <mo>=</mo> <mi>m</mi> <mo>+</mo> <mn>1</mn></mrow></math> . If <math alttext="m
    less-than-or-equal-to upper M"><mrow><mi>m</mi> <mo>‚â§</mo> <mi>M</mi></mrow></math>
    , go to step 2.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The boosted estimate is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="ModifyingAbove upper F With caret equals alpha 1 ModifyingAbove
    f With caret Subscript 1 Baseline plus alpha 2 ModifyingAbove f With caret Subscript
    2 Baseline plus ellipsis plus alpha Subscript upper M Baseline ModifyingAbove
    f With caret Subscript upper M" display="block"><mrow><mover accent="true"><mi>F</mi>
    <mo>^</mo></mover> <mo>=</mo> <msub><mi>Œ±</mi> <mn>1</mn></msub> <msub><mover
    accent="true"><mi>f</mi> <mo>^</mo></mover> <mn>1</mn></msub> <mo>+</mo> <msub><mi>Œ±</mi>
    <mn>2</mn></msub> <msub><mover accent="true"><mi>f</mi> <mo>^</mo></mover> <mn>2</mn></msub>
    <mo>+</mo> <mo>‚ãØ</mo> <mo>+</mo> <msub><mi>Œ±</mi> <mi>M</mi></msub> <msub><mover
    accent="true"><mi>f</mi> <mo>^</mo></mover> <mi>M</mi></msub></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: By increasing the weights for the observations that were misclassified, the
    algorithm forces the models to train more heavily on the data for which it performed
    poorly. The factor <math alttext="alpha Subscript m"><msub><mi>Œ±</mi> <mi>m</mi></msub></math>
    ensures that models with lower error have a bigger weight.
  prefs: []
  type: TYPE_NORMAL
- en: Gradient boosting is similar to Adaboost but casts the problem as an optimization
    of a cost function. Instead of adjusting weights, gradient boosting fits models
    to a *pseudo-residual*, which has the effect of training more heavily on the larger
    residuals. In the spirit of the random forest, stochastic gradient boosting adds
    randomness to the algorithm by sampling observations and predictor variables at
    each stage.
  prefs: []
  type: TYPE_NORMAL
- en: XGBoost
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The most widely used public domain software for boosting is XGBoost, an implementation
    of stochastic gradient boosting originally developed by Tianqi Chen and Carlos
    Guestrin at the University of Washington. A computationally efficient implementation
    with many options, it is available as a package for most major data science software
    languages. In *R*, XGBoost is available as [the package `xgboost`](https://xgboost.readthedocs.io)
    and with the same name also for *Python*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The method `xgboost` has many parameters that can, and should, be adjusted
    (see [‚ÄúHyperparameters and Cross-Validation‚Äù](#HyperparametersCV)). Two very important
    parameters are `subsample`, which controls the fraction of observations that should
    be sampled at each iteration, and `eta`, a shrinkage factor applied to <math alttext="alpha
    Subscript m"><msub><mi>Œ±</mi> <mi>m</mi></msub></math> in the boosting algorithm
    (see [‚ÄúThe Boosting Algorithm‚Äù](#BoostingAlgorithm)). Using `subsample` makes
    boosting act like the random forest except that the sampling is done without replacement.
    The shrinkage parameter `eta` is helpful to prevent overfitting by reducing the
    change in the weights (a smaller change in the weights means the algorithm is
    less likely to overfit to the training set). The following applies `xgboost` in
    *R* to the loan data with just two predictor variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Note that `xgboost` does not support the formula syntax, so the predictors need
    to be converted to a `data.matrix` and the response needs to be converted to 0/1
    variables. The `objective` argument tells `xgboost` what type of problem this
    is; based on this, `xgboost` will choose a metric to optimize.
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Python*, `xgboost` has two different interfaces: a `scikit-learn` API and
    a more functional interface like in *R*. To be consistent with other `scikit-learn`
    methods, some parameters were renamed. For example, `eta` is renamed to `learning_rate`;
    using `eta` will not fail, but it will not have the desired effect:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The predicted values can be obtained from the `predict` function in *R* and,
    since there are only two variables, plotted versus the predictors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The same figure can be created in *Python* using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: The result is shown in [Figure¬†6-9](#LoanXGB). Qualitatively, this is similar
    to the predictions from the random forest; see [Figure¬†6-7](#LoanRF). The predictions
    are somewhat noisy in that some borrowers with a very high borrower score still
    end up with a prediction of default.
  prefs: []
  type: TYPE_NORMAL
- en: '![The predicted outcomes from XGBoost applied to the loan default data.](Images/psd2_0609.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-9\. The predicted outcomes from XGBoost applied to the loan default
    data
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Regularization: Avoiding Overfitting'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Blind application of `xgboost` can lead to unstable models as a result of *overfitting*
    to the training data. The problem with overfitting is twofold:'
  prefs: []
  type: TYPE_NORMAL
- en: The accuracy of the model on new data not in the training set will be degraded.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The predictions from the model are highly variable, leading to unstable results.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Any modeling technique is potentially prone to overfitting. For example, if
    too many variables are included in a regression equation, the model may end up
    with spurious predictions. However, for most statistical techniques, overfitting
    can be avoided by a judicious selection of predictor variables. Even the random
    forest generally produces a reasonable model without tuning the parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'This, however, is not the case for `xgboost`. Fit `xgboost` to the loan data
    for a training set with all of the variables included in the model. In *R*, you
    can do this as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'We use the function `train_test_split` in *Python* to split the data set into
    training and test sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The test set consists of 10,000 randomly sampled records from the full data,
    and the training set consists of the remaining records. Boosting leads to an error
    rate of only 13.3% for the training set. The test set, however, has a much higher
    error rate of 35.3%. This is a result of overfitting: while boosting can explain
    the variability in the training set very well, the prediction rules do not apply
    to new data.'
  prefs: []
  type: TYPE_NORMAL
- en: Boosting provides several parameters to avoid overfitting, including the parameters
    `eta` (or `learning_rate`) and `subsample` (see [‚ÄúXGBoost‚Äù](#XGBoost)). Another
    approach is *regularization*, a technique that modifies the cost function in order
    to *penalize* the complexity of the model. Decision trees are fit by minimizing
    cost criteria such as Gini‚Äôs impurity score (see [‚ÄúMeasuring Homogeneity or Impurity‚Äù](#Gini)).
    In `xgboost`, it is possible to modify the cost function by adding a term that
    measures the complexity of the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two parameters in `xgboost` to regularize the model: `alpha` and
    `lambda`, which correspond to Manhattan distance (L1-regularization) and squared
    Euclidean distance (L2-regularization), respectively (see [‚ÄúDistance Metrics‚Äù](#DistanceMetrics)).
    Increasing these parameters will penalize more complex models and reduce the size
    of the trees that are fit. For example, see what happens if we set `lambda` to
    1,000 in *R*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'In the `scikit-learn` API, the parameters are called `reg_alpha` and `reg_lambda`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Now the training error is only slightly lower than the error on the test set.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `predict` method in *R* offers a convenient argument, `ntreelimit`, that
    forces only the first *i* trees to be used in the prediction. This lets us directly
    compare the in-sample versus out-of-sample error rates as more models are included:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'In *Python*, we can call the `predict_proba` method with the `ntree_limit`
    argument:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The output from the model returns the error for the training set in the component
    `xgb_default$evaluation_log`. By combining this with the out-of-sample errors,
    we can plot the errors versus the number of iterations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'We can use the `pandas` plot method to create the line graph. The axis returned
    from the first plot allows us to overlay additional lines onto the same graph.
    This is a pattern that many of *Python*‚Äôs graph packages support:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: The result, displayed in [Figure¬†6-10](#XGBoostError), shows how the default
    model steadily improves the accuracy for the training set but actually gets worse
    for the test set. The penalized model does not exhibit this behavior.
  prefs: []
  type: TYPE_NORMAL
- en: '![The error rate of the default XGBoost versus a penalized version of XGBoost.](Images/psd2_0610.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-10\. The error rate of the default XGBoost versus a penalized version
    of XGBoost
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Hyperparameters and Cross-Validation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`xgboost` has a daunting array of hyperparameters; see [‚ÄúXGBoost Hyperparameters‚Äù](#BoostingParameters)
    for a discussion. As seen in [‚ÄúRegularization: Avoiding Overfitting‚Äù](#Regularization),
    the specific choice can dramatically change the model fit. Given a huge combination
    of hyperparameters to choose from, how should we be guided in our choice? A standard
    solution to this problem is to use *cross-validation*; see [‚ÄúCross-Validation‚Äù](ch04.xhtml#CrossValidation).
    Cross-validation randomly splits up the data into *K* different groups, also called
    *folds*. For each fold, a model is trained on the data not in the fold and then
    evaluated on the data in the fold. This yields a measure of accuracy of the model
    on out-of-sample data. The best set of hyperparameters is the one given by the
    model with the lowest overall error as computed by averaging the errors from each
    of the folds.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To illustrate the technique, we apply it to parameter selection for `xgboost`.
    In this example, we explore two parameters: the shrinkage parameter `eta` (`learning_rate`‚Äîsee
    [‚ÄúXGBoost‚Äù](#XGBoost)) and the maximum depth of trees `max_depth`. The parameter
    `max_depth` is the maximum depth of a leaf node to the root of the tree with a
    default value of six. This gives us another way to control overfitting: deep trees
    tend to be more complex and may overfit the data. First we set up the folds and
    parameter list. In *R*, this is done as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we apply the preceding algorithm to compute the error for each model and
    each fold using five folds:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'In the following *Python* code, we create all possible combinations of hyperparameters
    and fit and evaluate models with each combination:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_statistical_machine_learning_CO3-1)'
  prefs: []
  type: TYPE_NORMAL
- en: We use the function `itertools.product` from the *Python* standard library to
    create all possible combinations of the two hyperparameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we are fitting 45 total models, this can take a while. The errors are
    stored as a matrix with the models along the rows and folds along the columns.
    Using the function `rowMeans`, we can compare the error rate for the different
    parameter sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Cross-validation suggests that using shallower trees with a smaller value of
    `eta`/`learning_rate` yields more accurate results. Since these models are also
    more stable, the best parameters to use are `eta=0.1` and `max_depth=3` (or possibly
    `max_depth=6`).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter has described two classification and prediction methods that ‚Äúlearn‚Äù
    flexibly and locally from data, rather than starting with a structural model (e.g.,
    a linear regression) that is fit to the entire data set. *K*-Nearest Neighbors
    is a simple process that looks around at similar records and assigns their majority
    class (or average value) to the record being predicted. Trying various cutoff
    (split) values of predictor variables, tree models iteratively divide the data
    into sections and subsections that are increasingly homogeneous with respect to
    class. The most effective split values form a path, and also a ‚Äúrule,‚Äù to a classification
    or prediction. Tree models are a very powerful and popular predictive tool, often
    outperforming other methods. They have given rise to various ensemble methods
    (random forests, boosting, bagging) that sharpen the predictive power of trees.
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch06.xhtml#idm46522845224392-marker)) This and subsequent sections in
    this chapter ¬© 2020 Datastats, LLC, Peter Bruce, Andrew Bruce, and Peter Gedeck;
    used with permission.
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch06.xhtml#idm46522845155448-marker)) For this example, we take the first
    row in the `loan200` data set as the `newloan` and exclude it from the data set
    for training.
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](ch06.xhtml#idm46522843869576-marker)) The term CART is a registered trademark
    of Salford Systems related to their specific implementation of tree models.
  prefs: []
  type: TYPE_NORMAL
- en: ^([4](ch06.xhtml#idm46522842847944-marker)) The term *random forest* is a trademark
    of Leo Breiman and Adele Cutler and licensed to Salford Systems. There is no standard
    nontrademark name, and the term random forest is as synonymous with the algorithm
    as Kleenex is with facial tissues.
  prefs: []
  type: TYPE_NORMAL
