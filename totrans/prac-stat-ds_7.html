<html><head></head><body><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 7. Unsupervised Learning"><div class="chapter" id="idm46522845254376">
<h1><span class="label">Chapter 7. </span>Unsupervised Learning</h1>


<p>The term <em>unsupervised learning</em> refers to <a data-type="indexterm" data-primary="unsupervised learning" id="ix_unslrn"/>statistical methods that extract meaning from data without training a model on labeled data (data where an outcome of interest is known). In Chapters <a data-type="xref" data-xrefstyle="select:labelnumber" href="ch04.xhtml#Regression">4</a> to <a data-type="xref" data-xrefstyle="select:labelnumber" href="ch06.xhtml#StatisticalML">6</a>, the goal is to build a model (set of rules) to predict
a response variable from a set of predictor variables. This is supervised learning. In contrast, unsupervised learning also constructs a model of the data, but it does not distinguish between a response variable and predictor variables.<a data-type="indexterm" data-primary="statistical machine learning" data-secondary="unsupervised learning" data-seealso="unsupervised learning" id="idm46522838516344"/></p>

<p>Unsupervised learning can be used to achieve different goals.<a data-type="indexterm" data-primary="unsupervised learning" data-secondary="goals achieved by" id="idm46522838514744"/>
In some cases, it can be used to create a predictive rule in the absence of a labeled response.<a data-type="indexterm" data-primary="clustering" id="idm46522838513528"/>
<em>Clustering</em> methods can be used to identify meaningful groups of data.
For example, using the web clicks and demographic data of a user on a website, we may be able to group together different types of users.
The website could then be personalized to these different types.</p>

<p>In other cases, the goal may be to <em>reduce the dimension</em> of the data to a more manageable set of variables.<a data-type="indexterm" data-primary="reducing the dimension of the data" id="idm46522838511208"/>
This reduced set could then be used as input into a predictive model, such as regression or classification.
For example, we may have thousands of sensors to monitor an  industrial process.
By reducing the data to a smaller set of features, we may be able to build a more powerful and interpretable model to predict process failure than could be built by including data streams from thousands of <span class="keep-together">sensors</span>.</p>

<p>Finally, unsupervised learning can be viewed as an extension of the exploratory data analysis (see <a data-type="xref" href="ch01.xhtml#EDA">Chapter 1</a>) to situations in which you are confronted with a large number of variables and records.<a data-type="indexterm" data-primary="exploratory data analysis" data-secondary="unsupervised learning as extension of" id="idm46522838507736"/>
The aim is to gain insight into a set of data and how the different variables relate to each other.
Unsupervised techniques allow you to sift through and analyze these variables and discover relationships.</p>
<div data-type="note" epub:type="note"><h1>Unsupervised Learning and Prediction</h1>
<p>Unsupervised learning  can play an important role in prediction, both for regression and classification problems.<a data-type="indexterm" data-primary="prediction" data-secondary="unsupervised learning and" id="idm46522838505048"/><a data-type="indexterm" data-primary="unsupervised learning" data-secondary="and prediction" data-secondary-sortas="prediction" id="idm46522838504008"/>
In some cases, we want to predict a category in the absence of any labeled data.
For example, we might want to predict the type of vegetation in an area from a set of satellite sensory data.
Since we don’t have a response variable to train a model,
clustering gives us a way to identify common patterns and  categorize the regions.<a data-type="indexterm" data-primary="clustering" data-secondary="uses of" id="idm46522838502312"/></p>

<p>Clustering is an especially important tool for the “cold-start problem.”
In this type of problem, such as launching a new marketing campaign or identifying potential new types of fraud or spam, we initially may not have any response to train a model.<a data-type="indexterm" data-primary="cold-start problems, using clustering for" id="idm46522838500712"/>
Over time, as data is collected, we can learn more about the system and build a traditional predictive model.
But clustering helps us start the learning process more quickly by identifying population segments.</p>

<p>Unsupervised learning is also important as a building block for regression and classification techniques.<a data-type="indexterm" data-primary="regression" data-secondary="unsupervised learning as building block" id="idm46522838499048"/><a data-type="indexterm" data-primary="classification" data-secondary="unsupervised learning as building block" id="idm46522838498056"/>
With big data, if a small subpopulation is not well represented in the overall population, the trained model may not perform well for that subpopulation.
With clustering, it is possible to identify and label subpopulations.
Separate models can then be fit to the different subpopulations.  Alternatively, the subpopulation can be represented with its own feature, forcing the overall model to explicitly consider subpopulation identity as a predictor.</p>
</div>






<section data-type="sect1" data-pdf-bookmark="Principal Components Analysis"><div class="sect1" id="PCA">
<h1>Principal Components Analysis</h1>

<p>Often, variables will vary together (covary), and some of the variation in one is actually duplicated by variation in another (e.g., restaurant checks and tips).<a data-type="indexterm" data-primary="unsupervised learning" data-secondary="principal components analysis" id="ix_unslrnPCA"/><a data-type="indexterm" data-primary="principal components analysis" id="ix_PCA"/><a data-type="indexterm" data-primary="PCA" data-see="principal components analysis" id="idm46522838491864"/>
Principal components analysis (PCA) is a technique to discover the way in which numeric variables covary.<sup><a data-type="noteref" id="idm46522838490648-marker" href="ch07.xhtml#idm46522838490648">1</a></sup></p>
<aside data-type="sidebar" epub:type="sidebar" class="pagebreak-before less_space"><div class="sidebar" id="idm46522838489848">
<h5>Key Terms for Principal Components Analysis</h5><dl>
<dt class="horizontal"><strong><em>Principal component</em></strong></dt>
<dd>
<p>A linear combination of the predictor variables.</p>
</dd>
<dt class="horizontal"><strong><em>Loadings</em></strong></dt>
<dd>
<p>The weights that transform the predictors into the components.</p>
<dl>
<dt><em>Synonym</em></dt>
<dd>
<p>Weights<a data-type="indexterm" data-primary="loadings" id="idm46522838483144"/></p>
</dd>
</dl>
</dd>
<dt class="horizontal"><strong><em>Screeplot</em></strong></dt>
<dd>
<p>A plot of the variances of the components, showing the relative importance of the components, either as explained variance or as proportion of explained variance.<a data-type="indexterm" data-primary="screeplots" id="idm46522838480168"/></p>
</dd>
</dl>
</div></aside>

<p>The idea in PCA is to combine multiple numeric predictor variables into a smaller set of variables, which are weighted linear combinations of the original set.<a data-type="indexterm" data-primary="principal components" id="idm46522838478648"/>
The smaller set of variables, the <em>principal components</em>, “explains” most of the variability of the full set of variables, reducing the dimension of the data.
The weights used to form the principal components reveal the relative contributions of the original variables to the new principal components.<a data-type="indexterm" data-primary="weights for principal components" data-see="loadings" id="idm46522838477160"/></p>

<p>PCA was<a data-type="indexterm" data-primary="Pearson, Karl" id="idm46522838475736"/> first <a href="https://oreil.ly/o4EeC">proposed by Karl Pearson</a>.
In what was perhaps the first paper on unsupervised learning,
Pearson recognized that in many problems there is variability in the predictor variables, so he developed PCA as a technique to model this variability.<a data-type="indexterm" data-primary="linear discriminant analysis (LDA)" data-secondary="principal components analysis as unsupervised version" id="idm46522838473960"/>
PCA can be viewed as the unsupervised version of linear discriminant analysis; see<a data-type="xref" href="ch05.xhtml#DiscriminantAnalysis">“Discriminant Analysis”</a>.</p>








<section data-type="sect2" data-pdf-bookmark="A Simple Example"><div class="sect2" id="idm46522838471800">
<h2>A Simple Example</h2>

<p>For two variables, <math alttext="upper X 1">
  <msub><mi>X</mi> <mn>1</mn> </msub>
</math> and <math alttext="upper X 2">
  <msub><mi>X</mi> <mn>2</mn> </msub>
</math>,
there are two<a data-type="indexterm" data-primary="unsupervised learning" data-secondary="principal components analysis" data-tertiary="simple example" id="ix_unslrnPCAex"/><a data-type="indexterm" data-primary="principal components analysis" data-secondary="simple example" id="ix_PCAex"/> principal components <math alttext="upper Z Subscript i">
  <msub><mi>Z</mi> <mi>i</mi> </msub>
</math> (<math alttext="i equals 1">
  <mrow>
    <mi>i</mi>
    <mo>=</mo>
    <mn>1</mn>
  </mrow>
</math> or 2):</p>
<div data-type="equation">
<math display="block">
  <mrow>
    <msub><mi>Z</mi> <mi>i</mi> </msub>
    <mo>=</mo>
    <msub><mi>w</mi> <mrow><mi>i</mi><mo>,</mo><mn>1</mn></mrow> </msub>
    <msub><mi>X</mi> <mn>1</mn> </msub>
    <mo>+</mo>
    <msub><mi>w</mi> <mrow><mi>i</mi><mo>,</mo><mn>2</mn></mrow> </msub>
    <msub><mi>X</mi> <mn>2</mn> </msub>
  </mrow>
</math>
</div>

<p>The weights <math alttext="left-parenthesis w Subscript i comma 1 Baseline comma w Subscript i comma 2 Baseline right-parenthesis">
  <mrow>
    <mo>(</mo>
    <msub><mi>w</mi> <mrow><mi>i</mi><mo>,</mo><mn>1</mn></mrow> </msub>
    <mo>,</mo>
    <msub><mi>w</mi> <mrow><mi>i</mi><mo>,</mo><mn>2</mn></mrow> </msub>
    <mo>)</mo>
  </mrow>
</math> are known as the component <em>loadings</em>.<a data-type="indexterm" data-primary="loadings" id="idm46522838440312"/>
These transform the original variables into the principal components.
The first principal component, <math alttext="upper Z 1">
  <msub><mi>Z</mi> <mn>1</mn> </msub>
</math>, is the linear combination that best explains the total variation.
The second principal component, <math alttext="upper Z 2">
  <msub><mi>Z</mi> <mn>2</mn> </msub>
</math>, is orthogonal to the first and explains as much of the remaining variation as it can.  (If there were additional components, each additional one would be orthogonal to the others.)</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>It is also common to compute principal components on deviations from the means of the predictor variables, rather than on the values themselves.</p>
</div>

<p>You can compute principal components in <em>R</em> using the <code>princomp</code> function.
The following performs a PCA on the stock price returns for Chevron (CVX) and ExxonMobil (XOM):</p>

<pre data-type="programlisting" data-code-language="r"><code class="n">oil_px</code> <code class="o">&lt;-</code> <code class="n">sp500_px</code><code class="p">[,</code> <code class="nf">c</code><code class="p">(</code><code class="s">'CVX'</code><code class="p">,</code> <code class="s">'XOM'</code><code class="p">)]</code>
<code class="n">pca</code> <code class="o">&lt;-</code> <code class="nf">princomp</code><code class="p">(</code><code class="n">oil_px</code><code class="p">)</code>
<code class="n">pca</code><code class="o">$</code><code class="n">loadings</code>

<code class="n">Loadings</code><code class="o">:</code>
    <code class="n">Comp.1</code> <code class="n">Comp.2</code>
<code class="n">CVX</code> <code class="m">-0.747</code>  <code class="m">0.665</code>
<code class="n">XOM</code> <code class="m">-0.665</code> <code class="m">-0.747</code>

               <code class="n">Comp.1</code> <code class="n">Comp.2</code>
<code class="n">SS</code> <code class="n">loadings</code>       <code class="m">1.0</code>    <code class="m">1.0</code>
<code class="n">Proportion</code> <code class="n">Var</code>    <code class="m">0.5</code>    <code class="m">0.5</code>
<code class="n">Cumulative</code> <code class="n">Var</code>    <code class="m">0.5</code>    <code class="m">1.0</code></pre>

<p>In <em>Python</em>, we can use the <code>scikit-learn</code> implementation <code>sklearn.decomposition.PCA</code>:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">pcs</code> <code class="o">=</code> <code class="n">PCA</code><code class="p">(</code><code class="n">n_components</code><code class="o">=</code><code class="mi">2</code><code class="p">)</code>
<code class="n">pcs</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">oil_px</code><code class="p">)</code>
<code class="n">loadings</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">pcs</code><code class="o">.</code><code class="n">components_</code><code class="p">,</code> <code class="n">columns</code><code class="o">=</code><code class="n">oil_px</code><code class="o">.</code><code class="n">columns</code><code class="p">)</code>
<code class="n">loadings</code></pre>

<p>The weights for CVX and XOM for the first principal component are –0.747 and <span class="keep-together">–0.665</span>, and for the second principal component they are 0.665 and –0.747.
How to interpret this?
The first principal component is essentially an average of CVX and XOM, reflecting the correlation between the two energy companies.
The second principal component measures when the stock prices of CVX and XOM diverge.</p>

<p>It is instructive to plot the principal components with the data. Here we create a visualization in <em>R</em>:</p>

<pre data-type="programlisting" data-code-language="r"><code class="n">loadings</code> <code class="o">&lt;-</code> <code class="n">pca</code><code class="o">$</code><code class="n">loadings</code>
<code class="nf">ggplot</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="n">oil_px</code><code class="p">,</code> <code class="nf">aes</code><code class="p">(</code><code class="n">x</code><code class="o">=</code><code class="n">CVX</code><code class="p">,</code> <code class="n">y</code><code class="o">=</code><code class="n">XOM</code><code class="p">))</code> <code class="o">+</code>
  <code class="nf">geom_point</code><code class="p">(</code><code class="n">alpha</code><code class="o">=</code><code class="m">.3</code><code class="p">)</code> <code class="o">+</code>
  <code class="nf">stat_ellipse</code><code class="p">(</code><code class="n">type</code><code class="o">=</code><code class="s">'norm'</code><code class="p">,</code> <code class="n">level</code><code class="o">=</code><code class="m">.99</code><code class="p">)</code> <code class="o">+</code>
  <code class="nf">geom_abline</code><code class="p">(</code><code class="n">intercept</code> <code class="o">=</code> <code class="m">0</code><code class="p">,</code> <code class="n">slope</code> <code class="o">=</code> <code class="n">loadings</code><code class="p">[</code><code class="m">2</code><code class="p">,</code><code class="m">1</code><code class="p">]</code><code class="o">/</code><code class="n">loadings</code><code class="p">[</code><code class="m">1</code><code class="p">,</code><code class="m">1</code><code class="p">])</code> <code class="o">+</code>
  <code class="nf">geom_abline</code><code class="p">(</code><code class="n">intercept</code> <code class="o">=</code> <code class="m">0</code><code class="p">,</code> <code class="n">slope</code> <code class="o">=</code> <code class="n">loadings</code><code class="p">[</code><code class="m">2</code><code class="p">,</code><code class="m">2</code><code class="p">]</code><code class="o">/</code><code class="n">loadings</code><code class="p">[</code><code class="m">1</code><code class="p">,</code><code class="m">2</code><code class="p">])</code></pre>

<p class="pagebreak-before">The following code creates a similar visualization in <em>Python</em>:</p>

<pre data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">abline</code><code class="p">(</code><code class="n">slope</code><code class="p">,</code> <code class="n">intercept</code><code class="p">,</code> <code class="n">ax</code><code class="p">):</code>
    <code class="sd">"""Calculate coordinates of a line based on slope and intercept"""</code>
    <code class="n">x_vals</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">(</code><code class="n">ax</code><code class="o">.</code><code class="n">get_xlim</code><code class="p">())</code>
    <code class="k">return</code> <code class="p">(</code><code class="n">x_vals</code><code class="p">,</code> <code class="n">intercept</code> <code class="o">+</code> <code class="n">slope</code> <code class="o">*</code> <code class="n">x_vals</code><code class="p">)</code>

<code class="n">ax</code> <code class="o">=</code> <code class="n">oil_px</code><code class="o">.</code><code class="n">plot</code><code class="o">.</code><code class="n">scatter</code><code class="p">(</code><code class="n">x</code><code class="o">=</code><code class="s1">'XOM'</code><code class="p">,</code> <code class="n">y</code><code class="o">=</code><code class="s1">'CVX'</code><code class="p">,</code> <code class="n">alpha</code><code class="o">=</code><code class="mf">0.3</code><code class="p">,</code> <code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">4</code><code class="p">,</code> <code class="mi">4</code><code class="p">))</code>
<code class="n">ax</code><code class="o">.</code><code class="n">set_xlim</code><code class="p">(</code><code class="o">-</code><code class="mi">3</code><code class="p">,</code> <code class="mi">3</code><code class="p">)</code>
<code class="n">ax</code><code class="o">.</code><code class="n">set_ylim</code><code class="p">(</code><code class="o">-</code><code class="mi">3</code><code class="p">,</code> <code class="mi">3</code><code class="p">)</code>
<code class="n">ax</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="o">*</code><code class="n">abline</code><code class="p">(</code><code class="n">loadings</code><code class="o">.</code><code class="n">loc</code><code class="p">[</code><code class="mi">0</code><code class="p">,</code> <code class="s1">'CVX'</code><code class="p">]</code> <code class="o">/</code> <code class="n">loadings</code><code class="o">.</code><code class="n">loc</code><code class="p">[</code><code class="mi">0</code><code class="p">,</code> <code class="s1">'XOM'</code><code class="p">],</code> <code class="mi">0</code><code class="p">,</code> <code class="n">ax</code><code class="p">),</code>
        <code class="s1">'--'</code><code class="p">,</code> <code class="n">color</code><code class="o">=</code><code class="s1">'C1'</code><code class="p">)</code>
<code class="n">ax</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="o">*</code><code class="n">abline</code><code class="p">(</code><code class="n">loadings</code><code class="o">.</code><code class="n">loc</code><code class="p">[</code><code class="mi">1</code><code class="p">,</code> <code class="s1">'CVX'</code><code class="p">]</code> <code class="o">/</code> <code class="n">loadings</code><code class="o">.</code><code class="n">loc</code><code class="p">[</code><code class="mi">1</code><code class="p">,</code> <code class="s1">'XOM'</code><code class="p">],</code> <code class="mi">0</code><code class="p">,</code> <code class="n">ax</code><code class="p">),</code>
        <code class="s1">'--'</code><code class="p">,</code> <code class="n">color</code><code class="o">=</code><code class="s1">'C1'</code><code class="p">)</code></pre>

<p>The result is shown in <a data-type="xref" href="#StockPCA">Figure 7-1</a>.</p>

<figure><div id="StockPCA" class="figure">
<img src="Images/psd2_0701.png" alt="The principal components for the stock returns for Chevron and ExxonMobil" width="1164" height="1160"/>
<h6><span class="label">Figure 7-1. </span>The principal components for the stock returns for Chevron (CVX) and ExxonMobil (XOM)</h6>
</div></figure>

<p>The dashed lines show the direction of the two principal components: the first one is along the long axis of the ellipse, and the second one is along the short axis.
You can see that a majority of the variability in the two stock returns is explained by the first principal component.
This makes sense since energy stock prices tend to move as a group.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>The weights for the first principal component are both negative, but reversing the sign of all the weights does not change the principal component.<a data-type="indexterm" data-primary="loadings" data-secondary="with negative signs" id="idm46522837808328"/>
For example, using weights of 0.747 and 0.665 for the first principal component is equivalent to the negative weights, just as an infinite line defined by the origin and 1,1 is the same as one defined by the origin and –1, –1.<a data-type="indexterm" data-primary="unsupervised learning" data-secondary="principal components analysis" data-tertiary="simple example" data-startref="ix_unslrnPCAex" id="idm46522837806984"/><a data-type="indexterm" data-primary="principal components analysis" data-secondary="simple example" data-startref="ix_PCAex" id="idm46522837805528"/></p>
</div>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Computing the Principal Components"><div class="sect2" id="AlgorithmPCA">
<h2>Computing the Principal Components</h2>

<p>Going from two variables to more variables is straightforward.<a data-type="indexterm" data-primary="principal components analysis" data-secondary="computing principal components" id="idm46522837802680"/><a data-type="indexterm" data-primary="unsupervised learning" data-secondary="principal components analysis" data-tertiary="computing principal components" id="idm46522837801672"/>  For the first component, simply include the additional predictor variables in the linear combination, assigning weights that optimize the collection of <a data-type="indexterm" data-primary="covariance" id="idm46522837800120"/>the covariation from all the predictor variables into this first principal component (<em>covariance</em> is the statistical term; see <a data-type="xref" href="ch05.xhtml#Covariance">“Covariance Matrix”</a>).
Calculation of principal components is a classic statistical method, relying on either the correlation matrix of the data or the covariance matrix, and it executes rapidly, not relying on iteration. As noted earlier, principal components analysis works only with numeric variables, not categorical ones. The full process can be described as follows:</p>
<ol>
<li>
<p>In creating the first principal component, PCA arrives at the linear combination of predictor variables that maximizes the percent of total variance explained.</p>
</li>
<li>
<p>This linear combination then becomes the first “new” predictor, <em>Z</em><sub>1</sub>.</p>
</li>
<li>
<p>PCA repeats this process, using the same variables with different weights, to create a second new predictor, <em>Z</em><sub>2</sub>. The weighting is done such that <em>Z</em><sub>1</sub> and <em>Z</em><sub>2</sub> are uncorrelated.</p>
</li>
<li>
<p>The process continues until you have as many new variables, or components,  <em>Z</em><sub>i</sub> as original variables  <em>X</em><sub>i</sub>.</p>
</li>
<li>
<p>Choose to retain as many components as are needed to account for most of the variance.</p>
</li>
<li>
<p>The result so far is a set of weights for each component. The final step is to convert the original data into new principal component scores by applying the weights to the original values.  These new scores can then be used as the reduced set of predictor variables.</p>
</li>

</ol>
</div></section>













<section data-type="sect2" class="pagebreak-before less_space" data-pdf-bookmark="Interpreting Principal Components"><div class="sect2" id="InterpretPCA">
<h2>Interpreting Principal Components</h2>

<p>The nature of the principal components often reveals information about the structure of the data.<a data-type="indexterm" data-primary="unsupervised learning" data-secondary="principal components analysis" data-tertiary="interpreting principal components" id="ix_unslrnPCAintr"/><a data-type="indexterm" data-primary="principal components analysis" data-secondary="interpreting principal components" id="ix_PCAintrp"/>
There are a couple of standard visualization displays to help you glean insight about the principal components.
One such method is a <em>screeplot</em>  to visualize the relative importance of principal components (the name derives from the resemblance of the plot to a scree slope; here, the y-axis is the eigenvalue).
The following <em>R</em> code shows an example for a few top companies in the S&amp;P 500:</p>

<pre data-type="programlisting" data-code-language="r"><code class="n">syms</code> <code class="o">&lt;-</code> <code class="nf">c</code><code class="p">(</code> <code class="s">'AAPL'</code><code class="p">,</code> <code class="s">'MSFT'</code><code class="p">,</code> <code class="s">'CSCO'</code><code class="p">,</code> <code class="s">'INTC'</code><code class="p">,</code> <code class="s">'CVX'</code><code class="p">,</code> <code class="s">'XOM'</code><code class="p">,</code>
   <code class="s">'SLB'</code><code class="p">,</code> <code class="s">'COP'</code><code class="p">,</code> <code class="s">'JPM'</code><code class="p">,</code> <code class="s">'WFC'</code><code class="p">,</code> <code class="s">'USB'</code><code class="p">,</code> <code class="s">'AXP'</code><code class="p">,</code> <code class="s">'WMT'</code><code class="p">,</code> <code class="s">'TGT'</code><code class="p">,</code> <code class="s">'HD'</code><code class="p">,</code> <code class="s">'COST'</code><code class="p">)</code>
<code class="n">top_sp</code> <code class="o">&lt;-</code> <code class="n">sp500_px</code><code class="nf">[row.names</code><code class="p">(</code><code class="n">sp500_px</code><code class="p">)</code><code class="o">&gt;=</code><code class="s">'2005-01-01'</code><code class="p">,</code> <code class="n">syms</code><code class="p">]</code>
<code class="n">sp_pca</code> <code class="o">&lt;-</code> <code class="nf">princomp</code><code class="p">(</code><code class="n">top_sp</code><code class="p">)</code>
<code class="nf">screeplot</code><code class="p">(</code><code class="n">sp_pca</code><code class="p">)</code></pre>

<p>The information to create a loading plot from the <code>scikit-learn</code> result is available in <code>explained_variance_</code>. Here, we convert it into a <code>pandas</code> data frame and use it to make a bar chart:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">syms</code> <code class="o">=</code> <code class="nb">sorted</code><code class="p">([</code><code class="s1">'AAPL'</code><code class="p">,</code> <code class="s1">'MSFT'</code><code class="p">,</code> <code class="s1">'CSCO'</code><code class="p">,</code> <code class="s1">'INTC'</code><code class="p">,</code> <code class="s1">'CVX'</code><code class="p">,</code> <code class="s1">'XOM'</code><code class="p">,</code> <code class="s1">'SLB'</code><code class="p">,</code> <code class="s1">'COP'</code><code class="p">,</code>
               <code class="s1">'JPM'</code><code class="p">,</code> <code class="s1">'WFC'</code><code class="p">,</code> <code class="s1">'USB'</code><code class="p">,</code> <code class="s1">'AXP'</code><code class="p">,</code> <code class="s1">'WMT'</code><code class="p">,</code> <code class="s1">'TGT'</code><code class="p">,</code> <code class="s1">'HD'</code><code class="p">,</code> <code class="s1">'COST'</code><code class="p">])</code>
<code class="n">top_sp</code> <code class="o">=</code> <code class="n">sp500_px</code><code class="o">.</code><code class="n">loc</code><code class="p">[</code><code class="n">sp500_px</code><code class="o">.</code><code class="n">index</code> <code class="o">&gt;=</code> <code class="s1">'2011-01-01'</code><code class="p">,</code> <code class="n">syms</code><code class="p">]</code>

<code class="n">sp_pca</code> <code class="o">=</code> <code class="n">PCA</code><code class="p">()</code>
<code class="n">sp_pca</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">top_sp</code><code class="p">)</code>

<code class="n">explained_variance</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">sp_pca</code><code class="o">.</code><code class="n">explained_variance_</code><code class="p">)</code>
<code class="n">ax</code> <code class="o">=</code> <code class="n">explained_variance</code><code class="o">.</code><code class="n">head</code><code class="p">(</code><code class="mi">10</code><code class="p">)</code><code class="o">.</code><code class="n">plot</code><code class="o">.</code><code class="n">bar</code><code class="p">(</code><code class="n">legend</code><code class="o">=</code><code class="bp">False</code><code class="p">,</code> <code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">4</code><code class="p">,</code> <code class="mi">4</code><code class="p">))</code>
<code class="n">ax</code><code class="o">.</code><code class="n">set_xlabel</code><code class="p">(</code><code class="s1">'Component'</code><code class="p">)</code></pre>

<p>As seen in <a data-type="xref" href="#Screeplot">Figure 7-2</a>, the variance of the first principal component is quite large (as is often the case), but the other top principal components are significant.<a data-type="indexterm" data-primary="screeplots" data-secondary="for PCA of top stocks" id="idm46522837743128"/></p>

<figure class="width-75"><div id="Screeplot" class="figure">
<img src="Images/psd2_0702.png" alt="A screeplot for a PCA of top stocks from the SP 500." width="1102" height="1091"/>
<h6><span class="label">Figure 7-2. </span>A screeplot for a PCA of top stocks from the S&amp;P 500</h6>
</div></figure>

<p>It can be especially revealing to plot the weights of the top principal components.<a data-type="indexterm" data-primary="loadings" data-secondary="plotting for top principal components" id="idm46522837536136"/>
One way to do this in <em>R</em> is to use the <code>gather</code> function from the <code>tidyr</code> package in conjunction with <code>ggplot</code>:</p>

<pre data-type="programlisting" data-code-language="r"><code class="nf">library</code><code class="p">(</code><code class="n">tidyr</code><code class="p">)</code>
<code class="n">loadings</code> <code class="o">&lt;-</code> <code class="n">sp_pca</code><code class="o">$</code><code class="n">loadings</code><code class="p">[,</code><code class="m">1</code><code class="o">:</code><code class="m">5</code><code class="p">]</code>
<code class="n">loadings</code><code class="o">$</code><code class="n">Symbol</code> <code class="o">&lt;-</code> <code class="nf">row.names</code><code class="p">(</code><code class="n">loadings</code><code class="p">)</code>
<code class="n">loadings</code> <code class="o">&lt;-</code> <code class="nf">gather</code><code class="p">(</code><code class="n">loadings</code><code class="p">,</code> <code class="s">'Component'</code><code class="p">,</code> <code class="s">'Weight'</code><code class="p">,</code> <code class="o">-</code><code class="n">Symbol</code><code class="p">)</code>
<code class="nf">ggplot</code><code class="p">(</code><code class="n">loadings</code><code class="p">,</code> <code class="nf">aes</code><code class="p">(</code><code class="n">x</code><code class="o">=</code><code class="n">Symbol</code><code class="p">,</code> <code class="n">y</code><code class="o">=</code><code class="n">Weight</code><code class="p">))</code> <code class="o">+</code>
  <code class="nf">geom_bar</code><code class="p">(</code><code class="n">stat</code><code class="o">=</code><code class="s">'identity'</code><code class="p">)</code> <code class="o">+</code>
  <code class="nf">facet_grid</code><code class="p">(</code><code class="n">Component</code> <code class="o">~</code> <code class="n">.,</code> <code class="n">scales</code><code class="o">=</code><code class="s">'free_y'</code><code class="p">)</code></pre>

<p>Here is the code to create the same visualization in <em>Python</em>:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">loadings</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">sp_pca</code><code class="o">.</code><code class="n">components_</code><code class="p">[</code><code class="mi">0</code><code class="p">:</code><code class="mi">5</code><code class="p">,</code> <code class="p">:],</code> <code class="n">columns</code><code class="o">=</code><code class="n">top_sp</code><code class="o">.</code><code class="n">columns</code><code class="p">)</code>
<code class="n">maxPC</code> <code class="o">=</code> <code class="mf">1.01</code> <code class="o">*</code> <code class="n">np</code><code class="o">.</code><code class="n">max</code><code class="p">(</code><code class="n">np</code><code class="o">.</code><code class="n">max</code><code class="p">(</code><code class="n">np</code><code class="o">.</code><code class="n">abs</code><code class="p">(</code><code class="n">loadings</code><code class="o">.</code><code class="n">loc</code><code class="p">[</code><code class="mi">0</code><code class="p">:</code><code class="mi">5</code><code class="p">,</code> <code class="p">:])))</code>

<code class="n">f</code><code class="p">,</code> <code class="n">axes</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">(</code><code class="mi">5</code><code class="p">,</code> <code class="mi">1</code><code class="p">,</code> <code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">5</code><code class="p">,</code> <code class="mi">5</code><code class="p">),</code> <code class="n">sharex</code><code class="o">=</code><code class="bp">True</code><code class="p">)</code>
<code class="k">for</code> <code class="n">i</code><code class="p">,</code> <code class="n">ax</code> <code class="ow">in</code> <code class="nb">enumerate</code><code class="p">(</code><code class="n">axes</code><code class="p">):</code>
    <code class="n">pc_loadings</code> <code class="o">=</code> <code class="n">loadings</code><code class="o">.</code><code class="n">loc</code><code class="p">[</code><code class="n">i</code><code class="p">,</code> <code class="p">:]</code>
    <code class="n">colors</code> <code class="o">=</code> <code class="p">[</code><code class="s1">'C0'</code> <code class="k">if</code> <code class="n">l</code> <code class="o">&gt;</code> <code class="mi">0</code> <code class="k">else</code> <code class="s1">'C1'</code> <code class="k">for</code> <code class="n">l</code> <code class="ow">in</code> <code class="n">pc_loadings</code><code class="p">]</code>
    <code class="n">ax</code><code class="o">.</code><code class="n">axhline</code><code class="p">(</code><code class="n">color</code><code class="o">=</code><code class="s1">'#888888'</code><code class="p">)</code>
    <code class="n">pc_loadings</code><code class="o">.</code><code class="n">plot</code><code class="o">.</code><code class="n">bar</code><code class="p">(</code><code class="n">ax</code><code class="o">=</code><code class="n">ax</code><code class="p">,</code> <code class="n">color</code><code class="o">=</code><code class="n">colors</code><code class="p">)</code>
    <code class="n">ax</code><code class="o">.</code><code class="n">set_ylabel</code><code class="p">(</code><code class="n">f</code><code class="s1">'PC{i+1}'</code><code class="p">)</code>
    <code class="n">ax</code><code class="o">.</code><code class="n">set_ylim</code><code class="p">(</code><code class="o">-</code><code class="n">maxPC</code><code class="p">,</code> <code class="n">maxPC</code><code class="p">)</code></pre>

<p>The loadings for the top five components are shown in <a data-type="xref" href="#Loadings">Figure 7-3</a>.
The loadings for the first principal component have the same sign: this is typical for data in which all the columns share a common factor (in this case, the overall stock market trend).
The second component captures the price changes of energy stocks as compared to the other stocks.
The third component is primarily a contrast in the movements of Apple and CostCo.
The fourth component contrasts the movements of Schlumberger (SLB) to the other energy stocks.
Finally, the fifth component is mostly dominated by financial companies.</p>

<figure><div id="Loadings" class="figure">
<img src="Images/psd2_0703.png" alt="The loadings for the top five principal components of stock price returns." width="1157" height="1165"/>
<h6><span class="label">Figure 7-3. </span>The loadings for the top five principal components of stock price returns</h6>
</div></figure>
<div data-type="note" epub:type="note" id="HowManyComponents"><h1>How Many Components to Choose?</h1>
<p>If your goal is to reduce the dimension of the data, you must decide how many principal components to select.<a data-type="indexterm" data-primary="unsupervised learning" data-secondary="principal components analysis" data-tertiary="interpreting principal components" data-startref="ix_unslrnPCAintr" id="idm46522837234712"/><a data-type="indexterm" data-primary="principal components analysis" data-secondary="interpreting principal components" data-startref="ix_PCAintrp" id="idm46522837233256"/><a data-type="indexterm" data-primary="principal components analysis" data-secondary="deciding how many components to choose" id="idm46522837232104"/>
The most common approach is to use an ad hoc  rule to select the components that explain  “most” of the variance.
You can do this visually through the screeplot, as, for example, in <a data-type="xref" href="#Screeplot">Figure 7-2</a>.
Alternatively, you could select the top components such that the cumulative variance exceeds a threshold, such as 80%.
Also, you can inspect the loadings to determine if the component has an intuitive interpretation.<a data-type="indexterm" data-primary="cross validation" data-secondary="using to select principal components" id="idm46522837229800"/>
Cross-validation provides a more formal method to select the number of significant components (see <a data-type="xref" href="ch04.xhtml#CrossValidation">“Cross-Validation”</a> for more).</p>
</div>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Correspondence Analysis"><div class="sect2" id="idm46522837786072">
<h2>Correspondence Analysis</h2>

<p>PCA cannot be used for categorical data; however, a somewhat related technique is <em>correspondence analysis</em>.<a data-type="indexterm" data-primary="principal components analysis" data-secondary="correspondence analysis" id="ix_PCAcorran"/><a data-type="indexterm" data-primary="correspondence analysis" id="ix_corran"/><a data-type="indexterm" data-primary="unsupervised learning" data-secondary="principal components analysis" data-tertiary="correspondence analysis" id="ix_unslrnPCACA"/>  The goal is to recognize associations between categories, or between categorical features. The similarities between correspondence analysis and principal components analysis are mainly under the hood—the matrix algebra for dimension scaling.  Correspondence analysis is used mainly for graphical analysis of low-dimensional categorical data and is not used in the same way that PCA is for dimension reduction as a preparatory step with big data.</p>

<p>The input can be seen as a table, with rows representing one variable and columns another, and the cells representing record counts.<a data-type="indexterm" data-primary="biplot" id="idm46522837220984"/><a data-type="indexterm" data-primary="scatterplots" data-secondary="biplot" id="idm46522837220280"/>  The output (after some matrix algebra) is a <em>biplot</em>—a scatterplot with axes scaled (and with percentages indicating how much variance is explained by that dimension).  The meaning of the units on the axes is not intuitively connected to the original data, and the main value of the scatterplot is to illustrate graphically variables that are associated with one another (by proximity on the plot).
See for example, <a data-type="xref" href="#Correspondence_Analysis">Figure 7-4</a>, in which household tasks are arrayed according to whether they are done jointly or solo (vertical axis), and whether wife or husband has primary responsibility (horizontal axis). Correspondence analysis is many decades old, as is the spirit of this example, judging by the assignment of tasks.</p>

<p>There are a variety of packages for correspondence analysis in <em>R</em>. Here, we use the package <code>ca</code>:</p>

<pre data-type="programlisting" data-code-language="r"><code class="n">ca_analysis</code> <code class="o">&lt;-</code> <code class="nf">ca</code><code class="p">(</code><code class="n">housetasks</code><code class="p">)</code>
<code class="nf">plot</code><code class="p">(</code><code class="n">ca_analysis</code><code class="p">)</code></pre>

<p class="pagebreak-before">In <em>Python</em>, we can use the <code>prince</code> package, which implements correspondence analysis using the <code>scikit-learn</code> API:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">ca</code> <code class="o">=</code> <code class="n">prince</code><code class="o">.</code><code class="n">CA</code><code class="p">(</code><code class="n">n_components</code><code class="o">=</code><code class="mi">2</code><code class="p">)</code>
<code class="n">ca</code> <code class="o">=</code> <code class="n">ca</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">housetasks</code><code class="p">)</code>

<code class="n">ca</code><code class="o">.</code><code class="n">plot_coordinates</code><code class="p">(</code><code class="n">housetasks</code><code class="p">,</code> <code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">6</code><code class="p">,</code> <code class="mi">6</code><code class="p">))</code></pre>

<figure><div id="Correspondence_Analysis" class="figure">
<img src="Images/psd2_0704.png" alt="Correspondence analysis of house task data." width="1457" height="1456"/>
<h6><span class="label">Figure 7-4. </span>Graphical representation of a correspondence analysis of house task data</h6>
</div></figure>
<aside data-type="sidebar" epub:type="sidebar" class="pagebreak-before less_space"><div class="sidebar" id="idm46522837154280">
<h5>Key Ideas</h5>
<ul>
<li>
<p>Principal components are linear combinations of the predictor variables (numeric data only).<a data-type="indexterm" data-primary="unsupervised learning" data-secondary="principal components analysis" data-tertiary="correspondence analysis" data-startref="ix_unslrnPCACA" id="idm46522837151736"/><a data-type="indexterm" data-primary="principal components analysis" data-secondary="correspondence analysis" data-startref="ix_PCAcorran" id="idm46522837150248"/></p>
</li>
<li>
<p>Principal components are calculated so as to minimize correlation between components, reducing redundancy.</p>
</li>
<li>
<p>A limited number of components will typically explain most of the variance in the outcome variable.</p>
</li>
<li>
<p>The limited set of principal components can then be used in place of the (more numerous) original predictors, reducing dimensionality.</p>
</li>
<li>
<p>A superficially similar technique for categorical data is correspondence analysis, but it is not useful in a big data context.<a data-type="indexterm" data-primary="correspondence analysis" data-startref="ix_corran" id="idm46522837145112"/></p>
</li>
</ul>
</div></aside>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Further Reading"><div class="sect2" id="idm46522837143752">
<h2>Further Reading</h2>

<p>For a detailed look at the use of cross-validation in principal components, see Rasmus Bro, K. Kjeldahl, A.K. Smilde, and Henk A. L. Kiers, <a href="https://oreil.ly/yVryf">“Cross-Validation of Component Models: A Critical Look at Current Methods”</a>, <em>Analytical and Bioanalytical Chemistry</em> 390, no. 5 (2008).<a data-type="indexterm" data-primary="unsupervised learning" data-secondary="principal components analysis" data-startref="ix_unslrnPCA" id="idm46522837141192"/><a data-type="indexterm" data-primary="principal components analysis" data-startref="ix_PCA" id="idm46522837139976"/></p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="K-Means Clustering"><div class="sect1" id="Kmeans">
<h1>K-Means Clustering</h1>

<p>Clustering is a technique to divide data into different groups, where the records in each group are similar to one another.<a data-type="indexterm" data-primary="unsupervised learning" data-secondary="K-means clustering" id="ix_unslrnKM"/><a data-type="indexterm" data-primary="K-means clustering" id="ix_KMclus"/><a data-type="indexterm" data-primary="clustering" data-secondary="K-means" id="ix_clusKM"/>
A goal of clustering is to identify significant and meaningful groups of data.
The groups can be used directly, analyzed in more depth, or passed as a feature or an outcome to a predictive regression or classification model.
<em>K-means</em> was the first clustering method to be developed; it is still widely used, owing its popularity to the relative simplicity of the algorithm and its ability to scale to large data sets.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46522837132328">
<h5>Key Terms for K-Means Clustering</h5><dl>
<dt class="horizontal"><strong><em>Cluster</em></strong></dt>
<dd>
<p>A group of records that are similar.<a data-type="indexterm" data-primary="clusters" id="idm46522837129352"/></p>
</dd>
<dt class="horizontal"><strong><em>Cluster mean</em></strong></dt>
<dd>
<p>The vector of variable means for the records in a cluster.</p>
</dd>
<dt class="horizontal"><strong><em>K</em></strong></dt>
<dd>
<p>The number of clusters.<a data-type="indexterm" data-primary="K (in K-means clustering)" id="idm46522837045256"/><a data-type="indexterm" data-primary="cluster mean" id="idm46522837044648"/></p>
</dd>
</dl>
</div></aside>

<p><em>K</em>-means divides the data into <em>K</em> clusters by minimizing the sum of the squared distances of each record to the <em>mean</em> of its assigned cluster.
This is referred to as the <em>within-cluster sum of squares</em> or <em>within-cluster SS</em>.<a data-type="indexterm" data-primary="within-cluster sum of squares (SS)" id="idm46522837041352"/>
<em>K</em>-means does not ensure the clusters will have the same size but finds the clusters that are the best separated.</p>
<div data-type="note" epub:type="note"><h1>Normalization</h1>
<p>It is<a data-type="indexterm" data-primary="standardization" data-secondary="of continuous variables" id="idm46522837038712"/> typical to normalize (standardize) continuous variables by subtracting the mean and dividing by the standard deviation.  Otherwise, variables with large scale will dominate the clustering process (see <a data-type="xref" href="ch06.xhtml#Standardization">“Standardization (Normalization, z-Scores)”</a>).</p>
</div>








<section data-type="sect2" data-pdf-bookmark="A Simple Example"><div class="sect2" id="idm46522837036296">
<h2>A Simple Example</h2>

<p>Start by considering a data set with <em>n</em> records and just two variables, <math alttext="x">
  <mi>x</mi>
</math> and <math alttext="y">
  <mi>y</mi>
</math>.<a data-type="indexterm" data-primary="K-means clustering" data-secondary="simple example" id="idm46522837031032"/><a data-type="indexterm" data-primary="clustering" data-secondary="K-means" data-tertiary="simple example" id="idm46522837030024"/><a data-type="indexterm" data-primary="unsupervised learning" data-secondary="K-means clustering" data-tertiary="simple example" id="idm46522837028808"/>
Suppose we want to split the data into <math alttext="upper K equals 4">
  <mrow>
    <mi>K</mi>
    <mo>=</mo>
    <mn>4</mn>
  </mrow>
</math> clusters.
This means assigning each record <math alttext="left-parenthesis x Subscript i Baseline comma y Subscript i Baseline right-parenthesis">
  <mrow>
    <mo>(</mo>
    <msub><mi>x</mi> <mi>i</mi> </msub>
    <mo>,</mo>
    <msub><mi>y</mi> <mi>i</mi> </msub>
    <mo>)</mo>
  </mrow>
</math> to a cluster <em>k</em>.
Given an assignment of <math alttext="n Subscript k">
  <msub><mi>n</mi> <mi>k</mi> </msub>
</math> records to cluster <em>k</em>, the center of the cluster <math alttext="left-parenthesis x overbar Subscript k Baseline comma y overbar Subscript k Baseline right-parenthesis">
  <mrow>
    <mo>(</mo>
    <msub><mover accent="true"><mi>x</mi> <mo>¯</mo></mover> <mi>k</mi> </msub>
    <mo>,</mo>
    <msub><mover accent="true"><mi>y</mi> <mo>¯</mo></mover> <mi>k</mi> </msub>
    <mo>)</mo>
  </mrow>
</math> is the mean of the points in the cluster:</p>
<div data-type="equation">
<math display="block">
  <mtable displaystyle="true">
    <mtr>
      <mtd><msub><mrow class="MJX-TeXAtom-ORD"><mover><mi>x</mi><mo stretchy="false">¯<!-- ¯ --></mo></mover>
          </mrow>
          <mi>k</mi>
        </msub>
      </mtd>
      <mtd>
        <mi/>
        <mo>=</mo>
        <mfrac><mn>1</mn><msub><mi>n</mi><mi>k</mi></msub></mfrac>
        <munder>
          <mo>∑<!-- ∑ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mtable rowspacing="0.1em" columnspacing="0em 0em 0em 0em" displaystyle="false">
              <mtr><mtd><mi>i</mi><mspace width="mediummathspace"/><mo>∈<!-- ∈ --></mo></mtd></mtr>
              <mtr><mtd><mrow class="MJX-TeXAtom-ORD"><mtext>Cluster</mtext></mrow>
                  <mspace width="thickmathspace"/><mi>k</mi></mtd></mtr>
            </mtable>
          </mrow>
        </munder>
        <msub>
          <mi>x</mi>
          <mi>i</mi>
        </msub>
      </mtd>
    </mtr>
    <mtr>
      <mtd><msub><mrow class="MJX-TeXAtom-ORD"><mover><mi>y</mi><mo stretchy="false">¯<!-- ¯ --></mo></mover>
          </mrow>
          <mi>k</mi>
        </msub>
      </mtd>
      <mtd>
        <mi/>
        <mo>=</mo>
        <mfrac><mn>1</mn><msub><mi>n</mi><mi>k</mi></msub></mfrac>
        <munder>
          <mo>∑<!-- ∑ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mtable rowspacing="0.1em" columnspacing="0em 0em 0em 0em" displaystyle="false">
              <mtr><mtd><mi>i</mi><mspace width="mediummathspace"/><mo>∈<!-- ∈ --></mo></mtd></mtr>
              <mtr><mtd><mrow class="MJX-TeXAtom-ORD"><mtext>Cluster</mtext></mrow>
                   <mspace width="thickmathspace"/><mi>k</mi></mtd></mtr>
            </mtable>
          </mrow>
        </munder>
        <msub><mi>y</mi><mi>i</mi></msub>
      </mtd>
    </mtr>
  </mtable>
</math>
</div>
<div data-type="caution"><h1>Cluster Mean</h1>
<p>In clustering records with multiple variables (the typical case), the term <em>cluster mean</em> refers not to a single number but to the vector of means of the variables.<a data-type="indexterm" data-primary="cluster mean" id="idm46522836978456"/></p>
</div>

<p>The sum of <a data-type="indexterm" data-primary="sum of squares (SS)" data-secondary="within-cluster SS" id="idm46522836977240"/>squares within a cluster is given by:</p>
<div id="WithinClusterSS" data-type="equation">
<math display="block">
  <mrow>
    <msub><mtext>SS</mtext> <mi>k</mi> </msub>
    <mo>=</mo>
    <munder><mo>∑</mo> <mrow><mi>i</mi><mo>∈</mo><mtext>Cluster</mtext><mspace width="4.pt"/><mi>k</mi></mrow> </munder>
    <msup><mfenced separators="" open="(" close=")"><msub><mi>x</mi> <mi>i</mi> </msub><mo>-</mo><msub><mover accent="true"><mi>x</mi> <mo>¯</mo></mover> <mi>k</mi> </msub></mfenced> <mn>2</mn> </msup>
    <mo>+</mo>
    <msup><mfenced separators="" open="(" close=")"><msub><mi>y</mi> <mi>i</mi> </msub><mo>-</mo><msub><mover accent="true"><mi>y</mi> <mo>¯</mo></mover> <mi>k</mi> </msub></mfenced> <mn>2</mn> </msup>
  </mrow>
</math>
</div>

<p><em>K</em>-means finds the assignment of records that minimizes within-cluster sum of squares across all four clusters <math alttext="SS Subscript 1 Baseline plus SS Subscript 2 Baseline plus SS Subscript 3 Baseline plus SS Subscript 4">
  <mrow>
    <msub><mtext>SS</mtext> <mn>1</mn> </msub>
    <mo>+</mo>
    <msub><mtext>SS</mtext> <mn>2</mn> </msub>
    <mo>+</mo>
    <msub><mtext>SS</mtext> <mn>3</mn> </msub>
    <mo>+</mo>
    <msub><mtext>SS</mtext> <mn>4</mn> </msub>
  </mrow>
</math>:</p>
<div data-type="equation">
<math display="block">
  <mrow>
    <munderover><mo>∑</mo> <mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow> <mn>4</mn> </munderover>
    <msub><mtext>SS</mtext> <mi>k</mi> </msub>
  </mrow>
</math>
</div>

<p>A typical use of clustering is to locate natural, separate clusters in the data.  Another application is to divide the data into a predetermined number of separate groups, where clustering is used to ensure the groups are as different as possible from one another.</p>

<p>For example, suppose we want to divide daily stock returns into four groups.  <em>K</em>-means clustering can be used to separate the data into the best groupings.
Note that daily stock returns are reported in a fashion that is, in effect, standardized, so we do not need to normalize the data.
In <em>R</em>, <em>K</em>-means clustering can be performed using the <code>kmeans</code> function.
For example, the following finds four clusters based on two variables—the daily stock returns for ExxonMobil (<code>XOM</code>) and Chevron (<code>CVX</code>):</p>

<pre data-type="programlisting" data-code-language="r"><code class="n">df</code> <code class="o">&lt;-</code> <code class="n">sp500_px</code><code class="nf">[row.names</code><code class="p">(</code><code class="n">sp500_px</code><code class="p">)</code><code class="o">&gt;=</code><code class="s">'2011-01-01'</code><code class="p">,</code> <code class="nf">c</code><code class="p">(</code><code class="s">'XOM'</code><code class="p">,</code> <code class="s">'CVX'</code><code class="p">)]</code>
<code class="n">km</code> <code class="o">&lt;-</code> <code class="nf">kmeans</code><code class="p">(</code><code class="n">df</code><code class="p">,</code> <code class="n">centers</code><code class="o">=</code><code class="m">4</code><code class="p">)</code></pre>

<p>We use the <code>sklearn.cluster.KMeans</code> method from <code>scikit-learn</code> in <em>Python</em>:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">df</code> <code class="o">=</code> <code class="n">sp500_px</code><code class="o">.</code><code class="n">loc</code><code class="p">[</code><code class="n">sp500_px</code><code class="o">.</code><code class="n">index</code> <code class="o">&gt;=</code> <code class="s1">'2011-01-01'</code><code class="p">,</code> <code class="p">[</code><code class="s1">'XOM'</code><code class="p">,</code> <code class="s1">'CVX'</code><code class="p">]]</code>
<code class="n">kmeans</code> <code class="o">=</code> <code class="n">KMeans</code><code class="p">(</code><code class="n">n_clusters</code><code class="o">=</code><code class="mi">4</code><code class="p">)</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">df</code><code class="p">)</code></pre>

<p>The cluster assignment for each record is returned as the <code>cluster</code> component (<em>R</em>):</p>

<pre data-type="programlisting" data-code-language="r"><code class="o">&gt;</code> <code class="n">df</code><code class="o">$</code><code class="n">cluster</code> <code class="o">&lt;-</code> <code class="nf">factor</code><code class="p">(</code><code class="n">km</code><code class="o">$</code><code class="n">cluster</code><code class="p">)</code>
<code class="o">&gt;</code> <code class="nf">head</code><code class="p">(</code><code class="n">df</code><code class="p">)</code>
                  <code class="n">XOM</code>        <code class="n">CVX</code> <code class="n">cluster</code>
<code class="m">2011-01-03</code> <code class="m">0.73680496</code>  <code class="m">0.2406809</code>       <code class="m">2</code>
<code class="m">2011-01-04</code> <code class="m">0.16866845</code> <code class="m">-0.5845157</code>       <code class="m">1</code>
<code class="m">2011-01-05</code> <code class="m">0.02663055</code>  <code class="m">0.4469854</code>       <code class="m">2</code>
<code class="m">2011-01-06</code> <code class="m">0.24855834</code> <code class="m">-0.9197513</code>       <code class="m">1</code>
<code class="m">2011-01-07</code> <code class="m">0.33732892</code>  <code class="m">0.1805111</code>       <code class="m">2</code>
<code class="m">2011-01-10</code> <code class="m">0.00000000</code> <code class="m">-0.4641675</code>       <code class="m">1</code></pre>

<p>In <code>scikit-learn</code>, the cluster labels are available in the <code>labels_</code> field:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">df</code><code class="p">[</code><code class="s1">'cluster'</code><code class="p">]</code> <code class="o">=</code> <code class="n">kmeans</code><code class="o">.</code><code class="n">labels_</code>
<code class="n">df</code><code class="o">.</code><code class="n">head</code><code class="p">()</code></pre>

<p>The first six records are assigned to either cluster 1 or cluster 2.
The means of the clusters are also returned (<em>R</em>):</p>

<pre data-type="programlisting" data-code-language="r"><code class="o">&gt;</code> <code class="n">centers</code> <code class="o">&lt;-</code> <code class="nf">data.frame</code><code class="p">(</code><code class="n">cluster</code><code class="o">=</code><code class="nf">factor</code><code class="p">(</code><code class="m">1</code><code class="o">:</code><code class="m">4</code><code class="p">),</code> <code class="n">km</code><code class="o">$</code><code class="n">centers</code><code class="p">)</code>
<code class="o">&gt;</code> <code class="n">centers</code>
  <code class="n">cluster</code>        <code class="n">XOM</code>        <code class="n">CVX</code>
<code class="m">1</code>       <code class="m">1</code> <code class="m">-0.3284864</code> <code class="m">-0.5669135</code>
<code class="m">2</code>       <code class="m">2</code>  <code class="m">0.2410159</code>  <code class="m">0.3342130</code>
<code class="m">3</code>       <code class="m">3</code> <code class="m">-1.1439800</code> <code class="m">-1.7502975</code>
<code class="m">4</code>       <code class="m">4</code>  <code class="m">0.9568628</code>  <code class="m">1.3708892</code></pre>

<p class="pagebreak-before">In <code>scikit-learn</code>, the cluster centers are available in the <code>cluster_centers_</code> field:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">centers</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">kmeans</code><code class="o">.</code><code class="n">cluster_centers_</code><code class="p">,</code> <code class="n">columns</code><code class="o">=</code><code class="p">[</code><code class="s1">'XOM'</code><code class="p">,</code> <code class="s1">'CVX'</code><code class="p">])</code>
<code class="n">centers</code></pre>

<p>Clusters 1 and 3 represent “down” markets, while clusters 2 and 4 represent “up <span class="keep-together">markets</span>.”</p>

<p>As the <em>K</em>-means algorithm uses randomized starting points, the results may differ between subsequent runs and different implementations of the method. In general, you should check that the fluctuations aren’t too large.</p>

<p>In this example, with just two variables, it is straightforward to visualize the clusters and their means:</p>

<pre data-type="programlisting" data-code-language="r"><code class="nf">ggplot</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="n">df</code><code class="p">,</code> <code class="nf">aes</code><code class="p">(</code><code class="n">x</code><code class="o">=</code><code class="n">XOM</code><code class="p">,</code> <code class="n">y</code><code class="o">=</code><code class="n">CVX</code><code class="p">,</code> <code class="n">color</code><code class="o">=</code><code class="n">cluster</code><code class="p">,</code> <code class="n">shape</code><code class="o">=</code><code class="n">cluster</code><code class="p">))</code> <code class="o">+</code>
  <code class="nf">geom_point</code><code class="p">(</code><code class="n">alpha</code><code class="o">=</code><code class="m">.3</code><code class="p">)</code> <code class="o">+</code>
  <code class="nf">geom_point</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="n">centers</code><code class="p">,</code>  <code class="nf">aes</code><code class="p">(</code><code class="n">x</code><code class="o">=</code><code class="n">XOM</code><code class="p">,</code> <code class="n">y</code><code class="o">=</code><code class="n">CVX</code><code class="p">),</code> <code class="n">size</code><code class="o">=</code><code class="m">3</code><code class="p">,</code> <code class="n">stroke</code><code class="o">=</code><code class="m">2</code><code class="p">)</code></pre>

<p>The <code>seaborn</code> <code>scatterplot</code> function makes it easy to color (<code>hue</code>) and style (<code>style</code>) the points by a property:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">fig</code><code class="p">,</code> <code class="n">ax</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">(</code><code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">4</code><code class="p">,</code> <code class="mi">4</code><code class="p">))</code>
<code class="n">ax</code> <code class="o">=</code> <code class="n">sns</code><code class="o">.</code><code class="n">scatterplot</code><code class="p">(</code><code class="n">x</code><code class="o">=</code><code class="s1">'XOM'</code><code class="p">,</code> <code class="n">y</code><code class="o">=</code><code class="s1">'CVX'</code><code class="p">,</code> <code class="n">hue</code><code class="o">=</code><code class="s1">'cluster'</code><code class="p">,</code> <code class="n">style</code><code class="o">=</code><code class="s1">'cluster'</code><code class="p">,</code>
                     <code class="n">ax</code><code class="o">=</code><code class="n">ax</code><code class="p">,</code> <code class="n">data</code><code class="o">=</code><code class="n">df</code><code class="p">)</code>
<code class="n">ax</code><code class="o">.</code><code class="n">set_xlim</code><code class="p">(</code><code class="o">-</code><code class="mi">3</code><code class="p">,</code> <code class="mi">3</code><code class="p">)</code>
<code class="n">ax</code><code class="o">.</code><code class="n">set_ylim</code><code class="p">(</code><code class="o">-</code><code class="mi">3</code><code class="p">,</code> <code class="mi">3</code><code class="p">)</code>
<code class="n">centers</code><code class="o">.</code><code class="n">plot</code><code class="o">.</code><code class="n">scatter</code><code class="p">(</code><code class="n">x</code><code class="o">=</code><code class="s1">'XOM'</code><code class="p">,</code> <code class="n">y</code><code class="o">=</code><code class="s1">'CVX'</code><code class="p">,</code> <code class="n">ax</code><code class="o">=</code><code class="n">ax</code><code class="p">,</code> <code class="n">s</code><code class="o">=</code><code class="mi">50</code><code class="p">,</code> <code class="n">color</code><code class="o">=</code><code class="s1">'black'</code><code class="p">)</code></pre>

<p>The resulting plot, shown in <a data-type="xref" href="#KmeansStockData">Figure 7-5</a>, shows the cluster assignments and the cluster means. Note that <em>K</em>-means will assign records to clusters, even if those clusters are not well separated (which can be useful if you need to optimally divide records into groups).</p>

<figure><div id="KmeansStockData" class="figure">
<img src="Images/psd2_0705.png" alt="The clusters of k-means applied to stock price data for ExxonMobil and Chevron (the cluster centers are highlighted with black symbols)." width="1134" height="847"/>
<h6><span class="label">Figure 7-5. </span>The clusters of K-means applied to daily stock returns for ExxonMobil and Chevron (the cluster centers are highlighted with black symbols)</h6>
</div></figure>
</div></section>













<section data-type="sect2" data-pdf-bookmark="K-Means Algorithm"><div class="sect2" id="idm46522837035352">
<h2>K-Means Algorithm</h2>

<p>In general, <em>K</em>-means can be applied to a data set with <em>p</em> variables <math alttext="upper X 1 comma ellipsis comma upper X Subscript p Baseline">
  <mrow>
    <msub><mi>X</mi> <mn>1</mn> </msub>
    <mo>,</mo>
    <mo>...</mo>
    <mo>,</mo>
    <msub><mi>X</mi> <mi>p</mi> </msub>
  </mrow>
</math>.<a data-type="indexterm" data-primary="K-means clustering" data-secondary="K-means algorithm" id="idm46522836411304"/><a data-type="indexterm" data-primary="unsupervised learning" data-secondary="K-means clustering" data-tertiary="K-means algorithm" id="idm46522836410296"/><a data-type="indexterm" data-primary="clustering" data-secondary="K-means" data-tertiary="K-means algorithm" id="idm46522836409080"/>
While the exact solution to <em>K</em>-means is computationally very difficult, heuristic algorithms provide an efficient way to compute a locally optimal solution.</p>

<p>The algorithm starts with a user-specified <em>K</em> and an initial set of cluster means and then iterates the following steps:</p>
<ol>
<li>
<p>Assign each record to the nearest cluster mean as measured by squared distance.</p>
</li>
<li>
<p>Compute the new cluster means based on the assignment of records.</p>
</li>

</ol>

<p>The algorithm converges when the assignment of records to clusters does not change.</p>

<p>For the first iteration, you need to specify an initial set of cluster means.
Usually you do this by randomly assigning each record to one of the <em>K</em> clusters and then finding the means of those clusters.</p>

<p>Since this algorithm isn’t guaranteed to find the best possible solution, it is recommended to run the algorithm several times using different random samples to initialize the algorithm.
When more than one set of iterations is used, the <em>K</em>-means result is given by the iteration that has the lowest within-cluster sum of squares.</p>

<p>The <code>nstart</code> parameter to the <em>R</em> function <code>kmeans</code> allows you to specify the number of random starts to try.
For example, the following code runs <em>K</em>-means to find 5 clusters using 10 different starting cluster means:</p>

<pre data-type="programlisting" data-code-language="r"><code class="n">syms</code> <code class="o">&lt;-</code> <code class="nf">c</code><code class="p">(</code> <code class="s">'AAPL'</code><code class="p">,</code> <code class="s">'MSFT'</code><code class="p">,</code> <code class="s">'CSCO'</code><code class="p">,</code> <code class="s">'INTC'</code><code class="p">,</code> <code class="s">'CVX'</code><code class="p">,</code> <code class="s">'XOM'</code><code class="p">,</code> <code class="s">'SLB'</code><code class="p">,</code> <code class="s">'COP'</code><code class="p">,</code>
           <code class="s">'JPM'</code><code class="p">,</code> <code class="s">'WFC'</code><code class="p">,</code> <code class="s">'USB'</code><code class="p">,</code> <code class="s">'AXP'</code><code class="p">,</code> <code class="s">'WMT'</code><code class="p">,</code> <code class="s">'TGT'</code><code class="p">,</code> <code class="s">'HD'</code><code class="p">,</code> <code class="s">'COST'</code><code class="p">)</code>
<code class="n">df</code> <code class="o">&lt;-</code> <code class="n">sp500_px</code><code class="nf">[row.names</code><code class="p">(</code><code class="n">sp500_px</code><code class="p">)</code> <code class="o">&gt;=</code> <code class="s">'2011-01-01'</code><code class="p">,</code> <code class="n">syms</code><code class="p">]</code>
<code class="n">km</code> <code class="o">&lt;-</code> <code class="nf">kmeans</code><code class="p">(</code><code class="n">df</code><code class="p">,</code> <code class="n">centers</code><code class="o">=</code><code class="m">5</code><code class="p">,</code> <code class="n">nstart</code><code class="o">=</code><code class="m">10</code><code class="p">)</code></pre>

<p>The function automatically returns the best solution out of the 10 different starting points.
You can use the argument <code>iter.max</code> to set the maximum number of iterations the algorithm is allowed for each random start.</p>

<p>The <code>scikit-learn</code> algorithm is repeated 10 times by default (<code>n_init</code>). The argument <code>max_iter</code> (default 300) can be used to control the number of iterations:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">syms</code> <code class="o">=</code> <code class="nb">sorted</code><code class="p">([</code><code class="s1">'AAPL'</code><code class="p">,</code> <code class="s1">'MSFT'</code><code class="p">,</code> <code class="s1">'CSCO'</code><code class="p">,</code> <code class="s1">'INTC'</code><code class="p">,</code> <code class="s1">'CVX'</code><code class="p">,</code> <code class="s1">'XOM'</code><code class="p">,</code> <code class="s1">'SLB'</code><code class="p">,</code> <code class="s1">'COP'</code><code class="p">,</code>
               <code class="s1">'JPM'</code><code class="p">,</code> <code class="s1">'WFC'</code><code class="p">,</code> <code class="s1">'USB'</code><code class="p">,</code> <code class="s1">'AXP'</code><code class="p">,</code> <code class="s1">'WMT'</code><code class="p">,</code> <code class="s1">'TGT'</code><code class="p">,</code> <code class="s1">'HD'</code><code class="p">,</code> <code class="s1">'COST'</code><code class="p">])</code>
<code class="n">top_sp</code> <code class="o">=</code> <code class="n">sp500_px</code><code class="o">.</code><code class="n">loc</code><code class="p">[</code><code class="n">sp500_px</code><code class="o">.</code><code class="n">index</code> <code class="o">&gt;=</code> <code class="s1">'2011-01-01'</code><code class="p">,</code> <code class="n">syms</code><code class="p">]</code>
<code class="n">kmeans</code> <code class="o">=</code> <code class="n">KMeans</code><code class="p">(</code><code class="n">n_clusters</code><code class="o">=</code><code class="mi">5</code><code class="p">)</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">top_sp</code><code class="p">)</code></pre>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Interpreting the Clusters"><div class="sect2" id="idm46522836418120">
<h2>Interpreting the Clusters</h2>

<p>An important part of cluster analysis can involve the interpretation of the clusters.<a data-type="indexterm" data-primary="unsupervised learning" data-secondary="K-means clustering" data-tertiary="interpreting the clusters" id="ix_unslrnKMint"/><a data-type="indexterm" data-primary="K-means clustering" data-secondary="interpreting the clusters" id="ix_KMclusintrp"/><a data-type="indexterm" data-primary="clustering" data-secondary="K-means" data-tertiary="interpreting the clusters" id="ix_clusKMintrp"/>
The two most important outputs from <code>kmeans</code> are the sizes of the clusters and the cluster means.
For the example in the previous subsection, the sizes of resulting clusters are given by this <em>R</em> command:</p>

<pre data-type="programlisting" data-code-language="r"><code class="n">km</code><code class="o">$</code><code class="n">size</code>
<code class="p">[</code><code class="m">1</code><code class="p">]</code> <code class="m">106</code> <code class="m">186</code> <code class="m">285</code> <code class="m">288</code> <code class="m">266</code></pre>

<p>In <em>Python</em>, we can use the <code>collections.Counter</code> class from the standard library to get this information. Due to differences in the implementation and the inherent randomness of the algorithm, results will vary:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">collections</code> <code class="kn">import</code> <code class="n">Counter</code>
<code class="n">Counter</code><code class="p">(</code><code class="n">kmeans</code><code class="o">.</code><code class="n">labels_</code><code class="p">)</code>

<code class="n">Counter</code><code class="p">({</code><code class="mi">4</code><code class="p">:</code> <code class="mi">302</code><code class="p">,</code> <code class="mi">2</code><code class="p">:</code> <code class="mi">272</code><code class="p">,</code> <code class="mi">0</code><code class="p">:</code> <code class="mi">288</code><code class="p">,</code> <code class="mi">3</code><code class="p">:</code> <code class="mi">158</code><code class="p">,</code> <code class="mi">1</code><code class="p">:</code> <code class="mi">111</code><code class="p">})</code></pre>

<p>The cluster sizes are relatively balanced.
Imbalanced clusters can result from distant outliers, or from groups of records very distinct from the rest of the data—both may warrant further inspection.</p>

<p class="pagebreak-before">You can plot the centers of the clusters using the <code>gather</code> function in conjunction with <code>ggplot</code>:</p>

<pre data-type="programlisting" data-code-language="r"><code class="n">centers</code> <code class="o">&lt;-</code> <code class="nf">as.data.frame</code><code class="p">(</code><code class="nf">t</code><code class="p">(</code><code class="n">centers</code><code class="p">))</code>
<code class="nf">names</code><code class="p">(</code><code class="n">centers</code><code class="p">)</code> <code class="o">&lt;-</code> <code class="nf">paste</code><code class="p">(</code><code class="s">"Cluster"</code><code class="p">,</code> <code class="m">1</code><code class="o">:</code><code class="m">5</code><code class="p">)</code>
<code class="n">centers</code><code class="o">$</code><code class="n">Symbol</code> <code class="o">&lt;-</code> <code class="nf">row.names</code><code class="p">(</code><code class="n">centers</code><code class="p">)</code>
<code class="n">centers</code> <code class="o">&lt;-</code> <code class="nf">gather</code><code class="p">(</code><code class="n">centers</code><code class="p">,</code> <code class="s">'Cluster'</code><code class="p">,</code> <code class="s">'Mean'</code><code class="p">,</code> <code class="o">-</code><code class="n">Symbol</code><code class="p">)</code>
<code class="n">centers</code><code class="o">$</code><code class="n">Color</code> <code class="o">=</code> <code class="n">centers</code><code class="o">$</code><code class="n">Mean</code> <code class="o">&gt;</code> <code class="m">0</code>
<code class="nf">ggplot</code><code class="p">(</code><code class="n">centers</code><code class="p">,</code> <code class="nf">aes</code><code class="p">(</code><code class="n">x</code><code class="o">=</code><code class="n">Symbol</code><code class="p">,</code> <code class="n">y</code><code class="o">=</code><code class="n">Mean</code><code class="p">,</code> <code class="n">fill</code><code class="o">=</code><code class="n">Color</code><code class="p">))</code> <code class="o">+</code>
  <code class="nf">geom_bar</code><code class="p">(</code><code class="n">stat</code><code class="o">=</code><code class="s">'identity'</code><code class="p">,</code> <code class="n">position</code><code class="o">=</code><code class="s">'identity'</code><code class="p">,</code> <code class="n">width</code><code class="o">=</code><code class="m">.75</code><code class="p">)</code> <code class="o">+</code>
  <code class="nf">facet_grid</code><code class="p">(</code><code class="n">Cluster</code> <code class="o">~</code> <code class="n">.,</code> <code class="n">scales</code><code class="o">=</code><code class="s">'free_y'</code><code class="p">)</code></pre>

<p>The code to create this visualization in <em>Python</em> is similar to what we used for PCA:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">centers</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">kmeans</code><code class="o">.</code><code class="n">cluster_centers_</code><code class="p">,</code> <code class="n">columns</code><code class="o">=</code><code class="n">syms</code><code class="p">)</code>

<code class="n">f</code><code class="p">,</code> <code class="n">axes</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">(</code><code class="mi">5</code><code class="p">,</code> <code class="mi">1</code><code class="p">,</code> <code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">5</code><code class="p">,</code> <code class="mi">5</code><code class="p">),</code> <code class="n">sharex</code><code class="o">=</code><code class="bp">True</code><code class="p">)</code>
<code class="k">for</code> <code class="n">i</code><code class="p">,</code> <code class="n">ax</code> <code class="ow">in</code> <code class="nb">enumerate</code><code class="p">(</code><code class="n">axes</code><code class="p">):</code>
    <code class="n">center</code> <code class="o">=</code> <code class="n">centers</code><code class="o">.</code><code class="n">loc</code><code class="p">[</code><code class="n">i</code><code class="p">,</code> <code class="p">:]</code>
    <code class="n">maxPC</code> <code class="o">=</code> <code class="mf">1.01</code> <code class="o">*</code> <code class="n">np</code><code class="o">.</code><code class="n">max</code><code class="p">(</code><code class="n">np</code><code class="o">.</code><code class="n">max</code><code class="p">(</code><code class="n">np</code><code class="o">.</code><code class="n">abs</code><code class="p">(</code><code class="n">center</code><code class="p">)))</code>
    <code class="n">colors</code> <code class="o">=</code> <code class="p">[</code><code class="s1">'C0'</code> <code class="k">if</code> <code class="n">l</code> <code class="o">&gt;</code> <code class="mi">0</code> <code class="k">else</code> <code class="s1">'C1'</code> <code class="k">for</code> <code class="n">l</code> <code class="ow">in</code> <code class="n">center</code><code class="p">]</code>
    <code class="n">ax</code><code class="o">.</code><code class="n">axhline</code><code class="p">(</code><code class="n">color</code><code class="o">=</code><code class="s1">'#888888'</code><code class="p">)</code>
    <code class="n">center</code><code class="o">.</code><code class="n">plot</code><code class="o">.</code><code class="n">bar</code><code class="p">(</code><code class="n">ax</code><code class="o">=</code><code class="n">ax</code><code class="p">,</code> <code class="n">color</code><code class="o">=</code><code class="n">colors</code><code class="p">)</code>
    <code class="n">ax</code><code class="o">.</code><code class="n">set_ylabel</code><code class="p">(</code><code class="n">f</code><code class="s1">'Cluster {i + 1}'</code><code class="p">)</code>
    <code class="n">ax</code><code class="o">.</code><code class="n">set_ylim</code><code class="p">(</code><code class="o">-</code><code class="n">maxPC</code><code class="p">,</code> <code class="n">maxPC</code><code class="p">)</code></pre>

<p>The resulting <a data-type="indexterm" data-primary="cluster mean" id="idm46522835947848"/>plot is shown in  <a data-type="xref" href="#ClusterMeans">Figure 7-6</a> and reveals the nature of each cluster. For example, clusters 4 and 5 correspond to days on which the market is down and up, respectively. Clusters 2 and 3 are characterized by up-market days for consumer stocks and down-market days for energy stocks, respectively.
Finally, cluster 1 captures the days in which energy stocks were up and consumer stocks were down.</p>

<figure><div id="ClusterMeans" class="figure">
<img src="Images/psd2_0706.png" alt="The means of the clusters." width="1157" height="1455"/>
<h6><span class="label">Figure 7-6. </span>The means of the variables in each cluster (“cluster means”)</h6>
</div></figure>
<div data-type="note" epub:type="note"><h1>Cluster Analysis Versus PCA</h1>
<p>The plot of cluster means is similar in spirit to looking at the loadings for principal components analysis (PCA); see <a data-type="xref" href="#InterpretPCA">“Interpreting Principal Components”</a>.
A major distinction is that unlike with PCA, the sign of the cluster means is meaningful.<a data-type="indexterm" data-primary="cluster mean" data-secondary="PCA loadings versus" id="idm46522835793272"/><a data-type="indexterm" data-primary="loadings" data-secondary="cluster mean versus" id="idm46522835792328"/><a data-type="indexterm" data-primary="principal components analysis" data-secondary="cluster analysis versus" id="idm46522835791384"/>
PCA identifies principal directions of variation, whereas cluster analysis finds groups of records located near one another.<a data-type="indexterm" data-primary="unsupervised learning" data-secondary="K-means clustering" data-tertiary="interpreting the clusters" data-startref="ix_unslrnKMint" id="idm46522835790200"/><a data-type="indexterm" data-primary="clustering" data-secondary="K-means" data-tertiary="interpreting the clusters" data-startref="ix_clusKMintrp" id="idm46522835788744"/><a data-type="indexterm" data-primary="K-means clustering" data-secondary="interpreting the clusters" data-startref="ix_KMclusintrp" id="idm46522835787288"/></p>
</div>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Selecting the Number of Clusters"><div class="sect2" id="NumberOfClusters">
<h2>Selecting the Number of Clusters</h2>

<p>The <em>K</em>-means algorithm requires that you specify the number of clusters <em>K</em>.<a data-type="indexterm" data-primary="K (in K-means clustering)" id="idm46522835783288"/><a data-type="indexterm" data-primary="K-means clustering" data-secondary="selecting the number of clusters" id="idm46522835782488"/><a data-type="indexterm" data-primary="clustering" data-secondary="K-means" data-tertiary="selecting the number of clusters" id="idm46522835781528"/><a data-type="indexterm" data-primary="unsupervised learning" data-secondary="K-means clustering" data-tertiary="selecting the number of clusters" id="idm46522835780296"/>
Sometimes the number of clusters is driven by the application.
For example, a company managing a sales force might want to cluster customers into “personas” to focus and guide sales calls.
In such a case, managerial considerations would dictate the number of desired customer segments—for example, two might not yield useful differentiation of customers, while eight might be too many to manage.</p>

<p>In the absence of a cluster number dictated by practical or managerial considerations, a statistical approach could be used.
There is no single standard method to find the “best” number of clusters.</p>

<p>A common approach, called the <em>elbow method</em>, is to identify when the set of clusters explains “most” of the variance in the data.<a data-type="indexterm" data-primary="elbow method" id="idm46522835776968"/>
Adding new clusters beyond this set contributes relatively little in the variance explained.
The elbow is the point where the cumulative variance explained flattens out after rising steeply, hence the name of the method.</p>

<p><a data-type="xref" href="#KmeansElbowMethod">Figure 7-7</a> shows the cumulative percent of variance explained for the default data for the number of clusters ranging from 2 to 15.
Where is the elbow in this example?
There is no obvious candidate, since the incremental increase in variance explained drops gradually.
This is fairly typical in data that does not have well-defined clusters.
This is perhaps a drawback of the elbow method, but it does reveal the nature of the data.</p>

<figure><div id="KmeansElbowMethod" class="figure">
<img src="Images/psd2_0707.png" alt="The elbow method applied to the stock data." width="1157" height="847"/>
<h6><span class="label">Figure 7-7. </span>The elbow method applied to the stock data</h6>
</div></figure>

<p>In <em>R</em>, the <code>kmeans</code> function doesn’t provide a single command for applying the elbow method, but it can be readily applied from the output of <code>kmeans</code> as shown here:</p>

<pre data-type="programlisting" data-code-language="r"><code class="n">pct_var</code> <code class="o">&lt;-</code> <code class="nf">data.frame</code><code class="p">(</code><code class="n">pct_var</code> <code class="o">=</code> <code class="m">0</code><code class="p">,</code>
                      <code class="n">num_clusters</code> <code class="o">=</code> <code class="m">2</code><code class="o">:</code><code class="m">14</code><code class="p">)</code>
<code class="n">totalss</code> <code class="o">&lt;-</code> <code class="nf">kmeans</code><code class="p">(</code><code class="n">df</code><code class="p">,</code> <code class="n">centers</code><code class="o">=</code><code class="m">14</code><code class="p">,</code> <code class="n">nstart</code><code class="o">=</code><code class="m">50</code><code class="p">,</code> <code class="n">iter.max</code><code class="o">=</code><code class="m">100</code><code class="p">)</code><code class="o">$</code><code class="n">totss</code>
<code class="nf">for </code><code class="p">(</code><code class="n">i</code> <code class="n">in</code> <code class="m">2</code><code class="o">:</code><code class="m">14</code><code class="p">)</code> <code class="p">{</code>
  <code class="n">kmCluster</code> <code class="o">&lt;-</code> <code class="nf">kmeans</code><code class="p">(</code><code class="n">df</code><code class="p">,</code> <code class="n">centers</code><code class="o">=</code><code class="n">i</code><code class="p">,</code> <code class="n">nstart</code><code class="o">=</code><code class="m">50</code><code class="p">,</code> <code class="n">iter.max</code><code class="o">=</code><code class="m">100</code><code class="p">)</code>
  <code class="n">pct_var</code><code class="p">[</code><code class="n">i</code><code class="m">-1</code><code class="p">,</code> <code class="s">'pct_var'</code><code class="p">]</code> <code class="o">&lt;-</code> <code class="n">kmCluster</code><code class="o">$</code><code class="n">betweenss</code> <code class="o">/</code> <code class="n">totalss</code>
<code class="p">}</code></pre>

<p>For the <code>KMeans</code> result, we get this information from the property <code>inertia_</code>. After conversion into a <code>pandas</code> data frame, we can use its <code>plot</code> method to create the graph:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">inertia</code> <code class="o">=</code> <code class="p">[]</code>
<code class="k">for</code> <code class="n">n_clusters</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">2</code><code class="p">,</code> <code class="mi">14</code><code class="p">):</code>
    <code class="n">kmeans</code> <code class="o">=</code> <code class="n">KMeans</code><code class="p">(</code><code class="n">n_clusters</code><code class="o">=</code><code class="n">n_clusters</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">top_sp</code><code class="p">)</code>
    <code class="n">inertia</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">kmeans</code><code class="o">.</code><code class="n">inertia_</code> <code class="o">/</code> <code class="n">n_clusters</code><code class="p">)</code>

<code class="n">inertias</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">({</code><code class="s1">'n_clusters'</code><code class="p">:</code> <code class="nb">range</code><code class="p">(</code><code class="mi">2</code><code class="p">,</code> <code class="mi">14</code><code class="p">),</code> <code class="s1">'inertia'</code><code class="p">:</code> <code class="n">inertia</code><code class="p">})</code>
<code class="n">ax</code> <code class="o">=</code> <code class="n">inertias</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">x</code><code class="o">=</code><code class="s1">'n_clusters'</code><code class="p">,</code> <code class="n">y</code><code class="o">=</code><code class="s1">'inertia'</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">xlabel</code><code class="p">(</code><code class="s1">'Number of clusters(k)'</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">ylabel</code><code class="p">(</code><code class="s1">'Average Within-Cluster Squared Distances'</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">ylim</code><code class="p">((</code><code class="mi">0</code><code class="p">,</code> <code class="mf">1.1</code> <code class="o">*</code> <code class="n">inertias</code><code class="o">.</code><code class="n">inertia</code><code class="o">.</code><code class="n">max</code><code class="p">()))</code>
<code class="n">ax</code><code class="o">.</code><code class="n">legend</code><code class="p">()</code><code class="o">.</code><code class="n">set_visible</code><code class="p">(</code><code class="bp">False</code><code class="p">)</code></pre>

<p>In evaluating how many clusters to retain, perhaps the most important test is this:
how likely are the clusters to be replicated on new data?
Are the clusters interpretable, and do they relate to a general characteristic of the data, or do they just reflect a specific instance?
You can assess this, in part, using cross-validation; see <a data-type="xref" href="ch04.xhtml#CrossValidation">“Cross-Validation”</a>.</p>

<p>In general, there is no single rule that will reliably guide how many clusters to <span class="keep-together">produce</span>.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>There are several more formal ways to determine the number of clusters based on statistical or information theory.
For example, <a href="https://oreil.ly/d-N3_">Robert Tibshirani, Guenther Walther, and Trevor Hastie propose a “gap” statistic</a> based on statistical theory to identify the elbow.
For most applications, a theoretical approach is probably not necessary, or even appropriate.</p>
</div>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46522835493656">
<h5>Key Ideas</h5>
<ul>
<li>
<p>The number of desired clusters, <em>K</em>, is chosen by the user.</p>
</li>
<li>
<p>The algorithm develops clusters by iteratively assigning records to the nearest cluster mean until cluster assignments do not change.</p>
</li>
<li>
<p>Practical considerations usually dominate the choice of <em>K</em>; there is no statistically determined optimal number of clusters.<a data-type="indexterm" data-primary="clustering" data-secondary="K-means" data-startref="ix_clusKM" id="idm46522835488824"/><a data-type="indexterm" data-primary="unsupervised learning" data-secondary="K-means clustering" data-startref="ix_unslrnKM" id="idm46522835487576"/><a data-type="indexterm" data-primary="K-means clustering" data-startref="ix_KMclus" id="idm46522835486360"/></p>
</li>
</ul>
</div></aside>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Hierarchical Clustering"><div class="sect1" id="HierarchicalClustering">
<h1>Hierarchical Clustering</h1>

<p><em>Hierarchical clustering</em> is an alternative to <em>K</em>-means that can yield very different clusters.<a data-type="indexterm" data-primary="unsupervised learning" data-secondary="hierarchical clustering" id="ix_unslrnhier"/><a data-type="indexterm" data-primary="clustering" data-secondary="hierarchical" id="ix_clshier"/><a data-type="indexterm" data-primary="hierarchical clustering" id="ix_hiercl"/>
Hierarchical clustering allows the user to visualize the effect of specifying different numbers of clusters.
It is more sensitive in discovering outlying or aberrant groups or records.
Hierarchical clustering also lends itself to an intuitive graphical display, leading to easier  interpretation of the clusters.</p>
<aside data-type="sidebar" epub:type="sidebar" class="pagebreak-before less_space"><div class="sidebar" id="idm46522835478664">
<h5>Key Terms for Hierarchical Clustering</h5><dl>
<dt class="horizontal"><strong><em>Dendrogram</em></strong></dt>
<dd>
<p>A visual representation of the records and the hierarchy of clusters to which they belong.<a data-type="indexterm" data-primary="dendrograms" id="idm46522835475368"/></p>
</dd>
<dt class="horizontal"><strong><em>Distance</em></strong></dt>
<dd>
<p>A measure of how close one <em>record</em> is to another.</p>
</dd>
<dt class="horizontal"><strong><em>Dissimilarity</em></strong></dt>
<dd>
<p>A measure of how close one <em>cluster</em> is to another.</p>
</dd>
</dl>
</div></aside>

<p>Hierarchical clustering’s flexibility comes with a cost, and hierarchical clustering does not scale well to large data sets with millions of records.
For even modest-sized data with just tens of thousands of records,
hierarchical clustering can require intensive computing resources.
Indeed, most of the applications of hierarchical clustering are focused on relatively small data sets.</p>








<section data-type="sect2" data-pdf-bookmark="A Simple Example"><div class="sect2" id="idm46522835468920">
<h2>A Simple Example</h2>

<p>Hierarchical clustering works on a data set with <em>n</em> records and <em>p</em> variables and is based on <a data-type="indexterm" data-primary="hierarchical clustering" data-secondary="simple example" id="idm46522835466488"/><a data-type="indexterm" data-primary="clustering" data-secondary="hierarchical" data-tertiary="simple example" id="idm46522835465512"/><a data-type="indexterm" data-primary="unsupervised learning" data-secondary="hierarchical clustering" data-tertiary="simple example" id="idm46522835464296"/>two basic building blocks:</p>

<ul>
<li>
<p>A distance<a data-type="indexterm" data-primary="distance metrics" data-secondary="in hierarchical clustering" id="idm46522835461880"/> metric <math alttext="d Subscript i comma j">
  <msub><mi>d</mi> <mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow> </msub>
</math> to measure the distance between two records <em>i</em> and <em>j</em>.</p>
</li>
<li>
<p>A dissimilarity metric <math alttext="upper D Subscript upper A comma upper B">
  <msub><mi>D</mi> <mrow><mi>A</mi><mo>,</mo><mi>B</mi></mrow> </msub>
</math> to measure the difference between two clusters <em>A</em> and <em>B</em> based on the distances <math alttext="d Subscript i comma j">
  <msub><mi>d</mi> <mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow> </msub>
</math> between the members of each cluster.<a data-type="indexterm" data-primary="dissimilarity metrics" id="idm46522835449640"/></p>
</li>
</ul>

<p>For applications involving numeric data, the most importance choice is the dissimilarity metric.
Hierarchical clustering starts by setting each record as its own cluster and iterates to combine the least dissimilar clusters.</p>

<p>In <em>R</em>, the <code>hclust</code> function can be used to perform hierarchical clustering.
One big difference with <code>hclust</code> versus <code>kmeans</code> is that it operates on the pairwise distances <math alttext="d Subscript i comma j">
  <msub><mi>d</mi> <mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow> </msub>
</math> rather than the data itself.
You can compute these using the <code>dist</code> function.
For example, the following applies hierarchical clustering to the stock returns for a set of <span class="keep-together">companies</span>:</p>

<pre data-type="programlisting" data-code-language="r"><code class="n">syms1</code> <code class="o">&lt;-</code> <code class="nf">c</code><code class="p">(</code><code class="s">'GOOGL'</code><code class="p">,</code> <code class="s">'AMZN'</code><code class="p">,</code> <code class="s">'AAPL'</code><code class="p">,</code> <code class="s">'MSFT'</code><code class="p">,</code> <code class="s">'CSCO'</code><code class="p">,</code> <code class="s">'INTC'</code><code class="p">,</code> <code class="s">'CVX'</code><code class="p">,</code> <code class="s">'XOM'</code><code class="p">,</code> <code class="s">'SLB'</code><code class="p">,</code>
           <code class="s">'COP'</code><code class="p">,</code> <code class="s">'JPM'</code><code class="p">,</code> <code class="s">'WFC'</code><code class="p">,</code> <code class="s">'USB'</code><code class="p">,</code> <code class="s">'AXP'</code><code class="p">,</code> <code class="s">'WMT'</code><code class="p">,</code> <code class="s">'TGT'</code><code class="p">,</code> <code class="s">'HD'</code><code class="p">,</code> <code class="s">'COST'</code><code class="p">)</code>
<code class="c1"># take transpose: to cluster companies, we need the stocks along the rows</code>
<code class="n">df</code> <code class="o">&lt;-</code> <code class="nf">t</code><code class="p">(</code><code class="n">sp500_px</code><code class="nf">[row.names</code><code class="p">(</code><code class="n">sp500_px</code><code class="p">)</code> <code class="o">&gt;=</code> <code class="s">'2011-01-01'</code><code class="p">,</code> <code class="n">syms1</code><code class="p">])</code>
<code class="n">d</code> <code class="o">&lt;-</code> <code class="nf">dist</code><code class="p">(</code><code class="n">df</code><code class="p">)</code>
<code class="n">hcl</code> <code class="o">&lt;-</code> <code class="nf">hclust</code><code class="p">(</code><code class="n">d</code><code class="p">)</code></pre>

<p>Clustering algorithms will cluster the records (rows) of a data frame.
Since we want to cluster the companies, we need to <em>transpose</em> (<code>t</code>) the data frame and put the stocks along the rows and the dates along the columns.</p>

<p>The <code>scipy</code> package offers a number of different methods for hierarchical clustering in the <code>scipy.cluster.hierarchy</code> module. Here we use the <code>linkage</code> function with the “complete” method:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">syms1</code> <code class="o">=</code> <code class="p">[</code><code class="s1">'AAPL'</code><code class="p">,</code> <code class="s1">'AMZN'</code><code class="p">,</code> <code class="s1">'AXP'</code><code class="p">,</code> <code class="s1">'COP'</code><code class="p">,</code> <code class="s1">'COST'</code><code class="p">,</code> <code class="s1">'CSCO'</code><code class="p">,</code> <code class="s1">'CVX'</code><code class="p">,</code> <code class="s1">'GOOGL'</code><code class="p">,</code> <code class="s1">'HD'</code><code class="p">,</code>
         <code class="s1">'INTC'</code><code class="p">,</code> <code class="s1">'JPM'</code><code class="p">,</code> <code class="s1">'MSFT'</code><code class="p">,</code> <code class="s1">'SLB'</code><code class="p">,</code> <code class="s1">'TGT'</code><code class="p">,</code> <code class="s1">'USB'</code><code class="p">,</code> <code class="s1">'WFC'</code><code class="p">,</code> <code class="s1">'WMT'</code><code class="p">,</code> <code class="s1">'XOM'</code><code class="p">]</code>
<code class="n">df</code> <code class="o">=</code> <code class="n">sp500_px</code><code class="o">.</code><code class="n">loc</code><code class="p">[</code><code class="n">sp500_px</code><code class="o">.</code><code class="n">index</code> <code class="o">&gt;=</code> <code class="s1">'2011-01-01'</code><code class="p">,</code> <code class="n">syms1</code><code class="p">]</code><code class="o">.</code><code class="n">transpose</code><code class="p">()</code>

<code class="n">Z</code> <code class="o">=</code> <code class="n">linkage</code><code class="p">(</code><code class="n">df</code><code class="p">,</code> <code class="n">method</code><code class="o">=</code><code class="s1">'complete'</code><code class="p">)</code></pre>
</div></section>













<section data-type="sect2" data-pdf-bookmark="The Dendrogram"><div class="sect2" id="Dendrogram">
<h2>The Dendrogram</h2>

<p>Hierarchical clustering lends itself to a natural graphical display as a tree, referred to as a <em>dendrogram</em>.<a data-type="indexterm" data-primary="clustering" data-secondary="hierarchical" data-tertiary="representation in a dendrogram" id="ix_clshierdend"/><a data-type="indexterm" data-primary="unsupervised learning" data-secondary="hierarchical clustering" data-tertiary="representation in a dendrogram" id="ix_unslrnhierden"/><a data-type="indexterm" data-primary="dendrograms" id="ix_dndro"/><a data-type="indexterm" data-primary="hierarchical clustering" data-secondary="representation in dendrogram" id="ix_hiercldend"/>
The name comes from the Greek words <em>dendro</em> (tree) and <em>gramma</em> (drawing).
In <em>R</em>, you can easily produce this using the <code>plot</code> command:</p>

<pre data-type="programlisting" data-code-language="r"><code class="nf">plot</code><code class="p">(</code><code class="n">hcl</code><code class="p">)</code></pre>

<p>We can use the <code>dendrogram</code> method to plot the result of the <code>linkage</code> function in <em>Python</em>:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">fig</code><code class="p">,</code> <code class="n">ax</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">(</code><code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">5</code><code class="p">,</code> <code class="mi">5</code><code class="p">))</code>
<code class="n">dendrogram</code><code class="p">(</code><code class="n">Z</code><code class="p">,</code> <code class="n">labels</code><code class="o">=</code><code class="n">df</code><code class="o">.</code><code class="n">index</code><code class="p">,</code> <code class="n">ax</code><code class="o">=</code><code class="n">ax</code><code class="p">,</code> <code class="n">color_threshold</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">xticks</code><code class="p">(</code><code class="n">rotation</code><code class="o">=</code><code class="mi">90</code><code class="p">)</code>
<code class="n">ax</code><code class="o">.</code><code class="n">set_ylabel</code><code class="p">(</code><code class="s1">'distance'</code><code class="p">)</code></pre>

<p>The result is shown in <a data-type="xref" href="#DendogramStocks">Figure 7-8</a> (note that we are now plotting companies that are similar to one another, not days).
The leaves of the tree correspond to the records.
The length of the branch in the tree indicates the degree of dissimilarity between corresponding clusters.
The returns for Google and Amazon are quite dissimilar to one another and to the returns for the other stocks.
The oil stocks (SLB, CVX, XOM, COP) are in their own cluster, Apple (AAPL) is by itself, and the rest are similar to one another.</p>

<figure><div id="DendogramStocks" class="figure">
<img src="Images/psd2_0708.png" alt="A dendrogram of stocks." width="1110" height="1159"/>
<h6><span class="label">Figure 7-8. </span>A dendrogram of stocks</h6>
</div></figure>

<p>In contrast to <em>K</em>-means, it is not necessary to prespecify the number of clusters.
Graphically, you can identify different numbers of clusters with a horizontal line that slides up or down; a cluster is defined wherever the horizontal line intersects the vertical lines.
To extract a specific number of clusters, you can use the <code>cutree</code> function:</p>

<pre data-type="programlisting" data-code-language="r"><code class="nf">cutree</code><code class="p">(</code><code class="n">hcl</code><code class="p">,</code> <code class="n">k</code><code class="o">=</code><code class="m">4</code><code class="p">)</code>
<code class="n">GOOGL</code>  <code class="n">AMZN</code>  <code class="n">AAPL</code>  <code class="n">MSFT</code>  <code class="n">CSCO</code>  <code class="n">INTC</code>   <code class="n">CVX</code>   <code class="n">XOM</code>   <code class="n">SLB</code>   <code class="n">COP</code>   <code class="n">JPM</code>   <code class="n">WFC</code>
    <code class="m">1</code>     <code class="m">2</code>     <code class="m">3</code>     <code class="m">3</code>     <code class="m">3</code>     <code class="m">3</code>     <code class="m">4</code>     <code class="m">4</code>     <code class="m">4</code>     <code class="m">4</code>     <code class="m">3</code>     <code class="m">3</code>
  <code class="n">USB</code>   <code class="n">AXP</code>   <code class="n">WMT</code>   <code class="n">TGT</code>    <code class="n">HD</code>  <code class="n">COST</code>
    <code class="m">3</code>     <code class="m">3</code>     <code class="m">3</code>     <code class="m">3</code>     <code class="m">3</code>     <code class="m">3</code></pre>

<p class="pagebreak-before">In <em>Python</em>, you achieve the same with the <code>fcluster</code> method:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">memb</code> <code class="o">=</code> <code class="n">fcluster</code><code class="p">(</code><code class="n">Z</code><code class="p">,</code> <code class="mi">4</code><code class="p">,</code> <code class="n">criterion</code><code class="o">=</code><code class="s1">'maxclust'</code><code class="p">)</code>
<code class="n">memb</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">Series</code><code class="p">(</code><code class="n">memb</code><code class="p">,</code> <code class="n">index</code><code class="o">=</code><code class="n">df</code><code class="o">.</code><code class="n">index</code><code class="p">)</code>
<code class="k">for</code> <code class="n">key</code><code class="p">,</code> <code class="n">item</code> <code class="ow">in</code> <code class="n">memb</code><code class="o">.</code><code class="n">groupby</code><code class="p">(</code><code class="n">memb</code><code class="p">):</code>
    <code class="k">print</code><code class="p">(</code><code class="n">f</code><code class="s2">"{key} : {', '.join(item.index)}"</code><code class="p">)</code></pre>

<p>The number of clusters to extract is set to 4, and you can see that Google and Amazon each belong to their own cluster.
The oil stocks all belong to another cluster.
The remaining stocks are in the fourth cluster.<a data-type="indexterm" data-primary="clustering" data-secondary="hierarchical" data-tertiary="representation in a dendrogram" data-startref="ix_clshierdend" id="idm46522834975256"/><a data-type="indexterm" data-primary="unsupervised learning" data-secondary="hierarchical clustering" data-tertiary="representation in a dendrogram" data-startref="ix_unslrnhierden" id="idm46522834923192"/><a data-type="indexterm" data-primary="hierarchical clustering" data-secondary="representation in dendrogram" data-startref="ix_hiercldend" id="idm46522834921736"/><a data-type="indexterm" data-primary="dendrograms" data-startref="ix_dndro" id="idm46522834920552"/></p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="The Agglomerative Algorithm"><div class="sect2" id="AgglomerativeAlgorithm">
<h2>The Agglomerative Algorithm</h2>

<p>The main algorithm for hierarchical clustering is the <em>agglomerative</em> algorithm, which <a data-type="indexterm" data-primary="agglomerative algorithm" id="idm46522834917256"/><a data-type="indexterm" data-primary="hierarchical clustering" data-secondary="agglomerative algorithm" id="idm46522834916520"/><a data-type="indexterm" data-primary="clustering" data-secondary="hierarchical" data-tertiary="agglomerative algorithm" id="idm46522834915576"/><a data-type="indexterm" data-primary="unsupervised learning" data-secondary="hierarchical clustering" data-tertiary="agglomerative algorithm" id="idm46522834914360"/>iteratively merges similar clusters. The agglomerative algorithm begins with each record constituting its own single-record cluster and then builds up larger and larger clusters.  The first step is to calculate distances between all pairs of records.<a data-type="indexterm" data-primary="distance metrics" data-secondary="in hierarchical clustering" id="idm46522834912744"/></p>

<p>For each pair of records <math alttext="left-parenthesis x 1 comma x 2 comma ellipsis comma x Subscript p Baseline right-parenthesis">
  <mrow>
    <mo>(</mo>
    <msub><mi>x</mi> <mn>1</mn> </msub>
    <mo>,</mo>
    <msub><mi>x</mi> <mn>2</mn> </msub>
    <mo>,</mo>
    <mo>...</mo>
    <mo>,</mo>
    <msub><mi>x</mi> <mi>p</mi> </msub>
    <mo>)</mo>
  </mrow>
</math> and <math alttext="left-parenthesis y 1 comma y 2 comma ellipsis comma y Subscript p Baseline right-parenthesis">
  <mrow>
    <mo>(</mo>
    <msub><mi>y</mi> <mn>1</mn> </msub>
    <mo>,</mo>
    <msub><mi>y</mi> <mn>2</mn> </msub>
    <mo>,</mo>
    <mo>...</mo>
    <mo>,</mo>
    <msub><mi>y</mi> <mi>p</mi> </msub>
    <mo>)</mo>
  </mrow>
</math>, we measure the distance between the two records,  <math alttext="d Subscript x comma y">
  <msub><mi>d</mi> <mrow><mi>x</mi><mo>,</mo><mi>y</mi></mrow> </msub>
</math>, using a distance metric (see <a data-type="xref" href="ch06.xhtml#DistanceMetrics">“Distance Metrics”</a>).
For example, we can use<a data-type="indexterm" data-primary="Euclidean distance" id="idm46522834892200"/> Euclidian distance:</p>
<div data-type="equation">
<math display="block">
  <mrow>
    <mi>d</mi>
    <mrow>
      <mo>(</mo>
      <mi>x</mi>
      <mo>,</mo>
      <mi>y</mi>
      <mo>)</mo>
    </mrow>
    <mo>=</mo>
    <msqrt>
      <mrow>
        <msup><mrow><mo>(</mo><msub><mi>x</mi> <mn>1</mn> </msub><mo>-</mo><msub><mi>y</mi> <mn>1</mn> </msub><mo>)</mo></mrow> <mn>2</mn> </msup>
        <mo>+</mo>
        <msup><mrow><mo>(</mo><msub><mi>x</mi> <mn>2</mn> </msub><mo>-</mo><msub><mi>y</mi> <mn>2</mn> </msub><mo>)</mo></mrow> <mn>2</mn> </msup>
        <mo>+</mo>
        <mo>⋯</mo>
        <mo>+</mo>
        <msup><mrow><mo>(</mo><msub><mi>x</mi> <mi>p</mi> </msub><mo>-</mo><msub><mi>y</mi> <mi>p</mi> </msub><mo>)</mo></mrow> <mn>2</mn> </msup>
      </mrow>
    </msqrt>
  </mrow>
</math>
</div>

<p>We now turn to inter-cluster distance.
Consider two clusters <em>A</em> and <em>B</em>, each with a distinctive set of records, <math alttext="upper A equals left-parenthesis a 1 comma a 2 comma ellipsis comma a Subscript m Baseline right-parenthesis">
  <mrow>
    <mi>A</mi>
    <mo>=</mo>
    <mo>(</mo>
    <msub><mi>a</mi> <mn>1</mn> </msub>
    <mo>,</mo>
    <msub><mi>a</mi> <mn>2</mn> </msub>
    <mo>,</mo>
    <mo>...</mo>
    <mo>,</mo>
    <msub><mi>a</mi> <mi>m</mi> </msub>
    <mo>)</mo>
  </mrow>
</math> and <math alttext="upper B equals left-parenthesis b 1 comma b 2 comma ellipsis comma b Subscript q Baseline right-parenthesis">
  <mrow>
    <mi>B</mi>
    <mo>=</mo>
    <mo>(</mo>
    <msub><mi>b</mi> <mn>1</mn> </msub>
    <mo>,</mo>
    <msub><mi>b</mi> <mn>2</mn> </msub>
    <mo>,</mo>
    <mo>...</mo>
    <mo>,</mo>
    <msub><mi>b</mi> <mi>q</mi> </msub>
    <mo>)</mo>
  </mrow>
</math>.
We can measure the dissimilarity between the clusters <math alttext="upper D left-parenthesis upper A comma upper B right-parenthesis">
  <mrow>
    <mi>D</mi>
    <mo>(</mo>
    <mi>A</mi>
    <mo>,</mo>
    <mi>B</mi>
    <mo>)</mo>
  </mrow>
</math> by using the distances between the members of <em>A</em> and the members of <em>B</em>.</p>

<p>One measure<a data-type="indexterm" data-primary="complete-linkage method" id="idm46522834821688"/><a data-type="indexterm" data-primary="dissimilarity metrics" data-secondary="complete-linkage method" id="idm46522834820952"/> of dissimilarity is the <em>complete-linkage</em> method, which is the maximum distance across all pairs of records between <em>A</em> and <em>B</em>:</p>
<div data-type="equation">
<math display="block">
  <mrow>
    <mi>D</mi>
    <mrow>
      <mo>(</mo>
      <mi>A</mi>
      <mo>,</mo>
      <mi>B</mi>
      <mo>)</mo>
    </mrow>
    <mo>=</mo>
    <mo movablelimits="true" form="prefix">max</mo>
    <mi>d</mi>
    <mrow>
      <mo>(</mo>
      <msub><mi>a</mi> <mi>i</mi> </msub>
      <mo>,</mo>
      <msub><mi>b</mi> <mi>j</mi> </msub>
      <mo>)</mo>
    </mrow>
    <mspace width="4.pt"/>
    <mtext>for</mtext>
    <mspace width="4.pt"/>
    <mtext>all</mtext>
    <mspace width="4.pt"/>
    <mtext>pairs</mtext>
    <mspace width="4.pt"/>
    <mi>i</mi>
    <mo>,</mo>
    <mi>j</mi>
  </mrow>
</math>
</div>

<p>This defines the dissimilarity as the biggest difference between all pairs.</p>

<p class="pagebreak-before">The main steps of the agglomerative algorithm are:</p>
<ol>
<li>
<p>Create an initial set of clusters with each cluster consisting of a single record for all records in the data.</p>
</li>
<li>
<p>Compute the dissimilarity <math alttext="upper D left-parenthesis upper C Subscript k Baseline comma upper C Subscript script l Baseline right-parenthesis">
  <mrow>
    <mi>D</mi>
    <mo>(</mo>
    <msub><mi>C</mi> <mi>k</mi> </msub>
    <mo>,</mo>
    <msub><mi>C</mi> <mi>ℓ</mi> </msub>
    <mo>)</mo>
  </mrow>
</math> between all pairs of clusters <math alttext="k comma script l">
  <mrow>
    <mi>k</mi>
    <mo>,</mo>
    <mi>ℓ</mi>
  </mrow>
</math>.</p>
</li>
<li>
<p>Merge the two clusters <math alttext="upper C Subscript k">
  <msub><mi>C</mi> <mi>k</mi> </msub>
</math> and <math alttext="upper C Subscript script l">
  <msub><mi>C</mi> <mi>ℓ</mi> </msub>
</math> that are least dissimilar as measured by <math alttext="upper D left-parenthesis upper C Subscript k Baseline comma upper C Subscript script l Baseline right-parenthesis">
  <mrow>
    <mi>D</mi>
    <mo>(</mo>
    <msub><mi>C</mi> <mi>k</mi> </msub>
    <mo>,</mo>
    <msub><mi>C</mi> <mi>ℓ</mi> </msub>
    <mo>)</mo>
  </mrow>
</math>.</p>
</li>
<li>
<p>If we have more than one cluster remaining, return to step 2. Otherwise, we are done.</p>
</li>

</ol>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Measures of Dissimilarity"><div class="sect2" id="idm46522834919016">
<h2>Measures of Dissimilarity</h2>

<p>There are four common measures of dissimilarity: <em>complete linkage</em>, <em>single linkage</em>, <em>average linkage</em>, and <em>minimum variance</em>.<a data-type="indexterm" data-primary="unsupervised learning" data-secondary="hierarchical clustering" data-tertiary="dissimilarity metrics" id="ix_unslrnhierdiss"/><a data-type="indexterm" data-primary="clustering" data-secondary="hierarchical" data-tertiary="dissimilarity metrics" id="ix_clshierdiss"/><a data-type="indexterm" data-primary="hierarchical clustering" data-secondary="dissimilarity metrics" id="ix_hiercldiss"/><a data-type="indexterm" data-primary="dissimilarity metrics" id="ix_dissim"/>
These (plus other measures) are all supported by most hierarchical clustering software, including <code>hclust</code> and <code>linkage</code>.<a data-type="indexterm" data-primary="complete-linkage method" id="idm46522834769736"/><a data-type="indexterm" data-primary="average linkage metric" id="idm46522834769000"/><a data-type="indexterm" data-primary="single linkage metric" id="idm46522834768328"/><a data-type="indexterm" data-primary="minimum variance metric" id="idm46522834767656"/>
The complete linkage method defined earlier tends to produce clusters with members that are similar.
The single linkage method is the minimum distance between the records in two clusters:</p>
<div data-type="equation">
<math display="block">
  <mrow>
    <mi>D</mi>
    <mrow>
      <mo>(</mo>
      <mi>A</mi>
      <mo>,</mo>
      <mi>B</mi>
      <mo>)</mo>
    </mrow>
    <mo>=</mo>
    <mo movablelimits="true" form="prefix">min</mo>
    <mi>d</mi>
    <mrow>
      <mo>(</mo>
      <msub><mi>a</mi> <mi>i</mi> </msub>
      <mo>,</mo>
      <msub><mi>b</mi> <mi>j</mi> </msub>
      <mo>)</mo>
    </mrow>
    <mspace width="4.pt"/>
    <mtext>for</mtext>
    <mspace width="4.pt"/>
    <mtext>all</mtext>
    <mspace width="4.pt"/>
    <mtext>pairs</mtext>
    <mspace width="4.pt"/>
    <mi>i</mi>
    <mo>,</mo>
    <mi>j</mi>
  </mrow>
</math>
</div>

<p>This is a “greedy” method and produces clusters that can contain quite disparate elements.
The average linkage method is the average of all distance pairs and represents a compromise between the single and complete linkage methods.
Finally, the minimum variance method, also referred to as <em>Ward’s</em> method, is similar to <em>K</em>-means since it minimizes the within-cluster sum of squares (see <a data-type="xref" href="#Kmeans">“K-Means Clustering”</a>).</p>

<p><a data-type="xref" href="#DissimilarityMeasures">Figure 7-9</a> applies hierarchical clustering using the four measures to the ExxonMobil and Chevron stock returns.
For each measure, four clusters are retained.</p>

<figure><div id="DissimilarityMeasures" class="figure">
<img src="Images/psd2_0709.png" alt="A comparison of measures of dissimilarity applied to stock returns; ExxonMobil on the x-axis and Chevron on the y-axis." width="1584" height="1147"/>
<h6><span class="label">Figure 7-9. </span>A comparison of measures of dissimilarity applied to stock data</h6>
</div></figure>

<p>The results are strikingly different: the single linkage measure assigns almost all of the points to a single cluster.
Except for the minimum variance method (<em>R</em>: <code>Ward.D</code>; <em>Python</em>: <code>ward</code>), all measures end up with at least one cluster with just a few outlying points.
The minimum variance method is most similar to the <em>K</em>-means cluster; compare with <a data-type="xref" href="#KmeansStockData">Figure 7-5</a>.<a data-type="indexterm" data-primary="unsupervised learning" data-secondary="hierarchical clustering" data-tertiary="dissimilarity metrics" data-startref="ix_unslrnhierdiss" id="idm46522834740760"/><a data-type="indexterm" data-primary="clustering" data-secondary="hierarchical" data-tertiary="dissimilarity metrics" data-startref="ix_clshierdiss" id="idm46522834739240"/><a data-type="indexterm" data-primary="hierarchical clustering" data-secondary="dissimilarity metrics" data-startref="ix_hiercldiss" id="idm46522834737752"/><a data-type="indexterm" data-primary="dissimilarity metrics" data-startref="ix_dissim" id="idm46522834736536"/></p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46522834735592">
<h5>Key Ideas</h5>
<ul>
<li>
<p>Hierarchical clustering starts with every record in its own cluster.</p>
</li>
<li>
<p>Progressively, clusters are joined to nearby clusters until all records belong to a single cluster (the agglomerative algorithm).</p>
</li>
<li>
<p>The agglomeration history is retained and plotted, and the user (without specifying the number of clusters beforehand) can visualize the number and structure of clusters at different stages.</p>
</li>
<li>
<p>Inter-cluster distances are computed in different ways, all relying on the set of all inter-record distances.<a data-type="indexterm" data-primary="unsupervised learning" data-secondary="hierarchical clustering" data-startref="ix_unslrnhier" id="idm46522834730536"/><a data-type="indexterm" data-primary="clustering" data-secondary="hierarchical" data-startref="ix_clshier" id="idm46522834729288"/><a data-type="indexterm" data-primary="hierarchical clustering" data-startref="ix_hiercl" id="idm46522834728072"/></p>
</li>
</ul>
</div></aside>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Model-Based Clustering"><div class="sect1" id="idm46522834726616">
<h1>Model-Based Clustering</h1>

<p>Clustering methods such as hierarchical clustering and <em>K</em>-means are based on heuristics <a data-type="indexterm" data-primary="unsupervised learning" data-secondary="model-based clustering" id="ix_unslrnMBC"/><a data-type="indexterm" data-primary="clustering" data-secondary="model-based" id="ix_clsmod"/><a data-type="indexterm" data-primary="model-based clustering" id="ix_modcl"/>and rely primarily on finding clusters whose members are close to one another, as measured directly with the data (no probability model involved).
In the past 20 years, significant effort has been devoted to developing <em>model-based clustering</em> methods.
Adrian Raftery and other researchers at the University of Washington made critical contributions to model-based clustering, including both theory and software.
The techniques are grounded in statistical theory and provide more rigorous ways to determine the nature and number of clusters.
They could be used, for example, in cases where there might be one group of records that are similar to one another but not necessarily close to one another (e.g., tech stocks with high variance of returns), and another group of records that are similar and also close (e.g., utility stocks with low variance).</p>








<section data-type="sect2" data-pdf-bookmark="Multivariate Normal Distribution"><div class="sect2" id="MultivariateNormal">
<h2>Multivariate Normal Distribution</h2>

<p>The most widely used model-based clustering methods rest on the <em>multivariate normal</em> distribution.<a data-type="indexterm" data-primary="multivariate normal distribution" id="idm46522834717000"/><a data-type="indexterm" data-primary="normal distribution" data-secondary="multivariate" id="idm46522834716328"/><a data-type="indexterm" data-primary="clustering" data-secondary="model-based" data-tertiary="multivariate normal distribution" id="idm46522834715352"/><a data-type="indexterm" data-primary="model-based clustering" data-secondary="multivariate normal distribution" id="idm46522834714168"/><a data-type="indexterm" data-primary="unsupervised learning" data-secondary="model-based clustering" data-tertiary="multivariate normal distribution" id="idm46522834713256"/>
The multivariate normal distribution is a generalization of the normal distribution to a set of <em>p</em> variables <math alttext="upper X 1 comma upper X 2 comma ellipsis comma upper X Subscript p Baseline">
  <mrow>
    <msub><mi>X</mi> <mn>1</mn> </msub>
    <mo>,</mo>
    <msub><mi>X</mi> <mn>2</mn> </msub>
    <mo>,</mo>
    <mo>...</mo>
    <mo>,</mo>
    <msub><mi>X</mi> <mi>p</mi> </msub>
  </mrow>
</math>.
The distribution is defined by a set of means <math alttext="mu bold equals mu bold 1 bold comma mu bold 2 bold comma ellipsis bold comma mu Subscript bold p Baseline">
  <mrow>
    <mi>μ</mi>
    <mo>=</mo>
    <msub><mi>μ</mi> <mn mathvariant="bold">1</mn> </msub>
    <mo>,</mo>
    <msub><mi>μ</mi> <mn mathvariant="bold">2</mn> </msub>
    <mo>,</mo>
    <mo>...</mo>
    <mo>,</mo>
    <msub><mi>μ</mi> <mi>𝐩</mi> </msub>
  </mrow>
</math> and a covariance matrix <math alttext="normal upper Sigma">
  <mi>Σ</mi>
</math>.
The covariance matrix is a measure of how the variables correlate with each other (see <a data-type="xref" href="ch05.xhtml#Covariance">“Covariance Matrix”</a> for details on the covariance).
The covariance matrix <math alttext="normal upper Sigma">
  <mi>Σ</mi>
</math> consists of <em>p</em> variances <math alttext="sigma 1 squared comma sigma 2 squared comma ellipsis comma sigma Subscript p Superscript 2">
  <mrow>
    <msubsup><mi>σ</mi> <mn>1</mn> <mn>2</mn> </msubsup>
    <mo>,</mo>
    <msubsup><mi>σ</mi> <mn>2</mn> <mn>2</mn> </msubsup>
    <mo>,</mo>
    <mo>...</mo>
    <mo>,</mo>
    <msubsup><mi>σ</mi> <mi>p</mi> <mn>2</mn> </msubsup>
  </mrow>
</math> and covariances <math alttext="sigma Subscript i comma j">
  <msub><mi>σ</mi> <mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow> </msub>
</math> for all pairs of variables <math alttext="i not-equals j">
  <mrow>
    <mi>i</mi>
    <mo>≠</mo>
    <mi>j</mi>
  </mrow>
</math>.
With the variables put along the rows and duplicated along the columns, the matrix looks like this:</p>
<div data-type="equation">
<math display="block">
  <mrow>
    <mi>Σ</mi>
    <mo>=</mo>
    <mfenced open="[" close="]">
      <mtable>
        <mtr>
          <mtd>
            <msubsup><mi>σ</mi> <mn>1</mn> <mn>2</mn> </msubsup>
          </mtd>
          <mtd>
            <msub><mi>σ</mi> <mrow><mn>1</mn><mo>,</mo><mn>2</mn></mrow> </msub>
          </mtd>
          <mtd>
            <mo>⋯</mo>
          </mtd>
          <mtd>
            <msub><mi>σ</mi> <mrow><mn>1</mn><mo>,</mo><mi>p</mi></mrow> </msub>
          </mtd>
        </mtr>
        <mtr>
          <mtd>
            <msub><mi>σ</mi> <mrow><mn>2</mn><mo>,</mo><mn>1</mn></mrow> </msub>
          </mtd>
          <mtd>
            <msubsup><mi>σ</mi> <mrow><mn>2</mn></mrow> <mn>2</mn> </msubsup>
          </mtd>
          <mtd>
            <mo>⋯</mo>
          </mtd>
          <mtd>
            <msub><mi>σ</mi> <mrow><mn>2</mn><mo>,</mo><mi>p</mi></mrow> </msub>
          </mtd>
        </mtr>
        <mtr>
          <mtd>
            <mo>⋮</mo>
          </mtd>
          <mtd>
            <mo>⋮</mo>
          </mtd>
          <mtd>
            <mo>⋱</mo>
          </mtd>
          <mtd>
            <mo>⋮</mo>
          </mtd>
        </mtr>
        <mtr>
          <mtd>
            <msub><mi>σ</mi> <mrow><mi>p</mi><mo>,</mo><mn>1</mn></mrow> </msub>
          </mtd>
          <mtd>
            <msubsup><mi>σ</mi> <mrow><mi>p</mi><mo>,</mo><mn>2</mn></mrow> <mn>2</mn> </msubsup>
          </mtd>
          <mtd>
            <mo>⋯</mo>
          </mtd>
          <mtd>
            <msubsup><mi>σ</mi> <mrow><mi>p</mi></mrow> <mn>2</mn> </msubsup>
          </mtd>
        </mtr>
      </mtable>
    </mfenced>
  </mrow>
</math>
</div>

<p>Note that <a data-type="indexterm" data-primary="covariance matrix" id="idm46522834645384"/>the covariance matrix is symmetric around the diagonal from upper left to lower right.  Since <math alttext="sigma Subscript i comma j Baseline equals sigma Subscript j comma i">
  <mrow>
    <msub><mi>σ</mi> <mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow> </msub>
    <mo>=</mo>
    <msub><mi>σ</mi> <mrow><mi>j</mi><mo>,</mo><mi>i</mi></mrow> </msub>
  </mrow>
</math>, there are only <math alttext="left-parenthesis p times left-parenthesis p minus 1 right-parenthesis right-parenthesis slash 2">
  <mrow>
    <mo>(</mo>
    <mi>p</mi>
    <mo>×</mo>
    <mo>(</mo>
    <mi>p</mi>
    <mo>-</mo>
    <mn>1</mn>
    <mo>)</mo>
    <mo>)</mo>
    <mo>/</mo>
    <mn>2</mn>
  </mrow>
</math> covariance terms.
In total, the covariance matrix has <math alttext="left-parenthesis p times left-parenthesis p minus 1 right-parenthesis right-parenthesis slash 2 plus p">
  <mrow>
    <mo>(</mo>
    <mi>p</mi>
    <mo>×</mo>
    <mo>(</mo>
    <mi>p</mi>
    <mo>-</mo>
    <mn>1</mn>
    <mo>)</mo>
    <mo>)</mo>
    <mo>/</mo>
    <mn>2</mn>
    <mo>+</mo>
    <mi>p</mi>
  </mrow>
</math> parameters.
The distribution is denoted by:</p>
<div data-type="equation">
<math display="block">
  <mrow>
    <mrow>
      <mo>(</mo>
      <msub><mi>X</mi> <mn>1</mn> </msub>
      <mo>,</mo>
      <msub><mi>X</mi> <mn>2</mn> </msub>
      <mo>,</mo>
      <mo>...</mo>
      <mo>,</mo>
      <msub><mi>X</mi> <mi>p</mi> </msub>
      <mo>)</mo>
    </mrow>
    <mo>∼</mo>
    <msub><mi>N</mi> <mi>p</mi> </msub>
    <mrow>
      <mo>(</mo>
      <mi>μ</mi>
      <mo>,</mo>
      <mi>Σ</mi>
      <mo>)</mo>
    </mrow>
  </mrow>
</math>
</div>

<p>This is a symbolic way of saying that the variables are all normally distributed, and the overall distribution is fully described by the vector of variable means and the covariance matrix.</p>

<p><a data-type="xref" href="#Normal2d">Figure 7-10</a> shows the probability contours for a multivariate normal distribution for two variables <em>X</em> and <em>Y</em> (the 0.5 probability contour, for example, contains 50% of the distribution).</p>

<p>The means are <math>
  <mrow>
    <msub><mi>μ</mi> <mi>x</mi> </msub>
    <mo>=</mo>
    <mn>0</mn><mspace width="-.15em"/>
    <mo>.</mo><mspace width="-.15em"/>
    <mn>5</mn>
  </mrow>
</math> and <math>
  <mrow>
    <msub><mi>μ</mi> <mi>y</mi> </msub>
    <mo>=</mo>
    <mo>-</mo>
    <mn>0</mn><mspace width="-.15em"/>
    <mo>.</mo><mspace width="-.15em"/>
    <mn>5</mn>
  </mrow>
</math>, and the covariance matrix is:</p>
<div data-type="equation">
<math display="block">
  <mrow>
    <mi>Σ</mi>
    <mo>=</mo>
    <mfenced separators="" open="[" close="]">
      <mtable>
        <mtr>
          <mtd>
            <mn>1</mn>
          </mtd>
          <mtd>
            <mn>1</mn>
          </mtd>
        </mtr>
        <mtr>
          <mtd>
            <mn>1</mn>
          </mtd>
          <mtd>
            <mn>2</mn>
          </mtd>
        </mtr>
      </mtable>
    </mfenced>
  </mrow>
</math>
</div>

<p>Since the covariance <math alttext="sigma Subscript x y">
  <msub><mi>σ</mi> <mrow><mi>x</mi><mi>y</mi></mrow> </msub>
</math> is positive, <em>X</em> and <em>Y</em> are positively correlated.</p>

<figure class="width-75"><div id="Normal2d" class="figure">
<img src="Images/psd2_0710.png" alt="images/2d_normal.png" width="1130" height="1146"/>
<h6><span class="label">Figure 7-10. </span>Probability contours for a two-dimensional normal distribution</h6>
</div></figure>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Mixtures of Normals"><div class="sect2" id="GaussianMixture">
<h2>Mixtures of Normals</h2>

<p>The key idea behind model-based clustering is that each record is assumed to be distributed as one of <em>K</em> multivariate normal distributions, where <em>K</em> is the number of <span class="keep-together">clusters</span>.<a data-type="indexterm" data-primary="multivariate normal distribution" data-secondary="mixtures of normals" id="ix_mulvarnrm"/><a data-type="indexterm" data-primary="unsupervised learning" data-secondary="model-based clustering" data-tertiary="mixtures of normals" id="ix_unslrnMBCnrm"/><a data-type="indexterm" data-primary="clustering" data-secondary="model-based" data-tertiary="mixtures of normals" id="ix_clsmodnrml"/><a data-type="indexterm" data-primary="model-based clustering" data-secondary="mixtures of normals" id="ix_modclnrml"/>
Each distribution has a different mean <math alttext="mu">
  <mi>μ</mi>
</math> and covariance matrix <math alttext="normal upper Sigma">
  <mi>Σ</mi>
</math>.
For <span class="keep-together">example</span>, if you have two variables, <em>X</em> and <em>Y</em>, then each row
<math alttext="left-parenthesis upper X Subscript i Baseline comma upper Y Subscript i Baseline right-parenthesis">
  <mrow>
    <mo>(</mo>
    <msub><mi>X</mi> <mi>i</mi> </msub>
    <mo>,</mo>
    <msub><mi>Y</mi> <mi>i</mi> </msub>
    <mo>)</mo>
  </mrow>
</math> is modeled as having <span class="keep-together">been sampled</span> from one of <em>K</em> multivariate normal distributions <math alttext="upper N left-parenthesis mu 1 comma normal upper Sigma 1 right-parenthesis comma upper N left-parenthesis mu 2 comma normal upper Sigma 2 right-parenthesis comma ellipsis comma upper N left-parenthesis mu Subscript upper K Baseline comma normal upper Sigma Subscript upper K Baseline right-parenthesis">
  <mrow>
    <mi>N</mi>
    <mrow>
      <mo>(</mo>
      <msub><mi>μ</mi> <mn>1</mn> </msub>
      <mo>,</mo>
      <msub><mi>Σ</mi> <mn>1</mn> </msub>
      <mo>)</mo>
    </mrow>
    <mo>,</mo>
    <mi>N</mi>
    <mrow>
      <mo>(</mo>
      <msub><mi>μ</mi> <mn>2</mn> </msub>
      <mo>,</mo>
      <msub><mi>Σ</mi> <mn>2</mn> </msub>
      <mo>)</mo>
    </mrow>
    <mo>,</mo>
    <mo>...</mo>
    <mo>,</mo>
    <mi>N</mi>
    <mrow>
      <mo>(</mo>
      <msub><mi>μ</mi> <mi>K</mi> </msub>
      <mo>,</mo>
      <msub><mi>Σ</mi> <mi>K</mi> </msub>
      <mo>)</mo>
    </mrow>
  </mrow>
</math>.</p>

<p><em>R</em> has a very rich package for model-based clustering called <code>mclust</code>, originally developed by Chris Fraley and Adrian Raftery.
With this package, we can apply model-based clustering to the stock return data we previously analyzed using <em>K</em>-means and hierarchical clustering:</p>

<pre data-type="programlisting" data-code-language="r"><code class="o">&gt;</code> <code class="nf">library</code><code class="p">(</code><code class="n">mclust</code><code class="p">)</code>
<code class="o">&gt;</code> <code class="n">df</code> <code class="o">&lt;-</code> <code class="n">sp500_px</code><code class="nf">[row.names</code><code class="p">(</code><code class="n">sp500_px</code><code class="p">)</code> <code class="o">&gt;=</code> <code class="s">'2011-01-01'</code><code class="p">,</code> <code class="nf">c</code><code class="p">(</code><code class="s">'XOM'</code><code class="p">,</code> <code class="s">'CVX'</code><code class="p">)]</code>
<code class="o">&gt;</code> <code class="n">mcl</code> <code class="o">&lt;-</code> <code class="nf">Mclust</code><code class="p">(</code><code class="n">df</code><code class="p">)</code>
<code class="o">&gt;</code> <code class="nf">summary</code><code class="p">(</code><code class="n">mcl</code><code class="p">)</code>
<code class="n">Mclust</code> <code class="nf">VEE </code><code class="p">(</code><code class="n">ellipsoidal</code><code class="p">,</code> <code class="n">equal</code> <code class="n">shape</code> <code class="n">and</code> <code class="n">orientation</code><code class="p">)</code> <code class="n">model</code> <code class="n">with</code> <code class="m">2</code> <code class="n">components</code><code class="o">:</code>


 <code class="n">log.likelihood</code>    <code class="n">n</code> <code class="n">df</code>       <code class="n">BIC</code>       <code class="n">ICL</code>
      <code class="m">-2255.134</code> <code class="m">1131</code>  <code class="m">9</code> <code class="m">-4573.546</code> <code class="m">-5076.856</code>

<code class="n">Clustering</code> <code class="n">table</code><code class="o">:</code>
  <code class="m">1</code>   <code class="m">2</code>
<code class="m">963</code> <code class="m">168</code></pre>

<p><code>scikit-learn</code> has the <code>sklearn.mixture.GaussianMixture</code> class for model-based clustering:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">df</code> <code class="o">=</code> <code class="n">sp500_px</code><code class="o">.</code><code class="n">loc</code><code class="p">[</code><code class="n">sp500_px</code><code class="o">.</code><code class="n">index</code> <code class="o">&gt;=</code> <code class="s1">'2011-01-01'</code><code class="p">,</code> <code class="p">[</code><code class="s1">'XOM'</code><code class="p">,</code> <code class="s1">'CVX'</code><code class="p">]]</code>
<code class="n">mclust</code> <code class="o">=</code> <code class="n">GaussianMixture</code><code class="p">(</code><code class="n">n_components</code><code class="o">=</code><code class="mi">2</code><code class="p">)</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">df</code><code class="p">)</code>
<code class="n">mclust</code><code class="o">.</code><code class="n">bic</code><code class="p">(</code><code class="n">df</code><code class="p">)</code></pre>

<p>If you execute this code, you will notice that the computation takes significantly longer than other procedures.
Extracting the cluster assignments using the <code>predict</code> function, we can visualize the clusters:</p>

<pre data-type="programlisting" data-code-language="r"><code class="n">cluster</code> <code class="o">&lt;-</code> <code class="nf">factor</code><code class="p">(</code><code class="nf">predict</code><code class="p">(</code><code class="n">mcl</code><code class="p">)</code><code class="o">$</code><code class="n">classification</code><code class="p">)</code>
<code class="nf">ggplot</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="n">df</code><code class="p">,</code> <code class="nf">aes</code><code class="p">(</code><code class="n">x</code><code class="o">=</code><code class="n">XOM</code><code class="p">,</code> <code class="n">y</code><code class="o">=</code><code class="n">CVX</code><code class="p">,</code> <code class="n">color</code><code class="o">=</code><code class="n">cluster</code><code class="p">,</code> <code class="n">shape</code><code class="o">=</code><code class="n">cluster</code><code class="p">))</code> <code class="o">+</code>
  <code class="nf">geom_point</code><code class="p">(</code><code class="n">alpha</code><code class="o">=</code><code class="m">.8</code><code class="p">)</code></pre>

<p>Here is the <em>Python</em> code to create a similar figure:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">fig</code><code class="p">,</code> <code class="n">ax</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">(</code><code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">4</code><code class="p">,</code> <code class="mi">4</code><code class="p">))</code>
<code class="n">colors</code> <code class="o">=</code> <code class="p">[</code><code class="n">f</code><code class="s1">'C{c}'</code> <code class="k">for</code> <code class="n">c</code> <code class="ow">in</code> <code class="n">mclust</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">df</code><code class="p">)]</code>
<code class="n">df</code><code class="o">.</code><code class="n">plot</code><code class="o">.</code><code class="n">scatter</code><code class="p">(</code><code class="n">x</code><code class="o">=</code><code class="s1">'XOM'</code><code class="p">,</code> <code class="n">y</code><code class="o">=</code><code class="s1">'CVX'</code><code class="p">,</code> <code class="n">c</code><code class="o">=</code><code class="n">colors</code><code class="p">,</code> <code class="n">alpha</code><code class="o">=</code><code class="mf">0.5</code><code class="p">,</code> <code class="n">ax</code><code class="o">=</code><code class="n">ax</code><code class="p">)</code>
<code class="n">ax</code><code class="o">.</code><code class="n">set_xlim</code><code class="p">(</code><code class="o">-</code><code class="mi">3</code><code class="p">,</code> <code class="mi">3</code><code class="p">)</code>
<code class="n">ax</code><code class="o">.</code><code class="n">set_ylim</code><code class="p">(</code><code class="o">-</code><code class="mi">3</code><code class="p">,</code> <code class="mi">3</code><code class="p">)</code></pre>

<p>The resulting plot is shown in <a data-type="xref" href="#StockMclust">Figure 7-11</a>.
There are two clusters: one cluster in the middle of the data, and a second cluster in the outer edge of the data.
This is very different from the clusters obtained using <em>K</em>-means (<a data-type="xref" href="#KmeansStockData">Figure 7-5</a>) and hierarchical clustering (<a data-type="xref" href="#DissimilarityMeasures">Figure 7-9</a>), which find clusters that are compact.</p>

<figure><div id="StockMclust" class="figure">
<img src="Images/psd2_0711.png" alt="Two clusters are obtained for stock return data using +Mclust+." width="1434" height="1147"/>
<h6><span class="label">Figure 7-11. </span>Two clusters are obtained for stock return data using <code>mclust</code></h6>
</div></figure>

<p>You can extract the parameters to the normal distributions using the <code>summary</code> <span class="keep-together">function</span>:</p>

<pre data-type="programlisting" data-code-language="r"><code class="o">&gt;</code> <code class="nf">summary</code><code class="p">(</code><code class="n">mcl</code><code class="p">,</code> <code class="n">parameters</code><code class="o">=</code><code class="kc">TRUE</code><code class="p">)</code><code class="o">$</code><code class="n">mean</code>
          <code class="p">[,</code><code class="m">1</code><code class="p">]</code>        <code class="p">[,</code><code class="m">2</code><code class="p">]</code>
<code class="n">XOM</code> <code class="m">0.05783847</code> <code class="m">-0.04374944</code>
<code class="n">CVX</code> <code class="m">0.07363239</code> <code class="m">-0.21175715</code>
<code class="o">&gt;</code> <code class="nf">summary</code><code class="p">(</code><code class="n">mcl</code><code class="p">,</code> <code class="n">parameters</code><code class="o">=</code><code class="kc">TRUE</code><code class="p">)</code><code class="o">$</code><code class="n">variance</code>
<code class="p">,</code> <code class="p">,</code> <code class="m">1</code>
          <code class="n">XOM</code>       <code class="n">CVX</code>
<code class="n">XOM</code> <code class="m">0.3002049</code> <code class="m">0.3060989</code>
<code class="n">CVX</code> <code class="m">0.3060989</code> <code class="m">0.5496727</code>
<code class="p">,</code> <code class="p">,</code> <code class="m">2</code>

         <code class="n">XOM</code>      <code class="n">CVX</code>
<code class="n">XOM</code> <code class="m">1.046318</code> <code class="m">1.066860</code>
<code class="n">CVX</code> <code class="m">1.066860</code> <code class="m">1.915799</code></pre>

<p class="pagebreak-before">In <em>Python</em>, you get this information from the <code>means_</code> and <code>covariances_</code> properties of the result:</p>

<pre data-type="programlisting" data-code-language="python"><code class="k">print</code><code class="p">(</code><code class="s1">'Mean'</code><code class="p">)</code>
<code class="k">print</code><code class="p">(</code><code class="n">mclust</code><code class="o">.</code><code class="n">means_</code><code class="p">)</code>
<code class="k">print</code><code class="p">(</code><code class="s1">'Covariances'</code><code class="p">)</code>
<code class="k">print</code><code class="p">(</code><code class="n">mclust</code><code class="o">.</code><code class="n">covariances_</code><code class="p">)</code></pre>

<p>The  distributions have similar means and correlations, but the second distribution has much larger variances and covariances. Due to the randomness of the algorithm, results can vary slightly between different runs.</p>

<p>The clusters from <code>mclust</code> may seem surprising, but in fact, they illustrate the statistical nature of the method.
The goal of model-based clustering is to find the best-fitting set of multivariate normal distributions.
The stock data appears to have a normal-looking shape: see the contours of <a data-type="xref" href="#Normal2d">Figure 7-10</a>.
In fact, though, stock returns have a longer-tailed distribution than a normal distribution.
To handle this, <code>mclust</code> fits a distribution to the bulk of the data but then fits a second distribution with a bigger variance.<a data-type="indexterm" data-primary="multivariate normal distribution" data-secondary="mixtures of normals" data-startref="ix_mulvarnrm" id="idm46522834036648"/><a data-type="indexterm" data-primary="unsupervised learning" data-secondary="model-based clustering" data-tertiary="mixtures of normals" data-startref="ix_unslrnMBCnrm" id="idm46522834035432"/><a data-type="indexterm" data-primary="clustering" data-secondary="model-based" data-tertiary="mixtures of normals" data-startref="ix_clsmodnrml" id="idm46522834033944"/><a data-type="indexterm" data-primary="model-based clustering" data-secondary="mixtures of normals" data-startref="ix_modclnrml" id="idm46522834032456"/></p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Selecting the Number of Clusters"><div class="sect2" id="idm46522834583256">
<h2>Selecting the Number of Clusters</h2>

<p>Unlike <em>K</em>-means and hierarchical clustering, <code>mclust</code> automatically selects the number of clusters in <em>R</em> (in this case, two).<a data-type="indexterm" data-primary="unsupervised learning" data-secondary="model-based clustering" data-tertiary="selecting the number of clusters" id="ix_unslrnMBCcls"/><a data-type="indexterm" data-primary="model-based clustering" data-secondary="selecting the number of clusters" id="ix_modclcl"/><a data-type="indexterm" data-primary="clustering" data-secondary="model-based" data-tertiary="selecting the number of clusters" id="ix_clsmodcls"/>
It does this by choosing the number of clusters for <a data-type="indexterm" data-primary="Bayesian information criteria (BIC)" id="ix_BIC"/>which the <em>Bayesian Information Criteria</em> (<em>BIC</em>) has the largest value
(BIC is similar to AIC; see <a data-type="xref" href="ch04.xhtml#StepwiseRegression">“Model Selection and Stepwise Regression”</a>).
BIC works by selecting the best-fitting model with a penalty for the number of parameters in the model.<a data-type="indexterm" data-primary="AIC (Akaike's Information Criteria)" id="idm46522834092520"/>
In the case of model-based clustering, adding more clusters will always improve the fit at the expense of introducing additional parameters in the model.</p>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>Note that in most cases BIC is usually minimized. The authors of the <code>mclust</code> package decided to define BIC to have the opposite sign to make interpretation of plots easier.</p>
</div>

<p><code>mclust</code> fits 14 different models with increasing number of components and chooses an optimal model automatically. You can plot the BIC values of these models using a function in <code>mclust</code>:</p>

<pre data-type="programlisting" data-code-language="r"><code class="nf">plot</code><code class="p">(</code><code class="n">mcl</code><code class="p">,</code> <code class="n">what</code><code class="o">=</code><code class="s">'BIC'</code><code class="p">,</code> <code class="n">ask</code><code class="o">=</code><code class="kc">FALSE</code><code class="p">)</code></pre>

<p>The number of clusters—or number of different multivariate normal models (components)—is shown on the x-axis (see <a data-type="xref" href="#MclustBIC">Figure 7-12</a>).</p>

<figure><div id="MclustBIC" class="figure">
<img src="Images/psd2_0712.png" alt="BIC values for 14 models of the stock return data with increasing numbers of components." width="1119" height="1188"/>
<h6><span class="label">Figure 7-12. </span>BIC values for 14 models of the stock return data with increasing numbers of components</h6>
</div></figure>

<p>The <code>GaussianMixture</code> implementation on the other hand will not try out various combinations. As shown here, it is straightforward to run multiple combinations using  <em>Python</em>. This implementation defines BIC as usual. Therefore, the calculated BIC value will be positive, and we need to minimize it.</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">results</code><code> </code><code class="o">=</code><code> </code><code class="p">[</code><code class="p">]</code><code>
</code><code class="n">covariance_types</code><code> </code><code class="o">=</code><code> </code><code class="p">[</code><code class="s1">'</code><code class="s1">full</code><code class="s1">'</code><code class="p">,</code><code> </code><code class="s1">'</code><code class="s1">tied</code><code class="s1">'</code><code class="p">,</code><code> </code><code class="s1">'</code><code class="s1">diag</code><code class="s1">'</code><code class="p">,</code><code> </code><code class="s1">'</code><code class="s1">spherical</code><code class="s1">'</code><code class="p">]</code><code>
</code><code class="k">for</code><code> </code><code class="n">n_components</code><code> </code><code class="ow">in</code><code> </code><code class="nb">range</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code><code> </code><code class="mi">9</code><code class="p">)</code><code class="p">:</code><code>
</code><code>    </code><code class="k">for</code><code> </code><code class="n">covariance_type</code><code> </code><code class="ow">in</code><code> </code><code class="n">covariance_types</code><code class="p">:</code><code>
</code><code>        </code><code class="n">mclust</code><code> </code><code class="o">=</code><code> </code><code class="n">GaussianMixture</code><code class="p">(</code><code class="n">n_components</code><code class="o">=</code><code class="n">n_components</code><code class="p">,</code><code> </code><code class="n">warm_start</code><code class="o">=</code><code class="bp">True</code><code class="p">,</code><code>
</code><code>                                 </code><code class="n">covariance_type</code><code class="o">=</code><code class="n">covariance_type</code><code class="p">)</code><code> </code><a class="co" id="co_unsupervised_learning_CO1-1" href="#callout_unsupervised_learning_CO1-1"><img src="Images/1.png" alt="1" width="12" height="12"/></a><code>
</code><code>        </code><code class="n">mclust</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">df</code><code class="p">)</code><code>
</code><code>        </code><code class="n">results</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="p">{</code><code>
</code><code>            </code><code class="s1">'</code><code class="s1">bic</code><code class="s1">'</code><code class="p">:</code><code> </code><code class="n">mclust</code><code class="o">.</code><code class="n">bic</code><code class="p">(</code><code class="n">df</code><code class="p">)</code><code class="p">,</code><code>
</code><code>            </code><code class="s1">'</code><code class="s1">n_components</code><code class="s1">'</code><code class="p">:</code><code> </code><code class="n">n_components</code><code class="p">,</code><code>
</code><code>            </code><code class="s1">'</code><code class="s1">covariance_type</code><code class="s1">'</code><code class="p">:</code><code> </code><code class="n">covariance_type</code><code class="p">,</code><code>
</code><code>        </code><code class="p">}</code><code class="p">)</code><code>
</code><code>
</code><code class="n">results</code><code> </code><code class="o">=</code><code> </code><code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">results</code><code class="p">)</code><code>
</code><code>
</code><code class="n">colors</code><code> </code><code class="o">=</code><code> </code><code class="p">[</code><code class="s1">'</code><code class="s1">C0</code><code class="s1">'</code><code class="p">,</code><code> </code><code class="s1">'</code><code class="s1">C1</code><code class="s1">'</code><code class="p">,</code><code> </code><code class="s1">'</code><code class="s1">C2</code><code class="s1">'</code><code class="p">,</code><code> </code><code class="s1">'</code><code class="s1">C3</code><code class="s1">'</code><code class="p">]</code><code>
</code><code class="n">styles</code><code> </code><code class="o">=</code><code> </code><code class="p">[</code><code class="s1">'</code><code class="s1">C0-</code><code class="s1">'</code><code class="p">,</code><code class="s1">'</code><code class="s1">C1:</code><code class="s1">'</code><code class="p">,</code><code class="s1">'</code><code class="s1">C0-.</code><code class="s1">'</code><code class="p">,</code><code> </code><code class="s1">'</code><code class="s1">C1--</code><code class="s1">'</code><code class="p">]</code><code>
</code><code>
</code><code class="n">fig</code><code class="p">,</code><code> </code><code class="n">ax</code><code> </code><code class="o">=</code><code> </code><code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">(</code><code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">4</code><code class="p">,</code><code> </code><code class="mi">4</code><code class="p">)</code><code class="p">)</code><code>
</code><code class="k">for</code><code> </code><code class="n">i</code><code class="p">,</code><code> </code><code class="n">covariance_type</code><code> </code><code class="ow">in</code><code> </code><code class="nb">enumerate</code><code class="p">(</code><code class="n">covariance_types</code><code class="p">)</code><code class="p">:</code><code>
</code><code>    </code><code class="n">subset</code><code> </code><code class="o">=</code><code> </code><code class="n">results</code><code class="o">.</code><code class="n">loc</code><code class="p">[</code><code class="n">results</code><code class="o">.</code><code class="n">covariance_type</code><code> </code><code class="o">==</code><code> </code><code class="n">covariance_type</code><code class="p">,</code><code> </code><code class="p">:</code><code class="p">]</code><code>
</code><code>    </code><code class="n">subset</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">x</code><code class="o">=</code><code class="s1">'</code><code class="s1">n_components</code><code class="s1">'</code><code class="p">,</code><code> </code><code class="n">y</code><code class="o">=</code><code class="s1">'</code><code class="s1">bic</code><code class="s1">'</code><code class="p">,</code><code> </code><code class="n">ax</code><code class="o">=</code><code class="n">ax</code><code class="p">,</code><code> </code><code class="n">label</code><code class="o">=</code><code class="n">covariance_type</code><code class="p">,</code><code>
</code><code>                </code><code class="n">kind</code><code class="o">=</code><code class="s1">'</code><code class="s1">line</code><code class="s1">'</code><code class="p">,</code><code> </code><code class="n">style</code><code class="o">=</code><code class="n">styles</code><code class="p">[</code><code class="n">i</code><code class="p">]</code><code class="p">)</code></pre>
<dl class="calloutlist">
<dt><a class="co" id="callout_unsupervised_learning_CO1-1" href="#co_unsupervised_learning_CO1-1"><img src="Images/1.png" alt="1" width="12" height="12"/></a></dt>
<dd><p>With the <code>warm_start</code> argument, the calculation will reuse information from the previous fit. This will speed up the convergence of subsequent calculations.</p></dd>
</dl>

<p>This <a data-type="indexterm" data-primary="elbow method" id="idm46522833885432"/>plot is similar to the elbow plot used to identify the number of clusters to choose for <em>K</em>-means, except the value being plotted is BIC instead of percent of variance explained (see <a data-type="xref" href="#KmeansElbowMethod">Figure 7-7</a>).
One big difference is that instead of one line, <code>mclust</code> shows 14 different lines!
This is because <code>mclust</code> is actually fitting 14 different models for each cluster size, and ultimately it chooses the best-fitting model. <code>GaussianMixture</code> implements fewer approaches, so the number of lines will be only four.</p>

<p>Why does <code>mclust</code> fit so many models to determine the best set of multivariate normals?
It’s because there are different ways to parameterize the covariance matrix <math alttext="normal upper Sigma">
  <mi>Σ</mi>
</math> for fitting a model.
For the most part, you do not need to worry about the details of the models and can simply use the model chosen by <code>mclust</code>. In this example, according to BIC, three different models (called VEE, VEV, and VVE) give the best fit using two components.<a data-type="indexterm" data-primary="Bayesian information criteria (BIC)" data-startref="ix_BIC" id="idm46522833938696"/><a data-type="indexterm" data-primary="unsupervised learning" data-secondary="model-based clustering" data-tertiary="selecting the number of clusters" data-startref="ix_unslrnMBCcls" id="idm46522833937752"/><a data-type="indexterm" data-primary="model-based clustering" data-secondary="selecting the number of clusters" data-startref="ix_modclcl" id="idm46522833936248"/><a data-type="indexterm" data-primary="clustering" data-secondary="model-based" data-tertiary="selecting the number of clusters" data-startref="ix_clsmodcls" id="idm46522833959192"/></p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Model-based clustering is a rich and rapidly developing area of study, and the coverage in this text spans only a small part of the field.
Indeed, the <code>mclust</code> help file is currently 154 pages long.
Navigating the nuances of model-based clustering is probably more effort than is needed for most problems encountered by data <span class="keep-together">scientists</span>.</p>
</div>

<p>Model-based clustering techniques do have some limitations.<a data-type="indexterm" data-primary="model-based clustering" data-secondary="limitations of" id="idm46522833954568"/>
The methods require an underlying assumption of a model for the data, and the cluster results are very dependent on that assumption.
The computations requirements are higher than even hierarchical clustering, making it difficult to scale to large data.
Finally, the algorithm is more sophisticated and less accessible than that of other methods.</p>
<aside data-type="sidebar" epub:type="sidebar" class="pagebreak-before less_space"><div class="sidebar" id="idm46522833952968">
<h5>Key Ideas</h5>
<ul>
<li>
<p>Clusters are assumed to derive from different data-generating processes with different probability distributions.</p>
</li>
<li>
<p>Different models are fit, assuming different numbers of (typically normal) <span class="keep-together">distributions</span>.</p>
</li>
<li>
<p>The method chooses the model (and the associated number of clusters) that fits the data well without using too many parameters (i.e., overfitting).</p>
</li>
</ul>
</div></aside>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Further Reading"><div class="sect2" id="idm46522834102472">
<h2>Further Reading</h2>

<p>For more detail on model-based clustering, see the <a href="https://oreil.ly/bHDvR"><code>mclust</code></a> and <a href="https://oreil.ly/GaVVv"><code>GaussianMixture</code></a> documentation.<a data-type="indexterm" data-primary="unsupervised learning" data-secondary="model-based clustering" data-startref="ix_unslrnMBC" id="idm46522833754760"/><a data-type="indexterm" data-primary="model-based clustering" data-startref="ix_modcl" id="idm46522833753512"/><a data-type="indexterm" data-primary="clustering" data-secondary="model-based" data-startref="ix_clsmod" id="idm46522833752568"/></p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Scaling and Categorical Variables"><div class="sect1" id="ScalingClusters">
<h1>Scaling and Categorical Variables</h1>

<p>Unsupervised learning techniques generally require that the data be appropriately scaled.<a data-type="indexterm" data-primary="scaling and categorical variables" id="ix_scale"/><a data-type="indexterm" data-primary="unsupervised learning" data-secondary="scaling and categorical variables" id="ix_unslrnscl"/>
This is different from many of the techniques for regression and classification in which scaling is not important (an exception is <em>K</em>-Nearest Neighbors; see <a data-type="xref" href="ch06.xhtml#KNN">“K-Nearest Neighbors”</a>).</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46522833965256">
<h5>Key Terms for Scaling Data</h5><dl>
<dt class="horizontal"><strong><em>Scaling</em></strong></dt>
<dd>
<p>Squashing or expanding data, usually to bring multiple variables to the same scale.<a data-type="indexterm" data-primary="scaling" id="idm46522833962232"/></p>
</dd>
<dt class="horizontal"><strong><em>Normalization</em></strong></dt>
<dd>
<p>One method of scaling—subtracting the mean and dividing by the standard <span class="keep-together">deviation</span>.<a data-type="indexterm" data-primary="normalization" id="idm46522833870264"/><a data-type="indexterm" data-primary="Gower's distance" id="idm46522833869528"/></p>
<dl>
<dt><em>Synonym</em></dt>
<dd>
<p>Standardization</p>
</dd>
</dl>
</dd>
<dt class="horizontal"><strong><em>Gower’s distance</em></strong></dt>
<dd>
<p>A scaling algorithm applied to mixed numeric and categorical data to bring all variables to a 0–1 range.</p>
</dd>
</dl>
</div></aside>

<p>For example, with the personal loan data,
the variables have widely different units and magnitude.
Some variables have relatively small values (e.g., number of years employed), while others have very large values (e.g., loan amount in dollars).
If the data is not scaled,
then the PCA, <em>K</em>-means, and other clustering methods will be dominated by the variables with large values and ignore the variables with small <span class="keep-together">values</span>.</p>

<p>Categorical data can pose a special<a data-type="indexterm" data-primary="categorical variables" data-secondary="scaling" data-see="scaling and categorical variables" id="idm46522833842328"/><a data-type="indexterm" data-primary="clustering" data-secondary="categorical variables posing problems in" id="idm46522833841016"/><a data-type="indexterm" data-primary="K-Nearest Neighbors" data-secondary="categorical data and" id="idm46522833840104"/> problem for some clustering procedures.
As with <em>K</em>-Nearest Neighbors,
unordered factor variables are generally<a data-type="indexterm" data-primary="binary variables" id="idm46522833838648"/><a data-type="indexterm" data-primary="one hot encoding" id="idm46522833837944"/> converted
to a set of binary (0/1) variables using one hot encoding (see
<a data-type="xref" href="ch06.xhtml#OneHotEncoder">“One Hot Encoder”</a>).
Not only are the binary variables likely on a different scale from other data, but the fact that binary variables have only two values can prove problematic with techniques such as PCA and <em>K</em>-means.</p>








<section data-type="sect2" data-pdf-bookmark="Scaling the Variables"><div class="sect2" id="ScalingPCA">
<h2>Scaling the Variables</h2>

<p>Variables with very different scale and units need to be normalized appropriately before you apply a clustering procedure.<a data-type="indexterm" data-primary="normalization" data-secondary="scaling the variables" id="ix_normscl"/><a data-type="indexterm" data-primary="unsupervised learning" data-secondary="scaling and categorical variables" data-tertiary="scaling the variables" id="ix_unslrnsclvar"/><a data-type="indexterm" data-primary="scaling and categorical variables" data-secondary="scaling the variables" id="ix_scalevar"/><a data-type="indexterm" data-primary="K-means clustering" data-secondary="applying to data without normalization" id="idm46522833940200"/>
For example, let’s apply <code>kmeans</code> to a set of data of loan defaults without normalizing:</p>

<pre data-type="programlisting" data-code-language="r"><code class="n">defaults</code> <code class="o">&lt;-</code> <code class="n">loan_data</code><code class="p">[</code><code class="n">loan_data</code><code class="o">$</code><code class="n">outcome</code><code class="o">==</code><code class="s">'default'</code><code class="p">,]</code>
<code class="n">df</code> <code class="o">&lt;-</code> <code class="n">defaults</code><code class="p">[,</code> <code class="nf">c</code><code class="p">(</code><code class="s">'loan_amnt'</code><code class="p">,</code> <code class="s">'annual_inc'</code><code class="p">,</code> <code class="s">'revol_bal'</code><code class="p">,</code> <code class="s">'open_acc'</code><code class="p">,</code>
                   <code class="s">'dti'</code><code class="p">,</code> <code class="s">'revol_util'</code><code class="p">)]</code>
<code class="n">km</code> <code class="o">&lt;-</code> <code class="nf">kmeans</code><code class="p">(</code><code class="n">df</code><code class="p">,</code> <code class="n">centers</code><code class="o">=</code><code class="m">4</code><code class="p">,</code> <code class="n">nstart</code><code class="o">=</code><code class="m">10</code><code class="p">)</code>
<code class="n">centers</code> <code class="o">&lt;-</code> <code class="nf">data.frame</code><code class="p">(</code><code class="n">size</code><code class="o">=</code><code class="n">km</code><code class="o">$</code><code class="n">size</code><code class="p">,</code> <code class="n">km</code><code class="o">$</code><code class="n">centers</code><code class="p">)</code>
<code class="nf">round</code><code class="p">(</code><code class="n">centers</code><code class="p">,</code> <code class="n">digits</code><code class="o">=</code><code class="m">2</code><code class="p">)</code>

   <code class="n">size</code> <code class="n">loan_amnt</code> <code class="n">annual_inc</code> <code class="n">revol_bal</code> <code class="n">open_acc</code>   <code class="n">dti</code> <code class="n">revol_util</code>
<code class="m">1</code>    <code class="m">52</code>  <code class="m">22570.19</code>  <code class="m">489783.40</code>  <code class="m">85161.35</code>    <code class="m">13.33</code>  <code class="m">6.91</code>      <code class="m">59.65</code>
<code class="m">2</code>  <code class="m">1192</code>  <code class="m">21856.38</code>  <code class="m">165473.54</code>  <code class="m">38935.88</code>    <code class="m">12.61</code> <code class="m">13.48</code>      <code class="m">63.67</code>
<code class="m">3</code> <code class="m">13902</code>  <code class="m">10606.48</code>   <code class="m">42500.30</code>  <code class="m">10280.52</code>     <code class="m">9.59</code> <code class="m">17.71</code>      <code class="m">58.11</code>
<code class="m">4</code>  <code class="m">7525</code>  <code class="m">18282.25</code>   <code class="m">83458.11</code>  <code class="m">19653.82</code>    <code class="m">11.66</code> <code class="m">16.77</code>      <code class="m">62.27</code></pre>

<p>Here is the corresponding <em>Python</em> code:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">defaults</code> <code class="o">=</code> <code class="n">loan_data</code><code class="o">.</code><code class="n">loc</code><code class="p">[</code><code class="n">loan_data</code><code class="p">[</code><code class="s1">'outcome'</code><code class="p">]</code> <code class="o">==</code> <code class="s1">'default'</code><code class="p">,]</code>
<code class="n">columns</code> <code class="o">=</code> <code class="p">[</code><code class="s1">'loan_amnt'</code><code class="p">,</code> <code class="s1">'annual_inc'</code><code class="p">,</code> <code class="s1">'revol_bal'</code><code class="p">,</code> <code class="s1">'open_acc'</code><code class="p">,</code>
           <code class="s1">'dti'</code><code class="p">,</code> <code class="s1">'revol_util'</code><code class="p">]</code>

<code class="n">df</code> <code class="o">=</code> <code class="n">defaults</code><code class="p">[</code><code class="n">columns</code><code class="p">]</code>
<code class="n">kmeans</code> <code class="o">=</code> <code class="n">KMeans</code><code class="p">(</code><code class="n">n_clusters</code><code class="o">=</code><code class="mi">4</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">df</code><code class="p">)</code>
<code class="n">counts</code> <code class="o">=</code> <code class="n">Counter</code><code class="p">(</code><code class="n">kmeans</code><code class="o">.</code><code class="n">labels_</code><code class="p">)</code>

<code class="n">centers</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">kmeans</code><code class="o">.</code><code class="n">cluster_centers_</code><code class="p">,</code> <code class="n">columns</code><code class="o">=</code><code class="n">columns</code><code class="p">)</code>
<code class="n">centers</code><code class="p">[</code><code class="s1">'size'</code><code class="p">]</code> <code class="o">=</code> <code class="p">[</code><code class="n">counts</code><code class="p">[</code><code class="n">i</code><code class="p">]</code> <code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">4</code><code class="p">)]</code>
<code class="n">centers</code></pre>

<p>The variables <code>annual_inc</code> and <code>revol_bal</code> dominate the clusters, and the clusters have very different sizes. Cluster 1 has only 52 members with comparatively high income and revolving credit balance.</p>

<p>A common approach<a data-type="indexterm" data-primary="z-scores" data-secondary="using to scale variables" id="idm46522833657768"/> to scaling the variables is to convert them to <em>z</em>-scores by <span class="keep-together">subtracting</span> the mean and dividing by the standard deviation. This is termed <span class="keep-together"><em>standardization</em></span> or <em>normalization</em> (see <a data-type="xref" href="ch06.xhtml#Standardization">“Standardization (Normalization, z-Scores)”</a> for more discussion about using <em>z</em>-scores):</p>
<div data-type="equation">
<math display="block">
  <mrow>
    <mi>z</mi>
    <mo>=</mo>
    <mfrac><mrow><mi>x</mi><mo>-</mo><mover accent="true"><mi>x</mi> <mo>¯</mo></mover></mrow> <mi>s</mi></mfrac>
  </mrow>
</math>
</div>

<p>See what <a data-type="indexterm" data-primary="K-means clustering" data-secondary="applying to normalized data" id="idm46522833513512"/>happens to the clusters when <code>kmeans</code> is applied to the normalized data:</p>

<pre data-type="programlisting" data-code-language="r"><code class="n">df0</code> <code class="o">&lt;-</code> <code class="nf">scale</code><code class="p">(</code><code class="n">df</code><code class="p">)</code>
<code class="n">km0</code> <code class="o">&lt;-</code> <code class="nf">kmeans</code><code class="p">(</code><code class="n">df0</code><code class="p">,</code> <code class="n">centers</code><code class="o">=</code><code class="m">4</code><code class="p">,</code> <code class="n">nstart</code><code class="o">=</code><code class="m">10</code><code class="p">)</code>
<code class="n">centers0</code> <code class="o">&lt;-</code> <code class="nf">scale</code><code class="p">(</code><code class="n">km0</code><code class="o">$</code><code class="n">centers</code><code class="p">,</code> <code class="n">center</code><code class="o">=</code><code class="kc">FALSE</code><code class="p">,</code>
                 <code class="n">scale</code><code class="o">=</code><code class="m">1</code> <code class="o">/</code> <code class="nf">attr</code><code class="p">(</code><code class="n">df0</code><code class="p">,</code> <code class="s">'scaled:scale'</code><code class="p">))</code>
<code class="n">centers0</code> <code class="o">&lt;-</code> <code class="nf">scale</code><code class="p">(</code><code class="n">centers0</code><code class="p">,</code> <code class="n">center</code><code class="o">=-</code><code class="nf">attr</code><code class="p">(</code><code class="n">df0</code><code class="p">,</code> <code class="s">'scaled:center'</code><code class="p">),</code> <code class="n">scale</code><code class="o">=</code><code class="kc">FALSE</code><code class="p">)</code>
<code class="n">centers0</code> <code class="o">&lt;-</code> <code class="nf">data.frame</code><code class="p">(</code><code class="n">size</code><code class="o">=</code><code class="n">km0</code><code class="o">$</code><code class="n">size</code><code class="p">,</code> <code class="n">centers0</code><code class="p">)</code>
<code class="nf">round</code><code class="p">(</code><code class="n">centers0</code><code class="p">,</code> <code class="n">digits</code><code class="o">=</code><code class="m">2</code><code class="p">)</code>

  <code class="n">size</code> <code class="n">loan_amnt</code> <code class="n">annual_inc</code> <code class="n">revol_bal</code> <code class="n">open_acc</code>   <code class="n">dti</code> <code class="n">revol_util</code>
<code class="m">1</code> <code class="m">7355</code>  <code class="m">10467.65</code>   <code class="m">51134.87</code>  <code class="m">11523.31</code>     <code class="m">7.48</code> <code class="m">15.78</code>      <code class="m">77.73</code>
<code class="m">2</code> <code class="m">5309</code>  <code class="m">10363.43</code>   <code class="m">53523.09</code>   <code class="m">6038.26</code>     <code class="m">8.68</code> <code class="m">11.32</code>      <code class="m">30.70</code>
<code class="m">3</code> <code class="m">3713</code>  <code class="m">25894.07</code>  <code class="m">116185.91</code>  <code class="m">32797.67</code>    <code class="m">12.41</code> <code class="m">16.22</code>      <code class="m">66.14</code>
<code class="m">4</code> <code class="m">6294</code>  <code class="m">13361.61</code>   <code class="m">55596.65</code>  <code class="m">16375.27</code>    <code class="m">14.25</code> <code class="m">24.23</code>      <code class="m">59.61</code></pre>

<p>In <em>Python</em>, we can use <code>scikit-learn</code>’s <code>StandardScaler</code>. The <code>inverse_transform</code> method allows converting the cluster centers back to the original scale:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">scaler</code> <code class="o">=</code> <code class="n">preprocessing</code><code class="o">.</code><code class="n">StandardScaler</code><code class="p">()</code>
<code class="n">df0</code> <code class="o">=</code> <code class="n">scaler</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">df</code> <code class="o">*</code> <code class="mf">1.0</code><code class="p">)</code>

<code class="n">kmeans</code> <code class="o">=</code> <code class="n">KMeans</code><code class="p">(</code><code class="n">n_clusters</code><code class="o">=</code><code class="mi">4</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">df0</code><code class="p">)</code>
<code class="n">counts</code> <code class="o">=</code> <code class="n">Counter</code><code class="p">(</code><code class="n">kmeans</code><code class="o">.</code><code class="n">labels_</code><code class="p">)</code>

<code class="n">centers</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">scaler</code><code class="o">.</code><code class="n">inverse_transform</code><code class="p">(</code><code class="n">kmeans</code><code class="o">.</code><code class="n">cluster_centers_</code><code class="p">),</code>
                       <code class="n">columns</code><code class="o">=</code><code class="n">columns</code><code class="p">)</code>
<code class="n">centers</code><code class="p">[</code><code class="s1">'size'</code><code class="p">]</code> <code class="o">=</code> <code class="p">[</code><code class="n">counts</code><code class="p">[</code><code class="n">i</code><code class="p">]</code> <code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">4</code><code class="p">)]</code>
<code class="n">centers</code></pre>

<p>The cluster sizes are more balanced, and the clusters are not dominated by <code>annual_inc</code> and <code>revol_bal</code>,
revealing more interesting structure in the data.
Note that the centers are rescaled to the original units in the preceding code.
If we had left them unscaled, the resulting values would be in terms of <em>z</em>-scores and would therefore be less interpretable.<a data-type="indexterm" data-primary="normalization" data-secondary="scaling the variables" data-startref="ix_normscl" id="idm46522833211240"/><a data-type="indexterm" data-primary="unsupervised learning" data-secondary="scaling and categorical variables" data-tertiary="scaling the variables" data-startref="ix_unslrnsclvar" id="idm46522833050328"/><a data-type="indexterm" data-primary="scaling and categorical variables" data-secondary="scaling the variables" data-startref="ix_scalevar" id="idm46522833048872"/></p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Scaling is also important for PCA.<a data-type="indexterm" data-primary="principal components analysis" data-secondary="scaling of variables" id="idm46522833046488"/>
Using the <em>z</em>-scores is equivalent to using the correlation matrix (see <a data-type="xref" href="ch01.xhtml#Correlations">“Correlation”</a>) instead of the covariance matrix in computing the principal components.<a data-type="indexterm" data-primary="correlation matrix" id="idm46522833044168"/>
Software to compute PCA usually has an option to use the correlation matrix (in <em>R</em>, the <code>princomp</code> function has the argument <code>cor</code>).</p>
</div>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Dominant Variables"><div class="sect2" id="idm46522833945656">
<h2>Dominant Variables</h2>

<p>Even in cases where the variables are measured on the same scale and accurately reflect relative importance (e.g., movement to stock prices), it can sometimes be useful to rescale the variables.<a data-type="indexterm" data-primary="unsupervised learning" data-secondary="scaling and categorical variables" data-tertiary="dominant variables" id="ix_unslrnscldom"/><a data-type="indexterm" data-primary="scaling and categorical variables" data-secondary="dominant variables" id="ix_scaledom"/><a data-type="indexterm" data-primary="dominant variables" id="ix_domvar"/></p>

<p>Suppose we add Google (GOOGL) and Amazon (AMZN) to the analysis in <a data-type="xref" href="#InterpretPCA">“Interpreting Principal Components”</a>. We see how this is done in <em>R</em> below:</p>

<pre data-type="programlisting" data-code-language="r"><code class="n">syms</code> <code class="o">&lt;-</code> <code class="nf">c</code><code class="p">(</code><code class="s">'GOOGL'</code><code class="p">,</code> <code class="s">'AMZN'</code><code class="p">,</code> <code class="s">'AAPL'</code><code class="p">,</code> <code class="s">'MSFT'</code><code class="p">,</code> <code class="s">'CSCO'</code><code class="p">,</code> <code class="s">'INTC'</code><code class="p">,</code> <code class="s">'CVX'</code><code class="p">,</code> <code class="s">'XOM'</code><code class="p">,</code>
          <code class="s">'SLB'</code><code class="p">,</code> <code class="s">'COP'</code><code class="p">,</code> <code class="s">'JPM'</code><code class="p">,</code> <code class="s">'WFC'</code><code class="p">,</code> <code class="s">'USB'</code><code class="p">,</code> <code class="s">'AXP'</code><code class="p">,</code> <code class="s">'WMT'</code><code class="p">,</code> <code class="s">'TGT'</code><code class="p">,</code> <code class="s">'HD'</code><code class="p">,</code> <code class="s">'COST'</code><code class="p">)</code>
<code class="n">top_sp1</code> <code class="o">&lt;-</code> <code class="n">sp500_px</code><code class="nf">[row.names</code><code class="p">(</code><code class="n">sp500_px</code><code class="p">)</code> <code class="o">&gt;=</code> <code class="s">'2005-01-01'</code><code class="p">,</code> <code class="n">syms</code><code class="p">]</code>
<code class="n">sp_pca1</code> <code class="o">&lt;-</code> <code class="nf">princomp</code><code class="p">(</code><code class="n">top_sp1</code><code class="p">)</code>
<code class="nf">screeplot</code><code class="p">(</code><code class="n">sp_pca1</code><code class="p">)</code></pre>

<p>In <em>Python</em>, we get the <a data-type="indexterm" data-primary="screeplots" id="idm46522833032712"/>screeplot as follows:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">syms</code> <code class="o">=</code> <code class="p">[</code><code class="s1">'GOOGL'</code><code class="p">,</code> <code class="s1">'AMZN'</code><code class="p">,</code> <code class="s1">'AAPL'</code><code class="p">,</code> <code class="s1">'MSFT'</code><code class="p">,</code> <code class="s1">'CSCO'</code><code class="p">,</code> <code class="s1">'INTC'</code><code class="p">,</code> <code class="s1">'CVX'</code><code class="p">,</code> <code class="s1">'XOM'</code><code class="p">,</code>
        <code class="s1">'SLB'</code><code class="p">,</code> <code class="s1">'COP'</code><code class="p">,</code> <code class="s1">'JPM'</code><code class="p">,</code> <code class="s1">'WFC'</code><code class="p">,</code> <code class="s1">'USB'</code><code class="p">,</code> <code class="s1">'AXP'</code><code class="p">,</code> <code class="s1">'WMT'</code><code class="p">,</code> <code class="s1">'TGT'</code><code class="p">,</code> <code class="s1">'HD'</code><code class="p">,</code> <code class="s1">'COST'</code><code class="p">]</code>
<code class="n">top_sp1</code> <code class="o">=</code> <code class="n">sp500_px</code><code class="o">.</code><code class="n">loc</code><code class="p">[</code><code class="n">sp500_px</code><code class="o">.</code><code class="n">index</code> <code class="o">&gt;=</code> <code class="s1">'2005-01-01'</code><code class="p">,</code> <code class="n">syms</code><code class="p">]</code>

<code class="n">sp_pca1</code> <code class="o">=</code> <code class="n">PCA</code><code class="p">()</code>
<code class="n">sp_pca1</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">top_sp1</code><code class="p">)</code>

<code class="n">explained_variance</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">sp_pca1</code><code class="o">.</code><code class="n">explained_variance_</code><code class="p">)</code>
<code class="n">ax</code> <code class="o">=</code> <code class="n">explained_variance</code><code class="o">.</code><code class="n">head</code><code class="p">(</code><code class="mi">10</code><code class="p">)</code><code class="o">.</code><code class="n">plot</code><code class="o">.</code><code class="n">bar</code><code class="p">(</code><code class="n">legend</code><code class="o">=</code><code class="bp">False</code><code class="p">,</code> <code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">4</code><code class="p">,</code> <code class="mi">4</code><code class="p">))</code>
<code class="n">ax</code><code class="o">.</code><code class="n">set_xlabel</code><code class="p">(</code><code class="s1">'Component'</code><code class="p">)</code></pre>

<p>The screeplot displays the variances for the top principal components.
In this case, the screeplot in <a data-type="xref" href="#Screeplot1">Figure 7-13</a> reveals that the variances of the first and second components are much larger than the others.
This often indicates that one or two variables dominate the loadings.
This is, indeed, the case in this example:</p>

<pre data-type="programlisting" data-code-language="r"><code class="nf">round</code><code class="p">(</code><code class="n">sp_pca1</code><code class="o">$</code><code class="n">loadings</code><code class="p">[,</code><code class="m">1</code><code class="o">:</code><code class="m">2</code><code class="p">],</code> <code class="m">3</code><code class="p">)</code>
      <code class="n">Comp.1</code> <code class="n">Comp.2</code>
<code class="n">GOOGL</code>  <code class="m">0.781</code>  <code class="m">0.609</code>
<code class="n">AMZN</code>   <code class="m">0.593</code> <code class="m">-0.792</code>
<code class="n">AAPL</code>   <code class="m">0.078</code>  <code class="m">0.004</code>
<code class="n">MSFT</code>   <code class="m">0.029</code>  <code class="m">0.002</code>
<code class="n">CSCO</code>   <code class="m">0.017</code> <code class="m">-0.001</code>
<code class="n">INTC</code>   <code class="m">0.020</code> <code class="m">-0.001</code>
<code class="n">CVX</code>    <code class="m">0.068</code> <code class="m">-0.021</code>
<code class="n">XOM</code>    <code class="m">0.053</code> <code class="m">-0.005</code>
<code class="kc">...</code></pre>

<p>In <em>Python</em>, we use the following:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">loadings</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">sp_pca1</code><code class="o">.</code><code class="n">components_</code><code class="p">[</code><code class="mi">0</code><code class="p">:</code><code class="mi">2</code><code class="p">,</code> <code class="p">:],</code> <code class="n">columns</code><code class="o">=</code><code class="n">top_sp1</code><code class="o">.</code><code class="n">columns</code><code class="p">)</code>
<code class="n">loadings</code><code class="o">.</code><code class="n">transpose</code><code class="p">()</code></pre>

<p>The first two principal components are almost completely dominated by GOOGL and AMZN.
This is because the stock price movements of GOOGL and AMZN dominate the variability.</p>

<p>To handle this situation, you can either include them as is, rescale the variables (see <a data-type="xref" href="#ScalingPCA">“Scaling the Variables”</a>), or exclude the dominant variables from the analysis and handle them separately.
There is no “correct” approach, and the treatment depends on the application.</p>

<figure class="width-75"><div id="Screeplot1" class="figure">
<img src="Images/psd2_0713.png" alt="A screeplot for a PCA of top stocks from the SP 500, including GOOGL and AMZN." width="1088" height="1091"/>
<h6><span class="label">Figure 7-13. </span>A screeplot for a PCA of top stocks from the S&amp;P 500, including GOOGL and AMZN</h6>
</div></figure>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Categorical Data and Gower’s Distance"><div class="sect2" id="GowersDistance">
<h2>Categorical Data and Gower’s Distance</h2>

<p>In the case of <a data-type="indexterm" data-primary="unsupervised learning" data-secondary="scaling and categorical variables" data-tertiary="dominant variables" data-startref="ix_unslrnscldom" id="idm46522832745752"/><a data-type="indexterm" data-primary="scaling and categorical variables" data-secondary="dominant variables" data-startref="ix_scaledom" id="idm46522832744232"/><a data-type="indexterm" data-primary="dominant variables" data-startref="ix_domvar" id="idm46522832743048"/>categorical data, you must convert it to numeric data, either by ranking (for an ordered factor) or by encoding as a set of binary (dummy) variables.<a data-type="indexterm" data-primary="unsupervised learning" data-secondary="scaling and categorical variables" data-tertiary="categorical variables and Gower's distance" id="idm46522832638392"/><a data-type="indexterm" data-primary="scaling and categorical variables" data-secondary="categorical variables and Gower's distance" id="idm46522832637240"/><a data-type="indexterm" data-primary="Gower's distance" data-secondary="using to scale categorical variables" id="idm46522832636296"/><a data-type="indexterm" data-primary="distance metrics" data-secondary="Gower's distance" id="idm46522832635384"/>
If the data consists of mixed continuous and binary variables, you will usually want to scale the variables so that the ranges are similar; see <a data-type="xref" href="#ScalingPCA">“Scaling the Variables”</a>.
One popular method is to use <em>Gower’s distance</em>.</p>

<p>The basic idea behind Gower’s distance is to apply a different distance metric to each variable depending on the type of data:</p>

<ul>
<li>
<p>For numeric<a data-type="indexterm" data-primary="Manhattan distance" id="idm46522832631448"/><a data-type="indexterm" data-primary="distance metrics" data-secondary="Manhattan distance" id="idm46522832630712"/> variables and ordered factors, distance is calculated as the absolute value of the difference between two records (<em>Manhattan distance</em>).</p>
</li>
<li>
<p>For categorical variables, the distance is 1 if the categories between two records are different, and the distance is 0 if the categories are the same.</p>
</li>
</ul>

<p>Gower’s distance is computed as follows:</p>
<ol>
<li>
<p>Compute the distance <math alttext="d Subscript i comma j">
  <msub><mi>d</mi> <mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow> </msub>
</math> for all pairs of variables <em>i</em> and <em>j</em> for each record.</p>
</li>
<li>
<p>Scale each pair  <math alttext="d Subscript i comma j">
  <msub><mi>d</mi> <mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow> </msub>
</math> so the minimum is 0 and the maximum is 1.</p>
</li>
<li>
<p>Add the pairwise scaled distances between variables together, using either a simple or a weighted mean,
to create the distance matrix.</p>
</li>

</ol>

<p>To illustrate Gower’s distance, take a few rows from the loan data in <em>R</em>:</p>

<pre data-type="programlisting" data-code-language="r"><code class="o">&gt;</code> <code class="n">x</code> <code class="o">&lt;-</code> <code class="n">loan_data</code><code class="p">[</code><code class="m">1</code><code class="o">:</code><code class="m">5</code><code class="p">,</code> <code class="nf">c</code><code class="p">(</code><code class="s">'dti'</code><code class="p">,</code> <code class="s">'payment_inc_ratio'</code><code class="p">,</code> <code class="s">'home_'</code><code class="p">,</code> <code class="s">'purpose_'</code><code class="p">)]</code>
<code class="o">&gt;</code> <code class="n">x</code>
<code class="c1"># A tibble: 5 × 4</code>
    <code class="n">dti</code> <code class="n">payment_inc_ratio</code>   <code class="n">home</code>            <code class="n">purpose</code>
  <code class="o">&lt;</code><code class="n">dbl</code><code class="o">&gt;</code>             <code class="o">&lt;</code><code class="n">dbl</code><code class="o">&gt;</code> <code class="o">&lt;</code><code class="n">fctr</code><code class="o">&gt;</code>             <code class="o">&lt;</code><code class="n">fctr</code><code class="o">&gt;</code>
<code class="m">1</code>  <code class="m">1.00</code>           <code class="m">2.39320</code>   <code class="n">RENT</code>                <code class="n">car</code>
<code class="m">2</code>  <code class="m">5.55</code>           <code class="m">4.57170</code>    <code class="n">OWN</code>     <code class="n">small_business</code>
<code class="m">3</code> <code class="m">18.08</code>           <code class="m">9.71600</code>   <code class="n">RENT</code>              <code class="n">other</code>
<code class="m">4</code> <code class="m">10.08</code>          <code class="m">12.21520</code>   <code class="n">RENT</code> <code class="n">debt_consolidation</code>
<code class="m">5</code>  <code class="m">7.06</code>           <code class="m">3.90888</code>   <code class="n">RENT</code>              <code class="n">other</code></pre>

<p>The function <code>daisy</code> in the <code>cluster</code> package in <em>R</em> can be used to compute Gower’s <span class="keep-together">distance</span>:</p>

<pre data-type="programlisting" data-code-language="r"><code class="nf">library</code><code class="p">(</code><code class="n">cluster</code><code class="p">)</code>
<code class="nf">daisy</code><code class="p">(</code><code class="n">x</code><code class="p">,</code> <code class="n">metric</code><code class="o">=</code><code class="s">'gower'</code><code class="p">)</code>
<code class="n">Dissimilarities</code> <code class="o">:</code>
          <code class="m">1</code>         <code class="m">2</code>         <code class="m">3</code>         <code class="m">4</code>
<code class="m">2</code> <code class="m">0.6220479</code>
<code class="m">3</code> <code class="m">0.6863877</code> <code class="m">0.8143398</code>
<code class="m">4</code> <code class="m">0.6329040</code> <code class="m">0.7608561</code> <code class="m">0.4307083</code>
<code class="m">5</code> <code class="m">0.3772789</code> <code class="m">0.5389727</code> <code class="m">0.3091088</code> <code class="m">0.5056250</code>

<code class="n">Metric</code> <code class="o">:</code>  <code class="n">mixed</code> <code class="p">;</code>  <code class="n">Types</code> <code class="o">=</code> <code class="n">I</code><code class="p">,</code> <code class="n">I</code><code class="p">,</code> <code class="n">N</code><code class="p">,</code> <code class="n">N</code>
<code class="n">Number</code> <code class="n">of</code> <code class="n">objects</code> <code class="o">:</code> <code class="m">5</code></pre>

<p>At the moment of this writing, Gower’s distance is not available in any of the popular Python packages. However, activities are ongoing to include it in <code>scikit-learn</code>. We will update the accompanying source code once the implementation is released.</p>

<p>All distances are between 0 and 1.
The pair of records with the biggest distance is 2 and 3: neither has the same values for <code>home</code> and <code>purpose</code>, and they have very different levels of <code>dti</code> (debt-to-income) and <code>payment_inc_ratio</code>.
Records 3 and 5 have the smallest distance because they share the same  values for <code>home</code> and <code>purpose</code>.</p>

<p>You can pass the Gower’s distance matrix calculated from <code>daisy</code> to <code>hclust</code> for hierarchical clustering<a data-type="indexterm" data-primary="hierarchical clustering" data-secondary="using with Gower's distance" id="idm46522832422216"/> (see <a data-type="xref" href="#HierarchicalClustering">“Hierarchical Clustering”</a>):</p>

<pre data-type="programlisting" data-code-language="r"><code class="n">df</code> <code class="o">&lt;-</code> <code class="n">defaults</code><code class="nf">[sample</code><code class="p">(</code><code class="nf">nrow</code><code class="p">(</code><code class="n">defaults</code><code class="p">),</code> <code class="m">250</code><code class="p">),</code>
               <code class="nf">c</code><code class="p">(</code><code class="s">'dti'</code><code class="p">,</code> <code class="s">'payment_inc_ratio'</code><code class="p">,</code> <code class="s">'home'</code><code class="p">,</code> <code class="s">'purpose'</code><code class="p">)]</code>
<code class="n">d</code> <code class="o">=</code> <code class="nf">daisy</code><code class="p">(</code><code class="n">df</code><code class="p">,</code> <code class="n">metric</code><code class="o">=</code><code class="s">'gower'</code><code class="p">)</code>
<code class="n">hcl</code> <code class="o">&lt;-</code> <code class="nf">hclust</code><code class="p">(</code><code class="n">d</code><code class="p">)</code>
<code class="n">dnd</code> <code class="o">&lt;-</code> <code class="nf">as.dendrogram</code><code class="p">(</code><code class="n">hcl</code><code class="p">)</code>
<code class="nf">plot</code><code class="p">(</code><code class="n">dnd</code><code class="p">,</code> <code class="n">leaflab</code><code class="o">=</code><code class="s">'none'</code><code class="p">)</code></pre>

<p>The resulting<a data-type="indexterm" data-primary="dendrograms" id="idm46522832418888"/> dendrogram is shown in <a data-type="xref" href="#DendroLoan">Figure 7-14</a>.
The individual records are not distinguishable on the x-axis, but we can cut the dendrogram horizontally at 0.5 and examine the records in one of the subtrees with this code:</p>

<pre data-type="programlisting" data-code-language="r"><code class="n">dnd_cut</code> <code class="o">&lt;-</code> <code class="nf">cut</code><code class="p">(</code><code class="n">dnd</code><code class="p">,</code> <code class="n">h</code><code class="o">=</code><code class="m">0.5</code><code class="p">)</code>
<code class="n">df</code><code class="nf">[labels</code><code class="p">(</code><code class="n">dnd_cut</code><code class="o">$</code><code class="n">lower</code><code class="p">[[</code><code class="m">1</code><code class="p">]]),]</code>
        <code class="n">dti</code> <code class="n">payment_inc_ratio</code> <code class="n">home_</code>           <code class="n">purpose_</code>
<code class="m">44532</code> <code class="m">21.22</code>           <code class="m">8.37694</code>   <code class="n">OWN</code> <code class="n">debt_consolidation</code>
<code class="m">39826</code> <code class="m">22.59</code>           <code class="m">6.22827</code>   <code class="n">OWN</code> <code class="n">debt_consolidation</code>
<code class="m">13282</code> <code class="m">31.00</code>           <code class="m">9.64200</code>   <code class="n">OWN</code> <code class="n">debt_consolidation</code>
<code class="m">31510</code> <code class="m">26.21</code>          <code class="m">11.94380</code>   <code class="n">OWN</code> <code class="n">debt_consolidation</code>
<code class="m">6693</code>  <code class="m">26.96</code>           <code class="m">9.45600</code>   <code class="n">OWN</code> <code class="n">debt_consolidation</code>
<code class="m">7356</code>  <code class="m">25.81</code>           <code class="m">9.39257</code>   <code class="n">OWN</code> <code class="n">debt_consolidation</code>
<code class="m">9278</code>  <code class="m">21.00</code>          <code class="m">14.71850</code>   <code class="n">OWN</code> <code class="n">debt_consolidation</code>
<code class="m">13520</code> <code class="m">29.00</code>          <code class="m">18.86670</code>   <code class="n">OWN</code> <code class="n">debt_consolidation</code>
<code class="m">14668</code> <code class="m">25.75</code>          <code class="m">17.53440</code>   <code class="n">OWN</code> <code class="n">debt_consolidation</code>
<code class="m">19975</code> <code class="m">22.70</code>          <code class="m">17.12170</code>   <code class="n">OWN</code> <code class="n">debt_consolidation</code>
<code class="m">23492</code> <code class="m">22.68</code>          <code class="m">18.50250</code>   <code class="n">OWN</code> <code class="n">debt_consolidation</code></pre>

<p>This subtree consists entirely of owners with a loan purpose labeled as “debt_consolidation.”
While strict separation is not true of all subtrees, this illustrates that the categorical variables tend to be grouped together in the clusters.</p>

<figure class="width-50"><div id="DendroLoan" class="figure">
<img src="Images/psd2_0714.png" alt="A dendrogram of hclust applied to a sample of loan default data with mixed variable types." width="1084" height="1135"/>
<h6><span class="label">Figure 7-14. </span>A dendrogram of <code>hclust</code> applied to a sample of loan default data with mixed variable types</h6>
</div></figure>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Problems with Clustering Mixed Data"><div class="sect2" id="MixedData">
<h2>Problems with Clustering Mixed Data</h2>

<p><em>K</em>-means and PCA are most appropriate for continuous variables.
For smaller data sets,  it is better to use hierarchical clustering with Gower’s distance.<a data-type="indexterm" data-primary="scaling and categorical variables" data-secondary="problems with clustering mixed data" id="idm46522832215160"/><a data-type="indexterm" data-primary="unsupervised learning" data-secondary="scaling and categorical variables" data-tertiary="problems with clustering mixed data" id="idm46522832214248"/><a data-type="indexterm" data-primary="clustering" data-secondary="problems with clustering mixed data" id="idm46522832213096"/>
In principle, there is no reason why <em>K</em>-means can’t be applied to binary or categorical data.<a data-type="indexterm" data-primary="K-means clustering" data-secondary="using with binary data" id="idm46522832211624"/><a data-type="indexterm" data-primary="principal components analysis" id="idm46522832210648"/>
You would usually use the “one hot encoder” representation (see <a data-type="xref" href="ch06.xhtml#OneHotEncoder">“One Hot Encoder”</a>) to convert the categorical data to numeric values.
In practice, however, using <em>K</em>-means and PCA with binary data can be difficult.<a data-type="indexterm" data-primary="principal components analysis" data-secondary="using with binary data" id="idm46522832208536"/></p>

<p>If the standard <em>z</em>-scores are used,
the binary variables will dominate the definition of the clusters.
This is because 0/1 variables take on only two values, and <em>K</em>-means can obtain a small within-cluster sum-of-squares by assigning all the records with a 0 or 1 to a single cluster.
For example, apply <code>kmeans</code> to loan default data including factor variables <code>home</code> and <code>pub_rec_zero</code>, shown here in <em>R</em>:</p>

<pre data-type="programlisting" data-code-language="r"><code class="n">df</code> <code class="o">&lt;-</code> <code class="nf">model.matrix</code><code class="p">(</code><code class="o">~</code> <code class="m">-1</code> <code class="o">+</code> <code class="n">dti</code> <code class="o">+</code> <code class="n">payment_inc_ratio</code> <code class="o">+</code> <code class="n">home_</code> <code class="o">+</code> <code class="n">pub_rec_zero</code><code class="p">,</code>
                   <code class="n">data</code><code class="o">=</code><code class="n">defaults</code><code class="p">)</code>
<code class="n">df0</code> <code class="o">&lt;-</code> <code class="nf">scale</code><code class="p">(</code><code class="n">df</code><code class="p">)</code>
<code class="n">km0</code> <code class="o">&lt;-</code> <code class="nf">kmeans</code><code class="p">(</code><code class="n">df0</code><code class="p">,</code> <code class="n">centers</code><code class="o">=</code><code class="m">4</code><code class="p">,</code> <code class="n">nstart</code><code class="o">=</code><code class="m">10</code><code class="p">)</code>
<code class="n">centers0</code> <code class="o">&lt;-</code> <code class="nf">scale</code><code class="p">(</code><code class="n">km0</code><code class="o">$</code><code class="n">centers</code><code class="p">,</code> <code class="n">center</code><code class="o">=</code><code class="kc">FALSE</code><code class="p">,</code>
                 <code class="n">scale</code><code class="o">=</code><code class="m">1</code><code class="o">/</code><code class="nf">attr</code><code class="p">(</code><code class="n">df0</code><code class="p">,</code> <code class="s">'scaled:scale'</code><code class="p">))</code>
<code class="nf">round</code><code class="p">(</code><code class="nf">scale</code><code class="p">(</code><code class="n">centers0</code><code class="p">,</code> <code class="n">center</code><code class="o">=-</code><code class="nf">attr</code><code class="p">(</code><code class="n">df0</code><code class="p">,</code> <code class="s">'scaled:center'</code><code class="p">),</code> <code class="n">scale</code><code class="o">=</code><code class="kc">FALSE</code><code class="p">),</code> <code class="m">2</code><code class="p">)</code>

    <code class="n">dti</code> <code class="n">payment_inc_ratio</code> <code class="n">home_MORTGAGE</code> <code class="n">home_OWN</code> <code class="n">home_RENT</code> <code class="n">pub_rec_zero</code>
<code class="m">1</code> <code class="m">17.20</code>              <code class="m">9.27</code>          <code class="m">0.00</code>        <code class="m">1</code>      <code class="m">0.00</code>         <code class="m">0.92</code>
<code class="m">2</code> <code class="m">16.99</code>              <code class="m">9.11</code>          <code class="m">0.00</code>        <code class="m">0</code>      <code class="m">1.00</code>         <code class="m">1.00</code>
<code class="m">3</code> <code class="m">16.50</code>              <code class="m">8.06</code>          <code class="m">0.52</code>        <code class="m">0</code>      <code class="m">0.48</code>         <code class="m">0.00</code>
<code class="m">4</code> <code class="m">17.46</code>              <code class="m">8.42</code>          <code class="m">1.00</code>        <code class="m">0</code>      <code class="m">0.00</code>         <code class="m">1.00</code></pre>

<p>In <em>Python</em>:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">columns</code> <code class="o">=</code> <code class="p">[</code><code class="s1">'dti'</code><code class="p">,</code> <code class="s1">'payment_inc_ratio'</code><code class="p">,</code> <code class="s1">'home_'</code><code class="p">,</code> <code class="s1">'pub_rec_zero'</code><code class="p">]</code>
<code class="n">df</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">get_dummies</code><code class="p">(</code><code class="n">defaults</code><code class="p">[</code><code class="n">columns</code><code class="p">])</code>

<code class="n">scaler</code> <code class="o">=</code> <code class="n">preprocessing</code><code class="o">.</code><code class="n">StandardScaler</code><code class="p">()</code>
<code class="n">df0</code> <code class="o">=</code> <code class="n">scaler</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">df</code> <code class="o">*</code> <code class="mf">1.0</code><code class="p">)</code>
<code class="n">kmeans</code> <code class="o">=</code> <code class="n">KMeans</code><code class="p">(</code><code class="n">n_clusters</code><code class="o">=</code><code class="mi">4</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">df0</code><code class="p">)</code>
<code class="n">centers</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">scaler</code><code class="o">.</code><code class="n">inverse_transform</code><code class="p">(</code><code class="n">kmeans</code><code class="o">.</code><code class="n">cluster_centers_</code><code class="p">),</code>
                       <code class="n">columns</code><code class="o">=</code><code class="n">df</code><code class="o">.</code><code class="n">columns</code><code class="p">)</code>
<code class="n">centers</code></pre>

<p>The top four clusters are essentially proxies for the different levels of the factor variables.
To avoid this behavior, you could scale the binary variables to have a smaller variance than other variables.
Alternatively, for very large data sets, you could apply clustering to different subsets of data taking on specific categorical values.
For example, you could apply clustering separately to those loans made to someone who has a mortgage, owns a home outright, or rents.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46522832070552">
<h5>Key Ideas</h5>
<ul>
<li>
<p>Variables measured on different scales need to be transformed to similar scales so that their impact on algorithms is not determined mainly by their scale.</p>
</li>
<li>
<p>A common scaling method is normalization (standardization)—subtracting the mean and dividing by the standard deviation.</p>
</li>
<li>
<p>Another method is Gower’s distance, which scales all variables to the 0–1 range (it is often used with mixed numeric and categorical data).</p>
</li>
</ul>
</div></aside>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Summary"><div class="sect1" id="idm46522833739880">
<h1>Summary</h1>

<p>For dimension reduction of numeric data, the main tools are either principal components analysis or <em>K</em>-means clustering.<a data-type="indexterm" data-primary="scaling and categorical variables" data-startref="ix_scale" id="idm46522831898440"/><a data-type="indexterm" data-primary="unsupervised learning" data-secondary="scaling and categorical variables" data-startref="ix_unslrnscl" id="idm46522831897464"/><a data-type="indexterm" data-primary="numeric data" data-secondary="dimension reduction of" id="idm46522831896280"/>
Both require attention to proper scaling of the data to ensure meaningful data reduction.</p>

<p>For clustering with highly structured data in which the clusters are well separated, all methods will likely produce a similar result.
Each method offers its own advantage.
<em>K</em>-means scales to very large data and is easily understood.
Hierarchical clustering can be applied to mixed data types—numeric and categorical—and lends itself to an intuitive display (the dendrogram).
Model-based clustering is founded on statistical theory and provides a more rigorous approach, as opposed to the heuristic methods.
For very large data, however, <em>K</em>-means is the main method used.</p>

<p>With noisy data, such as the loan and stock data (and much of the data that a data scientist will face), the choice is more stark.
<em>K</em>-means, hierarchical clustering, and especially model-based clustering all produce very different solutions.
How should a data scientist proceed?
Unfortunately, there is no simple rule of thumb to guide the choice.
Ultimately, the method used will depend on the data size and the goal of the application.<a data-type="indexterm" data-primary="unsupervised learning" data-startref="ix_unslrn" id="idm46522831892008"/></p>
</div></section>







<div data-type="footnotes"><p data-type="footnote" id="idm46522838490648"><sup><a href="ch07.xhtml#idm46522838490648-marker">1</a></sup> This and subsequent sections in this chapter © 2020 Datastats, LLC, Peter Bruce, Andrew Bruce, and Peter Gedeck; used with permission.</p></div></div></section></div>



  </body></html>