<html><head></head><body><section data-pdf-bookmark="Chapter 18. Neural Networks" data-type="chapter" epub:type="chapter"><div class="chapter" id="neural_networks">&#13;
<h1><span class="label">Chapter 18. </span>Neural Networks</h1>&#13;
&#13;
<blockquote data-type="epigraph" epub:type="epigraph">&#13;
    <p>I like nonsense; it wakes up the brain cells.</p>&#13;
    <p data-type="attribution">Dr. Seuss</p>&#13;
</blockquote>&#13;
&#13;
<p>An<a data-primary="artificial neural networks" data-type="indexterm" id="idm45635732727496"/> <em>artificial neural network</em> (or neural network for short)<a data-primary="predictive models" data-secondary="neural networks" data-type="indexterm" id="PMneural18"/> is a predictive model motivated by the way the brain operates.  Think of the brain as a collection of neurons wired together.  Each neuron looks at the outputs of the other neurons that feed into it, does a calculation, and then either fires (if the calculation exceeds some threshold) or doesn’t (if it doesn’t).</p>&#13;
&#13;
<p>Accordingly, artificial neural networks<a data-primary="neural networks" data-secondary="components of" data-type="indexterm" id="idm45635732724040"/> consist of artificial neurons, which perform similar calculations over their inputs.  Neural networks can solve a wide variety of problems like handwriting recognition and face detection, and they are used heavily in deep learning, one of the trendiest subfields of data science.  However, most neural networks are “black boxes”—inspecting their details doesn’t give you much understanding of <em>how</em> they’re solving a problem.  And large neural networks can be difficult to train.  For most problems you’ll encounter as a budding data scientist, they’re probably not the right choice.  Someday, when you’re trying to build an artificial intelligence to bring about the Singularity, they very well might be.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Perceptrons" data-type="sect1"><div class="sect1" id="idm45635732721672">&#13;
<h1>Perceptrons</h1>&#13;
&#13;
<p>Pretty<a data-primary="neural networks" data-secondary="perceptrons" data-type="indexterm" id="idm45635732719944"/><a data-primary="perceptrons" data-type="indexterm" id="idm45635732718936"/> much the simplest neural network is the <em>perceptron</em>, which approximates a single neuron with <em>n</em> binary inputs.  It computes a weighted sum of its inputs and “fires” if that weighted sum is 0 or greater:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="kn">from</code> <code class="nn">scratch.linear_algebra</code> <code class="kn">import</code> <code class="n">Vector</code><code class="p">,</code> <code class="n">dot</code>&#13;
&#13;
<code class="k">def</code> <code class="nf">step_function</code><code class="p">(</code><code class="n">x</code><code class="p">:</code> <code class="nb">float</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">float</code><code class="p">:</code>&#13;
    <code class="k">return</code> <code class="mf">1.0</code> <code class="k">if</code> <code class="n">x</code> <code class="o">&gt;=</code> <code class="mi">0</code> <code class="k">else</code> <code class="mf">0.0</code>&#13;
&#13;
<code class="k">def</code> <code class="nf">perceptron_output</code><code class="p">(</code><code class="n">weights</code><code class="p">:</code> <code class="n">Vector</code><code class="p">,</code> <code class="n">bias</code><code class="p">:</code> <code class="nb">float</code><code class="p">,</code> <code class="n">x</code><code class="p">:</code> <code class="n">Vector</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">float</code><code class="p">:</code>&#13;
    <code class="sd">"""Returns 1 if the perceptron 'fires', 0 if not"""</code>&#13;
    <code class="n">calculation</code> <code class="o">=</code> <code class="n">dot</code><code class="p">(</code><code class="n">weights</code><code class="p">,</code> <code class="n">x</code><code class="p">)</code> <code class="o">+</code> <code class="n">bias</code>&#13;
    <code class="k">return</code> <code class="n">step_function</code><code class="p">(</code><code class="n">calculation</code><code class="p">)</code></pre>&#13;
&#13;
<p>The perceptron is simply distinguishing between the half-spaces separated by the hyperplane of points <code>x</code> for which:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="n">dot</code><code class="p">(</code><code class="n">weights</code><code class="p">,</code> <code class="n">x</code><code class="p">)</code> <code class="o">+</code> <code class="n">bias</code> <code class="o">==</code> <code class="mi">0</code></pre>&#13;
&#13;
<p>With properly chosen weights, perceptrons can solve a number of simple problems (<a data-type="xref" href="#a_perceptron">Figure 18-1</a>).  For example, we can create an <em>AND gate</em> (which returns 1 if both its inputs are 1 but returns 0 if one of its inputs is 0) with:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="n">and_weights</code> <code class="o">=</code> <code class="p">[</code><code class="mf">2.</code><code class="p">,</code> <code class="mi">2</code><code class="p">]</code>&#13;
<code class="n">and_bias</code> <code class="o">=</code> <code class="o">-</code><code class="mf">3.</code>&#13;
&#13;
<code class="k">assert</code> <code class="n">perceptron_output</code><code class="p">(</code><code class="n">and_weights</code><code class="p">,</code> <code class="n">and_bias</code><code class="p">,</code> <code class="p">[</code><code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">])</code> <code class="o">==</code> <code class="mi">1</code>&#13;
<code class="k">assert</code> <code class="n">perceptron_output</code><code class="p">(</code><code class="n">and_weights</code><code class="p">,</code> <code class="n">and_bias</code><code class="p">,</code> <code class="p">[</code><code class="mi">0</code><code class="p">,</code> <code class="mi">1</code><code class="p">])</code> <code class="o">==</code> <code class="mi">0</code>&#13;
<code class="k">assert</code> <code class="n">perceptron_output</code><code class="p">(</code><code class="n">and_weights</code><code class="p">,</code> <code class="n">and_bias</code><code class="p">,</code> <code class="p">[</code><code class="mi">1</code><code class="p">,</code> <code class="mi">0</code><code class="p">])</code> <code class="o">==</code> <code class="mi">0</code>&#13;
<code class="k">assert</code> <code class="n">perceptron_output</code><code class="p">(</code><code class="n">and_weights</code><code class="p">,</code> <code class="n">and_bias</code><code class="p">,</code> <code class="p">[</code><code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">])</code> <code class="o">==</code> <code class="mi">0</code></pre>&#13;
&#13;
<p>If both inputs are 1, the <code>calculation</code> equals 2 + 2 – 3 = 1, and the output is 1.  If only one of the inputs is 1, the <code>calculation</code> equals 2 + 0 – 3 = –1, and the output is 0.  And if both of the inputs are 0, the <code>calculation</code> equals –3, and the output is 0.</p>&#13;
&#13;
<p>Using similar reasoning, we could build an <em>OR gate</em> with:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="n">or_weights</code> <code class="o">=</code> <code class="p">[</code><code class="mf">2.</code><code class="p">,</code> <code class="mi">2</code><code class="p">]</code>&#13;
<code class="n">or_bias</code> <code class="o">=</code> <code class="o">-</code><code class="mf">1.</code>&#13;
&#13;
<code class="k">assert</code> <code class="n">perceptron_output</code><code class="p">(</code><code class="n">or_weights</code><code class="p">,</code> <code class="n">or_bias</code><code class="p">,</code> <code class="p">[</code><code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">])</code> <code class="o">==</code> <code class="mi">1</code>&#13;
<code class="k">assert</code> <code class="n">perceptron_output</code><code class="p">(</code><code class="n">or_weights</code><code class="p">,</code> <code class="n">or_bias</code><code class="p">,</code> <code class="p">[</code><code class="mi">0</code><code class="p">,</code> <code class="mi">1</code><code class="p">])</code> <code class="o">==</code> <code class="mi">1</code>&#13;
<code class="k">assert</code> <code class="n">perceptron_output</code><code class="p">(</code><code class="n">or_weights</code><code class="p">,</code> <code class="n">or_bias</code><code class="p">,</code> <code class="p">[</code><code class="mi">1</code><code class="p">,</code> <code class="mi">0</code><code class="p">])</code> <code class="o">==</code> <code class="mi">1</code>&#13;
<code class="k">assert</code> <code class="n">perceptron_output</code><code class="p">(</code><code class="n">or_weights</code><code class="p">,</code> <code class="n">or_bias</code><code class="p">,</code> <code class="p">[</code><code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">])</code> <code class="o">==</code> <code class="mi">0</code></pre>&#13;
&#13;
<figure><div class="figure" id="a_perceptron">&#13;
<img alt="Perceptron." src="assets/dsf2_1801.png"/>&#13;
<h6><span class="label">Figure 18-1. </span>Decision space for a two-input perceptron</h6>&#13;
</div></figure>&#13;
&#13;
<p>We could also build a <em>NOT gate</em> (which has one input and converts 1 to 0 and 0 to 1) with:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="n">not_weights</code> <code class="o">=</code> <code class="p">[</code><code class="o">-</code><code class="mf">2.</code><code class="p">]</code>&#13;
<code class="n">not_bias</code> <code class="o">=</code> <code class="mf">1.</code>&#13;
&#13;
<code class="k">assert</code> <code class="n">perceptron_output</code><code class="p">(</code><code class="n">not_weights</code><code class="p">,</code> <code class="n">not_bias</code><code class="p">,</code> <code class="p">[</code><code class="mi">0</code><code class="p">])</code> <code class="o">==</code> <code class="mi">1</code>&#13;
<code class="k">assert</code> <code class="n">perceptron_output</code><code class="p">(</code><code class="n">not_weights</code><code class="p">,</code> <code class="n">not_bias</code><code class="p">,</code> <code class="p">[</code><code class="mi">1</code><code class="p">])</code> <code class="o">==</code> <code class="mi">0</code></pre>&#13;
&#13;
<p>However, there are some problems that simply can’t be solved by a single perceptron.  For example, no matter how hard you try, you cannot use a perceptron to build an <em>XOR gate</em> that outputs 1 if exactly one of its inputs is 1 and 0 otherwise.  This is where we start needing more complicated neural networks.</p>&#13;
&#13;
<p>Of course, you don’t need to approximate a neuron in order to build a logic gate:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="n">and_gate</code> <code class="o">=</code> <code class="nb">min</code>&#13;
<code class="n">or_gate</code> <code class="o">=</code> <code class="nb">max</code>&#13;
<code class="n">xor_gate</code> <code class="o">=</code> <code class="k">lambda</code> <code class="n">x</code><code class="p">,</code> <code class="n">y</code><code class="p">:</code> <code class="mi">0</code> <code class="k">if</code> <code class="n">x</code> <code class="o">==</code> <code class="n">y</code> <code class="k">else</code> <code class="mi">1</code></pre>&#13;
&#13;
<p>Like real neurons, artificial neurons start getting more interesting when you start connecting them together.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Feed-Forward Neural Networks" data-type="sect1"><div class="sect1" id="idm45635732721048">&#13;
<h1>Feed-Forward Neural Networks</h1>&#13;
&#13;
<p>The<a data-primary="neural networks" data-secondary="feed-forward neural networks" data-type="indexterm" id="idm45635732320808"/><a data-primary="feed-forward neural networks" data-type="indexterm" id="idm45635732319832"/> topology of the brain is enormously complicated, so it’s common to approximate it with an idealized <em>feed-forward</em> neural network that consists<a data-primary="layers of neurons" data-type="indexterm" id="idm45635732318568"/> of discrete <em>layers</em> of neurons, each connected to the next.  This typically entails an input layer (which receives inputs and feeds them forward unchanged), one or more “hidden layers” (each of which consists of neurons that take the outputs of the previous layer, performs some calculation, and passes the result to the next layer), and an output layer (which produces the final outputs).</p>&#13;
&#13;
<p>Just like in the perceptron, each (noninput) neuron has a weight corresponding to each of its inputs and a bias. To make our representation simpler, we’ll add the bias to the end of our weights vector and give<a data-primary="bias input" data-type="indexterm" id="idm45635732316296"/> each neuron a <em>bias input</em> that always equals 1.</p>&#13;
&#13;
<p>As with the perceptron, for each neuron we’ll sum up the products of its inputs and its weights.&#13;
But here, rather than outputting the <code>step_function</code> applied to that product, we’ll output a smooth approximation of it.  Here<a data-primary="sigmoid function" data-type="indexterm" id="idm45635732342472"/> we’ll use the <code>sigmoid</code> function (<a data-type="xref" href="#sigmoid_function">Figure 18-2</a>):</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="kn">import</code> <code class="nn">math</code>&#13;
&#13;
<code class="k">def</code> <code class="nf">sigmoid</code><code class="p">(</code><code class="n">t</code><code class="p">:</code> <code class="nb">float</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">float</code><code class="p">:</code>&#13;
    <code class="k">return</code> <code class="mi">1</code> <code class="o">/</code> <code class="p">(</code><code class="mi">1</code> <code class="o">+</code> <code class="n">math</code><code class="o">.</code><code class="n">exp</code><code class="p">(</code><code class="o">-</code><code class="n">t</code><code class="p">))</code></pre>&#13;
&#13;
<figure><div class="figure" id="sigmoid_function">&#13;
<img alt="Sigmoid." src="assets/dsf2_1802.png"/>&#13;
<h6><span class="label">Figure 18-2. </span>The sigmoid function</h6>&#13;
</div></figure>&#13;
&#13;
<p>Why use <code>sigmoid</code> instead of the simpler <code>step_function</code>? In order to train a neural network, we need to use calculus, and in order to use calculus, we need <em>smooth</em> functions. <code>step_function</code> isn’t even continuous, and <code>sigmoid</code> is a good smooth approximation of it.</p>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>You may remember <code>sigmoid</code> from <a data-type="xref" href="ch16.html#logistic_regression">Chapter 16</a>, where it was called <code>logistic</code>.  Technically “sigmoid” refers to the <em>shape</em> of the function and “logistic” to this particular function, although people often use the terms interchangeably.</p>&#13;
</div>&#13;
&#13;
<p>We then calculate the output as:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="k">def</code> <code class="nf">neuron_output</code><code class="p">(</code><code class="n">weights</code><code class="p">:</code> <code class="n">Vector</code><code class="p">,</code> <code class="n">inputs</code><code class="p">:</code> <code class="n">Vector</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">float</code><code class="p">:</code>&#13;
    <code class="c1"># weights includes the bias term, inputs includes a 1</code>&#13;
    <code class="k">return</code> <code class="n">sigmoid</code><code class="p">(</code><code class="n">dot</code><code class="p">(</code><code class="n">weights</code><code class="p">,</code> <code class="n">inputs</code><code class="p">))</code></pre>&#13;
&#13;
<p>Given this function, we can represent a neuron simply as a vector of weights whose length is one more than the number of inputs to that neuron (because of the bias weight).  Then we can represent a neural network as a list of (noninput) <em>layers</em>, where each layer is just a list of the neurons in that layer.</p>&#13;
&#13;
<p>That is, we’ll represent a neural network as a list (layers) of lists (neurons) of vectors (weights).</p>&#13;
&#13;
<p>Given such a representation, using the neural network is quite simple:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="kn">from</code> <code class="nn">typing</code> <code class="kn">import</code> <code class="n">List</code>&#13;
&#13;
<code class="k">def</code> <code class="nf">feed_forward</code><code class="p">(</code><code class="n">neural_network</code><code class="p">:</code> <code class="n">List</code><code class="p">[</code><code class="n">List</code><code class="p">[</code><code class="n">Vector</code><code class="p">]],</code>&#13;
                 <code class="n">input_vector</code><code class="p">:</code> <code class="n">Vector</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="n">List</code><code class="p">[</code><code class="n">Vector</code><code class="p">]:</code>&#13;
    <code class="sd">"""</code>&#13;
<code class="sd">    Feeds the input vector through the neural network.</code>&#13;
<code class="sd">    Returns the outputs of all layers (not just the last one).</code>&#13;
<code class="sd">    """</code>&#13;
    <code class="n">outputs</code><code class="p">:</code> <code class="n">List</code><code class="p">[</code><code class="n">Vector</code><code class="p">]</code> <code class="o">=</code> <code class="p">[]</code>&#13;
&#13;
    <code class="k">for</code> <code class="n">layer</code> <code class="ow">in</code> <code class="n">neural_network</code><code class="p">:</code>&#13;
        <code class="n">input_with_bias</code> <code class="o">=</code> <code class="n">input_vector</code> <code class="o">+</code> <code class="p">[</code><code class="mi">1</code><code class="p">]</code>              <code class="c1"># Add a constant.</code>&#13;
        <code class="n">output</code> <code class="o">=</code> <code class="p">[</code><code class="n">neuron_output</code><code class="p">(</code><code class="n">neuron</code><code class="p">,</code> <code class="n">input_with_bias</code><code class="p">)</code>  <code class="c1"># Compute the output</code>&#13;
                  <code class="k">for</code> <code class="n">neuron</code> <code class="ow">in</code> <code class="n">layer</code><code class="p">]</code>                    <code class="c1"># for each neuron.</code>&#13;
        <code class="n">outputs</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">output</code><code class="p">)</code>                            <code class="c1"># Add to results.</code>&#13;
&#13;
        <code class="c1"># Then the input to the next layer is the output of this one</code>&#13;
        <code class="n">input_vector</code> <code class="o">=</code> <code class="n">output</code>&#13;
&#13;
    <code class="k">return</code> <code class="n">outputs</code></pre>&#13;
&#13;
<p>Now it’s easy to build the XOR gate that we couldn’t build with a single perceptron.&#13;
We just need to scale the weights up so that the <code>neuron_output</code>s&#13;
are either really close to 0 or really close to 1:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="n">xor_network</code> <code class="o">=</code> <code class="p">[</code><code class="c1"># hidden layer</code>&#13;
               <code class="p">[[</code><code class="mf">20.</code><code class="p">,</code> <code class="mi">20</code><code class="p">,</code> <code class="o">-</code><code class="mi">30</code><code class="p">],</code>      <code class="c1"># 'and' neuron</code>&#13;
                <code class="p">[</code><code class="mf">20.</code><code class="p">,</code> <code class="mi">20</code><code class="p">,</code> <code class="o">-</code><code class="mi">10</code><code class="p">]],</code>     <code class="c1"># 'or'  neuron</code>&#13;
               <code class="c1"># output layer</code>&#13;
               <code class="p">[[</code><code class="o">-</code><code class="mf">60.</code><code class="p">,</code> <code class="mi">60</code><code class="p">,</code> <code class="o">-</code><code class="mi">30</code><code class="p">]]]</code>    <code class="c1"># '2nd input but not 1st input' neuron</code>&#13;
&#13;
<code class="c1"># feed_forward returns the outputs of all layers, so the [-1] gets the</code>&#13;
<code class="c1"># final output, and the [0] gets the value out of the resulting vector</code>&#13;
<code class="k">assert</code> <code class="mf">0.000</code> <code class="o">&lt;</code> <code class="n">feed_forward</code><code class="p">(</code><code class="n">xor_network</code><code class="p">,</code> <code class="p">[</code><code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">])[</code><code class="o">-</code><code class="mi">1</code><code class="p">][</code><code class="mi">0</code><code class="p">]</code> <code class="o">&lt;</code> <code class="mf">0.001</code>&#13;
<code class="k">assert</code> <code class="mf">0.999</code> <code class="o">&lt;</code> <code class="n">feed_forward</code><code class="p">(</code><code class="n">xor_network</code><code class="p">,</code> <code class="p">[</code><code class="mi">1</code><code class="p">,</code> <code class="mi">0</code><code class="p">])[</code><code class="o">-</code><code class="mi">1</code><code class="p">][</code><code class="mi">0</code><code class="p">]</code> <code class="o">&lt;</code> <code class="mf">1.000</code>&#13;
<code class="k">assert</code> <code class="mf">0.999</code> <code class="o">&lt;</code> <code class="n">feed_forward</code><code class="p">(</code><code class="n">xor_network</code><code class="p">,</code> <code class="p">[</code><code class="mi">0</code><code class="p">,</code> <code class="mi">1</code><code class="p">])[</code><code class="o">-</code><code class="mi">1</code><code class="p">][</code><code class="mi">0</code><code class="p">]</code> <code class="o">&lt;</code> <code class="mf">1.000</code>&#13;
<code class="k">assert</code> <code class="mf">0.000</code> <code class="o">&lt;</code> <code class="n">feed_forward</code><code class="p">(</code><code class="n">xor_network</code><code class="p">,</code> <code class="p">[</code><code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">])[</code><code class="o">-</code><code class="mi">1</code><code class="p">][</code><code class="mi">0</code><code class="p">]</code> <code class="o">&lt;</code> <code class="mf">0.001</code></pre>&#13;
&#13;
<p>For a given input (which is a two-dimensional vector),&#13;
the hidden layer produces a two-dimensional vector consisting of&#13;
the “and” of the two input values and the “or” of the two input values.</p>&#13;
&#13;
<p>And the output layer takes a two-dimensional vector and computes “second element but not first element.”&#13;
The result is a network that performs “or, but not and,” which is precisely XOR (<a data-type="xref" href="#xor_neural_network">Figure 18-3</a>).</p>&#13;
&#13;
<figure><div class="figure" id="xor_neural_network">&#13;
<img alt="Neural Network." src="assets/dsf2_1803.png"/>&#13;
<h6><span class="label">Figure 18-3. </span>A neural network for XOR</h6>&#13;
</div></figure>&#13;
&#13;
<p>One<a data-primary="features" data-type="indexterm" id="idm45635731874520"/> suggestive way of thinking about this is that the hidden layer is computing <em>features</em>&#13;
of the input data (in this case “and” and “or”) and the output layer is combining those&#13;
features in a way that generates the desired output.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Backpropagation" data-type="sect1"><div class="sect1" id="idm45635732321720">&#13;
<h1>Backpropagation</h1>&#13;
&#13;
<p>Usually<a data-primary="neural networks" data-secondary="backpropagation" data-type="indexterm" id="idm45635731871880"/><a data-primary="backpropagation" data-type="indexterm" id="idm45635731870872"/> we don’t build neural networks by hand.  This is in part because we use them to solve much bigger problems—an image recognition problem might involve hundreds or thousands of neurons.  And it’s in part because we usually won’t be able to “reason out” what the neurons should be.</p>&#13;
&#13;
<p>Instead (as usual) we use data to <em>train</em> neural networks.&#13;
The typical approach is an algorithm called <em>backpropagation</em>,&#13;
which uses gradient descent or one of its variants.</p>&#13;
&#13;
<p>Imagine we have a training set that consists of input vectors and corresponding target output vectors.  For example, in our previous <code>xor_network</code> example, the input vector <code>[1, 0]</code> corresponded to the target output <code>[1]</code>.  Imagine that our network has some set of weights.  We then adjust the weights using the following algorithm:</p>&#13;
<ol>&#13;
<li>&#13;
<p>Run <code>feed_forward</code> on an input vector to produce the outputs of all the neurons in the network.</p>&#13;
</li>&#13;
<li>&#13;
<p>We know the target output, so we can compute a <em>loss</em> that’s the sum of the squared errors.</p>&#13;
</li>&#13;
<li>&#13;
<p>Compute the gradient of this loss as a function of the output neuron’s weights.</p>&#13;
</li>&#13;
<li>&#13;
<p>“Propagate” the gradients and errors backward to compute the gradients&#13;
with respect to the hidden neurons’ weights.</p>&#13;
</li>&#13;
<li>&#13;
<p>Take a gradient descent step.</p>&#13;
</li>&#13;
&#13;
</ol>&#13;
&#13;
<p>Typically we run this algorithm many times for our entire training set until the network converges.</p>&#13;
&#13;
<p>To start with, let’s write the function to compute the gradients:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="k">def</code> <code class="nf">sqerror_gradients</code><code class="p">(</code><code class="n">network</code><code class="p">:</code> <code class="n">List</code><code class="p">[</code><code class="n">List</code><code class="p">[</code><code class="n">Vector</code><code class="p">]],</code>&#13;
                      <code class="n">input_vector</code><code class="p">:</code> <code class="n">Vector</code><code class="p">,</code>&#13;
                      <code class="n">target_vector</code><code class="p">:</code> <code class="n">Vector</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="n">List</code><code class="p">[</code><code class="n">List</code><code class="p">[</code><code class="n">Vector</code><code class="p">]]:</code>&#13;
    <code class="sd">"""</code>&#13;
<code class="sd">    Given a neural network, an input vector, and a target vector,</code>&#13;
<code class="sd">    make a prediction and compute the gradient of the squared error</code>&#13;
<code class="sd">    loss with respect to the neuron weights.</code>&#13;
<code class="sd">    """</code>&#13;
    <code class="c1"># forward pass</code>&#13;
    <code class="n">hidden_outputs</code><code class="p">,</code> <code class="n">outputs</code> <code class="o">=</code> <code class="n">feed_forward</code><code class="p">(</code><code class="n">network</code><code class="p">,</code> <code class="n">input_vector</code><code class="p">)</code>&#13;
&#13;
    <code class="c1"># gradients with respect to output neuron pre-activation outputs</code>&#13;
    <code class="n">output_deltas</code> <code class="o">=</code> <code class="p">[</code><code class="n">output</code> <code class="o">*</code> <code class="p">(</code><code class="mi">1</code> <code class="o">-</code> <code class="n">output</code><code class="p">)</code> <code class="o">*</code> <code class="p">(</code><code class="n">output</code> <code class="o">-</code> <code class="n">target</code><code class="p">)</code>&#13;
                     <code class="k">for</code> <code class="n">output</code><code class="p">,</code> <code class="n">target</code> <code class="ow">in</code> <code class="nb">zip</code><code class="p">(</code><code class="n">outputs</code><code class="p">,</code> <code class="n">target_vector</code><code class="p">)]</code>&#13;
&#13;
    <code class="c1"># gradients with respect to output neuron weights</code>&#13;
    <code class="n">output_grads</code> <code class="o">=</code> <code class="p">[[</code><code class="n">output_deltas</code><code class="p">[</code><code class="n">i</code><code class="p">]</code> <code class="o">*</code> <code class="n">hidden_output</code>&#13;
                     <code class="k">for</code> <code class="n">hidden_output</code> <code class="ow">in</code> <code class="n">hidden_outputs</code> <code class="o">+</code> <code class="p">[</code><code class="mi">1</code><code class="p">]]</code>&#13;
                    <code class="k">for</code> <code class="n">i</code><code class="p">,</code> <code class="n">output_neuron</code> <code class="ow">in</code> <code class="nb">enumerate</code><code class="p">(</code><code class="n">network</code><code class="p">[</code><code class="o">-</code><code class="mi">1</code><code class="p">])]</code>&#13;
&#13;
    <code class="c1"># gradients with respect to hidden neuron pre-activation outputs</code>&#13;
    <code class="n">hidden_deltas</code> <code class="o">=</code> <code class="p">[</code><code class="n">hidden_output</code> <code class="o">*</code> <code class="p">(</code><code class="mi">1</code> <code class="o">-</code> <code class="n">hidden_output</code><code class="p">)</code> <code class="o">*</code>&#13;
                         <code class="n">dot</code><code class="p">(</code><code class="n">output_deltas</code><code class="p">,</code> <code class="p">[</code><code class="n">n</code><code class="p">[</code><code class="n">i</code><code class="p">]</code> <code class="k">for</code> <code class="n">n</code> <code class="ow">in</code> <code class="n">network</code><code class="p">[</code><code class="o">-</code><code class="mi">1</code><code class="p">]])</code>&#13;
                     <code class="k">for</code> <code class="n">i</code><code class="p">,</code> <code class="n">hidden_output</code> <code class="ow">in</code> <code class="nb">enumerate</code><code class="p">(</code><code class="n">hidden_outputs</code><code class="p">)]</code>&#13;
&#13;
    <code class="c1"># gradients with respect to hidden neuron weights</code>&#13;
    <code class="n">hidden_grads</code> <code class="o">=</code> <code class="p">[[</code><code class="n">hidden_deltas</code><code class="p">[</code><code class="n">i</code><code class="p">]</code> <code class="o">*</code> <code class="nb">input</code> <code class="k">for</code> <code class="nb">input</code> <code class="ow">in</code> <code class="n">input_vector</code> <code class="o">+</code> <code class="p">[</code><code class="mi">1</code><code class="p">]]</code>&#13;
                    <code class="k">for</code> <code class="n">i</code><code class="p">,</code> <code class="n">hidden_neuron</code> <code class="ow">in</code> <code class="nb">enumerate</code><code class="p">(</code><code class="n">network</code><code class="p">[</code><code class="mi">0</code><code class="p">])]</code>&#13;
&#13;
    <code class="k">return</code> <code class="p">[</code><code class="n">hidden_grads</code><code class="p">,</code> <code class="n">output_grads</code><code class="p">]</code></pre>&#13;
&#13;
<p>The math behind the preceding calculations is not terribly difficult,&#13;
but it involves some tedious calculus and careful attention to detail,&#13;
so I’ll leave it as an exercise for you.</p>&#13;
&#13;
<p>Armed with the ability to compute gradients, we can now train neural&#13;
networks. Let’s try to learn the XOR network we previously designed by hand.</p>&#13;
&#13;
<p>We’ll start by generating the training data and initializing&#13;
our neural network with random weights:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="kn">import</code> <code class="nn">random</code>&#13;
<code class="n">random</code><code class="o">.</code><code class="n">seed</code><code class="p">(</code><code class="mi">0</code><code class="p">)</code>&#13;
&#13;
<code class="c1"># training data</code>&#13;
<code class="n">xs</code> <code class="o">=</code> <code class="p">[[</code><code class="mf">0.</code><code class="p">,</code> <code class="mi">0</code><code class="p">],</code> <code class="p">[</code><code class="mf">0.</code><code class="p">,</code> <code class="mi">1</code><code class="p">],</code> <code class="p">[</code><code class="mf">1.</code><code class="p">,</code> <code class="mi">0</code><code class="p">],</code> <code class="p">[</code><code class="mf">1.</code><code class="p">,</code> <code class="mi">1</code><code class="p">]]</code>&#13;
<code class="n">ys</code> <code class="o">=</code> <code class="p">[[</code><code class="mf">0.</code><code class="p">],</code> <code class="p">[</code><code class="mf">1.</code><code class="p">],</code> <code class="p">[</code><code class="mf">1.</code><code class="p">],</code> <code class="p">[</code><code class="mf">0.</code><code class="p">]]</code>&#13;
&#13;
<code class="c1"># start with random weights</code>&#13;
<code class="n">network</code> <code class="o">=</code> <code class="p">[</code> <code class="c1"># hidden layer: 2 inputs -&gt; 2 outputs</code>&#13;
            <code class="p">[[</code><code class="n">random</code><code class="o">.</code><code class="n">random</code><code class="p">()</code> <code class="k">for</code> <code class="n">_</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">2</code> <code class="o">+</code> <code class="mi">1</code><code class="p">)],</code>   <code class="c1"># 1st hidden neuron</code>&#13;
             <code class="p">[</code><code class="n">random</code><code class="o">.</code><code class="n">random</code><code class="p">()</code> <code class="k">for</code> <code class="n">_</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">2</code> <code class="o">+</code> <code class="mi">1</code><code class="p">)]],</code>  <code class="c1"># 2nd hidden neuron</code>&#13;
            <code class="c1"># output layer: 2 inputs -&gt; 1 output</code>&#13;
            <code class="p">[[</code><code class="n">random</code><code class="o">.</code><code class="n">random</code><code class="p">()</code> <code class="k">for</code> <code class="n">_</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">2</code> <code class="o">+</code> <code class="mi">1</code><code class="p">)]]</code>   <code class="c1"># 1st output neuron</code>&#13;
          <code class="p">]</code></pre>&#13;
&#13;
<p>As usual, we can train it using gradient descent. One difference&#13;
from our previous examples is that here we have several parameter vectors,&#13;
each with its own gradient, which means we’ll have to call <code>gradient_step</code>&#13;
for each of them.</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="kn">from</code> <code class="nn">scratch.gradient_descent</code> <code class="kn">import</code> <code class="n">gradient_step</code>&#13;
<code class="kn">import</code> <code class="nn">tqdm</code>&#13;
&#13;
<code class="n">learning_rate</code> <code class="o">=</code> <code class="mf">1.0</code>&#13;
&#13;
<code class="k">for</code> <code class="n">epoch</code> <code class="ow">in</code> <code class="n">tqdm</code><code class="o">.</code><code class="n">trange</code><code class="p">(</code><code class="mi">20000</code><code class="p">,</code> <code class="n">desc</code><code class="o">=</code><code class="s2">"neural net for xor"</code><code class="p">):</code>&#13;
    <code class="k">for</code> <code class="n">x</code><code class="p">,</code> <code class="n">y</code> <code class="ow">in</code> <code class="nb">zip</code><code class="p">(</code><code class="n">xs</code><code class="p">,</code> <code class="n">ys</code><code class="p">):</code>&#13;
        <code class="n">gradients</code> <code class="o">=</code> <code class="n">sqerror_gradients</code><code class="p">(</code><code class="n">network</code><code class="p">,</code> <code class="n">x</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code>&#13;
&#13;
        <code class="c1"># Take a gradient step for each neuron in each layer</code>&#13;
        <code class="n">network</code> <code class="o">=</code> <code class="p">[[</code><code class="n">gradient_step</code><code class="p">(</code><code class="n">neuron</code><code class="p">,</code> <code class="n">grad</code><code class="p">,</code> <code class="o">-</code><code class="n">learning_rate</code><code class="p">)</code>&#13;
                    <code class="k">for</code> <code class="n">neuron</code><code class="p">,</code> <code class="n">grad</code> <code class="ow">in</code> <code class="nb">zip</code><code class="p">(</code><code class="n">layer</code><code class="p">,</code> <code class="n">layer_grad</code><code class="p">)]</code>&#13;
                   <code class="k">for</code> <code class="n">layer</code><code class="p">,</code> <code class="n">layer_grad</code> <code class="ow">in</code> <code class="nb">zip</code><code class="p">(</code><code class="n">network</code><code class="p">,</code> <code class="n">gradients</code><code class="p">)]</code>&#13;
&#13;
<code class="c1"># check that it learned XOR</code>&#13;
<code class="k">assert</code> <code class="n">feed_forward</code><code class="p">(</code><code class="n">network</code><code class="p">,</code> <code class="p">[</code><code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">])[</code><code class="o">-</code><code class="mi">1</code><code class="p">][</code><code class="mi">0</code><code class="p">]</code> <code class="o">&lt;</code> <code class="mf">0.01</code>&#13;
<code class="k">assert</code> <code class="n">feed_forward</code><code class="p">(</code><code class="n">network</code><code class="p">,</code> <code class="p">[</code><code class="mi">0</code><code class="p">,</code> <code class="mi">1</code><code class="p">])[</code><code class="o">-</code><code class="mi">1</code><code class="p">][</code><code class="mi">0</code><code class="p">]</code> <code class="o">&gt;</code> <code class="mf">0.99</code>&#13;
<code class="k">assert</code> <code class="n">feed_forward</code><code class="p">(</code><code class="n">network</code><code class="p">,</code> <code class="p">[</code><code class="mi">1</code><code class="p">,</code> <code class="mi">0</code><code class="p">])[</code><code class="o">-</code><code class="mi">1</code><code class="p">][</code><code class="mi">0</code><code class="p">]</code> <code class="o">&gt;</code> <code class="mf">0.99</code>&#13;
<code class="k">assert</code> <code class="n">feed_forward</code><code class="p">(</code><code class="n">network</code><code class="p">,</code> <code class="p">[</code><code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">])[</code><code class="o">-</code><code class="mi">1</code><code class="p">][</code><code class="mi">0</code><code class="p">]</code> <code class="o">&lt;</code> <code class="mf">0.01</code></pre>&#13;
&#13;
<p>For me the resulting network has weights that look like:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="p">[</code>   <code class="c1"># hidden layer</code>&#13;
    <code class="p">[[</code><code class="mi">7</code><code class="p">,</code> <code class="mi">7</code><code class="p">,</code> <code class="o">-</code><code class="mi">3</code><code class="p">],</code>     <code class="c1"># computes OR</code>&#13;
     <code class="p">[</code><code class="mi">5</code><code class="p">,</code> <code class="mi">5</code><code class="p">,</code> <code class="o">-</code><code class="mi">8</code><code class="p">]],</code>    <code class="c1"># computes AND</code>&#13;
    <code class="c1"># output layer</code>&#13;
    <code class="p">[[</code><code class="mi">11</code><code class="p">,</code> <code class="o">-</code><code class="mi">12</code><code class="p">,</code> <code class="o">-</code><code class="mi">5</code><code class="p">]]</code>  <code class="c1"># computes "first but not second"</code>&#13;
<code class="p">]</code></pre>&#13;
&#13;
<p>which is conceptually pretty similar to our previous bespoke network.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Example: Fizz Buzz" data-type="sect1"><div class="sect1" id="fizzbuzz">&#13;
<h1>Example: Fizz Buzz</h1>&#13;
&#13;
<p>The<a data-primary="neural networks" data-secondary="Fizz Buzz example" data-type="indexterm" id="idm45635731127624"/><a data-primary="Fizz Buzz example" data-type="indexterm" id="idm45635731126616"/> VP of Engineering wants to interview technical candidates&#13;
by making them solve “Fizz Buzz,” the following well-trod programming challenge:</p>&#13;
&#13;
<pre data-type="programlisting">Print the numbers 1 to 100, except that if the number is divisible&#13;
by 3, print "fizz"; if the number is divisible by 5, print "buzz";&#13;
and if the number is divisible by 15, print "fizzbuzz".</pre>&#13;
&#13;
<p>He thinks the ability to solve this demonstrates extreme programming skill.&#13;
You think that this problem is so simple that a neural network could solve it.</p>&#13;
&#13;
<p>Neural networks take vectors as inputs and produce vectors as outputs.&#13;
As stated, the programming problem is to turn an integer into a string.&#13;
So the first challenge is to come up with a way to recast it as a vector problem.</p>&#13;
&#13;
<p>For the outputs it’s not tough: there are basically four classes of outputs,&#13;
so we can encode the output as a vector of four 0s and 1s:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="k">def</code> <code class="nf">fizz_buzz_encode</code><code class="p">(</code><code class="n">x</code><code class="p">:</code> <code class="nb">int</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="n">Vector</code><code class="p">:</code>&#13;
    <code class="k">if</code> <code class="n">x</code> <code class="o">%</code> <code class="mi">15</code> <code class="o">==</code> <code class="mi">0</code><code class="p">:</code>&#13;
        <code class="k">return</code> <code class="p">[</code><code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">1</code><code class="p">]</code>&#13;
    <code class="k">elif</code> <code class="n">x</code> <code class="o">%</code> <code class="mi">5</code> <code class="o">==</code> <code class="mi">0</code><code class="p">:</code>&#13;
        <code class="k">return</code> <code class="p">[</code><code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">1</code><code class="p">,</code> <code class="mi">0</code><code class="p">]</code>&#13;
    <code class="k">elif</code> <code class="n">x</code> <code class="o">%</code> <code class="mi">3</code> <code class="o">==</code> <code class="mi">0</code><code class="p">:</code>&#13;
        <code class="k">return</code> <code class="p">[</code><code class="mi">0</code><code class="p">,</code> <code class="mi">1</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">]</code>&#13;
    <code class="k">else</code><code class="p">:</code>&#13;
        <code class="k">return</code> <code class="p">[</code><code class="mi">1</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">]</code>&#13;
&#13;
<code class="k">assert</code> <code class="n">fizz_buzz_encode</code><code class="p">(</code><code class="mi">2</code><code class="p">)</code> <code class="o">==</code> <code class="p">[</code><code class="mi">1</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">]</code>&#13;
<code class="k">assert</code> <code class="n">fizz_buzz_encode</code><code class="p">(</code><code class="mi">6</code><code class="p">)</code> <code class="o">==</code> <code class="p">[</code><code class="mi">0</code><code class="p">,</code> <code class="mi">1</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">]</code>&#13;
<code class="k">assert</code> <code class="n">fizz_buzz_encode</code><code class="p">(</code><code class="mi">10</code><code class="p">)</code> <code class="o">==</code> <code class="p">[</code><code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">1</code><code class="p">,</code> <code class="mi">0</code><code class="p">]</code>&#13;
<code class="k">assert</code> <code class="n">fizz_buzz_encode</code><code class="p">(</code><code class="mi">30</code><code class="p">)</code> <code class="o">==</code> <code class="p">[</code><code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">1</code><code class="p">]</code></pre>&#13;
&#13;
<p>We’ll use this to generate our target vectors. The input vectors are less obvious.&#13;
You don’t want to just use a one-dimensional vector containing the input number, for a couple of reasons.&#13;
A single input captures an “intensity,” but the fact that 2 is twice as much as 1, and that 4 is twice as much&#13;
again, doesn’t feel relevant to this problem. Additionally, with just one input the hidden layer wouldn’t be able&#13;
to compute very interesting features, which means it probably wouldn’t be able to solve the problem.</p>&#13;
&#13;
<p>It turns out that one thing that works reasonably well is to&#13;
convert each number to its <em>binary</em> representation of 1s and 0s.&#13;
(Don’t worry, this isn’t obvious—at least it wasn’t to me.)</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="k">def</code> <code class="nf">binary_encode</code><code class="p">(</code><code class="n">x</code><code class="p">:</code> <code class="nb">int</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="n">Vector</code><code class="p">:</code>&#13;
    <code class="n">binary</code><code class="p">:</code> <code class="n">List</code><code class="p">[</code><code class="nb">float</code><code class="p">]</code> <code class="o">=</code> <code class="p">[]</code>&#13;
&#13;
    <code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">10</code><code class="p">):</code>&#13;
        <code class="n">binary</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">x</code> <code class="o">%</code> <code class="mi">2</code><code class="p">)</code>&#13;
        <code class="n">x</code> <code class="o">=</code> <code class="n">x</code> <code class="o">//</code> <code class="mi">2</code>&#13;
&#13;
    <code class="k">return</code> <code class="n">binary</code>&#13;
&#13;
<code class="c1">#                             1  2  4  8 16 32 64 128 256 512</code>&#13;
<code class="k">assert</code> <code class="n">binary_encode</code><code class="p">(</code><code class="mi">0</code><code class="p">)</code>   <code class="o">==</code> <code class="p">[</code><code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code>  <code class="mi">0</code><code class="p">,</code>  <code class="mi">0</code><code class="p">]</code>&#13;
<code class="k">assert</code> <code class="n">binary_encode</code><code class="p">(</code><code class="mi">1</code><code class="p">)</code>   <code class="o">==</code> <code class="p">[</code><code class="mi">1</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code>  <code class="mi">0</code><code class="p">,</code>  <code class="mi">0</code><code class="p">]</code>&#13;
<code class="k">assert</code> <code class="n">binary_encode</code><code class="p">(</code><code class="mi">10</code><code class="p">)</code>  <code class="o">==</code> <code class="p">[</code><code class="mi">0</code><code class="p">,</code> <code class="mi">1</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">1</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code>  <code class="mi">0</code><code class="p">,</code>  <code class="mi">0</code><code class="p">]</code>&#13;
<code class="k">assert</code> <code class="n">binary_encode</code><code class="p">(</code><code class="mi">101</code><code class="p">)</code> <code class="o">==</code> <code class="p">[</code><code class="mi">1</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">1</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code>  <code class="mi">0</code><code class="p">,</code>  <code class="mi">0</code><code class="p">]</code>&#13;
<code class="k">assert</code> <code class="n">binary_encode</code><code class="p">(</code><code class="mi">999</code><code class="p">)</code> <code class="o">==</code> <code class="p">[</code><code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">,</code>  <code class="mi">1</code><code class="p">,</code>  <code class="mi">1</code><code class="p">]</code></pre>&#13;
&#13;
<p>As the goal is to construct the outputs for the numbers 1 to 100,&#13;
it would be cheating to train on those numbers. Therefore, we’ll&#13;
train on the numbers 101 to 1,023 (which is the largest number&#13;
we can represent with 10 binary digits):</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="n">xs</code> <code class="o">=</code> <code class="p">[</code><code class="n">binary_encode</code><code class="p">(</code><code class="n">n</code><code class="p">)</code> <code class="k">for</code> <code class="n">n</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">101</code><code class="p">,</code> <code class="mi">1024</code><code class="p">)]</code>&#13;
<code class="n">ys</code> <code class="o">=</code> <code class="p">[</code><code class="n">fizz_buzz_encode</code><code class="p">(</code><code class="n">n</code><code class="p">)</code> <code class="k">for</code> <code class="n">n</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">101</code><code class="p">,</code> <code class="mi">1024</code><code class="p">)]</code></pre>&#13;
&#13;
<p>Next, let’s create a neural network with random initial weights.&#13;
It will have 10 input neurons (since we’re representing our inputs&#13;
as 10-dimensional vectors) and 4 output neurons (since we’re representing&#13;
our targets as 4-dimensional vectors). We’ll give it 25 hidden units,&#13;
but we’ll use a variable for that so it’s easy to change:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="n">NUM_HIDDEN</code> <code class="o">=</code> <code class="mi">25</code>&#13;
&#13;
<code class="n">network</code> <code class="o">=</code> <code class="p">[</code>&#13;
    <code class="c1"># hidden layer: 10 inputs -&gt; NUM_HIDDEN outputs</code>&#13;
    <code class="p">[[</code><code class="n">random</code><code class="o">.</code><code class="n">random</code><code class="p">()</code> <code class="k">for</code> <code class="n">_</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">10</code> <code class="o">+</code> <code class="mi">1</code><code class="p">)]</code> <code class="k">for</code> <code class="n">_</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">NUM_HIDDEN</code><code class="p">)],</code>&#13;
&#13;
    <code class="c1"># output_layer: NUM_HIDDEN inputs -&gt; 4 outputs</code>&#13;
    <code class="p">[[</code><code class="n">random</code><code class="o">.</code><code class="n">random</code><code class="p">()</code> <code class="k">for</code> <code class="n">_</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">NUM_HIDDEN</code> <code class="o">+</code> <code class="mi">1</code><code class="p">)]</code> <code class="k">for</code> <code class="n">_</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">4</code><code class="p">)]</code>&#13;
<code class="p">]</code></pre>&#13;
&#13;
<p>That’s it. Now we’re ready to train.&#13;
Because this is a more involved problem (and there are a lot more things to mess up),&#13;
we’d like to closely monitor the training process. In particular, for each epoch&#13;
we’ll track the sum of squared errors and print them out. We want to make sure&#13;
they decrease:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="kn">from</code> <code class="nn">scratch.linear_algebra</code> <code class="kn">import</code> <code class="n">squared_distance</code>&#13;
&#13;
<code class="n">learning_rate</code> <code class="o">=</code> <code class="mf">1.0</code>&#13;
&#13;
<code class="k">with</code> <code class="n">tqdm</code><code class="o">.</code><code class="n">trange</code><code class="p">(</code><code class="mi">500</code><code class="p">)</code> <code class="k">as</code> <code class="n">t</code><code class="p">:</code>&#13;
    <code class="k">for</code> <code class="n">epoch</code> <code class="ow">in</code> <code class="n">t</code><code class="p">:</code>&#13;
        <code class="n">epoch_loss</code> <code class="o">=</code> <code class="mf">0.0</code>&#13;
&#13;
        <code class="k">for</code> <code class="n">x</code><code class="p">,</code> <code class="n">y</code> <code class="ow">in</code> <code class="nb">zip</code><code class="p">(</code><code class="n">xs</code><code class="p">,</code> <code class="n">ys</code><code class="p">):</code>&#13;
            <code class="n">predicted</code> <code class="o">=</code> <code class="n">feed_forward</code><code class="p">(</code><code class="n">network</code><code class="p">,</code> <code class="n">x</code><code class="p">)[</code><code class="o">-</code><code class="mi">1</code><code class="p">]</code>&#13;
            <code class="n">epoch_loss</code> <code class="o">+=</code> <code class="n">squared_distance</code><code class="p">(</code><code class="n">predicted</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code>&#13;
            <code class="n">gradients</code> <code class="o">=</code> <code class="n">sqerror_gradients</code><code class="p">(</code><code class="n">network</code><code class="p">,</code> <code class="n">x</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code>&#13;
&#13;
            <code class="c1"># Take a gradient step for each neuron in each layer</code>&#13;
            <code class="n">network</code> <code class="o">=</code> <code class="p">[[</code><code class="n">gradient_step</code><code class="p">(</code><code class="n">neuron</code><code class="p">,</code> <code class="n">grad</code><code class="p">,</code> <code class="o">-</code><code class="n">learning_rate</code><code class="p">)</code>&#13;
                        <code class="k">for</code> <code class="n">neuron</code><code class="p">,</code> <code class="n">grad</code> <code class="ow">in</code> <code class="nb">zip</code><code class="p">(</code><code class="n">layer</code><code class="p">,</code> <code class="n">layer_grad</code><code class="p">)]</code>&#13;
                    <code class="k">for</code> <code class="n">layer</code><code class="p">,</code> <code class="n">layer_grad</code> <code class="ow">in</code> <code class="nb">zip</code><code class="p">(</code><code class="n">network</code><code class="p">,</code> <code class="n">gradients</code><code class="p">)]</code>&#13;
&#13;
        <code class="n">t</code><code class="o">.</code><code class="n">set_description</code><code class="p">(</code><code class="n">f</code><code class="s2">"fizz buzz (loss: {epoch_loss:.2f})"</code><code class="p">)</code></pre>&#13;
&#13;
<p>This will take a while to train, but eventually the loss should start to bottom out.</p>&#13;
&#13;
<p>At last we’re ready to solve our original problem. We have one remaining issue.&#13;
Our network will produce a four-dimensional vector of numbers, but we want a single&#13;
prediction. We’ll do that by taking the <code>argmax</code>, which is the index of the largest value:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="k">def</code> <code class="nf">argmax</code><code class="p">(</code><code class="n">xs</code><code class="p">:</code> <code class="nb">list</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">int</code><code class="p">:</code>&#13;
    <code class="sd">"""Returns the index of the largest value"""</code>&#13;
    <code class="k">return</code> <code class="nb">max</code><code class="p">(</code><code class="nb">range</code><code class="p">(</code><code class="nb">len</code><code class="p">(</code><code class="n">xs</code><code class="p">)),</code> <code class="n">key</code><code class="o">=</code><code class="k">lambda</code> <code class="n">i</code><code class="p">:</code> <code class="n">xs</code><code class="p">[</code><code class="n">i</code><code class="p">])</code>&#13;
&#13;
<code class="k">assert</code> <code class="n">argmax</code><code class="p">([</code><code class="mi">0</code><code class="p">,</code> <code class="o">-</code><code class="mi">1</code><code class="p">])</code> <code class="o">==</code> <code class="mi">0</code>               <code class="c1"># items[0] is largest</code>&#13;
<code class="k">assert</code> <code class="n">argmax</code><code class="p">([</code><code class="o">-</code><code class="mi">1</code><code class="p">,</code> <code class="mi">0</code><code class="p">])</code> <code class="o">==</code> <code class="mi">1</code>               <code class="c1"># items[1] is largest</code>&#13;
<code class="k">assert</code> <code class="n">argmax</code><code class="p">([</code><code class="o">-</code><code class="mi">1</code><code class="p">,</code> <code class="mi">10</code><code class="p">,</code> <code class="mi">5</code><code class="p">,</code> <code class="mi">20</code><code class="p">,</code> <code class="o">-</code><code class="mi">3</code><code class="p">])</code> <code class="o">==</code> <code class="mi">3</code>   <code class="c1"># items[3] is largest</code></pre>&#13;
&#13;
<p>Now we can finally solve “FizzBuzz”:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="n">num_correct</code> <code class="o">=</code> <code class="mi">0</code>&#13;
&#13;
<code class="k">for</code> <code class="n">n</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="mi">101</code><code class="p">):</code>&#13;
    <code class="n">x</code> <code class="o">=</code> <code class="n">binary_encode</code><code class="p">(</code><code class="n">n</code><code class="p">)</code>&#13;
    <code class="n">predicted</code> <code class="o">=</code> <code class="n">argmax</code><code class="p">(</code><code class="n">feed_forward</code><code class="p">(</code><code class="n">network</code><code class="p">,</code> <code class="n">x</code><code class="p">)[</code><code class="o">-</code><code class="mi">1</code><code class="p">])</code>&#13;
    <code class="n">actual</code> <code class="o">=</code> <code class="n">argmax</code><code class="p">(</code><code class="n">fizz_buzz_encode</code><code class="p">(</code><code class="n">n</code><code class="p">))</code>&#13;
    <code class="n">labels</code> <code class="o">=</code> <code class="p">[</code><code class="nb">str</code><code class="p">(</code><code class="n">n</code><code class="p">),</code> <code class="s2">"fizz"</code><code class="p">,</code> <code class="s2">"buzz"</code><code class="p">,</code> <code class="s2">"fizzbuzz"</code><code class="p">]</code>&#13;
    <code class="k">print</code><code class="p">(</code><code class="n">n</code><code class="p">,</code> <code class="n">labels</code><code class="p">[</code><code class="n">predicted</code><code class="p">],</code> <code class="n">labels</code><code class="p">[</code><code class="n">actual</code><code class="p">])</code>&#13;
&#13;
    <code class="k">if</code> <code class="n">predicted</code> <code class="o">==</code> <code class="n">actual</code><code class="p">:</code>&#13;
        <code class="n">num_correct</code> <code class="o">+=</code> <code class="mi">1</code>&#13;
&#13;
<code class="k">print</code><code class="p">(</code><code class="n">num_correct</code><code class="p">,</code> <code class="s2">"/"</code><code class="p">,</code> <code class="mi">100</code><code class="p">)</code></pre>&#13;
&#13;
<p>For me the trained network gets 96/100 correct, which is well above the VP of Engineering’s&#13;
hiring threshold. Faced with the evidence, he relents and changes the interview challenge&#13;
to “Invert a Binary Tree.”</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="For Further Exploration" data-type="sect1"><div class="sect1" id="idm45635731128472">&#13;
<h1>For Further Exploration</h1>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Keep reading: <a data-type="xref" href="ch19.html#deep_learning">Chapter 19</a> will explore these topics in much more detail.<a data-primary="" data-startref="PMneural18" data-type="indexterm" id="idm45635730170792"/></p>&#13;
</li>&#13;
<li>&#13;
<p>My blog post on <a href="http://joelgrus.com/2016/05/23/fizz-buzz-in-tensorflow/">“Fizz Buzz in Tensorflow”</a> is pretty good.</p>&#13;
</li>&#13;
</ul>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section></body></html>