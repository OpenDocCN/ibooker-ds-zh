- en: Chapter 2\. Extracting Textual Insights with APIs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When you want to determine the approach to a research question or start working
    on a text analytics project, the availability of data is often the first stumbling
    block. A simple Google search or the more specific [Dataset search](https://oreil.ly/SJoyG)
    will throw up curated datasets, and we will use some of these in subsequent chapters
    of this book. Depending on your project, such datasets may turn out to be generic
    and not suitable for your use case. You might have to create your own dataset,
    and application programming interfaces (APIs) are one way to extract data programmatically
    in an automated fashion.
  prefs: []
  type: TYPE_NORMAL
- en: What You‚Äôll Learn and What We‚Äôll Build
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will provide an overview of APIs and introduce blueprints
    to extract data for your project from popular websites like [GitHub](https://github.com)
    and [Twitter](https://twitter.com). You will learn about using authentication
    tokens, handling pagination, understanding rate limits, and automating data extraction.
    At the end of this chapter, you will be able to create your own datasets by making
    API calls to any identified service. While the blueprints are illustrated with
    specific examples such as GitHub and Twitter, they can be used to work with any
    API.
  prefs: []
  type: TYPE_NORMAL
- en: Application Programming Interfaces
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: APIs are interfaces that allow software applications or components to communicate
    with one another without having to know how they are implemented. The API provides
    a set of definitions and protocols including the kinds of requests that can be
    made, the data formats to be used, and the expected response. An API is a set
    of software interfaces that is commonly used by developers while building websites,
    apps, and services. For example, when you sign up for a new account with almost
    any service, you will be asked to verify your email address or telephone number
    with a one-time code or link. Typically, the developer would use the API provided
    by an authentication service to enable this functionality rather than build the
    entire flow. This allows decoupling of the core functionality that the service
    provides and uses APIs to build other necessary, but not unique, features. You
    can read an intuitive nontechnical introduction to APIs provided by [Zapier](https://oreil.ly/e9iUI)
    for a better understanding.
  prefs: []
  type: TYPE_NORMAL
- en: How are programming APIs connected with data for text analytics projects? In
    addition to enabling basic functionality such as authentication, common functionality
    on websites is also offered as APIs, providing us with an alternative way of accessing
    data. For example, third-party tools make use of APIs to create a post or add
    comments on social media. We can use these same APIs to read and store this information
    locally to create our dataset. For instance, say you are an analyst working at
    a Consumer Packaged Goods firm looking to evaluate the performance of a marketing
    campaign. You could extract data using the [Twitter Search API](https://oreil.ly/PCJsx),
    filter tweets that contain the campaign tagline or hashtag, and analyze the text
    to understand people‚Äôs reactions. Or consider that you are asked by a training
    provider to help identify upcoming technology areas for new courses. One approach
    could be to extract data on questions being asked using the [StackOverflow API](https://oreil.ly/kMsGs)
    and identify emerging topics using text analytics.
  prefs: []
  type: TYPE_NORMAL
- en: Using APIs is the preferred approach over scraping a website. They are designed
    to be callable functions, are easy to use, and can be automated. They are specifically
    recommended when working with data that changes frequently or when it‚Äôs critical
    that the project reflects the latest information. When working with any API, it‚Äôs
    important to take the time and read the documentation carefully. It provides granular
    information on the specific API call, data formats, and parameters as well as
    other details like user permissions, rate limits, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Not all APIs are provided free of charge, and some providers have different
    plans to support different kinds of customers. For example, the Twitter API has
    Standard, Premium, and Enterprise versions. The Standard API is a public API (available
    to anyone with a developer account), while the Premium and Enterprise APIs are
    only for paying customers. We will use only public APIs in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Blueprint: Extracting Data from an API Using the Requests Module'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With the popularity of the web driven by the HTTP standard, a URL is most often
    the primary specification for an API. We will use the requests library that is
    included in the standard Python distribution as the primary way to access and
    extract data from an API. To illustrate this blueprint, we will use the [GitHub
    API](https://oreil.ly/oUIG1). GitHub is a popular code hosting platform where
    several open source projects such as Python, scikit-learn, and TensorFlow, as
    well as the code for this book, are hosted. Let‚Äôs say that you would like to determine
    the popularity of different programming languages such as Python, Java, and JavaScript.
    We could extract data from GitHub on the languages used by popular repositories
    and determine the prevalence of each language. Or consider that your organization
    is hosting a project on GitHub and wants to ensure that users and contributors
    adhere to the Code of Conduct. We can extract the issues and comments written
    by contributors and ensure that offensive language is not used. In this blueprint,
    we will read and understand the documentation of an API, make requests, and parse
    the output and create a dataset that can be used to solve our use case.
  prefs: []
  type: TYPE_NORMAL
- en: The first API we want to call is to list all the repositories on GitHub. The
    entry point to the REST API documentation can be found on [GitHub](https://oreil.ly/oUIG1).
    You can either search for the specific method (also referred to as the *endpoint*)
    or navigate to the [GitHub page](https://oreil.ly/8HM5v) to see its details, as
    shown in [Figure¬†2-1](#fig-repositories-documentation).
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/btap_0201.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-1\. API documentation for listing public repositories.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'As stated in the documentation, this is a `GET` method that will provide you
    with a list of repositories in the order they were created. Let‚Äôs make a call
    using the `requests.get` method and view the response status:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'A [response code of 200](https://httpstatuses.com/200) indicates that the call
    to the API was successful. We can also evaluate the encoding of the response object
    to ensure that we process it correctly. One of the important elements contained
    in the response object is the `headers` object. It is a dictionary that contains
    more detailed information, such as the name of the server, response timestamp,
    status, and so on. In the following code, we only extract the type of content
    and server details that have been returned by the API, but you are encouraged
    to look at all of the elements of this object. Most of this information is present
    in the detailed API documentation, but inspecting the response is another way
    to ensure that you parse the response accurately:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Looking at the response parameters, we understand that the response follows
    a UTF-8 encoding, and the content is returned using the JSON format. The content
    can be directly accessed using the `content` element, which provides the payload
    in the form of bytes. Since we already know that the response is a JSON object,
    we can also use the `json()` command to read the response. This creates a list
    object where each element is a repository. We show the first element in the response
    that identifies the [first GitHub repository that was created](https://oreil.ly/L9b6L).
    We have limited the output to the first 200 characters for the sake of brevity:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'While the previous response contains a list of repositories, it is not helpful
    when looking for specific programming languages. It might be better to use the
    Search API, which we will use next:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The previous request was not successful as it returned with a [status code of
    422](https://httpstatuses.com/422). This code indicates that the request was correct,
    but the server was not able to process the request. This is because we have not
    provided any search query parameter as specified in the [documentation](https://oreil.ly/5EtSw).
    It is important to always check and understand the status before proceeding to
    view the response. You can view a detailed definition of each status code in the
    [HTTP specification](https://oreil.ly/SG6tf).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let‚Äôs say that we want to identify GitHub repositories related to data science
    that are written in Python. We will modify the request by adding a second argument
    called `params` with the search terms. The search query needs to be constructed
    following the rules described in [GitHub‚Äôs documentation](https://oreil.ly/jNCff).
    Based on these rules, our search query is encoded to look for `data_science`,
    filter the `language` by Python (`language:python`), and combine the two (`+`).
    This constructed query is passed as the query argument `q` to params. We also
    pass the argument `headers` containing the `Accept` parameter where we specify
    `text-match+json` so that the response contains the matching metadata and provides
    the response in JSON format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'As described in the example provided in the API documentation for the `/search/repositories`
    endpoint, the response contains a dictionary with `total_count`, `incomplete_results`,
    and `items`. It is important to note that this response format is different from
    the `/repositories` endpoint we saw earlier, and we must parse this structure
    accordingly. Here we list the names of the top five repositories returned by the
    search:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'We‚Äôve seen how to make requests and parse the response. Let‚Äôs consider the
    use case of monitoring the comments in a repository and ensuring that they adhere
    to community guidelines. We will use the [List Repository Issues](https://oreil.ly/9l-fy)
    endpoint for this. Here we must specify the owner and the repository name to get
    all of the issue comments, and the response will contain a list of all comments
    in that repository. Let‚Äôs make this request for the PyTorch repository, which
    is a popular deep learning framework:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: While we see that the response has succeeded, the number of comments returned
    is only 30\. PyTorch is a popular framework with a lot of collaborators and users.
    Checking the issues page of the repository in a browser would show us that the
    number of comments is much higher. So, what are we missing?
  prefs: []
  type: TYPE_NORMAL
- en: Pagination
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is a technique used by many APIs to limit the number of elements in the
    response. The total number of comments in a repository can be large, and attempting
    to respond with all of them would be time-intensive and costly. As a result, the
    GitHub API implements the pagination concept where it returns only one page at
    a time, and in this case each page contains 30 results. The `links` field in the
    response object provides details on the number of pages in the response.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The `next` field provides us with a URL to the next page, which would contain
    the next 30 results, while the `last` field provides a link to the last page,
    which provides an indication of how many search results there are in total. The
    number of 30 results per page is also specified in the documentation and usually
    can be configured up to a certain maximum value. What does this mean for us? To
    get all the results, we must implement a function that will parse all the results
    on one page and then call the next URL until the last page has been reached. This
    is implemented as a recursive function where we check to see if a `next` link
    exists and recursively call the same function. The comments from each page are
    appended to the `output_json` object, which is finally returned. To restrict the
    number of comments that we retrieve, we use a filter parameter to fetch only the
    comments since July 2020\. As per the documentation, the date must be specified
    using the ISO 8601 format and provided as a parameter using the `since` keyword:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '| ¬† | id | created_at | body |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 2176 | 286601372 | 2017-03-15T00:09:46Z | @soumith are you able to explain
    what dependency is broken? I can‚Äôt find the PR you mentioned. |'
  prefs: []
  type: TYPE_TB
- en: We have captured about 3,800 comments for the PyTorch repository by using the
    recursive pagination function, and we saw an example of one of these comments
    in the previous table. The dataset we have created here can be used to apply text
    analytics blueprints, for example, to identify comments that do not adhere to
    community guidelines and flag for moderation. It can also be augmented by running
    it at programmed time intervals to ensure that latest comments are always captured.
  prefs: []
  type: TYPE_NORMAL
- en: Rate Limiting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'One issue that you might have noticed while extracting the comments is that
    we were able to retrieve only 3,800 comments. However, the actual number of comments
    is much more than that. This was a result of the API applying a rate limit. To
    ensure that an API can continue serving all users and avoid load on their infrastructure,
    providers will often enforce rate limits. The rate limit specifies how many requests
    can be made to an endpoint in a certain time frame. GitHub‚Äôs [Rate Limiting policy](https://oreil.ly/PH7hm)
    states the following:'
  prefs: []
  type: TYPE_NORMAL
- en: For unauthenticated requests, the rate limit allows for up to 60 requests per
    hour. Unauthenticated requests are associated with the originating IP address,
    and not the user making requests.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'The information about usage is contained in the headers section of the response
    object. We can make a call to the API to only retrieve the headers by using the
    `head` method and then peering into the `X-Ratelimit-Limit`, `X-Ratelimit-Remaining`,
    and `X-RateLimit-Reset` header elements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '`X-Ratelimit-Limit` indicates how many requests can be made per unit of time
    (one hour in this case), `X-Ratelimit-Remaining` is the number of requests that
    can still be made without violating the rate limits, and `X-RateLimit-Reset` indicates
    the time at which the rate would be reset. It‚Äôs possible for different API endpoints
    to have different rate limits. For example, the GitHub Search API has a [per-minute
    rate limit](https://oreil.ly/95Fw7). If you exceed the rate limit by making requests
    that exceed the rate limit, then the API will respond with a status of 403.'
  prefs: []
  type: TYPE_NORMAL
- en: 'While making API calls, we must honor the rate limits and also adjust the way
    we make our calls to ensure that we do not overload the server. While extracting
    comments from the repository as in the previous example, we are allowed to make
    60 API calls every hour. We can make the requests one after the other, thereby
    quickly exhausting the limit, which is how our earlier blueprint works. The function
    `handle_rate_limits` shown next slows down the requests to ensure they are spaced
    out over the entire duration. It does so by distributing the remaining requests
    equally over the remaining time by applying a sleep function. This will ensure
    that our data extraction blueprint respects the rate limits and spaces the requests
    so that all the requested data is downloaded:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Network communication including API calls can fail for several reasons, such
    as interrupted connections, failed DNS lookups, connection timeouts, and so on.
    By default, the requests library does not implement any retries, and therefore
    a nice addition to our blueprint is an implementation of a retry strategy. This
    will allow API calls to be retried in case of specified failure conditions. It
    can be implemented with the `HTTPAdapter` library that allows more fine-grained
    control of the underlying HTTP connections being made. Here we initialize an adapter
    with the retry strategy that specifies five retries for a failed attempt. We also
    specify that these retries should be made only when the error status codes [500](https://httpstatuses.com/500),
    [503](https://httpstatuses.com/503), and [504](https://httpstatuses.com/504) are
    received. In addition, we specify the `backoff_factor`^([1](ch02.xhtml#idm45634214166328))
    value that determines the exponentially increasing time delay between attempts
    after the second try to ensure that we don‚Äôt hammer the server.
  prefs: []
  type: TYPE_NORMAL
- en: 'Every request object creates a default `Sessions` object that manages and persists
    connection settings across different requests, such as cookies, authentication,
    and proxies that should be stateless. Up to now we relied on the default `Sessions`
    object, but to override the connection behavior with our retry strategy, we have
    to specify a custom adapter that will enable us to use the retry strategy. This
    means that we will use the new `http Session` object to make our requests, as
    shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Putting all this together, we can modify the blueprint to handle pagination,
    rate limits, and retries as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: If you look closely at the rate limit documentation, you will observe that there
    are different rate limits based on the type of authentication used. All our requests
    up to now were unauthenticated requests, and the rate limits are much lower. We
    can identify our data extraction application to GitHub by registering for an account.
    We can then make authenticated requests to the API that increases the rate limits.
    This practice ensures that there is no abuse of the API by unidentified users
    or fraudulent applications, and most API providers do not allow access to an API
    without a form of authentication.
  prefs: []
  type: TYPE_NORMAL
- en: This blueprint shows you how to extract data from any API using the simple Python
    requests module and creating your own dataset. This is the fundamental way in
    which most API requests work and is useful for a one-off analysis and initial
    exploration of a new data source. Going back to our use case, if you were looking
    to identify the popular deep-learning frameworks for you to start learning, then
    this blueprint would be a good choice. Or let‚Äôs say that your organization already
    has a sales forecasting model and you would like to evaluate the benefit of adding
    financial market news on the accuracy of this model. Assuming there is an API
    that provides financial news, you can easily create a dataset, apply text analytics
    blueprints, and test the relevance to the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Blueprint: Extracting Twitter Data with Tweepy'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To make it easier for developers to work with their APIs, many of the popular
    services provide packages in multiple programming languages or at least have one
    or more community-supported modules. While the API is officially supported, these
    packages are well-maintained Python modules that incorporate additional functionality
    that makes them easy to use. This means you can focus on the kind of data that
    you would like to extract rather than the technical details of making API calls,
    authentication, and so on. In this blueprint, we will use one of the community-developed
    and supported Python modules for Twitter called [Tweepy](https://oreil.ly/yZOU7).
    Twitter maintains a list of [libraries for different languages](https://oreil.ly/lwrFM)
    that includes several libraries for Python. We chose Tweepy because it‚Äôs actively
    maintained and used by many researchers. While this blueprint uses Tweepy to extract
    data from the Twitter API, the steps described would be similar for any other
    API.
  prefs: []
  type: TYPE_NORMAL
- en: We described earlier how you might use Twitter to analyze the effectiveness
    of a new marketing campaign. Another use case could be to perform text analytics
    to understand the popularity and sentiment for cryptocurrencies as a way to predict
    their adoption and value in the economy. Twitter is a social media network where
    users spontaneously share short messages, often reacting in real time to world
    events such as major calamities or popular sporting events. The user can also
    add the geolocation if they want to, and this gives us the ability to understand
    the most trending current events in a certain city or geographical area. During
    the government-imposed lockdowns due to COVID-19, several researchers used Twitter
    data to understand the spread of the virus and the [impact of lockdowns](https://oreil.ly/J7pDT)
    and also used these as predictive variables of economic health.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Please note that when using a public API like Twitter, you will retrieve data
    from the public timelines of many users, and it could contain strong, maybe even
    offensive, language, including profanities. Please be aware of this and ensure
    that the data is handled appropriately depending on your use-case.
  prefs: []
  type: TYPE_NORMAL
- en: Obtaining Credentials
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first step when working with any API is authenticating yourself or your
    application. Twitter requires all users of their API to register as a developer
    and provide details for why they would like to use the API. This helps them identify
    you and prevent any unauthorized access. You must [register yourself as a developer](https://oreil.ly/vEnJp).
    If you do not already have a Twitter account, then you will also be required to
    create one. You will be asked about your purpose for creating a developer account
    and additional questions on how you intend to use the Twitter API. [Figure¬†2-2](#fig-twitter-developer-account)
    shows some examples of these screens. Please provide detailed responses to ensure
    that Twitter fully understands your purpose for creating a developer account.
    For example, in this blueprint we are looking to extract tweets using the API
    to illustrate how this is done. Since we are only going to use the extraction
    capability, the question ‚ÄúWill your app use Tweet, Retweet, like, follow, or Direct
    Message functionality?‚Äù is not applicable and can be deselected. You must read
    and understand each question before proceeding. Note that this requirement will
    be different for each API and is also subject to change.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/btap_0202.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-2\. Illustration of sign-up flow for creating a Twitter developer account.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Now that you have a developer account, the next step is to create an app. The
    credentials of the app are used when making API calls, and it‚Äôs important to specify
    the reason for creating the app. You have to provide details like the app name,
    the purpose for creating the app, and the website URL that is associated with
    the app. If you will use this app for research and learning purposes, then you
    could state this in the app description and provide the URL for your university
    page or GitHub repository associated with your project. Once the app is approved
    by Twitter, you can navigate to the tab *Keys and tokens*, as shown in [Figure¬†2-3](#fig-twitter-app-creation),
    where you will find the fields *API key* and *API secret key*. Please note that
    these are the credentials that will be used for authentication when making API
    calls, and it‚Äôs important to not reveal them.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/btap_0203.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-3\. Creating a Twitter app and obtaining credentials.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Installing and Configuring Tweepy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The project repository for [Tweepy](https://oreil.ly/OHfnn) and [documentation](https://oreil.ly/lDDo1)
    are the best source for all information about using Tweepy. We can install Tweepy
    by entering **`pip install tweepy`** into the terminal. Next, we have to authenticate
    the app with the Twitter API, and we do this with the help of the `tweepy.AppAuthHandler`
    module to which we pass the API key and API secret key we obtained in the previous
    step. Finally, we instantiate the `tweepy.API` class, which will be used to make
    all subsequent calls to the Twitter API. Once the connection is made, we can confirm
    the host and version of the API object. Please note that since we are interested
    in read-only access to public information, we use [application-only authentication](https://oreil.ly/4oWbP):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Extracting Data from the Search API
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let‚Äôs say we want to analyze the perception of cryptocurrency and determine
    its popularity. We will use the Search API to retrieve all tweets that mention
    this to create our dataset. The Twitter API also uses pagination to return multiple
    pages of results, but instead of implementing our own way of managing this, we
    will use the `Cursor` object provided by the Tweepy library to iterate through
    the results. We pass the search query to the API object and additionally specify
    the language of the tweets to be extracted (English in this case). We choose to
    retrieve only 100 items and create a `DataFrame` by loading the results as a JSON
    object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '| ¬† | text |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 59 | Hi! I‚Äôve been using OKEx which makes it really easy and safe to buy,
    sell, and store cryptocurrency (like Bitcoin).‚Ä¶ https://t.co/4m0mpyQTSN |'
  prefs: []
  type: TYPE_TB
- en: '| 17 | Get connected today üìâ #getconnected #bitcointrading #Bitcoin #BitcoinCash
    #bitcoinmining #cryptocurrency https://t.co/J60bCyFPUI |'
  prefs: []
  type: TYPE_TB
- en: '| 22 | RT @stoinkies: We reached over 100 followers!\nGiveaway time!\nFOLLOW
    +RETWEET + LIKE THIS TWEET = Win 200 Dogecoin!\nEvery participant also g‚Ä¶ |'
  prefs: []
  type: TYPE_TB
- en: We have successfully completed the API call and can see the text of the retrieved
    tweets in the previous table, which already show interesting aspects. For example,
    we see the use of the word *RT*, which indicates a retweet (where the user has
    shared another tweet). We see the usage of emojis, which is a strong characteristic
    of the medium, and also notice that some tweets are truncated. Twitter actually
    imposes a limit on the number of characters that each tweet can contain, which
    was originally 140 characters and later extended to 280\. This led to the creation
    of an [extended tweet object](https://oreil.ly/fvl-3), which we must specify explicitly
    while retrieving results in Tweepy. Additionally, you must be aware that the standard
    version of the Twitter Search API provides results only from the last week, and
    one must sign up for the Premium or Enterprise versions for historical tweets.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: For each endpoint, Twitter specifies a maximum value of `count`. This is the
    maximum number of results that is returned in a single page of the response. For
    example, the search endpoint specifies a maximum value of `count=100`, whereas
    `user_timeline` has a maximum value of `count=200`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let‚Äôs expand our search to include an additional keyword relevant to the cryptocurrency
    topic like `crypto` and filter out retweets for now. This is done by using the
    `filter` keyword appended with a minus sign in the search term. We also specify
    that we would like to fetch tweets with the `tweet_mode=extended` parameter, which
    ensures that we retrieve the full text of all tweets. The [Standard search API](https://oreil.ly/4IGcB)
    searches only a sample of recent Tweets published in the past seven days, but
    even this could potentially be a large number, and to avoid a large wait time
    to run the blueprint, we restrict ourselves to 12,000 tweets. We specify the parameter
    `count=30`, which is the maximum number of tweets that can be retrieved in one
    call. Therefore, we must make 400 such calls to obtain our dataset while taking
    into consideration the rate limits. This is within the rate limit of 450 requests
    every 15 minutes specified by the API. It‚Äôs possible that you might exceed this
    rate limit while experimenting with this blueprint, and therefore we enable the
    automatic wait functionality provided by Tweepy by setting the `wait_on_rate_limit`
    parameter. We also set `wait_on_rate_limit_notify` so that we are notified of
    such wait times. If you are within the rate limits, the following function should
    execute in about five minutes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '| ¬† | created_at | full_text | entities.hashtags |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 10505 | Sat Sep 19 22:30:12 +0000 2020 | Milk was created to let liquidity
    providers (people who have LP tokens) benefit because they can stake LP tokens
    at SpaceSwap, they get MILK token as a reward as well as 0.3% UniSwap commission.\n\nüëáüëáüëá\nhttps://t.co/M7sGbIDq4W\n#DeFi
    #cryptocurrency #UniSwap #altcoin | [{''text‚Äô: ‚ÄòDeFi'', ‚Äòindices‚Äô: [224, 229]},
    {''text‚Äô: ‚Äòcryptocurrency'', ‚Äòindices‚Äô: [230, 245]}, {''text‚Äô: ‚ÄòUniSwap'', ‚Äòindices‚Äô:
    [246, 254]}, {''text‚Äô: ‚Äòaltcoin'', ‚Äòindices‚Äô: [256, 264]}] |'
  prefs: []
  type: TYPE_TB
- en: '| 11882 | Sat Sep 19 20:57:45 +0000 2020 | You can EARN dividends from our
    curation activity. The minimum to participate is 2000 #steem delegation... with
    delegation there is no risk of losing your principal. We can process the payout
    in #bitcoin and all major #cryptocurrencies .. #cryptocurrency \nhttps://t.co/4b3iH2AI4S
    | [{''text‚Äô: ‚Äôsteem'', ‚Äòindices‚Äô: [86, 92]}, {''text‚Äô: ‚Äòbitcoin'', ‚Äòindices‚Äô:
    [195, 203]}, {''text‚Äô: ‚Äòcryptocurrencies'', ‚Äòindices‚Äô: [218, 235]}, {''text‚Äô:
    ‚Äòcryptocurrency'', ‚Äòindices‚Äô: [239, 254]}] |'
  prefs: []
  type: TYPE_TB
- en: 'There is a lot of information that the API provides, as shown in the sample
    of two previous tweets that contain important elements such as the date when the
    tweet was sent out, the content of the tweet, and so on. Twitter also returns
    several entities such as hashtags contained within the tweet, and it would be
    interesting to see which hashtags are used heavily when discussing cryptocurrency:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code creates the graph shown in [Figure¬†2-4](#fig02in01), which
    shows us the important hashtags being used in conjunction with cryptocurrencies.
    It includes examples of cryptocurrencies such as *bitcoin* and *ethereum* as well
    as their trading short-codes *btc* and *eth*. It also throws up related activities
    such as *trading* and *airdrops*. There are also mentions of entities like *fintech*
    and *applecash*. At a first glance, it already gives you insight into the various
    terms and entities being discussed, and the presence of trading short-codes indicates
    that there might be some market information contained in these tweets. While this
    is a simple count of entities, we can use this dataset to apply more advanced
    text analytics techniques to determine popular sentiment about cryptocurrencies
    that derive relationships between entities. Please note that the results may differ
    depending on when the Twitter search was run and the random selection by the API.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/btap_02in01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-4\. Common hashtags used when discussing cryptocurrency.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Extracting Data from a User‚Äôs Timeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Search is not the only way to interact with Twitter as we can use the API to
    also extract tweets by a specific user or account. This might be a person like
    a famous celebrity or world leader, or it might be an organization like a sports
    team. For instance, what if we would like to compare tweets from two popular Formula
    One teams, Mercedes and Ferrari? We can extract all the tweets that they have
    sent out and contrast their individual styles and the main themes that they focus
    on. We provide the screen name for the account (`MercedesAMGF1`) to retrieve all
    the tweets sent by this account:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, though we requested 5,000 tweets, we were able to retrieve
    only about 3,200 of them. This is a [restriction placed on the API](https://oreil.ly/RaNaQ).
    Let‚Äôs retrieve the tweets for the Ferrari team as well using their screen name
    (`ScuderiaFerrari`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: One of the quirks of the Tweepy implementation is that in the case of retweets,
    the `full_text` column is truncated, and the `retweeted_status.full_text` column
    must be used to retrieve all the characters of the tweet. For our use case, retweets
    are not important, and we filter them by checking if `retweeted_status.id` is
    empty. However, depending on the use case, you can add a condition to replace
    the column `full_text` with `retweeted_status.full_text` in the case of retweets.
  prefs: []
  type: TYPE_NORMAL
- en: 'When we remove retweets, the number of tweets authored by each team handle
    significantly drops. We will reuse the word cloud blueprint from [Chapter¬†1](ch01.xhtml#ch-exploration)
    with the function `wordcloud` to quickly visualize the tweets from each of the
    two teams and identify the keywords they focus on. Mercedes tweets seem to focus
    a lot on the races that the team participates in, such as *tuscangp*, *britishgp*
    and *race*, *day*. The Ferrari tweets, on the other hand, promote their merchandise,
    such as *ferraristore*, and drivers, such as *enzofitti* and *schumachermick*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/btap_02in02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Extracting Data from the Streaming API
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Some APIs provide near real-time data, which might also be referred to as *streaming
    data*. In such a scenario, the API would like to *push* the data to us rather
    than waiting for a *get* request as we have been doing so far. An example of this
    is the Twitter Streaming API. This API provides us with a sample of the tweets
    being sent out in real time and can be filtered on several criteria. Since this
    is a continuous stream of data, we have to handle the data extraction process
    in a different manner. Tweepy already provides basic functionality in the `StreamListener`
    class that contains the `on_data` function. This function is called each time
    a new tweet is pushed by the streaming API, and we can customize it to implement
    logic that is specific to certain use cases.
  prefs: []
  type: TYPE_NORMAL
- en: 'Staying with the cryptocurrency use case, let‚Äôs suppose that we want to have
    a continuously updated sentiment measure of different cryptocurrencies to make
    trading decisions. In this case, we would track real-time tweets mentioning cryptocurrencies
    and continuously update the popularity score. On the other hand, as researchers,
    we might be interested in analyzing the reactions of users during key live events
    such as the Super Bowl or announcement of election results. In such scenarios,
    we would listen for the entire duration of the event and store the results for
    subsequent analysis. To keep this blueprint generic, we have created the `FileStreamListener`
    class as shown next, which will manage all the actions to be taken on the stream
    of incoming tweets. For every tweet pushed by the Twitter API, the `on_data` method
    is called. In our implementation, we gather incoming tweets into batches of 100
    and then write to a file with the timestamp. The choice of 100 can be varied based
    on the memory available on the system:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'To get access to the streaming API, the basic app authentication is not enough.
    We must also provide the user authentication, which can be found on the same page
    as shown before. This means that the Streaming API requests are made by the app
    we created on behalf of the user (in this case our own account). This also means
    that we have to use the `OAuthHandler` class instead of the `AppAuthHandler` that
    we used up to now:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'When initializing an object of `FileStreamListener`, we also specify the maximum
    number of tweets that we would like to extract. This acts like a stopping condition,
    and if not specified, the process will run as long as it is not terminated by
    the user or stopped due to a server error. We initialize the Twitter stream by
    passing in the authentication object (`api.auth`) and the object that will manage
    the stream (`fileStreamListener`). We also ask for the extended tweets to be provided.
    Once this is done, we can start tracking live tweets from the stream using the
    filter function and providing keywords that we would like to track:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: If you would like to run the extractor in a separate thread, you can pass the
    keyword `async=True` to the filter function, and this will run continuously in
    a separate thread. Once it has run for some time and stored tweets, we can read
    this into a Pandas `DataFrame` as before. When an error occurs, the `FileStreamListener`
    does not attempt retries but only prints the error `status_code`. You are encouraged
    to implement failure handling and customize the `on_data` method to suit the use
    case.
  prefs: []
  type: TYPE_NORMAL
- en: 'These blueprints only provide guidance on accessing popular APIs for data extraction.
    Since each API is different, the functionality provided by the corresponding Python
    module will also be different. For instance, [Wikipedia](https://oreil.ly/zruJt)
    is another popular source for extracting text data, and [`wikipediaapi`](https://oreil.ly/Eyon3)
    is one of the supported Python modules for extracting this data. It can be installed
    by using the command **`pip install wikipediaapi`**, and since this is a publicly
    available data source, the authentication and generation of access tokens is not
    necessary. You only need to specify the version of Wikipedia (language) and the
    topic name for which you want to extract data. The following code snippet shows
    the steps to download the Wikipedia entry for ‚ÄúCryptocurrency‚Äù and shows the initial
    few lines of this article:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Closing Remarks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we first introduced blueprints that make use of the Python
    requests library to make API calls and extract data. We also introduced ways to
    work with paginated results, rate limits, and retries. These blueprints work for
    any kind of API and are great if you would like to control and customize several
    aspects for your data extraction. In the next set of blueprints, we used Tweepy
    to extract data from the Twitter API. This is an example of a community-developed
    Python library that supports a popular API and provides tested functionality out
    of the box. You often don‚Äôt have to worry about implementing your own pagination
    or backoff strategy and is therefore one less thing to worry about. If your use
    case needs to get data from a popular API, then it is convenient to use such a
    preexisting package.
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch02.xhtml#idm45634214166328-marker)) Delay introduced between subsequent
    calls defined as `time_delay={backoff factor} * (2 ** ({number of total retries}
    - 1))`.
  prefs: []
  type: TYPE_NORMAL
