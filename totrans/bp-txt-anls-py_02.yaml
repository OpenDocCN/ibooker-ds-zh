- en: Chapter 2\. Extracting Textual Insights with APIs
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二章。使用 API 提取文本洞见
- en: When you want to determine the approach to a research question or start working
    on a text analytics project, the availability of data is often the first stumbling
    block. A simple Google search or the more specific [Dataset search](https://oreil.ly/SJoyG)
    will throw up curated datasets, and we will use some of these in subsequent chapters
    of this book. Depending on your project, such datasets may turn out to be generic
    and not suitable for your use case. You might have to create your own dataset,
    and application programming interfaces (APIs) are one way to extract data programmatically
    in an automated fashion.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 当您想要确定研究问题的方法或开始进行文本分析项目时，数据的可用性通常是第一个障碍。一个简单的谷歌搜索或更具体的[数据集搜索](https://oreil.ly/SJoyG)将提供精选数据集，我们将在本书的后续章节中使用其中一些。根据您的项目，这些数据集可能是通用的，不适合您的用例。您可能需要创建自己的数据集，而应用程序编程接口（API）是以编程方式自动提取数据的一种方法。
- en: What You’ll Learn and What We’ll Build
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 你将学到什么，我们将构建什么
- en: In this chapter, we will provide an overview of APIs and introduce blueprints
    to extract data for your project from popular websites like [GitHub](https://github.com)
    and [Twitter](https://twitter.com). You will learn about using authentication
    tokens, handling pagination, understanding rate limits, and automating data extraction.
    At the end of this chapter, you will be able to create your own datasets by making
    API calls to any identified service. While the blueprints are illustrated with
    specific examples such as GitHub and Twitter, they can be used to work with any
    API.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将概述 API，并介绍从流行网站（如[GitHub](https://github.com)和[Twitter](https://twitter.com)）提取数据的蓝图。您将了解如何使用身份验证令牌，处理分页，了解速率限制，并自动化数据提取。在本章末尾，您将能够通过对任何已识别服务进行
    API 调用来创建自己的数据集。虽然这些蓝图以 GitHub 和 Twitter 等具体示例为例，但它们可以用来处理任何 API。
- en: Application Programming Interfaces
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 应用程序编程接口
- en: APIs are interfaces that allow software applications or components to communicate
    with one another without having to know how they are implemented. The API provides
    a set of definitions and protocols including the kinds of requests that can be
    made, the data formats to be used, and the expected response. An API is a set
    of software interfaces that is commonly used by developers while building websites,
    apps, and services. For example, when you sign up for a new account with almost
    any service, you will be asked to verify your email address or telephone number
    with a one-time code or link. Typically, the developer would use the API provided
    by an authentication service to enable this functionality rather than build the
    entire flow. This allows decoupling of the core functionality that the service
    provides and uses APIs to build other necessary, but not unique, features. You
    can read an intuitive nontechnical introduction to APIs provided by [Zapier](https://oreil.ly/e9iUI)
    for a better understanding.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: API 是允许软件应用程序或组件在无需知道它们如何实现的情况下进行通信的接口。API 提供一组定义和协议，包括可以进行的请求类型，要使用的数据格式以及预期的响应。
    API 是开发人员在构建网站，应用程序和服务时常用的一组软件接口。例如，当您几乎与任何服务注册新帐户时，将要求您使用一次性代码或链接验证您的电子邮件地址或电话号码。通常，开发人员会使用认证服务提供的
    API 来启用此功能，而不是构建整个流程。这允许将服务提供的核心功能与使用 API 构建其他必要但不唯一的功能分离。您可以阅读[Zapier](https://oreil.ly/e9iUI)提供的关于
    API 的直观非技术介绍，以更好地理解。
- en: How are programming APIs connected with data for text analytics projects? In
    addition to enabling basic functionality such as authentication, common functionality
    on websites is also offered as APIs, providing us with an alternative way of accessing
    data. For example, third-party tools make use of APIs to create a post or add
    comments on social media. We can use these same APIs to read and store this information
    locally to create our dataset. For instance, say you are an analyst working at
    a Consumer Packaged Goods firm looking to evaluate the performance of a marketing
    campaign. You could extract data using the [Twitter Search API](https://oreil.ly/PCJsx),
    filter tweets that contain the campaign tagline or hashtag, and analyze the text
    to understand people’s reactions. Or consider that you are asked by a training
    provider to help identify upcoming technology areas for new courses. One approach
    could be to extract data on questions being asked using the [StackOverflow API](https://oreil.ly/kMsGs)
    and identify emerging topics using text analytics.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 编程 API 如何与文本分析项目中的数据连接？除了允许基本功能如身份验证外，网站上的常见功能也作为 API 提供，为我们提供了另一种访问数据的方式。例如，第三方工具利用
    API 在社交媒体上创建帖子或添加评论。我们可以使用这些相同的 API 将这些信息读取并存储在本地以创建我们的数据集。例如，假设你是一家消费品公司的分析师，希望评估市场营销活动的表现。你可以使用
    [Twitter 搜索 API](https://oreil.ly/PCJsx) 提取数据，过滤包含活动口号或标签的推文，并分析文本以了解人们的反应。或者考虑到你被培训提供商要求帮助确定新课程的未来技术领域。一种方法是使用
    [StackOverflow API](https://oreil.ly/kMsGs) 提取关于正在提问的问题的数据，并使用文本分析识别出新兴主题。
- en: Using APIs is the preferred approach over scraping a website. They are designed
    to be callable functions, are easy to use, and can be automated. They are specifically
    recommended when working with data that changes frequently or when it’s critical
    that the project reflects the latest information. When working with any API, it’s
    important to take the time and read the documentation carefully. It provides granular
    information on the specific API call, data formats, and parameters as well as
    other details like user permissions, rate limits, and so on.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 API 是优于对网站进行抓取的首选方法。它们被设计为可调用的函数，易于使用，并且可以自动化。特别是在处理频繁变化的数据或项目必须反映最新信息时，它们特别推荐使用。在使用任何
    API 时，重要的是花时间仔细阅读文档。文档提供了关于具体 API 调用、数据格式、参数以及用户权限、速率限制等其他详细信息。
- en: Note
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Not all APIs are provided free of charge, and some providers have different
    plans to support different kinds of customers. For example, the Twitter API has
    Standard, Premium, and Enterprise versions. The Standard API is a public API (available
    to anyone with a developer account), while the Premium and Enterprise APIs are
    only for paying customers. We will use only public APIs in this chapter.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 并非所有 API 都是免费提供的，有些提供者有不同的计划以支持不同类型的客户。例如，Twitter API 有标准版、高级版和企业版。标准版是公共 API（任何具有开发者帐户的人都可以使用），而高级版和企业版则仅供付费客户使用。在本章中，我们将仅使用公共
    API。
- en: 'Blueprint: Extracting Data from an API Using the Requests Module'
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 蓝图：使用 Requests 模块从 API 中提取数据
- en: With the popularity of the web driven by the HTTP standard, a URL is most often
    the primary specification for an API. We will use the requests library that is
    included in the standard Python distribution as the primary way to access and
    extract data from an API. To illustrate this blueprint, we will use the [GitHub
    API](https://oreil.ly/oUIG1). GitHub is a popular code hosting platform where
    several open source projects such as Python, scikit-learn, and TensorFlow, as
    well as the code for this book, are hosted. Let’s say that you would like to determine
    the popularity of different programming languages such as Python, Java, and JavaScript.
    We could extract data from GitHub on the languages used by popular repositories
    and determine the prevalence of each language. Or consider that your organization
    is hosting a project on GitHub and wants to ensure that users and contributors
    adhere to the Code of Conduct. We can extract the issues and comments written
    by contributors and ensure that offensive language is not used. In this blueprint,
    we will read and understand the documentation of an API, make requests, and parse
    the output and create a dataset that can be used to solve our use case.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 随着基于 HTTP 标准驱动的 Web 的普及，URL 往往是 API 的主要规范。我们将使用包含在标准 Python 发行版中的 requests 库作为访问和提取
    API 数据的主要方式。为了说明这一蓝图，我们将使用 [GitHub API](https://oreil.ly/oUIG1)。GitHub 是一个流行的代码托管平台，其中托管了几个开源项目，如
    Python、scikit-learn、TensorFlow，以及本书的代码。假设您想确定不同编程语言（如 Python、Java 和 JavaScript）的流行程度。我们可以从
    GitHub 提取关于流行存储库使用的语言的数据，并确定每种语言的普及程度。或者考虑您的组织正在 GitHub 上托管一个项目，并希望确保用户和贡献者遵守行为准则。我们可以提取贡献者编写的问题和评论，并确保不使用冒犯性语言。在这个蓝图中，我们将阅读和理解
    API 的文档，发出请求，解析输出，并创建一个可用于解决我们用例的数据集。
- en: The first API we want to call is to list all the repositories on GitHub. The
    entry point to the REST API documentation can be found on [GitHub](https://oreil.ly/oUIG1).
    You can either search for the specific method (also referred to as the *endpoint*)
    or navigate to the [GitHub page](https://oreil.ly/8HM5v) to see its details, as
    shown in [Figure 2-1](#fig-repositories-documentation).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想要调用的第一个 API 是列出 GitHub 上的所有存储库。REST API 文档的入口点可以在 [GitHub](https://oreil.ly/oUIG1)
    上找到。您可以搜索特定方法（也称为*端点*）或导航到 [GitHub 页面](https://oreil.ly/8HM5v) 查看其详细信息，如图 [2-1](#fig-repositories-documentation)
    所示。
- en: '![](Images/btap_0201.jpg)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/btap_0201.jpg)'
- en: Figure 2-1\. API documentation for listing public repositories.
  id: totrans-14
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-1\. 列出公共存储库的 API 文档。
- en: 'As stated in the documentation, this is a `GET` method that will provide you
    with a list of repositories in the order they were created. Let’s make a call
    using the `requests.get` method and view the response status:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 如文档所述，这是一个使用`GET`方法的调用，将按创建顺序提供存储库列表。让我们使用`requests.get`方法进行调用，并查看响应状态：
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '`Out:`'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE1]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'A [response code of 200](https://httpstatuses.com/200) indicates that the call
    to the API was successful. We can also evaluate the encoding of the response object
    to ensure that we process it correctly. One of the important elements contained
    in the response object is the `headers` object. It is a dictionary that contains
    more detailed information, such as the name of the server, response timestamp,
    status, and so on. In the following code, we only extract the type of content
    and server details that have been returned by the API, but you are encouraged
    to look at all of the elements of this object. Most of this information is present
    in the detailed API documentation, but inspecting the response is another way
    to ensure that you parse the response accurately:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '[200 响应代码](https://httpstatuses.com/200)表示对 API 的调用成功。我们还可以评估响应对象的编码，以确保正确处理它。响应对象中包含的重要元素之一是`headers`对象。它是一个字典，包含更详细的信息，如服务器名称、响应时间戳、状态等。在下面的代码中，我们只提取了
    API 返回的内容类型和服务器详细信息，但建议您查看此对象的所有元素。大部分信息都在详细的 API 文档中，但检查响应是确保正确解析响应的另一种方式：'
- en: '[PRE2]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '`Out:`'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE3]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Looking at the response parameters, we understand that the response follows
    a UTF-8 encoding, and the content is returned using the JSON format. The content
    can be directly accessed using the `content` element, which provides the payload
    in the form of bytes. Since we already know that the response is a JSON object,
    we can also use the `json()` command to read the response. This creates a list
    object where each element is a repository. We show the first element in the response
    that identifies the [first GitHub repository that was created](https://oreil.ly/L9b6L).
    We have limited the output to the first 200 characters for the sake of brevity:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 查看响应参数，我们了解到响应遵循 UTF-8 编码，并且内容以 JSON 格式返回。内容可以直接通过 `content` 元素访问，它以字节形式提供有效载荷。由于我们已经知道响应是一个
    JSON 对象，因此我们还可以使用 `json()` 命令来读取响应。这将创建一个列表对象，其中每个元素都是一个仓库。我们展示了响应中的第一个元素，用于识别[创建的第一个
    GitHub 仓库](https://oreil.ly/L9b6L)。出于简洁起见，我们将输出限制为前 200 个字符：
- en: '[PRE4]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '`Out:`'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出：`'
- en: '[PRE5]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'While the previous response contains a list of repositories, it is not helpful
    when looking for specific programming languages. It might be better to use the
    Search API, which we will use next:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前一个响应包含仓库列表，但在寻找特定编程语言时并不有用。使用搜索 API 可能更好，我们将在下一步中使用它：
- en: '[PRE6]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '`Out:`'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出：`'
- en: '[PRE7]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The previous request was not successful as it returned with a [status code of
    422](https://httpstatuses.com/422). This code indicates that the request was correct,
    but the server was not able to process the request. This is because we have not
    provided any search query parameter as specified in the [documentation](https://oreil.ly/5EtSw).
    It is important to always check and understand the status before proceeding to
    view the response. You can view a detailed definition of each status code in the
    [HTTP specification](https://oreil.ly/SG6tf).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 上一个请求未成功，因为返回了 [422 状态码](https://httpstatuses.com/422)。此代码表示请求正确，但服务器无法处理请求。这是因为我们没有提供任何搜索查询参数，如
    [文档](https://oreil.ly/5EtSw) 中所述。在查看响应之前，检查和理解状态非常重要。您可以在 [HTTP 规范](https://oreil.ly/SG6tf)
    中查看每个状态码的详细定义。
- en: 'Let’s say that we want to identify GitHub repositories related to data science
    that are written in Python. We will modify the request by adding a second argument
    called `params` with the search terms. The search query needs to be constructed
    following the rules described in [GitHub’s documentation](https://oreil.ly/jNCff).
    Based on these rules, our search query is encoded to look for `data_science`,
    filter the `language` by Python (`language:python`), and combine the two (`+`).
    This constructed query is passed as the query argument `q` to params. We also
    pass the argument `headers` containing the `Accept` parameter where we specify
    `text-match+json` so that the response contains the matching metadata and provides
    the response in JSON format:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想要识别用 Python 编写的与数据科学相关的 GitHub 仓库。我们将通过添加第二个参数 `params` 并附上搜索条件来修改请求。搜索查询需要按照
    [GitHub 文档](https://oreil.ly/jNCff) 中描述的规则构建。根据这些规则，我们的搜索查询被编码为查找 `data_science`，将
    `language` 过滤为 Python (`language:python`)，并将两者组合 (`+`)。这个构造的查询作为参数 `q` 传递给了 params。我们还传递了包含
    `Accept` 参数的参数 `headers`，其中我们指定了 `text-match+json`，以便响应包含匹配的元数据并以 JSON 格式提供响应：
- en: '[PRE8]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '`Out:`'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出：`'
- en: '[PRE9]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'As described in the example provided in the API documentation for the `/search/repositories`
    endpoint, the response contains a dictionary with `total_count`, `incomplete_results`,
    and `items`. It is important to note that this response format is different from
    the `/repositories` endpoint we saw earlier, and we must parse this structure
    accordingly. Here we list the names of the top five repositories returned by the
    search:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 如 API 文档中为 `/search/repositories` 端点提供的示例所述，响应包含一个带有 `total_count`、`incomplete_results`
    和 `items` 的字典。重要的是要注意，此响应格式与我们之前看到的 `/repositories` 端点不同，我们必须相应地解析此结构。在这里，我们列出了搜索返回的前五个仓库的名称：
- en: '[PRE10]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '`Out:`'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出：`'
- en: '[PRE11]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We’ve seen how to make requests and parse the response. Let’s consider the
    use case of monitoring the comments in a repository and ensuring that they adhere
    to community guidelines. We will use the [List Repository Issues](https://oreil.ly/9l-fy)
    endpoint for this. Here we must specify the owner and the repository name to get
    all of the issue comments, and the response will contain a list of all comments
    in that repository. Let’s make this request for the PyTorch repository, which
    is a popular deep learning framework:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到如何发出请求并解析响应。现在考虑监控存储库中的评论并确保它们符合社区指南的用例。我们将使用[列出存储库问题](https://oreil.ly/9l-fy)端点来实现这一目标。在这里，我们必须指定所有者和存储库名称以获取所有问题评论，响应将包含该存储库中所有评论的列表。让我们为流行的深度学习框架PyTorch存储库发出此请求：
- en: '[PRE12]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '`Out:`'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE13]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: While we see that the response has succeeded, the number of comments returned
    is only 30\. PyTorch is a popular framework with a lot of collaborators and users.
    Checking the issues page of the repository in a browser would show us that the
    number of comments is much higher. So, what are we missing?
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们看到响应成功，但返回的评论数量仅为30个。PyTorch是一个受欢迎的框架，拥有许多合作者和用户。在浏览器中查看存储库的问题页面将显示评论数量要多得多。那么，我们缺少了什么？
- en: Pagination
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分页
- en: This is a technique used by many APIs to limit the number of elements in the
    response. The total number of comments in a repository can be large, and attempting
    to respond with all of them would be time-intensive and costly. As a result, the
    GitHub API implements the pagination concept where it returns only one page at
    a time, and in this case each page contains 30 results. The `links` field in the
    response object provides details on the number of pages in the response.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这是许多API用于限制响应中元素数量的技术。存储库中的评论总数可能很大，尝试响应所有评论将耗时且成本高昂。因此，GitHub API实现了分页概念，每次仅返回一页，本例中每页包含30个结果。响应对象中的`links`字段提供了响应中页面数的详细信息。
- en: '[PRE14]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '`Out:`'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE15]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The `next` field provides us with a URL to the next page, which would contain
    the next 30 results, while the `last` field provides a link to the last page,
    which provides an indication of how many search results there are in total. The
    number of 30 results per page is also specified in the documentation and usually
    can be configured up to a certain maximum value. What does this mean for us? To
    get all the results, we must implement a function that will parse all the results
    on one page and then call the next URL until the last page has been reached. This
    is implemented as a recursive function where we check to see if a `next` link
    exists and recursively call the same function. The comments from each page are
    appended to the `output_json` object, which is finally returned. To restrict the
    number of comments that we retrieve, we use a filter parameter to fetch only the
    comments since July 2020\. As per the documentation, the date must be specified
    using the ISO 8601 format and provided as a parameter using the `since` keyword:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '`next`字段为我们提供了下一页的URL，该页包含下一个30个结果，而`last`字段提供了指向最后一页的链接，显示了总共有多少搜索结果。每页30个结果的数量也在文档中指定，并且通常可以配置到某个最大值。这对我们意味着什么？为了获取所有结果，我们必须实现一个函数，该函数将解析一页上的所有结果，然后调用下一个URL，直到到达最后一页。这是一个递归函数，我们检查是否存在`next`链接并递归调用相同的函数。每页的评论都附加到`output_json`对象中，最终返回。为了限制我们检索的评论数量，我们使用过滤器参数仅获取自2020年7月以来的评论。根据文档，日期必须使用ISO
    8601格式指定，并使用`since`关键字作为参数提供：'
- en: '[PRE16]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '`Out:`'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE17]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '|   | id | created_at | body |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '|   | id | created_at | body |'
- en: '| --- | --- | --- | --- |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| 2176 | 286601372 | 2017-03-15T00:09:46Z | @soumith are you able to explain
    what dependency is broken? I can’t find the PR you mentioned. |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| 2176 | 286601372 | 2017-03-15T00:09:46Z | @soumith 能否解释哪个依赖项出了问题？我找不到你提到的PR。
    |'
- en: We have captured about 3,800 comments for the PyTorch repository by using the
    recursive pagination function, and we saw an example of one of these comments
    in the previous table. The dataset we have created here can be used to apply text
    analytics blueprints, for example, to identify comments that do not adhere to
    community guidelines and flag for moderation. It can also be augmented by running
    it at programmed time intervals to ensure that latest comments are always captured.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用递归分页函数，我们已经捕获了 PyTorch 仓库约 3,800 条评论，并且在之前的表格中看到了其中一个示例。我们创建的数据集可以用于应用文本分析蓝图，例如识别不符合社区指南的评论并标记进行审查。它还可以通过在程序化的时间间隔运行来增强，以确保始终捕获最新的评论。
- en: Rate Limiting
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 速率限制
- en: 'One issue that you might have noticed while extracting the comments is that
    we were able to retrieve only 3,800 comments. However, the actual number of comments
    is much more than that. This was a result of the API applying a rate limit. To
    ensure that an API can continue serving all users and avoid load on their infrastructure,
    providers will often enforce rate limits. The rate limit specifies how many requests
    can be made to an endpoint in a certain time frame. GitHub’s [Rate Limiting policy](https://oreil.ly/PH7hm)
    states the following:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在提取评论时可能注意到的一个问题是，我们只能检索到大约 3,800 条评论。然而，实际的评论数量要多得多。这是由于 API 应用了速率限制。为了确保 API
    能够继续为所有用户提供服务并避免对基础设施造成负载，供应商通常会实施速率限制。速率限制指定了在特定时间范围内可以向端点发出多少请求。GitHub 的[速率限制策略](https://oreil.ly/PH7hm)如下所述：
- en: For unauthenticated requests, the rate limit allows for up to 60 requests per
    hour. Unauthenticated requests are associated with the originating IP address,
    and not the user making requests.
  id: totrans-60
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 对于未经身份验证的请求，速率限制允许每小时最多 60 次请求。未经身份验证的请求与发起请求的用户相关联的是来源 IP 地址，而不是用户本身。
- en: 'The information about usage is contained in the headers section of the response
    object. We can make a call to the API to only retrieve the headers by using the
    `head` method and then peering into the `X-Ratelimit-Limit`, `X-Ratelimit-Remaining`,
    and `X-RateLimit-Reset` header elements:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `head` 方法可以从 API 中仅检索头部信息，然后查看 `X-Ratelimit-Limit`、`X-Ratelimit-Remaining`
    和 `X-RateLimit-Reset` 头部元素中的信息，这些信息包含在响应对象的头部部分中。
- en: '[PRE18]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '`Out:`'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE19]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '`X-Ratelimit-Limit` indicates how many requests can be made per unit of time
    (one hour in this case), `X-Ratelimit-Remaining` is the number of requests that
    can still be made without violating the rate limits, and `X-RateLimit-Reset` indicates
    the time at which the rate would be reset. It’s possible for different API endpoints
    to have different rate limits. For example, the GitHub Search API has a [per-minute
    rate limit](https://oreil.ly/95Fw7). If you exceed the rate limit by making requests
    that exceed the rate limit, then the API will respond with a status of 403.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '`X-Ratelimit-Limit` 指示每个时间单位内（在本例中为一小时）可以发出多少个请求，`X-Ratelimit-Remaining` 是仍然可以在不违反速率限制的情况下进行的请求数量，而
    `X-RateLimit-Reset` 则指示速率将重置的时间。不同的 API 端点可能具有不同的速率限制。例如，GitHub 搜索 API 拥有[每分钟的速率限制](https://oreil.ly/95Fw7)。如果通过超出速率限制的请求来超过速率限制，则
    API 将以状态码 403 响应。'
- en: 'While making API calls, we must honor the rate limits and also adjust the way
    we make our calls to ensure that we do not overload the server. While extracting
    comments from the repository as in the previous example, we are allowed to make
    60 API calls every hour. We can make the requests one after the other, thereby
    quickly exhausting the limit, which is how our earlier blueprint works. The function
    `handle_rate_limits` shown next slows down the requests to ensure they are spaced
    out over the entire duration. It does so by distributing the remaining requests
    equally over the remaining time by applying a sleep function. This will ensure
    that our data extraction blueprint respects the rate limits and spaces the requests
    so that all the requested data is downloaded:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行 API 调用时，我们必须遵守速率限制，并调整我们的调用方式，以确保不会过载服务器。就像在之前的例子中从仓库中提取评论一样，我们每小时可以允许进行
    60 次 API 调用。我们可以依次发起请求，从而快速耗尽限制，这就是我们先前的蓝图的工作方式。下面展示的函数 `handle_rate_limits` 会减慢请求速度，以确保它们在整个时间段内被间隔地发起。它通过应用休眠函数将剩余请求均匀分布在剩余时间内来实现这一点。这将确保我们的数据提取蓝图遵守速率限制，并且将请求间隔化，以确保所有请求的数据都被下载：
- en: '[PRE20]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Network communication including API calls can fail for several reasons, such
    as interrupted connections, failed DNS lookups, connection timeouts, and so on.
    By default, the requests library does not implement any retries, and therefore
    a nice addition to our blueprint is an implementation of a retry strategy. This
    will allow API calls to be retried in case of specified failure conditions. It
    can be implemented with the `HTTPAdapter` library that allows more fine-grained
    control of the underlying HTTP connections being made. Here we initialize an adapter
    with the retry strategy that specifies five retries for a failed attempt. We also
    specify that these retries should be made only when the error status codes [500](https://httpstatuses.com/500),
    [503](https://httpstatuses.com/503), and [504](https://httpstatuses.com/504) are
    received. In addition, we specify the `backoff_factor`^([1](ch02.xhtml#idm45634214166328))
    value that determines the exponentially increasing time delay between attempts
    after the second try to ensure that we don’t hammer the server.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 网络通信，包括 API 调用，可能因多种原因失败，例如中断的连接、DNS 查询失败、连接超时等。默认情况下，requests 库不实现任何重试机制，因此我们蓝图的一个很好的补充是实现一个重试策略的实现。这将允许在指定的失败条件下重试
    API 调用。可以使用`HTTPAdapter`库来实现，它允许更精细地控制正在进行的底层 HTTP 连接。在这里，我们初始化一个适配器，其中包含指定失败尝试时的五次重试策略。我们还指定了这些重试仅在接收到错误状态码
    [500](https://httpstatuses.com/500)、[503](https://httpstatuses.com/503) 和 [504](https://httpstatuses.com/504)
    时才执行。此外，我们还指定了`backoff_factor`^([1](ch02.xhtml#idm45634214166328))的值，该值确定了在第二次尝试后的指数增加时间延迟，以确保我们不会过度请求服务器。
- en: 'Every request object creates a default `Sessions` object that manages and persists
    connection settings across different requests, such as cookies, authentication,
    and proxies that should be stateless. Up to now we relied on the default `Sessions`
    object, but to override the connection behavior with our retry strategy, we have
    to specify a custom adapter that will enable us to use the retry strategy. This
    means that we will use the new `http Session` object to make our requests, as
    shown in the following code:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 每个请求对象都创建一个默认的`Sessions`对象，它管理和持久化跨不同请求的连接设置，如 cookies、认证和代理，应该是无状态的。到目前为止，我们依赖于默认的`Sessions`对象，但是为了使用我们的重试策略覆盖连接行为，我们必须指定一个自定义适配器，这将使我们能够使用重试策略。这意味着我们将使用新的`http
    Session`对象来发起我们的请求，如下面的代码所示：
- en: '[PRE21]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '`Out:`'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE22]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Putting all this together, we can modify the blueprint to handle pagination,
    rate limits, and retries as follows:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有这些内容整合在一起，我们可以修改蓝图以处理分页、速率限制和重试，如下所示：
- en: '[PRE23]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: If you look closely at the rate limit documentation, you will observe that there
    are different rate limits based on the type of authentication used. All our requests
    up to now were unauthenticated requests, and the rate limits are much lower. We
    can identify our data extraction application to GitHub by registering for an account.
    We can then make authenticated requests to the API that increases the rate limits.
    This practice ensures that there is no abuse of the API by unidentified users
    or fraudulent applications, and most API providers do not allow access to an API
    without a form of authentication.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你仔细查看速率限制的文档，你会发现根据所使用的身份验证类型有不同的速率限制。到目前为止，我们的所有请求都是未经认证的请求，速率限制较低。我们可以通过注册账户将我们的数据提取应用程序标识给
    GitHub。然后，我们可以对 API 发出经过身份验证的请求，从而增加速率限制。这种做法确保未经认证的用户或欺诈性应用程序无法滥用 API，大多数 API
    提供者都不允许未经身份验证的方式访问 API。
- en: This blueprint shows you how to extract data from any API using the simple Python
    requests module and creating your own dataset. This is the fundamental way in
    which most API requests work and is useful for a one-off analysis and initial
    exploration of a new data source. Going back to our use case, if you were looking
    to identify the popular deep-learning frameworks for you to start learning, then
    this blueprint would be a good choice. Or let’s say that your organization already
    has a sales forecasting model and you would like to evaluate the benefit of adding
    financial market news on the accuracy of this model. Assuming there is an API
    that provides financial news, you can easily create a dataset, apply text analytics
    blueprints, and test the relevance to the model.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这个蓝图展示了如何使用简单的Python requests模块从任何API中提取数据，并创建自己的数据集。这是大多数API请求工作的基本方式，适用于一次性分析和新数据源的初步探索。回到我们的用例，如果你想要识别流行的深度学习框架以便开始学习，那么这个蓝图将是一个不错的选择。或者假设您的组织已经有了销售预测模型，您想评估添加财经市场新闻对该模型准确性的影响。假设有一个提供财经新闻的API，你可以轻松地创建一个数据集，应用文本分析蓝图，并测试其对模型的相关性。
- en: 'Blueprint: Extracting Twitter Data with Tweepy'
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 蓝图：使用Tweepy提取Twitter数据
- en: To make it easier for developers to work with their APIs, many of the popular
    services provide packages in multiple programming languages or at least have one
    or more community-supported modules. While the API is officially supported, these
    packages are well-maintained Python modules that incorporate additional functionality
    that makes them easy to use. This means you can focus on the kind of data that
    you would like to extract rather than the technical details of making API calls,
    authentication, and so on. In this blueprint, we will use one of the community-developed
    and supported Python modules for Twitter called [Tweepy](https://oreil.ly/yZOU7).
    Twitter maintains a list of [libraries for different languages](https://oreil.ly/lwrFM)
    that includes several libraries for Python. We chose Tweepy because it’s actively
    maintained and used by many researchers. While this blueprint uses Tweepy to extract
    data from the Twitter API, the steps described would be similar for any other
    API.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使开发人员更容易使用其API，许多流行服务提供了多种编程语言的包，或者至少有一个或多个社区支持的模块。虽然API得到官方支持，但这些包是维护良好的Python模块，包含了额外的功能，使它们易于使用。这意味着你可以专注于你想要提取的数据类型，而不是API调用、身份验证等技术细节。在这个蓝图中，我们将使用一个名为[Tweepy](https://oreil.ly/yZOU7)的社区开发和支持的Python模块来从Twitter中提取数据。Twitter维护了一个[不同语言的库列表](https://oreil.ly/lwrFM)，其中包括几个Python的库。我们选择了Tweepy，因为它得到了积极的维护并被许多研究人员使用。虽然这个蓝图使用Tweepy从Twitter
    API中提取数据，但所描述的步骤对于任何其他API都是类似的。
- en: We described earlier how you might use Twitter to analyze the effectiveness
    of a new marketing campaign. Another use case could be to perform text analytics
    to understand the popularity and sentiment for cryptocurrencies as a way to predict
    their adoption and value in the economy. Twitter is a social media network where
    users spontaneously share short messages, often reacting in real time to world
    events such as major calamities or popular sporting events. The user can also
    add the geolocation if they want to, and this gives us the ability to understand
    the most trending current events in a certain city or geographical area. During
    the government-imposed lockdowns due to COVID-19, several researchers used Twitter
    data to understand the spread of the virus and the [impact of lockdowns](https://oreil.ly/J7pDT)
    and also used these as predictive variables of economic health.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前描述了如何使用Twitter分析新营销活动的有效性。另一个用例可能是执行文本分析，以了解加密货币的流行度和情感，以预测其在经济中的采纳和价值。Twitter是一个社交媒体网络，用户可以即时分享短消息，经常在实时反应世界事件中，如重大灾难或流行的体育赛事。用户还可以添加地理位置信息，这使我们能够了解某个城市或地理区域中最流行的当前事件。在由政府实施的COVID-19封锁期间，一些研究人员使用Twitter数据了解病毒的传播以及封锁的影响，并将这些作为经济健康的预测变量之一。
- en: Warning
  id: totrans-80
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Please note that when using a public API like Twitter, you will retrieve data
    from the public timelines of many users, and it could contain strong, maybe even
    offensive, language, including profanities. Please be aware of this and ensure
    that the data is handled appropriately depending on your use-case.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在使用像 Twitter 这样的公共 API 时，您将从许多用户的公共时间线检索数据，这些数据可能包含强烈甚至冒犯性的语言，包括脏话。请注意此点，并根据您的用例适当处理数据。
- en: Obtaining Credentials
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 获取凭证
- en: The first step when working with any API is authenticating yourself or your
    application. Twitter requires all users of their API to register as a developer
    and provide details for why they would like to use the API. This helps them identify
    you and prevent any unauthorized access. You must [register yourself as a developer](https://oreil.ly/vEnJp).
    If you do not already have a Twitter account, then you will also be required to
    create one. You will be asked about your purpose for creating a developer account
    and additional questions on how you intend to use the Twitter API. [Figure 2-2](#fig-twitter-developer-account)
    shows some examples of these screens. Please provide detailed responses to ensure
    that Twitter fully understands your purpose for creating a developer account.
    For example, in this blueprint we are looking to extract tweets using the API
    to illustrate how this is done. Since we are only going to use the extraction
    capability, the question “Will your app use Tweet, Retweet, like, follow, or Direct
    Message functionality?” is not applicable and can be deselected. You must read
    and understand each question before proceeding. Note that this requirement will
    be different for each API and is also subject to change.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 使用任何 API 的第一步是验证自己或您的应用程序。Twitter 要求其 API 的所有用户都注册为开发者，并提供使用 API 的原因的详细信息。这有助于他们识别您并防止任何未经授权的访问。您必须
    [注册自己作为开发者](https://oreil.ly/vEnJp)。如果您还没有 Twitter 账户，则还需要创建一个账户。您将被要求说明创建开发者账户的目的，并回答关于如何使用
    Twitter API 的其他问题。[图 2-2](#fig-twitter-developer-account) 显示了这些屏幕的示例。请提供详细的回答，以确保
    Twitter 充分了解您创建开发者账户的目的。例如，在这个蓝图中，我们希望使用 API 提取推文以说明其操作方式。由于我们只打算使用提取功能，因此问题“您的应用程序是否会使用推文、转推、喜欢、关注或直接消息功能？”不适用并且可以取消选择。在继续之前，您必须阅读并理解每个问题。请注意，这些要求对每个
    API 都可能有所不同，并且可能会随时更改。
- en: '![](Images/btap_0202.jpg)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/btap_0202.jpg)'
- en: Figure 2-2\. Illustration of sign-up flow for creating a Twitter developer account.
  id: totrans-85
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-2\. 创建 Twitter 开发者账户的注册流程示意图。
- en: Now that you have a developer account, the next step is to create an app. The
    credentials of the app are used when making API calls, and it’s important to specify
    the reason for creating the app. You have to provide details like the app name,
    the purpose for creating the app, and the website URL that is associated with
    the app. If you will use this app for research and learning purposes, then you
    could state this in the app description and provide the URL for your university
    page or GitHub repository associated with your project. Once the app is approved
    by Twitter, you can navigate to the tab *Keys and tokens*, as shown in [Figure 2-3](#fig-twitter-app-creation),
    where you will find the fields *API key* and *API secret key*. Please note that
    these are the credentials that will be used for authentication when making API
    calls, and it’s important to not reveal them.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经拥有开发者账户，下一步是创建应用程序。应用程序的凭证在进行 API 调用时使用，因此指定创建应用程序的原因非常重要。您需要提供应用程序名称、创建原因以及与应用程序相关联的网站
    URL。如果您将此应用程序用于研究和学习目的，则可以在应用程序描述中说明，并提供与项目相关的大学页面或 GitHub 存储库的 URL。一旦 Twitter
    批准了应用程序，您可以转到 *Keys and tokens* 标签，如 [图 2-3](#fig-twitter-app-creation) 所示，那里会显示
    *API key* 和 *API secret key* 字段。请注意，这些是在进行 API 调用时用于身份验证的凭证，不要泄露它们是非常重要的。
- en: '![](Images/btap_0203.jpg)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/btap_0203.jpg)'
- en: Figure 2-3\. Creating a Twitter app and obtaining credentials.
  id: totrans-88
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-3\. 创建 Twitter 应用程序并获取凭证。
- en: Installing and Configuring Tweepy
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装和配置 Tweepy
- en: 'The project repository for [Tweepy](https://oreil.ly/OHfnn) and [documentation](https://oreil.ly/lDDo1)
    are the best source for all information about using Tweepy. We can install Tweepy
    by entering **`pip install tweepy`** into the terminal. Next, we have to authenticate
    the app with the Twitter API, and we do this with the help of the `tweepy.AppAuthHandler`
    module to which we pass the API key and API secret key we obtained in the previous
    step. Finally, we instantiate the `tweepy.API` class, which will be used to make
    all subsequent calls to the Twitter API. Once the connection is made, we can confirm
    the host and version of the API object. Please note that since we are interested
    in read-only access to public information, we use [application-only authentication](https://oreil.ly/4oWbP):'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '[Tweepy](https://oreil.ly/OHfnn) 的项目存储库和 [文档](https://oreil.ly/lDDo1) 是关于使用
    Tweepy 的所有信息的最佳来源。我们可以通过在终端中输入 **`pip install tweepy`** 来安装 Tweepy。接下来，我们必须使用
    `tweepy.AppAuthHandler` 模块对应用进行 Twitter API 的身份验证，我们使用前一步骤中获取的 API 密钥和 API 秘密密钥进行此操作。最后，我们实例化
    `tweepy.API` 类，它将用于进行所有后续对 Twitter API 的调用。一旦连接建立，我们可以确认 API 对象的主机和版本。请注意，由于我们对公共信息的只读访问感兴趣，我们使用
    [仅应用程序身份验证](https://oreil.ly/4oWbP)：'
- en: '[PRE24]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '`Out:`'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出:`'
- en: '[PRE25]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Extracting Data from the Search API
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从搜索 API 中提取数据
- en: 'Let’s say we want to analyze the perception of cryptocurrency and determine
    its popularity. We will use the Search API to retrieve all tweets that mention
    this to create our dataset. The Twitter API also uses pagination to return multiple
    pages of results, but instead of implementing our own way of managing this, we
    will use the `Cursor` object provided by the Tweepy library to iterate through
    the results. We pass the search query to the API object and additionally specify
    the language of the tweets to be extracted (English in this case). We choose to
    retrieve only 100 items and create a `DataFrame` by loading the results as a JSON
    object:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想分析加密货币的感知并确定其受欢迎程度。我们将使用搜索 API 检索提到这一点的所有推文以创建我们的数据集。Twitter API 也使用分页来返回多页结果，但我们不会实现自己的管理方式，而是使用
    Tweepy 库提供的 `Cursor` 对象来遍历结果。我们将搜索查询传递给 API 对象，并另外指定要提取的推文的语言（在这种情况下为英语）。我们选择只检索
    100 项，并通过将结果加载为 JSON 对象来创建 `DataFrame`：
- en: '[PRE26]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '|   | text |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '|   | 文本 |'
- en: '| --- | --- |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| 59 | Hi! I’ve been using OKEx which makes it really easy and safe to buy,
    sell, and store cryptocurrency (like Bitcoin).… https://t.co/4m0mpyQTSN |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| 59 | 嗨！我一直在使用OKEx，它让购买、出售和存储加密货币（如比特币）变得非常简单和安全。… https://t.co/4m0mpyQTSN
    |'
- en: '| 17 | Get connected today 📉 #getconnected #bitcointrading #Bitcoin #BitcoinCash
    #bitcoinmining #cryptocurrency https://t.co/J60bCyFPUI |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| 17 | 今天连接上📉 #getconnected #bitcointrading #Bitcoin #BitcoinCash #bitcoinmining
    #cryptocurrency https://t.co/J60bCyFPUI |'
- en: '| 22 | RT @stoinkies: We reached over 100 followers!\nGiveaway time!\nFOLLOW
    +RETWEET + LIKE THIS TWEET = Win 200 Dogecoin!\nEvery participant also g… |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| 22 | RT @stoinkies: 我们已经有了超过 100 位关注者！\n赠品时间！\n关注 + 转推 + 喜欢此推文 = 赢取 200 个
    Dogecoin！\n每个参与者还将获得… |'
- en: We have successfully completed the API call and can see the text of the retrieved
    tweets in the previous table, which already show interesting aspects. For example,
    we see the use of the word *RT*, which indicates a retweet (where the user has
    shared another tweet). We see the usage of emojis, which is a strong characteristic
    of the medium, and also notice that some tweets are truncated. Twitter actually
    imposes a limit on the number of characters that each tweet can contain, which
    was originally 140 characters and later extended to 280\. This led to the creation
    of an [extended tweet object](https://oreil.ly/fvl-3), which we must specify explicitly
    while retrieving results in Tweepy. Additionally, you must be aware that the standard
    version of the Twitter Search API provides results only from the last week, and
    one must sign up for the Premium or Enterprise versions for historical tweets.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已成功完成了 API 调用，并可以在上一个表格中看到检索到的推文的文本，这些推文已显示出有趣的方面。例如，我们看到了 *RT* 这个词的使用，它表示转推（用户分享了另一条推文）。我们看到了表情符号的使用，这是该媒体的一个强烈特征，并且还注意到一些推文被截断了。Twitter
    实际上对每条推文所包含的字符数施加了限制，最初为 140 个字符，后来扩展到了 280 个字符。这导致创建了一个 [扩展的推文对象](https://oreil.ly/fvl-3)，我们必须在使用
    Tweepy 检索结果时显式指定它。此外，您必须知道，Twitter 搜索 API 的标准版本仅提供过去一周的结果，必须注册高级或企业版本才能获得历史推文。
- en: Note
  id: totrans-103
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: For each endpoint, Twitter specifies a maximum value of `count`. This is the
    maximum number of results that is returned in a single page of the response. For
    example, the search endpoint specifies a maximum value of `count=100`, whereas
    `user_timeline` has a maximum value of `count=200`.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 每个终端点，Twitter指定了`count`的最大值。这是单个响应页面返回的最大结果数。例如，搜索终端点指定了`count=100`的最大值，而`user_timeline`的最大值为`count=200`。
- en: 'Let’s expand our search to include an additional keyword relevant to the cryptocurrency
    topic like `crypto` and filter out retweets for now. This is done by using the
    `filter` keyword appended with a minus sign in the search term. We also specify
    that we would like to fetch tweets with the `tweet_mode=extended` parameter, which
    ensures that we retrieve the full text of all tweets. The [Standard search API](https://oreil.ly/4IGcB)
    searches only a sample of recent Tweets published in the past seven days, but
    even this could potentially be a large number, and to avoid a large wait time
    to run the blueprint, we restrict ourselves to 12,000 tweets. We specify the parameter
    `count=30`, which is the maximum number of tweets that can be retrieved in one
    call. Therefore, we must make 400 such calls to obtain our dataset while taking
    into consideration the rate limits. This is within the rate limit of 450 requests
    every 15 minutes specified by the API. It’s possible that you might exceed this
    rate limit while experimenting with this blueprint, and therefore we enable the
    automatic wait functionality provided by Tweepy by setting the `wait_on_rate_limit`
    parameter. We also set `wait_on_rate_limit_notify` so that we are notified of
    such wait times. If you are within the rate limits, the following function should
    execute in about five minutes:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们扩展我们的搜索，包括与加密货币主题相关的其他关键字，比如`crypto`，并且暂时过滤转发。这是通过在搜索词中附加带有减号的`filter`关键字来完成的。我们还指定了希望使用`tweet_mode=extended`参数获取所有推文的全文。[标准搜索API](https://oreil.ly/4IGcB)只搜索过去七天内发布的最新推文样本，但即使这样也可能是一个大数字，为了避免长时间等待来运行蓝图，我们限制了自己的推文数到12,000条。我们指定了参数`count=30`，这是一次调用中可以检索到的最大推文数。因此，我们必须进行400次这样的调用来获取我们的数据集，同时考虑到速率限制。这在API规定的每15分钟450个请求的速率限制内。在尝试这个蓝图时，可能会超过这个速率限制，因此我们通过设置`wait_on_rate_limit`参数启用Tweepy提供的自动等待功能。我们还设置了`wait_on_rate_limit_notify`以便在这种等待时间内得到通知。如果您在速率限制内，以下函数应该在约五分钟内执行完毕：
- en: '[PRE27]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '`Out:`'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE28]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '|   | created_at | full_text | entities.hashtags |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '|   | created_at | full_text | entities.hashtags |'
- en: '| --- | --- | --- | --- |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| 10505 | Sat Sep 19 22:30:12 +0000 2020 | Milk was created to let liquidity
    providers (people who have LP tokens) benefit because they can stake LP tokens
    at SpaceSwap, they get MILK token as a reward as well as 0.3% UniSwap commission.\n\n👇👇👇\nhttps://t.co/M7sGbIDq4W\n#DeFi
    #cryptocurrency #UniSwap #altcoin | [{''text’: ‘DeFi'', ‘indices’: [224, 229]},
    {''text’: ‘cryptocurrency'', ‘indices’: [230, 245]}, {''text’: ‘UniSwap'', ‘indices’:
    [246, 254]}, {''text’: ‘altcoin'', ‘indices’: [256, 264]}] |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| 10505 | Sat Sep 19 22:30:12 +0000 2020 | Milk被创造出来是为了让流动性提供者（持有LP代币的人）受益，因为他们可以在SpaceSwap抵押LP代币，作为奖励他们会得到MILK代币以及0.3%的UniSwap佣金。\n\n👇👇👇\nhttps://t.co/M7sGbIDq4W\n#DeFi
    #加密货币 #UniSwap #另类币 | [{''text’: ‘DeFi'', ‘indices’: [224, 229]}, {''text’: ‘加密货币'',
    ‘indices’: [230, 236]}, {''text’: ‘UniSwap'', ‘indices’: [246, 254]}, {''text’:
    ‘另类币'', ‘indices’: [256, 261]}] |'
- en: '| 11882 | Sat Sep 19 20:57:45 +0000 2020 | You can EARN dividends from our
    curation activity. The minimum to participate is 2000 #steem delegation... with
    delegation there is no risk of losing your principal. We can process the payout
    in #bitcoin and all major #cryptocurrencies .. #cryptocurrency \nhttps://t.co/4b3iH2AI4S
    | [{''text’: ’steem'', ‘indices’: [86, 92]}, {''text’: ‘bitcoin'', ‘indices’:
    [195, 203]}, {''text’: ‘cryptocurrencies'', ‘indices’: [218, 235]}, {''text’:
    ‘cryptocurrency'', ‘indices’: [239, 254]}] |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| 11882 | Sat Sep 19 20:57:45 +0000 2020 | 您可以从我们的策划活动中获得股息。参与的最低要求是2000 #steem
    代理... 通过代理，您不会失去本金。我们可以用#bitcoin和所有主要的#加密货币处理支付... #加密货币\nhttps://t.co/4b3iH2AI4S
    | [{''text’: ’steem'', ‘indices’: [86, 92]}, {''text’: ‘bitcoin'', ‘indices’:
    [195, 203]}, {''text’: ‘cryptocurrencies'', ‘indices’: [218, 235]}, {''text’:
    ‘加密货币'', ‘indices’: [239, 244]}] |'
- en: 'There is a lot of information that the API provides, as shown in the sample
    of two previous tweets that contain important elements such as the date when the
    tweet was sent out, the content of the tweet, and so on. Twitter also returns
    several entities such as hashtags contained within the tweet, and it would be
    interesting to see which hashtags are used heavily when discussing cryptocurrency:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: API 提供了大量信息，如前两条推文的示例所示，其中包含推文发送日期、推文内容等重要元素。Twitter 还返回了多个实体，如包含在推文中的标签，查看讨论加密货币时使用哪些标签将会很有趣：
- en: '[PRE29]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The preceding code creates the graph shown in [Figure 2-4](#fig02in01), which
    shows us the important hashtags being used in conjunction with cryptocurrencies.
    It includes examples of cryptocurrencies such as *bitcoin* and *ethereum* as well
    as their trading short-codes *btc* and *eth*. It also throws up related activities
    such as *trading* and *airdrops*. There are also mentions of entities like *fintech*
    and *applecash*. At a first glance, it already gives you insight into the various
    terms and entities being discussed, and the presence of trading short-codes indicates
    that there might be some market information contained in these tweets. While this
    is a simple count of entities, we can use this dataset to apply more advanced
    text analytics techniques to determine popular sentiment about cryptocurrencies
    that derive relationships between entities. Please note that the results may differ
    depending on when the Twitter search was run and the random selection by the API.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码创建了 [图 2-4](#fig02in01) 中显示的图表，显示了与加密货币一起使用的重要标签。其中包括比特币和以太坊等加密货币的示例，以及它们的交易简码
    *btc* 和 *eth*。还涉及到诸如 *交易* 和 *空投* 等相关活动。还提到了诸如 *金融科技* 和 *applecash* 的实体。乍一看，它已经让您了解到正在讨论的各种术语和实体，交易简码的存在表明这些推文中可能包含一些市场信息。虽然这只是一种实体的简单计数，但我们可以使用此数据集应用更高级的文本分析技术来确定关于加密货币的流行情绪及其实体之间的关系。请注意，由于
    Twitter 搜索运行的时间以及 API 的随机选择，结果可能会有所不同。
- en: '![](Images/btap_02in01.jpg)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/btap_02in01.jpg)'
- en: Figure 2-4\. Common hashtags used when discussing cryptocurrency.
  id: totrans-117
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-4\. 讨论加密货币时使用的常见标签。
- en: Extracting Data from a User’s Timeline
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从用户时间线提取数据
- en: 'Search is not the only way to interact with Twitter as we can use the API to
    also extract tweets by a specific user or account. This might be a person like
    a famous celebrity or world leader, or it might be an organization like a sports
    team. For instance, what if we would like to compare tweets from two popular Formula
    One teams, Mercedes and Ferrari? We can extract all the tweets that they have
    sent out and contrast their individual styles and the main themes that they focus
    on. We provide the screen name for the account (`MercedesAMGF1`) to retrieve all
    the tweets sent by this account:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 搜索并不是与 Twitter 互动的唯一方式，我们也可以使用 API 按特定用户或账户提取推文。这可能是像著名名人或世界领导人这样的个人，也可能是像体育队这样的组织。例如，假设我们想比较两个流行的一级方程式车队，梅赛德斯和法拉利的推文。我们可以提取他们发送的所有推文，并对比它们的个别风格和它们关注的主要主题。我们提供账户的用户名（`MercedesAMGF1`），以检索此账户发送的所有推文：
- en: '[PRE30]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '`Out:`'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE31]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'As you can see, though we requested 5,000 tweets, we were able to retrieve
    only about 3,200 of them. This is a [restriction placed on the API](https://oreil.ly/RaNaQ).
    Let’s retrieve the tweets for the Ferrari team as well using their screen name
    (`ScuderiaFerrari`):'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所见，尽管我们请求了 5,000 条推文，但我们只能检索到大约 3,200 条。这是 API 设置的一个 [限制](https://oreil.ly/RaNaQ)。让我们也使用他们的账户用户名
    (`ScuderiaFerrari`) 检索法拉利车队的推文：
- en: '[PRE32]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '[PRE33]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '`Out:`'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE34]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Warning
  id: totrans-128
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: One of the quirks of the Tweepy implementation is that in the case of retweets,
    the `full_text` column is truncated, and the `retweeted_status.full_text` column
    must be used to retrieve all the characters of the tweet. For our use case, retweets
    are not important, and we filter them by checking if `retweeted_status.id` is
    empty. However, depending on the use case, you can add a condition to replace
    the column `full_text` with `retweeted_status.full_text` in the case of retweets.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: Tweepy 实现中的一个怪癖是，在转发的情况下，`full_text` 列会被截断，必须使用 `retweeted_status.full_text`
    列来检索推文的所有字符。对于我们的用例，转发并不重要，我们通过检查 `retweeted_status.id` 是否为空来过滤它们。然而，根据用例的不同，您可以添加条件，在转发的情况下将
    `full_text` 列替换为 `retweeted_status.full_text` 列。
- en: 'When we remove retweets, the number of tweets authored by each team handle
    significantly drops. We will reuse the word cloud blueprint from [Chapter 1](ch01.xhtml#ch-exploration)
    with the function `wordcloud` to quickly visualize the tweets from each of the
    two teams and identify the keywords they focus on. Mercedes tweets seem to focus
    a lot on the races that the team participates in, such as *tuscangp*, *britishgp*
    and *race*, *day*. The Ferrari tweets, on the other hand, promote their merchandise,
    such as *ferraristore*, and drivers, such as *enzofitti* and *schumachermick*:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 移除转发后，每个团队句柄的推文数量显著减少。我们将重复使用来自[第一章](ch01.xhtml#ch-exploration)的词云蓝图，并使用函数 `wordcloud`
    快速可视化两个团队的推文，并识别他们关注的关键词。梅赛德斯的推文似乎主要关注车队参与的比赛，如 *tuscangp*、*britishgp* 和 *race*、*day*。另一方面，法拉利的推文则宣传他们的商品，如
    *ferraristore*，以及车手，如 *enzofitti* 和 *schumachermick*。
- en: '[PRE35]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '`Out:`'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '![](Images/btap_02in02.jpg)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/btap_02in02.jpg)'
- en: Extracting Data from the Streaming API
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从流 API 提取数据
- en: Some APIs provide near real-time data, which might also be referred to as *streaming
    data*. In such a scenario, the API would like to *push* the data to us rather
    than waiting for a *get* request as we have been doing so far. An example of this
    is the Twitter Streaming API. This API provides us with a sample of the tweets
    being sent out in real time and can be filtered on several criteria. Since this
    is a continuous stream of data, we have to handle the data extraction process
    in a different manner. Tweepy already provides basic functionality in the `StreamListener`
    class that contains the `on_data` function. This function is called each time
    a new tweet is pushed by the streaming API, and we can customize it to implement
    logic that is specific to certain use cases.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 一些 API 提供接近实时的数据，也可以称为 *流数据*。在这种情况下，API 希望将数据“推送”给我们，而不是像我们目前所做的那样等待“获取”请求。Twitter
    Streaming API 就是一个例子。该 API 实时提供发送的推文样本，并可以根据多个标准进行过滤。由于这是持续的数据流，我们必须以不同的方式处理数据提取过程。Tweepy
    在 `StreamListener` 类中已经提供了基本功能，其中包含 `on_data` 函数。每当流 API 推送新推文时，将调用此函数，并且我们可以根据特定用例定制它以实施特定逻辑。
- en: 'Staying with the cryptocurrency use case, let’s suppose that we want to have
    a continuously updated sentiment measure of different cryptocurrencies to make
    trading decisions. In this case, we would track real-time tweets mentioning cryptocurrencies
    and continuously update the popularity score. On the other hand, as researchers,
    we might be interested in analyzing the reactions of users during key live events
    such as the Super Bowl or announcement of election results. In such scenarios,
    we would listen for the entire duration of the event and store the results for
    subsequent analysis. To keep this blueprint generic, we have created the `FileStreamListener`
    class as shown next, which will manage all the actions to be taken on the stream
    of incoming tweets. For every tweet pushed by the Twitter API, the `on_data` method
    is called. In our implementation, we gather incoming tweets into batches of 100
    and then write to a file with the timestamp. The choice of 100 can be varied based
    on the memory available on the system:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 继续以加密货币用例为例，假设我们想要对不同加密货币的情绪进行持续更新以进行交易决策。在这种情况下，我们将追踪提到加密货币的实时推文，并持续更新其流行度分数。另一方面，作为研究人员，我们可能对分析用户在重大现场事件（如超级碗或选举结果公布）期间的反应感兴趣。在这种情况下，我们将监听整个事件的持续时间，并将结果存储以进行后续分析。为了使此蓝图通用化，我们创建了如下所示的
    `FileStreamListener` 类，它将管理流入推文的所有操作。对于 Twitter API 推送的每条推文，将调用 `on_data` 方法。在我们的实现中，我们将传入的推文收集到批次中，每批100条，然后带有时间戳写入文件。可以根据系统可用的内存选择不同的批次大小。
- en: '[PRE36]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'To get access to the streaming API, the basic app authentication is not enough.
    We must also provide the user authentication, which can be found on the same page
    as shown before. This means that the Streaming API requests are made by the app
    we created on behalf of the user (in this case our own account). This also means
    that we have to use the `OAuthHandler` class instead of the `AppAuthHandler` that
    we used up to now:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问流 API，基本的应用程序认证是不够的。我们还必须提供用户认证，这可以在之前显示的同一页找到。这意味着流 API 请求是由我们创建的应用程序代表用户（在本例中是我们自己的帐户）发出的。这也意味着我们必须使用
    `OAuthHandler` 类，而不是我们目前使用的 `AppAuthHandler`：
- en: '[PRE37]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'When initializing an object of `FileStreamListener`, we also specify the maximum
    number of tweets that we would like to extract. This acts like a stopping condition,
    and if not specified, the process will run as long as it is not terminated by
    the user or stopped due to a server error. We initialize the Twitter stream by
    passing in the authentication object (`api.auth`) and the object that will manage
    the stream (`fileStreamListener`). We also ask for the extended tweets to be provided.
    Once this is done, we can start tracking live tweets from the stream using the
    filter function and providing keywords that we would like to track:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在初始化`FileStreamListener`对象时，我们还指定了希望提取的最大推文数。这充当停止条件，如果未指定，则进程将持续运行，直到用户终止或由于服务器错误而停止。我们通过传递认证对象（`api.auth`）和管理流的对象（`fileStreamListener`）来初始化Twitter流。我们还要求提供扩展推文。完成这些步骤后，我们可以使用过滤函数并提供想要跟踪的关键字来开始跟踪流中的实时推文：
- en: '[PRE38]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: If you would like to run the extractor in a separate thread, you can pass the
    keyword `async=True` to the filter function, and this will run continuously in
    a separate thread. Once it has run for some time and stored tweets, we can read
    this into a Pandas `DataFrame` as before. When an error occurs, the `FileStreamListener`
    does not attempt retries but only prints the error `status_code`. You are encouraged
    to implement failure handling and customize the `on_data` method to suit the use
    case.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你希望在单独的线程中运行提取器，可以将关键字`async=True`传递给过滤函数，这将在单独的线程中持续运行。一旦它运行一段时间并存储了推文，我们可以像以前一样将其读入Pandas的`DataFrame`中。当发生错误时，`FileStreamListener`不会尝试重试，而是仅打印错误`status_code`。建议您实现失败处理并自定义`on_data`方法以适应使用情况。
- en: 'These blueprints only provide guidance on accessing popular APIs for data extraction.
    Since each API is different, the functionality provided by the corresponding Python
    module will also be different. For instance, [Wikipedia](https://oreil.ly/zruJt)
    is another popular source for extracting text data, and [`wikipediaapi`](https://oreil.ly/Eyon3)
    is one of the supported Python modules for extracting this data. It can be installed
    by using the command **`pip install wikipediaapi`**, and since this is a publicly
    available data source, the authentication and generation of access tokens is not
    necessary. You only need to specify the version of Wikipedia (language) and the
    topic name for which you want to extract data. The following code snippet shows
    the steps to download the Wikipedia entry for “Cryptocurrency” and shows the initial
    few lines of this article:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这些蓝图仅提供了访问流行API进行数据提取的指导。由于每个API都不同，相应的Python模块提供的功能也会不同。例如，[Wikipedia](https://oreil.ly/zruJt)是另一个用于提取文本数据的流行来源，而[`wikipediaapi`](https://oreil.ly/Eyon3)是支持此数据提取的Python模块之一。可以通过命令**`pip
    install wikipediaapi`**来安装它，由于这是一个公开可用的数据源，因此不需要进行身份验证或生成访问令牌。您只需要指定维基百科的版本（语言）和您要提取数据的主题名称。以下代码片段显示了下载“加密货币”维基百科条目的步骤，并显示了该文章的前几行：
- en: '[PRE39]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '`Out:`'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE40]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Closing Remarks
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结语
- en: In this chapter, we first introduced blueprints that make use of the Python
    requests library to make API calls and extract data. We also introduced ways to
    work with paginated results, rate limits, and retries. These blueprints work for
    any kind of API and are great if you would like to control and customize several
    aspects for your data extraction. In the next set of blueprints, we used Tweepy
    to extract data from the Twitter API. This is an example of a community-developed
    Python library that supports a popular API and provides tested functionality out
    of the box. You often don’t have to worry about implementing your own pagination
    or backoff strategy and is therefore one less thing to worry about. If your use
    case needs to get data from a popular API, then it is convenient to use such a
    preexisting package.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们首先介绍了蓝图，这些蓝图利用Python的requests库进行API调用和数据提取。我们还介绍了如何处理分页结果、速率限制和重试。这些蓝图适用于任何类型的API，并且非常适合如果您希望控制和定制数据提取的多个方面。在下一组蓝图中，我们使用了Tweepy从Twitter
    API提取数据。这是一个由社区开发的Python库的示例，支持流行的API，并提供经过测试的功能。您通常不必担心实现自己的分页或回退策略，因此这是少了一个要担心的事情。如果您的使用情况需要从流行的API获取数据，那么使用这样一个现成的包非常方便。
- en: ^([1](ch02.xhtml#idm45634214166328-marker)) Delay introduced between subsequent
    calls defined as `time_delay={backoff factor} * (2 ** ({number of total retries}
    - 1))`.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch02.xhtml#idm45634214166328-marker)) 延迟被定义为 `time_delay={backoff factor}
    * (2 ** ({number of total retries} - 1))`，在连续调用之间引入。
