["```py\n$ pip3 install pyspark\n```", "```py\n$ pip3 install pyspark[sql,ml,mllib]\n```", "```py\n$ pyspark --master local[*]\n```", "```py\n$ pyspark --master yarn --deploy-mode client\n```", "```py\nPython 3.6.12 |Anaconda, Inc.| (default, Sep  8 2020, 23:10:56)\n[GCC 7.3.0] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\nWelcome to\n      ____              __\n     / __/__  ___ _____/ /__\n    _\\ \\/ _ \\/ _ `/ __/  '_/\n   /__ / .__/\\_,_/_/ /_/\\_\\   version 3.0.1\n      /_/\n\nUsing Python version 3.6.12 (default, Sep  8 2020 23:10:56)\nSparkSession available as 'spark'.\n```", "```py\nspark\n...\n<pyspark.sql.session.SparkSession object at DEADBEEF>\n```", "```py\n spark.[\\t]\n...\nspark.Builder(           spark.conf\nspark.newSession(        spark.readStream\nspark.stop(              spark.udf\nspark.builder            spark.createDataFrame(\nspark.range(             spark.sparkContext\nspark.streams            spark.version\nspark.catalog            spark.getActiveSession(\nspark.read               spark.sql(\nspark.table(\n```", "```py\n$ mkdir linkage\n$ cd linkage/\n$ curl -L -o donation.zip https://bit.ly/1Aoywaq\n$ unzip donation.zip\n$ unzip 'block_*.zip'\n```", "```py\n$ hadoop dfs -mkdir linkage\n$ hadoop dfs -put block_*.csv linkage\n```", "```py\nprev = spark.read.csv(\"linkage/block*.csv\")\n...\nprev\n...\nDataFrame[_c0: string, _c1: string, _c2: string, _c3: string,...\n```", "```py\nprev.show(2)\n...\n+-----+-----+------------+------------+------------+------------+-------+------+\n|  _c0|  _c1|         _c2|         _c3|         _c4|         _c5|    _c6|   _c7|\n+-----+-----+------------+------------+------------+------------+-------+------+\n| id_1| id_2|cmp_fname_c1|cmp_fname_c2|cmp_lname_c1|cmp_lname_c2|cmp_sex|cmp_bd|\n| 3148| 8326|           1|           ?|           1|           ?|      1|     1|\n|14055|94934|           1|           ?|           1|           ?|      1|     1|\n|33948|34740|           1|           ?|           1|           ?|      1|     1|\n|  946|71870|           1|           ?|           1|           ?|      1|     1|\n```", "```py\nparsed = spark.read.option(\"header\", \"true\").option(\"nullValue\", \"?\").\\\n          option(\"inferSchema\", \"true\").csv(\"linkage/block*.csv\")\n```", "```py\nparsed.printSchema()\n...\nroot\n |-- id_1: integer (nullable = true)\n |-- id_2: integer (nullable = true)\n |-- cmp_fname_c1: double (nullable = true)\n |-- cmp_fname_c2: double (nullable = true)\n...\n```", "```py\nfrom pyspark.sql.types import *\nschema = StructType([StructField(\"id_1\", IntegerType(), False),\n  StructField(\"id_2\", StringType(), False),\n  StructField(\"cmp_fname_c1\", DoubleType(), False)])\n\nspark.read.schema(schema).csv(\"...\")\n```", "```py\nschema = \"id_1 INT, id_2 INT, cmp_fname_c1 DOUBLE\"\n```", "```py\nparsed.first()\n...\nRow(id_1=3148, id_2=8326, cmp_fname_c1=1.0, cmp_fname_c2=None,...\n```", "```py\nparsed.printSchema()\n...\nroot\n |-- id_1: integer (nullable = true)\n |-- id_2: integer (nullable = true)\n |-- cmp_fname_c1: double (nullable = true)\n |-- cmp_fname_c2: double (nullable = true)\n |-- cmp_lname_c1: double (nullable = true)\n |-- cmp_lname_c2: double (nullable = true)\n |-- cmp_sex: integer (nullable = true)\n |-- cmp_bd: integer (nullable = true)\n |-- cmp_bm: integer (nullable = true)\n |-- cmp_by: integer (nullable = true)\n |-- cmp_plz: integer (nullable = true)\n |-- is_match: boolean (nullable = true)\n\n...\n\nparsed.show(5)\n...\n+-----+-----+------------+------------+------------+------------+.....\n| id_1| id_2|cmp_fname_c1|cmp_fname_c2|cmp_lname_c1|cmp_lname_c2|.....\n+-----+-----+------------+------------+------------+------------+.....\n| 3148| 8326|         1.0|        null|         1.0|        null|.....\n|14055|94934|         1.0|        null|         1.0|        null|.....\n|33948|34740|         1.0|        null|         1.0|        null|.....\n|  946|71870|         1.0|        null|         1.0|        null|.....\n|64880|71676|         1.0|        null|         1.0|        null|.....\n```", "```py\nparsed.count()\n...\n5749132\n```", "```py\nparsed.cache()\n```", "```py\nfrom pyspark.sql.functions import col\n\nparsed.groupBy(\"is_match\").count().orderBy(col(\"count\").desc()).show()\n...\n+--------+-------+\n|is_match|  count|\n+--------+-------+\n|   false|5728201|\n|    true|  20931|\n+--------+-------+\n```", "```py\nparsed.createOrReplaceTempView(\"linkage\")\n```", "```py\nspark.sql(\"\"\"\n SELECT is_match, COUNT(*) cnt\n FROM linkage\n GROUP BY is_match\n ORDER BY cnt DESC\n\"\"\").show()\n...\n+--------+-------+\n|is_match|    cnt|\n+--------+-------+\n|   false|5728201|\n|    true|  20931|\n+--------+-------+\n```", "```py\nsummary = parsed.describe()\n...\nsummary.show()\n```", "```py\nsummary.select(\"summary\", \"cmp_fname_c1\", \"cmp_fname_c2\").show()\n+-------+------------------+------------------+\n|summary|      cmp_fname_c1|      cmp_fname_c2|\n+-------+------------------+------------------+\n|  count|           5748125|            103698|\n|   mean|0.7129024704436274|0.9000176718903216|\n| stddev|0.3887583596162788|0.2713176105782331|\n|    min|               0.0|               0.0|\n|    max|               1.0|               1.0|\n+-------+------------------+------------------+\n```", "```py\nmatches = parsed.where(\"is_match = true\")\nmatch_summary = matches.describe()\n\nmisses = parsed.filter(col(\"is_match\") == False)\nmiss_summary = misses.describe()\n```", "```py\nsummary_p = summary.toPandas()\n```", "```py\nsummary_p.head()\n...\nsummary_p.shape\n...\n(5,12)\n```", "```py\nsummary_p = summary_p.set_index('summary').transpose().reset_index()\n...\nsummary_p = summary_p.rename(columns={'index':'field'})\n...\nsummary_p = summary_p.rename_axis(None, axis=1)\n...\nsummary_p.shape\n...\n(11,6)\n```", "```py\nsummaryT = spark.createDataFrame(summary_p)\n...\nsummaryT.show()\n...\n+------------+-------+-------------------+-------------------+---+------+\n|       field|  count|               mean|             stddev|min|   max|\n+------------+-------+-------------------+-------------------+---+------+\n|        id_1|5749132|  33324.48559643438| 23659.859374488064|  1| 99980|\n|        id_2|5749132|  66587.43558331935| 23620.487613269695|  6|100000|\n|cmp_fname_c1|5748125| 0.7129024704437266|0.38875835961628014|0.0|   1.0|\n|cmp_fname_c2| 103698| 0.9000176718903189| 0.2713176105782334|0.0|   1.0|\n|cmp_lname_c1|5749132| 0.3156278193080383| 0.3342336339615828|0.0|   1.0|\n|cmp_lname_c2|   2464| 0.3184128315317443|0.36856706620066537|0.0|   1.0|\n|     cmp_sex|5749132|  0.955001381078048|0.20730111116897781|  0|     1|\n|      cmp_bd|5748337|0.22446526708507172|0.41722972238462636|  0|     1|\n|      cmp_bm|5748337|0.48885529849763504| 0.4998758236779031|  0|     1|\n|      cmp_by|5748337| 0.2227485966810923| 0.4160909629831756|  0|     1|\n|     cmp_plz|5736289|0.00552866147434343|0.07414914925420046|  0|     1|\n+------------+-------+-------------------+-------------------+---+------+\n```", "```py\nsummaryT.printSchema()\n...\nroot\n |-- field: string (nullable = true)\n |-- count: string (nullable = true)\n |-- mean: string (nullable = true)\n |-- stddev: string (nullable = true)\n |-- min: string (nullable = true)\n |-- max: string (nullable = true)\n```", "```py\nfrom pyspark.sql.types import DoubleType\nfor c in summaryT.columns:\n  if c == 'field':\n    continue\n  summaryT = summaryT.withColumn(c, summaryT[c].cast(DoubleType()))\n...\nsummaryT.printSchema()\n...\nroot\n |-- field: string (nullable = true)\n |-- count: double (nullable = true)\n |-- mean: double (nullable = true)\n |-- stddev: double (nullable = true)\n |-- min: double (nullable = true)\n |-- max: double (nullable = true)\n```", "```py\nfrom pyspark.sql import DataFrame\nfrom pyspark.sql.types import DoubleType\n\ndef pivot_summary(desc):\n  # convert to pandas dataframe\n  desc_p = desc.toPandas()\n  # transpose\n  desc_p = desc_p.set_index('summary').transpose().reset_index()\n  desc_p = desc_p.rename(columns={'index':'field'})\n  desc_p = desc_p.rename_axis(None, axis=1)\n  # convert to Spark dataframe\n  descT = spark.createDataFrame(desc_p)\n  # convert metric columns to double from string\n  for c in descT.columns:\n    if c == 'field':\n      continue\n    else:\n      descT = descT.withColumn(c, descT[c].cast(DoubleType()))\n  return descT\n```", "```py\nmatch_summaryT = pivot_summary(match_summary)\nmiss_summaryT = pivot_summary(miss_summary)\n```", "```py\nmatch_summaryT.createOrReplaceTempView(\"match_desc\")\nmiss_summaryT.createOrReplaceTempView(\"miss_desc\")\nspark.sql(\"\"\"\n SELECT a.field, a.count + b.count total, a.mean - b.mean delta\n FROM match_desc a INNER JOIN miss_desc b ON a.field = b.field\n WHERE a.field NOT IN (\"id_1\", \"id_2\")\n ORDER BY delta DESC, total DESC\n\"\"\").show()\n...\n+------------+---------+--------------------+\n|       field|    total|               delta|\n+------------+---------+--------------------+\n|     cmp_plz|5736289.0|  0.9563812499852176|\n|cmp_lname_c2|   2464.0|  0.8064147192926264|\n|      cmp_by|5748337.0|  0.7762059675300512|\n|      cmp_bd|5748337.0|   0.775442311783404|\n|cmp_lname_c1|5749132.0|  0.6838772482590526|\n|      cmp_bm|5748337.0|  0.5109496938298685|\n|cmp_fname_c1|5748125.0|  0.2854529057460786|\n|cmp_fname_c2| 103698.0| 0.09104268062280008|\n|     cmp_sex|5749132.0|0.032408185250332844|\n+------------+---------+--------------------+\n```", "```py\ngood_features = [\"cmp_lname_c1\", \"cmp_plz\", \"cmp_by\", \"cmp_bd\", \"cmp_bm\"]\n...\nsum_expression = \" + \".join(good_features)\n...\nsum_expression\n...\n'cmp_lname_c1 + cmp_plz + cmp_by + cmp_bd + cmp_bm'\n```", "```py\nfrom pyspark.sql.functions import expr\nscored = parsed.fillna(0, subset=good_features).\\\n                withColumn('score', expr(sum_expression)).\\\n                select('score', 'is_match')\n...\nscored.show()\n...\n+-----+--------+\n|score|is_match|\n+-----+--------+\n|  5.0|    true|\n|  5.0|    true|\n|  5.0|    true|\n|  5.0|    true|\n|  5.0|    true|\n|  5.0|    true|\n|  4.0|    true|\n...\n```", "```py\ndef crossTabs(scored: DataFrame, t: DoubleType) -> DataFrame:\n  return  scored.selectExpr(f\"score >= {t} as above\", \"is_match\").\\\n          groupBy(\"above\").pivot(\"is_match\", (\"true\", \"false\")).\\\n          count()\n```", "```py\ncrossTabs(scored, 4.0).show()\n...\n+-----+-----+-------+\n|above| true|  false|\n+-----+-----+-------+\n| true|20871|    637|\n|false|   60|5727564|\n+-----+-----+-------+\n```", "```py\ncrossTabs(scored, 2.0).show()\n...\n+-----+-----+-------+\n|above| true|  false|\n+-----+-----+-------+\n| true|20931| 596414|\n|false| null|5131787|\n+-----+-----+-------+\n```"]