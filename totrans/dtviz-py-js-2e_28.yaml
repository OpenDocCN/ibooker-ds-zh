- en: Chapter 22\. Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although this book had a guiding narrative—the transformation of some basic
    Wikipedia HTML pages into a modern, interactive JavaScript web visualization—it
    is meant to be dipped into as and when required. The different parts are self-contained,
    allowing for the existence of the dataset in its various stages, and can be used
    independently. Let’s have a short recap of what was covered before moving on to
    a few ideas for future visualization work.
  prefs: []
  type: TYPE_NORMAL
- en: Recap
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This book was divided into five parts. The first part introduced a basic Python
    and JavaScript dataviz toolkit, while the next four showed how to retrieve raw
    data, clean it, explore it, and finally transform it into a modern web visualization.
    This process of refinement and transformation used as its backbone a dataviz challenge:
    to take a fairly basic Wikipedia Nobel Prize list and transform the dataset contained
    into something more engaging and informative. Let’s summarize the key lessons
    of each part now.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Part I: Basic Toolkit'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Our basic toolkit consisted of:'
  prefs: []
  type: TYPE_NORMAL
- en: A language-learning bridge between Python and JavaScript. This was designed
    to smooth the transition between the two languages, highlighting their many similarities
    and setting the scene for the bilingual process of modern dataviz. Python and
    JavaScript share even more in common, making switching between them that much
    less stressful.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Being able to read from and write to the key data formats (e.g., JSON and CSV)
    and databases (both SQL and NoSQL) with ease is one of Python’s great strengths.
    We saw how easy it is to pass data around in Python, translating formats and changing
    databases as we go. This fluid movement of data is the main lubricant of any dataviz
    toolchain.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We covered the basic web development (webdev) skills needed to start producing
    modern, interactive, browser-based dataviz. By focusing on the concept of the
    [single-page application](https://oreil.ly/v0vDP) rather than building whole websites,
    we minimize conventional webdev and place the emphasis on programming your visual
    creations in JavaScript. An introduction to Scalable Vector Graphics (SVG), the
    chief building block of D3 visualizations, set the scene for the creation of our
    Nobel Prize visualization in [Part V](part05.xhtml#part_viz).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Part II: Getting Your Data'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this part of the book, we looked at how to get data from the web using Python,
    assuming a nice, clean data file hasn’t been provided to the data visualizer:'
  prefs: []
  type: TYPE_NORMAL
- en: If you’re lucky, a clean file in an easily usable data format (i.e., JSON or
    CSV) is at an open URL, a simple HTTP request away. Alternatively, there may be
    a dedicated web API for your dataset, with any luck a RESTful one. As an example,
    we looked at using the Twitter API (via Python’s Tweepy library). We also saw
    how to use Google spreadsheets, a widely used data-sharing resource in dataviz.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Things get more involved when the data of interest is present on the web in
    human-readable form, often in HTML tables, lists, or hierarchical content blocks.
    In this case, you have to resort to *scraping*, getting the raw HTML content and
    then using a parser to make its embedded content available. We saw how to use
    Python’s lightweight Beautiful Soup scraping library and the much more featureful
    and heavyweight Scrapy, the biggest star in the Python scraping firmament.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Part III: Cleaning and Exploring Data with pandas'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this part, we turned the big guns of pandas, Python’s powerful programmatic
    spreadsheet, on the problem of cleaning and then exploring datasets. We first
    saw how pandas is part of Python’s NumPy ecosystem, leveraging the power of very
    fast, powerful low-level array processing libraries but making them accessible.
    The focus was on using pandas to clean and then explore our Nobel Prize dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: Most data, even that which comes from official web APIs, is dirty. And making
    it clean and usable will occupy far more of your time as a data visualizer than
    you probably anticipated. Taking the Nobel dataset as an example, we progressively
    cleaned it, searching for dodgy dates, anomalous datatypes, missing fields, and
    all the common grime that needs cleaning before you can start to explore and then
    transform your data into a visualization.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With our clean (as we could make it) Nobel Prize dataset in hand, we saw how
    easy it is to use pandas and Matplotlib to interactively explore data, easily
    creating inline charts, slicing the data every which way, and generally getting
    a feel for it, while looking for those interesting nuggets you want to deliver
    with visualization.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Part IV: Delivering the Data'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this part, we saw how easy it is to create a minimal data API using Flask,
    to deliver data both statically and dynamically to the web browser.
  prefs: []
  type: TYPE_NORMAL
- en: First, we saw how to use Flask to serve static files and then how to roll your
    own basic data API, serving data from a local database. Flask’s minimalism allows
    you to create a very thin data-serving layer between the fruits of your Python
    data processing and their eventual visualization on the browser. The glory of
    open source software is that you can often find robust, easy-to-use libraries
    that solve your problem better than you could. In the second chapter of this part,
    we saw how easy it is to use best-of-breed Python (Flask) libraries to craft a
    robust, flexible RESTful API, ready to server your data online. We also covered
    the easy online deployment of this data server using Heroku, a favorite of Pythonistas.
  prefs: []
  type: TYPE_NORMAL
- en: 'Part V: Visualizing Your Data with D3 and Plotly'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the first chapter of this part, we saw how to take the fruits of your pandas-driven
    exploration, in the form of charts or maps, and put them on the web, where they
    belong. Matplotlib can produce publication-standard static charts, while Plotly
    brings user controls and dynamic charts to the table. We saw how to take a Plotly
    chart directly from a Jupyter notebook and put it in a web page.
  prefs: []
  type: TYPE_NORMAL
- en: I think it’s fair to say that taking on D3 was the most ambitious part of this
    book, but I was determined to demonstrate the construction of a multielement visualization,
    such as the kind you may well end up being employed to make. One of the joys of
    D3 is the [huge number of examples](https://oreil.ly/nYKx8) that can easily be
    found online, but most of them demonstrate a single technique and there are few
    showing how to orchestrate multiple visual elements. In these D3 chapters, we
    saw how to synchronize the update of a timeline (featuring all the Nobel Prizes),
    a map, a bar chart, and a list as the user filtered the Nobel Prize dataset or
    changed the prize-winning metric (absolute or per capita).
  prefs: []
  type: TYPE_NORMAL
- en: Mastery of the core themes demonstrated in these chapters should allow you to
    let loose your imagination and learn by doing. I’d recommend choosing some data
    close to your heart and designing a D3 creation around it.
  prefs: []
  type: TYPE_NORMAL
- en: Future Progress
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As mentioned, the Python and JavaScript data-processing and visualization ecosystems
    are incredibly active right now and are building from a very solid base.
  prefs: []
  type: TYPE_NORMAL
- en: While the business of acquiring and cleaning datasets learned in [Part II](part02.xhtml#part_getting_data)
    and [Chapter 9](ch09.xhtml#chapter_cleaning) improves incrementally, getting a
    lot easier as your craft skills (e.g., your pandas fu) improve, Python is throwing
    out new and powerful data-processing tools with abandon. There’s a [fairly comprehensive
    list](https://oreil.ly/ODNE1) at the Python wiki. Here are a few ideas you might
    want to use to create some visualizations.
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing Social Media Networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The advent of social media has provided a huge amount of interesting data, often
    available from a web API or eminently scrapeable. There are also curated collections
    of social media data such as [Stanford’s Large Network Dataset Collection](https://oreil.ly/2E02E)
    or the [UCIrvine collection](https://oreil.ly/x09oi). These datasets can provide
    an easy testing ground for adventures in network visualization, an increasingly
    popular area.
  prefs: []
  type: TYPE_NORMAL
- en: The two most popular Python libraries for network analysis are [graph-tool](https://graph-tool.skewed.de)
    and [NetworkX](https://networkx.org). While graph-tool is more heavily optimized,
    NetworkX is arguably more user-friendly. Both libraries produce graphs in the
    common [GraphML](http://graphml.graphdrawing.org) and [GML](https://oreil.ly/18AUU)
    formats. D3 cannot read GML files directly, but it’s easy enough to convert them
    to a JSON format it can read. You’ll find a nice example of that in this [blog
    post](https://oreil.ly/thuBE), with accompanying code on [GitHub](https://oreil.ly/3IHVR).
    Note that in D3 version 4, the forceSimulation API changed. You can find a gentle
    introduction to the new API, which uses a `forceSimulation` object to keep track
    of things, on [Pluralsight](https://oreil.ly/DZxAz).
  prefs: []
  type: TYPE_NORMAL
- en: Machine-Learning Visualizations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Machine learning is more than a little in vogue at the moment, and Python offers
    a fantastic set of tools to allow you to start analyzing and mining your data
    with a huge range of algorithms, from the supervised to unsupervised, from basic
    regression algorithms (such as linear or logistic regression) to more esoteric,
    cutting-edge stuff like the family of ensemble algorithms such as random forest.
    See this [nice tour](https://oreil.ly/IR8LZ) of the different flavors of algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Premier among Python’s machine-learning stable is [scikit-learn](https://oreil.ly/gjAKs),
    which is part of the NumPy ecosystem, also building on SciPy and Matplotlib. scikit-learn
    provides an amazing resource for efficient data mining and data analysis. Algorithms
    that only a few years back would have taken days or weeks to craft are available
    with a single import, well designed, easy to use, and able to get useful results
    in a few lines of code.
  prefs: []
  type: TYPE_NORMAL
- en: Tools like scikit-learn enable you to discover deep correlations in your data,
    if they exist. There’s a [nice demonstration](https://oreil.ly/Q0GVd) at R2D3
    that both introduces some machine-learning techniques and uses D3 to visualize
    the process and results. It’s a great example of the creative freedom that mastery
    of D3 provides and the way in which good web dataviz is pushing the boundaries,
    making novel visualizations that engage in a way that hasn’t been possible before—and,
    of course, are available to everybody.
  prefs: []
  type: TYPE_NORMAL
- en: There’s a [great collection](https://oreil.ly/wZ1bJ) of IPython (Jupyter) notebooks
    for statistics, machine learning, and data science at the IPython GitHub repo.
    Many of these demonstrate visualization techniques that can be adapted and extended
    in your own works.
  prefs: []
  type: TYPE_NORMAL
- en: Final Thoughts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The suggestions in the previous section just scratch the surface of where you
    might take your new Python and JavaScript dataviz skills. Hopefully this book
    has provided a solid bedrock on which to build your web dataviz efforts for the
    many jobs now opening up in the field or just to scratch a personal itch. The
    ability to harness Python’s immensely powerful data wrangling and general-purpose
    abilities to JavaScript’s (D3 being prominent here) increasingly powerful and
    mature visualization libraries represents the richest dataviz stack I know. Skills
    in this area are already very bankable, but the pace of change and scale of interest
    is increasing at a rapid rate. I hope you find this exciting and emergent field
    as fulfilling as I do.
  prefs: []
  type: TYPE_NORMAL
