["```py\nnewloan <- loan200[1, 2:3, drop=FALSE]\nknn_pred <- knn(train=loan200[-1, 2:3], test=newloan, cl=loan200[-1, 1], k=20)\nknn_pred == 'paid off'\n[1] TRUE\n```", "```py\npredictors = ['payment_inc_ratio', 'dti']\noutcome = 'outcome'\n\nnewloan = loan200.loc[0:0, predictors]\nX = loan200.loc[1:, predictors]\ny = loan200.loc[1:, outcome]\n\nknn = KNeighborsClassifier(n_neighbors=20)\nknn.fit(X, y)\nknn.predict(newloan)\n```", "```py\nnewloan\n  payment_inc_ratio dti revol_bal revol_util\n1            2.3932   1      1687        9.4\n```", "```py\nloan_df <- model.matrix(~ -1 + payment_inc_ratio + dti + revol_bal +\n                          revol_util, data=loan_data)\nnewloan <- loan_df[1, , drop=FALSE]\nloan_df <- loan_df[-1,]\noutcome <- loan_data[-1, 1]\nknn_pred <- knn(train=loan_df, test=newloan, cl=outcome, k=5)\nloan_df[attr(knn_pred, \"nn.index\"),]\n\n        payment_inc_ratio  dti revol_bal revol_util\n35537             1.47212 1.46      1686       10.0\n33652             3.38178 6.37      1688        8.4\n25864             2.36303 1.39      1691        3.5\n42954             1.28160 7.14      1684        3.9\n43600             4.12244 8.98      1684        7.2\n```", "```py\npredictors = ['payment_inc_ratio', 'dti', 'revol_bal', 'revol_util']\noutcome = 'outcome'\n\nnewloan = loan_data.loc[0:0, predictors]\nX = loan_data.loc[1:, predictors]\ny = loan_data.loc[1:, outcome]\n\nknn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(X, y)\n\nnbrs = knn.kneighbors(newloan)\nX.iloc[nbrs[1][0], :]\n```", "```py\nloan_df <- model.matrix(~ -1 + payment_inc_ratio + dti + revol_bal +\n                          revol_util, data=loan_data)\nloan_std <- scale(loan_df)\nnewloan_std <- loan_std[1, , drop=FALSE]\nloan_std <- loan_std[-1,]\nloan_df <- loan_df[-1,]  ![1](Images/1.png)\noutcome <- loan_data[-1, 1]\nknn_pred <- knn(train=loan_std, test=newloan_std, cl=outcome, k=5)\nloan_df[attr(knn_pred, \"nn.index\"),]\n        payment_inc_ratio   dti  revol_bal  revol_util\n2081            2.61091    1.03       1218         9.7\n1439            2.34343    0.51        278         9.9\n30216           2.71200    1.34       1075         8.5\n28543           2.39760    0.74       2917         7.4\n44738           2.34309    1.37        488         7.2\n```", "```py\nnewloan = loan_data.loc[0:0, predictors]\nX = loan_data.loc[1:, predictors]\ny = loan_data.loc[1:, outcome]\n\nscaler = preprocessing.StandardScaler()\nscaler.fit(X * 1.0)\n\nX_std = scaler.transform(X * 1.0)\nnewloan_std = scaler.transform(newloan * 1.0)\n\nknn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(X_std, y)\n\nnbrs = knn.kneighbors(newloan_std)\nX.iloc[nbrs[1][0], :]\n```", "```py\nborrow_df <- model.matrix(~ -1 + dti + revol_bal + revol_util + open_acc +\n                            delinq_2yrs_zero + pub_rec_zero, data=loan_data)\nborrow_knn <- knn(borrow_df, test=borrow_df, cl=loan_data[, 'outcome'],\n                  prob=TRUE, k=20)\nprob <- attr(borrow_knn, \"prob\")\nborrow_feature <- ifelse(borrow_knn == 'default', prob, 1 - prob)\nsummary(borrow_feature)\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.\n  0.000   0.400   0.500   0.501   0.600   0.950\n```", "```py\npredictors = ['dti', 'revol_bal', 'revol_util', 'open_acc',\n              'delinq_2yrs_zero', 'pub_rec_zero']\noutcome = 'outcome'\n\nX = loan_data[predictors]\ny = loan_data[outcome]\n\nknn = KNeighborsClassifier(n_neighbors=20)\nknn.fit(X, y)\n\nloan_data['borrower_score'] = knn.predict_proba(X)[:, 1]\nloan_data['borrower_score'].describe()\n```", "```py\nlibrary(rpart)\nloan_tree <- rpart(outcome ~ borrower_score + payment_inc_ratio,\n                   data=loan3000, control=rpart.control(cp=0.005))\nplot(loan_tree, uniform=TRUE, margin=0.05)\ntext(loan_tree)\n```", "```py\npredictors = ['borrower_score', 'payment_inc_ratio']\noutcome = 'outcome'\n\nX = loan3000[predictors]\ny = loan3000[outcome]\n\nloan_tree = DecisionTreeClassifier(random_state=1, criterion='entropy',\n                                   min_impurity_decrease=0.003)\nloan_tree.fit(X, y)\nplotDecisionTree(loan_tree, feature_names=predictors,\n                 class_names=loan_tree.classes_)\n```", "```py\nloan_tree\nn= 3000\n\nnode), split, n, loss, yval, (yprob)\n    * denotes terminal node\n\n1) root 3000 1445 paid off (0.5183333 0.4816667)\n  2) borrower_score>=0.575 878  261 paid off (0.7027335 0.2972665) *\n  3) borrower_score< 0.575 2122  938 default (0.4420358 0.5579642)\n    6) borrower_score>=0.375 1639  802 default (0.4893228 0.5106772)\n      12) payment_inc_ratio< 10.42265 1157  547 paid off (0.5272256 0.4727744)\n        24) payment_inc_ratio< 4.42601 334  139 paid off (0.5838323 0.4161677) *\n        25) payment_inc_ratio>=4.42601 823  408 paid off (0.5042527 0.4957473)\n          50) borrower_score>=0.475 418  190 paid off (0.5454545 0.4545455) *\n          51) borrower_score< 0.475 405  187 default (0.4617284 0.5382716) *\n      13) payment_inc_ratio>=10.42265 482  192 default (0.3983402 0.6016598) *\n    7) borrower_score< 0.375 483  136 default (0.2815735 0.7184265) *\n```", "```py\nprint(textDecisionTree(loan_tree))\n--\nnode=0 test node: go to node 1 if 0 <= 0.5750000178813934 else to node 6\n  node=1 test node: go to node 2 if 0 <= 0.32500000298023224 else to node 3\n    node=2 leaf node: [[0.785, 0.215]]\n    node=3 test node: go to node 4 if 1 <= 10.42264986038208 else to node 5\n      node=4 leaf node: [[0.488, 0.512]]\n      node=5 leaf node: [[0.613, 0.387]]\n  node=6 test node: go to node 7 if 1 <= 9.19082498550415 else to node 10\n    node=7 test node: go to node 8 if 0 <= 0.7249999940395355 else to node 9\n      node=8 leaf node: [[0.247, 0.753]]\n      node=9 leaf node: [[0.073, 0.927]]\n    node=10 leaf node: [[0.457, 0.543]]\n```", "```py\nrf <- randomForest(outcome ~ borrower_score + payment_inc_ratio,\n                   data=loan3000)\nrf\n\nCall:\n randomForest(formula = outcome ~ borrower_score + payment_inc_ratio,\n     data = loan3000)\n           \tType of random forest: classification\n                     Number of trees: 500\nNo. of variables tried at each split: 1\n\n    \tOOB estimate of error rate: 39.17%\nConfusion matrix:\n        default  paid off  class.error\ndefault     873       572   0.39584775\npaid off    603       952   0.38778135\n```", "```py\npredictors = ['borrower_score', 'payment_inc_ratio']\noutcome = 'outcome'\n\nX = loan3000[predictors]\ny = loan3000[outcome]\n\nrf = RandomForestClassifier(n_estimators=500, random_state=1, oob_score=True)\nrf.fit(X, y)\n```", "```py\nerror_df = data.frame(error_rate=rf$err.rate[,'OOB'],\n                      num_trees=1:rf$ntree)\nggplot(error_df, aes(x=num_trees, y=error_rate)) +\n  geom_line()\n```", "```py\nn_estimator = list(range(20, 510, 5))\noobScores = []\nfor n in n_estimator:\n    rf = RandomForestClassifier(n_estimators=n, criterion='entropy',\n                                max_depth=5, random_state=1, oob_score=True)\n    rf.fit(X, y)\n    oobScores.append(rf.oob_score_)\ndf = pd.DataFrame({ 'n': n_estimator, 'oobScore': oobScores })\ndf.plot(x='n', y='oobScore')\n```", "```py\npred <- predict(rf, prob=TRUE)\nrf_df <- cbind(loan3000, pred = pred)\nggplot(data=rf_df, aes(x=borrower_score, y=payment_inc_ratio,\n                       shape=pred, color=pred, size=pred)) +\n    geom_point(alpha=.8) +\n    scale_color_manual(values = c('paid off'='#b8e186', 'default'='#d95f02')) +\n    scale_shape_manual(values = c('paid off'=0, 'default'=1)) +\n    scale_size_manual(values = c('paid off'=0.5, 'default'=2))\n```", "```py\npredictions = X.copy()\npredictions['prediction'] = rf.predict(X)\npredictions.head()\n\nfig, ax = plt.subplots(figsize=(4, 4))\n\npredictions.loc[predictions.prediction=='paid off'].plot(\n    x='borrower_score', y='payment_inc_ratio', style='.',\n    markerfacecolor='none', markeredgecolor='C1', ax=ax)\npredictions.loc[predictions.prediction=='default'].plot(\n    x='borrower_score', y='payment_inc_ratio', style='o',\n    markerfacecolor='none', markeredgecolor='C0', ax=ax)\nax.legend(['paid off', 'default']);\nax.set_xlim(0, 1)\nax.set_ylim(0, 25)\nax.set_xlabel('borrower_score')\nax.set_ylabel('payment_inc_ratio')\n```", "```py\nrf_all <- randomForest(outcome ~ ., data=loan_data, importance=TRUE)\nrf_all\nCall:\n randomForest(formula = outcome ~ ., data = loan_data, importance = TRUE)\n               Type of random forest: classification\n                     Number of trees: 500\nNo. of variables tried at each split: 4\n\n        OOB estimate of  error rate: 33.79%\n\nConfusion matrix:\n         paid off default class.error\npaid off    14676    7995   0.3526532\ndefault      7325   15346   0.3231000\n```", "```py\npredictors = ['loan_amnt', 'term', 'annual_inc', 'dti', 'payment_inc_ratio',\n              'revol_bal', 'revol_util', 'purpose', 'delinq_2yrs_zero',\n              'pub_rec_zero', 'open_acc', 'grade', 'emp_length', 'purpose_',\n              'home_', 'emp_len_', 'borrower_score']\noutcome = 'outcome'\n\nX = pd.get_dummies(loan_data[predictors], drop_first=True)\ny = loan_data[outcome]\n\nrf_all = RandomForestClassifier(n_estimators=500, random_state=1)\nrf_all.fit(X, y)\n```", "```py\nvarImpPlot(rf_all, type=1) ![1](Images/1.png)\nvarImpPlot(rf_all, type=2) ![2](Images/2.png)\n```", "```py\nimportances = rf_all.feature_importances_\n```", "```py\nrf = RandomForestClassifier(n_estimators=500)\nscores = defaultdict(list)\n\n# cross-validate the scores on a number of different random splits of the data\nfor _ in range(3):\n    train_X, valid_X, train_y, valid_y = train_test_split(X, y, test_size=0.3)\n    rf.fit(train_X, train_y)\n    acc = metrics.accuracy_score(valid_y, rf.predict(valid_X))\n    for column in X.columns:\n        X_t = valid_X.copy()\n        X_t[column] = np.random.permutation(X_t[column].values)\n        shuff_acc = metrics.accuracy_score(valid_y, rf.predict(X_t))\n        scores[column].append((acc-shuff_acc)/acc)\n```", "```py\ndf = pd.DataFrame({\n    'feature': X.columns,\n    'Accuracy decrease': [np.mean(scores[column]) for column in X.columns],\n    'Gini decrease': rf_all.feature_importances_,\n})\ndf = df.sort_values('Accuracy decrease')\n\nfig, axes = plt.subplots(ncols=2, figsize=(8, 4.5))\nax = df.plot(kind='barh', x='feature', y='Accuracy decrease',\n             legend=False, ax=axes[0])\nax.set_ylabel('')\n\nax = df.plot(kind='barh', x='feature', y='Gini decrease',\n             legend=False, ax=axes[1])\nax.set_ylabel('')\nax.get_yaxis().set_visible(False)\n```", "```py\npredictors <- data.matrix(loan3000[, c('borrower_score', 'payment_inc_ratio')])\nlabel <- as.numeric(loan3000[,'outcome']) - 1\nxgb <- xgboost(data=predictors, label=label, objective=\"binary:logistic\",\n               params=list(subsample=0.63, eta=0.1), nrounds=100)\n[1]\ttrain-error:0.358333\n[2]\ttrain-error:0.346333\n[3]\ttrain-error:0.347333\n...\n[99]\ttrain-error:0.239333\n[100]\ttrain-error:0.241000\n```", "```py\npredictors = ['borrower_score', 'payment_inc_ratio']\noutcome = 'outcome'\n\nX = loan3000[predictors]\ny = loan3000[outcome]\n\nxgb = XGBClassifier(objective='binary:logistic', subsample=0.63)\nxgb.fit(X, y)\n--\nXGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n       colsample_bynode=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n       n_estimators=100, n_jobs=1, nthread=None, objective='binary:logistic',\n       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n       silent=None, subsample=0.63, verbosity=1)\n```", "```py\npred <- predict(xgb, newdata=predictors)\nxgb_df <- cbind(loan3000, pred_default = pred > 0.5, prob_default = pred)\nggplot(data=xgb_df, aes(x=borrower_score, y=payment_inc_ratio,\n                  color=pred_default, shape=pred_default, size=pred_default)) +\n         geom_point(alpha=.8) +\n         scale_color_manual(values = c('FALSE'='#b8e186', 'TRUE'='#d95f02')) +\n         scale_shape_manual(values = c('FALSE'=0, 'TRUE'=1)) +\n         scale_size_manual(values = c('FALSE'=0.5, 'TRUE'=2))\n```", "```py\nfig, ax = plt.subplots(figsize=(6, 4))\n\nxgb_df.loc[xgb_df.prediction=='paid off'].plot(\n    x='borrower_score', y='payment_inc_ratio', style='.',\n    markerfacecolor='none', markeredgecolor='C1', ax=ax)\nxgb_df.loc[xgb_df.prediction=='default'].plot(\n    x='borrower_score', y='payment_inc_ratio', style='o',\n    markerfacecolor='none', markeredgecolor='C0', ax=ax)\nax.legend(['paid off', 'default']);\nax.set_xlim(0, 1)\nax.set_ylim(0, 25)\nax.set_xlabel('borrower_score')\nax.set_ylabel('payment_inc_ratio')\n```", "```py\nseed <- 400820\npredictors <- data.matrix(loan_data[, -which(names(loan_data) %in%\n                                       'outcome')])\nlabel <- as.numeric(loan_data$outcome) - 1\ntest_idx <- sample(nrow(loan_data), 10000)\n\nxgb_default <- xgboost(data=predictors[-test_idx,], label=label[-test_idx],\n                       objective='binary:logistic', nrounds=250, verbose=0)\npred_default <- predict(xgb_default, predictors[test_idx,])\nerror_default <- abs(label[test_idx] - pred_default) > 0.5\nxgb_default$evaluation_log[250,]\nmean(error_default)\n-\niter train_error\n1:  250    0.133043\n\n[1] 0.3529\n```", "```py\npredictors = ['loan_amnt', 'term', 'annual_inc', 'dti', 'payment_inc_ratio',\n              'revol_bal', 'revol_util', 'purpose', 'delinq_2yrs_zero',\n              'pub_rec_zero', 'open_acc', 'grade', 'emp_length', 'purpose_',\n              'home_', 'emp_len_', 'borrower_score']\noutcome = 'outcome'\n\nX = pd.get_dummies(loan_data[predictors], drop_first=True)\ny = pd.Series([1 if o == 'default' else 0 for o in loan_data[outcome]])\n\ntrain_X, valid_X, train_y, valid_y = train_test_split(X, y, test_size=10000)\n\nxgb_default = XGBClassifier(objective='binary:logistic', n_estimators=250,\n                            max_depth=6, reg_lambda=0, learning_rate=0.3,\n                            subsample=1)\nxgb_default.fit(train_X, train_y)\n\npred_default = xgb_default.predict_proba(valid_X)[:, 1]\nerror_default = abs(valid_y - pred_default) > 0.5\nprint('default: ', np.mean(error_default))\n```", "```py\nxgb_penalty <- xgboost(data=predictors[-test_idx,], label=label[-test_idx],\n                       params=list(eta=.1, subsample=.63, lambda=1000),\n                       objective='binary:logistic', nrounds=250, verbose=0)\npred_penalty <- predict(xgb_penalty, predictors[test_idx,])\nerror_penalty <- abs(label[test_idx] - pred_penalty) > 0.5\nxgb_penalty$evaluation_log[250,]\nmean(error_penalty)\n-\niter train_error\n1:  250     0.30966\n\n[1] 0.3286\n```", "```py\nxgb_penalty = XGBClassifier(objective='binary:logistic', n_estimators=250,\n                            max_depth=6, reg_lambda=1000, learning_rate=0.1,\n                            subsample=0.63)\nxgb_penalty.fit(train_X, train_y)\npred_penalty = xgb_penalty.predict_proba(valid_X)[:, 1]\nerror_penalty = abs(valid_y - pred_penalty) > 0.5\nprint('penalty: ', np.mean(error_penalty))\n```", "```py\nerror_default <- rep(0, 250)\nerror_penalty <- rep(0, 250)\nfor(i in 1:250){\n  pred_def <- predict(xgb_default, predictors[test_idx,], ntreelimit=i)\n  error_default[i] <- mean(abs(label[test_idx] - pred_def) >= 0.5)\n  pred_pen <- predict(xgb_penalty, predictors[test_idx,], ntreelimit=i)\n  error_penalty[i] <- mean(abs(label[test_idx] - pred_pen) >= 0.5)\n}\n```", "```py\nresults = []\nfor i in range(1, 250):\n    train_default = xgb_default.predict_proba(train_X, ntree_limit=i)[:, 1]\n    train_penalty = xgb_penalty.predict_proba(train_X, ntree_limit=i)[:, 1]\n    pred_default = xgb_default.predict_proba(valid_X, ntree_limit=i)[:, 1]\n    pred_penalty = xgb_penalty.predict_proba(valid_X, ntree_limit=i)[:, 1]\n    results.append({\n        'iterations': i,\n        'default train': np.mean(abs(train_y - train_default) > 0.5),\n        'penalty train': np.mean(abs(train_y - train_penalty) > 0.5),\n        'default test': np.mean(abs(valid_y - pred_default) > 0.5),\n        'penalty test': np.mean(abs(valid_y - pred_penalty) > 0.5),\n    })\n\nresults = pd.DataFrame(results)\nresults.head()\n```", "```py\nerrors <- rbind(xgb_default$evaluation_log,\n                xgb_penalty$evaluation_log,\n                ata.frame(iter=1:250, train_error=error_default),\n                data.frame(iter=1:250, train_error=error_penalty))\nerrors$type <- rep(c('default train', 'penalty train',\n                     'default test', 'penalty test'), rep(250, 4))\nggplot(errors, aes(x=iter, y=train_error, group=type)) +\n  geom_line(aes(linetype=type, color=type))\n```", "```py\nax = results.plot(x='iterations', y='default test')\nresults.plot(x='iterations', y='penalty test', ax=ax)\nresults.plot(x='iterations', y='default train', ax=ax)\nresults.plot(x='iterations', y='penalty train', ax=ax)\n```", "```py\nN <- nrow(loan_data)\nfold_number <- sample(1:5, N, replace=TRUE)\nparams <- data.frame(eta = rep(c(.1, .5, .9), 3),\n                     max_depth = rep(c(3, 6, 12), rep(3,3)))\n```", "```py\nerror <- matrix(0, nrow=9, ncol=5)\nfor(i in 1:nrow(params)){\n  for(k in 1:5){\n    fold_idx <- (1:N)[fold_number == k]\n    xgb <- xgboost(data=predictors[-fold_idx,], label=label[-fold_idx],\n                   params=list(eta=params[i, 'eta'],\n                               max_depth=params[i, 'max_depth']),\n                   objective='binary:logistic', nrounds=100, verbose=0)\n    pred <- predict(xgb, predictors[fold_idx,])\n    error[i, k] <- mean(abs(label[fold_idx] - pred) >= 0.5)\n  }\n}\n```", "```py\nidx = np.random.choice(range(5), size=len(X), replace=True)\nerror = []\nfor eta, max_depth in product([0.1, 0.5, 0.9], [3, 6, 9]):  ![1](Images/1.png)\n    xgb = XGBClassifier(objective='binary:logistic', n_estimators=250,\n                        max_depth=max_depth, learning_rate=eta)\n    cv_error = []\n    for k in range(5):\n        fold_idx = idx == k\n        train_X = X.loc[~fold_idx]; train_y = y[~fold_idx]\n        valid_X = X.loc[fold_idx]; valid_y = y[fold_idx]\n\n        xgb.fit(train_X, train_y)\n        pred = xgb.predict_proba(valid_X)[:, 1]\n        cv_error.append(np.mean(abs(valid_y - pred) > 0.5))\n    error.append({\n        'eta': eta,\n        'max_depth': max_depth,\n        'avg_error': np.mean(cv_error)\n    })\n    print(error[-1])\nerrors = pd.DataFrame(error)\n```", "```py\navg_error <- 100 * round(rowMeans(error), 4)\ncbind(params, avg_error)\n  eta max_depth avg_error\n1 0.1         3     32.90\n2 0.5         3     33.43\n3 0.9         3     34.36\n4 0.1         6     33.08\n5 0.5         6     35.60\n6 0.9         6     37.82\n7 0.1        12     34.56\n8 0.5        12     36.83\n9 0.9        12     38.18\n```"]