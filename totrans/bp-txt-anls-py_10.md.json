["```py\nimport os\nos.environ['GENSIM_DATA_DIR'] = './models'\n\n```", "```py\nimport gensim.downloader as api\n\ninfo_df = pd.DataFrame.from_dict(api.info()['models'], orient='index')\ninfo_df[['file_size', 'base_dataset', 'parameters']].head(5)\n\n```", "```py\nmodel = api.load(\"glove-wiki-gigaword-50\")\n\n```", "```py\nv_king = model['king']\nv_queen = model['queen']\n\nprint(\"Vector size:\", model.vector_size)\nprint(\"v_king  =\", v_king[:10])\nprint(\"v_queen =\", v_queen[:10])\nprint(\"similarity:\", model.similarity('king', 'queen'))\n\n```", "```py\nVector size: 50\nv_king  = [ 0.5   0.69 -0.6  -0.02  0.6  -0.13 -0.09  0.47 -0.62 -0.31]\nv_queen = [ 0.38  1.82 -1.26 -0.1   0.36  0.6  -0.18  0.84 -0.06 -0.76]\nsimilarity: 0.7839043\n\n```", "```py\nmodel.most_similar('king', topn=3)\n\n```", "```py\n[('prince', 0.824), ('queen', 0.784), ('ii', 0.775)]\n\n```", "```py\nv_lion = model['lion']\nv_nano = model['nanotechnology']\n\nmodel.cosine_similarities(v_king, [v_queen, v_lion, v_nano])\n\n```", "```py\narray([ 0.784,  0.478, -0.255], dtype=float32)\n\n```", "```py\nmodel.most_similar(positive=['woman', 'king'], negative=['man'], topn=3)\n\n```", "```py\n[('queen', 0.852), ('throne', 0.766), ('prince', 0.759)]\n\n```", "```py\nmodel.most_similar(positive=['paris', 'germany'], negative=['france'], topn=3)\n\n```", "```py\n[('berlin', 0.920), ('frankfurt', 0.820), ('vienna', 0.818)]\n\n```", "```py\nmodel.most_similar(positive=['france', 'capital'], topn=1)\n\n```", "```py\n[('paris', 0.784)]\n\n```", "```py\nmodel.most_similar(positive=['greece', 'capital'], topn=3)\n\n```", "```py\n[('central', 0.797), ('western', 0.757), ('region', 0.750)]\n\n```", "```py\ndb_name = \"reddit-selfposts.db\"\ncon = sqlite3.connect(db_name)\ndf = pd.read_sql(\"select subreddit, lemmas, text from posts_nlp\", con)\ncon.close()\n\ndf['lemmas'] = df['lemmas'].str.lower().str.split() # lower case tokens\nsents = df['lemmas'] # our training \"sentences\"\n\n```", "```py\nfrom gensim.models.phrases import Phrases, npmi_scorer\n\nphrases = Phrases(sents, min_count=10, threshold=0.3,\n                  delimiter=b'-', scoring=npmi_scorer)\n\n```", "```py\nsent = \"I had to replace the timing belt in my mercedes c300\".split()\nphrased = phrases[sent]\nprint('|'.join(phrased))\n\n```", "```py\nI|had|to|replace|the|timing-belt|in|my|mercedes-c300\n\n```", "```py\nphrase_df = pd.DataFrame(phrases.export_phrases(sents),\n                         columns =['phrase', 'score'])\nphrase_df = phrase_df[['phrase', 'score']].drop_duplicates() \\\n            .sort_values(by='score', ascending=False).reset_index(drop=True)\nphrase_df['phrase'] = phrase_df['phrase'].map(lambda p: p.decode('utf-8'))\n\n```", "```py\nphrase_df[phrase_df['phrase'].str.contains('mercedes')]\n\n```", "```py\nphrases = Phrases(sents, min_count=10, threshold=0.7,\n                  delimiter=b'-', scoring=npmi_scorer)\n\ndf['phrased_lemmas'] = df['lemmas'].map(lambda s: phrases[s])\nsents = df['phrased_lemmas']\n\n```", "```py\nfrom gensim.models import Word2Vec\n\nmodel = Word2Vec(sents,       # tokenized input sentences\n                 size=100,    # size of word vectors (default 100)\n                 window=2,    # context window size (default 5)\n                 sg=1,        # use skip-gram (default 0 = CBOW)\n                 negative=5,  # number of negative samples (default 5)\n                 min_count=5, # ignore infrequent words (default 5)\n                 workers=4,   # number of threads (default 3)\n                 iter=5)      # number of epochs (default 5)\n\n```", "```py\nmodel.save('./models/autos_w2v_100_2_full.bin')\n\n```", "```py\nfrom gensim.models import Word2Vec, FastText\n\nmodel_path = './models'\nmodel_prefix = 'autos'\n\nparam_grid = {'w2v': {'variant': ['cbow', 'sg'], 'window': [2, 5, 30]},\n              'ft': {'variant': ['sg'], 'window': [5]}}\nsize = 100\n\nfor algo, params in param_grid.items():\n    for variant in params['variant']:\n        sg = 1 if variant == 'sg' else 0\n        for window in params['window']:\n            if algo == 'w2v':\n                model = Word2Vec(sents, size=size, window=window, sg=sg)\n            else:\n                model = FastText(sents, size=size, window=window, sg=sg)\n\n            file_name = f\"{model_path}/{model_prefix}_{algo}_{variant}_{window}\"\n            model.wv.save_word2vec_format(file_name + '.bin', binary=True)\n\n```", "```py\nfrom gensim.models import KeyedVectors\n\nnames = ['autos_w2v_cbow_2', 'autos_w2v_sg_2',\n         'autos_w2v_sg_5', 'autos_w2v_sg_30', 'autos_ft_sg_5']\nmodels = {}\n\nfor name in names:\n    file_name = f\"{model_path}/{name}.bin\"\n    models[name] = KeyedVectors.load_word2vec_format(file_name, binary=True)\n\n```", "```py\ndef compare_models(models, **kwargs):\n\n    df = pd.DataFrame()\n    for name, model in models:\n        df[name] = [f\"{word} {score:.3f}\"\n                    for word, score in model.most_similar(**kwargs)]\n    df.index = df.index + 1 # let row index start at 1\n    return df\n\n```", "```py\ncompare_models([(n, models[n]) for n in names], positive='bmw', topn=10)\n\n```", "```py\ncompare_models([(n, models[n]) for n in names],\n               positive=['f150', 'toyota'], negative=['ford'], topn=5).T\n\n```", "```py\nfrom umap import UMAP\n\nmodel = models['autos_w2v_sg_30']\nwords = model.vocab\nwv = [model[word] for word in words]\n\nreducer = UMAP(n_components=2, metric='cosine', n_neighbors = 15, min_dist=0.1)\nreduced_wv = reducer.fit_transform(wv)\n\n```", "```py\nimport plotly.express as px\n\nplot_df = pd.DataFrame.from_records(reduced_wv, columns=['x', 'y'])\nplot_df['word'] = words\nparams = {'hover_data': {c: False for c in plot_df.columns},\n          'hover_name': 'word'}\n\nfig = px.scatter(plot_df, x=\"x\", y=\"y\", opacity=0.3, size_max=3, **params)\nfig.show()\n\n```", "```py\nfrom blueprints.embeddings import plot_embeddings\n\nsearch = ['ford', 'lexus', 'vw', 'hyundai',\n          'goodyear', 'spark-plug', 'florida', 'navigation']\n\nplot_embeddings(model, search, topn=50, show_all=True, labels=False,\n                algo='umap', n_neighbors=15, min_dist=0.1)\n\n```", "```py\nsearch = ['ford', 'bmw', 'toyota', 'tesla', 'audi', 'mercedes', 'hyundai']\n\nplot_embeddings(model, search, topn=10, show_all=False, labels=True,\n    algo='umap', n_neighbors=15, min_dist=10, spread=25)\n\n```", "```py\nplot_embeddings(model, search, topn=30, n_dims=3,\n    algo='umap', n_neighbors=15, min_dist=.1, spread=40)\n\n```", "```py\nimport csv\n\nname = 'autos_w2v_sg_30'\nmodel = models[name]\n\nwith open(f'{model_path}/{name}_words.tsv', 'w', encoding='utf-8') as tsvfile:\n    tsvfile.write('\\n'.join(model.vocab))\n\nwith open(f'{model_path}/{name}_vecs.tsv', 'w', encoding='utf-8') as tsvfile:\n    writer = csv.writer(tsvfile, delimiter='\\t',\n                        dialect=csv.unix_dialect, quoting=csv.QUOTE_MINIMAL)\n    for w in model.vocab:\n        _ = writer.writerow(model[w].tolist())\n\n```", "```py\nimport networkx as nx\nfrom collections import deque\n\ndef sim_tree(model, word, top_n, max_dist):\n\n    graph = nx.Graph()\n    graph.add_node(word, dist=0)\n\n    to_visit = deque([word])\n    while len(to_visit) > 0:\n        source = to_visit.popleft() # visit next node\n        dist = graph.nodes[source]['dist']+1\n\n        if dist <= max_dist: # discover new nodes\n            for target, sim in model.most_similar(source, topn=top_n):\n                if target not in graph:\n                    to_visit.append(target)\n                    graph.add_node(target, dist=dist)\n                    graph.add_edge(source, target, sim=sim, dist=dist)\n    return graph\n\n```", "```py\nfrom networkx.drawing.nx_pydot import graphviz_layout\n\ndef plot_tree(graph, node_size=1000, font_size=12):\n\n    pos = graphviz_layout(graph, prog='twopi', root=list(graph.nodes)[0])\n\n    colors = [graph.nodes[n]['dist'] for n in graph] # colorize by distance\n    nx.draw_networkx_nodes(graph, pos, node_size=node_size, node_color=colors,\n                           cmap='Set1', alpha=0.4)\n    nx.draw_networkx_labels(graph, pos, font_size=font_size)\n\n    for (n1, n2, sim) in graph.edges(data='sim'):\n         nx.draw_networkx_edges(graph, pos, [(n1, n2)], width=sim, alpha=0.2)\n\n    plt.show()\n\n```", "```py\nmodel = models['autos_w2v_sg_2']\ngraph = sim_tree(model, 'noise', top_n=10, max_dist=3)\nplot_tree(graph, node_size=500, font_size=8)\n\n```", "```py\nmodel = models['autos_w2v_sg_30']\ngraph = sim_tree(model, 'spark-plug', top_n=8, max_dist=2)\nplot_tree(graph, node_size=500, font_size=8)\n\n```"]