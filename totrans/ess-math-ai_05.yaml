- en: Chapter 5\. Convolutional Neural Networks and Computer Vision
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*They. Can. See.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: H.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Convolutional neural networks have revolutionized the computer vision and the
    natural language processing fields. Application areas, irrespective of the ethical
    questions associated with them, are limitless: Self-driving cars, smart drones,
    facial recognition, speech recognition, medical imaging, generating audio, generating
    images, robotics, *etc.*'
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we start with the simple definitions and interpretations of
    *convolution* and *cross-correlation*, and highlight the fact that these two slightly
    different mathematical operations are conflated in machine learning terminology.
    We perpetrate the same sin and conflate them as well, but with a good reason.
  prefs: []
  type: TYPE_NORMAL
- en: We then apply the convolution operation to filtering grid-like signals, which
    it is perfectly suited for, such as time series data (one-dimensional), audio
    data (one-dimensional), and images (two-dimensional if the images are greyscale,
    and three-dimensional if they are color images, with the extra dimension corresponding
    to the red, green, and blue channels). When data is one dimensional, we use one
    dimensional convolutions, and when it is two dimensional, we use two dimensional
    convolutions (for the sake of simplicity and conciseness, we will not do three
    dimensional convolutions in this chapter, corresponding to three dimensional color
    images, called *tensors*). In other words, we adapt our network to the shape of
    the data, a process that has wildly contributed to the success of convolutional
    neural networks. This is in contrast to forcing the data to adapt to the shape
    of the network’s input, such as flattening a two dimensional image into one long
    vector to make it fit a network that only take one dimensional data as its input.
    In later chapters, we see that the same applies for the success of graph neural
    networks for graph-type data.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we incorporate convolution into the architecture of a feed forward neural
    network. The convolution operation has the effect of making the network *locally
    connected* as opposed to fully connected. Mathematically, the matrix containing
    the weights for each layer is not *dense* (so most of the weights for convolutional
    layers are zero). Moreover, the weights have similar values (*weight sharing*),
    unlike the case of fully connected neural networks, where a different weight is
    assigned to each input. Therefore, the matrix containing the weights of a convolutional
    layer is mostly zeros, and the nonzero parts are *localized and share similar
    values*. For image data or audio data, this is great, since most of the information
    is contained locally. Moreover, this dramatically reduces the number of weights
    that we need to store and compute during the optimization step, rendering convolutional
    neural networks ideal for data with a massive amount of input features (recall
    that for images, each pixel is a feature).
  prefs: []
  type: TYPE_NORMAL
- en: We then discuss *pooling*, which is another layer that is common to the architecture
    of convolutional neural networks. As in the previous chapter, both the multi-layer
    structure and the nonlinearity at each layer enable us to extract increasingly
    complex features from images, significantly enhancing computer vision tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Once we comprehend the basic anatomy of a convolutional neural network, it is
    straight forward to apply the same mathematics to tasks involving natural language
    processing, such as sentiment analysis, speech recognition, audio generation,
    and others. The fact that the same mathematics works for both computer vision
    and natural language processing is akin to the impressive ability of our brain
    to physically change in response to circumstances, experiences, and thoughts (the
    brain’s version of virtual simulations). Even when some portions of the brain
    are damaged, other parts can take over and perform new functions. For example,
    the parts of the brain devoted to sight can start performing hearing or remembering
    tasks, when those are impaired. In neuroscience, this is called neuroplasticity.
    We are very far from having a complete understanding of the brain and how it works,
    but the simplest explanation to this phenomenon is that each neuron in the brain
    performs the same basic function, similar to how neurons in a neural network perform
    one basic mathematical calculation (actually, two, linearly combine then activate),
    and it is the various neural connections over multiple layers that produce the
    observed complexity in perception and behavior. Convolutional neural networks
    are in fact inspired by the brain’s visual neocortex. It is their success in 2012
    for image classification ([AlexNet2012](https://en.wikipedia.org/wiki/AlexNet))
    that propelled AI back into main stream, inspiring many and bringing us here.
    If you happen to have some extra time, a good bedtime read accompanying this chapter
    would be about the function of the brain’s visual neocortex, and its analogy to
    convolutional neural networks designed for computer vision.
  prefs: []
  type: TYPE_NORMAL
- en: Convolution And Cross-Correlation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Convolution and cross-correlation are slightly different operations and measure
    different things in a signal, which can be a digital image, a digital audio signal,
    or others. They are exactly the same if we use a symmetric function k, called
    a *filter* or a *kernel*. Briefly speaking, convolution *flips* the filter then
    slides it across the function, while cross correlation slides the filter across
    the function *without flipping* it. Naturally, if the filter happens to be symmetric,
    then convolution and cross-correlation are exactly the same. Flipping the kernel
    has the advantage of making the convolution operation commutative, which in turn
    is good for writing theoretical proofs. That said, from a neural networks perspective,
    commutativity is not important, for three reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: First, the convolution operation does not usually appear alone in a neural network,
    but composed with other nonlinear functions, so we lose commutativity irrespective
    of whether we flip the kernel or not.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Second, a neural network usually *learns* the values of the entries in the kernel
    during the training process, thus, it learns the correct values in the correct
    locations, and flipping becomes immaterial.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Third, and this important for practical implementation, convolutional networks
    usually use *multichannel convolution*, for example, the input can be a color
    image with red-green-blue channels, or even a video, with red-green-blue space
    channels and one time channel. Furthermore, they use batch mode convolution, meaning
    they take input vectors, images, videos, or other data types, in batches and apply
    parallel convolution operations simultaneously. Even with kernel flipping, these
    operations are not guaranteed to be commutative, unless each operation has the
    same number of output channels as input channels. This is usually not the case,
    since the outputs of multiple channels are usually summed together as a whole
    or partially, producing different number of output channels than input channels.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For all these reasons, many machine learning libraries do not flip the kernel
    when implementing convolution, in essence implementing cross-correlation and calling
    it convolution. We do the same here.
  prefs: []
  type: TYPE_NORMAL
- en: 'The convolution operation between two real valued functions k (the filter)
    and f is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign StartLayout 1st Row 1st Column left-parenthesis k
    star f right-parenthesis left-parenthesis t right-parenthesis 2nd Column equals
    integral Subscript negative normal infinity Superscript normal infinity Baseline
    f left-parenthesis s right-parenthesis k left-parenthesis negative s plus t right-parenthesis
    d s 2nd Row 1st Column Blank 2nd Column equals integral Subscript negative normal
    infinity Superscript normal infinity Baseline f left-parenthesis negative s plus
    t right-parenthesis k left-parenthesis s right-parenthesis d s comma EndLayout
    dollar-sign"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mo>(</mo>
    <mi>k</mi> <mo>☆</mo> <mi>f</mi> <mo>)</mo> <mo>(</mo> <mi>t</mi> <mo>)</mo></mrow></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <msubsup><mo>∫</mo> <mrow><mo>-</mo><mi>∞</mi></mrow>
    <mi>∞</mi></msubsup> <mi>f</mi> <mrow><mo>(</mo> <mi>s</mi> <mo>)</mo></mrow>
    <mi>k</mi> <mrow><mo>(</mo> <mo>-</mo> <mi>s</mi> <mo>+</mo> <mi>t</mi> <mo>)</mo></mrow>
    <mi>d</mi> <mi>s</mi></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mrow><mo>=</mo>
    <msubsup><mo>∫</mo> <mrow><mo>-</mo><mi>∞</mi></mrow> <mi>∞</mi></msubsup> <mi>f</mi>
    <mrow><mo>(</mo> <mo>-</mo> <mi>s</mi> <mo>+</mo> <mi>t</mi> <mo>)</mo></mrow>
    <mi>k</mi> <mrow><mo>(</mo> <mi>s</mi> <mo>)</mo></mrow> <mi>d</mi> <mi>s</mi>
    <mo>,</mo></mrow></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'and the discrete analogue for discrete functions is:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign StartLayout 1st Row 1st Column left-parenthesis k
    star f right-parenthesis left-parenthesis n right-parenthesis 2nd Column equals
    sigma-summation Underscript s equals negative normal infinity Overscript normal
    infinity Endscripts f left-parenthesis s right-parenthesis k left-parenthesis
    negative s plus n right-parenthesis 2nd Row 1st Column Blank 2nd Column equals
    sigma-summation Underscript s equals negative normal infinity Overscript normal
    infinity Endscripts f left-parenthesis negative s plus n right-parenthesis k left-parenthesis
    s right-parenthesis period EndLayout dollar-sign"><mtable displaystyle="true"><mtr><mtd
    columnalign="right"><mrow><mo>(</mo> <mi>k</mi> <mo>☆</mo> <mi>f</mi> <mo>)</mo>
    <mo>(</mo> <mi>n</mi> <mo>)</mo></mrow></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <munderover><mo>∑</mo> <mrow><mi>s</mi><mo>=</mo><mo>-</mo><mi>∞</mi></mrow> <mi>∞</mi></munderover>
    <mi>f</mi> <mrow><mo>(</mo> <mi>s</mi> <mo>)</mo></mrow> <mi>k</mi> <mrow><mo>(</mo>
    <mo>-</mo> <mi>s</mi> <mo>+</mo> <mi>n</mi> <mo>)</mo></mrow></mrow></mtd></mtr>
    <mtr><mtd columnalign="left"><mrow><mo>=</mo> <munderover><mo>∑</mo> <mrow><mi>s</mi><mo>=</mo><mo>-</mo><mi>∞</mi></mrow>
    <mi>∞</mi></munderover> <mi>f</mi> <mrow><mo>(</mo> <mo>-</mo> <mi>s</mi> <mo>+</mo>
    <mi>n</mi> <mo>)</mo></mrow> <mi>k</mi> <mrow><mo>(</mo> <mi>s</mi> <mo>)</mo></mrow>
    <mo>.</mo></mrow></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'The cross-correlation operation between two real valued functions *k* (the
    filter) and *f* is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign StartLayout 1st Row 1st Column left-parenthesis k
    asterisk f right-parenthesis left-parenthesis t right-parenthesis 2nd Column equals
    integral Subscript negative normal infinity Superscript normal infinity Baseline
    f left-parenthesis s right-parenthesis k left-parenthesis s plus t right-parenthesis
    d s 2nd Row 1st Column Blank 2nd Column equals integral Subscript negative normal
    infinity Superscript normal infinity Baseline f left-parenthesis s minus t right-parenthesis
    k left-parenthesis s right-parenthesis d s comma EndLayout dollar-sign"><mtable
    displaystyle="true"><mtr><mtd columnalign="right"><mrow><mo>(</mo> <mi>k</mi>
    <mo>*</mo> <mi>f</mi> <mo>)</mo> <mo>(</mo> <mi>t</mi> <mo>)</mo></mrow></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <msubsup><mo>∫</mo> <mrow><mo>-</mo><mi>∞</mi></mrow>
    <mi>∞</mi></msubsup> <mi>f</mi> <mrow><mo>(</mo> <mi>s</mi> <mo>)</mo></mrow>
    <mi>k</mi> <mrow><mo>(</mo> <mi>s</mi> <mo>+</mo> <mi>t</mi> <mo>)</mo></mrow>
    <mi>d</mi> <mi>s</mi></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mrow><mo>=</mo>
    <msubsup><mo>∫</mo> <mrow><mo>-</mo><mi>∞</mi></mrow> <mi>∞</mi></msubsup> <mi>f</mi>
    <mrow><mo>(</mo> <mi>s</mi> <mo>-</mo> <mi>t</mi> <mo>)</mo></mrow> <mi>k</mi>
    <mrow><mo>(</mo> <mi>s</mi> <mo>)</mo></mrow> <mi>d</mi> <mi>s</mi> <mo>,</mo></mrow></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'and the discrete analogue for discrete functions is:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign StartLayout 1st Row 1st Column left-parenthesis k
    asterisk f right-parenthesis left-parenthesis n right-parenthesis 2nd Column equals
    sigma-summation Underscript s equals negative normal infinity Overscript normal
    infinity Endscripts f left-parenthesis s right-parenthesis k left-parenthesis
    s plus n right-parenthesis 2nd Row 1st Column Blank 2nd Column equals sigma-summation
    Underscript s equals negative normal infinity Overscript normal infinity Endscripts
    f left-parenthesis s minus n right-parenthesis k left-parenthesis s right-parenthesis
    period EndLayout dollar-sign"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mo>(</mo>
    <mi>k</mi> <mo>*</mo> <mi>f</mi> <mo>)</mo> <mo>(</mo> <mi>n</mi> <mo>)</mo></mrow></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <munderover><mo>∑</mo> <mrow><mi>s</mi><mo>=</mo><mo>-</mo><mi>∞</mi></mrow>
    <mi>∞</mi></munderover> <mi>f</mi> <mrow><mo>(</mo> <mi>s</mi> <mo>)</mo></mrow>
    <mi>k</mi> <mrow><mo>(</mo> <mi>s</mi> <mo>+</mo> <mi>n</mi> <mo>)</mo></mrow></mrow></mtd></mtr>
    <mtr><mtd columnalign="left"><mrow><mo>=</mo> <munderover><mo>∑</mo> <mrow><mi>s</mi><mo>=</mo><mo>-</mo><mi>∞</mi></mrow>
    <mi>∞</mi></munderover> <mi>f</mi> <mrow><mo>(</mo> <mi>s</mi> <mo>-</mo> <mi>n</mi>
    <mo>)</mo></mrow> <mi>k</mi> <mrow><mo>(</mo> <mi>s</mi> <mo>)</mo></mrow> <mo>.</mo></mrow></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: Note that the formulas defining convolution and cross-correlation look exactly
    the same, except that for convolution we use -*s* instead of *s*. This corresponds
    to flipping the involved function (before shifting). Note also that the indices
    involved in the convolution integral and sum add up to t or n, which is not the
    case for cross-correlation. This makes convolution commutative, in the sense that
    <math alttext="left-parenthesis f star k right-parenthesis left-parenthesis n
    right-parenthesis equals left-parenthesis k star f right-parenthesis left-parenthesis
    n right-parenthesis"><mrow><mo>(</mo> <mi>f</mi> <mo>☆</mo> <mi>k</mi> <mo>)</mo>
    <mo>(</mo> <mi>n</mi> <mo>)</mo> <mo>=</mo> <mo>(</mo> <mi>k</mi> <mo>☆</mo> <mi>f</mi>
    <mo>)</mo> <mo>(</mo> <mi>n</mi> <mo>)</mo></mrow></math> , and cross-correlation
    not necessarily commutative. We comment further on commutativity later.
  prefs: []
  type: TYPE_NORMAL
- en: Every time we encounter a new mathematical object, it is good to pause and ask
    ourselves a few questions, before diving in. This way, we can build a solid mathematical
    foundation, while managing to avoid the vast and dark ocean of technical and complicated
    mathematics.
  prefs: []
  type: TYPE_NORMAL
- en: What kind of mathematical object do we have at our hands?
  prefs: []
  type: TYPE_NORMAL
- en: 'For convolution and cross-correlation, we are looking at infinite sums for
    the discrete case, and integrals over infinite domains for the continuum case.
    This guides our next question:'
  prefs: []
  type: TYPE_NORMAL
- en: Since we are summing over infinite domains, what kind of functions can we allow
    without our computations blowing up to infinity?
  prefs: []
  type: TYPE_NORMAL
- en: In other words, for what kind of functions do these infinite sums and integrals
    exist and are well defined? Here, start with the easy answers, such as when *f*
    and *k* are compactly supported (are zeros except in a finite portion of the domain),
    or when *f* and *k* decay rapidly enough that allows the infinite sum or integral
    to converge. Most of the time, these simple cases are enough for our applications,
    such as image and audio data, and the filters that we accompany them with. Only
    when we find that our simple answer does not apply to our particular use case,
    that we seek more general answers. These come in the form of theorems and proofs.
    Do not seek the most general answers first, as these are usually built on top
    of a large mathematical theory that took centuries, and countless questions and
    searches for answers, to take form and materialize. Taking the most general road
    first is overwhelming, counterintuitive, counter historical, and time and resource
    draining. It is not how mathematics and analysis naturally evolve. Moreover, if
    we happen to encounter someone talking in the most general and technical language
    without providing any context or motivation for why this level of generality and
    complexity is needed, we just tune them out and move on peacefully with our lives,
    otherwise they might confuse us beyond repair.
  prefs: []
  type: TYPE_NORMAL
- en: What is this mathematical object used for?
  prefs: []
  type: TYPE_NORMAL
- en: 'The convolution operation has far reaching applications that range from purely
    theoretical mathematics to applied sciences to the engineering and systems’ design
    fields. In mathematics, it appears in many fields, such as: Differential equations,
    measure theory, probability, statistics, analysis, and numerical linear algebra.
    In the applied sciences and engineering, it is used in acoustics, spectroscopy,
    image processing and computer vision, and in the design and implementation of
    finite impulse response filters in signal processing.'
  prefs: []
  type: TYPE_NORMAL
- en: How is this mathematical object useful in my particular field of study and how
    does it apply to my particular interest or use case?
  prefs: []
  type: TYPE_NORMAL
- en: 'For our AI purposes, we will use the convolution operation to construct convolutional
    neural networks for both one dimensional text and audio data and two dimensional
    image data. The same ideas generalize to any type of high dimensional data where
    most of the information is contained locally. We use convolutional neural networks
    in two contexts: *Undertsanding* image, text and audio data, and *generating*
    image, text, and audio data.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Additionally, in the context of data and distributions of data, we use the
    following result that has to do with the probability distribution of the sum of
    two independent random variables: If <math alttext="mu"><mi>μ</mi></math> and
    <math alttext="nu"><mi>ν</mi></math> are probability measures on the topological
    group <math alttext="left-parenthesis double-struck upper R comma plus right-parenthesis"><mrow><mo>(</mo>
    <mi>ℝ</mi> <mo>,</mo> <mo>+</mo> <mo>)</mo></mrow></math> , and if X and Y are
    two independent random variables whose respective distributions are <math alttext="mu"><mi>μ</mi></math>
    and <math alttext="nu"><mi>ν</mi></math> , then the convolution <math alttext="mu
    star nu"><mrow><mi>μ</mi> <mo>☆</mo> <mi>ν</mi></mrow></math> is the probability
    distribution of the sum random variable X + Y. We elaborate on this in the Probability
    and Measure chapter.'
  prefs: []
  type: TYPE_NORMAL
- en: How did this mathematical object come to be?
  prefs: []
  type: TYPE_NORMAL
- en: 'It is worth investing a tad bit extra time learning some of the history and
    the chronological order of when, how, and why the object that we care about first
    appeared, along with the main results associated with it. In other words, rather
    than studying our mathematical object through a series of dry lemmas, propositions,
    and theorems, which are usually deprived of all context, we learn through its
    own story, and through the ups and downs that mathematicians encountered while
    attempting to develop it. One of the most valuable insights we learn here is that
    mathematics develops organically with our quest to answer various questions, establish
    connections, and gain a deeper understanding of something that we need to use.
    Modern mathematical analysis developed while attempting to answer very simple
    questions related to Fourier sine and cosine series (decomposing a function into
    its component frequencies), that turned out to be not so simple for many types
    of functions, for example: When can we interchange the integral and the infinite
    sum? What is an integral, and what is *dx* anyway?'
  prefs: []
  type: TYPE_NORMAL
- en: What comes as a surprise to many, especially to people who feel intimidated
    or scared of mathematics, is that during the quest to gain understanding, some
    of the biggest names in mathematics, including the fathers and mothers of certain
    fields, made multiple mistakes along the way, and corrected them at later times,
    or were corrected by others, until the theory finally took shape.
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the earliest uses of the convolution integral appeared in 1754, in D’Alembert’s
    derivation of Taylor’s theorem. Later between 1797 and 1800, it was used by Sylvestre
    François Lacroix in his book: *Treatise on Differences and Series*, which is part
    of his encyclopedic series: *Treatise on Differential Calculus and Integral Calculus*.
    Soon thereafter, convolution operations appear in the works of very famous names
    in mathematics, such as: Laplace, Fourier, Poisson, and Volterra. What is common
    here is that all of these studies have to do with integrals, derivatives, and
    series of functions. In other words, Calculus, and again, decomposing functions
    into their component frequencies (Fourier series and transform).'
  prefs: []
  type: TYPE_NORMAL
- en: What are the most important operations, manipulations, and/or theorems related
    to this mathematical object, that we must be aware of, before diving in?
  prefs: []
  type: TYPE_NORMAL
- en: Things do not spring into fame or viral success without being immensely beneficial
    to many, many people. The convolution operation is so simple yet so useful and
    generalizes neatly to more involved mathematical entities, such as measures and
    distributions. It is commutative, associative, distributive over addition and
    scalar multiplication, its integral becomes a product, and its derivative transforms
    to differentiating only one of its component functions.
  prefs: []
  type: TYPE_NORMAL
- en: '*Translation invariance and translation equivariance*: These properties of
    convolutional neural networks enable us to detect similar features in different
    parts of an image. That is, a pattern that occurs in one location of an image
    is easily recognized in other locations of the image. The main reason is that
    at a convolutional layer of a neural network, we convolute with the same filter
    (also called kernel, or template) throughout the image, picking up on the same
    patterns (such as edges, or horizontal, vertical, and diagonal orientations).
    This filter has a fixed set of weights. Recall that this is not the case for fully
    connected neural networks, where we would have to use different weights for different
    pixels of an image. Since we use convolution instead of matrix multiplication
    to perform image filtering, we have the benefit of translation invariance, since
    usually all we care for is the presence of a pattern, irrespective of its location.
    Mathematically, translation invariance looks like:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign t r a n s Subscript a Baseline left-parenthesis k
    right-parenthesis star f equals k star t r a n s Subscript a Baseline left-parenthesis
    f right-parenthesis equals t r a n s Subscript a Baseline left-parenthesis k star
    f right-parenthesis left-parenthesis t right-parenthesis dollar-sign"><mrow><mi>t</mi>
    <mi>r</mi> <mi>a</mi> <mi>n</mi> <msub><mi>s</mi> <mi>a</mi></msub> <mrow><mo>(</mo>
    <mi>k</mi> <mo>)</mo></mrow> <mo>☆</mo> <mi>f</mi> <mo>=</mo> <mi>k</mi> <mo>☆</mo>
    <mi>t</mi> <mi>r</mi> <mi>a</mi> <mi>n</mi> <msub><mi>s</mi> <mi>a</mi></msub>
    <mrow><mo>(</mo> <mi>f</mi> <mo>)</mo></mrow> <mo>=</mo> <mi>t</mi> <mi>r</mi>
    <mi>a</mi> <mi>n</mi> <msub><mi>s</mi> <mi>a</mi></msub> <mrow><mo>(</mo> <mi>k</mi>
    <mo>☆</mo> <mi>f</mi> <mo>)</mo></mrow> <mrow><mo>(</mo> <mi>t</mi> <mo>)</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: where <math alttext="t r a n s Subscript a"><mrow><mi>t</mi> <mi>r</mi> <mi>a</mi>
    <mi>n</mi> <msub><mi>s</mi> <mi>a</mi></msub></mrow></math> is the translation
    of a function by *a*. For our AI purposes, this implies that given a filter designed
    to pick up on certain feature in an image, convolving it with a translated image
    (in the horizontal or vertical directions) is the same as filtering the image
    *then* translating it. This property is sometimes called translation *equivariance*,
    and translation invariance is instead attributed to the pooling layer that is
    often built into the architecture of convolutional neural networks. We discuss
    this later in this chapter. In any case, the fact that we use one filter (one
    set of weights) throughout the whole image at each layer means that we detect
    one pattern at various locations of the image, when present. The same applies
    to audio data or any other type of grid-like data.
  prefs: []
  type: TYPE_NORMAL
- en: '*Convolution in usual space is a product in frequency space*: The Fourier transform
    of the convolution of two functions is the product of the Fourier transform of
    each function, up to a scaling. That is, the convolution operation does not create
    new frequencies, and the frequencies present in the convolution function are simply
    the product of the frequencies of the component functions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In mathematics, we are forever updating our arsenal of useful tools for tackling
    different problems, so depending on our domain of expertise, it is at this point
    that we branch out and study related but more involved results, such as *circular
    convolutions for periodic functions*, preferred algorithms for computing convolutions,
    and others. The best route for us to dive into convolution in a way that is useful
    for our AI purposes is through signal and system design, which is the topic of
    the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Convolution From A System’s Design Perspective
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We are surrounded by systems, each interacts with its environment and is designed
    to accomplish a certain task. Examples include HVAC systems in our buildings,
    adaptive cruise control systems in our cars, city transportation systems, irrigation
    systems, various communication systems, security systems, navigation systems,
    data centers *etc.* Some systems interact with each other, others do not. Some
    are very large, others are as small and simple as a single device, such as a thermostat,
    receiving signals from its environment, through sensors, processing them, and
    outputting other signals, for example to actuators.
  prefs: []
  type: TYPE_NORMAL
- en: 'The convolution operation appears when designing and analyzing such simple
    systems, which process an input signal and produce an output signal, if we impose
    two special constraints: *Linearity and time invariance*. Linearity and time invariance
    become linearity and *translation or shift invariance* if we are dealing with
    *space dependent signals*, such as images, instead of *time dependent signals*,
    such as electrical signals or audio signals. Note that a video is both space (two
    or three spatial dimensions) and time dependent. Linearity in this context has
    to do with the output of a scaled signal (amplified or reduced), and the output
    of two superimposed signals. Time and translation invariance have to do with the
    output of a delayed (time dependent) signal, or the output of a translated or
    shifted (space dependent) signal. We will detail these in the next section. Together,
    linearity and time or translation invariance are very powerful. They allow us
    to find the system’s output of *any signal*, only if we know the output of a simple
    *impulse signal*, called the system’s *impulse response*. The system’s output
    to *any input signal* is obtained by merely convolving the signal with the system’s
    impulse response. Therefore, imposing the conditions of linearity and time or
    translations invariance dramatically simplifies the analysis of signal processing
    systems.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The nagging question is then: How realistic are these wonder-making conditions?
    In other words, how prevalent are linear and time/translation invariant systems,
    or even only approximately linear and approximately time/translation invariant
    systems? Aren’t most realistic systems nonlinear and complex? Thankfully, we have
    control over systems’ designs, so we can just decide to design systems with these
    properties. One example is any electrical circuit consisting of capacitors, resistors,
    inductors and linear amplifiers. This is in fact mathematically equivalent to
    an ideal mechanical spring, mass, and damper system. Other examples that are relevant
    for us include processing and filtering various types of signals and images. We
    discuss these in the next few sections.'
  prefs: []
  type: TYPE_NORMAL
- en: Convolution And Impulse Response For Linear And Translation Invariant Systems
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s formalize the concepts of a linear system and a time/translation invariant
    system, then understand how the convolution operation naturally arises when attempting
    to quantify the response of a system possessing these properties to *any* signal.
    From a math perspective, a system is a function <math alttext="upper H"><mi>H</mi></math>
    that takes an input signal <math alttext="x"><mi>x</mi></math> and produces an
    output signal <math alttext="y"><mi>y</mi></math> . The signals <math alttext="x"><mi>x</mi></math>
    and <math alttext="y"><mi>y</mi></math> can either depend on time, space (single
    or multiple dimensions), or both. If we enforce linearity on such a function,
    then we are claiming two things:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Output of a scaled input signal is nothing but a scaling of the original output:
    <math alttext="upper H left-parenthesis a x right-parenthesis equals a upper H
    left-parenthesis x right-parenthesis equals a y"><mrow><mi>H</mi> <mo>(</mo> <mi>a</mi>
    <mi>x</mi> <mo>)</mo> <mo>=</mo> <mi>a</mi> <mi>H</mi> <mo>(</mo> <mi>x</mi> <mo>)</mo>
    <mo>=</mo> <mi>a</mi> <mi>y</mi></mrow></math>'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Output of two superimposed signals is nothing but the superposition of of the
    two original outputs: <math alttext="upper H left-parenthesis x 1 plus x 2 right-parenthesis
    equals upper H left-parenthesis x 1 right-parenthesis plus upper H left-parenthesis
    x 2 right-parenthesis equals y 1 plus y 2"><mrow><mi>H</mi> <mrow><mo>(</mo> <msub><mi>x</mi>
    <mn>1</mn></msub> <mo>+</mo> <msub><mi>x</mi> <mn>2</mn></msub> <mo>)</mo></mrow>
    <mo>=</mo> <mi>H</mi> <mrow><mo>(</mo> <msub><mi>x</mi> <mn>1</mn></msub> <mo>)</mo></mrow>
    <mo>+</mo> <mi>H</mi> <mrow><mo>(</mo> <msub><mi>x</mi> <mn>2</mn></msub> <mo>)</mo></mrow>
    <mo>=</mo> <msub><mi>y</mi> <mn>1</mn></msub> <mo>+</mo> <msub><mi>y</mi> <mn>2</mn></msub></mrow></math>
    .'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'If we enforce time/translation invariance, then we are claiming that the output
    of a delayed/translated/shifted signal is nothing but a delayed/translated/shifted
    original output: <math alttext="upper H left-parenthesis x left-parenthesis t
    minus t 0 right-parenthesis right-parenthesis equals y left-parenthesis t minus
    t 0 right-parenthesis"><mrow><mi>H</mi> <mrow><mo>(</mo> <mi>x</mi> <mrow><mo>(</mo>
    <mi>t</mi> <mo>-</mo> <msub><mi>t</mi> <mn>0</mn></msub> <mo>)</mo></mrow> <mo>)</mo></mrow>
    <mo>=</mo> <mi>y</mi> <mrow><mo>(</mo> <mi>t</mi> <mo>-</mo> <msub><mi>t</mi>
    <mn>0</mn></msub> <mo>)</mo></mrow></mrow></math> .'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can leverage the above conditions when we think of any arbitrary signal,
    whether discrete or continuous, in terms of a superposition of a bunch of *impulse
    signals* of various amplitudes. This way, if we are able to measure the system’s
    output to a single impulse signal, called the system’s *impulse response*, then
    that is enough information to measure the system’s response to any other signal.
    This becomes the basis of a very rich theory. We only walk through the discrete
    case in this chapter, since the signals we care for in AI (for example, for natural
    language processing, human machine interaction, and computer vision), whether
    one dimensional audio signals, or two or three dimensional images, are discrete.
    The continuous case is analogous except that we consider infinitesimal steps instead
    of discrete steps, we use integrals instead of sums, and we enforce continuity
    conditions (or whatever conditions we need in order to make the involved integrals
    well defined). Actually, the continuous case brings a little extra complication:
    Having to properly define an *impulse* in a mathematically sound way, for it is
    not a function in the usual sense. Thankfully there are multiple mathematical
    ways to make it well defined, such as the theory of distributions, or defining
    it as an operator acting on usual functions, or as a measure and using the help
    of Lebesgue integrals. However, I have been avoiding measures, Lebesgue integrals,
    and any deep mathematical theory until we truly need them with their added functionality,
    so for now, the discrete case is very sufficient.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We define a unit impulse <math alttext="delta left-parenthesis k right-parenthesis"><mrow><mi>δ</mi>
    <mo>(</mo> <mi>k</mi> <mo>)</mo></mrow></math> to be zero for each nonzero k,
    and one when k=0, and define its response as <math alttext="upper H left-parenthesis
    delta left-parenthesis k right-parenthesis right-parenthesis equals h left-parenthesis
    k right-parenthesis"><mrow><mi>H</mi> <mo>(</mo> <mi>δ</mi> <mo>(</mo> <mi>k</mi>
    <mo>)</mo> <mo>)</mo> <mo>=</mo> <mi>h</mi> <mo>(</mo> <mi>k</mi> <mo>)</mo></mrow></math>
    . Then <math alttext="delta left-parenthesis n minus k right-parenthesis"><mrow><mi>δ</mi>
    <mo>(</mo> <mi>n</mi> <mo>-</mo> <mi>k</mi> <mo>)</mo></mrow></math> would be
    zero for each k, and 1 for k=n. This represents a unit impulse located at k=n.
    Therefore, <math alttext="x left-parenthesis k right-parenthesis delta left-parenthesis
    n minus k right-parenthesis"><mrow><mi>x</mi> <mo>(</mo> <mi>k</mi> <mo>)</mo>
    <mi>δ</mi> <mo>(</mo> <mi>n</mi> <mo>-</mo> <mi>k</mi> <mo>)</mo></mrow></math>
    is an impulse of amplitude x(k) located at k=n. Now we can write the input signal
    x(n) as:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign x left-parenthesis n right-parenthesis equals sigma-summation
    Underscript k equals negative normal infinity Overscript normal infinity Endscripts
    x left-parenthesis k right-parenthesis delta left-parenthesis n minus k right-parenthesis
    dollar-sign"><mrow><mi>x</mi> <mrow><mo>(</mo> <mi>n</mi> <mo>)</mo></mrow> <mo>=</mo>
    <msubsup><mo>∑</mo> <mrow><mi>k</mi><mo>=</mo><mo>-</mo><mi>∞</mi></mrow> <mi>∞</mi></msubsup>
    <mi>x</mi> <mrow><mo>(</mo> <mi>k</mi> <mo>)</mo></mrow> <mi>δ</mi> <mrow><mo>(</mo>
    <mi>n</mi> <mo>-</mo> <mi>k</mi> <mo>)</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'The above sum might seem like such a convoluted way to write a signal, and
    it is, but it is very helpful, since it says that any discrete signal can be expressed
    as an infinite sum of unit impulses that are scaled correctly at the right locations.
    Now, using the linearity and translation invariance assumptions on H, it is straightforward
    to see that the system’s response to the signal x(n) is:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign StartLayout 1st Row 1st Column upper H left-parenthesis
    x left-parenthesis n right-parenthesis right-parenthesis 2nd Column equals upper
    H left-parenthesis sigma-summation Underscript k equals negative normal infinity
    Overscript normal infinity Endscripts x left-parenthesis k right-parenthesis delta
    left-parenthesis n minus k right-parenthesis right-parenthesis 2nd Row 1st Column
    Blank 2nd Column equals sigma-summation Underscript k equals negative normal infinity
    Overscript normal infinity Endscripts x left-parenthesis k right-parenthesis upper
    H left-parenthesis delta left-parenthesis n minus k right-parenthesis right-parenthesis
    3rd Row 1st Column Blank 2nd Column equals sigma-summation Underscript k equals
    negative normal infinity Overscript normal infinity Endscripts x left-parenthesis
    k right-parenthesis h left-parenthesis n minus k right-parenthesis 4th Row 1st
    Column Blank 2nd Column equals left-parenthesis x star h right-parenthesis left-parenthesis
    n right-parenthesis 5th Row 1st Column Blank 2nd Column equals y left-parenthesis
    n right-parenthesis EndLayout dollar-sign"><mtable displaystyle="true"><mtr><mtd
    columnalign="right"><mrow><mi>H</mi> <mo>(</mo> <mi>x</mi> <mo>(</mo> <mi>n</mi>
    <mo>)</mo> <mo>)</mo></mrow></mtd> <mtd columnalign="left"><mrow><mo>=</mo> <mi>H</mi>
    <mo>(</mo> <munderover><mo>∑</mo> <mrow><mi>k</mi><mo>=</mo><mo>-</mo><mi>∞</mi></mrow>
    <mi>∞</mi></munderover> <mi>x</mi> <mrow><mo>(</mo> <mi>k</mi> <mo>)</mo></mrow>
    <mi>δ</mi> <mrow><mo>(</mo> <mi>n</mi> <mo>-</mo> <mi>k</mi> <mo>)</mo></mrow>
    <mo>)</mo></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mrow><mo>=</mo> <munderover><mo>∑</mo>
    <mrow><mi>k</mi><mo>=</mo><mo>-</mo><mi>∞</mi></mrow> <mi>∞</mi></munderover>
    <mi>x</mi> <mrow><mo>(</mo> <mi>k</mi> <mo>)</mo></mrow> <mi>H</mi> <mrow><mo>(</mo>
    <mi>δ</mi> <mrow><mo>(</mo> <mi>n</mi> <mo>-</mo> <mi>k</mi> <mo>)</mo></mrow>
    <mo>)</mo></mrow></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mrow><mo>=</mo>
    <munderover><mo>∑</mo> <mrow><mi>k</mi><mo>=</mo><mo>-</mo><mi>∞</mi></mrow> <mi>∞</mi></munderover>
    <mi>x</mi> <mrow><mo>(</mo> <mi>k</mi> <mo>)</mo></mrow> <mi>h</mi> <mrow><mo>(</mo>
    <mi>n</mi> <mo>-</mo> <mi>k</mi> <mo>)</mo></mrow></mrow></mtd></mtr> <mtr><mtd
    columnalign="left"><mrow><mo>=</mo> <mo>(</mo> <mi>x</mi> <mo>☆</mo> <mi>h</mi>
    <mo>)</mo> <mo>(</mo> <mi>n</mi> <mo>)</mo></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mrow><mo>=</mo>
    <mi>y</mi> <mo>(</mo> <mi>n</mi> <mo>)</mo></mrow></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, a linear and translation invariant system is completely described
    by its impulse response h(n). But there is another way to look at this, which
    is independent of linear and translation invariant systems, and which is very
    useful for our purposes in the next few sections. The statement
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign y left-parenthesis n right-parenthesis equals left-parenthesis
    x star h right-parenthesis left-parenthesis n right-parenthesis dollar-sign"><mrow><mi>y</mi>
    <mo>(</mo> <mi>n</mi> <mo>)</mo> <mo>=</mo> <mo>(</mo> <mi>x</mi> <mo>☆</mo> <mi>h</mi>
    <mo>)</mo> <mo>(</mo> <mi>n</mi> <mo>)</mo></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'says that the signal x(n) can be transformed into the signal y(n) after we
    convolve it with the *filter* h(n). Hence, designing the filter h(n) carefully
    can produce a y(n) with some desired characteristics, or can *extract certain
    features* from the signal x(n), for example, all the edges. Moreover, using different
    filters h(n) extracts different features from the same signal x(n). We elaborate
    more on these concepts in the next few sections, meanwhile, keep the following
    in mind: As information, such as signals or images flows through a convolutional
    neural network, different features get extracted (mapped) at each convolutional
    layer.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Before leaving linear and time/translation invariant systems, we must mention
    that such systems have a very simple response for sinusoidal inputs: If the input
    to the system is a sine wave with a given frequency, then the output is also a
    sine wave with the same frequency, but possibly with a different amplitude and
    phase. Moreover, knowing the impulse response of the system allows us to compute
    its *frequency response*, which is the system’s response for sine waves at all
    frequencies, and vice-versa. That is, determining the system’s impulse response
    allows us to compute its frequency response, and determining the frequency response
    allows us to compute its impulse response, which in turn completely determines
    the system’s response to any arbitrary signal. This connection is very use from
    both theoretical and applications perspectives, and is closely connected to Fourier
    transforms and frequency domain represenations of signals. In short, the frequency
    response of a linear and translation invariant system is simply the Fourier transform
    of its impulse response. We do not go over the computational details here, since
    these concepts are not essential for the rest of this book, nevertheless, it is
    important to be aware of these connections and understand how things from seemingly
    different fields come together and relate to each other.'
  prefs: []
  type: TYPE_NORMAL
- en: Convolution And One Dimensional Discrete Signals
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let’s disect the convolution operation and understand how it creates a new
    signal from an input signal by sliding a filter (kernel) against it. We will not
    flip the kernel in the following, as we established the fact that flipping the
    kernel is irrelevant from a neural networks perspective. We start with a one dimensional
    discrete signal *x(n)*, then we convolve it with a filter (kernel) *k(n)*, and
    produce a new signal *z(n)*. We show how we obtain *z(n)* one entry at a time
    by sliding *k(n)* along *x(n)*. For simplicity, let <math alttext="x left-parenthesis
    n right-parenthesis equals left-parenthesis x 0 comma x 1 comma x 2 comma x 3
    comma x 4 right-parenthesis"><mrow><mi>x</mi> <mrow><mo>(</mo> <mi>n</mi> <mo>)</mo></mrow>
    <mo>=</mo> <mrow><mo>(</mo> <msub><mi>x</mi> <mn>0</mn></msub> <mo>,</mo> <msub><mi>x</mi>
    <mn>1</mn></msub> <mo>,</mo> <msub><mi>x</mi> <mn>2</mn></msub> <mo>,</mo> <msub><mi>x</mi>
    <mn>3</mn></msub> <mo>,</mo> <msub><mi>x</mi> <mn>4</mn></msub> <mo>)</mo></mrow></mrow></math>
    and <math alttext="k left-parenthesis n right-parenthesis equals left-parenthesis
    k 0 comma k 1 comma k 2 right-parenthesis"><mrow><mi>k</mi> <mrow><mo>(</mo> <mi>n</mi>
    <mo>)</mo></mrow> <mo>=</mo> <mrow><mo>(</mo> <msub><mi>k</mi> <mn>0</mn></msub>
    <mo>,</mo> <msub><mi>k</mi> <mn>1</mn></msub> <mo>,</mo> <msub><mi>k</mi> <mn>2</mn></msub>
    <mo>)</mo></mrow></mrow></math> . In this example the input signal *x(n)* only
    has five entries, and the kernel has three entries. In practice, such as in signal
    processing, image filtering, or AI’s neural networks, the input signal *x(n)*
    is orders of magnitude larger than the filter *k(n)*. We will see this very soon,
    but the example here is only for illustration. Recall the formula for discrete
    cross-correlation, which is the convolution without flipping the kernel:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign StartLayout 1st Row 1st Column left-parenthesis k
    asterisk x right-parenthesis left-parenthesis n right-parenthesis 2nd Column equals
    sigma-summation Underscript s equals negative normal infinity Overscript normal
    infinity Endscripts x left-parenthesis s right-parenthesis k left-parenthesis
    s plus n right-parenthesis 2nd Row 1st Column Blank 2nd Column equals ellipsis
    plus x left-parenthesis negative 1 right-parenthesis k left-parenthesis negative
    1 plus n right-parenthesis plus x left-parenthesis 0 right-parenthesis k left-parenthesis
    n right-parenthesis plus x left-parenthesis 1 right-parenthesis k left-parenthesis
    1 plus n right-parenthesis plus x left-parenthesis 2 right-parenthesis k left-parenthesis
    2 plus n right-parenthesis plus ellipsis EndLayout dollar-sign"><mtable displaystyle="true"><mtr><mtd
    columnalign="right"><mrow><mo>(</mo> <mi>k</mi> <mo>*</mo> <mi>x</mi> <mo>)</mo>
    <mo>(</mo> <mi>n</mi> <mo>)</mo></mrow></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <munderover><mo>∑</mo> <mrow><mi>s</mi><mo>=</mo><mo>-</mo><mi>∞</mi></mrow> <mi>∞</mi></munderover>
    <mi>x</mi> <mrow><mo>(</mo> <mi>s</mi> <mo>)</mo></mrow> <mi>k</mi> <mrow><mo>(</mo>
    <mi>s</mi> <mo>+</mo> <mi>n</mi> <mo>)</mo></mrow></mrow></mtd></mtr> <mtr><mtd
    columnalign="left"><mrow><mo>=</mo> <mo>⋯</mo> <mo>+</mo> <mi>x</mi> <mo>(</mo>
    <mo>-</mo> <mn>1</mn> <mo>)</mo> <mi>k</mi> <mo>(</mo> <mo>-</mo> <mn>1</mn> <mo>+</mo>
    <mi>n</mi> <mo>)</mo> <mo>+</mo> <mi>x</mi> <mo>(</mo> <mn>0</mn> <mo>)</mo> <mi>k</mi>
    <mo>(</mo> <mi>n</mi> <mo>)</mo> <mo>+</mo> <mi>x</mi> <mo>(</mo> <mn>1</mn> <mo>)</mo>
    <mi>k</mi> <mo>(</mo> <mn>1</mn> <mo>+</mo> <mi>n</mi> <mo>)</mo> <mo>+</mo> <mi>x</mi>
    <mo>(</mo> <mn>2</mn> <mo>)</mo> <mi>k</mi> <mo>(</mo> <mn>2</mn> <mo>+</mo> <mi>n</mi>
    <mo>)</mo> <mo>+</mo> <mo>⋯</mo></mrow></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Since neither *x(n)* nor *k(n)* have infinitely many entries, and the sum is
    infinite, we pretend that the entries are zero whenever the indices are not defined.
    The new filtered signal resulting from the convolution will only have nontrivial
    entries with indices: -4, -3, -2, -1, 0, 1, 2\. Let’s write each of these entries:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign StartLayout 1st Row 1st Column left-parenthesis k
    asterisk x right-parenthesis left-parenthesis negative 4 right-parenthesis 2nd
    Column equals x 4 k 0 2nd Row 1st Column left-parenthesis k asterisk x right-parenthesis
    left-parenthesis negative 3 right-parenthesis 2nd Column equals x 3 k 0 plus x
    4 k 1 3rd Row 1st Column left-parenthesis k asterisk x right-parenthesis left-parenthesis
    negative 2 right-parenthesis 2nd Column equals x 2 k 0 plus x 3 k 1 plus x 4 k
    2 4th Row 1st Column left-parenthesis k asterisk x right-parenthesis left-parenthesis
    negative 1 right-parenthesis 2nd Column equals x 1 k 0 plus x 2 k 1 plus x 3 k
    2 5th Row 1st Column left-parenthesis k asterisk x right-parenthesis left-parenthesis
    0 right-parenthesis 2nd Column equals x 0 k 0 plus x 1 k 1 plus x 2 k 2 6th Row
    1st Column left-parenthesis k asterisk x right-parenthesis left-parenthesis 1
    right-parenthesis 2nd Column equals x 0 k 1 plus x 1 k 2 7th Row 1st Column left-parenthesis
    k asterisk x right-parenthesis left-parenthesis 2 right-parenthesis 2nd Column
    equals x 0 k 2 EndLayout dollar-sign"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mo>(</mo>
    <mi>k</mi> <mo>*</mo> <mi>x</mi> <mo>)</mo> <mo>(</mo> <mo>-</mo> <mn>4</mn> <mo>)</mo></mrow></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <msub><mi>x</mi> <mn>4</mn></msub> <msub><mi>k</mi>
    <mn>0</mn></msub></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mrow><mo>(</mo>
    <mi>k</mi> <mo>*</mo> <mi>x</mi> <mo>)</mo> <mo>(</mo> <mo>-</mo> <mn>3</mn> <mo>)</mo></mrow></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <msub><mi>x</mi> <mn>3</mn></msub> <msub><mi>k</mi>
    <mn>0</mn></msub> <mo>+</mo> <msub><mi>x</mi> <mn>4</mn></msub> <msub><mi>k</mi>
    <mn>1</mn></msub></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mrow><mo>(</mo>
    <mi>k</mi> <mo>*</mo> <mi>x</mi> <mo>)</mo> <mo>(</mo> <mo>-</mo> <mn>2</mn> <mo>)</mo></mrow></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <msub><mi>x</mi> <mn>2</mn></msub> <msub><mi>k</mi>
    <mn>0</mn></msub> <mo>+</mo> <msub><mi>x</mi> <mn>3</mn></msub> <msub><mi>k</mi>
    <mn>1</mn></msub> <mo>+</mo> <msub><mi>x</mi> <mn>4</mn></msub> <msub><mi>k</mi>
    <mn>2</mn></msub></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mrow><mo>(</mo>
    <mi>k</mi> <mo>*</mo> <mi>x</mi> <mo>)</mo> <mo>(</mo> <mo>-</mo> <mn>1</mn> <mo>)</mo></mrow></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <msub><mi>x</mi> <mn>1</mn></msub> <msub><mi>k</mi>
    <mn>0</mn></msub> <mo>+</mo> <msub><mi>x</mi> <mn>2</mn></msub> <msub><mi>k</mi>
    <mn>1</mn></msub> <mo>+</mo> <msub><mi>x</mi> <mn>3</mn></msub> <msub><mi>k</mi>
    <mn>2</mn></msub></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mrow><mo>(</mo>
    <mi>k</mi> <mo>*</mo> <mi>x</mi> <mo>)</mo> <mo>(</mo> <mn>0</mn> <mo>)</mo></mrow></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <msub><mi>x</mi> <mn>0</mn></msub> <msub><mi>k</mi>
    <mn>0</mn></msub> <mo>+</mo> <msub><mi>x</mi> <mn>1</mn></msub> <msub><mi>k</mi>
    <mn>1</mn></msub> <mo>+</mo> <msub><mi>x</mi> <mn>2</mn></msub> <msub><mi>k</mi>
    <mn>2</mn></msub></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mrow><mo>(</mo>
    <mi>k</mi> <mo>*</mo> <mi>x</mi> <mo>)</mo> <mo>(</mo> <mn>1</mn> <mo>)</mo></mrow></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <msub><mi>x</mi> <mn>0</mn></msub> <msub><mi>k</mi>
    <mn>1</mn></msub> <mo>+</mo> <msub><mi>x</mi> <mn>1</mn></msub> <msub><mi>k</mi>
    <mn>2</mn></msub></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mrow><mo>(</mo>
    <mi>k</mi> <mo>*</mo> <mi>x</mi> <mo>)</mo> <mo>(</mo> <mn>2</mn> <mo>)</mo></mrow></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <msub><mi>x</mi> <mn>0</mn></msub> <msub><mi>k</mi>
    <mn>2</mn></msub></mrow></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'The above operation is easier to understand with a mental picture: Fix the
    signal <math alttext="x left-parenthesis n right-parenthesis equals left-parenthesis
    x 0 comma x 1 comma x 2 comma x 3 comma x 4 right-parenthesis"><mrow><mi>x</mi>
    <mrow><mo>(</mo> <mi>n</mi> <mo>)</mo></mrow> <mo>=</mo> <mrow><mo>(</mo> <msub><mi>x</mi>
    <mn>0</mn></msub> <mo>,</mo> <msub><mi>x</mi> <mn>1</mn></msub> <mo>,</mo> <msub><mi>x</mi>
    <mn>2</mn></msub> <mo>,</mo> <msub><mi>x</mi> <mn>3</mn></msub> <mo>,</mo> <msub><mi>x</mi>
    <mn>4</mn></msub> <mo>)</mo></mrow></mrow></math> and slide the filter <math alttext="k
    left-parenthesis n right-parenthesis equals left-parenthesis k 0 comma k 1 comma
    k 2 right-parenthesis"><mrow><mi>k</mi> <mrow><mo>(</mo> <mi>n</mi> <mo>)</mo></mrow>
    <mo>=</mo> <mrow><mo>(</mo> <msub><mi>k</mi> <mn>0</mn></msub> <mo>,</mo> <msub><mi>k</mi>
    <mn>1</mn></msub> <mo>,</mo> <msub><mi>k</mi> <mn>2</mn></msub> <mo>)</mo></mrow></mrow></math>
    against it from right to left. This can also be summarized concisely using linear
    algebra notation, where we multiply the input vector *x(n)* by a special kind
    of matrix, called *Teoplitz matrix* containing the filter weights. We will elaborate
    on this later in this chapter.'
  prefs: []
  type: TYPE_NORMAL
- en: When we slide a filter across the signal, the output signal will peak at the
    indices where *x(n)* and *k(n)* match, so we can design the filter in a way that
    picks up on certain patterns in *x(n)*. This way, convolution (unflipped) provides
    a measure of similarity between the signal and the kernel.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also view cross-correlation, or unflipped convolution, in a different
    way: Each entry of the output signal is a *weighted average* of the entries of
    the input signal. This way, we emphasize both the linearity of this tranformation,
    and the fact that we can choose the weights in the kernel in way that emphasizes
    certain features over others. We see this more clearly with image processing,
    discussed next, but for that we have to write a formula for convolution (unflipped)
    in two dimensions. In linear algebra notation, discrete two dimensional convolution
    amounts to multiplication of the two dimensional signal by another special kind
    of matrix, called the *doubly block circulant matrix*, also appearing later in
    this chapter. Keep in mind that multiplication by a matrix is a linear transformation.'
  prefs: []
  type: TYPE_NORMAL
- en: Convolution And Two Dimensional Discrete Signals
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The convolution (unflipped) operation in two dimensions looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign left-parenthesis k asterisk x right-parenthesis left-parenthesis
    m comma n right-parenthesis equals sigma-summation Underscript q equals negative
    normal infinity Overscript normal infinity Endscripts sigma-summation Underscript
    s equals negative normal infinity Overscript normal infinity Endscripts x left-parenthesis
    m plus q comma n plus s right-parenthesis k left-parenthesis q comma s right-parenthesis
    dollar-sign"><mrow><mrow><mo>(</mo> <mi>k</mi> <mo>*</mo> <mi>x</mi> <mo>)</mo></mrow>
    <mrow><mo>(</mo> <mi>m</mi> <mo>,</mo> <mi>n</mi> <mo>)</mo></mrow> <mo>=</mo>
    <msubsup><mo>∑</mo> <mrow><mi>q</mi><mo>=</mo><mo>-</mo><mi>∞</mi></mrow> <mi>∞</mi></msubsup>
    <msubsup><mo>∑</mo> <mrow><mi>s</mi><mo>=</mo><mo>-</mo><mi>∞</mi></mrow> <mi>∞</mi></msubsup>
    <mi>x</mi> <mrow><mo>(</mo> <mi>m</mi> <mo>+</mo> <mi>q</mi> <mo>,</mo> <mi>n</mi>
    <mo>+</mo> <mi>s</mi> <mo>)</mo></mrow> <mi>k</mi> <mrow><mo>(</mo> <mi>q</mi>
    <mo>,</mo> <mi>s</mi> <mo>)</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, the (2,1) entry of the convolution (unflipped) between the following
    <math alttext="4 times 4"><mrow><mn>4</mn> <mo>×</mo> <mn>4</mn></mrow></math>
    matrix A and the <math alttext="3 times 3"><mrow><mn>3</mn> <mo>×</mo> <mn>3</mn></mrow></math>
    kernel K:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign upper A asterisk upper K equals Start 4 By 4 Matrix
    1st Row 1st Column a 00 2nd Column a 01 3rd Column a 02 4th Column a 03 2nd Row
    1st Column a 10 2nd Column a 11 3rd Column a 12 4th Column a 13 3rd Row 1st Column
    a 20 2nd Column StartEnclose box a 21 EndEnclose 3rd Column a 22 4th Column a
    23 4th Row 1st Column a 30 2nd Column a 31 3rd Column a 32 4th Column a 33 EndMatrix
    asterisk Start 3 By 3 Matrix 1st Row 1st Column k 00 2nd Column k 01 3rd Column
    k 02 2nd Row 1st Column k 10 2nd Column StartEnclose box k 11 EndEnclose 3rd Column
    k 12 3rd Row 1st Column k 20 2nd Column k 21 3rd Column k 22 EndMatrix dollar-sign"><mrow><mi>A</mi>
    <mo>*</mo> <mi>K</mi> <mo>=</mo> <mfenced close=")" open="("><mtable><mtr><mtd><msub><mi>a</mi>
    <mn>00</mn></msub></mtd> <mtd><msub><mi>a</mi> <mn>01</mn></msub></mtd> <mtd><msub><mi>a</mi>
    <mn>02</mn></msub></mtd> <mtd><msub><mi>a</mi> <mn>03</mn></msub></mtd></mtr>
    <mtr><mtd><msub><mi>a</mi> <mn>10</mn></msub></mtd> <mtd><msub><mi>a</mi> <mn>11</mn></msub></mtd>
    <mtd><msub><mi>a</mi> <mn>12</mn></msub></mtd> <mtd><msub><mi>a</mi> <mn>13</mn></msub></mtd></mtr>
    <mtr><mtd><msub><mi>a</mi> <mn>20</mn></msub></mtd> <mtd><mtable frame="solid"><mtr><mtd><msub><mi>a</mi>
    <mn>21</mn></msub></mtd></mtr></mtable></mtd> <mtd><msub><mi>a</mi> <mn>22</mn></msub></mtd>
    <mtd><msub><mi>a</mi> <mn>23</mn></msub></mtd></mtr> <mtr><mtd><msub><mi>a</mi>
    <mn>30</mn></msub></mtd> <mtd><msub><mi>a</mi> <mn>31</mn></msub></mtd> <mtd><msub><mi>a</mi>
    <mn>32</mn></msub></mtd> <mtd><msub><mi>a</mi> <mn>33</mn></msub></mtd></mtr></mtable></mfenced>
    <mo>*</mo> <mfenced close=")" open="("><mtable><mtr><mtd><msub><mi>k</mi> <mn>00</mn></msub></mtd>
    <mtd><msub><mi>k</mi> <mn>01</mn></msub></mtd> <mtd><msub><mi>k</mi> <mn>02</mn></msub></mtd></mtr>
    <mtr><mtd><msub><mi>k</mi> <mn>10</mn></msub></mtd> <mtd><mtable frame="solid"><mtr><mtd><msub><mi>k</mi>
    <mn>11</mn></msub></mtd></mtr></mtable></mtd> <mtd><msub><mi>k</mi> <mn>12</mn></msub></mtd></mtr>
    <mtr><mtd><msub><mi>k</mi> <mn>20</mn></msub></mtd> <mtd><msub><mi>k</mi> <mn>21</mn></msub></mtd>
    <mtd><msub><mi>k</mi> <mn>22</mn></msub></mtd></mtr></mtable></mfenced></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'is <math alttext="z 21 equals a 10 k 00 plus a 11 k 01 plus a 12 k 02 plus
    a 20 k 10 plus a 21 k 11 plus a 22 k 12 plus a 30 k 20 plus a 31 k 21 plus a 32
    k 22"><mrow><msub><mi>z</mi> <mn>21</mn></msub> <mo>=</mo> <msub><mi>a</mi> <mn>10</mn></msub>
    <msub><mi>k</mi> <mn>00</mn></msub> <mo>+</mo> <msub><mi>a</mi> <mn>11</mn></msub>
    <msub><mi>k</mi> <mn>01</mn></msub> <mo>+</mo> <msub><mi>a</mi> <mn>12</mn></msub>
    <msub><mi>k</mi> <mn>02</mn></msub> <mo>+</mo> <msub><mi>a</mi> <mn>20</mn></msub>
    <msub><mi>k</mi> <mn>10</mn></msub> <mo>+</mo> <msub><mi>a</mi> <mn>21</mn></msub>
    <msub><mi>k</mi> <mn>11</mn></msub> <mo>+</mo> <msub><mi>a</mi> <mn>22</mn></msub>
    <msub><mi>k</mi> <mn>12</mn></msub> <mo>+</mo> <msub><mi>a</mi> <mn>30</mn></msub>
    <msub><mi>k</mi> <mn>20</mn></msub> <mo>+</mo> <msub><mi>a</mi> <mn>31</mn></msub>
    <msub><mi>k</mi> <mn>21</mn></msub> <mo>+</mo> <msub><mi>a</mi> <mn>32</mn></msub>
    <msub><mi>k</mi> <mn>22</mn></msub></mrow></math> . To see this, imagine placing
    the kernel K exactly on top of the matrix A with its center <math alttext="k 11"><msub><mi>k</mi>
    <mn>11</mn></msub></math> on top of <math alttext="a 21"><msub><mi>a</mi> <mn>21</mn></msub></math>
    , meaning the entry of A with the required index, then multiply the entries that
    are on top of each other and add all the results together. Note that here we only
    computed one entry of the output signal, meaning if we were working with images,
    it would be the value of only one pixel of the filtered image. We need all the
    others! For this, we have to know which indices are valid convolutions. By valid
    we mean *full*, as in they take all of the kernel entries into account during
    the computation. The word *valid* is a bit misleading since *all* entries are
    valid if we allow ourselves to *pad the boundaries of the matrix A by zeros*.
    To find the indices that take the full kernel into account, recall our mental
    image of placing K exactly on top of A, with K’s center at the index that we want
    to compute. With this placement, the rest of K should not exceed the boundaries
    of A, so for our example, the good indices will be (1,1), (1,2), (2,1) and (2,2),
    producing the output:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign upper Z equals Start 2 By 2 Matrix 1st Row 1st Column
    z 11 2nd Column z 12 2nd Row 1st Column z 21 2nd Column z 22 EndMatrix period
    dollar-sign"><mrow><mi>Z</mi> <mo>=</mo> <mfenced close=")" open="("><mtable><mtr><mtd><msub><mi>z</mi>
    <mn>11</mn></msub></mtd> <mtd><msub><mi>z</mi> <mn>12</mn></msub></mtd></mtr>
    <mtr><mtd><msub><mi>z</mi> <mn>21</mn></msub></mtd> <mtd><msub><mi>z</mi> <mn>22</mn></msub></mtd></mtr></mtable></mfenced>
    <mo>.</mo></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'This means that if we were working with images, the filtered image Z would
    be smaller in size than the original image A. If we want to produce an image with
    the same size as the original image, then we must pad the original image with
    zeros before applying the filter. For our example, we would need to pad with one
    layer of zeros around the full boundary of A, but if K was larger, then we would
    need more layers of zeros. The following is A padded with one layer of zeros:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign upper A Subscript p a d d e d Baseline equals Start
    6 By 6 Matrix 1st Row 1st Column 0 2nd Column 0 3rd Column 0 4th Column 0 5th
    Column 0 6th Column 0 2nd Row 1st Column 0 2nd Column a 00 3rd Column a 01 4th
    Column a 02 5th Column a 03 6th Column 0 3rd Row 1st Column 0 2nd Column a 10
    3rd Column a 11 4th Column a 12 5th Column a 13 6th Column 0 4th Row 1st Column
    0 2nd Column a 20 3rd Column a 21 4th Column a 22 5th Column a 23 6th Column 0
    5th Row 1st Column 0 2nd Column a 30 3rd Column a 31 4th Column a 32 5th Column
    a 33 6th Column 0 6th Row 1st Column 0 2nd Column 0 3rd Column 0 4th Column 0
    5th Column 0 6th Column 0 EndMatrix period dollar-sign"><mrow><msub><mi>A</mi>
    <mrow><mi>p</mi><mi>a</mi><mi>d</mi><mi>d</mi><mi>e</mi><mi>d</mi></mrow></msub>
    <mo>=</mo> <mfenced close=")" open="("><mtable><mtr><mtd><mn>0</mn></mtd> <mtd><mn>0</mn></mtd>
    <mtd><mn>0</mn></mtd> <mtd><mn>0</mn></mtd> <mtd><mn>0</mn></mtd> <mtd><mn>0</mn></mtd></mtr>
    <mtr><mtd><mn>0</mn></mtd> <mtd><msub><mi>a</mi> <mn>00</mn></msub></mtd> <mtd><msub><mi>a</mi>
    <mn>01</mn></msub></mtd> <mtd><msub><mi>a</mi> <mn>02</mn></msub></mtd> <mtd><msub><mi>a</mi>
    <mn>03</mn></msub></mtd> <mtd><mn>0</mn></mtd></mtr> <mtr><mtd><mn>0</mn></mtd>
    <mtd><msub><mi>a</mi> <mn>10</mn></msub></mtd> <mtd><msub><mi>a</mi> <mn>11</mn></msub></mtd>
    <mtd><msub><mi>a</mi> <mn>12</mn></msub></mtd> <mtd><msub><mi>a</mi> <mn>13</mn></msub></mtd>
    <mtd><mn>0</mn></mtd></mtr> <mtr><mtd><mn>0</mn></mtd> <mtd><msub><mi>a</mi> <mn>20</mn></msub></mtd>
    <mtd><msub><mi>a</mi> <mn>21</mn></msub></mtd> <mtd><msub><mi>a</mi> <mn>22</mn></msub></mtd>
    <mtd><msub><mi>a</mi> <mn>23</mn></msub></mtd> <mtd><mn>0</mn></mtd></mtr> <mtr><mtd><mn>0</mn></mtd>
    <mtd><msub><mi>a</mi> <mn>30</mn></msub></mtd> <mtd><msub><mi>a</mi> <mn>31</mn></msub></mtd>
    <mtd><msub><mi>a</mi> <mn>32</mn></msub></mtd> <mtd><msub><mi>a</mi> <mn>33</mn></msub></mtd>
    <mtd><mn>0</mn></mtd></mtr> <mtr><mtd><mn>0</mn></mtd> <mtd><mn>0</mn></mtd> <mtd><mn>0</mn></mtd>
    <mtd><mn>0</mn></mtd> <mtd><mn>0</mn></mtd> <mtd><mn>0</mn></mtd></mtr></mtable></mfenced>
    <mo>.</mo></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'There are other ways to retain the size of the output image than zero padding,
    even though zero padding is the simplest and most popular approach. These include:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Reflection*: Instead of adding a layer or multiple layers of zeros, add a
    layer or multiple layers of the same values as the boundary pixels of the image,
    and the ones below them, and so on if we need more. That is, instead of turning
    off the pixels outside an image boundary, extend the image using the same pixels
    that are already on or near the boundary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Wraparound*: This is used for periodic signals. Mathematically, this is where
    cyclic convolution, circulant matrix, discrete Fourier transform producing the
    eigenvalues of the circulant matrix, and the Fourier matrix containing its eigenvectors
    as columns. We will not dive into that here, but recall that periodicity usually
    simplifies things, and makes us think of periodic waves, which in turn make us
    think of Fourier stuff.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Multiple channels*: Apply more than one *channel* of independent filters (weight
    matrices), each down samples the original input, then combine their outputs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Filtering Images
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Figure 5-1](#Fig_image_filters) shows an example of how convolving the same
    image with various kernels extracts various features of the image.'
  prefs: []
  type: TYPE_NORMAL
- en: '![300](assets/emai_0501.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-1\. Applying various filters to an image ([image source](https://en.wikipedia.org/wiki/Kernel_(image_processing))).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: For example, the third kernel in the table has 8 at its center and the rest
    of its entries are -1\. This means that this kernel makes the current pixel 8
    times as intense then subtracts from that the values of all the pixels surrounding
    it. If we are in a uniform region of the image, meaning all the pixels are equal
    or very close in value, then this process will give zero, returning a black or
    turned off pixel. If on the other hand this pixel lays on an edge boundary, for
    example, the boundary of the eye, or the boundary of the face of the deer, then
    the output of the convolution will have a nonzero value so it will be a bright
    pixel. When we apply this process to the whole image the result is a new image
    with many the edges traced with bright pixels and the rest of the image is dark.
    As we see in the table, the choice of the kernel that is able to detect edges,
    blur, *etc.* is not unique. The same table includes two dimensional discrete Gaussian
    filters for blurring. When we discretize a one dimensional Gaussian function,
    we do not lose its symmetry, but when we discretize a two dimensional Gaussian
    function, we lose its radial symmetry, since we have to approximate its natural
    circular or elliptical shape by a square matrix. Note that a Gaussian peaks at
    the center and decays as it spreads away from the center. Moreover, the area under
    its curve (surface for two dimensions) is one. This has the overall effect of
    averaging and smoothing (removing noise) when we convolve this with another signal.
    The price we pay is removing sharp edges, which is exactly what blurring is (think
    that a sharp edge is replaced by a smooth decaying average of itself and all the
    surrounding pixels within a distance of few standard deviations from the center).
    The smaller the standard deviation (or the variance, which is the square of the
    standard deviation), the more detail we can retain from the image.
  prefs: []
  type: TYPE_NORMAL
- en: Another great example is shown in [Figure 5-2](#Fig_filters_gabor). Here, each
    image is the result of the convolution between the top image and the filter (kernel)
    pictured at the bottom left corner of each subplot. These filters are called [Gabor
    filters](https://en.wikipedia.org/wiki/Gabor_filter). They are designed to pick
    up on certain patterns/textures/features in an image, and they work in a similar
    way to the filters found in the human visual system.
  prefs: []
  type: TYPE_NORMAL
- en: '![emai 0502](assets/emai_0502.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-2\. Gabor filters
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Our eyes detect the parts of an image where things change, i.e., contrasts.
    They pick up on edges (horizontal, vertical, diagonal) and gradients (measuring
    the steepness of a change). We design filters that do the same things mathematically,
    sliding them via convolution across a signal. These produce smooth and uneventful
    results (zeros or numbers close to each other in value) when nothing is changing
    in the signal, and spiking when an edge or a gradient that lines up with the kernel
    is detected.
  prefs: []
  type: TYPE_NORMAL
- en: Feature Maps
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A convolutional neural network *learns* the kernels from the data by optimizing
    a loss function in the same way a fully connected network does. The unknown weights
    that enter the formula of the training function are the entries of each kernel
    at each convolutional layer, the biases, and the weights related to any fully
    connected layer involved in the network’s architecture. The output of a convolutional
    layer (which includes the nonlinear activation function) is called a *feature
    map* and the learned kernels are feature detectors. A common observation in neural
    networks is that earlier layers in the network (close to the input layer) learn
    low level features such as edges, and later layers learn higher level features
    such as shapes. This is naturally expected since at each new layer we compose
    with a nonlinear activation function, so complexity increases over multiple layers
    and so does the network’s ability to express more elaborate features.
  prefs: []
  type: TYPE_NORMAL
- en: How do we plot feature maps?
  prefs: []
  type: TYPE_NORMAL
- en: Feature maps help us open the black box and directly observe what a trained
    network detects at each of its convolutional layers. If the network is still in
    the training process, then feature maps allow us to pinpoint the sources of error,
    then tweak the model accordingly. Suppose we input an image to a trained convolutional
    neural network. At the first layer, using the convolution operation, a kernel
    slides through the whole image and produces a new filtered image. This filtered
    image then gets passed through a nonlinear activation function, producing yet
    another image. Finally, this gets passed through a pooling layer, explained soon,
    and produces the final output of the convolutional layer. This output is a different
    image than the one we started with, and possibly of different dimensions, but
    if the network was trained well, the output image would highlight some important
    features of the original image, such as edges, texture, *etc*. This output image
    is usually a matrix of numbers or a tensor of numbers (three dimensional for color
    images, or four dimensional if we are working in batches of images or with video
    data, where there is one extra dimension for the time series). It is easy to visualize
    these as feature maps using matplotlib library in Python, where each entry in
    the matrix or tensor is mapped to the intensity of a pixel in the same location
    as the matrix entry. [Figure 5-3](#Fig_feature_maps) shows various features maps
    at various convolutional layers of a convolutional neural network.
  prefs: []
  type: TYPE_NORMAL
- en: '![emai 0503](assets/emai_0503.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-3\. Feature maps in a convolutional neural network
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Linear Algebra Notation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We now know that the most fundamental operation inside a convolutional neural
    network is the convolution. Given a filter *k* (could be one dimensional, two
    dimensional, or three or higher dimensional), a convolution operation takes an
    input signal and applies *k* to it, by sliding it across the signal. This operation
    is linear, meaning each output is a linear combination (by the weights in *k*)
    of the input components, thus it can be efficiently represented as a matrix multiplication.
    We need this efficiency, since we must still write the training function representing
    the convolutional neural network in a way that is easy to evaluate and differentiate.
    The mathematical structure of the problem is still the same as in the case for
    many machine learning models, which we are very familiar with now:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Training function**: For convolutional neural networks, this usually includes
    linear combinations of components of the input, composed with activation functions,
    then a pooling function (dicussed soon), over multiple layers of various sizes
    and connections, finally topped with a logistic function, a support vector machine
    function, or other functions depending on the ultimate purpose of the network
    (classification, image segmentation, data generation, *etc.*). The difference
    from fully connected neural networks is that the linear combination happens now
    with the weights associated with the filter, which means they are not all different
    than each other (unless we are only doing locally connected layers instead of
    convolutional layers). Moreover, the sizes of the filters are usually orders of
    magnitude smaller than the input signals, so when we express the convolution operation
    in matrix notation, most of the weights in the matrix will be actually zero. Recall
    that there is a weight per each input feature at each layer of a network. For
    example, if the input is a color image, there is a different weight for each pixel
    in each channel, unless we decide to implement a convolutional layer, then there
    would only be few unique weights, and many, many zeros. This simplifies storage
    requirements and computation time tremendously, while at the same time capturing
    the important local interactions. That said, it is common in the architecture
    of a convolutional neural network to have fully connected layers (with a different
    weight assigned to each connection) near the output layer. One can rationalize
    this as a distillation of features: After important and locally dependent features,
    which increase in complexity with each layer, are captured, these are combined
    together to make a prediction. That is, when information arrives to a fully connected
    layer, it would’ve been distilled down to its most important components, which
    in turn act as unique features contributing to a final prediction.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Loss function**: This is similar to all the loss functions we went over in
    the previous chapters, always providing a measure of the error made by the network’s
    predictions and ground truths.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Optimization**: This again uses stochastic gradient descent, due to the enormity
    of the size of the problem and the number of variables that goes into it. Here,
    as usual, we need to evaluate one derivative of the loss function which includes
    one derivative of the training function. We compute the derivative with respect
    to all the unknown weights involved in the filters of all the network’s layers
    and channels, and the associated biases. Computationally, the backpropagation
    algorithm is still the workhorse of the differentiation process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Linear algebra and computational linear algebra have all the tools that we need
    to produce trainable convolutional neural networks. In general, the worst kind
    of matrix to be involved in our computations is a dense (mostly nonzero entries)
    matrix with no obvious structure or pattern to its entries (even worse if it is
    nondiagonalizable, *etc*.). But when we have a sparse matrix (mostly zeros), or
    matrix with a certain pattern to its entries (such as diagonal, tri-diagonal,
    circular, *etc.*), or both, then we are in a computationally friendly world, given
    that we learn how to exploit the special matrix structure to our advantage. Researchers
    who study large matrix computations and algorithms are the kings and queens of
    such necessary exploitation, and without their work we’d be left with theory that
    is very hard to implement in practice and at scale.
  prefs: []
  type: TYPE_NORMAL
- en: 'In one dimension, the convolution operation can be represented using a special
    kind of matrix, called the *Teoplitz* matrix, and in two dimensions, it can be
    represented using another special kind of matrix, called a *doubly block circulant*
    matrix. Let’s only focus on these two, but with the take home lesson that matrix
    notation, in general, is the best way to go, and we would be fools not to discover
    and make the most use of the inherent structure within our matrices. In other
    words, attacking the most general cases first might just be a sad waste of time,
    which happens to be a rare commodity in this life. A good compromise between working
    with the most general matrix and the most specific matrix is to accompany results
    with complexity analyses: This method is of order <math alttext="n cubed comma
    n log n"><mrow><msup><mi>n</mi> <mn>3</mn></msup> <mo>,</mo> <mi>n</mi> <mo form="prefix">log</mo>
    <mi>n</mi></mrow></math> , *etc.* so that stakeholders are made aware of the tradeoffs
    between implementing certain methods *vs* others.'
  prefs: []
  type: TYPE_NORMAL
- en: We borrow a simple example from the online free book [Deep Learning Book 2016
    (Chapter 9 page 334)](https://www.deeplearningbook.org) on the efficiency of using
    either convolution or exploiting the many zeros in a matrix *vs* using usual matrix
    multiplication in order to detect vertical edges within a certain image. *Convolution
    is an extremely eﬃcient way of describing transformations that apply the same
    linear transformation of a small local region across the entire input.*
  prefs: []
  type: TYPE_NORMAL
- en: '*The image on the right of [Figure 5-4](#Fig_dog_edges) was formed by taking
    each pixel in the original image and subtracting the value of its neighboring
    pixel on the left. This shows the strength of all the vertically oriented edges
    in the input image,which can be a useful operation for object detection.*'
  prefs: []
  type: TYPE_NORMAL
- en: '![300](assets/emai_0504.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-4\. Detecting the vertical edges in an image ([image source](https://www.deeplearningbook.org)).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '*Both images are 280 pixels tall.The input image is 320 pixels wide, while
    the output image is 319 pixels wide. This transformation can be described by a
    convolution kernel containing two elements, and requires 319×280×3 = 267,960 ﬂoating-point
    operations (two multiplications and one addition per output pixel) to compute
    using convolution.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*To describe the same transformation with a matrix multiplication would take
    320×280×319×280, or over eight billion, entries in the matrix, making convolution
    four billion times more eﬃcient for representing this transformation. The straightforward
    matrix multiplication algorithm performs over sixteen billion ﬂoating point operations,
    making convolution roughly 60,000 times more eﬃcient computationally. Of course,
    most of the entries of the matrix would be zero. If we stored only the nonzero
    entries of the matrix, then both matrix multiplication and convolution would require
    the same number of ﬂoating point operations to compute.The matrix would still
    need to contain 2×319×280 = 178,640 entries.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'The One Dimensional Case: Multiplication by a Teoplitz Matrix'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A *banded* Teoplitz matrix looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign upper T e o p l i t z equals Start 5 By 7 Matrix
    1st Row 1st Column k 0 2nd Column k 1 3rd Column k 2 4th Column 0 5th Column 0
    6th Column 0 7th Column 0 2nd Row 1st Column 0 2nd Column k 0 3rd Column k 1 4th
    Column k 2 5th Column 0 6th Column 0 7th Column 0 3rd Row 1st Column 0 2nd Column
    0 3rd Column k 0 4th Column k 1 5th Column k 2 6th Column 0 7th Column 0 4th Row
    1st Column 0 2nd Column 0 3rd Column 0 4th Column k 0 5th Column k 1 6th Column
    k 2 7th Column 0 5th Row 1st Column 0 2nd Column 0 3rd Column 0 4th Column 0 5th
    Column k 0 6th Column k 1 7th Column k 2 EndMatrix dollar-sign"><mrow><mi>T</mi>
    <mi>e</mi> <mi>o</mi> <mi>p</mi> <mi>l</mi> <mi>i</mi> <mi>t</mi> <mi>z</mi> <mo>=</mo>
    <mfenced close=")" open="("><mtable><mtr><mtd><msub><mi>k</mi> <mn>0</mn></msub></mtd>
    <mtd><msub><mi>k</mi> <mn>1</mn></msub></mtd> <mtd><msub><mi>k</mi> <mn>2</mn></msub></mtd>
    <mtd><mn>0</mn></mtd> <mtd><mn>0</mn></mtd> <mtd><mn>0</mn></mtd> <mtd><mn>0</mn></mtd></mtr>
    <mtr><mtd><mn>0</mn></mtd> <mtd><msub><mi>k</mi> <mn>0</mn></msub></mtd> <mtd><msub><mi>k</mi>
    <mn>1</mn></msub></mtd> <mtd><msub><mi>k</mi> <mn>2</mn></msub></mtd> <mtd><mn>0</mn></mtd>
    <mtd><mn>0</mn></mtd> <mtd><mn>0</mn></mtd></mtr> <mtr><mtd><mn>0</mn></mtd> <mtd><mn>0</mn></mtd>
    <mtd><msub><mi>k</mi> <mn>0</mn></msub></mtd> <mtd><msub><mi>k</mi> <mn>1</mn></msub></mtd>
    <mtd><msub><mi>k</mi> <mn>2</mn></msub></mtd> <mtd><mn>0</mn></mtd> <mtd><mn>0</mn></mtd></mtr>
    <mtr><mtd><mn>0</mn></mtd> <mtd><mn>0</mn></mtd> <mtd><mn>0</mn></mtd> <mtd><msub><mi>k</mi>
    <mn>0</mn></msub></mtd> <mtd><msub><mi>k</mi> <mn>1</mn></msub></mtd> <mtd><msub><mi>k</mi>
    <mn>2</mn></msub></mtd> <mtd><mn>0</mn></mtd></mtr> <mtr><mtd><mn>0</mn></mtd>
    <mtd><mn>0</mn></mtd> <mtd><mn>0</mn></mtd> <mtd><mn>0</mn></mtd> <mtd><msub><mi>k</mi>
    <mn>0</mn></msub></mtd> <mtd><msub><mi>k</mi> <mn>1</mn></msub></mtd> <mtd><msub><mi>k</mi>
    <mn>2</mn></msub></mtd></mtr></mtable></mfenced></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: Multiplying this Teoplitz matrix by a one dimensional signal <math alttext="x
    equals left-parenthesis x 0 comma x 1 comma x 2 comma x 3 comma x 4 comma x 5
    comma x 6 comma x 7 right-parenthesis"><mrow><mi>x</mi> <mo>=</mo> <mo>(</mo>
    <msub><mi>x</mi> <mn>0</mn></msub> <mo>,</mo> <msub><mi>x</mi> <mn>1</mn></msub>
    <mo>,</mo> <msub><mi>x</mi> <mn>2</mn></msub> <mo>,</mo> <msub><mi>x</mi> <mn>3</mn></msub>
    <mo>,</mo> <msub><mi>x</mi> <mn>4</mn></msub> <mo>,</mo> <msub><mi>x</mi> <mn>5</mn></msub>
    <mo>,</mo> <msub><mi>x</mi> <mn>6</mn></msub> <mo>,</mo> <msub><mi>x</mi> <mn>7</mn></msub>
    <mo>)</mo></mrow></math> yields the exact result of the convolution of a one dimensional
    filter <math alttext="k equals left-parenthesis k 1 comma k 2 comma k 3 right-parenthesis"><mrow><mi>k</mi>
    <mo>=</mo> <mo>(</mo> <msub><mi>k</mi> <mn>1</mn></msub> <mo>,</mo> <msub><mi>k</mi>
    <mn>2</mn></msub> <mo>,</mo> <msub><mi>k</mi> <mn>3</mn></msub> <mo>)</mo></mrow></math>
    with the signal *x*, namely, <math alttext="left-parenthesis upper T e o p l i
    t z right-parenthesis x Superscript t Baseline equals k asterisk x"><mrow><mrow><mo>(</mo>
    <mi>T</mi> <mi>e</mi> <mi>o</mi> <mi>p</mi> <mi>l</mi> <mi>i</mi> <mi>t</mi> <mi>z</mi>
    <mo>)</mo></mrow> <msup><mi>x</mi> <mi>t</mi></msup> <mo>=</mo> <mi>k</mi> <mo>*</mo>
    <mi>x</mi></mrow></math> . Performing the multiplication, we see the *sliding
    effect* of the filter across the signal.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Two Dimensional Case: Multiplication by a Doubly Block Circulant Matrix'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The two dimensional analogue involves the two dimensional convolution operation
    and filtering images. Here, instead of multiplying with a Teoplitz matrix, we
    end up in effect multiplying with a *doubly circulant matrix*, where each row
    is a circular shift of a given vector. It is a nice exercise in linear algebra
    to write this matrix down along with its equivalence to two dimensional convolution.
    In deep learning, we end up learning the weights, which are the entries of these
    matrices. This linear algebra notation (in terms of Teoplitz or circulant matrices)
    helps us find compact formulas for the derivatives of the loss function with respect
    to these weights.
  prefs: []
  type: TYPE_NORMAL
- en: Pooling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One step common to almost all convolutional neural networks is *pooling*. This
    is typically implemented after the input gets filtered via convolution, then passed
    through the nonlinear activation function. There is more than one type of pooling,
    but the idea is the same: Replace the current output at a certain location with
    a summary statistic of the nearby outputs. An example for images is to replace
    four pixels with one pixel containing the maximum value of the original four (*max
    pooling*), or with their average value, or a weighted average, or the square root
    of the sum of their squares, *etc.*.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 5-5](#Fig_max_pool) shows how max pooling works.'
  prefs: []
  type: TYPE_NORMAL
- en: '![300](assets/emai_0505.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-5\. Max pooling.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'In effect, this reduces the dimension and summarizes whole neighborhoods of
    outputs, on the expense of sacrificing fine detail. So pooling is not very good
    for use cases where fine detail is essential for making predictions. Nevertheless,
    pooling has many advantages:'
  prefs: []
  type: TYPE_NORMAL
- en: It provides approximate invariance to small spatial translations of the input.
    This is useful if we care more about whether some feature is present than its
    exact location.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It can greatly improve the statistical eﬃciency of the network.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It improves the computational eﬃciency and memory requirements of the network
    because it reduces the number on inputs to the next layer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It helps with handling inputs of varying sizes, because we can control the size
    of the pooled neighborhoods, and the size of the output after pooling.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A Convolutional Neural Network For Image Classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It is impossible to have an exhaustive list of all the different architechtures
    and variations that are involved in neural networks without diverting from the
    main purpose of the book: Understanding the mathematics that underlies the different
    models. It is possible, however to go over the essential components and how they
    all come together to accomplish an AI task, such as image classification for computer
    vision. During the training process, the steps of the previous chapter still apply:'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize random weights (according to the initialization processes we described
    in the previous chapter).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Forward pass a batch of images through the convolutional network and output
    a class for the image.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Evaluate the loss function for this particular choice of weights.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Backpropagate the error through the network.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Adjust the weights that contributed to the error (stochastic gradient descent).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat until a certain number of iterations, or until you converge.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Thankfully we do not have to do any of this on our own. Python’s library Keras
    has many pre-trained models, which means that their weights have already been
    fixed, and all we have to do is evaluate the trained model on our particular data
    set.
  prefs: []
  type: TYPE_NORMAL
- en: What we can and should do is observe and learn the architecture of successful
    and winning networks. [Figure 5-6](#Fig_LeNet_1) shows the simple architecture
    of LeNet1 by Le Cun et al (1989), and [Figure 5-7](#Fig_AlexNet) shows AlexNet’s
    (2012) architecture.
  prefs: []
  type: TYPE_NORMAL
- en: '![300](assets/emai_0506.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-6\. The architecture of LeNet1 (1989).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![300](assets/emai_0507.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-7\. The architecture of AlexNet with a whopping 62.3 million weights
    (2012).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'A good exercise is to count the number of the weights that goes into the training
    function of LeNet1 and of AlexNet. Note that more units in each layer (feature
    maps) means more weights. When I tried to count the weights involved in LeNet1
    based on the architecture in figure [Figure 5-6](#Fig_LeNet_1), I ended up with
    9484 weights, but the original paper mentions 9760 weights, so I do not know where
    the rest of the weights are. If you find them please let me know. Either way,
    the point is we need to solve an optimization problem in <math alttext="double-struck
    upper R Superscript 9760"><msup><mi>ℝ</mi> <mn>9760</mn></msup></math> . Now do
    the same computation for AlexNet in [Figure 5-7](#Fig_AlexNet): We have around
    62.3 million weights, so the optimization problem ends up in <math alttext="double-struck
    upper R Superscript 62.3 m i l l i o n"><msup><mi>ℝ</mi> <mrow><mn>62</mn><mo>.</mo><mn>3</mn><mi>m</mi><mi>i</mi><mi>l</mi><mi>l</mi><mi>i</mi><mi>o</mi><mi>n</mi></mrow></msup></math>
    . Another startling number: We need 1.1 billion computation units for one forward
    pass through the net.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 5-8](#Fig_eight_pass) shows a wonderful illustration of an image of
    the handwritten digit 8 passing through a pre-trained LeNet1 and ultimately getting
    correctly classified as 8.'
  prefs: []
  type: TYPE_NORMAL
- en: '![300](assets/emai_0508.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-8\. Passing an image of a handwritten 8 through a pre-trained LeNet1
    ([image source](https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/)).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Finally, if the choice of architecture seems arbitrary to you, meaning, if
    you are wondering: Could we accomplish similar performance with simpler architecture?
    Then join the club. The whole community is wondering the same thing.'
  prefs: []
  type: TYPE_NORMAL
- en: Summary and Looking Ahead
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we defined the convolution operation: The most significant
    component of convolutional neural networks. Concolutional neural networks are
    essential for computer vision, machine audio processing, and other AI applications.'
  prefs: []
  type: TYPE_NORMAL
- en: We presented convolution from a systems design perspective, then through filtering
    one dimensional and two dimensional signals. We highlighted the linear algebra
    equivalent of the convolution operation (multiplying by matrices of special structure),
    and ended with an example of image classification.
  prefs: []
  type: TYPE_NORMAL
- en: We will encounter convolutional neural networks frequently in this book, as
    they have become a staple in many AI systems that include vision and/or natural
    language.
  prefs: []
  type: TYPE_NORMAL
