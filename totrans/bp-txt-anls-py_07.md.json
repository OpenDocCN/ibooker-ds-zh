["```py\nsvc = SVC(kernel=\"linear\", C=1, probability=True, random_state=42)\nsvc.fit(X_train_tf, Y_train)\n\n```", "```py\nX_test_tf = tfidf.transform(X_test)\nY_pred = svc.predict(X_test_tf)\nresult = pd.DataFrame({ 'text': X_test.values, 'actual': Y_test.values,\n                        'predicted': Y_pred })\n\n```", "```py\nresult[result[\"actual\"] != result[\"predicted\"]].head()\n\n```", "```py\ntext = result.iloc[21][\"text\"]\nprint(text)\n\n```", "```py\nexchange left and right operands for comparison operators changes semantics\nFix for Bug 149803 was not good.; ; The right fix should do the following;\nif --> if --> if ; if ; if\n```", "```py\n svc.predict_proba(X_test_tf[21])\n\n```", "```py\narray([[0.002669, 0.46736578, 0.07725225, 0.00319434, 0.06874877,\n        0.38076986]])\n\n```", "```py\nclass_names = [\"APT\", \"Core\", \"Debug\", \"Doc\", \"Text\", \"UI\"]\nprob = svc.predict_proba(X_test_tf)\n# new dataframe for explainable results\ner = result.copy().reset_index()\nfor c in enumerate(class_names):\n    er[c] = prob[:, i]\n\n```", "```py\ner[[\"actual\", \"predicted\"] + class_names].sample(5, random_state=99)\n\n```", "```py\ner['max_probability'] = er[class_names].max(axis=1)\ncorrect = (er[er['actual'] == er['predicted']])\nwrong   = (er[er['actual'] != er['predicted']])\n\n```", "```py\ncorrect[\"max_probability\"].plot.hist(title=\"Correct\")\nwrong[\"max_probability\"].plot.hist(title=\"Wrong\")\n\n```", "```py\nhigh = er[er[\"max_probability\"] > 0.8]\nprint(classification_report(high[\"actual\"], high[\"predicted\"]))\n\n```", "```py\n                precision    recall  f1-score   support\n\n         APT       0.90      0.75      0.82        12\n        Core       0.94      0.89      0.92       264\n       Debug       0.94      0.99      0.96       202\n         Doc       1.00      0.67      0.80         3\n        Text       0.78      0.75      0.77        72\n          UI       0.90      0.92      0.91       342\n\n    accuracy                           0.91       895\n   macro avg       0.91      0.83      0.86       895\nweighted avg       0.91      0.91      0.91       895\n\n```", "```py\nprint(classification_report(er[\"actual\"], er[\"predicted\"]))\n\n```", "```py\n              precision    recall  f1-score   support\n\n         APT       0.90      0.56      0.69        16\n        Core       0.76      0.77      0.76       546\n       Debug       0.90      0.78      0.84       302\n         Doc       1.00      0.25      0.40        12\n        Text       0.64      0.51      0.57       236\n          UI       0.72      0.82      0.77       699\n\n    accuracy                           0.75      1811\n   macro avg       0.82      0.62      0.67      1811\nweighted avg       0.75      0.75      0.75      1811\n\n```", "```py\nsvc.coef_\n\n```", "```py\n<15x6403 sparse matrix of type '<class 'numpy.float64'>'\n       with 64451 stored elements in Compressed Sparse Row format>\n\n```", "```py\n# coef_[8] yields a matrix, A[0] converts to array and takes first row\ncoef = svc.coef_[8].A[0]\nvocabulary_positions = coef.argsort()\nvocabulary = tfidf.get_feature_names()\n\n```", "```py\ntop_words = 10\ntop_positive_coef = vocabulary_positions[-top_words:].tolist()\ntop_negative_coef = vocabulary_positions[:top_words].tolist()\n\n```", "```py\ncore_ui = pd.DataFrame([[vocabulary[c],\n                  coef[c]] for c in top_positive_coef + top_negative_coef],\n                  columns=[\"feature\", \"coefficient\"]).sort_values(\"coefficient\")\n\n```", "```py\ncore_ui.set_index(\"feature\").plot.barh()\n\n```", "```py\nc = svc.coef_\ncoef = (c[5] + c[6] + c[7] + c[8] - c[0]).A[0]\nvocabulary_positions = coef.argsort()\n\n```", "```py\ntop_words = 20\ntop_positive_coef = vocabulary_positions[-top_words:].tolist()\ntop_negative_coef = vocabulary_positions[:top_words].tolist()\ncore = pd.DataFrame([[vocabulary[c], coef[c]]\n                      for c in top_positive_coef + top_negative_coef], \n                    columns=[\"feature\", \"coefficient\"]).\\\n          sort_values(\"coefficient\")\ncore.set_index(\"feature\").plot.barh(figsize=(6, 10),\n              color=[['red']*top_words + ['green']*top_words])\n\n```", "```py\nfrom sklearn.pipeline import make_pipeline\npipeline = make_pipeline(tfidf, best_model)\n\n```", "```py\npipeline.predict_proba([\"compiler not working\"])\n\n```", "```py\narray([[0.00240522, 0.95605684, 0.00440957, 0.00100242, 0.00971824,\n        0.02640771]])\n\n```", "```py\nfrom lime.lime_text import LimeTextExplainer\nexplainer = LimeTextExplainer(class_names=class_names)\n\n```", "```py\ner[er[\"predicted\"] != er[\"actual\"]].head(5)\n\n```", "```py\nid = 21\nprint('Document id: %d' % id)\nprint('Predicted class =', er.iloc[id][\"predicted\"])\nprint('True class: %s' % er.iloc[id][\"actual\"])\n\n```", "```py\nDocument id: 21\nPredicted class = Core\nTrue class: UI\n\n```", "```py\nexp = explainer.explain_instance(result.iloc[id][\"text\"],\n      pipeline.predict_proba, num_features=10, labels=[1, 5])\nprint('Explanation for class %s' % class_names[1])\nprint('\\n'.join(map(str, exp.as_list(label=1))))\nprint()\nprint('Explanation for class %s' % class_names[5])\nprint('\\n'.join(map(str, exp.as_list(label=5))))\n\n```", "```py\nExplanation for class Core\n('fix', -0.14306948642919184)\n('Bug', 0.14077384623641856)\n('following', 0.11150012169630388)\n('comparison', 0.10122423126000728)\n('Fix', -0.0884162779420967)\n('right', 0.08315255286108318)\n('semantics', 0.08143857054730141)\n('changes', -0.079427782008582)\n('left', 0.03188240169394561)\n('good', -0.0027133756042246504)\n\nExplanation for class UI\n('fix', 0.15069083664026453)\n('Bug', -0.14853911521141774)\n('right', 0.11283930406785869)\n('comparison', -0.10654654371478504)\n('left', -0.10391669738035045)\n('following', -0.1003931859632352)\n('semantics', -0.056644426928774076)\n('Fix', 0.05365037666619837)\n('changes', 0.040806391076561165)\n('good', 0.0401761761717476)\n\n```", "```py\nexp = explainer.explain_instance(result.iloc[id][\"text\"],\n            pipeline.predict_proba, num_features=6, top_labels=3)\nexp.show_in_notebook(text=False)\n\n```", "```py\nfrom lime import submodular_pick\nimport numpy as np\nnp.random.seed(42)\nlsm = submodular_pick.SubmodularPick(explainer, er[\"text\"].values,\n                                        pipeline.predict_proba,\n                                        sample_size=100,\n                                        num_features=20,\n                                        num_exps_desired=5)\n\n```", "```py\nlsm.explanations[0].show_in_notebook()\n\n```", "```py\nfrom sklearn.linear_model import SGDClassifier\nsvm = SGDClassifier(loss='hinge', max_iter=1000, tol=1e-3, random_state=42)\nsvm.fit(X_train_tf, Y_train)\nY_pred_svm = svm.predict(X_test_tf)\nprint(classification_report(Y_test, Y_pred_svm))\n\n```", "```py\n              precision    recall  f1-score   support\n\n         APT       0.89      0.50      0.64        16\n        Core       0.77      0.78      0.77       546\n       Debug       0.85      0.84      0.85       302\n         Doc       0.75      0.25      0.38        12\n        Text       0.62      0.59      0.60       236\n          UI       0.76      0.79      0.78       699\n\n    accuracy                           0.76      1811\n   macro avg       0.77      0.62      0.67      1811\nweighted avg       0.76      0.76      0.76      1811\n\n```", "```py\nimport eli5\neli5.show_weights(svm, top=10, vec=tfidf, target_names=class_names)\n\n```", "```py\neli5.show_prediction(svm, X_test.iloc[21],  vec=tfidf, target_names=class_names)\n\n```", "```py\npython -m spacy download en_core_web_lg\n```", "```py\nnp.random.seed(42)\nexplainer_unk = anchor_text.AnchorText(nlp, class_names, \\\n                use_unk_distribution=True)\n\n```", "```py\ntext = er.iloc[21][\"text\"]\nactual = er.iloc[21][\"actual\"]\n# we want the class with the highest probability and must invert the order\npredicted_class_ids = np.argsort(pipeline.predict_proba([text])[0])[::-1]\npred = explainer_unk.class_names[predicted_class_ids[0]]\nalternative = explainer_unk.class_names[predicted_class_ids[1]]\nprint(f'predicted {pred}, alternative {alternative}, actual {actual}')\n\n```", "```py\npredicted Core, alternative UI, actual UI\n\n```", "```py\nexp_unk = explainer_unk.explain_instance(text, pipeline.predict, threshold=0.95)\n\n```", "```py\nprint(f'Rule: {\" AND \".join(exp_unk.names())}')\nprint(f'Precision: {exp_unk.precision()}')\n\n```", "```py\nRule: following AND comparison AND Bug AND semantics AND for\nPrecision: 0.9865771812080537\n\n```", "```py\nprint(f'Made-up examples where anchor rule matches and model predicts {pred}\\n')\nprint('\\n'.join([x[0] for x in exp_unk.examples(only_same_prediction=True)]))\n\n```", "```py\nMade-up examples where anchor rule matches and model predicts Core\n\nUNK left UNK UNK UNK UNK comparison operators UNK semantics Fix for Bug UNK UNK\nexchange left UNK UNK operands UNK comparison operators changes semantics Fix fo\nexchange UNK and UNK operands UNK comparison UNK UNK semantics UNK for Bug UNK U\nexchange UNK and right UNK for comparison UNK UNK semantics UNK for Bug 149803 U\nUNK left UNK UNK operands UNK comparison UNK changes semantics UNK for Bug 14980\nexchange left UNK right UNK UNK comparison UNK changes semantics Fix for Bug UNK\nUNK UNK and right operands for comparison operators UNK semantics Fix for Bug 14\nUNK left and right operands UNK comparison operators changes semantics UNK for B\nexchange left UNK UNK operands UNK comparison operators UNK semantics UNK for Bu\nUNK UNK UNK UNK operands for comparison operators changes semantics Fix for Bug\n\n```", "```py\nprint(f'Made-up examples where anchor rule matches and model predicts \\\n {alternative}\\n')\nprint('\\n'.join([x[0] for x in exp_unk.examples(partial_index=0, \\\n      only_different_prediction=True)]))\n\n```", "```py\nMade-up examples where anchor rule matches and model predicts UI\n\nexchange left and right UNK for UNK UNK UNK UNK Fix for UNK 149803 was not UNK .\nexchange left UNK UNK UNK for UNK UNK UNK semantics Fix for Bug 149803 UNK not U\nexchange left UNK UNK operands for comparison operators UNK UNK Fix UNK Bug 1498\nexchange left UNK right operands UNK comparison UNK UNK UNK Fix for UNK UNK UNK\nexchange left and right operands UNK UNK operators UNK UNK Fix UNK UNK UNK UNK U\nUNK UNK and UNK UNK UNK comparison UNK UNK UNK Fix for UNK UNK was not good UNK\nexchange left and UNK UNK UNK UNK operators UNK UNK Fix UNK Bug 149803 was not U\nexchange left and right UNK UNK UNK operators UNK UNK UNK for Bug 149803 UNK UNK\nexchange left UNK right UNK for UNK operators changes UNK Fix UNK UNK UNK was no\nUNK left UNK UNK operands UNK UNK operators changes UNK UNK for UNK 149803 was n\n\n```", "```py\nnp.random.seed(42)\nexplainer_no_unk = anchor_text.AnchorText(nlp, class_names,\n                   use_unk_distribution=False, use_bert=False)\nexp_no_unk = explainer_no_unk.explain_instance(text, pipeline.predict,\n             threshold=0.95)\nprint(f'Rule: {\" AND \".join(exp_no_unk.names())}')\nprint(f'Precision: {exp_no_unk.precision()}')\n\n```", "```py\nRule: following AND Bug AND comparison AND semantics AND left AND right\nPrecision: 0.9601990049751243\n\n```", "```py\nExamples where anchor applies and model predicts Core:\n\nexchange left and right suffixes for comparison operators affects semantics NEED\nexchange left and right operands for comparison operators depends semantics UPDA\nexchange left and right operands for comparison operators indicates semantics so\nexchange left and right operands for comparison operators changes semantics Firm\nexchange left and right operands into comparison dispatchers changes semantics F\nexchange left and right operands with comparison operators changes semantics Fix\nexchange left and right operands beyond comparison operators changes semantics M\nexchange left and right operands though comparison representatives changes seman\nexchange left and right operands before comparison operators depends semantics M\nexchange left and right operands as comparison operators changes semantics THING\n\n```", "```py\nExamples where anchor applies and model predicts UI:\n\nexchange left and good operands for comparison operators changes metaphors Fix i\nexchange landed and right operands for comparison supervisors changes derivation\nexchange left and happy operands for correlation operators changes equivalences\nexchange left and right operands for scenario operators changes paradigms Fix be\nexchange left and right operands for trade customers occurs semantics Fix as BoT\nexchange did and right operands than consumer operators changes analogies Instal\nexchange left and few operands for reason operators depends semantics Fix for Bu\nexchange left and right operands for percentage operators changes semantics MESS\nexchange left and right pathnames after comparison operators depends fallacies F\nexchange left and right operands of selection operators changes descriptors Fix\n\n```", "```py\nexp_unk.show_in_notebook()\n\n```"]