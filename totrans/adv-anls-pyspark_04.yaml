- en: Chapter 4\. Making Predictions with Decision Trees and Decision Forests
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第四章。使用决策树和决策森林进行预测
- en: Classification and regression are the oldest and most well-studied types of
    predictive analytics. Most algorithms you will likely encounter in analytics packages
    and libraries are classification or regression techniques, like support vector
    machines, logistic regression, neural networks, and deep learning. The common
    thread linking regression and classification is that both involve predicting one
    (or more) values given one (or more) other values. To do so, both require a body
    of inputs and outputs to learn from. They need to be fed both questions and known
    answers. For this reason, they are known as types of supervised learning.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类和回归是最古老和最研究充分的预测分析类型。在分析软件包和库中，您可能会遇到的大多数算法都是分类或回归技术，如支持向量机、逻辑回归、神经网络和深度学习。联系回归和分类的共同点是，两者都涉及根据一个或多个其他值预测一个（或多个）值。为此，两者都需要一系列输入和输出进行学习。它们需要被提供问题和已知答案。因此，它们被称为监督学习类型。
- en: PySpark MLlib offers implementations of a number of classification and regression
    algorithms. These include decision trees, naïve Bayes, logistic regression, and
    linear regression. The exciting thing about these algorithms is that they can
    help predict the future—or at least, predict the things we don’t yet know for
    sure, like the likelihood you will buy a car based on your online behavior, whether
    an email is spam given the words it contains, or which acres of land are likely
    to grow the most crops given their location and soil chemistry.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark MLlib 提供了多种分类和回归算法的实现。这些包括决策树、朴素贝叶斯、逻辑回归和线性回归。这些算法的令人兴奋之处在于它们可以帮助预测未来——或者至少可以预测我们尚不确定的事情，例如基于您的在线行为预测您购买汽车的可能性，给定其包含的单词判断一封电子邮件是否为垃圾，或者哪些土地可能根据其位置和土壤化学成分长出最多的作物。
- en: In this chapter, we will focus on a popular and flexible type of algorithm for
    both classification and regression (decision trees) and the algorithm’s extension
    (random decision forests). First, we will understand the basics of decision trees
    and forests and introduce the former’s PySpark implementation. The PySpark implementation
    of decision trees supports binary and multiclass classification, and regression.
    The implementation partitions data by rows, allowing distributed training with
    millions or even billions of instances. This will be followed by preparation of
    our dataset and creating our first decision tree. Then we’ll tune our decision
    tree model. We’ll finish up by training a random forest model on our processed
    dataset and making predictions.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将重点介绍一种用于分类和回归的流行且灵活的算法（决策树），以及该算法的扩展（随机决策森林）。首先，我们将了解决策树和森林的基础，并介绍前者的
    PySpark 实现。决策树的 PySpark 实现支持二元和多类分类以及回归。该实现通过行分割数据，允许使用数百万甚至数十亿个实例进行分布式训练。接下来是数据集的准备和第一棵决策树的创建。然后我们将调整我们的决策树模型。最后，我们将在处理过的数据集上训练一个随机森林模型并进行预测。
- en: Although PySpark’s decision tree implementation is easy to get started with,
    it is helpful to understand the fundamentals of decision tree and random forest
    algorithms. That is what we’ll cover in the next section.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 PySpark 的决策树实现很容易上手，但理解决策树和随机森林算法的基础是非常有帮助的。这是我们下一节要讨论的内容。
- en: Decision Trees and Forests
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 决策树和森林
- en: '*Decision trees* are a family of algorithms that can naturally handle both
    categorical and numeric features. Building a single tree can be done using parallel
    computing, and many trees can be built in parallel at once. They are robust to
    outliers in the data, meaning that a few extreme and possibly erroneous data points
    might not affect predictions at all. They can consume data of different types
    and on different scales without the need for preprocessing or normalization.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*决策树* 是一类算法，可以自然地处理分类和数值特征。可以使用并行计算构建单棵树，并且可以同时并行构建多棵树。它们对数据中的异常值具有鲁棒性，这意味着少数极端和可能错误的数据点可能根本不会影响预测。它们可以处理不同类型和不同尺度的数据，而无需预处理或归一化。'
- en: 'Decision tree–based algorithms have the advantage of being comparatively intuitive
    to understand and reason about. In fact, we all probably use the same reasoning
    embodied in decision trees, implicitly, in everyday life. For example, I sit down
    to have morning coffee with milk. Before I commit to that milk and add it to my
    brew, I want to predict: is the milk spoiled? I don’t know for sure. I might check
    if the use-by date has passed. If not, I predict no, it’s not spoiled. If the
    date has passed, but it was three or fewer days ago, I take my chances and predict
    no, it’s not spoiled. Otherwise, I sniff the milk. If it smells funny, I predict
    yes, and otherwise no.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 基于决策树的算法具有相对直观和理解的优势。实际上，我们在日常生活中可能都在隐式地使用决策树体现的相同推理方式。例如，我坐下来喝带有牛奶的早晨咖啡。在我决定使用这份牛奶之前，我想要预测：这牛奶是否变质了？我不能确定。我可能会检查是否过期日期已过。如果没有，我预测不会变质。如果日期过了，但是在三天内，我会冒险预测不会变质。否则，我会闻一闻这牛奶。如果闻起来有点怪，我预测会变质，否则预测不会。
- en: This series of yes/no decisions that leads to a prediction are what decision
    trees embody. Each decision leads to one of two results, which is either a prediction
    or another decision, as shown in [Figure 4-1](#MilkDecisionTree). In this sense,
    it is natural to think of the process as a tree of decisions, where each internal
    node in the tree is a decision, and each leaf node is a final answer.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 这一系列的是/否决策导致了一个预测，这正是决策树体现的内容。每个决策导致两种结果之一，即预测或另一个决策，如[图 4-1](#MilkDecisionTree)所示。从这个意义上说，将这个过程视为决策树是很自然的，其中树的每个内部节点都是一个决策，每个叶节点都是一个最终答案。
- en: That is a simplistic decision tree and was not built with any rigor. To elaborate,
    consider another example. A robot has taken a job in an exotic pet store. It wants
    to learn, before the shop opens, which animals in the shop would make a good pet
    for a child. The owner lists nine pets that would and wouldn’t be suitable before
    hurrying off. The robot compiles the information found in [Table 4-1](#Pet_Stats)
    from examining the animals.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个简单的决策树，没有经过严格构建。为了详细说明，考虑另一个例子。一个机器人在一家异国情调的宠物店找了份工作。它希望在店铺开门前了解哪些动物适合孩子作为宠物。店主匆匆列出了九只适合和不适合的宠物，然后匆忙离去。机器人根据观察到的信息从[表 4-1](#Pet_Stats)中整理了这些动物的特征向量。
- en: '![aaps 0401](assets/aaps_0401.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![aaps 0401](assets/aaps_0401.png)'
- en: 'Figure 4-1\. Decision tree: is milk spoiled?'
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-1\. 决策树：牛奶是否变质？
- en: Table 4-1\. Exotic pet store “feature vectors”
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4-1\. 异国情调宠物店的“特征向量”
- en: '| Name | Weight (kg) | # Legs | Color | Good pet? |'
  id: totrans-13
  prefs: []
  type: TYPE_TB
  zh: '| 名称 | 重量（公斤） | # 腿 | 颜色 | 适合作宠物？ |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Fido | 20.5 | 4 | Brown | Yes |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '| 菲多 | 20.5 | 4 | 棕色 | 是 |'
- en: '| Mr. Slither | 3.1 | 0 | Green | No |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| 斯莱瑟先生 | 3.1 | 0 | 绿色 | 否 |'
- en: '| Nemo | 0.2 | 0 | Tan | Yes |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| 尼莫 | 0.2 | 0 | 棕色 | 是 |'
- en: '| Dumbo | 1390.8 | 4 | Gray | No |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| 邓波 | 1390.8 | 4 | 灰色 | 否 |'
- en: '| Kitty | 12.1 | 4 | Gray | Yes |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| 基蒂 | 12.1 | 4 | 灰色 | 是 |'
- en: '| Jim | 150.9 | 2 | Tan | No |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| 吉姆 | 150.9 | 2 | 棕色 | 否 |'
- en: '| Millie | 0.1 | 100 | Brown | No |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| 米莉 | 0.1 | 100 | 棕色 | 否 |'
- en: '| McPigeon | 1.0 | 2 | Gray | No |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| 麦克鸽 | 1.0 | 2 | 灰色 | 否 |'
- en: '| Spot | 10.0 | 4 | Brown | Yes |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| 斯波特 | 10.0 | 4 | 棕色 | 是 |'
- en: The robot can make a decision for the nine listed pets. There are many more
    pets available in the store. It still needs a methodology for deciding which animals
    among the rest will be suitable as pets for kids. We can assume that the characteristics
    of all animals are available. Using the decision data provided by the store owner
    and a decision tree, we can help the robot learn what a good pet for a kid looks
    like.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 机器人可以为列出的九只宠物做出决策。店里还有更多的宠物可供选择。它仍然需要一个方法来决定哪些动物适合孩子作宠物。我们可以假设店里所有动物的特征都是可用的。使用店主提供的决策数据和一个决策树，我们可以帮助机器人学习什么样的动物适合孩子作宠物。
- en: Although a name is given, it will not be included as a feature in our decision
    tree model. There is little reason to believe the name alone is predictive; “Felix”
    could name a cat or a poisonous tarantula, for all the robot knows. So, there
    are two numeric features (weight, number of legs) and one categorical feature
    (color) predicting a categorical target (is/is not a good pet for a child.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然给出了一个名字，但不会将其作为我们决策树模型的特征包括进去。凭名字预测的依据有限；“菲利克斯”可能是一只猫，也可能是一只有毒的塔兰图拉蜘蛛。因此，有两个数值特征（重量、腿的数量）和一个分类特征（颜色）预测一个分类目标（适合/不适合孩子作宠物）。
- en: The way a decision tree works is by making one or more decisions in sequence
    based on provided features. To start off, the robot might try to fit a simple
    decision tree to this training data, consisting of a single decision based on
    weight, as shown in [Figure 4-2](#PetStoreDecisionTree1).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树的工作方式是基于提供的特征进行一个或多个顺序决策。首先，机器人可能会尝试将一个简单的决策树拟合到这些训练数据中，这棵树只有一个基于重量的决策，如在[图4-2](#PetStoreDecisionTree1)中所示。
- en: '![aaps 0402](assets/aaps_0402.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![aaps 0402](assets/aaps_0402.png)'
- en: Figure 4-2\. Robot’s first decision tree
  id: totrans-28
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-2\. 机器人的第一个决策树
- en: 'The logic of the decision tree is easy to read and make sense of: 500kg animals
    certainly sound unsuitable as pets. This rule predicts the correct value in five
    of nine cases. A quick glance suggests that we could improve the rule by lowering
    the weight threshold to 100kg. This gets six of nine examples correct. The heavy
    animals are now predicted correctly; the lighter animals are only partly correct.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树的逻辑易于理解和理解：500kg的动物听起来确实不适合作为宠物。这个规则在九个案例中预测了五次正确的值。快速浏览表明，我们可以通过将重量阈值降低到100kg来改进规则。这样可以在九个示例中正确预测六次。现在重的动物被正确预测了；轻的动物只部分正确。
- en: So, a second decision can be constructed to further refine the prediction for
    examples with weights less than 100kg. It would be good to pick a feature that
    changes some of the incorrect Yes predictions to No. For example, there is one
    small green animal, sounding suspiciously like a snake, that will be classified
    by our current model as a suitable pet candidate. The robot could predict correctly
    by adding a decision based on color, as shown in [Figure 4-3](#PetStoreDecisionTree2).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，可以构建第二个决策来进一步细化对重量小于100kg的示例的预测。选择一个可以将一些不正确的是预测改为不的特征是一个好主意。例如，有一种小的绿色动物，听起来可疑地像蛇，将被我们当前的模型分类为适合的宠物候选者。通过添加基于颜色的决策，机器人可以正确预测，如在[图4-3](#PetStoreDecisionTree2)中所示。
- en: '![aaps 0403](assets/aaps_0403.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![aaps 0403](assets/aaps_0403.png)'
- en: Figure 4-3\. Robot’s next decision tree
  id: totrans-32
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-3\. 机器人的下一个决策树
- en: 'Now, seven of nine examples are correct. Of course, decision rules could be
    added until all nine were correctly predicted. The logic embodied in the resulting
    decision tree would probably sound implausible when translated into common speech:
    “If the animal’s weight is less than 100kg, its color is brown instead of green,
    and it has fewer than 10 legs, then yes, it is a suitable pet.” While perfectly
    fitting the given examples, a decision tree like this would fail to predict that
    a small, brown, four-legged wolverine is not a suitable pet. Some balance is needed
    to avoid this phenomenon, known as *overfitting*.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，九个例子中有七个是正确的。当然，可以添加决策规则，直到所有九个都被正确预测。生成的决策树所体现的逻辑在转化为通俗的语言时可能听起来不太可信：“如果动物的重量小于100kg，它的颜色是棕色而不是绿色，并且它的腿少于10条，那么是，它是一个适合的宠物。”虽然完全符合给定的例子，但这样的决策树在预测小型、棕色、四条腿的狼獾不适合作为宠物时会失败。需要一些平衡来避免这种现象，称为*过拟合*。
- en: Decision trees generalize into a more powerful algorithm, called *random forests*.
    Random forests combine many decision trees to reduce the risk of overfitting and
    train the decision trees separately. The algorithm injects randomness into the
    training process so that each decision tree is a bit different. Combining the
    predictions reduces the variance of the predictions, makes the resulting model
    more generalizable, and improves performance on test data.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树推广为更强大的算法，称为*随机森林*。随机森林结合了许多决策树，以减少过拟合的风险，并单独训练决策树。该算法通过在训练过程中引入随机性，使每棵决策树略有不同。结合预测结果降低了预测的方差，使得生成的模型更具泛化能力，并提高了在测试数据上的表现。
- en: This is enough of an introduction to decision trees and random forests for us
    to begin using them with PySpark. In the next section, we will introduce the dataset
    that we’ll work with and prepare it for use in PySpark.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这已经足够介绍决策树和随机森林，我们将在PySpark中开始使用它们。在下一节中，我们将介绍我们将在PySpark中使用的数据集，并为其准备数据。
- en: Preparing the Data
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备数据
- en: The dataset used in this chapter is the well-known Covtype dataset, available
    [online](https://oreil.ly/spUWl) as a compressed CSV-format data file, *covtype.data.gz*,
    and accompanying info file, *covtype.info*.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 本章使用的数据集是著名的Covtype数据集，可在[网上](https://oreil.ly/spUWl)获取，以压缩的CSV格式数据文件*covtype.data.gz*和配套的信息文件*covtype.info*。
- en: The dataset records the types of forest-covered parcels of land in Colorado,
    USA. It’s only a coincidence that the dataset concerns real-world forests! Each
    data record contains several features describing each parcel of land—like its
    elevation, slope, distance to water, shade, and soil type—along with the known
    forest type covering the land. The forest cover type is to be predicted from the
    rest of the features, of which there are 54 in total.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集记录了美国科罗拉多州森林覆盖地块的类型。这个数据集关注现实世界的森林只是巧合！每个数据记录包含描述每块土地的几个特征，比如海拔、坡度、到水源的距离、阴影和土壤类型，以及覆盖该土地的已知森林类型。需要从其余的特征中预测森林覆盖类型，总共有54个特征。
- en: This dataset has been used in research and even a [Kaggle competition](https://oreil.ly/LpjgW).
    It is an interesting dataset to explore in this chapter because it contains both
    categorical and numeric features. There are 581,012 examples in the dataset, which
    does not exactly qualify as big data but is large enough to be manageable as an
    example and still highlight some issues of scale.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据集已经被用于研究，甚至是一个 [Kaggle 竞赛](https://oreil.ly/LpjgW)。这是一个有趣的数据集，在这一章中探索它是因为它包含分类和数值特征。数据集中有581,012个例子，虽然不完全符合大数据的定义，但足够作为一个例子管理，并且仍然突出了一些规模问题。
- en: Thankfully, the data is already in a simple CSV format and does not require
    much cleansing or other preparation to be used with PySpark MLlib. The *covtype.data*
    file should be extracted and copied into your local or cloud storage (such as
    AWS S3).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，数据已经以简单的CSV格式存在，并且不需要太多的清洗或其他准备工作即可与PySpark MLlib一起使用。 *covtype.data* 文件应该被提取并复制到您的本地或云存储（如AWS
    S3）中。
- en: Start `pyspark-shell`. You may find it helpful to give the shell a healthy amount
    of memory to work with, as building decision forests can be resource intensive.
    If you have the memory, specify `--driver-memory 8g` or similar.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 启动 `pyspark-shell`。如果你有足够的内存，指定`--driver-memory 8g`或类似的参数可能会有所帮助，因为构建决策森林可能会消耗大量资源。
- en: CSV files contain fundamentally tabular data, organized into rows of columns.
    Sometimes these columns are given names in a header line, although that’s not
    the case here. The column names are given in the companion file, *covtype.info*.
    Conceptually, each column of a CSV file has a type as well—a number, a string—but
    a CSV file doesn’t specify this.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: CSV文件包含基本的表格数据，组织成行和列。有时这些列在标题行中有名称，尽管这在这里不是这样。列名在配套文件 *covtype.info* 中给出。在概念上，CSV文件的每一列也有一个类型——数字、字符串——但CSV文件并未指定这一点。
- en: 'It’s natural to parse this data as a dataframe because this is PySpark’s abstraction
    for tabular data, with a defined column schema, including column names and types.
    PySpark has built-in support for reading CSV data. Let’s read our dataset as a
    DataFrame using the built-in CSV reader:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 将这些数据解析为数据框是很自然的，因为这是PySpark对表格数据的抽象，具有定义的列模式，包括列名和类型。 PySpark内置支持读取CSV数据。让我们使用内置的CSV读取器将我们的数据集读取为DataFrame：
- en: '[PRE0]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This code reads the input as CSV and does not attempt to parse the first line
    as a header of column names. It also requests that the type of each column be
    inferred by examining the data. It correctly infers that all of the columns are
    numbers, and, more specifically, integers. Unfortunately, it can name the columns
    only `_c0` and so on.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码将输入作为CSV读取，并且不试图解析第一行作为列名的标题。它还请求通过检查数据来推断每列的类型。它正确地推断出所有列都是数字，更具体地说是整数。不幸的是，它只能将列命名为
    `_c0` 等。
- en: We can look at the *covtype.info* file for the column names.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以查看 *covtype.info* 文件获取列名。
- en: '[PRE1]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Looking at the column information, it’s clear that some features are indeed
    numeric. `Elevation` is an elevation in meters; `Slope` is measured in degrees.
    However, `Wilderness_Area` is something different, because it is said to span
    four columns, each of which is a 0 or 1\. In reality, `Wilderness_Area` is a categorical
    value, not a numeric one.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 查看列信息时，显然有些特征确实是数值型的。 `Elevation` 是以米为单位的海拔；`Slope` 是以度为单位的。然而，`Wilderness_Area`
    是另一回事，因为它据说跨越四列，每列是0或1。实际上，`Wilderness_Area` 是一个分类值，而不是数值。
- en: These four columns are actually a one-hot or 1-of-N encoding. When this form
    of encoding is performed on a categorical feature, one categorical feature that
    takes on *N* distinct values becomes *N* numeric features, each taking on the
    value 0 or 1\. Exactly one of the *N* values has value 1, and the others are 0\.
    For example, a categorical feature for weather that can be `cloudy`, `rainy`,
    or `clear` would become three numeric features, where `cloudy` is represented
    by `1,0,0`; `rainy` by `0,1,0`; and so on. These three numeric features might
    be thought of as `is_cloudy`, `is_rainy`, and `is_clear` features. Likewise, 40
    of the columns are really one `Soil_Type` categorical feature.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这四列实际上是一种独热或 1-of-N 编码。当对分类特征执行这种编码时，一个分类特征，它有 *N* 个不同的值，就变成了 *N* 个数值特征，每个特征的值为
    0 或 1。这 *N* 个值中恰好有一个值为 1，其余为 0。例如，一个可以是 `cloudy`、`rainy` 或 `clear` 的天气分类特征会变成三个数值特征，其中
    `cloudy` 由 `1,0,0` 表示，`rainy` 由 `0,1,0` 表示，依此类推。这三个数值特征可以被视为 `is_cloudy`、`is_rainy`
    和 `is_clear` 特征。同样，其他 40 列实际上是一个 `Soil_Type` 分类特征。
- en: This isn’t the only possible way to encode a categorical feature as a number.
    Another possible encoding simply assigns a distinct numeric value to each possible
    value of the categorical feature. For example, `cloudy` may become 1.0, `rainy`
    2.0, and so on. The target itself, `Cover_Type`, is a categorical value encoded
    as a value 1 to 7.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 编码分类特征为数字并不是唯一的方法。另一种可能的编码方式是，将分类特征的每个可能值分配一个不同的数值。例如，`cloudy` 可以变成 1.0，`rainy`
    变成 2.0，依此类推。目标本身 `Cover_Type` 是一个被编码为 1 到 7 的分类值。
- en: Be careful when encoding a categorical feature as a single numeric feature.
    The original categorical values have no ordering, but when encoded as a number,
    they appear to. Treating the encoded feature as numeric leads to meaningless results
    because the algorithm is effectively pretending that `rainy` is somehow greater
    than, and two times larger than, `cloudy`. It’s OK as long as the encoding’s numeric
    value is not used as a number.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在将分类特征编码为单个数值特征时要小心。原始的分类值没有顺序，但是当编码为数字时，它们似乎有了顺序。将编码特征视为数值会导致毫无意义的结果，因为算法实际上是假装
    `rainy` 比 `cloudy` 更大，并且是 `cloudy` 的两倍大。只要编码的数值不被用作数字，这是可以接受的。
- en: We have seen both types of encodings of categorical features. It would have,
    perhaps, been simpler and more straightforward to not encode such features (and
    in two ways, no less) and to instead simply include their values directly, like
    “Rawah Wilderness Area.” This may be an artifact of history; the dataset was released
    in 1998\. For performance reasons or to match the format expected by libraries
    of the day, which were built more for regression problems, datasets often contain
    data encoded in these ways.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到了分类特征的两种编码类型。也许，如果不编码这些特征（而且是两种方式），而是直接包含它们的值，像“Rawah Wilderness Area”，可能会更简单和更直接。这可能是历史的遗留；数据集发布于
    1998 年。出于性能原因或者为了与当时更多用于回归问题的库的期望格式相匹配，数据集经常包含以这些方式编码的数据。
- en: 'In any event, before proceeding, it is useful to add column names to this DataFrame
    to make it easier to work with:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 无论如何，在继续之前，给这个 DataFrame 添加列名是很有用的，以便更容易地处理它：
- en: '[PRE2]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[![1](assets/1.png)](#co_making_predictions_with_decision_trees___span_class__keep_together__and_decision_forests__span__CO1-1)'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_making_predictions_with_decision_trees___span_class__keep_together__and_decision_forests__span__CO1-1)'
- en: + concatenates collections.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: + 连接集合。
- en: The wilderness- and soil-related columns are named `Wilderness_Area_0`, `Soil_Type_0`,
    etc., and a bit of Python can generate these 44 names without having to type them
    all out. Finally, the target `Cover_Type` column is cast to a `double` value up
    front, because it will actually be necessary to consume it as a `double` rather
    than `int` in all PySpark MLlib APIs. This will become apparent later.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 与荒野和土壤相关的列被命名为 `Wilderness_Area_0`、`Soil_Type_0` 等，一点点 Python 可以生成这些 44 个名称，而不必逐个打出。最后，目标
    `Cover_Type` 列被提前转换为 `double` 值，因为在所有 PySpark MLlib API 中实际上需要将其作为 `double` 而不是
    `int` 来使用。这将在稍后变得明显。
- en: You can call `data.show` to see some rows of the dataset, but the display is
    so wide that it will be difficult to read it all. `data.head` displays it as a
    raw `Row` object, which will be more readable in this case.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以调用 `data.show` 来查看数据集的一些行，但是显示的宽度非常宽，可能会难以阅读全部。`data.head` 将其显示为原始的 `Row`
    对象，在这种情况下更易读。
- en: Now that we’re familiar with our dataset and have processed it, we can train
    a decision tree model.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们熟悉了数据集并且已经处理过了，我们可以训练一个决策树模型。
- en: Our First Decision Tree
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我们的第一个决策树
- en: 'In [Chapter 3](ch03.xhtml#recommending_music_and_the_audioscrobbler_data_set),
    we built a recommender model right away on all of the available data. This created
    a recommender that could be sense-checked by anyone with some knowledge of music:
    looking at a user’s listening habits and recommendations, we got some sense that
    it was producing good results. Here, that is not possible. We would have no idea
    how to make up a 54-feature description of a new parcel of land in Colorado, or
    what kind of forest cover to expect from such a parcel.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第三章](ch03.xhtml#recommending_music_and_the_audioscrobbler_data_set)，我们立即在所有可用数据上构建了一个推荐模型。这创建了一个可以由任何对音乐有一定了解的人进行审查的推荐器：通过查看用户的听歌习惯和推荐，我们感觉到它产生了良好的结果。在这里，这是不可能的。我们无法想象如何为科罗拉多州的一块新地块编写一个包含54个特征的描述，或者期望从这样一个地块获得什么样的森林覆盖。
- en: 'Instead, we must jump straight to holding out some data for purposes of evaluating
    the resulting model. Before, the AUC metric was used to assess the agreement between
    held-out listening data and predictions from recommendations. AUC may be viewed
    as the probability that a randomly chosen good recommendation ranks above a randomly
    chosen bad recommendation. The principle is the same here, although the evaluation
    metric will be different: *accuracy*. The majority—90%—of the data will again
    be used for training, and, later, we’ll see that a subset of this training set
    will be held out for cross-validation (the CV set). The other 10% held out here
    is actually a third subset, a proper test set.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，我们必须直接跳到保留一些数据来评估生成的模型。之前，使用AUC指标来评估保留的听觉数据与推荐预测之间的一致性。AUC可以视为随机选择的好推荐优于随机选择的坏推荐的概率。这里的原则是相同的，尽管评估指标将会不同：*准确度*。大部分——90%——的数据将再次用于训练，稍后，我们将看到这个训练集的一个子集将被保留用于交叉验证（CV集）。这里保留的另外10%实际上是第三个子集，一个适当的测试集。
- en: '[PRE3]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The data needs a little more preparation to be used with a classifier in MLlib.
    The input DataFrame contains many columns, each holding one feature that could
    be used to predict the target column. MLlib requires all of the inputs to be collected
    into *one* column, whose value is a vector. PySpark’s `VectorAssembler` class
    is an abstraction for vectors in the linear algebra sense and contains only numbers.
    For most intents and purposes, they work like a simple array of `double` values
    (floating-point numbers). Of course, some of the input features are conceptually
    categorical, even if they’re all represented with numbers in the input.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 数据需要更多的准备工作才能与MLlib中的分类器一起使用。输入DataFrame包含许多列，每一列都包含一个特征，可以用来预测目标列。MLlib要求所有输入都收集到*一个*列中，其值是一个向量。PySpark的`VectorAssembler`类是在线性代数意义上向量的抽象，只包含数字。对于大多数意图和目的来说，它们工作起来就像一个简单的`double`值数组（浮点数）。当然，输入特征中有一些在概念上是分类的，即使它们在输入中都用数字表示。
- en: 'Fortunately, the `VectorAssembler` class can do this work:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，`VectorAssembler`类可以完成这项工作：
- en: '[PRE4]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[![1](assets/1.png)](#co_making_predictions_with_decision_trees___span_class__keep_together__and_decision_forests__span__CO2-1)'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_making_predictions_with_decision_trees___span_class__keep_together__and_decision_forests__span__CO2-1)'
- en: Excludes the label, Cover_Type
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 排除标签，Cover_Type
- en: The key parameters of `VectorAssembler` are the columns to combine into the
    feature vector, and the name of the new column containing the feature vector.
    Here, all columns—*except* the target, of course—are included as input features.
    The resulting DataFrame has a new `featureVector` column, as shown.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '`VectorAssembler`的关键参数是要合并成特征向量的列以及包含特征向量的新列的名称。在这里，所有列——当然除了目标列——都包含为输入特征。结果DataFrame有一个新的`featureVector`列，如所示。'
- en: The output doesn’t look exactly like a sequence of numbers, but that’s because
    this shows a raw representation of the vector, represented as a `SparseVector`
    instance to save storage. Because most of the 54 values are 0, it stores only
    nonzero values and their indices. This detail won’t matter in classification.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 输出看起来不完全像一系列数字，但这是因为它显示了一个原始的向量表示，表示为一个`sparseVector`实例以节省存储空间。因为大多数的54个值都是0，它只存储非零值及其索引。在分类中，这些细节并不重要。
- en: '`VectorAssembler` is an example of `Transformer` within the current MLlib Pipelines
    API. It transforms the input DataFrame into another DataFrame based on some logic,
    and is composable with other transformations into a pipeline. Later in this chapter,
    these transformations will be connected into an actual `Pipeline`. Here, the transformation
    is just invoked directly, which is sufficient to build a first decision tree classifier
    model:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '`VectorAssembler` 是当前 MLlib Pipelines API 中 `Transformer` 的一个示例。它根据某些逻辑将输入的
    DataFrame 转换为另一个 DataFrame，并且可以与其他转换组合成管道。在本章后面，这些转换将被连接成一个实际的 `Pipeline`。在这里，转换只是直接调用，这已足以构建第一个决策树分类器模型：'
- en: '[PRE5]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Again, the essential configuration for the classifier consists of column names:
    the column containing the input feature vectors and the column containing the
    target value to predict. Because the model will later be used to predict new values
    of the target, it is given the name of a column to store predictions.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，分类器的基本配置包括列名：包含输入特征向量的列和包含目标值以预测的列。因为模型将稍后用于预测目标的新值，所以给定了一个列名来存储预测。
- en: Printing a representation of the model shows some of its tree structure. It
    consists of a series of nested decisions about features, comparing feature values
    to thresholds. (Here, for historical reasons, the features are referred to only
    by number, not name, unfortunately.)
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 打印模型的表示形式显示了其树结构的一部分。它由一系列关于特征的嵌套决策组成，比较特征值与阈值。（在这里，由于历史原因，特征仅仅通过编号而不是名称来引用，这是个不幸的情况。）
- en: 'Decision trees are able to assess the importance of input features as part
    of their building process. That is, they can estimate how much each input feature
    contributes to making correct predictions. This information is simple to access
    from the model:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树能够在构建过程中评估输入特征的重要性。也就是说，它们可以估计每个输入特征对于做出正确预测的贡献。可以从模型中简单地访问这些信息：
- en: '[PRE6]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This pairs importance values (higher is better) with column names and prints
    them in order from most to least important. Elevation seems to dominate as the
    most important feature; most features are estimated to have virtually no importance
    when predicting the cover type!
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这些重要性值（数值越高越好）与列名配对，并按重要性从高到低的顺序打印出来。海拔似乎是最重要的特征；大多数特征在预测覆盖类型时被估计几乎没有任何重要性！
- en: The resulting `DecisionTreeClassificationModel` is itself a transformer because
    it can transform a dataframe containing feature vectors into a dataframe also
    containing predictions.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的 `DecisionTreeClassificationModel` 本身也是一个转换器，因为它可以将包含特征向量的 dataframe 转换为另一个包含预测的
    dataframe。
- en: 'For example, it might be interesting to see what the model predicts on the
    training data and compare its prediction with the known correct cover type:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，看看模型在训练数据上的预测，并将其预测与已知的正确覆盖类型进行比较可能很有趣：
- en: '[PRE7]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Interestingly, the output also contains a `probability` column that gives the
    model’s estimate of how likely it is that each possible outcome is correct. This
    shows that in these instances, it’s fairly sure the answer is 3 in several cases
    and quite sure the answer isn’t 1.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，输出还包含一个 `probability` 列，给出模型对每种可能结果正确性的估计。这表明在这些实例中，模型相当确定答案是 3，并且相当确定答案不是
    1。
- en: Eagle-eyed readers might note that the probability vectors actually have eight
    values even though there are only seven possible outcomes. The vector’s values
    at indices 1 to 7 do contain the probability of outcomes 1 to 7\. However, there
    is also a value at index 0, which always shows as probability 0.0\. This can be
    ignored, as 0 isn’t even a valid outcome, as this says. It’s a quirk of representing
    this information as a vector that’s worth being aware of.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 细心的读者可能会注意到，概率向量实际上有八个值，尽管只有七种可能的结果。索引为 1 到 7 的向量值包含了对应结果 1 到 7 的概率。然而，索引为 0
    的值始终显示为概率 0.0。这可以忽略，因为 0 不是一个有效的结果，正如这里所说的。这是表示这些信息为向量的一种特殊方式，值得注意。
- en: Based on the above snippet, it looks like the model could use some work. Its
    predictions look like they are often wrong. As with the ALS implementation in
    [Chapter 3](ch03.xhtml#recommending_music_and_the_audioscrobbler_data_set), the
    `DecisionTreeClassifier` implementation has several hyperparameters for which
    a value must be chosen, and they’ve all been left to defaults here. Here, the
    test set can be used to produce an unbiased evaluation of the expected accuracy
    of a model built with these default hyperparameters.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 根据上述片段，模型似乎需要改进。其预测结果经常是错误的。与[第三章](ch03.xhtml#recommending_music_and_the_audioscrobbler_data_set)中的ALS实现一样，`DecisionTreeClassifier`的实现有几个超参数需要选择数值，并且这些都被默认留在这里。在这里，测试集可用于对使用这些默认超参数构建的模型的预期准确性进行无偏评估。
- en: 'We will now use `MulticlassClassificationEvaluator` to compute accuracy and
    other metrics that evaluate the quality of the model’s predictions. It’s an example
    of an evaluator in MLlib, which is responsible for assessing the quality of an
    output DataFrame in some way:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将使用 `MulticlassClassificationEvaluator` 来计算准确性和其他评估模型预测质量的指标。这是 MLlib 中评估器的一个示例，负责以某种方式评估输出
    DataFrame 的质量：
- en: '[PRE8]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: After being given the column containing the “label” (target, or known correct
    output value) and the name of the column containing the prediction, it finds that
    the two match about 70% of the time. This is the accuracy of this classifier.
    It can compute other related measures, like the F1 score. For our purposes here,
    accuracy will be used to evaluate classifiers.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在给定包含“标签”（目标或已知正确输出值）的列和包含预测的列名之后，它发现两者大约有 70% 的匹配率。这就是该分类器的准确性。它还可以计算其他相关的度量，如
    F1 分数。在这里，准确性将用于评估分类器。
- en: This single number gives a good summary of the quality of the classifier’s output.
    Sometimes, however, it can be useful to look at the *confusion matrix*. This is
    a table with a row and a column for every possible value of the target. Because
    there are seven target category values, this is a 7×7 matrix, where each row corresponds
    to an actual correct value, and each column to a predicted value, in order. The
    entry at row *i* and column *j* counts the number of times an example with true
    category *i* was predicted as category *j*. So, the correct predictions are the
    counts along the diagonal, and the predictions are everything else.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这个单一数字很好地总结了分类器输出的质量。然而，有时查看*混淆矩阵*也很有用。这是一个表格，其中每个可能的目标值都有一行和一列。因为有七个目标类别值，所以这是一个
    7×7 的矩阵，其中每行对应一个实际正确值，每列对应一个预测值，按顺序排列。在第 *i* 行和第 *j* 列的条目计算了真实类别为 *i* 的示例被预测为类别
    *j* 的次数。因此，正确预测是对角线上的计数，而其他都是预测值。
- en: It’s possible to calculate a confusion matrix directly with the DataFrame API,
    using its more general operators.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 可以直接使用 DataFrame API 计算混淆矩阵，利用其更通用的操作符。
- en: '[PRE9]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[![1](assets/1.png)](#co_making_predictions_with_decision_trees___span_class__keep_together__and_decision_forests__span__CO3-1)'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_making_predictions_with_decision_trees___span_class__keep_together__and_decision_forests__span__CO3-1)'
- en: Replace null with 0.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 将 null 替换为 0。
- en: Spreadsheet users may have recognized the problem as just like that of computing
    a pivot table. A pivot table groups values by two dimensions, whose values become
    rows and columns of the output, and computes some aggregation within those groupings,
    like a count here. This is also available as a PIVOT function in several databases
    and is supported by Spark SQL. It’s arguably more elegant and powerful to compute
    it this way.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 电子表格用户可能已经意识到这个问题，就像计算数据透视表一样。数据透视表根据两个维度对值进行分组，这些值成为输出的行和列，并在这些分组内计算一些聚合值，例如这里的计数。这也可以作为几个数据库中的
    PIVOT 函数，并得到 Spark SQL 支持。这种方法计算起来可能更加优雅和强大。
- en: Although 70% accuracy sounds decent, it’s not immediately clear whether it is
    outstanding or poor. How well would a simplistic approach do to establish a baseline?
    Just as a broken clock is correct twice a day, randomly guessing a classification
    for each example would also occasionally produce the correct answer.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 70% 的准确率听起来还不错，但并不清楚它是优秀还是较差。使用简单方法建立一个基准，能有多好呢？就像一块坏了的时钟每天正确两次一样，对每个示例随机猜测一个分类也偶尔会得到正确答案。
- en: 'We could construct such a random “classifier” by picking a class at random
    in proportion to its prevalence in the training set. For example, if 30% of the
    training set were cover type 1, then the random classifier would guess “1” 30%
    of the time. Each classification would be correct in proportion to its prevalence
    in the test set. If 40% of the test set were cover type 1, then guessing “1” would
    be correct 40% of the time. Cover type 1 would then be guessed correctly 30% x
    40% = 12% of the time and contribute 12% to overall accuracy. Therefore, we can
    evaluate the accuracy by summing these products of probabilities:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过在训练集中按照其在训练集中的比例随机选择一个类来构建这样一个随机的“分类器”。例如，如果训练集的30%是cover type 1，则随机分类器将30%的时间猜测“1”。每个分类将按照其在测试集中的比例正确，如果测试集的40%是cover
    type 1，则猜测“1”将在40%的时间内正确。因此，cover type 1将在30% x 40% = 12%的时间内被正确猜测，并对总体准确度贡献12%。因此，我们可以通过总结这些概率的乘积来评估准确性：
- en: '[PRE10]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[![1](assets/1.png)](#co_making_predictions_with_decision_trees___span_class__keep_together__and_decision_forests__span__CO4-1)'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_making_predictions_with_decision_trees___span_class__keep_together__and_decision_forests__span__CO4-1)'
- en: Count by category
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 按类别计数
- en: '[![2](assets/2.png)](#co_making_predictions_with_decision_trees___span_class__keep_together__and_decision_forests__span__CO4-2)'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_making_predictions_with_decision_trees___span_class__keep_together__and_decision_forests__span__CO4-2)'
- en: Order counts by category
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 按类别顺序计数
- en: '[![3](assets/3.png)](#co_making_predictions_with_decision_trees___span_class__keep_together__and_decision_forests__span__CO4-3)'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_making_predictions_with_decision_trees___span_class__keep_together__and_decision_forests__span__CO4-3)'
- en: Sum products of pairs in training and test sets
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练集和测试集中求和产品对
- en: Random guessing achieves 37% accuracy then, which makes 70% seem like a good
    result after all. But the latter result was achieved with default hyperparameters.
    We can do even better by exploring what the hyperparameters actually mean for
    the tree-building process. That is what we will do in the next section.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 随机猜测达到37%的准确率，这使得70%的结果看起来像是一个很好的结果。但后一个结果是通过默认超参数实现的。通过探索超参数对于树构建过程实际意味着什么，我们甚至可以做得更好。这就是我们将在下一节中做的事情。
- en: Decision Tree Hyperparameters
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 决策树超参数
- en: 'In [Chapter 3](ch03.xhtml#recommending_music_and_the_audioscrobbler_data_set),
    the ALS algorithm exposed several hyperparameters whose values we had to choose
    by building models with various combinations of values and then assessing the
    quality of each result using some metric. The process is the same here, although
    the metric is now multiclass accuracy instead of AUC. The hyperparameters controlling
    how the tree’s decisions are chosen will be quite different as well: maximum depth,
    maximum bins, impurity measure, and minimum information gain.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第三章](ch03.xhtml#recommending_music_and_the_audioscrobbler_data_set)中，ALS算法公开了几个超参数，我们必须通过使用各种值的模型构建，并使用某些指标评估每个结果的质量来选择它们的值。这里的过程是相同的，尽管度量现在是多类准确度，而不是AUC。控制树决策选择的超参数也将大不相同：最大深度、最大bins、不纯度度量和最小信息增益。
- en: '*Maximum depth* simply limits the number of levels in the decision tree. It
    is the maximum number of chained decisions that the classifier will make to classify
    an example. It is useful to limit this to avoid overfitting the training data,
    as illustrated previously in the pet store example.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '*最大深度*简单地限制了决策树中的层级数。它是分类器将做出的用于分类示例的一系列链式决策的最大数量。限制这一点对于避免过度拟合训练数据是有用的，正如在宠物店示例中所示。'
- en: 'The decision tree algorithm is responsible for coming up with potential decision
    rules to try at each level, like the `weight >= 100` or `weight >= 500` decisions
    in the pet store example. Decisions are always of the same form: for numeric features,
    decisions are of the form `feature >= value`; and for categorical features, they
    are of the form `feature in (value1, value2, …)`. So, the set of decision rules
    to try is really a set of values to plug in to the decision rule. These are referred
    to as *bins* in the PySpark MLlib implementation. A larger number of bins requires
    more processing time but might lead to finding a more optimal decision rule.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树算法负责在每个级别提出潜在的决策规则，例如在宠物店示例中的`weight >= 100`或`weight >= 500`决策。决策始终具有相同的形式：对于数值特征，决策的形式为`feature
    >= value`；对于分类特征，形式为`feature in (value1, value2, …)`。因此，要尝试的决策规则集实际上是要插入决策规则的一组值。在PySpark
    MLlib实现中，这些被称为*bins*。更多的bin需要更多的处理时间，但可能会导致找到更优的决策规则。
- en: What makes a decision rule good? Intuitively, a good rule would meaningfully
    distinguish examples by target category value. For example, a rule that divides
    the Covtype dataset into examples with only categories 1–3 on the one hand and
    4–7 on the other would be excellent because it clearly separates some categories
    from others. A rule that resulted in about the same mix of all categories as are
    found in the whole dataset doesn’t seem helpful. Following either branch of such
    a decision leads to about the same distribution of possible target values and
    so doesn’t really make progress toward a confident classification.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 什么使得一个决策规则好？直觉上，一个好的规则会通过目标类别值有意义地区分示例。例如，一个将Covtype数据集划分为一方面仅包含类别1–3，另一方面包含类别4–7的规则将是优秀的，因为它清楚地将一些类别与其他类别分开。而导致与整个数据集中相同混合的规则似乎并不有用。遵循这种决策的任一分支导致可能目标值分布大致相同，因此并没有真正向自信的分类取得进展。
- en: 'Put another way, good rules divide the training data’s target values into relatively
    homogeneous, or “pure,” subsets. Picking a best rule means minimizing the impurity
    of the two subsets it induces. There are two commonly used measures of impurity:
    Gini impurity and entropy.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，好的规则将训练数据的目标值分成相对均匀或“纯净”的子集。选择最佳规则意味着最小化其引起的两个子集的不纯度。常用的不纯度度量有两种：基尼不纯度和熵。
- en: '*Gini impurity* is directly related to the accuracy of the random guess classifier.
    Within a subset, it is the probability that a randomly chosen classification of
    a randomly chosen example (both according to the distribution of classes in the
    subset) is *incorrect*. To calculate this value, we first multiply each class
    with its respective proportion among all classes. Then we subtract the sum of
    all the values from 1\. If a subset has *N* classes and *p*[*i*] is the proportion
    of examples of class *i*, then its Gini impurity is given in the Gini impurity
    equation:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '*基尼不纯度*与随机猜测分类器的准确性直接相关。在子集内，它是随机选择的分类在随机选择的示例上（根据子集中类的分布）是*错误*的概率。要计算此值，首先将每个类乘以其在所有类中的比例。然后从1中减去所有值的总和。如果一个子集有*N*个类，*p*[*i*]是类*i*示例的比例，则其基尼不纯度由基尼不纯度方程给出：'
- en: <math alttext="upper I Subscript upper G Baseline left-parenthesis p right-parenthesis
    equals 1 minus sigma-summation Underscript i equals 1 Overscript upper N Endscripts
    p Subscript i Superscript 2" display="block"><mrow><msub><mi>I</mi> <mi>G</mi></msub>
    <mrow><mo>(</mo> <mi>p</mi> <mo>)</mo></mrow> <mo>=</mo> <mn>1</mn> <mo>-</mo>
    <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>N</mi></munderover>
    <msubsup><mi>p</mi> <mi>i</mi> <mn>2</mn></msubsup></mrow></math>
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper I Subscript upper G Baseline left-parenthesis p right-parenthesis
    equals 1 minus sigma-summation Underscript i equals 1 Overscript upper N Endscripts
    p Subscript i Superscript 2" display="block"><mrow><msub><mi>I</mi> <mi>G</mi></msub>
    <mrow><mo>(</mo> <mi>p</mi> <mo>)</mo></mrow> <mo>=</mo> <mn>1</mn> <mo>-</mo>
    <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>N</mi></munderover>
    <msubsup><mi>p</mi> <mi>i</mi> <mn>2</mn></msubsup></mrow></math>
- en: If the subset contains only one class, this value is 0 because it is completely
    “pure.” When there are *N* classes in the subset, this value is larger than 0
    and is largest when the classes occur the same number of times—maximally impure.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 如果子集仅包含一个类，则该值为0，因为它是完全“纯净”的。当子集中有*N*个类时，此值大于0，并且当类出现相同次数时最大——最大不纯度。
- en: '*Entropy* is another measure of impurity, borrowed from information theory.
    Its nature is more difficult to explain, but it captures how much uncertainty
    the collection of target values in the subset implies about predictions for data
    that falls in that subset. A subset containing one class suggests that the outcome
    for the subset is completely certain and has 0 entropy—no uncertainty. A subset
    containing one of each possible class, on the other hand, suggests a lot of uncertainty
    about predictions for that subset because data has been observed with all kinds
    of target values. This has high entropy. Hence, low entropy, like low Gini impurity,
    is a good thing. Entropy is defined by the entropy equation:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '*熵*是来自信息论的另一种不纯度度量。其性质更难以解释，但它捕捉了在子集中目标值的集合对于落入该子集的数据预测意味着多少不确定性。包含一个类的子集表明子集的结果是完全确定的，熵为0——没有不确定性。另一方面，包含每种可能类的子集表明对于该子集的预测有很多不确定性，因为观察到了各种目标值的数据。这具有高熵。因此，低熵和低基尼不纯度一样，是一件好事。熵由熵方程定义：'
- en: <math alttext="upper I Subscript upper E Baseline left-parenthesis p right-parenthesis
    equals sigma-summation Underscript i equals 1 Overscript upper N Endscripts p
    Subscript i Baseline log left-parenthesis StartFraction 1 Over p Subscript i Baseline
    EndFraction right-parenthesis equals minus sigma-summation Underscript i equals
    1 Overscript upper N Endscripts p Subscript i Baseline log left-parenthesis p
    Subscript i Baseline right-parenthesis" display="block"><mrow><msub><mi>I</mi>
    <mi>E</mi></msub> <mrow><mo>(</mo> <mi>p</mi> <mo>)</mo></mrow> <mo>=</mo> <munderover><mo>∑</mo>
    <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>N</mi></munderover> <msub><mi>p</mi>
    <mi>i</mi></msub> <mo form="prefix">log</mo> <mrow><mo>(</mo> <mfrac><mn>1</mn>
    <msub><mi>p</mi> <mi>i</mi></msub></mfrac> <mo>)</mo></mrow> <mo>=</mo> <mo>-</mo>
    <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>N</mi></munderover>
    <msub><mi>p</mi> <mi>i</mi></msub> <mo form="prefix">log</mo> <mrow><mo>(</mo>
    <msub><mi>p</mi> <mi>i</mi></msub> <mo>)</mo></mrow></mrow></math>
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper I Subscript upper E Baseline left-parenthesis p right-parenthesis
    equals sigma-summation Underscript i equals 1 Overscript upper N Endscripts p
    Subscript i Baseline log left-parenthesis StartFraction 1 Over p Subscript i Baseline
    EndFraction right-parenthesis equals minus sigma-summation Underscript i equals
    1 Overscript upper N Endscripts p Subscript i Baseline log left-parenthesis p
    Subscript i Baseline right-parenthesis" display="block"><mrow><msub><mi>I</mi>
    <mi>E</mi></msub> <mrow><mo>(</mo> <mi>p</mi> <mo>)</mo></mrow> <mo>=</mo> <munderover><mo>∑</mo>
    <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>N</mi></munderover> <msub><mi>p</mi>
    <mi>i</mi></msub> <mo form="prefix">log</mo> <mrow><mo>(</mo> <mfrac><mn>1</mn>
    <msub><mi>p</mi> <mi>i</mi></msub></mfrac> <mo>)</mo></mrow> <mo>=</mo> <mo>-</mo>
    <munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>N</mi></munderover>
    <msub><mi>p</mi> <mi>i</mi></msub> <mo form="prefix">log</mo> <mrow><mo>(</mo>
    <msub><mi>p</mi> <mi>i</mi></msub> <mo>)</mo></mrow></mrow></math>
- en: Interestingly, uncertainty has units. Because the logarithm is the natural log
    (base *e*), the units are *nats*, the base *e* counterpart to more familiar *bits*
    (which we can obtain by using log base 2 instead). It really is measuring information,
    so it’s also common to talk about the *information gain* of a decision rule when
    using entropy with decision trees.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，不确定性有单位。因为对数是自然对数（以*e*为底），单位是*nats*，是更熟悉的*bits*（我们可以用对数2为底来获得）的*e*对应物。它确实在测量信息，因此在使用熵与决策树时，也常常讨论决策规则的*信息增益*。
- en: 'One or the other measure may be a better metric for picking decision rules
    in a given dataset. They are, in a way, similar. Both involve a weighted average:
    a sum over values weighted by *p*[*i*]. The default in PySpark’s implementation
    is Gini impurity.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在给定数据集中，一个度量值可能是选择决策规则的更好指标。它们在某种程度上是相似的。两者都涉及加权平均：通过 *p*[*i*] 加权值的总和。在 PySpark
    的实现中，默认是基尼不纯度。
- en: Finally, *minimum information gain* is a hyperparameter that imposes a minimum
    information gain, or decrease in impurity, for candidate decision rules. Rules
    that do not improve the subsets’ impurity enough are rejected. Like a lower maximum
    depth, this can help the model resist overfitting because decisions that barely
    help divide the training input may in fact not helpfully divide future data at
    all.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，*最小信息增益* 是一个超参数，它对候选决策规则施加了一个最小的信息增益，或者说减少了不纯度。不能显著提高子集不纯度的规则将被拒绝。就像较低的最大深度一样，这可以帮助模型抵抗过拟合，因为仅帮助划分训练输入的决策实际上可能根本不会有助于划分未来数据。
- en: Now that we understand the relevant hyperparameters of a decision tree algorithm,
    we will tune our model in the next section to improve its performance.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们了解了决策树算法的相关超参数，接下来将在下一节调整我们的模型，以提高其性能。
- en: Tuning Decision Trees
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调整决策树
- en: It’s not obvious from looking at the data which impurity measure leads to better
    accuracy or what maximum depth or number of bins is enough without being excessive.
    Fortunately, as in [Chapter 3](ch03.xhtml#recommending_music_and_the_audioscrobbler_data_set),
    it’s simple to let PySpark try a number of combinations of these values and report
    the results.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 从数据看，不明显哪种不纯度度量可以提高准确性，或者最大深度或箱数是足够的而不是过多。幸运的是，就像在[第 3 章](ch03.xhtml#recommending_music_and_the_audioscrobbler_data_set)中一样，让
    PySpark 尝试这些值的多种组合并报告结果是很简单的。
- en: 'First, it’s necessary to set up a pipeline encapsulating the two steps we performed
    in previous sections—creating a feature vector and using it to create a decision
    tree model. Creating the `VectorAssembler` and `DecisionTreeClassifier` and chaining
    these two `Transformer`s together results in a single `Pipeline` object that represents
    these two operations together as one operation:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，需要设置一个管道，将我们在前几节中执行的两个步骤封装起来——创建特征向量和使用它创建决策树模型。创建 `VectorAssembler` 和 `DecisionTreeClassifier`
    并将这两个 `Transformer` 链接在一起，生成一个单一的 `Pipeline` 对象，将这两个操作一起表示为一个操作：
- en: '[PRE11]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Naturally, pipelines can be much longer and more complex. This is about as
    simple as it gets. Now we can also define the combinations of hyperparameters
    that should be tested using the PySpark ML API’s built-in support, `ParamGridBuilder`.
    It’s also time to define the evaluation metric that will be used to pick the “best”
    hyperparameters, and that is again `MulticlassClassificationEvaluator`:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，管道可以更长，更复杂。这是尽可能简单的案例。现在，我们还可以使用 PySpark ML API 内置的 `ParamGridBuilder` 定义应该使用的超参数组合。还是该定义将用于选择“最佳”超参数的评估指标，再次是
    `MulticlassClassificationEvaluator`：
- en: '[PRE12]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This means that a model will be built and evaluated for two values of four
    hyperparameters. That’s 16 models. They’ll be evaluated by multiclass accuracy.
    Finally, `TrainValidationSplit` brings these components together—the pipeline
    that makes models, model evaluation metrics, and hyperparameters to try—and can
    run the evaluation on the training data. It’s worth noting that `CrossValidator`
    could be used here as well to perform full k-fold cross-validation, but it is
    *k* times more expensive and doesn’t add as much value in the presence of big
    data. So, `TrainValidationSplit` is used here:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着将为四个超参数的两个值构建和评估模型。总共 16 个模型。它们将通过多类准确性进行评估。最后，`TrainValidationSplit` 将这些组件结合在一起——制作模型的管道、模型评估指标和要尝试的超参数——并可以在训练数据上运行评估。值得注意的是，在大数据存在的情况下，也可以使用
    `CrossValidator` 进行完整的 k 折交叉验证，但它的成本是 *k* 倍，并且在这里添加的价值不如 `TrainValidationSplit`
    大。因此，这里使用 `TrainValidationSplit`：
- en: '[PRE13]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: This will take several minutes or more, depending on your hardware, because
    it’s building and evaluating many models. Note the train ratio parameter is set
    to 0.9\. This means that the training data is actually further subdivided by `TrainValidationSplit`
    into 90%/10% subsets. The former is used for training each model. The remaining
    10% of the input is held out as a cross-validation set to evaluate the model.
    If it’s already holding out some data for evaluation, then why did we hold out
    10% of the original data as a test set?
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 这将需要几分钟或更长时间，取决于您的硬件，因为它正在构建和评估许多模型。请注意，train ratio 参数设置为 0.9。这意味着实际上训练数据被 `TrainValidationSplit`
    进一步分割为 90%/10% 的子集。前者用于训练每个模型。剩余的 10% 输入作为交叉验证集用于评估模型。如果已经保留了一些数据进行评估，那么为什么我们要保留原始数据的
    10% 作为测试集？
- en: If the purpose of the CV set was to evaluate *parameters* that fit to the *training*
    set, then the purpose of the test set is to evaluate *hyperparameters* that were
    “fit” to the CV set. That is, the test set ensures an unbiased estimate of the
    accuracy of the final, chosen model and its hyperparameters.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 CV 集的目的是评估适合*训练*集的*参数*，那么测试集的目的就是评估适合 CV 集的*超参数*。也就是说，测试集确保了对最终选择的模型及其超参数准确性的无偏估计。
- en: Say that the best model chosen by this process exhibits 90% accuracy on the
    CV set. It seems reasonable to expect it will exhibit 90% accuracy on future data.
    However, there’s an element of randomness in how these models are built. By chance,
    this model and evaluation could have turned out unusually well. The top model
    and evaluation result could have benefited from a bit of luck, so its accuracy
    estimate is likely to be slightly optimistic. Put another way, hyperparameters
    can overfit too.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 假设这个过程选择的最佳模型在 CV 集上表现出 90% 的准确性。预计它在未来数据上也将表现出 90% 的准确性是合理的。但是，这些模型的构建有一定的随机性。由于偶然因素，这个模型和评估可能异常地好。顶级模型和评估结果可能受益于一点运气，因此它的准确性估计可能稍微乐观。换句话说，超参数也可能过拟合。
- en: To really assess how well this best model is likely to perform on future examples,
    we need to evaluate it on examples that were not used to train it. But we also
    need to avoid examples in the CV set that were used to evaluate it. That is why
    a third subset, the test set, was held out.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 要真正评估最佳模型在未来示例上的表现如何，我们需要在未用于训练它的示例上评估它。但是，我们也需要避免用于评估它的 CV 集中的示例。这就是为什么第三个子集，测试集，被保留出来的原因。
- en: 'The result of the validator contains the best model it found. This itself is
    a representation of the best overall *pipeline* it found, because we provided
    an instance of a pipeline to run. To query the parameters chosen by `DecisionTreeClassifier`,
    it’s necessary to manually extract `DecisionTreeClassificationModel` from the
    resulting `PipelineModel`, which is the final stage in the pipeline:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 验证器的结果包含它找到的最佳模型。这本身是找到的最佳整体*管道*的表示，因为我们提供了一个要运行的管道实例。要查询 `DecisionTreeClassifier`
    选择的参数，需要从结果的 `PipelineModel` 中手动提取 `DecisionTreeClassificationModel`，它是管道中的最终阶段：
- en: '[PRE14]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: This output contains a lot of information about the fitted model, but it also
    tells us that entropy apparently worked best as the impurity measure and that
    a max depth of 20 was not surprisingly better than 1\. It might be surprising
    that the best model was fit with just 40 bins, but this is probably a sign that
    40 was “plenty” rather than “better” than 300\. Lastly, no minimum information
    gain was better than a small minimum, which could imply that the model is more
    prone to underfit than overfit.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 这个输出包含了关于拟合模型的大量信息，但它也告诉我们熵显然作为不纯度度量表现最好，并且深度为 20 与深度为 1 相比并不奇怪地更好。也许最佳模型仅使用
    40 个箱子是令人惊讶的，但这可能是 40 是“足够”而不是“比 300 更好”的迹象。最后，没有最小信息增益比小最小信息增益更好，这可能意味着模型更容易欠拟合而不是过拟合。
- en: 'You may wonder if it is possible to see the accuracy that each of the models
    achieved for each combination of hyperparameters. The hyperparameters and the
    evaluations are exposed by `getEstimatorParamMaps` and `validationMetrics`, respectively.
    They can be combined to display all of the parameter combinations sorted by metric
    value:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能想知道是否可以查看每个模型在每组超参数组合下达到的准确性。超参数和评估由 `getEstimatorParamMaps` 和 `validationMetrics`
    提供。它们可以组合在一起，按度量值排序显示所有参数组合：
- en: '[PRE15]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: What was the accuracy that this model achieved on the CV set? And, finally,
    what accuracy does the model achieve on the test set?
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型在 CV 集上实现了多少准确性？最后，在测试集上模型达到了什么准确度？
- en: '[PRE16]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[![1](assets/1.png)](#co_making_predictions_with_decision_trees___span_class__keep_together__and_decision_forests__span__CO5-1)'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_making_predictions_with_decision_trees___span_class__keep_together__and_decision_forests__span__CO5-1)'
- en: '`best_Model` is a complete pipeline.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '`best_Model`是一个完整的管道。'
- en: The results are both about 91%. It happens that the estimate from the CV set
    was pretty fine to begin with. In fact, it is not usual for the test set to show
    a very different result.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 结果都是约为91%。情况是估计从CV集中开始就很好。事实上，测试集显示非常不同的结果并不常见。
- en: This is an interesting point at which to revisit the issue of overfitting. As
    discussed previously, it’s possible to build a decision tree so deep and elaborate
    that it fits the given training examples very well or perfectly but fails to generalize
    to other examples because it has fit the idiosyncrasies and noise of the training
    data too closely. This is a problem common to most machine learning algorithms,
    not just decision trees.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是重新讨论过拟合问题的有趣时刻。正如之前讨论的那样，可能会构建一个深度和复杂度非常高的决策树，它非常好地或完全地拟合给定的训练示例，但由于过于密切地适应了训练数据的特殊性和噪声，因此不能泛化到其他示例。这是大多数机器学习算法常见的问题，不仅仅是决策树。
- en: When a decision tree has overfit, it will exhibit high accuracy when run on
    the same training data that it fit the model to, but low accuracy on other examples.
    Here, the final model’s accuracy was about 91% on other, new examples. Accuracy
    can just as easily be evaluated over the same data that the model was trained
    on, `trainData`. This gives an accuracy of about 95%. The difference is not large
    but suggests that the decision tree has overfit the training data to some extent.
    A lower maximum depth might be a better choice.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 当决策树过度拟合时，在用于训练模型的相同训练数据上表现出高准确率，但在其他示例上准确率较低。在这里，最终模型在其他新示例上的准确率约为91%。准确率也可以轻松地在模型训练的相同数据`trainData`上评估。这给出了约95%的准确率。差异不大，但表明决策树在某种程度上过度拟合了训练数据。较低的最大深度可能是一个更好的选择。
- en: So far, we’ve implicitly treated all input features, including categoricals,
    as if they’re numeric. Can we improve our model’s performance further by treating
    categorical features as exactly that? We will explore this next.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们隐式地将所有输入特征，包括分类特征，视为数值特征。我们可以通过将分类特征视为精确的分类特征来进一步改善模型的性能吗？我们将在下一步中探讨这个问题。
- en: Categorical Features Revisited
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 重新审视分类特征
- en: The categorical features in our dataset are one-hot encoded as several binary
    0/1 values. Treating these individual features as numeric turns out to be fine,
    because any decision rule on the “numeric” features will choose thresholds between
    0 and 1, and all are equivalent since all values are 0 or 1.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们数据集中的分类特征被独热编码为几个二进制0/1值。将这些单独特征视为数值特征其实效果不错，因为任何对“数值”特征的决策规则都将选择0到1之间的阈值，而所有值都是等效的，因为所有值都是0或1。
- en: Of course, this encoding forces the decision tree algorithm to consider the
    values of the underlying categorical features individually. Because features like
    soil type are broken down into many features and because decision trees treat
    features individually, it is harder to relate information about related soil types.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这种编码方式迫使决策树算法单独考虑底层分类特征的值。因为像土壤类型这样的特征被分解成许多特征，并且决策树将特征视为独立的，所以更难以关联相关土壤类型的信息。
- en: For example, nine different soil types are actually part of the Leighton family,
    and they may be related in ways that the decision tree can exploit. If soil type
    were encoded as a single categorical feature with 40 soil values, then the tree
    could express rules like “if the soil type is one of the nine Leighton family
    types” directly. However, when encoded as 40 features, the tree would have to
    learn a sequence of nine decisions on soil type to do the same, this expressiveness
    may lead to better decisions and more efficient trees.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，九种不同的土壤类型实际上属于莱顿家族的一部分，它们可能以决策树可以利用的方式相关联。如果将土壤类型编码为单一的分类特征，并且有40种土壤值，那么树可以直接表达规则，比如“如果土壤类型是九种莱顿家族类型之一”。然而，如果将其编码为40个特征，则树必须学习一系列关于土壤类型的九个决策才能达到相同效果，这种表达能力可能导致更好的决策和更高效的树。
- en: However, having 40 numeric features represent one 40-valued categorical feature
    increases memory usage and slows things down.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，40个数值特征代表一个40值分类特征会增加内存使用并减慢速度。
- en: 'What about undoing the one-hot encoding? This would replace, for example, the
    four columns encoding wilderness type with one column that encodes the wilderness
    type as a number between 0 and 3, like `Cover_Type`:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 如何撤销 one-hot 编码呢？例如，用一个将荒野类型编码为 0 到 3 的数字的列来替换原来的四列，比如 `Cover_Type`：
- en: '[PRE17]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[![1](assets/1.png)](#co_making_predictions_with_decision_trees___span_class__keep_together__and_decision_forests__span__CO6-1)'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_making_predictions_with_decision_trees___span_class__keep_together__and_decision_forests__span__CO6-1)'
- en: Note UDF definition
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 UDF 定义
- en: '[![2](assets/2.png)](#co_making_predictions_with_decision_trees___span_class__keep_together__and_decision_forests__span__CO6-2)'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_making_predictions_with_decision_trees___span_class__keep_together__and_decision_forests__span__CO6-2)'
- en: Drop one-hot columns; no longer needed
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 删除 one-hot 列；不再需要
- en: Here `VectorAssembler` is deployed to combine the 4 and 40 wilderness and soil
    type columns into two `Vector` columns. The values in these `Vector`s are all
    0, except for one location that has a 1\. There’s no simple DataFrame function
    for this, so we have to define our own UDF that can be used to operate on columns.
    This turns these two new columns into numbers of just the type we need.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 这里使用 `VectorAssembler` 来将 4 个荒野类型和 40 个土壤类型列组合成两个 `Vector` 列。这些 `Vector` 中的值都是
    0，除了一个位置上的值是 1。没有简单的 DataFrame 函数可以做到这一点，因此我们必须定义自己的 UDF 来操作列。这将把这两个新列转换成我们需要的类型的数值。
- en: 'We can now transform our dataset by removing one-hot encoding using our function
    defined above:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以通过上面定义的函数，去除数据集中的 one-hot 编码：
- en: '[PRE18]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: From here, nearly the same process as above can be used to tune the hyperparameters
    of a decision tree model built on this data and to choose and evaluate a best
    model. There’s one important difference, however. The two new numeric columns
    have nothing about them that indicates they’re actually an encoding of categorical
    values. To treat them as numbers is not correct, as their ordering is meaningless.
    The model will still get built but because of some information in these features
    not being available, the accuracy may suffer.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 从这里开始，几乎与上述过程相同，可以用来调整基于这些数据构建的决策树模型的超参数，并选择和评估最佳模型。然而，有一个重要的区别。这两个新的数值列并没有任何信息表明它们实际上是分类值的编码。将它们视为数值是不正确的，因为它们的排序是没有意义的。模型仍然会构建，但由于这些特征中的一些信息不可用，准确性可能会受到影响。
- en: Internally MLlib can store additional metadata about each column. The details
    of this data are generally hidden from the caller but include information such
    as whether the column encodes a categorical value and how many distinct values
    it takes on. To add this metadata, it’s necessary to put the data through `VectorIndexer`.
    Its job is to turn input into properly labeled categorical feature columns. Although
    we did much of the work already to turn the categorical features into 0-indexed
    values, `VectorIndexer` will take care of the metadata.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: MLlib 内部可以存储关于每列的附加元数据。这些数据的详细信息通常对调用者隐藏，但包括诸如列是否编码为分类值以及它有多少个不同的值等信息。为了添加这些元数据，需要通过
    `VectorIndexer` 处理数据。它的任务是将输入转换为正确标记的分类特征列。虽然我们已经完成了将分类特征转换为 0 索引值的大部分工作，但 `VectorIndexer`
    将负责处理元数据。
- en: 'We need to add this stage to the `Pipeline`:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要将此阶段添加到 `Pipeline` 中：
- en: '[PRE19]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[![1](assets/1.png)](#co_making_predictions_with_decision_trees___span_class__keep_together__and_decision_forests__span__CO7-1)'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_making_predictions_with_decision_trees___span_class__keep_together__and_decision_forests__span__CO7-1)'
- en: '>= 40 because soil has 40 values'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '>= 40 是因为土壤有 40 个值'
- en: The approach assumes that the training set contains all possible values of each
    of the categorical features at least once. That is, it works correctly only if
    all 4 soil values and all 40 wilderness values appear in the training set so that
    all possible values get a mapping. Here, that happens to be true, but may not
    be for small training sets of data in which some labels appear very infrequently.
    In those cases, it could be necessary to manually create and add a `VectorIndexerModel`
    with the complete value mapping supplied manually.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法假设训练集至少包含每个分类特征的所有可能值。也就是说，它仅在所有 4 个土壤值和所有 40 个荒野值至少出现一次的训练集中才能正常工作，以便所有可能的值都有映射。在这里，情况确实如此，但是对于一些标签非常少见的小训练数据集，可能需要手动创建并添加一个包含完整值映射的
    `VectorIndexerModel`。
- en: Aside from that, the process is the same as before. You should find that it
    chose a similar best model but that accuracy on the test set is about 93%. By
    treating categorical features as actual categorical features in the previous sections,
    the classifier improved its accuracy by almost 2%.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 除此之外，流程与之前的相同。你应该会发现，它选择了一个类似的最佳模型，但测试集的准确率约为93%。通过在前几节中将分类特征视为实际的分类特征，分类器的准确率提高了近2%。
- en: We have trained and tuned a decision tree. Now, we will move on to random forests,
    a more powerful algorithm. As we will see in the next section, implementing them
    using PySpark will be surprisingly straightforward at this point.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经训练并调整了一棵决策树。现在，我们将转向随机森林，这是一种更强大的算法。正如我们将在下一节看到的那样，使用PySpark实现它们此时将会非常简单。
- en: Random Forests
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机森林
- en: If you have been following along with the code examples, you may have noticed
    that your results differ slightly from those presented in the code listings in
    the book. That is because there is an element of randomness in building decision
    trees, and the randomness comes into play when you’re deciding what data to use
    and what decision rules to explore.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你一直在跟着代码示例，你可能已经注意到，你的结果与书中的代码清单中的结果略有不同。这是因为在构建决策树时存在一定的随机性，当你决定使用哪些数据和探索哪些决策规则时，这种随机性就会起作用。
- en: The algorithm does not consider every possible decision rule at every level.
    To do so would take an incredible amount of time. For a categorical feature over
    *N* values, there are 2^(*N*)–2 possible decision rules (every subset except the
    empty set and entire set). For an even moderately large *N*, this would create
    billions of candidate decision rules.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 算法并不考虑每个级别的所有可能决策规则。这样做将需要大量时间。对于一个包含*N*个值的分类特征，存在2^(*N*)–2个可能的决策规则（除了空集和整个集合的每个子集）。对于一个甚至是中等大小的*N*，这将产生数十亿个候选决策规则。
- en: Instead, decision trees use several heuristics to determine which few rules
    to actually consider. The process of picking rules also involves some randomness;
    only a few features picked at random are looked at each time, and only values
    from a random subset of the training data. This trades a bit of accuracy for a
    lot of speed, but it also means that the decision tree algorithm won’t build the
    same tree every time. This is a good thing.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，决策树使用几个启发式方法来确定实际考虑的少数规则。选择规则的过程还涉及一些随机性；每次只查看随机挑选的少数特征，并且只使用随机子集的训练数据。这种做法在很大程度上换取了一点准确性以换取更快的速度，但也意味着决策树算法不会每次都构建相同的树。这是一件好事。
- en: 'It’s good for the same reason that the “wisdom of the crowds” usually beats
    individual predictions. To illustrate, take this quick quiz: how many black taxis
    operate in London?'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 出于和“群体的智慧”通常能胜过个体预测的同样原因，它是有好处的。为了说明这一点，做个简单的测验：伦敦有多少辆黑色出租车？
- en: Don’t peek at the answer; guess first.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 不要提前看答案；先猜测。
- en: 'I guessed 10,000, which is well off the correct answer of about 19,000\. Because
    I guessed low, you’re a bit more likely to have guessed higher than I did, and
    so the average of our answers will tend to be more accurate. There’s that regression
    to the mean again. The average guess from an informal poll of 13 people in the
    office was indeed closer: 11,170.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我猜测是10,000辆，这比正确答案大约19,000辆要少很多。因为我猜低了，你更有可能猜得比我高，所以我们的平均答案将更接近实际。再次回归到平均数。办公室里进行的一次非正式调查中的平均猜测确实更接近：11,170辆。
- en: A key to this effect is that the guesses were independent and didn’t influence
    one another. (You didn’t peek, did you?) The exercise would be useless if we had
    all agreed on and used the same methodology to make a guess, because the guesses
    would have been the same answer—the same potentially quite wrong answer. It would
    even have been different and worse if I’d merely influenced you by stating my
    guess up front.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 该效果的关键在于这些猜测是独立的，彼此不会互相影响。（你没有偷看，对吧？）如果我们都同意并使用相同的方法来猜测，那么这个练习就没有意义了，因为猜测的答案会是一样的——也就是说，可能是完全错误的答案。如果我仅仅通过提前陈述我的猜测来影响你，情况甚至会变得更糟。
- en: It would be great to have not one tree but many trees, each producing reasonable
    but different and independent estimations of the right target value. Their collective
    average prediction should fall close to the true answer, more than any individual
    tree’s does. It’s the *randomness* in the process of building that helps create
    this independence. This is the key to *random forests*.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不是只需要一棵树而是需要许多树，每棵树都产生合理但不同和独立的目标值估计，这将有助于预测的集体平均预测接近真实答案，超过任何一棵单独树的预测。建造过程中的*随机性*有助于创建这种独立性。这就是*随机森林*的关键。
- en: Randomness is injected by building many trees, each of which sees a different
    random subset of data—and even of features. This makes the forest as a whole less
    prone to overfitting. If a particular feature contains noisy data or is deceptively
    predictive only in the *training* set, then most trees will not consider this
    problem feature most of the time. Most trees will not fit the noise and will tend
    to “outvote” the trees that have fit the noise in the forest.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 通过构建许多树注入随机性，每棵树都会看到一个不同的随机数据子集 —— 甚至是特征子集。这使得整个森林对过拟合的倾向较小。如果特定特征包含嘈杂的数据或者在*训练*集中仅具有欺骗性的预测性，那么大多数树大部分时间都不会考虑这个问题特征。大多数树将不会适应噪声，并倾向于“否决”在森林中适应噪声的树。
- en: The prediction of a random forest is simply a weighted average of the trees’
    predictions. For a categorical target, this can be a majority vote or the most
    probable value based on the average of probabilities produced by the trees. Random
    forests, like decision trees, also support regression, and the forest’s prediction
    in this case is the average of the number predicted by each tree.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林的预测只是树预测的加权平均。对于分类目标，这可以是多数投票或基于树所产生的概率平均值的最可能值。随机森林与决策树一样支持回归，此时森林的预测是每棵树预测的平均数。
- en: 'While random forests are a more powerful and complex classification technique,
    the good news is that it’s virtually no different to use it in the pipeline that
    has been developed in this chapter. Simply drop in a `RandomForestClassifier`
    in place of `DecisionTreeClassifier` and proceed as before. There’s really no
    more code or API to understand in order to use it:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管随机森林是一种更强大且复杂的分类技术，好消息是，在本章开发的流水线中使用它实际上几乎没有任何不同。只需在 `DecisionTreeClassifier`
    的位置放置一个 `RandomForestClassifier`，然后像以前一样继续即可。实际上，没有更多的代码或API需要理解来使用它：
- en: '[PRE20]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Note that this classifier has another hyperparameter: the number of trees to
    build. Like the max bins hyperparameter, higher values should give better results
    up to a point. The cost, however, is that building many trees of course takes
    many times longer than building one.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，此分类器还有另一个超参数：要构建的树的数量。与最大箱数超参数一样，较高的值应该在一定程度上产生更好的结果。然而，代价是构建许多树当然比构建一棵树花费的时间要长得多。
- en: The accuracy of the best random forest model produced from a similar tuning
    process is 95% off the bat—about 2% better already, although viewed another way,
    that’s a 28% reduction in the error rate over the best decision tree built previously,
    from 7% down to 5%. You may do better with further tuning.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 从类似调整过程产生的最佳随机森林模型的准确率一开始就达到了95% —— 已经比最佳决策树的错误率低了大约2%，尽管从另一个角度看，这是错误率从之前的7%下降到5%的28%。您可能通过进一步调整获得更好的效果。
- en: 'Incidentally, at this point we have a more reliable picture of feature importance:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 顺便说一句，在这一点上，我们对特征重要性有了更可靠的图像：
- en: '[PRE21]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Random forests are appealing in the context of big data because trees are supposed
    to be built independently, and big data technologies like Spark and MapReduce
    inherently need *data-parallel* problems, where parts of the overall solution
    can be computed independently on parts of the data. The fact that trees can, and
    should, train on only a subset of features or input data makes it trivial to parallelize
    building the trees.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林在大数据背景下很有吸引力，因为树应该独立构建，大数据技术如Spark和MapReduce天生需要*数据并行*问题，在数据的各个部分上可以独立计算整体解决方案的部分。树可以且应该仅在特征或输入数据的子集上进行训练，使得并行构建树变得微不足道。
- en: Making Predictions
  id: totrans-184
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进行预测
- en: Building a classifier, while an interesting and nuanced process, is not the
    end goal. The goal is to make predictions. This is the payoff, and it is comparatively
    quite easy.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 构建分类器虽然是一个有趣且微妙的过程，但不是最终目标。目标是进行预测。这是回报，相比较而言，它相对容易得多。
- en: The resulting “best model” is actually a whole pipeline of operations. It encapsulates
    how input is transformed for use with the model and includes the model itself,
    which can make predictions. It can operate on a dataframe of new input. The only
    difference from the `data` DataFrame we started with is that it lacks the `Cover_Type`
    column. When we’re making predictions—especially about the future, says Mr. Bohr—the
    output is of course not known.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: “最佳模型”实际上是一个完整的操作流程。它封装了输入如何被转换以供模型使用，并包括模型本身，该模型可以进行预测。它可以在新输入的数据帧上操作。我们开始的`data`
    DataFrame唯一的不同之处在于它缺少`Cover_Type`列。当我们进行预测时——尤其是关于未来的预测，波尔先生说——输出当然是未知的。
- en: 'To prove it, try dropping the `Cover_Type` from the test data input and obtaining
    a prediction:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 为了证明它，请尝试从测试数据输入中删除`Cover_Type`并获取一个预测。
- en: '[PRE22]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The result should be 6.0, which corresponds to class 7 (the original feature
    was 1-indexed) in the original Covtype dataset. The predicted cover type for the
    land described in this example is Krummholz.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 结果应该是6.0，对应于原始Covtype数据集中的第7类（原始特征是从1开始编号的）。此示例中描述的土地的预测覆盖类型是Krummholz。
- en: Where to Go from Here
  id: totrans-190
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 何去何从
- en: 'This chapter introduced two related and important types of machine learning,
    classification and regression, along with some foundational concepts in building
    and tuning models: features, vectors, training, and cross-validation. It demonstrated
    how to predict a type of forest cover from things like location and soil type
    using the Covtype dataset, with decision trees and forests implemented in PySpark.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了两种相关且重要的机器学习类型，分类和回归，以及构建和调整模型的一些基本概念：特征、向量、训练和交叉验证。它演示了如何使用Covtype数据集，使用PySpark中实现的决策树和随机森林来预测森林覆盖类型，例如位置和土壤类型等。
- en: 'As with recommenders in [Chapter 3](ch03.xhtml#recommending_music_and_the_audioscrobbler_data_set),
    it could be useful to continue exploring the effect of hyperparameters on accuracy.
    Most decision tree hyperparameters trade time for accuracy: more bins and trees
    generally produce better accuracy but hit a point of diminishing returns.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 与[第三章](ch03.xhtml#recommending_music_and_the_audioscrobbler_data_set)中的推荐系统一样，继续探索超参数对准确性的影响可能很有用。大多数决策树超参数都在时间和准确性之间进行权衡：更多的箱子和树通常会产生更高的准确性，但会达到收益递减的点。
- en: 'The classifier here turned out to be very accurate. It’s unusual to achieve
    more than 95% accuracy. In general, you will achieve further improvements in accuracy
    by including more features or transforming existing features into a more predictive
    form. This is a common, repeated step in iteratively improving a classifier model.
    For example, for this dataset, the two features encoding horizontal and vertical
    distance-to-surface-water features could produce a third feature: straight-line
    distance-to-surface-water features. This might turn out to be more useful than
    either original feature. Or, if it were possible to collect more data, we might
    try adding new information like soil moisture to improve classification.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的分类器结果非常准确。超过95%的准确性是不寻常的。一般来说，通过包含更多特征或将现有特征转换为更具预测性的形式，你可以进一步提高准确性。这是在迭代改进分类器模型中的常见重复步骤。例如，对于这个数据集，编码水平和垂直距离到水表的两个特征可以产生第三个特征：直线距离到水表的特征。这可能比任何一个原始特征都更有用。或者，如果有可能收集更多数据，我们可以尝试添加新的信息，比如土壤湿度来改进分类。
- en: Of course, not all prediction problems in the real world are exactly like the
    Covtype dataset. For example, some problems require predicting a continuous numeric
    value, not a categorical value. Much of the same analysis and code applies to
    this type of *regression* problem; the `RandomForestRegressor` class will be of
    use in this case.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，并非所有现实世界中的预测问题都与Covtype数据集完全相同。例如，有些问题需要预测连续的数值，而不是分类值。对于这种*回归*问题，大部分相同的分析和代码都适用；在这种情况下，`RandomForestRegressor`类将会很有用。
- en: 'Furthermore, decision trees and forests are not the only classification or
    regression algorithms, and not the only ones implemented in PySpark. Each algorithm
    operates quite differently from decision trees and forests. However, many elements
    are the same: they plug into a `Pipeline` and operate on columns in a dataframe,
    and have hyperparameters that you must select using training, cross-validation,
    and test subsets of the input data. The same general principles, with these other
    algorithms, can also be deployed to model classification and regression problems.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，决策树和随机森林不是唯一的分类或回归算法，也不是仅在 PySpark 中实现的算法。每个算法的运作方式都与决策树和随机森林大不相同。然而，许多元素是相同的：它们都可以插入到一个`Pipeline`中，并在数据框架的列上操作，并且具有您必须使用输入数据的训练、交叉验证和测试子集来选择的超参数。对于这些其他算法，相同的一般原则也可以用来建模分类和回归问题。
- en: These have been examples of supervised learning. What happens when some, or
    all, of the target values are unknown? The following chapter will explore what
    can be done in this situation.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 这些都是监督学习的例子。当一些或全部目标值未知时会发生什么？接下来的章节将探讨在这种情况下可以做些什么。
