- en: Chapter 7\. A Case Study in Bilingual Data Science
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Rick J. Scavetta
  prefs: []
  type: TYPE_NORMAL
- en: Boyan Angelov
  prefs: []
  type: TYPE_NORMAL
- en: In this final chapter, our goal is to present a case study that demonstrates
    a sample of all the concepts and tools we’ve shown throughout this book. Although
    data science provides a practically overwhelming diversity of methods and applications,
    we typically rely on a core toolkit in our daily work. Thus, it’s unlikely that
    you’ll make use of *all* the tools presented in this book (or this case study,
    for that matter). But that’s alright! We hope that you’ll focus on those parts
    of the case study that are most relevant to your work and that you’ll be inspired
    to be a modern, bilingual data scientist.
  prefs: []
  type: TYPE_NORMAL
- en: 24 years and 1.88 million wildfires
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our case study will focus on the *US Wildfires dataset* ^([1](ch07.xhtml#idm45127447555192)).
    This dataset, released by the US Department of Agriculture (USDA), contains 1.88
    million geo-referenced wildfire records. Collectively, these fires have resulted
    in the loss of 140 million acres of forest over 24 years. If you want to execute
    the code in this chapter, download the SQLite data set from the [USDA website](https://doi.org/10.2737/RDS-2013-0009.4)
    directly or from [Kaggle](https://www.kaggle.com/rtatman/188-million-us-wildfires).
    Some preprocessing has already been performed, e.g., duplicate removal.
  prefs: []
  type: TYPE_NORMAL
- en: There are 39 features, plus another shape variable in raw format. Many of these
    are unique identifiers or redundant categorical and continuous representations.
    Thus, to simplify our case study, we’ll focus on a few features listed in [Table 7-1](#csFeatures).
  prefs: []
  type: TYPE_NORMAL
- en: Table 7-1\. The `fires` table contains 39 features describing over 1.88 million
    wildfires in the US from 1992-2015
  prefs: []
  type: TYPE_NORMAL
- en: '| Variable | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| STAT_CAUSE_DESCR | Cause of the fire (The target variable) |'
  prefs: []
  type: TYPE_TB
- en: '| OWNER_CODE | Code for primary owner of the land |'
  prefs: []
  type: TYPE_TB
- en: '| DISCOVERY_DOY | Day of year of fire discovery or confirmation |'
  prefs: []
  type: TYPE_TB
- en: '| FIRE_SIZE | Estimate of the final fire size (acres) |'
  prefs: []
  type: TYPE_TB
- en: '| LATITUDE | Latitude (NAD83) of the fire |'
  prefs: []
  type: TYPE_TB
- en: '| LONGITUDE | Longitude (NAD83) of the fire |'
  prefs: []
  type: TYPE_TB
- en: We’ll develop a classification model to predict the cause of a fire (`STAT_CAUSE_CODE`)
    using the five other features as features. The target and the model are secondary;
    this is not an ML case study. Thus, we’re not going to focus on details such as
    cross-validation or hyperparameter tuning^([2](ch07.xhtml#idm45127447535416)).
    We’ll also limit ourselves to observations from 2015 and exclude Hawaii and Alaska
    to reduce the data set to a more manageable size. The end product of our case
    study will be to produce an interactive document that will allow us to input new
    predictor values, as depicted in [Figure 7-1](#case_arch)^([3](ch07.xhtml#idm45127447533464)).
  prefs: []
  type: TYPE_NORMAL
- en: Before we dig in, it’s worth taking a moment to consider data lineage - from
    raw to product. Answering the following questions will help orientate us.
  prefs: []
  type: TYPE_NORMAL
- en: What is the end product?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How will it be used, and by whom?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Can we break down the project into component pieces?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How will each component be built? i.e., Python or R? Which additional packages
    may be necessary?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How will these component pieces work together?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Answering these questions allows us to draw a path from the raw data to the
    end product, hopefully avoiding bottlenecks along the way. For question 1, we’ve
    already stated that we want to build an interactive document. For the second question,
    to keep things simple, let’s assume it’s for us to easily input new feature values
    and see the model’s prediction.
  prefs: []
  type: TYPE_NORMAL
- en: Questions 3-5 are what we’ve considered in this book. In question 3, we imagine
    the parts as a series of steps for our overall workflow. Question 4 was addressed
    in [Chapter 4](ch04.xhtml#ch05) and [Chapter 5](ch05.xhtml#ch06). We summarize
    those steps in [Table 7-2](#csOverview).
  prefs: []
  type: TYPE_NORMAL
- en: Table 7-2\. The steps in our case study and their respective languages.
  prefs: []
  type: TYPE_NORMAL
- en: '| Component/Step | Language | Additional packages? |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1\. Data Importing | R | `RSQLite`, `DBI` |'
  prefs: []
  type: TYPE_TB
- en: '| 2\. EDA & Data Visualization | R | `ggplot2`, `GGally`, `visdat`, `nanair`
    |'
  prefs: []
  type: TYPE_TB
- en: '| 4\. Feature Engineering | Python | `scikit-learn` |'
  prefs: []
  type: TYPE_TB
- en: '| 5\. Machine Learning | Python | `scikit-learn` |'
  prefs: []
  type: TYPE_TB
- en: '| 6\. Mapping | R | `leaflet` |'
  prefs: []
  type: TYPE_TB
- en: '| 7\. Interactive web interface | R | `shiny` runtime in an RMarkdown |'
  prefs: []
  type: TYPE_TB
- en: Finally, question 5 asks us to consider the project architecture. The diagram
    presented in [Figure 7-1](#case_arch) shows how each of the steps in [Table 7-2](#csOverview)
    will be linked together.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/prds_0701.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-1\. Architecture for our case study project.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Alright, now that we know where we’re going, let’s choose our tools with care
    and assemble all the components into a unified whole.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We prepared this case study exclusively using the RStudio IDE. As we discussed
    in the [Chapter 6](ch06.xhtml#ch07), if we’re writing in R and accessing Python
    functions, this would be the way to go. The reason is the built-in capabilities
    in executing Python code chunks within RMarkdown, the features of the Environment
    and Plot panes, and finally, the tooling around `shiny`.
  prefs: []
  type: TYPE_NORMAL
- en: Setup and data import
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can see from our diagram that our end product will be an interactive RMarkdown
    document. So let’s begin as we have done in [Chapter 5](ch05.xhtml#ch06). Our
    YAML^([4](ch07.xhtml#idm45127447463752)) header will consist of at least:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: To have nicer formatting, we’ll exclude the characters specifying an RMarkdown
    chunk from the following examples. Naturally, if you are following along, you
    need to add them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since the data is stored in an SQLite database, we need to use some additional
    packages in addition to ones we’ve already seen. Our first code chunk is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: In our second code chunk, we’ll connect to the database and list all of the
    33 available tables.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Creating a connection (`con`) object is a standard practice in establishing
    programmatic access to databases. In contrast to R, Python has built-in support
    for opening such files with the `sqlite3` package. This is preferable to R since
    we don’t need to install and load two additional packages. Nonetheless, R is a
    core language for the initial steps, so we might as well just import the data
    in R from the outset.
  prefs: []
  type: TYPE_NORMAL
- en: Our data is stored in the `Fires` table. As we know the columns we want to access,
    we can specify that while importing.
  prefs: []
  type: TYPE_NORMAL
- en: It’s also important to remember to close the connections when working with remote
    or shared databases, since that might prevent other users from accessing the database
    and cause issues^([5](ch07.xhtml#idm45127447406008)).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: We limit our dataset size already at this very first importing step. It’s a
    shame to throw out so much data. Still, we do this since older data, especially
    in climate applications, tends to be less representative of the current or near-future
    situation. Predictions based on old data can be inherently biased. By limiting
    the size of the data set, we also reduce the amount of memory used, improving
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: Performance tip
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Often in the case of enormous datasets (those barely or not fitting into the
    memory of your machine), you can use such an ingestion command to select just
    a sample, such as `LIMIT 1000`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can get a quick preview of the data using the `tidyverse` function `dplyr::glimpse()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: EDA & Data Visualization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Since the dataset is still relatively large, we should think carefully about
    the best data visualization strategy. Our first instinct may be to plot a map
    since we have latitude and longitude coordinates. This can be fed into `ggplot2`
    directly as x and y-axis coordinates as such:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '![](Images/prds_0702.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-2\. Plotting the sizes of individual fires.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: By mapping `OWNER_CODE` onto the color aesthetic, we can see a strong correlation
    in some states. We can predict that this will have a substantial effect on our
    model’s performance. In the above code snippet, we assigned the plot to the object
    `g`. This is not strictly necessary, but we did it in this case to showcase the
    strength of the `ggplot2` layering method. We can add a `facet_wrap()` layer to
    this plot and separate it into 13 facets, or *small multiples*, one for each type
    of `STAT_CAUSE_DESCR`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![](Images/prds_0703.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-3\. Faceting the fires plot, based on the fire cause.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This allows us to appreciate that some causes are abundant while others are
    rare, an observation we’ll see again shortly in a different way. We can also begin
    to assess any strong associations between, e.g., region, owner code, and cause
    of a fire.
  prefs: []
  type: TYPE_NORMAL
- en: Returning to the entirety of the data set, an easy way to get a comprehensive
    overview is to use a pairs plot, sometimes called a splom (or “scatter plot matrix”
    if it consists of purely numeric data). The `GGally` package^([6](ch07.xhtml#idm45127447143208))
    provides an exceptional function, `ggpairs()` that produces a matrix of plots.
    Each pair-wise bi-variate plot is shown as univariate density plots or histograms
    on the diagonal. In the upper triangle, the correlation between continuous features
    is available.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '![](Images/prds_0704.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-4\. A pairs plot.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'This information-rich visualization demands some time to process. It’s handy
    as an *exploratory* plot, in EDA, but not necessarily as an *explanatory* in reporting
    our results. Can you spot any unusual patterns? First, `STAT_CAISE_DESCR` looks
    imbalanced^([7](ch07.xhtml#idm45127447128744)), meaning there is a significant
    difference between the number of observations per class. Additionally, `OWNER_CODE`
    appears to be bimodal (having two maxima). Those properties can negatively affect
    our analysis depending on which model we choose. Second, all correlations seem
    to be relatively low, making our job easier (since correlated data is not good
    for ML). Still, we already know there is a strong association between location
    (`LATITUDE` & `LONGITUDE`) and OWNER CODE from our previous plot. So we should
    take these correlations with a grain of salt. We would expect to detect this issue
    in feature engineering. Third, `FIRE_SIZE` has a very unusual distribution. It
    looks like that plot is empty, with just the x and y axes present. We see a density
    plot with a very high and narrow peak at the very low range and an extremely long
    positive skew. We can quickly generate a `log10` transformed density plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '![](Images/prds_0705.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-5\. Density plot of the log-transformed `FIRE_SIZE` feature.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Additional visualizations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For the case study, we’ll keep the tasks to a minimum, but there might be a
    few other interesting things to visualize that can help tell a story for the end-user.
    For example, note that the dataset has a temporal dimension. It would be interesting
    how forest fires’ quantity (and quality) has been changing over time. We’ll leave
    this to the motivated user to explore with the excellent `gganimate` package.
  prefs: []
  type: TYPE_NORMAL
- en: 'Interactive data visualization is often overused, without a special purpose
    in mind. Even for the most popular packages, the documentation shows just basic
    usage. In our case, since we have so many data points in a spatial setting, and
    we want to have a final deliverable that is accessible, creating an interactive
    map is an obvious choice. As in [Chapter 5](ch05.xhtml#ch06) we use `leaflet`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '![](Images/prds_0706.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-6\. Interactive map showing the locations of forest fires.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note how using `clusterOptions` allows us to simultaneously present all of the
    data without overwhelming the user or reducing visibility. For our purposes, this
    satisfies our curiosity using some great visualizations in EDA. There are plenty
    of other statistics we can apply, but let’s move machine learning in Python.
  prefs: []
  type: TYPE_NORMAL
- en: Machine Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: By now, we have some idea about the factors that may influence the cause of
    a fire. Let’s dive into building a machine learning model using `scikit-learn`
    in Python^([8](ch07.xhtml#idm45127446950648)).
  prefs: []
  type: TYPE_NORMAL
- en: 'We argued that ML is best done in Python as we saw in [Chapter 5](ch05.xhtml#ch06).
    We’ll use a Random Forest algorithm. There are several reasons for this choice:'
  prefs: []
  type: TYPE_NORMAL
- en: It’s a well-established algorithm
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It’s relatively easy to understand
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It does not require feature scaling before training
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There are other reasons why it’s good, such as working well with missing data
    and having out-of-the-box explainability.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up our Python Environment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As discussed in [Chapter 6](ch06.xhtml#ch07), there are a few ways to access
    Python using the `reticulate` package. The choice depends on the circumstances,
    which we laid out in our project architecture. Here, we’ll pass our R `data.frame`
    to a Python virtual environment. If you followed the steps in [Chapter 6](ch06.xhtml#ch07),
    you’d already have the `modern_data` virtual environment set up. We already installed
    some packages into this environment. To recap, we executed the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'If you don’t have the `modern_data` virtualenv or you’re using Windows, please
    refer to the steps in the files `0 - setup.R` and `1 - activate.R` and discussed
    in [Chapter 6](ch06.xhtml#ch07). You may want to restart R at this point to make
    sure that you’ll be able to activate your virtual environment using the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: We’ll include all the Python steps into a single script; you can find this script
    in the book [repository](https://github.com/moderndatadesign/PyR4MDS) under `ml.py`.
    First, we’ll import the necessary modules.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Feature engineering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are features in the dataset that might be informative to a data analyst
    but are at best useless for training the model, and at worst - can reduce its
    accuracy. This is called “adding noise” to the dataset, and we want to avoid it
    at all costs. This is the purpose behind feature engineering. Let’s select just
    the features we need, as specified in [Table 7-1](#csFeatures). We also use standard
    ML convention in storing them in `X`, and our target in ‘y’.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we create an instance of the `LaberEncoder`. We use this to encode a
    categorical feature to numeric. In our case, we apply it to our target:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we split the dataset into a training and a test set (note that we are
    also using the handy `stratify` parameter to make sure the splitting function
    samples our imbalanced classes fairly):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Model training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To apply the Random Forest classifier, we’ll make an instance of `RandomForestClassifier`.
    As in [Chapter 5](ch05.xhtml#ch06) we use the `fit/predict` paradigm and store
    the predicted values in `preds`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: In the final step, we’ll assign the confusion matrix and the accuracy score
    to objects.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'After we have complete our script, we can source it into R:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: After running this command, we’ll have access to all the Python objects directly
    in our environment. The accuracy is `0.58`, which is not phenomenal, but certainly
    much better than random!
  prefs: []
  type: TYPE_NORMAL
- en: The power of sourcing Python scripts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When we use the `source_python` function from `reticulate` we can significantly
    increase our productivity, especially if we are working in a bilingual team. Imagine
    the scenario when a coworker of yours builds the ML part in Python and you need
    to include their work in yours. It would be as easy as sourcing without worrying
    about re-coding everything. This scenario is also plausible when joining a new
    company or project and inheriting Python code that you need to use straight away.
  prefs: []
  type: TYPE_NORMAL
- en: If we want to take advantage of `ggplot` to examine the confusion matrix, we
    first need to convert to an R `data.frame`. The `value` is then the number of
    observations of each case, which we map onto `size`, and change the `shape` to
    1 (a circle). The result is shown on [Figure 7-7](#conf_mat_plot).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '![](Images/prds_0707.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-7\. Plot of the classifier confusion matrix.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: It’s not surprising that we have some groups with a very high match since we
    already knew that our data was imbalanced to begin with. Now, what do we do with
    this nice Python code and output? At the end of [Chapter 6](ch06.xhtml#ch07),
    we saw a simple and effective way to create an interactive document (remember
    what you learned in [Chapter 5](ch05.xhtml#ch06)) using an RMarkdown with a `shiny`
    runtime. Let’s implement the same concept here.
  prefs: []
  type: TYPE_NORMAL
- en: Prediction and UI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once we have established a Python model, it’s general practice to test it with
    mock input. This allows us to ensure our model can handle the correct input data
    and is standard practice in ML engineering before connecting it with real user
    input. To this end, we’ll create five `sliderInputs` for the five features of
    our model. Here, we’ve hard-coded the min and max values for the sake of simplicity,
    but these can, of course, be dynamic.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Similar to what we did at the end of [Chapter 6](ch06.xhtml#ch07), we’ll access
    these values in the internal `input` list and use a `shiny` package function to
    render the appropriate output.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '![](Images/prds_0708.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-8\. The result of our case study.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Those elements will respond dynamically to changes in user input. This is precisely
    what we need for our work since this is an interactive product and not a static
    one. You can see all of the different code blocks that we used in preparation
    for this project. They should require little change, with the most notable one
    being the ability to capture the user input in the inference part. This can be
    done by accessing the `input` object.
  prefs: []
  type: TYPE_NORMAL
- en: Final thoughts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this case study, we demonstrated how one could take the best of both worlds
    and combine such excellent tools that modern data scientists have at our disposal
    to create remarkable user experiences, which delight visually and inform decision-making.
    This is but a basic example of such an elegant system, and we are confident that
    by showing you what’s possible, you - our readers - will create the data science
    products of the future!
  prefs: []
  type: TYPE_NORMAL
- en: '^([1](ch07.xhtml#idm45127447555192-marker)) Short, Karen C. 2017\. Spatial
    wildfire occurrence data for the United States, 1992-2015, FPA_FOD_20170508\.
    4th Edition. Fort Collins, CO: Forest Service Research Data Archive. [*https://doi.org/10.2737/RDS-2013-0009.4*](https://doi.org/10.2737/RDS-2013-0009.4)'
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch07.xhtml#idm45127447535416-marker)) We’ll leave a thorough development
    of a robust classification model to our motivated readers. Indeed you may also
    be interested in a regression that predicts the final fire size in acres. Curious
    readers will note that a few interesting notebooks are available on Kaggle to
    get you started.
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](ch07.xhtml#idm45127447533464-marker)) This is a far cry from developing,
    hosting, and deploying robust ML models, which, in any case, is not the focus
    of this book.
  prefs: []
  type: TYPE_NORMAL
- en: ^([4](ch07.xhtml#idm45127447463752-marker)) Some readers might not be familiar
    with this language. It is commonly used to specify configuration options as code,
    such as in this case.
  prefs: []
  type: TYPE_NORMAL
- en: ^([5](ch07.xhtml#idm45127447406008-marker)) This part can also be done very
    well within R, by using packages such as `dbplyr` or the using the Connections
    panel in RStudio.
  prefs: []
  type: TYPE_NORMAL
- en: ^([6](ch07.xhtml#idm45127447143208-marker)) This package is used to extend the
    `ggplot2` functionality for transformed datasets.
  prefs: []
  type: TYPE_NORMAL
- en: ^([7](ch07.xhtml#idm45127447128744-marker)) As another example of the modularity
    of the Python ML ecosystem have a look at the `imbalanced-learn` package [here](https://imbalanced-learn.org/)
    if you are looking for a solution to this.
  prefs: []
  type: TYPE_NORMAL
- en: ^([8](ch07.xhtml#idm45127446950648-marker)) This is not a thorough exposition
    of all possible methods or optimizations since our focus is on building a bilingual
    workflow, not exploring machine learning techniques in detail. Readers may choose
    to refer to the official `scikit-learn` documentation for further guidance, in
    the aptly named [“Choosing the right estimator”](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html)
  prefs: []
  type: TYPE_NORMAL
