- en: Chapter 2\. Data, Data, Data
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第2章。数据，数据，数据
- en: '*Maybe if I know where it all came from, and why, I would know where it’s all
    headed, and why.*'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*也许如果我知道一切的来龙去脉，我就会知道一切的发展方向和原因。*'
- en: Data is the fuel that powers most of AI systems. In this chapter, we will understand
    how data, and devising methods for extracting useful and actionable information
    from data, are at the heart of *perception* AI.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 数据是大多数AI系统的动力源。在本章中，我们将了解数据以及设计从数据中提取有用和可操作信息的方法，这是*感知* AI的核心。
- en: '*Perception AI* is based on statistical learning from data, where an AI agent,
    or a machine, perceives data from its environment, then detects patterns within
    this data, allowing it to draw conclusions and/or make decisions.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '*感知AI*基于从数据中进行统计学习，其中AI代理或机器从其环境中感知数据，然后检测数据中的模式，使其能够得出结论和/或做出决策。'
- en: 'Perception AI is different from three other types of AI:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 感知AI不同于其他三种类型的AI：
- en: '*Understanding* AI, where an AI system understands that the image it classified
    as a chair serves the function of sitting, the image it classified as cancer means
    that the person is sick and needs further medical attention, or the text book
    it read about linear algebra can be used to extract useful information from data.'
  id: totrans-5
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*理解* AI，AI系统理解将图像分类为椅子意味着它可以用来坐，将图像分类为癌症意味着人生病需要进一步的医疗关注，或者它读过的线性代数教科书可以用来从数据中提取有用的信息。'
- en: '*Control* AI, which has to do with controlling the physical parts of the AI
    agent in order to navigate spaces, open doors, serve coffee, *etc.*. Robotics
    have made significant progress in this area. We need to augment robots with *brains*
    that include perception AI and understanding AI, and connect those to control
    AI. Ideally, like humans, control AI then learns from its *physical* interactions
    with its environment, by passing that information to its perception and understanding
    systems, which in turn pass control commands to the agent’s control systems.'
  id: totrans-6
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*控制* AI，涉及控制AI代理的物理部分，以便在空间中导航，打开门，倒咖啡等。机器人在这个领域取得了重大进展。我们需要用包括感知AI和理解AI的*大脑*来增强机器人，并将其连接到控制AI。理想情况下，像人类一样，控制AI然后从其与环境的*物理*交互中学习，通过将该信息传递给其感知和理解系统，然后将控制命令传递给代理的控制系统。'
- en: '*Awareness* AI, where an AI agent has an inner experience similar to the human
    experience. Since we do not know yet how to mathematically define awareness, we
    do not visit its concept at all in this book.'
  id: totrans-7
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*意识* AI，其中AI代理具有类似于人类经历的内在体验。由于我们还不知道如何在数学上定义意识，因此在本书中我们根本不讨论这个概念。'
- en: 'Ideally, true human-like intelligence combines all four aspects described above:
    perception, understanding, control, and awareness. The main focus of this chapter,
    and the next few chapters, is *perception AI*. AI and data have become so intertwined,
    to the extent that it is now common, though erroneous, to use the terms Data Science
    and AI synonymously.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，真正类似于人类的智能结合了上述四个方面：感知，理解，控制和意识。本章和接下来的几章的主要重点是*感知AI*。AI和数据已经紧密结合，以至于现在常见的，尽管错误的，是将数据科学和AI视为同义词。
- en: Data for AI
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AI的数据
- en: 'At the core of many popular machine learning models, including the highly successful
    neural networks that brought Artificial Intelligence back into popular spotlight
    since AlexNet in 2012, lies a very simple mathematical problem:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多流行的机器学习模型的核心，包括自2012年AlexNet以来重新引起人们关注的高度成功的神经网络，存在一个非常简单的数学问题：
- en: '***Fit a given set of data points into an appropriate function (mapping an
    input to an output), that picks up on the important signals in the data and ignores
    the noise, then make sure this function performs well on new data.***'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '***将给定的数据点拟合到适当的函数中（将输入映射到输出），从数据中捕捉重要信号并忽略噪音，然后确保该函数在新数据上表现良好。***'
- en: 'Complexity and challenges, however, arise from various sources:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，复杂性和挑战来自各种来源。
- en: Hypothesis and features
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 假设和特征
- en: 'Neither the true function that generated the data nor all the features it actually
    depends on are known. We simply observe the data then try to estimate a hypothetical
    function that generated it. Our function tries to learn which features of the
    data are important for our predictions, classifications, decisions, or general
    purposes. It also learns how these features interact in order to produce the observed
    results. One of the great potential of AI in this context is its ability to pick
    up on subtle interactions between features of data that humans do not usually
    pick up on, since we are very good at observing strong features but ignoring more
    subtle ones. For example, we as humans can tell that a person’s monthly income
    affects their ability to pay back a loan, but we might not observe that their
    daily commute, or morning routine, might have a nontrivial effect on that as well.
    Some feature interactions are much simpler than others, such as linear interactions.
    Others are more complex, and are nonlinear. From a mathematical point of view,
    whether our feature interactions are simple (linear) or complex (nonlinear), we
    still have the same goal: Find the hypothetical function that fits your data and
    is able to make good predictions on new data. One extra complication arises here:
    There are many hypothetical functions that can *fit* the same data set, how do
    we know which ones to choose?'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 生成数据的真实函数以及它实际依赖的所有特征都是未知的。我们只是观察数据，然后尝试估计生成它的假设函数。我们的函数试图学习数据中哪些特征对我们的预测、分类、决策或一般目的很重要。它还学习这些特征如何相互作用以产生观察到的结果。在这种情况下，人工智能的一个巨大潜力是它能够捕捉到人类通常不会注意到的数据特征之间的微妙相互作用，因为我们非常擅长观察强特征，但忽略更微妙的特征。例如，我们作为人类可以知道一个人的月收入影响他们偿还贷款的能力，但我们可能不会观察到他们的日常通勤或早晨例行公事对此也可能有非常重要的影响。一些特征之间的相互作用比其他的简单得多，比如线性相互作用。其他的则更复杂，是非线性的。从数学角度来看，无论我们的特征相互作用是简单的（线性）还是复杂的（非线性），我们的目标仍然是相同的：找到一个适合你的数据并能够在新数据上做出良好预测的假设函数。这里还有一个额外的复杂性：有许多假设函数可以*拟合*相同的数据集，我们如何知道选择哪一个？
- en: Performance
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 性能
- en: Even after computing a hypothetical function that fits our data, how do we know
    whether it will perform well on new and unseen data? How do we know which performance
    measure to choose, and how to monitor this performance after deploying into the
    real world? Real world data and scenarios do not come to us all labeled with ground
    truths, so we cannot easily measure whether our AI system is doing well and making
    correct or appropriate predictions and decisions. We do not know what to measure
    the AI system’s results against. If real world data and scenarios were labeled
    with ground truths, then we would all be out of business since we would know what
    to do in every situation, there would be peace on Earth, and we would live happily
    ever after (not really, I wish it was that simple).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 即使计算出一个适合我们数据的假设函数，我们如何知道它在新的未知数据上表现良好呢？我们如何知道选择哪种性能度量，并且在部署到真实世界后如何监控这种性能？真实世界的数据和场景并不都带有地面真相的标签，所以我们不能轻易衡量我们的人工智能系统是否表现良好并做出正确或适当的预测和决策。我们不知道如何衡量人工智能系统的结果。如果真实世界的数据和场景都带有地面真相的标签，那么我们都会失业，因为我们会知道在每种情况下该做什么，地球上会和平，我们会幸福地生活下去（实际上并非如此，我希望事情能够如此简单）。
- en: Volume
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 数量
- en: Almost everything in the AI field is very high dimensional! The number of data
    instances, observed features, and unknown weights to be computed could be in the
    millions, and the required computation steps in the billions. Efficient storage,
    transport, exploration, preprocessing, structuring, and computation on such volumes
    of data become centerfold goals. In addition, exploring the landscapes of the
    involved high dimensional mathematical functions is a nontrivial endeavor.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能领域几乎所有的东西都是非常高维的！数据实例的数量、观察到的特征以及需要计算的未知权重可能都在百万级别，所需的计算步骤则在十亿级别。在这样大量数据上的高效存储、传输、探索、预处理、结构化和计算成为中心目标。此外，探索涉及高维数学函数的景观是一项非常艰巨的任务。
- en: Structure
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 结构
- en: 'The vast majority of data created by the modern world is unstructured. It is
    not organized in easy to query tables that contain labeled fields such as names,
    phone numbers, gender, age, zip codes, house prices, income level, *etc*. Unstructured
    data is everywhere: Posts on social media, user activity, word documents, pdf
    files, images, audio and video files, collaboration software data, traffic, seismic,
    and weather data, GPS, military movement, emails, instant messenger, mobile chat
    data, and many others. Some of these examples, such as email data, can be considered
    semi-structured, since emails come with headings that include the email’s metadata:
    From, To, Date, Time, Subject, Content-Type, Spam Status, *etc*. Moreover, large
    volumes of important data are not available in digital format and are fragmented
    over multiple and non-communicating data bases. Examples here include historical
    military data, museums, and hospital records. Presently, there is great momentum
    towards digitalizing our world and our cities, in order to leverage more AI applications.
    Overall, it is easier to draw insights from structured and labeled data than unstructured
    data. Mining unstructured data requires innovative techniques that are currently
    driving forces in the fields of data science, machine learning, and artificial
    intelligence.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 现代世界创造的绝大部分数据都是非结构化的。它不是以易于查询的表格形式组织的，其中包含有标记的字段，比如姓名，电话号码，性别，年龄，邮政编码，房价，收入水平，*等等*。非结构化数据无处不在：社交媒体上的帖子，用户活动，文字文档，PDF文件，图像，音频和视频文件，协作软件数据，交通，地震和天气数据，GPS，军事行动，电子邮件，即时通讯软件，移动聊天数据等等。其中一些例子，比如电子邮件数据，可以被视为半结构化的，因为电子邮件带有包括邮件元数据的标题：发件人，收件人，日期，时间，主题，内容类型，垃圾邮件状态，*等等*。此外，大量重要数据并不以数字格式可用，并且分散在多个不相互通信的数据库中。这里的例子包括历史军事数据，博物馆和医院记录。目前，我们正努力将我们的世界和城市数字化，以利用更多的人工智能应用。总的来说，从结构化和标记的数据中获得见解比从非结构化数据中获得见解更容易。挖掘非结构化数据需要创新技术，这些技术目前是数据科学，机器学习和人工智能领域的推动力。
- en: Real Data *vs.* Simulated Data
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 真实数据*vs.*模拟数据
- en: When we work with data, it is very important to know the difference between
    real data and simulated data. Both types of data are extremely valuable for human
    discovery and progress.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们处理数据时，非常重要的是要知道真实数据和模拟数据之间的区别。这两种类型的数据对于人类的发现和进步都非常宝贵。
- en: Real data
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 真实数据
- en: This data is collected through real world observations, using measuring devices,
    sensors, surveys, structured forms like medical questionnaires, telescopes, imaging
    devices, websites, stock markets, controlled experiments, *etc*. This data is
    often imperfect and noisy, due to inaccuracies and failures in measuring methods
    and instruments. Mathematically, *we do not know the exact function or probability
    distribution that generated the real data* but we can hypothesize about them using
    models, theories, and simulations. We can then test our models, and finally use
    them to make predictions.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数据是通过真实世界观察收集的，使用测量设备，传感器，调查，结构化表格（如医学问卷），望远镜，成像设备，网站，股票市场，受控实验，*等等*。这些数据通常是不完美和嘈杂的，因为测量方法和仪器的不准确性和故障。从数学上讲，*我们不知道生成真实数据的确切函数或概率分布*，但我们可以使用模型，理论和模拟来假设它们。然后我们可以测试我们的模型，最终使用它们进行预测。
- en: Simulated data
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 模拟数据
- en: 'This is data generated using a *known* function or randomly sampled from a
    *known* probability distribution. Here, we have our known mathematical function(s),
    or *model*, and we plug numerical values into the model inorder to generate our
    data points. Examples are plentiful: Numerical solutions of partial differential
    equations modeling all kinds of natural phenomena, on all kinds of scales, such
    as turbulent flows, protein folding, heat diffusion, chemical reactions, planetary
    motion, fractured materials, traffic, and even Disney movie animations (Frozen,
    Moana, *etc.*).'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这是使用*已知*函数生成的数据，或者从*已知*概率分布中随机抽样得到的数据。在这里，我们有我们已知的数学函数（们），或者*模型*，我们将数值代入模型中以生成我们的数据点。例子很多：数值解决了模拟各种自然现象的偏微分方程，涵盖了各种规模，比如湍流流动，蛋白质折叠，热扩散，化学反应，行星运动，断裂材料，交通，甚至迪士尼电影动画（《冰雪奇缘》，《海洋奇缘》，*等等*）。
- en: 'In this chapter, we present two examples about human height and weight data
    in order to demonstrate the difference between real and simulated data. In the
    first example, we visit an online public database, then download and explore two
    real data sets containing measurements of the heights and weights of real individuals.
    In the second example, we simulate our own data set of heights and weights *based
    on a function that we hypothesize*: We assume that the weight of an individual
    depends linearly on their height, meaning that when we plot the weight data against
    the height data, we expect to see a straight, or flat, visual pattern.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们提供了两个关于人类身高和体重数据的例子，以演示真实数据和模拟数据之间的区别。在第一个例子中，我们访问一个在线公共数据库，然后下载并探索两个包含真实个体身高和体重测量的真实数据集。在第二个例子中，我们根据我们假设的函数模拟了自己的身高和体重数据集：我们假设一个人的体重在数值上取决于他们的身高，这意味着当我们将体重数据与身高数据绘制在一起时，我们期望看到一个直线或平坦的视觉模式。
- en: 'Mathematical Models: Linear *vs.* Nonlinear'
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数学模型：线性*vs.*非线性
- en: Linear dependencies model flatness in the world, like one dimensional straight-lines,
    two dimensional flat surfaces (called *planes*), and higher dimensional hyper-planes.
    The graph of a linear function, which models a linear dependency, is forever flat
    and does not bend. Every time you see a flat object, like a table, a rod, a ceiling,
    or a bunch of data points huddled together around a straight-line or a flat surface,
    know that their representative function is linear. Anything other than flat is
    nonlinear, so functions whose graphs bend are nonlinear, and data points which
    congregate around bending curves or surfaces must have been generated by nonlinear
    functions.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 线性依赖模拟了世界上的平坦，比如一维直线、二维平面（称为*平面*）和更高维的超平面。线性函数的图形，它模拟了线性依赖，永远是平的，不会弯曲。每当你看到一个平的物体，比如桌子、杆、天花板，或者一堆数据点聚集在一条直线或一个平面周围，知道它们代表的函数是线性的。除了平的东西是非线性的，所以图形弯曲的函数是非线性的，而聚集在弯曲曲线或曲面周围的数据点必须是由非线性函数生成的。
- en: 'The formula for a linear function, representing a linear dependency of the
    function output on the *features*, or *variables*, is very easy to write down.
    The features appear in the formula as just themselves, with no powers or roots,
    and are not embedded in any other functions, such as denominators of fractions,
    sine, cosine, exponential, logarithmic or other calculus functions. They can only
    be multiplied by *scalars* (real or complex numbers, not vectors or matrices),
    and added or subtracted to and from each other. For example, a function that depends
    linearly on three features <math alttext="x 1 comma x 2"><mrow><msub><mi>x</mi>
    <mn>1</mn></msub> <mo>,</mo> <msub><mi>x</mi> <mn>2</mn></msub></mrow></math>
    and <math alttext="x 3"><msub><mi>x</mi> <mn>3</mn></msub></math> can be written
    as:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 线性函数的公式，代表了函数输出对*特征*或*变量*的线性依赖，非常容易写下来。这些特征在公式中只是它们自己，没有幂或根，并且没有嵌入在任何其他函数中，比如分数的分母、正弦、余弦、指数、对数或其他微积分函数。它们只能被*标量*（实数或复数，而不是向量或矩阵）相乘，并且可以相互加减。例如，一个依赖于三个特征<x
    1，x 2>和<x 3>的线性函数可以写成：
- en: <math alttext="dollar-sign f left-parenthesis x 1 comma x 2 comma x 3 right-parenthesis
    equals omega 0 plus omega 1 x 1 plus omega 2 x 2 plus omega 3 x 3 comma dollar-sign"><mrow><mi>f</mi>
    <mrow><mo>(</mo> <msub><mi>x</mi> <mn>1</mn></msub> <mo>,</mo> <msub><mi>x</mi>
    <mn>2</mn></msub> <mo>,</mo> <msub><mi>x</mi> <mn>3</mn></msub> <mo>)</mo></mrow>
    <mo>=</mo> <msub><mi>ω</mi> <mn>0</mn></msub> <mo>+</mo> <msub><mi>ω</mi> <mn>1</mn></msub>
    <msub><mi>x</mi> <mn>1</mn></msub> <mo>+</mo> <msub><mi>ω</mi> <mn>2</mn></msub>
    <msub><mi>x</mi> <mn>2</mn></msub> <mo>+</mo> <msub><mi>ω</mi> <mn>3</mn></msub>
    <msub><mi>x</mi> <mn>3</mn></msub> <mo>,</mo></mrow></math>
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="dollar-sign f left-parenthesis x 1 comma x 2 comma x 3 right-parenthesis
    equals omega 0 plus omega 1 x 1 plus omega 2 x 2 plus omega 3 x 3 comma dollar-sign"><mrow><mi>f</mi>
    <mrow><mo>(</mo> <msub><mi>x</mi> <mn>1</mn></msub> <mo>,</mo> <msub><mi>x</mi>
    <mn>2</mn></msub> <mo>,</mo> <msub><mi>x</mi> <mn>3</mn></msub> <mo>)</mo></mrow>
    <mo>=</mo> <msub><mi>ω</mi> <mn>0</mn></sub> <mo>+</mo> <msub><mi>ω</mi> <mn>1</mn></sub>
    <msub><mi>x</mi> <mn>1</mn></sub> <mo>+</mo> <msub><mi>ω</mi> <mn>2</mn></sub>
    <msub><mi>x</mi> <mn>2</mn></sub> <mo>+</mo> <msub><mi>ω</mi> <mn>3</mn></sub>
    <msub><mi>x</mi> <mn>3</mn></sub> <mo>,</mo></mrow></math>
- en: where the parameters <math alttext="omega 0 comma omega 1 comma omega 2"><mrow><msub><mi>ω</mi>
    <mn>0</mn></msub> <mo>,</mo> <msub><mi>ω</mi> <mn>1</mn></msub> <mo>,</mo> <msub><mi>ω</mi>
    <mn>2</mn></msub></mrow></math> and <math alttext="omega 3"><msub><mi>ω</mi> <mn>3</mn></msub></math>
    are scalar numbers. The parameters, or *weights* <math alttext="omega 1 comma
    omega 2"><mrow><msub><mi>ω</mi> <mn>1</mn></msub> <mo>,</mo> <msub><mi>ω</mi>
    <mn>2</mn></msub></mrow></math> , and <math alttext="omega 3"><msub><mi>ω</mi>
    <mn>3</mn></msub></math> *linearly combine* the features, and produce the outcome
    of <math alttext="f left-parenthesis x 1 comma x 2 comma x 3 right-parenthesis"><mrow><mi>f</mi>
    <mo>(</mo> <msub><mi>x</mi> <mn>1</mn></msub> <mo>,</mo> <msub><mi>x</mi> <mn>2</mn></msub>
    <mo>,</mo> <msub><mi>x</mi> <mn>3</mn></msub> <mo>)</mo></mrow></math> after adding
    the *bias* term <math alttext="omega 0"><msub><mi>ω</mi> <mn>0</mn></msub></math>
    . In other words, the outcome is produced as a result of *linear interactions
    between the features* <math alttext="x 1 comma x 2"><mrow><msub><mi>x</mi> <mn>1</mn></msub>
    <mo>,</mo> <msub><mi>x</mi> <mn>2</mn></msub></mrow></math> and <math alttext="x
    3"><msub><mi>x</mi> <mn>3</mn></msub></math> , plus bias.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 其中参数<math alttext="omega 0，omega 1，omega 2"><mrow><msub><mi>ω</mi> <mn>0</mn></sub>
    <mo>,</mo> <msub><mi>ω</mi> <mn>1</mn></sub> <mo>,</mo> <msub><mi>ω</mi> <mn>2</mn></sub></mrow></math>和<math
    alttext="omega 3"><msub><mi>ω</mi> <mn>3</mn></sub></math>是标量数。这些参数，或*权重*<math
    alttext="omega 1，omega 2"><mrow><msub><mi>ω</mi> <mn>1</mn></sub> <mo>,</mo> <msub><mi>ω</mi>
    <mn>2</mn></sub></mrow></math>和<math alttext="omega 3"><msub><mi>ω</mi> <mn>3</mn></sub></math>
    *线性组合*特征，并在添加*偏置*项<math alttext="omega 0"><msub><mi>ω</mi> <mn>0</mn></sub></math>后产生<math
    alttext="f left-parenthesis x 1 comma x 2 comma x 3 right-parenthesis"><mrow><mi>f</mi>
    <mo>(</mo> <msub><mi>x</mi> <mn>1</mn></sub> <mo>,</mo> <msub><mi>x</mi> <mn>2</mn></sub>
    <mo>,</mo> <msub><mi>x</mi> <mn>3</mn></sub> <mo>)</mo></mrow></math>的结果。换句话说，结果是由特征之间的*线性相互作用*<math
    alttext="x 1，x 2"><mrow><msub><mi>x</mi> <mn>1</mn></sub> <mo>,</mo> <msub><mi>x</mi>
    <mn>2</mn></sub></mrow></math>和<math alttext="x 3"><msub><mi>x</mi> <mn>3</mn></sub></math>以及偏置产生的。
- en: 'The formula for a nonlinear function, representing a nonlinear dependency of
    the function output on the features, is very easy to spot as well: One or more
    features appear in the function formula with a power other than one, or multiplied
    or divided by other features, or embedded in some other calculus functions, such
    as sines, cosines, exponentials, logarithms, *etc*. The following are three examples
    of functions depending nonlinearly on three features <math alttext="x 1 comma
    x 2"><mrow><msub><mi>x</mi> <mn>1</mn></msub> <mo>,</mo> <msub><mi>x</mi> <mn>2</mn></msub></mrow></math>
    and <math alttext="x 3"><msub><mi>x</mi> <mn>3</mn></msub></math> :'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 非线性函数的公式，表示函数输出对特征的非线性依赖，也很容易识别：一个或多个特征以非一次幂出现在函数公式中，或者与其他特征相乘或相除，或者嵌入在其他微积分函数中，如正弦、余弦、指数、对数等。以下是三个关于三个特征x1，x2和x3非线性依赖的函数的例子：
- en: <math alttext="dollar-sign 1 period f left-parenthesis x 1 comma x 2 comma x
    3 right-parenthesis equals omega 0 plus omega 1 StartRoot x 1 EndRoot plus omega
    2 StartFraction x 2 Over x 3 EndFraction period dollar-sign"><mrow><mn>1</mn>
    <mo>.</mo> <mi>f</mi> <mrow><mo>(</mo> <msub><mi>x</mi> <mn>1</mn></msub> <mo>,</mo>
    <msub><mi>x</mi> <mn>2</mn></msub> <mo>,</mo> <msub><mi>x</mi> <mn>3</mn></msub>
    <mo>)</mo></mrow> <mo>=</mo> <msub><mi>ω</mi> <mn>0</mn></msub> <mo>+</mo> <msub><mi>ω</mi>
    <mn>1</mn></msub> <msqrt><msub><mi>x</mi> <mn>1</mn></msub></msqrt> <mo>+</mo>
    <msub><mi>ω</mi> <mn>2</mn></msub> <mfrac><msub><mi>x</mi> <mn>2</mn></msub> <msub><mi>x</mi>
    <mn>3</mn></msub></mfrac> <mo>.</mo></mrow></math><math alttext="dollar-sign 2
    period f left-parenthesis x 1 comma x 2 comma x 3 right-parenthesis equals omega
    0 plus omega 1 x 1 squared plus omega 2 x 2 squared plus omega 3 x 3 squared period
    dollar-sign"><mrow><mn>2</mn> <mo>.</mo> <mi>f</mi> <mrow><mo>(</mo> <msub><mi>x</mi>
    <mn>1</mn></msub> <mo>,</mo> <msub><mi>x</mi> <mn>2</mn></msub> <mo>,</mo> <msub><mi>x</mi>
    <mn>3</mn></msub> <mo>)</mo></mrow> <mo>=</mo> <msub><mi>ω</mi> <mn>0</mn></msub>
    <mo>+</mo> <msub><mi>ω</mi> <mn>1</mn></msub> <msubsup><mi>x</mi> <mn>1</mn> <mn>2</mn></msubsup>
    <mo>+</mo> <msub><mi>ω</mi> <mn>2</mn></msub> <msubsup><mi>x</mi> <mn>2</mn> <mn>2</mn></msubsup>
    <mo>+</mo> <msub><mi>ω</mi> <mn>3</mn></msub> <msubsup><mi>x</mi> <mn>3</mn> <mn>2</mn></msubsup>
    <mo>.</mo></mrow></math><math alttext="dollar-sign 3 period f left-parenthesis
    x 1 comma x 2 comma x 3 right-parenthesis equals omega 1 e Superscript x 1 Baseline
    plus omega 2 e Superscript x 2 Baseline plus omega 3 cosine left-parenthesis x
    3 right-parenthesis period dollar-sign"><mrow><mn>3</mn> <mo>.</mo> <mi>f</mi>
    <mrow><mo>(</mo> <msub><mi>x</mi> <mn>1</mn></msub> <mo>,</mo> <msub><mi>x</mi>
    <mn>2</mn></msub> <mo>,</mo> <msub><mi>x</mi> <mn>3</mn></msub> <mo>)</mo></mrow>
    <mo>=</mo> <msub><mi>ω</mi> <mn>1</mn></msub> <msup><mi>e</mi> <msub><mi>x</mi>
    <mn>1</mn></msub></msup> <mo>+</mo> <msub><mi>ω</mi> <mn>2</mn></msub> <msup><mi>e</mi>
    <msub><mi>x</mi> <mn>2</mn></msub></msup> <mo>+</mo> <msub><mi>ω</mi> <mn>3</mn></msub>
    <mo form="prefix">cos</mo> <mrow><mo>(</mo> <msub><mi>x</mi> <mn>3</mn></msub>
    <mo>)</mo></mrow> <mo>.</mo></mrow></math>
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="美元符号1期f左括号x1，x2，x3右括号等于omega0加omega1 StartRoot x1 EndRoot加omega2
    StartFraction x2 Over x3 EndFraction period dollar-sign"><mrow><mn>1</mn> <mo>。</mo>
    <mi>f</mi> <mrow><mo>（</mo> <msub><mi>x</mi> <mn>1</mn></msub> <mo>，</mo> <msub><mi>x</mi>
    <mn>2</mn></msub> <mo>，</mo> <msub><mi>x</mi> <mn>3</mn></msub> <mo>）</mo></mrow>
    <mo>=</mo> <msub><mi>ω</mi> <mn>0</mn></msub> <mo>+</mo> <msub><mi>ω</mi> <mn>1</mn></msub>
    <msqrt><msub><mi>x</mi> <mn>1</mn></msub></msqrt> <mo>+</mo> <msub><mi>ω</mi>
    <mn>2</mn></msub> <mfrac><msub><mi>x</mi> <mn>2</mn></msub> <msub><mi>x</mi> <mn>3</mn></msub></mfrac>
    <mo>。</mo></mrow></math><math alttext="美元符号2期f左括号x1，x2，x3右括号等于omega0加omega1 x1平方加omega2
    x2平方加omega3 x3平方 period dollar-sign"><mrow><mn>2</mn> <mo>。</mo> <mi>f</mi> <mrow><mo>（</mo>
    <msub><mi>x</mi> <mn>1</mn></msub> <mo>，</mo> <msub><mi>x</mi> <mn>2</mn></msub>
    <mo>，</mo> <msub><mi>x</mi> <mn>3</mn></msub> <mo>）</mo></mrow> <mo>=</mo> <msub><mi>ω</mi>
    <mn>0</mn></msub> <mo>+</mo> <msub><mi>ω</mi> <mn>1</mn></msub> <msubsup><mi>x</mi>
    <mn>1</mn> <mn>2</mn></msubsup> <mo>+</mo> <msub><mi>ω</mi> <mn>2</mn></msub>
    <msubsup><mi>x</mi> <mn>2</mn> <mn>2</mn></msubsup> <mo>+</mo> <msub><mi>ω</mi>
    <mn>3</mn></msub> <msubsup><mi>x</mi> <mn>3</mn> <mn>2</mn></msubsup> <mo>。</mo></mrow></math><math
    alttext="美元符号3期f左括号x1，x2，x3右括号等于omega1 e Superscript x1 Baseline加omega2 e Superscript
    x2 Baseline加omega3 cosine左括号x3右括号 period dollar-sign"><mrow><mn>3</mn> <mo>。</mo>
    <mi>f</mi> <mrow><mo>（</mo> <msub><mi>x</mi> <mn>1</mn></msub> <mo>，</mo> <msub><mi>x</mi>
    <mn>2</mn></msub> <mo>，</mo> <msub><mi>x</mi> <mn>3</mn></msub> <mo>）</mo></mrow>
    <mo>=</mo> <msub><mi>ω</mi> <mn>1</mn></msub> <msup><mi>e</mi> <msub><mi>x</mi>
    <mn>1</mn></msub></msup> <mo>+</mo> <msub><mi>ω</mi> <mn>2</mn></msub> <msup><mi>e</mi>
    <msub><mi>x</mi> <mn>2</mn></msub></msup> <mo>+</mo> <msub><mi>ω</mi> <mn>3</mn></msub>
    <mo form="prefix">cos</mo> <mrow><mo>（</mo> <msub><mi>x</mi> <mn>3</mn></msub>
    <mo>）</mo></mrow> <mo>。</mo></mrow></math>
- en: As you can tell, we can come up with all kinds of nonlinear functions, and the
    possibilities related to what we can do and how much of the world we can model
    using nonlinear interactions are limitless. In fact, neural networks are successful
    because of their ability to pick up on the relevant *nonlinear* interactions between
    the features of the data.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，我们可以提出各种各样的非线性函数，以及与非线性交互相关的可能性是无限的。事实上，神经网络之所以成功，是因为它们能够捕捉数据特征之间的相关*非线性*交互。
- en: We will use the above notation and terminology throughout the book, so you will
    become very familiar with terms like linear combination, weights, features, and
    linear and nonlinear interactions between features.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在整本书中使用上述符号和术语，因此您将非常熟悉线性组合、权重、特征以及特征之间的线性和非线性交互等术语。
- en: An Example of Real Data
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一个真实数据的例子
- en: You can find the python code to investigate the data and produce the figures
    in the following two examples at the [linked Jupyter notebook for Chapter 2](https://github.com/halanelson/Essential-Math-For-AI).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在[第2章的链接Jupyter笔记本](https://github.com/halanelson/Essential-Math-For-AI)中找到调查数据并生成图表的Python代码的两个示例。
- en: 'Note: Structured Data'
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 注意：结构化数据
- en: The two data sets for Height, Weight, and Gender that we will work with here
    are examples of *structured* data sets. They come organized in rows and columns.
    Columns contain the features, such as weight, height, gender, health index, *etc.*
    Rows contain the feature scores for each data instance, in this case, each person.
    On the other hand, data sets that are a bunch of audio files, Facebook posts,
    images, or videos are all examples of unstructured data sets.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在这里使用的身高、体重和性别的两个数据集是*结构化*数据集的示例。它们以行和列的形式组织。列包含特征，如体重、身高、性别、健康指数，*等等*。行包含每个数据实例的特征分数，也就是每个人的情况。另一方面，一堆音频文件、Facebook帖子、图像或视频都是非结构化数据集的示例。
- en: 'I downloaded two data sets from [Kaggle website](https://www.kaggle.com) for
    data scientists. Both data sets contain height, weight and gender information
    for a certain number of individuals. My goal is to learn how the weight of a person
    depends on their height. Mahematically, I want to write a formula for the weight
    as a function of one feature, the height:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我从数据科学家的[Kaggle网站](https://www.kaggle.com)下载了两个数据集。这两个数据集都包含了一定数量的个体的身高、体重和性别信息。我的目标是了解一个人的体重如何取决于他们的身高。从数学上讲，我想要写出一个公式，将体重作为一个特征，即身高的函数：
- en: <math alttext="dollar-sign w e i g h t equals f left-parenthesis h e i g h t
    right-parenthesis comma dollar-sign"><mrow><mi>w</mi> <mi>e</mi> <mi>i</mi> <mi>g</mi>
    <mi>h</mi> <mi>t</mi> <mo>=</mo> <mi>f</mi> <mo>(</mo> <mi>h</mi> <mi>e</mi> <mi>i</mi>
    <mi>g</mi> <mi>h</mi> <mi>t</mi> <mo>)</mo> <mo>,</mo></mrow></math>
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 数学符号“美元符号重量等于f左括号高度右括号逗号美元符号”
- en: so that if I am given the height of a *new* person, I would be able to *predict*
    their weight. Of course, there are other features than height that a person’s
    weight depends on, such as their gender, eating habits, workout habits, genetic
    predisposition, *etc.*. However, for the data sets that I downloaded, we only
    have height, weight, and gender data available. Unless we want to look for more
    detailed data sets, or go out and collect new data, we have to work with what
    we have. Moreover, the goal of this example is only to illustrate the difference
    between real data and simulated data. We will be working with more involved data
    sets with larger number of features when we have more involved goals.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果我得到一个*新*人的身高，我将能够*预测*他们的体重。当然，人的体重除了身高之外还取决于其他特征，比如他们的性别、饮食习惯、锻炼习惯、遗传倾向，*等等*。但是，对于我下载的数据集，我们只有身高、体重和性别数据可用。除非我们想要寻找更详细的数据集，或者出去收集新数据，我们必须使用我们手头的数据。此外，这个例子的目标只是为了说明真实数据和模拟数据之间的差异。当我们有更深入的目标时，我们将使用更多特征的更复杂的数据集。
- en: For the [first data set](https://www.kaggle.com/yersever/500-person-gender-height-weight-bodymassindex),
    I plot the weight column against the height column in [Figure 2-1](#Fig_height_weight_biased),
    and obtain something that seems to have no pattern at all!
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 对于[第一个数据集](https://www.kaggle.com/yersever/500-person-gender-height-weight-bodymassindex)，我在[图2-1](#Fig_height_weight_biased)中绘制了体重列与身高列，得到了似乎完全没有模式的结果！
- en: '![250](assets/emai_0201.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![250](assets/emai_0201.png)'
- en: Figure 2-1\. When plotting the weight against the height for [the first data
    set](https://www.kaggle.com/yersever/500-person-gender-height-weight-bodymassindex),
    we cannot detect a pattern. The graphs on the top and on the right of the scatter
    plot show the respective histograms and emperical distributions of the height
    and weight data.
  id: totrans-46
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-1。当绘制[第一个数据集](https://www.kaggle.com/yersever/500-person-gender-height-weight-bodymassindex)的体重与身高时，我们无法检测到模式。散点图上方和右侧的图显示了身高和体重数据的直方图和经验分布。
- en: For the [second data set](https://www.kaggle.com/mustafaali96/weight-height),
    I do the same, and I can visually observe an obvious linear dependency in [Figure 2-2](#Fig_height_weight).
    The data points seem to congregate around a straight-line!
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 对于[第二个数据集](https://www.kaggle.com/mustafaali96/weight-height)，我做了同样的事情，我可以在[图2-2](#Fig_height_weight)中明显观察到线性依赖。数据点似乎聚集在一条直线周围！
- en: '![250](assets/emai_0202.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![250](assets/emai_0202.png)'
- en: Figure 2-2\. When plotting the weight against the height for [the second data
    set](https://www.kaggle.com/mustafaali96/weight-height), we observe a linear pattern.
    Note that the emperical distribution of the weight data is plotted on the right
    hand side of the figure, and the emperical distribution of the height data is
    plotted on the top of the figure. Both seem to have two peaks (bimodal), suggesting
    the existence of a mixture distribution. In fact, both the height and weight data
    sets can be modeled using a mixture of two normal distributions, called Gaussian
    mixtures, representing mixing the underlying distributions for the male and female
    data. So if we plot the data for either the female or male subpopulations alone,
    as in [Figure 2-6](#Fig_height_weight_F), we observe that the height and weight
    data are normally distributed (bell shaped).
  id: totrans-49
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-2。当绘制[第二个数据集](https://www.kaggle.com/mustafaali96/weight-height)的体重与身高时，我们观察到线性模式。请注意，体重数据的经验分布绘制在图的右侧，身高数据的经验分布绘制在图的顶部。两者似乎都有两个峰值（双峰），表明存在混合分布。实际上，身高和体重数据集都可以使用两个正态分布的混合来建模，称为高斯混合，代表混合男性和女性数据的基础分布。因此，如果我们单独绘制女性或男性亚群的数据，就像[图2-6](#Fig_height_weight_F)中那样，我们会观察到身高和体重数据呈正态分布（钟形）。
- en: So what is going on? Why does my first real data set reflect no dependency between
    the height and weight of a person whatsoever, but my second one reflects a linear
    dependency? We need to investigate deeper into the data.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 发生了什么？为什么我的第一个真实数据集完全没有反映出人的身高和体重之间的任何依赖关系，但我的第二个数据集反映出了线性依赖？我们需要深入研究数据。
- en: This is one of the many challenges of working with real data. We do not know
    what function generated the data, and why it looks the way it looks. We investigate,
    gain insights, detect patterns, if any, and we propose a hypothesis function.
    Then we test our hypothesis, and if it performs well based on our measures of
    performance, which have to be thoughtfully crafted, we deploy it into the real
    world. We make predictions using our deployed model, until new data tells us that
    our hypothesis is no longer valid, in which case, we investigate the updated data,
    and formulate a new hypothesis. This process and feedback loop keeps going for
    as long as our models are in business.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这是处理真实数据时面临的许多挑战之一。我们不知道是什么函数生成了数据，以及为什么数据看起来是这个样子。我们调查、获得洞察力、检测模式（如果有的话），并提出一个假设函数。然后我们测试我们的假设，如果根据我们的性能指标表现良好，我们就将其部署到现实世界中。我们使用我们部署的模型进行预测，直到新数据告诉我们我们的假设不再有效为止，在这种情况下，我们调查更新的数据，并提出一个新的假设。只要我们的模型在运行，这个过程和反馈循环就会继续下去。
- en: Before moving on to simulated data, let’s explain why the first data set seemed
    to have no insight at all about the relationship between the height and the weight
    of an individual. Upon further inspection, we notice that the data set has an
    overrepresenation of individuals with Index scores 4 and 5, referring to Obesity
    and Extreme Obesity. So, I decided to split the data by Index score, and plot
    the weight against the height for all individuals with similar Index scores. This
    time around, a linear dependency between the height and the weight is evident
    in [Figure 2-3](#Fig_height_weight_3), and the mystery is resolved. This might
    feel like we are cheating our way to linearity, by conditioning on individuals’
    index scores. All is fair game in the name of data exploration.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在转向模拟数据之前，让我们解释一下为什么第一个数据集似乎对个体的身高和体重之间的关系没有任何见解。经过进一步检查，我们注意到数据集中有过多指数分数为4和5的个体，指的是肥胖和极度肥胖。因此，我决定按指数分数拆分数据，并为所有具有相似指数分数的个体绘制体重与身高的图。这一次，在[图2-3](#Fig_height_weight_3)中明显地显示了身高和体重之间的线性依赖关系，谜团得到了解决。这可能会让人觉得我们在通过个体的指数分数进行条件处理来获得线性关系，但在数据探索的名义下，一切都是公平竞争。
- en: '![250](assets/emai_0203.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![250](assets/emai_0203.png)'
- en: Figure 2-3\. When plotting the weight against the height for individuals with
    similar Index score in the [the first data set](https://www.kaggle.com/yersever/500-person-gender-height-weight-bodymassindex),
    we observe a linear pattern. This figure shows the weight against the height for
    individuals with Index score 3.
  id: totrans-54
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-3。当为[第一个数据集](https://www.kaggle.com/yersever/500-person-gender-height-weight-bodymassindex)中具有相似指数分数的个体绘制体重与身高时，我们观察到了线性模式。这张图显示了指数分数为3的个体的体重与身高。
- en: 'Now we can safely go ahead and hypothesize that the weight depends linearly
    on the height:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以放心地假设体重是线性依赖于身高的：
- en: <math alttext="dollar-sign w e i g h t equals omega 0 plus omega 1 times h e
    i g h t period dollar-sign"><mrow><mi>w</mi> <mi>e</mi> <mi>i</mi> <mi>g</mi>
    <mi>h</mi> <mi>t</mi> <mo>=</mo> <msub><mi>ω</mi> <mn>0</mn></msub> <mo>+</mo>
    <msub><mi>ω</mi> <mn>1</mn></msub> <mo>×</mo> <mi>h</mi> <mi>e</mi> <mi>i</mi>
    <mi>g</mi> <mi>h</mi> <mi>t</mi> <mo>.</mo></mrow></math>
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="dollar-sign w e i g h t equals omega 0 plus omega 1 times h e
    i g h t period dollar-sign"><mrow><mi>w</mi> <mi>e</mi> <mi>i</mi> <mi>g</mi>
    <mi>h</mi> <mi>t</mi> <mo>=</mo> <msub><mi>ω</mi> <mn>0</mn></msub> <mo>+</mo>
    <msub><mi>ω</mi> <mn>1</mn></msub> <mo>×</mo> <mi>h</mi> <mi>e</mi> <mi>i</mi>
    <mi>g</mi> <mi>h</mi> <mi>t</mi> <mo>.</mo></mrow></math>
- en: Of course, we are left with the task of finding appropriate values for the parameters
    <math alttext="omega 0"><msub><mi>ω</mi> <mn>0</mn></msub></math> and <math alttext="omega
    1"><msub><mi>ω</mi> <mn>1</mn></msub></math> . The next chapter teaches us how
    to do exactly that. In fact, the bulk of activity in machine learning, including
    deep learning, is about *learning* these <math alttext="omega"><mi>ω</mi></math>
    ’s *from the data*. In our very simple example, we only have two <math alttext="omega"><mi>ω</mi></math>
    ’s to learn, since we only had one feature, the height, and we *assumed* linear
    dependency, after observing a linear pattern in the real data. In the next few
    chapters, we will encounter some deep learning networks with millions of <math
    alttext="omega"><mi>ω</mi></math> ’s to learn, yet, we will see that the mathematical
    structure of the problem is in fact the same exact structure that we will learn
    in [Chapter 3](ch03.xhtml#ch03).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我们还需要找到参数<math alttext="omega 0"><msub><mi>ω</mi> <mn>0</mn></msub></math>和<math
    alttext="omega 1"><msub><mi>ω</mi> <mn>1</mn></msub></math>的适当值。下一章将教会我们如何做到这一点。事实上，机器学习，包括深度学习，的大部分活动都是关于*从数据中学习*这些<math
    alttext="omega"><mi>ω</mi></math>。在我们非常简单的例子中，我们只需要学习两个<math alttext="omega"><mi>ω</mi></math>，因为我们只有一个特征，身高，并且在观察到真实数据中的线性模式后，我们*假设*了线性依赖关系。在接下来的几章中，我们将遇到一些有数百万个<math
    alttext="omega"><mi>ω</mi></math>需要学习的深度学习网络，然而，我们将看到问题的数学结构实际上与我们将在[第3章](ch03.xhtml#ch03)中学到的完全相同的结构。
- en: An Example of Simulated Data
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模拟数据的一个例子
- en: In this example, I simulate my own Height-Weight data set. Simulating our own
    data circumvents the trouble of searching for data from the web, the real world,
    or even building a lab in order to obtain controlled measurements. This is incredibly
    valuable when the required data is not available, or very expensive to obtain.
    It also helps test different scenarios by only changing numbers in a function,
    as opposed to say, creating new materials, or building labs, and running new experiments.
    Simulating data is so convenient because all we need is a mathematical function,
    a probability distribution if we want to involve randomness and/or noise, and
    a computer.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我模拟了自己的身高-体重数据集。模拟我们自己的数据可以避免在网上搜索数据、现实世界中寻找数据，甚至建立实验室以获得受控的测量数据的麻烦。当所需数据不可用或非常昂贵时，这是非常有价值的。它还可以通过只改变函数中的数字来测试不同的场景，而不是创建新材料、建立实验室并进行新实验。模拟数据非常方便，因为我们只需要一个数学函数，如果我们想要涉及随机性和/或噪声，还需要一个概率分布，以及一台计算机。
- en: 'Let’s again assume linear dependency between the height and the weight, so
    the function that we will use is:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次假设身高和体重之间存在线性依赖关系，因此我们将使用的函数是：
- en: <math alttext="dollar-sign w e i g h t equals omega 0 plus omega 1 times h e
    i g h t period dollar-sign"><mrow><mi>w</mi> <mi>e</mi> <mi>i</mi> <mi>g</mi>
    <mi>h</mi> <mi>t</mi> <mo>=</mo> <msub><mi>ω</mi> <mn>0</mn></msub> <mo>+</mo>
    <msub><mi>ω</mi> <mn>1</mn></msub> <mo>×</mo> <mi>h</mi> <mi>e</mi> <mi>i</mi>
    <mi>g</mi> <mi>h</mi> <mi>t</mi> <mo>.</mo></mrow></math>
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="重量等于omega 0加omega 1乘以高度"><mrow><mi>w</mi> <mi>e</mi> <mi>i</mi>
    <mi>g</mi> <mi>h</mi> <mi>t</mi> <mo>=</mo> <msub><mi>ω</mi> <mn>0</mn></msub>
    <mo>+</mo> <msub><mi>ω</mi> <mn>1</mn></msub> <mo>×</mo> <mi>h</mi> <mi>e</mi>
    <mi>i</mi> <mi>g</mi> <mi>h</mi> <mi>t</mi> <mo>.</mo></mrow></math>
- en: For us to be able to simulate numerical <math alttext="left-parenthesis h e
    i g h t comma w e i g h t right-parenthesis"><mrow><mo>(</mo> <mi>h</mi> <mi>e</mi>
    <mi>i</mi> <mi>g</mi> <mi>h</mi> <mi>t</mi> <mo>,</mo> <mi>w</mi> <mi>e</mi> <mi>i</mi>
    <mi>g</mi> <mi>h</mi> <mi>t</mi> <mo>)</mo></mrow></math> pairs, or data points,
    we must assume numerical values for the parameters <math alttext="omega 0"><msub><mi>ω</mi>
    <mn>0</mn></msub></math> and <math alttext="omega 1"><msub><mi>ω</mi> <mn>1</mn></msub></math>
    . Without having insights from real data about the correct choices for these <math
    alttext="omega"><mi>ω</mi></math> ’s, we are left with making educated guesses
    from the context of the problem and experimenting with different values. Note
    that for the Height Weight case in this example, we happen to have real data that
    we can use to learn appropriate values for the <math alttext="omega"><mi>ω</mi></math>
    ’s, and one of the goals of [Chapter 3](ch03.xhtml#ch03) is to learn how to do
    that. However, in many other scenarios, we do not have real data, so the only
    way to go is experimenting with various numerical values for these <math alttext="omega"><mi>ω</mi></math>
    ’s.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 为了能够模拟数值<math alttext="（高度，重量）"><mrow><mo>(</mo> <mi>h</mi> <mi>e</mi> <mi>i</mi>
    <mi>g</mi> <mi>h</mi> <mi>t</mi> <mo>,</mo> <mi>w</mi> <mi>e</mi> <mi>i</mi> <mi>g</mi>
    <mi>h</mi> <mi>t</mi> <mo>)</mo></mrow></math>对，或者数据点，我们必须假设参数<math alttext="omega
    0"><msub><mi>ω</mi> <mn>0</mn></msub></math>和<math alttext="omega 1"><msub><mi>ω</mi>
    <mn>1</mn></msub></math>的数值。在没有来自真实数据的见解来选择这些<math alttext="omega"><mi>ω</mi></math>的正确值时，我们只能根据问题的背景做出合理的猜测并尝试不同的数值。请注意，在本例中的身高体重情况下，我们碰巧有真实数据可以用来学习<math
    alttext="omega"><mi>ω</mi></math>的适当值，[第3章](ch03.xhtml#ch03)的一个目标就是学习如何做到这一点。然而，在许多其他情况下，我们没有真实数据，所以唯一的方法就是尝试不同的<math
    alttext="omega"><mi>ω</mi></math>的数值。
- en: 'In the following simulations, we set <math alttext="omega 0 equals negative
    314.5"><mrow><msub><mi>ω</mi> <mn>0</mn></msub> <mo>=</mo> <mo>-</mo> <mn>314</mn>
    <mo>.</mo> <mn>5</mn></mrow></math> and <math alttext="omega 1 equals 7.07"><mrow><msub><mi>ω</mi>
    <mn>1</mn></msub> <mo>=</mo> <mn>7</mn> <mo>.</mo> <mn>07</mn></mrow></math> ,
    so the function becomes:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下模拟中，我们设置<math alttext="omega 0等于负314.5"><mrow><msub><mi>ω</mi> <mn>0</mn></msub>
    <mo>=</mo> <mo>-</mo> <mn>314</mn> <mo>.</mo> <mn>5</mn></mrow></math>和<math alttext="omega
    1等于7.07"><mrow><msub><mi>ω</mi> <mn>1</mn></msub> <mo>=</mo> <mn>7</mn> <mo>.</mo>
    <mn>07</mn></mrow></math>，所以函数变为：
- en: <math alttext="dollar-sign w e i g h t equals negative 314.5 plus 7.07 times
    h e i g h t period dollar-sign"><mrow><mi>w</mi> <mi>e</mi> <mi>i</mi> <mi>g</mi>
    <mi>h</mi> <mi>t</mi> <mo>=</mo> <mo>-</mo> <mn>314</mn> <mo>.</mo> <mn>5</mn>
    <mo>+</mo> <mn>7</mn> <mo>.</mo> <mn>07</mn> <mo>×</mo> <mi>h</mi> <mi>e</mi>
    <mi>i</mi> <mi>g</mi> <mi>h</mi> <mi>t</mi> <mo>.</mo></mrow></math>
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="重量等于负314.5加7.07乘以高度"><mrow><mi>w</mi> <mi>e</mi> <mi>i</mi> <mi>g</mi>
    <mi>h</mi> <mi>t</mi> <mo>=</mo> <mo>-</mo> <mn>314</mn> <mo>.</mo> <mn>5</mn>
    <mo>+</mo> <mn>7</mn> <mo>.</mo> <mn>07</mn> <mo>×</mo> <mi>h</mi> <mi>e</mi>
    <mi>i</mi> <mi>g</mi> <mi>h</mi> <mi>t</mi> <mo>.</mo></mrow></math>
- en: 'Now we can generate as many numerical <math alttext="left-parenthesis h e i
    g h t comma w e i g h t right-parenthesis"><mrow><mo>(</mo> <mi>h</mi> <mi>e</mi>
    <mi>i</mi> <mi>g</mi> <mi>h</mi> <mi>t</mi> <mo>,</mo> <mi>w</mi> <mi>e</mi> <mi>i</mi>
    <mi>g</mi> <mi>h</mi> <mi>t</mi> <mo>)</mo></mrow></math> pairs as we want. For
    example, plugging <math alttext="h e i g h t equals 60"><mrow><mi>h</mi> <mi>e</mi>
    <mi>i</mi> <mi>g</mi> <mi>h</mi> <mi>t</mi> <mo>=</mo> <mn>60</mn></mrow></math>
    in the formula for the weight function, we get <math alttext="w e i g h t equals
    negative 314.5 plus 7.07 times 60 equals 109.7"><mrow><mi>w</mi> <mi>e</mi> <mi>i</mi>
    <mi>g</mi> <mi>h</mi> <mi>t</mi> <mo>=</mo> <mo>-</mo> <mn>314</mn> <mo>.</mo>
    <mn>5</mn> <mo>+</mo> <mn>7</mn> <mo>.</mo> <mn>07</mn> <mo>×</mo> <mn>60</mn>
    <mo>=</mo> <mn>109</mn> <mo>.</mo> <mn>7</mn></mrow></math> . So our linear model
    *predicts* that a person whose height is 60 inches weighs <math alttext="109.7"><mrow><mn>109</mn>
    <mo>.</mo> <mn>7</mn></mrow></math> lbs, and the data point that we can plot on
    the Height Weight graph has coordinates <math alttext="left-parenthesis 60 comma
    109.7 right-parenthesis"><mrow><mo>(</mo> <mn>60</mn> <mo>,</mo> <mn>109</mn>
    <mo>.</mo> <mn>7</mn> <mo>)</mo></mrow></math> . In [Figure 2-4](#Fig_height_weight_sim),
    we generate 5000 of these data points: We choose 5000 values for the height between
    54 and 79 inches and plug them into the weight function. We notice that the graph
    in [Figure 2-4](#Fig_height_weight_sim) is a perfect straight-line, with no noise
    or variation in the simulated data, since we did not incorporate those into our
    linear model. That’s a hallmark of simulated data: It does what the function(s)
    it was generated from does. If we understand the function (called *model*) that
    we used to build our simulation, and if our computation does not accumulate too
    many numerical errors and/or very large numbers that go rogue, then we understand
    the data that our model generates, and we can use this data in any way we see
    fit. There isn’t much space for surprises. In our example, our proposed function
    is linear, so its equation is that of a straight-line, and as you see in [Figure 2-4](#Fig_height_weight_sim),
    the generated data lies perfectly on this straight-line.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以生成尽可能多的数值<math alttext="left-parenthesis h e i g h t comma w e i g h t
    right-parenthesis"><mrow><mo>(</mo> <mi>h</mi> <mi>e</mi> <mi>i</mi> <mi>g</mi>
    <mi>h</mi> <mi>t</mi> <mo>,</mo> <mi>w</mi> <mi>e</mi> <mi>i</mi> <mi>g</mi> <mi>h</mi>
    <mi>t</mi> <mo>)</mo></mrow></math>对，我们想要。例如，将<math alttext="h e i g h t equals
    60"><mrow><mi>h</mi> <mi>e</mi> <mi>i</mi> <mi>g</mi> <mi>h</mi> <mi>t</mi> <mo>=</mo>
    <mn>60</mn></mrow></math>插入重量函数的公式中，我们得到<math alttext="w e i g h t equals negative
    314.5 plus 7.07 times 60 equals 109.7"><mrow><mi>w</mi> <mi>e</mi> <mi>i</mi>
    <mi>g</mi> <mi>h</mi> <mi>t</mi> <mo>=</mo> <mo>-</mo> <mn>314</mn> <mo>.</mo>
    <mn>5</mn> <mo>+</mo> <mn>7</mn> <mo>.</mo> <mn>07</mn> <mo>×</mo> <mn>60</mn>
    <mo>=</mo> <mn>109</mn> <mo>.</mo> <mn>7</mn></mrow></math>。因此，我们的线性模型*预测*，身高为60英寸的人体重为<math
    alttext="109.7"><mrow><mn>109</mn> <mo>.</mo> <mn>7</mn></mrow></math>磅，我们可以在身高体重图上绘制的数据点的坐标为<math
    alttext="left-parenthesis 60 comma 109.7 right-parenthesis"><mrow><mo>(</mo> <mn>60</mn>
    <mo>,</mo> <mn>109</mn> <mo>.</mo> <mn>7</mn> <mo>)</mo></mrow></math>。在[图2-4](#Fig_height_weight_sim)中，我们生成了5000个这些数据点：我们选择了身高在54至79英寸之间的5000个值，并将它们代入重量函数。我们注意到[图2-4](#Fig_height_weight_sim)中的图是一条完美的直线，模拟数据中没有噪音或变化，因为我们没有将这些因素纳入我们的线性模型中。这是模拟数据的特点：它做了生成它的函数所做的事情。如果我们了解我们用来构建模拟的函数（称为*模型*）以及我们的计算没有积累太多的数值误差和/或非常大的数值，那么我们就了解了我们的模型生成的数据，并且我们可以以任何我们认为合适的方式使用这些数据。这里没有太多的惊喜。在我们的例子中，我们提出的函数是线性的，因此它的方程式是一条直线的方程式，正如你在[图2-4](#Fig_height_weight_sim)中看到的，生成的数据完全位于这条直线上。
- en: '![200](assets/emai_0204.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![200](assets/emai_0204.png)'
- en: 'Figure 2-4\. Simulated data: We generated five thousand (height, weight) points
    using the linear function <math alttext="w e i g h t equals negative 314.5 plus
    7.07 times h e i g h t"><mrow><mi>w</mi> <mi>e</mi> <mi>i</mi> <mi>g</mi> <mi>h</mi>
    <mi>t</mi> <mo>=</mo> <mo>-</mo> <mn>314</mn> <mo>.</mo> <mn>5</mn> <mo>+</mo>
    <mn>7</mn> <mo>.</mo> <mn>07</mn> <mo>×</mo> <mi>h</mi> <mi>e</mi> <mi>i</mi>
    <mi>g</mi> <mi>h</mi> <mi>t</mi></mrow></math> .'
  id: totrans-67
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-4。模拟数据：我们使用线性函数<math alttext="w e i g h t equals negative 314.5 plus 7.07
    times h e i g h t"><mrow><mi>w</mi> <mi>e</mi> <mi>i</mi> <mi>g</mi> <mi>h</mi>
    <mi>t</mi> <mo>=</mo> <mo>-</mo> <mn>314</mn> <mo>.</mo> <mn>5</mn> <mo>+</mo>
    <mn>7</mn> <mo>.</mo> <mn>07</mn> <mo>×</mo> <mi>h</mi> <mi>e</mi> <mi>i</mi>
    <mi>g</mi> <mi>h</mi> <mi>t</mi></mrow></math>生成了五千个（身高，体重）点。
- en: 'What if we want to simulate more realistic data for height and weight? Then
    we can sample the height values from a more realistic distribution for the heights
    of a human population: The bell shaped normal distribution! Again, we *know* the
    probability distribution that we are sampling from, which is different from the
    case for real data. After we sample the height values, we plug those into the
    linear model for the weight, then we add some noise, since we want our simulated
    data to be realistic. Since noise has a random nature then we must also pick the
    probability distribution it will be sampled from. We again choose the bell shaped
    normal distribution, but we could’ve chosen the uniform distribution to model
    uniform random fluctuations. Our more realistic Height Weight model becomes:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要模拟更真实的身高和体重数据呢？那么我们可以从更真实的人类身高分布中对身高值进行抽样：钟形正态分布！同样，我们*知道*我们正在抽样的概率分布，这与真实数据的情况不同。在我们抽样身高值之后，我们将其代入体重的线性模型中，然后添加一些噪音，因为我们希望我们的模拟数据更加真实。由于噪音具有随机性质，因此我们还必须选择它将从哪个概率分布中抽样。我们再次选择钟形正态分布，但我们也可以选择均匀分布来模拟均匀的随机波动。我们更真实的身高体重模型变为：
- en: <math alttext="dollar-sign w e i g h t equals negative 314.5 plus 7.07 times
    h e i g h t plus n o i s e period dollar-sign"><mrow><mi>w</mi> <mi>e</mi> <mi>i</mi>
    <mi>g</mi> <mi>h</mi> <mi>t</mi> <mo>=</mo> <mo>-</mo> <mn>314</mn> <mo>.</mo>
    <mn>5</mn> <mo>+</mo> <mn>7</mn> <mo>.</mo> <mn>07</mn> <mo>×</mo> <mi>h</mi>
    <mi>e</mi> <mi>i</mi> <mi>g</mi> <mi>h</mi> <mi>t</mi> <mo>+</mo> <mi>n</mi> <mi>o</mi>
    <mi>i</mi> <mi>s</mi> <mi>e</mi> <mo>.</mo></mrow></math>
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="dollar-sign w e i g h t equals negative 314.5 plus 7.07 times
    h e i g h t plus n o i s e period dollar-sign"><mrow><mi>w</mi> <mi>e</mi> <mi>i</mi>
    <mi>g</mi> <mi>h</mi> <mi>t</mi> <mo>=</mo> <mo>-</mo> <mn>314</mn> <mo>.</mo>
    <mn>5</mn> <mo>+</mo> <mn>7</mn> <mo>.</mo> <mn>07</mn> <mo>×</mo> <mi>h</mi>
    <mi>e</mi> <mi>i</mi> <mi>g</mi> <mi>h</mi> <mi>t</mi> <mo>+</mo> <mi>n</mi> <mi>o</mi>
    <mi>i</mi> <mi>s</mi> <mi>e</mi> <mo>.</mo></mrow></math>
- en: We obtain [Figure 2-5](#Fig_height_weight_sim_noise).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们获得[图2-5](#Fig_height_weight_sim_noise)。
- en: '![200](assets/emai_0205.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![200](assets/emai_0205.png)'
- en: 'Figure 2-5\. Simulated data: We generated five thousand (height, weight) points
    using the linear function <math alttext="w e i g h t equals negative 314.5 plus
    7.07 times h e i g h t"><mrow><mi>w</mi> <mi>e</mi> <mi>i</mi> <mi>g</mi> <mi>h</mi>
    <mi>t</mi> <mo>=</mo> <mo>-</mo> <mn>314</mn> <mo>.</mo> <mn>5</mn> <mo>+</mo>
    <mn>7</mn> <mo>.</mo> <mn>07</mn> <mo>×</mo> <mi>h</mi> <mi>e</mi> <mi>i</mi>
    <mi>g</mi> <mi>h</mi> <mi>t</mi></mrow></math> . The height points are normally
    distributed and we added normally distributed noise as well. Note that the distributions
    of the weight and height data at the right hand side and at the top of the figure
    respectively are normally distributed. This is not surprising since we designed
    them that way in our simulation.'
  id: totrans-72
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-5。模拟数据：我们使用线性函数<math alttext="w e i g h t equals negative 314.5 plus 7.07
    times h e i g h t"><mrow><mi>w</mi> <mi>e</mi> <mi>i</mi> <mi>g</mi> <mi>h</mi>
    <mi>t</mi> <mo>=</mo> <mo>-</mo> <mn>314</mn> <mo>.</mo> <mn>5</mn> <mo>+</mo>
    <mn>7</mn> <mo>.</mo> <mn>07</mn> <mo>×</mo> <mi>h</mi> <mi>e</mi> <mi>i</mi>
    <mi>g</mi> <mi>h</mi> <mi>t</mi></mrow></math>生成了五千个（身高，体重）点。身高点是正态分布的，我们也添加了正态分布的噪声。请注意图的右侧和顶部分别的体重和身高数据的分布都是正态分布的。这并不奇怪，因为我们在模拟中就是这样设计的。
- en: Now compare [Figure 2-5](#Fig_height_weight_sim_noise) containing our simulated
    Height Weight data to [Figure 2-6](#Fig_height_weight_F) containing real Height
    Weight data of 5000 females from [the second Kaggle data set](https://www.kaggle.com/mustafaali96/weight-height)
    that we used. Not too bad, given that it only took five minutes of code writing
    to generate this data, as opposed to collecting real data! Had we spent more time
    tweaking the values of our <math alttext="omega"><mi>ω</mi></math> ’s, and the
    parameters for the normally distributed noise that we added (mean and standard
    deviation), we would’ve obtained an even better looking simulated data set. However,
    we will leave this simulation here since very soon our whole focus will be on
    *learning* the appropriate parameter values for our hypothesized models.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 现在比较包含我们模拟的身高体重数据的[图2-5](#Fig_height_weight_sim_noise)和包含来自我们使用的[第二个Kaggle数据集](https://www.kaggle.com/mustafaali96/weight-height)中5000名女性真实身高体重数据的[图2-6](#Fig_height_weight_F)。考虑到只花了五分钟的代码编写来生成这些数据，情况还不错，而不是收集真实数据！如果我们花更多时间调整我们的<math
    alttext="omega"><mi>ω</mi></math>的值，以及我们添加的正态分布噪声的参数（均值和标准差），我们将获得一个看起来更好的模拟数据集。然而，我们将在这里结束这个模拟，因为很快我们的整个重点将是*学习*我们假设模型的适当参数值。
- en: '![200](assets/emai_0206.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![200](assets/emai_0206.png)'
- en: 'Figure 2-6\. Real Data: Weight data plotted against the height data of the
    5000 females in [the second Kaggle data set](https://www.kaggle.com/mustafaali96/weight-height).
    Note that the distributions of the female weight and height data at the right
    hand side and at the top of the figure respectively are normally distributed.
    Check the [linked Jupyter Notebook](https://github.com/halanelson/Essential-Math-For-AI)
    for more details.'
  id: totrans-75
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-6。真实数据：[第二个Kaggle数据集](https://www.kaggle.com/mustafaali96/weight-height)中5000名女性的体重数据与身高数据的图。请注意图的右侧和顶部分别的女性体重和身高数据的分布都是正态分布的。请查看[链接的Jupyter笔记本](https://github.com/halanelson/Essential-Math-For-AI)以获取更多详细信息。
- en: 'Mathematical Models: Simulations and AI'
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数学模型：模拟和人工智能
- en: We can always adjust our mathematical models to make them more realistic. We
    are the designers so we get to decide what goes into these models. It is often
    the case that the more a model mimics nature the more mathematical objects get
    incorporated within it. Therefore, while building a mathematical model, the usual
    tradeoff is between getting closer to reality, and the model’s simplicity and
    accessibility for mathematical analysis and computation. Different designers come
    up with different mathematical models, and some capture certain phenomena better
    than others. These models keep improving and evolving as the quest to capture
    natural behaviors continues. Thankfully, our computational capabilities have dramatically
    improved in the past decades, enabling us to create and test more involved and
    realistic mathematical models.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们总是可以调整我们的数学模型使其更加现实。我们是设计者，所以我们决定这些模型中包含什么。通常情况下，模型越接近自然，其中包含的数学对象就越多。因此，在构建数学模型时，通常需要在接近现实和模型的简单性以及数学分析和计算的可访问性之间进行权衡。不同的设计者提出不同的数学模型，有些模型比其他模型更好地捕捉了某些现象。随着捕捉自然行为的探索不断进行，这些模型不断改进和演变。值得庆幸的是，过去几十年中我们的计算能力已经大大提高，使我们能够创建和测试更复杂和更现实的数学模型。
- en: Nature is at the same time very finely detailed and enormously vast. Interactions
    in nature range from the subatomic quantum realm all the way to intergalactic
    interactions. We, as humans, are forever trying to undertsand nature and capture
    its intricate components with their numerous interconnections and interplays.
    Our reasons for this are varied. They range from pure curiosity about the origins
    of life and the universe, to creating new technologies, to enhancing communication
    systems, to designing drugs and discovering cures for diseases, to building weapons
    and defense systems, to traveling to distant planets and perhaps inhabiting them
    in the future. Mathematical models provide an excellent and almost miraculous
    way to describe nature with all its details using only numbers, functions, equations,
    and invoking quantified randomness through probability when faced with uncertainty.
    Computer simulations of these mathematical models enable us to investigate and
    visualize various simple and complex behaviors of the modeled systems or phenomena.
    In turn, insights from computer simulations aid in model enhancement and design,
    in addition to supplying deeper mathematical insights. This incredibly positive
    feedback cycle makes mathematical modeling and simulations an indespensible tool
    that is enhanced greatly with our increased computational power.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 自然同时非常细致和巨大。自然界的相互作用范围从亚原子量子领域一直到星系间的相互作用。我们作为人类，永远试图理解自然，并捕捉其复杂的组成部分及其众多的相互联系和相互作用。我们这样做的原因是多种多样的。它们从对生命和宇宙起源的纯好奇，到创造新技术，增强通信系统，设计药物和发现治疗方法，建造武器和防御系统，到未来可能前往遥远行星并在那里居住。数学模型以仅仅使用数字、函数、方程式来描述自然的所有细节的方式，以及在面对不确定性时通过概率引入量化的随机性，提供了一种极好而几乎奇迹般的方式。这些数学模型的计算机模拟使我们能够研究和可视化所建模系统或现象的各种简单和复杂行为。反过来，来自计算机模拟的见解有助于模型的增强和设计，除了提供更深入的数学见解。这种令人难以置信的积极反馈循环使数学建模和模拟成为一种不可或缺的工具，而我们的计算能力的增强使其得到了极大的增强。
- en: It is a mystery of the universe that its various phenomena can be accurately
    modeled using the abstract language of mathematics, and it is a marvel of the
    human mind that it can discover and comprehend mathematics, and build powerful
    technological devices that are useful for all kinds of applications. Equally impressive
    is that these devices, at their core, are doing nothing but computing or transmitting
    mathematics, more specifically, a bunch of zeros and ones.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 宇宙的奥秘在于，它的各种现象可以用数学的抽象语言准确地建模，而人类的头脑之所以了解和理解数学，并构建强大的技术设备，对各种应用都有用，这也是一个奇迹。同样令人印象深刻的是，这些设备在其核心上所做的无非是计算或传输数学，更具体地说，就是一堆零和一。
- en: 'The fact that humans are able to generalize their understanding of simple numbers
    all the way to building and applying mathematical models for natural phenomena
    at all kinds of scales is a spectacular example of *generalization* of learned
    knowledge, and is a hallmark of human’s intelligence. In the AI field, a common
    goal for both general AI (human-like) and narrow AI (specific task oriented) is
    *generalization*: The ability of an AI agent to *generalize* learned abilities
    to new and unknown situations. In the next chapter, we will understand this principle
    for narrow and task oriented AI: *An AI agent learns from data then produces good
    predictions for new and unseen data*.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 人类能够将对简单数字的理解推广到建立和应用各种规模的自然现象的数学模型，这是学习知识的*泛化*的一个壮观例子，也是人类智慧的标志。在人工智能领域，通用人工智能（类似人类）和狭义人工智能（特定任务导向）的一个共同目标是*泛化*：人工智能代理能够将学到的能力推广到新的和未知的情况。在下一章中，我们将了解狭义和任务导向人工智能的这一原则：*人工智能代理从数据中学习，然后为新的和未见过的数据产生良好的预测*。
- en: 'AI interacts in three ways with mathematical models and simulations:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能以三种方式与数学模型和模拟进行交互：
- en: '*Mathematical models and simulations create data for AI systems to train on*:
    Self driving cars are by some considered to be a benchmark for AI. It will be
    inconvenient to let intelligent car prototypes drive off cliffs, hit pedestrians,
    or crash into new work zones, before the car’s AI system learns that these are
    unfavorable events that must be avoided. Training on simulated data is especially
    valuable here, as simulations can create all kinds of hazardous virtual situations
    for a car to train on before releasing it out on the roads. Similarly, simulated
    data is tremendously helpful for training AI systems for rovers on Mars, drug
    discovery, materials design, weather forecasting, aviation, military training,
    and so on.'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*数学模型和模拟产生数据，供人工智能系统进行训练*：自动驾驶汽车被一些人认为是人工智能的一个基准。让智能汽车原型车在没有人工智能系统学会这些不利事件必须避免之前，开车冲下悬崖，撞到行人，或者撞上新的工作区域，这将是不方便的。在这里，模拟数据的训练尤其有价值，因为模拟可以为汽车创造各种危险的虚拟情况，供其在上路之前进行训练。同样，模拟数据对于火星车、药物发现、材料设计、天气预报、航空、军事训练等人工智能系统的训练非常有帮助。'
- en: '*AI enhances existing mathematical models and simulations*: AI has a great
    potential to assist in areas that have traditionally been difficult and limiting
    for mathematical models and simulations, such as learning appropriate values for
    the parameters involved in the models, appropriate probability distributions,
    mesh shapes and sizes when discretizing equations (fine meshes capture fine details
    and delicate behaviors at various spacial and time scales), and scaling computational
    methods to longer times or to larger domains with complicated shapes. Fields like
    navigation, aviation, finance, materials science, fluid dynamics, operations research,
    molecular and nuclear sciences, atmospheric and ocean sciences, astrophysics,
    physical and cyber security, and many others rely heavily on mathematical modeling
    and simulations. Integrating AI capabilities into these domains is starting to
    take place with very positive outcomes. We will come across examples of AI enhancing
    simulations in later chapters of this book.'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 人工智能增强了现有的数学模型和模拟：人工智能在传统上对数学模型和模拟困难和限制的领域有很大潜力，例如学习模型中涉及的参数的适当值、适当的概率分布、在离散化方程时的网格形状和大小（细网格捕捉各种空间和时间尺度上的细节和微妙行为），以及将计算方法扩展到更长的时间或具有复杂形状的更大领域。诸如导航、航空、金融、材料科学、流体动力学、运筹学、分子和核科学、大气和海洋科学、天体物理学、物理和网络安全等领域都严重依赖数学建模和模拟。将人工智能能力整合到这些领域正在开始，并取得了非常积极的成果。在本书的后面章节中，我们将遇到人工智能增强模拟的例子。
- en: '*AI itself is a mathematical model and simulation*: One of the big aspirations
    of AI is to *computationally* replicate human intelligence. Successful machine
    learning systems, including neural networks with all their architechtures and
    variations, are mathematical models aimed at simulating tasks that humans associate
    with intelligence, such as vision, pattern recognition and generalization, communication
    through natural language, and logical reasoning. Understanding, emotional experience,
    empathy, and collaboration, are also associated with intelligence and have contributed
    tremendously to the success and domination of humankind, so we must also find
    ways to replicate them if we want to achieve general AI and at the same time gain
    deeper understanding of the nature of intelligence and the workings of the human
    brain. Efforts in these areas are already on the way. What we want to keep in
    mind is that in all these areas, what machines are doing is *computing*: Machines
    *compute* meanings of documents for natural language processing, combine and *compute*
    digital image pixels for computer vision, convert audio signals to vectors of
    numbers and *compute* new audio for human-machine interaction, and so on. It is
    then easy to see how software AI is one big mathematical model and simulation.
    This will become more evident as we progress in this book.'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 人工智能本身是一个数学模型和模拟：人工智能的一个重要愿景是通过计算来复制人类智能。成功的机器学习系统，包括神经网络及其所有架构和变体，都是旨在模拟人类智能相关任务的数学模型，如视觉、模式识别和泛化、自然语言交流和逻辑推理。理解、情感体验、共情和合作也与智能相关，并且对人类的成功和统治做出了巨大贡献，因此如果我们想实现通用人工智能并同时更深入地了解智能的本质和人脑的运作，我们也必须找到复制它们的方法。在这些领域的努力已经在进行中。我们要记住的是，在所有这些领域，机器所做的是计算：机器计算自然语言处理文档的含义，组合和计算计算机视觉的数字图像像素，将音频信号转换为数字向量并计算人机交互的新音频，等等。因此很容易看出，软件人工智能是一个大型数学模型和模拟。随着我们在本书中的进展，这一点将变得更加明显。
- en: Where Do We Get our Data From?
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我们的数据从哪里获取？
- en: When I first decided to enter the AI field, I wanted to apply my mathematical
    knowledge to help solve real world problems that I felt passionate about. I grew
    up in war and I saw many problems erupt, disrupt, then eventually dissipiate or
    get resolved, either by direct fixes or by the human network adjusting around
    them and settling into completely new (unstable) equilibria. Common problems in
    war were sudden and massive disruptions to different supply chains, sudden destruction
    of large parts of the power grid, sudden paralysis of entire road networks by
    targeted bombing of certain bridges, sudden emergence of terrorist networks, blackmarkets,
    trafficking, inflation, and poverty. The number of problems that math can help
    solve in these scenarios, including war tactics and strategy, is limitless. From
    the safety of the United States, my Ph.D. in mathematics, and my tenure at the
    university, I started approaching companies, government agencies, and the military,
    looking for real projects with real data to work on. I offered to help find solutions
    to their problems for free. What I did not know, and I learned the hard way, was
    that getting real data was the biggest hurdle in the way. There are many regulations,
    privacy issues, Institutional Review Boards, and other obstacles standing in the
    way. Even after jumping through all these hoops, companies, institutions, and
    organizations tend to hold onto their data, even when they know they are not making
    the best use of it, and one almost has to beg in order to get real data. It turned
    out the experience I had was not unique. The same had happened to many others
    in the field.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 当我决定进入人工智能领域时，我想要运用我的数学知识来帮助解决我热衷的现实世界问题。我在战争中长大，看到许多问题的爆发、破坏，然后最终消散或得到解决，要么通过直接修复，要么通过人类网络的调整并最终稳定在全新的（不稳定的）平衡状态。战争中常见的问题包括对不同供应链的突然和大规模破坏，大部分电网的突然毁坏，通过有针对性的炸桥导致整个道路网络的突然瘫痪，恐怖网络的突然出现，黑市、贩卖、通货膨胀和贫困的突然出现。数学在这些情况下可以帮助解决的问题数量是无限的，包括战争战术和战略。在美国的安全环境下，我拥有数学博士学位，并在大学任教，我开始接触公司、政府机构和军方，寻找真实的项目和真实的数据来进行研究。我提出免费帮助他们解决问题。我不知道的是，我以艰难的方式学到的，获取真实数据是最大的障碍。有许多法规、隐私问题、机构审查委员会和其他障碍阻碍着我们。即使跳过了所有这些障碍，公司、机构和组织也倾向于保留他们的数据，即使他们知道他们没有充分利用它，我们几乎不得不乞求才能获取真实数据。结果证明，我遇到的经历并不是独一无二的。同样的事情也发生在这个领域的许多其他人身上。
- en: The story above is not meant to discourage you from getting the real data that
    you need to train your AI systems. The point is not to get surprised and disheartened
    if you encounter hesitation and resistance from the owners of the data that you
    need. Keep asking, and someone will be willing to take that one leap of faith.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 以上故事并不是要阻止你获取训练AI系统所需的真实数据。重点不是要让你在遇到数据所有者的犹豫和抵抗时感到惊讶和沮丧。继续询问，总会有人愿意冒险一试。
- en: Sometimes the data you need is available publicly on the web. For the simple
    models in this chapter, I am using data sets from [Kaggle](https://www.kaggle.com)
    website. There are other great public data repositories, which I will not list
    here, but a simple google search with keywords like *best data repositories* will
    return excellent results. Some repositories are geared towards computer vision,
    others for natural language processing, audio generation and transcription, scientific
    research, and so on.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 有时你需要的数据可以在网上公开获取。在本章中，我使用了[Kaggle](https://www.kaggle.com)网站的数据集来构建简单的模型。还有其他很棒的公共数据存储库，我就不一一列举了，但是简单的谷歌搜索关键词“最佳数据存储库”会返回出色的结果。一些存储库专门用于计算机视觉，其他用于自然语言处理、音频生成和转录、科学研究等等。
- en: 'Crawling the web for acquiring data is common, but you have to abide by the
    rules of the websites you are crawling. You also have to learn how to crawl (some
    people say that the difference between data scientists and statisticians is that
    data scientists know how to hack!). Some websites require you to obtain written
    permission before you crawl. For example, if you are interested in social media
    user behavior, or in collaboration networks, you can crawl social media and professional
    networks: Facebook, Instagram, YouTube, Flickr, LinkedIn, *etc*. for statistics
    on user accounts, such as their number of friends or connections, number of likes,
    comments, and their own activity on these sites. You will end up with very large
    data sets with hundreds of thousands of records, which you can then do your computations
    on.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在网上获取数据是常见的，但你必须遵守你爬取的网站的规则。你还必须学会如何爬取（有人说数据科学家和统计学家的区别在于数据科学家知道如何“黑客”！）。有些网站要求你在爬取之前获得书面许可。例如，如果你对社交媒体用户行为或合作网络感兴趣，你可以爬取社交媒体和专业网络：Facebook、Instagram、YouTube、Flickr、LinkedIn等等，以获取用户账户的统计数据，比如他们的朋友或联系人数量，点赞、评论以及他们在这些网站上的活动。你最终会得到包含数十万条记录的非常庞大的数据集，然后你可以进行计算。
- en: 'In order to gain intuitive understanding of how data gets integrated into AI,
    and the type of data that goes into various systems, while at the same time avoid
    feeling overwhelmed by all the information and the data that is out there, it
    is beneficial to develop a habit of exploring the data sets that successful AI
    systems were trained on, if they are available. You do not have to download them
    and work on them. Browsing the data set, its meta data, what features and labels
    (if any) it comes with, *etc.*, is enough to get you comfortable with data. For
    example, DeepMind’s [WaveNet](https://deepmind.com/blog/article/wavenet-generative-model-raw-audio)(2016),
    which we will learn about in [Chapter 7](ch07.xhtml#ch07), is a neural network
    that generates raw machine audio, with realistic sounding human voices or enjoyable
    pieces of music. It accomplishes tasks like text to audio conversion with natural
    sounding human voice connotations, even with a specific person’s voice, if the
    network gets conditioned on this person’s voice. We will understand the mathematical
    meaning of conditioning when we study WaveNet in [Chapter 7](ch07.xhtml#ch07).
    For now, think of it as a restriction imposed artificially on a problem so it
    restricts its results to a certain set of outcomes. So what data was WaveNet trained
    on? For multi-speaker audio generation which is not conditioned on text, WaveNet
    was trained on a data set of audio files consisting of 44 hours of audio from
    109 different speakers: [English Multispeaker Corpus from CSTR Voice Cloning Toolkit](https://datashare.ed.ac.uk/handle/10283/3443)(2012).
    For converting text to speech, WaveNet was trained on the [North American English
    Data Set](https://catalog.ldc.upenn.edu/LDC2009V01), which contains 24 hours of
    speech data, and on the [Mandarine Chinese Data Set](https://www.openslr.org/68/)
    that has 34.8 hours of speech data. For generating music, WaveNet was trained
    on the YouTube Piano Data Set, which has 60 hours of solo piano music obtained
    from YouTube videos, and the [MagnaTagATune Data Set](https://mirg.city.ac.uk/codeapps/the-magnatagatune-dataset)(2009),
    which consists of about 200 hours of music audio, where each 29 second clip is
    labeled with 188 tags, describing the genre, instrumentation, tempo, volume, mood,
    and various other labels for the music. Labeled data is extremely valuable for
    AI systems, because it provides a ground truth to measure the output of your hypothesis
    function against. We will learn this in the next few sections.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 为了直观地了解数据如何集成到人工智能中，以及进入各种系统的数据类型，同时避免被所有信息和数据所压倒，有益的是养成一个习惯，即探索成功的人工智能系统所训练的数据集，如果可以的话。你不必下载并处理这些数据。浏览数据集、其元数据、特征和标签（如果有的话）等就足以让你对数据感到舒适。例如，DeepMind的[WaveNet](https://deepmind.com/blog/article/wavenet-generative-model-raw-audio)(2016)是一个神经网络，可以生成真实的人声或愉悦的音乐。它可以完成文本到音频的转换，甚至可以模仿特定人的声音。我们将在[第7章](ch07.xhtml#ch07)学习WaveNet时了解条件的数学含义。现在，可以将其视为对问题施加的人为限制，以将结果限制在一定的结果集中。那么WaveNet是在哪些数据上训练的呢？对于不受文本条件限制的多说话者音频生成，WaveNet是在包含来自109位不同说话者的44小时音频的数据集上进行训练的：[英语多说话者语料库来自CSTR
    Voice Cloning Toolkit](https://datashare.ed.ac.uk/handle/10283/3443)(2012)。对于文本转语音，WaveNet是在包含24小时语音数据的[北美英语数据集](https://catalog.ldc.upenn.edu/LDC2009V01)和包含34.8小时语音数据的[中文数据集](https://www.openslr.org/68/)上进行训练的。对于生成音乐，WaveNet是在包含来自YouTube视频的60小时独奏钢琴音乐的数据集和包含约200小时音乐音频的[MagnaTagATune数据集](https://mirg.city.ac.uk/codeapps/the-magnatagatune-dataset)(2009)上进行训练的。标记数据对于人工智能系统非常有价值，因为它提供了一个基准来衡量你的假设函数的输出。我们将在接下来的几节中学习这一点。
- en: How about the famous image classification (for computer vision) AlexNet (2012)?
    What data was its convolutional neural network trained and tested on? AlexNet
    was trained on [ImageNet](https://www.image-net.org/about.php), a data set containing
    millions of images (scraped from the internet) and labeled (crowd-sourced human
    labelers) with thousands of classes.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 那么著名的图像分类（用于计算机视觉）AlexNet（2012）呢？它的卷积神经网络是在哪些数据上进行训练和测试的？AlexNet是在包含数百万图像（从互联网上获取）并标记了数千个类别的[ImageNet](https://www.image-net.org/about.php)数据集上进行训练的。
- en: Note that the above examples were all examples of unstructured data.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，上述示例都是关于非结构化数据的示例。
- en: If the data that a certain system was trained on is not publicly available,
    it is good to look up the published paper on the system or its documentation and
    read about how the required data was obtained. That alone will teach you a lot.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 如果某个系统所训练的数据不是公开可用的，最好查阅该系统的发表论文或文档，并了解所需数据是如何获取的。这本身就会让你学到很多。
- en: 'Before moving on to doing mathematics, keep in mind the following takeaways:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续进行数学之前，请记住以下要点：
- en: AI systems need digital data.
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 人工智能系统需要数字数据。
- en: Sometimes, the data you need is not easy to acquire.
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 有时，你需要的数据并不容易获取。
- en: There is a movement to digitalize our whole world.
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 有一个数字化我们整个世界的运动。
- en: The Vocabulary of Data Distributions, Probability, and Statistics
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据分布、概率和统计的词汇
- en: When you enter a new field, the first thing you want to learn is the vocabulary
    of that field. It is similar to learning a new language. You can learn it in a
    classroom, and suffer, or you can travel to a country that speaks the language,
    and listen to frequently used terms. You don’t have to know what “Bonjour” means
    in French. But while you are in France you notice that people say it to each other
    all the time, so you start saying it as well. Sometimes you will not use it in
    the right context, like when you have to say “Bonsoir” instead of “Bonjour”. But
    slowly, as you find yourself staying longer in France, you will be using the right
    vocabulary in the right context.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 当你进入一个新的领域时，你想要学习的第一件事是该领域的词汇。这类似于学习一门新语言。你可以在课堂上学习，然后受苦，或者你可以去一个说这种语言的国家，听频繁使用的术语。你不必知道法语中的“Bonjour”是什么意思。但当你在法国时，你会注意到人们经常互相说这个词，所以你也开始说。有时候你可能不会在正确的上下文中使用它，比如当你必须说“Bonsoir”而不是“Bonjour”。但慢慢地，当你发现自己在法国呆得越来越久时，你会在正确的上下文中使用正确的词汇。
- en: One more advantage of learning the vocabulary as fast as you can, without necessarily
    mastering any of the details, is that different fields refer to the same concepts
    with different terms, since there is a massive vocabulary collision out there.
    This ends up being a big source of confusion, therefore the embodiment of *language
    barriers*. When you learn the common vocabulary of the field, you will realize
    that you might already know the concepts, except that now you have new names for
    them.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 尽快学习词汇的另一个优势是，不一定要掌握任何细节，因为不同领域用不同的术语指代相同的概念，因此存在大量的词汇冲突。这最终成为混淆的一个重要来源，因此体现了*语言障碍*。当你学习该领域的常用词汇时，你会意识到你可能已经了解这些概念，只是现在你有了新的名称。
- en: 'The vocabulary terms from probability and statistics that you want to know
    for the purposes of AI applications are not too many. I will define each term
    once we get to use it, but note that the goal of probability theory is to make
    deterministic statements about random or stochastic quantities and events, since
    humans hate uncertainty and like their world to be controllable and predictable.
    Watch for the following language from the fields of probability and statistics
    whenever you are reading about AI, Machine Learning or Data Science. Again, you
    do not have to know any of the definitions yet, you just need to hear the following
    terms and be familiar with the way they progress after each other:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 你想要了解的概率和统计学的词汇术语并不多。我会在我们使用它时定义每个术语，但请注意，概率论的目标是对随机或随机数量和事件做出确定性的陈述，因为人类讨厌不确定性，喜欢他们的世界是可控和可预测的。当你阅读有关人工智能、机器学习或数据科学的内容时，留意以下来自概率和统计领域的语言。再次强调，你不必立刻了解任何定义，你只需要听到以下术语，并熟悉它们在彼此之后的进展方式：
- en: 'It all starts with *random variables*. Math people talk about functions nonstop.
    Functions have certain or deterministic outcomes. When you evaluate a function,
    you know exactly what value it will return: Evaluate the function <math alttext="x
    squared"><msup><mi>x</mi> <mn>2</mn></msup></math> at 3 and you are certain you
    will get <math alttext="3 squared equals 9"><mrow><msup><mn>3</mn> <mn>2</mn></msup>
    <mo>=</mo> <mn>9</mn></mrow></math> . Random variables, on the other hand, do
    not have deterministic outcomes. Their outcomes are uncertain, unpredictable,
    or stochastic. When you call a random variable, you do not know, before you actually
    see the outcome, what value it will return. Since you cannot aim for certainty
    anymore, what you can instead aim for is quantifying how likely it is to get an
    outcome. For example, when you roll a die, you can confidently say that your chance
    to get the outcome 4 is 1/6, assuming that the die you rolled is not tampered
    with. You never know ahead of time what outcome you will get before you actually
    roll the die. If you did, then casinos would run out of business, and the finance
    sector would eliminate its entire predictive analytics and risk management departments.
    Just like deterministic functions, a random variable can return outcomes from
    a discrete set (discrete random variable) or from the continuum (continuous random
    variable). The key distinction between a random variable and a function is in
    the randomness *vs.* the certainty of the outcomes.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一切都始于*随机变量*。数学家们不停地谈论函数。函数有确定的结果。当你评估一个函数时，你知道它将返回什么值：在3处评估函数<math alttext="x
    squared"><msup><mi>x</mi> <mn>2</mn></msup></math>，你可以肯定会得到<math alttext="3 squared
    equals 9"><mrow><msup><mn>3</mn> <mn>2</mn></msup> <mo>=</mo> <mn>9</mn></mrow></math>。另一方面，随机变量没有确定的结果。它们的结果是不确定的、不可预测的或随机的。当你调用一个随机变量时，你在实际看到结果之前不知道它将返回什么值。由于你不再能够追求确定性，你可以追求的是量化得到某个结果的可能性有多大。例如，当你掷骰子时，你可以自信地说你得到4的机会是1/6，假设你掷的骰子没有被篡改。你在实际掷骰子之前永远不知道你会得到什么结果。如果知道的话，赌场就会破产，金融部门就会取消其整个预测分析和风险管理部门。就像确定性函数一样，随机变量可以返回离散集合的结果（离散随机变量）或连续集合的结果（连续随机变量）。随机变量和函数之间的关键区别在于结果的随机性*vs.*确定性。
- en: After random variables we define *probability density functions* for continuous
    random variables and *probability mass functions* for discrete random variables.
    We call both *distributions* in order to add to our confusion. Usually, whether
    a distribution represents a discrete or a continuous random variable is understood
    from the context. Using this terminology, we sometimes say that one random variable,
    whether continuous or discrete, is *sampled* from a probability distribution,
    and multiple random variables are sampled from a *joint probability distribution*.
    In practice, it is rare that we know the full joint probability distribution of
    all the random variables involved in our data. When we do, or if we are able to
    *learn it*, it is a powerful thing.
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在随机变量之后，我们为连续随机变量定义*概率密度函数*，为离散随机变量定义*概率质量函数*。我们都称之为*分布*，以增加我们的困惑。通常，一个分布代表离散还是连续随机变量是从上下文中理解的。使用这个术语，我们有时会说一个随机变量，无论是连续还是离散的，是从一个概率分布中*抽样*，多个随机变量是从一个*联合概率分布*中抽样。在实践中，我们很少知道我们数据中涉及的所有随机变量的完整联合概率分布。当我们知道，或者我们能够*学习*它时，这是一件强大的事情。
- en: '*Marginal probability distributions* sit literary on the margins of a joint
    probability distribution (if we represent the joint probability distribution with
    a table containing the probabilities of all the combined states of the involved
    variables, see for example the first table on [this Wikipedia page](https://en.wikipedia.org/wiki/Marginal_distribution)).
    In this setting, you are lucky enough to have access to the full joint probability
    distribution of multiple random variables, and you are interested in finding out
    the probability distribution of only one or few of them. You can find these *marginal
    probability distributions* easily using the *sum rule* for probabilities, for
    example:'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*边际概率分布*就像坐落在联合概率分布的边缘上（如果我们用包含所涉变量所有组合状态的概率的表来表示联合概率分布，例如[此维基百科页面](https://en.wikipedia.org/wiki/Marginal_distribution)上的第一个表）。在这种情况下，你很幸运能够访问多个随机变量的完整联合概率分布，并且你有兴趣找出其中一个或几个的概率分布。你可以使用概率的*求和规则*轻松地找到这些*边际概率分布*，例如：'
- en: <math alttext="dollar-sign p left-parenthesis x right-parenthesis equals sigma-summation
    Underscript y element-of all states of y Endscripts p left-parenthesis x comma
    y right-parenthesis dollar-sign"><mrow><mi>p</mi> <mrow><mo>(</mo> <mi>x</mi>
    <mo>)</mo></mrow> <mo>=</mo> <msub><mo>∑</mo> <mrow><mi>y</mi><mo>∈</mo><mtext>all</mtext><mtext>states</mtext><mtext>of</mtext><mtext>y</mtext></mrow></msub>
    <mi>p</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>,</mo> <mi>y</mi> <mo>)</mo></mrow></mrow></math>
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="dollar-sign p left-parenthesis x right-parenthesis equals sigma-summation
    Underscript y element-of all states of y Endscripts p left-parenthesis x comma
    y right-parenthesis dollar-sign"><mrow><mi>p</mi> <mrow><mo>(</mo> <mi>x</mi>
    <mo>)</mo></mrow> <mo>=</mo> <msub><mo>∑</mo> <mrow><mi>y</mi><mo>∈</mo><mtext>all</mtext><mtext>states</mtext><mtext>of</mtext><mtext>y</mtext></mrow></msub>
    <mi>p</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>,</mo> <mi>y</mi> <mo>)</mo></mrow></mrow></math>
- en: The *Uniform Distribution* and the *Normal Distribution* are the most popular
    continuous distributions so we start with them. The normal distribution and the
    fundamental *Central Limit Theorem* from probability theory are initimately related.
    There are many other useful distributions representing the different random variables
    involved in our data, but we do not need them right away, so we postpone them
    until we need to use them.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*均匀分布*和*正态分布*是最受欢迎的连续分布，因此我们从它们开始。正态分布和概率论中的基本*中心极限定理*密切相关。在我们的数据中涉及的许多其他有用分布代表不同的随机变量，但我们不需要立即使用它们，所以我们推迟到需要使用它们时再讨论。'
- en: The moment we start dealing with multiple random variables (such as our gender,
    height, weight, and health index data), which is almost always the case, we introduce
    *conditional probabilities*, *Bayes Rule or Theorem*, and the *Product or Chain
    Rule* for conditional probabilities;
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一旦我们开始处理多个随机变量（例如我们的性别、身高、体重和健康指数数据），这几乎总是情况，我们引入*条件概率*、*贝叶斯定理或定理*和条件概率的*乘积或链规则*；
- en: Along with the concepts of *independent and conditionally independent random
    variables* (knowing the value of one does not change the probability of the other).
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 还有*独立和条件独立的随机变量*的概念（知道一个变量的值不会改变另一个变量的概率）。
- en: 'Both conditional probabilities and joint distributions involve multiple random
    variables, so it makes sense that they have something to do with each other: Slice
    the graph of a joint probability distribution (when we fix the value of one of
    the variables) and we get a conditional probability distribution (see Fig_slice_joint).'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 条件概率和联合分布都涉及多个随机变量，因此它们之间有某种联系是有意义的：切割联合概率分布的图表（当我们固定一个变量的值时），我们得到一个条件概率分布（见图_slice_joint）。
- en: 'A Very Important Note: Bayes Rule *vs.* Joint Probability Distribution'
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 非常重要的一点：贝叶斯定理 *vs.* 联合概率分布
- en: If we happen to have access to the full joint probability distribution of all
    the multiple random variables that we care for in our setting, then we would not
    need Bayes Rule. In other words, Bayes Rule helps us calculate the desired conditional
    probabilities *when we do not have access* to the full joint probability distribution
    of the involved random variables.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们恰好可以访问我们设置中关心的所有多个随机变量的完整联合概率分布，那么我们就不需要贝叶斯定理。换句话说，贝叶斯定理帮助我们计算所需的条件概率*当我们无法访问*所涉随机变量的完整联合概率分布时。
- en: 'From logical and mathematical standpoints, we can define conditional probabilities
    then move on smoothly with our calculations and our lives. Practitioners, however,
    give different names to different conditional probabilities, depending on whether
    they are conditioning on data that has been observed or on weights (also called
    parameters) that they still need to estimate. The vocabulary words here are: *prior
    distribution* (general probability ditribution for the weights of our model prior
    to observing any data), *posterior distribution* (probability distribution for
    the weights given the observed data), and the *likelihood function* (function
    encoding the probability of observing a data point given a particular weight distribution).
    All of these can be related through Bayes Rule, as well as through the joint distribution.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从逻辑和数学的角度来看，我们可以定义条件概率，然后顺利进行我们的计算和生活。然而，从业者根据他们是在对已观察到的数据进行条件概率还是对他们仍需要估计的权重（也称为参数）进行条件概率，给不同的条件概率赋予不同的名称。这里的词汇包括：*先验分布*（在观察任何数据之前，模型权重的一般概率分布），*后验分布*（给定观察数据的权重的概率分布），以及*似然函数*（编码给定特定权重分布的情况下观察数据点的概率）。所有这些都可以通过贝叶斯定理以及联合分布来联系起来。
- en: 'Note: We say likelihood function *not* likelihood distribution'
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 注：我们说似然函数*不是*似然分布。
- en: We refer to the likelihood as a function and not as a distribution because probability
    distributions *must* add up to one (or integrate to one if we are dealing with
    continuous random variables) but the likelihood function does not necessarily
    add up to one (or integrate to one in the case of continuous random variables).
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将似然性称为函数，而不是分布，因为概率分布*必须*加起来为一（或者如果我们处理连续随机变量，则积分为一），但似然函数不一定加起来为一（或者在连续随机变量的情况下积分为一）。
- en: We can mix probability distributions and produce *mixtures of distributions*.
    *Gaussian mixtures* are pretty famous. The Height data above that contains height
    measurements for both males and females is a good example of a Gaussian mixture.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以混合概率分布并产生*分布混合物*。*高斯混合*非常有名。上面包含男性和女性身高测量值的身高数据就是高斯混合的一个很好的例子。
- en: 'We can add or multiply random variables sampled from simple distributions to
    produce new random variables with more complex distributions, representing more
    complex random events. The natural question that’s usually investigated here is:
    *What is the distribution of the sum random variable or the product random variable?*'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以将从简单分布中抽样的随机变量相加或相乘，以产生具有更复杂分布的新随机变量，代表更复杂的随机事件。通常在这里调查的自然问题是：*随机变量的和或积的分布是什么？*
- en: Finally we use directed and undirected graph representations in order to efficiently
    decompose joint probability distributions. This makes our computational life much
    cheaper and tractable.
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，我们使用有向和无向图表示来有效地分解联合概率分布。这使得我们的计算生活变得更加便宜和可处理。
- en: 'Four quantities are central to probability, statistics, and data science: *Expectation*
    and *mean*, quantifying an average value, and the *variance* and *standard deviation*,
    quantifying the spread around that average value, hence encoding uncertainty.
    Our goal is to have control over the variance in order to reduce the uncertainty.
    The larger the variance the more error you can commit when using your average
    value to make predictions. Therefore, when you explore the field, you often notice
    that mathematical statements, inequalities, and theorems mostly involve some control
    over the expectation and variance of any quantities that involve randomness.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于概率、统计和数据科学来说，有四个关键的量：*期望*和*均值*，用于量化平均值，以及*方差*和*标准差*，用于量化围绕平均值的扩散，因此编码不确定性。我们的目标是控制方差，以减少不确定性。方差越大，使用平均值进行预测时可能犯的错误就越多。因此，当你探索这个领域时，你经常会注意到数学陈述、不等式和定理主要涉及对涉及随机性的任何量的期望和方差的一些控制。
- en: When we have one random variable with a corresponding probability distribution
    we calculate the *expectation* (expected average outcome), *variance* (expected
    squared distance from the expected average), and *standard deviation* (expected
    distance from the average). For data that has been already sampled or observed,
    for example our height and weight data above, we calculate the *sample mean* (average
    value), variance (average squared distance from the mean), and standard deviation
    (average distance from the mean, so this measures the spread around the mean).
    So if the data we care for has not been sampled or observed yet, we speculate
    on it using the language of *expectations*, but if we already have an observed
    or measured sample, we calculate its *statistics*. Naturally we are interested
    in how far off our speculations are from our computed statistics for the observed
    data, and what happens in the limiting (but idealistic) case where we can in fact
    measure data for the entire population. The *Law of Large Numbers* answers that
    for us and tells us that in this limiting case (when the sample size goes to infinity)
    our expectation matches the sample mean.
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当我们有一个随机变量及其相应的概率分布时，我们计算*期望*（预期平均结果）、*方差*（与预期平均值的平方距离的期望）和*标准差*（与平均值的期望距离）。对于已经抽样或观察到的数据，例如上面的身高和体重数据，我们计算*样本均值*（平均值）、方差（与均值的平方距离的平均值）和标准差（与均值的平均距离，因此这测量了均值周围的扩散）。因此，如果我们关心的数据尚未被抽样或观察到，我们使用*期望*的语言进行推测，但如果我们已经有了观察或测量的样本，我们计算其*统计量*。自然地，我们对我们的推测与我们计算出的观察数据的统计量有多大偏差感兴趣，以及在极限（但理想化）情况下，我们实际上可以测量整个人口的数据。*大数定律*为我们解答了这个问题，并告诉我们，在这种极限情况下（样本量趋于无穷大时），我们的期望与样本均值相匹配。
- en: When we have two or more random variables we calculate the *covariance*, *correlation*,
    and *covariance matrix*. This is when the field of linear algebra with its language
    of vectors, matrices, and matrix decompositions (such as eigenvalue and singular
    value decompositions), gets married to the field of probability and statistics.
    The variance of each of the random variables sits on the diagonal of the covariance
    matrix, and the covariances of each possible pair sit off the diagonal. The covariance
    matrix is symmetric. When you diagonalize it, using standard linear algebra techniques,
    you *uncorrelate* the involved random variables.
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当我们有两个或更多的随机变量时，我们计算*协方差*、*相关性*和*协方差矩阵*。这时，线性代数领域的向量、矩阵和矩阵分解（如特征值和奇异值分解）的语言与概率和统计领域结合在一起。每个随机变量的方差都位于协方差矩阵的对角线上，每对可能的协方差都位于对角线之外。协方差矩阵是对称的。当你对其进行对角化时，使用标准线性代数技术，你会*去相关*涉及的随机变量。
- en: Meanwhile, we pause and make sure we know the difference between independence
    and zero covariance. Covariance and correlation are all about capturing a linear
    relationship between two random variables. Correlation works on *normalized* random
    variables, so that we can still detect linear relationships even if random variables
    or data measurements have vastly different scales. When you normalize a quantity,
    its scale doesn’t matter anymore. It wouldn’t matter whether it is measured on
    a scale of millions or on a 0.001 scale. Covariance works on unnormalized random
    variables. Life is not all linear. Independence is stronger than zero covariance.
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与此同时，我们暂停一下，确保我们知道独立性和零协方差之间的区别。协方差和相关性都是关于捕捉两个随机变量之间的线性关系。相关性适用于*标准化*的随机变量，这样我们即使随机变量或数据测量具有非常不同的尺度，仍然可以检测到线性关系。当你对数量进行标准化时，它的尺度就不再重要了。无论它是以百万为单位还是以0.001为单位测量都无关紧要。协方差适用于未标准化的随机变量。生活并不都是线性的。独立性比零协方差更强。
- en: '*Markov processes* are very important for AI’s reinforcement learning paradigm.
    They are characterized by a system’s all possible states, a set of all possible
    actions that can be performed by an agent (move left, move right, *etc.*), *a
    matrix containing the transition probabilities between all states*, or the probability
    distribution for what states an agent will transition to after taking a certain
    action, and a reward function, which we want to maximize. Two popular examples
    from AI include board games, and a *smart* thermostat such as NEST. We will go
    over these in the reinforcement learning chapter.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*马尔可夫过程*对于人工智能的强化学习范式非常重要。它们的特征是系统的所有可能状态，代理可以执行的所有可能动作的集合（向左移动、向右移动，*等*），*包含所有状态之间转移概率的矩阵*，或者代理在采取某个动作后将转移到哪个状态的概率分布，以及一个奖励函数，我们希望最大化。人工智能中的两个流行例子包括棋盘游戏和*智能*恒温器，如NEST。我们将在强化学习章节中介绍这些内容。'
- en: 'Note: Normalizing, Scaling, and/or Standarizing a Random Variable or a Data
    Set'
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 注：标准化、缩放和/或标准化随机变量或数据集
- en: This is one of the many cases where there is a vocabulary collision. Normalizing,
    scaling, and standarizing are used synonymously in various contexts. The goal
    is always the same. Subtract a number (shift) from the data or from all possible
    outcomes of a random variable then divide by a constant number (scale). If you
    subtract the mean of your data sample (or the expectation of your random variable)
    and divide by their standard deviation, then you get new *standarized* or *normalized*
    data values (or new standarized or normalized random variable) that have mean
    equal to zero (or expectation zero) and standard deviation equal to one. If instead
    you subtract the minimum and divide by the range (max value minus min value) then
    you get new data values or a new random variable with outcomes all between zero
    and one. Sometimes we talk about normalizing vectors of numbers. In this case,
    what we mean is we divide every number in our vector by the length of the vector
    itself, so that we obtain a new vector of length one. So whether we say we are
    normalizing, scaling, or standarazing a collection of numbers, the goal is to
    try to control the values of these numbers, and center them around zero, and/or
    restrict their spread to be less than or equal to one while at the same time preserving
    their inherent variability.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 这是许多情况下词汇冲突的一个例子。在各种上下文中，标准化、缩放和标准化通常是同义词。目标总是相同的。从数据或随机变量的所有可能结果中减去一个数字（偏移），然后除以一个常数（比例）。如果你从数据样本的均值（或随机变量的期望）中减去并除以它们的标准差，那么你会得到新的*标准化*或*归一化*数据值（或新的标准化或归一化的随机变量），它们的均值等于零（或期望值等于零），标准差等于一。如果你减去最小值并除以范围（最大值减去最小值），那么你会得到新的数据值或新的随机变量，其结果都在零和一之间。有时我们谈论标准化数字向量。在这种情况下，我们的意思是我们将向量中的每个数字除以向量本身的长度，这样我们就得到一个长度为一的新向量。因此，无论我们说我们正在标准化、缩放还是标准化一组数字，目标都是尝试控制这些数字的值，并将它们居中在零周围，并/或限制它们的扩散小于或等于一，同时保留它们固有的变异性。
- en: Mathematicians like to express probability concepts in terms of flipping coins,
    rolling dice, drawing balls from urns, drawing cards from decks, trains arriving
    at stations, customers calling a hotline, customers clicking on an ad or a website
    link, diseases and their symptoms, criminal trials and evidence, and times until
    something happens, such as a machine failing. Do not be surprised that these examples
    are everywhere as they generalize nicely to many other real life situations.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 数学家喜欢用抛硬币、掷骰子、从瓮中取球、从牌组中抽牌、火车到站、顾客打电话、顾客点击广告或网站链接、疾病及其症状、刑事审判和证据，以及直到某事发生的时间等概率概念。不要感到惊讶，这些例子随处可见，因为它们很好地概括了许多其他现实生活情况。
- en: In addition to the above map of probability theory, we will borrow very few
    terms and functions from Statistical Mechanics (for example, the *partition function*)
    and Information Theory (for example, signal *vs.* noise, entropy, and the *cross
    entropy function*). We will explain these when we encounter them in later chapters.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 除了上述概率理论的映射，我们还将从统计力学（例如，*分区函数*）和信息论（例如，信号*与*噪声、熵和*交叉熵函数*）中借用很少的术语和函数。我们将在后面的章节中遇到它们时进行解释。
- en: Continuous Distributions *vs.* Discrete Distributions (Density *vs.* Mass)
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 连续分布*与*离散分布（密度*与*质量）
- en: When we deal with continuous distributions, it is important to use terms like
    observing or sampling a data point *near* or *around* a certain value instead
    of observing or sampling an *exact* value. In fact, the probability of observing
    an exact value in this case is zero.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们处理连续分布时，重要的是使用诸如观察或抽样数据点*接近*或*围绕*某个值的术语，而不是观察或抽样*确切*值。实际上，在这种情况下观察到确切值的概率为零。
- en: When our numbers are in the continuum, there is no discrete separation between
    one value and the next value. Real numbers have an infinite precision. For example,
    if I measure the height of a male and I get 6 feet, I wouldn’t know whether my
    measurement is exaclty 6 feet or 6.00000000785 feet or 5.9999111134255 feet. It’s
    better to set my observation in an interval around 6 feet, for example 5.95 <
    height < 6.05, then quantify the probability of observing a height between 5.95
    and 6.05 feet.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们的数字处于连续状态时，一个值和下一个值之间没有离散的分隔。实数具有无限的精度。例如，如果我测量一个男性的身高，得到6英尺，我就不知道我的测量是确切的6英尺，还是6.00000000785英尺，或者是5.9999111134255英尺。最好将我的观察设置在6英尺左右的区间内，例如5.95
    < height < 6.05，然后量化观察到身高在5.95和6.05英尺之间的概率。
- en: We do not have such a worry for discrete random variables, as we can easily
    separate the possible values from each other. For example, when we roll a die,
    our possible values are 1, 2, 3, 4, 5, or 6\. So we can confidently assert that
    the probability of rolling an exact 5 is 1/6\. Moreover, a discrete random variable
    can have non-numerical outcomes, for example, when we flip a coin, our possible
    values are Head or Tail. A continuous random variable can only have numerical
    outcomes.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 对于离散随机变量，我们不必担心这样的问题，因为我们可以轻松地将可能的值彼此分开。例如，当我们掷骰子时，我们的可能值是1、2、3、4、5或6。因此，我们可以自信地断言掷出确切的5的概率为1/6。此外，离散随机变量可以具有非数值结果，例如，当我们抛硬币时，我们的可能值是正面或反面。连续随机变量只能具有数值结果。
- en: Because of the above reasoning, when we have a continuous random variable, we
    define its probability *density* function, not its probability *mass* function,
    as in the case for discrete random variables. A density specifies how much of
    a substance is present within a certain length or area or volume of space (depending
    on the dimension we’re in). In order to find the mass of a substance in a specified
    region, we multiply the density with the length, area, or volume of the considered
    region. If we are given the density per an infinitesimally small region, then
    we must integrate over the whole region in order to find the mass within that
    region, because an integral is akin to a sum over infinitely many infinitesimally
    small regions.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 由于上述推理，当我们有一个连续随机变量时，我们定义其概率*密度*函数，而不是其概率*质量*函数，就像离散随机变量的情况一样。密度指定了在一定长度、面积或空间体积内存在多少物质（取决于我们所处的维度）。为了找到指定区域内物质的质量，我们将密度与所考虑区域的长度、面积或体积相乘。如果我们给定每个无限小区域的密度，那么我们必须在整个区域上进行积分，以便找到该区域内的质量，因为积分类似于对无限多个无限小区域进行求和。
- en: 'We will elaborate on these ideas and mathematically formalize them in the probability
    chapter. For now, we stress the following:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在概率章节中详细阐述这些想法，并在数学上加以形式化。目前，我们强调以下内容：
- en: 'If we only have one continuous random variable, such as only the height of
    males in a certain population, then we use a one dimensional probability density
    function to represent its probability distribution: <math alttext="f left-parenthesis
    x 1 right-parenthesis"><mrow><mi>f</mi> <mo>(</mo> <msub><mi>x</mi> <mn>1</mn></msub>
    <mo>)</mo></mrow></math> . In order to find the probability of the height to be
    between 5.95 < height < 6.05, we integrate the probability density function <math
    alttext="f left-parenthesis x 1 right-parenthesis"><mrow><mi>f</mi> <mo>(</mo>
    <msub><mi>x</mi> <mn>1</mn></msub> <mo>)</mo></mrow></math> over the interval
    (5.95, 6.05), and we write:'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们只有一个连续随机变量，比如某一人口中男性的身高，那么我们使用一维概率密度函数来表示其概率分布：<math alttext="f left-parenthesis
    x 1 right-parenthesis"><mrow><mi>f</mi> <mo>(</mo> <msub><mi>x</mi> <mn>1</mn></msub>
    <mo>)</mo></mrow></math>。为了找到身高在5.95 < height < 6.05之间的概率，我们对区间（5.95, 6.05）上的概率密度函数<math
    alttext="f left-parenthesis x 1 right-parenthesis"><mrow><mi>f</mi> <mo>(</mo>
    <msub><mi>x</mi> <mn>1</mn></msub> <mo>)</mo></mrow></math>进行积分，我们写成：
- en: <math alttext="dollar-sign upper P left-parenthesis 5.95 less-than h e i g h
    t less-than 6.05 right-parenthesis equals integral Subscript 5.95 Superscript
    6.05 Baseline f left-parenthesis x 1 right-parenthesis d x 1 period dollar-sign"><mrow><mi>P</mi>
    <mrow><mo>(</mo> <mn>5</mn> <mo>.</mo> <mn>95</mn> <mo><</mo> <mi>h</mi> <mi>e</mi>
    <mi>i</mi> <mi>g</mi> <mi>h</mi> <mi>t</mi> <mo><</mo> <mn>6</mn> <mo>.</mo> <mn>05</mn>
    <mo>)</mo></mrow> <mo>=</mo> <msubsup><mo>∫</mo> <mrow><mn>5</mn><mo>.</mo><mn>95</mn></mrow>
    <mrow><mn>6</mn><mo>.</mo><mn>05</mn></mrow></msubsup> <mi>f</mi> <mrow><mo>(</mo>
    <msub><mi>x</mi> <mn>1</mn></msub> <mo>)</mo></mrow> <mi>d</mi> <msub><mi>x</mi>
    <mn>1</mn></msub> <mo>.</mo></mrow></math>
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="dollar-sign upper P left-parenthesis 5.95 less-than h e i g h
    t less-than 6.05 right-parenthesis equals integral Subscript 5.95 Superscript
    6.05 Baseline f left-parenthesis x 1 right-parenthesis d x 1 period dollar-sign"><mrow><mi>P</mi>
    <mrow><mo>(</mo> <mn>5</mn> <mo>.</mo> <mn>95</mn> <mo><</mo> <mi>h</mi> <mi>e</mi>
    <mi>i</mi> <mi>g</mi> <mi>h</mi> <mi>t</mi> <mo><</mo> <mn>6</mn> <mo>.</mo> <mn>05</mn>
    <mo>)</mo></mrow> <mo>=</mo> <msubsup><mo>∫</mo> <mrow><mn>5</mn><mo>.</mo><mn>95</mn></mrow>
    <mrow><mn>6</mn><mo>.</mo><mn>05</mn></mrow></msubsup> <mi>f</mi> <mrow><mo>(</mo>
    <msub><mi>x</mi> <mn>1</mn></msub> <mo>)</mo></mrow> <mi>d</mi> <msub><mi>x</mi>
    <mn>1</mn></msub> <mo>.</mo></mrow></math>
- en: 'If we have two continuous random variables, such as the height and weight of
    males in a certain population, or the true height and the measured height of a
    person (which usually includes random noise), then we use a two dimensional probability
    density function to represent their *joint probability distribution*: <math alttext="f
    left-parenthesis x 1 comma x 2 right-parenthesis"><mrow><mi>f</mi> <mo>(</mo>
    <msub><mi>x</mi> <mn>1</mn></msub> <mo>,</mo> <msub><mi>x</mi> <mn>2</mn></msub>
    <mo>)</mo></mrow></math> . So in order to find the joint probability that the
    height is between 5.95 < height < 6.05 *and* the weight between 160 < weight <
    175, we *double integrate* the joint probability density function <math alttext="f
    left-parenthesis x 1 comma x 2 right-parenthesis"><mrow><mi>f</mi> <mo>(</mo>
    <msub><mi>x</mi> <mn>1</mn></msub> <mo>,</mo> <msub><mi>x</mi> <mn>2</mn></msub>
    <mo>)</mo></mrow></math> , assuming that we know the formula for <math alttext="f
    left-parenthesis x 1 comma x 2 right-parenthesis"><mrow><mi>f</mi> <mo>(</mo>
    <msub><mi>x</mi> <mn>1</mn></msub> <mo>,</mo> <msub><mi>x</mi> <mn>2</mn></msub>
    <mo>)</mo></mrow></math> , over the intervals (5.95, 6.05) and (160,175), and
    we write:'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们有两个连续随机变量，比如某一人群中男性的身高和体重，或者一个人的真实身高和测量身高（通常包括随机噪声），那么我们使用一个二维概率密度函数来表示它们的*联合概率分布*：<math
    alttext="f left-parenthesis x 1 comma x 2 right-parenthesis"><mrow><mi>f</mi>
    <mo>(</mo> <msub><mi>x</mi> <mn>1</mn></msub> <mo>,</mo> <msub><mi>x</mi> <mn>2</mn></msub>
    <mo>)</mo></mrow></math>。因此，为了找到身高在5.95 < height < 6.05 *和* 体重在160 < weight <
    175之间的联合概率，我们对联合概率密度函数<math alttext="f left-parenthesis x 1 comma x 2 right-parenthesis"><mrow><mi>f</mi>
    <mo>(</mo> <msub><mi>x</mi> <mn>1</mn></msub> <mo>,</mo> <msub><mi>x</mi> <mn>2</mn></msub>
    <mo>)</mo></mrow></math>进行*双重积分*，假设我们知道<math alttext="f left-parenthesis x 1 comma
    x 2 right-parenthesis"><mrow><mi>f</mi> <mo>(</mo> <msub><mi>x</mi> <mn>1</mn></msub>
    <mo>,</mo> <msub><mi>x</mi> <mn>2</mn></msub> <mo>)</mo></mrow></math>的公式，对区间(5.95,
    6.05)和(160,175)进行积分，并写成：
- en: <math alttext="dollar-sign upper P left-parenthesis 5.95 less-than h e i g h
    t less-than 6.05 comma 160 less-than w e i g h t less-than 175 right-parenthesis
    equals integral Subscript 160 Superscript 175 Baseline integral Subscript 5.95
    Superscript 6.05 Baseline f left-parenthesis x 1 comma x 2 right-parenthesis d
    x 1 d x 2 period dollar-sign"><mrow><mi>P</mi> <mrow><mo>(</mo> <mn>5</mn> <mo>.</mo>
    <mn>95</mn> <mo><</mo> <mi>h</mi> <mi>e</mi> <mi>i</mi> <mi>g</mi> <mi>h</mi>
    <mi>t</mi> <mo><</mo> <mn>6</mn> <mo>.</mo> <mn>05</mn> <mo>,</mo> <mn>160</mn>
    <mo><</mo> <mi>w</mi> <mi>e</mi> <mi>i</mi> <mi>g</mi> <mi>h</mi> <mi>t</mi> <mo><</mo>
    <mn>175</mn> <mo>)</mo></mrow> <mo>=</mo> <msubsup><mo>∫</mo> <mrow><mn>160</mn></mrow>
    <mn>175</mn></msubsup> <msubsup><mo>∫</mo> <mrow><mn>5</mn><mo>.</mo><mn>95</mn></mrow>
    <mrow><mn>6</mn><mo>.</mo><mn>05</mn></mrow></msubsup> <mi>f</mi> <mrow><mo>(</mo>
    <msub><mi>x</mi> <mn>1</mn></msub> <mo>,</mo> <msub><mi>x</mi> <mn>2</mn></msub>
    <mo>)</mo></mrow> <mi>d</mi> <msub><mi>x</mi> <mn>1</mn></msub> <mi>d</mi> <msub><mi>x</mi>
    <mn>2</mn></msub> <mo>.</mo></mrow></math>
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="dollar-sign upper P left-parenthesis 5.95 less-than h e i g h
    t less-than 6.05 comma 160 less-than w e i g h t less-than 175 right-parenthesis
    equals integral Subscript 160 Superscript 175 Baseline integral Subscript 5.95
    Superscript 6.05 Baseline f left-parenthesis x 1 comma x 2 right-parenthesis d
    x 1 d x 2 period dollar-sign"><mrow><mi>P</mi> <mrow><mo>(</mo> <mn>5</mn> <mo>.</mo>
    <mn>95</mn> <mo><</mo> <mi>h</mi> <mi>e</mi> <mi>i</mi> <mi>g</mi> <mi>h</mi>
    <mi>t</mi> <mo><</mo> <mn>6</mn> <mo>.</mo> <mn>05</mn> <mo>,</mo> <mn>160</mn>
    <mo><</mo> <mi>w</mi> <mi>e</mi> <mi>i</mi> <mi>g</mi> <mi>h</mi> <mi>t</mi> <mo><</mo>
    <mn>175</mn> <mo>)</mo></mrow> <mo>=</mo> <msubsup><mo>∫</mo> <mrow><mn>160</mn></mrow>
    <mn>175</mn></msubsup> <msubsup><mo>∫</mo> <mrow><mn>5</mn><mo>.</mo><mn>95</mn></mrow>
    <mrow><mn>6</mn><mo>.</mo><mn>05</mn></mrow></msubsup> <mi>f</mi> <mrow><mo>(</mo>
    <msub><mi>x</mi> <mn>1</mn></msub> <mo>,</mo> <msub><mi>x</mi> <mn>2</mn></msub>
    <mo>)</mo></mrow> <mi>d</mi> <msub><mi>x</mi> <mn>1</mn></msub> <mi>d</mi> <msub><mi>x</mi>
    <mn>2</mn></msub> <mo>.</mo></mrow></math>
- en: 'If we have more than two continuous random variables, then we use a higher
    dimensional probability density function to represent their joint distribution.
    For example, if we have the height, weight, and blood pressure, of males in a
    certain population, then we use a three dimensional joint probability distribution
    function: <math alttext="f left-parenthesis x 1 comma x 2 comma x 3 right-parenthesis"><mrow><mi>f</mi>
    <mo>(</mo> <msub><mi>x</mi> <mn>1</mn></msub> <mo>,</mo> <msub><mi>x</mi> <mn>2</mn></msub>
    <mo>,</mo> <msub><mi>x</mi> <mn>3</mn></msub> <mo>)</mo></mrow></math> . Similar
    to the reasoning in the above two bullets, in order to find the joint probability
    that the first random variable is between <math alttext="a less-than x 1 less-than
    b"><mrow><mi>a</mi> <mo><</mo> <msub><mi>x</mi> <mn>1</mn></msub> <mo><</mo> <mi>b</mi></mrow></math>
    , the second random variable is between <math alttext="c less-than x 2 less-than
    d"><mrow><mi>c</mi> <mo><</mo> <msub><mi>x</mi> <mn>2</mn></msub> <mo><</mo> <mi>d</mi></mrow></math>
    , and the third random variable is between <math alttext="e less-than x 3 less-than
    f"><mrow><mi>e</mi> <mo><</mo> <msub><mi>x</mi> <mn>3</mn></msub> <mo><</mo> <mi>f</mi></mrow></math>
    , we *triple integrate* the joint probability density function over the intervals
    (a,b), (c,d), and (e,f), and we write:'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们有两个以上的连续随机变量，那么我们使用更高维的概率密度函数来表示它们的联合分布。例如，如果我们有某一人群中男性的身高、体重和血压，那么我们使用一个三维联合概率分布函数：<math
    alttext="f left-parenthesis x 1 comma x 2 comma x 3 right-parenthesis"><mrow><mi>f</mi>
    <mo>(</mo> <msub><mi>x</mi> <mn>1</mn></msub> <mo>,</mo> <msub><mi>x</mi> <mn>2</mn></msub>
    <mo>,</mo> <msub><mi>x</mi> <mn>3</mn></msub> <mo>)</mo></mrow></math>。类似于上述两个项目的推理，为了找到第一个随机变量在<math
    alttext="a less-than x 1 less-than b"><mrow><mi>a</mi> <mo><</mo> <msub><mi>x</mi>
    <mn>1</mn></msub> <mo><</mo> <mi>b</mi></mrow></math>之间的联合概率，第二个随机变量在<math alttext="c
    less-than x 2 less-than d"><mrow><mi>c</mi> <mo><</mo> <msub><mi>x</mi> <mn>2</mn></msub>
    <mo><</mo> <mi>d</mi></mrow></math>之间，第三个随机变量在<math alttext="e less-than x 3 less-than
    f"><mrow><mi>e</mi> <mo><</mo> <msub><mi>x</mi> <mn>3</mn></msub> <mo><</mo> <mi>f</mi></mrow></math>之间，我们对区间(a,b)、(c,d)和(e,f)上的联合概率密度函数进行*三重积分*，并写成：
- en: <math alttext="dollar-sign upper P left-parenthesis a less-than x 1 less-than
    b comma c less-than x 2 less-than d comma e less-than x 3 less-than f right-parenthesis
    equals integral Subscript e Superscript f Baseline integral Subscript c Superscript
    d Baseline integral Subscript a Superscript b Baseline f left-parenthesis x 1
    comma x 2 comma x 3 right-parenthesis d x 1 d x 2 d x 3 period dollar-sign"><mrow><mi>P</mi>
    <mrow><mo>(</mo> <mi>a</mi> <mo><</mo> <msub><mi>x</mi> <mn>1</mn></msub> <mo><</mo>
    <mi>b</mi> <mo>,</mo> <mi>c</mi> <mo><</mo> <msub><mi>x</mi> <mn>2</mn></msub>
    <mo><</mo> <mi>d</mi> <mo>,</mo> <mi>e</mi> <mo><</mo> <msub><mi>x</mi> <mn>3</mn></msub>
    <mo><</mo> <mi>f</mi> <mo>)</mo></mrow> <mo>=</mo> <msubsup><mo>∫</mo> <mrow><mi>e</mi></mrow>
    <mi>f</mi></msubsup> <msubsup><mo>∫</mo> <mrow><mi>c</mi></mrow> <mi>d</mi></msubsup>
    <msubsup><mo>∫</mo> <mrow><mi>a</mi></mrow> <mi>b</mi></msubsup> <mi>f</mi> <mrow><mo>(</mo>
    <msub><mi>x</mi> <mn>1</mn></msub> <mo>,</mo> <msub><mi>x</mi> <mn>2</mn></msub>
    <mo>,</mo> <msub><mi>x</mi> <mn>3</mn></msub> <mo>)</mo></mrow> <mi>d</mi> <msub><mi>x</mi>
    <mn>1</mn></msub> <mi>d</mi> <msub><mi>x</mi> <mn>2</mn></msub> <mi>d</mi> <msub><mi>x</mi>
    <mn>3</mn></msub> <mo>.</mo></mrow></math>
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: Not all our worries (that things will add up mathematically) are eliminated
    even after defining the probability density function for a continuous random variable.
    Again, the culprit is the infinite precision of real numbers. If we allow *all
    sets* to have probability, we encounter paradoxes in the sense that we can contruct
    disjoint sets (such as fractal shaped sets or sets formulated by transforming
    the set of rational numbers) whose probabilities add up to more than one! One
    must admit that these sets are pathological and must be carefully constructed
    by a person who has plenty on time on their hands, however, they exist and they
    produce paradoxes. *Measure Theory* in mathematics steps in and provides a mathematical
    framework where we can work with probability density functions without encountering
    paradoxes. It defines sets of measure zero (these occupy no volume in the space
    that we are working in), then gives us plenty of theorems that allow us to do
    our computations *almost everywhere*, that is, except on sets of measure zero.
    This, turns out, to be more than enough for our applications.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Power Of The Joint Probability Density Function
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Having access to the joint probability distribution of many random variables
    is a powerful, but rare, thing. The reason is that the joint probability distribution
    encodes within it the probability distribution of each separate random variable
    (marginal distributions), as well as all the possible co-occurences (and the conditional
    probabilities) that we ever encounter between these random variables. This is
    akin to seeing a whole town from above, rather than being inside the town and
    observing only one intersection between two or more alleys.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: If the random variables are independent, then the joint distribution is simply
    the product of each of their individual distributions. However, when the random
    variables are not independent, such as the height and the weight of a person,
    or the observed height of a person (which includes measurement noise) and the
    true height of a person (which doesn’t include noise), accessing the joint distribution
    is much more difficult and expensive storage wise. The joint distribution in the
    case of dependent random variables is not separable, so we cannot only store each
    of its parts alone. We need to store every value for every co-occurence between
    the two or more variables. This exponential increase in storage requirements (and
    computations or search spaces) as you increase the number of dependent random
    variables is one embodiment of the infamous *curse of dimensionality*.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: When we *slice* a joint probability density function, say <math alttext="f left-parenthesis
    x 1 comma x 2 right-parenthesis"><mrow><mi>f</mi> <mo>(</mo> <msub><mi>x</mi>
    <mn>1</mn></msub> <mo>,</mo> <msub><mi>x</mi> <mn>2</mn></msub> <mo>)</mo></mrow></math>
    , meaning when we fix one of the random variables (or more in higher dimensions)
    to be an exact value, we retrieve a distribution proportional to the *posterior
    probability distribution* (probability distribution of the model parameters given
    the observations), which we are usually interested in. For example, slice through
    <math alttext="f left-parenthesis x 1 comma x 2 right-parenthesis"><mrow><mi>f</mi>
    <mo>(</mo> <msub><mi>x</mi> <mn>1</mn></msub> <mo>,</mo> <msub><mi>x</mi> <mn>2</mn></msub>
    <mo>)</mo></mrow></math> at <math alttext="x 1 equals a"><mrow><msub><mi>x</mi>
    <mn>1</mn></msub> <mo>=</mo> <mi>a</mi></mrow></math> , we get <math alttext="f
    left-parenthesis a comma x 2 right-parenthesis"><mrow><mi>f</mi> <mo>(</mo> <mi>a</mi>
    <mo>,</mo> <msub><mi>x</mi> <mn>2</mn></msub> <mo>)</mo></mrow></math> , which
    happens to be proportional to the probability distribution <math alttext="f left-parenthesis
    x 2 vertical-bar x 1 equals a right-parenthesis"><mrow><mi>f</mi> <mo>(</mo> <msub><mi>x</mi>
    <mn>2</mn></msub> <mo>|</mo> <msub><mi>x</mi> <mn>1</mn></msub> <mo>=</mo> <mi>a</mi>
    <mo>)</mo></mrow></math> (see [Figure 2-7](#Fig_slice_joint)).
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们*切片*一个联合概率密度函数，比如 <math alttext="f left-parenthesis x 1 comma x 2 right-parenthesis"><mrow><mi>f</mi>
    <mo>(</mo> <msub><mi>x</mi> <mn>1</mn></msub> <mo>,</mo> <msub><mi>x</mi> <mn>2</mn></msub>
    <mo>)</mo></mrow></math> ，意味着当我们固定一个随机变量（或者在更高维度中固定多个）为一个确切的值时，我们会得到与*后验概率分布*成比例的分布（给定观测的模型参数的概率分布），这通常是我们感兴趣的。例如，通过
    <math alttext="f left-parenthesis x 1 comma x 2 right-parenthesis"><mrow><mi>f</mi>
    <mo>(</mo> <msub><mi>x</mi> <mn>1</mn></msub> <mo>,</mo> <msub><mi>x</mi> <mn>2</mn></msub>
    <mo>)</mo></mrow></math> 在 <math alttext="x 1 equals a"><mrow><msub><mi>x</mi>
    <mn>1</mn></msub> <mo>=</mo> <mi>a</mi></mrow></math> 处切片，我们得到 <math alttext="f
    left-parenthesis a comma x 2 right-parenthesis"><mrow><mi>f</mi> <mo>(</mo> <mi>a</mi>
    <mo>,</mo> <msub><mi>x</mi> <mn>2</mn></msub> <mo>)</mo></mrow></math> ，它恰好与概率分布
    <math alttext="f left-parenthesis x 2 vertical-bar x 1 equals a right-parenthesis"><mrow><mi>f</mi>
    <mo>(</mo> <msub><mi>x</mi> <mn>2</mn></msub> <mo>|</mo> <msub><mi>x</mi> <mn>1</mn></msub>
    <mo>=</mo> <mi>a</mi> <mo>)</mo></mrow></math> 成比例（见[图2-7](#Fig_slice_joint)）。
- en: '![300](assets/emai_0207.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![300](assets/emai_0207.png)'
- en: Figure 2-7\. Slicing through the joint probability distribution.
  id: totrans-145
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-7. 通过联合概率分布进行切片。
- en: Again, this is in the luxurious case that we know the joint probability distribution,
    otherwise, we use Bayes’ rule in order to obtain the same posterior probability
    distribution (using the prior distribution and the likelihood function).
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，这是在我们知道联合概率分布的豪华情况下，否则，我们使用贝叶斯定理来获得相同的后验概率分布（使用先验分布和似然函数）。
- en: In some AI applications, the AI system learns the joint probability distribution,
    by separating it into a product of conditional probabilities using the product
    rule for probabilities. Once it learns the joint distribution, it then samples
    from it in order to generate new and interesting data. DeepMind’s WaveNet does
    that in its process of generating raw audio.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在一些人工智能应用中，人工智能系统通过使用概率的乘法规则将联合概率分布分解为条件概率的乘积来学习联合概率分布。一旦学习了联合分布，它就会从中进行采样，以生成新的有趣数据。DeepMind的WaveNet在生成原始音频的过程中就是这样做的。
- en: The next sections introduce the most useful probability distributions for AI
    applications. Two ubiquitous continuous distributions are the *uniform distribution*
    and the *normal distribution* (also known as the *Gaussian distribution*), so
    we start there. Refer to the [Jupyter Notebook](https://github.com/halanelson/Essential-Math-For-AI)
    for reproducing figures and more details.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的章节介绍了人工智能应用中最有用的概率分布。两个普遍存在的连续分布是*均匀分布*和*正态分布*（也称为*高斯分布*），因此我们从这里开始。有关再现图表和更多细节，请参阅[Jupyter
    Notebook](https://github.com/halanelson/Essential-Math-For-AI)。
- en: 'Distribution of Data: The Uniform Distribution'
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据分布：均匀分布
- en: In order to intuitively understand the uniform distribution, let’s give an example
    of a *nonuniform* distribution, which we have already seen earlier in this chapter.
    In our real Height Weight data sets above, we cannot use the *uniform distribution*
    to model the height data. We also cannot use it to model the weight data. The
    reason is that human heights and weights are not evenly distributed. In the general
    population, it is not *equally likely* to encounter a person with height around
    7 feet as it is to encounter a person with height around 5 feet 6 inches.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 为了直观地理解均匀分布，让我们举一个*非均匀*分布的例子，我们在本章前面已经看到过。在我们上面的真实身高体重数据集中，我们不能使用*均匀分布*来建模身高数据。我们也不能用它来建模体重数据。原因是人类的身高和体重并不是均匀分布的。在一般人群中，遇到身高约7英尺的人和遇到身高约5英尺6英寸的人并不是*同样可能*的。
- en: The *uniform distribution* only models data that is evenly distributed. If we
    have an interval <math alttext="left-parenthesis x Subscript m i n Baseline comma
    x Subscript m a x Baseline right-parenthesis"><mrow><mo>(</mo> <msub><mi>x</mi>
    <mrow><mi>m</mi><mi>i</mi><mi>n</mi></mrow></msub> <mo>,</mo> <msub><mi>x</mi>
    <mrow><mi>m</mi><mi>a</mi><mi>x</mi></mrow></msub> <mo>)</mo></mrow></math> containing
    all the values in the continuum between <math alttext="x Subscript m i n"><msub><mi>x</mi>
    <mrow><mi>m</mi><mi>i</mi><mi>n</mi></mrow></msub></math> and <math alttext="x
    Subscript m a x"><msub><mi>x</mi> <mrow><mi>m</mi><mi>a</mi><mi>x</mi></mrow></msub></math>
    of our data, and our data is uniformly distributed over our interval, then the
    probability of a observing a data point near any particular value in our interval
    is the same for all values in this interval. That is, if our interval is <math
    alttext="left-parenthesis 0 comma 1 right-parenthesis"><mrow><mo>(</mo> <mn>0</mn>
    <mo>,</mo> <mn>1</mn> <mo>)</mo></mrow></math> , it is equally likely to pick
    a point near 0.2 as it is to pick a point near 0.75.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '*均匀分布*只模拟均匀分布的数据。如果我们有一个区间<mrow><mo>(</mo> <msub><mi>x</mi> <mrow><mi>m</mi><mi>i</mi><mi>n</mi></mrow></msub>
    <mo>,</mo> <msub><mi>x</mi> <mrow><mi>m</mi><mi>a</mi><mi>x</mi></mrow></msub>
    <mo>)</mo></mrow>，其中包含我们的数据在<msub><mi>x</mi> <mrow><mi>m</mi><mi>i</mi><mi>n</mi></mrow></msub>和<msub><mi>x</mi>
    <mrow><mi>m</mi><mi>a</mi><mi>x</mi></mrow></msub>之间的连续值，我们的数据在我们的区间内均匀分布，那么在我们的区间内观察到接近任何特定值的数据点的概率对于该区间内的所有值来说是相同的。也就是说，如果我们的区间是<mrow><mo>(</mo>
    <mn>0</mn> <mo>,</mo> <mn>1</mn> <mo>)</mo></mrow>，那么选择接近0.2的点和选择接近0.75的点是同样可能的。'
- en: 'The probability density function for the uniform distribution is therefore
    constant. For one random variable x over an interval <math alttext="left-parenthesis
    x Subscript m i n Baseline comma x Subscript m a x Baseline right-parenthesis"><mrow><mo>(</mo>
    <msub><mi>x</mi> <mrow><mi>m</mi><mi>i</mi><mi>n</mi></mrow></msub> <mo>,</mo>
    <msub><mi>x</mi> <mrow><mi>m</mi><mi>a</mi><mi>x</mi></mrow></msub> <mo>)</mo></mrow></math>
    , the formula for the probability density function for the continuous uniform
    distribution is given by:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，均匀分布的概率密度函数是常数。对于区间<mrow><mo>(</mo> <msub><mi>x</mi> <mrow><mi>m</mi><mi>i</mi><mi>n</mi></mrow></msub>
    <mo>,</mo> <msub><mi>x</mi> <mrow><mi>m</mi><mi>a</mi><mi>x</mi></mrow></msub>
    <mo>)</mo></mrow>上的一个随机变量x，连续均匀分布的概率密度函数的公式如下：
- en: <math alttext="dollar-sign f left-parenthesis x semicolon x Subscript m i n
    Baseline comma x Subscript m a x Baseline right-parenthesis equals StartFraction
    1 Over x Subscript m a x Baseline minus x Subscript m i n Baseline EndFraction
    for x Subscript m i n Baseline less-than x less-than x Subscript m a x Baseline
    comma dollar-sign"><mrow><mi>f</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>;</mo> <msub><mi>x</mi>
    <mrow><mi>m</mi><mi>i</mi><mi>n</mi></mrow></msub> <mo>,</mo> <msub><mi>x</mi>
    <mrow><mi>m</mi><mi>a</mi><mi>x</mi></mrow></msub> <mo>)</mo></mrow> <mo>=</mo>
    <mfrac><mn>1</mn> <mrow><msub><mi>x</mi> <mrow><mi>m</mi><mi>a</mi><mi>x</mi></mrow></msub>
    <mo>-</mo><msub><mi>x</mi> <mrow><mi>m</mi><mi>i</mi><mi>n</mi></mrow></msub></mrow></mfrac>
    <mtext>for</mtext> <msub><mi>x</mi> <mrow><mi>m</mi><mi>i</mi><mi>n</mi></mrow></msub>
    <mo><</mo> <mi>x</mi> <mo><</mo> <msub><mi>x</mi> <mrow><mi>m</mi><mi>a</mi><mi>x</mi></mrow></msub>
    <mo>,</mo></mrow></math>
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: <mrow><mi>f</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>;</mo> <msub><mi>x</mi> <mrow><mi>m</mi><mi>i</mi><mi>n</mi></mrow></msub>
    <mo>,</mo> <msub><mi>x</mi> <mrow><mi>m</mi><mi>a</mi><mi>x</mi></mrow></msub>
    <mo>)</mo></mrow> <mo>=</mo> <mfrac><mn>1</mn> <mrow><msub><mi>x</mi> <mrow><mi>m</mi><mi>a</mi><mi>x</mi></mrow></msub>
    <mo>-</mo><msub><mi>x</mi> <mrow><mi>m</mi><mi>i</mi><mi>n</mi></mrow></msub></mrow></mfrac>
    <mtext>for</mtext> <msub><mi>x</mi> <mrow><mi>m</mi><mi>i</mi><mi>n</mi></mrow></msub>
    <mo><</mo> <mi>x</mi> <mo><</mo> <msub><mi>x</mi> <mrow><mi>m</mi><mi>a</mi><mi>x</mi></mrow></msub>
    <mo>,</mo>
- en: and zero otherwise.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在其他情况下为零。
- en: Let’s plot the probability density function for the uniform distribution over
    an interval <math alttext="left-parenthesis x Subscript m i n Baseline comma x
    Subscript m a x Baseline right-parenthesis"><mrow><mo>(</mo> <msub><mi>x</mi>
    <mrow><mi>m</mi><mi>i</mi><mi>n</mi></mrow></msub> <mo>,</mo> <msub><mi>x</mi>
    <mrow><mi>m</mi><mi>a</mi><mi>x</mi></mrow></msub> <mo>)</mo></mrow></math> .
    The graph in [Figure 2-8](#Fig_uniform) is a straight segment because uniformly
    distributed data, whether real or simulated, appears evenly distributed across
    the entire interval under consideration. No data values within the interval are
    more favored to appear than others.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们绘制均匀分布在区间<mrow><mo>(</mo> <msub><mi>x</mi> <mrow><mi>m</mi><mi>i</mi><mi>n</mi></mrow></msub>
    <mo>,</mo> <msub><mi>x</mi> <mrow><mi>m</mi><mi>a</mi><mi>x</mi></mrow></msub>
    <mo>)</mo></mrow>上的概率密度函数。[图2-8](#Fig_uniform)中的图表是一条直线段，因为均匀分布的数据，无论是真实的还是模拟的，都在考虑的整个区间内均匀分布。区间内没有比其他更有利于出现的数据值。
- en: '![300](assets/emai_0208.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![300](assets/emai_0208.png)'
- en: Figure 2-8\. Graph of the probability density function of the uniform distribution
    over the interval [0,1].
  id: totrans-157
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-8。概率密度函数的图表，显示了在区间[0,1]上的均匀分布。
- en: The uniform distribution is extremely useful in computer simulations for generating
    random numbers *from any other probability distribution*. If you peek into the
    random number generators that Python uses, you would see the uniform distribution
    used somewhere in the underlying algorithms.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 均匀分布在计算机模拟中非常有用，用于生成来自*任何其他概率分布*的随机数。如果你瞥一眼Python使用的随机数生成器，你会看到均匀分布在底层算法中的某个地方被使用。
- en: 'Distribution of Data: The Bell Shaped Normal (Gaussian) Distribution'
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据分布：钟形正态（高斯）分布
- en: A continuous probability ditribution better suited to model human height data
    (when restricted to one gender) is the *bell shaped normal distribution*, also
    called the *Gaussian distribution*. Samples from the normal distribution tend
    to congregate around an average value where the distribution peaks, called the
    *mean* <math alttext="mu"><mi>μ</mi></math> , then dwindle symmetrically as we
    get farther away from the mean. How far from the mean the distribution spreads
    out as it dwindles down is controlled by the second parameter of the normal distribution,
    called the *standard deviation* <math alttext="sigma"><mi>σ</mi></math> . About
    68% of the data falls within one standard deviation of the mean, 95% of the data
    falls within two standard deviations of the mean, and about 99.7% of the data
    falls within three standard deviations of the mean.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 连续概率分布更适合模拟人类身高数据（当限定为一种性别时）的是*钟形正态分布*，也称为*高斯分布*。来自正态分布的样本倾向于聚集在分布达到峰值的平均值周围，称为*均值*<math
    alttext="mu"><mi>μ</mi></math>，然后随着远离均值而对称减少。分布在远离均值处的扩散程度由正态分布的第二个参数控制，称为*标准差*<math
    alttext="sigma"><mi>σ</mi></math>。大约68%的数据落在均值的一个标准差范围内，95%的数据落在均值的两个标准差范围内，大约99.7%的数据落在均值的三个标准差范围内。
- en: '![300](assets/emai_0209.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![300](assets/emai_0209.png)'
- en: Figure 2-9\. Graph of the probability density function of the bell shaped normal
    distribution with parameters <math alttext="mu equals 0"><mrow><mi>μ</mi> <mo>=</mo>
    <mn>0</mn></mrow></math> and <math alttext="sigma equals 1"><mrow><mi>σ</mi> <mo>=</mo>
    <mn>1</mn></mrow></math> .
  id: totrans-162
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-9. 带有参数<math alttext="mu equals 0"><mrow><mi>μ</mi> <mo>=</mo> <mn>0</mn></mrow></math>和<math
    alttext="sigma equals 1"><mrow><mi>σ</mi> <mo>=</mo> <mn>1</mn></mrow></math>的钟形正态分布的概率密度函数图。
- en: Values near the mean are more likely to be picked (or to occur, or to be observed)
    when we are sampling data from the normal distribution, and values that are very
    small ( <math alttext="negative normal infinity"><mrow><mo>→</mo> <mo>-</mo> <mi>∞</mi></mrow></math>
    ) or very large ( <math alttext="normal infinity"><mrow><mo>→</mo> <mi>∞</mi></mrow></math>
    ) are less likely to be picked. This peaking near the mean value and decaying
    on the outer skirts of the distribution gives this distribution its famous bell
    shape. Note that there are other bell shaped continuous distributions out there,
    but the normal distribution is the most prevelant. It has a neat mathematical
    justification for this well deserved fame, based on an important theorem in probability
    theory called *The Central Limit Theorem*.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们从正态分布中抽取数据时，靠近均值的值更有可能被选中（或出现，或被观察到），而非常小的值（<math alttext="negative normal
    infinity"><mrow><mo>→</mo> <mo>-</mo> <mi>∞</mi></mrow></math>）或非常大的值（<math alttext="normal
    infinity"><mrow><mo>→</mo> <mi>∞</mi></mrow></math>）更不可能被选中。这种在分布的外围呈现出钟形的分布形状。请注意，还有其他钟形连续分布，但正态分布是最常见的。它有一个整洁的数学理由来解释这个当之无愧的名声，这是基于概率论中一个重要的定理，称为*中心极限定理*。
- en: The Central Limit Theorem states that the average of many independent random
    variables, that all have the same distribution (not necessarily the normal distribution),
    is normally distributed. This explains why the normal distribution appears everywhere
    in society and nature. It models baby birth weights, student grade distributions,
    countries’ income distributions, distribution of blood pressure measurements,
    *etc*. There are special statistical tests that help us determine whether a real
    data set can be modeled using the normal distribution. We will expand on these
    ideas later in the probability chapter.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 中心极限定理表明，许多相互独立的具有相同分布（不一定是正态分布）的随机变量的平均值是正态分布的。这解释了为什么正态分布在社会和自然界中随处可见。它模拟了婴儿出生体重、学生成绩分布、国家收入分布、血压测量分布等。有特殊的统计检验可以帮助我们确定一个真实数据集是否可以用正态分布来建模。我们将在后面的概率章节中扩展这些想法。
- en: If you happen to find yourself in a situation where you are uncertain and have
    no prior knowledge about which distribution to use for your application, the normal
    distribution is usually a reasonable choice. In fact, among all choices of distributions
    with the same variance, the normal distribution is the choice with maximum *uncertainty*,
    so it does in fact encode the least amount of prior knowledge into your model.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你发现自己处于一个不确定的情况，并且对于你的应用程序不知道使用哪种分布，正态分布通常是一个合理的选择。事实上，在所有具有相同方差的分布选择中，正态分布是具有最大*不确定性*的选择，因此实际上它将最少的先验知识编码到你的模型中。
- en: 'The formula for the probability density function of the normal distribution
    for one random variable x (univariate) with mean <math alttext="mu"><mi>μ</mi></math>
    and standard deviation <math alttext="sigma"><mi>σ</mi></math> is:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 对于具有均值<math alttext="mu"><mi>μ</mi></math>和标准差<math alttext="sigma"><mi>σ</mi></math>的一个随机变量x（单变量）的正态分布的概率密度函数的公式为：
- en: <math alttext="dollar-sign g left-parenthesis x semicolon mu comma sigma right-parenthesis
    equals StartFraction 1 Over StartRoot 2 pi sigma squared EndRoot EndFraction e
    Superscript minus StartFraction left-parenthesis x minus mu right-parenthesis
    squared Over 2 sigma squared EndFraction Baseline comma dollar-sign"><mrow><mi>g</mi>
    <mrow><mo>(</mo> <mi>x</mi> <mo>;</mo> <mi>μ</mi> <mo>,</mo> <mi>σ</mi> <mo>)</mo></mrow>
    <mo>=</mo> <mfrac><mn>1</mn> <msqrt><mrow><mn>2</mn><mi>π</mi><msup><mi>σ</mi>
    <mn>2</mn></msup></mrow></msqrt></mfrac> <msup><mi>e</mi> <mrow><mo>-</mo><mfrac><msup><mrow><mo>(</mo><mi>x</mi><mo>-</mo><mi>μ</mi><mo>)</mo></mrow>
    <mn>2</mn></msup> <mrow><mn>2</mn><msup><mi>σ</mi> <mn>2</mn></msup></mrow></mfrac></mrow></msup>
    <mo>,</mo></mrow></math>
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="dollar-sign g left-parenthesis x semicolon mu comma sigma right-parenthesis
    equals StartFraction 1 Over StartRoot 2 pi sigma squared EndRoot EndFraction e
    Superscript minus StartFraction left-parenthesis x minus mu right-parenthesis
    squared Over 2 sigma squared EndFraction Baseline comma dollar-sign"><mrow><mi>g</mi>
    <mrow><mo>(</mo> <mi>x</mi> <mo>;</mo> <mi>μ</mi> <mo>,</mo> <mi>σ</mi> <mo>)</mo></mrow>
    <mo>=</mo> <mfrac><mn>1</mn> <msqrt><mrow><mn>2</mn><mi>π</mi><msup><mi>σ</mi>
    <mn>2</mn></msup></mrow></msqrt></mfrac> <msup><mi>e</mi> <mrow><mo>-</mo><mfrac><msup><mrow><mo>(</mo><mi>x</mi><mo>-</mo><mi>μ</mi><mo>)</mo></mrow>
    <mn>2</mn></msup> <mrow><mn>2</mn><msup><mi>σ</mi> <mn>2</mn></msup></mrow></mfrac></mrow></msup>
    <mo>,</mo></mrow></math>
- en: and its graph for <math alttext="mu equals 0"><mrow><mi>μ</mi> <mo>=</mo> <mn>0</mn></mrow></math>
    and <math alttext="sigma equals 1"><mrow><mi>σ</mi> <mo>=</mo> <mn>1</mn></mrow></math>
    is plotted in [Figure 2-9](#Fig_standard_normal).
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 <math alttext="mu equals 0"><mrow><mi>μ</mi> <mo>=</mo> <mn>0</mn></mrow></math>
    和 <math alttext="sigma equals 1"><mrow><mi>σ</mi> <mo>=</mo> <mn>1</mn></mrow></math>，其图表绘制在
    [Figure 2-9](#Fig_standard_normal) 中。
- en: 'The formula for the probability density function for the normal distribution
    of two random variables x and y (bivariate) is:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 双变量 x 和 y（双变量）的正态分布概率密度函数的公式为：
- en: <math alttext="dollar-sign g left-parenthesis x comma y semicolon mu 1 comma
    sigma 1 comma mu 2 comma sigma 2 comma rho right-parenthesis equals StartFraction
    1 Over StartRoot left-parenthesis 2 pi right-parenthesis squared det Start 2 By
    2 Matrix 1st Row 1st Column sigma 1 squared 2nd Column rho sigma 1 sigma 2 2nd
    Row 1st Column rho sigma 1 sigma 2 2nd Column sigma 2 squared EndMatrix EndRoot
    EndFraction e Superscript minus Baseline left-brace one-half Start 1 By 2 Matrix
    1st Row 1st Column x minus mu 1 2nd Column y minus mu 2 EndMatrix Start 2 By 2
    Matrix 1st Row 1st Column sigma 1 squared 2nd Column rho sigma 1 sigma 2 2nd Row
    1st Column rho sigma 1 sigma 2 2nd Column sigma 2 squared EndMatrix Superscript
    negative 1 Baseline StartBinomialOrMatrix x minus mu 1 Choose y minus mu 2 EndBinomialOrMatrix
    right-brace comma dollar-sign"><mrow><mi>g</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>,</mo>
    <mi>y</mi> <mo>;</mo> <msub><mi>μ</mi> <mn>1</mn></msub> <mo>,</mo> <msub><mi>σ</mi>
    <mn>1</mn></msub> <mo>,</mo> <msub><mi>μ</mi> <mn>2</mn></msub> <mo>,</mo> <msub><mi>σ</mi>
    <mn>2</mn></msub> <mo>,</mo> <mi>ρ</mi> <mo>)</mo></mrow> <mo>=</mo> <mfrac><mn>1</mn>
    <msqrt><mrow><msup><mrow><mo>(</mo><mn>2</mn><mi>π</mi><mo>)</mo></mrow> <mn>2</mn></msup>
    <mo form="prefix" movablelimits="true">det</mo><mfenced close=")" open="("><mtable><mtr><mtd><msubsup><mi>σ</mi>
    <mn>1</mn> <mn>2</mn></msubsup></mtd> <mtd><mrow><mi>ρ</mi><msub><mi>σ</mi> <mn>1</mn></msub>
    <msub><mi>σ</mi> <mn>2</mn></msub></mrow></mtd></mtr> <mtr><mtd><mrow><mi>ρ</mi><msub><mi>σ</mi>
    <mn>1</mn></msub> <msub><mi>σ</mi> <mn>2</mn></msub></mrow></mtd> <mtd><msubsup><mi>σ</mi>
    <mn>2</mn> <mn>2</mn></msubsup></mtd></mtr></mtable></mfenced></mrow></msqrt></mfrac>
    <msup><mi>e</mi> <mo>-</mo></msup> <mfenced close="}" open="{" separators=""><mrow><mfrac><mn>1</mn>
    <mn>2</mn></mfrac> <mfenced close=")" open="("><mtable><mtr><mtd><mrow><mi>x</mi>
    <mo>-</mo> <msub><mi>μ</mi> <mn>1</mn></msub></mrow></mtd> <mtd><mrow><mi>y</mi>
    <mo>-</mo> <msub><mi>μ</mi> <mn>2</mn></msub></mrow></mtd></mtr></mtable></mfenced>
    <msup><mfenced close=")" open="("><mtable><mtr><mtd><msubsup><mi>σ</mi> <mn>1</mn>
    <mn>2</mn></msubsup></mtd> <mtd><mrow><mi>ρ</mi><msub><mi>σ</mi> <mn>1</mn></msub>
    <msub><mi>σ</mi> <mn>2</mn></msub></mrow></mtd></mtr> <mtr><mtd><mrow><mi>ρ</mi><msub><mi>σ</mi>
    <mn>1</mn></msub> <msub><mi>σ</mi> <mn>2</mn></msub></mrow></mtd> <mtd><msubsup><mi>σ</mi>
    <mn>2</mn> <mn>2</mn></msubsup></mtd></mtr></mtable></mfenced> <mrow><mo>-</mo><mn>1</mn></mrow></msup>
    <mfenced close=")" open="("><mtable><mtr><mtd><mrow><mi>x</mi> <mo>-</mo> <msub><mi>μ</mi>
    <mn>1</mn></msub></mrow></mtd></mtr> <mtr><mtd><mrow><mi>y</mi> <mo>-</mo> <msub><mi>μ</mi>
    <mn>2</mn></msub></mrow></mtd></mtr></mtable></mfenced></mrow></mfenced> <mo>,</mo></mrow></math>
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="dollar-sign g left-parenthesis x comma y semicolon mu 1 comma
    sigma 1 comma mu 2 comma sigma 2 comma rho right-parenthesis equals StartFraction
    1 Over StartRoot left-parenthesis 2 pi right-parenthesis squared det Start 2 By
    2 Matrix 1st Row 1st Column sigma 1 squared 2nd Column rho sigma 1 sigma 2 2nd
    Row 1st Column rho sigma 1 sigma 2 2nd Column sigma 2 squared EndMatrix EndRoot
    EndFraction e Superscript minus Baseline left-brace one-half Start 1 By 2 Matrix
    1st Row 1st Column x minus mu 1 2nd Column y minus mu 2 EndMatrix Start 2 By 2
    Matrix 1st Row 1st Column sigma 1 squared 2nd Column rho sigma 1 sigma 2 2nd Row
    1st Column rho sigma 1 sigma 2 2nd Column sigma 2 squared EndMatrix Superscript
    negative 1 Baseline StartBinomialOrMatrix x minus mu 1 Choose y minus mu 2 EndBinomialOrMatrix
    right-brace comma dollar-sign"><mrow><mi>g</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>,</mo>
    <mi>y</mi> <mo>;</mo> <msub><mi>μ</mi> <mn>1</mn></msub> <mo>,</mo> <msub><mi>σ</mi>
    <mn>1</mn></msub> <mo>,</mo> <msub><mi>μ</mi> <mn>2</mn></msub> <mo>,</mo> <msub><mi>σ</mi>
    <mn>2</mn></msub> <mo>,</mo> <mi>ρ</mi> <mo>)</mo></mrow> <mo>=</mo> <mfrac><mn>1</mn>
    <msqrt><mrow><msup><mrow><mo>(</mo><mn>2</mn><mi>π</mi><mo>)</mo></mrow> <mn>2</mn></msup>
    <mo form="prefix" movablelimits="true">det</mo><mfenced close=")" open="("><mtable><mtr><mtd><msubsup><mi>σ</mi>
    <mn>1</mn> <mn>2</mn></msubsup></mtd> <mtd><mrow><mi>ρ</mi><msub><mi>σ</mi> <mn>1</mn></msub>
    <msub><mi>σ</mi> <mn>2</mn></msub></mrow></mtd></mtr> <mtr><mtd><mrow><mi>ρ</mi><msub><mi>σ</mi>
    <mn>1</mn></msub> <msub><mi>σ</mi> <mn>2</mn></msub></mrow></mtd> <mtd><msubsup><mi>σ</mi>
    <mn>2</mn> <mn>2</mn></msubsup></mtd></mtr></mtable></mfenced></mrow></msqrt></mfrac>
    <msup><mi>e</mi> <mo>-</mo></msup> <mfenced close="}" open="{" separators=""><mrow><mfrac><mn>1</mn>
    <mn>2</mn></mfrac> <mfenced close=")" open="("><mtable><mtr><mtd><mrow><mi>x</mi>
    <mo>-</mo> <msub><mi>μ</mi> <mn>1</mn></msub></mrow></mtd> <mtd><mrow><mi>y</mi>
    <mo>-</mo> <msub><mi>μ</mi> <mn>2</mn></msub></mrow></mtd></mtr></mtable></mfenced>
    <msup><mfenced close=")" open="("><mtable><mtr><mtd><msubsup><mi>σ</mi> <mn>1</mn>
    <mn>2</mn></msubsup></mtd> <mtd><mrow><mi>ρ</mi><msub><mi>σ</mi> <mn>1</mn></msub>
    <msub><mi>σ</mi> <mn>2</mn></msub></mrow></mtd></mtr> <mtr><mtd><mrow><mi>ρ</mi><msub><mi>σ</mi>
    <mn>1</mn></msub> <msub><mi>σ</mi> <mn>2</mn></msub></mrow></mtd> <mtd><msubsup><mi>σ</mi>
    <mn>2</mn> <mn>2</mn></msubsup></mtd></mtr></mtable></mfenced> <mrow><mo>-</mo><mn>1</mn></mrow></msup>
    <mfenced close=")" open="("><mtable><mtr><mtd><mrow><mi>x</mi> <mo>-</mo> <msub><mi>μ</mi>
    <mn>1</mn></msub></mrow></mtd></mtr> <mtr><mtd><mrow><mi>y</mi> <mo>-</mo> <msub><mi>μ</mi>
    <mn>2</mn></msub></mrow></mtd></mtr></mtable></mfenced></mrow></mfenced> <mo>,</mo></mrow></math>
- en: and its graph is plotted in [Figure 2-10](#Fig_bivariate_normal).
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 并且它的图在[图2-10](#Fig_bivariate_normal)中绘制。
- en: 'We can write the above bivariate formula in more compact notation using the
    language of linear algebra:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用线性代数的语言更简洁地写出上述双变量公式：
- en: <math alttext="dollar-sign g left-parenthesis x comma y semicolon mu comma normal
    upper Sigma right-parenthesis equals StartFraction 1 Over StartRoot left-parenthesis
    2 pi right-parenthesis squared det left-parenthesis normal upper Sigma right-parenthesis
    EndRoot EndFraction e Superscript minus Baseline left-brace one-half left-parenthesis
    u minus mu right-parenthesis Superscript upper T Baseline normal upper Sigma Superscript
    negative 1 Baseline left-parenthesis u minus mu right-parenthesis right-brace
    dollar-sign"><mrow><mi>g</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>,</mo> <mi>y</mi>
    <mo>;</mo> <mi>μ</mi> <mo>,</mo> <mi>Σ</mi> <mo>)</mo></mrow> <mo>=</mo> <mfrac><mn>1</mn>
    <msqrt><mrow><msup><mrow><mo>(</mo><mn>2</mn><mi>π</mi><mo>)</mo></mrow> <mn>2</mn></msup>
    <mo form="prefix" movablelimits="true">det</mo><mrow><mo>(</mo><mi>Σ</mi><mo>)</mo></mrow></mrow></msqrt></mfrac>
    <msup><mi>e</mi> <mo>-</mo></msup> <mfenced close="}" open="{" separators=""><mrow><mfrac><mn>1</mn>
    <mn>2</mn></mfrac> <msup><mrow><mo>(</mo><mi>u</mi><mo>-</mo><mi>μ</mi><mo>)</mo></mrow>
    <mi>T</mi></msup> <msup><mi>Σ</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup> <mrow><mo>(</mo>
    <mi>u</mi> <mo>-</mo> <mi>μ</mi> <mo>)</mo></mrow></mrow></mfenced></mrow></math>![250](assets/emai_0210.png)
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="dollar-sign g left-parenthesis x comma y semicolon mu comma normal
    upper Sigma right-parenthesis equals StartFraction 1 Over StartRoot left-parenthesis
    2 pi right-parenthesis squared det left-parenthesis normal upper Sigma right-parenthesis
    EndRoot EndFraction e Superscript minus Baseline left-brace one-half left-parenthesis
    u minus mu right-parenthesis Superscript upper T Baseline normal upper Sigma Superscript
    negative 1 Baseline left-parenthesis u minus mu right-parenthesis right-brace
    dollar-sign"><mrow><mi>g</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>,</mo> <mi>y</mi>
    <mo>;</mo> <mi>μ</mi> <mo>,</mo> <mi>Σ</mi> <mo>)</mo></mrow> <mo>=</mo> <mfrac><mn>1</mn>
    <msqrt><mrow><msup><mrow><mo>(</mo><mn>2</mn><mi>π</mi><mo>)</mo></mrow> <mn>2</mn></msup>
    <mo form="prefix" movablelimits="true">det</mo><mrow><mo>(</mo><mi>Σ</mi><mo>)</mo></mrow></mrow></msqrt></mfrac>
    <msup><mi>e</mi> <mo>-</mo></msup> <mfenced close="}" open="{" separators=""><mrow><mfrac><mn>1</mn>
    <mn>2</mn></mfrac> <msup><mrow><mo>(</mo><mi>u</mi><mo>-</mo><mi>μ</mi><mo>)</mo></mrow>
    <mi>T</mi></msup> <msup><mi>Σ</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup> <mrow><mo>(</mo>
    <mi>u</mi> <mo>-</mo> <mi>μ</mi> <mo>)</mo></mrow></mrow></mfenced></mrow></math>![250](assets/emai_0210.png)
- en: Figure 2-10\. Graph of the probability density function of the bell shaped bivariate
    normal distribution.
  id: totrans-174
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-10。钟形双变量正态分布的概率密度函数图。
- en: In [Figure 2-11](#Fig_bivariate_normal_sampling), we sample 6000 points from
    the bivariate normal distribution.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图2-11](#Fig_bivariate_normal_sampling)中，我们从双变量正态分布中抽取了6000个点。
- en: '![250](assets/emai_0211.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![250](assets/emai_0211.png)'
- en: Figure 2-11\. Sampling 6000 points from the bivariate normal distribution. Points
    near the center are more likey to be picked, and points away from the center are
    less likely to be picked. The lines rougly trace the contour lines of the normal
    distribution, had we only observed the sample points without knowing which distribution
    they were sampled from.
  id: totrans-177
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-11。从双变量正态分布中抽取6000个点。
- en: 'Let’s pause and compare the formula of the probability density function for
    the bivariate normal distribution to formula of the probability density function
    for the univariate normal distribution:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们暂停一下，比较双变量正态分布的概率密度函数公式和单变量正态分布的概率密度函数公式：
- en: When there is only one random variable we only have one mean <math alttext="mu"><mi>μ</mi></math>
    and one standard deviation <math alttext="sigma"><mi>σ</mi></math> .
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当只有一个随机变量时，我们只有一个均值<math alttext="mu"><mi>μ</mi></math>和一个标准差<math alttext="sigma"><mi>σ</mi></math>。
- en: When there are two random variables, we have two means <math alttext="StartBinomialOrMatrix
    mu 1 Choose mu 2 EndBinomialOrMatrix"><mfenced close=")" open="("><mtable><mtr><mtd><msub><mi>μ</mi>
    <mn>1</mn></msub></mtd></mtr> <mtr><mtd><msub><mi>μ</mi> <mn>2</mn></msub></mtd></mtr></mtable></mfenced></math>
    , two standard deviations <math alttext="StartBinomialOrMatrix sigma 1 Choose
    sigma 2 EndBinomialOrMatrix"><mfenced close=")" open="("><mtable><mtr><mtd><msub><mi>σ</mi>
    <mn>1</mn></msub></mtd></mtr> <mtr><mtd><msub><mi>σ</mi> <mn>2</mn></msub></mtd></mtr></mtable></mfenced></math>
    . The product <math alttext="sigma squared"><msup><mi>σ</mi> <mn>2</mn></msup></math>
    will be replaced by the covariance matrix <math alttext="normal upper Sigma equals
    Start 2 By 2 Matrix 1st Row 1st Column sigma 1 squared 2nd Column rho sigma 1
    sigma 2 2nd Row 1st Column rho sigma 1 sigma 2 2nd Column sigma 2 squared EndMatrix"><mrow><mi>Σ</mi>
    <mo>=</mo> <mfenced close=")" open="("><mtable><mtr><mtd><msubsup><mi>σ</mi> <mn>1</mn>
    <mn>2</mn></msubsup></mtd> <mtd><mrow><mi>ρ</mi> <msub><mi>σ</mi> <mn>1</mn></msub>
    <msub><mi>σ</mi> <mn>2</mn></msub></mrow></mtd></mtr> <mtr><mtd><mrow><mi>ρ</mi>
    <msub><mi>σ</mi> <mn>1</mn></msub> <msub><mi>σ</mi> <mn>2</mn></msub></mrow></mtd>
    <mtd><msubsup><mi>σ</mi> <mn>2</mn> <mn>2</mn></msubsup></mtd></mtr></mtable></mfenced></mrow></math>
    and its determinant. <math alttext="rho"><mi>ρ</mi></math> is the correlation
    between the two random variables, which is the covariance of the two normalized
    versions of the random variables.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当有两个随机变量时，我们有两个均值<math alttext="StartBinomialOrMatrix mu 1 Choose mu 2 EndBinomialOrMatrix"><mfenced
    close=")" open="("><mtable><mtr><mtd><msub><mi>μ</mi> <mn>1</mn></msub></mtd></mtr>
    <mtr><mtd><msub><mi>μ</mi> <mn>2</mn></msub></mtd></mtr></mtable></mfenced></math>，两个标准差<math
    alttext="StartBinomialOrMatrix sigma 1 Choose sigma 2 EndBinomialOrMatrix"><mfenced
    close=")" open="("><mtable><mtr><mtd><msub><mi>σ</mi> <mn>1</mn></msub></mtd></mtr>
    <mtr><mtd><msub><mi>σ</mi> <mn>2</mn></msub></mtd></mtr></mtable></mfenced></math>。乘积<math
    alttext="sigma squared"><msup><mi>σ</mi> <mn>2</mn></msup></math>将被协方差矩阵<math
    alttext="normal upper Sigma equals Start 2 By 2 Matrix 1st Row 1st Column sigma
    1 squared 2nd Column rho sigma 1 sigma 2 2nd Row 1st Column rho sigma 1 sigma
    2 2nd Column sigma 2 squared EndMatrix"><mrow><mi>Σ</mi> <mo>=</mo> <mfenced close=")"
    open="("><mtable><mtr><mtd><msubsup><mi>σ</mi> <mn>1</mn> <mn>2</mn></msubsup></mtd>
    <mtd><mrow><mi>ρ</mi> <msub><mi>σ</mi> <mn>1</mn></msub> <msub><mi>σ</mi> <mn>2</mn></msub></mrow></mtd></mtr>
    <mtr><mtd><mrow><mi>ρ</mi> <msub><mi>σ</mi> <mn>1</mn></msub> <msub><mi>σ</mi>
    <mn>2</mn></msub></mrow></mtd> <mtd><msubsup><mi>σ</mi> <mn>2</mn> <mn>2</mn></msubsup></mtd></mtr></mtable></mfenced></mrow></math>及其行列式替代。
    <math alttext="rho"><mi>ρ</mi></math>是两个随机变量之间的相关性，它是两个随机变量的标准化版本的协方差。
- en: The same exact formula for the probability density function of the bivariate
    normal distribution generalizes to any dimension, where we have many random variables
    instead of only two random variables. For example, if we have 100 random variables,
    representing 100 features in a data set, the mean vector in the formula will have
    100 entries in it, and the covariance matrix will have size <math alttext="100
    times 100"><mrow><mn>100</mn> <mo>×</mo> <mn>100</mn></mrow></math> , with the
    variance of each random variable on the diagonal and the covariance of each of
    the 4950 pairs of random variables off the diagonal.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 双变量正态分布的概率密度函数的完全相同的公式推广到任意维度，当我们有许多随机变量而不仅仅是两个随机变量时。例如，如果我们有100个随机变量，代表数据集中的100个特征，公式中的均值向量将有100个条目，协方差矩阵的大小将为100×100，对角线上是每个随机变量的方差，而非对角线上是4950对随机变量的协方差。
- en: 'Distribution of Data: Other Important and Commonly Used Distributions'
  id: totrans-182
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据分布：其他重要和常用的分布
- en: Almost everything you did not understand in this chapter will be revisited many
    times throughout the book, and [Chapter 11](ch11.xhtml#ch11) focuses exclusively
    on probability. The concepts will get reinforced as they appear again and again
    in various interesting contexts. Our goal for this chapter is to get exposed to
    the vocabulary of probability and statistics, and have a guiding map for the important
    ideas that frequently appear in AI applications. We also want to acquire a good
    probabilistic intuition for the following chapters without having to take a deep
    dive and delay our progress for no necessary reason.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中你不理解的几乎所有内容都将在整本书中多次重温，第11章专门讨论概率。这些概念将在各种有趣的情境中反复出现，加强我们对它们的理解。本章的目标是熟悉概率和统计的词汇，并为人工智能应用中经常出现的重要思想提供指导。我们还希望在不必要的情况下获得对接下来章节的良好概率直觉，而不必深入研究和拖延我们的进展。
- en: There are many probability distributions out there. Each models a different
    type of real world scenario. The uniform and normal distributions are very common,
    but we have other important distributions that frequently appear in the AI field.
    Recall that our goal is to model the world around us in order to make good designs,
    predictions and/or decisions. Probability distributions help us make predictions
    when our models involve randomness or when we are uncertain about our outcomes.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多概率分布。每个模拟不同类型的真实世界场景。均匀分布和正态分布非常常见，但我们还有其他在人工智能领域经常出现的重要分布。回想一下，我们的目标是模拟我们周围的世界，以便进行良好的设计、预测和/或决策。概率分布帮助我们在我们的模型涉及随机性或我们对结果不确定时进行预测。
- en: When we study distributions, one frustrating part is that most of them have
    weird names that provide zero intuition about what kind of phenomena a given distribution
    would be useful for. This makes us either expend extra mental energy to memorize
    these names, or keep a distribution cheat sheet in our pocket. I prefer to keep
    a cheat sheet. Another frustrating part is that most text book examples involve
    either flipping a coin, rolling a die, or drawing colored balls from urns. This
    leaves us with no real life examples or motivation to understand the subject,
    as I never met anyone walking around flipping coins and counting heads or tails,
    except for Two Face (Harvey Dent) in the Dark Night (a really good 2008 movie,
    where The Joker (Heath Ledger) says some of my favorite and profound statements
    about randomness and chance, like this one, "*The world is cruel. And the only
    morality in a cruel world is chance. Unbiased. Unprejudiced. Fair*.”). I will
    try to amend this as much as I can in this book, pointing to as many real world
    examples as my page limit allows.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们研究分布时，令人沮丧的一点是，大多数分布都有奇怪的名称，对于给定的分布会有什么样的现象提供零直觉。这要么让我们花费额外的精力去记住这些名称，要么在口袋里放一张分布小抄。我更喜欢放一张小抄。另一个令人沮丧的地方是，大多数教科书的例子要么涉及抛硬币，要么掷骰子，要么从瓮中取出彩色球。这让我们没有真实生活的例子或动力去理解这个主题，因为我从未见过有人在那里抛硬币并数头或数尾，除了《黑暗骑士》中的两面人（哈维·登特）。我会尽量在本书中修正这一点，指出尽可能多的真实世界例子，尽管受到页面限制。
- en: 'Some of the following distributions are mathematically related to each other,
    or follow naturally from others. We will explore these relationships in [Chapter 10](ch10.xhtml#ch10).
    For now, let’s name a popular distribution, state whether it is discrete (predicts
    a count of something that we care for) or continuous (predicts a quantity that
    exists in the continuum, such as the time needed to elapse before something happens:
    Careful, this is not the number of hours since the number of hours is discrete,
    but it is the length of the time period), state the parameters that control it,
    and its defining properties that are useful for our AI applications.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 以下一些分布在数学上相互关联，或者自然地从其他分布中产生。我们将在第10章中探讨这些关系。现在，让我们命名一个流行的分布，说明它是离散的（预测我们关心的某种计数）还是连续的（预测存在于连续体中的数量，比如某事发生前需要经过的时间：注意，这不是小时数，因为小时数是离散的，而是时间段的长度），说明控制它的参数，以及对我们的人工智能应用有用的定义属性。
- en: '*The Binomial Distribution*: This is discrete. It represents the probability
    of obtaining a certain number of successes when repeating one experiment, independently,
    multiple times. Its controlling parameters are n, the number of experiments we
    perform, and *p*, the *predefined* probability of success. Real world examples
    include predicting the number of patients that will develop side effects for a
    vaccine or a new medication in a clinial trial, predicting the number of ad clicks
    that will result in a purchase, or the number of customers that will default on
    their monthly credit card payments. When we model examples from the real world
    using a probability distribution that requires independence of trials, it means
    that we *are assuming* independence even if the real world trials are not really
    independent. It is good etiquette to point our models’ assumptions.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 二项分布：这是离散的。它表示在重复进行一个实验时，独立地多次获得一定数量的成功的概率。它的控制参数是n，我们进行的实验次数，以及p，预先定义的成功概率。现实世界的例子包括预测在临床试验中接种疫苗或新药物的患者出现副作用的数量，预测广告点击会导致购买的数量，或者客户月度信用卡付款违约的数量。当我们使用需要试验独立性的概率分布来对现实世界的例子建模时，这意味着我们假设试验是独立的，即使实际上并非如此。指出我们模型的假设是一个良好的礼仪。
- en: '*The Poisson Distribution*: This is discrete. It predicts the number of rare
    events that will occur in a given period of time. These events are independent
    or weakly dependent, meaning that the occurence of the event once does not affect
    the probability of its next occurence in the same time period. They also occur
    at a *known and constant* average rate <math alttext="lamda"><mi>λ</mi></math>
    . Thus, we know the average rate, and we want to predict *how many* of these events
    will happen during a certain time period. The Poisson distribution’s controlling
    parameter is the predefined rare event rate <math alttext="lamda"><mi>λ</mi></math>
    . Real world examples include predicting the number of babies born in a given
    hour, the number of people in a population who age past 98, the number of alpha-particles
    discharged from a radioactive system during a certain time period, the number
    of duplicate bills sent out by the IRS, the number of a not too popular product
    sold on a particular day, the number of typos that one page of this book contains,
    the number of defective items produced by a certain machine on a certain day,
    the number of people entering a store at a certain hour, the number of car crashes
    an insurance company needs to cover within a certain time period, and the number
    of earthquakes happening within a particular time period.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 泊松分布：这是离散的。它预测在一定时间内会发生多少次罕见事件。这些事件是独立的或者弱相关的，意味着事件的发生一次不会影响在同一时间段内下一次发生的概率。它们也以已知和恒定的平均速率<math
    alttext="lamda"><mi>λ</mi></math>发生。因此，我们知道平均速率，并且想要预测在一定时间内会发生多少次这样的事件。泊松分布的控制参数是预定义的罕见事件速率<math
    alttext="lamda"><mi>λ</mi></math>。现实世界的例子包括预测在一个小时内出生的婴儿数量，人口中年龄超过98岁的人数，放射系统在一定时间段内释放的α粒子数量，国税局寄出的重复账单数量，某一天销售的不太受欢迎产品数量，本书一页中的错别字数量，某一天某台机器生产的次品数量，某个小时进入商店的人数，保险公司需要在一定时间内覆盖的车祸数量，以及在特定时间段内发生的地震数量。
- en: '*The Geometric Distribution*: This is discrete. It predicts the number of trials
    needed before we obtain a success, when performing independent trials each with
    a *known* probability p for success. The controlling parameter here is obviously
    the probability p for success. Real-world examples include estimating the number
    of weeks that a company can function without experiencing a network failure, the
    number of hours a machine can function before producing a defective item, or the
    number of people we need to interview before meeting someone who opposes a certain
    bill that we want to pass. Again, for these real world examples, we might be assuming
    independence if modeling using the geometric distribution while in reality the
    trials might not be independent.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 几何分布：这是离散的。它预测在进行独立试验时，每次成功之前需要多少次试验。这里的控制参数显然是成功的概率p。现实世界的例子包括估计一家公司在不经历网络故障的情况下能够运行的周数，一台机器在生产次品之前能够运行的小时数，或者我们需要采访多少人才能遇到反对我们想通过的某项法案的人。同样，对于这些现实世界的例子，如果使用几何分布进行建模，我们可能会假设试验是独立的，而实际上试验可能并不独立。
- en: '*The Exponential Distribution*: This is continuous. If we happen to know that
    a certain event occurs at a constant rate <math alttext="lamda"><mi>λ</mi></math>
    , then exponential distribution predicts the waiting time until this event occurs.
    It is *memoryless*, in the sense that the remaining lifetime of an item which
    belongs to this exponential distribution is also exponential. The controlling
    parameter is the constant rate <math alttext="lamda"><mi>λ</mi></math> . Real
    world examples include the amount of time we have to wait until an earthquake
    occurs, the time until the default on a loan happens, the time until a machine
    part fails, or the time before a terrorist attack strikes. This is very useful
    for the reliability field, where the reliability of a certain machine part is
    calculated, hence statements such as 10-year guarantee, *etc.*'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 指数分布：这是连续的。如果我们知道某个事件以恒定速率<math alttext="lamda"><mi>λ</mi></math>发生，那么指数分布可以预测等待该事件发生的时间。它是“无记忆”的，因为属于指数分布的物品的剩余寿命也是指数分布。控制参数是恒定速率<math
    alttext="lamda"><mi>λ</mi></math>。现实世界的例子包括等待地震发生的时间，贷款违约发生的时间，机器零件损坏的时间，或者恐怖袭击发生之前的时间。这对可靠性领域非常有用，可用于计算某个机器零件的可靠性，因此会有“10年保修”之类的声明。
- en: '*The Weibull Distribution*: This is continuous. It is widely used in engineering
    in the field of predicting products’ lifetimes (10 year warranty statements are
    appropriate here as well). Here, a product consists of many parts, and if any
    of its parts fails, then the product stops working. For example, a car would not
    work if its battery fails, or if a fuse in its gearbox burns out. A Weibull distribution
    provides a good approximation for the lifetime of a car before it stops working,
    after accounting for its many parts and their *weakest link* (assuming we are
    not maintaining the car and resetting the clock). It is controlled by three parameters:
    the shape, scale, and location parameters. The exponential distribution is a special
    case of this distribution, because the exponential distribution has a constant
    rate of event occurance, but the Weibull distribution can model rates of occurence
    that increase or decrease with time.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 威布尔分布：这是连续的。在工程领域广泛用于预测产品的寿命（10年保修声明在这里也是适用的）。在这里，一个产品由许多部分组成，如果任何部分出现故障，那么产品就会停止工作。例如，如果汽车的电池故障，或者变速箱中的保险丝烧断，汽车就无法工作。威布尔分布在考虑了许多部分及其“最薄弱环节”后（假设我们不对汽车进行维护和重置计时器），为汽车在停止工作之前的寿命提供了良好的近似。它由三个参数控制：形状、比例和位置参数。指数分布是这个分布的特例，因为指数分布具有恒定的事件发生率，但威布尔分布可以模拟随时间增加或减少的发生率。
- en: '*The Log-Normal Distribution*: This is continuous. If we take the logarithms
    of each value provided in this distribution, then we get normally distributed
    data. Meaning in the beginning your data might not appear normally distributed,
    but try transforming it using the log function, then you would see normally distributed
    data. This is a good distribution to use when encounter skewed data with low mean
    value, large variance, and *assumes only positive values*. Just like the normal
    distribution appears when you average many independent samples of a random variable
    (using the central limit theorem), the log-normal distribution appears when you
    take the *product* of many positive sample values. Mathematically, this is due
    to an awesome property of log functions: The log of a product is a sum of logs.
    This distribution is controlled by three parameters: shape, scale, and location
    parameters. Real world examples include the volume of gas in a petroleum reserve,
    and the ratio of the price of a security at the end of one day to its price at
    the end of the day before.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对数正态分布：这是连续的。如果我们对该分布中提供的每个值取对数，那么我们得到的是正态分布的数据。这意味着起初你的数据可能看起来不是正态分布的，但尝试使用对数函数进行转换，然后你会看到正态分布的数据。当遇到偏斜数据、低均值、大方差和“仅假设正值”时，这是一个很好的分布。就像当你对一个随机变量的许多独立样本取平均值时会出现正态分布（使用中心极限定理），当你取许多正值样本的“乘积”时，就会出现对数正态分布。从数学上讲，这是由于对数函数的一个很棒的性质：乘积的对数是对数的和。这个分布由三个参数控制：形状、比例和位置参数。现实世界的例子包括石油储量中的气体体积，以及一天结束时证券价格与前一天结束时价格的比率。
- en: '*The Chi-Squared Distribution*: This is continuous. It is a distribution for
    the sum of squares of normally distributed independent random variables. You might
    wonder why would we care about squaring normally distributed random variables,
    then adding them up. The answer is that this is how we usually compute the variance
    of a random variable or of a data sample, and one of our main goals is controlling
    the variance, in order to lower our uncertainties. There are three types of significance
    tests associated with this distribution: The goodness of fit test, which measures
    how far off our expectation is from our observation, and the independence and
    homogeneity of features of data test.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卡方分布：这是连续的。它是一种用于正态分布独立随机变量平方和的分布。你可能会想为什么我们要关心平方正态分布的随机变量，然后把它们加起来。答案是这通常是我们计算随机变量或数据样本的方差的方法，而我们的主要目标之一是控制方差，以降低我们的不确定性。与这个分布相关的有三种显著性检验：拟合优度检验，衡量我们的期望与观察值的偏差，以及数据特征的独立性和均匀性检验。
- en: '*The Pareto Distribution*: This is continuous. It is useful for many real world
    applications, such as, the time to complete a job assigned to a supercomputer
    (think machine learning computations), the household income level in a certain
    population, the number of friends in a social network, and the file size of the
    internet traffic. This distribution is controlled by only one parameter <math
    alttext="alpha"><mi>α</mi></math> , and it is *heavy tailed* (its tail is heavier
    than the exponential distribution).'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 帕累托分布：这是连续的。它对许多现实世界的应用很有用，比如，分配给超级计算机的作业完成时间（考虑机器学习计算）、特定人群的家庭收入水平、社交网络中的朋友数量以及互联网流量的文件大小。这个分布只由一个参数<math
    alttext="alpha"><mi>α</mi></math>控制，它是“重尾”的（其尾部比指数分布更重）。
- en: 'Let’s throw in few other distributions before moving on, without fussing about
    any of the details. These are all more or less related to the aforementioned distributions:
    The *Student t-Distribution* (continuous, similar to the normal distribution,
    but used when the sample size is small and the population variance is unknown),
    the *Beta Distribution* (continuous, produces random values in a given interval),
    the *Cauchy Distribution* (continuous, pathological because neither its mean nor
    its variance are defined, can be obtained using the tangents of randomly chosen
    angles), the *Gamma Distribution* (continuous, has to do with the waiting time
    until n independent events occur, instead of only one event, as in the exponential
    distribution), the *Negative Binomial Distribution* (discrete, has to do with
    the number of independent trials needed in order to obtain a certain number of
    successes), the *Hypergeometric Distribution* (discrete, similar to the binomial
    but the trials are not independent), and the *Negative Hypergeometric Distribution*
    (discrete, captures the number of dependent trials needed before we obtain a certain
    number of successes).'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，让我们再列举一些其他分布，不要纠结于任何细节。这些分布或多或少与前述分布相关：*学生 t-分布*（连续，类似于正态分布，但在样本量较小且总体方差未知时使用），*贝塔分布*（连续，在给定区间内产生随机值），*柯西分布*（连续，病态，因为它的均值和方差都未定义，可以使用随机选择的角的正切来获得），*伽玛分布*（连续，与等待
    n 个独立事件发生的时间有关，而不是像指数分布中只有一个事件），*负二项分布*（离散，与获得一定数量的成功所需的独立试验次数有关），*超几何分布*（离散，类似于二项分布，但试验不是独立的），以及*负超几何分布*（离散，捕捉在获得一定数量的成功之前需要的依赖试验次数）。
- en: The Various Uses Of The Word *Distribution*
  id: totrans-196
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*分布*这个词的各种用法'
- en: You might have already noticed that the word *distribution* refers to many different
    (but related) concepts, depending on the context. This inconsistent use of the
    same word could be a source of confusion and an immediate turn off for some people
    who are trying to enter the field.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到，*分布*这个词在不同的上下文中指的是许多不同（但相关）的概念。同一个词的不一致使用可能会让一些试图进入该领域的人感到困惑并立刻失去兴趣。
- en: 'Let’s list the different concepts that the word distribution refers to, so
    that we easily recognize its intended meaning in a given context:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们列出*分布*这个词所指的不同概念，以便我们能够轻松地在特定上下文中识别其意图：
- en: If you have real data, such as the Height Weight data in this chapter, and plot
    the histogram of one feature of your data set, such as the Height, then you get
    the *emperical distribution* of the height data. You usually do not know the underlying
    probability density function of the height of the entire population, also called
    distribution, since the real data you have is only a sample of that population,
    so you try to estimate it, or model it, using the probability distributions given
    by probability theory. For the height and weight features, when separated by gender,
    a Gaussian distribution is appropriate.
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你有真实数据，比如本章的身高体重数据，并绘制数据集的一个特征（如身高）的直方图，那么你就得到了身高数据的*经验分布*。通常你不知道整个人口身高的潜在概率密度函数，也称为分布，因为你拥有的真实数据只是该人口的一个样本，所以你尝试使用概率论给出的概率分布来估计或建模它。对于身高和体重特征，按性别分开时，高斯分布是合适的。
- en: If you have a discrete random variable, the word distribution could refer to
    either its probability mass function or its cummulative distribution function
    (which specifies the probability that the random variable is less than or equal
    a certain value <math alttext="f left-parenthesis x right-parenthesis equals p
    r o b left-parenthesis upper X less-than-or-equal-to x right-parenthesis"><mrow><mi>f</mi>
    <mo>(</mo> <mi>x</mi> <mo>)</mo> <mo>=</mo> <mi>p</mi> <mi>r</mi> <mi>o</mi> <mi>b</mi>
    <mo>(</mo> <mi>X</mi> <mo>≤</mo> <mi>x</mi> <mo>)</mo></mrow></math> ).
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你有一个离散随机变量，*分布*这个词可能指的是它的概率质量函数或累积分布函数（指定了随机变量小于或等于某个值的概率<math alttext="f
    left-parenthesis x right-parenthesis equals p r o b left-parenthesis upper X less-than-or-equal-to
    x right-parenthesis"><mrow><mi>f</mi> <mo>(</mo> <mi>x</mi> <mo>)</mo> <mo>=</mo>
    <mi>p</mi> <mi>r</mi> <mi>o</mi> <mi>b</mi> <mo>(</mo> <mi>X</mi> <mo>≤</mo> <mi>x</mi>
    <mo>)</mo></mrow></math>）。
- en: If you have a continuous random variable, the word distribution could refer
    to either its probability density function or its cummulative distribution function,
    whose integral gives the probability that the random variable is less than or
    equal a certain value.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你有一个连续随机变量，*分布*这个词可能指的是它的概率密度函数或累积分布函数，其积分给出了随机变量小于或等于某个值的概率。
- en: If you have multiple random variables (discrete, continuous, or a mix of both),
    then the word distribution refers to their joint probability distribution.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你有多个随机变量（离散、连续或混合），那么*分布*这个词指的是它们的联合概率分布。
- en: A common goal is to establish an appropriate correpondence between an idealized
    mathematical function, such as a random variable with an appropriate distribution,
    and real observed data or phenomena, with an observed emperical distribution.
    When working with real data, each feature of the data set can be modeled using
    a random variable. So in a way a mathematical random variable with its corresponding
    distribution is an idealized version of our measured or observed feature.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 一个常见的目标是建立一个理想化的数学函数（如具有适当分布的随机变量）与真实观察到的数据或现象之间的适当对应关系，观察到的经验分布。在处理真实数据时，数据集的每个特征都可以用随机变量来建模。因此，数学随机变量及其相应的分布在某种程度上是我们测量或观察到的特征的理想化版本。
- en: Finally, distributions appear everywhere in AI applications. We will encounter
    them plenty of times in the next chapters, for example, the distribution of the
    weights at each layer of a neural network, and the distribution of the noise and
    the errors committed by various machine learning models.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，分布在人工智能应用中随处可见。在接下来的章节中，我们将多次遇到它们，例如神经网络每一层的权重分布，以及各种机器学习模型产生的噪音和误差的分布。
- en: A/B Testing
  id: totrans-205
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AB测试。
- en: Before leaving this chapter, we make a tiny detour into the world of *A/B testing*,
    also called *split testing*, or *randomized single-blind* or *double-blind trials*.
    We make this detour because this is a topic important for data scientists, and
    the idea of it is simple enough to fit into a small section.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在离开本章之前，我们将稍微偏离一下*AB测试*的世界，也称为*分割测试*，或*随机单盲*或*双盲试验*。我们之所以偏离一下，是因为这是数据科学家们重要的话题，而且这个想法足够简单，可以放入一个小节中。
- en: Countless companies rely on data from A/B tests in order to increase engagement,
    revenue, and customer satisfaction. Microsoft, Amazon, LinkedIn, Google, and others
    each conduct thousands A/B tests annually.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 无数公司依靠AB测试的数据来增加参与度、收入和客户满意度。微软、亚马逊、LinkedIn、谷歌等每年都进行数千次AB测试。
- en: 'An A/B test is simple: Split the population into two groups. Roll out a version
    of something you want to test (a new webpage design, a different font size, a
    new medicine, a new political ad) to one group, the test group, and keep the other
    group as a control group. Compare the data between the two groups.'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: AB测试很简单：将人群分成两组。将你想要测试的东西的一个版本（新的网页设计、不同的字体大小、新药、新的政治广告）推出给一个组，即测试组，并将另一组作为对照组。比较两组之间的数据。
- en: The test is *single blind* if the subjects do not know which group they belong
    to (some do not even know that they are in a test at all), but the experimenters
    know. The test is *double blind* if neither the experimenters nor the subjects
    know which group they are interacting with.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 如果受试者不知道自己属于哪个组（有些人甚至根本不知道自己在测试中），而实验者知道，那么这个测试就是*单盲*的。如果实验者和受试者都不知道自己与哪个组互动，那么这个测试就是*双盲*的。
- en: Summary And Looking Ahead
  id: totrans-210
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结和展望
- en: 'In this chapter, we emphasized the fact that data is central to AI. We also
    clarified the differences between concepts that are usually a source of confusion:
    structured and unstructured data, linear and nonlinear models, real and simulated
    data, deterministic functions and random variables, discrete and continuous distributions,
    posterior probabilities and likelihood functions. We also provided a map for the
    probability and statistics needed for AI without diving into any of the details,
    and we introduced the most popular probability distributions.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们强调了数据对于人工智能的重要性。我们还澄清了通常会引起混淆的概念之间的区别：结构化和非结构化数据，线性和非线性模型，真实和模拟数据，确定性函数和随机变量，离散和连续分布，后验概率和似然函数。我们还提供了人工智能所需的概率和统计的地图，而没有深入任何细节，并介绍了最流行的概率分布。
- en: If you find yourself lost in some new probability concept, you might want to
    consult the map provided in this chapter and see how that concept fits within
    the big picture of probability theory, and most importantly, how it relates to
    AI. Without knowing how a particular mathematical concept relates to AI, you are
    left with having some tool that you know how to turn on, but you have no idea
    what it is used for.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在某个新的概率概念中感到迷茫，你可能想要查阅本章提供的地图，看看这个概念如何融入概率论的大局，最重要的是，它如何与人工智能相关。如果不知道某个数学概念与人工智能的关系，你就只是拥有一个你知道如何打开的工具，但你不知道它的用途。
- en: We have not yet mentioned *random matrices* and *high dimensional probability*.
    In these fields, probability theory, with its constant tracking of distributions,
    expectations and variances of any relevant random quantities, merges with linear
    algebra, with its hyperfocus on eigenvalues and various matrix decompositions.
    These fields are very important for the extremely high dimensional data that is
    involved in AI applications. We discuss them in [Chapter 11](ch11.xhtml#ch11)
    on probability.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还没有提到*随机矩阵*和*高维概率*。在这些领域中，概率论通过不断跟踪分布、期望和任何相关随机量的方差，与线性代数相结合，后者则专注于特征值和各种矩阵分解。这些领域对于人工智能应用中涉及的极高维数据非常重要。我们将在[第11章](ch11.xhtml#ch11)中讨论它们。
- en: In the next chapter, we learn how to fit our data into a function, then use
    this function to make predictions and/or decisions. Mathematically, we find the
    weights (the <math alttext="omega"><mi>ω</mi></math> ’s) that characterize the
    strengths of various interactions between the features of our data. When we characterize
    the involved types of interactions (the formula of the fitting function, called
    the *learning* or *training* function) and the strengths of these interactions
    (the values of the <math alttext="omega"><mi>ω</mi></math> ’s), we can make our
    predictions. In AI, this one concept of *characterizing the fitting function with
    its suitable weight values* can be used successfully for computer vision, natural
    language processing, predictive analytics (like house prices, time until maintainance,
    *etc.*), and many other applications.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将学习如何将数据拟合到一个函数中，然后使用这个函数进行预测和/或决策。从数学上讲，我们找到了表征数据特征之间各种相互作用强度的权重（<math
    alttext="omega"><mi>ω</mi></math>）。当我们表征所涉及的相互作用类型（拟合函数的公式，称为*学习*或*训练*函数）和这些相互作用的强度（<math
    alttext="omega"><mi>ω</mi></math>的值）时，我们就可以进行预测。在人工智能中，这个*表征拟合函数及其适当的权重值*的概念可以成功应用于计算机视觉、自然语言处理、预测分析（如房价、维护时间等）和许多其他应用。
