<html><head></head><body>
<div id="sbo-rt-content"><section data-pdf-bookmark="Chapter 46. In Depth: Manifold Learning" data-type="chapter" epub:type="chapter"><div class="chapter" id="section-0510-manifold-learning">
<h1><span class="label">Chapter 46. </span>In Depth: Manifold Learning</h1>
<p><a data-primary="machine learning" data-secondary="manifold learning" data-type="indexterm" id="ix_ch46-asciidoc0"/><a data-primary="manifold learning" data-type="indexterm" id="ix_ch46-asciidoc1"/>In the previous chapter we saw how PCA can be used for dimensionality
reduction, reducing the number of features of a dataset while
maintaining the essential relationships between the points. While PCA is
flexible, fast, and easily interpretable, it does not perform so well
when there are <em>nonlinear</em> relationships within the data, some examples
of which we will see shortly.</p>
<p><a data-primary="manifold learning" data-secondary="defined" data-type="indexterm" id="idm45858727010480"/>To address this deficiency, we can turn to <em>manifold learning
algorithms</em>—a class of unsupervised estimators that seek to describe
datasets as low-dimensional manifolds embedded in high-dimensional
spaces. When you think of a manifold, I’d suggest imagining
a sheet of paper: this is a two-dimensional object that lives in our
familiar three-dimensional world.</p>
<p>In the parlance of manifold learning, you can think of this sheet as a
two-dimensional manifold embedded in three-dimensional space. Rotating,
reorienting, or stretching the piece of paper in three-dimensional space
doesn’t change its flat geometry: such operations are akin
to linear embeddings. If you bend, curl, or crumple the paper, it is
still a two-dimensional manifold, but the embedding into the
three-dimensional space is no longer linear. Manifold learning
algorithms seek to learn about the fundamental two-dimensional nature of
the paper, even as it is contorted to fill the three-dimensional space.</p>
<p>Here we will examine a number of manifold methods, going most deeply
into a subset of these techniques: multidimensional scaling (MDS),
locally linear embedding (LLE), and isometric mapping (Isomap).</p>
<p class="pagebreak-before less_space">We begin with the standard imports:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">1</code><code class="p">]:</code> <code class="o">%</code><code class="k">matplotlib</code> inline
        <code class="kn">import</code> <code class="nn">matplotlib.pyplot</code> <code class="k">as</code> <code class="nn">plt</code>
        <code class="n">plt</code><code class="o">.</code><code class="n">style</code><code class="o">.</code><code class="n">use</code><code class="p">(</code><code class="s1">'seaborn-whitegrid'</code><code class="p">)</code>
        <code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code></pre>
<section data-pdf-bookmark="Manifold Learning: “HELLO”" data-type="sect1"><div class="sect1" id="ch_0510-manifold-learning_manifold-learning-hello">
<h1>Manifold Learning: “HELLO”</h1>
<p><a data-primary="manifold learning" data-secondary="HELLO function" data-type="indexterm" id="idm45858726950256"/>To make these concepts more clear, let’s start by generating
some two-dimensional data that we can use to define a manifold. Here is
a function that will create data in the shape of the word “HELLO”:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">2</code><code class="p">]:</code> <code class="k">def</code> <code class="nf">make_hello</code><code class="p">(</code><code class="n">N</code><code class="o">=</code><code class="mi">1000</code><code class="p">,</code> <code class="n">rseed</code><code class="o">=</code><code class="mi">42</code><code class="p">):</code>
            <code class="c1"># Make a plot with "HELLO" text; save as PNG</code>
            <code class="n">fig</code><code class="p">,</code> <code class="n">ax</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">(</code><code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">4</code><code class="p">,</code> <code class="mi">1</code><code class="p">))</code>
            <code class="n">fig</code><code class="o">.</code><code class="n">subplots_adjust</code><code class="p">(</code><code class="n">left</code><code class="o">=</code><code class="mi">0</code><code class="p">,</code> <code class="n">right</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code> <code class="n">bottom</code><code class="o">=</code><code class="mi">0</code><code class="p">,</code> <code class="n">top</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>
            <code class="n">ax</code><code class="o">.</code><code class="n">axis</code><code class="p">(</code><code class="s1">'off'</code><code class="p">)</code>
            <code class="n">ax</code><code class="o">.</code><code class="n">text</code><code class="p">(</code><code class="mf">0.5</code><code class="p">,</code> <code class="mf">0.4</code><code class="p">,</code> <code class="s1">'HELLO'</code><code class="p">,</code> <code class="n">va</code><code class="o">=</code><code class="s1">'center'</code><code class="p">,</code> <code class="n">ha</code><code class="o">=</code><code class="s1">'center'</code><code class="p">,</code>
                    <code class="n">weight</code><code class="o">=</code><code class="s1">'bold'</code><code class="p">,</code> <code class="n">size</code><code class="o">=</code><code class="mi">85</code><code class="p">)</code>
            <code class="n">fig</code><code class="o">.</code><code class="n">savefig</code><code class="p">(</code><code class="s1">'hello.png'</code><code class="p">)</code>
            <code class="n">plt</code><code class="o">.</code><code class="n">close</code><code class="p">(</code><code class="n">fig</code><code class="p">)</code>

            <code class="c1"># Open this PNG and draw random points from it</code>
            <code class="kn">from</code> <code class="nn">matplotlib.image</code> <code class="kn">import</code> <code class="n">imread</code>
            <code class="n">data</code> <code class="o">=</code> <code class="n">imread</code><code class="p">(</code><code class="s1">'hello.png'</code><code class="p">)[::</code><code class="o">-</code><code class="mi">1</code><code class="p">,</code> <code class="p">:,</code> <code class="mi">0</code><code class="p">]</code><code class="o">.</code><code class="n">T</code>
            <code class="n">rng</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">RandomState</code><code class="p">(</code><code class="n">rseed</code><code class="p">)</code>
            <code class="n">X</code> <code class="o">=</code> <code class="n">rng</code><code class="o">.</code><code class="n">rand</code><code class="p">(</code><code class="mi">4</code> <code class="o">*</code> <code class="n">N</code><code class="p">,</code> <code class="mi">2</code><code class="p">)</code>
            <code class="n">i</code><code class="p">,</code> <code class="n">j</code> <code class="o">=</code> <code class="p">(</code><code class="n">X</code> <code class="o">*</code> <code class="n">data</code><code class="o">.</code><code class="n">shape</code><code class="p">)</code><code class="o">.</code><code class="n">astype</code><code class="p">(</code><code class="nb">int</code><code class="p">)</code><code class="o">.</code><code class="n">T</code>
            <code class="n">mask</code> <code class="o">=</code> <code class="p">(</code><code class="n">data</code><code class="p">[</code><code class="n">i</code><code class="p">,</code> <code class="n">j</code><code class="p">]</code> <code class="o">&lt;</code> <code class="mi">1</code><code class="p">)</code>
            <code class="n">X</code> <code class="o">=</code> <code class="n">X</code><code class="p">[</code><code class="n">mask</code><code class="p">]</code>
            <code class="n">X</code><code class="p">[:,</code> <code class="mi">0</code><code class="p">]</code> <code class="o">*=</code> <code class="p">(</code><code class="n">data</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code> <code class="o">/</code> <code class="n">data</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">1</code><code class="p">])</code>
            <code class="n">X</code> <code class="o">=</code> <code class="n">X</code><code class="p">[:</code><code class="n">N</code><code class="p">]</code>
            <code class="k">return</code> <code class="n">X</code><code class="p">[</code><code class="n">np</code><code class="o">.</code><code class="n">argsort</code><code class="p">(</code><code class="n">X</code><code class="p">[:,</code> <code class="mi">0</code><code class="p">])]</code></pre>
<p>Let’s call the function and visualize the resulting data
(<a data-type="xref" href="#fig_0510-manifold-learning_files_in_output_6_0">Figure 46-1</a>).</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">3</code><code class="p">]:</code> <code class="n">X</code> <code class="o">=</code> <code class="n">make_hello</code><code class="p">(</code><code class="mi">1000</code><code class="p">)</code>
        <code class="n">colorize</code> <code class="o">=</code> <code class="nb">dict</code><code class="p">(</code><code class="n">c</code><code class="o">=</code><code class="n">X</code><code class="p">[:,</code> <code class="mi">0</code><code class="p">],</code> <code class="n">cmap</code><code class="o">=</code><code class="n">plt</code><code class="o">.</code><code class="n">cm</code><code class="o">.</code><code class="n">get_cmap</code><code class="p">(</code><code class="s1">'rainbow'</code><code class="p">,</code> <code class="mi">5</code><code class="p">))</code>
        <code class="n">plt</code><code class="o">.</code><code class="n">scatter</code><code class="p">(</code><code class="n">X</code><code class="p">[:,</code> <code class="mi">0</code><code class="p">],</code> <code class="n">X</code><code class="p">[:,</code> <code class="mi">1</code><code class="p">],</code> <code class="o">**</code><code class="n">colorize</code><code class="p">)</code>
        <code class="n">plt</code><code class="o">.</code><code class="n">axis</code><code class="p">(</code><code class="s1">'equal'</code><code class="p">);</code></pre>
<p>The output is two dimensional, and consists of points drawn in the shape
of the word “HELLO”. This data form will help us to see visually what
these algorithms are doing.</p>
<figure><div class="figure" id="fig_0510-manifold-learning_files_in_output_6_0">
<img alt="output 6 0" height="383" src="assets/output_6_0.png" width="600"/>
<h6><span class="label">Figure 46-1. </span>Data for use with manifold learning</h6>
</div></figure>
</div></section>
<section data-pdf-bookmark="Multidimensional Scaling" data-type="sect1"><div class="sect1" id="ch_0510-manifold-learning_multidimensional-scaling">
<h1>Multidimensional Scaling</h1>
<p><a data-primary="manifold learning" data-secondary="multidimensional scaling" data-type="indexterm" id="ix_ch46-asciidoc2"/><a data-primary="multidimensional scaling (MDS)" data-type="indexterm" id="ix_ch46-asciidoc3"/>Looking <a data-primary="multidimensional scaling (MDS)" data-secondary="basics" data-type="indexterm" id="ix_ch46-asciidoc4"/>at data like this, we can see that the particular choices of <em>x</em>
and <em>y</em> values of the dataset are not the most fundamental description
of the data: we can scale, shrink, or rotate the data, and the “HELLO”
will still be apparent. For example, if we use a rotation matrix to
rotate the data, the <em>x</em> and <em>y</em> values change, but the data is still
fundamentally the same (see <a data-type="xref" href="#fig_0510-manifold-learning_files_in_output_9_0">Figure 46-2</a>).</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">4</code><code class="p">]:</code> <code class="k">def</code> <code class="nf">rotate</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">angle</code><code class="p">):</code>
            <code class="n">theta</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">deg2rad</code><code class="p">(</code><code class="n">angle</code><code class="p">)</code>
            <code class="n">R</code> <code class="o">=</code> <code class="p">[[</code><code class="n">np</code><code class="o">.</code><code class="n">cos</code><code class="p">(</code><code class="n">theta</code><code class="p">),</code> <code class="n">np</code><code class="o">.</code><code class="n">sin</code><code class="p">(</code><code class="n">theta</code><code class="p">)],</code>
                 <code class="p">[</code><code class="o">-</code><code class="n">np</code><code class="o">.</code><code class="n">sin</code><code class="p">(</code><code class="n">theta</code><code class="p">),</code> <code class="n">np</code><code class="o">.</code><code class="n">cos</code><code class="p">(</code><code class="n">theta</code><code class="p">)]]</code>
            <code class="k">return</code> <code class="n">np</code><code class="o">.</code><code class="n">dot</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">R</code><code class="p">)</code>

        <code class="n">X2</code> <code class="o">=</code> <code class="n">rotate</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="mi">20</code><code class="p">)</code> <code class="o">+</code> <code class="mi">5</code>
        <code class="n">plt</code><code class="o">.</code><code class="n">scatter</code><code class="p">(</code><code class="n">X2</code><code class="p">[:,</code> <code class="mi">0</code><code class="p">],</code> <code class="n">X2</code><code class="p">[:,</code> <code class="mi">1</code><code class="p">],</code> <code class="o">**</code><code class="n">colorize</code><code class="p">)</code>
        <code class="n">plt</code><code class="o">.</code><code class="n">axis</code><code class="p">(</code><code class="s1">'equal'</code><code class="p">);</code></pre>
<figure><div class="figure" id="fig_0510-manifold-learning_files_in_output_9_0">
<img alt="output 9 0" height="398" src="assets/output_9_0.png" width="600"/>
<h6><span class="label">Figure 46-2. </span>Rotated dataset</h6>
</div></figure>
<p>This confirms that the <em>x</em> and <em>y</em> values are not necessarily
fundamental to the relationships in the data. What <em>is</em> fundamental, in
this case, is the <em>distance</em> between each point within the dataset. A
common way to represent this is to use a distance matrix: for
<math alttext="upper N">
<mi>N</mi>
</math> points, we construct an <math alttext="upper N times upper N">
<mrow>
<mi>N</mi>
<mo>×</mo>
<mi>N</mi>
</mrow>
</math> array
such that entry <math alttext="left-parenthesis i comma j right-parenthesis">
<mrow>
<mo>(</mo>
<mi>i</mi>
<mo>,</mo>
<mi>j</mi>
<mo>)</mo>
</mrow>
</math> contains the distance between point
<math alttext="i">
<mi>i</mi>
</math> and point <math alttext="j">
<mi>j</mi>
</math>. Let’s use
Scikit-Learn’s efficient <code>pairwise_distances</code> function to do
this for our original data:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">5</code><code class="p">]:</code> <code class="kn">from</code> <code class="nn">sklearn.metrics</code> <code class="kn">import</code> <code class="n">pairwise_distances</code>
        <code class="n">D</code> <code class="o">=</code> <code class="n">pairwise_distances</code><code class="p">(</code><code class="n">X</code><code class="p">)</code>
        <code class="n">D</code><code class="o">.</code><code class="n">shape</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">5</code><code class="p">]:</code> <code class="p">(</code><code class="mi">1000</code><code class="p">,</code> <code class="mi">1000</code><code class="p">)</code></pre>
<p>As promised, for our <em>N</em>=1,000 points, we obtain a 1,000 × 1,000 matrix,
which can be visualized as shown here (see <a data-type="xref" href="#fig_0510-manifold-learning_files_in_output_13_0">Figure 46-3</a>).</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">6</code><code class="p">]:</code> <code class="n">plt</code><code class="o">.</code><code class="n">imshow</code><code class="p">(</code><code class="n">D</code><code class="p">,</code> <code class="n">zorder</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code> <code class="n">cmap</code><code class="o">=</code><code class="s1">'viridis'</code><code class="p">,</code> <code class="n">interpolation</code><code class="o">=</code><code class="s1">'nearest'</code><code class="p">)</code>
        <code class="n">plt</code><code class="o">.</code><code class="n">colorbar</code><code class="p">();</code></pre>
<figure class="width-75"><div class="figure" id="fig_0510-manifold-learning_files_in_output_13_0">
<img alt="output 13 0" height="392" src="assets/output_13_0.png" width="600"/>
<h6><span class="label">Figure 46-3. </span>Visualization of the pairwise distances between points</h6>
</div></figure>
<p>If we similarly construct a distance matrix for our rotated and
translated data, we see that it is the same:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">7</code><code class="p">]:</code> <code class="n">D2</code> <code class="o">=</code> <code class="n">pairwise_distances</code><code class="p">(</code><code class="n">X2</code><code class="p">)</code>
        <code class="n">np</code><code class="o">.</code><code class="n">allclose</code><code class="p">(</code><code class="n">D</code><code class="p">,</code> <code class="n">D2</code><code class="p">)</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">7</code><code class="p">]:</code> <code class="kc">True</code></pre>
<p>This distance matrix gives us a representation of our data that is
invariant to rotations and translations, but the visualization of the
matrix in <a data-type="xref" href="#fig_0510-manifold-learning_files_in_output_13_0">Figure 46-3</a> is not entirely intuitive. In the
representation shown there, we have lost any visible sign of the
interesting structure in the data: the “HELLO” that we saw before.</p>
<p>Further, while computing this distance matrix from the (<em>x</em>, <em>y</em>)
coordinates is straightforward, transforming the distances back into <em>x</em>
and <em>y</em> coordinates is rather difficult. This is exactly what the
multidimensional scaling algorithm aims to do: given a distance matrix
between points, it recovers a <math alttext="upper D">
<mi>D</mi>
</math>-dimensional coordinate
representation of the data. Let’s see how it works for our
distance matrix, using the <code>precomputed</code> dissimilarity to specify that
we are passing a distance matrix (<a data-type="xref" href="#fig_0510-manifold-learning_files_in_output_17_0">Figure 46-4</a>).</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">8</code><code class="p">]:</code> <code class="kn">from</code> <code class="nn">sklearn.manifold</code> <code class="kn">import</code> <code class="n">MDS</code>
        <code class="n">model</code> <code class="o">=</code> <code class="n">MDS</code><code class="p">(</code><code class="n">n_components</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code> <code class="n">dissimilarity</code><code class="o">=</code><code class="s1">'precomputed'</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">1701</code><code class="p">)</code>
        <code class="n">out</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">D</code><code class="p">)</code>
        <code class="n">plt</code><code class="o">.</code><code class="n">scatter</code><code class="p">(</code><code class="n">out</code><code class="p">[:,</code> <code class="mi">0</code><code class="p">],</code> <code class="n">out</code><code class="p">[:,</code> <code class="mi">1</code><code class="p">],</code> <code class="o">**</code><code class="n">colorize</code><code class="p">)</code>
        <code class="n">plt</code><code class="o">.</code><code class="n">axis</code><code class="p">(</code><code class="s1">'equal'</code><code class="p">);</code></pre>
<figure><div class="figure" id="fig_0510-manifold-learning_files_in_output_17_0">
<img alt="output 17 0" height="401" src="assets/output_17_0.png" width="600"/>
<h6><span class="label">Figure 46-4. </span>An MDS embedding computed from the pairwise distances</h6>
</div></figure>
<p>The MDS algorithm recovers one of the possible two-dimensional
coordinate representations of our data, using <em>only</em> the
<math alttext="upper N times upper N">
<mrow>
<mi>N</mi>
<mo>×</mo>
<mi>N</mi>
</mrow>
</math> distance matrix describing the relationship
between the data points.<a data-startref="ix_ch46-asciidoc4" data-type="indexterm" id="idm45858726194048"/></p>
<section data-pdf-bookmark="MDS as Manifold Learning" data-type="sect2"><div class="sect2" id="ch_0510-manifold-learning_mds-as-manifold-learning">
<h2>MDS as Manifold Learning</h2>
<p><a data-primary="multidimensional scaling (MDS)" data-secondary="as manifold learning" data-secondary-sortas="manifold learning" data-type="indexterm" id="idm45858726191440"/>The usefulness of this becomes more apparent when we consider the fact
that distance matrices can be computed from data in <em>any</em> dimension. So,
for example, instead of simply rotating the data in the two-dimensional
plane, we can project it into three dimensions using the following
function (essentially a three-dimensional generalization of the rotation
matrix used earlier):</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">9</code><code class="p">]:</code> <code class="k">def</code> <code class="nf">random_projection</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">dimension</code><code class="o">=</code><code class="mi">3</code><code class="p">,</code> <code class="n">rseed</code><code class="o">=</code><code class="mi">42</code><code class="p">):</code>
            <code class="k">assert</code> <code class="n">dimension</code> <code class="o">&gt;=</code> <code class="n">X</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">1</code><code class="p">]</code>
            <code class="n">rng</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">RandomState</code><code class="p">(</code><code class="n">rseed</code><code class="p">)</code>
            <code class="n">C</code> <code class="o">=</code> <code class="n">rng</code><code class="o">.</code><code class="n">randn</code><code class="p">(</code><code class="n">dimension</code><code class="p">,</code> <code class="n">dimension</code><code class="p">)</code>
            <code class="n">e</code><code class="p">,</code> <code class="n">V</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">linalg</code><code class="o">.</code><code class="n">eigh</code><code class="p">(</code><code class="n">np</code><code class="o">.</code><code class="n">dot</code><code class="p">(</code><code class="n">C</code><code class="p">,</code> <code class="n">C</code><code class="o">.</code><code class="n">T</code><code class="p">))</code>
            <code class="k">return</code> <code class="n">np</code><code class="o">.</code><code class="n">dot</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">V</code><code class="p">[:</code><code class="n">X</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">1</code><code class="p">]])</code>

        <code class="n">X3</code> <code class="o">=</code> <code class="n">random_projection</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="mi">3</code><code class="p">)</code>
        <code class="n">X3</code><code class="o">.</code><code class="n">shape</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">9</code><code class="p">]:</code> <code class="p">(</code><code class="mi">1000</code><code class="p">,</code> <code class="mi">3</code><code class="p">)</code></pre>
<p>Let’s visualize these points to see what we’re
working with (<a data-type="xref" href="#fig_0510-manifold-learning_files_in_output_22_0">Figure 46-5</a>).</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">10</code><code class="p">]:</code> <code class="kn">from</code> <code class="nn">mpl_toolkits</code> <code class="kn">import</code> <code class="n">mplot3d</code>
         <code class="n">ax</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">axes</code><code class="p">(</code><code class="n">projection</code><code class="o">=</code><code class="s1">'3d'</code><code class="p">)</code>
         <code class="n">ax</code><code class="o">.</code><code class="n">scatter3D</code><code class="p">(</code><code class="n">X3</code><code class="p">[:,</code> <code class="mi">0</code><code class="p">],</code> <code class="n">X3</code><code class="p">[:,</code> <code class="mi">1</code><code class="p">],</code> <code class="n">X3</code><code class="p">[:,</code> <code class="mi">2</code><code class="p">],</code>
                      <code class="o">**</code><code class="n">colorize</code><code class="p">);</code></pre>
<figure class="width-75"><div class="figure" id="fig_0510-manifold-learning_files_in_output_22_0">
<img alt="output 22 0" height="398" src="assets/output_22_0.png" width="600"/>
<h6><span class="label">Figure 46-5. </span>Data embedded linearly into three dimensions</h6>
</div></figure>
<p>We can now ask the <code>MDS</code> estimator to input this three-dimensional data,
compute the distance matrix, and then determine the optimal
two-dimensional embedding for this distance matrix. The result recovers
a representation of the original data, as shown in <a data-type="xref" href="#fig_0510-manifold-learning_files_in_output_24_0">Figure 46-6</a>.</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">11</code><code class="p">]:</code> <code class="n">model</code> <code class="o">=</code> <code class="n">MDS</code><code class="p">(</code><code class="n">n_components</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">1701</code><code class="p">)</code>
         <code class="n">out3</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">X3</code><code class="p">)</code>
         <code class="n">plt</code><code class="o">.</code><code class="n">scatter</code><code class="p">(</code><code class="n">out3</code><code class="p">[:,</code> <code class="mi">0</code><code class="p">],</code> <code class="n">out3</code><code class="p">[:,</code> <code class="mi">1</code><code class="p">],</code> <code class="o">**</code><code class="n">colorize</code><code class="p">)</code>
         <code class="n">plt</code><code class="o">.</code><code class="n">axis</code><code class="p">(</code><code class="s1">'equal'</code><code class="p">);</code></pre>
<p>This is essentially the goal of a manifold learning estimator: given
high-dimensional embedded data, it seeks a low-dimensional
representation of the data that preserves certain relationships within
the data. In the case of MDS, the quantity preserved is the distance
between every pair of points.</p>
<figure><div class="figure" id="fig_0510-manifold-learning_files_in_output_24_0">
<img alt="output 24 0" height="383" src="assets/output_24_0.png" width="600"/>
<h6><span class="label">Figure 46-6. </span>The MDS embedding of the three-dimensional data recovers the input up to a rotation and reflection</h6>
</div></figure>
</div></section>
<section data-pdf-bookmark="Nonlinear Embeddings: Where MDS Fails" data-type="sect2"><div class="sect2" id="ch_0510-manifold-learning_nonlinear-embeddings-where-mds-fails">
<h2>Nonlinear Embeddings: Where MDS Fails</h2>
<p><a data-primary="multidimensional scaling (MDS)" data-secondary="nonlinear embeddings" data-type="indexterm" id="ix_ch46-asciidoc5"/><a data-primary="nonlinear embeddings, MDS and" data-type="indexterm" id="ix_ch46-asciidoc6"/>Our discussion thus far has considered <em>linear</em> embeddings, which
essentially consist of rotations, translations, and scalings of data
into higher-dimensional spaces. Where MDS breaks down is when the
embedding is nonlinear—that is, when it goes beyond this simple set of
operations. Consider the following embedding, which takes the input and
contorts it into an “S” shape in three dimensions:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">12</code><code class="p">]:</code> <code class="k">def</code> <code class="nf">make_hello_s_curve</code><code class="p">(</code><code class="n">X</code><code class="p">):</code>
             <code class="n">t</code> <code class="o">=</code> <code class="p">(</code><code class="n">X</code><code class="p">[:,</code> <code class="mi">0</code><code class="p">]</code> <code class="o">-</code> <code class="mi">2</code><code class="p">)</code> <code class="o">*</code> <code class="mf">0.75</code> <code class="o">*</code> <code class="n">np</code><code class="o">.</code><code class="n">pi</code>
             <code class="n">x</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">sin</code><code class="p">(</code><code class="n">t</code><code class="p">)</code>
             <code class="n">y</code> <code class="o">=</code> <code class="n">X</code><code class="p">[:,</code> <code class="mi">1</code><code class="p">]</code>
             <code class="n">z</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">sign</code><code class="p">(</code><code class="n">t</code><code class="p">)</code> <code class="o">*</code> <code class="p">(</code><code class="n">np</code><code class="o">.</code><code class="n">cos</code><code class="p">(</code><code class="n">t</code><code class="p">)</code> <code class="o">-</code> <code class="mi">1</code><code class="p">)</code>
             <code class="k">return</code> <code class="n">np</code><code class="o">.</code><code class="n">vstack</code><code class="p">((</code><code class="n">x</code><code class="p">,</code> <code class="n">y</code><code class="p">,</code> <code class="n">z</code><code class="p">))</code><code class="o">.</code><code class="n">T</code>

         <code class="n">XS</code> <code class="o">=</code> <code class="n">make_hello_s_curve</code><code class="p">(</code><code class="n">X</code><code class="p">)</code></pre>
<p>This is again three-dimensional data, but as we can see in <a data-type="xref" href="#fig_0510-manifold-learning_files_in_output_29_0">Figure 46-7</a> the embedding is much more complicated.</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">13</code><code class="p">]:</code> <code class="kn">from</code> <code class="nn">mpl_toolkits</code> <code class="kn">import</code> <code class="n">mplot3d</code>
         <code class="n">ax</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">axes</code><code class="p">(</code><code class="n">projection</code><code class="o">=</code><code class="s1">'3d'</code><code class="p">)</code>
         <code class="n">ax</code><code class="o">.</code><code class="n">scatter3D</code><code class="p">(</code><code class="n">XS</code><code class="p">[:,</code> <code class="mi">0</code><code class="p">],</code> <code class="n">XS</code><code class="p">[:,</code> <code class="mi">1</code><code class="p">],</code> <code class="n">XS</code><code class="p">[:,</code> <code class="mi">2</code><code class="p">],</code>
                      <code class="o">**</code><code class="n">colorize</code><code class="p">);</code></pre>
<figure><div class="figure" id="fig_0510-manifold-learning_files_in_output_29_0">
<img alt="output 29 0" height="565" src="assets/output_29_0.png" width="600"/>
<h6><span class="label">Figure 46-7. </span>Data embedded nonlinearly into three dimensions</h6>
</div></figure>
<p>The fundamental relationships between the data points are still there,
but this time the data has been transformed in a nonlinear way: it has
been wrapped up into the shape of an “S.”</p>
<p>If we try a simple MDS algorithm on this data, it is not able to
“unwrap” this nonlinear embedding, and we lose track of the
fundamental relationships in the embedded manifold (see <a data-type="xref" href="#fig_0510-manifold-learning_files_in_output_31_0">Figure 46-8</a>).</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">14</code><code class="p">]:</code> <code class="kn">from</code> <code class="nn">sklearn.manifold</code> <code class="kn">import</code> <code class="n">MDS</code>
         <code class="n">model</code> <code class="o">=</code> <code class="n">MDS</code><code class="p">(</code><code class="n">n_components</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">2</code><code class="p">)</code>
         <code class="n">outS</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">XS</code><code class="p">)</code>
         <code class="n">plt</code><code class="o">.</code><code class="n">scatter</code><code class="p">(</code><code class="n">outS</code><code class="p">[:,</code> <code class="mi">0</code><code class="p">],</code> <code class="n">outS</code><code class="p">[:,</code> <code class="mi">1</code><code class="p">],</code> <code class="o">**</code><code class="n">colorize</code><code class="p">)</code>
         <code class="n">plt</code><code class="o">.</code><code class="n">axis</code><code class="p">(</code><code class="s1">'equal'</code><code class="p">);</code></pre>
<figure><div class="figure" id="fig_0510-manifold-learning_files_in_output_31_0">
<img alt="output 31 0" height="383" src="assets/output_31_0.png" width="600"/>
<h6><span class="label">Figure 46-8. </span>The MDS algorithm applied to the nonlinear data; it fails to recover the underlying structure</h6>
</div></figure>
<p>The best two-dimensional <em>linear</em> embedding does not unwrap the S-curve,
but instead discards the original y-axis<a data-startref="ix_ch46-asciidoc6" data-type="indexterm" id="idm45858725613568"/><a data-startref="ix_ch46-asciidoc5" data-type="indexterm" id="idm45858725612864"/>.<a data-startref="ix_ch46-asciidoc3" data-type="indexterm" id="idm45858725612064"/><a data-startref="ix_ch46-asciidoc2" data-type="indexterm" id="idm45858725611360"/></p>
</div></section>
</div></section>
<section data-pdf-bookmark="Nonlinear Manifolds: Locally Linear Embedding" data-type="sect1"><div class="sect1" id="ch_0510-manifold-learning_nonlinear-manifolds-locally-linear-embedding">
<h1>Nonlinear Manifolds: Locally Linear Embedding</h1>
<p><a data-primary="locally linear embedding (LLE)" data-type="indexterm" id="ix_ch46-asciidoc7"/><a data-primary="multidimensional scaling (MDS)" data-secondary="locally linear embedding and" data-type="indexterm" id="ix_ch46-asciidoc8"/>How can we move forward here? Stepping back, we can see that the source
of the problem is that MDS tries to preserve distances between faraway
points when constructing the embedding. But what if we instead modified
the algorithm such that it only preserves distances between nearby
points? The resulting embedding would be closer to what we want.</p>
<p>Visually, we can think of it as illustrated <a data-type="xref" href="#fig_images_in_0510-lle-vs-mds">Figure 46-9</a>.</p>
<p>Here each faint line represents a distance that should be preserved in
the embedding. On the left is a representation of the model used by MDS:
it tries to preserve the distances between each pair of points in the
dataset. On the right is a representation of the model used by a
manifold learning algorithm called <em>locally linear embedding</em>: rather
than preserving <em>all</em> distances, it instead tries to preserve only the
distances between <em>neighboring points</em> (in this case, the nearest 100
neighbors of each point).</p>
<p class="pagebreak-before less_space">Thinking about the left panel, we can see why MDS fails: there is no way
to unroll this data while adequately preserving the length of every line
drawn between the two points. For the right panel, on the other hand,
things look a bit more optimistic. We could imagine unrolling the data
in a way that keeps the lengths of the lines approximately the same.
This is precisely what LLE does, through a global optimization of a cost
function reflecting this logic.</p>
<figure><div class="figure" id="fig_images_in_0510-lle-vs-mds">
<img alt="05.10 LLE vs MDS" height="226" src="assets/05.10-LLE-vs-MDS.png" width="600"/>
<h6><span class="label">Figure 46-9. </span>Representation of linkages between points within MDS and LLE<sup><a data-type="noteref" href="ch46.xhtml#idm45858725565312" id="idm45858725565312-marker">1</a></sup></h6>
</div></figure>
<p>LLE comes in a number of flavors; here we will use the <em>modified LLE</em>
algorithm to recover the embedded two-dimensional manifold. In general,
modified LLE does better than other flavors of the algorithm at
recovering well-defined manifolds with very little distortion (see <a data-type="xref" href="#fig_0510-manifold-learning_files_in_output_36_0">Figure 46-10</a>).</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">15</code><code class="p">]:</code> <code class="kn">from</code> <code class="nn">sklearn.manifold</code> <code class="kn">import</code> <code class="n">LocallyLinearEmbedding</code>
         <code class="n">model</code> <code class="o">=</code> <code class="n">LocallyLinearEmbedding</code><code class="p">(</code>
             <code class="n">n_neighbors</code><code class="o">=</code><code class="mi">100</code><code class="p">,</code> <code class="n">n_components</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code>
             <code class="n">method</code><code class="o">=</code><code class="s1">'modified'</code><code class="p">,</code> <code class="n">eigen_solver</code><code class="o">=</code><code class="s1">'dense'</code><code class="p">)</code>
         <code class="n">out</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">XS</code><code class="p">)</code>

         <code class="n">fig</code><code class="p">,</code> <code class="n">ax</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">()</code>
         <code class="n">ax</code><code class="o">.</code><code class="n">scatter</code><code class="p">(</code><code class="n">out</code><code class="p">[:,</code> <code class="mi">0</code><code class="p">],</code> <code class="n">out</code><code class="p">[:,</code> <code class="mi">1</code><code class="p">],</code> <code class="o">**</code><code class="n">colorize</code><code class="p">)</code>
         <code class="n">ax</code><code class="o">.</code><code class="n">set_ylim</code><code class="p">(</code><code class="mf">0.15</code><code class="p">,</code> <code class="o">-</code><code class="mf">0.15</code><code class="p">);</code></pre>
<p>The result remains somewhat distorted compared to our original manifold,
but captures the essential relationships in the data!<a data-startref="ix_ch46-asciidoc8" data-type="indexterm" id="idm45858725558848"/><a data-startref="ix_ch46-asciidoc7" data-type="indexterm" id="idm45858725558240"/></p>
<figure><div class="figure" id="fig_0510-manifold-learning_files_in_output_36_0">
<img alt="output 36 0" height="381" src="assets/output_36_0.png" width="600"/>
<h6><span class="label">Figure 46-10. </span>Locally linear embedding can recover the underlying data from a nonli‐ nearly embedded input</h6>
</div></figure>
</div></section>
<section data-pdf-bookmark="Some Thoughts on Manifold Methods" data-type="sect1"><div class="sect1" id="ch_0510-manifold-learning_some-thoughts-on-manifold-methods">
<h1>Some Thoughts on Manifold Methods</h1>
<p><a data-primary="manifold learning" data-secondary="advantages/disadvantages" data-type="indexterm" id="idm45858725483936"/>Compelling as these examples may be, in practice manifold learning
techniques tend to be finicky enough that they are rarely used for
anything more than simple qualitative visualization of high-dimensional
data.</p>
<p>The following are some of the particular challenges of manifold
learning, which all contrast poorly with PCA:</p>
<ul>
<li>
<p>In manifold learning, there is no good framework for handling missing
data. In contrast, there are straightforward iterative approaches for
dealing with missing data in PCA.</p>
</li>
<li>
<p>In manifold learning, the presence of noise in the data can
“short-circuit” the manifold and drastically change the embedding. In
contrast, PCA naturally filters noise from the most important
components.</p>
</li>
<li>
<p>The manifold embedding result is generally highly dependent on the
number of neighbors chosen, and there is generally no solid quantitative
way to choose an optimal number of neighbors. In contrast, PCA does not
involve such a choice.</p>
</li>
<li>
<p>In manifold learning, the globally optimal number of output dimensions
is difficult to determine. In contrast, PCA lets you find the number of
output dimensions based on the explained variance.</p>
</li>
<li>
<p>In manifold learning, the meaning of the embedded dimensions is not
always clear. In PCA, the principal components have a very clear
meaning.</p>
</li>
<li>
<p>In manifold learning, the computational expense of manifold methods
scales as <math alttext="upper O left-bracket upper N squared right-bracket">
<mrow>
<mi>O</mi>
<mo>[</mo>
<msup><mi>N</mi> <mn>2</mn> </msup>
<mo>]</mo>
</mrow>
</math> or <math alttext="upper O left-bracket upper N cubed right-bracket">
<mrow>
<mi>O</mi>
<mo>[</mo>
<msup><mi>N</mi> <mn>3</mn> </msup>
<mo>]</mo>
</mrow>
</math>. For PCA, there
exist randomized approaches that are generally much faster (though see
the <a href="https://oreil.ly/VLBly"><em>megaman</em> package</a> for some more
scalable implementations of manifold learning).</p>
</li>
</ul>
<p>With all that on the table, the only clear advantage of manifold
learning methods over PCA is their ability to preserve nonlinear
relationships in the data; for that reason I tend to explore data with
manifold methods only after first exploring it with PCA.</p>
<p>Scikit-Learn implements several common variants of manifold learning
beyond LLE and Isomap (which we’ve used in a few of the
previous chapters and will look at in the next section): the
Scikit-Learn documentation has a
<a href="https://oreil.ly/tFzS5">nice discussion and
comparison of them</a>. Based on my own experience, I would give the
following recommendations:</p>
<ul>
<li>
<p>For toy problems such as the S-curve we saw before, LLE and its
variants (especially modified LLE) perform very well. This is
implemented in <code>sklearn.manifold.LocallyLinearEmbedding</code>.</p>
</li>
<li>
<p>For high-dimensional data from real-world sources, LLE often produces
poor results, and Isomap seems to generally lead to more meaningful
embeddings. This is implemented in <code>sklearn.manifold.Isomap</code>.</p>
</li>
<li>
<p><a data-primary="t-distributed stochastic neighbor embedding (t-SNE)" data-type="indexterm" id="idm45858725428096"/>For data that is highly clustered, <em>t-distributed stochastic neighbor
embedding</em> (t-SNE) seems to work very well, though it can be very slow
compared to other methods. This is implemented in
<code>sklearn.manifold.TSNE</code>.</p>
</li>
</ul>
<p>If you’re interested in getting a feel for how these work,
I’d suggest running each of the methods on the data in this
section.</p>
</div></section>
<section data-pdf-bookmark="Example: Isomap on Faces" data-type="sect1"><div class="sect1" id="ch_0510-manifold-learning_example-isomap-on-faces">
<h1>Example: Isomap on Faces</h1>
<p><a data-primary="face recognition" data-secondary="Isomap" data-type="indexterm" id="ix_ch46-asciidoc9"/><a data-primary="Isomap" data-secondary="face data" data-type="indexterm" id="ix_ch46-asciidoc10"/><a data-primary="manifold learning" data-secondary="applying Isomap on faces data" data-type="indexterm" id="ix_ch46-asciidoc11"/>One place manifold learning is often used is in understanding the
relationship between high-dimensional data points. A common case of
high-dimensional data is images: for example, a set of images with 1,000
pixels each can be thought of as a collection of points in 1,000
dimensions, with the brightness of each pixel in each image defining the
coordinate in that dimension.</p>
<p>To illustrate, let’s apply Isomap on some data from the
Labeled Faces in the Wild dataset, which we previously saw in Chapters
<a href="ch43.xhtml#section-0507-support-vector-machines">43</a> and <a href="ch45.xhtml#section-0509-principal-component-analysis">45</a>. Running this command will download the
dataset and cache it in your home directory for later use:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">16</code><code class="p">]:</code> <code class="kn">from</code> <code class="nn">sklearn.datasets</code> <code class="kn">import</code> <code class="n">fetch_lfw_people</code>
         <code class="n">faces</code> <code class="o">=</code> <code class="n">fetch_lfw_people</code><code class="p">(</code><code class="n">min_faces_per_person</code><code class="o">=</code><code class="mi">30</code><code class="p">)</code>
         <code class="n">faces</code><code class="o">.</code><code class="n">data</code><code class="o">.</code><code class="n">shape</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">16</code><code class="p">]:</code> <code class="p">(</code><code class="mi">2370</code><code class="p">,</code> <code class="mi">2914</code><code class="p">)</code></pre>
<p>We have 2,370 images, each with 2,914 pixels. In other words, the images
can be thought of as data points in a 2,914-dimensional space!</p>
<p>Let’s display several of these images to remind us what
we’re working with (see <a data-type="xref" href="#fig_0510-manifold-learning_files_in_output_43_0">Figure 46-11</a>).</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">17</code><code class="p">]:</code> <code class="n">fig</code><code class="p">,</code> <code class="n">ax</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">(</code><code class="mi">4</code><code class="p">,</code> <code class="mi">8</code><code class="p">,</code> <code class="n">subplot_kw</code><code class="o">=</code><code class="nb">dict</code><code class="p">(</code><code class="n">xticks</code><code class="o">=</code><code class="p">[],</code> <code class="n">yticks</code><code class="o">=</code><code class="p">[]))</code>
         <code class="k">for</code> <code class="n">i</code><code class="p">,</code> <code class="n">axi</code> <code class="ow">in</code> <code class="nb">enumerate</code><code class="p">(</code><code class="n">ax</code><code class="o">.</code><code class="n">flat</code><code class="p">):</code>
             <code class="n">axi</code><code class="o">.</code><code class="n">imshow</code><code class="p">(</code><code class="n">faces</code><code class="o">.</code><code class="n">images</code><code class="p">[</code><code class="n">i</code><code class="p">],</code> <code class="n">cmap</code><code class="o">=</code><code class="s1">'gray'</code><code class="p">)</code></pre>
<figure class="width-75"><div class="figure" id="fig_0510-manifold-learning_files_in_output_43_0">
<img alt="output 43 0" height="595" src="assets/output_43_0.png" width="600"/>
<h6><span class="label">Figure 46-11. </span>Examples of the input faces</h6>
</div></figure>
<p>When we encountered this data in
<a data-type="xref" href="ch45.xhtml#section-0509-principal-component-analysis">Chapter 45</a>, our goal was essentially compression: to use the
components to reconstruct the inputs from the lower-dimensional
representation.</p>
<p>PCA is versatile enough that we can also use it in this context, where
we would like to plot a low-dimensional embedding of the
2,914-dimensional data to learn the fundamental relationships between
the images. Let’s again look at the explained variance
ratio, which will give us an idea of how many linear features are
required to describe the data (see <a data-type="xref" href="#fig_0510-manifold-learning_files_in_output_45_0">Figure 46-12</a>).</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">18</code><code class="p">]:</code> <code class="kn">from</code> <code class="nn">sklearn.decomposition</code> <code class="kn">import</code> <code class="n">PCA</code>
         <code class="n">model</code> <code class="o">=</code> <code class="n">PCA</code><code class="p">(</code><code class="mi">100</code><code class="p">,</code> <code class="n">svd_solver</code><code class="o">=</code><code class="s1">'randomized'</code><code class="p">)</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">faces</code><code class="o">.</code><code class="n">data</code><code class="p">)</code>
         <code class="n">plt</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">np</code><code class="o">.</code><code class="n">cumsum</code><code class="p">(</code><code class="n">model</code><code class="o">.</code><code class="n">explained_variance_ratio_</code><code class="p">))</code>
         <code class="n">plt</code><code class="o">.</code><code class="n">xlabel</code><code class="p">(</code><code class="s1">'n components'</code><code class="p">)</code>
         <code class="n">plt</code><code class="o">.</code><code class="n">ylabel</code><code class="p">(</code><code class="s1">'cumulative variance'</code><code class="p">);</code></pre>
<figure><div class="figure" id="fig_0510-manifold-learning_files_in_output_45_0">
<img alt="output 45 0" height="399" src="assets/output_45_0.png" width="600"/>
<h6><span class="label">Figure 46-12. </span>Cumulative variance from the PCA projection</h6>
</div></figure>
<p>We see that for this data, nearly 100 components are required to
preserve 90% of the variance. This tells us that the data is
intrinsically very high-dimensional—it can’t be described
linearly with just a few components.</p>
<p>When this is the case, nonlinear manifold embeddings like LLE and Isomap
may be helpful. We can compute an Isomap embedding on these faces using
the same pattern shown before:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">19</code><code class="p">]:</code> <code class="kn">from</code> <code class="nn">sklearn.manifold</code> <code class="kn">import</code> <code class="n">Isomap</code>
         <code class="n">model</code> <code class="o">=</code> <code class="n">Isomap</code><code class="p">(</code><code class="n">n_components</code><code class="o">=</code><code class="mi">2</code><code class="p">)</code>
         <code class="n">proj</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">faces</code><code class="o">.</code><code class="n">data</code><code class="p">)</code>
         <code class="n">proj</code><code class="o">.</code><code class="n">shape</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">19</code><code class="p">]:</code> <code class="p">(</code><code class="mi">2370</code><code class="p">,</code> <code class="mi">2</code><code class="p">)</code></pre>
<p>The output is a two-dimensional projection of all the input images. To
get a better idea of what the projection tells us, let’s
define a function that will output image thumbnails at the locations of
the projections:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">20</code><code class="p">]:</code> <code class="kn">from</code> <code class="nn">matplotlib</code> <code class="kn">import</code> <code class="n">offsetbox</code>

         <code class="k">def</code> <code class="nf">plot_components</code><code class="p">(</code><code class="n">data</code><code class="p">,</code> <code class="n">model</code><code class="p">,</code> <code class="n">images</code><code class="o">=</code><code class="kc">None</code><code class="p">,</code> <code class="n">ax</code><code class="o">=</code><code class="kc">None</code><code class="p">,</code>
                             <code class="n">thumb_frac</code><code class="o">=</code><code class="mf">0.05</code><code class="p">,</code> <code class="n">cmap</code><code class="o">=</code><code class="s1">'gray'</code><code class="p">):</code>
             <code class="n">ax</code> <code class="o">=</code> <code class="n">ax</code> <code class="ow">or</code> <code class="n">plt</code><code class="o">.</code><code class="n">gca</code><code class="p">()</code>

             <code class="n">proj</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">data</code><code class="p">)</code>
             <code class="n">ax</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">proj</code><code class="p">[:,</code> <code class="mi">0</code><code class="p">],</code> <code class="n">proj</code><code class="p">[:,</code> <code class="mi">1</code><code class="p">],</code> <code class="s1">'.k'</code><code class="p">)</code>

             <code class="k">if</code> <code class="n">images</code> <code class="ow">is</code> <code class="ow">not</code> <code class="kc">None</code><code class="p">:</code>
                 <code class="n">min_dist_2</code> <code class="o">=</code> <code class="p">(</code><code class="n">thumb_frac</code> <code class="o">*</code> <code class="nb">max</code><code class="p">(</code><code class="n">proj</code><code class="o">.</code><code class="n">max</code><code class="p">(</code><code class="mi">0</code><code class="p">)</code> <code class="o">-</code> <code class="n">proj</code><code class="o">.</code><code class="n">min</code><code class="p">(</code><code class="mi">0</code><code class="p">)))</code> <code class="o">**</code> <code class="mi">2</code>
                 <code class="n">shown_images</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">([</code><code class="mi">2</code> <code class="o">*</code> <code class="n">proj</code><code class="o">.</code><code class="n">max</code><code class="p">(</code><code class="mi">0</code><code class="p">)])</code>
                 <code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">data</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">0</code><code class="p">]):</code>
                     <code class="n">dist</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">sum</code><code class="p">((</code><code class="n">proj</code><code class="p">[</code><code class="n">i</code><code class="p">]</code> <code class="o">-</code> <code class="n">shown_images</code><code class="p">)</code> <code class="o">**</code> <code class="mi">2</code><code class="p">,</code> <code class="mi">1</code><code class="p">)</code>
                     <code class="k">if</code> <code class="n">np</code><code class="o">.</code><code class="n">min</code><code class="p">(</code><code class="n">dist</code><code class="p">)</code> <code class="o">&lt;</code> <code class="n">min_dist_2</code><code class="p">:</code>
                         <code class="c1"># don't show points that are too close</code>
                         <code class="k">continue</code>
                     <code class="n">shown_images</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">vstack</code><code class="p">([</code><code class="n">shown_images</code><code class="p">,</code> <code class="n">proj</code><code class="p">[</code><code class="n">i</code><code class="p">]])</code>
                     <code class="n">imagebox</code> <code class="o">=</code> <code class="n">offsetbox</code><code class="o">.</code><code class="n">AnnotationBbox</code><code class="p">(</code>
                         <code class="n">offsetbox</code><code class="o">.</code><code class="n">OffsetImage</code><code class="p">(</code><code class="n">images</code><code class="p">[</code><code class="n">i</code><code class="p">],</code> <code class="n">cmap</code><code class="o">=</code><code class="n">cmap</code><code class="p">),</code>
                                               <code class="n">proj</code><code class="p">[</code><code class="n">i</code><code class="p">])</code>
                     <code class="n">ax</code><code class="o">.</code><code class="n">add_artist</code><code class="p">(</code><code class="n">imagebox</code><code class="p">)</code></pre>
<p>Calling this function now, we see the result in <a data-type="xref" href="#fig_0510-manifold-learning_files_in_output_51_0">Figure 46-13</a>.</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">21</code><code class="p">]:</code> <code class="n">fig</code><code class="p">,</code> <code class="n">ax</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">(</code><code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">10</code><code class="p">,</code> <code class="mi">10</code><code class="p">))</code>
         <code class="n">plot_components</code><code class="p">(</code><code class="n">faces</code><code class="o">.</code><code class="n">data</code><code class="p">,</code>
                         <code class="n">model</code><code class="o">=</code><code class="n">Isomap</code><code class="p">(</code><code class="n">n_components</code><code class="o">=</code><code class="mi">2</code><code class="p">),</code>
                         <code class="n">images</code><code class="o">=</code><code class="n">faces</code><code class="o">.</code><code class="n">images</code><code class="p">[:,</code> <code class="p">::</code><code class="mi">2</code><code class="p">,</code> <code class="p">::</code><code class="mi">2</code><code class="p">])</code></pre>
<figure><div class="figure" id="fig_0510-manifold-learning_files_in_output_51_0">
<img alt="output 51 0" height="564" src="assets/output_51_0.png" width="600"/>
<h6><span class="label">Figure 46-13. </span>Isomap embedding of the LFW data</h6>
</div></figure>
<p>The result is interesting. The first two Isomap dimensions seem to
describe global image features: the overall brightness of the image from
left to right, and the general orientation of the face from bottom to
top. This gives us a nice visual indication of some of the fundamental
features in our data.</p>
<p>From here, we could then go on to classify this data (perhaps using
manifold features as inputs to the classification algorithm) as we did
in <a data-type="xref" href="ch43.xhtml#section-0507-support-vector-machines">Chapter 43</a>.<a data-startref="ix_ch46-asciidoc11" data-type="indexterm" id="idm45858724815184"/><a data-startref="ix_ch46-asciidoc10" data-type="indexterm" id="idm45858724814480"/><a data-startref="ix_ch46-asciidoc9" data-type="indexterm" id="idm45858724813808"/></p>
</div></section>
<section data-pdf-bookmark="Example: Visualizing Structure in Digits" data-type="sect1"><div class="sect1" id="ch_0510-manifold-learning_example-visualizing-structure-in-digits">
<h1>Example: Visualizing Structure in Digits</h1>
<p><a data-primary="manifold learning" data-secondary="visualizing structure in digits" data-type="indexterm" id="ix_ch46-asciidoc12"/><a data-primary="optical character recognition" data-secondary="visualizing structure in digits" data-type="indexterm" id="ix_ch46-asciidoc13"/>As another example of using manifold learning for visualization,
let’s take a look at the MNIST handwritten digits dataset.
This is similar to the digits dataset we saw in
<a data-type="xref" href="ch44.xhtml#section-0508-random-forests">Chapter 44</a>, but with many more pixels per image. It can be downloaded from
<a class="bare" href="http://openml.org"><em class="hyperlink">http://openml.org</em></a> with the Scikit-Learn utility:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">22</code><code class="p">]:</code> <code class="kn">from</code> <code class="nn">sklearn.datasets</code> <code class="kn">import</code> <code class="n">fetch_openml</code>
         <code class="n">mnist</code> <code class="o">=</code> <code class="n">fetch_openml</code><code class="p">(</code><code class="s1">'mnist_784'</code><code class="p">)</code>
         <code class="n">mnist</code><code class="o">.</code><code class="n">data</code><code class="o">.</code><code class="n">shape</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">22</code><code class="p">]:</code> <code class="p">(</code><code class="mi">70000</code><code class="p">,</code> <code class="mi">784</code><code class="p">)</code></pre>
<p>The dataset consists of 70,000 images, each with 784 pixels (i.e., the
images are 28 × 28). As before, we can take a look at the first few
images (see <a data-type="xref" href="#fig_0510-manifold-learning_files_in_output_56_0">Figure 46-14</a>).</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">23</code><code class="p">]:</code> <code class="n">mnist_data</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">asarray</code><code class="p">(</code><code class="n">mnist</code><code class="o">.</code><code class="n">data</code><code class="p">)</code>
         <code class="n">mnist_target</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">asarray</code><code class="p">(</code><code class="n">mnist</code><code class="o">.</code><code class="n">target</code><code class="p">,</code> <code class="n">dtype</code><code class="o">=</code><code class="nb">int</code><code class="p">)</code>

         <code class="n">fig</code><code class="p">,</code> <code class="n">ax</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">(</code><code class="mi">6</code><code class="p">,</code> <code class="mi">8</code><code class="p">,</code> <code class="n">subplot_kw</code><code class="o">=</code><code class="nb">dict</code><code class="p">(</code><code class="n">xticks</code><code class="o">=</code><code class="p">[],</code> <code class="n">yticks</code><code class="o">=</code><code class="p">[]))</code>
         <code class="k">for</code> <code class="n">i</code><code class="p">,</code> <code class="n">axi</code> <code class="ow">in</code> <code class="nb">enumerate</code><code class="p">(</code><code class="n">ax</code><code class="o">.</code><code class="n">flat</code><code class="p">):</code>
             <code class="n">axi</code><code class="o">.</code><code class="n">imshow</code><code class="p">(</code><code class="n">mnist_data</code><code class="p">[</code><code class="mi">1250</code> <code class="o">*</code> <code class="n">i</code><code class="p">]</code><code class="o">.</code><code class="n">reshape</code><code class="p">(</code><code class="mi">28</code><code class="p">,</code> <code class="mi">28</code><code class="p">),</code> <code class="n">cmap</code><code class="o">=</code><code class="s1">'gray_r'</code><code class="p">)</code></pre>
<figure class="width-75"><div class="figure" id="fig_0510-manifold-learning_files_in_output_56_0">
<img alt="output 56 0" height="395" src="assets/output_56_0.png" width="600"/>
<h6><span class="label">Figure 46-14. </span>Examples of MNIST digits</h6>
</div></figure>
<p>This gives us an idea of the variety of handwriting styles in the
dataset.</p>
<p>Let’s compute a manifold learning projection across the
data. For speed here, we’ll only use 1/30 of the data, which
is about ~2,000 points (because of the relatively poor scaling of
manifold learning, I find that a few thousand samples is a good number
to start with for relatively quick exploration before moving to a full
calculation). <a data-type="xref" href="#fig_0510-manifold-learning_files_in_output_58_0">Figure 46-15</a> shows the result.</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">24</code><code class="p">]:</code> <code class="c1"># Use only 1/30 of the data: full dataset takes a long time!</code>
         <code class="n">data</code> <code class="o">=</code> <code class="n">mnist_data</code><code class="p">[::</code><code class="mi">30</code><code class="p">]</code>
         <code class="n">target</code> <code class="o">=</code> <code class="n">mnist_target</code><code class="p">[::</code><code class="mi">30</code><code class="p">]</code>

         <code class="n">model</code> <code class="o">=</code> <code class="n">Isomap</code><code class="p">(</code><code class="n">n_components</code><code class="o">=</code><code class="mi">2</code><code class="p">)</code>
         <code class="n">proj</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">data</code><code class="p">)</code>

         <code class="n">plt</code><code class="o">.</code><code class="n">scatter</code><code class="p">(</code><code class="n">proj</code><code class="p">[:,</code> <code class="mi">0</code><code class="p">],</code> <code class="n">proj</code><code class="p">[:,</code> <code class="mi">1</code><code class="p">],</code> <code class="n">c</code><code class="o">=</code><code class="n">target</code><code class="p">,</code>
                                 <code class="n">cmap</code><code class="o">=</code><code class="n">plt</code><code class="o">.</code><code class="n">cm</code><code class="o">.</code><code class="n">get_cmap</code><code class="p">(</code><code class="s1">'jet'</code><code class="p">,</code> <code class="mi">10</code><code class="p">))</code>
         <code class="n">plt</code><code class="o">.</code><code class="n">colorbar</code><code class="p">(</code><code class="n">ticks</code><code class="o">=</code><code class="nb">range</code><code class="p">(</code><code class="mi">10</code><code class="p">))</code>
         <code class="n">plt</code><code class="o">.</code><code class="n">clim</code><code class="p">(</code><code class="o">-</code><code class="mf">0.5</code><code class="p">,</code> <code class="mf">9.5</code><code class="p">);</code></pre>
<figure><div class="figure" id="fig_0510-manifold-learning_files_in_output_58_0">
<img alt="output 58 0" height="409" src="assets/output_58_0.png" width="600"/>
<h6><span class="label">Figure 46-15. </span>Isomap embedding of the MNIST digit data</h6>
</div></figure>
<p>The resulting scatter plot shows some of the relationships between the
data points, but is a bit crowded. We can gain more insight by looking
at just a single number at a time (see <a data-type="xref" href="#fig_0510-manifold-learning_files_in_output_60_0">Figure 46-16</a>).</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">25</code><code class="p">]:</code> <code class="c1"># Choose 1/4 of the "1" digits to project</code>
         <code class="n">data</code> <code class="o">=</code> <code class="n">mnist_data</code><code class="p">[</code><code class="n">mnist_target</code> <code class="o">==</code> <code class="mi">1</code><code class="p">][::</code><code class="mi">4</code><code class="p">]</code>

         <code class="n">fig</code><code class="p">,</code> <code class="n">ax</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">(</code><code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">10</code><code class="p">,</code> <code class="mi">10</code><code class="p">))</code>
         <code class="n">model</code> <code class="o">=</code> <code class="n">Isomap</code><code class="p">(</code><code class="n">n_neighbors</code><code class="o">=</code><code class="mi">5</code><code class="p">,</code> <code class="n">n_components</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code> <code class="n">eigen_solver</code><code class="o">=</code><code class="s1">'dense'</code><code class="p">)</code>
         <code class="n">plot_components</code><code class="p">(</code><code class="n">data</code><code class="p">,</code> <code class="n">model</code><code class="p">,</code> <code class="n">images</code><code class="o">=</code><code class="n">data</code><code class="o">.</code><code class="n">reshape</code><code class="p">((</code><code class="o">-</code><code class="mi">1</code><code class="p">,</code> <code class="mi">28</code><code class="p">,</code> <code class="mi">28</code><code class="p">)),</code>
                         <code class="n">ax</code><code class="o">=</code><code class="n">ax</code><code class="p">,</code> <code class="n">thumb_frac</code><code class="o">=</code><code class="mf">0.05</code><code class="p">,</code> <code class="n">cmap</code><code class="o">=</code><code class="s1">'gray_r'</code><code class="p">)</code></pre>
<figure><div class="figure" id="fig_0510-manifold-learning_files_in_output_60_0">
<img alt="output 60 0" height="564" src="assets/output_60_0.png" width="600"/>
<h6><span class="label">Figure 46-16. </span>Isomap embedding of only the 1s within the MNIST dataset</h6>
</div></figure>
<p>The result gives you an idea of the variety of forms that the number 1
can take within the dataset. The data lies along a broad curve in the
projected space, which appears to trace the orientation of the digit. As
you move up the plot, you find 1s that have hats and/or bases, though
these are very sparse within the dataset. The projection lets us
identify outliers that have data issues: for example, pieces of the
neighboring digits that snuck into the extracted images.</p>
<p>Now, this in itself may not be useful for the task of classifying
digits, but it does help us get an understanding of the data, and may
give us ideas about how to move forward—such as how we might want to
preprocess the data before building a classification pipeline<a data-startref="ix_ch46-asciidoc13" data-type="indexterm" id="idm45858724399232"/><a data-startref="ix_ch46-asciidoc12" data-type="indexterm" id="idm45858724398528"/>.<a data-startref="ix_ch46-asciidoc1" data-type="indexterm" id="idm45858724397728"/><a data-startref="ix_ch46-asciidoc0" data-type="indexterm" id="idm45858724397024"/></p>
</div></section>
<div data-type="footnotes"><p data-type="footnote" id="idm45858725565312"><sup><a href="ch46.xhtml#idm45858725565312-marker">1</a></sup> Code to produce this figure can be found in the <a href="https://oreil.ly/gu4iE">online appendix</a>.</p></div></div></section></div></body></html>