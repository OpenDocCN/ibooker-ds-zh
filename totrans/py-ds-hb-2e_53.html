<html><head></head><body>
<div id="sbo-rt-content"><section data-pdf-bookmark="Chapter 48. In Depth: Gaussian Mixture Models" data-type="chapter" epub:type="chapter"><div class="chapter" id="section-0512-gaussian-mixtures">
<h1><span class="label">Chapter 48. </span>In Depth: Gaussian Mixture Models</h1>
<p><a data-primary="clustering" data-secondary="Gaussian mixture models" data-type="indexterm" id="ix_ch48-asciidoc0"/><a data-primary="Gaussian mixture models (GMMs)" data-type="indexterm" id="ix_ch48-asciidoc1"/><a data-primary="machine learning" data-secondary="Gaussian mixture models" data-type="indexterm" id="ix_ch48-asciidoc2"/>The <em>k</em>-means clustering model explored in the previous chapter is
simple and relatively easy to understand, but its simplicity leads to
practical challenges in its application. In particular, the
nonprobabilistic nature of <em>k</em>-means and its use of simple distance from
cluster center to assign cluster membership leads to poor performance
for many real-world situations. In this chapter we will take a look at
Gaussian mixture models, which can be viewed as an extension of the
ideas behind <em>k</em>-means, but can also be a powerful tool for estimation
beyond simple clustering.</p>
<p>We begin with the standard imports:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">1</code><code class="p">]:</code> <code class="o">%</code><code class="k">matplotlib</code> inline
        <code class="kn">import</code> <code class="nn">matplotlib.pyplot</code> <code class="k">as</code> <code class="nn">plt</code>
        <code class="n">plt</code><code class="o">.</code><code class="n">style</code><code class="o">.</code><code class="n">use</code><code class="p">(</code><code class="s1">'seaborn-whitegrid'</code><code class="p">)</code>
        <code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code></pre>
<section data-pdf-bookmark="Motivating Gaussian Mixtures: Weaknesses of k-Means" data-type="sect1"><div class="sect1" id="ch_0512-gaussian-mixtures_motivating-gaussian-mixtures-weaknesses-of-k-means">
<h1>Motivating Gaussian Mixtures: Weaknesses of k-Means</h1>
<p><a data-primary="Gaussian mixture models (GMMs)" data-secondary="k-means weaknesses addressed by" data-type="indexterm" id="ix_ch48-asciidoc3"/><a data-primary="k-means clustering" data-secondary="Gaussian mixture model as means of addressing weaknesses of" data-type="indexterm" id="ix_ch48-asciidoc4"/>Let’s take a look at some of the weaknesses of <em>k</em>-means and
think about how we might improve the cluster model. As we saw in the
previous chapter, given simple, well-separated data, <em>k</em>-means finds
suitable clustering results.</p>
<p>For example, if we have simple blobs of data, the <em>k</em>-means algorithm
can quickly label those clusters in a way that closely matches what we
might do by eye (see <a data-type="xref" href="#fig_0512-gaussian-mixtures_files_in_output_5_0">Figure 48-1</a>).</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">2</code><code class="p">]:</code> <code class="c1"># Generate some data</code>
        <code class="kn">from</code> <code class="nn">sklearn.datasets</code> <code class="kn">import</code> <code class="n">make_blobs</code>
        <code class="n">X</code><code class="p">,</code> <code class="n">y_true</code> <code class="o">=</code> <code class="n">make_blobs</code><code class="p">(</code><code class="n">n_samples</code><code class="o">=</code><code class="mi">400</code><code class="p">,</code> <code class="n">centers</code><code class="o">=</code><code class="mi">4</code><code class="p">,</code>
                               <code class="n">cluster_std</code><code class="o">=</code><code class="mf">0.60</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code>
        <code class="n">X</code> <code class="o">=</code> <code class="n">X</code><code class="p">[:,</code> <code class="p">::</code><code class="o">-</code><code class="mi">1</code><code class="p">]</code> <code class="c1"># flip axes for better plotting</code></pre>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">3</code><code class="p">]:</code> <code class="c1"># Plot the data with k-means labels</code>
        <code class="kn">from</code> <code class="nn">sklearn.cluster</code> <code class="kn">import</code> <code class="n">KMeans</code>
        <code class="n">kmeans</code> <code class="o">=</code> <code class="n">KMeans</code><code class="p">(</code><code class="mi">4</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code>
        <code class="n">labels</code> <code class="o">=</code> <code class="n">kmeans</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">)</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X</code><code class="p">)</code>
        <code class="n">plt</code><code class="o">.</code><code class="n">scatter</code><code class="p">(</code><code class="n">X</code><code class="p">[:,</code> <code class="mi">0</code><code class="p">],</code> <code class="n">X</code><code class="p">[:,</code> <code class="mi">1</code><code class="p">],</code> <code class="n">c</code><code class="o">=</code><code class="n">labels</code><code class="p">,</code> <code class="n">s</code><code class="o">=</code><code class="mi">40</code><code class="p">,</code> <code class="n">cmap</code><code class="o">=</code><code class="s1">'viridis'</code><code class="p">);</code></pre>
<figure><div class="figure" id="fig_0512-gaussian-mixtures_files_in_output_5_0">
<img alt="output 5 0" height="392" src="assets/output_5_0.png" width="600"/>
<h6><span class="label">Figure 48-1. </span><span class="roman">k</span>-means labels for simple data</h6>
</div></figure>
<p>From an intuitive standpoint, we might expect that the clustering
assignment for some points is more certain than others: for example,
there appears to be a very slight overlap between the two middle
clusters, such that we might not have complete confidence in the cluster
assignment of points between them. Unfortunately, the <em>k</em>-means model
has no intrinsic measure of probability or uncertainty of cluster
assignments (although it may be possible to use a bootstrap approach to
estimate this uncertainty). For this, we must think about generalizing
the model.</p>
<p>One way to think about the <em>k</em>-means model is that it places a circle
(or, in higher dimensions, a hypersphere) at the center of each cluster,
with a radius defined by the most distant point in the cluster. This
radius acts as a hard cutoff for cluster assignment within the training
set: any point outside this circle is not considered a member of the
cluster. We can visualize this cluster model with the following function
(see <a data-type="xref" href="#fig_0512-gaussian-mixtures_files_in_output_8_0">Figure 48-2</a>).</p>
<pre class="pagebreak-before less_space" data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">4</code><code class="p">]:</code> <code class="kn">from</code> <code class="nn">sklearn.cluster</code> <code class="kn">import</code> <code class="n">KMeans</code>
        <code class="kn">from</code> <code class="nn">scipy.spatial.distance</code> <code class="kn">import</code> <code class="n">cdist</code>

        <code class="k">def</code> <code class="nf">plot_kmeans</code><code class="p">(</code><code class="n">kmeans</code><code class="p">,</code> <code class="n">X</code><code class="p">,</code> <code class="n">n_clusters</code><code class="o">=</code><code class="mi">4</code><code class="p">,</code> <code class="n">rseed</code><code class="o">=</code><code class="mi">0</code><code class="p">,</code> <code class="n">ax</code><code class="o">=</code><code class="kc">None</code><code class="p">):</code>
            <code class="n">labels</code> <code class="o">=</code> <code class="n">kmeans</code><code class="o">.</code><code class="n">fit_predict</code><code class="p">(</code><code class="n">X</code><code class="p">)</code>

            <code class="c1"># plot the input data</code>
            <code class="n">ax</code> <code class="o">=</code> <code class="n">ax</code> <code class="ow">or</code> <code class="n">plt</code><code class="o">.</code><code class="n">gca</code><code class="p">()</code>
            <code class="n">ax</code><code class="o">.</code><code class="n">axis</code><code class="p">(</code><code class="s1">'equal'</code><code class="p">)</code>
            <code class="n">ax</code><code class="o">.</code><code class="n">scatter</code><code class="p">(</code><code class="n">X</code><code class="p">[:,</code> <code class="mi">0</code><code class="p">],</code> <code class="n">X</code><code class="p">[:,</code> <code class="mi">1</code><code class="p">],</code> <code class="n">c</code><code class="o">=</code><code class="n">labels</code><code class="p">,</code> <code class="n">s</code><code class="o">=</code><code class="mi">40</code><code class="p">,</code> <code class="n">cmap</code><code class="o">=</code><code class="s1">'viridis'</code><code class="p">,</code> <code class="n">zorder</code><code class="o">=</code><code class="mi">2</code><code class="p">)</code>

            <code class="c1"># plot the representation of the KMeans model</code>
            <code class="n">centers</code> <code class="o">=</code> <code class="n">kmeans</code><code class="o">.</code><code class="n">cluster_centers_</code>
            <code class="n">radii</code> <code class="o">=</code> <code class="p">[</code><code class="n">cdist</code><code class="p">(</code><code class="n">X</code><code class="p">[</code><code class="n">labels</code> <code class="o">==</code> <code class="n">i</code><code class="p">],</code> <code class="p">[</code><code class="n">center</code><code class="p">])</code><code class="o">.</code><code class="n">max</code><code class="p">()</code>
                     <code class="k">for</code> <code class="n">i</code><code class="p">,</code> <code class="n">center</code> <code class="ow">in</code> <code class="nb">enumerate</code><code class="p">(</code><code class="n">centers</code><code class="p">)]</code>
            <code class="k">for</code> <code class="n">c</code><code class="p">,</code> <code class="n">r</code> <code class="ow">in</code> <code class="nb">zip</code><code class="p">(</code><code class="n">centers</code><code class="p">,</code> <code class="n">radii</code><code class="p">):</code>
                <code class="n">ax</code><code class="o">.</code><code class="n">add_patch</code><code class="p">(</code><code class="n">plt</code><code class="o">.</code><code class="n">Circle</code><code class="p">(</code><code class="n">c</code><code class="p">,</code> <code class="n">r</code><code class="p">,</code> <code class="n">ec</code><code class="o">=</code><code class="s1">'black'</code><code class="p">,</code> <code class="n">fc</code><code class="o">=</code><code class="s1">'lightgray'</code><code class="p">,</code>
                                        <code class="n">lw</code><code class="o">=</code><code class="mi">3</code><code class="p">,</code> <code class="n">alpha</code><code class="o">=</code><code class="mf">0.5</code><code class="p">,</code> <code class="n">zorder</code><code class="o">=</code><code class="mi">1</code><code class="p">))</code></pre>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">5</code><code class="p">]:</code> <code class="n">kmeans</code> <code class="o">=</code> <code class="n">KMeans</code><code class="p">(</code><code class="n">n_clusters</code><code class="o">=</code><code class="mi">4</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code>
        <code class="n">plot_kmeans</code><code class="p">(</code><code class="n">kmeans</code><code class="p">,</code> <code class="n">X</code><code class="p">)</code></pre>
<figure><div class="figure" id="fig_0512-gaussian-mixtures_files_in_output_8_0">
<img alt="output 8 0" height="392" src="assets/output_8_0.png" width="600"/>
<h6><span class="label">Figure 48-2. </span>The circular clusters implied by the <span class="roman">k</span>-means model</h6>
</div></figure>
<p>An important observation for <em>k</em>-means is that these cluster models
<em>must be circular</em>: <em>k</em>-means has no built-in way of accounting for
oblong or elliptical clusters. So, for example, if we take the same data
and transform it, the cluster assignments end up becoming muddled, as
you can see in <a data-type="xref" href="#fig_0512-gaussian-mixtures_files_in_output_10_0">Figure 48-3</a>.</p>
<pre class="pagebreak-before less_space" data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">6</code><code class="p">]:</code> <code class="n">rng</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">RandomState</code><code class="p">(</code><code class="mi">13</code><code class="p">)</code>
        <code class="n">X_stretched</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">dot</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">rng</code><code class="o">.</code><code class="n">randn</code><code class="p">(</code><code class="mi">2</code><code class="p">,</code> <code class="mi">2</code><code class="p">))</code>

        <code class="n">kmeans</code> <code class="o">=</code> <code class="n">KMeans</code><code class="p">(</code><code class="n">n_clusters</code><code class="o">=</code><code class="mi">4</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code>
        <code class="n">plot_kmeans</code><code class="p">(</code><code class="n">kmeans</code><code class="p">,</code> <code class="n">X_stretched</code><code class="p">)</code></pre>
<figure><div class="figure" id="fig_0512-gaussian-mixtures_files_in_output_10_0">
<img alt="output 10 0" height="398" src="assets/output_10_0.png" width="600"/>
<h6><span class="label">Figure 48-3. </span>Poor performance of <span class="roman">k</span>-means for noncircular clusters</h6>
</div></figure>
<p>By eye, we recognize that these transformed clusters are noncircular,
and thus circular clusters would be a poor fit. Nevertheless, <em>k</em>-means
is not flexible enough to account for this, and tries to force-fit the
data into four circular clusters. This results in a mixing of cluster
assignments where the resulting circles overlap: see especially the
bottom-right of this plot. One might imagine addressing this particular
situation by preprocessing the data with PCA (see
<a data-type="xref" href="ch45.xhtml#section-0509-principal-component-analysis">Chapter 45</a>), but in practice there is no guarantee that such a
global operation will circularize the individual groups.</p>
<p>These two disadvantages of <em>k</em>-means—its lack of flexibility in cluster
shape and lack of probabilistic cluster assignment—mean that for many
datasets (especially low-dimensional datasets) it may not perform as
well as you might hope.</p>
<p>You might imagine addressing these weaknesses by generalizing the
<em>k</em>-means model: for example, you could measure uncertainty in cluster
assignment by comparing the distances of each point to <em>all</em> cluster
centers, rather than focusing on just the closest. You might also
imagine allowing the cluster boundaries to be ellipses rather than
circles, so as to account for noncircular clusters. It turns out these
are two essential components of a different type of clustering model,
Gaussian mixture models.<a data-startref="ix_ch48-asciidoc4" data-type="indexterm" id="idm45858721494816"/><a data-startref="ix_ch48-asciidoc3" data-type="indexterm" id="idm45858721494208"/></p>
</div></section>
<section class="pagebreak-before less_space" data-pdf-bookmark="Generalizing E–M: Gaussian Mixture Models" data-type="sect1"><div class="sect1" id="ch_0512-gaussian-mixtures_generalizing-e-m-gaussian-mixture-models">
<h1>Generalizing E–M: Gaussian Mixture Models</h1>
<p><a data-primary="expectation-maximization (E-M) algorithm" data-secondary="Gaussian mixture model as generalization of" data-type="indexterm" id="ix_ch48-asciidoc5"/><a data-primary="Gaussian mixture models (GMMs)" data-secondary="E–M generalization" data-type="indexterm" id="ix_ch48-asciidoc6"/>A Gaussian mixture model (GMM) attempts to find a mixture of
multidimensional Gaussian probability distributions that best model any
input dataset. In the simplest case, GMMs can be used for finding
clusters in the same manner as <em>k</em>-means (see <a data-type="xref" href="#fig_0512-gaussian-mixtures_files_in_output_13_0">Figure 48-4</a>).</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">7</code><code class="p">]:</code> <code class="kn">from</code> <code class="nn">sklearn.mixture</code> <code class="kn">import</code> <code class="n">GaussianMixture</code>
        <code class="n">gmm</code> <code class="o">=</code> <code class="n">GaussianMixture</code><code class="p">(</code><code class="n">n_components</code><code class="o">=</code><code class="mi">4</code><code class="p">)</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">)</code>
        <code class="n">labels</code> <code class="o">=</code> <code class="n">gmm</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X</code><code class="p">)</code>
        <code class="n">plt</code><code class="o">.</code><code class="n">scatter</code><code class="p">(</code><code class="n">X</code><code class="p">[:,</code> <code class="mi">0</code><code class="p">],</code> <code class="n">X</code><code class="p">[:,</code> <code class="mi">1</code><code class="p">],</code> <code class="n">c</code><code class="o">=</code><code class="n">labels</code><code class="p">,</code> <code class="n">s</code><code class="o">=</code><code class="mi">40</code><code class="p">,</code> <code class="n">cmap</code><code class="o">=</code><code class="s1">'viridis'</code><code class="p">);</code></pre>
<figure><div class="figure" id="fig_0512-gaussian-mixtures_files_in_output_13_0">
<img alt="output 13 0" height="392" src="assets/output_13_0.png" width="600"/>
<h6><span class="label">Figure 48-4. </span>Gaussian mixture model labels for the data</h6>
</div></figure>
<p>But because a GMM contains a probabilistic model under the hood, it is
also possible to find probabilistic cluster assignments—in Scikit-Learn
this is done using the 
<span class="keep-together"><code>predict_proba</code></span> method. This returns a matrix of
size <code>[n_samples, n_clusters]</code> which measures the probability that any
point belongs to the given cluster:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">8</code><code class="p">]:</code> <code class="n">probs</code> <code class="o">=</code> <code class="n">gmm</code><code class="o">.</code><code class="n">predict_proba</code><code class="p">(</code><code class="n">X</code><code class="p">)</code>
        <code class="nb">print</code><code class="p">(</code><code class="n">probs</code><code class="p">[:</code><code class="mi">5</code><code class="p">]</code><code class="o">.</code><code class="n">round</code><code class="p">(</code><code class="mi">3</code><code class="p">))</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">8</code><code class="p">]:</code> <code class="p">[[</code><code class="mf">0.</code>    <code class="mf">0.531</code> <code class="mf">0.469</code> <code class="mf">0.</code>   <code class="p">]</code>
         <code class="p">[</code><code class="mf">0.</code>    <code class="mf">0.</code>    <code class="mf">0.</code>    <code class="mf">1.</code>   <code class="p">]</code>
         <code class="p">[</code><code class="mf">0.</code>    <code class="mf">0.</code>    <code class="mf">0.</code>    <code class="mf">1.</code>   <code class="p">]</code>
         <code class="p">[</code><code class="mf">0.</code>    <code class="mf">1.</code>    <code class="mf">0.</code>    <code class="mf">0.</code>   <code class="p">]</code>
         <code class="p">[</code><code class="mf">0.</code>    <code class="mf">0.</code>    <code class="mf">0.</code>    <code class="mf">1.</code>   <code class="p">]]</code></pre>
<p>We can visualize this uncertainty by, for example, making the size of
each point proportional to the certainty of its prediction; looking at
<a data-type="xref" href="#fig_0512-gaussian-mixtures_files_in_output_17_0">Figure 48-5</a>, we can see that it is precisely the points at the
boundaries between clusters that reflect this uncertainty of cluster
assignment:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">9</code><code class="p">]:</code> <code class="n">size</code> <code class="o">=</code> <code class="mi">50</code> <code class="o">*</code> <code class="n">probs</code><code class="o">.</code><code class="n">max</code><code class="p">(</code><code class="mi">1</code><code class="p">)</code> <code class="o">**</code> <code class="mi">2</code>  <code class="c1"># square emphasizes differences</code>
        <code class="n">plt</code><code class="o">.</code><code class="n">scatter</code><code class="p">(</code><code class="n">X</code><code class="p">[:,</code> <code class="mi">0</code><code class="p">],</code> <code class="n">X</code><code class="p">[:,</code> <code class="mi">1</code><code class="p">],</code> <code class="n">c</code><code class="o">=</code><code class="n">labels</code><code class="p">,</code> <code class="n">cmap</code><code class="o">=</code><code class="s1">'viridis'</code><code class="p">,</code> <code class="n">s</code><code class="o">=</code><code class="n">size</code><code class="p">);</code></pre>
<figure><div class="figure" id="fig_0512-gaussian-mixtures_files_in_output_17_0">
<img alt="output 17 0" height="401" src="assets/output_17_0.png" width="600"/>
<h6><span class="label">Figure 48-5. </span>GMM probabilistic labels: probabilities are shown by the size of points</h6>
</div></figure>
<p>Under the hood, a Gaussian mixture model is very similar to <em>k</em>-means:
it uses an expectation–maximization approach, which qualitatively does
the following:</p>
<ol>
<li>
<p>Choose starting guesses for the location and shape.</p>
</li>
<li>
<p>Repeat until converged:</p>
<ol>
<li>
<p><em>E-step</em>: For each point, find weights encoding the probability of
membership in each cluster.</p>
</li>
<li>
<p><em>M-step</em>: For each cluster, update
its location, normalization, and shape based on <em>all</em> data points,
making use of the weights.</p>
</li>
</ol>
</li>
</ol>
<p>The result of this is that each cluster is associated not with a
hard-edged sphere, but with a smooth Gaussian model. Just as in the
<em>k</em>-means expectation–maximization approach, this algorithm can
sometimes miss the globally optimal solution, and thus in practice
multiple random initializations are used.</p>
<p class="pagebreak-before less_space">Let’s create a function that will help us visualize the
locations and shapes of the GMM clusters by drawing ellipses based on
the GMM output:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">10</code><code class="p">]:</code> <code class="kn">from</code> <code class="nn">matplotlib.patches</code> <code class="kn">import</code> <code class="n">Ellipse</code>

         <code class="k">def</code> <code class="nf">draw_ellipse</code><code class="p">(</code><code class="n">position</code><code class="p">,</code> <code class="n">covariance</code><code class="p">,</code> <code class="n">ax</code><code class="o">=</code><code class="kc">None</code><code class="p">,</code> <code class="o">**</code><code class="n">kwargs</code><code class="p">):</code>
             <code class="sd">"""Draw an ellipse with a given position and covariance"""</code>
             <code class="n">ax</code> <code class="o">=</code> <code class="n">ax</code> <code class="ow">or</code> <code class="n">plt</code><code class="o">.</code><code class="n">gca</code><code class="p">()</code>

             <code class="c1"># Convert covariance to principal axes</code>
             <code class="k">if</code> <code class="n">covariance</code><code class="o">.</code><code class="n">shape</code> <code class="o">==</code> <code class="p">(</code><code class="mi">2</code><code class="p">,</code> <code class="mi">2</code><code class="p">):</code>
                 <code class="n">U</code><code class="p">,</code> <code class="n">s</code><code class="p">,</code> <code class="n">Vt</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">linalg</code><code class="o">.</code><code class="n">svd</code><code class="p">(</code><code class="n">covariance</code><code class="p">)</code>
                 <code class="n">angle</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">degrees</code><code class="p">(</code><code class="n">np</code><code class="o">.</code><code class="n">arctan2</code><code class="p">(</code><code class="n">U</code><code class="p">[</code><code class="mi">1</code><code class="p">,</code> <code class="mi">0</code><code class="p">],</code> <code class="n">U</code><code class="p">[</code><code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">]))</code>
                 <code class="n">width</code><code class="p">,</code> <code class="n">height</code> <code class="o">=</code> <code class="mi">2</code> <code class="o">*</code> <code class="n">np</code><code class="o">.</code><code class="n">sqrt</code><code class="p">(</code><code class="n">s</code><code class="p">)</code>
             <code class="k">else</code><code class="p">:</code>
                 <code class="n">angle</code> <code class="o">=</code> <code class="mi">0</code>
                 <code class="n">width</code><code class="p">,</code> <code class="n">height</code> <code class="o">=</code> <code class="mi">2</code> <code class="o">*</code> <code class="n">np</code><code class="o">.</code><code class="n">sqrt</code><code class="p">(</code><code class="n">covariance</code><code class="p">)</code>

             <code class="c1"># Draw the ellipse</code>
             <code class="k">for</code> <code class="n">nsig</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="mi">4</code><code class="p">):</code>
                 <code class="n">ax</code><code class="o">.</code><code class="n">add_patch</code><code class="p">(</code><code class="n">Ellipse</code><code class="p">(</code><code class="n">position</code><code class="p">,</code> <code class="n">nsig</code> <code class="o">*</code> <code class="n">width</code><code class="p">,</code> <code class="n">nsig</code> <code class="o">*</code> <code class="n">height</code><code class="p">,</code>
                                      <code class="n">angle</code><code class="p">,</code> <code class="o">**</code><code class="n">kwargs</code><code class="p">))</code>

         <code class="k">def</code> <code class="nf">plot_gmm</code><code class="p">(</code><code class="n">gmm</code><code class="p">,</code> <code class="n">X</code><code class="p">,</code> <code class="n">label</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code> <code class="n">ax</code><code class="o">=</code><code class="kc">None</code><code class="p">):</code>
             <code class="n">ax</code> <code class="o">=</code> <code class="n">ax</code> <code class="ow">or</code> <code class="n">plt</code><code class="o">.</code><code class="n">gca</code><code class="p">()</code>
             <code class="n">labels</code> <code class="o">=</code> <code class="n">gmm</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">)</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X</code><code class="p">)</code>
             <code class="k">if</code> <code class="n">label</code><code class="p">:</code>
                 <code class="n">ax</code><code class="o">.</code><code class="n">scatter</code><code class="p">(</code><code class="n">X</code><code class="p">[:,</code> <code class="mi">0</code><code class="p">],</code> <code class="n">X</code><code class="p">[:,</code> <code class="mi">1</code><code class="p">],</code> <code class="n">c</code><code class="o">=</code><code class="n">labels</code><code class="p">,</code> <code class="n">s</code><code class="o">=</code><code class="mi">40</code><code class="p">,</code> <code class="n">cmap</code><code class="o">=</code><code class="s1">'viridis'</code><code class="p">,</code>
                            <code class="n">zorder</code><code class="o">=</code><code class="mi">2</code><code class="p">)</code>
             <code class="k">else</code><code class="p">:</code>
                 <code class="n">ax</code><code class="o">.</code><code class="n">scatter</code><code class="p">(</code><code class="n">X</code><code class="p">[:,</code> <code class="mi">0</code><code class="p">],</code> <code class="n">X</code><code class="p">[:,</code> <code class="mi">1</code><code class="p">],</code> <code class="n">s</code><code class="o">=</code><code class="mi">40</code><code class="p">,</code> <code class="n">zorder</code><code class="o">=</code><code class="mi">2</code><code class="p">)</code>
             <code class="n">ax</code><code class="o">.</code><code class="n">axis</code><code class="p">(</code><code class="s1">'equal'</code><code class="p">)</code>

             <code class="n">w_factor</code> <code class="o">=</code> <code class="mf">0.2</code> <code class="o">/</code> <code class="n">gmm</code><code class="o">.</code><code class="n">weights_</code><code class="o">.</code><code class="n">max</code><code class="p">()</code>
             <code class="k">for</code> <code class="n">pos</code><code class="p">,</code> <code class="n">covar</code><code class="p">,</code> <code class="n">w</code> <code class="ow">in</code> <code class="nb">zip</code><code class="p">(</code><code class="n">gmm</code><code class="o">.</code><code class="n">means_</code><code class="p">,</code> <code class="n">gmm</code><code class="o">.</code><code class="n">covariances_</code><code class="p">,</code> <code class="n">gmm</code><code class="o">.</code><code class="n">weights_</code><code class="p">):</code>
                 <code class="n">draw_ellipse</code><code class="p">(</code><code class="n">pos</code><code class="p">,</code> <code class="n">covar</code><code class="p">,</code> <code class="n">alpha</code><code class="o">=</code><code class="n">w</code> <code class="o">*</code> <code class="n">w_factor</code><code class="p">)</code></pre>
<p>With this in place, we can take a look at what the four-component GMM
gives us for our initial data (see <a data-type="xref" href="#fig_0512-gaussian-mixtures_files_in_output_21_0">Figure 48-6</a>).</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">11</code><code class="p">]:</code> <code class="n">gmm</code> <code class="o">=</code> <code class="n">GaussianMixture</code><code class="p">(</code><code class="n">n_components</code><code class="o">=</code><code class="mi">4</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">)</code>
         <code class="n">plot_gmm</code><code class="p">(</code><code class="n">gmm</code><code class="p">,</code> <code class="n">X</code><code class="p">)</code></pre>
<figure><div class="figure" id="fig_0512-gaussian-mixtures_files_in_output_21_0">
<img alt="output 21 0" height="392" src="assets/output_21_0.png" width="600"/>
<h6><span class="label">Figure 48-6. </span>The four-component GMM in the presence of circular clusters</h6>
</div></figure>
<p>Similarly, we can use the GMM approach to fit our stretched dataset;
allowing for a full covariance the model will fit even very oblong,
stretched-out clusters, as we can see in <a data-type="xref" href="#fig_0512-gaussian-mixtures_files_in_output_23_0">Figure 48-7</a>.</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">12</code><code class="p">]:</code> <code class="n">gmm</code> <code class="o">=</code> <code class="n">GaussianMixture</code><code class="p">(</code><code class="n">n_components</code><code class="o">=</code><code class="mi">4</code><code class="p">,</code> <code class="n">covariance_type</code><code class="o">=</code><code class="s1">'full'</code><code class="p">,</code>
                               <code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">)</code>
         <code class="n">plot_gmm</code><code class="p">(</code><code class="n">gmm</code><code class="p">,</code> <code class="n">X_stretched</code><code class="p">)</code></pre>
<figure><div class="figure" id="fig_0512-gaussian-mixtures_files_in_output_23_0">
<img alt="output 23 0" height="398" src="assets/output_23_0.png" width="600"/>
<h6><span class="label">Figure 48-7. </span>The four-component GMM in the presence of noncircular clusters</h6>
</div></figure>
<p>This makes clear that GMMs address the two main practical issues with
<em>k</em>-means encountered before.<a data-startref="ix_ch48-asciidoc6" data-type="indexterm" id="idm45858720766016"/><a data-startref="ix_ch48-asciidoc5" data-type="indexterm" id="idm45858720765216"/></p>
</div></section>
<section data-pdf-bookmark="Choosing the Covariance Type" data-type="sect1"><div class="sect1" id="ch_0512-gaussian-mixtures_choosing-the-covariance-type">
<h1>Choosing the Covariance Type</h1>
<p><a data-primary="Gaussian mixture models (GMMs)" data-secondary="choosing covariance type" data-type="indexterm" id="idm45858720763552"/>If you look at the details of the preceding fits, you will see that the
<code>covariance_type</code> option was set differently within each. This
hyperparameter controls the degrees of freedom in the shape of each
cluster; it’s essential to set this carefully for any given problem.
The default is <code>covariance_type="diag"</code>, which means that the size of
the cluster along each dimension can be set independently, with the
resulting ellipse constrained to align with the axes. 
<span class="keep-together"><code>covariance_type="spherical"</code></span> is a slightly simpler
and faster model, which constrains the
shape of the cluster such that all dimensions are equal. The resulting
clustering will have similar characteristics to that of <em>k</em>-means,
though it’s not entirely equivalent. A more complicated and
computationally expensive model (especially as the number of dimensions
grows) is to use <code>covariance_type="full"</code>, which allows each cluster to
be modeled as an ellipse with arbitrary orientation. <a data-type="xref" href="#fig_images_in_0512-covariance-type">Figure 48-8</a> represents these three choices for a single cluster.</p>
<figure><div class="figure" id="fig_images_in_0512-covariance-type">
<img alt="05.12 covariance type" height="180" src="assets/05.12-covariance-type.png" width="600"/>
<h6><span class="label">Figure 48-8. </span>Visualization of GMM covariance types<sup><a data-type="noteref" href="ch48.xhtml#idm45858720757328" id="idm45858720757328-marker">1</a></sup></h6>
</div></figure>
</div></section>
<section data-pdf-bookmark="Gaussian Mixture Models as Density Estimation" data-type="sect1"><div class="sect1" id="ch_0512-gaussian-mixtures_gaussian-mixture-models-as-density-estimation">
<h1>Gaussian Mixture Models as Density Estimation</h1>
<p><a data-primary="density estimator" data-secondary="Gaussian mixture models" data-type="indexterm" id="ix_ch48-asciidoc7"/><a data-primary="Gaussian mixture models (GMMs)" data-secondary="density estimation algorithm" data-type="indexterm" id="ix_ch48-asciidoc8"/>Though the GMM is often categorized as a clustering algorithm,
fundamentally it is an algorithm for <em>density estimation</em>. That is to
say, the result of a GMM fit to some data is technically not a
clustering model, but a generative probabilistic model describing the
distribution of the data.</p>
<p>As an example, consider some data generated from
Scikit-Learn’s <code>make_moons</code> function, introduced in
<a data-type="xref" href="ch47.xhtml#section-0511-k-means">Chapter 47</a> (see <a data-type="xref" href="#fig_0512-gaussian-mixtures_files_in_output_28_0">Figure 48-9</a>).</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">13</code><code class="p">]:</code> <code class="kn">from</code> <code class="nn">sklearn.datasets</code> <code class="kn">import</code> <code class="n">make_moons</code>
         <code class="n">Xmoon</code><code class="p">,</code> <code class="n">ymoon</code> <code class="o">=</code> <code class="n">make_moons</code><code class="p">(</code><code class="mi">200</code><code class="p">,</code> <code class="n">noise</code><code class="o">=</code><code class="mf">.05</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code>
         <code class="n">plt</code><code class="o">.</code><code class="n">scatter</code><code class="p">(</code><code class="n">Xmoon</code><code class="p">[:,</code> <code class="mi">0</code><code class="p">],</code> <code class="n">Xmoon</code><code class="p">[:,</code> <code class="mi">1</code><code class="p">]);</code></pre>
<figure><div class="figure" id="fig_0512-gaussian-mixtures_files_in_output_28_0">
<img alt="output 28 0" height="685" src="assets/output_28_0.png" width="600"/>
<h6><span class="label">Figure 48-9. </span>GMM applied to clusters with nonlinear boundaries</h6>
</div></figure>
<p>If we try to fit this with a two-component GMM viewed as a clustering
model, the results are not particularly useful (see <a data-type="xref" href="#fig_0512-gaussian-mixtures_files_in_output_30_0">Figure 48-10</a>).</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">14</code><code class="p">]:</code> <code class="n">gmm2</code> <code class="o">=</code> <code class="n">GaussianMixture</code><code class="p">(</code><code class="n">n_components</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code> <code class="n">covariance_type</code><code class="o">=</code><code class="s1">'full'</code><code class="p">,</code>
                                <code class="n">random_state</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code>
         <code class="n">plot_gmm</code><code class="p">(</code><code class="n">gmm2</code><code class="p">,</code> <code class="n">Xmoon</code><code class="p">)</code></pre>
<figure><div class="figure" id="fig_0512-gaussian-mixtures_files_in_output_30_0">
<img alt="output 30 0" height="377" src="assets/output_30_0.png" width="600"/>
<h6><span class="label">Figure 48-10. </span>Two-component GMM fit to nonlinear clusters</h6>
</div></figure>
<p>But if we instead use many more components and ignore the cluster
labels, we find a fit that is much closer to the input data (see <a data-type="xref" href="#fig_0512-gaussian-mixtures_files_in_output_32_0">Figure 48-11</a>).</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">15</code><code class="p">]:</code> <code class="n">gmm16</code> <code class="o">=</code> <code class="n">GaussianMixture</code><code class="p">(</code><code class="n">n_components</code><code class="o">=</code><code class="mi">16</code><code class="p">,</code> <code class="n">covariance_type</code><code class="o">=</code><code class="s1">'full'</code><code class="p">,</code>
                                 <code class="n">random_state</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code>
         <code class="n">plot_gmm</code><code class="p">(</code><code class="n">gmm16</code><code class="p">,</code> <code class="n">Xmoon</code><code class="p">,</code> <code class="n">label</code><code class="o">=</code><code class="kc">False</code><code class="p">)</code></pre>
<figure><div class="figure" id="fig_0512-gaussian-mixtures_files_in_output_32_0">
<img alt="output 32 0" height="377" src="assets/output_32_0.png" width="600"/>
<h6><span class="label">Figure 48-11. </span>Using many GMM clusters to model the distribution of points</h6>
</div></figure>
<p>Here the mixture of 16 Gaussian components serves not to find separated
clusters of data, but rather to model the overall <em>distribution</em> of the
input data. This is a generative model of the distribution, meaning that
the GMM gives us the recipe to generate new random data distributed
similarly to our input. For example, here are 400 new points drawn from
this 16-component GMM fit to our original data (see <a data-type="xref" href="#fig_0512-gaussian-mixtures_files_in_output_34_0">Figure 48-12</a>).</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">16</code><code class="p">]:</code> <code class="n">Xnew</code><code class="p">,</code> <code class="n">ynew</code> <code class="o">=</code> <code class="n">gmm16</code><code class="o">.</code><code class="n">sample</code><code class="p">(</code><code class="mi">400</code><code class="p">)</code>
         <code class="n">plt</code><code class="o">.</code><code class="n">scatter</code><code class="p">(</code><code class="n">Xnew</code><code class="p">[:,</code> <code class="mi">0</code><code class="p">],</code> <code class="n">Xnew</code><code class="p">[:,</code> <code class="mi">1</code><code class="p">]);</code></pre>
<figure><div class="figure" id="fig_0512-gaussian-mixtures_files_in_output_34_0">
<img alt="output 34 0" height="685" src="assets/output_34_0.png" width="600"/>
<h6><span class="label">Figure 48-12. </span>New data drawn from the 16-component GMM</h6>
</div></figure>
<p>A GMM is convenient as a flexible means of modeling an arbitrary
multidimensional distribution of data.</p>
<p><a data-primary="Gaussian mixture models (GMMs)" data-secondary="correcting overfitting with" data-type="indexterm" id="idm45858720509584"/>The fact that a GMM is a generative model gives us a natural means of
determining the optimal number of components for a given dataset. A
generative model is inherently a probability distribution for the
dataset, and so we can simply evaluate the <em>likelihood</em> of the data
under the model, using cross-validation to avoid overfitting. <a data-primary="Bayesian information criterion (BIC)" data-type="indexterm" id="idm45858720508000"/>Another
means of correcting for overfitting is to adjust the model likelihoods
using some analytic criterion such as <a data-primary="Akaike information criterion (AIC)" data-type="indexterm" id="ix_ch48-asciidoc9"/>the
<a href="https://oreil.ly/BmH9X">Akaike
information criterion (AIC)</a> or the
<a href="https://oreil.ly/Ewivh">Bayesian
information criterion (BIC)</a>. Scikit-Learn’s
<code>GaussianMixture</code> estimator actually includes built-in methods that
compute both of these, so it is very easy to operate using this
approach.</p>
<p>Let’s look at the AIC and BIC versus the number of GMM
components for our moons dataset (see <a data-type="xref" href="#fig_0512-gaussian-mixtures_files_in_output_37_0">Figure 48-13</a>).</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">17</code><code class="p">]:</code> <code class="n">n_components</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">arange</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="mi">21</code><code class="p">)</code>
         <code class="n">models</code> <code class="o">=</code> <code class="p">[</code><code class="n">GaussianMixture</code><code class="p">(</code><code class="n">n</code><code class="p">,</code> <code class="n">covariance_type</code><code class="o">=</code><code class="s1">'full'</code><code class="p">,</code>
                                   <code class="n">random_state</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">Xmoon</code><code class="p">)</code>
                   <code class="k">for</code> <code class="n">n</code> <code class="ow">in</code> <code class="n">n_components</code><code class="p">]</code>

         <code class="n">plt</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">n_components</code><code class="p">,</code> <code class="p">[</code><code class="n">m</code><code class="o">.</code><code class="n">bic</code><code class="p">(</code><code class="n">Xmoon</code><code class="p">)</code> <code class="k">for</code> <code class="n">m</code> <code class="ow">in</code> <code class="n">models</code><code class="p">],</code> <code class="n">label</code><code class="o">=</code><code class="s1">'BIC'</code><code class="p">)</code>
         <code class="n">plt</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">n_components</code><code class="p">,</code> <code class="p">[</code><code class="n">m</code><code class="o">.</code><code class="n">aic</code><code class="p">(</code><code class="n">Xmoon</code><code class="p">)</code> <code class="k">for</code> <code class="n">m</code> <code class="ow">in</code> <code class="n">models</code><code class="p">],</code> <code class="n">label</code><code class="o">=</code><code class="s1">'AIC'</code><code class="p">)</code>
         <code class="n">plt</code><code class="o">.</code><code class="n">legend</code><code class="p">(</code><code class="n">loc</code><code class="o">=</code><code class="s1">'best'</code><code class="p">)</code>
         <code class="n">plt</code><code class="o">.</code><code class="n">xlabel</code><code class="p">(</code><code class="s1">'n_components'</code><code class="p">);</code></pre>
<figure><div class="figure" id="fig_0512-gaussian-mixtures_files_in_output_37_0">
<img alt="output 37 0" height="413" src="assets/output_37_0.png" width="600"/>
<h6><span class="label">Figure 48-13. </span>Visualization of AIC and BIC for choosing the number of GMM 
<span class="keep-together">components</span></h6>
</div></figure>
<p>The optimal number of clusters is the value that minimizes the AIC or
BIC, depending on which approximation we wish to use. The AIC tells us
that our choice of 16 components earlier was probably too many: around
8–12 components would have been a better choice. As is typical with this
sort of problem, the BIC recommends a simpler model.</p>
<p>Notice the important point: this choice of number of components measures
how well a GMM works <em>as a density estimator</em>, not how well it works <em>as
a clustering algorithm</em>. I’d encourage you to think of the
GMM primarily as a density estimator, and use it for clustering only
when warranted within simple datasets.<a data-startref="ix_ch48-asciidoc9" data-type="indexterm" id="idm45858720363280"/><a data-startref="ix_ch48-asciidoc8" data-type="indexterm" id="idm45858720362608"/></p>
</div></section>
<section data-pdf-bookmark="Example: GMMs for Generating New Data" data-type="sect1"><div class="sect1" id="ch_0512-gaussian-mixtures_example-gmms-for-generating-new-data">
<h1>Example: GMMs for Generating New Data</h1>
<p><a data-primary="Gaussian mixture models (GMMs)" data-secondary="handwritten data generation example" data-type="indexterm" id="ix_ch48-asciidoc10"/><a data-primary="optical character recognition" data-secondary="Gaussian mixture models" data-type="indexterm" id="ix_ch48-asciidoc11"/>We just saw a simple example of using a GMM as a generative model in
order to create new samples from the distribution defined by the input
data. Here we will run with this idea and generate <em>new handwritten
digits</em> from the standard digits corpus that we have used before.</p>
<p>To start with, let’s load the digits data using
Scikit-Learn’s data tools:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">18</code><code class="p">]:</code> <code class="kn">from</code> <code class="nn">sklearn.datasets</code> <code class="kn">import</code> <code class="n">load_digits</code>
         <code class="n">digits</code> <code class="o">=</code> <code class="n">load_digits</code><code class="p">()</code>
         <code class="n">digits</code><code class="o">.</code><code class="n">data</code><code class="o">.</code><code class="n">shape</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">18</code><code class="p">]:</code> <code class="p">(</code><code class="mi">1797</code><code class="p">,</code> <code class="mi">64</code><code class="p">)</code></pre>
<p>Next, let’s plot the first 50 of these to recall exactly
what we’re looking at (see <a data-type="xref" href="#fig_0512-gaussian-mixtures_files_in_output_42_0">Figure 48-14</a>).</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">19</code><code class="p">]:</code> <code class="k">def</code> <code class="nf">plot_digits</code><code class="p">(</code><code class="n">data</code><code class="p">):</code>
             <code class="n">fig</code><code class="p">,</code> <code class="n">ax</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">(</code><code class="mi">5</code><code class="p">,</code> <code class="mi">10</code><code class="p">,</code> <code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">8</code><code class="p">,</code> <code class="mi">4</code><code class="p">),</code>
                                    <code class="n">subplot_kw</code><code class="o">=</code><code class="nb">dict</code><code class="p">(</code><code class="n">xticks</code><code class="o">=</code><code class="p">[],</code> <code class="n">yticks</code><code class="o">=</code><code class="p">[]))</code>
             <code class="n">fig</code><code class="o">.</code><code class="n">subplots_adjust</code><code class="p">(</code><code class="n">hspace</code><code class="o">=</code><code class="mf">0.05</code><code class="p">,</code> <code class="n">wspace</code><code class="o">=</code><code class="mf">0.05</code><code class="p">)</code>
             <code class="k">for</code> <code class="n">i</code><code class="p">,</code> <code class="n">axi</code> <code class="ow">in</code> <code class="nb">enumerate</code><code class="p">(</code><code class="n">ax</code><code class="o">.</code><code class="n">flat</code><code class="p">):</code>
                 <code class="n">im</code> <code class="o">=</code> <code class="n">axi</code><code class="o">.</code><code class="n">imshow</code><code class="p">(</code><code class="n">data</code><code class="p">[</code><code class="n">i</code><code class="p">]</code><code class="o">.</code><code class="n">reshape</code><code class="p">(</code><code class="mi">8</code><code class="p">,</code> <code class="mi">8</code><code class="p">),</code> <code class="n">cmap</code><code class="o">=</code><code class="s1">'binary'</code><code class="p">)</code>
                 <code class="n">im</code><code class="o">.</code><code class="n">set_clim</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="mi">16</code><code class="p">)</code>
         <code class="n">plot_digits</code><code class="p">(</code><code class="n">digits</code><code class="o">.</code><code class="n">data</code><code class="p">)</code></pre>
<figure><div class="figure" id="fig_0512-gaussian-mixtures_files_in_output_42_0">
<img alt="output 42 0" height="293" src="assets/output_42_0.png" width="600"/>
<h6><span class="label">Figure 48-14. </span>Handwritten digits input</h6>
</div></figure>
<p>We have nearly 1,800 digits in 64 dimensions, and we can build a GMM on
top of these to generate more. GMMs can have difficulty converging in
such a high-dimensional space, so we will start with an invertible
dimensionality reduction algorithm on the data. Here we will use a
straightforward PCA, asking it to preserve 99% of the variance in the
projected data:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">20</code><code class="p">]:</code> <code class="kn">from</code> <code class="nn">sklearn.decomposition</code> <code class="kn">import</code> <code class="n">PCA</code>
         <code class="n">pca</code> <code class="o">=</code> <code class="n">PCA</code><code class="p">(</code><code class="mf">0.99</code><code class="p">,</code> <code class="n">whiten</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>
         <code class="n">data</code> <code class="o">=</code> <code class="n">pca</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">digits</code><code class="o">.</code><code class="n">data</code><code class="p">)</code>
         <code class="n">data</code><code class="o">.</code><code class="n">shape</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">20</code><code class="p">]:</code> <code class="p">(</code><code class="mi">1797</code><code class="p">,</code> <code class="mi">41</code><code class="p">)</code></pre>
<p>The result is 41 dimensions, a reduction of nearly 1/3 with almost no
information loss. Given this projected data, let’s use the
AIC to get a gauge for the number of GMM components we should use (see
<a data-type="xref" href="#fig_0512-gaussian-mixtures_files_in_output_46_0">Figure 48-15</a>).</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">21</code><code class="p">]:</code> <code class="n">n_components</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">arange</code><code class="p">(</code><code class="mi">50</code><code class="p">,</code> <code class="mi">210</code><code class="p">,</code> <code class="mi">10</code><code class="p">)</code>
         <code class="n">models</code> <code class="o">=</code> <code class="p">[</code><code class="n">GaussianMixture</code><code class="p">(</code><code class="n">n</code><code class="p">,</code> <code class="n">covariance_type</code><code class="o">=</code><code class="s1">'full'</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code>
                   <code class="k">for</code> <code class="n">n</code> <code class="ow">in</code> <code class="n">n_components</code><code class="p">]</code>
         <code class="n">aics</code> <code class="o">=</code> <code class="p">[</code><code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">data</code><code class="p">)</code><code class="o">.</code><code class="n">aic</code><code class="p">(</code><code class="n">data</code><code class="p">)</code> <code class="k">for</code> <code class="n">model</code> <code class="ow">in</code> <code class="n">models</code><code class="p">]</code>
         <code class="n">plt</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">n_components</code><code class="p">,</code> <code class="n">aics</code><code class="p">);</code></pre>
<figure><div class="figure" id="fig_0512-gaussian-mixtures_files_in_output_46_0">
<img alt="output 46 0" height="363" src="assets/output_46_0.png" width="600"/>
<h6><span class="label">Figure 48-15. </span>AIC curve for choosing the appropriate number of GMM components</h6>
</div></figure>
<p>It appears that around 140 components minimizes the AIC; we will use
this model. Let’s quickly fit this to the data and confirm
that it has converged:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">22</code><code class="p">]:</code> <code class="n">gmm</code> <code class="o">=</code> <code class="n">GaussianMixture</code><code class="p">(</code><code class="mi">140</code><code class="p">,</code> <code class="n">covariance_type</code><code class="o">=</code><code class="s1">'full'</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code>
         <code class="n">gmm</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">data</code><code class="p">)</code>
         <code class="nb">print</code><code class="p">(</code><code class="n">gmm</code><code class="o">.</code><code class="n">converged_</code><code class="p">)</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">22</code><code class="p">]:</code> <code class="kc">True</code></pre>
<p>Now<a data-startref="ix_ch48-asciidoc11" data-type="indexterm" id="idm45858719936848"/> we can draw samples of 100 new points within this 41-dimensional
projected space, using the GMM as a generative model:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">23</code><code class="p">]:</code> <code class="n">data_new</code><code class="p">,</code> <code class="n">label_new</code> <code class="o">=</code> <code class="n">gmm</code><code class="o">.</code><code class="n">sample</code><code class="p">(</code><code class="mi">100</code><code class="p">)</code>
         <code class="n">data_new</code><code class="o">.</code><code class="n">shape</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">23</code><code class="p">]:</code> <code class="p">(</code><code class="mi">100</code><code class="p">,</code> <code class="mi">41</code><code class="p">)</code></pre>
<p>Finally, we can use the inverse transform of the PCA object to construct
the new digits (see <a data-type="xref" href="#fig_0512-gaussian-mixtures_files_in_output_52_0">Figure 48-16</a>).</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">24</code><code class="p">]:</code> <code class="n">digits_new</code> <code class="o">=</code> <code class="n">pca</code><code class="o">.</code><code class="n">inverse_transform</code><code class="p">(</code><code class="n">data_new</code><code class="p">)</code>
         <code class="n">plot_digits</code><code class="p">(</code><code class="n">digits_new</code><code class="p">)</code></pre>
<figure><div class="figure" id="fig_0512-gaussian-mixtures_files_in_output_52_0">
<img alt="output 52 0" height="293" src="assets/output_52_0.png" width="600"/>
<h6><span class="label">Figure 48-16. </span>“New” digits randomly drawn from the underlying model of the GMM estimator</h6>
</div></figure>
<p>The results for the most part look like plausible digits from the
dataset!</p>
<p>Consider what we’ve done here: given a sampling of
handwritten digits, we have modeled the distribution of that data in
such a way that we can generate brand new samples of digits from the
data: these are “handwritten digits,” which do not individually appear
in the original dataset, but rather capture the general features of the
input data as modeled by the mixture model. Such a generative model of
digits can prove very useful as a component of a Bayesian generative
classifier, as we shall see in the next chapter<a data-startref="ix_ch48-asciidoc10" data-type="indexterm" id="idm45858719816624"/><a data-startref="ix_ch48-asciidoc7" data-type="indexterm" id="idm45858719815888"/>.<a data-startref="ix_ch48-asciidoc2" data-type="indexterm" id="idm45858719815088"/><a data-startref="ix_ch48-asciidoc1" data-type="indexterm" id="idm45858719814384"/><a data-startref="ix_ch48-asciidoc0" data-type="indexterm" id="idm45858719813712"/></p>
</div></section>
<div data-type="footnotes"><p data-type="footnote" id="idm45858720757328"><sup><a href="ch48.xhtml#idm45858720757328-marker">1</a></sup> Code to produce this figure can be found in the <a href="https://oreil.ly/MLsk8">online appendix</a>.</p></div></div></section></div></body></html>