<html><head></head><body><section data-pdf-bookmark="Chapter 17. Decision Trees" data-type="chapter" epub:type="chapter"><div class="chapter" id="decision_trees">&#13;
<h1><span class="label">Chapter 17. </span>Decision Trees</h1>&#13;
&#13;
<blockquote data-type="epigraph" epub:type="epigraph">&#13;
    <p>A tree is an incomprehensible mystery.</p>&#13;
    <p data-type="attribution">Jim Woodring</p>&#13;
</blockquote>&#13;
&#13;
<p>DataSciencester’s<a data-primary="predictive models" data-secondary="decision trees" data-type="indexterm" id="PMdecision17"/> VP of Talent has interviewed a number of job candidates from the site, with varying degrees of success.  He’s collected a dataset consisting of several (qualitative) attributes of each candidate, as well as whether that candidate interviewed well or poorly.  Could you, he asks, use this data to build a model identifying which candidates will interview well, so that he doesn’t have to waste time conducting interviews?</p>&#13;
&#13;
<p>This seems like a good fit for a <em>decision tree</em>, another predictive modeling tool in the data scientist’s kit.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="What Is a Decision Tree?" data-type="sect1"><div class="sect1" id="idm45635734759912">&#13;
<h1>What Is a Decision Tree?</h1>&#13;
&#13;
<p>A<a data-primary="decision trees" data-secondary="decision paths in" data-type="indexterm" id="idm45635734758168"/> decision tree uses a tree structure to represent a number of possible <em>decision paths</em> and an outcome for each path.</p>&#13;
&#13;
<p>If you have ever played the game <a href="http://en.wikipedia.org/wiki/Twenty_Questions">Twenty Questions</a>, then you are familiar with decision trees. For example:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>“I am thinking of an animal.”</p>&#13;
</li>&#13;
<li>&#13;
<p>“Does it have more than five legs?”</p>&#13;
</li>&#13;
<li>&#13;
<p>“No.”</p>&#13;
</li>&#13;
<li>&#13;
<p>“Is it delicious?”</p>&#13;
</li>&#13;
<li>&#13;
<p>“No.”</p>&#13;
</li>&#13;
<li>&#13;
<p>“Does it appear on the back of the Australian five-cent coin?”</p>&#13;
</li>&#13;
<li>&#13;
<p>“Yes.”</p>&#13;
</li>&#13;
<li>&#13;
<p>“Is it an echidna?”</p>&#13;
</li>&#13;
<li>&#13;
<p>“Yes, it is!”</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>This corresponds to the path:</p>&#13;
&#13;
<p>“Not more than 5 legs” → “Not delicious” → “On the 5-cent coin” → “Echidna!”</p>&#13;
&#13;
<p>in an idiosyncratic (and not very comprehensive) “guess the animal” decision tree (<a data-type="xref" href="#guess_the_animal">Figure 17-1</a>).</p>&#13;
&#13;
<figure><div class="figure" id="guess_the_animal">&#13;
<img alt="Guess the animal." src="assets/dsf2_1701.png"/>&#13;
<h6><span class="label">Figure 17-1. </span>A “guess the animal” decision tree</h6>&#13;
</div></figure>&#13;
&#13;
<p>Decision<a data-primary="decision trees" data-secondary="benefits and drawbacks of" data-type="indexterm" id="idm45635734634904"/> trees have a lot to recommend them.  They’re very easy to understand and interpret, and the process by which they reach a prediction is completely transparent.  Unlike the other models we’ve looked at so far, decision trees can easily handle a mix of numeric (e.g., number of legs) and categorical (e.g., delicious/not delicious) attributes and can even classify data for which attributes are missing.</p>&#13;
&#13;
<p>At the same time, finding an “optimal” decision tree for a set of training data is computationally a very hard problem.  (We will get around this by trying to build a good-enough tree rather than an optimal one, although for large datasets this can still be a lot of work.)  More important, it is very easy (and very bad) to build decision trees that are <em>overfitted</em> to the training data, and that don’t generalize well to unseen data.  We’ll look at ways to address this.</p>&#13;
&#13;
<p>Most<a data-primary="decision trees" data-secondary="types of" data-type="indexterm" id="idm45635734631576"/><a data-primary="classification trees" data-type="indexterm" id="idm45635734630568"/><a data-primary="regression trees" data-type="indexterm" id="idm45635734629896"/> people divide decision trees into <em>classification trees</em> (which produce categorical outputs) and <em>regression trees</em> (which produce numeric outputs).  In this chapter, we’ll focus on classification trees, and we’ll work through the ID3 algorithm for learning a decision tree from a set of labeled data, which should help us understand how decision trees actually work.  To make things simple, we’ll restrict ourselves to problems with binary outputs like “Should I hire this candidate?” or “Should I show this website visitor advertisement A or advertisement B?” or “Will eating this food I found in the office fridge make me sick?”</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Entropy" data-type="sect1"><div class="sect1" id="idm45635734759288">&#13;
<h1>Entropy</h1>&#13;
&#13;
<p>In<a data-primary="decision trees" data-secondary="entropy and" data-type="indexterm" id="idm45635734626120"/><a data-primary="entropy" data-type="indexterm" id="idm45635734625112"/> order to build a decision tree, we will need to decide what questions to ask and in what order.   At each stage of the tree there are some possibilities we’ve eliminated and some that we haven’t.  After learning that an animal doesn’t have more than five legs, we’ve eliminated the possibility that it’s a grasshopper.  We haven’t eliminated the possibility that it’s a duck.&#13;
Each possible question partitions the remaining possibilities according to its answer.</p>&#13;
&#13;
<p>Ideally, we’d like to choose questions whose answers give a lot of information about what our tree should predict.  If there’s a single yes/no question for which “yes” answers always correspond to <code>True</code> outputs and “no” answers to <code>False</code> outputs (or vice versa), this would be an awesome question to pick.  Conversely, a yes/no question for which neither answer gives you much new information about what the prediction should be is probably not a good choice.</p>&#13;
&#13;
<p>We capture this notion of “how much information” with <em>entropy</em>.  You have probably heard this term used to mean disorder. We use it to represent the uncertainty associated with data.</p>&#13;
&#13;
<p>Imagine that we have a set <em>S</em> of data, each member of which is labeled as belonging to one of a finite number of classes <math>&#13;
  <mrow>&#13;
    <msub><mi>C</mi> <mn>1</mn> </msub>&#13;
    <mo>,</mo>&#13;
    <mo>...</mo>&#13;
    <mo>,</mo>&#13;
    <msub><mi>C</mi> <mi>n</mi> </msub>&#13;
  </mrow>&#13;
</math>.  If all the data points belong to a single class, then there is no real uncertainty, which means we’d like there to be low entropy.  If the data points are evenly spread across the classes, there is a lot of uncertainty and we’d like there to be high entropy.</p>&#13;
&#13;
<p>In math terms, if <math>&#13;
  <msub><mi>p</mi> <mi>i</mi> </msub>&#13;
</math> is the proportion of data labeled as class <math>&#13;
  <msub><mi>c</mi> <mi>i</mi> </msub>&#13;
</math>, we define the entropy as:</p>&#13;
<div data-type="equation">&#13;
<math alttext="upper H left-parenthesis upper S right-parenthesis equals minus p 1 log Subscript 2 Baseline p 1 minus ellipsis minus p Subscript n Baseline log Subscript 2 Baseline p Subscript n" display="block">&#13;
  <mrow>&#13;
    <mi>H</mi>&#13;
    <mrow>&#13;
      <mo>(</mo>&#13;
      <mi>S</mi>&#13;
      <mo>)</mo>&#13;
    </mrow>&#13;
    <mo>=</mo>&#13;
    <mo>-</mo>&#13;
    <msub><mi>p</mi> <mn>1</mn> </msub>&#13;
    <msub><mo form="prefix">log</mo> <mn>2</mn> </msub>&#13;
    <msub><mi>p</mi> <mn>1</mn> </msub>&#13;
    <mo>-</mo>&#13;
    <mo>...</mo>&#13;
    <mo>-</mo>&#13;
    <msub><mi>p</mi> <mi>n</mi> </msub>&#13;
    <msub><mo form="prefix">log</mo> <mn>2</mn> </msub>&#13;
    <msub><mi>p</mi> <mi>n</mi> </msub>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
&#13;
<p>with the (standard) convention that <math>&#13;
  <mrow>&#13;
    <mn>0</mn>&#13;
    <mo form="prefix">log</mo>&#13;
    <mn>0</mn>&#13;
    <mo>=</mo>&#13;
    <mn>0</mn>&#13;
  </mrow>&#13;
</math>.</p>&#13;
&#13;
<p>Without worrying too much about the grisly details, each term <math>&#13;
  <mrow>&#13;
    <mo>-</mo>&#13;
    <msub><mi>p</mi> <mi>i</mi> </msub>&#13;
    <msub><mo form="prefix">log</mo> <mn>2</mn> </msub>&#13;
    <msub><mi>p</mi> <mi>i</mi> </msub>&#13;
  </mrow>&#13;
</math> is non-negative and is close to 0 precisely when <math>&#13;
  <msub><mi>p</mi> <mi>i</mi> </msub>&#13;
</math> is either close to 0 or close to 1 (<a data-type="xref" href="#p_log_p">Figure 17-2</a>).</p>&#13;
&#13;
<figure><div class="figure" id="p_log_p">&#13;
<img alt="A graph of –p log p." src="assets/dsf2_1702.png"/>&#13;
<h6><span class="label">Figure 17-2. </span>A graph of -p log p</h6>&#13;
</div></figure>&#13;
&#13;
<p>This means the entropy will be small when every <math>&#13;
  <msub><mi>p</mi> <mi>i</mi> </msub>&#13;
</math> is close to 0 or 1 (i.e., when most of the data is in a single class), and it will be larger when many of the <math>&#13;
  <msub><mi>p</mi> <mi>i</mi> </msub>&#13;
</math>’s are not close to 0 (i.e., when the data is spread across multiple classes).  This is exactly the behavior we desire.</p>&#13;
&#13;
<p>It is easy enough to roll all of this into a function:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="kn">from</code> <code class="nn">typing</code> <code class="kn">import</code> <code class="n">List</code>&#13;
<code class="kn">import</code> <code class="nn">math</code>&#13;
&#13;
<code class="k">def</code> <code class="nf">entropy</code><code class="p">(</code><code class="n">class_probabilities</code><code class="p">:</code> <code class="n">List</code><code class="p">[</code><code class="nb">float</code><code class="p">])</code> <code class="o">-&gt;</code> <code class="nb">float</code><code class="p">:</code>&#13;
    <code class="sd">"""Given a list of class probabilities, compute the entropy"""</code>&#13;
    <code class="k">return</code> <code class="nb">sum</code><code class="p">(</code><code class="o">-</code><code class="n">p</code> <code class="o">*</code> <code class="n">math</code><code class="o">.</code><code class="n">log</code><code class="p">(</code><code class="n">p</code><code class="p">,</code> <code class="mi">2</code><code class="p">)</code>&#13;
               <code class="k">for</code> <code class="n">p</code> <code class="ow">in</code> <code class="n">class_probabilities</code>&#13;
               <code class="k">if</code> <code class="n">p</code> <code class="o">&gt;</code> <code class="mi">0</code><code class="p">)</code>                     <code class="c1"># ignore zero probabilities</code>&#13;
&#13;
<code class="k">assert</code> <code class="n">entropy</code><code class="p">([</code><code class="mf">1.0</code><code class="p">])</code> <code class="o">==</code> <code class="mi">0</code>&#13;
<code class="k">assert</code> <code class="n">entropy</code><code class="p">([</code><code class="mf">0.5</code><code class="p">,</code> <code class="mf">0.5</code><code class="p">])</code> <code class="o">==</code> <code class="mi">1</code>&#13;
<code class="k">assert</code> <code class="mf">0.81</code> <code class="o">&lt;</code> <code class="n">entropy</code><code class="p">([</code><code class="mf">0.25</code><code class="p">,</code> <code class="mf">0.75</code><code class="p">])</code> <code class="o">&lt;</code> <code class="mf">0.82</code></pre>&#13;
&#13;
<p>Our data will consist of pairs <code>(input, label)</code>, which means that we’ll need to compute the class probabilities ourselves. Notice that we don’t actually care which label is associated with each probability, only what the probabilities are:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="kn">from</code> <code class="nn">typing</code> <code class="kn">import</code> <code class="n">Any</code>&#13;
<code class="kn">from</code> <code class="nn">collections</code> <code class="kn">import</code> <code class="n">Counter</code>&#13;
&#13;
<code class="k">def</code> <code class="nf">class_probabilities</code><code class="p">(</code><code class="n">labels</code><code class="p">:</code> <code class="n">List</code><code class="p">[</code><code class="n">Any</code><code class="p">])</code> <code class="o">-&gt;</code> <code class="n">List</code><code class="p">[</code><code class="nb">float</code><code class="p">]:</code>&#13;
    <code class="n">total_count</code> <code class="o">=</code> <code class="nb">len</code><code class="p">(</code><code class="n">labels</code><code class="p">)</code>&#13;
    <code class="k">return</code> <code class="p">[</code><code class="n">count</code> <code class="o">/</code> <code class="n">total_count</code>&#13;
            <code class="k">for</code> <code class="n">count</code> <code class="ow">in</code> <code class="n">Counter</code><code class="p">(</code><code class="n">labels</code><code class="p">)</code><code class="o">.</code><code class="n">values</code><code class="p">()]</code>&#13;
&#13;
<code class="k">def</code> <code class="nf">data_entropy</code><code class="p">(</code><code class="n">labels</code><code class="p">:</code> <code class="n">List</code><code class="p">[</code><code class="n">Any</code><code class="p">])</code> <code class="o">-&gt;</code> <code class="nb">float</code><code class="p">:</code>&#13;
    <code class="k">return</code> <code class="n">entropy</code><code class="p">(</code><code class="n">class_probabilities</code><code class="p">(</code><code class="n">labels</code><code class="p">))</code>&#13;
&#13;
<code class="k">assert</code> <code class="n">data_entropy</code><code class="p">([</code><code class="s1">'a'</code><code class="p">])</code> <code class="o">==</code> <code class="mi">0</code>&#13;
<code class="k">assert</code> <code class="n">data_entropy</code><code class="p">([</code><code class="bp">True</code><code class="p">,</code> <code class="bp">False</code><code class="p">])</code> <code class="o">==</code> <code class="mi">1</code>&#13;
<code class="k">assert</code> <code class="n">data_entropy</code><code class="p">([</code><code class="mi">3</code><code class="p">,</code> <code class="mi">4</code><code class="p">,</code> <code class="mi">4</code><code class="p">,</code> <code class="mi">4</code><code class="p">])</code> <code class="o">==</code> <code class="n">entropy</code><code class="p">([</code><code class="mf">0.25</code><code class="p">,</code> <code class="mf">0.75</code><code class="p">])</code></pre>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="The Entropy of a Partition" data-type="sect1"><div class="sect1" id="idm45635734627160">&#13;
<h1>The Entropy of a Partition</h1>&#13;
&#13;
<p>What<a data-primary="decision trees" data-secondary="entropy of partitions" data-type="indexterm" id="idm45635734364440"/> we’ve done so far is compute the entropy (think “uncertainty”) of a single set of labeled data.  Now, each stage of a decision tree involves asking a question whose answer partitions data into one or (hopefully) more subsets.  For instance, our “does it have more than five legs?” question partitions animals into those that have more than five legs (e.g., spiders) and those that don’t (e.g., echidnas).</p>&#13;
&#13;
<p>Correspondingly, we’d like some notion of the entropy that results from partitioning a set of data in a certain way.  We want a partition to have low entropy if it splits the data into subsets that themselves have low entropy (i.e., are highly certain), and high entropy if it contains subsets that (are large and) have high entropy (i.e., are highly uncertain).</p>&#13;
&#13;
<p>For example, my “Australian five-cent coin” question was pretty dumb (albeit pretty lucky!), as it partitioned the remaining animals at that point into <math>&#13;
  <msub><mi>S</mi> <mn>1</mn> </msub>&#13;
</math> = {echidna} and <math>&#13;
  <msub><mi>S</mi> <mn>2</mn> </msub>&#13;
</math> = {everything else}, where <math>&#13;
  <msub><mi>S</mi> <mn>2</mn> </msub>&#13;
</math> is both large and high-entropy.  (<math>&#13;
  <msub><mi>S</mi> <mn>1</mn> </msub>&#13;
</math> has no entropy, but it represents a small fraction of the remaining “classes.”)</p>&#13;
&#13;
<p>Mathematically, if we partition our data <em>S</em> into subsets <math>&#13;
  <mrow>&#13;
    <msub><mi>S</mi> <mn>1</mn> </msub>&#13;
    <mo>,</mo>&#13;
    <mo>...</mo>&#13;
    <mo>,</mo>&#13;
    <msub><mi>S</mi> <mi>m</mi> </msub>&#13;
  </mrow>&#13;
</math> containing proportions <math>&#13;
  <mrow>&#13;
    <msub><mi>q</mi> <mn>1</mn> </msub>&#13;
    <mo>,</mo>&#13;
    <mo>...</mo>&#13;
    <mo>,</mo>&#13;
    <msub><mi>q</mi> <mi>m</mi> </msub>&#13;
  </mrow>&#13;
</math> of the data, then we compute the entropy of the partition as a weighted sum:</p>&#13;
<div data-type="equation">&#13;
<math alttext="upper H equals q 1 upper H left-parenthesis upper S 1 right-parenthesis plus period period period plus q Subscript m Baseline upper H left-parenthesis upper S Subscript m Baseline right-parenthesis" display="block">&#13;
  <mrow>&#13;
    <mi>H</mi>&#13;
    <mo>=</mo>&#13;
    <msub><mi>q</mi> <mn>1</mn> </msub>&#13;
    <mi>H</mi>&#13;
    <mrow>&#13;
      <mo>(</mo>&#13;
      <msub><mi>S</mi> <mn>1</mn> </msub>&#13;
      <mo>)</mo>&#13;
    </mrow>&#13;
    <mo>+</mo>&#13;
    <mo>...</mo>&#13;
    <mo>+</mo>&#13;
    <msub><mi>q</mi> <mi>m</mi> </msub>&#13;
    <mi>H</mi>&#13;
    <mrow>&#13;
      <mo>(</mo>&#13;
      <msub><mi>S</mi> <mi>m</mi> </msub>&#13;
      <mo>)</mo>&#13;
    </mrow>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
&#13;
<p>which we can implement as:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="k">def</code> <code class="nf">partition_entropy</code><code class="p">(</code><code class="n">subsets</code><code class="p">:</code> <code class="n">List</code><code class="p">[</code><code class="n">List</code><code class="p">[</code><code class="n">Any</code><code class="p">]])</code> <code class="o">-&gt;</code> <code class="nb">float</code><code class="p">:</code>&#13;
    <code class="sd">"""Returns the entropy from this partition of data into subsets"""</code>&#13;
    <code class="n">total_count</code> <code class="o">=</code> <code class="nb">sum</code><code class="p">(</code><code class="nb">len</code><code class="p">(</code><code class="n">subset</code><code class="p">)</code> <code class="k">for</code> <code class="n">subset</code> <code class="ow">in</code> <code class="n">subsets</code><code class="p">)</code>&#13;
&#13;
    <code class="k">return</code> <code class="nb">sum</code><code class="p">(</code><code class="n">data_entropy</code><code class="p">(</code><code class="n">subset</code><code class="p">)</code> <code class="o">*</code> <code class="nb">len</code><code class="p">(</code><code class="n">subset</code><code class="p">)</code> <code class="o">/</code> <code class="n">total_count</code>&#13;
               <code class="k">for</code> <code class="n">subset</code> <code class="ow">in</code> <code class="n">subsets</code><code class="p">)</code></pre>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>One problem with this approach is that partitioning by&#13;
an attribute with many different values will result in a very low entropy&#13;
due to overfitting.  For example, imagine you work for a bank and are&#13;
trying to build a decision tree to predict which of your customers are likely&#13;
to default on their mortgages, using some historical data as your training set.&#13;
Imagine further that the dataset contains each customer’s Social Security number.&#13;
Partitioning on SSN will produce one-person subsets, each of which necessarily has&#13;
zero entropy.  But a model that relies on SSN is <em>certain</em> not to generalize&#13;
beyond the training set.  For this reason, you should probably try to avoid&#13;
(or bucket, if appropriate) attributes with large numbers of possible values&#13;
when creating decision trees.</p>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Creating a Decision Tree" data-type="sect1"><div class="sect1" id="idm45635734227912">&#13;
<h1>Creating a Decision Tree</h1>&#13;
&#13;
<p>The<a data-primary="decision trees" data-secondary="creating" data-type="indexterm" id="idm45635734226248"/> VP provides you with the interviewee data, consisting of (per your specification)&#13;
a <code>NamedTuple</code> of the relevant attributes for each candidate—her level, her preferred language, whether she is active on Twitter, whether she has a PhD, and whether she interviewed well:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="kn">from</code> <code class="nn">typing</code> <code class="kn">import</code> <code class="n">NamedTuple</code><code class="p">,</code> <code class="n">Optional</code>&#13;
&#13;
<code class="k">class</code> <code class="nc">Candidate</code><code class="p">(</code><code class="n">NamedTuple</code><code class="p">):</code>&#13;
    <code class="n">level</code><code class="p">:</code> <code class="nb">str</code>&#13;
    <code class="n">lang</code><code class="p">:</code> <code class="nb">str</code>&#13;
    <code class="n">tweets</code><code class="p">:</code> <code class="nb">bool</code>&#13;
    <code class="n">phd</code><code class="p">:</code> <code class="nb">bool</code>&#13;
    <code class="n">did_well</code><code class="p">:</code> <code class="n">Optional</code><code class="p">[</code><code class="nb">bool</code><code class="p">]</code> <code class="o">=</code> <code class="bp">None</code>  <code class="c1"># allow unlabeled data</code>&#13;
&#13;
                  <code class="c1">#  level     lang     tweets  phd  did_well</code>&#13;
<code class="n">inputs</code> <code class="o">=</code> <code class="p">[</code><code class="n">Candidate</code><code class="p">(</code><code class="s1">'Senior'</code><code class="p">,</code> <code class="s1">'Java'</code><code class="p">,</code>   <code class="bp">False</code><code class="p">,</code> <code class="bp">False</code><code class="p">,</code> <code class="bp">False</code><code class="p">),</code>&#13;
          <code class="n">Candidate</code><code class="p">(</code><code class="s1">'Senior'</code><code class="p">,</code> <code class="s1">'Java'</code><code class="p">,</code>   <code class="bp">False</code><code class="p">,</code> <code class="bp">True</code><code class="p">,</code>  <code class="bp">False</code><code class="p">),</code>&#13;
          <code class="n">Candidate</code><code class="p">(</code><code class="s1">'Mid'</code><code class="p">,</code>    <code class="s1">'Python'</code><code class="p">,</code> <code class="bp">False</code><code class="p">,</code> <code class="bp">False</code><code class="p">,</code> <code class="bp">True</code><code class="p">),</code>&#13;
          <code class="n">Candidate</code><code class="p">(</code><code class="s1">'Junior'</code><code class="p">,</code> <code class="s1">'Python'</code><code class="p">,</code> <code class="bp">False</code><code class="p">,</code> <code class="bp">False</code><code class="p">,</code> <code class="bp">True</code><code class="p">),</code>&#13;
          <code class="n">Candidate</code><code class="p">(</code><code class="s1">'Junior'</code><code class="p">,</code> <code class="s1">'R'</code><code class="p">,</code>      <code class="bp">True</code><code class="p">,</code>  <code class="bp">False</code><code class="p">,</code> <code class="bp">True</code><code class="p">),</code>&#13;
          <code class="n">Candidate</code><code class="p">(</code><code class="s1">'Junior'</code><code class="p">,</code> <code class="s1">'R'</code><code class="p">,</code>      <code class="bp">True</code><code class="p">,</code>  <code class="bp">True</code><code class="p">,</code>  <code class="bp">False</code><code class="p">),</code>&#13;
          <code class="n">Candidate</code><code class="p">(</code><code class="s1">'Mid'</code><code class="p">,</code>    <code class="s1">'R'</code><code class="p">,</code>      <code class="bp">True</code><code class="p">,</code>  <code class="bp">True</code><code class="p">,</code>  <code class="bp">True</code><code class="p">),</code>&#13;
          <code class="n">Candidate</code><code class="p">(</code><code class="s1">'Senior'</code><code class="p">,</code> <code class="s1">'Python'</code><code class="p">,</code> <code class="bp">False</code><code class="p">,</code> <code class="bp">False</code><code class="p">,</code> <code class="bp">False</code><code class="p">),</code>&#13;
          <code class="n">Candidate</code><code class="p">(</code><code class="s1">'Senior'</code><code class="p">,</code> <code class="s1">'R'</code><code class="p">,</code>      <code class="bp">True</code><code class="p">,</code>  <code class="bp">False</code><code class="p">,</code> <code class="bp">True</code><code class="p">),</code>&#13;
          <code class="n">Candidate</code><code class="p">(</code><code class="s1">'Junior'</code><code class="p">,</code> <code class="s1">'Python'</code><code class="p">,</code> <code class="bp">True</code><code class="p">,</code>  <code class="bp">False</code><code class="p">,</code> <code class="bp">True</code><code class="p">),</code>&#13;
          <code class="n">Candidate</code><code class="p">(</code><code class="s1">'Senior'</code><code class="p">,</code> <code class="s1">'Python'</code><code class="p">,</code> <code class="bp">True</code><code class="p">,</code>  <code class="bp">True</code><code class="p">,</code>  <code class="bp">True</code><code class="p">),</code>&#13;
          <code class="n">Candidate</code><code class="p">(</code><code class="s1">'Mid'</code><code class="p">,</code>    <code class="s1">'Python'</code><code class="p">,</code> <code class="bp">False</code><code class="p">,</code> <code class="bp">True</code><code class="p">,</code>  <code class="bp">True</code><code class="p">),</code>&#13;
          <code class="n">Candidate</code><code class="p">(</code><code class="s1">'Mid'</code><code class="p">,</code>    <code class="s1">'Java'</code><code class="p">,</code>   <code class="bp">True</code><code class="p">,</code>  <code class="bp">False</code><code class="p">,</code> <code class="bp">True</code><code class="p">),</code>&#13;
          <code class="n">Candidate</code><code class="p">(</code><code class="s1">'Junior'</code><code class="p">,</code> <code class="s1">'Python'</code><code class="p">,</code> <code class="bp">False</code><code class="p">,</code> <code class="bp">True</code><code class="p">,</code>  <code class="bp">False</code><code class="p">)</code>&#13;
         <code class="p">]</code></pre>&#13;
&#13;
<p>Our<a data-primary="decision nodes" data-type="indexterm" id="idm45635734221688"/> tree will consist of <em>decision nodes</em> (which ask a question and direct us differently depending on the answer) and <em>leaf nodes</em> (which give us a prediction).  We will build it using the<a data-primary="ID3 algorithm" data-type="indexterm" id="idm45635734220120"/> relatively simple <em>ID3</em> algorithm, which operates in the following manner.  Let’s say we’re given some labeled data, and a list of attributes to consider branching on:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>If the data all have the same label, create a leaf node that predicts that label and then stop.</p>&#13;
</li>&#13;
<li>&#13;
<p>If the list of attributes is empty (i.e., there are no more possible questions to ask), create a leaf node that predicts the most common label and then stop.</p>&#13;
</li>&#13;
<li>&#13;
<p>Otherwise, try partitioning the data by each of the attributes.</p>&#13;
</li>&#13;
<li>&#13;
<p>Choose the partition with the lowest partition entropy.</p>&#13;
</li>&#13;
<li>&#13;
<p>Add a decision node based on the chosen attribute.</p>&#13;
</li>&#13;
<li>&#13;
<p>Recur on each partitioned subset using the remaining attributes.</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>This is what’s known as a “greedy” algorithm because, at each step, it chooses the most immediately best option.  Given a dataset, there may be a better tree with a worse-looking first move.  If so, this algorithm won’t find it.  Nonetheless, it is relatively easy to understand and implement, which makes it a good place to begin exploring decision trees.</p>&#13;
&#13;
<p>Let’s manually go through these steps on the interviewee dataset.  The dataset has both <code>True</code> and <code>False</code> labels, and we have four attributes we can split on.  So our first step will be to find the partition with the least entropy.  We’ll start by writing a function that does the partitioning:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="kn">from</code> <code class="nn">typing</code> <code class="kn">import</code> <code class="n">Dict</code><code class="p">,</code> <code class="n">TypeVar</code>&#13;
<code class="kn">from</code> <code class="nn">collections</code> <code class="kn">import</code> <code class="n">defaultdict</code>&#13;
&#13;
<code class="n">T</code> <code class="o">=</code> <code class="n">TypeVar</code><code class="p">(</code><code class="s1">'T'</code><code class="p">)</code>  <code class="c1"># generic type for inputs</code>&#13;
&#13;
<code class="k">def</code> <code class="nf">partition_by</code><code class="p">(</code><code class="n">inputs</code><code class="p">:</code> <code class="n">List</code><code class="p">[</code><code class="n">T</code><code class="p">],</code> <code class="n">attribute</code><code class="p">:</code> <code class="nb">str</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="n">Dict</code><code class="p">[</code><code class="n">Any</code><code class="p">,</code> <code class="n">List</code><code class="p">[</code><code class="n">T</code><code class="p">]]:</code>&#13;
    <code class="sd">"""Partition the inputs into lists based on the specified attribute."""</code>&#13;
    <code class="n">partitions</code><code class="p">:</code> <code class="n">Dict</code><code class="p">[</code><code class="n">Any</code><code class="p">,</code> <code class="n">List</code><code class="p">[</code><code class="n">T</code><code class="p">]]</code> <code class="o">=</code> <code class="n">defaultdict</code><code class="p">(</code><code class="nb">list</code><code class="p">)</code>&#13;
    <code class="k">for</code> <code class="nb">input</code> <code class="ow">in</code> <code class="n">inputs</code><code class="p">:</code>&#13;
        <code class="n">key</code> <code class="o">=</code> <code class="nb">getattr</code><code class="p">(</code><code class="nb">input</code><code class="p">,</code> <code class="n">attribute</code><code class="p">)</code>  <code class="c1"># value of the specified attribute</code>&#13;
        <code class="n">partitions</code><code class="p">[</code><code class="n">key</code><code class="p">]</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="nb">input</code><code class="p">)</code>    <code class="c1"># add input to the correct partition</code>&#13;
    <code class="k">return</code> <code class="n">partitions</code></pre>&#13;
&#13;
<p>and one that uses it to compute entropy:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="k">def</code> <code class="nf">partition_entropy_by</code><code class="p">(</code><code class="n">inputs</code><code class="p">:</code> <code class="n">List</code><code class="p">[</code><code class="n">Any</code><code class="p">],</code>&#13;
                         <code class="n">attribute</code><code class="p">:</code> <code class="nb">str</code><code class="p">,</code>&#13;
                         <code class="n">label_attribute</code><code class="p">:</code> <code class="nb">str</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">float</code><code class="p">:</code>&#13;
    <code class="sd">"""Compute the entropy corresponding to the given partition"""</code>&#13;
    <code class="c1"># partitions consist of our inputs</code>&#13;
    <code class="n">partitions</code> <code class="o">=</code> <code class="n">partition_by</code><code class="p">(</code><code class="n">inputs</code><code class="p">,</code> <code class="n">attribute</code><code class="p">)</code>&#13;
&#13;
    <code class="c1"># but partition_entropy needs just the class labels</code>&#13;
    <code class="n">labels</code> <code class="o">=</code> <code class="p">[[</code><code class="nb">getattr</code><code class="p">(</code><code class="nb">input</code><code class="p">,</code> <code class="n">label_attribute</code><code class="p">)</code> <code class="k">for</code> <code class="nb">input</code> <code class="ow">in</code> <code class="n">partition</code><code class="p">]</code>&#13;
              <code class="k">for</code> <code class="n">partition</code> <code class="ow">in</code> <code class="n">partitions</code><code class="o">.</code><code class="n">values</code><code class="p">()]</code>&#13;
&#13;
    <code class="k">return</code> <code class="n">partition_entropy</code><code class="p">(</code><code class="n">labels</code><code class="p">)</code></pre>&#13;
&#13;
<p>Then we just need to find the minimum-entropy partition for the whole dataset:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="k">for</code> <code class="n">key</code> <code class="ow">in</code> <code class="p">[</code><code class="s1">'level'</code><code class="p">,</code><code class="s1">'lang'</code><code class="p">,</code><code class="s1">'tweets'</code><code class="p">,</code><code class="s1">'phd'</code><code class="p">]:</code>&#13;
    <code class="k">print</code><code class="p">(</code><code class="n">key</code><code class="p">,</code> <code class="n">partition_entropy_by</code><code class="p">(</code><code class="n">inputs</code><code class="p">,</code> <code class="n">key</code><code class="p">,</code> <code class="s1">'did_well'</code><code class="p">))</code>&#13;
&#13;
<code class="k">assert</code> <code class="mf">0.69</code> <code class="o">&lt;</code> <code class="n">partition_entropy_by</code><code class="p">(</code><code class="n">inputs</code><code class="p">,</code> <code class="s1">'level'</code><code class="p">,</code> <code class="s1">'did_well'</code><code class="p">)</code>  <code class="o">&lt;</code> <code class="mf">0.70</code>&#13;
<code class="k">assert</code> <code class="mf">0.86</code> <code class="o">&lt;</code> <code class="n">partition_entropy_by</code><code class="p">(</code><code class="n">inputs</code><code class="p">,</code> <code class="s1">'lang'</code><code class="p">,</code> <code class="s1">'did_well'</code><code class="p">)</code>   <code class="o">&lt;</code> <code class="mf">0.87</code>&#13;
<code class="k">assert</code> <code class="mf">0.78</code> <code class="o">&lt;</code> <code class="n">partition_entropy_by</code><code class="p">(</code><code class="n">inputs</code><code class="p">,</code> <code class="s1">'tweets'</code><code class="p">,</code> <code class="s1">'did_well'</code><code class="p">)</code> <code class="o">&lt;</code> <code class="mf">0.79</code>&#13;
<code class="k">assert</code> <code class="mf">0.89</code> <code class="o">&lt;</code> <code class="n">partition_entropy_by</code><code class="p">(</code><code class="n">inputs</code><code class="p">,</code> <code class="s1">'phd'</code><code class="p">,</code> <code class="s1">'did_well'</code><code class="p">)</code>    <code class="o">&lt;</code> <code class="mf">0.90</code></pre>&#13;
&#13;
<p>The lowest entropy comes from splitting on <code>level</code>, so we’ll need to make a subtree for each possible <code>level</code> value.  Every <code>Mid</code> candidate is labeled <code>True</code>, which means that the <code>Mid</code> subtree is simply a leaf node predicting <code>True</code>.  For <code>Senior</code> candidates, we have a mix of <code>True</code>s and <code>False</code>s, so we need to split again:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="n">senior_inputs</code> <code class="o">=</code> <code class="p">[</code><code class="nb">input</code> <code class="k">for</code> <code class="nb">input</code> <code class="ow">in</code> <code class="n">inputs</code> <code class="k">if</code> <code class="nb">input</code><code class="o">.</code><code class="n">level</code> <code class="o">==</code> <code class="s1">'Senior'</code><code class="p">]</code>&#13;
&#13;
<code class="k">assert</code> <code class="mf">0.4</code> <code class="o">==</code> <code class="n">partition_entropy_by</code><code class="p">(</code><code class="n">senior_inputs</code><code class="p">,</code> <code class="s1">'lang'</code><code class="p">,</code> <code class="s1">'did_well'</code><code class="p">)</code>&#13;
<code class="k">assert</code> <code class="mf">0.0</code> <code class="o">==</code> <code class="n">partition_entropy_by</code><code class="p">(</code><code class="n">senior_inputs</code><code class="p">,</code> <code class="s1">'tweets'</code><code class="p">,</code> <code class="s1">'did_well'</code><code class="p">)</code>&#13;
<code class="k">assert</code> <code class="mf">0.95</code> <code class="o">&lt;</code> <code class="n">partition_entropy_by</code><code class="p">(</code><code class="n">senior_inputs</code><code class="p">,</code> <code class="s1">'phd'</code><code class="p">,</code> <code class="s1">'did_well'</code><code class="p">)</code> <code class="o">&lt;</code> <code class="mf">0.96</code></pre>&#13;
&#13;
<p>This shows us that our next split should be on <code>tweets</code>, which results in a zero-entropy partition.  For these <code>Senior</code>-level candidates, “yes” tweets always result in <code>True</code> while “no” tweets always result in <code>False</code>.</p>&#13;
&#13;
<p>Finally, if we do the same thing for the <code>Junior</code> candidates, we end up splitting on <code>phd</code>, after which we find that no PhD always results in <code>True</code> and PhD always results in <code>False</code>.</p>&#13;
&#13;
<p><a data-type="xref" href="#hiring_decision_tree">Figure 17-3</a> shows the complete decision tree.</p>&#13;
&#13;
<figure><div class="figure" id="hiring_decision_tree">&#13;
<img alt="Hiring Decision Tree." src="assets/dsf2_1703.png"/>&#13;
<h6><span class="label">Figure 17-3. </span>The decision tree for hiring</h6>&#13;
</div></figure>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Putting It All Together" data-type="sect1"><div class="sect1" id="idm45635734227320">&#13;
<h1>Putting It All Together</h1>&#13;
&#13;
<p>Now<a data-primary="decision trees" data-secondary="implementing" data-type="indexterm" id="idm45635733484680"/> that we’ve seen how the algorithm works, we would like to implement it more generally.  This means we need to decide how we want to represent trees.  We’ll use pretty much the most lightweight representation possible.  We define a <em>tree</em> to be either:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>a <code>Leaf</code> (that predicts a single value), or</p>&#13;
</li>&#13;
<li>&#13;
<p>a <code>Split</code> (containing an attribute to split on, subtrees for specific values of that attribute, and possibly a default value to use if we see an unknown value).</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="kn">from</code> <code class="nn">typing</code> <code class="kn">import</code> <code class="n">NamedTuple</code><code class="p">,</code> <code class="n">Union</code><code class="p">,</code> <code class="n">Any</code>&#13;
&#13;
<code class="k">class</code> <code class="nc">Leaf</code><code class="p">(</code><code class="n">NamedTuple</code><code class="p">):</code>&#13;
    <code class="n">value</code><code class="p">:</code> <code class="n">Any</code>&#13;
&#13;
<code class="k">class</code> <code class="nc">Split</code><code class="p">(</code><code class="n">NamedTuple</code><code class="p">):</code>&#13;
    <code class="n">attribute</code><code class="p">:</code> <code class="nb">str</code>&#13;
    <code class="n">subtrees</code><code class="p">:</code> <code class="nb">dict</code>&#13;
    <code class="n">default_value</code><code class="p">:</code> <code class="n">Any</code> <code class="o">=</code> <code class="bp">None</code>&#13;
&#13;
<code class="n">DecisionTree</code> <code class="o">=</code> <code class="n">Union</code><code class="p">[</code><code class="n">Leaf</code><code class="p">,</code> <code class="n">Split</code><code class="p">]</code></pre>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>With this representation, our hiring tree would look like:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="n">hiring_tree</code> <code class="o">=</code> <code class="n">Split</code><code class="p">(</code><code class="s1">'level'</code><code class="p">,</code> <code class="p">{</code>   <code class="c1"># first, consider "level"</code>&#13;
    <code class="s1">'Junior'</code><code class="p">:</code> <code class="n">Split</code><code class="p">(</code><code class="s1">'phd'</code><code class="p">,</code> <code class="p">{</code>     <code class="c1"># if level is "Junior", next look at "phd"</code>&#13;
        <code class="bp">False</code><code class="p">:</code> <code class="n">Leaf</code><code class="p">(</code><code class="bp">True</code><code class="p">),</code>       <code class="c1">#   if "phd" is False, predict True</code>&#13;
        <code class="bp">True</code><code class="p">:</code> <code class="n">Leaf</code><code class="p">(</code><code class="bp">False</code><code class="p">)</code>        <code class="c1">#   if "phd" is True, predict False</code>&#13;
    <code class="p">}),</code>&#13;
    <code class="s1">'Mid'</code><code class="p">:</code> <code class="n">Leaf</code><code class="p">(</code><code class="bp">True</code><code class="p">),</code>           <code class="c1"># if level is "Mid", just predict True</code>&#13;
    <code class="s1">'Senior'</code><code class="p">:</code> <code class="n">Split</code><code class="p">(</code><code class="s1">'tweets'</code><code class="p">,</code> <code class="p">{</code>  <code class="c1"># if level is "Senior", look at "tweets"</code>&#13;
        <code class="bp">False</code><code class="p">:</code> <code class="n">Leaf</code><code class="p">(</code><code class="bp">False</code><code class="p">),</code>      <code class="c1">#   if "tweets" is False, predict False</code>&#13;
        <code class="bp">True</code><code class="p">:</code> <code class="n">Leaf</code><code class="p">(</code><code class="bp">True</code><code class="p">)</code>         <code class="c1">#   if "tweets" is True, predict True</code>&#13;
    <code class="p">})</code>&#13;
<code class="p">})</code></pre>&#13;
&#13;
<p>There’s still the question of what to do if we encounter an unexpected (or missing) attribute value.  What should our hiring tree do if it encounters a candidate whose <code>level</code> is <code>Intern</code>?  We’ll handle this case by populating the <code>default_value</code> attribute with the most common label.</p>&#13;
&#13;
<p>Given such a representation, we can classify an input with:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="k">def</code> <code class="nf">classify</code><code class="p">(</code><code class="n">tree</code><code class="p">:</code> <code class="n">DecisionTree</code><code class="p">,</code> <code class="nb">input</code><code class="p">:</code> <code class="n">Any</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="n">Any</code><code class="p">:</code>&#13;
    <code class="sd">"""classify the input using the given decision tree"""</code>&#13;
&#13;
    <code class="c1"># If this is a leaf node, return its value</code>&#13;
    <code class="k">if</code> <code class="nb">isinstance</code><code class="p">(</code><code class="n">tree</code><code class="p">,</code> <code class="n">Leaf</code><code class="p">):</code>&#13;
        <code class="k">return</code> <code class="n">tree</code><code class="o">.</code><code class="n">value</code>&#13;
&#13;
    <code class="c1"># Otherwise this tree consists of an attribute to split on</code>&#13;
    <code class="c1"># and a dictionary whose keys are values of that attribute</code>&#13;
    <code class="c1"># and whose values are subtrees to consider next</code>&#13;
    <code class="n">subtree_key</code> <code class="o">=</code> <code class="nb">getattr</code><code class="p">(</code><code class="nb">input</code><code class="p">,</code> <code class="n">tree</code><code class="o">.</code><code class="n">attribute</code><code class="p">)</code>&#13;
&#13;
    <code class="k">if</code> <code class="n">subtree_key</code> <code class="ow">not</code> <code class="ow">in</code> <code class="n">tree</code><code class="o">.</code><code class="n">subtrees</code><code class="p">:</code>   <code class="c1"># If no subtree for key,</code>&#13;
        <code class="k">return</code> <code class="n">tree</code><code class="o">.</code><code class="n">default_value</code>          <code class="c1"># return the default value.</code>&#13;
&#13;
    <code class="n">subtree</code> <code class="o">=</code> <code class="n">tree</code><code class="o">.</code><code class="n">subtrees</code><code class="p">[</code><code class="n">subtree_key</code><code class="p">]</code>   <code class="c1"># Choose the appropriate subtree</code>&#13;
    <code class="k">return</code> <code class="n">classify</code><code class="p">(</code><code class="n">subtree</code><code class="p">,</code> <code class="nb">input</code><code class="p">)</code>        <code class="c1"># and use it to classify the input.</code></pre>&#13;
&#13;
<p>All that’s left is to build the tree representation from our training data:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="k">def</code> <code class="nf">build_tree_id3</code><code class="p">(</code><code class="n">inputs</code><code class="p">:</code> <code class="n">List</code><code class="p">[</code><code class="n">Any</code><code class="p">],</code>&#13;
                   <code class="n">split_attributes</code><code class="p">:</code> <code class="n">List</code><code class="p">[</code><code class="nb">str</code><code class="p">],</code>&#13;
                   <code class="n">target_attribute</code><code class="p">:</code> <code class="nb">str</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="n">DecisionTree</code><code class="p">:</code>&#13;
    <code class="c1"># Count target labels</code>&#13;
    <code class="n">label_counts</code> <code class="o">=</code> <code class="n">Counter</code><code class="p">(</code><code class="nb">getattr</code><code class="p">(</code><code class="nb">input</code><code class="p">,</code> <code class="n">target_attribute</code><code class="p">)</code>&#13;
                           <code class="k">for</code> <code class="nb">input</code> <code class="ow">in</code> <code class="n">inputs</code><code class="p">)</code>&#13;
    <code class="n">most_common_label</code> <code class="o">=</code> <code class="n">label_counts</code><code class="o">.</code><code class="n">most_common</code><code class="p">(</code><code class="mi">1</code><code class="p">)[</code><code class="mi">0</code><code class="p">][</code><code class="mi">0</code><code class="p">]</code>&#13;
&#13;
    <code class="c1"># If there's a unique label, predict it</code>&#13;
    <code class="k">if</code> <code class="nb">len</code><code class="p">(</code><code class="n">label_counts</code><code class="p">)</code> <code class="o">==</code> <code class="mi">1</code><code class="p">:</code>&#13;
        <code class="k">return</code> <code class="n">Leaf</code><code class="p">(</code><code class="n">most_common_label</code><code class="p">)</code>&#13;
&#13;
    <code class="c1"># If no split attributes left, return the majority label</code>&#13;
    <code class="k">if</code> <code class="ow">not</code> <code class="n">split_attributes</code><code class="p">:</code>&#13;
        <code class="k">return</code> <code class="n">Leaf</code><code class="p">(</code><code class="n">most_common_label</code><code class="p">)</code>&#13;
&#13;
    <code class="c1"># Otherwise split by the best attribute</code>&#13;
&#13;
    <code class="k">def</code> <code class="nf">split_entropy</code><code class="p">(</code><code class="n">attribute</code><code class="p">:</code> <code class="nb">str</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">float</code><code class="p">:</code>&#13;
        <code class="sd">"""Helper function for finding the best attribute"""</code>&#13;
        <code class="k">return</code> <code class="n">partition_entropy_by</code><code class="p">(</code><code class="n">inputs</code><code class="p">,</code> <code class="n">attribute</code><code class="p">,</code> <code class="n">target_attribute</code><code class="p">)</code>&#13;
&#13;
    <code class="n">best_attribute</code> <code class="o">=</code> <code class="nb">min</code><code class="p">(</code><code class="n">split_attributes</code><code class="p">,</code> <code class="n">key</code><code class="o">=</code><code class="n">split_entropy</code><code class="p">)</code>&#13;
&#13;
    <code class="n">partitions</code> <code class="o">=</code> <code class="n">partition_by</code><code class="p">(</code><code class="n">inputs</code><code class="p">,</code> <code class="n">best_attribute</code><code class="p">)</code>&#13;
    <code class="n">new_attributes</code> <code class="o">=</code> <code class="p">[</code><code class="n">a</code> <code class="k">for</code> <code class="n">a</code> <code class="ow">in</code> <code class="n">split_attributes</code> <code class="k">if</code> <code class="n">a</code> <code class="o">!=</code> <code class="n">best_attribute</code><code class="p">]</code>&#13;
&#13;
    <code class="c1"># Recursively build the subtrees</code>&#13;
    <code class="n">subtrees</code> <code class="o">=</code> <code class="p">{</code><code class="n">attribute_value</code> <code class="p">:</code> <code class="n">build_tree_id3</code><code class="p">(</code><code class="n">subset</code><code class="p">,</code>&#13;
                                                 <code class="n">new_attributes</code><code class="p">,</code>&#13;
                                                 <code class="n">target_attribute</code><code class="p">)</code>&#13;
                <code class="k">for</code> <code class="n">attribute_value</code><code class="p">,</code> <code class="n">subset</code> <code class="ow">in</code> <code class="n">partitions</code><code class="o">.</code><code class="n">items</code><code class="p">()}</code>&#13;
&#13;
    <code class="k">return</code> <code class="n">Split</code><code class="p">(</code><code class="n">best_attribute</code><code class="p">,</code> <code class="n">subtrees</code><code class="p">,</code> <code class="n">default_value</code><code class="o">=</code><code class="n">most_common_label</code><code class="p">)</code></pre>&#13;
&#13;
<p>In the tree we built, every leaf consisted entirely of <code>True</code> inputs or entirely of <code>False</code> inputs.  This means that the tree predicts perfectly on the training dataset.  But we can also apply it to new data that wasn’t in the training set:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="n">tree</code> <code class="o">=</code> <code class="n">build_tree_id3</code><code class="p">(</code><code class="n">inputs</code><code class="p">,</code>&#13;
                      <code class="p">[</code><code class="s1">'level'</code><code class="p">,</code> <code class="s1">'lang'</code><code class="p">,</code> <code class="s1">'tweets'</code><code class="p">,</code> <code class="s1">'phd'</code><code class="p">],</code>&#13;
                      <code class="s1">'did_well'</code><code class="p">)</code>&#13;
&#13;
<code class="c1"># Should predict True</code>&#13;
<code class="k">assert</code> <code class="n">classify</code><code class="p">(</code><code class="n">tree</code><code class="p">,</code> <code class="n">Candidate</code><code class="p">(</code><code class="s2">"Junior"</code><code class="p">,</code> <code class="s2">"Java"</code><code class="p">,</code> <code class="bp">True</code><code class="p">,</code> <code class="bp">False</code><code class="p">))</code>&#13;
&#13;
<code class="c1"># Should predict False</code>&#13;
<code class="k">assert</code> <code class="ow">not</code> <code class="n">classify</code><code class="p">(</code><code class="n">tree</code><code class="p">,</code> <code class="n">Candidate</code><code class="p">(</code><code class="s2">"Junior"</code><code class="p">,</code> <code class="s2">"Java"</code><code class="p">,</code> <code class="bp">True</code><code class="p">,</code> <code class="bp">True</code><code class="p">))</code></pre>&#13;
&#13;
<p>And also to data with unexpected values:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="c1"># Should predict True</code>&#13;
<code class="k">assert</code> <code class="n">classify</code><code class="p">(</code><code class="n">tree</code><code class="p">,</code> <code class="n">Candidate</code><code class="p">(</code><code class="s2">"Intern"</code><code class="p">,</code> <code class="s2">"Java"</code><code class="p">,</code> <code class="bp">True</code><code class="p">,</code> <code class="bp">True</code><code class="p">))</code></pre>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>Since our goal was mainly to demonstrate <em>how</em> to build a tree, we built the tree using the entire dataset. As always, if we were really trying to create a good model for something, we would have collected more data and split it into train/validation/test subsets.</p>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Random Forests" data-type="sect1"><div class="sect1" id="idm45635733485784">&#13;
<h1>Random Forests</h1>&#13;
&#13;
<p>Given<a data-primary="decision trees" data-secondary="random forests technique" data-type="indexterm" id="idm45635732947496"/><a data-primary="random forests technique" data-type="indexterm" id="idm45635732946520"/> how closely decision trees can fit themselves to their training data,&#13;
it’s not surprising that they have a tendency to overfit.  One way of avoiding&#13;
this is a technique called <em>random forests</em>, in which we build multiple decision&#13;
trees and combine their outputs. If they’re classification trees, we might let them vote; if they’re regression trees, we might average their predictions.</p>&#13;
&#13;
<p>Our tree-building process was deterministic, so how do we get random trees?</p>&#13;
&#13;
<p>One piece involves bootstrapping data (recall <a data-type="xref" href="ch15.html#the_bootstrap">“Digression: The Bootstrap”</a>).  Rather than training each tree on all the <code>inputs</code> in the training set, we train each tree on the result of <code>bootstrap_sample(inputs)</code>.  Since each tree is built using different data, each tree will be different from every other tree.  (A side benefit is that it’s totally fair to use the nonsampled data to test each tree, which means you can get away with using all of your data as the training set if you are clever in how you measure performance.)  This<a data-primary="bagging" data-type="indexterm" id="idm45635732934152"/><a data-primary="bootstrap aggregating" data-type="indexterm" id="idm45635732933480"/> technique is known as <em>bootstrap aggregating</em> or <em>bagging</em>.</p>&#13;
&#13;
<p>A second source of randomness involves changing the way we choose the <code>best_attribute</code> to split on.&#13;
Rather than looking at all the remaining attributes, we first choose a random subset of them&#13;
and then split on whichever of those is best:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting">    <code class="c1"># if there are already few enough split candidates, look at all of them</code>&#13;
    <code class="k">if</code> <code class="nb">len</code><code class="p">(</code><code class="n">split_candidates</code><code class="p">)</code> <code class="o">&lt;=</code> <code class="bp">self</code><code class="o">.</code><code class="n">num_split_candidates</code><code class="p">:</code>&#13;
        <code class="n">sampled_split_candidates</code> <code class="o">=</code> <code class="n">split_candidates</code>&#13;
    <code class="c1"># otherwise pick a random sample</code>&#13;
    <code class="k">else</code><code class="p">:</code>&#13;
        <code class="n">sampled_split_candidates</code> <code class="o">=</code> <code class="n">random</code><code class="o">.</code><code class="n">sample</code><code class="p">(</code><code class="n">split_candidates</code><code class="p">,</code>&#13;
                                                 <code class="bp">self</code><code class="o">.</code><code class="n">num_split_candidates</code><code class="p">)</code>&#13;
&#13;
    <code class="c1"># now choose the best attribute only from those candidates</code>&#13;
    <code class="n">best_attribute</code> <code class="o">=</code> <code class="nb">min</code><code class="p">(</code><code class="n">sampled_split_candidates</code><code class="p">,</code> <code class="n">key</code><code class="o">=</code><code class="n">split_entropy</code><code class="p">)</code>&#13;
&#13;
    <code class="n">partitions</code> <code class="o">=</code> <code class="n">partition_by</code><code class="p">(</code><code class="n">inputs</code><code class="p">,</code> <code class="n">best_attribute</code><code class="p">)</code></pre>&#13;
&#13;
<p>This<a data-primary="ensemble learning" data-type="indexterm" id="idm45635732928728"/><a data-primary="weak learners" data-type="indexterm" id="idm45635732747592"/> is an example of a broader technique called <em>ensemble learning</em> in which we combine&#13;
several <em>weak learners</em> (typically high-bias, low-variance models)&#13;
in order to produce an overall strong model.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="For Further Exploration" data-type="sect1"><div class="sect1" id="idm45635732745752">&#13;
<h1>For Further Exploration</h1>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>scikit-learn<a data-primary="scikit-learn" data-type="indexterm" id="idm45635732743784"/><a data-primary="decision trees" data-secondary="tools for" data-type="indexterm" id="idm45635732743048"/> has many <a href="https://scikit-learn.org/stable/modules/tree.html">decision tree</a> models.  It also has an <a href="https://scikit-learn.org/stable/modules/classes.html#module-sklearn.ensemble"><code>ensemble</code></a> module that includes a <code>RandomForestClassifier</code> as well as other ensemble methods.</p>&#13;
</li>&#13;
<li>&#13;
<p><a href="https://xgboost.ai/">XGBoost</a> is a<a data-primary="XGBoost" data-type="indexterm" id="idm45635732738520"/> library for training <em>gradient boosted</em> decision<a data-primary="gradient boosted decision trees" data-type="indexterm" id="idm45635732737240"/><a data-primary="decision trees" data-secondary="gradient boosted decision trees" data-type="indexterm" id="idm45635732736440"/> trees that tends to win a lot of Kaggle-style machine learning competitions.</p>&#13;
</li>&#13;
<li>&#13;
<p>We’ve barely<a data-primary="decision trees" data-secondary="resources for learning about" data-type="indexterm" id="idm45635732734520"/> scratched the surface of decision trees and their algorithms.  <a href="https://en.wikipedia.org/wiki/Decision_tree_learning">Wikipedia</a> is a good starting point for broader exploration.<a data-primary="" data-startref="PMdecision17" data-type="indexterm" id="idm45635732732680"/></p>&#13;
</li>&#13;
</ul>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section></body></html>