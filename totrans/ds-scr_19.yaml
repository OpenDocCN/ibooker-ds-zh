- en: Chapter 18\. Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I like nonsense; it wakes up the brain cells.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Dr. Seuss
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: An *artificial neural network* (or neural network for short) is a predictive
    model motivated by the way the brain operates. Think of the brain as a collection
    of neurons wired together. Each neuron looks at the outputs of the other neurons
    that feed into it, does a calculation, and then either fires (if the calculation
    exceeds some threshold) or doesn’t (if it doesn’t).
  prefs: []
  type: TYPE_NORMAL
- en: Accordingly, artificial neural networks consist of artificial neurons, which
    perform similar calculations over their inputs. Neural networks can solve a wide
    variety of problems like handwriting recognition and face detection, and they
    are used heavily in deep learning, one of the trendiest subfields of data science.
    However, most neural networks are “black boxes”—inspecting their details doesn’t
    give you much understanding of *how* they’re solving a problem. And large neural
    networks can be difficult to train. For most problems you’ll encounter as a budding
    data scientist, they’re probably not the right choice. Someday, when you’re trying
    to build an artificial intelligence to bring about the Singularity, they very
    well might be.
  prefs: []
  type: TYPE_NORMAL
- en: Perceptrons
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Pretty much the simplest neural network is the *perceptron*, which approximates
    a single neuron with *n* binary inputs. It computes a weighted sum of its inputs
    and “fires” if that weighted sum is 0 or greater:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The perceptron is simply distinguishing between the half-spaces separated by
    the hyperplane of points `x` for which:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'With properly chosen weights, perceptrons can solve a number of simple problems
    ([Figure 18-1](#a_perceptron)). For example, we can create an *AND gate* (which
    returns 1 if both its inputs are 1 but returns 0 if one of its inputs is 0) with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: If both inputs are 1, the `calculation` equals 2 + 2 – 3 = 1, and the output
    is 1\. If only one of the inputs is 1, the `calculation` equals 2 + 0 – 3 = –1,
    and the output is 0\. And if both of the inputs are 0, the `calculation` equals
    –3, and the output is 0.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using similar reasoning, we could build an *OR gate* with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![Perceptron.](assets/dsf2_1801.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 18-1\. Decision space for a two-input perceptron
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'We could also build a *NOT gate* (which has one input and converts 1 to 0 and
    0 to 1) with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: However, there are some problems that simply can’t be solved by a single perceptron.
    For example, no matter how hard you try, you cannot use a perceptron to build
    an *XOR gate* that outputs 1 if exactly one of its inputs is 1 and 0 otherwise.
    This is where we start needing more complicated neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Of course, you don’t need to approximate a neuron in order to build a logic
    gate:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Like real neurons, artificial neurons start getting more interesting when you
    start connecting them together.
  prefs: []
  type: TYPE_NORMAL
- en: Feed-Forward Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The topology of the brain is enormously complicated, so it’s common to approximate
    it with an idealized *feed-forward* neural network that consists of discrete *layers*
    of neurons, each connected to the next. This typically entails an input layer
    (which receives inputs and feeds them forward unchanged), one or more “hidden
    layers” (each of which consists of neurons that take the outputs of the previous
    layer, performs some calculation, and passes the result to the next layer), and
    an output layer (which produces the final outputs).
  prefs: []
  type: TYPE_NORMAL
- en: Just like in the perceptron, each (noninput) neuron has a weight corresponding
    to each of its inputs and a bias. To make our representation simpler, we’ll add
    the bias to the end of our weights vector and give each neuron a *bias input*
    that always equals 1.
  prefs: []
  type: TYPE_NORMAL
- en: 'As with the perceptron, for each neuron we’ll sum up the products of its inputs
    and its weights. But here, rather than outputting the `step_function` applied
    to that product, we’ll output a smooth approximation of it. Here we’ll use the
    `sigmoid` function ([Figure 18-2](#sigmoid_function)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '![Sigmoid.](assets/dsf2_1802.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 18-2\. The sigmoid function
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Why use `sigmoid` instead of the simpler `step_function`? In order to train
    a neural network, we need to use calculus, and in order to use calculus, we need
    *smooth* functions. `step_function` isn’t even continuous, and `sigmoid` is a
    good smooth approximation of it.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You may remember `sigmoid` from [Chapter 16](ch16.html#logistic_regression),
    where it was called `logistic`. Technically “sigmoid” refers to the *shape* of
    the function and “logistic” to this particular function, although people often
    use the terms interchangeably.
  prefs: []
  type: TYPE_NORMAL
- en: 'We then calculate the output as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Given this function, we can represent a neuron simply as a vector of weights
    whose length is one more than the number of inputs to that neuron (because of
    the bias weight). Then we can represent a neural network as a list of (noninput)
    *layers*, where each layer is just a list of the neurons in that layer.
  prefs: []
  type: TYPE_NORMAL
- en: That is, we’ll represent a neural network as a list (layers) of lists (neurons)
    of vectors (weights).
  prefs: []
  type: TYPE_NORMAL
- en: 'Given such a representation, using the neural network is quite simple:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Now it’s easy to build the XOR gate that we couldn’t build with a single perceptron.
    We just need to scale the weights up so that the `neuron_output`s are either really
    close to 0 or really close to 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: For a given input (which is a two-dimensional vector), the hidden layer produces
    a two-dimensional vector consisting of the “and” of the two input values and the
    “or” of the two input values.
  prefs: []
  type: TYPE_NORMAL
- en: And the output layer takes a two-dimensional vector and computes “second element
    but not first element.” The result is a network that performs “or, but not and,”
    which is precisely XOR ([Figure 18-3](#xor_neural_network)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Neural Network.](assets/dsf2_1803.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 18-3\. A neural network for XOR
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: One suggestive way of thinking about this is that the hidden layer is computing
    *features* of the input data (in this case “and” and “or”) and the output layer
    is combining those features in a way that generates the desired output.
  prefs: []
  type: TYPE_NORMAL
- en: Backpropagation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Usually we don’t build neural networks by hand. This is in part because we use
    them to solve much bigger problems—an image recognition problem might involve
    hundreds or thousands of neurons. And it’s in part because we usually won’t be
    able to “reason out” what the neurons should be.
  prefs: []
  type: TYPE_NORMAL
- en: Instead (as usual) we use data to *train* neural networks. The typical approach
    is an algorithm called *backpropagation*, which uses gradient descent or one of
    its variants.
  prefs: []
  type: TYPE_NORMAL
- en: 'Imagine we have a training set that consists of input vectors and corresponding
    target output vectors. For example, in our previous `xor_network` example, the
    input vector `[1, 0]` corresponded to the target output `[1]`. Imagine that our
    network has some set of weights. We then adjust the weights using the following
    algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: Run `feed_forward` on an input vector to produce the outputs of all the neurons
    in the network.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We know the target output, so we can compute a *loss* that’s the sum of the
    squared errors.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the gradient of this loss as a function of the output neuron’s weights.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: “Propagate” the gradients and errors backward to compute the gradients with
    respect to the hidden neurons’ weights.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Take a gradient descent step.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Typically we run this algorithm many times for our entire training set until
    the network converges.
  prefs: []
  type: TYPE_NORMAL
- en: 'To start with, let’s write the function to compute the gradients:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The math behind the preceding calculations is not terribly difficult, but it
    involves some tedious calculus and careful attention to detail, so I’ll leave
    it as an exercise for you.
  prefs: []
  type: TYPE_NORMAL
- en: Armed with the ability to compute gradients, we can now train neural networks.
    Let’s try to learn the XOR network we previously designed by hand.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll start by generating the training data and initializing our neural network
    with random weights:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: As usual, we can train it using gradient descent. One difference from our previous
    examples is that here we have several parameter vectors, each with its own gradient,
    which means we’ll have to call `gradient_step` for each of them.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'For me the resulting network has weights that look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: which is conceptually pretty similar to our previous bespoke network.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: Fizz Buzz'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The VP of Engineering wants to interview technical candidates by making them
    solve “Fizz Buzz,” the following well-trod programming challenge:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: He thinks the ability to solve this demonstrates extreme programming skill.
    You think that this problem is so simple that a neural network could solve it.
  prefs: []
  type: TYPE_NORMAL
- en: Neural networks take vectors as inputs and produce vectors as outputs. As stated,
    the programming problem is to turn an integer into a string. So the first challenge
    is to come up with a way to recast it as a vector problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the outputs it’s not tough: there are basically four classes of outputs,
    so we can encode the output as a vector of four 0s and 1s:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: We’ll use this to generate our target vectors. The input vectors are less obvious.
    You don’t want to just use a one-dimensional vector containing the input number,
    for a couple of reasons. A single input captures an “intensity,” but the fact
    that 2 is twice as much as 1, and that 4 is twice as much again, doesn’t feel
    relevant to this problem. Additionally, with just one input the hidden layer wouldn’t
    be able to compute very interesting features, which means it probably wouldn’t
    be able to solve the problem.
  prefs: []
  type: TYPE_NORMAL
- en: It turns out that one thing that works reasonably well is to convert each number
    to its *binary* representation of 1s and 0s. (Don’t worry, this isn’t obvious—at
    least it wasn’t to me.)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'As the goal is to construct the outputs for the numbers 1 to 100, it would
    be cheating to train on those numbers. Therefore, we’ll train on the numbers 101
    to 1,023 (which is the largest number we can represent with 10 binary digits):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, let’s create a neural network with random initial weights. It will have
    10 input neurons (since we’re representing our inputs as 10-dimensional vectors)
    and 4 output neurons (since we’re representing our targets as 4-dimensional vectors).
    We’ll give it 25 hidden units, but we’ll use a variable for that so it’s easy
    to change:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'That’s it. Now we’re ready to train. Because this is a more involved problem
    (and there are a lot more things to mess up), we’d like to closely monitor the
    training process. In particular, for each epoch we’ll track the sum of squared
    errors and print them out. We want to make sure they decrease:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: This will take a while to train, but eventually the loss should start to bottom
    out.
  prefs: []
  type: TYPE_NORMAL
- en: 'At last we’re ready to solve our original problem. We have one remaining issue.
    Our network will produce a four-dimensional vector of numbers, but we want a single
    prediction. We’ll do that by taking the `argmax`, which is the index of the largest
    value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can finally solve “FizzBuzz”:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: For me the trained network gets 96/100 correct, which is well above the VP of
    Engineering’s hiring threshold. Faced with the evidence, he relents and changes
    the interview challenge to “Invert a Binary Tree.”
  prefs: []
  type: TYPE_NORMAL
- en: For Further Exploration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Keep reading: [Chapter 19](ch19.html#deep_learning) will explore these topics
    in much more detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: My blog post on [“Fizz Buzz in Tensorflow”](http://joelgrus.com/2016/05/23/fizz-buzz-in-tensorflow/)
    is pretty good.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
