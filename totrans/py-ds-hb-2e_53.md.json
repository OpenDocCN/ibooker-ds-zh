["```py\nIn [1]: %matplotlib inline\n        import matplotlib.pyplot as plt\n        plt.style.use('seaborn-whitegrid')\n        import numpy as np\n```", "```py\nIn [2]: # Generate some data\n        from sklearn.datasets import make_blobs\n        X, y_true = make_blobs(n_samples=400, centers=4,\n                               cluster_std=0.60, random_state=0)\n        X = X[:, ::-1] # flip axes for better plotting\n```", "```py\nIn [3]: # Plot the data with k-means labels\n        from sklearn.cluster import KMeans\n        kmeans = KMeans(4, random_state=0)\n        labels = kmeans.fit(X).predict(X)\n        plt.scatter(X[:, 0], X[:, 1], c=labels, s=40, cmap='viridis');\n```", "```py\nIn [4]: from sklearn.cluster import KMeans\n        from scipy.spatial.distance import cdist\n\n        def plot_kmeans(kmeans, X, n_clusters=4, rseed=0, ax=None):\n            labels = kmeans.fit_predict(X)\n\n            # plot the input data\n            ax = ax or plt.gca()\n            ax.axis('equal')\n            ax.scatter(X[:, 0], X[:, 1], c=labels, s=40, cmap='viridis', zorder=2)\n\n            # plot the representation of the KMeans model\n            centers = kmeans.cluster_centers_\n            radii = [cdist(X[labels == i], [center]).max()\n                     for i, center in enumerate(centers)]\n            for c, r in zip(centers, radii):\n                ax.add_patch(plt.Circle(c, r, ec='black', fc='lightgray',\n                                        lw=3, alpha=0.5, zorder=1))\n```", "```py\nIn [5]: kmeans = KMeans(n_clusters=4, random_state=0)\n        plot_kmeans(kmeans, X)\n```", "```py\nIn [6]: rng = np.random.RandomState(13)\n        X_stretched = np.dot(X, rng.randn(2, 2))\n\n        kmeans = KMeans(n_clusters=4, random_state=0)\n        plot_kmeans(kmeans, X_stretched)\n```", "```py\nIn [7]: from sklearn.mixture import GaussianMixture\n        gmm = GaussianMixture(n_components=4).fit(X)\n        labels = gmm.predict(X)\n        plt.scatter(X[:, 0], X[:, 1], c=labels, s=40, cmap='viridis');\n```", "```py\nIn [8]: probs = gmm.predict_proba(X)\n        print(probs[:5].round(3))\nOut[8]: [[0.    0.531 0.469 0.   ]\n         [0.    0.    0.    1.   ]\n         [0.    0.    0.    1.   ]\n         [0.    1.    0.    0.   ]\n         [0.    0.    0.    1.   ]]\n```", "```py\nIn [9]: size = 50 * probs.max(1) ** 2  # square emphasizes differences\n        plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', s=size);\n```", "```py\nIn [10]: from matplotlib.patches import Ellipse\n\n         def draw_ellipse(position, covariance, ax=None, **kwargs):\n             \"\"\"Draw an ellipse with a given position and covariance\"\"\"\n             ax = ax or plt.gca()\n\n             # Convert covariance to principal axes\n             if covariance.shape == (2, 2):\n                 U, s, Vt = np.linalg.svd(covariance)\n                 angle = np.degrees(np.arctan2(U[1, 0], U[0, 0]))\n                 width, height = 2 * np.sqrt(s)\n             else:\n                 angle = 0\n                 width, height = 2 * np.sqrt(covariance)\n\n             # Draw the ellipse\n             for nsig in range(1, 4):\n                 ax.add_patch(Ellipse(position, nsig * width, nsig * height,\n                                      angle, **kwargs))\n\n         def plot_gmm(gmm, X, label=True, ax=None):\n             ax = ax or plt.gca()\n             labels = gmm.fit(X).predict(X)\n             if label:\n                 ax.scatter(X[:, 0], X[:, 1], c=labels, s=40, cmap='viridis',\n                            zorder=2)\n             else:\n                 ax.scatter(X[:, 0], X[:, 1], s=40, zorder=2)\n             ax.axis('equal')\n\n             w_factor = 0.2 / gmm.weights_.max()\n             for pos, covar, w in zip(gmm.means_, gmm.covariances_, gmm.weights_):\n                 draw_ellipse(pos, covar, alpha=w * w_factor)\n```", "```py\nIn [11]: gmm = GaussianMixture(n_components=4, random_state=42)\n         plot_gmm(gmm, X)\n```", "```py\nIn [12]: gmm = GaussianMixture(n_components=4, covariance_type='full',\n                               random_state=42)\n         plot_gmm(gmm, X_stretched)\n```", "```py\nIn [13]: from sklearn.datasets import make_moons\n         Xmoon, ymoon = make_moons(200, noise=.05, random_state=0)\n         plt.scatter(Xmoon[:, 0], Xmoon[:, 1]);\n```", "```py\nIn [14]: gmm2 = GaussianMixture(n_components=2, covariance_type='full',\n                                random_state=0)\n         plot_gmm(gmm2, Xmoon)\n```", "```py\nIn [15]: gmm16 = GaussianMixture(n_components=16, covariance_type='full',\n                                 random_state=0)\n         plot_gmm(gmm16, Xmoon, label=False)\n```", "```py\nIn [16]: Xnew, ynew = gmm16.sample(400)\n         plt.scatter(Xnew[:, 0], Xnew[:, 1]);\n```", "```py\nIn [17]: n_components = np.arange(1, 21)\n         models = [GaussianMixture(n, covariance_type='full',\n                                   random_state=0).fit(Xmoon)\n                   for n in n_components]\n\n         plt.plot(n_components, [m.bic(Xmoon) for m in models], label='BIC')\n         plt.plot(n_components, [m.aic(Xmoon) for m in models], label='AIC')\n         plt.legend(loc='best')\n         plt.xlabel('n_components');\n```", "```py\nIn [18]: from sklearn.datasets import load_digits\n         digits = load_digits()\n         digits.data.shape\nOut[18]: (1797, 64)\n```", "```py\nIn [19]: def plot_digits(data):\n             fig, ax = plt.subplots(5, 10, figsize=(8, 4),\n                                    subplot_kw=dict(xticks=[], yticks=[]))\n             fig.subplots_adjust(hspace=0.05, wspace=0.05)\n             for i, axi in enumerate(ax.flat):\n                 im = axi.imshow(data[i].reshape(8, 8), cmap='binary')\n                 im.set_clim(0, 16)\n         plot_digits(digits.data)\n```", "```py\nIn [20]: from sklearn.decomposition import PCA\n         pca = PCA(0.99, whiten=True)\n         data = pca.fit_transform(digits.data)\n         data.shape\nOut[20]: (1797, 41)\n```", "```py\nIn [21]: n_components = np.arange(50, 210, 10)\n         models = [GaussianMixture(n, covariance_type='full', random_state=0)\n                   for n in n_components]\n         aics = [model.fit(data).aic(data) for model in models]\n         plt.plot(n_components, aics);\n```", "```py\nIn [22]: gmm = GaussianMixture(140, covariance_type='full', random_state=0)\n         gmm.fit(data)\n         print(gmm.converged_)\nOut[22]: True\n```", "```py\nIn [23]: data_new, label_new = gmm.sample(100)\n         data_new.shape\nOut[23]: (100, 41)\n```", "```py\nIn [24]: digits_new = pca.inverse_transform(data_new)\n         plot_digits(digits_new)\n```"]