- en: Chapter 3\. Understanding Data Quality
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第三章 理解数据质量
- en: Data is everywhere. It’s automatically generated by our mobile devices, our
    shopping activities, and our physical movements. It’s captured by our electric
    meters, public transportation systems, and communications infrastructure. And
    it’s used to estimate our health outcomes, our earning potential, and our credit
    worthiness.^([1](ch03.html#idm45143427340352)) Economists have even declared that
    data is the “new oil,”^([2](ch03.html#idm45143427337760)) given its potential
    to transform so many aspects of human life.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 数据无处不在。它由我们的移动设备自动生成，我们的购物活动和身体活动。它由我们的电表、公共交通系统和通信基础设施捕获。它用于估计我们的健康结果、收入潜力和信用价值^([1](ch03.html#idm45143427340352))。经济学家甚至宣称数据是“新的石油”^([2](ch03.html#idm45143427337760))，因其有可能转变人类生活的许多方面。
- en: While data may be plentiful, however, the truth is that *good* data is scarce.
    The claim of “the data revolution” is that, with enough data, we can better understand
    the present and improve—or even predict—the future. For any of that to even be
    possible, however, the data underlying those insights has to be high quality.
    Without good-quality data, all of our efforts to wrangle, analyze, visualize,
    and communicate it will, at best, leave us with no more insight about the world
    than when we started. While that would be an unfortunate waste of effort, the
    consequences of failing to recognize that we have poor-quality data is even worse,
    because it can lead us to develop a seemingly rational but dangerously distorted
    view of reality. What’s more, because data-driven systems are used to make decisions
    at scale, the harms caused by even a small amount of bad data can be significant.
    Sure, data about hundreds or even thousands of people may be used to “train” a
    machine learning model. But if that data is not representative of the population
    to which the model will be applied, the repercussions of that system can affect
    hundreds or thousands of times the number of people in the original dataset. Because
    the stakes are so high, ensuring data quality is an essential part of data wrangling.
    But what does it mean for data to be “high quality”? My view is that data is high
    quality only if it is both *fit* for purpose and has high internal *integrity*.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管数据可能非常丰富，但事实是*好的*数据却是稀缺的。所谓的“数据革命”声称，有了足够的数据，我们可以更好地理解现在，改进甚至预测未来。然而，要实现任何这些可能性，这些洞察背后的数据必须是高质量的。没有好质量的数据，我们所有的努力去整理、分析、可视化和传播数据，充其量只会让我们对世界的了解毫无进展。虽然这将是一种不幸的努力浪费，但没有意识到我们拥有质量低劣的数据的后果更为严重，因为这可能导致我们形成一个看似合理但危险扭曲的现实观。更重要的是，因为数据驱动的系统被用来大规模做决策，即使有少量不好的数据也可能造成重大危害。当然，可以用来“训练”机器学习模型的数据可能涉及数百甚至数千人的数据。但是，如果这些数据不代表模型将应用到的人群，那么系统可能造成的后果将影响到原始数据集中人数的数百甚至数千倍。由于风险如此之高，确保数据质量是数据整理的一个重要部分。但是，数据“高质量”意味着什么呢？在我看来，只有数据既*适合*特定目的，又具有高度的内部*完整性*，才能称得上是高质量的数据。
- en: 'What does each of those terms actually mean? That is exactly what we’ll explore,
    in depth, in this chapter. We’ll begin by discussing the concept of data *fit*,
    which relates to the appropriateness of data for use in a particular context,
    or to answer a particular question. We’ll then break down the many aspects of
    data *integrity*: the characteristics of a dataset that influence both its fitness
    for purpose and the types of analyses we can responsibly use it for. Finally,
    we’ll discuss some tools and strategies for finding and working with data that
    can help you maximize its overall quality, lending confidence and credibility
    to the work that you produce with it.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，每一个术语实际上意味着什么呢？这正是我们将在本章深入探讨的内容。我们将从讨论数据*适合性*的概念开始，这与数据在特定上下文中的适用性或用于回答特定问题相关。然后，我们将详细分析数据*完整性*的许多方面：影响数据适合性及我们可以负责使用的分析类型的数据集特征。最后，我们将讨论一些工具和策略，帮助您找到和处理数据，以提升其整体质量，从而增强您与其相关工作的信心和可信度。
- en: 'In case you start to find any of this tedious, let me repeat my exhortations
    from [“What Is “Data Wrangling”?”](ch01.html#describing_data_wrangling): trying
    to “skip over” the work of assessing data quality can only undermine your data
    wrangling efforts. At best, you’ll go to share your work and encounter questions
    about your process that you don’t have answers for. At worst, you’ll end up promoting
    “insights” that are both wrong and do active harm. Along the way, you’ll *also*
    be cheating yourself out of good technical skills, because solving data quality
    problems is where you’ll expand your programming knowledge the most. If you truly
    want to be good at the work of data wrangling, assessing data quality *has* to
    be part of your practice.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你开始觉得这些内容有点乏味，让我从[《什么是“数据整理”？》](ch01.html#describing_data_wrangling)中再次劝告你：试图“跳过”评估数据质量的工作只会削弱你在数据整理方面的努力。充其量，你在分享工作时会遇到关于你的流程的问题，而你却无法回答。最糟糕的情况下，你可能会推广一些既错误又有害的“见解”。在这个过程中，你也会欺骗自己，因为解决数据质量问题是你扩展编程知识的最佳途径。如果你真的想在数据整理工作中表现出色，评估数据质量必须成为你实践的一部分。
- en: Assessing Data Fit
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估数据的适应性
- en: 'Perhaps one of the most common misconceptions about data wrangling is that
    it is a predominantly *quantitative* process, that is, that data wrangling is
    mostly about working with numbers, formulas, and code. In fact, irrespective of
    the type of data you’re dealing with—it could be anything from temperature readings
    to social media posts—the core work of data wrangling involves making judgment
    calls: from whether your data accurately represents the phenomenon you’re investigating,
    to what to do about missing data points and whether you have enough data to generate
    any real insight at all. That first concept—the extent to which a given dataset
    accurately represents the phenomenon you’re investigating—is broadly what I mean
    by its *fit*, and assessing your dataset’s *fit*ness for purpose is much more
    about applying informed judgment than it is about applying mathematical formulas.
    The reason for this is quite simple: the world is a messy place, and what may
    seem like even the simplest data about it is always filtered through some kind
    of human lens. Take something as straightforward as measuring the temperature
    in your workspace over the course of a week. In theory, all you need to do is
    get a thermometer, put it in the space, and note down the reading every day. Done,
    right?'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 也许关于数据整理最常见的误解之一是它是一个主要*定量*的过程，也就是说，数据整理主要是处理数字、公式和代码。事实上，无论你处理的是什么类型的数据——从温度读数到社交媒体帖子——数据整理的核心工作都涉及做出判断：从你的数据是否准确地反映了你正在调查的现象，到如何处理缺失的数据点，以及是否有足够的数据来生成任何真正的见解。第一个概念——一个给定数据集如何准确地反映你正在调查的现象——大体上就是我所说的它的*适配性*，评估你的数据集对于特定目的的*适配*远不只是应用数学公式那么简单。其原因很简单：这个世界是一个混乱的地方，即使是最简单的关于它的数据也总是通过某种人类视角来过滤。以测量你工作空间一周内温度为例。理论上，你只需要一个温度计，把它放在空间里，每天记录一下读数就行了。完成了，对吧？
- en: Or are you? Let’s start with your equipment. Did you use a digital thermometer
    or a mercury thermometer? Where in the space did you place it? Is it near a door,
    a window, or a heating or cooling source? Did you take the reading at the same
    time every day? Is the thermometer ever in direct sunlight? What is the typical
    humidity level?
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 或者呢？让我们从你的设备开始说起。你用的是数字温度计还是水银温度计？你把它放在哪里？是在靠近门、窗户、还是加热或冷却源的地方？你每天都在同一时间测量吗？温度计是否曾直接暴露在阳光下？典型的湿度水平是多少？
- en: You may think I’m introducing a contrived level of complexity here, but if you’ve
    ever lived in a shared space (like an apartment building), you’ve probably been
    through the experience of *feeling* like it’s much warmer or colder than what
    some thermometer said. Likewise, if you’ve ever looked after a child who’s ill,
    you’re likely all too familiar with the different body temperature readings that
    you’ll get with different types of thermometers—or even with the same one, just
    minutes apart.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能认为我在这里引入了一种人为的复杂性水平，但如果你曾经生活在共享空间（比如公寓大楼）中，你可能经历过感觉像比某个温度计所说的温度更高或更低的体验。同样，如果你曾经照顾过生病的孩子，你很可能对不同类型温度计或甚至同一温度计不同时间读数的体验感到非常熟悉。
- en: In other words, there are a huge number of factors contributing to that two-
    or three-digit temperature you record—and the number itself doesn’t provide information
    about any of them. That’s why when you begin the process of trying to answer a
    question with data, it’s not enough to know just what is in the dataset; you need
    to know about the processes and mechanisms used to *collect* it. Then, given everything
    you know about how the data was gathered, you need to determine if it can really
    be used to answer your specific question in a meaningful way.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，导致你记录的两位数或三位数温度的因素有很多，而这个数字本身并不提供任何信息。这就是为什么当你开始尝试用数据回答问题时，仅仅知道数据集中包含什么是不够的；你需要了解收集数据时使用的过程和机制。然后，根据你对数据收集方式的了解，你需要确定它是否真的可以用来有意义地回答你的具体问题。
- en: 'Of course, this problem is neither new nor unique; it’s the same challenge
    that all scientific fields face in their efforts to discover new information about
    the world. Cancer research could hardly advance if every single researcher had
    to conduct every single study themselves; without the ability to build on the
    work of others, scientific and technological progress would grind to a halt (if
    not go off the rails entirely). Because of this, over time the scientific community
    has developed three key metrics for determining the appropriateness or fit of
    a dataset for answering a given question: *validity*, *reliability*, and *representativeness*.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这个问题既不新鲜也不独特；这是所有科学领域在努力探索世界新信息时面临的挑战。如果每个研究人员都必须亲自进行每项研究，癌症研究几乎无法取得进展；如果没有利用他人工作的能力，科学和技术进步将停滞不前（如果不是完全偏离轨道）。因此，科学界随着时间的推移已经发展出了三个用于确定数据集是否适合回答给定问题的关键指标：*有效性*、*可靠性*和*代表性*。
- en: Validity
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**有效性**'
- en: At its most basic, *validity* describes the extent to which something measures
    what it is supposed to. In our room temperature example, this would mean ensuring
    that the type of thermometer you’ve chosen will actually measure the air temperature
    rather than something else. For example, while traditional liquid-in-glass thermometers
    will probably capture air temperature well, infrared thermometers will tend to
    capture the temperature of whatever surface they’re pointed at. So even with something
    as seemingly basic as room temperature, you need to understand the tools and methods
    used to collect your data readings in order to ensure their *validity* with respect
    to your question.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在其最基本的层面上，*有效性*描述了某物测量其应测量的程度。在我们的室温例子中，这意味着确保你选择的温度计确实能够测量空气温度，而不是其他什么东西。例如，传统的液体玻璃温度计可能很好地捕捉空气温度，而红外线温度计则倾向于捕捉其指向的任何表面的温度。因此，即使是看似基础的室温问题，你也需要理解用于收集数据读数的工具和方法，以确保它们在与你的问题相关的*有效性*方面的适用性。
- en: Unsurprisingly, things only get more involved when we’re not collecting data
    about common physical phenomena. *Construct validity* describes the extent to
    which your data measurements effectively capture the (usually abstract) *construct*,
    or idea, you’re trying to understand. For example, let’s say you want to know
    which are the “best” schools in your area. What data can help you answer that
    question? First we have to recognize that the term *best* is imprecise. Best in
    what way? Are you interested in which school has the highest graduation rate?
    Standardized test scores? School-assigned grades? Teacher evaluations? Student
    satisfaction? Extracurricular participation?
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 毫不奇怪，当我们不是收集关于常见物理现象的数据时，事情会变得更加复杂。*建构有效性*描述了你的数据测量有效地捕捉了（通常是抽象的）*建构*或思想，你试图理解的内容。例如，假设你想知道你所在地区哪些学校是“最好的”。哪些数据可以帮助你回答这个问题？首先，我们必须认识到术语*最好*是不精确的。最好是指什么？你关心哪所学校拥有最高的毕业率？标准化考试成绩？学校分配的成绩？教师评价？学生满意度？课外活动参与度？
- en: In order to use data to begin to answer this question, you first need to articulate
    two things. First, “best” *for whom*? Are you trying to answer this question for
    your own child? A friend’s? Having answered that, you’ll be better able to complete
    the second task, which is *operationalizing* your specific idea of “best.” If
    your friend’s child loves sports, for example, extracurricular activities might
    be more important than academics.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 为了利用数据开始回答这个问题，您首先需要阐明两件事。首先，“最佳” *适合谁*？您是在为自己的孩子回答这个问题吗？朋友的孩子？回答了这个问题后，您将更能够完成第二项任务，即*操作化*
    您对“最佳”具体想法的理解。例如，如果您朋友的孩子喜欢运动，课外活动可能比学术更重要。
- en: In data analysis, this process of selecting measures is known as *operationalizing
    a construct*, and it inevitably requires choosing among—and balancing—proxies
    for the idea or concept you are trying to understand. These proxies—like graduation
    rates, test scores, extracurricular activities, and so on—are things *about which
    you can* *collect* *data* that you are choosing to use to represent an abstract
    concept (“best” school) *that cannot be measured directly*. Good-quality data,
    to say the least, must have good *construct validity* with respect to your question,
    otherwise your data wrangling results will be meaningless.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据分析中，选择测量的过程被称为*操作化构建*，它无可避免地需要在您尝试理解的概念或想法中选择并平衡代理。这些代理 —— 如毕业率、考试成绩、课外活动等
    —— 是您可以*收集* *数据*的事物，您选择使用它们来代表无法直接测量的抽象概念（“最佳”学校）。良好的数据至少必须在您的问题方面具有良好的*结构有效性*，否则您的数据处理结果将毫无意义。
- en: The other type of validity that is important for data fit is *content validity*.
    This type of validity has to do with how complete your data is for a given proxy
    measurement. In the “best” school example, let’s say you have determined that
    grades are relevant for determining what school is best, but you only have grades
    for history and physical education courses available. Though for many people grade
    data might, in principle, have *construct validity* for identifying the best school,
    having grade data for only two types of courses wouldn’t be sufficient to satisfy
    the requirement for *content validity*—and for high-quality data, you need to
    have *both*.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 数据拟合中另一种重要的有效性类型是*内容有效性*。这种有效性类型涉及到给定代理测量的数据完整性。在“最佳”学校的例子中，假设您已确定成绩对于确定最佳学校很重要，但您只有历史和体育课程的成绩数据可用。尽管对于许多人来说，成绩数据原则上可能具有*结构有效性*以确定最佳学校，但仅有两种类型课程的成绩数据是不足以满足*内容有效性*的要求的
    —— 而对于高质量的数据，您需要*两者兼顾*。
- en: Reliability
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可靠性
- en: 'Within a dataset, the *reliability* of a given measure describes its *accuracy*
    and *stability*. Together, these help us assess whether the same measure taken
    twice in the same circumstances will give us the same—or at least very similar—results.
    To revisit our temperature example: taking a child’s temperature with an oral
    thermometer is not likely to be very *reliable*, because the process requires
    that the child keep their mouth closed for a relatively long time (which, in my
    experience, they’re not great at). By contrast, taking a child’s temperature under
    their arm might be more reliable—because you can hug them to keep the thermometer
    in place—but it may not provide as *accurate* a reading of the child’s true internal
    body temperature as some other methods. This is why most medical advice lists
    different temperature thresholds for a fever in children, depending on which method
    you use to take their temperature.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据集内部，给定测量的*可靠性*描述了其*准确性*和*稳定性*。这两者帮助我们评估在相同情况下两次进行相同测量是否会得到相同或非常相似的结果。回顾我们的体温例子：用口温度计测量儿童体温可能不太*可靠*，因为这个过程需要孩子闭口相对较长时间（根据我的经验，他们在这方面表现不佳）。相比之下，腋下测量可能更可靠
    —— 因为您可以拥抱他们以保持温度计的位置，但可能不能像其他方法那样提供孩子真实的体内温度读数的*准确性*。这就是为什么大多数医疗建议根据使用哪种方法测量体温为儿童设置不同的发热温度阈值的原因。
- en: With abstract concepts and real-world data, determining the reliability of a
    data measure is especially tricky, because it is never really possible to collect
    the data more than once—whether because the cost is prohibitive, the circumstances
    can’t be replicated, or both. In those cases, we typically estimate reliability
    by comparing one similar group to another, using either previously or newly collected
    data. So even though a dramatic fluctuation in a school’s standardized test scores
    from one year to the next indicates that those scores may not be a *reliable*
    measure of school quality, this inconsistency itself is only part of the story.
    After all, those test scores *might* reflect the quality of teaching, but they
    might also reflect a change in the test being administered, how it was scored,
    or some other disruption to the learning or test-taking environment. In order
    to determine whether the standardized test data is *reliable* enough to be part
    of your “best” school assessment, you would need to look at comparison data from
    other years or other schools, in addition to learning more about the broader circumstances
    that may have led to the fluctuation. In the end, you may conclude that *most*
    of the test score information is reliable enough to be included but that a few
    particular data points should be removed, or you may conclude that the data is
    too unreliable to be part of a high-quality data process.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 对于抽象概念和现实世界的数据，确定数据测量的可靠性尤为棘手，因为真的无法再次收集数据——无论是因为成本过高、情况无法复制，还是两者兼而有之。在这些情况下，我们通常通过将一个类似群体与另一个类似群体进行比较来估计可靠性，使用之前或新收集的数据。因此，即使学校在一年内的标准化考试成绩出现显著波动表明这些分数可能不是学校质量的*可靠*衡量标准，这种不一致本身只是故事的一部分。毕竟，这些考试成绩可能反映了教学质量，但它们也可能反映了考试被实施的变化、评分方式或其他对学习或考试环境的干扰。为了确定标准化考试数据是否足够*可靠*以成为你的“最佳”学校评估的一部分，你需要查看其他年份或其他学校的比较数据，同时了解可能导致波动的更广泛情况。最终，你可能会得出结论，大多数考试成绩信息足够可靠以纳入，但应删除一些特定数据点，或者你可能会得出结论，数据过于不可靠，不能成为高质量数据流程的一部分。
- en: Representativeness
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '*代表性*'
- en: The key value proposition for data-driven systems is that they allow us to generate
    insights—or even predictions—about people and phenomena that are too massive or
    too complex for humans to reason about effectively. By wrangling and analyzing
    data, the logic goes, we can make decisions faster and more fairly. Given the
    powerful computational tools that even individuals—to say nothing of companies—have
    access to these days, there’s no doubt that data-driven systems can generate “decisions”
    more quickly than humans can. Whether those insights are an accurate portrait
    of a particular population or situation, however, depends directly on the *representativeness*
    of the data being used.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 数据驱动系统的关键价值主张是它们允许我们生成关于人和现象的见解——甚至预测——这些人或现象对于人类来说过于庞大或过于复杂，无法有效推理。通过整理和分析数据，逻辑是，我们可以做出比人类更快速、更公平的决策。鉴于即使是个人——更不用说公司了——如今都可以访问的强大计算工具，毫无疑问，数据驱动系统可以比人类更快地生成“决策”。然而，这些见解是否是特定人群或情况的准确描述，直接取决于所使用数据的*代表性*。
- en: Whether a dataset is sufficiently representative depends on a few things, the
    most significant of which goes back to the “for whom?” question we discussed in
    [“Validity”](#validity). If you’re trying to design a new course schedule for
    a specific grade school, you may be able to collect data about its entire population.
    If all the other criteria for data fitness have been met, then you already know
    that your data is representative, because you have collected data directly from
    or about the entire population to which it will apply.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集是否足够代表性取决于几个因素，其中最重要的回到了我们在[“有效性”](#validity)中讨论过的“为谁？”问题。如果你试图为特定小学设计新的课程表，你可能能够收集关于整个学生群体的数据。如果数据适应性的所有其他标准都已满足，那么你已经知道你的数据是具有代表性的，因为你已经直接从或关于将适用的整个群体收集了数据。
- en: But what if you’re trying to complete the same task for an entire city’s worth
    of schools? It’s deeply unlikely that you’ll succeed in collecting data about
    every single student in every single school, which means that you’ll be relying
    on input from only a subset of the students when you try to design the new schedule.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，如果你试图为整个城市的学校完成相同的任务呢？几乎不可能收集到每个学校每个学生的数据，这意味着当你尝试设计新的课程表时，你将仅依赖于部分学生的输入。
- en: Anytime you’re working with a subset or *sample* in this way, it’s crucial to
    make sure that it is *representative* of the broader population to which you plan
    to apply your findings. While proper sampling methodology is beyond the scope
    of this book,^([4](ch03.html#idm45143427230496)) the basic idea is that in order
    for your insights to accurately generalize to a particular community of people,
    the data sample you use must proportionally reflect that community’s makeup. That
    means that you need to invest the time and resources to understand a number of
    things about that community *as a whole* before you can even know if your sample
    is representative.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 无论何时，当你以这种方式处理子集或*样本*时，确保它*代表*你计划应用发现的更广泛人群至关重要。虽然适当的抽样方法学超出了本书的范围，^([4](ch03.html#idm45143427230496))
    基本思想是，为了你的见解能准确地推广到特定社群，你使用的数据样本必须按比例反映该社群的构成。这意味着在你甚至能知道你的样本是否代表性之前，你需要投入时间和资源来全面了解该社群的一些事情*作为一个整体*。
- en: 'At this point you may be thinking: wait, if we could already get information
    about the whole population, we wouldn’t need a sample in the first place! And
    that’s true—sort of. In many cases, it’s possible to get *some* information about
    an entire community—just not precisely the information we need. In our school-scheduling
    scenario, for example, we would ideally get information about how—and how long—students
    travel to and from school each day, as well as some sense of their caretakers’
    schedules. But the information we might *have* about the entire school population
    (if we are working cooperatively with the school system) will probably include
    only things like home address, school address, and perhaps the [type of transportation
    support](https://schools.nyc.gov/school-life/transportation/bus-eligibility).
    Using this information, likely in conjunction with some additional administrative
    information, we could begin to create estimates for the proportion of certain
    types of student commuters that exist in the *entire* population, and then seek
    to replicate those proportions in selecting a *representative sample* from our
    survey results. Only at that point would we be ready to move on to the next step
    of the data wrangling process.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 此时你可能会想：等等，如果我们已经可以获得整个人群的信息，我们一开始就不需要样本了！这是真的——在某种程度上是真的。在许多情况下，我们可以获得一些关于整个社群的信息——只是不是我们需要的精确信息。例如，在我们的学校排课场景中，我们理想情况下会获取有关学生每天如何以及多长时间通勤的信息，以及其监护人的日程安排的一些了解。但是，我们可能*已经*有关于整个学校人口的信息（如果我们与学校系统合作），可能仅包括家庭地址、学校地址，也许还有[交通支持类型](https://schools.nyc.gov/school-life/transportation/bus-eligibility)。利用这些信息，可能还结合一些额外的行政信息，我们可以开始为特定类型学生通勤者在*整个*人群中所占比例创建估计，然后试图在调查结果中选择*代表性样本*以复制这些比例。只有在这一点上，我们才准备好进入数据整理过程的下一步。
- en: As you can see, ensuring representativeness demands that we carefully consider
    which characteristics of a population are relevant to our data wrangling question
    *and* that we seek out enough additional information to ensure that our dataset
    proportionally represents those characteristics. Perhaps unsurprisingly, this
    is the data fitness test that many data-driven goods and services fail on, again
    and again. While companies and researchers may tout the *quantity* of data they
    use to develop in their systems, the reality is that most datasets that are readily
    available to companies and researchers tend to *not* be representative of, say,
    the US or global population. For example, data about search engine trends, social
    media activity, public transit usage, or smartphone ownership, for example, are
    all extremely *unlikely* to be representative of the broader population, since
    they are inevitably influenced by things like internet access and income level.
    This means that communities are *over*represented in these datasets while others
    are (sometimes severely) *under*represented. The result is systems that don’t
    *generalize*—like facial recognition systems [that cannot “see” Black faces](http://proceedings.mlr.press/v81/buolamwini18a/buolamwini18a.pdf).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所见，确保代表性要求我们仔细考虑人群的哪些特征与我们的数据整理问题相关，并寻找足够的额外信息，以确保我们的数据集比例地代表这些特征。也许并不奇怪的是，这是许多数据驱动产品和服务在数据适应性测试中屡屡失败的原因。虽然公司和研究人员可能吹嘘他们用于在系统中开发的数据的*数量*，但事实是，大多数对公司和研究人员来说容易获取的数据集往往不代表例如美国或全球人口。例如，关于搜索引擎趋势、社交媒体活动、公共交通使用或智能手机拥有情况的数据，例如，都极不可能代表更广泛的人口，因为它们不可避免地受到诸如互联网访问和收入水平之类的影响。这意味着一些社区在这些数据集中*过度*代表，而其他一些社区（有时严重地）*不足*代表。其结果是系统不*泛化*，比如面部识别系统[无法“看到”黑人面孔](http://proceedings.mlr.press/v81/buolamwini18a/buolamwini18a.pdf)。
- en: If you are faced with nonrepresentative data, what do you do? At the very least,
    you will need to revise (and clearly communicate) your “for whom?” assessment
    to reflect whatever population it *does* represent; this is the only community
    for whom your data wrangling insights will be valid. Also keep in mind that representativeness
    can only ensure that the outcome of your data wrangling efforts accurately reflects
    reality; it is not a value judgment on whether that reality should be *perpetuated*.
    If the outcome of your data wrangling effort will be used to make changes to a
    system or organization, the end of your data wrangling process is really just
    the starting point for thinking about what should be done with your insights,
    especially with respect to complex issues like [fairness](https://youtube.com/watch?v=jIXIuYdnyyk).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 面对不具代表性的数据，你会怎么做呢？至少，你需要修订（并清楚地传达）你的“为谁？”评估，以反映它实际上代表的人群；这是你的数据整理洞察力有效的唯一社区。还要记住，代表性只能确保你的数据整理努力的结果准确反映现实；这并不是对是否应该“持续”这种现实的价值判断。如果你的数据整理工作的结果将用于对系统或组织进行更改，那么你的数据整理过程的结束实际上只是开始考虑如何处理你的洞察力，特别是对复杂问题如[公平性](https://youtube.com/watch?v=jIXIuYdnyyk)。
- en: 'This is true even if you have data about an entire population. For example,
    if you want to know which organizations have received a certain type of grant,
    then the “population” or community you’re interested in is just those grant recipients.
    At the same time, checking your data for representativeness against the population
    at large still has value: if one or more communities are over- or underrepresented
    in your population of grant recipients, it may hint at hidden factors influencing
    who receives that money.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 即使你拥有整个人口的数据，这也是真实的。例如，如果你想知道哪些组织获得了某种类型的资助，那么你感兴趣的“人口”或社区只是那些资助接收者。同时，检查你的数据是否代表整体人口仍然有价值：如果一个或多个社区在你的资助接收者群体中过度或不足代表，这可能暗示着影响谁获得这笔资金的隐藏因素。
- en: Assessing Data Integrity
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估数据完整性
- en: Data *fit* is essentially about whether you have the right data for answering
    your data wrangling question. Data *integrity*, on the other hand, is largely
    about whether the data you have can support the analyses you’ll need to perform
    in order to answer that question. As you’ll see throughout this section, there
    are a *lot* of different aspects of the data to consider when performing an integrity
    assessment. But does a given dataset need to have *all* of them in order to be
    high integrity, and therefore high quality? Not necessarily. While some are essential,
    the importance of others depends on your specific question and the methods you’ll
    need to answer it. And with rare exceptions, many are characteristics of the data
    that you will enhance and develop as part of your data wrangling process.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 数据“适合”实质上是关于你是否拥有正确的数据来回答你的数据整理问题。另一方面，“完整性”主要是关于你拥有的数据是否能支持你需要执行的分析来回答这个问题。正如你将在本节中看到的，执行完整性评估时需要考虑数据的许多不同方面。但是，一个给定的数据集是否需要拥有所有这些特征才能具备高完整性和高质量？不一定。虽然有些是必不可少的，但其他特征的重要性取决于你具体的问题和你需要回答它的方法。而且除了少数例外，许多特征是你将作为数据整理过程的一部分增强和开发的数据特征。
- en: In other words, while ensuring data *fit* is nonoptional, the types and degree
    of *integrity* that your particular data wrangling project requires will vary.
    Of course, the more of these requirements your data meets, the more useful it
    will be—both to you and others.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，确保数据“适合”的情况是不可选的，但你的数据整理项目需要的“完整性”类型和程度会有所不同。当然，你的数据满足的要求越多，它对你和他人都越有用。
- en: In general, a high-integrity dataset will, to one degree or another, be:^([5](ch03.html#idm45143427185920))
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 一般而言，一个高完整性的数据集将在某种程度上包括以下特征：^([5](ch03.html#idm45143427185920))
- en: Necessary, but not sufficient
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 必要但不足的
- en: Of known provenance
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 已知来源的
- en: Well-annotated
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 良好注释的
- en: Important
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的
- en: Timely
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 及时的
- en: Complete
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 完整的
- en: High volume
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大容量的
- en: Multivariate
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多变量的
- en: Atomic
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 原子的
- en: Achievable
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 可实现的
- en: Consistent
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一致的
- en: Clear
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 清晰的
- en: Dimensionally structured
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 维度结构化的
- en: As I have already mentioned, however, not all of these data integrity characteristics
    are equally important. Some of them are nonnegotiable, while others are almost
    always the *result* of certain steps in the data wrangling process, not precursors
    to it. And while the goal is for your data to have as many of these characteristics
    as possible before you begin your analysis, there is always a balance to be struck
    between more fully elaborating your data and completing your work in time for
    your insights to be useful.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我之前提到的，然而，并非所有这些数据完整性特征同等重要。其中一些是必不可少的，而另一些几乎总是数据整理过程中某些步骤的结果，而不是其先导条件。而在你开始分析之前，希望你的数据具备尽可能多的这些特征之间，总是需要在更详细地描述你的数据和及时完成你的工作之间取得平衡。
- en: In this sense, assessing data integrity can be a useful way to prioritize your
    data wrangling efforts. For example, if a dataset is missing either of the “necessary,
    but not sufficient” characteristics, you may as well move on from it entirely.
    If it’s missing one or two of the characteristics in [“Important”](#important),
    it may still be possible to salvage the data you’re working with by combining
    it with others or limiting the scope of your analyses—and claims. Meanwhile, developing
    the characteristics in [“Achievable”](#achievable) is often the *goal* of your
    data wrangling process’s “cleaning” step, rather than something you can expect
    most real-world data to have when you first encounter it.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个意义上，评估数据完整性可以成为优先考虑数据整理工作的一种有用方式。例如，如果一个数据集缺少“必要但不足的”特征中的任何一个，你可能需要完全放弃它。如果它在“重要的”特征中缺少一两个，可能仍然有可能通过与其他数据的合并或限制分析和声明的范围来挽救你正在处理的数据。与此同时，开发“可实现的”特征通常是你的数据整理过程中“清理”步骤的目标，而不是你第一次遇到它时就可以期待大多数现实世界数据具备的内容。
- en: In the end, the degree to which your dataset embodies many of the characteristics
    that follow will depend on the time you have to invest in your data wrangling
    project, but without a good solid majority of them, your insights will be limited.
    As will be illustrated (in detail!) in [Chapter 6](ch06.html#chapter6), this variability
    is precisely why data wrangling and data quality are so thoroughly intertwined.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，你的数据集能够体现以下特征的程度，取决于你投入数据整理项目的时间，但如果缺乏其中大部分特征，你的洞察力将受限。正如将在[第6章](ch06.html#chapter6)详细展示的那样，这种变异性正是数据整理和数据质量如此紧密交织的原因。
- en: Necessary, but Not Sufficient
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 必要，但不足以
- en: Most of the time, the data we’re wrangling was compiled by someone else—or a
    whole collection of people and processes that we don’t have direct access to.
    At the same time, we need to be able to stand behind both our data wrangling process
    and any insights we derive from it. This means that there are a couple of data
    characteristics that are really *essential* to a successful data wrangling process.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数情况下，我们正在处理的数据是由其他人编制的——或者是一群我们无法直接接触的人和流程。与此同时，我们需要能够支持我们的数据处理过程以及从中得出的任何见解。这意味着有几个数据特征对于成功的数据处理过程非常*重要*。
- en: Of known provenance
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 来源已知
- en: 'As discussed in [“What Is “Data Quality”?”](ch01.html#what_is_data_quality),
    data is the output of human decisions about what to measure and how. This means
    that using a dataset collected by others requires putting a significant amount
    of trust in them, especially because independently verifying *every single data
    point* is rarely possible; if it were, you would probably just collect your own
    data instead. This is why knowing the *provenance* of a dataset is so important:
    if you don’t know who compiled the data, the methods that they used, and/or the
    purpose for which they collected it, you will have a very hard time judging whether
    it is *fit* for your data wrangling purpose, or how to correctly interpret it.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 如[“什么是“数据质量”？”](ch01.html#what_is_data_quality)中所讨论的，数据是关于衡量什么以及如何衡量的人类决策的产物。这意味着使用他人收集的数据集需要对他们进行相当程度的信任，尤其是因为很少可能独立验证*每一个数据点*；如果可以的话，你可能会选择自己收集数据。这就是为什么了解数据集的*来源*如此重要：如果你不知道谁编制了数据、他们使用的方法和/或他们收集数据的目的，你将很难判断它是否适合你的数据处理目的，或者如何正确解释它。
- en: Of course, this doesn’t mean you need to know the birthdays and favorite colors
    of everyone who helped build a given dataset. But you *should* try to find out
    enough about their professional backgrounds, motivations for collecting the data
    (is it legally mandated, for example?), and the methods they employed so that
    you have some sense of which measures you’ll want to corroborate versus those
    that might be okay to take at face value. Ideally, both information about the
    data authors and sufficient documentation about these processes will be readily
    enough available that you can answer all of these questions about the data’s *provenance*
    fairly quickly. If they prove hard to locate, however, you may wish to move on;
    since you need this information to assess data *fit*, your time may be better
    spent looking for a different dataset or even collecting the data yourself.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这并不意味着你需要知道每个帮助构建给定数据集的人的生日和最喜欢的颜色。但是，你*应该*尽力了解足够的关于他们的专业背景、收集数据的动机（例如，它是否被法律要求）、以及他们使用的方法，这样你就能大致了解哪些测量值需要核实，哪些可能可以直接采用。理想情况下，关于数据作者的信息和关于这些过程的充分文档应该很容易获得，以便你可以迅速回答关于数据*来源*的所有这些问题。然而，如果他们难以找到，你可能会考虑放弃；因为你需要这些信息来评估数据*适合性*，你的时间可能花在寻找其他数据集上，甚至是自己收集数据。
- en: Well-annotated
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 良好注释
- en: 'A well-annotated dataset has enough surrounding information, or *metadata*,
    to make interpretation possible. This will include everything from high-level
    explanations of the data collection methodology to the “data dictionaries” that
    describe each data measure right down to its units. While this may seem straightforward,
    there are not always well-accepted standards for how such annotation information
    should be provided: sometimes it appears directly in data files or data entries
    themselves, and sometimes the information might be contained in separate files
    or documents in a location totally separate from where you retrieved the data.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 一个良好注释的数据集有足够的周围信息，或者*元数据*，使得解释成为可能。这将包括从数据收集方法的高层次解释到描述每个数据测量及其单位的“数据字典”等一切。尽管这可能看起来很简单，但并不总是有关于如何提供这些注释信息的公认标准：有时它直接出现在数据文件或数据条目本身中，有时信息可能包含在完全与检索数据位置不同的单独文件或文档中。
- en: However they are structured or provided, robust data annotation documents are
    an essential component of high-integrity data because without them, it’s impossible
    to apply any analyses or draw any inferences from it. For example, imagine trying
    to interpret a budget without knowing if the figures provided refer to dollars,
    thousands of dollars, or millions of dollars—it’s clearly impossible. Or for a
    very American expression of the importance of annotation documents like data dictionaries,
    know that they sometimes [require a lawsuit](https://trac.syr.edu/foia/ice/20210805)
    to get.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 无论数据的结构或提供方式如何，健壮的数据注释文档都是高完整性数据的重要组成部分，因为没有这些文档，就不可能应用任何分析或从中得出任何推断。例如，试想一下，试图解释一个预算而不知道提供的数字是美元、千美元还是百万美元，显然是不可能的。或者以一种非常美国化的表达来说明数据字典等注释文档的重要性，有时可能需要[起诉](https://trac.syr.edu/foia/ice/20210805)才能获得。
- en: Important
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 重要
- en: While it’s hard to get *anywhere* with data wrangling unless you have sufficient
    provenance and metadata information about a dataset, you’re still not going to
    get very far unless you have enough data, from the right time period(s), and at
    the right level of detail. That said, the following data characteristics are ones
    that we may often assess—and sometimes improve—with our own data wrangling work.
    So while it will certainly be easier to work with datasets that have these characteristics
    already, even those that fall short on a few of these may still be worth exploring.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在数据整理中，除非有关于数据集的充分溯源和元数据信息，否则你将无法进展，但是除非你有足够、来自正确时间段且具有适当详细级别的数据，否则你仍然无法走得太远。话虽如此，以下数据特征是我们可能经常评估的，并且有时可以通过我们自己的数据整理工作来改进。因此，尽管处理起来已经拥有这些特征的数据集肯定会更容易，但即使在一些方面表现不佳的数据集，仍然值得探索。
- en: Timely
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 及时
- en: How up to date is the data that you’re using? Unless you’re studying a historical
    period, ensuring that your data is recent enough to meaningfully describe the
    *current* state of the world is important—though how old is “too old” will depend
    on both the phenomenon you’re exploring and the frequency with which data about
    it is collected and released.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 你使用的数据有多新？除非你正在研究一个历史时期，确保你的数据足够新以有意义地描述当前世界的状态是重要的——尽管“太旧”是多老将取决于你正在探索的现象以及收集和发布关于它的数据的频率。
- en: For example, if you’re interested in neighborhood demographics but your most
    recent data is several years old, there’s a strong likelihood that things may
    have changed considerably since. For unemployment data, data older than a month
    will no longer be timely, while for stock market data, it takes just a few seconds
    for information to be considered too old to inform trades. Unless it’s about a
    field you’re already familiar with, assessing whether your data is timely will
    likely require some research with experts as well as the data publisher.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果你对社区人口统计数据感兴趣，但你最近的数据已经几年前的，那么很可能自那时以来情况已经发生了很大变化。对于失业数据来说，超过一个月的数据将不再及时，而对于股市数据来说，只需几秒钟信息就可能被认为已经太旧，无法用于交易。除非涉及到你已经熟悉的领域，否则评估你的数据是否及时可能需要一些专家研究以及数据发布者的数据。
- en: Complete
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 完整
- en: Does the dataset contain all of the data values it should? In the USDA apple
    data in [“USDA Data Sleuthing”](#usda_data), for example, only a few of the rows
    contain appearance descriptions, while most are left blank. Can we still generate
    useful data insights when parts of the data are so incomplete? Addressing this
    question means first answering two others. First, *why* is the data missing? Second,
    do you *need* that data measure in order to perform a specific analysis needed
    for your data wrangling process?
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集是否包含了它应该包含的所有数据值？例如，在[“USDA 数据追踪”](#usda_data)中的 USDA 苹果数据中，只有少数行包含了外观描述，而大多数行都是空白的。即使数据的部分内容如此不完整，我们仍然能够生成有用的数据洞察吗？回答这个问题首先意味着要回答另外两个问题。首先，为什么数据丢失了？其次，你是否需要这些数据测量结果来执行你的数据整理过程中所需的特定分析？
- en: For example, your data may be incomplete because individual values are missing,
    or because the data has been reported irregularly—perhaps there is a half-year
    gap in data that is usually recorded every month. The dataset may also have been
    truncated, which is a common problem when large datasets are opened in a spreadsheet
    program. Whatever the reason, discovering *why* some part of the data is missing
    is essential in order to know how to proceed. In [“USDA Data Sleuthing”](#usda_data),
    we could potentially ignore the “appearance” category if our primary interest
    is in the way that different varietals of apples are priced, or we could contact
    the terminal market again to clarify if blank values in the appearance column
    actually correspond to some default value that they don’t bother to indicate explicitly.
    Even truncated data may not be a problem if what we have available covers a sufficient
    time period for our purposes, but it is still useful to learn the true number
    of records and date range of the data for context. And while there are statistical
    tricks that can sometimes make gaps in data collection less problematic, learning
    that the recording gap is due to some disruptive event may change the type of
    analysis you do, or even the question you’re pursuing altogether.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，你的数据可能不完整，因为个别数值缺失，或者数据报告不规律——也许数据通常每月记录，但却存在半年间隔的数据空白。数据集也可能被截断，当大数据集在电子表格程序中打开时，这是一个常见问题。无论什么原因，找出数据缺失的*原因*是至关重要的，以便知道如何继续进行。在[“USDA
    数据调查”](#usda_data)中，如果我们主要关心不同苹果品种的定价方式，我们可以考虑忽略“外观”类别，或者我们可以再次联系市场终端，澄清外观列中的空白值是否实际上对应于他们未明确指出的某个默认值。即使是截断的数据，如果我们可用的数据涵盖了我们的目的所需的足够时间段，也可能不成问题，但了解数据的真实记录数和日期范围以进行上下文分析仍然很有用。虽然有时可以使用统计技巧使数据收集中的间隙变得不那么问题，但了解记录间隙是由于某些破坏性事件造成的可能会改变您进行分析的类型，甚至是您完全追求的问题。
- en: In other words, while having *complete* data is always preferable, once you
    know *why* that data is missing, you may be able to proceed with your data wrangling
    process, regardless. But you should always find out—and be sure to document what
    you learn!
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，虽然拥有*完整*的数据始终是可取的，一旦您知道数据缺失的*原因*，您可能可以继续进行数据整理过程。但是，您应该始终了解并确保记录所学到的内容！
- en: High volume
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 高容量
- en: How many data points are “enough”? At minimum, a dataset will need to have sufficient
    records to support the type of analysis needed to answer your particular question.
    If what you need is a *count*—for example, the number of 311 calls that involved
    noise complaints in a particular year—then having “enough” data means having records
    of *all* of the 311 calls for that particular year.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: “足够”数据点有多少才够？至少，数据集需要有足够的记录来支持回答您特定问题所需的分析类型。如果您需要的是*计数*——例如，某一年中涉及噪音投诉的311呼叫数量——那么拥有“足够”数据意味着拥有那一年中所有311呼叫的记录。
- en: If your question is about general or generalizable patterns or trends, however,
    what counts as “enough” is a little less clear. For example, if you wanted to
    determine which Citi Bike station is the “busiest,” how long a time period should
    you consider? A week? A month? A year? Should only weekdays be considered, or
    all days? The correct answer will partly depend on more thoroughly specifying
    your question. Are you interested in the experiences of commuters or visitors?
    Are you trying to generate insights to drive transit planning, retail placement,
    or service quality? Also, are you *really* interested in what station is “busiest,”
    or is it really more about a particular rate of turnover? As is so often the case,
    the correct answer is largely about specifying the question correctly—and that
    requires being, in most cases, *very* specific.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果您的问题涉及一般或可普遍化的模式或趋势，那么“足够”的定义就不那么清晰了。例如，如果您想确定哪个Citi Bike站点最“繁忙”，您应该考虑多长时间段？一周？一个月？一年？应该只考虑工作日，还是所有天？正确的答案部分取决于更详细地说明您的问题。您是对通勤者或访客的体验感兴趣吗？您是在生成驱动交通规划、零售布局或服务质量的见解吗？另外，您是否*真的*关心哪个站点最“繁忙”，还是更关心特定周转率？如常见的情况一样，正确的答案主要取决于正确说明问题——而这通常需要非常具体。
- en: One of the trickiest parts of assessing data “completeness,” however, is that
    accounting for factors that may influence the trend or pattern you’re investigating
    is difficult without knowing the subject area pretty well already. For example,
    while we might easily expect that Citi Bike usage might vary across seasons, what
    about a reduction in public transit service? An increase in fares? These changes
    *might* have implications for our analysis, but how can we know that when we’re
    still starting out?
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，评估数据“完整性”的其中一个棘手部分是，在不太了解主题领域的情况下，很难考虑可能影响你正在调查的趋势或模式的因素。例如，尽管我们可能很容易预期Citi
    Bike的使用量可能随季节变化而变化，但公共交通服务的减少呢？票价的增加呢？这些变化*可能*对我们的分析有影响，但我们在刚刚开始时如何知道呢？
- en: The answer—as it is so often—is (human) experts. Maybe fare increases temporarily
    increase bike share ridership, but only for a few months. Maybe bicycle commuters
    are prepared for bad weather and stick with their patterns even when it’s snowing.
    With infinite time and infinite data, we might be able to answer these questions
    for ourselves; talking to *humans* is just so much faster and so much more informative.
    And believe it or not, there is an expert in almost everything. For example, a
    quick search on [Google Scholar](https://oreil.ly/eGJzD) for “seasonal ridership
    bike sharing” returns everything from blog posts to peer-reviewed research on
    the topic.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 答案——就像往常一样——是（人类）专家。也许票价上涨暂时增加了共享单车的骑行人数，但这种增长只持续了几个月。也许自行车通勤者已经做好了应对恶劣天气的准备，即使下雪也会坚持他们的习惯。在无限的时间和数据条件下，我们可能能够自己回答这些问题；但与*人类*交流要快得多，信息量也要丰富得多。信不信由你，几乎每个领域都有专家存在。例如，快速搜索[Google
    Scholar](https://oreil.ly/eGJzD)上关于“季节性共享单车骑行量”的内容，从博客文章到同行评审的研究都应有尽有。
- en: What does this mean for data completeness? Existing research or—even better—a
    real-time conversation with a subject matter expert (see [“Subject Matter Experts”](app03.html#smes)
    for more on this) will help you decide which factors you’re going to include in
    your analysis, and as a result, how much data you need for your chosen analysis
    in order for it to be complete.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这对数据的完整性意味着什么？现有的研究，或者更好的是与学科专家的实时交流（参见[“学科专家”](app03.html#smes)了解更多），将帮助你决定在分析中包含哪些因素，从而决定你选择的分析需要多少数据才能使其完整。
- en: Multivariate
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 多变量
- en: 'Wrangling real-world data inevitably means encountering real-world complexity,
    where a huge range of factors may influence the phenomenon we’re trying to investigate.
    Our “busiest” Citi Bike station is one example: beyond seasonality and transit
    service, the [surrounding terrain](https://www.sciencedirect.com/science/article/abs/pii/S136192091731057X)
    or the density of stations could play a role. If our Citi Bike data contained
    only information about how many trips started and ended at a particular station,
    then it would be very difficult to create an analysis that could say more than
    “this station had the most bikes removed and returned” in a given time period.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 调整现实世界的数据不可避免地意味着遇到真实世界的复杂性，各种因素可能影响我们试图调查的现象。我们“最繁忙”的Citi Bike站点就是一个例子：除了季节性和交通服务外，[周围的地形](https://www.sciencedirect.com/science/article/abs/pii/S136192091731057X)或站点的密度也可能起到作用。如果我们的Citi
    Bike数据仅包含关于特定站点启程和结束的骑行次数的信息，那么很难创建出超出“该站点在特定时间段内有最多自行车被取出和归还”的分析。
- en: 'When data is multivariate, though, it means that it has multiple *attributes*
    or *features* associated with each record. For the [historical Citi Bike data](https://citibikenyc.com/system-data),
    for example, we know we have all of the following, thanks to our little bit of
    wrangling in [“Hitting the Road with Citi Bike Data”](ch02.html#hitting_the_road_intro):'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据是多变量的时候，意味着每条记录都有多个*属性*或*特征*与之关联。例如，对于[历史Citi Bike数据](https://citibikenyc.com/system-data)，我们知道在[“利用Citi
    Bike数据上路”](ch02.html#hitting_the_road_intro)中经过整理之后，我们拥有以下所有信息：
- en: '[PRE0]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: That’s 15 different features about each recorded ride, any number of which might
    be able to leverage toward a more meaningful or nuanced way to understand which
    Citi Bike station is the “busiest.”
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 每次记录的骑行都涉及15种不同的特征，其中任意数量可能能够利用起来更有意义或更细致地理解哪个Citi Bike站点最“繁忙”。
- en: Atomic
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 原子的
- en: 'Atomic data is highly granular; it is both measured precisely and not aggregated
    into summary statistics or measures. In general, summary measures like rates and
    averages aren’t great candidates for further analysis, because so much of the
    underlying data’s detail has already been lost. For example, the *arithmetic average*
    or *mean* of both of the following sets of numbers is 30: 20, 25, 30, 45 and 15,
    20, 40, 45\. While summary statistics are often helpful when making comparisons
    across different datasets, they offer too little insight into the data’s underlying
    structure to support further analysis.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 原子数据非常细粒度；它既精确测量，又没有汇总为摘要统计或指标。一般来说，摘要统计量如比率和平均数并不是进一步分析的好选择，因为已经丢失了大量底层数据的细节。例如，以下两组数字的*算术平均数*或*均值*都是
    30：20、25、30、45 和 15、20、40、45。当进行不同数据集的比较时，摘要统计量通常很有帮助，但它们对数据底层结构的洞察力太少，以支持进一步分析。
- en: Achievable
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可实现性
- en: No matter how a dataset appears at first, the reality is that truly clean, well-organized,
    and error-free data is a near impossibility—in part just because, as time goes
    by, some things (like dollar values) simply *become* inconsistent. As a result,
    there are data quality characteristics that, as data wranglers, we should just
    always expect we’ll need to review and improve. Fortunately, this is where the
    flexibility and reusability of Python really shines—meaning these tasks will get
    easier and faster over time as we build up our programming skills.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 无论数据集一开始看起来如何，事实是，真正干净、组织良好且无误的数据几乎是不可能的——部分原因是，随着时间的推移，某些事物（如金额）简单地*变得*不一致。因此，作为数据处理者，有些数据质量特征我们应该始终期待需要审查和改进。幸运的是，这正是
    Python 的灵活性和可重用性真正发光的地方——这意味着随着我们积累编程技能，这些任务会变得更容易、更快速。
- en: Consistent
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 一致性
- en: 'High-integrity data needs to be consistent in a number of different ways. Often
    the most obvious type of consistency in a dataset has to do with the frequency
    of data that has been collected over time. Are the time intervals between individual
    records *consistent*? Are there gaps? Irregular intervals between data records
    are important to investigate (if not resolve) in part because they can be a first
    indicator of disruptions or disparities within the data collection process itself.
    Another source of pervasively *in*consistent data tends to turn up anytime a data
    field has text involved: fields that contain names or descriptors will almost
    inevitably contain multiple spellings of what is supposed to be the same term.
    Even fields that might seem straightforward to standardize may contain varying
    degrees of detail. For example, a “zip code” field might contain both five-digit
    zip code entries and more precise “Zip+4” values.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 高完整性数据需要在多种不同方式上保持一致。数据集中最明显的一致性类型通常与随时间收集的数据频率有关。个体记录之间的时间间隔*一致*吗？是否存在间隔？数据记录之间的不规则间隔是重要的调查对象（如果不解决），部分原因是因为它们可能是数据收集过程中的干扰或不一致的第一个指标。另一个普遍出现*不*一致数据的来源通常是每当涉及文本的数据字段时：包含名称或描述符的字段几乎不可避免地会包含相同术语的多种拼写。甚至可能看起来很容易标准化的字段也可能包含不同程度的细节。例如，“邮政编码”字段可能包含五位邮政编码条目和更精确的“Zip+4”值。
- en: 'Other types of consistency may be less obvious but no less important. Units
    of measure, for example, need to be consistent across the dataset. While this
    might seem obvious, it’s actually easier than you might first imagine for this
    *not* to be the case. Let’s say you’re looking at the cost of an apple over a
    period of a decade. Sure, your data may record all of the prices in dollars, but
    inflation will be constantly changing what a dollar is *worth*. And while most
    of us are aware that Celsius and Fahrenheit “degrees” are of different sizes,
    it doesn’t stop there: an imperial pint is about 568 ml, whereas an American pint
    is about 473 ml. In fact, even accounts of Napoleon Bonaparte’s famously short
    stature is likely the result of an [inconsistency](https://britannica.com/story/was-napoleon-short)
    between the size of the 19th-century French inch (about 2.71cm) and today’s version
    (about 2.54 cm).'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 其他类型的一致性可能不那么明显，但同样重要。例如，度量单位需要在整个数据集中保持一致。虽然这可能看起来很明显，但实际上比你想象的更容易出现*不*一致的情况。比如说，你想看一下十年间苹果的价格。当然，你的数据可能记录了所有价格的美元，但通货膨胀会不断改变一个美元的*价值*。虽然大多数人都知道摄氏度和华氏度的“度”大小不同，但问题并不仅限于此：英制品脉约为568毫升，而美制品脉约为473毫升。事实上，甚至拿拿破仑·波拿巴（Napoleon
    Bonaparte）身高矮小的描述，可能是19世纪法国英寸（约2.71厘米）和今天版本（约2.54厘米）之间的[不一致](https://britannica.com/story/was-napoleon-short)的结果。
- en: 'The solution to such inconsistencies is to *normalize* your data before doing
    any comparisons or analyses. In most cases, this is a matter of simple arithmetic:
    you simply have to choose which interpretation of the unit to which you will convert
    all others (another important moment for documentation!). This is true even of
    currency, which analysts often begin by converting to “real” (read: inflation-controlled)
    dollars, using some chosen year as a benchmark. For example, if we wanted to compare
    the real dollar value of the US federal minimum wage in 2009 (when it was last
    increased) to 2021, we could use an inflation calculator [like the one maintained
    by the Bureau of Labor Statistics (BLS)](https://bls.gov/data/inflation_calculator.htm)
    to see that the real dollar value of $7.25 per hour in 2009 is equivalent to only
    $5.72 per hour in 2021.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这类不一致性的解决方案是在进行任何比较或分析之前*标准化*您的数据。在大多数情况下，这只是简单算术的问题：您只需选择将所有其他单位转换为其何种解释（文档化的另一重要时刻！）。即使是货币，分析师们通常也会开始将其转换为“真实”（即通货膨胀控制的）美元，使用某一年作为基准。例如，如果我们想比较2009年（上次调整时）美国联邦最低工资的实际美元价值与2021年的差异，我们可以使用由劳工统计局（BLS）维护的通货膨胀计算器[（如此类的）](https://bls.gov/data/inflation_calculator.htm)，看到2009年每小时7.25美元的实际美元价值相当于2021年每小时5.72美元。
- en: Clear
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 清晰
- en: Like our Python code, our data and its labels will ideally be easy to read and
    interpret. Realistically, field (and even dataset) descriptors can sometimes be
    little more than cryptic codes that require constant cross-referencing with a
    data dictionary or other resources. This is part of why this type of data integrity
    is almost always the *product of* rather than the *precursor to* some degree of
    data wrangling.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们的Python代码一样，我们的数据及其标签理想上应该易于阅读和解释。现实情况是，字段（甚至数据集）描述有时可能只是需要不断与数据字典或其他资源交叉参考的晦涩代码。这也是为什么这种数据完整性几乎总是*数据整理*的结果，而不是其*前提*的一部分。
- en: For example, is there some logic to the fact that the table code for the US
    Census Bureau’s American Community Survey Demographic and Housing Estimates is
    [`DP05`](https://oreil.ly/kD48L)? Perhaps. But it’s hardly obvious to an occasional
    user of Census data, any more than the column label `DP05_0001E` is likely to
    be.^([6](ch03.html#idm45143427035984)) While a download of this Census table does
    include multiple files that can help you piece together the meaning of the filenames
    and column headers, developing a clear, high-integrity dataset may well require
    a fair amount of relabeling, reformatting, and reindexing—especially where government-produced
    data is involved. As always, however, documenting your information sources and
    renaming processes as you go is crucial.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，美国人口调查局的美国社区调查人口和住房估计的表代码是[`DP05`](https://oreil.ly/kD48L)。也许有一些逻辑在其中。但对于人口普查数据的偶尔使用者来说，这显然并不明显，就像列标签`DP05_0001E`一样可能不会^([6](ch03.html#idm45143427035984))。虽然下载这个人口普查表会包含多个文件，帮助你理解文件名和列标题的含义，但要开发出清晰、高完整性的数据集很可能需要相当多的重命名、重新格式化和重新索引，特别是涉及政府生产的数据。然而，无论如何，随着进行的信息来源记录和重命名过程至关重要。
- en: Dimensionally structured
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 结构化维度
- en: Dimensionally structured data contains fields that have been grouped into or
    additionally labeled with useful categories, such as geographic region, year,
    or language. Such features often provide quick entry points for both data augmentation
    (which we’ll address in [“Data Augmentation”](#data_augmentation)) and data analysis,
    because they reduce the correlation and aggregation effort we have to do ourselves.
    These features can also serve as an indicator of what the data creator thought
    was important to record.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 结构化的维度数据包含已分组或额外标记的字段，例如地理区域、年份或语言，这些特征通常为数据增强（我们将在[“数据增强”](#data_augmentation)中讨论）和数据分析提供快速入口，因为它们减少了我们自己必须进行的相关性和聚合工作。这些特征还可以作为数据创建者认为重要记录的指示器。
- en: For example, the dimensions of our Citi Bike data include whether a bike was
    checked out to the account of a “Subscriber” or a “Customer,” as well as the account
    holder’s birth year and gender—suggesting that the data’s designers believed these
    features might yield useful insights about Citi Bike trips and usage. As we’ll
    see in [Chapter 7](ch07.html#chapter7), however, they did *not* choose to include
    any sort of “weekday/holiday” indicator—meaning that is a *dimension* of the data
    we’ll have to derive ourselves if we need it.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们的 Citi Bike 数据的维度包括自行车是否被分配给“订阅者”或“顾客”的账户，以及账户持有人的出生年份和性别—表明数据的设计者认为这些特征可能会提供关于
    Citi Bike 行程和使用的有用见解。然而，正如我们将在[第7章](ch07.html#chapter7)中看到的那样，他们并*没有*选择包括任何“工作日/假日”指示器—这意味着这是我们如果需要的话需要自己推导的数据*维度*。
- en: Improving Data Quality
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提高数据质量
- en: As previously noted, many aspects of data quality are the product of a data
    wrangling process—whether that involves reconciling and normalizing units, clarifying
    the meaning of obscure data labels, or seeking out background information about
    the representativeness of your dataset. Part of what this illustrates is that,
    in the real world, ensuring data *quality* is at least partly the result of multiple,
    iterative data wrangling processes. While the terms for these phases of the data
    wrangling process vary, I usually describe them as data *cleaning* and data *augmentation*.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 正如前面所述，数据质量的许多方面是数据整理过程的产物，无论是调和和标准化单位、澄清不明确数据标签的含义还是寻找关于数据集代表性的背景信息。这部分说明的是，在现实世界中，确保数据*质量*至少部分上是多次迭代的数据整理过程的结果。虽然数据整理过程中这些阶段的术语各不相同，我通常将它们描述为数据*清理*和数据*增强*。
- en: Data Cleaning
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据清理
- en: In reality, data “cleaning” is not so much its own step in the data wrangling
    process as it is a constant activity that accompanies every *other* step, both
    because most data is *not* clean when we encounter it and because *how* a dataset
    (or part of it) needs to be “cleaned” is often revealed progressively as we work.
    At a high level, clean data might be summarized as being free from errors or typos—such
    as mismatched units, multiple spellings of the same word or term, and fields that
    are not well separated—and missing or impossible values. While many of these are
    at least somewhat straightforward to recognize (though not always to correct),
    deeper data problems may still persist. Measurement changes, calculation errors,
    and other oversights—especially in system-generated data—often don’t reveal themselves
    until some level of analysis has been done and the data has been “reality-checked”
    with folks who have significant expertise and/or firsthand experience with the
    subject.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，“数据清理”不是数据整理过程中的独立步骤，而是伴随每一个*其他*步骤而进行的持续活动，这是因为大多数数据在我们接触时都*不*是干净的，而且数据集（或其部分）需要“清理”的方式通常在工作中逐步显现。从高层次来看，干净的数据可能被概括为没有错误或拼写错误，如不匹配的单位、同一单词或术语的多种拼写以及未分隔良好的字段和缺失或不可能的值。虽然这些问题中许多至少在某种程度上是容易识别的（尽管并非总是容易纠正），但更深层次的数据问题仍然可能存在。测量变化、计算错误以及其他疏忽，尤其是在系统生成的数据中，通常在进行了一定程度的分析并与具有重要专业知识和/或第一手经验的人员进行了“现实检查”后才会显现出来。
- en: 'The iterative nature of data cleaning is an example of why data wrangling is
    a cycle rather than a linear series of steps: as your work reveals more about
    the data and your understanding of its relationship to the world deepens, you
    may find that you need to revisit earlier work that you’ve done and repeat or
    adjust certain aspects of the data. Of course, this is just another reason why
    documenting your data wrangling work is so crucial: you can use your documentation
    to quickly identify where and how you made any changes or updates, and to reconcile
    them with what you now know. Without robust documentation to guide you, you may
    quickly find yourself needing to start from scratch.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 数据清洗的迭代性质说明了数据整理是一个循环过程，而不是线性的步骤序列：随着你的工作揭示了关于数据的更多信息，你对其与世界关系的理解加深，你可能发现需要重新审视之前完成的工作，并重复或调整数据的某些方面。当然，这也是为什么记录你的数据整理工作如此关键的另一个原因：你可以利用你的文档快速识别何处以及如何进行了任何更改或更新，并与现在的知识相调和。如果没有健全的文档来指导你，你可能很快就会发现自己需要从头开始。
- en: Data Augmentation
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据增强
- en: '*Augmenting* a dataset is the process of expanding or elaborating it, usually
    by connecting it with other datasets—this is really the nature of “big data” in
    the 21st century.^([7](ch03.html#idm45143426993168)) By using features shared
    among datasets, it’s possible to bring together data from multiple sources in
    order to get a more complete portrait of what is happening in the world by filling
    in gaps, providing corroborating measures, or adding contextual data that helps
    us better assess data fit. In our “best” school example, this might mean using
    school codes to bring together the several types of data collected by different
    entities, such as the state-level standardized test scores and local building
    information mentioned in [“Data Decoding”](#data_decoding). Through a combination
    of effective research and data wrangling, data augmentation can help us build
    data quality and answer questions far too nuanced to address through any single
    dataset.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '*增强*数据集是通过扩展或详细说明来实现的，通常是通过与其他数据集连接——这实际上是21世纪“大数据”的本质。通过使用在数据集之间共享的特征，可以将来自多个来源的数据汇集在一起，以更完整地描述发生在世界中的情况，填补空白，提供协同措施或添加帮助我们更好评估数据适应性的上下文数据。在我们的“最佳”学校示例中，这可能意味着使用学校代码将不同实体收集的几种数据，如州级标准化考试成绩和本地建筑信息，汇总到一起，如[“数据解码”](#data_decoding)中提到的。通过有效的研究和数据整理的结合，数据增强可以帮助我们建立数据质量，并回答通过任何单一数据集无法解决的细微问题。'
- en: 'Like data cleaning, opportunities for data augmentation may arise at almost
    any point in the data wrangling process. At the same time, each new dataset we
    introduce will spawn a data wrangling process of its own. This means that *unlike*
    data cleaning, data augmentation has no definitive “end state”—there will *always*
    be another dataset we can add. This is yet another reason why specifically and
    succinctly stating our data wrangling question up front is so essential to the
    process: without a clearly articulated statement about what you’re trying to investigate,
    it’s all too easy to run out of time or other resources for your data wrangling
    effort. The good news is that if you’ve been keeping up your data diary, you’ll
    never lose track of a promising dataset for use in a future project.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 与数据清洗类似，数据增强的机会可能在数据整理过程的几乎任何时候出现。同时，我们引入的每个新数据集都将产生其自身的数据整理过程。这意味着，与数据清洗不同，数据增强没有明确的“结束状态”——总会有另一个可以添加的数据集。这也是为什么明确和简明地在数据整理问题前提上述明是如此重要的另一个原因：如果没有明确表达关于你试图调查的内容的声明，你很容易在数据整理工作中耗尽时间或其他资源。好消息是，如果你一直在保持你的数据日记，你将永远不会失去对未来项目中有用的有前途的数据集的追踪。
- en: Conclusion
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Since our hands-on work with actual data has been limited up to this point,
    many of the concepts discussed in this chapter may seem a little bit abstract
    right now. Don’t worry! Things are about to get *very* hands on. In the coming
    chapters, we’ll start wrangling data that comes in various formats and from different
    sources, offering an inside look at how various characteristics of data quality
    play into the decisions we make as we access, evaluate, clean, analyze, and present
    our data. And you can be confident that by the end of this volume, you’ll be able
    to create meaningful, accurate, and compelling data analyses and visualizations
    to share your insights with the world!
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 由于到目前为止我们的实际数据操作有限，本章讨论的许多概念可能现在看起来有点抽象。别担心！接下来的章节中，我们将开始处理来自不同格式和不同来源的数据，深入了解数据质量的各种特征是如何影响我们在访问、评估、清理、分析和呈现数据时做出决策的。到本卷结束时，您将能够创建有意义、准确和引人入胜的数据分析和可视化，与世界分享您的见解！
- en: In the next chapter, we’ll start this process by working through how to wrangle
    data from a wide variety of formats into a structure that will let us do the cleaning,
    augmentation, and analyses we need. Let’s dive in!
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将通过处理各种格式的数据，将其结构化，以便进行所需的清理、增强和分析。让我们开始吧！
- en: ^([1](ch03.html#idm45143427340352-marker)) For example, see [“The Secret Bias
    Hidden in Mortgage-Approval Algorithms”](https://themarkup.org/denied/2021/08/25/the-secret-bias-hidden-in-mortgage-approval-algorithms)
    by Emmanuel Martinez and Lauren Kirschner (*The Markup*) .
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch03.html#idm45143427340352-marker)) 例如，参见埃曼纽尔·马丁内斯和劳伦·克尔施纳在[*The Markup*](https://themarkup.org/denied/2021/08/25/the-secret-bias-hidden-in-mortgage-approval-algorithms)上的文章“抵押贷款批准算法中隐藏的秘密偏见”。
- en: ^([2](ch03.html#idm45143427337760-marker)) “The World’s Most Valuable Resource
    Is No Longer Oil, but Data,” in [*The Economist*](https://economist.com/leaders/2017/05/06/the-worlds-most-valuable-resource-is-no-longer-oil-but-data).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch03.html#idm45143427337760-marker)) “世界上最有价值的资源不再是石油，而是数据”，见[*经济学人*](https://economist.com/leaders/2017/05/06/the-worlds-most-valuable-resource-is-no-longer-oil-but-data)。
- en: ^([3](ch03.html#idm45143427320640-marker)) For example, see “Raiders of the
    Lost Web” by Adrienne LaFrance in [*The Atlantic*](https://theatlantic.com/technology/archive/2015/10/raiders-of-the-lost-web/409210).
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch03.html#idm45143427320640-marker)) 例如，参见亚德里安·拉弗兰斯在[*大西洋*](https://theatlantic.com/technology/archive/2015/10/raiders-of-the-lost-web/409210)上的文章“失落网络的突袭”。
- en: ^([4](ch03.html#idm45143427230496-marker)) For a highly readable overview, see
    [“Statistics Without Tears” by Amitav Banerjee and Suprakash Chaudhury](https://ncbi.nlm.nih.gov/pmc/articles/PMC3105563).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch03.html#idm45143427230496-marker)) 要了解简明易懂的概述，请参见阿米塔夫·巴纳吉和苏普拉卡什·乔杜里的[“无泪统计”](https://ncbi.nlm.nih.gov/pmc/articles/PMC3105563)。
- en: '^([5](ch03.html#idm45143427185920-marker)) This list is adapted from Stephen
    Few’s excellent book *Now You See It: Simple Visualization Techniques for Quantitative
    Analysis* (Analytics Press), with adjustments based on my own data wrangling experience.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '^([5](ch03.html#idm45143427185920-marker)) 此列表改编自斯蒂芬·尤的优秀著作*《Now You See It:
    Simple Visualization Techniques for Quantitative Analysis》*（Analytics Press），并根据我的数据处理经验进行了调整。'
- en: ^([6](ch03.html#idm45143427035984-marker)) It indicates the total population
    estimate, by the way.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: ^([6](ch03.html#idm45143427035984-marker)) 顺便提一下，它表示总人口估计。
- en: ^([7](ch03.html#idm45143426993168-marker)) See danah boyd and Kate Crawford’s
    [“Critical Questions for Big Data”](https://tandfonline.com/doi/full/10.1080/1369118X.2012.678878)
    for a discussion on big data.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: ^([7](ch03.html#idm45143426993168-marker)) 详细讨论大数据，请参阅丹娜·博伊德和凯特·克劳福德的[“大数据的关键问题”](https://tandfonline.com/doi/full/10.1080/1369118X.2012.678878)。
