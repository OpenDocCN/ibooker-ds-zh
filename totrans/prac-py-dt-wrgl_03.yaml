- en: Chapter 3\. Understanding Data Quality
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data is everywhere. It’s automatically generated by our mobile devices, our
    shopping activities, and our physical movements. It’s captured by our electric
    meters, public transportation systems, and communications infrastructure. And
    it’s used to estimate our health outcomes, our earning potential, and our credit
    worthiness.^([1](ch03.html#idm45143427340352)) Economists have even declared that
    data is the “new oil,”^([2](ch03.html#idm45143427337760)) given its potential
    to transform so many aspects of human life.
  prefs: []
  type: TYPE_NORMAL
- en: While data may be plentiful, however, the truth is that *good* data is scarce.
    The claim of “the data revolution” is that, with enough data, we can better understand
    the present and improve—or even predict—the future. For any of that to even be
    possible, however, the data underlying those insights has to be high quality.
    Without good-quality data, all of our efforts to wrangle, analyze, visualize,
    and communicate it will, at best, leave us with no more insight about the world
    than when we started. While that would be an unfortunate waste of effort, the
    consequences of failing to recognize that we have poor-quality data is even worse,
    because it can lead us to develop a seemingly rational but dangerously distorted
    view of reality. What’s more, because data-driven systems are used to make decisions
    at scale, the harms caused by even a small amount of bad data can be significant.
    Sure, data about hundreds or even thousands of people may be used to “train” a
    machine learning model. But if that data is not representative of the population
    to which the model will be applied, the repercussions of that system can affect
    hundreds or thousands of times the number of people in the original dataset. Because
    the stakes are so high, ensuring data quality is an essential part of data wrangling.
    But what does it mean for data to be “high quality”? My view is that data is high
    quality only if it is both *fit* for purpose and has high internal *integrity*.
  prefs: []
  type: TYPE_NORMAL
- en: 'What does each of those terms actually mean? That is exactly what we’ll explore,
    in depth, in this chapter. We’ll begin by discussing the concept of data *fit*,
    which relates to the appropriateness of data for use in a particular context,
    or to answer a particular question. We’ll then break down the many aspects of
    data *integrity*: the characteristics of a dataset that influence both its fitness
    for purpose and the types of analyses we can responsibly use it for. Finally,
    we’ll discuss some tools and strategies for finding and working with data that
    can help you maximize its overall quality, lending confidence and credibility
    to the work that you produce with it.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In case you start to find any of this tedious, let me repeat my exhortations
    from [“What Is “Data Wrangling”?”](ch01.html#describing_data_wrangling): trying
    to “skip over” the work of assessing data quality can only undermine your data
    wrangling efforts. At best, you’ll go to share your work and encounter questions
    about your process that you don’t have answers for. At worst, you’ll end up promoting
    “insights” that are both wrong and do active harm. Along the way, you’ll *also*
    be cheating yourself out of good technical skills, because solving data quality
    problems is where you’ll expand your programming knowledge the most. If you truly
    want to be good at the work of data wrangling, assessing data quality *has* to
    be part of your practice.'
  prefs: []
  type: TYPE_NORMAL
- en: Assessing Data Fit
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Perhaps one of the most common misconceptions about data wrangling is that
    it is a predominantly *quantitative* process, that is, that data wrangling is
    mostly about working with numbers, formulas, and code. In fact, irrespective of
    the type of data you’re dealing with—it could be anything from temperature readings
    to social media posts—the core work of data wrangling involves making judgment
    calls: from whether your data accurately represents the phenomenon you’re investigating,
    to what to do about missing data points and whether you have enough data to generate
    any real insight at all. That first concept—the extent to which a given dataset
    accurately represents the phenomenon you’re investigating—is broadly what I mean
    by its *fit*, and assessing your dataset’s *fit*ness for purpose is much more
    about applying informed judgment than it is about applying mathematical formulas.
    The reason for this is quite simple: the world is a messy place, and what may
    seem like even the simplest data about it is always filtered through some kind
    of human lens. Take something as straightforward as measuring the temperature
    in your workspace over the course of a week. In theory, all you need to do is
    get a thermometer, put it in the space, and note down the reading every day. Done,
    right?'
  prefs: []
  type: TYPE_NORMAL
- en: Or are you? Let’s start with your equipment. Did you use a digital thermometer
    or a mercury thermometer? Where in the space did you place it? Is it near a door,
    a window, or a heating or cooling source? Did you take the reading at the same
    time every day? Is the thermometer ever in direct sunlight? What is the typical
    humidity level?
  prefs: []
  type: TYPE_NORMAL
- en: You may think I’m introducing a contrived level of complexity here, but if you’ve
    ever lived in a shared space (like an apartment building), you’ve probably been
    through the experience of *feeling* like it’s much warmer or colder than what
    some thermometer said. Likewise, if you’ve ever looked after a child who’s ill,
    you’re likely all too familiar with the different body temperature readings that
    you’ll get with different types of thermometers—or even with the same one, just
    minutes apart.
  prefs: []
  type: TYPE_NORMAL
- en: In other words, there are a huge number of factors contributing to that two-
    or three-digit temperature you record—and the number itself doesn’t provide information
    about any of them. That’s why when you begin the process of trying to answer a
    question with data, it’s not enough to know just what is in the dataset; you need
    to know about the processes and mechanisms used to *collect* it. Then, given everything
    you know about how the data was gathered, you need to determine if it can really
    be used to answer your specific question in a meaningful way.
  prefs: []
  type: TYPE_NORMAL
- en: 'Of course, this problem is neither new nor unique; it’s the same challenge
    that all scientific fields face in their efforts to discover new information about
    the world. Cancer research could hardly advance if every single researcher had
    to conduct every single study themselves; without the ability to build on the
    work of others, scientific and technological progress would grind to a halt (if
    not go off the rails entirely). Because of this, over time the scientific community
    has developed three key metrics for determining the appropriateness or fit of
    a dataset for answering a given question: *validity*, *reliability*, and *representativeness*.'
  prefs: []
  type: TYPE_NORMAL
- en: Validity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At its most basic, *validity* describes the extent to which something measures
    what it is supposed to. In our room temperature example, this would mean ensuring
    that the type of thermometer you’ve chosen will actually measure the air temperature
    rather than something else. For example, while traditional liquid-in-glass thermometers
    will probably capture air temperature well, infrared thermometers will tend to
    capture the temperature of whatever surface they’re pointed at. So even with something
    as seemingly basic as room temperature, you need to understand the tools and methods
    used to collect your data readings in order to ensure their *validity* with respect
    to your question.
  prefs: []
  type: TYPE_NORMAL
- en: Unsurprisingly, things only get more involved when we’re not collecting data
    about common physical phenomena. *Construct validity* describes the extent to
    which your data measurements effectively capture the (usually abstract) *construct*,
    or idea, you’re trying to understand. For example, let’s say you want to know
    which are the “best” schools in your area. What data can help you answer that
    question? First we have to recognize that the term *best* is imprecise. Best in
    what way? Are you interested in which school has the highest graduation rate?
    Standardized test scores? School-assigned grades? Teacher evaluations? Student
    satisfaction? Extracurricular participation?
  prefs: []
  type: TYPE_NORMAL
- en: In order to use data to begin to answer this question, you first need to articulate
    two things. First, “best” *for whom*? Are you trying to answer this question for
    your own child? A friend’s? Having answered that, you’ll be better able to complete
    the second task, which is *operationalizing* your specific idea of “best.” If
    your friend’s child loves sports, for example, extracurricular activities might
    be more important than academics.
  prefs: []
  type: TYPE_NORMAL
- en: In data analysis, this process of selecting measures is known as *operationalizing
    a construct*, and it inevitably requires choosing among—and balancing—proxies
    for the idea or concept you are trying to understand. These proxies—like graduation
    rates, test scores, extracurricular activities, and so on—are things *about which
    you can* *collect* *data* that you are choosing to use to represent an abstract
    concept (“best” school) *that cannot be measured directly*. Good-quality data,
    to say the least, must have good *construct validity* with respect to your question,
    otherwise your data wrangling results will be meaningless.
  prefs: []
  type: TYPE_NORMAL
- en: The other type of validity that is important for data fit is *content validity*.
    This type of validity has to do with how complete your data is for a given proxy
    measurement. In the “best” school example, let’s say you have determined that
    grades are relevant for determining what school is best, but you only have grades
    for history and physical education courses available. Though for many people grade
    data might, in principle, have *construct validity* for identifying the best school,
    having grade data for only two types of courses wouldn’t be sufficient to satisfy
    the requirement for *content validity*—and for high-quality data, you need to
    have *both*.
  prefs: []
  type: TYPE_NORMAL
- en: Reliability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Within a dataset, the *reliability* of a given measure describes its *accuracy*
    and *stability*. Together, these help us assess whether the same measure taken
    twice in the same circumstances will give us the same—or at least very similar—results.
    To revisit our temperature example: taking a child’s temperature with an oral
    thermometer is not likely to be very *reliable*, because the process requires
    that the child keep their mouth closed for a relatively long time (which, in my
    experience, they’re not great at). By contrast, taking a child’s temperature under
    their arm might be more reliable—because you can hug them to keep the thermometer
    in place—but it may not provide as *accurate* a reading of the child’s true internal
    body temperature as some other methods. This is why most medical advice lists
    different temperature thresholds for a fever in children, depending on which method
    you use to take their temperature.'
  prefs: []
  type: TYPE_NORMAL
- en: With abstract concepts and real-world data, determining the reliability of a
    data measure is especially tricky, because it is never really possible to collect
    the data more than once—whether because the cost is prohibitive, the circumstances
    can’t be replicated, or both. In those cases, we typically estimate reliability
    by comparing one similar group to another, using either previously or newly collected
    data. So even though a dramatic fluctuation in a school’s standardized test scores
    from one year to the next indicates that those scores may not be a *reliable*
    measure of school quality, this inconsistency itself is only part of the story.
    After all, those test scores *might* reflect the quality of teaching, but they
    might also reflect a change in the test being administered, how it was scored,
    or some other disruption to the learning or test-taking environment. In order
    to determine whether the standardized test data is *reliable* enough to be part
    of your “best” school assessment, you would need to look at comparison data from
    other years or other schools, in addition to learning more about the broader circumstances
    that may have led to the fluctuation. In the end, you may conclude that *most*
    of the test score information is reliable enough to be included but that a few
    particular data points should be removed, or you may conclude that the data is
    too unreliable to be part of a high-quality data process.
  prefs: []
  type: TYPE_NORMAL
- en: Representativeness
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The key value proposition for data-driven systems is that they allow us to generate
    insights—or even predictions—about people and phenomena that are too massive or
    too complex for humans to reason about effectively. By wrangling and analyzing
    data, the logic goes, we can make decisions faster and more fairly. Given the
    powerful computational tools that even individuals—to say nothing of companies—have
    access to these days, there’s no doubt that data-driven systems can generate “decisions”
    more quickly than humans can. Whether those insights are an accurate portrait
    of a particular population or situation, however, depends directly on the *representativeness*
    of the data being used.
  prefs: []
  type: TYPE_NORMAL
- en: Whether a dataset is sufficiently representative depends on a few things, the
    most significant of which goes back to the “for whom?” question we discussed in
    [“Validity”](#validity). If you’re trying to design a new course schedule for
    a specific grade school, you may be able to collect data about its entire population.
    If all the other criteria for data fitness have been met, then you already know
    that your data is representative, because you have collected data directly from
    or about the entire population to which it will apply.
  prefs: []
  type: TYPE_NORMAL
- en: But what if you’re trying to complete the same task for an entire city’s worth
    of schools? It’s deeply unlikely that you’ll succeed in collecting data about
    every single student in every single school, which means that you’ll be relying
    on input from only a subset of the students when you try to design the new schedule.
  prefs: []
  type: TYPE_NORMAL
- en: Anytime you’re working with a subset or *sample* in this way, it’s crucial to
    make sure that it is *representative* of the broader population to which you plan
    to apply your findings. While proper sampling methodology is beyond the scope
    of this book,^([4](ch03.html#idm45143427230496)) the basic idea is that in order
    for your insights to accurately generalize to a particular community of people,
    the data sample you use must proportionally reflect that community’s makeup. That
    means that you need to invest the time and resources to understand a number of
    things about that community *as a whole* before you can even know if your sample
    is representative.
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point you may be thinking: wait, if we could already get information
    about the whole population, we wouldn’t need a sample in the first place! And
    that’s true—sort of. In many cases, it’s possible to get *some* information about
    an entire community—just not precisely the information we need. In our school-scheduling
    scenario, for example, we would ideally get information about how—and how long—students
    travel to and from school each day, as well as some sense of their caretakers’
    schedules. But the information we might *have* about the entire school population
    (if we are working cooperatively with the school system) will probably include
    only things like home address, school address, and perhaps the [type of transportation
    support](https://schools.nyc.gov/school-life/transportation/bus-eligibility).
    Using this information, likely in conjunction with some additional administrative
    information, we could begin to create estimates for the proportion of certain
    types of student commuters that exist in the *entire* population, and then seek
    to replicate those proportions in selecting a *representative sample* from our
    survey results. Only at that point would we be ready to move on to the next step
    of the data wrangling process.'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, ensuring representativeness demands that we carefully consider
    which characteristics of a population are relevant to our data wrangling question
    *and* that we seek out enough additional information to ensure that our dataset
    proportionally represents those characteristics. Perhaps unsurprisingly, this
    is the data fitness test that many data-driven goods and services fail on, again
    and again. While companies and researchers may tout the *quantity* of data they
    use to develop in their systems, the reality is that most datasets that are readily
    available to companies and researchers tend to *not* be representative of, say,
    the US or global population. For example, data about search engine trends, social
    media activity, public transit usage, or smartphone ownership, for example, are
    all extremely *unlikely* to be representative of the broader population, since
    they are inevitably influenced by things like internet access and income level.
    This means that communities are *over*represented in these datasets while others
    are (sometimes severely) *under*represented. The result is systems that don’t
    *generalize*—like facial recognition systems [that cannot “see” Black faces](http://proceedings.mlr.press/v81/buolamwini18a/buolamwini18a.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: If you are faced with nonrepresentative data, what do you do? At the very least,
    you will need to revise (and clearly communicate) your “for whom?” assessment
    to reflect whatever population it *does* represent; this is the only community
    for whom your data wrangling insights will be valid. Also keep in mind that representativeness
    can only ensure that the outcome of your data wrangling efforts accurately reflects
    reality; it is not a value judgment on whether that reality should be *perpetuated*.
    If the outcome of your data wrangling effort will be used to make changes to a
    system or organization, the end of your data wrangling process is really just
    the starting point for thinking about what should be done with your insights,
    especially with respect to complex issues like [fairness](https://youtube.com/watch?v=jIXIuYdnyyk).
  prefs: []
  type: TYPE_NORMAL
- en: 'This is true even if you have data about an entire population. For example,
    if you want to know which organizations have received a certain type of grant,
    then the “population” or community you’re interested in is just those grant recipients.
    At the same time, checking your data for representativeness against the population
    at large still has value: if one or more communities are over- or underrepresented
    in your population of grant recipients, it may hint at hidden factors influencing
    who receives that money.'
  prefs: []
  type: TYPE_NORMAL
- en: Assessing Data Integrity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data *fit* is essentially about whether you have the right data for answering
    your data wrangling question. Data *integrity*, on the other hand, is largely
    about whether the data you have can support the analyses you’ll need to perform
    in order to answer that question. As you’ll see throughout this section, there
    are a *lot* of different aspects of the data to consider when performing an integrity
    assessment. But does a given dataset need to have *all* of them in order to be
    high integrity, and therefore high quality? Not necessarily. While some are essential,
    the importance of others depends on your specific question and the methods you’ll
    need to answer it. And with rare exceptions, many are characteristics of the data
    that you will enhance and develop as part of your data wrangling process.
  prefs: []
  type: TYPE_NORMAL
- en: In other words, while ensuring data *fit* is nonoptional, the types and degree
    of *integrity* that your particular data wrangling project requires will vary.
    Of course, the more of these requirements your data meets, the more useful it
    will be—both to you and others.
  prefs: []
  type: TYPE_NORMAL
- en: In general, a high-integrity dataset will, to one degree or another, be:^([5](ch03.html#idm45143427185920))
  prefs: []
  type: TYPE_NORMAL
- en: Necessary, but not sufficient
  prefs: []
  type: TYPE_NORMAL
- en: Of known provenance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Well-annotated
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Important
  prefs: []
  type: TYPE_NORMAL
- en: Timely
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Complete
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: High volume
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multivariate
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Atomic
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Achievable
  prefs: []
  type: TYPE_NORMAL
- en: Consistent
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clear
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dimensionally structured
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As I have already mentioned, however, not all of these data integrity characteristics
    are equally important. Some of them are nonnegotiable, while others are almost
    always the *result* of certain steps in the data wrangling process, not precursors
    to it. And while the goal is for your data to have as many of these characteristics
    as possible before you begin your analysis, there is always a balance to be struck
    between more fully elaborating your data and completing your work in time for
    your insights to be useful.
  prefs: []
  type: TYPE_NORMAL
- en: In this sense, assessing data integrity can be a useful way to prioritize your
    data wrangling efforts. For example, if a dataset is missing either of the “necessary,
    but not sufficient” characteristics, you may as well move on from it entirely.
    If it’s missing one or two of the characteristics in [“Important”](#important),
    it may still be possible to salvage the data you’re working with by combining
    it with others or limiting the scope of your analyses—and claims. Meanwhile, developing
    the characteristics in [“Achievable”](#achievable) is often the *goal* of your
    data wrangling process’s “cleaning” step, rather than something you can expect
    most real-world data to have when you first encounter it.
  prefs: []
  type: TYPE_NORMAL
- en: In the end, the degree to which your dataset embodies many of the characteristics
    that follow will depend on the time you have to invest in your data wrangling
    project, but without a good solid majority of them, your insights will be limited.
    As will be illustrated (in detail!) in [Chapter 6](ch06.html#chapter6), this variability
    is precisely why data wrangling and data quality are so thoroughly intertwined.
  prefs: []
  type: TYPE_NORMAL
- en: Necessary, but Not Sufficient
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Most of the time, the data we’re wrangling was compiled by someone else—or a
    whole collection of people and processes that we don’t have direct access to.
    At the same time, we need to be able to stand behind both our data wrangling process
    and any insights we derive from it. This means that there are a couple of data
    characteristics that are really *essential* to a successful data wrangling process.
  prefs: []
  type: TYPE_NORMAL
- en: Of known provenance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As discussed in [“What Is “Data Quality”?”](ch01.html#what_is_data_quality),
    data is the output of human decisions about what to measure and how. This means
    that using a dataset collected by others requires putting a significant amount
    of trust in them, especially because independently verifying *every single data
    point* is rarely possible; if it were, you would probably just collect your own
    data instead. This is why knowing the *provenance* of a dataset is so important:
    if you don’t know who compiled the data, the methods that they used, and/or the
    purpose for which they collected it, you will have a very hard time judging whether
    it is *fit* for your data wrangling purpose, or how to correctly interpret it.'
  prefs: []
  type: TYPE_NORMAL
- en: Of course, this doesn’t mean you need to know the birthdays and favorite colors
    of everyone who helped build a given dataset. But you *should* try to find out
    enough about their professional backgrounds, motivations for collecting the data
    (is it legally mandated, for example?), and the methods they employed so that
    you have some sense of which measures you’ll want to corroborate versus those
    that might be okay to take at face value. Ideally, both information about the
    data authors and sufficient documentation about these processes will be readily
    enough available that you can answer all of these questions about the data’s *provenance*
    fairly quickly. If they prove hard to locate, however, you may wish to move on;
    since you need this information to assess data *fit*, your time may be better
    spent looking for a different dataset or even collecting the data yourself.
  prefs: []
  type: TYPE_NORMAL
- en: Well-annotated
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A well-annotated dataset has enough surrounding information, or *metadata*,
    to make interpretation possible. This will include everything from high-level
    explanations of the data collection methodology to the “data dictionaries” that
    describe each data measure right down to its units. While this may seem straightforward,
    there are not always well-accepted standards for how such annotation information
    should be provided: sometimes it appears directly in data files or data entries
    themselves, and sometimes the information might be contained in separate files
    or documents in a location totally separate from where you retrieved the data.'
  prefs: []
  type: TYPE_NORMAL
- en: However they are structured or provided, robust data annotation documents are
    an essential component of high-integrity data because without them, it’s impossible
    to apply any analyses or draw any inferences from it. For example, imagine trying
    to interpret a budget without knowing if the figures provided refer to dollars,
    thousands of dollars, or millions of dollars—it’s clearly impossible. Or for a
    very American expression of the importance of annotation documents like data dictionaries,
    know that they sometimes [require a lawsuit](https://trac.syr.edu/foia/ice/20210805)
    to get.
  prefs: []
  type: TYPE_NORMAL
- en: Important
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While it’s hard to get *anywhere* with data wrangling unless you have sufficient
    provenance and metadata information about a dataset, you’re still not going to
    get very far unless you have enough data, from the right time period(s), and at
    the right level of detail. That said, the following data characteristics are ones
    that we may often assess—and sometimes improve—with our own data wrangling work.
    So while it will certainly be easier to work with datasets that have these characteristics
    already, even those that fall short on a few of these may still be worth exploring.
  prefs: []
  type: TYPE_NORMAL
- en: Timely
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: How up to date is the data that you’re using? Unless you’re studying a historical
    period, ensuring that your data is recent enough to meaningfully describe the
    *current* state of the world is important—though how old is “too old” will depend
    on both the phenomenon you’re exploring and the frequency with which data about
    it is collected and released.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if you’re interested in neighborhood demographics but your most
    recent data is several years old, there’s a strong likelihood that things may
    have changed considerably since. For unemployment data, data older than a month
    will no longer be timely, while for stock market data, it takes just a few seconds
    for information to be considered too old to inform trades. Unless it’s about a
    field you’re already familiar with, assessing whether your data is timely will
    likely require some research with experts as well as the data publisher.
  prefs: []
  type: TYPE_NORMAL
- en: Complete
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Does the dataset contain all of the data values it should? In the USDA apple
    data in [“USDA Data Sleuthing”](#usda_data), for example, only a few of the rows
    contain appearance descriptions, while most are left blank. Can we still generate
    useful data insights when parts of the data are so incomplete? Addressing this
    question means first answering two others. First, *why* is the data missing? Second,
    do you *need* that data measure in order to perform a specific analysis needed
    for your data wrangling process?
  prefs: []
  type: TYPE_NORMAL
- en: For example, your data may be incomplete because individual values are missing,
    or because the data has been reported irregularly—perhaps there is a half-year
    gap in data that is usually recorded every month. The dataset may also have been
    truncated, which is a common problem when large datasets are opened in a spreadsheet
    program. Whatever the reason, discovering *why* some part of the data is missing
    is essential in order to know how to proceed. In [“USDA Data Sleuthing”](#usda_data),
    we could potentially ignore the “appearance” category if our primary interest
    is in the way that different varietals of apples are priced, or we could contact
    the terminal market again to clarify if blank values in the appearance column
    actually correspond to some default value that they don’t bother to indicate explicitly.
    Even truncated data may not be a problem if what we have available covers a sufficient
    time period for our purposes, but it is still useful to learn the true number
    of records and date range of the data for context. And while there are statistical
    tricks that can sometimes make gaps in data collection less problematic, learning
    that the recording gap is due to some disruptive event may change the type of
    analysis you do, or even the question you’re pursuing altogether.
  prefs: []
  type: TYPE_NORMAL
- en: In other words, while having *complete* data is always preferable, once you
    know *why* that data is missing, you may be able to proceed with your data wrangling
    process, regardless. But you should always find out—and be sure to document what
    you learn!
  prefs: []
  type: TYPE_NORMAL
- en: High volume
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: How many data points are “enough”? At minimum, a dataset will need to have sufficient
    records to support the type of analysis needed to answer your particular question.
    If what you need is a *count*—for example, the number of 311 calls that involved
    noise complaints in a particular year—then having “enough” data means having records
    of *all* of the 311 calls for that particular year.
  prefs: []
  type: TYPE_NORMAL
- en: If your question is about general or generalizable patterns or trends, however,
    what counts as “enough” is a little less clear. For example, if you wanted to
    determine which Citi Bike station is the “busiest,” how long a time period should
    you consider? A week? A month? A year? Should only weekdays be considered, or
    all days? The correct answer will partly depend on more thoroughly specifying
    your question. Are you interested in the experiences of commuters or visitors?
    Are you trying to generate insights to drive transit planning, retail placement,
    or service quality? Also, are you *really* interested in what station is “busiest,”
    or is it really more about a particular rate of turnover? As is so often the case,
    the correct answer is largely about specifying the question correctly—and that
    requires being, in most cases, *very* specific.
  prefs: []
  type: TYPE_NORMAL
- en: One of the trickiest parts of assessing data “completeness,” however, is that
    accounting for factors that may influence the trend or pattern you’re investigating
    is difficult without knowing the subject area pretty well already. For example,
    while we might easily expect that Citi Bike usage might vary across seasons, what
    about a reduction in public transit service? An increase in fares? These changes
    *might* have implications for our analysis, but how can we know that when we’re
    still starting out?
  prefs: []
  type: TYPE_NORMAL
- en: The answer—as it is so often—is (human) experts. Maybe fare increases temporarily
    increase bike share ridership, but only for a few months. Maybe bicycle commuters
    are prepared for bad weather and stick with their patterns even when it’s snowing.
    With infinite time and infinite data, we might be able to answer these questions
    for ourselves; talking to *humans* is just so much faster and so much more informative.
    And believe it or not, there is an expert in almost everything. For example, a
    quick search on [Google Scholar](https://oreil.ly/eGJzD) for “seasonal ridership
    bike sharing” returns everything from blog posts to peer-reviewed research on
    the topic.
  prefs: []
  type: TYPE_NORMAL
- en: What does this mean for data completeness? Existing research or—even better—a
    real-time conversation with a subject matter expert (see [“Subject Matter Experts”](app03.html#smes)
    for more on this) will help you decide which factors you’re going to include in
    your analysis, and as a result, how much data you need for your chosen analysis
    in order for it to be complete.
  prefs: []
  type: TYPE_NORMAL
- en: Multivariate
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Wrangling real-world data inevitably means encountering real-world complexity,
    where a huge range of factors may influence the phenomenon we’re trying to investigate.
    Our “busiest” Citi Bike station is one example: beyond seasonality and transit
    service, the [surrounding terrain](https://www.sciencedirect.com/science/article/abs/pii/S136192091731057X)
    or the density of stations could play a role. If our Citi Bike data contained
    only information about how many trips started and ended at a particular station,
    then it would be very difficult to create an analysis that could say more than
    “this station had the most bikes removed and returned” in a given time period.'
  prefs: []
  type: TYPE_NORMAL
- en: 'When data is multivariate, though, it means that it has multiple *attributes*
    or *features* associated with each record. For the [historical Citi Bike data](https://citibikenyc.com/system-data),
    for example, we know we have all of the following, thanks to our little bit of
    wrangling in [“Hitting the Road with Citi Bike Data”](ch02.html#hitting_the_road_intro):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: That’s 15 different features about each recorded ride, any number of which might
    be able to leverage toward a more meaningful or nuanced way to understand which
    Citi Bike station is the “busiest.”
  prefs: []
  type: TYPE_NORMAL
- en: Atomic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Atomic data is highly granular; it is both measured precisely and not aggregated
    into summary statistics or measures. In general, summary measures like rates and
    averages aren’t great candidates for further analysis, because so much of the
    underlying data’s detail has already been lost. For example, the *arithmetic average*
    or *mean* of both of the following sets of numbers is 30: 20, 25, 30, 45 and 15,
    20, 40, 45\. While summary statistics are often helpful when making comparisons
    across different datasets, they offer too little insight into the data’s underlying
    structure to support further analysis.'
  prefs: []
  type: TYPE_NORMAL
- en: Achievable
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: No matter how a dataset appears at first, the reality is that truly clean, well-organized,
    and error-free data is a near impossibility—in part just because, as time goes
    by, some things (like dollar values) simply *become* inconsistent. As a result,
    there are data quality characteristics that, as data wranglers, we should just
    always expect we’ll need to review and improve. Fortunately, this is where the
    flexibility and reusability of Python really shines—meaning these tasks will get
    easier and faster over time as we build up our programming skills.
  prefs: []
  type: TYPE_NORMAL
- en: Consistent
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'High-integrity data needs to be consistent in a number of different ways. Often
    the most obvious type of consistency in a dataset has to do with the frequency
    of data that has been collected over time. Are the time intervals between individual
    records *consistent*? Are there gaps? Irregular intervals between data records
    are important to investigate (if not resolve) in part because they can be a first
    indicator of disruptions or disparities within the data collection process itself.
    Another source of pervasively *in*consistent data tends to turn up anytime a data
    field has text involved: fields that contain names or descriptors will almost
    inevitably contain multiple spellings of what is supposed to be the same term.
    Even fields that might seem straightforward to standardize may contain varying
    degrees of detail. For example, a “zip code” field might contain both five-digit
    zip code entries and more precise “Zip+4” values.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Other types of consistency may be less obvious but no less important. Units
    of measure, for example, need to be consistent across the dataset. While this
    might seem obvious, it’s actually easier than you might first imagine for this
    *not* to be the case. Let’s say you’re looking at the cost of an apple over a
    period of a decade. Sure, your data may record all of the prices in dollars, but
    inflation will be constantly changing what a dollar is *worth*. And while most
    of us are aware that Celsius and Fahrenheit “degrees” are of different sizes,
    it doesn’t stop there: an imperial pint is about 568 ml, whereas an American pint
    is about 473 ml. In fact, even accounts of Napoleon Bonaparte’s famously short
    stature is likely the result of an [inconsistency](https://britannica.com/story/was-napoleon-short)
    between the size of the 19th-century French inch (about 2.71cm) and today’s version
    (about 2.54 cm).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The solution to such inconsistencies is to *normalize* your data before doing
    any comparisons or analyses. In most cases, this is a matter of simple arithmetic:
    you simply have to choose which interpretation of the unit to which you will convert
    all others (another important moment for documentation!). This is true even of
    currency, which analysts often begin by converting to “real” (read: inflation-controlled)
    dollars, using some chosen year as a benchmark. For example, if we wanted to compare
    the real dollar value of the US federal minimum wage in 2009 (when it was last
    increased) to 2021, we could use an inflation calculator [like the one maintained
    by the Bureau of Labor Statistics (BLS)](https://bls.gov/data/inflation_calculator.htm)
    to see that the real dollar value of $7.25 per hour in 2009 is equivalent to only
    $5.72 per hour in 2021.'
  prefs: []
  type: TYPE_NORMAL
- en: Clear
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Like our Python code, our data and its labels will ideally be easy to read and
    interpret. Realistically, field (and even dataset) descriptors can sometimes be
    little more than cryptic codes that require constant cross-referencing with a
    data dictionary or other resources. This is part of why this type of data integrity
    is almost always the *product of* rather than the *precursor to* some degree of
    data wrangling.
  prefs: []
  type: TYPE_NORMAL
- en: For example, is there some logic to the fact that the table code for the US
    Census Bureau’s American Community Survey Demographic and Housing Estimates is
    [`DP05`](https://oreil.ly/kD48L)? Perhaps. But it’s hardly obvious to an occasional
    user of Census data, any more than the column label `DP05_0001E` is likely to
    be.^([6](ch03.html#idm45143427035984)) While a download of this Census table does
    include multiple files that can help you piece together the meaning of the filenames
    and column headers, developing a clear, high-integrity dataset may well require
    a fair amount of relabeling, reformatting, and reindexing—especially where government-produced
    data is involved. As always, however, documenting your information sources and
    renaming processes as you go is crucial.
  prefs: []
  type: TYPE_NORMAL
- en: Dimensionally structured
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Dimensionally structured data contains fields that have been grouped into or
    additionally labeled with useful categories, such as geographic region, year,
    or language. Such features often provide quick entry points for both data augmentation
    (which we’ll address in [“Data Augmentation”](#data_augmentation)) and data analysis,
    because they reduce the correlation and aggregation effort we have to do ourselves.
    These features can also serve as an indicator of what the data creator thought
    was important to record.
  prefs: []
  type: TYPE_NORMAL
- en: For example, the dimensions of our Citi Bike data include whether a bike was
    checked out to the account of a “Subscriber” or a “Customer,” as well as the account
    holder’s birth year and gender—suggesting that the data’s designers believed these
    features might yield useful insights about Citi Bike trips and usage. As we’ll
    see in [Chapter 7](ch07.html#chapter7), however, they did *not* choose to include
    any sort of “weekday/holiday” indicator—meaning that is a *dimension* of the data
    we’ll have to derive ourselves if we need it.
  prefs: []
  type: TYPE_NORMAL
- en: Improving Data Quality
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As previously noted, many aspects of data quality are the product of a data
    wrangling process—whether that involves reconciling and normalizing units, clarifying
    the meaning of obscure data labels, or seeking out background information about
    the representativeness of your dataset. Part of what this illustrates is that,
    in the real world, ensuring data *quality* is at least partly the result of multiple,
    iterative data wrangling processes. While the terms for these phases of the data
    wrangling process vary, I usually describe them as data *cleaning* and data *augmentation*.
  prefs: []
  type: TYPE_NORMAL
- en: Data Cleaning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In reality, data “cleaning” is not so much its own step in the data wrangling
    process as it is a constant activity that accompanies every *other* step, both
    because most data is *not* clean when we encounter it and because *how* a dataset
    (or part of it) needs to be “cleaned” is often revealed progressively as we work.
    At a high level, clean data might be summarized as being free from errors or typos—such
    as mismatched units, multiple spellings of the same word or term, and fields that
    are not well separated—and missing or impossible values. While many of these are
    at least somewhat straightforward to recognize (though not always to correct),
    deeper data problems may still persist. Measurement changes, calculation errors,
    and other oversights—especially in system-generated data—often don’t reveal themselves
    until some level of analysis has been done and the data has been “reality-checked”
    with folks who have significant expertise and/or firsthand experience with the
    subject.
  prefs: []
  type: TYPE_NORMAL
- en: 'The iterative nature of data cleaning is an example of why data wrangling is
    a cycle rather than a linear series of steps: as your work reveals more about
    the data and your understanding of its relationship to the world deepens, you
    may find that you need to revisit earlier work that you’ve done and repeat or
    adjust certain aspects of the data. Of course, this is just another reason why
    documenting your data wrangling work is so crucial: you can use your documentation
    to quickly identify where and how you made any changes or updates, and to reconcile
    them with what you now know. Without robust documentation to guide you, you may
    quickly find yourself needing to start from scratch.'
  prefs: []
  type: TYPE_NORMAL
- en: Data Augmentation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Augmenting* a dataset is the process of expanding or elaborating it, usually
    by connecting it with other datasets—this is really the nature of “big data” in
    the 21st century.^([7](ch03.html#idm45143426993168)) By using features shared
    among datasets, it’s possible to bring together data from multiple sources in
    order to get a more complete portrait of what is happening in the world by filling
    in gaps, providing corroborating measures, or adding contextual data that helps
    us better assess data fit. In our “best” school example, this might mean using
    school codes to bring together the several types of data collected by different
    entities, such as the state-level standardized test scores and local building
    information mentioned in [“Data Decoding”](#data_decoding). Through a combination
    of effective research and data wrangling, data augmentation can help us build
    data quality and answer questions far too nuanced to address through any single
    dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Like data cleaning, opportunities for data augmentation may arise at almost
    any point in the data wrangling process. At the same time, each new dataset we
    introduce will spawn a data wrangling process of its own. This means that *unlike*
    data cleaning, data augmentation has no definitive “end state”—there will *always*
    be another dataset we can add. This is yet another reason why specifically and
    succinctly stating our data wrangling question up front is so essential to the
    process: without a clearly articulated statement about what you’re trying to investigate,
    it’s all too easy to run out of time or other resources for your data wrangling
    effort. The good news is that if you’ve been keeping up your data diary, you’ll
    never lose track of a promising dataset for use in a future project.'
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Since our hands-on work with actual data has been limited up to this point,
    many of the concepts discussed in this chapter may seem a little bit abstract
    right now. Don’t worry! Things are about to get *very* hands on. In the coming
    chapters, we’ll start wrangling data that comes in various formats and from different
    sources, offering an inside look at how various characteristics of data quality
    play into the decisions we make as we access, evaluate, clean, analyze, and present
    our data. And you can be confident that by the end of this volume, you’ll be able
    to create meaningful, accurate, and compelling data analyses and visualizations
    to share your insights with the world!
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we’ll start this process by working through how to wrangle
    data from a wide variety of formats into a structure that will let us do the cleaning,
    augmentation, and analyses we need. Let’s dive in!
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch03.html#idm45143427340352-marker)) For example, see [“The Secret Bias
    Hidden in Mortgage-Approval Algorithms”](https://themarkup.org/denied/2021/08/25/the-secret-bias-hidden-in-mortgage-approval-algorithms)
    by Emmanuel Martinez and Lauren Kirschner (*The Markup*) .
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch03.html#idm45143427337760-marker)) “The World’s Most Valuable Resource
    Is No Longer Oil, but Data,” in [*The Economist*](https://economist.com/leaders/2017/05/06/the-worlds-most-valuable-resource-is-no-longer-oil-but-data).
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](ch03.html#idm45143427320640-marker)) For example, see “Raiders of the
    Lost Web” by Adrienne LaFrance in [*The Atlantic*](https://theatlantic.com/technology/archive/2015/10/raiders-of-the-lost-web/409210).
  prefs: []
  type: TYPE_NORMAL
- en: ^([4](ch03.html#idm45143427230496-marker)) For a highly readable overview, see
    [“Statistics Without Tears” by Amitav Banerjee and Suprakash Chaudhury](https://ncbi.nlm.nih.gov/pmc/articles/PMC3105563).
  prefs: []
  type: TYPE_NORMAL
- en: '^([5](ch03.html#idm45143427185920-marker)) This list is adapted from Stephen
    Few’s excellent book *Now You See It: Simple Visualization Techniques for Quantitative
    Analysis* (Analytics Press), with adjustments based on my own data wrangling experience.'
  prefs: []
  type: TYPE_NORMAL
- en: ^([6](ch03.html#idm45143427035984-marker)) It indicates the total population
    estimate, by the way.
  prefs: []
  type: TYPE_NORMAL
- en: ^([7](ch03.html#idm45143426993168-marker)) See danah boyd and Kate Crawford’s
    [“Critical Questions for Big Data”](https://tandfonline.com/doi/full/10.1080/1369118X.2012.678878)
    for a discussion on big data.
  prefs: []
  type: TYPE_NORMAL
