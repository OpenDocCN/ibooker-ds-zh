["```py\ndata = [ (\"big data\", 100, 15), (\"Hadoop\", 95, 25), (\"Python\", 75, 50),\n         (\"R\", 50, 40), (\"machine learning\", 80, 20), (\"statistics\", 20, 60),\n         (\"data science\", 60, 70), (\"analytics\", 90, 3),\n         (\"team player\", 85, 85), (\"dynamic\", 2, 90), (\"synergies\", 70, 0),\n         (\"actionable insights\", 40, 30), (\"think out of the box\", 45, 10),\n         (\"self-starter\", 30, 50), (\"customer focus\", 65, 15),\n         (\"thought leadership\", 35, 35)]\n```", "```py\nfrom matplotlib import pyplot as plt\n\ndef text_size(total: int) -> float:\n    \"\"\"equals 8 if total is 0, 28 if total is 200\"\"\"\n    return 8 + total / 200 * 20\n\nfor word, job_popularity, resume_popularity in data:\n    plt.text(job_popularity, resume_popularity, word,\n             ha='center', va='center',\n             size=text_size(job_popularity + resume_popularity))\nplt.xlabel(\"Popularity on Job Postings\")\nplt.ylabel(\"Popularity on Resumes\")\nplt.axis([0, 100, 0, 100])\nplt.xticks([])\nplt.yticks([])\nplt.show()\n```", "```py\ndef fix_unicode(text: str) -> str:\n    return text.replace(u\"\\u2019\", \"'\")\n```", "```py\nimport re\nfrom bs4 import BeautifulSoup\nimport requests\n\nurl = \"https://www.oreilly.com/ideas/what-is-data-science\"\nhtml = requests.get(url).text\nsoup = BeautifulSoup(html, 'html5lib')\n\ncontent = soup.find(\"div\", \"article-body\")   # find article-body div\nregex = r\"[\\w']+|[\\.]\"                       # matches a word or a period\n\ndocument = []\n\nfor paragraph in content(\"p\"):\n    words = re.findall(regex, fix_unicode(paragraph.text))\n    document.extend(words)\n```", "```py\nfrom collections import defaultdict\n\ntransitions = defaultdict(list)\nfor prev, current in zip(document, document[1:]):\n    transitions[prev].append(current)\n```", "```py\ndef generate_using_bigrams() -> str:\n    current = \".\"   # this means the next word will start a sentence\n    result = []\n    while True:\n        next_word_candidates = transitions[current]    # bigrams (current, _)\n        current = random.choice(next_word_candidates)  # choose one at random\n        result.append(current)                         # append it to results\n        if current == \".\": return \" \".join(result)     # if \".\" we're done\n```", "```py\ntrigram_transitions = defaultdict(list)\nstarts = []\n\nfor prev, current, next in zip(document, document[1:], document[2:]):\n\n    if prev == \".\":              # if the previous \"word\" was a period\n        starts.append(current)   # then this is a start word\n\n    trigram_transitions[(prev, current)].append(next)\n```", "```py\ndef generate_using_trigrams() -> str:\n    current = random.choice(starts)   # choose a random starting word\n    prev = \".\"                        # and precede it with a '.'\n    result = [current]\n    while True:\n        next_word_candidates = trigram_transitions[(prev, current)]\n        next_word = random.choice(next_word_candidates)\n\n        prev, current = current, next_word\n        result.append(current)\n\n        if current == \".\":\n            return \" \".join(result)\n```", "```py\nfrom typing import List, Dict\n\n# Type alias to refer to grammars later\nGrammar = Dict[str, List[str]]\n\ngrammar = {\n    \"_S\"  : [\"_NP _VP\"],\n    \"_NP\" : [\"_N\",\n             \"_A _NP _P _A _N\"],\n    \"_VP\" : [\"_V\",\n             \"_V _NP\"],\n    \"_N\"  : [\"data science\", \"Python\", \"regression\"],\n    \"_A\"  : [\"big\", \"linear\", \"logistic\"],\n    \"_P\"  : [\"about\", \"near\"],\n    \"_V\"  : [\"learns\", \"trains\", \"tests\", \"is\"]\n}\n```", "```py\n['_S']\n['_NP','_VP']\n['_N','_VP']\n['Python','_VP']\n['Python','_V','_NP']\n['Python','trains','_NP']\n['Python','trains','_A','_NP','_P','_A','_N']\n['Python','trains','logistic','_NP','_P','_A','_N']\n['Python','trains','logistic','_N','_P','_A','_N']\n['Python','trains','logistic','data science','_P','_A','_N']\n['Python','trains','logistic','data science','about','_A', '_N']\n['Python','trains','logistic','data science','about','logistic','_N']\n['Python','trains','logistic','data science','about','logistic','Python']\n```", "```py\ndef is_terminal(token: str) -> bool:\n    return token[0] != \"_\"\n```", "```py\ndef expand(grammar: Grammar, tokens: List[str]) -> List[str]:\n    for i, token in enumerate(tokens):\n        # If this is a terminal token, skip it.\n        if is_terminal(token): continue\n\n        # Otherwise, it's a nonterminal token,\n        # so we need to choose a replacement at random.\n        replacement = random.choice(grammar[token])\n\n        if is_terminal(replacement):\n            tokens[i] = replacement\n        else:\n            # Replacement could be, e.g., \"_NP _VP\", so we need to\n            # split it on spaces and splice it in.\n            tokens = tokens[:i] + replacement.split() + tokens[(i+1):]\n\n        # Now call expand on the new list of tokens.\n        return expand(grammar, tokens)\n\n    # If we get here, we had all terminals and are done.\n    return tokens\n```", "```py\ndef generate_sentence(grammar: Grammar) -> List[str]:\n    return expand(grammar, [\"_S\"])\n```", "```py\nrandom.random()\n```", "```py\ninverse_normal_cdf(random.random())\n```", "```py\nfrom typing import Tuple\nimport random\n\ndef roll_a_die() -> int:\n    return random.choice([1, 2, 3, 4, 5, 6])\n\ndef direct_sample() -> Tuple[int, int]:\n    d1 = roll_a_die()\n    d2 = roll_a_die()\n    return d1, d1 + d2\n```", "```py\ndef random_y_given_x(x: int) -> int:\n    \"\"\"equally likely to be x + 1, x + 2, ... , x + 6\"\"\"\n    return x + roll_a_die()\n```", "```py\ndef random_x_given_y(y: int) -> int:\n    if y <= 7:\n        # if the total is 7 or less, the first die is equally likely to be\n        # 1, 2, ..., (total - 1)\n        return random.randrange(1, y)\n    else:\n        # if the total is 7 or more, the first die is equally likely to be\n        # (total - 6), (total - 5), ..., 6\n        return random.randrange(y - 6, 7)\n```", "```py\ndef gibbs_sample(num_iters: int = 100) -> Tuple[int, int]:\n    x, y = 1, 2 # doesn't really matter\n    for _ in range(num_iters):\n        x = random_x_given_y(y)\n        y = random_y_given_x(x)\n    return x, y\n```", "```py\ndef compare_distributions(num_samples: int = 1000) -> Dict[int, List[int]]:\n    counts = defaultdict(lambda: [0, 0])\n    for _ in range(num_samples):\n        counts[gibbs_sample()][0] += 1\n        counts[direct_sample()][1] += 1\n    return counts\n```", "```py\ndocuments[3][4]\n```", "```py\ndocument_topics[3][4]\n```", "```py\ndef sample_from(weights: List[float]) -> int:\n    \"\"\"returns i with probability weights[i] / sum(weights)\"\"\"\n    total = sum(weights)\n    rnd = total * random.random()      # uniform between 0 and total\n    for i, w in enumerate(weights):\n        rnd -= w                       # return the smallest i such that\n        if rnd <= 0: return i          # weights[0] + ... + weights[i] >= rnd\n```", "```py\nfrom collections import Counter\n\n# Draw 1000 times and count\ndraws = Counter(sample_from([0.1, 0.1, 0.8]) for _ in range(1000))\nassert 10 < draws[0] < 190   # should be ~10%, this is a really loose test\nassert 10 < draws[1] < 190   # should be ~10%, this is a really loose test\nassert 650 < draws[2] < 950  # should be ~80%, this is a really loose test\nassert draws[0] + draws[1] + draws[2] == 1000\n```", "```py\ndocuments = [\n    [\"Hadoop\", \"Big Data\", \"HBase\", \"Java\", \"Spark\", \"Storm\", \"Cassandra\"],\n    [\"NoSQL\", \"MongoDB\", \"Cassandra\", \"HBase\", \"Postgres\"],\n    [\"Python\", \"scikit-learn\", \"scipy\", \"numpy\", \"statsmodels\", \"pandas\"],\n    [\"R\", \"Python\", \"statistics\", \"regression\", \"probability\"],\n    [\"machine learning\", \"regression\", \"decision trees\", \"libsvm\"],\n    [\"Python\", \"R\", \"Java\", \"C++\", \"Haskell\", \"programming languages\"],\n    [\"statistics\", \"probability\", \"mathematics\", \"theory\"],\n    [\"machine learning\", \"scikit-learn\", \"Mahout\", \"neural networks\"],\n    [\"neural networks\", \"deep learning\", \"Big Data\", \"artificial intelligence\"],\n    [\"Hadoop\", \"Java\", \"MapReduce\", \"Big Data\"],\n    [\"statistics\", \"R\", \"statsmodels\"],\n    [\"C++\", \"deep learning\", \"artificial intelligence\", \"probability\"],\n    [\"pandas\", \"R\", \"Python\"],\n    [\"databases\", \"HBase\", \"Postgres\", \"MySQL\", \"MongoDB\"],\n    [\"libsvm\", \"regression\", \"support vector machines\"]\n]\n```", "```py\nK = 4\n```", "```py\n    # a list of Counters, one for each document\n    document_topic_counts = [Counter() for _ in documents]\n    ```", "```py\n    # a list of Counters, one for each topic\n    topic_word_counts = [Counter() for _ in range(K)]\n    ```", "```py\n    # a list of numbers, one for each topic\n    topic_counts = [0 for _ in range(K)]\n    ```", "```py\n    # a list of numbers, one for each document\n    document_lengths = [len(document) for document in documents]\n    ```", "```py\n    distinct_words = set(word for document in documents for word in document)\n    W = len(distinct_words)\n    ```", "```py\n    D = len(documents)\n    ```", "```py\ndocument_topic_counts[3][1]\n```", "```py\ntopic_word_counts[2][\"nlp\"]\n```", "```py\ndef p_topic_given_document(topic: int, d: int, alpha: float = 0.1) -> float:\n    \"\"\"\n The fraction of words in document 'd'\n that are assigned to 'topic' (plus some smoothing)\n \"\"\"\n    return ((document_topic_counts[d][topic] + alpha) /\n            (document_lengths[d] + K * alpha))\n\ndef p_word_given_topic(word: str, topic: int, beta: float = 0.1) -> float:\n    \"\"\"\n The fraction of words assigned to 'topic'\n that equal 'word' (plus some smoothing)\n \"\"\"\n    return ((topic_word_counts[topic][word] + beta) /\n            (topic_counts[topic] + W * beta))\n```", "```py\ndef topic_weight(d: int, word: str, k: int) -> float:\n    \"\"\"\n Given a document and a word in that document,\n return the weight for the kth topic\n \"\"\"\n    return p_word_given_topic(word, k) * p_topic_given_document(k, d)\n\ndef choose_new_topic(d: int, word: str) -> int:\n    return sample_from([topic_weight(d, word, k)\n                        for k in range(K)])\n```", "```py\nrandom.seed(0)\ndocument_topics = [[random.randrange(K) for word in document]\n                   for document in documents]\n\nfor d in range(D):\n    for word, topic in zip(documents[d], document_topics[d]):\n        document_topic_counts[d][topic] += 1\n        topic_word_counts[topic][word] += 1\n        topic_counts[topic] += 1\n```", "```py\nimport tqdm\n\nfor iter in tqdm.trange(1000):\n    for d in range(D):\n        for i, (word, topic) in enumerate(zip(documents[d],\n                                              document_topics[d])):\n\n            # remove this word / topic from the counts\n            # so that it doesn't influence the weights\n            document_topic_counts[d][topic] -= 1\n            topic_word_counts[topic][word] -= 1\n            topic_counts[topic] -= 1\n            document_lengths[d] -= 1\n\n            # choose a new topic based on the weights\n            new_topic = choose_new_topic(d, word)\n            document_topics[d][i] = new_topic\n\n            # and now add it back to the counts\n            document_topic_counts[d][new_topic] += 1\n            topic_word_counts[new_topic][word] += 1\n            topic_counts[new_topic] += 1\n            document_lengths[d] += 1\n```", "```py\nfor k, word_counts in enumerate(topic_word_counts):\n    for word, count in word_counts.most_common():\n        if count > 0:\n            print(k, word, count)\n```", "```py\ntopic_names = [\"Big Data and programming languages\",\n               \"Python and statistics\",\n               \"databases\",\n               \"machine learning\"]\n```", "```py\nfor document, topic_counts in zip(documents, document_topic_counts):\n    print(document)\n    for topic, count in topic_counts.most_common():\n        if count > 0:\n            print(topic_names[topic], count)\n    print()\n```", "```py\n['Hadoop', 'Big Data', 'HBase', 'Java', 'Spark', 'Storm', 'Cassandra']\nBig Data and programming languages 4 databases 3\n['NoSQL', 'MongoDB', 'Cassandra', 'HBase', 'Postgres']\ndatabases 5\n['Python', 'scikit-learn', 'scipy', 'numpy', 'statsmodels', 'pandas']\nPython and statistics 5 machine learning 1\n```", "```py\nfrom scratch.linear_algebra import dot, Vector\nimport math\n\ndef cosine_similarity(v1: Vector, v2: Vector) -> float:\n    return dot(v1, v2) / math.sqrt(dot(v1, v1) * dot(v2, v2))\n\nassert cosine_similarity([1., 1, 1], [2., 2, 2]) == 1, \"same direction\"\nassert cosine_similarity([-1., -1], [2., 2]) == -1,    \"opposite direction\"\nassert cosine_similarity([1., 0], [0., 1]) == 0,       \"orthogonal\"\n```", "```py\ncolors = [\"red\", \"green\", \"blue\", \"yellow\", \"black\", \"\"]\nnouns = [\"bed\", \"car\", \"boat\", \"cat\"]\nverbs = [\"is\", \"was\", \"seems\"]\nadverbs = [\"very\", \"quite\", \"extremely\", \"\"]\nadjectives = [\"slow\", \"fast\", \"soft\", \"hard\"]\n\ndef make_sentence() -> str:\n    return \" \".join([\n        \"The\",\n        random.choice(colors),\n        random.choice(nouns),\n        random.choice(verbs),\n        random.choice(adverbs),\n        random.choice(adjectives),\n        \".\"\n    ])\n\nNUM_SENTENCES = 50\n\nrandom.seed(0)\nsentences = [make_sentence() for _ in range(NUM_SENTENCES)]\n```", "```py\nfrom scratch.deep_learning import Tensor\n\nclass Vocabulary:\n    def __init__(self, words: List[str] = None) -> None:\n        self.w2i: Dict[str, int] = {}  # mapping word -> word_id\n        self.i2w: Dict[int, str] = {}  # mapping word_id -> word\n\n        for word in (words or []):     # If words were provided,\n            self.add(word)             # add them.\n\n    @property\n    def size(self) -> int:\n        \"\"\"how many words are in the vocabulary\"\"\"\n        return len(self.w2i)\n\n    def add(self, word: str) -> None:\n        if word not in self.w2i:        # If the word is new to us:\n            word_id = len(self.w2i)     # Find the next id.\n            self.w2i[word] = word_id    # Add to the word -> word_id map.\n            self.i2w[word_id] = word    # Add to the word_id -> word map.\n\n    def get_id(self, word: str) -> int:\n        \"\"\"return the id of the word (or None)\"\"\"\n        return self.w2i.get(word)\n\n    def get_word(self, word_id: int) -> str:\n        \"\"\"return the word with the given id (or None)\"\"\"\n        return self.i2w.get(word_id)\n\n    def one_hot_encode(self, word: str) -> Tensor:\n        word_id = self.get_id(word)\n        assert word_id is not None, f\"unknown word {word}\"\n\n        return [1.0 if i == word_id else 0.0 for i in range(self.size)]\n```", "```py\nvocab = Vocabulary([\"a\", \"b\", \"c\"])\nassert vocab.size == 3,              \"there are 3 words in the vocab\"\nassert vocab.get_id(\"b\") == 1,       \"b should have word_id 1\"\nassert vocab.one_hot_encode(\"b\") == [0, 1, 0]\nassert vocab.get_id(\"z\") is None,    \"z is not in the vocab\"\nassert vocab.get_word(2) == \"c\",     \"word_id 2 should be c\"\nvocab.add(\"z\")\nassert vocab.size == 4,              \"now there are 4 words in the vocab\"\nassert vocab.get_id(\"z\") == 3,       \"now z should have id 3\"\nassert vocab.one_hot_encode(\"z\") == [0, 0, 0, 1]\n```", "```py\nimport json\n\ndef save_vocab(vocab: Vocabulary, filename: str) -> None:\n    with open(filename, 'w') as f:\n        json.dump(vocab.w2i, f)       # Only need to save w2i\n\ndef load_vocab(filename: str) -> Vocabulary:\n    vocab = Vocabulary()\n    with open(filename) as f:\n        # Load w2i and generate i2w from it\n        vocab.w2i = json.load(f)\n        vocab.i2w = {id: word for word, id in vocab.w2i.items()}\n    return vocab\n```", "```py\nfrom typing import Iterable\nfrom scratch.deep_learning import Layer, Tensor, random_tensor, zeros_like\n\nclass Embedding(Layer):\n    def __init__(self, num_embeddings: int, embedding_dim: int) -> None:\n        self.num_embeddings = num_embeddings\n        self.embedding_dim = embedding_dim\n\n        # One vector of size embedding_dim for each desired embedding\n        self.embeddings = random_tensor(num_embeddings, embedding_dim)\n        self.grad = zeros_like(self.embeddings)\n\n        # Save last input id\n        self.last_input_id = None\n```", "```py\n    def forward(self, input_id: int) -> Tensor:\n        \"\"\"Just select the embedding vector corresponding to the input id\"\"\"\n        self.input_id = input_id    # remember for use in backpropagation\n\n        return self.embeddings[input_id]\n```", "```py\n    def backward(self, gradient: Tensor) -> None:\n        # Zero out the gradient corresponding to the last input.\n        # This is way cheaper than creating a new all-zero tensor each time.\n        if self.last_input_id is not None:\n            zero_row = [0 for _ in range(self.embedding_dim)]\n            self.grad[self.last_input_id] = zero_row\n\n        self.last_input_id = self.input_id\n        self.grad[self.input_id] = gradient\n```", "```py\n    def params(self) -> Iterable[Tensor]:\n        return [self.embeddings]\n\n    def grads(self) -> Iterable[Tensor]:\n        return [self.grad]\n```", "```py\nclass TextEmbedding(Embedding):\n    def __init__(self, vocab: Vocabulary, embedding_dim: int) -> None:\n        # Call the superclass constructor\n        super().__init__(vocab.size, embedding_dim)\n\n        # And hang onto the vocab\n        self.vocab = vocab\n```", "```py\n    def __getitem__(self, word: str) -> Tensor:\n        word_id = self.vocab.get_id(word)\n        if word_id is not None:\n            return self.embeddings[word_id]\n        else:\n            return None\n```", "```py\nword_vector = embedding[\"black\"]\n```", "```py\n    def closest(self, word: str, n: int = 5) -> List[Tuple[float, str]]:\n        \"\"\"Returns the n closest words based on cosine similarity\"\"\"\n        vector = self[word]\n\n        # Compute pairs (similarity, other_word), and sort most similar first\n        scores = [(cosine_similarity(vector, self.embeddings[i]), other_word)\n                  for other_word, i in self.vocab.w2i.items()]\n        scores.sort(reverse=True)\n\n        return scores[:n]\n```", "```py\nimport re\n\n# This is not a great regex, but it works on our data.\ntokenized_sentences = [re.findall(\"[a-z]+|[.]\", sentence.lower())\n                       for sentence in sentences]\n```", "```py\n# Create a vocabulary (that is, a mapping word -> word_id) based on our text.\nvocab = Vocabulary(word\n                   for sentence_words in tokenized_sentences\n                   for word in sentence_words)\n```", "```py\nfrom scratch.deep_learning import Tensor, one_hot_encode\n\ninputs: List[int] = []\ntargets: List[Tensor] = []\n\nfor sentence in tokenized_sentences:\n    for i, word in enumerate(sentence):          # For each word\n        for j in [i - 2, i - 1, i + 1, i + 2]:   # take the nearby locations\n            if 0 <= j < len(sentence):           # that aren't out of bounds\n                nearby_word = sentence[j]        # and get those words.\n\n                # Add an input that's the original word_id\n                inputs.append(vocab.get_id(word))\n\n                # Add a target that's the one-hot-encoded nearby word\n                targets.append(vocab.one_hot_encode(nearby_word))\n```", "```py\nfrom scratch.deep_learning import Sequential, Linear\n\nrandom.seed(0)\nEMBEDDING_DIM = 5  # seems like a good size\n\n# Define the embedding layer separately, so we can reference it.\nembedding = TextEmbedding(vocab=vocab, embedding_dim=EMBEDDING_DIM)\n\nmodel = Sequential([\n    # Given a word (as a vector of word_ids), look up its embedding.\n    embedding,\n    # And use a linear layer to compute scores for \"nearby words.\"\n    Linear(input_dim=EMBEDDING_DIM, output_dim=vocab.size)\n])\n```", "```py\nfrom scratch.deep_learning import SoftmaxCrossEntropy, Momentum, GradientDescent\n\nloss = SoftmaxCrossEntropy()\noptimizer = GradientDescent(learning_rate=0.01)\n\nfor epoch in range(100):\n    epoch_loss = 0.0\n    for input, target in zip(inputs, targets):\n        predicted = model.forward(input)\n        epoch_loss += loss.loss(predicted, target)\n        gradient = loss.gradient(predicted, target)\n        model.backward(gradient)\n        optimizer.step(model)\n    print(epoch, epoch_loss)            # Print the loss\n    print(embedding.closest(\"black\"))   # and also a few nearest words\n    print(embedding.closest(\"slow\"))    # so we can see what's being\n    print(embedding.closest(\"car\"))     # learned.\n```", "```py\npairs = [(cosine_similarity(embedding[w1], embedding[w2]), w1, w2)\n         for w1 in vocab.w2i\n         for w2 in vocab.w2i\n         if w1 < w2]\npairs.sort(reverse=True)\nprint(pairs[:5])\n```", "```py\n[(0.9980283554864815, 'boat', 'car'),\n (0.9975147744587706, 'bed', 'cat'),\n (0.9953153441218054, 'seems', 'was'),\n (0.9927107440377975, 'extremely', 'quite'),\n (0.9836183658415987, 'bed', 'car')]\n```", "```py\nfrom scratch.working_with_data import pca, transform\nimport matplotlib.pyplot as plt\n\n# Extract the first two principal components and transform the word vectors\ncomponents = pca(embedding.embeddings, 2)\ntransformed = transform(embedding.embeddings, components)\n\n# Scatter the points (and make them white so they're \"invisible\")\nfig, ax = plt.subplots()\nax.scatter(*zip(*transformed), marker='.', color='w')\n\n# Add annotations for each word at its transformed location\nfor word, idx in vocab.w2i.items():\n    ax.annotate(word, transformed[idx])\n\n# And hide the axes\nax.get_xaxis().set_visible(False)\nax.get_yaxis().set_visible(False)\n\nplt.show()\n```", "```py\noutput[o] = dot(w[o], input) + b[o]\n```", "```py\noutput[o] = dot(w[o], input) + dot(u[o], hidden) + b[o]\n```", "```py\nfrom scratch.deep_learning import tensor_apply, tanh\n\nclass SimpleRnn(Layer):\n    \"\"\"Just about the simplest possible recurrent layer.\"\"\"\n    def __init__(self, input_dim: int, hidden_dim: int) -> None:\n        self.input_dim = input_dim\n        self.hidden_dim = hidden_dim\n\n        self.w = random_tensor(hidden_dim, input_dim, init='xavier')\n        self.u = random_tensor(hidden_dim, hidden_dim, init='xavier')\n        self.b = random_tensor(hidden_dim)\n\n        self.reset_hidden_state()\n\n    def reset_hidden_state(self) -> None:\n        self.hidden = [0 for _ in range(self.hidden_dim)]\n```", "```py\n    def forward(self, input: Tensor) -> Tensor:\n        self.input = input              # Save both input and previous\n        self.prev_hidden = self.hidden  # hidden state to use in backprop.\n\n        a = [(dot(self.w[h], input) +           # weights @ input\n              dot(self.u[h], self.hidden) +     # weights @ hidden\n              self.b[h])                        # bias\n             for h in range(self.hidden_dim)]\n\n        self.hidden = tensor_apply(tanh, a)  # Apply tanh activation\n        return self.hidden                   # and return the result.\n```", "```py\n    def backward(self, gradient: Tensor):\n        # Backpropagate through the tanh\n        a_grad = [gradient[h] * (1 - self.hidden[h] ** 2)\n                  for h in range(self.hidden_dim)]\n\n        # b has the same gradient as a\n        self.b_grad = a_grad\n\n        # Each w[h][i] is multiplied by input[i] and added to a[h],\n        # so each w_grad[h][i] = a_grad[h] * input[i]\n        self.w_grad = [[a_grad[h] * self.input[i]\n                        for i in range(self.input_dim)]\n                       for h in range(self.hidden_dim)]\n\n        # Each u[h][h2] is multiplied by hidden[h2] and added to a[h],\n        # so each u_grad[h][h2] = a_grad[h] * prev_hidden[h2]\n        self.u_grad = [[a_grad[h] * self.prev_hidden[h2]\n                        for h2 in range(self.hidden_dim)]\n                       for h in range(self.hidden_dim)]\n\n        # Each input[i] is multiplied by every w[h][i] and added to a[h],\n        # so each input_grad[i] = sum(a_grad[h] * w[h][i] for h in ...)\n        return [sum(a_grad[h] * self.w[h][i] for h in range(self.hidden_dim))\n                for i in range(self.input_dim)]\n```", "```py\n    def params(self) -> Iterable[Tensor]:\n        return [self.w, self.u, self.b]\n\n    def grads(self) -> Iterable[Tensor]:\n        return [self.w_grad, self.u_grad, self.b_grad]\n```", "```py\nfrom bs4 import BeautifulSoup\nimport requests\n\nurl = \"https://www.ycombinator.com/topcompanies/\"\nsoup = BeautifulSoup(requests.get(url).text, 'html5lib')\n\n# We get the companies twice, so use a set comprehension to deduplicate.\ncompanies = list({b.text\n                  for b in soup(\"b\")\n                  if \"h4\" in b.get(\"class\", ())})\nassert len(companies) == 101\n```", "```py\nvocab = Vocabulary([c for company in companies for c in company])\n```", "```py\nSTART = \"^\"\nSTOP = \"$\"\n\n# We need to add them to the vocabulary too.\nvocab.add(START)\nvocab.add(STOP)\n```", "```py\nHIDDEN_DIM = 32  # You should experiment with different sizes!\n\nrnn1 =  SimpleRnn(input_dim=vocab.size, hidden_dim=HIDDEN_DIM)\nrnn2 =  SimpleRnn(input_dim=HIDDEN_DIM, hidden_dim=HIDDEN_DIM)\nlinear = Linear(input_dim=HIDDEN_DIM, output_dim=vocab.size)\n\nmodel = Sequential([\n    rnn1,\n    rnn2,\n    linear\n])\n```", "```py\nfrom scratch.deep_learning import softmax\n\ndef generate(seed: str = START, max_len: int = 50) -> str:\n    rnn1.reset_hidden_state()  # Reset both hidden states\n    rnn2.reset_hidden_state()\n    output = [seed]            # Start the output with the specified seed\n\n    # Keep going until we produce the STOP character or reach the max length\n    while output[-1] != STOP and len(output) < max_len:\n        # Use the last character as the input\n        input = vocab.one_hot_encode(output[-1])\n\n        # Generate scores using the model\n        predicted = model.forward(input)\n\n        # Convert them to probabilities and draw a random char_id\n        probabilities = softmax(predicted)\n        next_char_id = sample_from(probabilities)\n\n        # Add the corresponding char to our output\n        output.append(vocab.get_word(next_char_id))\n\n    # Get rid of START and END characters and return the word\n    return ''.join(output[1:-1])\n```", "```py\nloss = SoftmaxCrossEntropy()\noptimizer = Momentum(learning_rate=0.01, momentum=0.9)\n\nfor epoch in range(300):\n    random.shuffle(companies)  # Train in a different order each epoch.\n    epoch_loss = 0             # Track the loss.\n    for company in tqdm.tqdm(companies):\n        rnn1.reset_hidden_state()  # Reset both hidden states.\n        rnn2.reset_hidden_state()\n        company = START + company + STOP   # Add START and STOP characters.\n\n        # The rest is just our usual training loop, except that the inputs\n        # and target are the one-hot-encoded previous and next characters.\n        for prev, next in zip(company, company[1:]):\n            input = vocab.one_hot_encode(prev)\n            target = vocab.one_hot_encode(next)\n            predicted = model.forward(input)\n            epoch_loss += loss.loss(predicted, target)\n            gradient = loss.gradient(predicted, target)\n            model.backward(gradient)\n            optimizer.step(model)\n\n    # Each epoch, print the loss and also generate a name.\n    print(epoch, epoch_loss, generate())\n\n    # Turn down the learning rate for the last 100 epochs.\n    # There's no principled reason for this, but it seems to work.\n    if epoch == 200:\n        optimizer.lr *= 0.1\n```"]