- en: Chapter 6\. Understanding Wikipedia with LDA and Spark NLP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With the growing amount of unstructured text data in recent years, it has become
    difficult to obtain the relevant and desired information. Language technology
    provides powerful methods that can be used to mine through text data and fetch
    the information that we are looking for. In this chapter, we will use PySpark
    and the Spark NLP (natural language processing) library to use one such technique—topic
    modeling. Specifically, we will use the latent Dirichlet algorithm (LDA) to understand
    a dataset of Wikipedia documents.
  prefs: []
  type: TYPE_NORMAL
- en: '*Topic modeling*, one of the most common tasks in natural language processing,
    is a statistical approach for data modeling that helps in discovering underlying
    topics that are present in a collection of documents. Extracting topic distribution
    from millions of documents can be useful in many ways—for example, identifying
    the reasons for complaints about a particular product or all products, or identifying
    topics in news articles. The most popular algorithm for topic modeling is LDA.
    It is a generative model that assumes that documents are represented by a distribution
    of topics. Topics, in turn, are represented by a distribution of words. PySpark
    MLlib offers an optimized version of LDA that is specifically designed to work
    in a distributed environment. We will build a simple topic modeling pipeline using
    Spark NLP for preprocessing the data and Spark MLlib’s LDA to extract topics from
    the data.'
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we’ll embark upon the modest task of distilling the human knowledge
    based on latent (hidden) topics and relationships. We’ll apply LDA to a corpus
    consisting of the articles contained in Wikipedia. We will start by understanding
    the basics of LDA and go over its implementation in PySpark. Then we’ll download
    the dataset and set up our programming environment by installing Spark NLP. This
    will be followed by data preprocessing. You will witness the power of Spark NLP
    library’s out-of-the-box methods, which make NLP tasks significantly easier.
  prefs: []
  type: TYPE_NORMAL
- en: Then we will score the terms in our documents using the TF-IDF (term frequency-inverse
    document frequency) technique and feed the resulting output into our LDA model.
    To finish up, we’ll go through the topics assigned by our model to the input documents.
    We should be able to understand which bucket an entry belongs in without the need
    to read it. Let’s begin by going over the fundamentals of LDA.
  prefs: []
  type: TYPE_NORMAL
- en: Latent Dirichlet Allocation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The idea behind latent Dirichlet allocation is that documents are generated
    based on a set of topics. In this process, we assume that each document is distributed
    over the topics, and each topic is distributed over a set of terms. Each document
    and each word are generated from sampling these distributions. The LDA learner
    works backward and tries to identify the distributions where the observed is most
    probable.
  prefs: []
  type: TYPE_NORMAL
- en: 'It attempts to distill the corpus into a set of relevant *topics*. Each topic
    captures a thread of variation in the data and often corresponds to one of the
    ideas that the corpus discusses. A document can be a part of multiple topics.
    You can think of LDA as providing a way to *soft cluster* the documents, too.
    Without delving into the mathematics, an LDA topic model describes two primary
    attributes: a chance of selecting a topic when sampling a particular document,
    and a chance of selecting a particular term when selecting a topic. For example,
    LDA might discover a topic with strong association with the terms “Asimov” and
    “robot,” and with the documents “foundation series” and “science fiction.” By
    selecting only the most important concepts, LDA can throw away some irrelevant
    noise and merge co-occurring strands to come up with a simpler representation
    of the data.'
  prefs: []
  type: TYPE_NORMAL
- en: We can employ this technique in a variety of tasks. For example, it can help
    us recommend similar Wikipedia entries when provided with an input entry. By encapsulating
    the patterns of variance in the corpus, it can base scores on a deeper understanding
    than simply on counting occurrences and co-occurrences of words. Up next, let’s
    have a look at PySpark’s LDA implementation.
  prefs: []
  type: TYPE_NORMAL
- en: LDA in PySpark
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'PySpark MLlib offers an LDA implementation as one of its clustering algorithms.
    Here’s some example code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_understanding_wikipedia___span_class__keep_together__with_lda_and_spark_nlp__span__CO1-1)'
  prefs: []
  type: TYPE_NORMAL
- en: We apply LDA to our dataframe with the number of topics (*k*) set to 2.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_understanding_wikipedia___span_class__keep_together__with_lda_and_spark_nlp__span__CO1-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Dataframe describing the probability weight associated with each term in our
    topics.
  prefs: []
  type: TYPE_NORMAL
- en: We will explore PySpark’s LDA implementation and associated parameters when
    applying it to the Wikipedia dataset. First, though, we need to download the relevant
    dataset. That’s what we will do next.
  prefs: []
  type: TYPE_NORMAL
- en: Getting the Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Wikipedia makes dumps of all its articles available. The full dump comes in
    a single, large XML file. These can be [downloaded](https://oreil.ly/DhGlJ) and
    then placed on a cloud storage solution (such as AWS S3 or GCS, Google Cloud Storage)
    or HDFS. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: This can take a little while.
  prefs: []
  type: TYPE_NORMAL
- en: Chugging through this volume of data makes the most sense with a cluster of
    a few nodes to work with. To run this chapter’s code on a local machine, a better
    option is to generate a smaller dump using [Wikipedia’s export pages](https://oreil.ly/Rrpmr).
    Try getting all the pages from multiple categories that have many pages and few
    subcategories, such as biology, health, and geometry. For the following code to
    work, download the dump into the *ch06-LDA/* directory and rename it to *wikidump.xml*.
  prefs: []
  type: TYPE_NORMAL
- en: We need to convert the Wikipedia XML dump into a format that we can easily work
    with in PySpark. When working on our local machine, we can use the convenient
    [WikiExtractor tool](https://oreil.ly/pfwrE) for this. It extracts and cleans
    text from Wikipedia database dumps such as what we have.
  prefs: []
  type: TYPE_NORMAL
- en: 'Install it using pip:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Then it’s as simple as running the following command in the directory containing
    the downloaded file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is stored in a single or several files of similar size in a given
    directory named `text`. Each file will contains several documents in the following
    format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_understanding_wikipedia___span_class__keep_together__with_lda_and_spark_nlp__span__CO2-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Rename text directory to wikidump
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s get familiar with the Spark NLP library before we start working
    on the data.
  prefs: []
  type: TYPE_NORMAL
- en: Spark NLP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Spark NLP library was originally designed by [John Snow Labs](https://oreil.ly/E9KVt)
    in early 2017 as an annotation library native to Spark to take full advantage
    of Spark SQL and MLlib modules. The inspiration came from trying to use Spark
    to distribute other NLP libraries, which were generally not implemented with concurrency
    or distributed computing in mind.
  prefs: []
  type: TYPE_NORMAL
- en: Spark NLP has the same concepts as any other annotation library but differs
    in how it stores annotations. Most annotation libraries store the annotations
    in the document object, but Spark NLP creates columns for the different types
    of annotations. The annotators are implemented as transformers, estimators, and
    models. We will look at these in the next section when applying them to our dataset
    for preprocessing. Before that, let’s download and set up Spark NLP on our system.
  prefs: []
  type: TYPE_NORMAL
- en: Setting Up Your Environment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Install Spark NLP via pip:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Start the PySpark shell:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s import Spark NLP in our PySpark shell:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, you can import the relevant Spark NLP modules that we’ll use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have set up our programming environment, let’s start working on
    our dataset. We’ll start by parsing the data as a PySpark DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: Parsing the Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The output from WikiExtractor can create multiple directories depending on
    the size of the input dump. We want to import all the data as a single DataFrame.
    Let’s start by specifying the input directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We import the data using the `wholeTextFiles` method accessible through `sparkContext`.
    This method reads the data in as an RDD. We convert it into a DataFrame since
    that’s what we want:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting DataFrame will consist of two columns. The number of records
    will correspond to the number of files that were read. The first column consists
    of the filepath and the second contains the corresponding text content. The text
    contains multiple entries, but we want each row to correspond to a single entry.
    Based on the entry structure that we had seen earlier, we can separate entries
    using a couple of PySpark utilities: `split` and `explode`.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The `split` function is used to split the DataFrame string `Column` into an
    array based on matches of a provided pattern. In the previous code, we split the
    combined document XML string into an array based on the *</doc>* string. This
    effectively gives us an array of multiple documents. Then, we use `explode` to
    create new rows for each element in the array returned by the `split` function.
    This results in rows being created corresponding to each document.
  prefs: []
  type: TYPE_NORMAL
- en: 'Go through the structure obtained in the `content` column by our previous operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'We can further split our `content` column by extracting the entries’ titles:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have our parsed dataset, let’s move on to preprocessing using Spark
    NLP.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the Data Using Spark NLP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We had earlier mentioned that a library based on the document-annotator model
    such as Spark NLP has the concept of “documents.” There does not exist such a
    concept natively in PySpark. Hence, one of Spark NLP’s core design tenets is strong
    interoperability with MLlib. This is done by providing DataFrame-compatible transformers
    that convert text columns into documents and convert annotations into PySpark
    data types.
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by creating our `document` column using `DocumentAssembler`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: We could have utilized Spark NLP’s [`DocumentNormalizer`](https://oreil.ly/UL1vp)
    annotator in the parsing section.
  prefs: []
  type: TYPE_NORMAL
- en: We can transform the input dataframe directly as we have done in the previous
    code. However, we will instead use `DocumentAssembler` and other required annotators
    as part of an ML pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use the following annotators as part of our preprocessing pipeline:
    `Tokenizer`, `Normalizer`, `StopWordsCleaner`, and `Stemmer`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start with the `Tokenizer`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The `Tokenizer` is a fundamental annotator. Almost all text-based data processing
    begins with some form of tokenization, which is the process of breaking raw text
    into small chunks. Tokens can be words, characters, or subwords (n-grams). Most
    classical NLP algorithms expect tokens as the basic input. Many deep learning
    algorithms are being developed that take characters as basic input. Most NLP applications
    still use tokenization.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next up is the `Normalizer`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The `Normalizer` cleans out tokens from the previous step and removes all unwanted
    characters from the text.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next up is the `StopWordsCleaner`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: This annotator removes *stop words* from text. Stop words such as “the,” “is,”
    and “at,” which are so common that they can be removed without significantly altering
    the meaning of a text. Removing stop words is useful when one wants to deal with
    only the most semantically important words in a text and ignore words that are
    rarely semantically relevant, such as articles and prepositions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Last up is the `Stemmer`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '`Stemmer` returns hard stems out of words with the objective of retrieving
    the meaningful part of the word. *Stemming* is the process of reducing a word
    to its root word stem with the objective of retrieving the meaningful part. For
    example, “picking,” “picked,” and “picks” all have “pick” as the root.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We are almost done. Before we can complete our NLP pipeline, we need to add
    the `Finisher`. Spark NLP adds its own structure when we convert each row in the
    dataframe to a document. `Finisher` is critical because it helps us to bring back
    the expected structure, an array of tokens:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we have all the required pieces in place. Let’s build our pipeline so that
    each phase can be executed in sequence:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Execute the pipeline and transform the dataframe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_understanding_wikipedia___span_class__keep_together__with_lda_and_spark_nlp__span__CO3-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Train the pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_understanding_wikipedia___span_class__keep_together__with_lda_and_spark_nlp__span__CO3-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Apply the pipeline to transform the dataframe.
  prefs: []
  type: TYPE_NORMAL
- en: 'The NLP pipeline created intermediary columns that we do not need. Let’s remove
    the redundant columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Next, we will understand the basics of TF-IDF and implement it on the preprocessed
    dataset, `token_df`, that we have obtained before building an LDA model.
  prefs: []
  type: TYPE_NORMAL
- en: TF-IDF
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before applying LDA, we need to convert our data into a numeric representation.
    We will obtain such a representation using the term frequency-inverse document
    frequency method. Loosely, TF-IDF is used to determine the importance of terms
    corresponding to given documents. Here’s a representation in Python code of the
    formula. We won’t actually end up using this code because PySpark provides its
    own implementation.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: TF-IDF captures two intuitions about the relevance of a term to a document.
    First, we would expect that the more often a term occurs in a document, the more
    important it is to that document. Second, not all terms are equal in a global
    sense. It is more meaningful to encounter a word that occurs rarely in the entire
    corpus than a word that appears in most of the documents; thus, the metric uses
    the *inverse* of the word’s appearance in documents in the full corpus.
  prefs: []
  type: TYPE_NORMAL
- en: The frequency of words in a corpus tends to be distributed exponentially. A
    common word might appear ten times as often as a mildly common word, which in
    turn might appear ten or a hundred times as often as a rare word. Basing a metric
    on the raw inverse document frequency would give rare words enormous weight and
    practically ignore the impact of all other words. To capture this distribution,
    the scheme uses the *log* of the inverse document frequency. This mellows the
    differences in document frequencies by transforming the multiplicative gaps between
    them into additive gaps.
  prefs: []
  type: TYPE_NORMAL
- en: The model relies on a few assumptions. It treats each document as a “bag of
    words,” meaning that it pays no attention to the ordering of words, sentence structure,
    or negations. By representing each term once, the model has difficulty dealing
    with *polysemy*, the use of the same word for multiple meanings. For example,
    the model can’t distinguish between the use of “band” in “Radiohead is the best
    band ever” and “I broke a rubber band.” If both sentences appear often in the
    corpus, it may come to associate “Radiohead” with “rubber.”
  prefs: []
  type: TYPE_NORMAL
- en: Let’s proceed now to the implementation of TF-IDF using PySpark.
  prefs: []
  type: TYPE_NORMAL
- en: Computing the TF-IDFs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, we’ll calculate TF (term frequency; that is, the frequency of each term
    in a document) with `CountVectorizer`, which keeps track of the vocabulary that’s
    being created so we can map our topics back to their corresponding words. TF creates
    a matrix that counts how many times each word in the vocabulary appears in each
    body of text. This then gives each word a weight based on its frequency. We derive
    the vocabulary of our data while fitting and get the counts at the transform step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we proceed with IDF (the inverse frequency of documents where a term
    occurred), which reduces the weights of commonly appearing terms:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'This is what the result will look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: With all the preprocessing and feature engineering done, we can now create our
    LDA model. That’s what we’ll do in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Creating Our LDA Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We had mentioned previously that LDA distills a corpus into a set of relevant
    topics. We will get to have a look at examples of such topics further ahead in
    this section. Before that, we need to decide on two hyperparameters that our LDA
    model requires. They are number of topics (referred to as *k*) and number of iterations.
  prefs: []
  type: TYPE_NORMAL
- en: There are multiple ways that you can go about choosing *k*. Two popular metrics
    used for doing this are perplexity and topic coherence. The former is made available
    by PySpark’s implementation. The basic idea is to try to figure out the *k* where
    the improvements to these metrics start to become insignificant. If you are familiar
    with the *elbow method* for finding the number of clusters for K-means, this is
    similar. Depending on the size of your corpus, it can be a resource-intensive
    and time-consuming process since you will need to build the model for multiple
    values of *k*. An alternative could be to try to create a representative sample
    of the dataset in hand and use it to determine *k*. It is left as an exercise
    for you to read up on this and try it.
  prefs: []
  type: TYPE_NORMAL
- en: Since you may be working locally right now, we will go ahead and assign reasonable
    values (*k* as 5 and `max_iter` as 50) for now.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s create our LDA model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Perplexity is a measurement of how well a model predicts a sample. A low perplexity
    indicates the probability distribution is good at predicting the sample. When
    comparing different models, go for the one with the lower value of perplexity.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have created our model, we want to output the topics as human-readable.
    We will get the vocabulary generated from our preprocessing steps, get the topics
    from the LDA model, and map both of them.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_understanding_wikipedia___span_class__keep_together__with_lda_and_spark_nlp__span__CO4-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Create a reference to our vocabulary.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_understanding_wikipedia___span_class__keep_together__with_lda_and_spark_nlp__span__CO4-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Get topics generated by the LDA model using `describeTopics` and load them into
    a Python list.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_understanding_wikipedia___span_class__keep_together__with_lda_and_spark_nlp__span__CO4-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Get indices of the vocabulary terms from our topics.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us now generate the mappings from our topic indices to our vocabulary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: The previous result is not perfect, but there are some patterns that can be
    noticed in the topics. Topic 1 is primarily related to health. It also contains
    references to Islam and empire. Could it be because of them being referenced in
    medicinal history and vice versa or something else? Topics 2 and 3 are related
    to mathematics with the latter inclined toward geometry. Topic 5 is a mix of computing
    and mathematics. Even if you hadn’t read any of the documents, you can already
    guess with a reasonable accuracy about their categories. This is exciting!
  prefs: []
  type: TYPE_NORMAL
- en: We can now also check which topics our input documents are most closely related
    to. A single document can have multiple topic associations that are significant.
    For now, we’ll only look at the most strongly associated topics.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s run the LDA model’s transform operation on our input dataframe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, each document has a topic probability distribution associated
    with it. To get the associated topic for each document, we want to find out the
    topic index with the highest probability score. We can then map it to the topics
    that we obtained previously.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will write a PySpark UDF to find the highest topic probability score for
    each record:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Topic 2, if you remember, was associated with mathematics. The output is in
    line with our expectations. You can scan more of the dataset to see how it performed.
    You can select particular topics using the `where` or `filter` commands and compare
    them against the topic list generated earlier to get a better sense of the clusters
    that have been created. As promised at the beginning of the chapter, we’re able
    to cluster articles into different topics without reading them!
  prefs: []
  type: TYPE_NORMAL
- en: Where to Go from Here
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we performed LDA on the Wikipedia corpus. In the process, we
    also learned about text preprocessing using the amazing Spark NLP library and
    the TF-IDF technique. You can further build on this by improving the model by
    better preprocessing and hyperparameter tuning. In addition, you can even try
    to recommend similar entries based on document similarity when provided with user
    input. Such a similarity measure may be obtained by using the probability distribution
    vector obtained from LDA.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, a variety of other methods exist for understanding large corpora
    of text. For example, a technique known as latent semantic analysis (LSA) is useful
    in similar applications and was used in the previous edition of this book on the
    same dataset. Deep learning, which is explored in [Chapter 10](ch10.xhtml#image_similarity_detection_with_deep_learning_and_pyspark_lsh),
    also offers avenues to perform topic modeling. You can explore them on your own.
  prefs: []
  type: TYPE_NORMAL
