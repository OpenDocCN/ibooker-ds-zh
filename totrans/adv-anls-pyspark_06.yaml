- en: Chapter 6\. Understanding Wikipedia with LDA and Spark NLP
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With the growing amount of unstructured text data in recent years, it has become
    difficult to obtain the relevant and desired information. Language technology
    provides powerful methods that can be used to mine through text data and fetch
    the information that we are looking for. In this chapter, we will use PySpark
    and the Spark NLP (natural language processing) library to use one such technique—topic
    modeling. Specifically, we will use the latent Dirichlet algorithm (LDA) to understand
    a dataset of Wikipedia documents.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: '*Topic modeling*, one of the most common tasks in natural language processing,
    is a statistical approach for data modeling that helps in discovering underlying
    topics that are present in a collection of documents. Extracting topic distribution
    from millions of documents can be useful in many ways—for example, identifying
    the reasons for complaints about a particular product or all products, or identifying
    topics in news articles. The most popular algorithm for topic modeling is LDA.
    It is a generative model that assumes that documents are represented by a distribution
    of topics. Topics, in turn, are represented by a distribution of words. PySpark
    MLlib offers an optimized version of LDA that is specifically designed to work
    in a distributed environment. We will build a simple topic modeling pipeline using
    Spark NLP for preprocessing the data and Spark MLlib’s LDA to extract topics from
    the data.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we’ll embark upon the modest task of distilling the human knowledge
    based on latent (hidden) topics and relationships. We’ll apply LDA to a corpus
    consisting of the articles contained in Wikipedia. We will start by understanding
    the basics of LDA and go over its implementation in PySpark. Then we’ll download
    the dataset and set up our programming environment by installing Spark NLP. This
    will be followed by data preprocessing. You will witness the power of Spark NLP
    library’s out-of-the-box methods, which make NLP tasks significantly easier.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Then we will score the terms in our documents using the TF-IDF (term frequency-inverse
    document frequency) technique and feed the resulting output into our LDA model.
    To finish up, we’ll go through the topics assigned by our model to the input documents.
    We should be able to understand which bucket an entry belongs in without the need
    to read it. Let’s begin by going over the fundamentals of LDA.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: Latent Dirichlet Allocation
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The idea behind latent Dirichlet allocation is that documents are generated
    based on a set of topics. In this process, we assume that each document is distributed
    over the topics, and each topic is distributed over a set of terms. Each document
    and each word are generated from sampling these distributions. The LDA learner
    works backward and tries to identify the distributions where the observed is most
    probable.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: 'It attempts to distill the corpus into a set of relevant *topics*. Each topic
    captures a thread of variation in the data and often corresponds to one of the
    ideas that the corpus discusses. A document can be a part of multiple topics.
    You can think of LDA as providing a way to *soft cluster* the documents, too.
    Without delving into the mathematics, an LDA topic model describes two primary
    attributes: a chance of selecting a topic when sampling a particular document,
    and a chance of selecting a particular term when selecting a topic. For example,
    LDA might discover a topic with strong association with the terms “Asimov” and
    “robot,” and with the documents “foundation series” and “science fiction.” By
    selecting only the most important concepts, LDA can throw away some irrelevant
    noise and merge co-occurring strands to come up with a simpler representation
    of the data.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: We can employ this technique in a variety of tasks. For example, it can help
    us recommend similar Wikipedia entries when provided with an input entry. By encapsulating
    the patterns of variance in the corpus, it can base scores on a deeper understanding
    than simply on counting occurrences and co-occurrences of words. Up next, let’s
    have a look at PySpark’s LDA implementation.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: LDA in PySpark
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'PySpark MLlib offers an LDA implementation as one of its clustering algorithms.
    Here’s some example code:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[![1](assets/1.png)](#co_understanding_wikipedia___span_class__keep_together__with_lda_and_spark_nlp__span__CO1-1)'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: We apply LDA to our dataframe with the number of topics (*k*) set to 2.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_understanding_wikipedia___span_class__keep_together__with_lda_and_spark_nlp__span__CO1-2)'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: Dataframe describing the probability weight associated with each term in our
    topics.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: We will explore PySpark’s LDA implementation and associated parameters when
    applying it to the Wikipedia dataset. First, though, we need to download the relevant
    dataset. That’s what we will do next.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: Getting the Data
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Wikipedia makes dumps of all its articles available. The full dump comes in
    a single, large XML file. These can be [downloaded](https://oreil.ly/DhGlJ) and
    then placed on a cloud storage solution (such as AWS S3 or GCS, Google Cloud Storage)
    or HDFS. For example:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This can take a little while.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: Chugging through this volume of data makes the most sense with a cluster of
    a few nodes to work with. To run this chapter’s code on a local machine, a better
    option is to generate a smaller dump using [Wikipedia’s export pages](https://oreil.ly/Rrpmr).
    Try getting all the pages from multiple categories that have many pages and few
    subcategories, such as biology, health, and geometry. For the following code to
    work, download the dump into the *ch06-LDA/* directory and rename it to *wikidump.xml*.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: We need to convert the Wikipedia XML dump into a format that we can easily work
    with in PySpark. When working on our local machine, we can use the convenient
    [WikiExtractor tool](https://oreil.ly/pfwrE) for this. It extracts and cleans
    text from Wikipedia database dumps such as what we have.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: 'Install it using pip:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Then it’s as simple as running the following command in the directory containing
    the downloaded file:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The output is stored in a single or several files of similar size in a given
    directory named `text`. Each file will contains several documents in the following
    format:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[![1](assets/1.png)](#co_understanding_wikipedia___span_class__keep_together__with_lda_and_spark_nlp__span__CO2-1)'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: Rename text directory to wikidump
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s get familiar with the Spark NLP library before we start working
    on the data.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: Spark NLP
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Spark NLP library was originally designed by [John Snow Labs](https://oreil.ly/E9KVt)
    in early 2017 as an annotation library native to Spark to take full advantage
    of Spark SQL and MLlib modules. The inspiration came from trying to use Spark
    to distribute other NLP libraries, which were generally not implemented with concurrency
    or distributed computing in mind.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: Spark NLP has the same concepts as any other annotation library but differs
    in how it stores annotations. Most annotation libraries store the annotations
    in the document object, but Spark NLP creates columns for the different types
    of annotations. The annotators are implemented as transformers, estimators, and
    models. We will look at these in the next section when applying them to our dataset
    for preprocessing. Before that, let’s download and set up Spark NLP on our system.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: Setting Up Your Environment
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Install Spark NLP via pip:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Start the PySpark shell:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Let’s import Spark NLP in our PySpark shell:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Now, you can import the relevant Spark NLP modules that we’ll use:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Now that we have set up our programming environment, let’s start working on
    our dataset. We’ll start by parsing the data as a PySpark DataFrame.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: Parsing the Data
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The output from WikiExtractor can create multiple directories depending on
    the size of the input dump. We want to import all the data as a single DataFrame.
    Let’s start by specifying the input directory:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We import the data using the `wholeTextFiles` method accessible through `sparkContext`.
    This method reads the data in as an RDD. We convert it into a DataFrame since
    that’s what we want:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The resulting DataFrame will consist of two columns. The number of records
    will correspond to the number of files that were read. The first column consists
    of the filepath and the second contains the corresponding text content. The text
    contains multiple entries, but we want each row to correspond to a single entry.
    Based on the entry structure that we had seen earlier, we can separate entries
    using a couple of PySpark utilities: `split` and `explode`.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The `split` function is used to split the DataFrame string `Column` into an
    array based on matches of a provided pattern. In the previous code, we split the
    combined document XML string into an array based on the *</doc>* string. This
    effectively gives us an array of multiple documents. Then, we use `explode` to
    create new rows for each element in the array returned by the `split` function.
    This results in rows being created corresponding to each document.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: 'Go through the structure obtained in the `content` column by our previous operation:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We can further split our `content` column by extracting the entries’ titles:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Now that we have our parsed dataset, let’s move on to preprocessing using Spark
    NLP.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the Data Using Spark NLP
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We had earlier mentioned that a library based on the document-annotator model
    such as Spark NLP has the concept of “documents.” There does not exist such a
    concept natively in PySpark. Hence, one of Spark NLP’s core design tenets is strong
    interoperability with MLlib. This is done by providing DataFrame-compatible transformers
    that convert text columns into documents and convert annotations into PySpark
    data types.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by creating our `document` column using `DocumentAssembler`:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: We could have utilized Spark NLP’s [`DocumentNormalizer`](https://oreil.ly/UL1vp)
    annotator in the parsing section.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: We can transform the input dataframe directly as we have done in the previous
    code. However, we will instead use `DocumentAssembler` and other required annotators
    as part of an ML pipeline.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use the following annotators as part of our preprocessing pipeline:
    `Tokenizer`, `Normalizer`, `StopWordsCleaner`, and `Stemmer`.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start with the `Tokenizer`:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The `Tokenizer` is a fundamental annotator. Almost all text-based data processing
    begins with some form of tokenization, which is the process of breaking raw text
    into small chunks. Tokens can be words, characters, or subwords (n-grams). Most
    classical NLP algorithms expect tokens as the basic input. Many deep learning
    algorithms are being developed that take characters as basic input. Most NLP applications
    still use tokenization.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: 'Next up is the `Normalizer`:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The `Normalizer` cleans out tokens from the previous step and removes all unwanted
    characters from the text.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: 'Next up is the `StopWordsCleaner`:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: This annotator removes *stop words* from text. Stop words such as “the,” “is,”
    and “at,” which are so common that they can be removed without significantly altering
    the meaning of a text. Removing stop words is useful when one wants to deal with
    only the most semantically important words in a text and ignore words that are
    rarely semantically relevant, such as articles and prepositions.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: 'Last up is the `Stemmer`:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '`Stemmer` returns hard stems out of words with the objective of retrieving
    the meaningful part of the word. *Stemming* is the process of reducing a word
    to its root word stem with the objective of retrieving the meaningful part. For
    example, “picking,” “picked,” and “picks” all have “pick” as the root.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '`Stemmer`返回单词的硬干部，目的是检索单词的有意义部分。*词干提取*是将单词减少到其根单词词干的过程，目的是检索有意义的部分。例如，“picking”，“picked”和“picks”都有“pick”作为根。'
- en: 'We are almost done. Before we can complete our NLP pipeline, we need to add
    the `Finisher`. Spark NLP adds its own structure when we convert each row in the
    dataframe to a document. `Finisher` is critical because it helps us to bring back
    the expected structure, an array of tokens:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们快要完成了。在完成NLP流水线之前，我们需要添加`Finisher`。当我们将数据框中的每一行转换为文档时，Spark NLP会添加自己的结构。`Finisher`至关重要，因为它帮助我们恢复预期的结构，即一个标记数组：
- en: '[PRE19]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Now we have all the required pieces in place. Let’s build our pipeline so that
    each phase can be executed in sequence:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备就绪。让我们构建我们的流水线，以便每个阶段可以按顺序执行：
- en: '[PRE20]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Execute the pipeline and transform the dataframe:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 执行流水线并转换数据框：
- en: '[PRE21]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[![1](assets/1.png)](#co_understanding_wikipedia___span_class__keep_together__with_lda_and_spark_nlp__span__CO3-1)'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_understanding_wikipedia___span_class__keep_together__with_lda_and_spark_nlp__span__CO3-1)'
- en: Train the pipeline.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 训练流水线。
- en: '[![2](assets/2.png)](#co_understanding_wikipedia___span_class__keep_together__with_lda_and_spark_nlp__span__CO3-2)'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_understanding_wikipedia___span_class__keep_together__with_lda_and_spark_nlp__span__CO3-2)'
- en: Apply the pipeline to transform the dataframe.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 应用流水线来转换数据框。
- en: 'The NLP pipeline created intermediary columns that we do not need. Let’s remove
    the redundant columns:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: NLP流水线创建了中间列，我们不需要这些列。让我们移除冗余列：
- en: '[PRE22]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Next, we will understand the basics of TF-IDF and implement it on the preprocessed
    dataset, `token_df`, that we have obtained before building an LDA model.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将理解TF-IDF的基础并在预处理的数据集`token_df`上实现它，之后再建立LDA模型。
- en: TF-IDF
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TF-IDF
- en: Before applying LDA, we need to convert our data into a numeric representation.
    We will obtain such a representation using the term frequency-inverse document
    frequency method. Loosely, TF-IDF is used to determine the importance of terms
    corresponding to given documents. Here’s a representation in Python code of the
    formula. We won’t actually end up using this code because PySpark provides its
    own implementation.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在应用LDA之前，我们需要将我们的数据转换为数值表示。我们将使用词频-逆文档频率方法来获取这样的表示。大致上，TF-IDF用于确定与给定文档对应的术语的重要性。以下是Python代码中该公式的表示。实际上我们不会使用这段代码，因为PySpark提供了自己的实现。
- en: '[PRE23]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: TF-IDF captures two intuitions about the relevance of a term to a document.
    First, we would expect that the more often a term occurs in a document, the more
    important it is to that document. Second, not all terms are equal in a global
    sense. It is more meaningful to encounter a word that occurs rarely in the entire
    corpus than a word that appears in most of the documents; thus, the metric uses
    the *inverse* of the word’s appearance in documents in the full corpus.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: TF-IDF捕捉了关于术语对文档相关性的两种直觉。首先，我们期望术语在文档中出现得越频繁，它对该文档的重要性就越高。其次，全局意义上，并非所有术语都是平等的。在整个语料库中遇到很少出现的单词比在大多数文档中出现的单词更有意义；因此，该度量使用了单词在全语料库中出现的*逆数*。
- en: The frequency of words in a corpus tends to be distributed exponentially. A
    common word might appear ten times as often as a mildly common word, which in
    turn might appear ten or a hundred times as often as a rare word. Basing a metric
    on the raw inverse document frequency would give rare words enormous weight and
    practically ignore the impact of all other words. To capture this distribution,
    the scheme uses the *log* of the inverse document frequency. This mellows the
    differences in document frequencies by transforming the multiplicative gaps between
    them into additive gaps.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 语料库中单词的频率通常呈指数分布。一个常见单词的出现次数可能是一个较不常见单词的十倍，而后者又可能比一个稀有单词出现的十到一百倍更常见。如果基于原始逆文档频率来衡量指标，稀有单词将会占据主导地位，而几乎忽略其他所有单词的影响。为了捕捉这种分布，该方案使用了逆文档频率的*对数*。这种方式通过将它们之间的乘法差距转换为加法差距，从而减轻了文档频率之间的差异。
- en: The model relies on a few assumptions. It treats each document as a “bag of
    words,” meaning that it pays no attention to the ordering of words, sentence structure,
    or negations. By representing each term once, the model has difficulty dealing
    with *polysemy*, the use of the same word for multiple meanings. For example,
    the model can’t distinguish between the use of “band” in “Radiohead is the best
    band ever” and “I broke a rubber band.” If both sentences appear often in the
    corpus, it may come to associate “Radiohead” with “rubber.”
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型基于几个假设。它将每个文档视为“词袋”，即不关注词语的顺序、句子结构或否定形式。通过一次性地表示每个术语，模型在处理多义性（同一词语具有多种含义）时遇到困难。例如，模型无法区分“Radiohead
    is the best band ever” 和 “I broke a rubber band” 中“band” 的用法。如果这两个句子在语料库中频繁出现，模型可能将“Radiohead”
    关联到 “rubber”。
- en: Let’s proceed now to the implementation of TF-IDF using PySpark.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们使用 PySpark 实现 TF-IDF。
- en: Computing the TF-IDFs
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算 TF-IDF
- en: 'First, we’ll calculate TF (term frequency; that is, the frequency of each term
    in a document) with `CountVectorizer`, which keeps track of the vocabulary that’s
    being created so we can map our topics back to their corresponding words. TF creates
    a matrix that counts how many times each word in the vocabulary appears in each
    body of text. This then gives each word a weight based on its frequency. We derive
    the vocabulary of our data while fitting and get the counts at the transform step:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将使用 `CountVectorizer` 计算 TF（词项频率；即文档中每个词项的频率），该过程保留了正在创建的词汇表，以便我们可以将主题映射回相应的单词。TF
    创建一个矩阵，计算词汇表中每个词在每个文本主体中出现的次数。然后，根据其频率给每个词赋予权重。在拟合过程中获得我们数据的词汇表，并在转换步骤中获取计数：
- en: '[PRE24]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Then, we proceed with IDF (the inverse frequency of documents where a term
    occurred), which reduces the weights of commonly appearing terms:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们进行 IDF 计算（文档中术语出现的逆频率），以减少常见术语的权重：
- en: '[PRE25]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'This is what the result will look like:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如下所示：
- en: '[PRE26]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: With all the preprocessing and feature engineering done, we can now create our
    LDA model. That’s what we’ll do in the next section.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 经过所有的预处理和特征工程处理，我们现在可以创建我们的 LDA 模型。这是我们将在下一节中完成的工作。
- en: Creating Our LDA Model
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建我们的 LDA 模型
- en: We had mentioned previously that LDA distills a corpus into a set of relevant
    topics. We will get to have a look at examples of such topics further ahead in
    this section. Before that, we need to decide on two hyperparameters that our LDA
    model requires. They are number of topics (referred to as *k*) and number of iterations.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前提到过，LDA 将语料库提炼为一组相关主题。在本节的后面部分，我们将看到这些主题的示例。在此之前，我们需要确定我们的 LDA 模型需要的两个超参数。它们是主题数（称为
    *k*）和迭代次数。
- en: There are multiple ways that you can go about choosing *k*. Two popular metrics
    used for doing this are perplexity and topic coherence. The former is made available
    by PySpark’s implementation. The basic idea is to try to figure out the *k* where
    the improvements to these metrics start to become insignificant. If you are familiar
    with the *elbow method* for finding the number of clusters for K-means, this is
    similar. Depending on the size of your corpus, it can be a resource-intensive
    and time-consuming process since you will need to build the model for multiple
    values of *k*. An alternative could be to try to create a representative sample
    of the dataset in hand and use it to determine *k*. It is left as an exercise
    for you to read up on this and try it.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 选择 *k* 的多种方法。两种流行的用于此目的的度量是困惑度和主题一致性。前者可以通过 PySpark 实现获得。基本思想是尝试找出在这些指标改善开始变得不显著的
    *k* 值。如果你熟悉 K-means 中用于找到聚类数的“拐点方法”，那么这类似。根据语料库的大小，这可能是一个资源密集型和耗时的过程，因为您需要为多个
    *k* 值构建模型。另一种方法可能是尝试创建数据集的代表性样本，并使用它来确定 *k*。这留给你作为一个练习去阅读和尝试。
- en: Since you may be working locally right now, we will go ahead and assign reasonable
    values (*k* as 5 and `max_iter` as 50) for now.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 由于您可能正在本地工作，我们现在将为其分配合理的值（*k* 设置为 5，`max_iter` 设置为 50）。
- en: 'Let’s create our LDA model:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建我们的 LDA 模型：
- en: '[PRE27]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Perplexity is a measurement of how well a model predicts a sample. A low perplexity
    indicates the probability distribution is good at predicting the sample. When
    comparing different models, go for the one with the lower value of perplexity.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 困惑度是衡量模型预测样本能力的度量。低困惑度表明概率分布在预测样本时表现良好。在比较不同模型时，选择困惑度值较低的那个模型。
- en: Now that we have created our model, we want to output the topics as human-readable.
    We will get the vocabulary generated from our preprocessing steps, get the topics
    from the LDA model, and map both of them.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经创建了我们的模型，我们希望将主题输出为人类可读的形式。我们将获取从我们的预处理步骤生成的词汇表，从LDA模型获取主题，并将它们映射起来。
- en: '[PRE28]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[![1](assets/1.png)](#co_understanding_wikipedia___span_class__keep_together__with_lda_and_spark_nlp__span__CO4-1)'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_understanding_wikipedia___span_class__keep_together__with_lda_and_spark_nlp__span__CO4-1)'
- en: Create a reference to our vocabulary.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个对我们的词汇表的引用。
- en: '[![2](assets/2.png)](#co_understanding_wikipedia___span_class__keep_together__with_lda_and_spark_nlp__span__CO4-2)'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_understanding_wikipedia___span_class__keep_together__with_lda_and_spark_nlp__span__CO4-2)'
- en: Get topics generated by the LDA model using `describeTopics` and load them into
    a Python list.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`describeTopics`方法从LDA模型获取生成的主题，并将它们加载到Python列表中。
- en: '[![3](assets/3.png)](#co_understanding_wikipedia___span_class__keep_together__with_lda_and_spark_nlp__span__CO4-3)'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_understanding_wikipedia___span_class__keep_together__with_lda_and_spark_nlp__span__CO4-3)'
- en: Get indices of the vocabulary terms from our topics.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 获取来自我们主题的词汇术语的索引。
- en: 'Let us now generate the mappings from our topic indices to our vocabulary:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们生成从主题索引到词汇表的映射：
- en: '[PRE29]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The previous result is not perfect, but there are some patterns that can be
    noticed in the topics. Topic 1 is primarily related to health. It also contains
    references to Islam and empire. Could it be because of them being referenced in
    medicinal history and vice versa or something else? Topics 2 and 3 are related
    to mathematics with the latter inclined toward geometry. Topic 5 is a mix of computing
    and mathematics. Even if you hadn’t read any of the documents, you can already
    guess with a reasonable accuracy about their categories. This is exciting!
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 上述结果并不完美，但在主题中可以注意到一些模式。主题1主要与健康相关。它还包含对伊斯兰教和帝国的引用。这可能是因为它们在医学史上被引用，或者其他原因？主题2和3与数学有关，后者倾向于几何学。主题5是计算机和数学的混合。即使您没有阅读任何文档，您也可以合理准确地猜测它们的类别。这很令人兴奋！
- en: We can now also check which topics our input documents are most closely related
    to. A single document can have multiple topic associations that are significant.
    For now, we’ll only look at the most strongly associated topics.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在还可以检查我们的输入文档与哪些主题最相关。单个文档可以有多个显著的主题关联。现在，我们只会查看最强关联的主题。
- en: 'Let’s run the LDA model’s transform operation on our input dataframe:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在输入数据框上运行LDA模型的转换操作：
- en: '[PRE30]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: As you can see, each document has a topic probability distribution associated
    with it. To get the associated topic for each document, we want to find out the
    topic index with the highest probability score. We can then map it to the topics
    that we obtained previously.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您可以看到的那样，每个文档都有与其关联的主题概率分布。为了获取每个文档的相关主题，我们想要找出具有最高概率分数的主题索引。然后我们可以将其映射到之前获取的主题。
- en: 'We will write a PySpark UDF to find the highest topic probability score for
    each record:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将编写一个PySpark UDF来找到每条记录的最高主题概率分数：
- en: '[PRE31]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Topic 2, if you remember, was associated with mathematics. The output is in
    line with our expectations. You can scan more of the dataset to see how it performed.
    You can select particular topics using the `where` or `filter` commands and compare
    them against the topic list generated earlier to get a better sense of the clusters
    that have been created. As promised at the beginning of the chapter, we’re able
    to cluster articles into different topics without reading them!
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你还记得，主题2与数学相关联。输出符合我们的期望。您可以扫描数据集的更多部分，以查看其执行情况。您可以使用`where`或`filter`命令选择特定主题，并与之前生成的主题列表进行比较，以更好地了解已创建的聚类。正如本章开头所承诺的，我们能够在不阅读文章的情况下将其聚类到不同的主题中！
- en: Where to Go from Here
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 接下来的步骤
- en: In this chapter, we performed LDA on the Wikipedia corpus. In the process, we
    also learned about text preprocessing using the amazing Spark NLP library and
    the TF-IDF technique. You can further build on this by improving the model by
    better preprocessing and hyperparameter tuning. In addition, you can even try
    to recommend similar entries based on document similarity when provided with user
    input. Such a similarity measure may be obtained by using the probability distribution
    vector obtained from LDA.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们对维基百科语料库进行了LDA分析。在此过程中，我们还学习了使用令人惊叹的Spark NLP库和TF-IDF技术进行文本预处理。您可以通过改进模型的预处理和超参数调整进一步完善它。此外，甚至可以尝试基于文档相似性推荐类似条目，当用户提供输入时。这样的相似度度量可以通过从LDA获得的概率分布向量来获得。
- en: In addition, a variety of other methods exist for understanding large corpora
    of text. For example, a technique known as latent semantic analysis (LSA) is useful
    in similar applications and was used in the previous edition of this book on the
    same dataset. Deep learning, which is explored in [Chapter 10](ch10.xhtml#image_similarity_detection_with_deep_learning_and_pyspark_lsh),
    also offers avenues to perform topic modeling. You can explore them on your own.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还存在多种理解大型文本语料库的方法。例如，一种称为潜在语义分析（LSA）的技术在类似的应用中很有用，并且在本书的前一版中使用了相同的数据集。深度学习，这在[第10章](ch10.xhtml#image_similarity_detection_with_deep_learning_and_pyspark_lsh)中有探讨，也提供了进行主题建模的途径。您可以自行探索这些方法。
