- en: Chapter 23\. Recommender Systems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: O nature, nature, why art thou so dishonest, as ever to send men with these
    false recommendations into the world!
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Henry Fielding
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Another common data problem is producing *recommendations* of some sort. Netflix
    recommends movies you might want to watch. Amazon recommends products you might
    want to buy. Twitter recommends users you might want to follow. In this chapter,
    we’ll look at several ways to use data to make recommendations.
  prefs: []
  type: TYPE_NORMAL
- en: 'In particular, we’ll look at the dataset of `users_interests` that we’ve used
    before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: And we’ll think about the problem of recommending new interests to a user based
    on her currently specified interests.
  prefs: []
  type: TYPE_NORMAL
- en: Manual Curation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before the internet, when you needed book recommendations you would go to the
    library, where a librarian was available to suggest books that were relevant to
    your interests or similar to books you liked.
  prefs: []
  type: TYPE_NORMAL
- en: Given DataSciencester’s limited number of users and interests, it would be easy
    for you to spend an afternoon manually recommending interests for each user. But
    this method doesn’t scale particularly well, and it’s limited by your personal
    knowledge and imagination. (Not that I’m suggesting that your personal knowledge
    and imagination are limited.) So let’s think about what we can do with *data*.
  prefs: []
  type: TYPE_NORMAL
- en: Recommending What’s Popular
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One easy approach is to simply recommend what’s popular:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'which looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Having computed this, we can just suggest to a user the most popular interests
    that he’s not already interested in:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'So, if you are user 1, with interests:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'then we’d recommend you:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'If you are user 3, who’s already interested in many of those things, you’d
    instead get:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Of course, “lots of people are interested in Python, so maybe you should be
    too” is not the most compelling sales pitch. If someone is brand new to our site
    and we don’t know anything about them, that’s possibly the best we can do. Let’s
    see how we can do better by basing each user’s recommendations on her existing
    interests.
  prefs: []
  type: TYPE_NORMAL
- en: User-Based Collaborative Filtering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One way of taking a user’s interests into account is to look for users who are
    somehow *similar* to her, and then suggest the things that those users are interested
    in.
  prefs: []
  type: TYPE_NORMAL
- en: In order to do that, we’ll need a way to measure how similar two users are.
    Here we’ll use cosine similarity, which we used in [Chapter 21](ch21.html#natural_language_processing)
    to measure how similar two word vectors were.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll apply this to vectors of 0s and 1s, each vector `v` representing one user’s
    interests. `v[i]` will be 1 if the user specified the *i*th interest, and 0 otherwise.
    Accordingly, “similar users” will mean “users whose interest vectors most nearly
    point in the same direction.” Users with identical interests will have similarity
    1\. Users with no identical interests will have similarity 0\. Otherwise, the
    similarity will fall in between, with numbers closer to 1 indicating “very similar”
    and numbers closer to 0 indicating “not very similar.”
  prefs: []
  type: TYPE_NORMAL
- en: 'A good place to start is collecting the known interests and (implicitly) assigning
    indices to them. We can do this by using a set comprehension to find the unique
    interests, and then sorting them into a list. The first interest in the resulting
    list will be interest 0, and so on:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives us a list that starts:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Next we want to produce an “interest” vector of 0s and 1s for each user. We
    just need to iterate over the `unique_interests` list, substituting a 1 if the
    user has each interest, and a 0 if not:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'And now we can make a list of user interest vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Now `user_interest_vectors[i][j]` equals 1 if user `i` specified interest `j`,
    and 0 otherwise.
  prefs: []
  type: TYPE_NORMAL
- en: 'Because we have a small dataset, it’s no problem to compute the pairwise similarities
    between all of our users:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'after which `user_similarities[i][j]` gives us the similarity between users
    `i` and `j`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'In particular, `user_similarities[i]` is the vector of user `i`’s similarities
    to every other user. We can use this to write a function that finds the most similar
    users to a given user. We’ll make sure not to include the user herself, nor any
    users with zero similarity. And we’ll sort the results from most similar to least
    similar:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'For instance, if we call `most_similar_users_to(0)` we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'How do we use this to suggest new interests to a user? For each interest, we
    can just add up the user similarities of the other users interested in it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'If we call `user_based_suggestions(0)`, the first several suggested interests
    are:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: These seem like pretty decent suggestions for someone whose stated interests
    are “Big Data” and database-related. (The weights aren’t intrinsically meaningful;
    we just use them for ordering.)
  prefs: []
  type: TYPE_NORMAL
- en: This approach doesn’t work as well when the number of items gets very large.
    Recall the curse of dimensionality from [Chapter 12](ch12.html#nearest_neighbors)—in
    large-dimensional vector spaces most vectors are very far apart (and also point
    in very different directions). That is, when there are a large number of interests
    the “most similar users” to a given user might not be similar at all.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine a site like Amazon.com, from which I’ve bought thousands of items over
    the last couple of decades. You could attempt to identify similar users to me
    based on buying patterns, but most likely in all the world there’s no one whose
    purchase history looks even remotely like mine. Whoever my “most similar” shopper
    is, he’s probably not similar to me at all, and his purchases would almost certainly
    make for lousy recommendations.
  prefs: []
  type: TYPE_NORMAL
- en: Item-Based Collaborative Filtering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An alternative approach is to compute similarities between interests directly.
    We can then generate suggestions for each user by aggregating interests that are
    similar to her current interests.
  prefs: []
  type: TYPE_NORMAL
- en: 'To start with, we’ll want to *transpose* our user-interest matrix so that rows
    correspond to interests and columns correspond to users:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: What does this look like? Row `j` of `interest_user_matrix` is column `j` of
    `user_interest_matrix`. That is, it has 1 for each user with that interest and
    0 for each user without that interest.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, `unique_interests[0]` is Big Data, and so `interest_user_matrix[0]`
    is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: because users 0, 8, and 9 indicated interest in Big Data.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now use cosine similarity again. If precisely the same users are interested
    in two topics, their similarity will be 1\. If no two users are interested in
    both topics, their similarity will be 0:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'For example, we can find the interests most similar to Big Data (interest 0)
    using:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'which suggests the following similar interests:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can create recommendations for a user by summing up the similarities
    of the interests similar to his:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'For user 0, this generates the following (seemingly reasonable) recommendations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Matrix Factorization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we’ve seen, we can represent our users’ preferences as a `[num_users, num_items]`
    matrix of 0s and 1s, where the 1s represent liked items and the 0s unliked items.
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes you might actually have numeric *ratings*; for example, when you write
    an Amazon review you assign the item a score ranging from 1 to 5 stars. You could
    still represent these by numbers in a `[num_users, num_items]` matrix (ignoring
    for now the problem of what to do about unrated items).
  prefs: []
  type: TYPE_NORMAL
- en: In this section we’ll assume we have such ratings data and try to learn a model
    that can predict the rating for a given user and item.
  prefs: []
  type: TYPE_NORMAL
- en: One way of approaching the problem is to assume that every user has some latent
    “type,” which can be represented as a vector of numbers, and that each item similarly
    has some latent “type.”
  prefs: []
  type: TYPE_NORMAL
- en: If the user types are represented as a `[num_users, dim]` matrix, and the transpose
    of the item types is represented as a `[dim, num_items]` matrix, their product
    is a `[num_users, num_items]` matrix. Accordingly, one way of building such a
    model is by “factoring” the preferences matrix into the product of a user matrix
    and an item matrix.
  prefs: []
  type: TYPE_NORMAL
- en: (Possibly this idea of latent types reminds you of the word embeddings we developed
    in [Chapter 21](ch21.html#natural_language_processing). Hold on to that idea.)
  prefs: []
  type: TYPE_NORMAL
- en: Rather than working with our made-up 10-user dataset, we’ll work with the MovieLens
    100k dataset, which contains ratings from 0 to 5 for many movies from many users.
    Each user has only rated a small subset of the movies. We’ll use this to try to
    build a system that can predict the rating for any given (user, movie) pair. We’ll
    train it to predict well on the movies each user has rated; hopefully then it
    will generalize to movies the user hasn’t rated.
  prefs: []
  type: TYPE_NORMAL
- en: To start with, let’s acquire the dataset. You can download it from [*http://files.grouplens.org/datasets/movielens/ml-100k.zip*](http://files.grouplens.org/datasets/movielens/ml-100k.zip).
  prefs: []
  type: TYPE_NORMAL
- en: 'Unzip it and extract the files; we’ll only use two of them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'As is often the case, we’ll introduce a `NamedTuple` to make things easier
    to work with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The movie ID and user IDs are actually integers, but they’re not consecutive,
    which means if we worked with them as integers we’d end up with a lot of wasted
    dimensions (unless we renumbered everything). So to keep it simpler we’ll just
    treat them as strings.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let’s read in the data and explore it. The movies file is pipe-delimited
    and has many columns. We only care about the first two, which are the ID and the
    title:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The ratings file is tab-delimited and contains four columns for `user_id`,
    `movie_id`, `rating` (1 to 5), and `timestamp`. We’ll ignore the timestamp, as
    we don’t need it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'There’s a lot of interesting exploratory analysis you can do on this data;
    for instance, you might be interested in the average ratings for *Star Wars* movies
    (the dataset is from 1998, which means it predates *The Phantom Menace* by a year):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'They’re all pretty highly rated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'So let’s try to come up with a model to predict these ratings. As a first step,
    let’s split the ratings data into train, validation, and test sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'It’s always good to have a simple baseline model and make sure that ours does
    better than that. Here a simple baseline model might be “predict the average rating.”
    We’ll be using mean squared error as our metric, so let’s see how the baseline
    does on our test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Given our embeddings, the predicted ratings are given by the matrix product
    of the user embeddings and the movie embeddings. For a given user and movie, that
    value is just the dot product of the corresponding embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: 'So let’s start by creating the embeddings. We’ll represent them as `dict`s
    where the keys are IDs and the values are vectors, which will allow us to easily
    retrieve the embedding for a given ID:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'By now we should be pretty expert at writing training loops:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'And now we can train our model (that is, find the optimal embeddings). For
    me it worked best if I decreased the learning rate a little each epoch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: This model is pretty apt to overfit the training set. I got the best results
    with `EMBEDDING_DIM=2`, which got me an average loss on the test set of about
    0.89.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If you wanted higher-dimensional embeddings, you could try regularization like
    we used in [“Regularization”](ch15.html#regularization). In particular, at each
    gradient update you could shrink the weights toward 0. I was not able to get any
    better results that way.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, inspect the learned vectors. There’s no reason to expect the two components
    to be particularly meaningful, so we’ll use principal component analysis:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s transform our vectors to represent the principal components and join
    in the movie IDs and average ratings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: The top 25 are all highly rated, while the bottom 25 are mostly low-rated (or
    unrated in the training data), which suggests that the first principal component
    is mostly capturing “how good is this movie?”
  prefs: []
  type: TYPE_NORMAL
- en: It’s hard for me to make much sense of the second component; and, indeed the
    two-dimensional embeddings performed only slightly better than the one-dimensional
    embeddings, suggesting that whatever the second component captured is possibly
    very subtle. (Presumably one of the larger MovieLens datasets would have more
    interesting things going on.)
  prefs: []
  type: TYPE_NORMAL
- en: For Further Exploration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Surprise](http://surpriselib.com/) is a Python library for “building and analyzing
    recommender systems” that seems reasonably popular and up-to-date.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The [Netflix Prize](http://www.netflixprize.com) was a somewhat famous competition
    to build a better system to recommend movies to Netflix users.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
