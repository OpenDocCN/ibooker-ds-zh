- en: Chapter 9\. Wrangling Dataframes
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 9 章。整理数据框架
- en: 'We often need to perform preparatory work on our data before we can begin our
    analysis. The amount of preparation can vary widely, but there are a few basic
    steps to move from raw data to data ready for analysis. [Chapter 8](ch08.html#ch-files)
    addressed the initial steps of creating a dataframe from a plain-text source.
    In this chapter, we assess quality. To do this, we perform validity checks on
    individual data values and entire columns. In addition to checking the quality
    of the data, we determine whether or not the data need to be transformed and reshaped
    to get ready for analysis. Quality checking (and fixing) and transformation are
    often cyclical: the quality checks point us toward transformations we need to
    make, and when we check the transformed columns to confirm that our data are ready
    for analysis, we may discover they need further cleaning.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常需要在分析之前对数据进行准备工作。准备工作的量可能差异很大，但从原始数据到准备好进行分析的数据，有几个基本步骤。[第 8 章](ch08.html#ch-files)
    讨论了从纯文本源创建数据框架的初始步骤。在本章中，我们评估数据的质量。为此，我们对单个数据值和整个列执行有效性检查。除了检查数据的质量外，我们还确定数据是否需要转换和重塑以准备进行分析。质量检查（和修复）以及转换通常是循环的：质量检查指导我们进行必要的转换，当我们检查转换后的列以确认数据准备好进行分析时，我们可能会发现它们需要进一步清理。
- en: 'Depending on the data source, we often have different expectations for quality.
    Some datasets require extensive wrangling to get them into an analyzable form,
    and others arrive clean and we can quickly launch into modeling. Here are some
    examples of data sources and how much wrangling we might expect to do:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 根据数据源的不同，我们对质量有不同的期望。一些数据集可能需要大量的整理工作才能使其达到可分析的形式，而其他数据可能已经很干净，我们可以直接进行建模。以下是一些数据源的示例以及我们可能预期进行的整理工作量：
- en: Data from a scientific experiment or study are typically clean, are well documented,
    and have a simple structure. These data are organized to be broadly shared so
    that others can build on or reproduce the findings. They are typically ready for
    analysis after little to no wrangling.
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自科学实验或研究的数据通常是干净的，有良好的文档记录，并且具有简单的结构。这些数据被组织成可以广泛分享的形式，以便其他人可以在其基础上建立或重现发现。通常情况下，经过少量或无需整理后，即可进行分析。
- en: Data from government surveys often come with very detailed codebooks and metadata
    describing how the data are collected and formatted, and these datasets are also
    typically ready for exploration and analysis right out of the box.
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 政府调查数据通常附有非常详细的代码书和描述数据收集及格式化方式的元数据，这些数据集通常也是即开即用的，可以直接进行探索和分析。
- en: Administrative data can be clean, but without inside knowledge of the source,
    we may need to extensively check their quality. Also, since we often use these
    data for a purpose other than why they were collected in the first place, we may
    need to transform features or combine data tables.
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 行政数据可能是干净的，但如果没有关于数据源的内部知识，我们可能需要广泛检查它们的质量。此外，由于我们经常将这些数据用于与最初收集它们的目的不同的用途，我们可能需要转换特征或合并数据表。
- en: Informally collected data, such as data scraped from the web, can be quite messy
    and tends to come with little documentation. For example, texts, tweets, blogs,
    and Wikipedia tables usually require formatting and cleaning to transform them
    into information ready for analysis.
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从网页抓取等非正式收集的数据通常会非常混乱，并且往往缺乏文档记录。例如，文本、推特、博客和维基百科表格通常需要格式化和清理，才能将它们转化为可以分析的信息。
- en: 'In this chapter, we break down data wrangling into the following stages: assess
    data quality, handle missing values, transform features, and reshape the data
    by modifying its structure and granularity. An important step in assessing the
    quality of the data is to consider its scope. Data scope was covered in [Chapter 2](ch02.html#ch-data-scope),
    and we refer you there for a fuller treatment of the topic.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将数据整理分解为以下几个阶段：评估数据质量，处理缺失值，转换特征，并通过修改其结构和粒度来重塑数据。评估数据质量的重要步骤是考虑其范围。数据范围已在[第
    2 章](ch02.html#ch-data-scope)中介绍，我们建议您参考该章节以获取更详细的内容。
- en: To clean and prepare data, we also rely on exploratory data analysis, especially
    visualizations. In this chapter, however, we focus on data wrangling and cover
    these other, related topics in more detail in Chapters [10](ch10.html#ch-eda)
    and [11](ch11.html#ch-viz).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 要清理和准备数据，我们还依赖探索性数据分析，尤其是可视化。然而，在本章中，我们专注于数据整理，并将更详细地讨论这些其他相关主题，这些内容在第 [10](ch10.html#ch-eda)
    和第 [11](ch11.html#ch-viz) 章节中。
- en: 'We use the datasets introduced in [Chapter 8](ch08.html#ch-files): the DAWN
    government survey of emergency room visits related to drug abuse, and the San
    Francisco administrative data on food safety inspections of restaurants. But we
    begin by introducing the various data wrangling concepts through another example
    that is simple enough and clean enough that we can limit our focus in each of
    the wrangling steps.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用在[第八章](ch08.html#ch-files)中介绍的数据集：DAWN政府调查与药物滥用有关的急诊室访问情况，以及旧金山餐馆食品安全检查的行政数据。但我们首先通过另一个足够简单且干净的例子介绍各种数据整理概念，以便我们可以在每个整理步骤中集中精力。
- en: 'Example: Wrangling CO[2] Measurements from the Mauna Loa Observatory'
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 示例：从毛纳罗亚观测站收集二氧化碳（CO[2]）测量数据
- en: 'We saw in [Chapter 2](ch02.html#ch-data-scope) that the [National Oceanic and
    Atmospheric Administration (NOAA)](https://www.noaa.gov) monitors CO[2] concentrations
    in the air at the [Mauna Loa Observatory](https://oreil.ly/7HsQh). We continue
    with this example and use it to introduce how to make data-quality checks, handle
    missing values, transform features, and reshape tables. These data are in the
    file *data/co2_mm_mlo.txt*. Let’s begin by figuring out the formatting, encoding,
    and size of the source before we load it into a dataframe (see [Chapter 8](ch08.html#ch-files)):'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第二章](ch02.html#ch-data-scope)中看到，[国家海洋和大气管理局（NOAA）](https://www.noaa.gov)监测毛纳罗亚观测站空气中的CO[2]浓度。我们继续以此为例，介绍如何进行数据质量检查、处理缺失值、转换特征和重塑表格。这些数据位于文件*data/co2_mm_mlo.txt*中。在将其加载到数据框之前，让我们先了解源数据的格式、编码和大小（见[第八章](ch08.html#ch-files)）：
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We have found that the file is plain text with ASCII encoding and about 50
    KiB in size. Since the file is not particularly large, we should have no trouble
    loading it into a dataframe, but first we need to determine the file’s format.
    Let’s look at the first few lines in the file:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现文件是纯文本，使用ASCII编码，大小约为50 KiB。由于文件并不特别大，因此我们应该可以轻松地将其加载到数据框中，但首先需要确定文件的格式。让我们先看一下文件的前几行：
- en: '[PRE2]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We see that the file begins with information about the data source. We should
    read this documentation before starting our analysis, but sometimes the urge to
    plunge into the analysis wins over and we just start mucking about and discover
    properties of the data as we go. So let’s quickly find where the actual data values
    are located:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到文件以数据源信息开头。在开始分析之前，我们应该先阅读这些文档，但有时沉浸于分析中的冲动会胜过一切，我们会开始随意地发现数据的各种属性。所以让我们快速找出实际数据值的位置：
- en: '[PRE6]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We have found that the data begins on the 73rd line of the file. We also spot
    some relevant characteristics:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现数据从文件的第73行开始。我们还发现了一些相关特征：
- en: The values are separated by whitespace, possibly tabs.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 值由空白分隔，可能是制表符。
- en: The data line up in precise columns. For example, the month appears in the seventh
    to eighth position of each line.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据以精确的列对齐。例如，每行的第七到第八位置出现了月份。
- en: The column headings are split over two lines.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 列标题分为两行。
- en: 'We can use `read_csv` to read the data into a `pandas` `DataFrame` and provide
    arguments to specify that the separators are whitespace, there is no header (we
    will set our own column names), and to skip the first 72 rows of the file:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`read_csv`将数据读入`pandas`的`DataFrame`中，并提供参数指定分隔符是空白、没有表头（我们将设置自己的列名），并跳过文件的前72行：
- en: '[PRE8]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '|   | Yr | Mo | DecDate | Avg | Int | Trend | days |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '|   | 年 | 月 | 日期 | 平均 | 股息 | 趋势 | 天数 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| **0** | 1958 | 3 | 1958.21 | 315.71 | 315.71 | 314.62 | -1 |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| **0** | 1958 | 3 | 1958.21 | 315.71 | 315.71 | 314.62 | -1 |'
- en: '| **1** | 1958 | 4 | 1958.29 | 317.45 | 317.45 | 315.29 | -1 |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| **1** | 1958 | 4 | 1958.29 | 317.45 | 317.45 | 315.29 | -1 |'
- en: '| **2** | 1958 | 5 | 1958.38 | 317.50 | 317.50 | 314.71 | -1 |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| **2** | 1958 | 5 | 1958.38 | 317.50 | 317.50 | 314.71 | -1 |'
- en: We have successfully loaded the file contents into a dataframe, and we can see
    that the granularity of the data is a monthly average CO[2], from 1958 through
    2019\. Also, the table shape is 738 by 7.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已成功将文件内容加载到数据框中，可以看出数据的粒度是1958年至2019年的月均CO[2]浓度。此外，表格形状为738行7列。
- en: 'Since scientific studies tend to have very clean data, it’s tempting to jump
    right in and make a plot to see how CO[2] monthly averages have changed. The field
    `DecDate` conveniently represents the month and year as a numeric feature, so
    we can easily make a line plot:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 由于科学研究往往具有非常干净的数据，我们很容易就跳进去绘制一张CO[2]月均值如何变化的图表。字段`DecDate`方便地将月份和年份表示为数值特征，因此我们可以轻松地制作一张折线图：
- en: '[PRE9]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![](assets/leds_09in01.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/leds_09in01.png)'
- en: 'Yikes! Plotting the data has uncovered a problem. The four dips in the line
    plot look odd. What happened here? We can check a few percentiles of the dataframe
    to see if we can spot the problem:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 哎呀！绘制数据后发现了一个问题。折线图中的四个低谷看起来很奇怪。这里发生了什么？我们可以检查数据帧的一些百分位数，看看是否能找出问题：
- en: '[PRE10]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '|   | Yr | Mo | DecDate | Avg | Int | Trend | days |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '|   | Yr | Mo | DecDate | Avg | Int | Trend | days |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| **min** | 1958.0 | 1.0 | 1958.21 | -99.99 | 312.66 | 314.62 | -1.0 |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| **min** | 1958.0 | 1.0 | 1958.21 | -99.99 | 312.66 | 314.62 | -1.0 |'
- en: '| **25%** | 1973.0 | 4.0 | 1973.56 | 328.59 | 328.79 | 329.73 | -1.0 |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| **25%** | 1973.0 | 4.0 | 1973.56 | 328.59 | 328.79 | 329.73 | -1.0 |'
- en: '| **50%** | 1988.0 | 6.0 | 1988.92 | 351.73 | 351.73 | 352.38 | 25.0 |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| **50%** | 1988.0 | 6.0 | 1988.92 | 351.73 | 351.73 | 352.38 | 25.0 |'
- en: '| **75%** | 2004.0 | 9.0 | 2004.27 | 377.00 | 377.00 | 377.18 | 28.0 |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| **75%** | 2004.0 | 9.0 | 2004.27 | 377.00 | 377.00 | 377.18 | 28.0 |'
- en: '| **max** | 2019.0 | 12.0 | 2019.62 | 414.66 | 414.66 | 411.84 | 31.0 |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| **max** | 2019.0 | 12.0 | 2019.62 | 414.66 | 414.66 | 411.84 | 31.0 |'
- en: This time, looking a bit more closely at the range of values, we see that some
    data have unusual values like `-1` and `-99.99`. If we read the information at
    the top of the file more carefully, we find that `-99.99` denotes a missing monthly
    average and `-1` signifies a missing value for the number of days the equipment
    was in operation that month. Even with relatively clean data, it’s a good practice
    to read the documentation and make a few quality checks before jumping into the
    analysis stage.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这一次，我们更仔细地查看数值范围，发现一些数据有异常值，如`-1`和`-99.99`。如果我们仔细阅读文件顶部的信息，我们会发现`-99.99`表示缺失的月平均值，`-1`表示设备当月运行天数的缺失值。即使数据相对干净，也应该在进入分析阶段前阅读文档并进行一些质量检查是个好习惯。
- en: Quality Checks
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 质量检查
- en: Let’s step back for a moment and perform some quality checks. We might confirm
    that we have the expected number of observations, look for unusual values, and
    cross-check anomalies that we find against the values in other features.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们暂停一会儿，进行一些质量检查。我们可以确认我们拥有预期的观测数量，寻找异常值，并将发现的异常与其他特征的值进行交叉验证。
- en: 'First, we consider the shape of the data. How many rows should we have? From
    looking at the head and tail of the dataframe, the data appear to be in chronological
    order, beginning with March 1958 and ending with August 2019\. This means we should
    have <math><mn>12</mn> <mo>×</mo> <mo stretchy="false">(</mo> <mn>2019</mn> <mo>−</mo>
    <mn>1957</mn> <mo stretchy="false">)</mo> <mo>−</mo> <mn>2</mn> <mo>−</mo> <mn>4</mn>
    <mo>=</mo> <mn>738</mn></math> records, which we can check against the shape of
    the dataframe:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们考虑数据的形状。我们应该有多少行数据？从数据框的头部和尾部看，数据按时间顺序排列，从1958年3月开始，到2019年8月结束。这意味着我们应该有
    <math><mn>12</mn> <mo>×</mo> <mo stretchy="false">(</mo> <mn>2019</mn> <mo>−</mo>
    <mn>1957</mn> <mo stretchy="false">)</mo> <mo>−</mo> <mn>2</mn> <mo>−</mo> <mn>4</mn>
    <mo>=</mo> <mn>738</mn></math> 条记录，我们可以与数据框的形状进行对比：
- en: '[PRE11]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Our calculations match the number of rows in the data table.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的计算与数据表中的行数匹配。
- en: 'Next, let’s check the quality of the features, starting with `Mo`. We expect
    the values to range from 1 to 12, and each month should have 2019 – 1957 = 62
    or 61 instances (since the recordings begin in March of the first year and end
    in August of the most recent year):'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们检查特征的质量，从`Mo`开始。我们期望值在1到12之间，每个月应有2019-1957=62或61个实例（因为记录从第一年的三月开始，到最近一年的八月结束）：
- en: '[PRE13]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: As expected, Jan, Feb, Sep, Oct, Nov, and Dec have 61 occurrences and the rest
    62.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期，一月、二月、九月、十月、十一月和十二月各有61次出现，其余62次。
- en: 'Now let’s examine the column called `days` with a histogram:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们用直方图检查名为`days`的列：
- en: '[PRE15]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '![](assets/leds_09in02.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/leds_09in02.png)'
- en: 'We see that a handful of months have averages based on measurements taken on
    fewer than half the days. In addition, there are nearly 200 missing values. A
    scatterplot can help us cross-check missing data against the year of the recording:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现有少数几个月份的平均值是基于少于一半天数的测量值。此外，有近200个缺失值。散点图可以帮助我们交叉检查缺失数据与记录年份：
- en: '[PRE16]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '![](assets/leds_09in03.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/leds_09in03.png)'
- en: The line along the bottom left of the plot shows us that all of the missing
    data are in the early years of operation. The number of days of operation of the
    equipment may not have been collected in the early days. It also appears that
    there might have been problems with the equipment in the mid- to late ’80s. What
    do we do with these conjectures? We can try to confirm them by looking through
    documentation about the historical readings. If we are concerned about the impact
    on the CO[2] averages for records with missing values for the number of days of
    operation, then a simple solution would be to drop the earliest recordings. However,
    we would want to delay such action until after we have examined the time trends
    and assess whether there are any potential problems with the CO[2] averages in
    those early days.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图表底部的左侧线条显示，所有缺失数据都在设备运行初期。设备运行天数可能在早期并未收集。此外，从80年代中期到80年代末，设备可能存在问题。针对这些推测，我们该如何处理呢？我们可以通过查阅历史记录的文件来确认这些推测。如果我们担心缺失设备运行天数的记录对CO[2]平均值的影响，那么一个简单的解决方案是删除最早的记录。不过，在我们检查时间趋势并评估这些早期天数是否存在潜在问题之后再采取行动会更好。
- en: 'Next, let’s return to the `-99.99` values for the average CO[2] measurement
    and begin our checks with a histogram:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们再次关注平均CO[2]测量值中的`-99.99`值，并从直方图开始我们的检查：
- en: '[PRE17]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '![](assets/leds_09in04.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/leds_09in04.png)'
- en: 'The recorded values are in the 300–400 range, which is what we expect based
    on our research into CO[2] levels. We also see that there are only a few missing
    values. Since there aren’t many missing values, we can examine all of them:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们对二氧化碳（CO[2]）水平的研究，记录的数值在300至400的范围内，这符合我们的预期。我们还注意到只有少量的缺失数值。由于缺失值不多，我们可以检查所有这些值：
- en: '[PRE18]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '|   | Yr | Mo | DecDate | Avg | Int | Trend | days |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '|   | Yr | Mo | DecDate | Avg | Int | Trend | days |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| **3** | 1958 | 6 | 1958.46 | -99.99 | 317.10 | 314.85 | -1 |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| **3** | 1958 | 6 | 1958.46 | -99.99 | 317.10 | 314.85 | -1 |'
- en: '| **7** | 1958 | 10 | 1958.79 | -99.99 | 312.66 | 315.61 | -1 |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| **7** | 1958 | 10 | 1958.79 | -99.99 | 312.66 | 315.61 | -1 |'
- en: '| **71** | 1964 | 2 | 1964.12 | -99.99 | 320.07 | 319.61 | -1 |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| **71** | 1964 | 2 | 1964.12 | -99.99 | 320.07 | 319.61 | -1 |'
- en: '| **72** | 1964 | 3 | 1964.21 | -99.99 | 320.73 | 319.55 | -1 |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| **72** | 1964 | 3 | 1964.21 | -99.99 | 320.73 | 319.55 | -1 |'
- en: '| **73** | 1964 | 4 | 1964.29 | -99.99 | 321.77 | 319.48 | -1 |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| **73** | 1964 | 4 | 1964.29 | -99.99 | 321.77 | 319.48 | -1 |'
- en: '| **213** | 1975 | 12 | 1975.96 | -99.99 | 330.59 | 331.60 | 0 |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| **213** | 1975 | 12 | 1975.96 | -99.99 | 330.59 | 331.60 | 0 |'
- en: '| **313** | 1984 | 4 | 1984.29 | -99.99 | 346.84 | 344.27 | 2 |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| **313** | 1984 | 4 | 1984.29 | -99.99 | 346.84 | 344.27 | 2 |'
- en: We are faced with the question of what to do with the `-99.99` values. We have
    seen already the problems of leaving these values as is in a line plot. There
    are several options, and we describe them next.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们面临的问题是如何处理`-99.99`的数值。我们已经看到在折线图中保留这些数值会带来问题。有几种选择，我们接下来会描述它们。
- en: Addressing Missing Data
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 处理缺失数据
- en: The `-99.99`s for average CO[2] levels indicate missing recordings. These interfere
    with our statistical summaries and plots. It’s good to know which values are missing,
    but we need to do something about them. We might drop those records, replace `-99.99`
    with `NaN`, or substitute `99.99` with a likely value for the average CO[2]. Let’s
    examine each of these three options.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 平均CO[2]水平中的`-99.99`表示缺失记录。这些值影响了我们的统计摘要和图表。知道哪些值是缺失的很重要，但我们需要采取措施。我们可以删除这些记录，用`NaN`替换`-99.99`，或者用一个可能的平均CO[2]值替换`-99.99`。让我们逐个检查这三种选择。
- en: Note that the table already comes with a substitute value for the `-99.99`.
    The column labeled `Int` has values that exactly match those in `Avg`, except
    when `Avg` is `-99.99`, and then a “reasonable” estimate is used instead.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，表格中已经有了一个替代值来代替`-99.99`。标记为`Int`的列中的值与`Avg`中的值完全相同，只有当`Avg`为`-99.99`时，才会使用“合理”的估计值。
- en: 'To see the effect of each option, let’s zoom in on a short time period—say
    the measurements in 1958—where we know we have two missing values. We can create
    a time-series plot for the three cases: drop the records with `-99.99`s (left
    plot), use `NaN` for missing values (middle plot), and substitute an estimate
    for `-99.99` (right plot):'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 为了看清每种选择的影响，让我们放大一个短时间段，比如说1958年的测量数据，我们知道在这里有两个缺失值。我们可以为三种情况创建一个时间序列图：删除带有`-99.99`的记录（左侧图）、使用`NaN`表示缺失值（中间图）、替换`-99.99`为估计值（右侧图）：
- en: '![](assets/leds_09in05.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/leds_09in05.png)'
- en: When we look closely, we can see the difference between each of these plots.
    The leftmost plot connects dots across a two-month time period, rather than one
    month. In the middle plot, the line breaks where the data are missing, and on
    the right, we can see that months 6 and 10 now have values. In the big picture,
    since there are only seven values missing from the 738 months, all of these options
    work. However, there is some appeal to the right plot since the seasonal trends
    are more cleanly discernible.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 仔细观察时，我们可以看到每个图表之间的差异。最左边的图表连接了一个两个月的时间段内的点，而不是一个月。在中间的图表中，数据缺失处断开了线，而在右边，我们可以看到第
    6 和第 10 月现在有值了。总体上来说，由于 738 个月中只有七个值缺失，所有这些选项都有效。然而，右图更吸引人的地方在于季节性趋势更清晰可辨。
- en: The method used to interpolate the CO[2] measurements for the missing values
    is an averaging process that takes into consideration the month and year. The
    idea is to reflect both seasonal changes and the long-term trend. This technique
    is described in greater detail in the documentation at the top of the datafile.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 用于插值 CO[2] 测量值的方法是考虑到月份和年份的平均处理过程。其思想是反映季节性变化和长期趋势。这一技术在数据文件顶部的文档中有更详细的描述。
- en: These plots have shown the granularity of the data to be monthly measurements,
    but other granularity options are available to us. We discuss this next.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这些图表显示数据的粒度为每月测量，但我们还可以选择其他粒度选项。接下来我们将讨论这一点。
- en: Reshaping the Data Table
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据表重塑
- en: The CO[2] measurements taken at the Mauna Loa Observatory are also available
    both daily and hourly. The hourly data has a *finer granularity* than the daily
    data; reciprocally, the daily data is *coarser* than the hourly data.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 毛纳罗亚观测站获取的 CO[2] 测量数据还有每天和每小时的数据。每小时数据的*粒度更细*，而每日数据则*比每小时数据粗*。
- en: 'Why not always just use the data with the finest granularity available? On
    a computational level, fine-grained data can become quite large. The Mauna Loa
    Observatory started recording CO[2] levels in 1958\. Imagine how many rows the
    data table would contain if the facility provided measurements every single second!
    But more importantly, we want the granularity of the data to match our research
    question. Suppose we want to see whether CO[2] levels have risen over the past
    50+ years, consistent with global warming predictions. We don’t need a CO[2] measurement
    every second. In fact, we might well be content with yearly averages where the
    seasonal patterns are smoothed away. We can aggregate the monthly measurements,
    changing the granularity to annual averages, and make a plot to display the general
    trend. We can use *aggregation* to go to a coarser granularity—in `pandas`, we
    use `.groupby()` and `.agg()`:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么不总是使用最精细的数据粒度？在计算层面上，细粒度数据可能会变得非常大。毛纳罗亚观测站从 1958 年开始记录 CO[2] 水平。想象一下，如果该设施每秒提供一次测量，数据表会包含多少行！但更重要的是，我们希望数据的粒度与我们的研究问题相匹配。假设我们想要查看过去
    50 多年来 CO[2] 水平是否上升，这与全球变暖预测一致。我们并不需要每秒一次的 CO[2] 测量。事实上，我们可能对年均值感到满意，因为这样可以平滑掉季节模式。我们可以聚合每月测量值，将粒度更改为年均值，并制作一个图表显示总体趋势。我们可以使用*聚合*来转向更粗粒度——在`pandas`中，我们使用`.groupby()`和`.agg()`：
- en: '![](assets/leds_09in06.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/led   ![](assets/leds_09in06.png)'
- en: Indeed, we see a rise by nearly 100 ppm of CO[2] since Mauna Loa began recording
    in 1958.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 的确，自 1958 年以来，毛纳罗亚观测站记录的 CO[2] 测量值上升了将近 100 ppm。
- en: 'To recap, after reading the whitespace-separated, plain-text file into a dataframe,
    we began to check its quality. We used the scope and context of the data to affirm
    that its shape matched the range of dates of collection. We confirmed that the
    values and counts for the month were as expected. We ascertained the extent of
    missing values in the features, and we looked for connections between missing
    values and other features. We considered three approaches to handling the missing
    data: drop records, work with `NaN` values, and impute values to have a full table.
    And, finally, we changed the granularity of the dataframe by rolling it up from
    a monthly to an annual average. This change in granularity removed seasonal fluctuations
    and focused on the long-term trend in the level of CO[2] in the atmosphere. The
    next four sections of this chapter expand on these actions to wrangle data into
    a form suitable for analysis: quality checks, missing value treatments, transformations,
    and shape adjustments. We begin with quality checks.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，在将空格分隔的纯文本文件读入数据框后，我们开始检查其质量。我们使用数据的范围和上下文来确认其形状是否与收集日期的范围匹配。我们确认了月份的值和计数是否符合预期。我们确定了功能中缺失值的程度，并查找缺失值与其他功能之间的关系。我们考虑了三种处理缺失数据的方法：删除记录、处理`NaN`值和填补值以获得完整的表格。最后，我们通过将数据框的粒度从每月平均值升级到年度平均值来改变数据的粒度。这种粒度变化消除了季节性波动，并集中在大气中CO[2]水平的长期趋势上。本章的接下来四个部分将扩展这些操作，将数据整理成适合分析的形式：质量检查、缺失值处理、转换和形状调整。我们从质量检查开始。
- en: Quality Checks
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 质量检查
- en: 'Once your data are in a table and you understand the scope and granularity,
    it’s time to inspect for quality. You may have come across errors in the source
    as you examined and wrangled the file into a dataframe. In this section, we describe
    how to continue this inspection and carry out a more comprehensive assessment
    of the quality of the features and their values. We consider data quality from
    four vantage points:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您的数据进入表格，并且您理解了范围和粒度，就是检查质量的时候了。在您检查和整理文件到数据框时，您可能会发现源数据中的错误。在本节中，我们描述如何继续这一检查，并进行更全面的功能和值质量评估。我们从四个角度考虑数据质量：
- en: Scope
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 范围
- en: Do the data match your understanding of the population?
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 数据是否与您对人口的理解相匹配？
- en: Measurements and values
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 测量和值
- en: Are the values reasonable?
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 值是否合理？
- en: Relationships
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 关系
- en: Are related features in agreement?
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 相关特征是否一致？
- en: Analysis
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 分析
- en: Which features might be useful in a future analysis?
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 哪些功能可能在未来的分析中有用？
- en: We describe each of these points in turn, beginning with scope.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们依次描述每个点，从范围开始。
- en: Quality Based on Scope
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于范围的质量
- en: In [Chapter 2](ch02.html#ch-data-scope), we addressed whether or not the data
    that have been collected can adequately address the problem at hand. There, we
    identified the target population, access frame, and sample in collecting the data.
    That framework helps us consider possible limitations that might impact the generalizability
    of our findings.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第2章](ch02.html#ch-data-scope)中，我们讨论了收集的数据是否能够充分解决当前问题。在那里，我们确定了目标人口、访问框架和样本收集数据。该框架帮助我们考虑可能影响研究结果普适性的潜在限制。
- en: 'While these broader data-scope considerations are important as we deliberate
    our final conclusions, they are also useful for checking data quality. For example,
    for the San Francisco restaurant inspections data introduced in [Chapter 8](ch08.html#ch-files),
    a side investigation tells us that zip codes in the city should start with 941\.
    But a quick check shows that several zip codes begin with other digits:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然在我们审议最终结论时，这些更广泛的数据范围考虑是重要的，但它们也有助于检查数据质量。例如，在[第8章](ch08.html#ch-files)介绍的旧金山餐厅检查数据中，一项侧面调查告诉我们，城市的邮政编码应以941开头。但快速检查显示，有几个邮政编码以其他数字开头：
- en: '[PRE19]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: This verification using scope helps us spot potential problems.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 使用范围进行的这种验证有助于我们发现潜在问题。
- en: As another example, a bit of background reading at [Climate.gov](https://www.climate.gov)
    and [NOAA](https://oreil.ly/UBPDY) on the topic of atmospheric CO[2] reveals that
    typical measurements are about 400 ppm worldwide. So we can check whether the
    monthly averages of CO[2] at Mauna Loa lie between 300 and 450 ppm.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个例子是，在[Climate.gov](https://www.climate.gov)和[NOAA](https://oreil.ly/UBPDY)上关于大气CO[2]的背景阅读中，Typical
    measurements约为全球400 ppm。因此，我们可以检查夏威夷火山月均CO[2]浓度是否介于300到450 ppm之间。
- en: Next, we check data values against codebooks and the like.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将数据值与代码手册等进行比对。
- en: Quality of Measurements and Recorded Values
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 测量和记录值的质量
- en: 'We can use also check the quality of measurements by considering what might
    be a reasonable value for a feature. For example, imagine what might be a reasonable
    range for the number of violations in a restaurant inspection: possibly, 0 to
    5\. Other checks can be based on common knowledge of ranges: a restaurant inspection
    score must be between 0 and 100; months run between 1 and 12\. We can use documentation
    to tell us the expected values for a feature. For example, the type of emergency
    room visit in the DAWN survey, introduced in [Chapter 8](ch08.html#ch-files),
    has been coded as 1, 2, …, 8 (see [Figure 9-1](#dawn-codebook)). So we can confirm
    that all values for the type of visit are indeed integers between 1 and 8.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以通过考虑特征的合理值来检查测量的质量。例如，想象一下餐厅检查中违规数量的合理范围可能是0到5\. 其他检查可以基于常识的范围：餐厅检查分数必须在0到100之间；月份必须在1到12之间\.
    我们可以使用文档来告诉我们特征的预期值。例如，在DAWN调查中的急诊室访问类型，介绍在[第8章](ch08.html#ch-files)，已编码为1、2、...、8（参见[图9-1](#dawn-codebook)）。因此，我们可以确认访问类型的所有值确实是介于1到8之间的整数。
- en: '![](assets/leds_0901.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/leds_0901.png)'
- en: Figure 9-1\. Screenshot of the description of the emergency room visit type
    (CASETYPE) variable in the DAWN survey (the typo SUICICDE appears in the actual
    codebook)
  id: totrans-115
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-1\. DAWN调查中急诊室访问类型（CASETYPE）变量描述的屏幕截图（实际代码书中出现了拼写错误SUICICDE）
- en: We also want to ensure that the data type matches our expectations. For example,
    we expect a price to be a number, whether or not it’s stored as integer, floating
    point, or string. Confirming that the units of measurement match what is expected
    can be another useful quality check to perform (for example, weight values recorded
    in pounds, not kilograms). We can devise checks for all of these situations.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还希望确保数据类型符合我们的预期。例如，我们希望价格是一个数字，无论它是存储为整数、浮点数还是字符串。确认测量单位与预期相符可以是另一个有用的质量检查（例如，以磅为单位记录的重量值，而不是公斤）。我们可以为所有这些情况设计检查。
- en: Other checks can be devised by comparing two related features.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过比较两个相关特征来设计其他检查。
- en: Quality Across Related Features
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 相关特征的质量
- en: 'At times, two features have built-in conditions on their values that we can
    cross-check for internal consistency. For example, according to the documentation
    for the DAWN study, alcohol consumption is only considered a valid reason for
    a visit to the ER for patients under age 21, so we can check that any record with
    “alcohol” for the type of visit has an age under 21\. A cross-tabulation of the
    features `type` and `age` can confirm that this constraint is met:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，两个特征对其值有内置条件，我们可以交叉检查其内部一致性。例如，根据DAWN研究的文档，饮酒只被认为是年龄在21岁以下的患者急诊访问的有效原因，因此我们可以检查任何记录中“饮酒”类型的访问是否年龄在21岁以下。
    `type` 和 `age` 的交叉表可以确认满足此约束：
- en: '[PRE21]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '| type | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| 类型 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| age |   |   |   |   |   |   |   |   |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| 年龄 |   |   |   |   |   |   |   |   |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| **-8** | 2 | 2 | 0 | 21 | 5 | 1 | 1 | 36 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| **-8** | 2 | 2 | 0 | 21 | 5 | 1 | 1 | 36 |'
- en: '| **1** | 0 | 6 | 20 | 6231 | 313 | 4 | 2101 | 69 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| **1** | 0 | 6 | 20 | 6231 | 313 | 4 | 2101 | 69 |'
- en: '| **2** | 8 | 2 | 15 | 1774 | 119 | 4 | 119 | 61 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| **2** | 8 | 2 | 15 | 1774 | 119 | 4 | 119 | 61 |'
- en: '| **3** | 914 | 121 | 2433 | 2595 | 1183 | 48 | 76 | 4563 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| **3** | 914 | 121 | 2433 | 2595 | 1183 | 48 | 76 | 4563 |'
- en: '| **4** | 817 | 796 | 4953 | 3111 | 1021 | 95 | 44 | 6188 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| **4** | 817 | 796 | 4953 | 3111 | 1021 | 95 | 44 | 6188 |'
- en: '| **5** | 983 | 1650 | 0 | 4404 | 1399 | 170 | 48 | 9614 |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| **5** | 983 | 1650 | 0 | 4404 | 1399 | 170 | 48 | 9614 |'
- en: '| **6** | 1068 | 1965 | 0 | 5697 | 1697 | 140 | 62 | 11408 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| **6** | 1068 | 1965 | 0 | 5697 | 1697 | 140 | 62 | 11408 |'
- en: '| **7** | 957 | 1748 | 0 | 5262 | 1527 | 100 | 60 | 10296 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| **7** | 957 | 1748 | 0 | 5262 | 1527 | 100 | 60 | 10296 |'
- en: '| **8** | 1847 | 3411 | 0 | 10221 | 2845 | 113 | 115 | 18366 |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| **8** | 1847 | 3411 | 0 | 10221 | 2845 | 113 | 115 | 18366 |'
- en: '| **9** | 1616 | 3770 | 0 | 12404 | 3407 | 75 | 150 | 18381 |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| **9** | 1616 | 3770 | 0 | 12404 | 3407 | 75 | 150 | 18381 |'
- en: '| **10** | 616 | 1207 | 0 | 12291 | 2412 | 31 | 169 | 7109 |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| **10** | 616 | 1207 | 0 | 12291 | 2412 | 31 | 169 | 7109 |'
- en: '| **11** | 205 | 163 | 0 | 24085 | 2218 | 12 | 308 | 1537 |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| **11** | 205 | 163 | 0 | 24085 | 2218 | 12 | 308 | 1537 |'
- en: The cross-tabulation confirms that all of the alcohol cases (`type` is 3) have
    an age under 21 (these are coded as 1, 2, 3, and 4). The data values are as expected.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉表确认所有酒精案例（`type`为3）年龄在21岁以下（这些编码为1、2、3和4）。数据值符合预期。
- en: One last type of quality check pertains to the amount of information found in
    a feature.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一种质量检查类型涉及特征中所含信息的量。
- en: Quality for Analysis
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分析质量
- en: Even when data pass the previous quality checks, problems can arise with its
    usefulness. For example, if all but a handful of values for a feature are identical,
    then that feature adds little to the understanding of underlying patterns and
    relationships. Or if there are too many missing values, especially if there is
    a discernible pattern in the missing values, our findings may be limited. Plus,
    if a feature has many bad/corrupted values, then we might question the accuracy
    of even those values that fall in the appropriate range.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 即使数据通过了之前的质量检查，它的有效性仍然可能存在问题。例如，如果一个特征的几乎所有值都相同，那么这个特征对于理解底层模式和关系的贡献就很少。或者如果存在太多缺失值，尤其是在缺失值中存在可辨识的模式时，我们的发现可能会受限。此外，如果一个特征有许多坏/损坏的值，那么我们可能会质疑即使在适当范围内的那些值的准确性。
- en: 'We see in the following code that the type of restaurant inspection in San
    Francisco can be either routine or from a complaint. Since only one of the 14,000+
    inspections was from a complaint, we lose little if we drop this feature, and
    we might also want to drop that single inspection since it represents an anomaly:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在下面的代码中看到，旧金山的餐馆检查类型可以是例行或投诉。由于14,000多次检查中只有一次是投诉，如果我们放弃这个特征，我们几乎不会损失什么，而且我们可能也想删除那个单独的检查，因为它代表了一个异常：
- en: '[PRE22]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Once we find problems with our data, we need to figure out what to do.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们发现数据的问题，我们需要弄清楚该如何处理。
- en: Fixing the Data or Not
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据修复与否
- en: 'When you uncover problems with the data, essentially you have four options:
    leave the data as is, modify values, remove features, or drop records.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 当你揭示数据的问题时，基本上你有四个选择：保留数据如其所是，修改数值，移除特征，或删除记录。
- en: Leave it as is
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 保留数据如其所是
- en: Not every unusual aspect of the data needs to be fixed. You might have discovered
    a characteristic of your data that will inform you about how to do your analysis
    and otherwise does not need correcting. Or you might find that the problem is
    relatively minor and most likely will not impact your analysis, so you can leave
    the data as is. Or, you might want to replace corrupted values with `NaN`.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 并非数据的每一个异常方面都需要修正。你可能已经发现了数据的一个特征，它将告诉你如何进行分析，而且不需要修正。或者你可能发现问题相对较小，很可能不会影响你的分析，因此你可以保留数据。或者，你可能希望用`NaN`替换损坏的数值。
- en: Modify individual values
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 修改单个数值
- en: If you have figured out what went wrong and can correct the value, then you
    can opt to change it. In this case, it’s a good practice to create a new feature
    with the modified value and preserve the original feature, like in the CO[2] example.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经找出了问题所在并可以修正数值，那么你可以选择进行更改。在这种情况下，创造一个带有修改数值的新特征，并保留原始特征是一个好的做法，就像二氧化碳（CO[2]）的例子中那样。
- en: Remove a column
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 移除一列
- en: If many values in a feature have problems, then consider eliminating that feature
    entirely. Rather than excluding a feature, there may be a transformation that
    allows you to keep the feature while reducing the level of detail recorded.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个特征中的许多值存在问题，那么考虑完全消除该特征。与排除一个特征不同，可能存在一种转换可以使你保留该特征同时降低记录的详细级别。
- en: Drop records
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 删除记录
- en: In general, we do not want to drop a large number of observations from a dataset
    without good reason. Instead, try to scale back your investigation to a particular
    subgroup of the data that is clearly defined by some criteria, and does not simply
    correspond dropped records with corrupted values. When you discover that an unusual
    value is in fact correct, you still might decide to exclude the record from your
    analysis because it’s so different from the rest of your data and you do not want
    it to overly influence your analysis.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 一般而言，我们不希望无故从数据集中删除大量观察结果。相反，尝试将你的调查范围缩小到某个明确定义的数据子集，而不是简单地对应着删除带有损坏数值的记录。当你发现一个异常值实际上是正确的时候，你可能仍然决定将该记录排除在你的分析之外，因为它与你的其他数据有显著不同，而你不希望它过度影响你的分析。
- en: Whatever approach you take, you will want to study the possible impact of the
    changes that you make on your analysis. For example, try to determine whether
    the records with corrupted values are similar to one another, and different from
    the rest of the data.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 无论你采取什么方法，你都需要研究你所做改变对分析的可能影响。例如，尝试确定带有损坏数值的记录是否彼此相似，并且与其他数据不同。
- en: Quality checks can reveal issues in the data that need to be addressed before
    proceeding with analysis. One particularly important type of check is to look
    for missing values. We suggested that there may be times when you want to replace
    corrupted data values with `NaN`, and hence treat them as missing. At other times,
    data might arrive missing. What to do with missing data is an important topic,
    and there is a lot of research on this problem; we cover ways to address missing
    data in the next section.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 质量检查可以揭示需要在进行分析之前解决的数据问题。一种特别重要的检查类型是查找缺失值。我们建议有时您可能希望将损坏的数据值替换为`NaN`，因此将其视为缺失。在其他时候，数据可能会缺失。如何处理缺失数据是一个重要的话题，有很多研究在解决这个问题；我们将在下一节中介绍处理缺失数据的方法。
- en: Missing Values and Records
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 缺失值与记录
- en: In [Chapter 3](ch03.html#ch-theory-datadesign), we considered the potential
    problems when the population and the access frame are not in alignment, so we
    can’t access everyone we want to study. We also described problems when someone
    refuses to participate in the study. In these cases, entire records/rows are missing,
    and we discussed the kinds of bias that can occur due to missing records. If nonrespondents
    differ in critical ways from respondents or if the nonresponse rate is not negligible,
    then our analysis may be seriously flawed. The example in [Chapter 3](ch03.html#ch-theory-datadesign)
    on election polls showed that increasing the sample size without addressing nonresponse
    does not reduce nonresponse bias. Also in that chapter, we discussed ways to prevent
    nonresponse. These preventive measures include using incentives to encourage response,
    keeping surveys short, writing clear questions, training interviewers, and investing
    in extensive follow-up procedures. Unfortunately, despite these efforts, some
    amount of nonresponse is unavoidable.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第三章](ch03.html#ch-theory-datadesign)中，我们考虑了当人群和访问框架不对齐时可能出现的问题，因此我们无法访问我们想要研究的所有人。我们还描述了当有人拒绝参与研究时可能出现的问题。在这些情况下，整个记录/行可能会丢失，并且我们讨论了由于缺失记录可能出现的偏差类型。如果未响应者在关键方面与响应者不同，或者非响应率不可忽略，则我们的分析可能会严重有误。第三章中关于选举民意测验的例子表明，增加样本大小而不解决非响应问题并不会减少非响应偏差。此外，在该章中，我们讨论了预防非响应的方法。这些预防措施包括使用激励措施鼓励响应，保持调查简短，编写清晰的问题，培训访问员，并投入广泛的后续程序。不幸的是，尽管这些努力，一定程度的非响应是不可避免的。
- en: When a record is not entirely missing, but a particular field in a record is
    unavailable, we have nonresponse at the field level. Some datasets use a special
    coding to signify that the information is missing. We saw that the Mauna Loa data
    uses `-99.99` to indicate a missing CO[2] measurement. We found only seven of
    these values among 738 rows in the table. In this case, we showed that these missing
    values have little impact on the analysis.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 当记录不完全丢失，但记录中的特定字段不可用时，我们称之为字段级的非响应。一些数据集使用特殊编码来表示信息丢失的情况。我们发现毛纳罗亚数据使用`-99.99`表示缺失的CO[2]测量。在表中的738行中，我们只发现了七个这样的值。在这种情况下，我们表明这些缺失值对分析影响不大。
- en: The values for a feature are called *missing completely at random* when those
    records with the missing data are like a randomly chosen subset of records. That
    is, whether or not a record has a missing value does not depend on the unobserved
    feature, the values of other features, or the sampling design. For example, if
    someone accidentally breaks the laboratory equipment at Mauna Loa and CO[2] is
    not recorded for a day, there is no reason to think that the level of CO[2] that
    day had something to do with the lost measurements.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 特征的值被称为*完全随机缺失*，当缺失数据的记录就像随机选择的记录子集时。也就是说，记录是否缺失不依赖于未观察到的特征、其他特征的值或抽样设计。例如，如果有人在毛纳罗亚意外损坏了实验设备，导致某天未记录CO[2]，那么没有理由认为那天的CO[2]水平与丢失的测量有关。
- en: At other times, we consider values *missing at random given covariates* (covariates
    are other features in the dataset). For example, the type of an ER visit in the
    DAWN survey is missing at random given covariates if, say, the nonresponse depends
    only on race and sex (and not on the type of visit or anything else). In these
    limited cases, the observed data can be weighted to accommodate for nonresponse.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在其他时候，我们考虑*给定协变量缺失随机*的值（协变量是数据集中的其他特征）。例如，在DAWN调查中，急诊访问类型在给定协变量情况下是随机缺失的，如果，例如，非响应仅依赖于种族和性别（而不依赖于访问类型或其他任何因素）。在这些有限的情况下，可以对观察数据进行加权以适应非响应。
- en: In some surveys, missing information is further categorized as to whether the
    respondent refused to answer, the respondent was unsure of the answer, or the
    interviewer didn’t ask the question. Each of these types of missing values is
    recorded using a different value. For example, according to the [codebook](https://oreil.ly/lwBYh),
    many questions in the DAWN survey use a code of `-7` for not applicable, `-8`
    for not documented, and `-9` for missing. Codings such as these can help us further
    refine our study of nonresponse.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些调查中，缺失信息进一步分类为受访者拒绝回答、受访者不确定答案或面试官未问问题。每种类型的缺失值使用不同的值记录。例如，根据[代码书](https://oreil.ly/lwBYh)，DAWN调查中的许多问题使用`-7`表示不适用，`-8`表示未记录，`-9`表示缺失。这些编码可以帮助我们进一步完善非响应的研究。
- en: After nonresponse has occurred, it is sometimes possible to use models to predict
    the missing data. We describe this process next. But remember, predicting missing
    observations is never as good as observing them in the first place.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在非响应发生后，有时可以使用模型预测缺失的数据。我们接下来描述这个过程。但请记住，预测缺失的观察结果永远不如首次观察到它们好。
- en: At times, we substitute a reasonable value for a missing one to create a “clean”
    dataframe. This process is called *imputation*. Some common approaches for imputing
    values are *deductive*, *mean*, and *hot-deck* imputation.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，我们会为缺失的值替换一个合理的值，以创建一个“干净”的数据框架。这个过程称为*填补*。填补值的一些常见方法包括*演绎*、*均值*和*热卡*填补。
- en: 'In deductive imputation, we fill in a value through logical relationships with
    other features. For example, here is a row in the business dataframe for San Francisco
    restaurant inspections. The zip code is erroneously marked as “Ca” and latitude
    and longitude are missing:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在演绎填补中，我们通过与其他特征的逻辑关系填补值。例如，这是旧金山餐馆检查的业务数据框架中的一行。邮政编码错误地标记为“Ca”，纬度和经度缺失：
- en: '[PRE24]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '|   | business_id | name | address | city | ... | postal_code | latitude |
    longitude | phone_number |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '|   | business_id | name | address | city | ... | postal_code | latitude |
    longitude | phone_number |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| **5480** | 88139 | TACOLICIOUS | 2250 CHESTNUT ST | San Francisco | ... |
    Ca | NaN | NaN | +14156496077 |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| **5480** | 88139 | TACOLICIOUS | 2250 CHESTNUT ST | San Francisco | ... |
    Ca | NaN | NaN | +14156496077 |'
- en: '[PRE25]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: We can look up the address on the USPS website to get the correct zip code,
    and we can use Google Maps to find the latitude and longitude of the restaurant
    to fill in these missing values.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在USPS网站上查找地址以获取正确的邮政编码，并可以使用Google Maps查找餐馆的纬度和经度来填补这些缺失值。
- en: Mean imputation uses an average value from rows in the dataset that aren’t missing.
    As a simple example, if a dataset on test scores is missing scores for some students,
    mean imputation would fill in the missing value using the mean of the nonmissing
    scores. A key issue with mean imputation is that the variability in the imputed
    feature will be smaller because the feature now has values that are identical
    to the mean. This affects later analysis if not handled properly—for instance,
    confidence intervals will be smaller than they should be (these topics are covered
    in [Chapter 17](ch17.html#ch-inf-pred-theory)). The missing values for CO[2] in
    Mauna Loa used a more sophisticated averaging technique that included neighboring
    seasonal values.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 均值填补使用数据集中非缺失行的平均值。例如，如果一个测试分数数据集中一些学生的分数缺失，均值填补将使用非缺失分数的平均值填补缺失值。均值填补的一个关键问题是，由于该特征现在具有与均值相同的值，因此填补后特征的变异性将较小。如果不正确处理，这将影响后续分析，例如，置信区间将比预期小（这些主题在[第17章](ch17.html#ch-inf-pred-theory)中有详细介绍）。在马乌纳罗亚的CO[2]的缺失值中，使用了更复杂的平均技术，其中包括邻近的季节性值。
- en: Hot-deck imputation uses a chance process to select a value at random from rows
    that have values. As a simple example, hot-deck imputation could fill in missing
    test scores by randomly choosing another test score in the dataset. A potential
    problem with hot-deck imputation is that the strength of a relationship between
    the features might weaken because we have added randomness.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 热卡填补使用机会过程从具有值的行中随机选择一个值。例如，热卡填补可以通过随机选择数据集中的另一个测试分数来填补缺失的测试分数。热卡填补的一个潜在问题是，特征之间的关系强度可能会因为我们增加了随机性而减弱。
- en: For mean and hot-deck imputation, we often impute values based on other records
    in the dataset that have similar values in other features. More sophisticated
    imputation techniques use nearest-neighbor methods to find similar subgroups of
    records and others use regression techniques to predict the missing value.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 对于均值和热补卡填补，我们通常基于数据集中具有其他特征中类似值的记录来填补值。更复杂的填补技术使用最近邻方法来找到相似记录子组，其他技术使用回归技术来预测缺失值。
- en: With all of these types of imputation, we should create a new feature that contains
    the altered data or a new feature to indicate whether or not the response in the
    original feature has been imputed so that we can track our changes.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有这些填补类型中，我们应该创建一个包含修改后数据的新特征，或者创建一个新特征来指示原始特征中的响应是否已被填补，以便我们可以跟踪我们的更改。
- en: Decisions to keep or drop a record with a missing value, to change a value,
    or to remove a feature may seem small, but they can be critical. One anomalous
    record can seriously impact your findings. Whatever you decide, be sure to check
    the impact of dropping or changing features and records. And be transparent and
    thorough in reporting any modifications you make to the data. It’s best to make
    these changes programmatically to reduce potential errors and enable others to
    confirm exactly what you have done by reviewing your code.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 决定保留或丢弃具有缺失值的记录、更改值或删除特征可能看起来微不足道，但它们可能至关重要。一个异常记录可能严重影响您的发现。无论您做出什么决定，都要确保检查删除或更改特征和记录的影响。在报告您对数据所做修改时，一定要透明和彻底。最好通过编程方式进行这些更改，以减少潜在错误，并使其他人能够通过审查您的代码确认您所做的确切更改。
- en: The same transparency and reproducible precautions hold for data transformations,
    which we discuss next.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 数据转换也需要同样的透明度和可重现性预防措施，接下来我们会讨论这些。
- en: Transformations and Timestamps
  id: totrans-178
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 转换和时间戳
- en: 'Sometimes a feature is not in a form well-suited for analysis, and so we transform
    it. There are many reasons a feature might need a transformation: the value codings
    might not be useful for analysis, we may want to apply a mathematical function
    to a feature, or we might want to pull information out of a feature and create
    a new feature. We describe these three basic kinds of transformations: type conversions,
    mathematical transformations, and extractions:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 有时特征的形式不适合分析，因此我们对其进行转换。特征可能需要转换的原因有很多：值编码可能对分析无用，我们可能想对特征应用数学函数，或者我们可能想从特征中提取信息并创建新特征。我们描述了这三种基本类型的转换：类型转换、数学转换和提取：
- en: Type conversion
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 类型转换
- en: This kind of transformation occurs when we convert the data from one format
    to another to make the data more useful for analysis. We might convert information
    stored as a string to another format. For example, we would want to convert prices
    reported as strings to numbers (like changing the string `"$2.17"` to the number
    2.17) so that we can compute summary statistics. Or we might want to convert a
    time stored as a string, such as `"1955-10-12"`, to a `pandas Timestamp` object.
    Yet another example occurs when we lump categories together, such as reducing
    the 11 categories for age in DAWN to 5 groupings.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 这种转换发生在我们将数据从一种格式转换为另一种格式以使数据更适合分析时。我们可能会将存储为字符串的信息转换为另一种格式。例如，我们可能希望将报价字符串`"$2.17"`转换为数字2.17，以便计算汇总统计数据。或者我们可能希望将存储为字符串的时间，如`"1955-10-12"`，转换为`pandas
    Timestamp`对象。另一个示例是在将类别合并在一起时发生，例如将DAWN中的11个年龄类别减少为5个分组。
- en: Mathematical transformation
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 数学转换
- en: One kind of mathematical transformation is when we change the units of a measurement
    from, say, pounds to kilograms. We might make unit conversions so that statistics
    on our data can be directly compared to statistics on other datasets. Yet another
    reason to transform a feature is to make its distribution more symmetric (this
    notion is covered in more detail in [Chapter 10](ch10.html#ch-eda)). The most
    common transformation for handling asymmetry is the logarithm. Lastly, we might
    want to create a new feature from arithmetic operations. For example, we can combine
    heights and weights to create body mass indexes by calculating <math><mtext>height</mtext>
    <mrow><mo>/</mo></mrow> <msup><mtext>weight</mtext> <mn>2</mn></msup></math> .
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 数学转换的一种类型是当我们从一个测量单位，比如从磅到公斤，进行单位转换。我们可能进行单位转换，以便我们的数据统计可以直接与其他数据集的统计进行比较。进行特征转换的另一个原因是使其分布更对称（这个概念在[第10章](ch10.html#ch-eda)中有更详细的介绍）。处理不对称性最常见的转换是对数。最后，我们可能希望通过算术运算创建一个新的特征。例如，我们可以结合身高和体重，通过计算
    <math><mtext>height</mtext> <mrow><mo>/</mo></mrow> <msup><mtext>weight</mtext>
    <mn>2</mn></msup></math> 来创建身体质量指数。
- en: Extraction
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 提取
- en: Sometimes we want to create a feature by extraction, where the new feature contains
    partial information taken from another feature. For example, the inspection violations
    consist of strings with descriptions of violations, and we may only be interested
    in whether the violation is related to, say, vermin. We can create a new feature
    that is `True` if the violation contains the word *vermin* in its text description
    and `False` otherwise. This conversion of information to logical values (or 0–1
    values) is extremely useful in data science. The upcoming example in this chapter
    gives a concrete use-case for these binary features.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候我们想通过提取创建一个特征，新特征包含从另一个特征中提取的部分信息。例如，检查违规行为包含违规描述的字符串，我们可能只关心违规是否涉及，比如，害虫。如果违规描述中包含单词
    *vermin*，我们可以创建一个新特征，如果是，则为`True`，否则为`False`。将信息转换为逻辑值（或0-1值）在数据科学中非常有用。本章中即将介绍的示例为这些二元特征提供了一个具体的用例。
- en: We cover many other examples of useful transformations in [Chapter 10](ch10.html#ch-eda).
    For the rest of this section, we explain one more kind of transformation related
    to working with dates and times. Dates and times appear in many kinds of data,
    so it’s worth learning how to work with these data types.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第10章](ch10.html#ch-eda)中涵盖了许多其他有用转换的示例。在本节的其余部分，我们解释了与处理日期和时间相关的另一种转换方式。日期和时间出现在许多类型的数据中，因此学习如何处理这些数据类型是值得的。
- en: Transforming Timestamps
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 转换时间戳
- en: A *timestamp* is a data value that records a specific date and time. For instance,
    a timestamp could be recorded like `Jan 1 2020 2pm` or `2021-01-31 14:00:00` or
    `2017 Mar 03 05:12:41.211 PDT`. Timestamps come in many different formats! This
    kind of information can be useful for analysis, because it lets us answer questions
    like, “What times of day do we have the most website traffic?” When we work with
    timestamps, we often need to parse them for easier analysis.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '*时间戳* 是记录特定日期和时间的数据值。例如，时间戳可以记录为 `Jan 1 2020 2pm` 或 `2021-01-31 14:00:00` 或
    `2017 Mar 03 05:12:41.211 PDT`。时间戳有许多不同的格式！这种信息对于分析非常有用，因为它让我们能够回答诸如“一天中哪个时段的网站流量最高？”的问题。当我们处理时间戳时，通常需要对其进行解析以便于分析。'
- en: 'Let’s take a look at an example. The inspections dataframe for the San Francisco
    restaurants includes the date when restaurant inspections happened:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个例子。旧金山餐馆的检查数据包括餐厅检查发生的日期：
- en: '[PRE26]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '|   | business_id | score | date | type |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '|   | business_id | score | date | type |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| **0** | 19 | 94 | 20160513 | routine |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| **0** | 19 | 94 | 20160513 | routine |'
- en: '| **1** | 19 | 94 | 20171211 | routine |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| **1** | 19 | 94 | 20171211 | routine |'
- en: '| **2** | 24 | 98 | 20171101 | routine |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| **2** | 24 | 98 | 20171101 | routine |'
- en: '| **3** | 24 | 98 | 20161005 | routine |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| **3** | 24 | 98 | 20161005 | routine |'
- en: 'By default, however, `pandas` reads in the `date` column as an integer:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，`pandas`将`date`列读取为整数：
- en: '[PRE27]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[PRE28]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: This storage type makes it hard to answer some useful questions about the data.
    Let’s say we want to know whether inspections happen more often on weekends or
    weekdays. To answer this question, we want to convert the `date` column to the
    `pandas` `Timestamp` storage type and extract the day of the week.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 这种存储类型使得回答一些有用的数据问题变得困难。假设我们想知道检查是否更频繁发生在周末还是工作日。为了回答这个问题，我们想将`date`列转换为`pandas`的`Timestamp`存储类型，并提取星期几。
- en: 'The date values appear to come in the format `YYYYMMDD`, where `YYYY`, `MM`,
    and `DD` correspond to the four-digit year, two-digit month, and two-digit day,
    respectively. The `pd.to_datetime()` method can parse the date strings into objects,
    where we can pass in the format of the dates as a [date format](https://oreil.ly/TFWcU)
    string:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 日期值似乎采用`YYYYMMDD`格式，其中`YYYY`、`MM`和`DD`分别对应四位数年份、两位数月份和两位数日期。`pd.to_datetime()`方法可以将日期字符串解析为对象，我们可以传入日期格式作为[日期格式](https://oreil.ly/TFWcU)字符串：
- en: '[PRE29]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[PRE30]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: We can see that `insp_dates` now has a `dtype` of `datetime64[ns]`, which means
    that the values were successfully converted into `pd.Timestamp` objects.^([1](ch09.html#id1153))
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以看到`insp_dates`现在具有`datetime64[ns]`的`dtype`，这意味着值已成功转换为`pd.Timestamp`对象。^([1](ch09.html#id1153))
- en: '`pandas` has special methods and properties for `Series` objects that hold
    timestamps using the `.dt` accessor. For instance, we can easily pull out the
    year for each timestamp:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '`pandas`为使用`.dt`访问器保持时间戳的`Series`对象提供了特殊方法和属性。例如，我们可以轻松地提取每个时间戳的年份：'
- en: '[PRE31]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The `pandas` documentation has the complete details on the [`.dt` accessor](https://oreil.ly/_ceNL).
    By looking at the documentation, we see that the `.dt.day_of_week` attribute gets
    the day of the week for each timestamp (Monday = 0, Tuesday = 1, …, Sunday = 6).
    So let’s assign new columns to the dataframe that contain both the parsed timestamps
    and the day of the week:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '`pandas`文档详细介绍了[`.dt`访问器](https://oreil.ly/_ceNL)的所有细节。通过查看文档，我们可以看到`.dt.day_of_week`属性获取每个时间戳的星期几（星期一=0，星期二=1，…，星期日=6）。因此，让我们向数据框中分配新列，这些列包含解析的时间戳和星期几：'
- en: '[PRE33]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '|   | business_id | score | date | type | timestamp | dow |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '|   | business_id | score | date | type | timestamp | dow |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| **0** | 19 | 94 | 20160513 | routine | 2016-05-13 | 4 |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| **0** | 19 | 94 | 20160513 | routine | 2016-05-13 | 4 |'
- en: '| **1** | 19 | 94 | 20171211 | routine | 2017-12-11 | 0 |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| **1** | 19 | 94 | 20171211 | routine | 2017-12-11 | 0 |'
- en: '| **2** | 24 | 98 | 20171101 | routine | 2017-11-01 | 2 |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| **2** | 24 | 98 | 20171101 | routine | 2017-11-01 | 2 |'
- en: 'Now we can see whether restaurant inspectors favor a certain day of the week
    by grouping on the day of the week:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以看出，餐厅检查员是否偏爱某一周的某一天，通过对星期几进行分组来实现：
- en: '[PRE34]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '|   | dow | count |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '|   | dow | count |'
- en: '| --- | --- | --- |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| **0** | 2 | 3281 |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| **0** | 2 | 3281 |'
- en: '| **1** | 1 | 3264 |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| **1** | 1 | 3264 |'
- en: '| **2** | 3 | 2497 |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| **2** | 3 | 2497 |'
- en: '| **3** | 0 | 2464 |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| **3** | 0 | 2464 |'
- en: '| **4** | 4 | 2101 |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| **4** | 4 | 2101 |'
- en: '| **5** | 6 | 474 |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| **5** | 6 | 474 |'
- en: '| **6** | 5 | 141 |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| **6** | 5 | 141 |'
- en: '![](assets/leds_09in07.png)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/leds_09in07.png)'
- en: As expected, inspections rarely happen on the weekend. We also find that Tuesday
    and Wednesday are the most popular days for an inspection.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 正如预期的那样，检查很少在周末进行。我们还发现星期二和星期三是最受欢迎的检查日。
- en: We have performed many wranglings on the inspections table. One approach to
    tracking these modifications is to pipe these actions from one to the next. We
    describe the idea of piping next.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经对检查表执行了许多操作。跟踪这些修改的一种方法是将这些操作从一个操作到下一个进行管道传输。接下来我们将讨论管道的概念。
- en: Piping for Transformations
  id: totrans-229
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 转换管道
- en: In data analyses, we typically apply many transformations to the data, and it
    is easy to introduce bugs when we repeatedly mutate a dataframe, in part because
    Jupyter notebooks let us run cells in any order we want. As a good practice, we
    recommend putting transformation code into functions with helpful names and using
    the `DataFrame.pipe()` method to chain transformations together.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据分析中，我们通常对数据应用许多转换，当我们反复变异数据框时，很容易引入错误，部分原因是Jupyter笔记本允许我们按任何顺序运行单元格。作为良好的实践，我们建议将转换代码放入具有有用名称的函数中，并使用`DataFrame.pipe()`方法将转换链接在一起。
- en: 'Let’s rewrite the earlier timestamp parsing code into a function and add the
    timestamps back into the dataframe as a new column, along with a second column
    containing the year of the timestamp:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将早期的时间戳解析代码重写为函数，并将时间戳作为新列添加回数据框中，同时添加第二列，其中包含时间戳的年份：
- en: '[PRE35]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Now we can pipe the `insp` dataframe through this function using `.pipe()`:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以使用`.pipe()`方法将`insp`数据框通过此函数管道化：
- en: '[PRE36]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'We can chain many `.pipe()` calls together. For example, we can extract the
    day of the week from the timestamps:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以链接许多`.pipe()`调用在一起。例如，我们可以从时间戳中提取星期几：
- en: '[PRE37]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '|   | business_id | score | date | type | timestamp | year | dow |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '|   | business_id | score | date | type | timestamp | year | dow |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| **0** | 19 | 94 | 20160513 | routine | 2016-05-13 | 2016 | 4 |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| **0** | 19 | 94 | 20160513 | routine | 2016-05-13 | 2016 | 4 |'
- en: '| **1** | 19 | 94 | 20171211 | routine | 2017-12-11 | 2017 | 0 |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| **1** | 19 | 94 | 20171211 | 日常 | 2017-12-11 | 2017 | 0 |'
- en: '| **2** | 24 | 98 | 20171101 | routine | 2017-11-01 | 2017 | 2 |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| **2** | 24 | 98 | 20171101 | 日常 | 2017-11-01 | 2017 | 2 |'
- en: '| **...** | ... | ... | ... | ... | ... | ... | ... |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| **...** | ... | ... | ... | ... | ... | ... | ... |'
- en: '| **14219** | 94142 | 100 | 20171220 | routine | 2017-12-20 | 2017 | 2 |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| **14219** | 94142 | 100 | 20171220 | 日常 | 2017-12-20 | 2017 | 2 |'
- en: '| **14220** | 94189 | 96 | 20171130 | routine | 2017-11-30 | 2017 | 3 |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| **14220** | 94189 | 96 | 20171130 | 日常 | 2017-11-30 | 2017 | 3 |'
- en: '| **14221** | 94231 | 85 | 20171214 | routine | 2017-12-14 | 2017 | 3 |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| **14221** | 94231 | 85 | 20171214 | 日常 | 2017-12-14 | 2017 | 3 |'
- en: '[PRE38]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: There are several key advantages of using `pipe()`. When there are many transformations
    on a single dataframe, it’s easier to see what transformations happen since we
    can simply read the function names. Also, we can reuse transformation functions
    for different dataframes. For instance, the `viol` dataframe, which contains restaurant
    safety violations, also has a `date` column. This means we can use `.pipe()` to
    reuse the timestamp parsing function without needing to write extra code. Convenient!
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `pipe()` 的几个关键优势。当在单个数据框上有许多转换时，我们可以更容易地看到发生了哪些转换，因为我们只需读取函数名。此外，我们可以将转换函数重用于不同的数据框。例如，`viol`
    数据框包含有关餐厅安全违规的信息，同时也有一个 `date` 列。这意味着我们可以使用 `.pipe()` 重新使用时间戳解析函数，而无需编写额外的代码。方便！
- en: '[PRE39]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '|   | business_id | date | description | timestamp | year |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '|   | business_id | date | description | timestamp | year |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| **0** | 19 | 20171211 | Inadequate food safety knowledge or lack of ce...
    | 2017-12-11 | 2017 |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| **0** | 19 | 20171211 | 食品安全知识不足或缺乏ce… | 2017-12-11 | 2017 |'
- en: '| **1** | 19 | 20171211 | Unapproved or unmaintained equipment or utensils
    | 2017-12-11 | 2017 |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| **1** | 19 | 20171211 | 未批准或未维护的设备或器具 | 2017-12-11 | 2017 |'
- en: A different sort of transformation changes the shape of a dataframe by dropping
    unneeded columns, taking a subset of the rows, or rolling up the rows to a coarser
    granularity. We describe these structural changes next.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种转换方式是通过删除不需要的列、获取行的子集或将行滚动到更粗粒度来改变数据框的形状。接下来我们描述这些结构变化。
- en: Modifying Structure
  id: totrans-254
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 修改结构
- en: 'If a dataframe has an inconvenient structure, it can be difficult to do the
    analysis that we want. The wrangling process often reshapes the dataframe in some
    way to make the analysis easier and more natural. These changes can simply take
    a subset of the rows and/or columns from the table or change the table’s granularity
    in a more fundamental way. In this section, we use the techniques from [Chapter 6](ch06.html#ch-pandas)
    to show how to modify structure in the following ways:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个数据框的结构不方便，我们可能很难进行我们想要的分析。整理过程通常以某种方式重塑数据框，以使分析更容易和更自然。这些变化可以简单地从表中获取一部分行和/或列，或者以更基本的方式改变表的粒度。在本节中，我们使用
    [第 6 章](ch06.html#ch-pandas) 中的技术来展示如何以以下方式修改结构：
- en: Simplify the structure
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 简化结构
- en: If a dataframe has features that are not needed in our analysis, then we may
    want to drop these extraneous columns to make handling the dataframe easier. Or
    if we want to focus on a particular period of time or geographic area, we may
    want to take a subset of the rows (subsetting is covered in [Chapter 6](ch06.html#ch-pandas)).
    In [Chapter 8](ch08.html#ch-files), we’ll read into our dataframe a small set
    of features from the hundreds available in the DAWN survey because we are interested
    in understanding the patterns of types of ER visit by demographics of the patient.
    In [Chapter 10](ch10.html#ch-eda), we’ll restrict an analysis of home sale prices
    to one year and a few cities in an effort to reduce the impact of inflation and
    to better study the effect of location on sale price.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个数据框有不需要在我们分析中的特征，那么我们可能希望删除这些多余的列，以便更轻松地处理数据框。或者，如果我们想专注于特定时间段或地理区域，我们可能希望获取行的子集（子集在
    [第 6 章](ch06.html#ch-pandas) 中有所介绍）。在 [第 8 章](ch08.html#ch-files) 中，我们将从 DAWN
    调查中的数百个特征中读取数据框的一个小集合，因为我们有兴趣了解患者人口学特征对急诊访问类型模式的影响。在 [第 10 章](ch10.html#ch-eda)
    中，我们将限制对家庭销售价格的分析到一年和几个城市，以减少通货膨胀的影响，并更好地研究位置对销售价格的影响。
- en: Adjust the granularity
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 调整粒度
- en: In an earlier example in this chapter, CO[2] measurements were aggregated from
    monthly averages to yearly averages in order to better visualize annual trends.
    In the next section, we provide another example where we aggregate violation-level
    data to the inspection level so that it can be combined with the restaurant inspection
    scores. In both of these examples, we adjust the granularity of the dataframe
    to work with a coarser granularity by grouping together records and aggregating
    values. With the CO[2] measurements, we grouped the monthly values from the same
    year and then averaged them. Other common aggregations of a group are the number
    of records, sum, minimum, maximum, and first or last value in the group. The details
    of adjusting granularity of `pandas` dataframes can be found in [Chapter 6](ch06.html#ch-pandas),
    including how to group by multiple column values.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的早些示例中，CO[2]测量结果已从月平均值聚合到年平均值，以更好地可视化年度趋势。在接下来的部分中，我们提供另一个示例，其中我们将违规级别数据聚合到检查级别，以便与餐厅检查分数合并。在这两个示例中，我们调整了数据框的粒度，通过分组记录和聚合值来处理更粗略的粒度。对于CO[2]测量结果，我们对同一年的月值进行了分组然后求平均值。其他常见的组合方式包括记录数、总和、最小值、最大值以及组内的第一个或最后一个值。有关如何调整`pandas`数据框的详细信息可以在[第六章](ch06.html#ch-pandas)找到，包括如何按多列值进行分组。
- en: Address mixed granularity
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 处理混合粒度
- en: At times, a dataset might have mixed granularity, where records are at different
    levels of detail. A common case is in data provided by government agencies where
    data at the county and state levels are included in the same file. When this happens,
    we usually want to split the dataframe into two, one at the county level and the
    other at the state level. This makes county-level and state-level analyses much
    easier, even feasible, to perform.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，数据集可能存在混合粒度的情况，即记录处于不同的详细级别。政府机构提供的数据中常见的情况是在同一文件中包含县级和州级的数据。发生这种情况时，我们通常希望将数据框拆分为两个部分，一个是县级的，另一个是州级的。这样可以使县级和州级分析更加容易，甚至可行。
- en: Reshape the structure
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 重塑结构
- en: Data, especially from government sources, can be shared as pivot tables. These
    *wide* tables have data values as column names and are often difficult to use
    in analysis. We may need to reshape them into a *long* form. [Figure 9-2](#wide-vs-long)
    depicts the same data stored in both wide and long data tables. Each row of the
    wide data table maps to three rows in the long data table, as highlighted in the
    tables. Notice that in the wide data table, each row has three values, one for
    each month. In the long data table, each row only has a value for one month. Long
    data tables are generally easier to aggregate for future analysis. Because of
    this, long-form data is also frequently called [*tidy data*](https://doi.org/10.18637/jss.v059.i10).
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 数据，尤其是来自政府来源的数据，可以作为数据透视表进行共享。这些*宽*表格以数据值作为列名，通常在分析中难以使用。我们可能需要将它们重塑为*长*格式。[图 9-2](#wide-vs-long)
    展示了相同数据存储在宽和长数据表中的情况。宽数据表的每一行对应长数据表中的三行，如表中所示。请注意，在宽数据表中，每一行有三个值，分别对应每个月。而在长数据表中，每一行只有一个月的值。长数据表通常更容易聚合以供未来分析使用。因此，长格式数据也经常被称为[*整洁数据*](https://doi.org/10.18637/jss.v059.i10)。
- en: '![](assets/leds_0902.png)'
  id: totrans-264
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/leds_0902.png)'
- en: Figure 9-2\. An example of a wide data table (top) and a long data table (bottom)
    containing the same data
  id: totrans-265
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-2\. 宽数据表（顶部）和长数据表（底部）的示例，包含相同的数据
- en: 'To demonstrate reshaping, we can put the CO[2] data into a wide dataframe that
    is like a pivot table in shape. There is a column for each month and a row for
    each year:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示重塑，我们可以将CO[2]数据放入一个类似于数据透视表形状的宽数据框中。每个月份都有一列，每年都有一行：
- en: '[PRE40]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '| Mo | Yr | 1 | 2 | 3 | 4 | ... | 8 | 9 | 10 | 11 | 12 |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| Mo | Yr | 1 | 2 | 3 | 4 | ... | 8 | 9 | 10 | 11 | 12 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| **0** | 1959 | 315.62 | 316.38 | 316.71 | 317.72 | ... | 314.80 | 313.84
    | 313.26 | 314.8 | 315.58 |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| **0** | 1959 | 315.62 | 316.38 | 316.71 | 317.72 | ... | 314.80 | 313.84
    | 313.26 | 314.8 | 315.58 |'
- en: '| **1** | 1960 | 316.43 | 316.97 | 317.58 | 319.02 | ... | 315.91 | 314.16
    | 313.83 | 315.0 | 316.19 |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| **1** | 1960 | 316.43 | 316.97 | 317.58 | 319.02 | ... | 315.91 | 314.16
    | 313.83 | 315.0 | 316.19 |'
- en: '[PRE41]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The column headings are months, and the cell values in the grid are the CO[2]
    monthly averages. We can turn this dataframe back into a long, aka *tall*, dataframe,
    where the column names become a feature, called `month`, and the values in the
    grid are reorganized into a second feature, called `average`:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 列标题是月份，网格中的单元格值是CO[2]的月平均值。我们可以将此数据框转换回长格式，其中列名变为一个特征，称为`month`，并将网格中的值重新组织为第二个特征，称为`average`：
- en: '[PRE42]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '|   | Yr | month | average |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '|   | 年 | 月 | 平均值 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| **0** | 1959 | 1 | 315.62 |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| **0** | 1959 | 1 | 315.62 |'
- en: '| **1** | 1960 | 1 | 316.43 |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| **1** | 1960 | 1 | 316.43 |'
- en: '| **...** | ... | ... | ... |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| **...** | ... | ... | ... |'
- en: '| **22** | 1959 | 12 | 315.58 |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| **22** | 1959 | 12 | 315.58 |'
- en: '| **23** | 1960 | 12 | 316.19 |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| **23** | 1960 | 12 | 316.19 |'
- en: '[PRE43]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Notice that the data has been recaptured in its original shape (although the
    rows are not in their original order). Wide-form data is more common when we expect
    readers to look at the data table itself, like in an economics article or news
    story. But long-form data is more useful for data analysis. For instance, `co2_long`
    lets us write short `pandas` code to group by either year or month, while the
    wide-form data makes it difficult to group by year. The `.melt()` method is particularly
    useful for converting wide-form into long-form data.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 注意数据已恢复到其原始形状（尽管行不是原始顺序）。当我们期望读者查看数据表本身时，宽格式数据更常见，例如在经济文章或新闻报道中。但是，长格式数据对数据分析更有用。例如，`co2_long`允许我们编写简短的`pandas`代码，按年份或月份分组，而宽格式数据则使按年份分组变得困难。`.melt()`方法特别适用于将宽格式转换为长格式数据。
- en: These structural modifications have focused on a single table. However, we often
    want to combine information that is spread across multiple tables. In the next
    section, we combine the techniques introduced in this chapter to wrangle the restaurant
    inspection data and address joining tables.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结构修改已集中在单个表上。然而，我们经常希望将分散在多个表中的信息组合在一起。在下一节中，我们将结合本章介绍的技术来处理餐厅检查数据，并解决表的连接问题。
- en: 'Example: Wrangling Restaurant Safety Violations'
  id: totrans-285
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 示例：整理餐厅安全违规
- en: 'We wrap up this chapter with an example that demonstrates many data wrangling
    techniques. Recall from [Chapter 8](ch08.html#ch-files) that the San Francisco
    restaurant inspection data are stored in three tables: `bus` (for businesses/restaurants),
    `insp` (for inspections), and `viol` (for safety violations). The violations dataset
    contains detailed descriptions of violations found during an inspection. We would
    like to capture some of this information and connect it to the inspection score,
    which is an inspection-level dataset.'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本章结束时通过一个示例展示了许多数据整理技术。回顾[第8章](ch08.html#ch-files)，旧金山餐厅检查数据存储在三个表中：`bus`（企业/餐厅）、`insp`（检查）和`viol`（安全违规）。违规数据集包含检查期间发现的详细违规描述。我们希望捕捉部分信息，并将其与检查评分连接，这是一个检查级别的数据集。
- en: 'Our goal is to figure out the kinds of safety violations associated with lower
    restaurant safety scores. This example covers several key ideas in data wrangling
    related to changing structure:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是找出与较低餐厅安全评分相关的安全违规类型。这个例子涵盖了数据整理中与更改结构相关的几个关键概念：
- en: Filtering to focus on a narrower segment of data
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 过滤以便专注于数据的较窄部分
- en: Aggregation to modify the granularity of a table
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚合以修改表的粒度
- en: Joining to bring together information across tables
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 连接以汇总跨表信息
- en: Additionally, an important part of this example demonstrates how we transform
    text data into numeric quantities for analysis.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，本示例的一个重要部分展示了如何将文本数据转换为数值量进行分析。
- en: 'As a first step, let’s simplify the structure by reducing the data to inspections
    from one year. (Recall that this dataset contains four years of inspection information.)
    In the following code, we tally the number of records for each year in the inspections
    table:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 作为第一步，让我们通过将数据简化为一年的检查来简化结构。（回想一下，该数据集包含四年的检查信息。）在以下代码中，我们统计了检查表中每年的记录数：
- en: '[PRE44]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '[PRE45]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Reducing the data to cover one year of inspections will simplify our analysis.
    Later, if we want, we can return to carry out an analysis with all four years
    of data.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据减少到一年的检查将简化我们的分析。稍后，如果需要，我们可以返回并使用所有四年的数据进行分析。
- en: Narrowing the Focus
  id: totrans-296
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 缩小焦点
- en: 'We restrict our data wrangling to inspections that took place in 2016\. Here,
    we can use the `pipe` function again in order to apply the same reshaping to both
    the inspections and violations dataframes:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将数据整理限定在2016年进行的检查中。在这里，我们可以再次使用`pipe`函数，以便对检查和违规数据框应用相同的重塑：
- en: '[PRE46]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '[PRE47]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '|   | business_id | score | date | type | timestamp | year |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '|   | business_id | score | date | type | timestamp | year |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| **0** | 19 | 94 | 20160513 | routine | 2016-05-13 | 2016 |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| **0** | 19 | 94 | 20160513 | routine | 2016-05-13 | 2016 |'
- en: '| **3** | 24 | 98 | 20161005 | routine | 2016-10-05 | 2016 |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| **3** | 24 | 98 | 20161005 | routine | 2016-10-05 | 2016 |'
- en: '| **4** | 24 | 96 | 20160311 | routine | 2016-03-11 | 2016 |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| **4** | 24 | 96 | 20160311 | routine | 2016-03-11 | 2016 |'
- en: '| **6** | 45 | 78 | 20160104 | routine | 2016-01-04 | 2016 |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| **6** | 45 | 78 | 20160104 | routine | 2016-01-04 | 2016 |'
- en: '| **9** | 45 | 84 | 20160614 | routine | 2016-06-14 | 2016 |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| **9** | 45 | 84 | 20160614 | routine | 2016-06-14 | 2016 |'
- en: 'In [Chapter 8](ch08.html#ch-files), we found that `business_id` and `timestamp`
    together uniquely identify the inspections (with a couple of exceptions). We also
    see here that restaurants can receive multiple inspections in a year—business
    #24 had two inspections in 2016, one in March and another in October.'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第8章](ch08.html#ch-files)中，我们发现`business_id`和`timestamp`共同唯一标识了检查（除了几个例外）。我们还看到这里，餐馆在一年内可能接受多次检查——例如，商家＃24在2016年进行了两次检查，分别在三月和十月。
- en: 'Next, let’s look at a few records from the violations table:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看违规表中的几条记录：
- en: '[PRE48]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '|   | business_id | date | description | timestamp | year |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '|   | business_id | date | description | timestamp | year |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| **2** | 19 | 20160513 | Unapproved or unmaintained equipment or utensi...
    | 2016-05-13 | 2016 |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| **2** | 19 | 20160513 | 未批准或未维护的设备或器具... | 2016-05-13 | 2016 |'
- en: '| **3** | 19 | 20160513 | Unclean or degraded floors walls or ceilings ...
    | 2016-05-13 | 2016 |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '| **3** | 19 | 20160513 | 地板、墙壁或天花板不洁或破损... | 2016-05-13 | 2016 |'
- en: '| **4** | 19 | 20160513 | Food safety certificate or food handler card n...
    | 2016-05-13 | 2016 |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '| **4** | 19 | 20160513 | 食品安全证书或食品处理者证未... | 2016-05-13 | 2016 |'
- en: '| **6** | 24 | 20161005 | Unclean or degraded floors walls or ceilings ...
    | 2016-10-05 | 2016 |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '| **6** | 24 | 20161005 | 地板、墙壁或天花板不洁或破损... | 2016-10-05 | 2016 |'
- en: '| **7** | 24 | 20160311 | Unclean or degraded floors walls or ceilings ...
    | 2016-03-11 | 2016 |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '| **7** | 24 | 20160311 | 地板、墙壁或天花板不洁或破损... | 2016-03-11 | 2016 |'
- en: Notice that the first few records are for the same restaurant. If we want to
    bring violation information into the inspections table, we need to address the
    different granularities of these tables. One approach is to aggregate the violations
    in some way. We discuss this next.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，前几条记录是同一家餐厅的。如果我们想将违规信息带入检查表中，我们需要处理这些表的不同粒度。一种方法是以某种方式聚合违规行为。我们将在接下来讨论这一点。
- en: Aggregating Violations
  id: totrans-318
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 聚合违规行为
- en: 'One simple aggregation of the violations is to count them and add that count
    to the inspections data table. To find the number of violations at an inspection,
    we can group the violations by `business_id` and `timestamp` and then find the
    size of each group. Essentially, this grouping changes the granularity of violations
    to an inspection level:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 将违规行为的一个简单聚合是计算它们的数量，并将该计数添加到检查数据表中。为了找出检查中的违规次数，我们可以按`business_id`和`timestamp`对违规行为进行分组，然后找出每个组的大小。基本上，这种分组将违规行为的粒度变更为检查级别：
- en: '[PRE49]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '|   | business_id | timestamp | num_vio |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '|   | business_id | timestamp | num_vio |'
- en: '| --- | --- | --- | --- |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| **0** | 19 | 2016-05-13 | 3 |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '| **0** | 19 | 2016-05-13 | 3 |'
- en: '| **1** | 24 | 2016-03-11 | 2 |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '| **1** | 24 | 2016-03-11 | 2 |'
- en: '| **2** | 24 | 2016-10-05 | 1 |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '| **2** | 24 | 2016-10-05 | 1 |'
- en: 'Now we need to merge this new information with `ins2016`. Specifically, we
    want to *left-join* `ins2016` with `num_vios` because there could be inspections
    that do not have any violations and we don’t want to lose them:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要将这些新信息与`ins2016`合并。具体来说，我们想要*左连接*`ins2016`和`num_vios`，因为可能有些检查没有任何违规行为，我们不希望丢失它们：
- en: '[PRE50]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '|   | business_id | score | date | type | timestamp | year | num_vio |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
  zh: '|   | business_id | score | date | type | timestamp | year | num_vio |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| **0** | 19 | 94 | 20160513 | routine | 2016-05-13 | 2016 | 3.0 |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '| **0** | 19 | 94 | 20160513 | routine | 2016-05-13 | 2016 | 3.0 |'
- en: '| **1** | 24 | 98 | 20161005 | routine | 2016-10-05 | 2016 | 1.0 |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
  zh: '| **1** | 24 | 98 | 20161005 | routine | 2016-10-05 | 2016 | 1.0 |'
- en: '| **2** | 24 | 96 | 20160311 | routine | 2016-03-11 | 2016 | 2.0 |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '| **2** | 24 | 96 | 20160311 | routine | 2016-03-11 | 2016 | 2.0 |'
- en: '| **...** | ... | ... | ... | ... | ... | ... | ... |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '| **...** | ... | ... | ... | ... | ... | ... | ... |'
- en: '| **5440** | 90096 | 91 | 20161229 | routine | 2016-12-29 | 2016 | 2.0 |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '| **5440** | 90096 | 91 | 20161229 | routine | 2016-12-29 | 2016 | 2.0 |'
- en: '| **5441** | 90268 | 100 | 20161229 | routine | 2016-12-29 | 2016 | NaN |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
  zh: '| **5441** | 90268 | 100 | 20161229 | routine | 2016-12-29 | 2016 | NaN |'
- en: '| **5442** | 90269 | 100 | 20161229 | routine | 2016-12-29 | 2016 | NaN |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
  zh: '| **5442** | 90269 | 100 | 20161229 | routine | 2016-12-29 | 2016 | NaN |'
- en: '[PRE51]'
  id: totrans-337
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'When there are no violations at an inspection, the feature `num_vio` has a
    missing value (`NaN`). We can check how many values are missing:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 在检查时如果没有违规，特征`num_vio`将会是缺失值(`NaN`)。我们可以检查有多少缺失的数值：
- en: '[PRE52]'
  id: totrans-339
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '[PRE53]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'About 15% of restaurant inspections in 2016 had no safety violations recorded.
    We can correct these missing values by setting them to 0 if the restaurant had
    a perfect safety score of 100\. This is an example of deductive imputation since
    we’re using domain knowledge to fill in missing values:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 关于2016年的餐厅检查，约15%没有记录安全违规。如果餐厅的安全得分为100，我们可以通过将它们设置为0来修正这些缺失值。这是归纳填充的一个例子，因为我们使用领域知识来填补缺失值：
- en: '[PRE54]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'We can count the number of inspections with missing violation counts again:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以再次统计有缺失违规数量的检查次数：
- en: '[PRE55]'
  id: totrans-344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '[PRE56]'
  id: totrans-345
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: We have corrected a large number of missing values. With further investigation,
    we find that some of the businesses have inspection dates that are close but don’t
    quite match. We could do a fuzzy match where inspections with dates that are only
    one or two days apart are matched. But for now, we just leave them as `NaN`.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经纠正了大量缺失的数值。进一步调查后，我们发现一些企业的检查日期接近但不完全匹配。我们可以进行模糊匹配，将日期仅相差一两天的检查归为一类。但目前，我们将它们留为空值`NaN`。
- en: 'Let’s examine the relationship between the number of violations and the inspection
    score:'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来研究违规数量与检查得分之间的关系：
- en: '![](assets/leds_09in08.png)'
  id: totrans-348
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/leds_09in08.png)'
- en: As we might expect, there is a negative relationship between the inspection
    score and the number of violations. We can also see variability in scores. The
    variability in scores grows with the number of violations. It appears that some
    violations are more serious than others and have a greater impact on the score.
    We extract information about the kinds of violations next.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们预期的那样，检查得分与违规数量之间存在负相关关系。我们还可以看到得分的变异性。随着违规数量的增加，得分的变异性也增加。似乎某些违规比其他违规更为严重，对得分的影响更大。接下来我们提取违规种类的信息。
- en: Extracting Information from Violation Descriptions
  id: totrans-350
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从违规描述中提取信息
- en: 'We saw earlier that the feature description in the violations dataframe has
    a lot of text, including information in square brackets about when the violation
    was corrected. We can tally the descriptions and examine the most common violations:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前看到违规数据框架中的特征描述有很多文本，包括方括号中关于何时纠正违规的信息。我们可以汇总描述并查看最常见的违规情况：
- en: '[PRE57]'
  id: totrans-352
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: '|   | description |'
  id: totrans-353
  prefs: []
  type: TYPE_TB
  zh: '|   | 描述 |'
- en: '| --- | --- |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| **Unclean or degraded floors walls or ceilings** | 161 |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
  zh: '| **地板、墙壁或天花板不干净或已磨损** | 161 |'
- en: '| **Unapproved or unmaintained equipment or utensils** | 99 |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
  zh: '| **未批准或未维护的设备或器具** | 99 |'
- en: '| **Moderate risk food holding temperature** | 95 |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
  zh: '| **中度风险的食品持有温度** | 95 |'
- en: '| **Inadequate and inaccessible handwashing facilities** | 93 |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
  zh: '| **清洗不充分或无法接近的洗手设施** | 93 |'
- en: '| **Inadequately cleaned or sanitized food contact surfaces** | 92 |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
  zh: '| **清洁或消毒不充分的食品接触表面** | 92 |'
- en: '| **Improper food storage** | 81 |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
  zh: '| **食品存储不当** | 81 |'
- en: '| **Wiping cloths not clean or properly stored or inadequate sanitizer** |
    71 |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
  zh: '| **擦拭布不干净或存放不当或消毒剂不足** | 71 |'
- en: '| **Food safety certificate or food handler card not available** | 64 |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
  zh: '| **食品安全证书或持证食品处理人员卡不可用** | 64 |'
- en: '| **Moderate risk vermin infestation** | 58 |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
  zh: '| **中度风险的害虫侵扰** | 58 |'
- en: '| **Foods not protected from contamination** | 56 |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
  zh: '| **食品未受到污染保护** | 56 |'
- en: '| **Unclean nonfood contact surfaces** | 54 |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
  zh: '| **非食品接触表面不干净** | 54 |'
- en: '| **Inadequate food safety knowledge or lack of certified food safety manager**
    | 52 |'
  id: totrans-366
  prefs: []
  type: TYPE_TB
  zh: '| **食品安全知识不足或缺乏持证食品安全经理** | 52 |'
- en: '| **Permit license or inspection report not posted** | 41 |'
  id: totrans-367
  prefs: []
  type: TYPE_TB
  zh: '| **许可证或检查报告未张贴** | 41 |'
- en: '| **Improper storage of equipment utensils or linens** | 41 |'
  id: totrans-368
  prefs: []
  type: TYPE_TB
  zh: '| **设备、器具或亚麻布存储不当** | 41 |'
- en: '| **Low risk vermin infestation** | 34 |'
  id: totrans-369
  prefs: []
  type: TYPE_TB
  zh: '| **低风险的害虫侵扰** | 34 |'
- en: Reading through these wordy descriptions, we see that some are related to the
    cleanliness of facilities, others to food storage, and still others to cleanliness
    of the staff.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 通过阅读这些冗长的描述，我们发现其中一些与设施的清洁有关，另一些与食品存储有关，还有一些与员工的清洁有关。
- en: Since there are many types of violations, we can try to group them together
    into larger categories. One way to do this is to create a simple boolean flag
    depending on whether the text contains a special term, like *vermin*, *hand*,
    or *high risk*.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 由于有许多类型的违规行为，我们可以尝试将它们分组到更大的类别中。一种方法是根据文本是否包含特定术语（如*害虫*、*手*或*高风险*）创建一个简单的布尔标志。
- en: 'With this approach, we create eight new features for different categories of
    violations. Don’t worry about the particular details of the code for now—this
    code uses regular expressions, covered in [Chapter 13](ch13.html#ch-text). The
    important idea is that this code creates features containing `True` or `False`
    based on whether the violation description contains specific words:'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方法，我们为不同类别的违规行为创建了八个新特征。暂时不必担心代码的具体细节——此代码使用了正则表达式，详见[第13章](ch13.html#ch-text)。重要的是，此代码根据违规描述中是否包含特定单词创建包含`True`或`False`的特征：
- en: '[PRE58]'
  id: totrans-373
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: '[PRE59]'
  id: totrans-374
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: '|   | business_id | timestamp | high_risk | clean | ... | storage | permit
    | non_food_surface | human |'
  id: totrans-375
  prefs: []
  type: TYPE_TB
  zh: '|   | business_id | timestamp | high_risk | clean | ... | storage | permit
    | non_food_surface | human |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-376
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| **2** | 19 | 2016-05-13 | False | False | ... | False | False | False | False
    |'
  id: totrans-377
  prefs: []
  type: TYPE_TB
  zh: '| **2** | 19 | 2016-05-13 | False | False | ... | False | False | False | False
    |'
- en: '| **3** | 19 | 2016-05-13 | False | True | ... | False | False | True | False
    |'
  id: totrans-378
  prefs: []
  type: TYPE_TB
  zh: '| **3** | 19 | 2016-05-13 | False | True | ... | False | False | True | False
    |'
- en: '| **4** | 19 | 2016-05-13 | False | False | ... | False | True | False | True
    |'
  id: totrans-379
  prefs: []
  type: TYPE_TB
  zh: '| **4** | 19 | 2016-05-13 | False | False | ... | False | True | False | True
    |'
- en: '| **...** | ... | ... | ... | ... | ... | ... | ... | ... | ... |'
  id: totrans-380
  prefs: []
  type: TYPE_TB
  zh: '| **...** | ... | ... | ... | ... | ... | ... | ... | ... | ... |'
- en: '| **38147** | 89900 | 2016-12-06 | False | False | ... | False | False | False
    | False |'
  id: totrans-381
  prefs: []
  type: TYPE_TB
  zh: '| **38147** | 89900 | 2016-12-06 | False | False | ... | False | False | False
    | False |'
- en: '| **38220** | 90096 | 2016-12-29 | False | False | ... | False | False | False
    | False |'
  id: totrans-382
  prefs: []
  type: TYPE_TB
  zh: '| **38220** | 90096 | 2016-12-29 | False | False | ... | False | False | False
    | False |'
- en: '| **38221** | 90096 | 2016-12-29 | False | True | ... | False | False | True
    | False |'
  id: totrans-383
  prefs: []
  type: TYPE_TB
  zh: '| **38221** | 90096 | 2016-12-29 | False | True | ... | False | False | True
    | False |'
- en: '[PRE60]'
  id: totrans-384
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: Now that we have these new features in `vio_ctg`, we can find out whether certain
    violation categories are more impactful than others. For example, are restaurant
    scores impacted more for vermin-related violations than permit-related violations?
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们在`vio_ctg`中有了这些新特征，我们可以找出某些违规类别是否比其他类别更具影响力。例如，餐厅的评分是否更多受到与害虫相关的违规行为的影响，而不是与许可相关的违规行为？
- en: 'To do this, we want to first count up the violations per business. Then we
    can merge this information with the inspection information. First, let’s sum the
    number of violations for each business:'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 要做到这一点，我们首先要统计每个企业的违规次数。然后我们可以将此信息与检查信息合并。首先，让我们对每个企业的违规次数进行求和：
- en: '[PRE61]'
  id: totrans-387
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: '|   | business_id | timestamp | high_risk | clean | ... | storage | permit
    | non_food_surface | human |'
  id: totrans-388
  prefs: []
  type: TYPE_TB
  zh: '|   | business_id | timestamp | high_risk | clean | ... | storage | permit
    | non_food_surface | human |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-389
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| **0** | 19 | 2016-05-13 | 0 | 1 | ... | 0 | 1 | 1 | 1 |'
  id: totrans-390
  prefs: []
  type: TYPE_TB
  zh: '| **0** | 19 | 2016-05-13 | 0 | 1 | ... | 0 | 1 | 1 | 1 |'
- en: '| **1** | 24 | 2016-03-11 | 0 | 2 | ... | 0 | 0 | 2 | 0 |'
  id: totrans-391
  prefs: []
  type: TYPE_TB
  zh: '| **1** | 24 | 2016-03-11 | 0 | 2 | ... | 0 | 0 | 2 | 0 |'
- en: '| **2** | 24 | 2016-10-05 | 0 | 1 | ... | 0 | 0 | 1 | 0 |'
  id: totrans-392
  prefs: []
  type: TYPE_TB
  zh: '| **2** | 24 | 2016-10-05 | 0 | 1 | ... | 0 | 0 | 1 | 0 |'
- en: '| **...** | ... | ... | ... | ... | ... | ... | ... | ... | ... |'
  id: totrans-393
  prefs: []
  type: TYPE_TB
  zh: '| **...** | ... | ... | ... | ... | ... | ... | ... | ... | ... |'
- en: '| **4803** | 89790 | 2016-11-29 | 0 | 0 | ... | 0 | 0 | 0 | 1 |'
  id: totrans-394
  prefs: []
  type: TYPE_TB
  zh: '| **4803** | 89790 | 2016-11-29 | 0 | 0 | ... | 0 | 0 | 0 | 1 |'
- en: '| **4804** | 89900 | 2016-12-06 | 0 | 0 | ... | 0 | 0 | 0 | 0 |'
  id: totrans-395
  prefs: []
  type: TYPE_TB
  zh: '| **4804** | 89900 | 2016-12-06 | 0 | 0 | ... | 0 | 0 | 0 | 0 |'
- en: '| **4805** | 90096 | 2016-12-29 | 0 | 1 | ... | 0 | 0 | 1 | 0 |'
  id: totrans-396
  prefs: []
  type: TYPE_TB
  zh: '| **4805** | 90096 | 2016-12-29 | 0 | 1 | ... | 0 | 0 | 1 | 0 |'
- en: '[PRE62]'
  id: totrans-397
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Once again, we use a left join to merge these new features into the inspection-level
    dataframe. And for the special case of a score of 100, we set all of the new features
    to `0`:'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们使用左连接将这些新特征合并到检查级别的数据框中。对于得分为100的特殊情况，我们将所有新特征设置为`0`：
- en: '[PRE63]'
  id: totrans-399
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: '[PRE64]'
  id: totrans-400
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: '|   | business_id | timestamp | score | high_risk | ... | storage | permit
    | non_food_surface | human |'
  id: totrans-401
  prefs: []
  type: TYPE_TB
  zh: '|   | business_id | timestamp | score | high_risk | ... | storage | permit
    | non_food_surface | human |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-402
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| 0 | 19 | 2016-05-13 | 94 | 0.0 | ... | 0.0 | 1.0 | 1.0 | 1.0 |'
  id: totrans-403
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 19 | 2016-05-13 | 94 | 0.0 | ... | 0.0 | 1.0 | 1.0 | 1.0 |'
- en: '| 1 | 24 | 2016-10-05 | 98 | 0.0 | ... | 0.0 | 0.0 | 1.0 | 0.0 |'
  id: totrans-404
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 24 | 2016-10-05 | 98 | 0.0 | ... | 0.0 | 0.0 | 1.0 | 0.0 |'
- en: '| 2 | 24 | 2016-03-11 | 96 | 0.0 | ... | 0.0 | 0.0 | 2.0 | 0.0 |'
  id: totrans-405
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 24 | 2016-03-11 | 96 | 0.0 | ... | 0.0 | 0.0 | 2.0 | 0.0 |'
- en: '[PRE65]'
  id: totrans-406
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'To see how each violation category relates to the score, we can make a collection
    of box plots that compare the score distributions with and without each violation.
    Since our focus here is on the data’s patterns, not the visualization code, we
    hide the code (you can see it larger [online](https://oreil.ly/go29H)):'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看每个违规类别与分数的关系，我们可以制作一系列箱线图，比较包含和不包含每个违规的分数分布。由于我们这里关注的是数据的模式，而不是可视化代码，我们隐藏了代码（您可以在[网上](https://oreil.ly/go29H)查看更大的图像）：
- en: '![](assets/leds_09in09.png)'
  id: totrans-408
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/leds_09in09.png)'
- en: Summary
  id: totrans-409
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概要
- en: Data wrangling is an essential part of data analysis. Without it, we risk overlooking
    problems in data that can have major consequences for our future analysis. This
    chapter covered several important data wrangling steps that we use in nearly every
    analysis.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 数据整理是数据分析的重要组成部分。如果没有数据整理，我们可能会忽略数据中可能导致未来分析严重后果的问题。本章涵盖了几个在几乎每一次分析中都会使用的重要数据整理步骤。
- en: 'We described what to look for in a dataset after we’ve read it into a dataframe.
    Quality checks help us spot problems in the data. To find bad and missing values,
    we can take many approaches:'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 在将数据读入数据框后，我们描述了在数据集中寻找什么。质量检查帮助我们发现数据中的问题。为了找到不良和缺失值，我们可以采取许多方法：
- en: Check summary statistics, distributions, and value counts. [Chapter 10](ch10.html#ch-eda)
    provides examples and guidance on how to go about checking the quality of your
    data using visualizations and summary statistics. We briefly mentioned a few approaches
    here. A table of counts of unique values in a feature can uncover unexpected encodings
    and lopsided distributions, where one option is a rare occurrence. Percentiles
    can be helpful in revealing the proportion of values with unusually high (or low)
    values.
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查摘要统计数据、分布和值计数。[第10章](ch10.html#ch-eda) 提供了如何使用可视化和摘要统计检查数据质量的示例和指导。我们在这里简要提到了几种方法。特征中唯一值计数的表格可以揭示意外编码和倾斜分布，其中一个选项是罕见的。百分位数可以帮助揭示具有异常高（或低）值的比例。
- en: Use logical expressions to identify records with values that are out of range
    or relationships that are out of whack. Simply computing the number of records
    that do not pass the quality check can quickly reveal the size of the problem.
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用逻辑表达式来识别数值超出范围或关系失调的记录。仅计算未通过质量检查的记录数量可以快速显示问题的规模。
- en: Examine the whole record for those records with problematic values in a particular
    feature. At times, an entire record is garbled when, for example, a comma is misplaced
    in a CSV-formatted file. Or the record might represent an unusual situation (such
    as ranches being included in data on house sales), and you will need to decide
    whether it should be included in your analysis.
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查具有特定特征中问题值的整个记录。有时，在CSV格式文件中逗号放错位置时，整个记录会混乱。或者该记录可能代表一个不寻常的情况（例如在房屋销售数据中包含牧场），您需要决定是否应该包含在您的分析中。
- en: Refer to an external source to figure out if there’s a reason for the anomaly.
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参考外部来源以找出异常的原因。
- en: The biggest takeaway for this chapter is to be curious about your data. Look
    for clues that can reveal the quality of your data. The more evidence you find,
    the more confidence you will have in your findings. And if you uncover problems,
    dig deeper. Try to understand and explain any unusual phenomena. A good understanding
    of your data will help you assess whether an issue that you found is small and
    can be ignored or corrected, or whether it poses a serious limitation on the usefulness
    of your data. This curiosity mindset is closely connected to exploratory data
    analysis, the topic of the next chapter.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的最大收获是对数据保持好奇心。寻找能揭示数据质量的线索。找到的证据越多，您对发现的结果越有信心。如果发现问题，请深入挖掘。尝试理解和解释任何异常现象。对数据的深入了解将帮助您评估您发现的问题是可以忽略或修正的小问题，还是可能严重影响数据可用性的问题。这种好奇心思维与探索性数据分析密切相关，这是下一章的主题。
- en: ^([1](ch09.html#id1153-marker)) This means that each uses 64 bits of memory
    for each value and that each is accurate to the nanosecond (or ns, for short).
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch09.html#id1153-marker)) 这意味着每个值使用64位内存，并且精确到纳秒（简称ns）。
