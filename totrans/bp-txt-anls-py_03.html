<html><head></head><body><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 3. Scraping Websites and Extracting Data"><div class="chapter" id="ch-scraping">
<h1><span class="label">Chapter 3. </span>Scraping Websites and Extracting Data</h1>

<p>Often, it will happen that you visit a website and find the content interesting. If there are only a few pages, it’s possible to read everything on your own. But as soon as there is a considerable amount of content, reading everything on your own will not be <span class="keep-together">possible.</span></p>

<p>To use the powerful text analytics blueprints described in this book, you have to acquire the content first. <a contenteditable="false" data-type="indexterm" data-primary="scraping websites" data-secondary="about" id="ch3_term1"/>Most websites won’t have a “download all content” button, so we have to find a clever way to download (“scrape”) the pages.</p>

<p>Usually we are mainly interested in the content part of each individual web page, less so in navigation, etc. As soon as we have the data locally available, we can use powerful extraction techniques to dissect the pages into elements such as title, content, and also some meta-information (publication date, author, and so on).</p>

<section data-type="sect1" data-pdf-bookmark="What You’ll Learn and What We’ll Build"><div class="sect1" id="idm45634212279304">
<h1>What You’ll Learn and What We’ll Build</h1>


<p>In this chapter, we will <a contenteditable="false" data-type="indexterm" data-primary="Reuters News Archive" id="idm45634212277768"/><a contenteditable="false" data-type="indexterm" data-primary="datasets, examples of" data-secondary="Reuters News Archive" id="idm45634212276664"/>show you how to acquire HTML data from websites and use powerful tools to extract the content from these HTML files. We will show this with content from one specific data source, the Reuters news archive.</p>

<p>In the first step, we will <a contenteditable="false" data-type="indexterm" data-primary="downloading HTML pages" id="idm45634212274520"/><a contenteditable="false" data-type="indexterm" data-primary="HTML pages, downloading" id="idm45634212273416"/>download single HTML files and extract data from each one with different methods.</p>

<p>Normally, you will not be interested in single pages. Therefore, we will build a blueprint solution. We will download and analyze a news archive page (which contains links to all articles). After completing this, we know the URLs of the referred documents. Then you can download the documents at the URLs and extract their content to a Pandas <code>DataFrame</code>.</p>

<p>After studying this chapter, you will have a good overview of methods that download HTML and extract data. You will be familiar with the different extraction methods for content provided by Python. We will have seen a complete example for downloading and extracting data. For your own work, you will be able to select an appropriate framework. In this chapter, we will provide standard blueprints for extracting often-used elements that you can reuse.</p>
</div></section>

<section data-type="sect1" data-pdf-bookmark="Scraping and Data Extraction"><div class="sect1" id="idm45634212269944">
<h1>Scraping and Data Extraction</h1>

<p>Scraping websites is a <a contenteditable="false" data-type="indexterm" data-primary="text data extraction from web" id="idm45634212268520"/><a contenteditable="false" data-type="indexterm" data-primary="URL generation" id="idm45634212267320"/><a contenteditable="false" data-type="indexterm" data-primary="scraping websites" data-secondary="downloading HTML pages for" id="idm45634212266216"/><a contenteditable="false" data-type="indexterm" data-primary="scraping websites" data-secondary="URL generation for" id="idm45634212264824"/>complex process consisting of typically three different phases, as illustrated in <a data-type="xref" href="#outline_of_scraping_process">Figure 3-1</a>.</p>

<figure><div id="outline_of_scraping_process" class="figure"><img src="Images/btap_0301.jpg" width="1315" height="596"/>
<h6><span class="label">Figure 3-1. </span>Outline of scraping process.</h6>
</div></figure>

<p>In the first step, we have to generate all interesting URLs of a website. Afterward, we can use different tools to download the pages from the corresponding URLs. Finally, we will extract the “net” data from the downloaded pages; we can also use different strategies in this phase. Of course, it is crucial to permanently save extracted data. <span class="keep-together">In this</span> chapter, we use a <a contenteditable="false" data-type="indexterm" data-primary="DataFrame (Pandas)" id="idm45634212259272"/><a contenteditable="false" data-type="indexterm" data-primary="Pandas library" data-secondary="dataframes in" id="idm45634212232232"/>Pandas <code>DataFrame</code> that offers a variety of persistence <span class="keep-together">mechanisms</span>.</p>

<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45634212229416">
<h5>Scraping Is Not Always Necessary: Sources for Existing Datasets</h5>

<p>Often, datasets <a contenteditable="false" data-type="indexterm" data-primary="datasets, examples of" data-secondary="overview of" id="idm45634212227624"/>are already available, and you can download them as a whole. This <a contenteditable="false" data-type="indexterm" data-primary="use cases" data-secondary="for scraping websites" data-secondary-sortas="scraping websites" id="idm45634212226088"/><a contenteditable="false" data-type="indexterm" data-primary="scraping websites" data-secondary="use cases for" id="idm45634212224440"/>might be special datasets that have a focus on specific content or use cases. If you are interested in <a contenteditable="false" data-type="indexterm" data-primary="datasets, examples of" data-secondary="for sentiment analysis" data-secondary-sortas="sentiment analysis" id="idm45634212222824"/><a contenteditable="false" data-type="indexterm" data-primary="sentiment analysis of text data" data-secondary="datasets for" id="idm45634212221176"/>sentiment detection, both the comment dataset from the <a href="https://oreil.ly/mljhA">Internet Movie Database</a> and the <a href="https://oreil.ly/CszzV">Rotten Tomatoes dataset</a> for movies are used quite frequently. Using more structured data, the <a href="https://oreil.ly/3la0V">Yelp dataset</a> contains both text and metadata.</p>

<p>Apart from these domain-specific datasets, there more <a contenteditable="false" data-type="indexterm" data-primary="Common Crawl dataset" id="idm45634212217208"/>generic ones like <a href="http://commoncrawl.org">Common Crawl</a>. To generate this, almost each month, parts of the whole Internet are crawled and stored. For compression purposes, data is stored in a special format called WARC. The dataset is really huge, containing roughly two billion web pages. You can freely download the archive or access it directly via an S3 bucket on Amazon Web Services (AWS).</p>

<p>For experimenting, one of the simplest and directly accessible datasets is the so-called <a contenteditable="false" data-type="indexterm" data-primary="newsgroups dataset" id="idm45634212214376"/><a contenteditable="false" data-type="indexterm" data-primary="scikit-learn" data-secondary="dataset in" id="idm45634212213272"/>newsgroups dataset, which is directly built into <a href="https://oreil.ly/h2mIl">scikit-learn</a>. Although it is quite dated, it is useful to have a dataset for immediate experimentation without having to download and transform anything.</p>

<p>As we want to show how to download and extract HTML content from the Internet in this chapter, these existing datasets <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch3_term1" id="idm45634212210312"/>will not be used.</p>
</div></aside>
</div></section>

<section data-type="sect1" data-pdf-bookmark="Introducing the Reuters News Archive"><div class="sect1" id="idm45634212208680">
<h1>Introducing the Reuters News Archive</h1>

<p>Let’s assume <a contenteditable="false" data-type="indexterm" data-primary="scraping websites" data-secondary="with Reuters News Archive" data-secondary-sortas="Reuters News Archive" id="idm45634212207112"/><a contenteditable="false" data-type="indexterm" data-primary="datasets, examples of" data-secondary="Reuters News Archive" id="idm45634212205464"/><a contenteditable="false" data-type="indexterm" data-primary="news archives" id="idm45634212204088"/><a contenteditable="false" data-type="indexterm" data-primary="Reuters News Archive" id="idm45634212202984"/>we are interested in analyzing the current and past political situation and are looking for an appropriate dataset. We want to find some trends, uncover when a word or topic was introduced for the first time, and so on. For this, our aim is to convert the documents to a Pandas <code>DataFrame</code>.</p>

<p>Obviously, news headlines and articles are well suited as a database for these requirements. If possible, we should find an archive that goes back a few years, ideally even some decades. </p>

<p>Some newspapers have such archives, but most of them will also have a certain political bias that we want to avoid if possible. We are looking for content that is as neutral as possible.</p>

<p>This is why we decided to use the Reuters news archive. Reuters is an international news organization and works as a news agency; in other words, it provides news to many different publications. It was founded more than a hundred years ago and has a lot of news articles in its archives. It’s a good source of content for many reasons:</p>

<ul>
	<li>It is politically neutral.</li>
	<li>It has a big archive of news.</li>
	<li>News articles are categorized in sections.</li>
	<li>The focus is not on a specific region.</li>
	<li>Almost everybody will find some interesting headlines there.</li>
	<li>It has a liberal policy for downloading data.</li>
	<li>It is very well connected, and the website itself is fast.</li>
</ul>

<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45634212195304">
<h5>Searching for Data</h5>

<p>If we are interested in <a contenteditable="false" data-type="indexterm" data-primary="search engines for data sources" id="idm45634212193736"/><a contenteditable="false" data-type="indexterm" data-primary="internet search engines" id="idm45634212192568"/>news, using the archive of a large newspaper or news agency is an obvious solution. Sometimes, however, finding appropriate content is not easy. Let’s assume we are interested in diggers and want to download content for that. As we do not know which websites carry the corresponding content, we have to find them first.</p>

<p>Fortunately, today’s internet search engines use extremely sophisticated algorithms and yield good results when you provide them with specific search queries. If data is (openly) available on the web, search engines will almost certainly be able to find it. One of the most difficult parts of searching for data is knowing which search terms <span class="keep-together">to use.</span></p>

<p>Running various search terms across different search engines, we can use statistics (or more sophisticated methods) to find the most relevant websites. This requires the repetition of many search requests and counting the number of results. APIs are helpful for running these searches and are available for the big search engines (like Google and Bing).</p>

<p>After finding a useful data source, you must check its number of pages. Search engines can also help in this case, by using specialized idioms as your search term:</p>

<ul>
	<li>Use <code>site:domain</code> to restrict searches to a single domain.</li>
	<li>In more specific cases, <code>inurl:/path/</code> can also be helpful, but the syntax differs among different search engines.</li>
	<li>The approaches can be combined; let’s assume you want to search for Python articles with NumPy in the URL and compare stackoverflow.com with quora.com. The corresponding search terms would be <code>site:quora.com python inurl:numpy</code> versus <code>site:stackoverflow.com python inurl:numpy</code>.</li>
	<li>More information is available at the help pages of the search engines, such as <a class="orm:hideurl" href="https://oreil.ly/PWHOS">Google’s Programmable Search Engine page</a>.</li>
</ul>

<p>Note that search engines are built for interactive operations. If you perform too many (automated) searches, the engines will start sending captchas and eventually block you.</p>
</div></aside>


</div></section>

<section data-type="sect1" class="pagebreak-before less_space" data-pdf-bookmark="URL Generation"><div class="sect1" id="idm45634212182296">
<h1>URL Generation</h1>

<p>For downloading content from the Reuters’ archive, we <a contenteditable="false" data-type="indexterm" data-primary="scraping websites" data-secondary="URL generation for" id="ch3_term2"/><a contenteditable="false" data-type="indexterm" data-primary="URL generation " id="ch3_term3"/>need to know the URLs of the content pages. After we know the URLs, the download itself is easy as there are powerful Python tools available to accomplish that.</p>

<p>At first sight it might seem easy to find URLs, but in practice it is often not so simple. The process is called <em>URL generation</em>, and in many crawling projects it is one of the most difficult tasks. We have to make sure that we do not systematically miss URLs; therefore, thinking carefully about the process in the beginning is crucial. Performed correctly, URL generation can also be a tremendous time-saver.</p>

<div data-type="warning" epub:type="warning">
<h1>Before You Download</h1>

<p>Be careful: sometimes <a contenteditable="false" data-type="indexterm" data-primary="scraping websites" data-secondary="legal issues with" id="idm45634212174440"/><a contenteditable="false" data-type="indexterm" data-primary="downloading data, legal issues with" id="idm45634212173032"/>downloading data is illegal. The rules and legal situation might depend on the country where the data is hosted and into which country it is downloaded. Often, websites have a page called “terms of use” or something similar that might be worth taking a look at.</p>

<p>If data is saved only temporarily, the same rules for search engines might apply. As <a contenteditable="false" data-type="indexterm" data-primary="robot exclusion standard (robots.txt)" id="idm45634212171080"/><a contenteditable="false" data-type="indexterm" data-primary="scraping websites" data-secondary="downloading robots.txt for" id="idm45634212169864"/>search engines like Google cannot read and understand the terms of use of every single page they index, there is a really old protocol called the <a href="https://oreil.ly/IWysG">robots exclusion standard</a>. Websites using this have a file called <em>robots.txt</em> at the top level. This file can be downloaded and interpreted automatically. For single websites, it is also possible to read it manually and interpret the data. The rule of thumb is that if there is no <code>Disallow: *</code>, you should be allowed to download and (temporarily) save the content.</p>
</div>

<p>There are many different possibilities:</p>

<dl>
	<dt>Crawling</dt>
    <dd>Start <a contenteditable="false" data-type="indexterm" data-primary="crawling websites for URLs" id="idm45634212164312"/>on the home page (or a section) of the website and download all links on the same website. Crawling might take some time.</dd>
	<dt>URL generators</dt>
    <dd>Writing a URL generator is a slightly more sophisticated solution. This is most suitable for use on hierarchically organized content like forums, blogs, etc.</dd>
	<dt>Search engines</dt>
    <dd>Ask search engines for specific URLs and download only these specific URLs.</dd>
	<dt>Sitemaps</dt>
    <dd>A standard <a contenteditable="false" data-type="indexterm" data-primary="sitemap.xml for URL generation" id="idm45634212159896"/>called <a href="https://oreil.ly/XANO0"><em>sitemap.xml</em></a>, which was originally conceived for search engines, is an interesting alternative. A file called <em>sitemap.xml</em> contains a list of all pages on a website (or references to sub-sitemaps). Contrary to <em>robots.txt</em>, the filename is not fixed and can sometimes be found in <em>robots.txt</em> itself. The best guess is to look for <em>sitemap.xml</em> on the top level of a website.</dd>
	<dt>RSS</dt>
    <dd>The <a contenteditable="false" data-type="indexterm" data-primary="RSS feeds" id="idm45634212155144"/><a href="https://oreil.ly/_aOOM">RSS format</a> was originally conceived for newsfeeds and is still in wide use for subscribing to frequently changing content sources. It works via XML files and does not only contain URLs but also document titles and sometimes summaries of articles.</dd>
	<dt>Specialized programs</dt>
    <dd>Downloading data from social networks and similar content is often simplified by using specialized programs that are available on GitHub (such as <a href="https://oreil.ly/ThyNf">Facebook Chat Downloader</a> for Facebook Chats, <a href="https://oreil.ly/utGsC">Instaloader</a> for Instagram, and so on).</dd>
</dl>

<p>In the following sections, we focus on <em>robots.txt</em>, <em>sitemaps.xml</em>, and RSS feeds. Later in the chapter, we show a multistage download that uses URL generators.</p>

<div data-type="note" epub:type="note">
<h1>Note: Use an API for Downloading Data If It’s Available</h1>

<p>Instead of generating the URLs, <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch3_term2" id="idm45634212147832"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch3_term3" id="idm45634212146360"/>downloading the content, and extracting it, using an <a contenteditable="false" data-type="indexterm" data-primary="APIs (application programming interfaces)" data-seealso="GitHub API; REST (Representational State Transfer) API; text data extraction with APIs" id="idm45634212144792"/>API is much easier and more stable. You will find more information about that in <a data-type="xref" href="ch02.xhtml#ch-api">Chapter 2</a>.</p>
</div>
</div></section>
<section data-type="sect1" class="blueprint" data-pdf-bookmark="Blueprint: Downloading and Interpreting robots.txt"><div class="sect1" id="idm45634212141816">
<h1>Blueprint: Downloading and Interpreting robots.txt</h1>

<p>Finding the content on a website is often not so easy. To see the techniques mentioned earlier in action, we’ll take a look at the Reuters news archive. Of course, (almost) any other website will work in a similar fashion.</p>
<p>As <a contenteditable="false" data-type="indexterm" data-primary="robot exclusion standard (robots.txt)" id="idm45634212139320"/><a contenteditable="false" data-type="indexterm" data-primary="scraping websites" data-secondary="downloading robots.txt for" id="idm45634212138168"/>discussed, <a href="https://www.reuters.com/robots.txt"><em>robots.txt</em></a> is a good starting point:</p>

<pre data-type="programlisting">
# robots_allow.txt for www.reuters.com
# Disallow: /*/key-developments/article/*

User-agent: *
Disallow: /finance/stocks/option
[...]
Disallow: /news/archive/commentary

SITEMAP: https://www.reuters.com/sitemap_index.xml
SITEMAP: https://www.reuters.com/sitemap_news_index.xml
SITEMAP: https://www.reuters.com/sitemap_video_index.xml
SITEMAP: https://www.reuters.com/sitemap_market_index.xml
SITEMAP: https://www.reuters.com/brandfeature/sitemap

User-agent: Pipl
Disallow: /
[...]
</pre>

<p>Some user agents are not allowed to download anything, but the rest may do that. We can check that programmatically in Python:</p>

<pre data-code-language="python" data-executable="true" data-type="programlisting">
<code class="kn">import</code> <code class="nn">urllib.robotparser</code>
<code class="n">rp</code> <code class="o">=</code> <code class="n">urllib</code><code class="o">.</code><code class="n">robotparser</code><code class="o">.</code><code class="n">RobotFileParser</code><code class="p">()</code>
<code class="n">rp</code><code class="o">.</code><code class="n">set_url</code><code class="p">(</code><code class="s2">"https://www.reuters.com/robots.txt"</code><code class="p">)</code>
<code class="n">rp</code><code class="o">.</code><code class="n">read</code><code class="p">()</code>
<code class="n">rp</code><code class="o">.</code><code class="n">can_fetch</code><code class="p">(</code><code class="s2">"*"</code><code class="p">,</code> <code class="s2">"https://www.reuters.com/sitemap.xml"</code><code class="p">)</code>
</pre>

<p><code>Out:</code></p>

<pre data-code-language="xml" data-type="programlisting">
True
</pre>
</div></section>
<section data-type="sect1" class="blueprint pagebreak-after" data-pdf-bookmark="Blueprint: Finding URLs from sitemap.xml"><div class="sect1" id="idm45634211883816">
<h1>Blueprint: Finding URLs from sitemap.xml</h1>

<p>Reuters is <a contenteditable="false" data-type="indexterm" data-primary="datasets, examples of" data-secondary="Reuters News Archive" id="ch3_term4"/><a contenteditable="false" data-type="indexterm" data-primary="news archives" id="ch3_term5"/><a contenteditable="false" data-type="indexterm" data-primary="Reuters News Archive" id="ch3_term6"/><a contenteditable="false" data-type="indexterm" data-primary="sitemap.xml for URL generation" id="idm45634208408216"/>even nice enough to mention the URLs of the <a href="https://reuters.com/sitemap_news_index.xml">sitemap for the news</a>, which actually contains only a reference to <a href="https://www.reuters.com/sitemap_news_index1.xml" class="orm:hideurl">other sitemap files</a>. Let’s download that. An excerpt at the time of writing looks <span class="keep-together">like this:</span><sup><a data-type="noteref" id="idm45634207731272-marker" href="ch03.xhtml#idm45634207731272">1</a></sup></p>

<pre data-code-language="xml" data-type="programlisting">
[...]
<code class="nt">&lt;url&gt;</code>
  <code class="nt">&lt;loc&gt;</code>https://www.reuters.com/article/
us-health-vaping-marijuana-idUSKBN1WG4KT<code class="nt">&lt;/loc&gt;</code>
  <code class="nt">&lt;news:news&gt;</code>
    <code class="nt">&lt;news:publication&gt;</code>
      <code class="nt">&lt;news:name&gt;</code>Reuters<code class="nt">&lt;/news:name&gt;</code>
      <code class="nt">&lt;news:language&gt;</code>eng<code class="nt">&lt;/news:language&gt;</code>
    <code class="nt">&lt;/news:publication&gt;</code>
    <code class="nt">&lt;news:publication_date&gt;</code>2019-10-01T08:37:37+00:00<code class="nt">&lt;/news:publication_date&gt;</code>
    <code class="nt">&lt;news:title&gt;</code>Banned in Boston: Without vaping, medical marijuana patients
               must adapt<code class="nt">&lt;/news:title&gt;</code>
    <code class="nt">&lt;news:keywords&gt;</code>Headlines,Credit RSS<code class="nt">&lt;/news:keywords&gt;</code>
  <code class="nt">&lt;/news:news&gt;</code>
<code class="nt">&lt;/url&gt;</code>
[...]
</pre>

<p>The most interesting part is the line with <code>&lt;loc&gt;</code>, as it contains the URL of the article. Filtering out all these <code>&lt;loc&gt;</code> lines leads to a list of URLs for news articles that can be downloaded afterward.</p>

<p>As Python <a contenteditable="false" data-type="indexterm" data-primary="sitemap parsers" id="idm45634207476680"/>has an incredibly rich ecosystem of libraries, it’s not hard to find a sitemap parser. There are several available, such as <a href="https://oreil.ly/XgY9z"><code>ultimate-sitemap-parser</code></a>. However, this parser downloads the whole sitemap hierarchy, which is a bit too sophisticated for us as we just want the URLs.</p>

<p>It’s easy to convert <em>sitemap.xml</em> to an associative array (hash) that is called a <code>dict</code> in Python:<sup><a data-type="noteref" id="idm45634208553592-marker" href="ch03.xhtml#idm45634208553592">2</a></sup></p>

<pre data-code-language="python" data-executable="true" data-type="programlisting">
<code class="kn">import</code> <code class="nn">xmltodict</code>
<code class="kn">import</code> <code class="nn">requests</code>

<code class="n">sitemap</code> <code class="o">=</code> <code class="n">xmltodict</code><code class="o">.</code><code class="n">parse</code><code class="p">(</code><code class="n">requests</code><code class="o">.</code><code class="n">get</code><code class="p">(</code>
          <code class="s1">'https://www.reuters.com/sitemap_news_index1.xml'</code><code class="p">)</code><code class="o">.</code><code class="n">text</code><code class="p">)</code>
</pre>

<p>Let’s check what is in the <code>dict</code> before actually downloading the files<sup><a data-type="noteref" id="idm45634212104584-marker" href="ch03.xhtml#idm45634212104584">3</a></sup>:</p>

<pre data-code-language="python" data-executable="true" data-type="programlisting">
<code class="n">urls</code> <code class="o">=</code> <code class="p">[</code><code class="n">url</code><code class="p">[</code><code class="s2">"loc"</code><code class="p">]</code> <code class="k">for</code> <code class="n">url</code> <code class="ow">in</code> <code class="n">sitemap</code><code class="p">[</code><code class="s2">"urlset"</code><code class="p">][</code><code class="s2">"url"</code><code class="p">]]</code>
<code class="c1"># just print the first few URLs to avoid using too much space</code>
<code class="k">print</code><code class="p">(</code><code class="s2">"</code><code class="se">\n</code><code class="s2">"</code><code class="o">.</code><code class="n">join</code><code class="p">(</code><code class="n">urls</code><code class="p">[</code><code class="mi">0</code><code class="p">:</code><code class="mi">3</code><code class="p">))</code>
</pre>

<p><code>Out:</code></p>

<pre data-type="programlisting">
https://www.reuters.com/article/us-japan-fukushima/ex-tepco-bosses-cleared-
over-fukushima-nuclear-disaster-idUSKBN1W40CP
https://www.reuters.com/article/us-global-oil/oil-prices-rise-as-saudi-supply-
risks-come-into-focus-idUSKBN1W405X
https://www.reuters.com/article/us-saudi-aramco/iran-warns-against-war-as-us-
and-saudi-weigh-response-to-oil-attack-idUSKBN1W40VN
</pre>

<p>We will use this list of URLs in the following section and download their content.</p>
</div></section>

<section data-type="sect1" class="blueprint" data-pdf-bookmark="Blueprint: Finding URLs from RSS"><div class="sect1" id="idm45634211772520">
<h1>Blueprint: Finding URLs from RSS</h1>

<p>As Reuters is a news website, it <a contenteditable="false" data-type="indexterm" data-primary="RSS feeds" id="ch3_term7"/><a contenteditable="false" data-type="indexterm" data-primary="scraping websites" data-secondary="RSS feeds for" id="ch3_term8"/>also offers access to its articles via an RSS feed. Several years ago, browsers would show an RSS icon next to the <span class="keep-together">URL if</span> you could subscribe to this source. While those days are gone, it is still not too <span class="keep-together">difficult</span> to find the URLs for RSS feeds. At the bottom of the website, we can see a line with navigation icons, as shown in <a data-type="xref" href="#part_of_the_reuters_website_which_links_to_the_rss_feed">Figure 3-2</a>.</p>

<figure><div id="part_of_the_reuters_website_which_links_to_the_rss_feed" class="figure"><img src="Images/btap_0302.jpg" width="566" height="28"/>
<h6><span class="label">Figure 3-2. </span>Part of the Reuters website that links to the RSS feed.</h6>
</div></figure>

<p>The icon that looks like a WIFI indicator is the link to the RSS feeds page. Often (and sometimes more easily) this can be found by taking a look at the source code of the corresponding webpage and searching for <em>RSS</em>.</p>

<p>The world news RSS feed has the URL <a href="http://feeds.reuters.com/Reuters/worldNews" class="orm:hideurl"><em>http://feeds.reuters.com/Reuters/worldNews</em></a><sup><a data-type="noteref" id="idm45634206884968-marker" href="ch03.xhtml#idm45634206884968">4</a></sup> and can easily be parsed in Python, as follows:</p>

<pre data-code-language="python" data-executable="true" data-type="programlisting">
<code class="kn">import</code> <code class="nn">feedparser</code>
<code class="n">feed</code> <code class="o">=</code> <code class="n">feedparser</code><code class="o">.</code><code class="n">parse</code><code class="p">(</code><code class="s1">'http://feeds.reuters.com/Reuters/worldNews'</code><code class="p">)</code>
</pre>

<p>The individual format of the RSS file might differ from site to site. However, most of the time we will find title and link as fields<sup><a data-type="noteref" id="idm45634206875096-marker" href="ch03.xhtml#idm45634206875096">5</a></sup>:</p>

<pre data-code-language="python" data-executable="true" data-type="programlisting">
<code class="p">[(</code><code class="n">e</code><code class="o">.</code><code class="n">title</code><code class="p">,</code> <code class="n">e</code><code class="o">.</code><code class="n">link</code><code class="p">)</code> <code class="k">for</code> <code class="n">e</code> <code class="ow">in</code> <code class="n">feed</code><code class="o">.</code><code class="n">entries</code><code class="p">]</code>
</pre>

<p><code>Out:</code></p>

<pre data-type="programlisting">
[('Cambodian police search for British woman, 21, missing from beach',
  'http://feeds.reuters.com/~r/Reuters/worldNews/~3/xq6Hy6R9lxo/cambodian-
police-search-for-british-woman-21-missing-from-beach-idUSKBN1X70HX'),
 ('Killing the leader may not be enough to stamp out Islamic State',
  'http://feeds.reuters.com/~r/Reuters/worldNews/~3/jbDXkbcQFPA/killing-the-
leader-may-not-be-enough-to-stamp-out-islamic-state-idUSKBN1X7203'), [...]
]
</pre>

<p>In our case, we are more interested in the “real” URLs, which are contained in the <code>id</code> field:</p>

<pre data-code-language="python" data-executable="true" data-type="programlisting">
<code class="p">[</code><code class="n">e</code><code class="o">.</code><code class="n">id</code> <code class="k">for</code> <code class="n">e</code> <code class="ow">in</code> <code class="n">feed</code><code class="o">.</code><code class="n">entries</code><code class="p">]</code>
</pre>

<p class="pagebreak-before"><code>Out:</code></p>

<pre data-type="programlisting">
['https://www.reuters.com/article/us-cambodia-britain-tourist/cambodian-
police-search-for-british-woman-21-missing-from-beach-
idUSKBN1X70HX?feedType=RSS&amp;feedName=worldNews',
 'https://www.reuters.com/article/us-mideast-crisis-baghdadi-future-analys/
killing-the-leader-may-not-be-enough-to-stamp-out-islamic-state-
idUSKBN1X7203?feedType=RSS&amp;feedName=worldNews',
 'https://www.reuters.com/article/us-britain-eu/eu-approves-brexit-delay-
until-january-31-as-pm-johnson-pursues-election-
idUSKBN1X70NT?feedType=RSS&amp;feedName=worldNews', [...]
]
</pre>

<p>Great, we have found an alternative way to get a list of URLs that can be used when no <em>sitemap.xml</em> is available.</p>

<p>Sometimes you <a contenteditable="false" data-type="indexterm" data-primary="Atom feeds for URLs" id="idm45634206784424"/>will still encounter so-called <a href="https://oreil.ly/Jcdgi"><em>Atom feeds</em></a>, which basically offer the same information as RSS in a different format.</p>

<p>If you wanted to implement a website monitoring tool, taking a periodic look at Reuters news (or other news sources) or RSS (or Atom) would be a good way to go ahead.</p>

<p>If you are interested in whole websites, looking for <em>sitemap.xml</em> is an excellent idea. Sometimes it might be difficult to find (hints might be in <em>robots.txt</em>), but it is almost always worth the extra effort to find it.</p>

<p>If you cannot find <em>sitemap.xml</em> and you plan to regularly download content, going for RSS is a good second choice.</p>

<p>Whenever possible, try to avoid <a contenteditable="false" data-type="indexterm" data-primary="crawling websites for URLs" id="idm45634206756584"/>crawling websites for URLs. The process is largely uncontrollable, can take a long time, and might yield <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch3_term4" id="idm45634206755256"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch3_term5" id="idm45634206753880"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch3_term6" id="idm45634206752504"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch3_term7" id="idm45634206751128"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch3_term8" id="idm45634206749752"/>incomplete results.</p>
</div></section>


<section data-type="sect1" data-pdf-bookmark="Downloading Data"><div class="sect1" id="idm45634206939800">
<h1>Downloading Data</h1>

<p>At first sight, <a contenteditable="false" data-type="indexterm" data-primary="downloading HTML pages" id="ch3_term9"/><a contenteditable="false" data-type="indexterm" data-primary="scraping websites" data-secondary="downloading HTML pages for" id="ch3_term10"/><a contenteditable="false" data-type="indexterm" data-primary="HTML pages, downloading" id="ch3_term32"/>downloading data might seem like the most difficult and time-consuming part of the scraping process. Often, that’s not true as you can accomplish it in a highly standardized way.</p>

<p>In this section, we show different methods for downloading data, both with Python libraries and external tools. Especially for big projects, using external programs has some advantages.</p>

<p>Compared to several years ago, the Internet is much faster today. Big websites have reacted to this development by using content-delivery networks, which can speed them up by orders of magnitude. This helps us a lot as the actual downloading process is not as slow as it used to be but is more or less limited by our own bandwidth.</p>

<aside data-type="sidebar" epub:type="sidebar" class=""><div class="sidebar" id="ch3-tips-for-efficient-downloads">
<h5>Tips for Efficient Downloads</h5>

<p>The following strategies will help you download websites efficiently:</p>

<dl>
<dt>Compression</dt>
  <dd><p>HTML <a contenteditable="false" data-type="indexterm" data-primary="compression for downloads" id="idm45634206718760"/>normally is very verbose and can easily be compressed by several factors. To <a contenteditable="false" data-type="indexterm" data-primary="bandwidth, saving" id="idm45634206717480"/>save bandwidth, a download program supporting gzip compression (deflate or brotli is also possible) will save considerable bandwidth and time.</p></dd>

<dt>Content distribution networks (CDNs)</dt>
  <dd><p>Cloud <a contenteditable="false" data-type="indexterm" data-primary="content distribution networks (CDNs)" id="idm45634206715128"/><a contenteditable="false" data-type="indexterm" data-primary="CDNs (content distribution networks)" id="idm45634206713928"/>servers like AWS have an immensely powerful connection to the internet. To avoid overloading servers, parallel downloads should be used carefully. Definitely work with a “grace period” and wait some time between requests. To make the traffic look more human (and avoid automatic exclusions from the servers), randomizing this period is often a good idea.</p></dd>

<dt>Keep alive</dt>
  <dd><p>For <a contenteditable="false" data-type="indexterm" data-primary="HTTPS/TLS protocol" id="idm45634206711320"/><a contenteditable="false" data-type="indexterm" data-primary="keep-alive protocol" id="idm45634206710184"/>the past few years, most servers have begun using the HTTPS/TLS protocol for secure data transmission. The handshake of the protocol is quite complicated. It includes checking public/private keys and creating a symmetric session key before the actual encrypted transmission can start (<a href="https://oreil.ly/rOzKH">Diffie-Hellman key exchange</a>). </p>
    <p>Browsers are quite clever and have a special cache for this session key. When looking for a dedicated download program, choose one that also has such a cache. To achieve even smaller latencies, HTTP keep-alive can be used to recycle TCP connections. The <a contenteditable="false" data-type="indexterm" data-primary="requests library (Python)" id="idm45634206707272"/>Python requests library supports this functionality using the <code>Session</code> abstraction.</p>
  </dd>

<dt>Save files</dt>
  <dd><p>In many projects, it has proven useful to save the downloaded HTML pages (temporarily) in the filesystem. Of course, the structured content can be extracted on the fly, but if something goes wrong or pages have a different structure than expected, it will be hard to find and debug. This is extremely useful, especially during development.</p></dd>

<dt>Start simple</dt>
  <dd><p>In most blueprints, <code>requests</code> is well-suited for downloading pages. It offers a decent interface and works in Python environments.</p></dd>

<dt>Avoid getting banned</dt>
  <dd><p>Most websites <a contenteditable="false" data-type="indexterm" data-primary="scraping websites" data-secondary="blocks and bans as result of" id="idm45634206701480"/>are not keen on getting scraped, and quite a few have implemented countermeasures. Be polite and add a grace period between requests if you plan to download many pages.</p>
    <p>If you get banned anyway, you should actively notice this by checking the content and the response code. Changing IP addresses or using using IPv6, proxy servers, VPNs, or even the Tor network are possible options then.</p>
  </dd>
<dt>Legal aspects</dt>
  <dd><p>Depending <a contenteditable="false" data-type="indexterm" data-primary="scraping websites" data-secondary="legal issues with" id="idm45634206697928"/><a contenteditable="false" data-type="indexterm" data-primary="downloading data, legal issues with" id="idm45634206696520"/>on where you live and the terms of use of a website, scraping might not be allowed at all.</p></dd>
</dl>
</div></aside>
</div></section>

<section data-type="sect1" class="blueprint pagebreak-after" data-pdf-bookmark="Blueprint: Downloading HTML Pages with Python"><div class="sect1" id="idm45634206695032">
<h1>Blueprint: Downloading HTML Pages with Python</h1>

<p>To download HTML pages, it’s <a contenteditable="false" data-type="indexterm" data-primary="URLs for downloading HTML pages" id="ch3_term11"/>necessary to know the URLs. As we have seen, the URLs are contained in the sitemap. Let’s use this list to download the content:</p>

<pre data-code-language="python" data-executable="true" data-type="programlisting">
<code class="o">%%</code><code class="n">time</code>
<code class="n">s</code> <code class="o">=</code> <code class="n">requests</code><code class="o">.</code><code class="n">Session</code><code class="p">()</code>
<code class="k">for</code> <code class="n">url</code> <code class="ow">in</code> <code class="n">urls</code><code class="p">[</code><code class="mi">0</code><code class="p">:</code><code class="mi">10</code><code class="p">]:</code>
<code class="err"> </code> <code class="err"> </code> <code class="c1"># get the part after the last / in URL and use as filename</code>
<code class="err"> </code> <code class="err"> </code> <code class="nb">file</code> <code class="o">=</code> <code class="n">url</code><code class="o">.</code><code class="n">split</code><code class="p">(</code><code class="s2">"/"</code><code class="p">)[</code><code class="o">-</code><code class="mi">1</code><code class="p">]</code>
<code class="err"> </code> <code class="err">  </code>
<code class="err"> </code> <code class="err"> </code> <code class="n">r</code> <code class="o">=</code> <code class="n">s</code><code class="o">.</code><code class="n">get</code><code class="p">(</code><code class="n">url</code><code class="p">)</code>
<code class="err"> </code> <code class="err"> </code> <code class="k">if</code> <code class="n">r</code><code class="o">.</code><code class="n">ok</code><code class="p">:</code>
<code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="k">with</code> <code class="nb">open</code><code class="p">(</code><code class="nb">file</code><code class="p">,</code> <code class="s2">"w+b"</code><code class="p">)</code> <code class="k">as</code> <code class="n">f</code><code class="p">:</code>
<code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="n">f</code><code class="o">.</code><code class="n">write</code><code class="p">(</code><code class="n">r</code><code class="o">.</code><code class="n">text</code><code class="o">.</code><code class="n">encode</code><code class="p">(</code><code class="s1">'utf-8'</code><code class="p">))</code>
<code class="err"> </code> <code class="err"> </code> <code class="k">else</code><code class="p">:</code>
<code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="k">print</code><code class="p">(</code><code class="s2">"error with URL </code><code class="si">%s</code><code class="s2">"</code> <code class="o">%</code> <code class="n">url</code><code class="p">)</code>
</pre>

<p><code>Out:</code></p>

<pre data-code-language="python" data-executable="true" data-type="programlisting">
<code class="n">CPU</code> <code class="n">times</code><code class="p">:</code> <code class="n">user</code> <code class="mi">117</code> <code class="n">ms</code><code class="p">,</code> <code class="n">sys</code><code class="p">:</code> <code class="mf">7.71</code> <code class="n">ms</code><code class="p">,</code> <code class="n">total</code><code class="p">:</code> <code class="mi">124</code> <code class="n">ms</code>
<code class="n">Wall</code> <code class="n">time</code><code class="p">:</code> <code class="mi">314</code> <code class="n">ms</code>
</pre>

<p>Depending on your Internet connection, it might take longer, but that was quite fast. Using the session abstraction, we make sure to have maximum speed by leveraging keep-alive, SSL session caching, and so on.</p>

<div data-type="warning" epub:type="warning">
<h1>Use Proper Error Handling When Downloading URLs</h1>

<p>When downloading URLs, you are using a network protocol to communicate with remote servers. There are many kinds of errors that can happen, such as changed URLs, servers not responding, etc. The example just shows an error message; in real life, your solution should probably be more sophisticated.</p>
</div>
</div></section>

<section data-type="sect1" class="blueprint" data-pdf-bookmark="Blueprint: Downloading HTML Pages with wget"><div class="sect1" id="idm45634206559608">
<h1>Blueprint: Downloading HTML Pages with wget</h1>

<p>A good <a contenteditable="false" data-type="indexterm" data-primary="wget tool for downloading" id="idm45634206557640"/>tool for mass downloading pages is <a href="https://oreil.ly/wget">wget</a>, which is a command line tool available for almost all platforms. On Linux and macOS, <code>wget</code> should already be installed or can easily be installed using a package manager. On Windows, there is a port available at <a href="https://oreil.ly/2Nl0b" class="orm:hideurl"><em>https://oreil.ly/2Nl0b</em></a>.</p>

<p><code>wget</code> supports lists of URLs for downloads and HTTP keep-alive. Normally, each HTTP request needs a separate TCP connection (or a Diffie-Hellman key exchange; see <a data-type="xref" href="#ch3-tips-for-efficient-downloads">“Tips for Efficient Downloads”</a>). The <code>-nc</code> option of <code>wget</code> will check whether files have already been downloaded. This way, we can avoid downloading content twice. We can now stop the process at any time and restart without losing data, which is important if a web server blocks us, our Internet connection goes down, etc. Let’s save the list of URLs from the last blueprint to a file and use that as a template for downloading:</p>

<pre data-code-language="python" data-executable="true" data-type="programlisting">
<code class="k">with</code> <code class="nb">open</code><code class="p">(</code><code class="s2">"urls.txt"</code><code class="p">,</code> <code class="s2">"w+b"</code><code class="p">)</code> <code class="k">as</code> <code class="n">f</code><code class="p">:</code>
<code class="err"> </code> <code class="err"> </code> <code class="n">f</code><code class="o">.</code><code class="n">write</code><code class="p">(</code><code class="s2">"</code><code class="se">\n</code><code class="s2">"</code><code class="o">.</code><code class="n">join</code><code class="p">(</code><code class="n">urls</code><code class="p">)</code><code class="o">.</code><code class="n">encode</code><code class="p">(</code><code class="s1">'utf-8'</code><code class="p">))</code>
</pre>

<p>Now go to your command line (or a terminal tab in Jupyter) and call <code>wget</code>:</p>

<pre data-code-language="python" data-executable="true" data-type="programlisting">
<code class="n">wget</code> <code class="o">-</code><code class="n">nc</code> <code class="o">-</code><code class="n">i</code> <code class="n">urls</code><code class="o">.</code><code class="n">txt</code>
</pre>

<p>The <code>-i</code> option tells <code>wget</code> the list of URLs to download. It’s fun to see how <code>wget</code> skips the existing files (due to the <code>-nc</code> option) and how fast the downloading works.</p>

<p><code>wget</code> can also be used for recursively downloading websites with the option <code>-r</code>.</p>

<div data-type="warning" epub:type="warning">
<h1>Danger of Lockout!</h1>

<p>Be careful, <a contenteditable="false" data-type="indexterm" data-primary="scraping websites" data-secondary="blocks and bans as result of" id="idm45634206392232"/>this might lead to long-running processes, and eventually you might get locked out of the website. It’s often a good idea to combine <code>-r</code> with <code>-l</code> (recursion level) when experimenting with recursive downloads.</p>
</div>

<p>There are several different ways to download data. For a moderate number of pages (like a few hundred to a thousand), a download directly in a Python program is the standard way to go. We recommend the <a contenteditable="false" data-type="indexterm" data-primary="requests library (Python)" id="idm45634206388936"/><code>requests</code> library, as it is easy to use.</p>

<p>Downloading more than a few thousand pages normally works better in a multistage process by first generating a list of URLs and then downloading them externally via a dedicated <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch3_term9" id="idm45634206441464"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch3_term10" id="idm45634206440088"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch3_term11" id="idm45634206438712"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch3_term32" id="idm45634206437336"/>program like <code>wget</code>.</p>
</div></section>


<section data-type="sect1" class="pagebreak-before less_space" data-pdf-bookmark="Extracting Semistructured Data"><div class="sect1" id="idm45634206558984">
<h1>Extracting Semistructured Data</h1>

<p>In the following section, we will <a contenteditable="false" data-type="indexterm" data-primary="text data extraction from web" id="ch3_term14"/>explore different methods to extract data from <a contenteditable="false" data-type="indexterm" data-primary="Reuters News Archive" id="idm45634206369496"/><a contenteditable="false" data-type="indexterm" data-primary="news archives" id="idm45634206368392"/><a contenteditable="false" data-type="indexterm" data-primary="datasets, examples of" data-secondary="Reuters News Archive" id="idm45634206367288"/>Reuters articles. We will start with using regular expressions and then turn to a full-fledged HTML parser.</p>

<p>Eventually we will be interested in the data of more than one article, but as a first step we will concentrate on a single one. Let’s take <a href="https://oreil.ly/jg0Jr">“Banned in Boston: Without vaping, medical marijuana patients must adapt”</a> as our example.</p>
</div></section>
<section data-type="sect1" class="blueprint" data-pdf-bookmark="Blueprint: Extracting Data with Regular Expressions"><div class="sect1" id="idm45634206364136">
<h1>Blueprint: Extracting Data with Regular Expressions</h1>

<p>The browser <a contenteditable="false" data-type="indexterm" data-primary="regular expressions" id="ch3_term12"/><a contenteditable="false" data-type="indexterm" data-primary="text data extraction from web" data-secondary="with regular expressions" id="ch3_term13"/>will be one of the most important tools for dissecting the article. Start by opening the URL and using the View Source functionality. In the first step, we can see that the title is interesting. Taking a look at the HTML, the title is surrounded by both <code>&lt;title&gt;</code> and <code>&lt;h1&gt;</code>.</p>

<pre data-code-language="python" data-executable="true" data-type="programlisting">
<code class="p">[</code><code class="o">...</code><code class="p">]</code>
<code class="o">&lt;</code><code class="n">title</code><code class="o">&gt;</code><code class="n">Banned</code> <code class="ow">in</code> <code class="n">Boston</code><code class="p">:</code> <code class="n">Without</code> <code class="n">vaping</code><code class="p">,</code> <code class="n">medical</code> <code class="n">marijuana</code> <code class="n">patients</code>
<code class="n">must</code> <code class="n">adapt</code> <code class="o">-</code> <code class="n">Reuters</code><code class="o">&lt;/</code><code class="n">title</code><code class="o">&gt;</code>
<code class="p">[</code><code class="o">...</code><code class="p">]</code>
<code class="o">&lt;</code><code class="n">h1</code> <code class="n">class</code><code class="o">=</code><code class="s2">"ArticleHeader_headline"</code><code class="o">&gt;</code><code class="n">Banned</code> <code class="ow">in</code> <code class="n">Boston</code><code class="p">:</code> <code class="n">Without</code> <code class="n">vaping</code><code class="p">,</code>
<code class="n">medical</code> <code class="n">marijuana</code> <code class="n">patients</code> <code class="n">must</code> <code class="n">adapt</code><code class="o">&lt;/</code><code class="n">h1</code><code class="o">&gt;</code>
<code class="p">[</code><code class="o">...</code><code class="p">]</code>
</pre>

<div data-type="warning" epub:type="warning">
<h1>HTML Code Changes Over Time</h1>

<p>The programs described in this section work with the HTML code that was current when the book was written. However, publishers are free to change their website structure anytime and even remove content. An <a contenteditable="false" data-type="indexterm" data-primary="Wayback Machine" id="idm45634206304056"/>alternative is to use the data from the <a href="https://archive.org">Wayback Machine</a>. The Reuters website is mirrored there, and snapshots are kept that preserve the layout and the HTML structure.</p>

<p>Also take a look at the <a contenteditable="false" data-type="indexterm" data-primary="GitHub API" data-secondary="archives in" id="idm45634206301656"/>GitHub archive of the book. If the layout has changed and the programs would not work anymore, alternative links (and sitemaps) will be provided there.</p>
</div>

<p>Programmatically, the extraction of the title can be achieved with regular expressions without using any other libraries. Let’s first download the article and save it to a local file called <em>us-health-vaping-marijuana-idUSKBN1WG4KT.html</em>.</p>

<pre data-code-language="python" data-executable="true" data-type="programlisting">
<code class="kn">import</code> <code class="nn">requests</code>

<code class="n">url</code> <code class="o">=</code> <code class="s1">'https://www.reuters.com/article/us-health-vaping-marijuana-idUSKBN1WG4KT'</code>

<code class="c1"># use the part after the last / as filename</code>
<code class="nb">file</code> <code class="o">=</code> <code class="n">url</code><code class="o">.</code><code class="n">split</code><code class="p">(</code><code class="s2">"/"</code><code class="p">)[</code><code class="o">-</code><code class="mi">1</code><code class="p">]</code> <code class="o">+</code> <code class="s2">".html"</code>
<code class="n">r</code> <code class="o">=</code> <code class="n">requests</code><code class="o">.</code><code class="n">get</code><code class="p">(</code><code class="n">url</code><code class="p">)</code>
<code class="k">with</code> <code class="nb">open</code><code class="p">(</code><code class="nb">file</code><code class="p">,</code> <code class="s2">"w+b"</code><code class="p">)</code> <code class="k">as</code> <code class="n">f</code><code class="p">:</code>
<code class="err"> </code> <code class="err"> </code> <code class="n">f</code><code class="o">.</code><code class="n">write</code><code class="p">(</code><code class="n">r</code><code class="o">.</code><code class="n">text</code><code class="o">.</code><code class="n">encode</code><code class="p">(</code><code class="s1">'utf-8'</code><code class="p">))</code>
</pre>

<p>A Python blueprint for extracting the title might look like this:</p>

<pre data-code-language="python" data-executable="true" data-type="programlisting">
<code class="kn">import</code> <code class="nn">re</code>

<code class="k">with</code> <code class="nb">open</code><code class="p">(</code><code class="nb">file</code><code class="p">,</code> <code class="s2">"r"</code><code class="p">)</code> <code class="k">as</code> <code class="n">f</code><code class="p">:</code>
<code class="err"> </code> <code class="n">html</code> <code class="o">=</code> <code class="n">f</code><code class="o">.</code><code class="n">read</code><code class="p">()</code>
<code class="err"> </code> <code class="n">g</code> <code class="o">=</code> <code class="n">re</code><code class="o">.</code><code class="n">search</code><code class="p">(</code><code class="s-Affix">r</code><code class="s1">'&lt;title&gt;(.*)&lt;/title&gt;'</code><code class="p">,</code> <code class="n">html</code><code class="p">,</code> <code class="n">re</code><code class="o">.</code><code class="n">MULTILINE</code><code class="o">|</code><code class="n">re</code><code class="o">.</code><code class="n">DOTALL</code><code class="p">)</code>
<code class="err"> </code> <code class="k">if</code> <code class="n">g</code><code class="p">:</code>
<code class="err"> </code> <code class="err"> </code> <code class="k">print</code><code class="p">(</code><code class="n">g</code><code class="o">.</code><code class="n">groups</code><code class="p">()[</code><code class="mi">0</code><code class="p">])</code>
</pre>

<p><code>Out:</code></p>

<pre data-type="programlisting">
Banned in Boston: Without vaping, medical marijuana patients must adapt - Reuters
</pre>

<p>The <code>re</code> library <a contenteditable="false" data-type="indexterm" data-primary="re library (Python)" id="idm45634206119304"/>is not fully integrated into Python string handling. In other words, it cannot be invoked as methods of string. As our HTML documents consist of many lines, we have to use <code>re.MULTILINE|re.DOTALL</code>. Sometimes cascaded calls to <code>re.search</code> are necessary, but they do make the code harder to read.</p>

<p>It is crucial to use <code>re.search</code> and not <code>re.match</code> in Python, which is different than in many other programming languages. The latter tries to match the whole string, and as <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch3_term12" id="idm45634206115336"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch3_term13" id="idm45634206113960"/>there is data before <code>&lt;title&gt;</code> and after <code>&lt;/title&gt;</code>, it fails.</p>
</div></section>


<section data-type="sect1" class="blueprint" data-pdf-bookmark="Blueprint: Using an HTML Parser for Extraction"><div class="sect1" id="idm45634206363512">
<h1>Blueprint: Using an HTML Parser for Extraction</h1>

<p>The article has more interesting parts that are tedious to extract with regular expressions. There’s <a contenteditable="false" data-type="indexterm" data-primary="news articles, text data extraction from" id="ch3_term18"/>text in the article, a publication date is associated with it, and the authors are named. This <a contenteditable="false" data-type="indexterm" data-primary="HTML parser for extracting data" id="ch3_term15"/><a contenteditable="false" data-type="indexterm" data-primary="text data extraction from web" data-secondary="with HTML parser " data-secondary-sortas="HTML parser " id="ch3_term16"/>is much easier to accomplish with an HTML parser.<sup><a data-type="noteref" id="idm45634206104264-marker" href="ch03.xhtml#idm45634206104264">6</a></sup> Fortunately, with the <a contenteditable="false" data-type="indexterm" data-primary="Beautiful Soup library (Python)" id="ch3_term17"/>Python package called <a href="https://oreil.ly/I2VJh">Beautiful Soup</a>, we have an extremely powerful library for handling this. If you don’t have Beautiful Soup installed, install it now with <code>pip install bs4</code> or <code>conda install bs4</code>. Beautiful Soup is tolerant and can also parse “bad” HTML that is often found on sloppily managed websites.</p>

<p>The next sections make use of the fact that all articles have the same structure in the news archive. Fortunately, this is true for most big websites as the pages are not hand-crafted but rather generated by a content management system from a database.</p>
<section data-type="sect3" data-pdf-bookmark="Extracting the title/headline"><div class="sect3" id="idm45634206098568">
<h3>Extracting the title/headline</h3>

<p>Selecting content in Beautiful Soup uses so-called <a contenteditable="false" data-type="indexterm" data-primary="Web Inspector in browsers" id="idm45634206096552"/>selectors that need to be given in the Python program. Finding them is a bit tricky, but there are structural approaches for that. Almost all modern browsers support a Web Inspector, which is useful for finding the <a contenteditable="false" data-type="indexterm" data-primary="CSS selectors" id="ch3_term19"/><a contenteditable="false" data-type="indexterm" data-primary="selectors, CSS" id="ch3_term20"/>CSS selectors. Open the Web Inspector in the browser (most commonly achieved by pressing F12) when the article is loaded, and click the Web Inspector icon, as shown in <a data-type="xref" href="#web_inspector_icon_in_chrome_browser">Figure 3-3</a>.</p>

<figure><div id="web_inspector_icon_in_chrome_browser" class="figure"><img src="Images/btap_0303.jpg" width="548" height="79"/>
<h6><span class="label">Figure 3-3. </span>Web Inspector icon in the Chrome browser.</h6>
</div></figure>

<p>Hover over the <a contenteditable="false" data-type="indexterm" data-primary="titles, extraction of" id="idm45634206088904"/><a contenteditable="false" data-type="indexterm" data-primary="headlines, extracting" id="idm45634206087768"/>headline and you will see the corresponding element highlighted, as shown in <a data-type="xref" href="#screenshot_of_the_chrome_browser_with_the_web_inspector">Figure 3-4</a>.</p>

<figure><div id="screenshot_of_the_chrome_browser_with_the_web_inspector" class="figure"><img src="Images/btap_0304.jpg" width="852" height="255"/>
<h6><span class="label">Figure 3-4. </span>Chrome browser using the Web Inspector.</h6>
</div></figure>

<p>Clicking the headline to show it in the Web Inspector. It should look like this:</p>

<pre data-code-language="python" data-executable="true" data-type="programlisting">
<code class="o">&lt;</code><code class="n">h1</code> <code class="n">class</code><code class="o">=</code><code class="s2">"ArticleHeader_headline"</code><code class="o">&gt;</code><code class="n">Banned</code> <code class="ow">in</code> <code class="n">Boston</code><code class="p">:</code> <code class="n">Without</code> <code class="n">vaping</code><code class="p">,</code> <code class="n">medical</code>
<code class="n">marijuana</code> <code class="n">patients</code> <code class="n">must</code> <code class="n">adapt</code><code class="o">&lt;/</code><code class="n">h1</code><code class="o">&gt;</code>
</pre>

<p>Using CSS notation,<sup><a data-type="noteref" id="idm45634206031320-marker" href="ch03.xhtml#idm45634206031320">7</a></sup> this element can be selected with <code>h1.ArticleHeader_headline</code>. Beautiful Soup understands that:</p>

<pre data-code-language="python" data-executable="true" data-type="programlisting">
<code class="kn">from</code> <code class="nn">bs4</code> <code class="kn">import</code> <code class="n">Beautiful</code> <code class="n">Soup</code>
<code class="n">soup</code> <code class="o">=</code> <code class="n">Beautiful</code> <code class="n">Soup</code><code class="p">(</code><code class="n">html</code><code class="p">,</code> <code class="s1">'html.parser'</code><code class="p">)</code>
<code class="n">soup</code><code class="o">.</code><code class="n">select</code><code class="p">(</code><code class="s2">"h1.ArticleHeader_headline"</code><code class="p">)</code>
</pre>

<p><code>Out:</code></p>

<pre data-type="programlisting">
[&lt;h1 class="ArticleHeader_headline"&gt;Banned in Boston: Without vaping, medical
marijuana patients must adapt&lt;/h1&gt;]
</pre>

<p>Beautiful Soup makes it even easier and lets us use the tag names directly:</p>

<pre data-code-language="python" data-executable="true" data-type="programlisting">
<code class="n">soup</code><code class="o">.</code><code class="n">h1</code>
</pre>

<p><code>Out:</code></p>

<pre data-type="programlisting">
&lt;h1 class="ArticleHeader_headline"&gt;Banned in Boston: Without vaping, medical
marijuana patients must adapt&lt;/h1&gt;
</pre>

<p>Normally, the most interesting part of the previous HTML fragment is the real text without the HTML clutter around it. Beautiful Soup can extract that:</p>

<pre data-code-language="python" data-executable="true" data-type="programlisting">
<code class="n">soup</code><code class="o">.</code><code class="n">h1</code><code class="o">.</code><code class="n">text</code>
</pre>

<p><code>Out:</code></p>

<pre data-type="programlisting">
'Banned in Boston: Without vaping, medical marijuana patients must adapt'
</pre>

<p>Note that in contrast to the regular expression solution, unnecessary whitespaces have been stripped by Beautiful Soup.</p>

<p>Unfortunately, that does not work as well for the title:</p>

<pre data-code-language="python" data-executable="true" data-type="programlisting">
<code class="n">soup</code><code class="o">.</code><code class="n">title</code><code class="o">.</code><code class="n">text</code>
</pre>

<p><code>Out:</code></p>

<pre data-type="programlisting">
'\n                Banned in Boston: Without vaping, medical marijuana patients
must adapt - Reuters'
</pre>

<p>Here, we would need to manually strip the data and eliminate the <code>- Reuters</code> suffix.</p>
</div></section>

<section data-type="sect3" data-pdf-bookmark="Extracting the article text"><div class="sect3" id="idm45634206097624">
<h3>Extracting the article text</h3>

<p>In a similar way to the previously described procedure for finding the headline selector, you can easily find the text content at the selector <code>div.StandardArticleBody_body</code>. When <a contenteditable="false" data-type="indexterm" data-primary="select in Beautiful Soup" id="idm45634205938920"/>using <code>select</code>, Beautiful Soup returns a list. Often it is clear from the underlying HTML structure that the list consists of only one item or we are interested only in the first element. We can use the convenience <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch3_term17" id="idm45634205937048"/>method <code>select_one</code> here:</p>

<pre data-code-language="python" data-executable="true" data-type="programlisting">
<code class="n">soup</code><code class="o">.</code><code class="n">select_one</code><code class="p">(</code><code class="s2">"div.StandardArticleBody_body"</code><code class="p">)</code><code class="o">.</code><code class="n">text</code>
</pre>

<p><code>Out:</code></p>

<pre data-code-language="python" data-executable="true" data-type="programlisting">
<code class="s2">"WASHINGTON (Reuters) - In the first few days of the four-month ban [...]"</code>
</pre>
</div></section>

<section data-type="sect3" data-pdf-bookmark="Extracting image captions"><div class="sect3" id="idm45634205847256">
<h3>Extracting image captions</h3>

<p>But wait, apart from the text, this part also contains <a contenteditable="false" data-type="indexterm" data-primary="image captions, extracting" id="idm45634205848920"/>images with captions that might be relevant separately. So again, use the Web Inspector to hover over the images and find the corresponding CSS selectors. All images are contained in <code>&lt;figure&gt;</code> elements, so let’s select them:</p>

<pre data-code-language="python" data-executable="true" data-type="programlisting">
<code class="n">soup</code><code class="o">.</code><code class="n">select</code><code class="p">(</code><code class="s2">"div.StandardArticleBody_body figure img"</code><code class="p">)</code>
</pre>

<p><code>Out:</code></p>

<pre data-code-language="python" data-executable="true" data-type="programlisting">
<code class="p">[</code><code class="o">&lt;</code><code class="n">img</code> <code class="n">aria</code><code class="o">-</code><code class="n">label</code><code class="o">=</code><code class="s2">"FILE PHOTO: An employee puts down an eighth of an ounce</code>
  <code class="n">marijuana</code> <code class="n">after</code> <code class="n">letting</code> <code class="n">a</code> <code class="n">customer</code> <code class="n">smell</code> <code class="n">it</code> <code class="n">outside</code> <code class="n">the</code> <code class="n">Magnolia</code> <code class="n">cannabis</code>
  <code class="n">lounge</code> <code class="ow">in</code> <code class="n">Oakland</code><code class="p">,</code> <code class="n">California</code><code class="p">,</code> <code class="n">U</code><code class="o">.</code><code class="n">S</code><code class="o">.</code> <code class="n">April</code> <code class="mi">20</code><code class="p">,</code> <code class="mf">2018.</code> <code class="n">REUTERS</code><code class="o">/</code><code class="n">Elijah</code> <code class="n">Nouvelage</code><code class="s2">"</code>
  <code class="n">src</code><code class="o">=</code><code class="s2">"//s3.reutersmedia.net/resources/r/</code>
  <code class="err">?</code><code class="n">m</code><code class="o">=</code><code class="mo">02</code><code class="o">&amp;</code><code class="n">amp</code><code class="p">;</code><code class="n">d</code><code class="o">=</code><code class="mi">20191001</code><code class="o">&amp;</code><code class="n">amp</code><code class="p">;</code><code class="n">t</code><code class="o">=</code><code class="mi">2</code><code class="o">&amp;</code><code class="n">amp</code><code class="p">;</code><code class="n">i</code><code class="o">=</code><code class="mi">1435991144</code><code class="o">&amp;</code><code class="n">amp</code><code class="p">;</code><code class="n">r</code><code class="o">=</code><code class="n">LYNXMPEF90</code>
  <code class="il">39L</code><code class="o">&amp;</code><code class="n">amp</code><code class="p">;</code><code class="n">w</code><code class="o">=</code><code class="mi">20</code><code class="s2">"/&gt;, &lt;img src="</code><code class="o">//</code><code class="n">s3</code><code class="o">.</code><code class="n">reutersmedia</code><code class="o">.</code><code class="n">net</code><code class="o">/</code><code class="n">resources</code><code class="o">/</code><code class="n">r</code><code class="o">/</code>
  <code class="err">?</code><code class="n">m</code><code class="o">=</code><code class="mo">02</code><code class="o">&amp;</code><code class="n">amp</code><code class="p">;</code><code class="n">d</code><code class="o">=</code><code class="mi">20191001</code><code class="o">&amp;</code><code class="n">amp</code><code class="p">;</code><code class="n">t</code><code class="o">=</code><code class="mi">2</code><code class="o">&amp;</code><code class="n">amp</code><code class="p">;</code><code class="n">i</code><code class="o">=</code><code class="mi">1435991145</code><code class="o">&amp;</code><code class="n">amp</code><code class="p">;</code><code class="n">r</code><code class="o">=</code><code class="n">LYNXMPEF90</code>
  <code class="mi">39</code><code class="n">M</code><code class="s2">"/&gt;]</code>
</pre>

<p>Inspecting the result closely, this code contains only one image, whereas the browser displays many images. This is a pattern that can often be found in web pages. Code for the images is not in the page itself but is added later by client-side JavaScript. Technically this is possible, although it is not the best style. From a content perspective, it would be better if the image source were contained in the original server-generated page and made visible by CSS later. This would also help our extraction process.  Anyway, we are more interested in the caption of the image, so the correct selector would be to replace <code>img</code> with <code>figcaption</code>.</p>

<pre data-code-language="python" data-executable="true" data-type="programlisting">
<code class="n">soup</code><code class="o">.</code><code class="n">select</code><code class="p">(</code><code class="s2">"div.StandardArticleBody_body figcaption"</code><code class="p">)</code>
</pre>

<p><code>Out:</code></p>

<pre data-code-language="python" data-executable="true" data-type="programlisting">
<code class="p">[</code><code class="o">&lt;</code><code class="n">figcaption</code><code class="o">&gt;&lt;</code><code class="n">div</code> <code class="n">class</code><code class="o">=</code><code class="s2">"Image_caption"</code><code class="o">&gt;&lt;</code><code class="n">span</code><code class="o">&gt;</code><code class="n">FILE</code> <code class="n">PHOTO</code><code class="p">:</code>
  <code class="n">An</code> <code class="n">employee</code> <code class="n">puts</code> <code class="n">down</code> <code class="n">an</code> <code class="n">eighth</code> <code class="n">of</code> <code class="n">an</code> <code class="n">ounce</code> <code class="n">marijuana</code> <code class="n">after</code> <code class="n">letting</code> <code class="n">a</code>
  <code class="n">customer</code> <code class="n">smell</code> <code class="n">it</code> <code class="n">outside</code> <code class="n">the</code> <code class="n">Magnolia</code> <code class="n">cannabis</code> <code class="n">lounge</code> <code class="ow">in</code> <code class="n">Oakland</code><code class="p">,</code>
  <code class="n">California</code><code class="p">,</code> <code class="n">U</code><code class="o">.</code><code class="n">S</code><code class="o">.</code> <code class="n">April</code> <code class="mi">20</code><code class="p">,</code> <code class="mf">2018.</code> <code class="n">REUTERS</code><code class="o">/</code><code class="n">Elijah</code> <code class="n">Nouvelage</code><code class="o">&lt;/</code><code class="n">span</code><code class="o">&gt;&lt;/</code>
  <code class="n">div</code><code class="o">&gt;&lt;/</code><code class="n">figcaption</code><code class="o">&gt;</code><code class="p">,</code>

<code class="err"> </code><code class="o">&lt;</code><code class="n">figcaption</code> <code class="n">class</code><code class="o">=</code><code class="s2">"Slideshow_caption"</code><code class="o">&gt;</code><code class="n">Slideshow</code><code class="o">&lt;</code><code class="n">span</code> <code class="n">class</code><code class="o">=</code><code class="s2">"Slideshow_count"</code><code class="o">&gt;</code>
  <code class="p">(</code><code class="mi">2</code> <code class="n">Images</code><code class="p">)</code><code class="o">&lt;/</code><code class="n">span</code><code class="o">&gt;&lt;/</code><code class="n">figcaption</code><code class="o">&gt;</code><code class="p">]</code>
</pre>
</div></section>

<section data-type="sect3" data-pdf-bookmark="Extracting the URL"><div class="sect3" id="idm45634205695176">
<h3>Extracting the URL</h3>

<p>When downloading many HTML files, it is often difficult to find the original <a contenteditable="false" data-type="indexterm" data-primary="URLs, extraction of" id="idm45634205509128"/>URLs of the files if they have not been saved separately. Moreover, URLs might change, and normally it is best to use the standard (called <em>canonical</em>) URL. Fortunately, there is an HTML tag called <code>&lt;link rel="canonical"&gt;</code> that can be used for this purpose. The tag is not mandatory, but it is extremely common, as it is also taken into account by search engines and contributes to a good ranking:</p>

<pre data-code-language="python" data-executable="true" data-type="programlisting">
<code class="n">soup</code><code class="o">.</code><code class="n">find</code><code class="p">(</code><code class="s2">"link"</code><code class="p">,</code> <code class="p">{</code><code class="s1">'rel'</code><code class="p">:</code> <code class="s1">'canonical'</code><code class="p">})[</code><code class="s1">'href'</code><code class="p">]</code>
</pre>

<p><code>Out:</code></p>

<pre data-code-language="python" data-executable="true" data-type="programlisting">
<code class="s1">'https://www.reuters.com/article/us-health-vaping-marijuana-idUSKBN1WG4KT'</code>
</pre>
</div></section>

<section data-type="sect3" data-pdf-bookmark="Extracting list information (authors)"><div class="sect3" id="idm45634205495928">
<h3>Extracting list information (authors)</h3>

<p>Taking a look at the source code, the author of the article is mentioned in a <code>&lt;meta name="Author"&gt;</code> tag.</p>

<pre data-code-language="python" data-executable="true" data-type="programlisting">
<code class="n">soup</code><code class="o">.</code><code class="n">find</code><code class="p">(</code><code class="s2">"meta"</code><code class="p">,</code> <code class="p">{</code><code class="s1">'name'</code><code class="p">:</code> <code class="s1">'Author'</code><code class="p">})[</code><code class="s1">'content'</code><code class="p">]</code>
</pre>

<p><code>Out:</code></p>

<pre data-code-language="python" data-executable="true" data-type="programlisting">
<code class="s1">'Jacqueline Tempera'</code>
</pre>

<p>However, this returns only one author. Reading the text, there is another author, which is unfortunately not contained in the meta-information of the page. Of course, it can be extracted again by selecting the elements in the browser and using the CSS selector:</p>

<pre data-code-language="python" data-executable="true" data-type="programlisting">
<code class="n">sel</code> <code class="o">=</code> <code class="s2">"div.BylineBar_first-container.ArticleHeader_byline-bar </code><code class="se">\</code>
<code class="s2">      div.BylineBar_byline span"</code>
<code class="n">soup</code><code class="o">.</code><code class="n">select</code><code class="p">(</code><code class="n">sel</code><code class="p">)</code>
</pre>

<p><code>Out:</code></p>

<pre data-code-language="python" data-executable="true" data-type="programlisting">
<code class="p">[</code><code class="o">&lt;</code><code class="n">span</code><code class="o">&gt;&lt;</code><code class="n">a</code> <code class="n">href</code><code class="o">=</code><code class="s2">"/journalists/jacqueline-tempera"</code> <code class="n">target</code><code class="o">=</code><code class="s2">"_blank"</code><code class="o">&gt;</code>
  <code class="n">Jacqueline</code> <code class="n">Tempera</code><code class="o">&lt;/</code><code class="n">a</code><code class="o">&gt;</code><code class="p">,</code> <code class="o">&lt;/</code><code class="n">span</code><code class="o">&gt;</code><code class="p">,</code>
<code class="err"> </code><code class="o">&lt;</code><code class="n">span</code><code class="o">&gt;&lt;</code><code class="n">a</code> <code class="n">href</code><code class="o">=</code><code class="s2">"/journalists/jonathan-allen"</code> <code class="n">target</code><code class="o">=</code><code class="s2">"_blank"</code><code class="o">&gt;</code>
  <code class="n">Jonathan</code> <code class="n">Allen</code><code class="o">&lt;/</code><code class="n">a</code><code class="o">&gt;&lt;/</code><code class="n">span</code><code class="o">&gt;</code><code class="p">]</code>
</pre>

<p>Extracting the author names is then straightforward:</p>

<pre data-code-language="python" data-executable="true" data-type="programlisting">
<code class="p">[</code><code class="n">a</code><code class="o">.</code><code class="n">text</code> <code class="k">for</code> <code class="n">a</code> <code class="ow">in</code> <code class="n">soup</code><code class="o">.</code><code class="n">select</code><code class="p">(</code><code class="n">sel</code><code class="p">)]</code>
</pre>

<p><code>Out:</code></p>

<pre data-code-language="python" data-executable="true" data-type="programlisting">
<code class="p">[</code><code class="s1">'Jacqueline Tempera, '</code><code class="p">,</code> <code class="s1">'Jonathan Allen'</code><code class="p">]</code>
</pre>
</div></section>

<section data-type="sect3" data-pdf-bookmark="Semantic and nonsemantic content"><div class="sect3" id="idm45634205495336">
<h3>Semantic and nonsemantic content</h3>

<p>In contrast to the previous examples, the <code>sel</code> selector is not <em>semantic</em>. Selection is performed based on layout-like classes. This works well for the moment but is likely to break if the layout is changed. Therefore, it’s a good idea to avoid these kinds of selections if the code is likely to be executed not only once or in a batch but should also run in the future.</p>
</div></section>

<section data-type="sect3" data-pdf-bookmark="Extracting text of links (section)"><div class="sect3" id="idm45634205262440">
<h3>Extracting text of links (section)</h3>

<p>The section is easy to extract. Using the <a contenteditable="false" data-type="indexterm" data-primary="Web Inspector in browsers" id="idm45634205251400"/>Web Inspector again, we can find that the CSS selector is the following:</p>

<pre class="pre" data-code-language="python" data-executable="true" data-type="programlisting">
<code class="n">soup</code><code class="o">.</code><code class="n">select_one</code><code class="p">(</code><code class="s2">"div.ArticleHeader_channel a"</code><code class="p">)</code><code class="o">.</code><code class="n">text</code>
</pre>

<p><code>Out:</code></p>

<pre data-code-language="python" data-executable="true" data-type="programlisting">
<code class="s1">'Politics'</code>
</pre>
</div></section>


<section data-type="sect3" data-pdf-bookmark="Extracting reading time"><div class="sect3" id="idm45634205154024">
<h3>Extracting reading time</h3>

<p>Reading time can be found easily via the Web Inspector:</p>

<pre class="pre" data-code-language="python" data-executable="true" data-type="programlisting">
<code class="n">soup</code><code class="o">.</code><code class="n">select_one</code><code class="p">(</code><code class="s2">"p.BylineBar_reading-time"</code><code class="p">)</code><code class="o">.</code><code class="n">text</code>
</pre>

<p><code>Out:</code></p>

<pre class="pre" data-code-language="python" data-executable="true" data-type="programlisting">
<code class="s1">'6 Min Read'</code>
</pre>
</div></section>


<section data-type="sect3" data-pdf-bookmark="Extracting attributes (ID)"><div class="sect3" id="idm45634205134088">
<h3>Extracting attributes (ID)</h3>

<p>Having a primary key that uniquely identifies an article is helpful. The ID is also present in the URL, but there might be some heuristics and advanced splitting necessary to find it. Using the browser’s View Source functionality and searching for this ID, we see that it is the <code>id</code> attribute of the article container:</p>

<pre data-code-language="python" data-executable="true" data-type="programlisting">
<code class="n">soup</code><code class="o">.</code><code class="n">select_one</code><code class="p">(</code><code class="s2">"div.StandardArticle_inner-container"</code><code class="p">)[</code><code class="s1">'id'</code><code class="p">]</code>
</pre>

<p><code>Out:</code></p>

<pre data-code-language="python" data-executable="true" data-type="programlisting">
<code class="s1">'USKBN1WG4KT'</code>
</pre>
</div></section>

<section data-type="sect3" data-pdf-bookmark="Extracting attribution"><div class="sect3" id="idm45634205055944">
<h3>Extracting attribution</h3>

<p>Apart from the authors, the article carries more attributions. They can be found at the end of the text and reside in a <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch3_term19" id="idm45634205054424"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch3_term20" id="idm45634205105352"/>special container:</p>

<pre data-code-language="python" data-executable="true" data-type="programlisting">
<code class="n">soup</code><code class="o">.</code><code class="n">select_one</code><code class="p">(</code><code class="s2">"p.Attribution_content"</code><code class="p">)</code><code class="o">.</code><code class="n">text</code>
</pre>

<p><code>Out:</code></p>

<pre data-type="programlisting">
'Reporting Jacqueline Tempera in Brookline and Boston, Massachusetts, and
Jonathan Allen in New York; Editing by Frank McGurty and Bill Berkrot'
</pre>
</div></section>

<section data-type="sect3" data-pdf-bookmark="Extracting timestamp"><div class="sect3" id="idm45634205102424">
<h3>Extracting timestamp</h3>

<p>For many <a contenteditable="false" data-type="indexterm" data-primary="timestamp of news article, extracting" id="idm45634205100728"/>statistical purposes, it is crucial to know the time that the article was posted. This is mentioned next to the section, but unfortunately it is constructed to be human-readable (like “3 days ago”). This can be parsed but is tedious. Knowing the real publishing time, the correct element can be found in the HTML head element:</p>

<pre data-code-language="python" data-executable="true" data-type="programlisting">
<code class="n">ptime</code> <code class="o">=</code> <code class="n">soup</code><code class="o">.</code><code class="n">find</code><code class="p">(</code><code class="s2">"meta"</code><code class="p">,</code> <code class="p">{</code> <code class="s1">'property'</code><code class="p">:</code> <code class="s2">"og:article:published_time"</code><code class="p">})[</code><code class="s1">'content'</code><code class="p">]</code>
<code class="k">print</code><code class="p">(</code><code class="n">ptime</code><code class="p">)</code>
</pre>

<p><code>Out:</code></p>

<pre data-type="programlisting">
2019-10-01T19:23:16+0000
</pre>

<p>A string is already helpful (especially in this notation, as we will see later), but Python offers facilities to convert that to a datetime object easily:</p>

<pre data-code-language="python" data-executable="true" data-type="programlisting">
<code class="kn">from</code> <code class="nn">dateutil</code> <code class="kn">import</code> <code class="n">parser</code>
<code class="n">parser</code><code class="o">.</code><code class="n">parse</code><code class="p">(</code><code class="n">ptime</code><code class="p">)</code>
</pre>

<p><code>Out:</code></p>

<pre data-code-language="python" data-executable="true" data-type="programlisting">
<code class="n">datetime</code><code class="o">.</code><code class="n">datetime</code><code class="p">(</code><code class="mi">2019</code><code class="p">,</code> <code class="mi">10</code><code class="p">,</code> <code class="mi">1</code><code class="p">,</code> <code class="mi">19</code><code class="p">,</code> <code class="mi">23</code><code class="p">,</code> <code class="mi">16</code><code class="p">,</code> <code class="n">tzinfo</code><code class="o">=</code><code class="n">tzutc</code><code class="p">())</code>
</pre>

<p>The same can be done for <code>modified_time</code> instead of <code>published_time</code>, if that is more relevant.</p>

<p>Use regular expressions only for crude extraction. An HTML parser is slower but much easier to use and more stable.</p>

<p>Often, it makes sense to take a look at the semantic structure of the documents and use HTML tags that have semantic class names to find the value of structural elements. These tags have the advantage that they are the same over a large class of web pages. Extraction of their content therefore has to be implemented only once and can be reused.</p>

<p>Apart from extremely simple cases, try to use an HTML parser whenever possible. Some standard structures that can be found in almost any HTML document are discussed in the following sidebar.</p>

<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45634204881864">
<h5>Standardized Extraction</h5>

<p>Normally, it is a <a contenteditable="false" data-type="indexterm" data-primary="text data extraction from web" data-secondary="standardized parts for" id="idm45634204880536"/><a contenteditable="false" data-type="indexterm" data-primary="standardized parts for document extraction" id="idm45634204879160"/><a contenteditable="false" data-type="indexterm" data-primary="document extraction, standardized parts for" id="idm45634204878088"/>good idea to extract at least these standardized parts of each <span class="keep-together">document:</span></p>

<ul>
	<li>Title: <a contenteditable="false" data-type="indexterm" data-primary="titles, extraction of" id="idm45634204875304"/>Use the <code>&lt;title&gt;</code> or <code>&lt;h1&gt;</code> tag.</li>
	<li>Summary of web page: Look for <code>&lt;meta name="description"&gt;</code>.</li>
	<li>Structured header information: Standardized in the <a href="https://ogp.me">OpenGraph</a>. Search for <code>og:</code> in the source code of a page.</li>
	<li>URL <a contenteditable="false" data-type="indexterm" data-primary="URLs, extraction of" id="idm45634204870312"/>of a web page: The URL itself might contain valuable information and can be found in <code>&lt;link rel="canonical&gt;</code>.</li>
	<li>URL structure: Modern URLs are often not cryptic but contain a lot of information, like categories (sections) organized in folders, IDs, or <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch3_term15" id="idm45634204868024"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch3_term16" id="idm45634204866648"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch3_term18" id="idm45634204865272"/>even timestamps in blogs.</li>
</ul>
</div></aside>
</div></section>
</div></section>


<section data-type="sect1" class="blueprint pagebreak-before less_space" data-pdf-bookmark="Blueprint: Spidering"><div class="sect1" id="idm45634206111224">
<h1>Blueprint: Spidering</h1>

<p>So far we have taken a look at how we can download web pages and extract the content using HTML parsing techniques. From a business perspective, looking at single pages is often not so interesting, but<a contenteditable="false" data-type="indexterm" data-primary="spidering process" id="ch3_term21"/><a contenteditable="false" data-type="indexterm" data-primary="text data extraction from web" data-secondary="spidering process for" id="ch3_term22"/><a contenteditable="false" data-type="indexterm" data-primary="downloading with spidering" id="ch3_term23"/>you want to see the whole picture. For this, you need much more content.</p>

<p>Fortunately, our acquired knowledge can be combined to download content archives or whole websites. This is often a multistage process where you need to generate URLs first, download the content, find more URLs, and so on.</p>

<p>This section explains one of these “spidering” examples in detail and creates a scalable blueprint that can be used for downloading thousands (or millions) of pages.</p>

<section data-type="sect2" data-pdf-bookmark="Introducing the Use Case"><div class="sect2" id="idm45634204838552">
<h2>Introducing the Use Case</h2>

<p>Parsing a single <a contenteditable="false" data-type="indexterm" data-primary="use cases" data-secondary="for scraping websites" data-secondary-sortas="scraping websites" id="idm45634204836856"/><a contenteditable="false" data-type="indexterm" data-primary="scraping websites" data-secondary="use cases for" id="idm45634204835176"/><a contenteditable="false" data-type="indexterm" data-primary="datasets, examples of" data-secondary="Reuters News Archive" id="idm45634204833800"/><a contenteditable="false" data-type="indexterm" data-primary="news archives" id="idm45634204832424"/><a contenteditable="false" data-type="indexterm" data-primary="Reuters News Archive" id="idm45634204831320"/><a contenteditable="false" data-type="indexterm" data-primary="scraping websites" data-secondary="with Reuters News Archive" data-secondary-sortas="Reuters News Archive" id="idm45634204830216"/>Reuters article is a nice exercise, but the Reuters archive is much larger and contains many articles.<a> </a>It is also possible to use the techniques we have covered to parse a larger amount. Imagine that you want to download and extract, for example, a whole forum with user-generated content or a website with scientific articles. As mentioned previously, it is often most difficult to find the correct URLs of the articles.</p>

<p>Not in this case, though. It would be possible to use <em>sitemap.xml</em>, but Reuters is generous enough to offer a dedicated archive page at <a href="https://www.reuters.com/news/archive/" class="orm:hideurl"><em>https://www.reuters.com/news/archive</em></a>. A paging functionality is also available, so it’s possible to go backward in time.</p>

<p><a data-type="xref" href="#flowchart_for_spidering_process">Figure 3-5</a> shows the steps for downloading part of the archive (called <em>spidering</em>). The process works as follows:</p>

<ol>
	<li>Define how many pages of the archive should be downloaded.</li>
	<li>Download each page of the archive into a file called <em>page-000001.html</em>, <em>page-000002.html</em>, and so on for easier inspection. Skip this step if the file is already present.</li>
	<li>For each <em>page-*.html</em> file, extract the URLs of the referenced articles.</li>
	<li>For each article URL, download the article into a local HTML file. Skip this step if the article file is already present.</li>
	<li>For each article file, extract the <a contenteditable="false" data-type="indexterm" data-primary="DataFrame (Pandas)" id="idm45634204819720"/><a contenteditable="false" data-type="indexterm" data-primary="Pandas library" data-secondary="dataframes in" id="idm45634204818616"/>content into a <code>dict</code> and combine these <code>dict</code>s into a Pandas <code>DataFrame</code>.</li>
</ol>




<figure><div id="flowchart_for_spidering_process" class="figure"><img src="Images/btap_0305.jpg" width="922" height="1114"/>
  <h6><span class="label">Figure 3-5. </span>Flowchart for spidering process.</h6>
</div></figure>

<p>In a more generic approach, it might be necessary to create intermediate URLs in step 3 (if there is an overview page for years, months, etc.) before we finally arrive at the article URLs.</p>

<p>The procedure is constructed in a way that each step can be run individually and downloads have to be performed only once. This has proven to be useful, especially when we have to extract a large number of articles/URLs, as a single missing download or malformed HTML page does not mean that the whole procedure including downloading has to be started again. Moreover, <a contenteditable="false" data-type="indexterm" data-primary="idempotent, spidering process as" id="idm45634204812760"/>the process can be restarted anytime and downloads only data that has not yet been downloaded. This is called <em>idempotence</em> and is often a useful concept when interacting with “expensive” APIs.</p>

<p>The finished program looks like this:</p>

<pre data-code-language="python" data-executable="true" data-type="programlisting">
<code class="kn">import</code> <code class="nn">requests</code>
<code class="kn">from</code> <code class="nn">bs4</code> <code class="kn">import</code> <code class="n">Beautiful</code> <code class="n">Soup</code>
<code class="kn">import</code> <code class="nn">os.path</code>
<code class="kn">from</code> <code class="nn">dateutil</code> <code class="kn">import</code> <code class="n">parser</code>

<code class="k">def</code> <code class="nf">download_archive_page</code><code class="p">(</code><code class="n">page</code><code class="p">):</code>
<code class="err"> </code> <code class="err"> </code> <code class="n">filename</code> <code class="o">=</code> <code class="s2">"page-</code><code class="si">%06d</code><code class="s2">.html"</code> <code class="o">%</code> <code class="n">page</code>
<code class="err"> </code> <code class="err"> </code> <code class="k">if</code> <code class="ow">not</code> <code class="n">os</code><code class="o">.</code><code class="n">path</code><code class="o">.</code><code class="n">isfile</code><code class="p">(</code><code class="n">filename</code><code class="p">):</code>
<code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="n">url</code> <code class="o">=</code> <code class="s2">"https://www.reuters.com/news/archive/"</code> <code class="o">+</code> \
<code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="s2">"?view=page&amp;page=</code><code class="si">%d</code><code class="s2">&amp;pageSize=10"</code> <code class="o">%</code> <code class="n">page</code>
<code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="n">r</code> <code class="o">=</code> <code class="n">requests</code><code class="o">.</code><code class="n">get</code><code class="p">(</code><code class="n">url</code><code class="p">)</code>
<code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="k">with</code> <code class="nb">open</code><code class="p">(</code><code class="n">filename</code><code class="p">,</code> <code class="s2">"w+"</code><code class="p">)</code> <code class="k">as</code> <code class="n">f</code><code class="p">:</code>
<code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="n">f</code><code class="o">.</code><code class="n">write</code><code class="p">(</code><code class="n">r</code><code class="o">.</code><code class="n">text</code><code class="p">)</code>

<code class="k">def</code> <code class="nf">parse_archive_page</code><code class="p">(</code><code class="n">page_file</code><code class="p">):</code>
<code class="err"> </code> <code class="err"> </code> <code class="k">with</code> <code class="nb">open</code><code class="p">(</code><code class="n">page_file</code><code class="p">,</code> <code class="s2">"r"</code><code class="p">)</code> <code class="k">as</code> <code class="n">f</code><code class="p">:</code>
<code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="n">html</code> <code class="o">=</code> <code class="n">f</code><code class="o">.</code><code class="n">read</code><code class="p">()</code>

<code class="err"> </code> <code class="err"> </code> <code class="n">soup</code> <code class="o">=</code> <code class="n">Beautiful</code> <code class="n">Soup</code><code class="p">(</code><code class="n">html</code><code class="p">,</code> <code class="s1">'html.parser'</code><code class="p">)</code>
<code class="err"> </code> <code class="err"> </code> <code class="n">hrefs</code> <code class="o">=</code> <code class="p">[</code><code class="s2">"https://www.reuters.com"</code> <code class="o">+</code> <code class="n">a</code><code class="p">[</code><code class="s1">'href'</code><code class="p">]</code>
<code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code><code class="k">for</code> <code class="n">a</code> <code class="ow">in</code> <code class="n">soup</code><code class="o">.</code><code class="n">select</code><code class="p">(</code><code class="s2">"article.story div.story-content a"</code><code class="p">)]</code>
<code class="err"> </code> <code class="err"> </code> <code class="k">return</code> <code class="n">hrefs</code>

<code class="k">def</code> <code class="nf">download_article</code><code class="p">(</code><code class="n">url</code><code class="p">):</code>
<code class="err"> </code> <code class="err"> </code> <code class="c1"># check if article already there</code>
<code class="err"> </code> <code class="err"> </code> <code class="n">filename</code> <code class="o">=</code> <code class="n">url</code><code class="o">.</code><code class="n">split</code><code class="p">(</code><code class="s2">"/"</code><code class="p">)[</code><code class="o">-</code><code class="mi">1</code><code class="p">]</code> <code class="o">+</code> <code class="s2">".html"</code>
<code class="err"> </code> <code class="err"> </code> <code class="k">if</code> <code class="ow">not</code> <code class="n">os</code><code class="o">.</code><code class="n">path</code><code class="o">.</code><code class="n">isfile</code><code class="p">(</code><code class="n">filename</code><code class="p">):</code>
<code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="n">r</code> <code class="o">=</code> <code class="n">requests</code><code class="o">.</code><code class="n">get</code><code class="p">(</code><code class="n">url</code><code class="p">)</code>
<code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="k">with</code> <code class="nb">open</code><code class="p">(</code><code class="n">filename</code><code class="p">,</code> <code class="s2">"w+"</code><code class="p">)</code> <code class="k">as</code> <code class="n">f</code><code class="p">:</code>
<code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="n">f</code><code class="o">.</code><code class="n">write</code><code class="p">(</code><code class="n">r</code><code class="o">.</code><code class="n">text</code><code class="p">)</code>

<code class="k">def</code> <code class="nf">parse_article</code><code class="p">(</code><code class="n">article_file</code><code class="p">):</code>
<code class="err"> </code> <code class="err"> </code> <code class="k">with</code> <code class="nb">open</code><code class="p">(</code><code class="n">article_file</code><code class="p">,</code> <code class="s2">"r"</code><code class="p">)</code> <code class="k">as</code> <code class="n">f</code><code class="p">:</code>
<code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="n">html</code> <code class="o">=</code> <code class="n">f</code><code class="o">.</code><code class="n">read</code><code class="p">()</code>
<code class="err"> </code> <code class="err"> </code> <code class="n">r</code> <code class="o">=</code> <code class="p">{}</code>
<code class="err"> </code> <code class="err"> </code> <code class="n">soup</code> <code class="o">=</code> <code class="n">Beautiful</code> <code class="n">Soup</code><code class="p">(</code><code class="n">html</code><code class="p">,</code> <code class="s1">'html.parser'</code><code class="p">)</code>
<code class="err"> </code> <code class="err"> </code> <code class="n">r</code><code class="p">[</code><code class="s1">'id'</code><code class="p">]</code> <code class="o">=</code> <code class="n">soup</code><code class="o">.</code><code class="n">select_one</code><code class="p">(</code><code class="s2">"div.StandardArticle_inner-container"</code><code class="p">)[</code><code class="s1">'id'</code><code class="p">]</code>
<code class="err"> </code> <code class="err"> </code> <code class="n">r</code><code class="p">[</code><code class="s1">'url'</code><code class="p">]</code> <code class="o">=</code> <code class="n">soup</code><code class="o">.</code><code class="n">find</code><code class="p">(</code><code class="s2">"link"</code><code class="p">,</code> <code class="p">{</code><code class="s1">'rel'</code><code class="p">:</code> <code class="s1">'canonical'</code><code class="p">})[</code><code class="s1">'href'</code><code class="p">]</code>
<code class="err"> </code> <code class="err"> </code> <code class="n">r</code><code class="p">[</code><code class="s1">'headline'</code><code class="p">]</code> <code class="o">=</code> <code class="n">soup</code><code class="o">.</code><code class="n">h1</code><code class="o">.</code><code class="n">text</code>
<code class="err"> </code> <code class="err"> </code> <code class="n">r</code><code class="p">[</code><code class="s1">'section'</code><code class="p">]</code> <code class="o">=</code> <code class="n">soup</code><code class="o">.</code><code class="n">select_one</code><code class="p">(</code><code class="s2">"div.ArticleHeader_channel a"</code><code class="p">)</code><code class="o">.</code><code class="n">text</code><code class="err"> </code> <code class="err">  </code>
<code class="err"> </code> <code class="err"> </code> <code class="n">r</code><code class="p">[</code><code class="s1">'text'</code><code class="p">]</code> <code class="o">=</code> <code class="n">soup</code><code class="o">.</code><code class="n">select_one</code><code class="p">(</code><code class="s2">"div.StandardArticleBody_body"</code><code class="p">)</code><code class="o">.</code><code class="n">text</code>
<code class="err"> </code> <code class="err"> </code> <code class="n">r</code><code class="p">[</code><code class="s1">'authors'</code><code class="p">]</code> <code class="o">=</code> <code class="p">[</code><code class="n">a</code><code class="o">.</code><code class="n">text</code>
<code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="k">for</code> <code class="n">a</code> <code class="ow">in</code> <code class="n">soup</code><code class="o">.</code><code class="n">select</code><code class="p">(</code><code class="s2">"div.BylineBar_first-container.</code><code class="se">\</code>
<code class="s2">                                          ArticleHeader_byline-bar</code><code class="se">\</code>
<code class="s2">                                          div.BylineBar_byline span"</code><code class="p">)]</code>
<code class="err"> </code> <code class="err"> </code> <code class="n">r</code><code class="p">[</code><code class="s1">'time'</code><code class="p">]</code> <code class="o">=</code> <code class="n">soup</code><code class="o">.</code><code class="n">find</code><code class="p">(</code><code class="s2">"meta"</code><code class="p">,</code> <code class="p">{</code> <code class="s1">'property'</code><code class="p">:</code>
                                    <code class="s2">"og:article:published_time"</code><code class="p">})[</code><code class="s1">'content'</code><code class="p">]</code>
<code class="err"> </code> <code class="err"> </code> <code class="k">return</code> <code class="n">r</code>
</pre>

<p>Having defined these functions, they can be invoked with parameters (which can easily be changed):</p>

<pre data-code-language="python" data-type="programlisting">
<code class="c1"># download 10 pages of archive</code>
<code class="k">for</code> <code class="n">p</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="mi">10</code><code class="p">):</code>
<code class="err"> </code> <code class="err"> </code> <code class="n">download_archive_page</code><code class="p">(</code><code class="n">p</code><code class="p">)</code>

<code class="c1"># parse archive and add to article_urls</code>
<code class="kn">import</code> <code class="nn">glob</code>

<code class="n">article_urls</code> <code class="o">=</code> <code class="p">[]</code>
<code class="k">for</code> <code class="n">page_file</code> <code class="ow">in</code> <code class="n">glob</code><code class="o">.</code><code class="n">glob</code><code class="p">(</code><code class="s2">"page-*.html"</code><code class="p">):</code>
<code class="err"> </code> <code class="err"> </code> <code class="n">article_urls</code> <code class="o">+=</code> <code class="n">parse_archive_page</code><code class="p">(</code><code class="n">page_file</code><code class="p">)</code>

<code class="c1"># download articles</code>
<code class="k">for</code> <code class="n">url</code> <code class="ow">in</code> <code class="n">article_urls</code><code class="p">:</code>
<code class="err"> </code> <code class="err"> </code> <code class="n">download_article</code><code class="p">(</code><code class="n">url</code><code class="p">)</code>

<code class="c1"># arrange in pandas DataFrame</code>
<code class="kn">import</code> <code class="nn">pandas</code> <code class="kn">as</code> <code class="nn">pd</code>

<code class="n">df</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">()</code>
<code class="k">for</code> <code class="n">article_file</code> <code class="ow">in</code> <code class="n">glob</code><code class="o">.</code><code class="n">glob</code><code class="p">(</code><code class="s2">"*-id???????????.html"</code><code class="p">):</code>
<code class="err"> </code> <code class="err"> </code> <code class="n">df</code> <code class="o">=</code> <code class="n">df</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">parse_article</code><code class="p">(</code><code class="n">article_file</code><code class="p">),</code> <code class="n">ignore_index</code><code class="o">=</code><code class="bp">True</code><code class="p">)</code>

<code class="n">df</code><code class="p">[</code><code class="s1">'time'</code><code class="p">]</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">to_datetime</code><code class="p">(</code><code class="n">df</code><code class="o">.</code><code class="n">time</code><code class="p">)</code>
</pre>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Error Handling and Production-Quality Software"><div class="sect2" id="idm45634204837928">
<h2>Error Handling and Production-Quality Software</h2>

<p>For simplicity, all <a contenteditable="false" data-type="indexterm" data-primary="error handling" id="idm45634204299320"/>example programs discussed in this chapter do not use error handling. For production software, however, you should use <a contenteditable="false" data-type="indexterm" data-primary="exception handling" id="idm45634204297960"/>exception handling. As HTML can change frequently and pages might be incomplete, errors can happen at any time, so it is a good idea to use try/except generously and log the errors. If systematic errors occur, you should look for the root cause and eliminate it. If errors occur only sporadically or due to malformed HTML, you can probably ignore them, as they might also be due to server software.</p>

<p>Using the download and save file mechanism described earlier, the extraction procedure can be restarted anytime or also be applied to certain problematic files <span class="keep-together">separately.</span> This is often a big advantage and helps to achieve a cleanly extracted dataset fast.</p>

<p>Generating URLs is often as difficult as extracting content and is frequently related to it. In many cases, this has to be repeated several times to download, for example, hierarchical content.</p>

<p>When you download data, always <a contenteditable="false" data-type="indexterm" data-primary="backups" id="idm45634204293992"/><a contenteditable="false" data-type="indexterm" data-primary="copies for traceability" id="idm45634204292888"/>find a filename for each URL and save it to the filesystem. You will have to restart the process more often than you think. Not having to download everything over and over is immensely useful, especially during the development process.</p>

<p>If you have downloaded and extracted the data, you will probably want to persist it for later use. An easy way is to save it in individual JSON files. If you have many files, using a directory structure might be a good option. With an increasing number of pages, even this might not scale well, and it’s a better idea to use a database or another columnar <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch3_term21" id="idm45634204290616"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch3_term22" id="idm45634204289240"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch3_term23" id="idm45634204287864"/>data store.</p>
</div></section>
</div></section>

<section data-type="sect1" data-pdf-bookmark="Density-Based Text Extraction"><div class="sect1" id="idm45634204286232">
<h1>Density-Based Text Extraction</h1>

<p>Extracting <a contenteditable="false" data-type="indexterm" data-primary="density-based text extraction" id="ch3_term25"/><a contenteditable="false" data-type="indexterm" data-primary="text data extraction from web" data-secondary="density of information as basis for" id="ch3_term26"/>structured data from HTML is not complicated, but it is tedious. If you want to extract data from a whole website, it is well worth the effort as you only have to implement the extraction for a limited number of page types.</p>

<p>However, you may need to extract text from many different websites. Implementing the extraction for each of them does not scale well. There is some metadata that can be found easily, such as title, description, etc. But the text itself is not so easy to find.</p>

<p>Taking a look at the information density, there are some heuristics that allow extraction of the text. The algorithm behind it measures the <em>density of information</em> and therefore automatically eliminates repeated information such as headers, navigation, footers, and so on. The implementation is not so simple but is <a contenteditable="false" data-type="indexterm" data-primary="python-readability library" id="idm45634204279160"/><a contenteditable="false" data-type="indexterm" data-primary="readability library (Python)" id="idm45634204278088"/>fortunately available in a library called <a href="https://oreil.ly/AemZh"><code>python-readability</code></a>. The name originates from a now-orphaned browser plugin called Readability, which was conceived to remove clutter from web pages and make them easily readable—exactly what is needed here. To get started, we must first install <code>python-readability</code> (<strong><code>pip install readability-lxml</code></strong>).</p>
<section data-type="sect2" data-pdf-bookmark="Extracting Reuters Content with Readability"><div class="sect2" id="idm45634204274760">
<h2>Extracting Reuters Content with Readability</h2>

<p>Let’s see how this works in the <a contenteditable="false" data-type="indexterm" data-primary="Reuters News Archive" id="idm45634204273096"/><a contenteditable="false" data-type="indexterm" data-primary="news archives" id="idm45634204271992"/><a contenteditable="false" data-type="indexterm" data-primary="datasets, examples of" data-secondary="Reuters News Archive" id="idm45634204270888"/>Reuters example. We keep the HTML we have downloaded, but of course you can also use a file or URL:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="kn">from</code> <code class="nn">readability</code> <code class="kn">import</code> <code class="n">Document</code>

<code class="n">doc</code> <code class="o">=</code> <code class="n">Document</code><code class="p">(</code><code class="n">html</code><code class="p">)</code>
<code class="n">doc</code><code class="o">.</code><code class="n">title</code><code class="p">()</code>
</pre>

<p><code>Out:</code></p>

<pre data-type="programlisting">
'Banned in Boston: Without vaping, medical marijuana patients must adapt -
Reuters'
</pre>

<p>As you can see, that was easy. The title can be extracted via the corresponding element. However, the library can do some additional tricks, such as finding the title or the summary of the page:</p>

<pre class="pre" data-code-language="python" data-type="programlisting">
<code class="n">doc</code><code class="o">.</code><code class="n">short_title</code><code class="p">()</code>
</pre>

<p><code>Out:</code></p>

<pre class="pre" data-code-language="python" data-type="programlisting">
<code class="s1">'Banned in Boston: Without vaping, medical marijuana patients must adapt'</code>
</pre>

<p>That is already quite good. Let’s check how well it works for the actual content:</p>

<pre data-code-language="python" data-executable="true" data-type="programlisting">
<code class="n">doc</code><code class="o">.</code><code class="n">summary</code><code class="p">()</code>
</pre>

<p class="pagebreak-before"><code>Out:</code></p>

<pre data-type="programlisting">
'&lt;html&gt;&lt;body&gt;&lt;div&gt;&lt;div class="StandardArticleBody_body"&gt;&lt;p&gt;BOSTON (Reuters) -
In the first few days of [...] &lt;/p&gt;

&lt;div class="Attribution_container"&gt;&lt;div class="Attribution_attribution"&gt;
&lt;p class="Attribution_content"&gt;Reporting Jacqueline Tempera in Brookline
and Boston, Massachusetts, and Jonathan Allen in New York; Editing by Frank
McGurty and Bill Berkrot&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;'
</pre>

<p>The data still has some remaining HTML structure, which can be useful to keep because paragraphs are included. Of course, the body part can be extracted again with <a contenteditable="false" data-type="indexterm" data-primary="Beautiful Soup library (Python)" id="idm45634204008024"/>Beautiful Soup:</p>

<pre data-code-language="python" data-executable="true" data-type="programlisting">
<code class="n">density_soup</code> <code class="o">=</code> <code class="n">Beautiful</code> <code class="n">Soup</code><code class="p">(</code><code class="n">doc</code><code class="o">.</code><code class="n">summary</code><code class="p">(),</code> <code class="s1">'html.parser'</code><code class="p">)</code>
<code class="n">density_soup</code><code class="o">.</code><code class="n">body</code><code class="o">.</code><code class="n">text</code>
</pre>

<p><code>Out:</code></p>

<pre data-type="programlisting">
'BOSTON (Reuters) - In the first few days of the four-month ban on all vaping
products in Massachusetts, Laura Lee Medeiros, a medical marijuana patient,
began to worry.\xa0 FILE PHOTO: An employee puts down an eighth of an ounce
marijuana after letting a customer smell it outside the Magnolia cannabis
lounge in Oakland, California, U.S. [...]

Reporting Jacqueline Tempera in Brookline and Boston, Massachusetts, and
Jonathan Allen in New York; Editing by Frank McGurty and Bill Berkrot'
</pre>

<p>In this case, the results are excellent. In most of the cases, <code>python-readability</code> works reasonably well and removes the need to implement too many special cases. However, the cost of using this library is uncertainty. Will it always work in the expected way with the impossibility of extracting structured data such as timestamps, authors, and so on (although there might be other heuristics for that)?</p>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Summary Density-Based Text Extraction"><div class="sect2" id="idm45634204274168">
<h2>Summary Density-Based Text Extraction</h2>

<p>Density-based text extraction is powerful when using both heuristics and statistical information about information distribution on an HTML page. You should keep in mind that the results are almost always worse when compared to implementing a specific extractor. However, if you need to extract content from many different page types or from an archive where you don’t have a fixed layout at all, it might well be worth it to go that way.</p>

<p>Performing a detailed quality assurance afterward is even more essential compared to the structured approach as both the heuristics and the statistics might sometimes go in the <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch3_term25" id="idm45634203901128"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch3_term26" id="idm45634203899752"/>wrong direction.</p>
</div></section>
</div></section>

<section data-type="sect1" class="pagebreak-before less_space" data-pdf-bookmark="All-in-One Approach"><div class="sect1" id="idm45634203897992">
<h1>All-in-One Approach</h1>

<p><a href="https://scrapy.org"><em>Scrapy</em></a> is another Python package that offers an all-in-one approach to <a contenteditable="false" data-type="indexterm" data-primary="spidering process" id="idm45634203895272"/><a contenteditable="false" data-type="indexterm" data-primary="text data extraction from web" data-secondary="spidering process for" id="idm45634203894200"/><a contenteditable="false" data-type="indexterm" data-primary="downloading with spidering" id="idm45634203892856"/><a contenteditable="false" data-type="indexterm" data-primary="downloading HTML pages" id="ch3_term27"/><a contenteditable="false" data-type="indexterm" data-primary="scraping websites" data-secondary="downloading HTML pages for" id="ch3_term28"/><a contenteditable="false" data-type="indexterm" data-primary="HTML pages, downloading" id="ch3_term33"/><a contenteditable="false" data-type="indexterm" data-primary="scraping websites" data-secondary="Scrapy all-in-one approach for" id="ch3_term29"/><a contenteditable="false" data-type="indexterm" data-primary="Scrapy package" id="ch3_term30"/><a contenteditable="false" data-type="indexterm" data-primary="text data extraction from web" data-secondary="Scrapy all-in-one approach for" id="ch3_term31"/>spidering and content extraction. The methods are similar to the ones described in the earlier sections, although Scrapy is more suited for downloading <em>whole</em> websites and not only parts of them.</p>

<p>The object-oriented, holistic approach of Scrapy is definitely nice, and the code is readable. However, it turns out to be quite difficult to restart spidering and extraction without having to download the whole website again.</p>

<p>Compared to the approach described earlier, downloading must also happen in Python. For websites with a huge number of pages, HTTP keep-alive cannot be used, and gzip encoding is also difficult. Both can be easily integrated in the modular method by externalizing the downloads via tools such as wget.</p>
</div></section>
<section data-type="sect1" class="blueprint" data-pdf-bookmark="Blueprint: Scraping the Reuters Archive with Scrapy"><div class="sect1" id="idm45634203863864">
<h1>Blueprint: Scraping the Reuters Archive with Scrapy</h1>

<p>Let’s see how the download of the archive and the articles would look in Scrapy. Go ahead and install Scrapy (either via <strong><code>conda install scrapy</code></strong> or <strong><code>pip install scrapy</code></strong>).</p>

<pre class="pre drop-element-attached-top drop-element-attached-center drop-target-attached-bottom drop-target-attached-center" data-code-language="python" data-executable="true" data-type="programlisting">
<code class="kn">import</code> <code class="nn">scrapy</code>
<code class="kn">import</code> <code class="nn">logging</code>

<code class="k">class</code> <code class="nc">ReutersArchiveSpider</code><code class="p">(</code><code class="n">scrapy</code><code class="o">.</code><code class="n">Spider</code><code class="p">):</code>
<code class="err"> </code> <code class="err"> </code> <code class="n">name</code> <code class="o">=</code> <code class="s1">'reuters-archive'</code>

<code class="err"> </code> <code class="err"> </code> <code class="n">custom_settings</code> <code class="o">=</code> <code class="p">{</code>
<code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="s1">'LOG_LEVEL'</code><code class="p">:</code> <code class="n">logging</code><code class="o">.</code><code class="n">WARNING</code><code class="p">,</code>
<code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="s1">'FEED_FORMAT'</code><code class="p">:</code> <code class="s1">'json'</code><code class="p">,</code>
<code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="s1">'FEED_URI'</code><code class="p">:</code> <code class="s1">'reuters-archive.json'</code>
<code class="err"> </code> <code class="err"> </code> <code class="p">}</code>

<code class="err"> </code> <code class="err"> </code> <code class="n">start_urls</code> <code class="o">=</code> <code class="p">[</code>
<code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="s1">'https://www.reuters.com/news/archive/'</code><code class="p">,</code>
<code class="err"> </code> <code class="err"> </code> <code class="p">]</code>

<code class="err"> </code> <code class="err"> </code> <code class="k">def</code> <code class="nf">parse</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">response</code><code class="p">):</code>
<code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="k">for</code> <code class="n">article</code> <code class="ow">in</code> <code class="n">response</code><code class="o">.</code><code class="n">css</code><code class="p">(</code><code class="s2">"article.story div.story-content a"</code><code class="p">):</code>
<code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="k">yield</code> <code class="n">response</code><code class="o">.</code><code class="n">follow</code><code class="p">(</code><code class="n">article</code><code class="o">.</code><code class="n">css</code><code class="p">(</code><code class="s2">"a::attr(href)"</code><code class="p">)</code><code class="o">.</code><code class="n">extract_first</code><code class="p">(),</code>
                                  <code class="bp">self</code><code class="o">.</code><code class="n">parse_article</code><code class="p">)</code>
<code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="n">next_page_url</code> <code class="o">=</code> <code class="n">response</code><code class="o">.</code><code class="n">css</code><code class="p">(</code><code class="s1">'a.control-nav-next::attr(href)'</code><code class="p">)</code><code class="o">.</code>\
                        <code class="n">extract_first</code><code class="p">()</code>
<code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="k">if</code> <code class="p">(</code><code class="n">next_page_url</code> <code class="ow">is</code> <code class="ow">not</code> <code class="bp">None</code><code class="p">)</code> <code class="o">&amp;</code> <code class="p">(</code><code class="s1">'page=2'</code> <code class="ow">not</code> <code class="ow">in</code> <code class="n">next_page_url</code><code class="p">):</code>
<code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="k">yield</code> <code class="n">response</code><code class="o">.</code><code class="n">follow</code><code class="p">(</code><code class="n">next_page_url</code><code class="p">,</code> <code class="bp">self</code><code class="o">.</code><code class="n">parse</code><code class="p">)</code>

<code class="err"> </code> <code class="err"> </code> <code class="k">def</code> <code class="nf">parse_article</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">response</code><code class="p">):</code>
<code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="k">yield</code> <code class="p">{</code>
<code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="s1">'title'</code><code class="p">:</code> <code class="n">response</code><code class="o">.</code><code class="n">css</code><code class="p">(</code><code class="s1">'h1::text'</code><code class="p">)</code><code class="o">.</code><code class="n">extract_first</code><code class="p">()</code><code class="o">.</code><code class="n">strip</code><code class="p">(),</code>
<code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="s1">'section'</code><code class="p">:</code> <code class="n">response</code><code class="o">.</code><code class="n">css</code><code class="p">(</code><code class="s1">'div.ArticleHeader_channel a::text'</code><code class="p">)</code><code class="o">.</code>\
                     <code class="n">extract_first</code><code class="p">()</code><code class="o">.</code><code class="n">strip</code><code class="p">(),</code>
<code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="s1">'text'</code><code class="p">:</code> <code class="s2">"</code><code class="se">\n</code><code class="s2">"</code><code class="o">.</code><code class="n">join</code><code class="p">(</code><code class="n">response</code><code class="o">.</code>\
                  <code class="n">css</code><code class="p">(</code><code class="s1">'div.StandardArticleBody_body p::text'</code><code class="p">)</code><code class="o">.</code><code class="n">extract</code><code class="p">())</code>
<code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="p">}</code>
</pre>

<p>Scrapy works in an object-oriented way. For each so-called spider, a class needs to be implemented that is derived from <code>scrapy.Spider</code>. Scrapy adds a lot of debug output, which is reduced in the previous example by <code>logging.WARNING</code>. The <a contenteditable="false" data-type="indexterm" data-primary="parse function (Scrapy)" id="idm45634203848520"/>base class automatically calls the parse function with the <code>start_urls</code>. This function extracts the links to the article and invokes <code>yield</code> with the function <code>parse_article</code> as a parameter. This function in turn extracts some attributes from the articles and yields them <span class="keep-together">in a</span> <code>dict</code>. Finally, the next page link is crawled, but we stop here before getting the <span class="keep-together">second</span> page.</p>

<p><code>yield</code> has a double functionality in Scrapy. If a <code>dict</code> is yielded, it is added to the results. If a <code>Request</code> object is yielded, the object is fetched and gets parsed.</p>

<div data-type="warning" epub:type="warning">
<h1>Scrapy and Jupyter</h1>

<p>Scrapy is optimized for command-line usage, but it can also be invoked in a Jupyter notebook. Because of <a contenteditable="false" data-type="indexterm" data-primary="Twisted environment (Scrapy)" id="idm45634203619592"/>Scrapy’s usage of the (ancient) <a href="https://oreil.ly/j6HCm">Twisted environment</a>, the scraping cannot be restarted, so you have only one shot if you try it in the notebook (otherwise you have to restart the notebook):</p>

<pre data-code-language="python" data-executable="true" data-type="programlisting">
<code class="c1"># this can be run only once from a Jupyter notebook</code>
<code class="c1"># due to Twisted</code>
<code class="kn">from</code> <code class="nn">scrapy.crawler</code> <code class="kn">import</code> <code class="n">CrawlerProcess</code>
<code class="n">process</code> <code class="o">=</code> <code class="n">CrawlerProcess</code><code class="p">()</code>

<code class="n">process</code><code class="o">.</code><code class="n">crawl</code><code class="p">(</code><code class="n">ReutersArchiveSpider</code><code class="p">)</code>
<code class="n">process</code><code class="o">.</code><code class="n">start</code><code class="p">()</code>
</pre>
</div>

<p>Here are a few things worth mentioning:</p>

<ul>
	<li>The all-in-one approach looks elegant and concise.</li>
	<li>As most of the coding is spent in extracting data in the articles, this code has to change frequently. For this, <a contenteditable="false" data-type="indexterm" data-primary="spidering process" id="idm45634203592600"/><a contenteditable="false" data-type="indexterm" data-primary="text data extraction from web" data-secondary="spidering process for" id="idm45634203591528"/><a contenteditable="false" data-type="indexterm" data-primary="downloading with spidering" id="idm45634203590184"/>spidering has to be restarted (and if you are running the script in Jupyter, you also have to start the Jupyter notebook server), which tremendously increases turnaround times.</li>
	<li>It’s nice that <a contenteditable="false" data-type="indexterm" data-primary="JSON format" id="idm45634203588376"/>JSON can directly be produced. Be careful as the JSON file is appended, which can result in an invalid JSON if you don’t delete the file before starting the spidering process. This can be solved by using the so-called jl format (JSON lines), but it is a workaround.</li>
	<li>Scrapy has some nice ideas. In our day-to-day work, we do not use it, mainly because debugging is hard. If persistence of the HTML files is needed (which we strongly suggest), it loses lots of advantages. The object-oriented approach is useful and can be implemented outside of Scrapy without too much effort.</li>
</ul>

<p>As Scrapy also uses <a contenteditable="false" data-type="indexterm" data-primary="CSS selectors" id="idm45634203585544"/><a contenteditable="false" data-type="indexterm" data-primary="selectors, CSS" id="idm45634203584408"/>CSS selectors for extracting HTML content, the basic technologies are the same as with the other approaches. There are considerable differences in the downloading method, though. Having <a contenteditable="false" data-type="indexterm" data-primary="Twisted environment (Scrapy)" id="idm45634203582968"/>Twisted as a backend creates some overhead and imposes a special programming model.</p>

<p>Decide carefully whether an all-in-one approach suits your project needs. For some websites, ready-made Scrapy spiders might already be <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch3_term27" id="idm45634203581192"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch3_term28" id="idm45634203579816"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch3_term29" id="idm45634203578440"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch3_term30" id="idm45634203577064"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch3_term31" id="idm45634203575688"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch3_term33" id="idm45634203574312"/>available and can be reused.</p>
</div></section>


<section data-type="sect1" data-pdf-bookmark="Possible Problems with Scraping"><div class="sect1" id="idm45634203863240">
<h1>Possible Problems with Scraping</h1>

<p>Before <a contenteditable="false" data-type="indexterm" data-primary="scraping websites" data-secondary="problems with" id="idm45634203548104"/>scraping content, it is always <a contenteditable="false" data-type="indexterm" data-primary="downloading data, legal issues with" id="idm45634203546520"/><a contenteditable="false" data-type="indexterm" data-primary="scraping websites" data-secondary="legal issues with" id="idm45634203545352"/>worthwhile to consider possible copyright and data protection issues.</p>

<p>More and more web applications are constructed using frameworks like <a href="https://reactjs.org">React</a>. They have only a single page, and data is transferred via an API. This <a contenteditable="false" data-type="indexterm" data-primary="JavaScript " id="idm45634203542744"/>often leads to websites not working without JavaScript. Sometimes there are specialized URLs constructed for search engines that are also useful for spidering. Usually, those can be found in <em>sitemap.xml</em>. You can try it by switching off JavaScript in your browser and then see whether the website still works.</p>

<p>If JavaScript is needed, you can find requests on the Network tab by using the Web Inspector of the browser and clicking around the application. Sometimes, JSON is used to transfer the data, which makes extraction often much easier compared to HTML. However, the individual JSON URLs still have to be generated, and there might be additional parameters to avoid <a href="https://oreil.ly/_6O_Q">cross-site request forgery (CSRF)</a>.</p>

<p>Requests can become quite complicated, such as in the Facebook timeline, on Instagram, or on Twitter. Obviously, these websites try to keep their content for themselves and avoid spidering.</p>

<p>For complicated cases, it can be useful to “remote control” the browser by using <a href="https://oreil.ly/YssLD">Selenium</a>, a framework that was originally conceived for the automated testing of web applications, or a <a href="https://oreil.ly/CH2ZI">headless browser</a>.</p>

<p>Websites like <a contenteditable="false" data-type="indexterm" data-primary="scraping websites" data-secondary="blocks and bans as result of" id="idm45634203536296"/>Google try to detect automatic download attempts and start sending captchas. This can also happen with other websites. Most of the time this is bound to certain IP addresses. The website must then be “unlocked” with a normal browser, and the automatic requests should be sent with larger pauses between them.</p>

<p>Another method to avoid content extraction is obfuscated HTML code where CSS classes have totally random names. If the names do not change, this is more work initially to find the correct selectors but should work automatically afterward. If <span class="keep-together">the names</span> change every day (for example), content extraction becomes <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch3_term14" id="idm45634203533000"/>extremely <span class="keep-together">difficult</span>.</p>
</div></section>

<section data-type="sect1" data-pdf-bookmark="Closing Remarks and Recommendation"><div class="sect1" id="idm45634203530648">
<h1>Closing Remarks and Recommendation</h1>

<p>Web scraping <a contenteditable="false" data-type="indexterm" data-primary="scraping websites" data-secondary="about" id="idm45634203529240"/>is a powerful and scalable technique to acquire content. The necessary Python infrastructure supports scraping projects in an excellent way. The combination of the requests library and <a contenteditable="false" data-type="indexterm" data-primary="Beautiful Soup library (Python)" id="idm45634203527496"/>Beautiful Soup is comfortable and works well for moderately large scraping jobs.</p>

<p>As we have seen throughout the chapter, we can systematically split up large scraping projects into URL generation and downloading phases. If the number of documents becomes really big, external tools like <code>wget</code> might be more appropriate compared to requests. As soon as everything is downloaded, Beautiful Soup can be used to extract the content.</p>

<p>If you want to minimize waiting time, all stages can be run in parallel.</p>

<p>In any case, you should be aware of the legal aspects and behave as an “ethical scraper” by respecting the rules in <em>robots.txt</em>.</p>
</div></section>
<div data-type="footnotes"><p data-type="footnote" id="idm45634207731272"><sup><a href="ch03.xhtml#idm45634207731272-marker">1</a></sup> Reuters is a news website and changes daily. Therefore, expect completely different results when running the code!</p><p data-type="footnote" id="idm45634208553592"><sup><a href="ch03.xhtml#idm45634208553592-marker">2</a></sup> You might have to install the package first with <strong><code>pip install xmltodict</code></strong>.</p><p data-type="footnote" id="idm45634212104584"><sup><a href="ch03.xhtml#idm45634212104584-marker">3</a></sup> Reuters is a news site, and the content is continually updated. Note that your results will definitely be <span class="keep-together">different!</span></p><p data-type="footnote" id="idm45634206884968"><sup><a href="ch03.xhtml#idm45634206884968-marker">4</a></sup> Just after the time of writing, Reuters stopped providing RSS feeds, which led to a public outcry. We hope that RSS feeds will be restored. The Jupyter notebook for this chapter <a href="https://oreil.ly/Wamlu">on GitHub</a> uses an archived version of the RSS feed from the Internet archive.</p><p data-type="footnote" id="idm45634206875096"><sup><a href="ch03.xhtml#idm45634206875096-marker">5</a></sup> As stated previously, Reuters is a dynamically generated website, and your results will be different!</p><p data-type="footnote" id="idm45634206104264"><sup><a href="ch03.xhtml#idm45634206104264-marker">6</a></sup> HTML cannot be parsed with <a href="https://oreil.ly/EeCjy">regular expressions</a>.</p><p data-type="footnote" id="idm45634206031320"><sup><a href="ch03.xhtml#idm45634206031320-marker">7</a></sup> See <em>CSS: The Definitive Guide, 4th Edition</em> by Eric A. Meyer and Estelle Weyl (O’Reilly, 2017)</p></div></div></section></div>



  </body></html>