- en: Chapter 7\. Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A regression and classification technique that has enjoyed a renaissance over
    the past 10 years is neural networks. In the simplest definition, a *neural network*
    is a multilayered regression containing layers of weights, biases, and nonlinear
    functions that reside between input variables and output variables. *Deep learning*
    is a popular variant of neural networks that utilizes multiple “hidden” (or middle)
    layers of nodes containing weights and biases. Each node resembles a linear function
    before being passed to a nonlinear function (called an activation function). Just
    like linear regression, which we learned about in [Chapter 5](ch05.xhtml#ch05),
    optimization techniques like stochastic gradient descent are used to find the
    optimal weight and bias values to minimize the residuals.
  prefs: []
  type: TYPE_NORMAL
- en: Neural networks offer exciting solutions to problems previously difficult for
    computers to solve. From identifying objects in images to processing words in
    audio, neural networks have created tools that affect our everyday lives. This
    includes virtual assistants and search engines, as well as photo tools in our
    iPhones.
  prefs: []
  type: TYPE_NORMAL
- en: Given the media hoopla and bold claims dominating news headlines about neural
    networks, it may be surprising that they have been around since the 1950s. The
    reason for their sudden popularity after 2010 is due to the growing availability
    of data and computing power. The ImageNet challenge between 2011 and 2015 was
    probably the largest driver of the renaissance, boosting performance on classifying
    one thousand categories on 1.4 million images to an accuracy of 96.4%.
  prefs: []
  type: TYPE_NORMAL
- en: However, like any machine learning technique it only works on narrowly defined
    problems. Even projects to create “self-driving” cars do not use end-to-end deep
    learning, and primarily use hand-coded rule systems with convoluted neural networks
    acting as a “label maker” for identifying objects on the road. We will discuss
    this later in this chapter to understand where neural networks are actually used.
    But first we will build a simple neural network in NumPy, and then use scikit-learn
    as a library implementation.
  prefs: []
  type: TYPE_NORMAL
- en: When to Use Neural Networks and Deep Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Neural networks and deep learning can be used for classification and regression,
    so how do they size up to linear regression, logistic regression, and other types
    of machine learning? You might have heard the expression “when all you have is
    a hammer, everything starts to look like a nail.” There are advantages and disadvantages
    that are situational for each type of algorithm. Linear regression and logistic
    regression, as well as gradient boosted trees (which we did not cover in this
    book), do a pretty fantastic job making predictions on structured data. Think
    of structured data as data that is easily represented as a table, with rows and
    columns. But perceptual problems like image classification are much less structured,
    as we are trying to find fuzzy correlations between groups of pixels to identify
    shapes and patterns, not rows of data in a table. Trying to predict the next four
    or five words in a sentence being typed, or deciphering the words being said in
    an audio clip, are also perceptual problems and examples of neural networks being
    used for natural language processing.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will primarily focus on simple neural networks with only
    one hidden layer.
  prefs: []
  type: TYPE_NORMAL
- en: Is Using a Neural Network Overkill?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Using neural networks for the upcoming example is probably overkill, as a logistic
    regression would probably be more practical. Even a [formulaic approach can be
    used](https://oreil.ly/M4W8i). However, I have always been a fan of understanding
    complex techniques by applying them to simple problems. You learn about the strengths
    and limitations of the technique rather than be distracted by large datasets.
    So with that in mind, try not to use neural networks where simpler models will
    be more practical. We will break this rule in this chapter for the sake of understanding
    the technique.
  prefs: []
  type: TYPE_NORMAL
- en: A Simple Neural Network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Here is a simple example to get a feel for neural networks. I want to predict
    whether a font should be light (1) or dark (0) for a given color background. Here
    are a few examples of different background colors in [Figure 7-1](#bQLVHMTtrn).
    The top row looks best with light font, and the bottom row looks best with dark
    font.
  prefs: []
  type: TYPE_NORMAL
- en: '![emds 0701](Images/emds_0701.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-1\. Light background colors look best with dark font, and dark background
    colors look best with light font
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In computer science one way to represent a color is with RGB values, or the
    red, green, and blue values. Each of these values is between 0 and 255 and expresses
    how these three colors are mixed to create the desired color. For example, if
    we express the RGB as (red, green, blue), then dark orange would have an RGB of
    (255,140,0) and pink would be (255,192,203). Black would be (0,0,0) and white
    would be (255,255,255).
  prefs: []
  type: TYPE_NORMAL
- en: From a machine learning and regression perspective, we have three numeric input
    variables `red`, `green`, and `blue` to capture a given background color. We need
    to fit a function to these input variables and output whether a light (1) or dark
    (0) font should be used for that background color.
  prefs: []
  type: TYPE_NORMAL
- en: Representing Colors Through RGB
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are hundreds of color picker palettes online to experiment with RGB values.
    W3 Schools has one [here](https://oreil.ly/T57gu).
  prefs: []
  type: TYPE_NORMAL
- en: Note this example is not far from how neural networks work recognizing images,
    as each pixel is often modeled as three numeric RGB values. In this case, we are
    just focusing on one “pixel” as a background color.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start high level and put all the implementation details aside. We are
    going to approach this topic like an onion, starting with a higher understanding
    and peeling away slowly into the details. For now, this is why we simply label
    as “mystery math” a process that takes inputs and produces outputs. We have three
    numeric input variables R, G, and B, which are processed by this mystery math.
    Then it outputs a prediction between 0 and 1 as shown in [Figure 7-2](#VtUKGwbfou).
  prefs: []
  type: TYPE_NORMAL
- en: '![emds 0702](Images/emds_0702.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-2\. We have three numeric RGB values used to make a prediction for
    a light or dark font
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This prediction output expresses a probability. Outputting probabilities is
    the most common model for classification with neural networks. Once we replace
    RGB with their numerical values, we see that less than 0.5 will suggest a dark
    font whereas greater than 0.5 will suggest a light font as demonstrated in [Figure 7-3](#jnOWiEgusW).
  prefs: []
  type: TYPE_NORMAL
- en: '![emds 0703](Images/emds_0703.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-3\. If we input a background color of pink (255,192,203), then the
    mystery math recommends a light font because the output probability 0.89 is greater
    than 0.5
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: So what is going on inside that mystery math black box? Let’s take a look in
    [Figure 7-4](#sGQdjdjUMw).
  prefs: []
  type: TYPE_NORMAL
- en: We are missing another piece of this neural network, the activation functions,
    but we will get to that shortly. Let’s first understand what’s going on here.
    The first layer on the left is simply an input of the three variables, which in
    this case are the red, green, and blue values. In the hidden (middle) layer, notice
    that we produce three *nodes*, or functions of weights and biases, between the
    inputs and outputs. Each node essentially is a linear function with slopes <math
    alttext="upper W Subscript i"><msub><mi>W</mi> <mi>i</mi></msub></math> and intercepts
    <math alttext="upper B Subscript i"><msub><mi>B</mi> <mi>i</mi></msub></math>
    being multiplied and summed with input variables <math alttext="upper X Subscript
    i"><msub><mi>X</mi> <mi>i</mi></msub></math> . There is a weight <math alttext="upper
    W Subscript i"><msub><mi>W</mi> <mi>i</mi></msub></math> between each input node
    and hidden node, and another set of weights between each hidden node and output
    node. Each hidden and output node gets an additional bias <math alttext="upper
    B Subscript i"><msub><mi>B</mi> <mi>i</mi></msub></math> added.
  prefs: []
  type: TYPE_NORMAL
- en: '![emds 0704](Images/emds_0704.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-4\. The hidden layer of the neural network applies weight and bias
    values to each input variable, and the output layer applies another set of weights
    and biases to that output
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Notice the output node repeats the same operation, taking the resulting weighted
    and summed outputs from the hidden layer and making them inputs into the final
    layer, where another set of weights and biases will be applied.
  prefs: []
  type: TYPE_NORMAL
- en: In a nutshell, this is a regression just like linear or logistic regression,
    but with many more parameters to solve for. The weight and bias values are analogous
    to the *m* and *b*, or <math alttext="beta 1"><msub><mi>β</mi> <mn>1</mn></msub></math>
    and <math alttext="beta 0"><msub><mi>β</mi> <mn>0</mn></msub></math> , parameters
    in a linear regression. We do use stochastic gradient descent and minimize loss
    just like linear regression, but we need an additional tool called backpropagation
    to untangle the weight <math alttext="upper W Subscript i"><msub><mi>W</mi> <mi>i</mi></msub></math>
    and bias <math alttext="upper B Subscript i"><msub><mi>B</mi> <mi>i</mi></msub></math>
    values and calculate their partial derivatives using the chain rule. We will get
    to that later in this chapter, but for now let’s assume we have the weight and
    bias values optimized. We need to talk about activation functions first.
  prefs: []
  type: TYPE_NORMAL
- en: Activation Functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s bring in the activation functions next. An *activation function* is a
    nonlinear function that transforms or compresses the weighted and summed values
    in a node, helping the neural network separate the data effectively so it can
    be classified. Let’s take a look at [Figure 7-5](#PvLebFIsiT). If you do not have
    the activation functions, your hidden layers will not be productive and will perform
    no better than a linear regression.
  prefs: []
  type: TYPE_NORMAL
- en: '![emds 0705](Images/emds_0705.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-5\. Applying activation functions
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The *ReLU activation function* will zero out any negative outputs from the hidden
    nodes. If the weights, biases, and inputs multiply and sum to a negative number,
    it will be converted to 0\. Otherwise the output is left alone. Here is the graph
    for ReLU ([Figure 7-6](#tKkerIrVkt)) using SymPy ([Example 7-1](#WIqRPnWuNe)).
  prefs: []
  type: TYPE_NORMAL
- en: Example 7-1\. Plotting the ReLU function
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '![emds 0706](Images/emds_0706.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-6\. Graph for ReLU function
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: ReLU is short for “rectified linear unit,” but that is just a fancy way of saying
    “turn negative values into 0.” ReLU has gotten popular for hidden layers in neural
    networks and deep learning because of its speed and mitigation of the [vanishing
    gradient problem](https://oreil.ly/QGlM7). Vanishing gradients occur when the
    partial derivative slopes get so small they prematurely approach 0 and bring training
    to a screeching halt.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output layer has an important job: it takes the piles of math from the
    hidden layers of the neural network and turns them into an interpretable result,
    such as presenting classification predictions. The output layer for this particular
    neural network uses the *logistic activation function*, which is a simple sigmoid
    curve. If you read [Chapter 6](ch06.xhtml#ch06), the logistic (or sigmoid) function
    should be familiar, and it demonstrates that logistic regression is acting as
    a layer in our neural network. The output node weights, biases, and sums each
    of the incoming values from the hidden layer. After that, it passes the resulting
    value through the logistic function so it outputs a number between 0 and 1\. Much
    like logistic regression in [Chapter 6](ch06.xhtml#ch06), this represents a probability
    that the given color input into the neural network recommends a light font. If
    it is greater than or equal to 0.5, the neural network is suggesting a light font,
    but less than that will advise a dark font.'
  prefs: []
  type: TYPE_NORMAL
- en: Here is the graph for the logistic function ([Figure 7-7](#kIPouDqVgq)) using
    SymPy ([Example 7-2](#EFehQtrsWp)).
  prefs: []
  type: TYPE_NORMAL
- en: Example 7-2\. Logistic activation function in SymPy
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![emds 0707](Images/emds_0707.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-7\. Logistic activation function
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note that when we pass a node’s weighted, biased, and summed value through an
    activation function, we now call that an *activated output*, meaning it has been
    filtered through the activation function. When the activated output leaves the
    hidden layer, the signal is ready to be fed into the next layer. The activation
    function could have strengthened, weakened, or left the signal as is. This is
    where the brain and synapse metaphor for neural networks comes from.
  prefs: []
  type: TYPE_NORMAL
- en: Given the potential for complexity, you might be wondering if there are other
    activation functions. Some common ones are shown in [Table 7-1](#GhMbfHKelT).
  prefs: []
  type: TYPE_NORMAL
- en: Table 7-1\. Common activation functions
  prefs: []
  type: TYPE_NORMAL
- en: '| Name | Typical layer used | Description | Notes |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Linear | Output | Leaves values as is | Not commonly used |'
  prefs: []
  type: TYPE_TB
- en: '| Logistic | Output | S-shaped sigmoid curve | Compresses values between 0
    and 1, often assists binary classification |'
  prefs: []
  type: TYPE_TB
- en: '| Tangent Hyperbolic | Hidden | tanh, S-shaped sigmoid curve between -1 and
    1 | Assists in “centering” data by bringing mean close to 0 |'
  prefs: []
  type: TYPE_TB
- en: '| ReLU | Hidden | Turns negative values to 0 | Popular activation faster than
    sigmoid and tanh, mitigates vanishing gradient problems and computationally cheap
    |'
  prefs: []
  type: TYPE_TB
- en: '| Leaky ReLU | Hidden | Multiplies negative values by 0.01 | Controversial
    variant of ReLU that marginalizes rather than eliminates negative values |'
  prefs: []
  type: TYPE_TB
- en: '| Softmax | Output | Ensures all output nodes add up to 1.0 | Useful for multiple
    classifications and rescaling outputs so they add up to 1.0 |'
  prefs: []
  type: TYPE_TB
- en: This is not a comprehensive list of activation functions, and in theory any
    function could be an activation function in a neural network.
  prefs: []
  type: TYPE_NORMAL
- en: 'While this neural network seemingly supports two classes (light or dark font),
    it actually is modeled to one class: whether or not a font should be light (1)
    or not (0). If you wanted to support multiple classes, you could add more output
    nodes for each class. For instance, if you are trying to recognize handwritten
    digits 0–9, there would be 10 output nodes representing the probability a given
    image is each of those numbers. You might consider using softmax as the output
    activation when you have multiple classes as well. [Figure 7-8](#uPTTSNePsO) shows
    an example of taking a pixellated image of a digit, where the pixels are broken
    up as individual neural network inputs and then passed through two middle layers,
    and then an output layer with 10 nodes representing probabilities for 10 classes
    (for the digits 0–9).'
  prefs: []
  type: TYPE_NORMAL
- en: '![emds 0708](Images/emds_0708.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-8\. A neural network that takes each pixel as an input and predicts
    what digit the image contains
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: An example of using the MNIST dataset on a neural network can be found in [Appendix A](app01.xhtml#appendix).
  prefs: []
  type: TYPE_NORMAL
- en: I Don’t Know What Activation Function to Use!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you are unsure what activations to use, current best practices gravitate
    toward ReLU for middle layers and logistic (sigmoid) for output layer. If you
    have multiple classifications in the output, use softmax for the output layer.
  prefs: []
  type: TYPE_NORMAL
- en: Forward Propagation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s capture what we have learned so far using NumPy. Note I have not optimized
    the parameters (our weight and bias values) yet. We are going to initialize those
    with random values.
  prefs: []
  type: TYPE_NORMAL
- en: '[Example 7-3](#OMrNeTihfU) is the Python code to create a simple feed-forward
    neural network that is not optimized yet. *Feed forward* means we are simply inputting
    a color into the neural network and seeing what it outputs. The weights and biases
    are randomly initialized and will be optimized later in this chapter, so do not
    expect a useful output yet.'
  prefs: []
  type: TYPE_NORMAL
- en: Example 7-3\. A simple forward propagation network with random weight and bias
    values
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: A couple of things to note here. The dataset containing the RGB input values
    as well as output value (1 for light and 0 for dark) are contained in [this CSV
    file](https://oreil.ly/1TZIK). I am scaling down the input columns R, G, and B
    values by a factor of 1/255 so they are between 0 and 1\. This will help the training
    later so the number space is compressed.
  prefs: []
  type: TYPE_NORMAL
- en: Note I also separated 2/3 of the data for training and 1/3 for testing using
    scikit-learn, which we learned how to do in [Chapter 5](ch05.xhtml#ch05). `n`
    is simply the number of training data records.
  prefs: []
  type: TYPE_NORMAL
- en: Now bring your attention to the lines of code shown in [Example 7-4](#omJOMqoOMh).
  prefs: []
  type: TYPE_NORMAL
- en: Example 7-4\. The weight matrices and bias vectors in NumPy
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: These are declaring our weights and biases for both the hidden and output layers
    of our neural network. This may not be obvious yet but matrix multiplication is
    going to make our code powerfully simple using linear algebra and NumPy.
  prefs: []
  type: TYPE_NORMAL
- en: 'The weights and biases are going to be initialized as random values between
    0 and 1\. Let’s look at the weight matrices first. When I ran the code I got these
    matrices:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><msub><mi>W</mi> <mrow><mi>h</mi><mi>i</mi><mi>d</mi><mi>d</mi><mi>e</mi><mi>n</mi></mrow></msub>
    <mo>=</mo> <mfenced open="[" close="]"><mtable><mtr><mtd><mrow><mn>0.034535</mn></mrow></mtd>
    <mtd><mrow><mn>0.5185636</mn></mrow></mtd> <mtd><mrow><mn>0.81485028</mn></mrow></mtd></mtr>
    <mtr><mtd><mrow><mn>0.3329199</mn></mrow></mtd> <mtd><mrow><mn>0.53873853</mn></mrow></mtd>
    <mtd><mrow><mn>0.96359003</mn></mrow></mtd></mtr> <mtr><mtd><mrow><mn>0.19808306</mn></mrow></mtd>
    <mtd><mrow><mn>0.45422182</mn></mrow></mtd> <mtd><mrow><mn>0.36618893</mn></mrow></mtd></mtr></mtable></mfenced></mrow></math><math
    display="block"><mrow><msub><mi>W</mi> <mrow><mi>o</mi><mi>u</mi><mi>t</mi><mi>p</mi><mi>u</mi><mi>t</mi></mrow></msub>
    <mo>=</mo> <mfenced open="[" close="]"><mtable><mtr><mtd><mrow><mn>0.82652072</mn></mrow></mtd>
    <mtd><mrow><mn>0.30781539</mn></mrow></mtd> <mtd><mrow><mn>0.93095565</mn></mrow></mtd></mtr></mtable></mfenced></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: Note that <math alttext="upper W Subscript h i d d e n"><msub><mi>W</mi> <mrow><mi>h</mi><mi>i</mi><mi>d</mi><mi>d</mi><mi>e</mi><mi>n</mi></mrow></msub></math>
    are the weights in the hidden layer. The first row represents the first node weights
    <math alttext="upper W 1"><msub><mi>W</mi> <mn>1</mn></msub></math> , <math alttext="upper
    W 2"><msub><mi>W</mi> <mn>2</mn></msub></math> , and <math alttext="upper W 3"><msub><mi>W</mi>
    <mn>3</mn></msub></math> . The second row is the second node with weights <math
    alttext="upper W 4"><msub><mi>W</mi> <mn>4</mn></msub></math> , <math alttext="upper
    W 5"><msub><mi>W</mi> <mn>5</mn></msub></math> , and <math alttext="upper W 6"><msub><mi>W</mi>
    <mn>6</mn></msub></math> . The third row is the third node with weights <math
    alttext="upper W 7"><msub><mi>W</mi> <mn>7</mn></msub></math> , <math alttext="upper
    W 8"><msub><mi>W</mi> <mn>8</mn></msub></math> , and <math alttext="upper W 9"><msub><mi>W</mi>
    <mn>9</mn></msub></math> .
  prefs: []
  type: TYPE_NORMAL
- en: The output layer has only one node, meaning its matrix has only one row with
    weights <math alttext="upper W 10"><msub><mi>W</mi> <mn>10</mn></msub></math>
    , <math alttext="upper W 11"><msub><mi>W</mi> <mn>11</mn></msub></math> , and
    <math alttext="upper W 12"><msub><mi>W</mi> <mn>12</mn></msub></math> .
  prefs: []
  type: TYPE_NORMAL
- en: See a pattern here? Each node is represented as a row in a matrix. If there
    are three nodes, there are three rows. If there is one node, there is one row.
    Each column holds a weight value for that node.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at the biases, too. Since there is one bias per node, there are
    going to be three rows of biases for the hidden layer and one row of biases for
    the output layer. There’s only one bias per node so there will be only one column:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><msub><mi>B</mi> <mrow><mi>h</mi><mi>i</mi><mi>d</mi><mi>d</mi><mi>e</mi><mi>n</mi></mrow></msub>
    <mo>=</mo> <mfenced open="[" close="]"><mtable><mtr><mtd><mrow><mn>0.41379442</mn></mrow></mtd></mtr>
    <mtr><mtd><mrow><mn>0.81666079</mn></mrow></mtd></mtr> <mtr><mtd><mrow><mn>0.07511252</mn></mrow></mtd></mtr></mtable></mfenced></mrow></math><math
    display="block"><mrow><msub><mi>B</mi> <mrow><mi>o</mi><mi>u</mi><mi>t</mi><mi>p</mi><mi>u</mi><mi>t</mi></mrow></msub>
    <mo>=</mo> <mfenced open="[" close="]"><mtable><mtr><mtd><mrow><mn>0.58018555</mn></mrow></mtd></mtr></mtable></mfenced></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s compare these matrix values to our visualized neural network as shown
    in [Figure 7-9](#vSdgvskUoQ).
  prefs: []
  type: TYPE_NORMAL
- en: '![emds 0709](Images/emds_0709.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-9\. Visualizing our neural network against the weight and bias matrix
    values
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: So besides being esoterically compact, what is the benefit of these weights
    and biases in this matrix form? Let’s bring our attention to these lines of code
    in [Example 7-5](#cKHHrQlImr).
  prefs: []
  type: TYPE_NORMAL
- en: Example 7-5\. The activation functions and forward propagation function for
    our neural network
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: This code is important because it concisely executes our entire neural network
    using matrix multiplication and matrix-vector multiplication. We learned about
    these operations in [Chapter 4](ch04.xhtml#ch04). It runs a color of three RGB
    inputs through the weights, biases, and activation functions in just a few lines
    of code.
  prefs: []
  type: TYPE_NORMAL
- en: 'I first declare the `relu()` and `logistic()` activation functions, which literally
    take a given input value and return the output value from the curve. The `forward_prop()`
    function executes our entire neural network for a given color input `X` containing
    the R, G, and B values. It returns the matrix outputs from four stages: `Z1`,
    `A1`, `Z2`, and `A2`. The “1” and “2” indicate the operations belong to layers
    1 and 2 respectively. The “Z” indicates an unactivated output from the layer,
    and “A” is activated output from the layer.'
  prefs: []
  type: TYPE_NORMAL
- en: The hidden layer is represented by `Z1` and `A1`. `Z1` is the weights and biases
    applied to `X`. Then `A1` takes that output from `Z1` and pushes it through the
    activation ReLU function. `Z2` takes the output from `A1` and applies the output
    layer weights and biases. That output is in turn pushed through the activation
    function, the logistic curve, and becomes `A2`. The final stage, `A2`, is the
    prediction probability from the output layer, returning a value between 0 and
    1\. We call it `A2` because it is the “activated” output from layer 2.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s break this down in more detail starting with `Z1`:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="upper Z 1 equals upper W Subscript h i d d e n Baseline upper
    X plus upper B Subscript h i d d e n" display="block"><mrow><msub><mi>Z</mi> <mn>1</mn></msub>
    <mo>=</mo> <msub><mi>W</mi> <mrow><mi>h</mi><mi>i</mi><mi>d</mi><mi>d</mi><mi>e</mi><mi>n</mi></mrow></msub>
    <mi>X</mi> <mo>+</mo> <msub><mi>B</mi> <mrow><mi>h</mi><mi>i</mi><mi>d</mi><mi>d</mi><mi>e</mi><mi>n</mi></mrow></msub></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: First we perform matrix-vector multiplication between <math alttext="upper W
    Subscript h i d d e n"><msub><mi>W</mi> <mrow><mi>h</mi><mi>i</mi><mi>d</mi><mi>d</mi><mi>e</mi><mi>n</mi></mrow></msub></math>
    and the input color `X`. We multiply each row of <math alttext="upper W Subscript
    h i d d e n"><msub><mi>W</mi> <mrow><mi>h</mi><mi>i</mi><mi>d</mi><mi>d</mi><mi>e</mi><mi>n</mi></mrow></msub></math>
    (each row being a set of weights for a node) with the vector `X` (the RGB color
    input values). We then add the biases to that result, as shown in [Figure 7-10](#jPQPcShSLf).
  prefs: []
  type: TYPE_NORMAL
- en: '![emds 0710](Images/emds_0710.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-10\. Applying the hidden layer weights and biases to an input `X` using
    matrix-vector multiplication as well as vector addition
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: That <math alttext="upper Z 1"><msub><mi>Z</mi> <mn>1</mn></msub></math> vector
    is the raw output from the hidden layer, but we still need to pass it through
    the activation function to turn <math alttext="upper Z 1"><msub><mi>Z</mi> <mn>1</mn></msub></math>
    into <math alttext="upper A 1"><msub><mi>A</mi> <mn>1</mn></msub></math> . Easy
    enough. Just pass each value in that vector through the ReLU function and it will
    give us <math alttext="upper A 1"><msub><mi>A</mi> <mn>1</mn></msub></math> .
    Because all the values are positive, it should not have an impact.
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="upper A 1 equals upper R e upper L upper U left-parenthesis upper
    Z 1 right-parenthesis" display="block"><mrow><msub><mi>A</mi> <mn>1</mn></msub>
    <mo>=</mo> <mi>R</mi> <mi>e</mi> <mi>L</mi> <mi>U</mi> <mrow><mo>(</mo> <msub><mi>Z</mi>
    <mn>1</mn></msub> <mo>)</mo></mrow></mrow></math><math display="block"><mrow><msub><mi>A</mi>
    <mn>1</mn></msub> <mo>=</mo> <mfenced separators="" open="[" close="]"><mtable><mtr><mtd><mrow><mi>R</mi>
    <mi>e</mi> <mi>L</mi> <mi>U</mi> <mo>(</mo> <mn>1.36054964190909</mn> <mo>)</mo></mrow></mtd></mtr>
    <mtr><mtd><mrow><mi>R</mi> <mi>e</mi> <mi>L</mi> <mi>U</mi> <mo>(</mo> <mn>2.15471757888247</mn>
    <mo>)</mo></mrow></mtd></mtr> <mtr><mtd><mrow><mi>R</mi> <mi>e</mi> <mi>L</mi>
    <mi>U</mi> <mo>(</mo> <mn>0.719554393391768</mn> <mo>)</mo></mrow></mtd></mtr></mtable></mfenced>
    <mo>=</mo> <mfenced separators="" open="[" close="]"><mtable><mtr><mtd><mrow><mn>1.36054964190909</mn></mrow></mtd></mtr>
    <mtr><mtd><mrow><mn>2.15471757888247</mn></mrow></mtd></mtr> <mtr><mtd><mrow><mn>0.719554393391768</mn></mrow></mtd></mtr></mtable></mfenced></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s take that hidden layer output <math alttext="upper A 1"><msub><mi>A</mi>
    <mn>1</mn></msub></math> and pass it through the final layer to get <math alttext="upper
    Z 2"><msub><mi>Z</mi> <mn>2</mn></msub></math> and then <math alttext="upper A
    2"><msub><mi>A</mi> <mn>2</mn></msub></math> . <math alttext="upper A 1"><msub><mi>A</mi>
    <mn>1</mn></msub></math> becomes the input into the output layer.
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="upper Z 2 equals upper W Subscript o u t p u t Baseline upper
    A 1 plus upper B Subscript o u t p u t" display="block"><mrow><msub><mi>Z</mi>
    <mn>2</mn></msub> <mo>=</mo> <msub><mi>W</mi> <mrow><mi>o</mi><mi>u</mi><mi>t</mi><mi>p</mi><mi>u</mi><mi>t</mi></mrow></msub>
    <msub><mi>A</mi> <mn>1</mn></msub> <mo>+</mo> <msub><mi>B</mi> <mrow><mi>o</mi><mi>u</mi><mi>t</mi><mi>p</mi><mi>u</mi><mi>t</mi></mrow></msub></mrow></math><math
    display="block"><mrow><msub><mi>Z</mi> <mn>2</mn></msub> <mo>=</mo> <mfenced open="["
    close="]"><mtable><mtr><mtd><mrow><mn>0.82652072</mn></mrow></mtd> <mtd><mrow><mn>0.3078159</mn></mrow></mtd>
    <mtd><mrow><mn>0.93095565</mn></mrow></mtd></mtr></mtable></mfenced> <mfenced
    separators="" open="[" close="]"><mtable><mtr><mtd><mrow><mn>1.36054964190909</mn></mrow></mtd></mtr>
    <mtr><mtd><mrow><mn>2.15471757888247</mn></mrow></mtd></mtr> <mtr><mtd><mrow><mn>0.719554393391768</mn></mrow></mtd></mtr></mtable></mfenced>
    <mo>+</mo> <mfenced open="[" close="]"><mtable><mtr><mtd><mrow><mn>0.58018555</mn></mrow></mtd></mtr></mtable></mfenced></mrow></math><math
    display="block"><mrow><msub><mi>Z</mi> <mn>2</mn></msub> <mo>=</mo> <mfenced open="["
    close="]"><mtable><mtr><mtd><mrow><mn>2.45765202842636</mn></mrow></mtd></mtr></mtable></mfenced>
    <mo>+</mo> <mfenced open="[" close="]"><mtable><mtr><mtd><mrow><mn>0.58018555</mn></mrow></mtd></mtr></mtable></mfenced></mrow></math><math
    display="block"><mrow><msub><mi>Z</mi> <mn>2</mn></msub> <mo>=</mo> <mfenced open="["
    close="]"><mtable><mtr><mtd><mrow><mn>3.03783757842636</mn></mrow></mtd></mtr></mtable></mfenced></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, pass this single value in <math alttext="upper Z 2"><msub><mi>Z</mi>
    <mn>2</mn></msub></math> through the activation function to get <math alttext="upper
    A 2"><msub><mi>A</mi> <mn>2</mn></msub></math> . This will produce a prediction
    of approximately 0.95425:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block" class="mathml_bottom_space"><mrow><msub><mi>A</mi> <mn>2</mn></msub>
    <mo>=</mo> <mi>l</mi> <mi>o</mi> <mi>g</mi> <mi>i</mi> <mi>s</mi> <mi>t</mi> <mi>i</mi>
    <mi>c</mi> <mrow><mo>(</mo> <msub><mi>Z</mi> <mn>2</mn></msub> <mo>)</mo></mrow></mrow></math>
    <math display="block" class="mathml_bottom_space"><mrow><msub><mi>A</mi> <mn>2</mn></msub>
    <mo>=</mo> <mi>l</mi> <mi>o</mi> <mi>g</mi> <mi>i</mi> <mi>s</mi> <mi>t</mi> <mi>i</mi>
    <mi>c</mi> <mrow><mo>(</mo> <mfenced open="[" close="]"><mtable><mtr><mtd><mrow><mn>3.0378364795204</mn></mrow></mtd></mtr></mtable></mfenced>
    <mo>)</mo></mrow></mrow></math> <math display="block"><mrow><msub><mi>A</mi> <mn>2</mn></msub>
    <mo>=</mo> <mn>0.954254478103241</mn></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: That executes our entire neural network, although we have not trained it yet.
    But take a moment to appreciate that we have taken all these input values, weights,
    biases, and nonlinear functions and turned them all into a single value that will
    provide a prediction.
  prefs: []
  type: TYPE_NORMAL
- en: Again, `A2` is the final output that makes a prediction whether that background
    color need a light (1) or dark (1) font. Even though our weights and biases have
    not been optimized yet, let’s calculate our accuracy as shown in [Example 7-6](#mLlirJipiN).
    Take the testing dataset `X_test`, transpose it, and pass it through the `forward_prop()`
    function but only grab the `A2` vector with the predictions for each test color.
    Then compare the predictions to the actuals and calculate the percentage of correct
    predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Example 7-6\. Calculating accuracy
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'When I run the whole code in [Example 7-3](#OMrNeTihfU), I roughly get anywhere
    from 55% to 67% accuracy. Remember, the weights and biases are randomly generated
    so answers will vary. While this may seem high given the parameters were randomly
    generated, remember that the output predictions are binary: light or dark. Therefore,
    a random coin flip might as well produce this outcome for each prediction, so
    this number should not be surprising.'
  prefs: []
  type: TYPE_NORMAL
- en: Do Not Forget to Check for Imbalanced Data!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As discussed in [Chapter 6](ch06.xhtml#ch06), do not forget to analyze your
    data to check for imbalanced classes. This whole background color dataset is a
    little imbalanced: 512 colors have output of 0 and 833 have an output of 1\. This
    can skew accuracy and might be why our random weights and biases gravitate higher
    than 50% accuracy. If the data is extremely imbalanced (as in 99% of the data
    is one class), then remember to use confusion matrices to track the false positives
    and false negatives.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Does everything structurally make sense so far? Feel free to review everything
    up to this point before moving on. We just have one final piece to cover: optimizing
    the weights and biases. Hit the espresso machine or nitro coffee bar, because
    this is the most involved math we will be doing in this book!'
  prefs: []
  type: TYPE_NORMAL
- en: Backpropagation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we start using stochastic gradient descent to optimize our neural network,
    a challenge we have is figuring out how to change each of the weight and bias
    values accordingly, even though they all are tangled together to create the output
    variable, which then is used to calculate the residuals. How do we find the derivative
    of each weight <math alttext="upper W Subscript i"><msub><mi>W</mi> <mi>i</mi></msub></math>
    and bias <math alttext="upper B Subscript i"><msub><mi>B</mi> <mi>i</mi></msub></math>
    variable? We need to use the chain rule, which we covered in [Chapter 1](ch01.xhtml#ch01).
  prefs: []
  type: TYPE_NORMAL
- en: Calculating the Weight and Bias Derivatives
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We are not quite ready to apply stochastic gradient descent to train our neural
    network. We have to get the partial derivatives with respect to the weights <math
    alttext="upper W Subscript i"><msub><mi>W</mi> <mi>i</mi></msub></math> and biases
    <math alttext="upper B Subscript i"><msub><mi>B</mi> <mi>i</mi></msub></math>
    , and we have the chain rule to help us.
  prefs: []
  type: TYPE_NORMAL
- en: While the process is largely the same, there is a complication using stochastic
    gradient descent on neural networks. The nodes in one layer feed their weights
    and biases into the next layer, which then applies another set of weights and
    biases. This creates an onion-like nesting we need to untangle, starting with
    the output layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'During gradient descent, we need to figure out which weights and biases should
    be adjusted, and by how much, to reduce the overall cost function. The cost for
    a single prediction is going to be the squared output of the neural network <math
    alttext="upper A 2"><msub><mi>A</mi> <mn>2</mn></msub></math> minus the actual
    value <math alttext="upper Y"><mi>Y</mi></math> :'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="upper C equals left-parenthesis upper A 2 minus upper Y right-parenthesis
    squared" display="block"><mrow><mi>C</mi> <mo>=</mo> <msup><mrow><mo>(</mo><msub><mi>A</mi>
    <mn>2</mn></msub> <mo>-</mo><mi>Y</mi><mo>)</mo></mrow> <mn>2</mn></msup></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'But let’s peel back a layer. That activated output <math alttext="upper A 2"><msub><mi>A</mi>
    <mn>2</mn></msub></math> is just <math alttext="upper Z 2"><msub><mi>Z</mi> <mn>2</mn></msub></math>
    with the activation function:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="upper A 2 equals s i g m o i d left-parenthesis upper Z 2 right-parenthesis"
    display="block"><mrow><msub><mi>A</mi> <mn>2</mn></msub> <mo>=</mo> <mi>s</mi>
    <mi>i</mi> <mi>g</mi> <mi>m</mi> <mi>o</mi> <mi>i</mi> <mi>d</mi> <mrow><mo>(</mo>
    <msub><mi>Z</mi> <mn>2</mn></msub> <mo>)</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: '<math alttext="upper Z 2"><msub><mi>Z</mi> <mn>2</mn></msub></math> in turn
    is the output weights and biases applied to activation output <math alttext="upper
    A 1"><msub><mi>A</mi> <mn>1</mn></msub></math> , which comes from the hidden layer:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="upper Z 2 equals upper W 2 upper A 1 plus upper B 2" display="block"><mrow><msub><mi>Z</mi>
    <mn>2</mn></msub> <mo>=</mo> <msub><mi>W</mi> <mn>2</mn></msub> <msub><mi>A</mi>
    <mn>1</mn></msub> <mo>+</mo> <msub><mi>B</mi> <mn>2</mn></msub></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: '<math alttext="upper A 1"><msub><mi>A</mi> <mn>1</mn></msub></math> is built
    off <math alttext="upper Z 1"><msub><mi>Z</mi> <mn>1</mn></msub></math> which
    is passed through the ReLU activation function:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="upper A 1 equals upper R e upper L upper U left-parenthesis upper
    Z 1 right-parenthesis" display="block"><mrow><msub><mi>A</mi> <mn>1</mn></msub>
    <mo>=</mo> <mi>R</mi> <mi>e</mi> <mi>L</mi> <mi>U</mi> <mrow><mo>(</mo> <msub><mi>Z</mi>
    <mn>1</mn></msub> <mo>)</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, <math alttext="upper Z 1"><msub><mi>Z</mi> <mn>1</mn></msub></math>
    is the input x-values weighted and biased by the hidden layer:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="upper Z 1 equals upper W 1 upper X plus upper B 1" display="block"><mrow><msub><mi>Z</mi>
    <mn>1</mn></msub> <mo>=</mo> <msub><mi>W</mi> <mn>1</mn></msub> <mi>X</mi> <mo>+</mo>
    <msub><mi>B</mi> <mn>1</mn></msub></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: We need to find the weights and biases contained in the <math alttext="upper
    W 1"><msub><mi>W</mi> <mn>1</mn></msub></math> , <math alttext="upper B 1"><msub><mi>B</mi>
    <mn>1</mn></msub></math> , <math alttext="upper W 2"><msub><mi>W</mi> <mn>2</mn></msub></math>
    , and <math alttext="upper B 2"><msub><mi>B</mi> <mn>2</mn></msub></math> matrices
    and vectors that will minimize our loss. By nudging their slopes, we can change
    the weights and biases that have the most impact in minimizing loss. However,
    each little nudge on a weight or bias is going to propagate all the way to the
    loss function on the outer layer. This is where the chain rule can help us figure
    out this impact.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s focus on finding the relationship on a weight from the output layer <math
    alttext="upper W 2"><msub><mi>W</mi> <mn>2</mn></msub></math> and the cost function
    <math alttext="upper C"><mi>C</mi></math> . A change in the weight <math alttext="upper
    W 2"><msub><mi>W</mi> <mn>2</mn></msub></math> results in a change to the unactivated
    output <math alttext="upper Z 2"><msub><mi>Z</mi> <mn>2</mn></msub></math> . That
    then changes the activated output <math alttext="upper A 2"><msub><mi>A</mi> <mn>2</mn></msub></math>
    , which changes the cost function <math alttext="upper C"><mi>C</mi></math> .
    Using the chain rule, we can define the derivative of <math alttext="upper C"><mi>C</mi></math>
    with respect to <math alttext="upper W 2"><msub><mi>W</mi> <mn>2</mn></msub></math>
    as this:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="StartFraction d upper C Over d upper W 2 EndFraction equals StartFraction
    d upper Z 2 Over d upper W 2 EndFraction StartFraction d upper A 2 Over d upper
    Z 2 EndFraction StartFraction d upper C Over d upper A 2 EndFraction" display="block"><mrow><mfrac><mrow><mi>d</mi><mi>C</mi></mrow>
    <mrow><mi>d</mi><msub><mi>W</mi> <mn>2</mn></msub></mrow></mfrac> <mo>=</mo> <mfrac><mrow><mi>d</mi><msub><mi>Z</mi>
    <mn>2</mn></msub></mrow> <mrow><mi>d</mi><msub><mi>W</mi> <mn>2</mn></msub></mrow></mfrac>
    <mfrac><mrow><mi>d</mi><msub><mi>A</mi> <mn>2</mn></msub></mrow> <mrow><mi>d</mi><msub><mi>Z</mi>
    <mn>2</mn></msub></mrow></mfrac> <mfrac><mrow><mi>d</mi><mi>C</mi></mrow> <mrow><mi>d</mi><msub><mi>A</mi>
    <mn>2</mn></msub></mrow></mfrac></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: When we multiply these three gradients together, we get a measure of how much
    a change to <math alttext="upper W 2"><msub><mi>W</mi> <mn>2</mn></msub></math>
    will change the cost function <math alttext="upper C"><mi>C</mi></math> .
  prefs: []
  type: TYPE_NORMAL
- en: Now we will calculate these three derivatives. Let’s use SymPy to calculate
    the derivative of the cost function with respect to <math alttext="upper A 2"><msub><mi>A</mi>
    <mn>2</mn></msub></math> in [Example 7-7](#bJeDOpjeQi).
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="StartFraction d upper C Over d upper A 2 EndFraction equals 2
    upper A 2 minus 2 y" display="block"><mrow><mfrac><mrow><mi>d</mi><mi>C</mi></mrow>
    <mrow><mi>d</mi><msub><mi>A</mi> <mn>2</mn></msub></mrow></mfrac> <mo>=</mo> <mn>2</mn>
    <msub><mi>A</mi> <mn>2</mn></msub> <mo>-</mo> <mn>2</mn> <mi>y</mi></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: Example 7-7\. Calculating the derivative of the cost function with respect to
    <math alttext="upper A 2"><msub><mi>A</mi> <mn>2</mn></msub></math>
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Next, let’s get the derivative of <math alttext="upper A 2"><msub><mi>A</mi>
    <mn>2</mn></msub></math> with respect to <math alttext="upper Z 2"><msub><mi>Z</mi>
    <mn>2</mn></msub></math> ([Example 7-8](#FiirvKJUSW)). Remember that <math alttext="upper
    A 2"><msub><mi>A</mi> <mn>2</mn></msub></math> is the output of an activation
    function, in this case the logistic function. So we really are just taking the
    derivative of a sigmoid curve.
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="StartFraction d upper A 2 Over d upper Z 2 EndFraction equals
    StartFraction e Superscript minus upper Z 2 Baseline Over left-parenthesis 1 plus
    e Superscript minus upper Z 2 Baseline right-parenthesis squared EndFraction"
    display="block"><mrow><mfrac><mrow><mi>d</mi><msub><mi>A</mi> <mn>2</mn></msub></mrow>
    <mrow><mi>d</mi><msub><mi>Z</mi> <mn>2</mn></msub></mrow></mfrac> <mo>=</mo> <mfrac><msup><mi>e</mi>
    <mrow><mo>-</mo><msub><mi>Z</mi> <mn>2</mn></msub></mrow></msup> <msup><mfenced
    separators="" open="(" close=")"><mn>1</mn><mo>+</mo><msup><mi>e</mi> <mrow><mo>-</mo><msub><mi>Z</mi>
    <mn>2</mn></msub></mrow></msup></mfenced> <mn>2</mn></msup></mfrac></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: Example 7-8\. Finding the derivative of <math alttext="upper A 2"><msub><mi>A</mi>
    <mn>2</mn></msub></math> with respect to <math alttext="upper Z 2"><msub><mi>Z</mi>
    <mn>2</mn></msub></math>
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The derivative of <math alttext="upper Z 2"><msub><mi>Z</mi> <mn>2</mn></msub></math>
    with respect to <math alttext="upper W 2"><msub><mi>W</mi> <mn>2</mn></msub></math>
    is going to work out to be <math alttext="upper A 1"><msub><mi>A</mi> <mn>1</mn></msub></math>
    , as it is just a linear function and going to return the slope ([Example 7-9](#kPubQpquMU)).
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="StartFraction d upper Z 2 Over d upper W 1 EndFraction equals
    upper A 1" display="block"><mrow><mfrac><mrow><mi>d</mi><msub><mi>Z</mi> <mn>2</mn></msub></mrow>
    <mrow><mi>d</mi><msub><mi>W</mi> <mn>1</mn></msub></mrow></mfrac> <mo>=</mo> <msub><mi>A</mi>
    <mn>1</mn></msub></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: Example 7-9\. Derivative of <math alttext="upper Z 2"><msub><mi>Z</mi> <mn>2</mn></msub></math>
    with respect to <math alttext="upper W 2"><msub><mi>W</mi> <mn>2</mn></msub></math>
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Putting it all together, here is the derivative to find how much a change in
    a weight in <math alttext="upper W 2"><msub><mi>W</mi> <mn>2</mn></msub></math>
    affects the cost function <math alttext="upper C"><mi>C</mi></math> :'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="StartFraction d upper C Over d w 2 EndFraction equals StartFraction
    d upper Z 2 Over d w 2 EndFraction StartFraction d upper A 2 Over d upper Z 2
    EndFraction StartFraction d upper C Over d upper A 2 EndFraction equals left-parenthesis
    upper A 1 right-parenthesis left-parenthesis StartFraction e Superscript minus
    upper Z 2 Baseline Over left-parenthesis 1 plus e Superscript minus upper Z 2
    Baseline right-parenthesis squared EndFraction right-parenthesis left-parenthesis
    2 upper A 2 minus 2 y right-parenthesis" display="block"><mrow><mfrac><mrow><mi>d</mi><mi>C</mi></mrow>
    <mrow><mi>d</mi><msub><mi>w</mi> <mn>2</mn></msub></mrow></mfrac> <mo>=</mo> <mfrac><mrow><mi>d</mi><msub><mi>Z</mi>
    <mn>2</mn></msub></mrow> <mrow><mi>d</mi><msub><mi>w</mi> <mn>2</mn></msub></mrow></mfrac>
    <mfrac><mrow><mi>d</mi><msub><mi>A</mi> <mn>2</mn></msub></mrow> <mrow><mi>d</mi><msub><mi>Z</mi>
    <mn>2</mn></msub></mrow></mfrac> <mfrac><mrow><mi>d</mi><mi>C</mi></mrow> <mrow><mi>d</mi><msub><mi>A</mi>
    <mn>2</mn></msub></mrow></mfrac> <mo>=</mo> <mrow><mo>(</mo> <msub><mi>A</mi>
    <mn>1</mn></msub> <mo>)</mo></mrow> <mrow><mo>(</mo> <mfrac><msup><mi>e</mi> <mrow><mo>-</mo><msub><mi>Z</mi>
    <mn>2</mn></msub></mrow></msup> <msup><mfenced separators="" open="(" close=")"><mn>1</mn><mo>+</mo><msup><mi>e</mi>
    <mrow><mo>-</mo><msub><mi>Z</mi> <mn>2</mn></msub></mrow></msup></mfenced> <mn>2</mn></msup></mfrac>
    <mo>)</mo></mrow> <mrow><mo>(</mo> <mn>2</mn> <msub><mi>A</mi> <mn>2</mn></msub>
    <mo>-</mo> <mn>2</mn> <mi>y</mi> <mo>)</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: When we run an input `X` with the three input R, G, and B values, we will have
    values for <math alttext="upper A 1"><msub><mi>A</mi> <mn>1</mn></msub></math>
    , <math alttext="upper A 2"><msub><mi>A</mi> <mn>2</mn></msub></math> , <math
    alttext="upper Z 2"><msub><mi>Z</mi> <mn>2</mn></msub></math> , and <math alttext="y"><mi>y</mi></math>
    .
  prefs: []
  type: TYPE_NORMAL
- en: Don’t Get Lost in the Math!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is easy to get lost in the math at this point and forget what you were trying
    to achieve in the first place, which is finding the derivative of the cost function
    with respect to a weight ( <math alttext="upper W 2"><msub><mi>W</mi> <mn>2</mn></msub></math>
    ) in the output layer. When you find yourself in the weeds and forgetting what
    you were trying to do, then step back, go for a walk, get a coffee, and remind
    yourself what you were trying to accomplish. If you cannot, you should start over
    from the beginning and work your way to the point you got lost.
  prefs: []
  type: TYPE_NORMAL
- en: However, this is just one component of the neural network, the derivative for
    <math alttext="upper W 2"><msub><mi>W</mi> <mn>2</mn></msub></math> . Here are
    the SymPy calculations in [Example 7-10](#hqtBpgBaGT) for the rest of the partial
    derivatives we will need for chaining.
  prefs: []
  type: TYPE_NORMAL
- en: Example 7-10\. Calculating all the partial derivatives we will need for our
    neural network
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Notice that ReLU was calculated manually rather than using SymPy’s `diff()`
    function. This is because derivatives work with smooth curves, not jagged corners
    that exist on ReLU. But it’s easy to hack around that simply by declaring the
    slope to be 1 for positive numbers and 0 for negative numbers. This makes sense
    because negative numbers have a flat line with slope 0\. But positive numbers
    are left as is with a 1-to-1 slope.
  prefs: []
  type: TYPE_NORMAL
- en: 'These partial derivatives can be chained together to create new partial derivatives
    with respect to the weights and biases. Let’s get all four partial derivatives
    for the weights in <math alttext="upper W 1"><msub><mi>W</mi> <mn>1</mn></msub></math>
    , <math alttext="upper W 2"><msub><mi>W</mi> <mn>2</mn></msub></math> , <math
    alttext="upper B 1"><msub><mi>B</mi> <mn>1</mn></msub></math> , and <math alttext="upper
    B 2"><msub><mi>B</mi> <mn>2</mn></msub></math> with respect to the cost function.
    We already walked through <math alttext="StartFraction d upper C Over d w 2 EndFraction"><mfrac><mrow><mi>d</mi><mi>C</mi></mrow>
    <mrow><mi>d</mi><msub><mi>w</mi> <mn>2</mn></msub></mrow></mfrac></math> . Let’s
    show it alongside the other three chained derivatives we need:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="StartFraction d upper C Over d upper W 2 EndFraction equals StartFraction
    d upper Z 2 Over d upper W 2 EndFraction StartFraction d upper A 2 Over d upper
    Z 2 EndFraction StartFraction d upper C Over d upper A 2 EndFraction equals left-parenthesis
    upper A 1 right-parenthesis left-parenthesis StartFraction e Superscript minus
    upper Z 2 Baseline Over left-parenthesis 1 plus e Superscript minus upper Z 2
    Baseline right-parenthesis squared EndFraction right-parenthesis left-parenthesis
    2 upper A 2 minus 2 y right-parenthesis" display="block"><mrow><mfrac><mrow><mi>d</mi><mi>C</mi></mrow>
    <mrow><mi>d</mi><msub><mi>W</mi> <mn>2</mn></msub></mrow></mfrac> <mo>=</mo> <mfrac><mrow><mi>d</mi><msub><mi>Z</mi>
    <mn>2</mn></msub></mrow> <mrow><mi>d</mi><msub><mi>W</mi> <mn>2</mn></msub></mrow></mfrac>
    <mfrac><mrow><mi>d</mi><msub><mi>A</mi> <mn>2</mn></msub></mrow> <mrow><mi>d</mi><msub><mi>Z</mi>
    <mn>2</mn></msub></mrow></mfrac> <mfrac><mrow><mi>d</mi><mi>C</mi></mrow> <mrow><mi>d</mi><msub><mi>A</mi>
    <mn>2</mn></msub></mrow></mfrac> <mo>=</mo> <mrow><mo>(</mo> <msub><mi>A</mi>
    <mn>1</mn></msub> <mo>)</mo></mrow> <mrow><mo>(</mo> <mfrac><msup><mi>e</mi> <mrow><mo>-</mo><msub><mi>Z</mi>
    <mn>2</mn></msub></mrow></msup> <msup><mfenced separators="" open="(" close=")"><mn>1</mn><mo>+</mo><msup><mi>e</mi>
    <mrow><mo>-</mo><msub><mi>Z</mi> <mn>2</mn></msub></mrow></msup></mfenced> <mn>2</mn></msup></mfrac>
    <mo>)</mo></mrow> <mrow><mo>(</mo> <mn>2</mn> <msub><mi>A</mi> <mn>2</mn></msub>
    <mo>-</mo> <mn>2</mn> <mi>y</mi> <mo>)</mo></mrow></mrow></math><math alttext="StartFraction
    d upper C Over d upper B 2 EndFraction equals StartFraction d upper Z 2 Over d
    upper B 2 EndFraction StartFraction d upper A 2 Over d upper Z 2 EndFraction StartFraction
    d upper C Over d upper A 2 EndFraction equals left-parenthesis 1 right-parenthesis
    left-parenthesis StartFraction e Superscript minus upper Z 2 Baseline Over left-parenthesis
    1 plus e Superscript minus upper Z 2 Baseline right-parenthesis squared EndFraction
    right-parenthesis left-parenthesis 2 upper A 2 minus 2 y right-parenthesis" display="block"><mrow><mfrac><mrow><mi>d</mi><mi>C</mi></mrow>
    <mrow><mi>d</mi><msub><mi>B</mi> <mn>2</mn></msub></mrow></mfrac> <mo>=</mo> <mfrac><mrow><mi>d</mi><msub><mi>Z</mi>
    <mn>2</mn></msub></mrow> <mrow><mi>d</mi><msub><mi>B</mi> <mn>2</mn></msub></mrow></mfrac>
    <mfrac><mrow><mi>d</mi><msub><mi>A</mi> <mn>2</mn></msub></mrow> <mrow><mi>d</mi><msub><mi>Z</mi>
    <mn>2</mn></msub></mrow></mfrac> <mfrac><mrow><mi>d</mi><mi>C</mi></mrow> <mrow><mi>d</mi><msub><mi>A</mi>
    <mn>2</mn></msub></mrow></mfrac> <mo>=</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>)</mo></mrow>
    <mrow><mo>(</mo> <mfrac><msup><mi>e</mi> <mrow><mo>-</mo><msub><mi>Z</mi> <mn>2</mn></msub></mrow></msup>
    <msup><mfenced separators="" open="(" close=")"><mn>1</mn><mo>+</mo><msup><mi>e</mi>
    <mrow><mo>-</mo><msub><mi>Z</mi> <mn>2</mn></msub></mrow></msup></mfenced> <mn>2</mn></msup></mfrac>
    <mo>)</mo></mrow> <mrow><mo>(</mo> <mn>2</mn> <msub><mi>A</mi> <mn>2</mn></msub>
    <mo>-</mo> <mn>2</mn> <mi>y</mi> <mo>)</mo></mrow></mrow></math><math alttext="StartFraction
    d upper C Over d upper W 1 EndFraction equals StartFraction d upper C Over upper
    D upper A 2 EndFraction StartFraction upper D upper A 2 Over d upper Z 2 EndFraction
    StartFraction d upper Z 2 Over d upper A 1 EndFraction StartFraction d upper A
    1 Over d upper Z 1 EndFraction StartFraction d upper Z 1 Over d upper W 1 EndFraction
    equals left-parenthesis 2 upper A 2 minus 2 y right-parenthesis left-parenthesis
    StartFraction e Superscript minus upper Z 2 Baseline Over left-parenthesis 1 plus
    e Superscript minus upper Z 2 Baseline right-parenthesis squared EndFraction right-parenthesis
    left-parenthesis upper W 2 right-parenthesis left-parenthesis upper Z 1 greater-than
    0 right-parenthesis left-parenthesis upper X right-parenthesis" display="block"><mrow><mfrac><mrow><mi>d</mi><mi>C</mi></mrow>
    <mrow><mi>d</mi><msub><mi>W</mi> <mn>1</mn></msub></mrow></mfrac> <mo>=</mo> <mfrac><mrow><mi>d</mi><mi>C</mi></mrow>
    <mrow><mi>D</mi><msub><mi>A</mi> <mn>2</mn></msub></mrow></mfrac> <mfrac><mrow><mi>D</mi><msub><mi>A</mi>
    <mn>2</mn></msub></mrow> <mrow><mi>d</mi><msub><mi>Z</mi> <mn>2</mn></msub></mrow></mfrac>
    <mfrac><mrow><mi>d</mi><msub><mi>Z</mi> <mn>2</mn></msub></mrow> <mrow><mi>d</mi><msub><mi>A</mi>
    <mn>1</mn></msub></mrow></mfrac> <mfrac><mrow><mi>d</mi><msub><mi>A</mi> <mn>1</mn></msub></mrow>
    <mrow><mi>d</mi><msub><mi>Z</mi> <mn>1</mn></msub></mrow></mfrac> <mfrac><mrow><mi>d</mi><msub><mi>Z</mi>
    <mn>1</mn></msub></mrow> <mrow><mi>d</mi><msub><mi>W</mi> <mn>1</mn></msub></mrow></mfrac>
    <mo>=</mo> <mrow><mo>(</mo> <mn>2</mn> <msub><mi>A</mi> <mn>2</mn></msub> <mo>-</mo>
    <mn>2</mn> <mi>y</mi> <mo>)</mo></mrow> <mrow><mo>(</mo> <mfrac><msup><mi>e</mi>
    <mrow><mo>-</mo><msub><mi>Z</mi> <mn>2</mn></msub></mrow></msup> <msup><mfenced
    separators="" open="(" close=")"><mn>1</mn><mo>+</mo><msup><mi>e</mi> <mrow><mo>-</mo><msub><mi>Z</mi>
    <mn>2</mn></msub></mrow></msup></mfenced> <mn>2</mn></msup></mfrac> <mo>)</mo></mrow>
    <mrow><mo>(</mo> <msub><mi>W</mi> <mn>2</mn></msub> <mo>)</mo></mrow> <mrow><mo>(</mo>
    <msub><mi>Z</mi> <mn>1</mn></msub> <mo>></mo> <mn>0</mn> <mo>)</mo></mrow> <mrow><mo>(</mo>
    <mi>X</mi> <mo>)</mo></mrow></mrow></math><math alttext="StartFraction d upper
    C Over d upper B 1 EndFraction equals StartFraction d upper C Over upper D upper
    A 2 EndFraction StartFraction upper D upper A 2 Over d upper Z 2 EndFraction StartFraction
    d upper Z 2 Over d upper A 1 EndFraction StartFraction d upper A 1 Over d upper
    Z 1 EndFraction StartFraction d upper Z 1 Over d upper B 1 EndFraction equals
    left-parenthesis 2 upper A 2 minus 2 y right-parenthesis left-parenthesis StartFraction
    e Superscript minus upper Z 2 Baseline Over left-parenthesis 1 plus e Superscript
    minus upper Z 2 Baseline right-parenthesis squared EndFraction right-parenthesis
    left-parenthesis upper W 2 right-parenthesis left-parenthesis upper Z 1 greater-than
    0 right-parenthesis left-parenthesis 1 right-parenthesis" display="block"><mrow><mfrac><mrow><mi>d</mi><mi>C</mi></mrow>
    <mrow><mi>d</mi><msub><mi>B</mi> <mn>1</mn></msub></mrow></mfrac> <mo>=</mo> <mfrac><mrow><mi>d</mi><mi>C</mi></mrow>
    <mrow><mi>D</mi><msub><mi>A</mi> <mn>2</mn></msub></mrow></mfrac> <mfrac><mrow><mi>D</mi><msub><mi>A</mi>
    <mn>2</mn></msub></mrow> <mrow><mi>d</mi><msub><mi>Z</mi> <mn>2</mn></msub></mrow></mfrac>
    <mfrac><mrow><mi>d</mi><msub><mi>Z</mi> <mn>2</mn></msub></mrow> <mrow><mi>d</mi><msub><mi>A</mi>
    <mn>1</mn></msub></mrow></mfrac> <mfrac><mrow><mi>d</mi><msub><mi>A</mi> <mn>1</mn></msub></mrow>
    <mrow><mi>d</mi><msub><mi>Z</mi> <mn>1</mn></msub></mrow></mfrac> <mfrac><mrow><mi>d</mi><msub><mi>Z</mi>
    <mn>1</mn></msub></mrow> <mrow><mi>d</mi><msub><mi>B</mi> <mn>1</mn></msub></mrow></mfrac>
    <mo>=</mo> <mrow><mo>(</mo> <mn>2</mn> <msub><mi>A</mi> <mn>2</mn></msub> <mo>-</mo>
    <mn>2</mn> <mi>y</mi> <mo>)</mo></mrow> <mrow><mo>(</mo> <mfrac><msup><mi>e</mi>
    <mrow><mo>-</mo><msub><mi>Z</mi> <mn>2</mn></msub></mrow></msup> <msup><mfenced
    separators="" open="(" close=")"><mn>1</mn><mo>+</mo><msup><mi>e</mi> <mrow><mo>-</mo><msub><mi>Z</mi>
    <mn>2</mn></msub></mrow></msup></mfenced> <mn>2</mn></msup></mfrac> <mo>)</mo></mrow>
    <mrow><mo>(</mo> <msub><mi>W</mi> <mn>2</mn></msub> <mo>)</mo></mrow> <mrow><mo>(</mo>
    <msub><mi>Z</mi> <mn>1</mn></msub> <mo>></mo> <mn>0</mn> <mo>)</mo></mrow> <mrow><mo>(</mo>
    <mn>1</mn> <mo>)</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: We will use these chained gradients to calculate the slope for the cost function
    *C* with respect to <math alttext="upper W 1"><msub><mi>W</mi> <mn>1</mn></msub></math>
    , <math alttext="upper B 1"><msub><mi>B</mi> <mn>1</mn></msub></math> , <math
    alttext="upper W 2"><msub><mi>W</mi> <mn>2</mn></msub></math> , and <math alttext="upper
    B 2"><msub><mi>B</mi> <mn>2</mn></msub></math> .
  prefs: []
  type: TYPE_NORMAL
- en: Stochastic Gradient Descent
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We are now ready to integrate the chain rule to perform stochastic gradient
    descent. To keep things simple, we are going to sample only one training record
    on every iteration. Batch and mini-batch gradient descent are commonly used in
    neural networks and deep learning, but there’s enough linear algebra and calculus
    to juggle just one sample per iteration.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take a look at our full implementation of our neural network, with backpropagated
    stochastic gradient descent, in [Example 7-11](#MfDTsSwbfB).
  prefs: []
  type: TYPE_NORMAL
- en: Example 7-11\. Implementing a neural network using stochastic gradient descent
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: There is a lot going on here, but it builds on everything else we learned in
    this chapter. We perform 100,000 iterations of stochastic gradient descent. Splitting
    the training and testing data by 2/3 and 1/3, respectively, I get approximately
    97–99% accuracy in my testing dataset depending on how the randomness works out.
    This means after training, my neural network correctly identifies 97–99% of the
    testing data with the right light/dark font predictions.
  prefs: []
  type: TYPE_NORMAL
- en: The `backward_prop()` function is key here, implementing the chain rule to take
    the error in the output node (the squared residual), and then divide it up and
    distribute it backward to the output and hidden weights/biases to get the slopes
    with respect to each weight/bias. We then take those slopes and nudge the weights/biases
    in the `for` loop, respectively, multiplying with the learning rate `L` just like
    we did in Chapters [5](ch05.xhtml#ch05) and [6](ch06.xhtml#ch06). We do some matrix-vector
    multiplication to distribute the error backward based on the slopes, and we transpose
    matrices and vectors when needed so the dimensions between rows and columns match
    up.
  prefs: []
  type: TYPE_NORMAL
- en: If you want to make the neural network a bit more interactive, here’s a snippet
    of code in [Example 7-12](#wjmhutDoNG) where we can type in different background
    colors (through an R, G, and B value) and see if it predicts a light or dark font.
    Append it to the bottom of the previous code [Example 7-11](#MfDTsSwbfB) and give
    it a try!
  prefs: []
  type: TYPE_NORMAL
- en: Example 7-12\. Adding an interactive shell to our neural network
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Building your own neural network from scratch is a lot of work and math, but
    it gives you insight into their true nature. By working through the layers, the
    calculus, and the linear algebra, we get a stronger sense of what deep learning
    libraries like PyTorch and TensorFlow do behind the scenes.
  prefs: []
  type: TYPE_NORMAL
- en: As you have gathered from reading this entire chapter, there are a lot of moving
    parts to make a neural network tick. It can be helpful to put a breakpoint in
    different parts of the code to see what each matrix operation is doing. You can
    also port the code into a Jupyter Notebook to get more visual insight into each
    step.
  prefs: []
  type: TYPE_NORMAL
- en: 3Blue1Brown on Backpropagation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 3Blue1Brown has some classic videos talking about [backpropagation](https://youtu.be/Ilg3gGewQ5U)
    and the [calculus behind neural networks](https://youtu.be/tIeHLnjs5U8).
  prefs: []
  type: TYPE_NORMAL
- en: Using scikit-learn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There is some limited neural network functionality in scikit-learn. If you are
    serious about deep learning, you will probably want to study PyTorch or TensorFlow
    and get a computer with a strong GPU (there’s a great excuse to get that gaming
    computer you always wanted!). I have been told that all the cool kids are using
    PyTorch now. However, scikit-learn does have some convenient models available,
    including the `MLPClassifier`, which stands for “multi-layer perceptron classifier.”
    This is a neural network designed for classification, and it uses a logistic output
    activation by default.
  prefs: []
  type: TYPE_NORMAL
- en: '[Example 7-13](#rchKuwCjUt) is a scikit-learn version of the background color
    classification application we developed. The `activation` argument specifies the
    hidden layer.'
  prefs: []
  type: TYPE_NORMAL
- en: Example 7-13\. Using scikit-learn neural network classifier
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Running this code I get about 99.3% accuracy on my testing data.
  prefs: []
  type: TYPE_NORMAL
- en: MNIST Example Using scikit-learn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To see a scikit-learn example predicting handwritten digits using the MNIST
    dataset, turn to [Appendix A](app01.xhtml#appendix).
  prefs: []
  type: TYPE_NORMAL
- en: Limitations of Neural Networks and Deep Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For all of their strengths, neural networks struggle with certain types of
    tasks. This flexibility with layers, nodes, and activation functions makes it
    flexible fitting to data in a nonlinear manner…probably too flexible. Why? It
    can overfit to the data. Andrew Ng, a pioneer in deep learning education and the
    former head of Google Brain, mentioned this as a problem in a press conference
    in 2021\. Asked why machine learning has not replaced radiologists yet, this was
    his answer in an [*IEEE Spectrum* article](https://oreil.ly/ljXsz):'
  prefs: []
  type: TYPE_NORMAL
- en: It turns out that when we collect data from Stanford Hospital, then we train
    and test on data from the same hospital, indeed, we can publish papers showing
    [the algorithms] are comparable to human radiologists in spotting certain conditions.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: It turns out [that when] you take that same model, that same AI system, to an
    older hospital down the street, with an older machine, and the technician uses
    a slightly different imaging protocol, that data drifts to cause the performance
    of AI system to degrade significantly. In contrast, any human radiologist can
    walk down the street to the older hospital and do just fine.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: So even though at a moment in time, on a specific data set, we can show this
    works, the clinical reality is that these models still need a lot of work to reach
    production.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In other words, the machine learning overfitted to the Stanford hospital training
    and testing dataset. When taken to other hospitals with different machinery, the
    performance degraded significantly due to the overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: 'The same challenges occur with autonomous vehicles and self-driving cars. It
    is not enough to just train a neural network on one stop sign! It has to be trained
    on countless combinations of conditions around that stop sign: good weather, rainy
    weather, night and day, with graffiti, blocked by a tree, in different locales,
    and so on. In traffic scenarios, think of all the different types of vehicles,
    pedestrians, pedestrians dressed in costumes, and infinite number of edge cases
    that will be encountered! There is simply no effective way to capture every type
    of event that is encountered on the road just by having more weights and biases
    in a neural network.'
  prefs: []
  type: TYPE_NORMAL
- en: This is why autonomous vehicles themselves do not use neural networks in an
    end-to-end manner. Instead, different software and sensor modules are broken up
    where one module may use a neural network to draw a box around an object. Then
    another module will use a different neural network to classify the object in that
    box, such as a pedestrian. From there, traditional rule-based logic will attempt
    to predict the path of the pedestrian and hardcoded logic will choose from different
    conditions on how to react. The machine learning was limited to label-making activity,
    not the tactics and maneuvers of the vehicle. On top of that, basic sensors like
    radar will simply stop if an unknown object is detected in front of the vehicle,
    and this is just another piece of the technology stack that does not use machine
    learning or deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: This might be surprising given all the media headlines about neural networks
    and deep learning [beating humans in games like Chess and Go](https://oreil.ly/9zFxM),
    or even besting [pilots in combat flight simulations](https://oreil.ly/hbdYI).
    It is important to remember in reinforcement learning environments like these
    that simulations are closed worlds, where infinite amounts of labeled data can
    be generated and learned through a virtual finite world. However, the real world
    is not a simulation where we can generate unlimited amounts of data. Also, this
    is not a philosophy book so we will pass on discussions whether we live in a simulation.
    Sorry, Elon! Collecting data in the real world is expensive and hard. On top of
    that, the real world is filled with infinite unpredictability and rare events.
    All these factors drive machine learning practitioners to [resort to data entry
    labor to label pictures of traffic objects](https://oreil.ly/mhjvz) and other
    data. Autonomous vehicle startups often have to pair this kind of data entry work
    with simulated data, because the miles and edge case scenarios needed to generate
    training data are too astronomical to gather simply by driving a fleet of vehicles
    millions of miles.
  prefs: []
  type: TYPE_NORMAL
- en: 'These are all reasons why AI research likes to use board games and video games,
    because unlimited labeled data can be generated easily and cleanly. Francis Chollet,
    a renowned engineer at Google who developed Keras for TensorFlow (and also wrote
    a great book, *Deep Learning with Python*), shared some insight on this in a [*Verge*
    article](https://oreil.ly/4PDLf):'
  prefs: []
  type: TYPE_NORMAL
- en: The thing is, once you pick a measure, you’re going to take whatever shortcut
    is available to game it. For instance, if you set chess-playing as your measure
    of intelligence (which we started doing in the 1970s until the 1990s), you’re
    going to end up with a system that plays chess, and that’s it. There’s no reason
    to assume it will be good for anything else at all. You end up with tree search
    and minimax, and that doesn’t teach you anything about human intelligence. Today,
    pursuing skill at video games like Dota or StarCraft as a proxy for general intelligence
    falls into the exact same intellectual trap…
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: If I set out to “solve” Warcraft III at a superhuman level using deep learning,
    you can be quite sure that I will get there as long as I have access to sufficient
    engineering talent and computing power (which is on the order of tens of millions
    of dollars for a task like this). But once I’d have done it, what would I have
    learned about intelligence or generalization? Well, nothing. At best, I’d have
    developed engineering knowledge about scaling up deep learning. So I don’t really
    see it as scientific research because it doesn’t teach us anything we didn’t already
    know. It doesn’t answer any open question. If the question was, “Can we play X
    at a superhuman level?,” the answer is definitely, “Yes, as long as you can generate
    a sufficiently dense sample of training situations and feed them into a sufficiently
    expressive deep learning model.” We’ve known this for some time.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: That is, we have to be careful to not conflate an algorithm’s performance in
    a game with broader capabilities that have yet to be solved. Machine learning,
    neural networks, and deep learning all work narrowly on defined problems. They
    cannot broadly reason or choose their own tasks, or ponder objects they have not
    seen before. Like any coded application, they do only what they were programmed
    to do.
  prefs: []
  type: TYPE_NORMAL
- en: With whatever tool it takes, solve the problem. There should be no partiality
    to neural networks or any other tool at your disposal. With all of this in mind,
    using a neural network might not be the best option for the task in front of you.
    It is important to always consider what problem you are striving to solve without
    making a specific tool your primary objective. The use of deep learning has to
    be strategic and warranted. There are certainly use cases, but in most of your
    everyday work you will likely have more success with simpler and more biased models
    like linear regression, logistic regression, or traditional rule-based systems.
    But if you find yourself having to classify objects in images, and you have the
    budget and labor to build that dataset, then deep learning is going to be your
    best bet.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Neural networks and deep learning offer some exciting applications, and we just
    scratched the surface in this one chapter. From recognizing images to processing
    natural language, there continue to be use cases for applying neural networks
    and their different flavors of deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: From scratch, we learned how to structure a simple neural network with one hidden
    layer to predict whether or not a light or dark font should be used against a
    background color. We also applied some advanced calculus concepts to calculate
    partial derivatives of nested functions and applied it to stochastic gradient
    descent to train our neural network. We also touched on libraries like scikit-learn.
    While we do not have the bandwidth in this book to talk about TensorFlow, PyTorch,
    and more advanced applications, there are great resources out there to expand
    your knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: '[3Blue1Brown has a fantastic playlist on neural networks and backpropagation](https://oreil.ly/VjwBr),
    and it is worth watching multiple times. [Josh Starmer’s StatQuest playlist on
    neural networks](https://oreil.ly/YWnF2) is helpful as well, particularly in visualizing
    neural networks as manifold manipulation. Another great video about manifold theory
    and neural networks can be [found here on the Art of the Problem](https://youtu.be/e5xKayCBOeU).
    Finally, when you are ready to deep-dive, check out Aurélien Géron’s *Hands-On
    Machine Learning with Scikit-Learn, Keras, and TensorFlow* (O’Reilly) and Francois
    Chollet’s *Deep Learning with Python* (Manning).'
  prefs: []
  type: TYPE_NORMAL
- en: If you made it to the end of this chapter and feel you absorbed everything within
    reason, congrats! You have not just effectively learned probability, statistics,
    calculus, and linear algebra but also applied it to practical applications like
    linear regression, logistic regression, and neural networks. We will talk about
    how you can proceed in the next chapter and start a new phase of your professional
    growth.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apply a neural network to the employee retention data we worked with in [Chapter 6](ch06.xhtml#ch06).
    You can import the data [here](https://tinyurl.com/y6r7qjrp). Try to build this
    neural network so it predicts on this dataset and use accuracy and confusion matrices
    to evaluate performance. Is it a good model for this problem? Why or why not?
  prefs: []
  type: TYPE_NORMAL
- en: While you are welcome to build the neural network from scratch, consider using
    scikit-learn, PyTorch, or another deep learning library to save time.
  prefs: []
  type: TYPE_NORMAL
- en: Answers are in [Appendix B](app02.xhtml#exercise_answers).
  prefs: []
  type: TYPE_NORMAL
