<html><head></head><body><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 1. The Causal-Behavioral Framework for Data Analysis"><div class="chapter" id="the_causal_behavioral_framework_for_da">
<h1><span class="label">Chapter 1. </span>The Causal-Behavioral <span class="keep-together">Framework for Data Analysis</span></h1>

<p>As we discussed in the preface, understanding what drives behaviors<a contenteditable="false" data-primary="causal-behavioral framework" data-secondary="about goal of" data-type="indexterm" id="idm45968167244344"/> in order to change them is one of the key goals of applied analytics, whether in a business, a nonprofit, or a public organization. We want to figure out why someone bought something and why someone else <em>didn’t</em> buy it. We want to understand why someone renewed their subscription, contacted a call center instead of paying online, registered to be an organ donor, or gave to a nonprofit. Having this knowledge allows us to predict what people will do under different scenarios and helps us to determine what our organization can do to encourage them to do it again (or not). I believe that this goal is best achieved by combining data analysis with a behavioral science mindset and a causal analytics toolkit to create an integrated approach I have dubbed the “causal-behavioral framework.” In this framework, <em>behaviors</em> are at the top because understanding them is our ultimate goal. <a contenteditable="false" data-primary="causal diagrams (CDs)" data-secondary="about causal-behavioral framework" data-type="indexterm" id="idm45968167241064"/><a contenteditable="false" data-primary="data" data-secondary="about causal-behavioral framework" data-type="indexterm" id="idm45968167239592"/>This understanding is achieved by using <em>causal diagrams</em> and <em>data</em>, which form the two supporting pillars of the triangle (<a data-type="xref" href="#the_causal_behavioral_framework_for_dat">Figure 1-1</a>).</p>

<figure><div id="the_causal_behavioral_framework_for_dat" class="figure"><img alt="The causal-behavioral framework for data analysis" src="Images/BEDA_0101.png" width="596" height="371"/>
<h6><span class="label">Figure 1-1. </span>The causal-behavioral framework for data analysis</h6>
</div></figure>

<p>Over the course of the book, we’ll explore each leg of the triangle and see how they connect to each other. In the final chapter, we’ll see all of our work come together by achieving with one line of code what would be a daunting task with traditional approaches: measuring the degree to which customer satisfaction increases future customer spending. In addition to performing such extraordinary feats, this new framework will also allow you to more effectively perform common analyses such as determining the effect of an email campaign or a product feature on purchasing behavior.</p>

<p>Before getting to that, readers familiar with predictive analytics may <a contenteditable="false" data-primary="predictive analytics" data-secondary="causal analytics versus" data-type="indexterm" id="idm45968171188664"/><a contenteditable="false" data-primary="causal analytics" data-secondary="predictive analytics versus" data-type="indexterm" id="idm45968171187208"/>wonder why I’m advocating for causal analytics instead. The answer is that even though predictive analytics have been (and will remain) very successful in business settings, they can fall short when your analyses pertain to human behaviors. In particular, adopting a causal approach can help us identify and resolve “confounding,” a very common problem with behavioral data. I’ll elaborate on these points in the rest of this first chapter.</p>

<section data-type="sect1" data-pdf-bookmark="Why We Need Causal Analytics to Explain Human Behavior"><div class="sect1" id="why_we_need_causal_analytics_to_explain">
<h1>Why We Need Causal Analytics to Explain <span class="keep-together">Human Behavior</span></h1>

<p>Understanding where causal analytics fits into the analytics landscape will help us better identify why it is needed in business settings. As we’ll see, that need stems from the complexity of human behavior.</p>

<section data-type="sect2" data-pdf-bookmark="The Different Types of Analytics"><div class="sect2" id="the_different_types_of_analytics">
<h2>The Different Types of Analytics</h2>

<p>There are three different types of analytics: descriptive, predictive, and causal. <a contenteditable="false" data-primary="causal analytics" data-secondary="about analytics" data-type="indexterm" id="idm45968171180536"/><a contenteditable="false" data-primary="analytics, types of" data-type="indexterm" id="idm45968171179160"/><a contenteditable="false" data-primary="descriptive analytics" data-type="indexterm" id="idm45968171178056"/>Descriptive analytics provides a <em>description</em> of data. In simple terms, I think of it as “what is” or “what we’ve measured” analytics. Business reporting falls under that umbrella. How many customers canceled their subscriptions last month? How much profit did we make last year? Whenever we’re calculating an average or other simple metrics, we’re implicitly using descriptive analytics. Descriptive analytics is the simplest form of analytics, but it is also underappreciated. Many organizations actually struggle to get a clear and unified view of their operations. To see the extent of that problem in an organization, just ask the same question of the finance department and the operations department and measure how different the answers are.<sup><a data-type="noteref" id="ch01fn1-marker" href="ch01.xhtml#ch01fn1">1</a></sup></p>

<p>Predictive analytics provides a <em>prediction</em>. <a contenteditable="false" data-primary="predictive analytics" data-secondary="about" data-type="indexterm" id="idm45968171173256"/>I think of it as “what will be, assuming current conditions persist” or “what we haven’t yet measured” analytics. Most machine learning methods (e.g., neural networks and gradient boosting models) belong to this type of analytics and help us answer questions like “How many customers will cancel their subscription next month?” and “Is that order fraudulent?” Over the past few decades, predictive analytics has transformed the world; the legions of data scientists employed in business are a testament to its success.</p>

<p>Finally, causal analytics provides the <em>causes</em> of data. <a contenteditable="false" data-primary="causal analytics" data-secondary="about" data-type="indexterm" id="idm45968171170280"/>I think of it as “what if?” or “what will be, under different conditions” analytics. It answers questions such as “How many customers will cancel their subscription next month <em>unless we send them a coupon</em>?” <a contenteditable="false" data-primary="A/B tests" data-secondary="about causal analytics" data-type="indexterm" id="idm45968171168072"/><a contenteditable="false" data-primary="randomized controlled trials as causal analytics tool" data-type="indexterm" id="idm45968171166696"/>The most well-known tool of causal analytics is the A/B test, a.k.a. a randomized experiment or randomized controlled trial (RCT). That’s because the simplest and most effective way to answer the preceding question is to send a coupon to a randomly selected group of customers and see how many of them cancel their subscription compared to a control group.</p>

<p>We’ll cover experimentation in <a data-type="xref" href="part04.xhtml#designing_and_analyzing_experiments">Part IV</a> of the book, but before that, in <a data-type="xref" href="part02.xhtml#causal_diagrams_and_deconfounding">Part II</a>, we’ll look at another tool from that toolkit, namely causal diagrams, which can be used even when we can’t experiment. Indeed, it is one of my goals to get you to think more broadly about causal analytics rather than just equate it with experimentation.</p>

<div data-type="note" epub:type="note"><h6>Note</h6>
<p>While these labels may give the impression of a neat categorization, in reality, there is more of a gradient between these three categories, and questions and methods get blurred between them. You may also encounter other terms, such as <em>prescriptive analytics</em>, that further blur the lines and add other nuances without dramatically altering the overall picture.</p>
</div>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Human Beings Are Complicated"><div class="sect2" id="human_beings_are_complicated">
<h2>Human Beings Are Complicated</h2>

<p>If predictive analytics has been so successful and causal analytics<a contenteditable="false" data-primary="causal analytics" data-secondary="about" data-tertiary="human complexity" data-type="indexterm" id="idm45968171158360"/><a contenteditable="false" data-primary="human behavior" data-secondary="about human complexity" data-type="indexterm" id="idm45968171156632"/><a contenteditable="false" data-primary="predictive analytics" data-secondary="complexity of human behavior" data-type="indexterm" id="idm45968171155256"/> uses the same data analysis tools like regression, why not stick with predictive analytics? In short, because human beings are more complicated than wind turbines. Human behavior:</p>

<dl>
	<dt>Has multiple causes</dt>
	<dd>
	<p>A turbine’s behavior is not influenced by its personality, the social norms of the turbine community, or the circumstances of its upbringing, <a contenteditable="false" data-primary="variables" data-secondary="predictive power with human behavior" data-type="indexterm" id="idm45968171151448"/>whereas the predictive power of any single variable on human behavior is almost always disappointing because of those factors.</p></dd>
	<dt>Is context-dependent</dt>
	<dd>
	<p>Minor or cosmetic alterations to the environment, such as a change<a contenteditable="false" data-primary="context dependency of human behavior" data-type="indexterm" id="idm45968171148680"/><a contenteditable="false" data-primary="environment and behavior" data-secondary="complexity of human behavior" data-type="indexterm" id="idm45968171147480"/> in the default option of a choice, can have large impacts on behavior. This is a blessing from a behavioral <em>design</em> perspective because it allows us to drive changes in behaviors, but it’s a curse from a behavioral <em>analytics</em> perspective because it means that every situation is unique in ways that are hard to predict.</p></dd>
	<dt>Is variable (scientists would say nondeterministic)</dt>
	<dd>
	<p>The same person may behave very differently when placed repeatedly in what seems like exactly the same situation, even after controlling for cosmetic factors. This may be due to transient effects, such as moods, or long-term effects, such as getting bored with having the same breakfast every day. Both of these can dramatically change behavior but are hard to capture.</p></dd>
	<dt>Is innovative</dt>
	<dd>
	<p>When conditions in the environment change, a human can switch to a behavior they have literally never exhibited before, and it happens even under the most mundane circumstances. For example, there’s a car accident ahead on your normal commuting path and so you decide at the last minute to take a right turn.</p></dd>
	<dt>Is strategic</dt>
	<dd>
	<p>Humans infer and react to the behaviors and intentions of others. In some cases, that can mean “repairing” a cooperation that was derailed by external circumstances, making it more robustly predictable. But in other cases, it can involve voluntarily obfuscating one’s behavior to make it unpredictable when playing a competitive game like chess (or fraud!).</p>
	</dd>
</dl>

<p>All these aspects of human behavior make it less predictable than that of physical objects. <a contenteditable="false" data-primary="human behavior" data-secondary="predicting future from past" data-type="indexterm" id="idm45968171139752"/>To find regularities that are more reliable for analysis, we must go one level deeper to understand and measure the causes of behavior. The fact that someone had oatmeal for breakfast and took a certain route on Monday doesn’t mean that they will do the same on Tuesday, but you can be more confident that they’ll have <em>some</em> breakfast and will take <em>some</em> route to their work.</p>

<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="extrapolation_in_analyticscomma_the_cur">
<h5>Extrapolation in Analytics, the Curse of <span class="keep-together">Dimensionality, and the Lucas Critique</span></h5>

<p>Readers with a quantitative background may not be fully satisfied with my statement that “human behavior is hard to predict because it’s complicated,” so here’s the math-ier version of the argument. <a contenteditable="false" data-primary="interpolation" data-type="indexterm" id="idm45968171134328"/><a contenteditable="false" data-primary="extrapolation" data-type="indexterm" id="idm45968171133224"/><a contenteditable="false" data-primary="data" data-secondary="interpolation" data-type="indexterm" id="idm45968171132120"/><a contenteditable="false" data-primary="data" data-secondary="extrapolation" data-type="indexterm" id="idm45968171130744"/><a contenteditable="false" data-primary="predictive analytics" data-secondary="interpolation versus extrapolation" data-type="indexterm" id="idm45968171129368"/>I’ll start by describing the difference between interpolation and extrapolation. <a data-type="xref" href="#linear_relationship_between_two_variabl">Figure 1-2</a> shows some simulated data with a linear relationship between two variables.</p>

<p>The line in the figure is the regression line of best fit, <a contenteditable="false" data-primary="variables" data-secondary="regression line of best fit predicting values" data-type="indexterm" id="idm45968171126200"/><a contenteditable="false" data-primary="regression" data-secondary="line of best fit" data-type="indexterm" id="idm45968171124648"/><a contenteditable="false" data-primary="linear regression" data-secondary="line of best fit" data-type="indexterm" id="idm45968171123272"/>the line corresponding to the linear regression between the two variables, with a slope of approximately 3. We can use it to predict unknown Y values based on a known X value (and vice versa). For example, given the value X = 50, we would predict that Y is equal to 150. There are observed points to the left of that value, that is, points for which X &lt; 50, as well as points to the right of that value, for which X &gt; 50. This predictive process is called interpolation because our point is between observed points (the prefix <em>inter</em> means “between”; e.g., <em>international</em> = “between nations”). Conversely, if we used the regression line with X = 0 to predict that Y = 0, this would be called extrapolation, as the point we’re trying to predict is outside of the cloud of observed points (<em>extra</em> means “outside”; e.g., <em>extraordinary</em> = “outside of the ordinary”). In statistics and in everyday life, to extrapolate is to leave the realm of the observed and the known in order to make predictions. Whereas interpolation is usually safe and reliable, extrapolation is always somewhat speculative: it takes a leap of faith to assume that the rules that applied within certain boundaries will still apply outside of them.</p>

<figure><div id="linear_relationship_between_two_variabl" class="figure"><img alt="Linear relationship between two variables, with the regression line" src="Images/BEDA_0102.png" width="1918" height="904"/>
<h6><span class="label">Figure 1-2. </span>Linear relationship between two variables, with the regression line</h6>
</div></figure>

<p>Physical objects like a wind turbine are affected by a reasonably small and constant number of factors (it’s not like some laws of physics take days off or new ones appear randomly). Therefore we have a lot of data points relative to the dimensions of the problem space, which means that we’re almost always interpolating. For simplicity’s sake, a model may neglect secondary or rare phenomena, such as a 1-in-100-year storm, but even when such outliers occur, the outcome remains somewhat predictable: the blade will break and fall in the water, not fly away.</p>

<p>On the other hand, human behavior is affected by a large number of different factors,<a contenteditable="false" data-primary="data" data-secondary="about human behavioral data" data-type="indexterm" id="idm45968171115464"/><a contenteditable="false" data-primary="human behavior" data-secondary="data extrapolation" data-type="indexterm" id="idm45968171114120"/><a contenteditable="false" data-primary="data" data-secondary="extrapolation" data-tertiary="human behavior data" data-type="indexterm" id="idm45968171112744"/> which may or may not be relevant on a given day and may grow or fade over time. Therefore we usually end up having few data points relative to the dimensions of the problem space, which means that we’re much more frequently extrapolating—an<a contenteditable="false" data-primary="extrapolation" data-secondary="curse of dimensionality" data-type="indexterm" id="idm45968171110712"/><a contenteditable="false" data-primary="curse of dimensionality" data-type="indexterm" id="idm45968171109336"/><a contenteditable="false" data-primary="dimensionality curse" data-type="indexterm" id="idm45968171108232"/> issue known in statistics as the “curse of dimensionality.” <a contenteditable="false" data-primary="human behavior" data-secondary="predicting future from past" data-type="indexterm" id="idm45968171107000"/><a contenteditable="false" data-primary="predictive analytics" data-secondary="complexity of human behavior" data-type="indexterm" id="idm45968171105528"/><a contenteditable="false" data-primary="variables" data-secondary="predictive power with human behavior" data-type="indexterm" id="idm45968171104136"/>In addition, minor changes in the environment can lead to major changes in behavior, which makes trying to predict future human behavior based on past behavior alone a gamble with poor odds.</p>

<p>For people interested in the genealogy of behavioral economics, the <a contenteditable="false" data-primary="Lucas, Robert" data-type="indexterm" id="idm45968171102120"/>macroeconomist Robert Lucas articulated that point in the 1970s (the “Lucas critique” of Keynesian models). He recommended instead identifying the deeper parameters of human behaviors such as consumer preferences, another version of the argument I made <span class="keep-together">earlier.</span></p>
</div></aside>
</div></section>
</div></section>

<section data-type="sect1" data-pdf-bookmark="Confound It! The Hidden Dangers of Letting Regression Sort It Out"><div class="sect1" id="confound_itexclamation_mark_the_hidden">
<h1>Confound It! The Hidden Dangers of Letting Regression Sort It Out</h1>

<p>I mentioned in the previous section that causal analytics often uses the same tools as predictive analytics. However, because they have different goals, the tools are used in different ways. <a contenteditable="false" data-primary="regression" data-secondary="predictive versus causal analytics" data-type="indexterm" id="idm45968171097624"/><a contenteditable="false" data-primary="predictive analytics" data-secondary="causal analytics versus" data-tertiary="regression demonstrating" data-type="indexterm" id="idm45968171096280"/><a contenteditable="false" data-primary="causal analytics" data-secondary="predictive analytics versus" data-tertiary="regression demonstrating" data-type="indexterm" id="idm45968171094664"/>Since regression is one of the main tools for both types of analytics, it can be a great way to illustrate the difference between predictive and causal analytics. A regression appropriate for predictive analytics would often make a terrible regression for causal analytics purposes, and vice versa.</p>

<p>A regression for predictive analytics is used to estimate an unknown value (often, but not always, in the future). It does this by taking known information and using a variety of factors to triangulate the best guess value for a given variable. What is important is the predicted value and its accuracy, not why or how it was predicted.</p>

<p>Causal analytics also uses regression, but the focus is not on estimating a value of the target variable. <a contenteditable="false" data-primary="dependent variable" data-secondary="causal analytics" data-type="indexterm" id="idm45968171091272"/>Instead, the focus is on the cause of that value. In regression terms, our interest is no longer in the dependent variable itself but in its relationship with a given independent variable. <a contenteditable="false" data-primary="correlation" data-secondary="causation implied" data-tertiary="coefficient of correlation as measure of causal effect" data-type="indexterm" id="idm45968171089560"/><a contenteditable="false" data-primary="causes" data-secondary="correlation implying causation" data-tertiary="coefficient of correlation as measure of causal effect" data-type="indexterm" id="idm45968171087944"/><a contenteditable="false" data-primary="regression" data-secondary="coefficient of correlation" data-type="indexterm" id="idm45968171086248"/><a contenteditable="false" data-primary="coefficient of correlation as measure of causal effect" data-type="indexterm" id="idm45968171084856"/><a contenteditable="false" data-primary="causal analytics" data-secondary="coefficient of correlation" data-type="indexterm" id="idm45968171083720"/><a contenteditable="false" data-primary="variables" data-secondary="coefficient of correlation" data-type="indexterm" id="idm45968171082328"/>With a correctly structured regression, the coefficient of correlation can be a portable measure of the causal effect of an independent variable on a dependent variable.</p>

<p>But what does it mean to have a correctly structured regression for that purpose? <a contenteditable="false" data-primary="regression" data-secondary="coefficient of correlation" data-tertiary="correctly structured regression" data-type="indexterm" id="idm45968171080200"/>Why can’t we just take the regressions we already use for predictive analytics and treat the provided coefficients as measures of the causal relationship? We can’t do that because each variable in the regression has the potential to modify the coefficients of other variables. Therefore our variable mix has to be crafted not to create the most accurate prediction but to create the most accurate coefficients. The two sets of variables are generally different because a variable can be highly correlated with our target variable (and therefore be highly predictive) without actually affecting that variable.</p>

<p>In this section, we will explore why this difference in perspective matters and <a contenteditable="false" data-primary="variables" data-secondary="variable selection examples" data-type="indexterm" id="idm45968171077352"/><a contenteditable="false" data-primary="data" data-secondary="variable selection examples" data-type="indexterm" id="idm45968171075960"/><a contenteditable="false" data-primary="C-Mart fictional supermarket chain" data-secondary="variable selection" data-type="indexterm" id="idm45968171074568"/>why variable selection is more than half the battle in behavioral analytics. We’ll do so with a concrete example from C-Mart, a fictional supermarket chain with stores across the United States. The first of two fictional companies used throughout the book, C-Mart will help us understand the opportunities and challenges of data analysis for brick-and-mortar companies in the digital age.</p>

<section class="pagebreak-before" data-type="sect2" data-pdf-bookmark="Data"><div class="sect2" id="data">
<h2 class="less_space">Data</h2>

<p>The <a href="https://oreil.ly/BehavioralDataAnalysisCh1">GitHub folder for this chapter</a><a contenteditable="false" data-primary="variables" data-secondary="variable selection examples" data-tertiary="data for" data-type="indexterm" id="idm45968171069576"/><a contenteditable="false" data-primary="data" data-secondary="variable selection examples" data-tertiary="data for" data-type="indexterm" id="idm45968171067896"/><a contenteditable="false" data-primary="GitHub" data-secondary="variable selection data" data-type="indexterm" id="idm45968171066232"/> contains two CSV files, <em>chap1-stand_data.csv</em> and <em>chap1-survey_data.csv</em> with the data sets for the two examples in this chapter.</p>

<p><a data-type="xref" href="#Sales_info">Table 1-1</a> shows the information contained by the CSV file <em>chap1-stand_data.csv</em> at the daily level about sales of ice cream and iced coffee in C-Mart’s stands.</p>

<table class="border" id="Sales_info">
	<caption><span class="label">Table 1-1. </span>Sales information in chap1-stand_data.csv</caption>
	<thead>
		<tr>
			<th>Variable name</th>
			<th>Variable description</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td><em>IceCreamSales</em></td>
			<td>Daily sales of ice cream in C-Mart’s stands</td>
		</tr>
		<tr>
			<td><em>IcedCoffeeSales</em></td>
			<td>Daily sales of iced coffee in C-Mart’s stands</td>
		</tr>
		<tr>
			<td><em>SummerMonth</em></td>
			<td>Binary variable indicating whether the day is in the summer months</td>
		</tr>
		<tr>
			<td><em>Temp</em></td>
			<td>The average temperature for that day and that stand</td>
		</tr>
	</tbody>
</table>

<p><a data-type="xref" href="#Survey_info">Table 1-2</a> shows the information contained in the CSV file <em>chap1-survey_data.csv</em> from a survey of passersby outside of C-Mart’s stands.</p>

<table class="border" id="Survey_info">
	<caption><span class="label">Table 1-2. </span>Survey information in chap1-survey_data.csv</caption>
	<thead>
		<tr>
			<th>Variable name</th>
			<th>Variable description</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td><em>VanillaTaste</em></td>
			<td>Interviewee’s taste for vanilla, 0-25</td>
		</tr>
		<tr>
			<td><em>ChocTaste</em></td>
			<td>Interviewee’s taste for chocolate, 0-25</td>
		</tr>
		<tr>
			<td><em>Shopped</em></td>
			<td>Binary variable indicating whether the interviewee has ever shopped at the local C-Mart stand</td>
		</tr>
	</tbody>
</table>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Why Correlation Is Not Causation: A Confounder in Action"><div class="sect2" id="why_correlation_is_not_causation_a_conf">
<h2>Why Correlation Is Not Causation: A Confounder in Action</h2>

<p>C-Mart has an ice cream stand in each store. <a contenteditable="false" data-primary="C-Mart fictional supermarket chain" data-secondary="variable selection" data-tertiary="confounder in action" data-type="indexterm" id="idm45968171039944"/><a contenteditable="false" data-primary="variables" data-secondary="variable selection examples" data-tertiary="confounder in action" data-type="indexterm" id="idm45968171038280"/><a contenteditable="false" data-primary="data" data-secondary="variable selection examples" data-tertiary="confounder in action" data-type="indexterm" id="idm45968171036616"/><a contenteditable="false" data-primary="confounders" data-secondary="C-Mart fictional supermarket data" data-type="indexterm" id="idm45968171034952"/>It is the company’s belief that the weather influences daily sales—or, to cast it in causality jargon, that the weather is a cause of sales. In other words, everything else being equal, we assume that people are more likely to buy ice cream on hotter days, which makes intuitive sense. This belief is supported by a strong correlation in historical data between temperature and sales as shown in <a data-type="xref" href="#sales_of_ice_cream_as_a_function_of_obs">Figure 1-3</a> (the corresponding data and code are on the book’s GitHub).</p>

<figure><div id="sales_of_ice_cream_as_a_function_of_obs" class="figure"><img alt="Sales of ice cream as a function of observed temperature" src="Images/BEDA_0103.png" width="1912" height="1166"/>
<h6><span class="label">Figure 1-3. </span>Sales of ice cream as a function of observed temperature</h6>
</div></figure>

<p>As indicated in the Preface, we’ll be using regression as our main tool for data analysis. Running a linear regression of the sales of ice cream on the temperature takes a single line of code:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="c1">## Python (output not shown)</code>
<code class="k">print</code><code class="p">(</code><code class="n">ols</code><code class="p">(</code><code class="s2">"icecream_sales ~ temps"</code><code class="p">,</code> <code class="n">data</code><code class="o">=</code><code class="n">stand_data_df</code><code class="p">)</code><code class="o">.</code><code class="n">fit</code><code class="p">()</code><code class="o">.</code><code class="n">summary</code><code class="p">())</code></pre>

<pre data-code-language="r" data-type="programlisting">
<code class="c1">## R</code>
<code class="o">&gt;</code> <code class="nf">summary</code><code class="p">(</code><code class="nf">lm</code><code class="p">(</code><code class="n">icecream_sales</code> <code class="o">~</code> <code class="n">temps</code><code class="p">,</code> <code class="n">data</code><code class="o">=</code><code class="n">stand_dat</code><code class="p">))</code>
<code class="kc">...</code>
<code class="n">Coefficients</code><code class="o">:</code>
             <code class="n">Estimate</code> <code class="n">Std.</code> <code class="n">Error</code> <code class="n">t</code> <code class="n">value</code> <code class="nf">Pr</code><code class="p">(</code><code class="o">&gt;|</code><code class="n">t</code><code class="o">|</code><code class="p">)</code>    
<code class="p">(</code><code class="n">Intercept</code><code class="p">)</code> <code class="m">-4519.055</code>    <code class="m">454.566</code>  <code class="m">-9.941</code>   <code class="o">&lt;</code><code class="m">2e-16</code> <code class="o">***</code>
<code class="n">temps</code>        <code class="m">1145.320</code>      <code class="m">7.826</code> <code class="m">146.348</code>   <code class="o">&lt;</code><code class="m">2e-16</code> <code class="o">***</code>
<code class="kc">...</code></pre>

<p>For our purposes in this book, the most relevant piece of the output is the <em>coefficients</em> section, which tells us that the estimated intercept—the theoretical average ice cream sales for a temperature of zero—is −4,519, which is obviously a nonsensical extrapolation. It also tells us that the estimated coefficient for the temperature is 1,145, which means that each additional degree of temperature is expected to increase sales of ice cream by $1,145.</p>

<p>Now, let’s imagine that we’re at the end of a particularly warm week of October, and based on the predictions of the model, the company had increased the stock of the ice cream stands ahead of time. However, the weekly sales, while higher than usual for this week of October, have fallen quite short of the quantity predicted by the model. Oops! What happened? Should the data analyst be fired?</p>

<p>What happened is that the model doesn’t account for a crucial fact: most of the sales of ice cream take place during the summer months when kids are out of school. The regression model made its best prediction with the data available, but part of the cause of increased ice cream sales (summer break for students) was misattributed to temperature because summer months are positively correlated with temperature. Since the temperature increase in October did not suddenly make it summer break (sorry, kids!), we saw lower sales than we did on summer days at that temperature.</p>

<p>In technical terms, the month of the year is a confounder of our relationship between temperature and sales. <a contenteditable="false" data-primary="confounders" data-secondary="definition" data-type="indexterm" id="idm45968170942488"/><a contenteditable="false" data-primary="regression" data-secondary="confounders introducing bias" data-type="indexterm" id="idm45968170941144"/><a contenteditable="false" data-primary="variables" data-secondary="confounder definition" data-type="indexterm" id="idm45968170939800"/>A <em>confounder</em> is a variable that introduces bias in a regression; when a confounder is present in the situation you’re analyzing, it means that interpreting the regression coefficient as causal will lead to improper conclusions.</p>

<p>Let’s think of a place like Chicago, which has a continental climate: winter is very cold and summer is very hot. When comparing sales on a random hot day with sales on a random cold day without accounting for their respective month of the year, you’re very likely to be comparing sales on a hot day of summer, when kids are out of school, with sales on a cold day of winter, when kids are in school; this inflates the apparent relationship between temperature and sales.</p>

<p>In this example, we might also expect to see a consistent underprediction of sales in colder weather. In truth, there is a paradigm shift in summer months, and when that shift has to be managed exclusively through temperature in a linear regression, the regression coefficient for temperature will invariably be too high for warmer temperatures and too low for colder temperatures.</p>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Too Many Variables Can Spoil the Broth"><div class="sect2" id="too_many_variables_can_spoil_the_broth">
<h2>Too Many Variables Can Spoil the Broth</h2>

<p>A potential solution to the problem of confounders would be to add to the regression all the variables we can.<a contenteditable="false" data-primary="variables" data-secondary="variable selection examples" data-tertiary="adding variables" data-type="indexterm" id="idm45968170933352"/><a contenteditable="false" data-primary="data" data-secondary="variable selection examples" data-tertiary="adding variables" data-type="indexterm" id="idm45968170931736"/><a contenteditable="false" data-primary="C-Mart fictional supermarket chain" data-secondary="variable selection" data-tertiary="adding variables" data-type="indexterm" id="idm45968170930120"/><a contenteditable="false" data-primary="confounders" data-secondary="adding variables" data-type="indexterm" id="idm45968170928504"/> This mindset of “everything and the kitchen sink” still has proponents among statisticians. <a contenteditable="false" data-primary="The Book of Why (Pearl and Mackenzie)" data-primary-sortas="Book of Why" data-type="indexterm" id="idm45968170926888"/><a contenteditable="false" data-primary="Pearl, Judea" data-type="indexterm" id="idm45968170925496"/><a contenteditable="false" data-primary="Mackenzie, Dana" data-type="indexterm" id="idm45968170924392"/><a contenteditable="false" data-primary="Rubin, Donald" data-type="indexterm" id="idm45968170923288"/>In <em>The Book of Why</em>, Judea Pearl and Dana Mackenzie mention that “a leading statistician even recently wrote, ‘to avoid conditioning on some observed covariates…is nonscientific ad hockery’” (Pearl &amp; Mackenzie 2018, p. 160).<sup><a data-type="noteref" id="ch01fn2-marker" href="ch01.xhtml#ch01fn2">2</a></sup> It is also quite common among data scientists. To be fair, if your goal is only to predict a variable, you have a model that is carefully designed to generalize adequately beyond your testing data, and you don’t care about why the predicted variable is taking a certain value, then that’s a perfectly valid stance. But this does not work if your goal is to understand causal relationships in order to act upon them. Because of this, just adding as many variables as you can to your model not only is inefficient but can be downright counterproductive and misleading.</p>

<p>Let’s demonstrate this with our example by adding a variable that we might be inclined to include but will bias our regression. I created the variable <em>IcedCoffeeSales</em> to be correlated with <em>Temperature</em> but not with <em>SummerMonth</em>. Let’s look at what happens to our regression if we add this variable in addition to <em>Temperature</em> and <em>SummerMonth</em> (a binary 1/0 variable that indicates if the month was July or August (1) or any other month (0)):</p>

<pre data-code-language="r" data-type="programlisting">
<code class="c1">## R (output not shown)</code>
<code class="o">&gt;</code> <code class="nf">summary</code><code class="p">(</code><code class="nf">lm</code><code class="p">(</code><code class="n">icecream_sales</code> <code class="o">~</code> <code class="n">iced_coffee_sales</code> <code class="o">+</code> <code class="n">temps</code> <code class="o">+</code> <code class="n">summer_months</code><code class="p">))</code>
</pre>

<pre data-code-language="python" data-type="programlisting">
<code class="c1">## Python </code>
<code class="k">print</code><code class="p">(</code><code class="n">ols</code><code class="p">(</code><code class="s2">"icecream_sales ~ temps + summer_months + iced_coffee_sales"</code><code class="p">,</code> 
             <code class="n">data</code><code class="o">=</code><code class="n">stand_data_df</code><code class="p">)</code><code class="o">.</code><code class="n">fit</code><code class="p">()</code><code class="o">.</code><code class="n">summary</code><code class="p">())</code>
<code class="o">...</code>
                    <code class="n">coef</code>    <code class="n">std</code> <code class="n">err</code>     <code class="n">t</code>      <code class="n">P</code><code class="o">&gt;|</code><code class="n">t</code><code class="o">|</code>    <code class="p">[</code><code class="mf">0.025</code>     <code class="mf">0.975</code><code class="p">]</code>
<code class="o">----------------------------------------------------------------------------</code>
<code class="n">Intercept</code>         <code class="mf">24.5560</code>   <code class="mf">308.872</code>   <code class="mf">0.080</code>    <code class="mf">0.937</code>   <code class="o">-</code><code class="mf">581.127</code>    <code class="mf">630.239</code>
<code class="n">temps</code>          <code class="o">-</code><code class="mf">1651.3728</code>  <code class="mf">1994.826</code>  <code class="o">-</code><code class="mf">0.828</code>    <code class="mf">0.408</code>  <code class="o">-</code><code class="mf">5563.136</code>   <code class="mf">2260.391</code>
<code class="n">summer_months</code>   <code class="mf">1.976e+04</code>   <code class="mf">351.717</code>  <code class="mf">56.179</code>    <code class="mf">0.000</code>   <code class="mf">1.91e+04</code>   <code class="mf">2.04e+04</code>
<code class="n">iced_coffee_sales</code>  <code class="mf">2.6500</code>     <code class="mf">1.995</code>   <code class="mf">1.328</code>    <code class="mf">0.184</code>     <code class="o">-</code><code class="mf">1.262</code>      <code class="mf">6.562</code>
<code class="o">...</code></pre>

<p>We see that the coefficient for <em>Temperature</em> has shifted dramatically from our prior example to the point that it is now negative. The high p-values for <em>Temperature</em> and <em>IcedCoffeeSales</em> would usually be taken as signs that something is amiss, but since the p-value for <em>Temperature</em> is “worse,” an analyst may conclude that they should remove it from the regression. How is this possible?</p>

<p>The truth behind the data (which is known, since I manufactured the relationships and randomized data around those relationships) is that when it is hot out people are more likely to buy iced coffee. On hot days, people are also more likely to buy more ice cream. But a purchase of iced coffee itself does not make customers any more or less likely to buy ice cream. Summer months are also not correlated with iced coffee purchases since schoolchildren are not a significant factor in the demand for iced coffee (see the sidebar for the details of the math at hand).</p>

<aside class="pagebreak-before less_space" data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="technical_deeper_dive_what_happened_her">
<h5>Technical Deeper Dive: What Happened Here?</h5>

<p>The equation for ice cream sales that I used to generate the simulated data is:<a contenteditable="false" data-primary="confounders" data-secondary="adding variables" data-tertiary="multicollinearity" data-type="indexterm" id="idm45968168200984"/><a contenteditable="false" data-primary="variables" data-secondary="variable selection examples" data-tertiary="multicollinearity" data-type="indexterm" id="idm45968168199336"/><a contenteditable="false" data-primary="data" data-secondary="variable selection examples" data-tertiary="multicollinearity" data-type="indexterm" id="idm45968168197672"/><a contenteditable="false" data-primary="multicollinearity in adding variables" data-type="indexterm" id="idm45968168196008"/></p>

<div data-type="equation">
<p><em>IceCreamSales</em>:= 1,000.<em>Temperature</em> + 20,000.<em>SummerMonth</em> + <em>ε</em><sub>1</sub></p>
</div>

<p>where <em>ε</em><sub>1</sub> represents some random noise with a mean of zero and the “:=” sign indicates that this equation represents how the variable on the left, <em>IceCreamSales</em>, is defined or constructed.</p>

<p>However, the equation we’re estimating in our linear regression is:</p>

<div data-type="equation">
<p><em>IceCreamSales = β<sub><em>T</em></sub>.Temperature + β<sub><em>s</em></sub>.SummerMonth + β<sub><em>c</em></sub>.IcedCoffeeSales</em></p>
</div>

<p>The true equation that was used to generate iced coffee sales is:</p>

<div data-type="equation">
<p><em>IcedCoffeeSales:</em>= 1,000.<em>Temperature</em> + <em>ε</em><sub>2</sub></p>
</div>

<p>Which means we can rewrite the previous equation as:</p>

<div data-type="equation">
<p><em>IceCreamSales</em> = <em>β</em><sub><em>T</em></sub>.<em>Temperature</em> + <em>β</em><sub><em>s</em></sub>.<em>SummerMonth</em> + <em>β</em><sub><em>c</em></sub>.(1,000.Temperature + <em>ε</em><sub>2</sub>) <em>= (<em>β</em><sub><em>T</em></sub> + 1,000 <em>β</em><sub><em>C</em></sub>).Temperature + <em>β</em><sub><em>s</em></sub>.SummerMonth</em></p>
</div>

<p>Barring some random fluke, our coefficient <em>β</em><sub><em>s</em></sub> should be close to the true value 20,000. But for temperature, our software will try to solve this equation:</p>

<div data-type="equation">
<p><em>β</em><sub><em>T</em></sub> + 1,000.<em>β</em><sub><em>c</em></sub> = 1,000</p>
</div>

<p>This is a single equation with two unknowns, so it has an infinite number of solutions. <em>β</em><sub><em>T</em></sub> = 0 and <em>β</em><sub><em>C</em></sub> = 1 would work, but so would <em>β</em><sub><em>T</em></sub> = 500 and <em>β</em><sub><em>C</em></sub> = 0.5, or <em>β</em><sub><em>T</em></sub> = 5,000 and <em>β</em><sub><em>C</em></sub> = -4. The least-square algorithm will pick the combination that provides the highest <em>R</em><sup>2</sup> value, but it will not be reliable (although the unreliability will generally be much smaller in practice than in this simulated example). In technical terms, we have introduced multicollinearity.</p>
</div></aside>

<p><a data-type="xref" href="#plot_of_iced_coffee_sales_versus_ice_cr">Figure 1-4</a> shows a positive correlation between iced coffee sales and ice cream sales since both increase when it is warmer out, but any increase in sales of iced coffee during summer months can be explained by a shared correlation with the temperature variable. When the regression algorithm tries to explain ice cream sales using the three variables at hand, the explanatory power of temperature on iced coffee sales was added to the temperature variable while iced coffee was forced to compensate for the overpowering of temperature. Even though iced coffee sales are not statistically significant and the coefficient is relatively small, the dollars of sales are much higher than degrees of temperature, so ultimately iced coffee sales cancel the inflation of the coefficient for temperature.</p>

<figure><div id="plot_of_iced_coffee_sales_versus_ice_cr" class="figure"><img alt="Plot of iced coffee sales versus ice cream sales" src="Images/BEDA_0104.png" width="1906" height="1158"/>
<h6><span class="label">Figure 1-4. </span>Plot of iced coffee sales versus ice cream sales</h6>
</div></figure>

<p>In the previous example, adding the variable <em>IcedCoffeeSales</em> to the regression muddled the relationship between temperature and ice cream sales. <a contenteditable="false" data-primary="variables" data-secondary="variable selection examples" data-tertiary="adding wrong variables" data-type="indexterm" id="idm45968168159080"/><a contenteditable="false" data-primary="data" data-secondary="variable selection examples" data-tertiary="adding wrong variables" data-type="indexterm" id="idm45968168157416"/><a contenteditable="false" data-primary="C-Mart fictional supermarket chain" data-secondary="variable selection" data-tertiary="adding wrong variables" data-type="indexterm" id="idm45968168155752"/>Unfortunately, the reverse can also be true: including the wrong variable in a regression can create the illusion of a relationship when there is none.</p>

<p>Sticking with our ice cream example at C-Mart, let’s say that the category manager is interested in understanding customer tastes, so they ask an employee to stand outside the store and survey people walking by, asking them how much they like vanilla ice cream and how much they like chocolate ice cream (both on a scale from 0 to 25), as well as whether they have ever purchased ice cream from the stand. To keep things simple, we’ll assume that the stand only sells chocolate and vanilla ice cream.</p>

<p>Let’s assume for the sake of the example that taste for vanilla ice cream and taste for chocolate ice cream are entirely uncorrelated. Some people like one but not the other, some like both equally, some like one <em>more than</em> the other, and so on. But all of these preferences impact whether someone buys from the stand, a binary (Yes/No) variable.</p>

<p class="pagebreak-before less_space">Because the variable <em>Shopped</em> is binary, we would use a logistic regression if we wanted to measure the impact of either of the <em>Taste</em> variables on shopping behavior. Since the two <em>Taste</em> variables are uncorrelated, we would see a regular cloud with no apparent correlation if we were to plot them against each other; however, they each impact the probability of shopping at the ice cream stand (<a data-type="xref" href="#left_panel_tastes_for_vanilla_and_choco">Figure 1-5</a>).</p>

<figure><div id="left_panel_tastes_for_vanilla_and_choco" class="figure"><img alt="Left panel: tastes for vanilla and chocolate are uncorrelated in the overall population; middle panel: taste for vanilla is higher for people who shop at the ice cream stand than for people who don’t; right panel: same result for taste for chocolate" src="Images/BEDA_0105.png" width="1888" height="620"/>
<h6><span class="label">Figure 1-5. </span>Left panel: tastes for vanilla and chocolate are uncorrelated in the overall population; middle panel: taste for vanilla is higher for people who shop at the ice cream stand than for people who don’t; right panel: same result for taste for chocolate</h6>
</div></figure>

<p>In the first graph, I added a line of best fit, which is almost perfectly <a contenteditable="false" data-primary="linear regression" data-secondary="line of best fit" data-tertiary="flat lacking correlation" data-type="indexterm" id="idm45968168145464"/><a contenteditable="false" data-primary="correlation" data-secondary="line of best fit, flat" data-type="indexterm" id="idm45968168143800"/><a contenteditable="false" data-primary="regression" data-secondary="line of best fit" data-tertiary="flat lacking correction" data-type="indexterm" id="idm45968168142424"/>flat, reflecting the lack of correlation between the variables (the correlation coefficient is equal to 0.004, reflecting sampling error). On the second and third graphs, we can see that taste for vanilla and chocolate is higher on average for customers (<em>Shopped</em> = 1) than for noncustomers, which makes sense.</p>

<p>So far, so good. Let’s say that once you get the survey data, your business partner tells you that they are considering introducing a coupon incentive for the ice cream stand: when you purchase ice cream, you get a coupon for future visits. This loyalty incentive won’t impact the respondents who have never shopped at the stand, so the relevant population are those who have shopped at the store. The business partner is considering using flavor restrictions on the coupons to balance stock but does not know how much flavor choices can be influenced. If someone who purchased vanilla ice cream were given a coupon for 50% off chocolate ice cream, would it do anything beyond adding more paper to the recycle bin? How favorably do the people who like vanilla ice cream view chocolate ice cream anyway?</p>

<p>You plot the same graph again, this time restricting the data to people who have answered “Yes” to the shopping question (<a data-type="xref" href="#taste_for_vanilla_and_chocolate_among_s">Figure 1-6</a>).</p>

<figure><div id="taste_for_vanilla_and_chocolate_among_s" class="figure"><img alt="Taste for vanilla and chocolate among shoppers" src="Images/BEDA_0106.png" width="639" height="521"/>
<h6><span class="label">Figure 1-6. </span>Taste for vanilla and chocolate among shoppers</h6>
</div></figure>

<p>There is now a strong negative correlation between the two variables (the correlation coefficient is equal to −0.39). What happened? Do vanilla lovers who come to your stand turn into chocolate haters and vice versa? Of course not. This correlation was artificially created when you restrained yourself to customers.</p>

<p>Let’s get back to our true causal relationships: the stronger someone’s taste for vanilla, the more likely they are to shop at your stand, and similarly for chocolate. This means that there is a cumulative effect of these two variables. If someone has a weak taste for both vanilla and chocolate ice creams, they are very unlikely to shop at your stand; in other words, most of the people with a weak taste for vanilla among your customers have a strong taste for chocolate. On the other hand, if someone has a strong taste for vanilla, they might shop at your stand even if they don’t have a strong taste for chocolate. You can see it reflected in the earlier graph: for high values for vanilla (say above 15), there are data points with lower values for chocolate (below 15), whereas, for low values of vanilla (below 5), the only data points in the graph have a high value for chocolate (above 17). No one’s preferences have changed, but people with a weak taste for both vanilla and chocolate are excluded from your data set.</p>

<p>The technical term for this phenomenon is <a contenteditable="false" data-primary="Berkson paradox" data-type="indexterm" id="idm45968168133480"/><a contenteditable="false" data-primary="explain-away effect" data-type="indexterm" id="idm45968168112472"/><a contenteditable="false" data-primary="The Book of Why (Pearl and Mackenzie)" data-primary-sortas="Book of Why" data-type="indexterm" id="idm45968168111368"/><a contenteditable="false" data-primary="Pearl, Judea" data-type="indexterm" id="idm45968168109976"/><a contenteditable="false" data-primary="Mackenzie, Dana" data-type="indexterm" id="idm45968168108872"/>the <a href="https://oreil.ly/KwJ1R">Berkson paradox</a>, but Judea Pearl and Dana Mackenzie call it by a more intuitive name: the “explain-away effect.” If one of your customers has a strong taste for vanilla, this completely explains why they are shopping at your stand, and they don’t “need” to have a strong taste for chocolate. On the other hand, if one of your customers has a weak taste for vanilla, this can’t explain why they are shopping at your stand, and they must have a stronger than average taste for chocolate.</p>

<p>The Berkson paradox is counterintuitive and hard to understand at first. <a contenteditable="false" data-primary="data" data-secondary="biases" data-see="biases in data" data-type="indexterm" id="idm45968168106008"/><a contenteditable="false" data-primary="biases in data and analyses" data-secondary="Berkson paradox causing" data-type="indexterm" id="idm45968168104264"/>It can cause biases in your data, depending on how it was collected, even before you start any analysis. A classic example of how this situation can create artificial correlations is that some diseases show a higher degree of correlation when looking at the population of hospital patients compared to the general population. In reality of course, what happens is that either disease is not enough for someone to go to a hospital; someone’s health status gets bad enough to justify hospitalization only when they are both present.<sup><a data-type="noteref" id="ch01fn3-marker" href="ch01.xhtml#ch01fn3">3</a></sup></p>
</div></section>
</div></section>

<section data-type="sect1" data-pdf-bookmark="Conclusion"><div class="sect1" id="conclusion-id00006">
<h1>Conclusion</h1>

<p>Predictive analytics has been extremely successful over the past few decades and will remain so. On the other hand, when trying to understand and—more importantly—change human behavior, causal analytics offers a compelling alternative.</p>

<p>Causal analytics, however, demands a different approach than what we are used to with predictive analytics. Hopefully, the examples in this chapter have convinced you that you can’t just throw a bunch of variables in a linear or logistic regression and hope for the best (which we might think of as the “include them all, God will recognize His own” approach). You may still wonder, though, about other types of models and algorithms. Are gradient boosting or deep learning models somehow immune to confounders, multicollinearity, and spurious correlations? Unfortunately, the answer is no. If anything, the “black box” nature of these models means that confounders can be harder to catch.</p>

<p>In the next chapter, we will explore how to think about the behavioral data itself.</p>
</div></section>
<div data-type="footnotes"><p data-type="footnote" id="ch01fn1"><sup><a href="ch01.xhtml#ch01fn1-marker">1</a></sup> To be fair, in many circumstances, they <em>should</em> be different, because the data is used for different purposes and obeys different conventions. But even questions that you would expect to have a single true answer (e.g., “How many employees do we have right now?”) will generally show discrepancies.</p><p data-type="footnote" id="ch01fn2"><sup><a href="ch01.xhtml#ch01fn2-marker">2</a></sup> In case you’re wondering, the aforementioned statistician is Donald Rubin.</p><p data-type="footnote" id="ch01fn3"><sup><a href="ch01.xhtml#ch01fn3-marker">3</a></sup> Technically speaking, this is a slightly different situation, because there is a threshold effect instead of two linear (or logistic) relationships, but the underlying principle that including the wrong variable can create artificial correlations still applies.</p></div></div></section></div></body></html>