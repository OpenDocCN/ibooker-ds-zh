- en: Chapter 8\. Introduction to pandas
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: pandas is a key element in our dataviz toolchain, as we will use it for both
    cleaning and exploring our recently scraped dataset (see [Chapter 6](ch06.xhtml#chapter_heavy_scraping)).
    The last chapter introduced NumPy, the Python array processing library that is
    the foundation of pandas. Before we move on to applying pandas, this chapter will
    introduce its key concepts and show how it interacts with existing data files
    and database tables. The rest of your pandas learning will be on the job over
    the next couple of chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Why pandas Is Tailor-Made for Dataviz
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Take any dataviz, whether web-based or in print, and chances are that the data
    visualized was at one point stored in row-columnar form in a spreadsheet like
    Excel, a CSV file, or HDF5\. There are certainly visualizations, like network
    graphs, for which row-columnar data is not the best form, but they are in the
    minority. pandas is tailor-made to manipulate row-columnar data tables with its
    core datatype, the DataFrame, which is best thought of as a very fast, programmatic
    spreadsheet.
  prefs: []
  type: TYPE_NORMAL
- en: Why pandas Was Developed
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First revealed by Wes Kinney in 2008, pandas was built to solve a particular
    problem—​namely, that while Python was great for manipulating data, it was weak
    in the area of data analysis and modeling, certainly compared with big hitters
    like R.
  prefs: []
  type: TYPE_NORMAL
- en: pandas is designed to work with the kind of heterogenous^([1](ch08.xhtml#idm45607780439264))
    data found in row-columnar spreadsheets, but cleverly manages to leverage some
    of the speed of NumPy’s homogeneous numeric arrays used by mathematicians, physicists,
    computer graphics, and the like. Combined with the Jupyter notebook and the Matplotlib
    plotting library (with auxiliary libraries like seaborn), pandas represents a
    first-class interactive data analysis tool. Because it’s part of the NumPy ecosystem,
    its data modeling is easily enhanced by such libraries as SciPy, statsmodels,
    and scikit-learn, to name but a few.
  prefs: []
  type: TYPE_NORMAL
- en: Categorizing Data and Measurements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I’ll cover the core concepts of pandas in the next section, focusing on the
    DataFrame and how to get your data into and out of it via the common datastores,
    CSV files, and SQL databases. But first let’s take a little diversion to consider
    what we really mean by the heterogeneous datasets that pandas was designed to
    work with and that are the mainstay of data visualizers.
  prefs: []
  type: TYPE_NORMAL
- en: Chances are that a visualization, maybe a bar chart or line graph used to illustrate
    an article or a modern web dashboard, presents the results of measurements in
    the real world, the price of commodities as they change over time, changes in
    rainfall over a year, voting intentions by ethnic group, and so forth. These measurements
    can be broadly broken into two groups, numerical and categorical. Numerical values
    can be divided into interval and ratio scales, and categorical values can in turn
    be divided into nominal and ordinal measurements. This gives four broad categories
    of observation available to the data visualizer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take a set of tweets as an example in order to draw out these measurement
    categories. Each tweet has various data fields:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_introduction_to_pandas_CO1-1)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `text` and `id` fields are unique indicators. The former might contain
    categorical information (e.g., the category of tweets containing the #Python hashtag),
    and the latter might be used to create a category (e.g., the set of all users
    retweeting this tweet), but they are not per se visualizable fields.'
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_introduction_to_pandas_CO1-3)'
  prefs: []
  type: TYPE_NORMAL
- en: '`favorited` is Boolean, categorical information, dividing the tweets into two
    sets. This would count as a *nominal* category, as it can be counted but not ordered.'
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_introduction_to_pandas_CO1-4)'
  prefs: []
  type: TYPE_NORMAL
- en: '`filter_level` is also categorical information, but it is ordinal. There is
    an order, low→medium→high, to the filter levels.'
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_introduction_to_pandas_CO1-5)'
  prefs: []
  type: TYPE_NORMAL
- en: The `created_at` field is a timestamp, a numerical value on an interval scale.
    We would probably want to order the tweets on this scale, something pandas does
    automatically, and then maybe box into broader intervals, say by the day or week.
    Again, pandas makes this trivial.
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](assets/5.png)](#co_introduction_to_pandas_CO1-6)'
  prefs: []
  type: TYPE_NORMAL
- en: '`retweet_count` is likewise on a numerical scale, but it is a ratio one. A
    ratio scale, as opposed to an interval scale, has a meaningful concept of zero—​in
    this case, no retweets. Our `created_at` timestamp, on the other hand, can have
    an arbitrary baseline (e.g., unixtime or Gregorian year 0), much in the same way
    as temperature scales, with 0 degrees Celsius being the same as 273.15 degrees
    Kelvin.'
  prefs: []
  type: TYPE_NORMAL
- en: '[![6](assets/6.png)](#co_introduction_to_pandas_CO1-7)'
  prefs: []
  type: TYPE_NORMAL
- en: '`coordinates`, if available, has two numerical scales for longitude and latitude.
    Both are interval scales, though, as it doesn’t make much sense to speak of ratios
    of degrees.'
  prefs: []
  type: TYPE_NORMAL
- en: So a small subset of our humble tweet’s fields contains heterogeneous information
    covering all the generally accepted divisions of measurement. Whereas the NumPy
    array is generally used for homogeneous, numerical number crunching, pandas is
    designed to deal with categorical data, time series, and items that reflect the
    heterogeneous nature of real-world data. This makes it a great fit for the data
    visualization.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know the type of data pandas is designed to deal with, let’s look
    at the data structures it uses.
  prefs: []
  type: TYPE_NORMAL
- en: The DataFrame
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The first step in a pandas session is usually to load some data into a DataFrame.
    We’ll cover the various ways we can do this in a later section. For now, let’s
    read our *nobel_winners.json* JSON data from a file. `read_json` returns a DataFrame,
    parsed from the JSON file specified. By convention, DataFrame variables start
    with `df`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: With our DataFrame in hand, let’s inspect its content. A quick way to get the
    row-columnar structure of the DataFrame is to use its `head` method to show (by
    default) the top five items. [Figure 8-1](#pandas_dataframe) shows the output
    from a [Jupyter notebook](https://jupyter.org), with key elements of the DataFrame
    highlighted.
  prefs: []
  type: TYPE_NORMAL
- en: '![dpj2 0801](assets/dpj2_0801.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-1\. The key elements of a pandas DataFrame
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Indices
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The DataFrame’s columns are indexed by a `columns` property, which is a pandas
    `index` instance. Let’s select the columns in [Figure 8-1](#pandas_dataframe):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Initially, pandas rows have a single numeric index (pandas can handle multiple
    indices if necessary) that can be accessed by the `index` property. This is a
    memory-saving [`RangeIndex`](https://oreil.ly/7Qzia) by default:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'As well as integers, row indices can be strings, `DatetimeIndice`s, or `PeriodIndice`s
    for time-based data, and so on. Often, to aid selections, a column of the DataFrame
    will be set to the index via the `set_index` method. In the following code, we
    first use the `set_index` method to set our Nobel DataFrame’s index to the name
    column and then use the `loc` method to select a row by the index label (`name`
    in this case):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_introduction_to_pandas_CO2-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Set the index to the name column.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_introduction_to_pandas_CO2-2)'
  prefs: []
  type: TYPE_NORMAL
- en: You can now select a row by the `name` label.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_introduction_to_pandas_CO2-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Return the index to original integer-based state.
  prefs: []
  type: TYPE_NORMAL
- en: Rows and Columns
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The rows and columns of a DataFrame are stored as [pandas Series](https://oreil.ly/z7PF4),
    a heterogeneous counterpart to NumPy’s array. These are essentially a labeled
    one-dimensional array that can contain any datatype from integers, strings, and
    floats to Python objects and lists.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two ways to select a row from the DataFrame. We’ve seen the `loc`
    method, which selects by label. There’s also an `iloc` method, which selects by
    position. So to select the row in [Figure 8-1](#pandas_dataframe), we grab row
    number two:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'You can grab a column of your DataFrame using dot notation^([2](ch08.xhtml#idm45607779978240))
    or conventional array access by keyword string. This returns a pandas Series with
    all the column fields with their DataFrame indices preserved:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Selecting Groups
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are various ways we can select groups (or subsets of rows) of our DataFrame.
    Often we want to select all rows with a specific column value (e.g., all rows
    with category Physics). One way to do this is to use the DataFrame’s `groupby`
    method to group a column (or list of columns) and then use the `get_group` method
    to select the required group. Let’s use these two methods to select all Nobel
    Physics Prize winners:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Another way to select row subsets is to use a Boolean mask to create a new
    DataFrame. You can apply Boolean operators to all rows in a DataFrame in much
    the same way as you can to all members of a NumPy array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting Boolean mask can then be applied to the original DataFrame to
    select a subset of its rows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: We’ll cover a lot more examples of data selections in the coming chapters. For
    now, let’s see how we create DataFrames from existing data and how to save the
    results of our data frame manipulations.
  prefs: []
  type: TYPE_NORMAL
- en: Creating and Saving DataFrames
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The easiest way to create a DataFrame is to use a Python dictionary. It’s also
    a way you won’t be using very often, as you will likely be accessing your data
    from files or databases. Nevertheless, it has its use cases.
  prefs: []
  type: TYPE_NORMAL
- en: 'By default, we specify the columns separately, in the following example creating
    three rows with name and category columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We can use the `from_dict` method to allow us to use our preferred record-based
    object arrays. `from_dict` has an `orient` argument to allow us to specify record-like
    data, but pandas is smart enough to work out the data form:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_introduction_to_pandas_CO3-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Here we pass in an array of objects, each corresponding to a row in our DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: 'The methods just shown produce an identical DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: As mentioned, you probably won’t be creating DataFrames from Python containers
    directly. Instead, you will probably use one of the pandas data-reading methods.
  prefs: []
  type: TYPE_NORMAL
- en: pandas has an impressive array of `read_[format]`/`to_[format]` methods, covering
    most conceivable data-loading use cases, from CSV through binary HDF5 to SQL databases.
    We’ll cover the subset most relevant to dataviz work. For a full list, see [the
    pandas documentation](https://oreil.ly/b3VFR).
  prefs: []
  type: TYPE_NORMAL
- en: Note that by default pandas will try to convert the loaded data sensibly. The
    `convert_axes` (try to convert the axes to the proper `dtype`s), `dtype` (guess
    datatype), and `convert_dates` arguments to the read methods are all `True` by
    default. See the [pandas documentation](https://oreil.ly/MkmIx) for an example
    of the available options, in this case for reading JSON files into a DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s cover file-based DataFrames first, then see how to interact with (No)SQL
    databases.
  prefs: []
  type: TYPE_NORMAL
- en: JSON
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Loading data from our preferred JSON format is trivial in pandas:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'There are various forms the JSON file can take, specified by an optional `orient`
    argument, one of [`split`, `records`, `index`, `columns`, `values`]. An array
    of records, our standard form, will be detected:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The default for a JSON object is `columns`, in the form:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: As discussed, for web-based visualization work, particularly D3, record-based
    JSON arrays are the most common way of passing row-columnar data to the browser.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note that you will need valid JSON files to work with pandas because the `read_json`
    method and Python JSON parsers in general tend to be fairly unforgiving, and exceptions
    not as informative as they might be.^([3](ch08.xhtml#idm45607779218800)) A common
    JSON error is failing to enclose keys in double-quote marks or using single quotes
    where double quotes are expected. The latter is particularly common for those
    coming from languages where single- and double-string quotes are essentially interchangeable
    and one reason why you should never build JSON documents yourself—​always use
    an official or well-respected library.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are various ways to store DataFrames in JSON, but the format that will
    play most nicely with any dataviz work is the array of records. This is the most
    common form of D3 data and the one I recommend outputting from pandas.^([4](ch08.xhtml#idm45607779160128))
    Writing a DataFrame as records to JSON is then simply a case of specifying the
    `orient` field in the `to_json` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_introduction_to_pandas_CO4-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Override the default save to store the JSON as dataviz-friendly records.
  prefs: []
  type: TYPE_NORMAL
- en: We also have the parameters `date_format` (*epoch* timestamp, *iso* for ISO8601,
    etc.), `double_precision`, and `default_handler` to call if the object cannot
    be converted into JSON using pandas’s parser. Check [the pandas documentation](https://oreil.ly/wqnI0)
    for more details.
  prefs: []
  type: TYPE_NORMAL
- en: CSV
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As befits pandas’s data-table ethos, its handling of CSV files is sophisticated
    enough to cope with pretty much all conceivable data. Conventional CSV files,
    which is the large majority, will load without parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Although you might expect all CSV files to be comma-separated, you will often
    find files with a CSV suffix with different delimiters such as semicolons or pipes
    (`|`). They may also use idiosyncratic quoting for strings containing spaces or
    special characters. In this case, we can specify any nonstandard elements in our
    read request. We’ll use Python’s handy `StringIO` module to emulate reading from
    a file:^([5](ch08.xhtml#idm45607779004224))
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_introduction_to_pandas_CO5-1)'
  prefs: []
  type: TYPE_NORMAL
- en: The fields are pipe-separated, not the default comma-separated.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_introduction_to_pandas_CO5-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Here we provide the missing column headers.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have the same degree of flexibility when saving CSV files, here setting
    the encoding to Unicode `utf-8`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: For full coverage of the CSV options, see the [pandas documentation](https://oreil.ly/QPCs1).
  prefs: []
  type: TYPE_NORMAL
- en: Excel Files
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'pandas uses Python’s `xlrd` module to read Excel 2003 (*.xls*) and the `openpyxl`
    module to read Excel 2007+ (*.xlsx*) files. The latter is an optional dependency
    that will need installing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Excel documents have multiple named sheets, each of which can be passed to
    a DataFrame. There are two ways to read a datasheet into a DataFrame. The first
    is by creating and then parsing an `ExcelFile` object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_introduction_to_pandas_CO6-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Grab a sheet by name and save to a dictionary.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_introduction_to_pandas_CO6-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Specify the column, by position, to use as DataFrame’s row labels.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_introduction_to_pandas_CO6-3)'
  prefs: []
  type: TYPE_NORMAL
- en: A list of additional strings to recognize as NaN.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_introduction_to_pandas_CO6-4)'
  prefs: []
  type: TYPE_NORMAL
- en: The number of rows (e.g., metadata) to skip before processing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Alternatively you can use the `read_excel` method, which is a convenience method
    for loading multiple spreadsheets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s check the content of the second Excel sheet using the resulting DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: The only reason not to use `read_excel` is if you need different arguments for
    reading each Excel sheet.
  prefs: []
  type: TYPE_NORMAL
- en: You can specify sheets by index or name using the second (`sheetname`) parameter.
    `sheetname` can be a single name string or index (beginning at 0) or a mixed list.
    By default `sheetname` is `0`, returning the first sheet. [Example 8-1](#excel_loads)
    shows some variations. Setting `sheetname` to `None` returns a sheetname-keyed
    dictionary of DataFrames.
  prefs: []
  type: TYPE_NORMAL
- en: Example 8-1\. Loading Excel sheets
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The `parse_cols` parameter lets you select the sheet columns to be parsed.
    Setting `parse_cols` to an integer value selects all columns up to that ordinal.
    Setting `parse_cols` to a list of integers allows you to select specific columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: For more information on `read_excel`, see the [pandas documentation](https://oreil.ly/Js7Le).
  prefs: []
  type: TYPE_NORMAL
- en: 'You can save a DataFrame to the sheet of an Excel file with the `to_excel`
    method, giving the Excel filename and a sheetname, `''nobel_winners''` and `''WinnersSheet1''`,
    respectively, in this example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: There are various options similar to `to_csv` covered in the [pandas docs](https://oreil.ly/g15Al).
    Because pandas `Panel`s and Excel files can store multiple DataFrames, there is
    a `Panel` `to_excel` method to write all its DataFrames to an Excel file.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you need to select multiple DataFrames to write to a shared Excel file,
    you can use an `ExcelWriter` object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: SQL
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: By preference pandas uses Python’s `SQLAlchemy` module to do the database abstraction.
    If using `SQLAlchemy`, you’ll also need the driver library for your database.
  prefs: []
  type: TYPE_NORMAL
- en: 'The easiest way to load a database table or the results of an SQL query is
    with the `read_sql` method. Let’s use our preferred SQLite database and read its
    winners table into a DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_introduction_to_pandas_CO7-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Here, we use an existing SQLite (file-based) database. SQLAlchemy can create
    engines for all the commonly used databases, for example *mysql://USER:PASSWORD@localhost/db*.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_introduction_to_pandas_CO7-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Read the contents of the `'nobel_winners'` SQL table into a DataFrame. `read_sql`
    is a convenience wrapper around the `read_sql_table` and `read_sql_query` methods
    and will do the right thing depending on its first argument.
  prefs: []
  type: TYPE_NORMAL
- en: 'Writing DataFrames to an SQL database is simple enough. Using the engine we
    just created, we can add a copy of the winners table to our SQLite database:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'If you encounter errors due to packet-size limitations, the `chunksize` parameter
    can set the number of rows to be written at a time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'pandas will do the sensible thing and try to map your data to a suitable SQL
    type, inferring the datatype of objects. If necessary, the default type can be
    overridden in the load call:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_introduction_to_pandas_CO8-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Override pandas’s inference, and specify year as a `String` column.
  prefs: []
  type: TYPE_NORMAL
- en: Further details of pandas-SQL interaction can be found [in the pandas documentation](https://oreil.ly/kiLyQ).
  prefs: []
  type: TYPE_NORMAL
- en: MongoDB
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For dataviz work, there’s a lot to be said for the convenience of document-based
    NoSQL databases like MongoDB. In MongoDB’s case, things are even better, as it
    uses a binary form of JSON for its datastore—​namely BSON, short for binary JSON.
    Since JSON is our data glue of choice, as it connects our web dataviz with its
    backend server, there’s a good reason to consider storing your datasets in Mongo.
    It also plays nicely with pandas.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we’ve seen, pandas DataFrames convert nicely to and from JSON format, so
    getting a Mongo document collection into a pandas DataFrame is a pretty easy affair:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_introduction_to_pandas_CO9-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Create a Mongo client, using the default host and ports.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_introduction_to_pandas_CO9-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Get the `nobel_prize` database.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_introduction_to_pandas_CO9-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Find all documents in the `winner` collection.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_introduction_to_pandas_CO9-4)'
  prefs: []
  type: TYPE_NORMAL
- en: Load all documents from the cursor into a list and use to create a DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](assets/5.png)](#co_introduction_to_pandas_CO9-5)'
  prefs: []
  type: TYPE_NORMAL
- en: The winners collection is empty at this point—let’s fill it with some DataFrame
    data.
  prefs: []
  type: TYPE_NORMAL
- en: 'It’s just as easy to insert a DataFrame’s records into a MongoDB database.
    Here, we use the `get_mongo_database` method we defined in [Example 3-5](ch03.xhtml#data_get_mongo)
    to get our `nobel_prize` database and save the DataFrame to its winners collection:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_introduction_to_pandas_CO10-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Converts the DataFrame to a `dict`, using the `records` argument to convert
    the rows into individual objects.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_introduction_to_pandas_CO10-2)'
  prefs: []
  type: TYPE_NORMAL
- en: For PyMongo version 2, use the `insert` method.
  prefs: []
  type: TYPE_NORMAL
- en: 'pandas doesn’t have MongoDB convenience methods comparable to `to_csv` or `read_csv`,
    but it’s easy enough to roll a couple of utility functions to convert from MongoDB
    to DataFrames and back again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_introduction_to_pandas_CO11-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Mongo’s `_id` field will be included in the DataFrame. By default, remove the
    column.
  prefs: []
  type: TYPE_NORMAL
- en: 'Having inserted the DataFrame’s records into Mongo, let’s make sure they have
    been successfully stored:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_introduction_to_pandas_CO12-1)'
  prefs: []
  type: TYPE_NORMAL
- en: The collection’s `find` method returns a cursor, which we convert to a Python
    list to see the contents.
  prefs: []
  type: TYPE_NORMAL
- en: Another way to create DataFrames is to build them from a collection of Series.
    Let’s have a look at that, taking the opportunity to explore Series in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: Series into DataFrames
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Series are the building block of pandas’s DataFrames. They can be manipulated
    independently with methods that mirror those of the DataFrame and they can be
    combined to form DataFrames, as we’ll see in the subsection.
  prefs: []
  type: TYPE_NORMAL
- en: The key idea with pandas Series is the index. These indices function as labels
    for the heterogeneous data contained in, say, a row of data. When pandas operates
    on more than one data object, these indices are used to align the fields.
  prefs: []
  type: TYPE_NORMAL
- en: 'Series can be created in one of three ways. The first is from a Python list
    or NumPy array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that integer indices are automatically created for our Series. If we were
    adding a row of data to a DataFrame (table), we would want to specify the column
    indices by passing them as a list of integers or labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Note that the length of the index array should match the length of the data
    array.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can specify both data and index using a Python `dict`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'If we pass an index array along with the `dict`, pandas will do the sensible
    thing, matching the indices to the data array. Any unmatched indices will be set
    to `NaN` (not a number), and any unmatched data discarded. Note one consequence
    of having fewer elements than indices is that the series is cast to a `float64`
    type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can pass a single, scalar value as data to the Series, provided
    we also specify an index. The scalar value is then applied to all indices:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Series are like NumPy arrays (`ndarray`), which means they can be passed to
    most NumPy functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Slicing operations work as they would with Python lists or `ndarray`s, but
    note that the index labels are preserved:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Unlike NumPy’s arrays, pandas’s series can take data of multiple types. Adding
    two series demonstrates this utility with numbers being added while strings are
    concatenated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: The ability to create and manipulate individual Series is particularly important
    when you are interacting with the NumPy ecosystem, manipulating data from a DataFrame,
    or creating visualizations outside of pandas’s Matplotlib wrapper.
  prefs: []
  type: TYPE_NORMAL
- en: 'As Series are the building block of DataFrames, it’s easy to join them together
    to create a DataFrame, using pandas’s `concat` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_introduction_to_pandas_CO13-1)'
  prefs: []
  type: TYPE_NORMAL
- en: We use the `names` and `categories` series to provide the data and column names
    (the series `name` property) for a DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_introduction_to_pandas_CO13-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Concatenate the two Series using the `axis` argument of `1` to indicate that
    the Series are columns.
  prefs: []
  type: TYPE_NORMAL
- en: Along with the many ways to create DataFrames from files and databases just
    discussed, you should now have a solid grounding in getting data into and out
    of DataFrames.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter laid a foundation for the two pandas-based chapters to come. The
    core concepts of pandas—​the DataFrame, Index, and Series—were discussed and we
    saw why pandas is such a good fit with the type of real-world data that data visualizers
    deal with, extending the NumPy `ndarray` by allowing the storage of heterogeneous
    data and adding a powerful indexing system.
  prefs: []
  type: TYPE_NORMAL
- en: With pandas’s core data structures under our belts, the next few chapters will
    show you how to use them to clean and process your dataset of Nobel Prize winners,
    extending your knowledge of the pandas toolkit and showing you how to go about
    applying it in a data visualization context.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know how to get data into and out of a DataFrame, it’s time to see
    what pandas can do with it. We’ll first see how to give your data a clean bill
    of health, discovering and fixing anomalies such as duplicate rows, missing fields,
    and corrupted data.
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch08.xhtml#idm45607780439264-marker)) The columns in a typical spreadsheet
    will typically have different datatypes (dtypes), like floats, date-times, integers
    etc.
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch08.xhtml#idm45607779978240-marker)) Only if the column name is a string
    without spaces.
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](ch08.xhtml#idm45607779218800-marker)) If you have problems, you might
    try a subset of your data in [JSONLint’s validator](https://jsonlint.com) for
    better feedback.
  prefs: []
  type: TYPE_NORMAL
- en: ^([4](ch08.xhtml#idm45607779160128-marker)) D3 takes a number of other data
    formats, such as hierarchical (tree type) data or node and link graph formats.
    Here’s an example of [a tree hierarchy specified in JSON](https://oreil.ly/WsBCI).
  prefs: []
  type: TYPE_NORMAL
- en: ^([5](ch08.xhtml#idm45607779004224-marker)) I recommend using this approach
    if you want to get a feel for the CSV or JSON parsers. It’s much more convenient
    than managing local files.
  prefs: []
  type: TYPE_NORMAL
