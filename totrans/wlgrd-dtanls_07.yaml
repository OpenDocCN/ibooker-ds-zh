- en: '8 Time series data: Data preparation'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Preparing time series data for analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Determining what subset of time series data to use
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cleaning time series data by handling gaps and missing values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analyzing patterns in time series data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Most datasets you will come across have a time component. If the process to
    generate the data involves taking the same measurement at recurring intervals,
    the data is called *time series data*. An example is measuring the yearly GDP
    of a country or the output of machinery in a production line. However, even something
    seemingly static, such as a customer database, has a time component if we look
    at the date customer records were created. We might not explicitly think of the
    data as a time series, but using this time component allows us to unlock additional
    insights in our data. For example, you could analyze the rate at which new customer
    records are being created or what times of the day your operations team are inputting
    data into the database.
  prefs: []
  type: TYPE_NORMAL
- en: 'Real business case: Forecasting'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The project covered by chapters 8 and 9 was inspired by multiple forecasting
    projects I have worked on and the fact that most data analysis curricula spend
    proportionally little time on the topic of working with temporal data.
  prefs: []
  type: TYPE_NORMAL
- en: In late 2020, I had to provide forecasts for where the used car market was heading
    after the initial COVID lockdown restrictions were lifted in the United Kingdom.
    Accurately forecasting an entire market is hard enough, but the added complexity
    of having patchy data for the lockdown period and having to forecast in a scenario
    no one had encountered before made this project particularly difficult.
  prefs: []
  type: TYPE_NORMAL
- en: In the end, we arrived at a prediction using a combination of fundamental forecasting
    principles, some concrete assumptions about the pandemic, and domain knowledge
    from our experts. It was a good example of a project where technical skills weren’t
    enough to solve the business problem.
  prefs: []
  type: TYPE_NORMAL
- en: Working with time series data is more than knowing how to work with date formats.
    It involves extracting time-related components from data, handling time data at
    different resolutions, handling gaps, forecasting into the future, and working
    out whether the data can be forecasted at all.
  prefs: []
  type: TYPE_NORMAL
- en: Knowing how to extract temporal patterns from your data is a vital skill in
    the real world and one that we will practice in this chapter through the project.
  prefs: []
  type: TYPE_NORMAL
- en: 8.1 Working with time series data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A time series is a repeated measurement taken at different, ideally uniform,
    time intervals. A typical tabular dataset, such as a customer dataset, will contain
    one row per customer, and each column will represent a different property of a
    customer, such as age, employment status, address, and so forth. A time series,
    however, typically contains fewer columns: one to represent the date of a measurement
    and one or more columns to represent the individual measurement value at that
    time. Each row, therefore, represents the same measurement, and it is the measurement
    time that makes each row unique.'
  prefs: []
  type: TYPE_NORMAL
- en: 8.1.1 The hidden depth of time series data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s take a simple example—customer satisfaction over time. Imagine one of
    those smiley-face-based satisfaction surveys you can find at airports, supermarket
    checkouts, or any other public place. As a customer walks by, they can press a
    smiley face to indicate their level of satisfaction. They see smiley faces, and
    the database records a simple value from 1 to 5 on a Likert scale to measure the
    value from most dissatisfied to most satisfied. Table 8.1 shows an example of
    what this dataset might look like.
  prefs: []
  type: TYPE_NORMAL
- en: Table 8.1 An example of a time series dataset
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Date | Satisfaction score |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 2023-11-01 11:03:55  | 4  |'
  prefs: []
  type: TYPE_TB
- en: '| 2023-11-01 11:17:02  | 5  |'
  prefs: []
  type: TYPE_TB
- en: '| 2023-11-01 13:41:11  | 3  |'
  prefs: []
  type: TYPE_TB
- en: '| 2023-11-01 14:06:43  | 4  |'
  prefs: []
  type: TYPE_TB
- en: To capture satisfaction over time, you just need a timestamp and the satisfaction
    value. If you had such a system set up across multiple locations, you might also
    find a location ID column, but the data wouldn’t be more complex than that.
  prefs: []
  type: TYPE_NORMAL
- en: What kind of analysis could we do with what is, at first glance, a very simple
    dataset? We could
  prefs: []
  type: TYPE_NORMAL
- en: Use clusters of rows as a proxy to identify busier periods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calculate average satisfaction over time at various levels of granularity (daily,
    weekly, monthly, etc.)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Investigate trends and seasonal patterns in the data (if customers are more
    satisfied at different points of the day or different days of the week)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Find anomalies where satisfaction rose or dropped to unexpected levels
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cross-reference this with other data to identify how satisfaction relates to
    external factors, such as special events
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compare satisfaction scores across different locations where that data is available
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The fact that most of these questions can be answered with just two to three
    columns of data shows the hidden depth that time series data can have.
  prefs: []
  type: TYPE_NORMAL
- en: 8.1.2 How to work with time series data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'What does it mean to work with time series? When exploring time data, we care
    about a lot of the same things as when exploring tabular data. We want to understand
    each column, ensure data types are consistent, and check for missing values. However,
    there are also specific considerations for time data:'
  prefs: []
  type: TYPE_NORMAL
- en: What is the granularity of the time series? Is it consistent?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Are there any gaps in the time series? Are these gaps there by design?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is there a trend in the data?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Are there seasonal patterns?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Are there any outliers worth investigating?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Once you have explored your time series dataset and want to proceed to forecasting,
    there are additional considerations:'
  prefs: []
  type: TYPE_NORMAL
- en: What is the right granularity for forecasting? This will depend partly on how
    much noise there is in the data. Hourly data might be too noisy, and although
    daily averages might be smoother and easier to forecast, there might not be enough
    data at a daily level.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Does the time series contain autocorrelation: do past values inform future
    values? There are statistical tests for this, and time series models will take
    advantage of this property.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is the time series stationary? Some time series models require the data to be
    stationary, which means having a roughly constant mean and variance over time.
    In reality, a lot of time series have some trend and seasonality, so we either
    need to handle those directly or use forecasting models that take care of them
    for us.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can any outliers be explained by external factors? For example, are certain
    spikes in your sales data due to one-off special sale days, like Black Friday?
    These external factors, technically “exogenous variables,” can be used in many
    forecasting models to improve predictions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A final note on forecasting: the most important question you should ask is,
    “How will the forecast be used?” This requires answering these questions:'
  prefs: []
  type: TYPE_NORMAL
- en: How often are forecasts required?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is the required granularity of the forecasts?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How far into the future should the forecasts go?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What does an acceptable level of accuracy look like?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is the value of an accurate forecast in business terms? What is, therefore,
    the return on investment of additional work to improve existing forecasts?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Will the data required by the forecasting model be available in time?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Answers to these questions will inform your analytical decisions at least as
    much as the technical considerations. When completing this project, and as with
    any project, you should always focus on the business outcomes you are trying to
    improve.
  prefs: []
  type: TYPE_NORMAL
- en: '8.2 Project 6: Analyzing time series to improve cycling infrastructure'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s look at the project in which we will analyze road traffic data to understand
    where cycling infrastructure should be improved. In this chapter, we will explore
    the available data and prepare it for analysis, which will happen in chapter 9.
  prefs: []
  type: TYPE_NORMAL
- en: The data is available for you to attempt the project yourself at [https://davidasboth.com/book-code](https://davidasboth.com/book-code).
    You will find the data you can use for the project, as well as the example solution
    in the form of a Jupyter notebook.
  prefs: []
  type: TYPE_NORMAL
- en: This project is all about using time series data to find answers to our business
    questions. As usual, we will start by looking at the problem statement, the data
    dictionary, the outputs we are aiming for, and what tools we need to tackle the
    problem. We will then formulate our action plan using the results-oriented framework
    before diving into the example solution.
  prefs: []
  type: TYPE_NORMAL
- en: 8.2.1 Problem statement
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You have been hired to work on a new government initiative, Bikes4Britain, which
    aims to improve cycling infrastructure in the United Kingdom. The aim of the first
    phase of the project is to identify the most suitable places around the country
    to improve infrastructure for cyclists. Specifically, your stakeholders are looking
    for recommendations of places with either substantial existing or increasing cycling
    traffic.
  prefs: []
  type: TYPE_NORMAL
- en: They want to start with open data sources and have identified the Department
    for Transport’s road traffic statistics ([https://roadtraffic.dft.gov.uk](https://roadtraffic.dft.gov.uk))
    as a way to measure cycling traffic across the country. This is the dataset we
    will use in this project to look for patterns and make recommendations.
  prefs: []
  type: TYPE_NORMAL
- en: NOTE  Data originally taken from [https://roadtraffic.dft.gov.uk/downloads](https://roadtraffic.dft.gov.uk/downloads).
    Thank you to the Department for Transport for making this data available under
    the Open Government Licence.
  prefs: []
  type: TYPE_NORMAL
- en: The data we will use from the Department’s statistics is the raw count data.
    This is a record of raw counts of vehicles that passed a particular counting location
    at various times. Some of the datasets are too high-level, such as area-level
    annual summaries, and some of them are estimates, such as the estimated annual
    average daily flows data (AADFs). The raw count dataset contains data at the most
    granular level, and we can always aggregate it to higher levels (e.g., annual
    values) if needed.
  prefs: []
  type: TYPE_NORMAL
- en: 8.2.2 Data dictionary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Before we think further about the project, we should take a look at what data
    we have available. The data dictionary document ([https://mng.bz/4ajw](https://mng.bz/4ajw))
    is included in the project files, and table 8.2 shows the columns in detail. The
    data dictionary is shown as is, without modification from the original.
  prefs: []
  type: TYPE_NORMAL
- en: Table 8.2 The data dictionary, showing all column definitions
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Column name | Definition |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `Count_point_id`  | A unique reference for the road link that links the AADFs
    to the road network  |'
  prefs: []
  type: TYPE_TB
- en: '| `Direction_of_travel`  | Direction of travel  |'
  prefs: []
  type: TYPE_TB
- en: '| `Year`  | Counts are shown for each year from 2000 onwards  |'
  prefs: []
  type: TYPE_TB
- en: '| `Count_date`  | The date when the actual count took place  |'
  prefs: []
  type: TYPE_TB
- en: '| `Hour`  | The time when the counts in question took place, where 7 represents
    between 7 a.m. and 8 a.m., and 17 represents between 5 p.m. and 6 p.m.  |'
  prefs: []
  type: TYPE_TB
- en: '| `Region_id`  | Website region identifier  |'
  prefs: []
  type: TYPE_TB
- en: '| `Region_name`  | The name of the region that the count point (CP) sits within  |'
  prefs: []
  type: TYPE_TB
- en: '| `Region_ons_code`  | The Office for National Statistics code identifier for
    the region  |'
  prefs: []
  type: TYPE_TB
- en: '| `Local_authority_id`  | Website local authority identifier  |'
  prefs: []
  type: TYPE_TB
- en: '| `Local_authority_name`  | The local authority that the CP sits within  |'
  prefs: []
  type: TYPE_TB
- en: '| `Local_authority_code`  | The Office for National Statistics code identifier
    for the local authority  |'
  prefs: []
  type: TYPE_TB
- en: '| `Road_name`  | The road name (for instance, M25 or A3)  |'
  prefs: []
  type: TYPE_TB
- en: '| `Road_category`  | The classification of the road type (see data definitions
    for the full list)  |'
  prefs: []
  type: TYPE_TB
- en: '| `Road_type`  | Whether the road is a major or minor road  |'
  prefs: []
  type: TYPE_TB
- en: '| `Start_junction_road_name`  | The road name of the start junction of the
    link  |'
  prefs: []
  type: TYPE_TB
- en: '| `End_junction_road_name`  | The road name of the end junction of the link  |'
  prefs: []
  type: TYPE_TB
- en: '| `Easting`  | Easting coordinates of the CP location  |'
  prefs: []
  type: TYPE_TB
- en: '| `Northing`  | Northing coordinates of the CP location  |'
  prefs: []
  type: TYPE_TB
- en: '| `Latitude`  | Latitude of the CP location  |'
  prefs: []
  type: TYPE_TB
- en: '| `Longitude`  | Longitude of the CP location  |'
  prefs: []
  type: TYPE_TB
- en: '| `Link_length_km`  | Total length of the network road link for that CP (in
    kilometers)  |'
  prefs: []
  type: TYPE_TB
- en: '| `Link_length_miles`  | Total length of the network road link for that CP
    (in miles)  |'
  prefs: []
  type: TYPE_TB
- en: '| `Pedal_cycles`  | Counts for pedal cycles  |'
  prefs: []
  type: TYPE_TB
- en: '| `Two_wheeled_motor_vehicles`  | Counts for two-wheeled motor vehicles  |'
  prefs: []
  type: TYPE_TB
- en: '| `Cars_and_taxis`  | Counts for cars and taxis  |'
  prefs: []
  type: TYPE_TB
- en: '| `Buses_and_coaches`  | Counts for buses and coaches  |'
  prefs: []
  type: TYPE_TB
- en: '| `LGVs`  | Counts for LGVs  |'
  prefs: []
  type: TYPE_TB
- en: '| `HGVs_2_rigid_axle`  | Counts for two-rigid axle HGVs  |'
  prefs: []
  type: TYPE_TB
- en: '| `HGVs_3_rigid_axle`  | Counts for three-rigid axle HGVs  |'
  prefs: []
  type: TYPE_TB
- en: '| `HGVs_4_or_more_rigid_axle`  | Counts for four or more rigid axle HGVs  |'
  prefs: []
  type: TYPE_TB
- en: '| `HGVs_3_or_4_articulated_axle`  | Counts for three- or four-articulated axle
    HGVs  |'
  prefs: []
  type: TYPE_TB
- en: '| `HGVs_5_articulated_axle`  | Counts for five-articulated axle HGVs  |'
  prefs: []
  type: TYPE_TB
- en: '| `HGVs_6_articulated_axle`  | Counts for six-articulated axle HGVs  |'
  prefs: []
  type: TYPE_TB
- en: '| `All_HGVs`  | Counts for all HGVs  |'
  prefs: []
  type: TYPE_TB
- en: '| `All_motor_vehicles`  | Counts for all motor vehicles  |'
  prefs: []
  type: TYPE_TB
- en: The data dictionary shows that we have data about the day and time that vehicle
    counts were recorded. There is a column specifically recording the count of bicycles,
    as well as plenty of information about the stretch of road where the counting
    took place. We can already see that we will be able to look at vehicle counts
    at different locations as separate time series since we satisfy the definition
    of a time series by having both date and time information, as well as the same
    measurement taken at different time intervals.
  prefs: []
  type: TYPE_NORMAL
- en: 8.2.3 Desired outcomes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The output of the project is a recommendation of which area, or areas, to concentrate
    on for further analysis. These might be areas that already have a lot of high
    bicycle traffic, or they might be areas where cycling is on the rise or forecasted
    to have high cycling demand in the future. Our recommendation will likely contain
    suggested additional datasets we could incorporate to continue the analysis. There
    is likely more that we would like to know about these areas before any infrastructure
    work is undertaken, and we should outline this additional work to our stakeholders.
  prefs: []
  type: TYPE_NORMAL
- en: As this project spans multiple chapters, the desired outcome of this chapter,
    which is the data preparation part, is a filtered and cleaned version of the raw
    data, ready to be analyzed. The outcome of chapter 9 will then be the results
    of the analysis and final recommendations.
  prefs: []
  type: TYPE_NORMAL
- en: 8.2.4 Required tools
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As with most projects, your data analysis toolkit needs to read, explore, and
    visualize data to be suitable for the project. In the example solution, I use
    Python and the `pandas` and `matplotlib` libraries for data exploration and visualization,
    respectively. In the following chapter, I also introduce some time series functions
    from the `statsmodels` library when investigating time-specific aspects of the
    data and the `pmdarima` module for automatically choosing the best forecasting
    model. For this project, your tool should be able to
  prefs: []
  type: TYPE_NORMAL
- en: Load a large dataset from a CSV or Excel file containing millions of rows
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perform basic data manipulation tasks, such as filtering, sorting, grouping,
    and reshaping data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create data visualizations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Produce statistical analysis, specifically of time series data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optionally create forecasts based on time series data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 8.3 Applying the results-driven method to analyzing road traffic data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s now see how we will address this problem in a results-driven way and formulate
    our action plan. We will follow the steps of the results-driven process to explore
    the data with our stakeholders’ requests and areas of interest in mind.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/8-unnumb-1.png)'
  prefs: []
  type: TYPE_IMG
- en: Do we fully understand the problem statement? Our stakeholders are interested
    in seeing how the number of cyclists has changed over time and across different
    areas. We know that’s the part of the data we will focus on. However, their request
    is not as specific as we might like.
  prefs: []
  type: TYPE_NORMAL
- en: We need to define key terms, such as what it means for a place to be suitable
    for upgrading the cycling infrastructure. Is it somewhere where there are already
    a lot of cyclists? Or do we want to find places with potentially lower cycling
    traffic but where cycling is increasing the most over time? In that case, what
    are our criteria for “increasing”? If we can forecast our time series, we might
    even be able to make recommendations based on predicted future traffic.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/8-unnumb-2.png)'
  prefs: []
  type: TYPE_IMG
- en: Let’s now think about the end result. We must focus on patterns in cycling traffic,
    so there will be parts of the data we can largely ignore for our analysis. Knowing
    that we are interested in a particular aspect of the data will help us at the
    start when we are figuring out where to go next after the standard exploratory
    steps.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/8-unnumb-3.png)'
  prefs: []
  type: TYPE_IMG
- en: This is the step in which we decided to use the raw count data over the other
    available datasets. Again, this was driven directly by the problem statement.
    Area-level annual summaries are too high level to tease out cycling traffic, and
    using estimated measurements reduces the usefulness of our findings, leaving us
    with the raw count data to analyze. In this case, we do not have to make further
    decisions about which dataset to download, as the entire raw dataset comes as
    one file.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/8-unnumb-4.png)'
  prefs: []
  type: TYPE_IMG
- en: As with most of the projects, the data has already been downloaded, but no other
    changes have been made to it to best simulate the experience of exploring it for
    the first time. In the real world, obtaining the data might be a surprising obstacle,
    especially if you need permission, and there are privacy and governance concerns.
    This is not the case here, as we are working with open government data.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/8-unnumb-5.png)'
  prefs: []
  type: TYPE_IMG
- en: Let’s now think about the steps we will take in our analysis. Before we turn
    our attention to the recommendation portion, we need to explore the dataset thoroughly.
    Specifically, we want to
  prefs: []
  type: TYPE_NORMAL
- en: '*Investigate the granularity of our data* —What does one row represent? Is
    it one row per location per day or something else? The granularity of data is
    one of the first things to investigate because it informs all other data transformations,
    like aggregations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Understand the coverage of the data both geographically and in time* —For
    example, because the dataset is not a single time series but many, we need to
    know if every available location has the same amount of data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Identify gaps in the time series* —Does every location have measurements at
    constant intervals? This is important to ensure we have enough of a sample at
    each location and is also a critical requirement for forecasting. Most forecasting
    algorithms do not work with gaps in the data or inconsistent intervals.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Investigate the distribution of bicycle counts* —What is a typical cycling
    volume for one row of data? Knowing this will immediately help identify the places
    with the highest cycling traffic.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Look at temporal patterns* —This includes looking at how cycling traffic fluctuates
    at different times of day, different days of the week, and across multiple years.
    Are there seasonal patterns we can identify? Which locations are showing a growing
    trend in cycling traffic?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Reduce the search space* —By this I mean we may not be able to analyze every
    location in equal detail because of gaps. We may have to filter the data down
    to locations that have more complete records across a longer time horizon, especially
    if we are interested in looking for temporal patterns and forecasting.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*![figure](../Images/8-unnumb-6.png)'
  prefs: []
  type: TYPE_NORMAL
- en: The output of this project is likely to be a combination of line charts and
    conversations. Line charts are the de facto time series visualization tool because
    they best represent the temporal component of an analytical result. We will likely
    end up creating other visualizations, too, but this is a project where the first
    iteration will spark a lot of conversation with our stakeholders. As we identify
    the limitations of the available data, we will be able to make recommendations
    about other datasets, and deciding on which ones to focus on will be done in collaboration
    with our domain experts.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/8-unnumb-7.png)'
  prefs: []
  type: TYPE_IMG
- en: Since the dataset focuses narrowly on traffic volume, there will be multiple
    angles to explore after our initial recommendations. In the example solution,
    we’ll explore some of these possible directions in which we could take a future
    iteration.
  prefs: []
  type: TYPE_NORMAL
- en: '8.4 An example solution: Where should cycling infrastructure improvements be
    focused?'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now, it’s time to look at an example walkthrough of analyzing this data. As
    always, I strongly recommend attempting the project yourself first. The example
    solution will be more relevant if you have your own analysis to compare it to.
    It bears repeating that the solution is not *the* solution, just one series of
    decisions you could make and conclusions you could reach along the way. Use it
    to generate more ideas and gain a different perspective on how you could have
    approached the same project brief.
  prefs: []
  type: TYPE_NORMAL
- en: Knowing what our end goal is and having thought about the various steps we want
    to take, our action plan will start with investigating the data. Only then will
    we understand what specific questions we can answer with what’s available. Then,
    we can focus on looking for patterns and trends in cycling behavior and perhaps
    even attempt to forecast cycling trends into the future.
  prefs: []
  type: TYPE_NORMAL
- en: 8.4.1 Investigating available data and extracting time series
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As with any data problem, our first step is to look at the data itself. We
    know what columns to expect, but seeing a few sample rows will help us understand.
    We’ll import the necessary libraries and examine a few rows of data. The output
    is shown in figure 8.1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '![figure](../Images/8-1.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.1 A glimpse of the first few rows of traffic count data
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The shape of the data is `(4815504, 35)`, meaning we have 35 columns and close
    to 5 million observations. From the columns, we can tell that this isn’t a single
    time series. It is, in fact, lots of time series at various “count points,” that
    is, measurement locations. Measurements at each count point can be treated as
    a separate time series, but we also have the option to aggregate by region, local
    authority, or even different time periods. From our data dictionary, we can also
    tell that we will be interested in the `Pedal_cycles` column, which measures the
    number of bicycles observed in a measurement period.
  prefs: []
  type: TYPE_NORMAL
- en: Investigating time series completeness
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Let’s examine the data to see how complete it is. First, we will look at missing
    data. The following code produces the output in figure 8.2:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![figure](../Images/8-2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.2 Missing values per column in the traffic data
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'It looks like the records are mostly complete, with only road names and lengths
    missing a significant amount. This might have something to do with the different
    road types since not all roads in the country, and therefore, in the dataset,
    necessarily have a name. We will leave them missing since we have no reason to
    believe those rows are erroneous. There is a small number of measurements missing,
    which we will assume can be filled with zeros with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'For completeness, we can also investigate what regions are covered by the data
    by inspecting the `Region_name` column. The following code produces the output
    in figure 8.3:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![figure](../Images/8-3.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.3 Distribution of regions in the traffic data
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: It looks like the data covers England, Scotland, and Wales, as well as various
    regions in England. Before moving on with our investigation, let’s start building
    the diagram to document the analysis. Figure 8.4 shows the first step, in which
    we had to make a decision about missing values.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/8-4.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.4 The first step in the analysis
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Our next question is concerning granularity: What precisely does one row of
    data represent?'
  prefs: []
  type: TYPE_NORMAL
- en: Investigating time series granularity
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: It is important to establish what one row of our data represents. It is a measurement
    at a particular time and location, but what specific combination of columns makes
    a row unique? We can test this by counting the number of rows for the combination
    of columns we believe to be unique and verifying if that matches the number of
    rows in the entire dataset.
  prefs: []
  type: TYPE_NORMAL
- en: A note on composite primary keys
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: When multiple columns make a record unique, this is called a *composite primary
    key*. It is when uniqueness does not come from a single ID column but a combination
    of more than one column. For example, customer IDs might not be unique if multiple
    customer databases are combined. In that case, the customer ID and the source
    database name might be what makes a record unique.
  prefs: []
  type: TYPE_NORMAL
- en: This is another way in which foundational training can differ from the real
    world. In reality, databases often have complex structures, including composite
    primary keys.
  prefs: []
  type: TYPE_NORMAL
- en: 'The unique key, in this case, must at least contain `Count_point_id`, the `Count_date`,
    and therefore also the `Year`. There is also an `hour` column, suggesting the
    data is at an hourly granularity. If we assume these columns are the composite
    key, we can count the unique combinations and verify that they match the row count:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives us `2435120`, which is too few rows and suggests there is another
    column we haven’t taken into account. The fact that this number is roughly half
    of the data suggests the column we are looking for typically has two values, so
    for every hour at every location, there is also another kind of measurement. Looking
    at the columns, this could be the `Direction_of_travel`, meaning traffic is counted
    separately in both directions at each location. Let’s add that column to the key
    to see if it matches the number of rows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'This returns `4815480`, which is much closer to the number of rows, suggesting
    we have found the right combination of columns, but the data contains duplicates.
    Let’s investigate these. The following code finds duplicate keys and produces
    the output in figure 8.5:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '![figure](../Images/8-5.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.5 Duplicate records with the same composite primary key
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'It looks like there are two locations with duplicate measurements on two dates.
    We want to ascertain whether the measurements are also duplicated or whether the
    rows are perfect duplicates. We do this by taking one of the keys as an example
    and looking at which values the duplicate rows differ in. The following code does
    this and produces the output in figure 8.6:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Finds a specific example of a duplicate'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Uses the shift method to check whether values are equal in both rows'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/8-6.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.6 Columns where values don’t match in the example duplicate rows
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'This tells us that for that location and date, the columns that differ are
    the ones measuring how many vehicles passed by. Let’s look at the specific measurements
    to determine how different they are. The following code extracts these columns.
    The returned data contains many columns, and by default, they are not all shown.
    Even if we could show them all, which is possible to do, they wouldn’t fit horizontally
    on the screen without the need to scroll. One trick is to take a row or two of
    data and *transpose* it to show it as only one or two columns instead. We’ll do
    this here, and the output is shown in figure 8.7:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '![figure](../Images/8-7.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.7 Side-by-side comparison of duplicate records
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The values are quite different for the same combination of location and measurement
    date and time, so we are now confronted with a decision to make. What are our
    options when handling these duplicates?
  prefs: []
  type: TYPE_NORMAL
- en: Should we combine the values somehow? This would make sense if these were partial
    measurements, but we have no evidence of this, and the numbers are too similar.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is one of them newer data, making the other row obsolete, in which case we should
    drop the first row? This is possible, but if it’s the case, we have no way of
    knowing which would be the newer measurement apart from assuming the one that
    appears later is more recent. This feels like a strong assumption to make with
    no evidence.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We could drop these rows entirely, but this would introduce a gap into some
    of our time series, which is problematic for time series analysis.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We could average the counts across the two records. This preserves the time
    series and keeps our numbers in the right ballpark, but we are essentially making
    up data this way.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is no correct answer here. Each choice has its own assumptions and consequences.
    We will err on the side of preserving the time series and go with the averaging
    approach. While this does make up measurements that were not actually recorded,
    these duplicates are a small enough percentage of the overall data not to make
    this a big problem.
  prefs: []
  type: TYPE_NORMAL
- en: To combine these duplicates, we can group our data by the composite key, creating
    one group per unique identifier and averaging the measurement rows. In most cases,
    since the keys are unique, we will be averaging a single row, leaving it unaffected.
    The only additional trick is to handle missing columns. Our road name and link
    length columns, which form part of the composite key, contain missing values.
    The `pandas` library in particular will not group the records correctly when some
    grouping columns are missing.
  prefs: []
  type: TYPE_NORMAL
- en: 'To avoid this, we will temporarily fill missing values with a placeholder,
    do the deduplication, and remove the placeholder values afterward. For the `Road_name`
    column, we can use the text “PLACEHOLDER,” but for the numeric columns, we need
    to find a value that doesn’t already appear in the data. Negative numbers work
    well here, but we should double-check that there aren’t already negative values
    for link length for whatever reason:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is 0.1 and 0.06, respectively, telling us that there are no negative
    values, and we can use one as a placeholder. The process for deduplication is
    therefore the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Replace missing values with placeholders.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Group by all columns except the measurements.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Within each group, which is mostly one row each, average the measurement values.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the grouped and aggregated dataset, replace placeholders with missing data
    again.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following code does this and verifies that we have reduced the number of
    rows to the number of unique groups:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The output is `(4815504, 35) (4815480, 35)`, where the second pair of values
    shows us that we have reduced the data down to one row per unique combination
    of columns in the composite primary key. This feels like a lot of work to remove
    a few duplicates, but the presence of duplicates can cause multiple problems with
    analysis, so it is best to address them.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.8 shows the latest version of our process, including the steps we have
    just taken to merge duplicate records.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/8-8.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.8 The diagram of our analysis two steps in
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: So far, we’ve investigated and handled missing data and ensured we understand
    its granularity. Now, it’s time to look at the coverage.
  prefs: []
  type: TYPE_NORMAL
- en: Investigating time series coverage
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'When I say we will look at the coverage, in this instance, I mean the date
    range of values in the data. We’ve looked briefly at geographic coverage, and
    now we want to investigate the following:'
  prefs: []
  type: TYPE_NORMAL
- en: What is the date range of the data in general?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Does the date range vary across smaller time series (e.g., per location)?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Are there consistent measurement intervals in the data?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Are there gaps in any of the time series?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Answers to these questions will determine not only the quality of our final
    analysis but also whether we need to focus on certain parts of the country purely
    because of a lack of consistent data coverage everywhere. First, let’s understand
    the date range of the data after we convert the `Count_date` column to be the
    right type. The output is shown in figure 8.9:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '![figure](../Images/8-9.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.9 The date range of the entire dataset
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The output of this code is that the first date encountered in the dataset is
    March 2000, and the latest is November 2022\. We have 20-year coverage, though
    it remains to be seen whether this is consistent across measuring locations. We
    want to know
  prefs: []
  type: TYPE_NORMAL
- en: Does every location have 20 years of data?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Are there gaps in any of the time series of the different locations?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'One way to investigate this is to calculate the first and last date per location,
    calculate the difference, and investigate the distribution of this difference
    number. This will tell us how long each location-specific time series is at a
    glance. The following code achieves this, and the output is shown in figure 8.10:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '![figure](../Images/8-10.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.10 Coverage (in years) by location
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'This table tells us a few important things:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Coverage varies a lot across locations.* There are locations with a single
    day’s worth of measurements and some that have data for the entire 22-year period.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Measurement dates vary.* This wasn’t obvious looking at the data initially,
    but it turns out there is no consistent start or end point for any of the measurement
    time series.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To get a better sense of the distribution of these values, let’s create a histogram.
    The following code produces the histogram in figure 8.11:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '![figure](../Images/8-11.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.11 Histogram showing coverage in years across different locations
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'It looks like locations overwhelmingly have a coverage of near zero. That is,
    most locations only have one day of measurement to their name. This presents a
    problem because the data in those locations does not constitute much of a time
    series, except for hourly measurements on a single day. That’s not enough data
    to draw much insight from. Again, we are facing the following choice:'
  prefs: []
  type: TYPE_NORMAL
- en: Do we include locations with only one day of measurements? Doing so means we
    don’t lose a lot of coverage, but we also can’t answer questions about increasing
    trends in those areas.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Do we focus only on locations with enough data? This will mean we have more
    robust results but for far fewer locations and areas.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is a good decision point to remind ourselves of our framework. We want
    to be results driven and have the research question at the forefront of our minds.
    We will likely need to follow up our recommendations with further work and certainly
    wouldn’t want to base any infrastructure decisions on sparse data. On that basis,
    we will seek to trim down the data to keep only the time series that have the
    most data, the highest coverage, and no gaps.
  prefs: []
  type: TYPE_NORMAL
- en: Note  This is one of those decisions that will drastically affect how different
    our solutions will be. If your results don’t match mine, do not assume it is because
    you have made a mistake. We might have just made different decisions, leading
    to different results. As long as those decisions and their key assumptions are
    documented, different results may be equally valuable and useful.
  prefs: []
  type: TYPE_NORMAL
- en: Whenever we investigate missing data, we want to know whether there are any
    patterns in the gaps. Is any specific factor causing a part of our data to be
    missing? In this instance, we are working with low coverage instead of missing
    data, but the idea holds. Let’s investigate whether there are certain areas of
    the country that have less coverage. Why could this be?
  prefs: []
  type: TYPE_NORMAL
- en: Some locations may have been added to the “traffic measurement program” later
    than others.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There might be logistic difficulties with measuring in certain locations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: New roads have been built around newly built housing estates, and measurements
    could not have started at an earlier date.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Whatever the reason, we want to know whether low coverage is randomly distributed
    across the country or whether there is a pattern we should be aware of. Let’s
    use the table from figure 8.10 to focus on location points with only one day of
    data. We will drop duplicate rows for the same location ID because we want to
    look at how they are distributed and not at their granular measurements. Figure
    8.12 shows the number of locations with only one day of coverage, split by region,
    as obtained by the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Locations with only one day of measurements are referred to as "zero" because
    the difference between the first and last measurement dates is zero.'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/8-12.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.12 Number of locations with one day of data across regions
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: There is variation here, but we must not fall into the trap of using absolute
    numbers to make a judgment. It might simply be that the South East has more single-day
    locations than Wales because there are more location points. Let’s calculate these
    as a percentage of the total number of locations in each region to get a fair
    comparison.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we calculate the number of location points by region and then use that
    number to calculate the numbers in figure 8.12 as a percentage. The following
    code calculates the locations by region, as shown in figure 8.13:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '![figure](../Images/8-13.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.13 Number of count points by region
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The following code joins the two tables together and produces the output table
    shown in figure 8.14:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '![figure](../Images/8-14.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.14 Number of total locations and single-day locations per region
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: If there was a real problem with single-day locations being limited to only
    certain regions, we would find considerable variation in this table. As it stands,
    the percentage of locations that only have data on a single date is consistent
    across the regions, with only Wales and London being noticeably lower. We could
    investigate this much deeper, but in the spirit of getting to our result, we will
    assume we are satisfied that the existence of single-day locations is just something
    that happens everywhere and is not something to address directly.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have looked at missing data, granularity, and coverage, let’s turn
    our attention to gaps. Gaps are a problem when it comes to time series, so we
    want to reduce our data to locations where we can get a longer and complete time
    series.
  prefs: []
  type: TYPE_NORMAL
- en: Investigating gaps in time series
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We’ve established that different locations have tracked data since a different
    starting point and for varying lengths of time. To identify gaps, we can’t just
    count the number of unique dates seen at a location; we need to calculate the
    difference between each encountered date and the previously encountered date and
    flag any cases with more than a one-day gap.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s first look at the number of data points per location per date to get
    an idea whether there might be continuity problems. The following code calculates
    this and produces the table in figure 8.15:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '![figure](../Images/8-15.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.15 Number of data points per location ID and per date
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: This table shows us something important. We made an incorrect assumption that
    measurements are taken daily across a year. The data is daily in its granularity,
    but there is only one day of measurement data for each year. We hadn’t sliced
    the data in the right way before to find this earlier, but this gives us a clearer
    picture of what we have.
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand whether there might be gaps, first, we can check how many unique
    values there are for the `Year` column in each location. Those that have 23 are
    the ones that have measurements in every year between 2000 and 2022, inclusive.
    We will focus only on locations that have at least 10 years of data, but that’s
    somewhat arbitrary. We could also restrict time series that are complete for the
    most recent 5 to 10 years. Here, we’ll choose completeness over recency, and the
    following code calculates this and outputs the result in figure 8.16:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '![figure](../Images/8-16.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.16 Distribution of the number of unique calendar years per location
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Let’s also look at the time series for the first of those locations to get
    a sense of what a complete time series looks like in our data. The following code
    produces the plot in figure 8.17:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '![figure](../Images/8-17.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.17 An example time series for location 26010
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: This is the total number of vehicles seen at a particular location on the day
    counting took place each year. There are already some interesting aspects, such
    as the dip in 2020 due to the COVID-19 lockdowns.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step is to filter our location list to only the time series that have
    no gaps. To do this, we will ensure our data is sorted and create a temporary
    column to capture the year of the previous row so that we can find instances where
    the gap between a row and the previous one is more than a year. The following
    code adds these additional columns, and a snapshot of the new `gaps` DataFrame
    is shown in figure 8.18:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '![figure](../Images/8-18.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.18 A snapshot of the new `gaps` DataFrame, with important rows highlighted
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: These new columns help us identify places where the previously encountered measurements
    were from more than one year ago. In the highlighted section of figure 8.18, we
    can notice that there was no measurement at location ID 501 in the year 2018,
    so the gap between rows is two years. When the next location ID is encountered,
    the gap can become negative when the last year of the previous location ID is
    later than the first year of the next location ID.
  prefs: []
  type: TYPE_NORMAL
- en: To identify gaps, we could simply filter this dataset down to where the `diff`
    column is greater than 1\. However, we might encounter edge cases where the next
    location ID happens to start two years after the previous one, and we would erroneously
    mark it as having a gap.
  prefs: []
  type: TYPE_NORMAL
- en: 'To make sure we filter gaps properly, we also need to track the location ID
    of the previous column so that when we encounter a gap greater than one year,
    we also check whether the location ID is still the same. The following code does
    this, and some of the rows with problematic gaps are shown in figure 8.19:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '![figure](../Images/8-19.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.19 Some of the rows representing problematic gaps in the time series
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'We can now use this `gaps` DataFrame to find all the unique location IDs that
    we want to exclude from our final time series data. This is done with the following
    code. A part of the resulting DataFrame is shown in figure 8.20:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '![figure](../Images/8-20.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.20 A snapshot of rows from the filtered traffic data
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Figure 8.20 now shows filtered rows from the original, raw `traffic` DataFrame.
    It contains only location IDs that have at least 10 years of continuous, gap-free
    data. Let’s now aggregate this to actually be a time series summarized at a location
    level so that we better understand how much data we are left with. The following
    code performs this aggregation, and figure 8.21 shows the first few rows of our
    newly aggregated data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '![figure](../Images/8-21.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.21 A snapshot of filtered traffic data now aggregated as an annual
    time series
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: This is now one row per location ID and measurement date. As the top of figure
    8.21 shows, we still have data for just over 1,400 unique location IDs. These
    are now all-time series where there are measurements every year, so it is a time
    series with no gaps.
  prefs: []
  type: TYPE_NORMAL
- en: 'To be precise, we have a time series showing the total number of vehicles that
    passed a count point in a single day in a particular year. There is only one day
    where measurements take place each year. This is an important detail because it
    leads to some caveats:'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.21 shows that measurements are not taken on the same day each year.
    If we want to investigate traffic patterns over time, the measurements should
    at least be taken around the same time of year because, otherwise, we might be
    comparing summer traffic to winter traffic, for example. One option is to keep
    only time series where the measurements are consistently taken around the same
    time of year.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Following this, whichever part of the year our measurements are taken from will
    introduce bias. Cycling patterns for locations where only winter measurements
    exist might not be helpful when cycling is likely reduced everywhere around that
    time of year.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the date *is* the same each year, that might actually be a problem because
    we might be comparing different days of the week, even weekdays to weekends.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s check that last point. What days of the week is the data spread across?
    The following code investigates this, and the output is shown in figure 8.22:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '![figure](../Images/8-22.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.22 The percentage of rows across different days of the week
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: This tells us that roughly 20% of rows are spread across Tuesday to Friday,
    with slightly less on Mondays. Because we are counting days of the week starting
    with Monday as zero, we now also know there are no weekends in our remaining data,
    so that’s one concern we’ve alleviated.
  prefs: []
  type: TYPE_NORMAL
- en: We can still use this data as a proxy for traffic over time to help us hone
    in on locations that have interesting cycling traffic patterns, but we must be
    fully aware of the limitations, especially when presenting results to our stakeholders.
  prefs: []
  type: TYPE_NORMAL
- en: Before moving on to the analysis of these time series, our final step is to
    see what would happen if we were to keep only time series where every measurement
    was made in the same month each year.
  prefs: []
  type: TYPE_NORMAL
- en: Finding time series recorded at the same time each year
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The following code identifies the location points where measurements were only
    ever made in the same month every year. Figure 8.23 shows the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '![figure](../Images/8-23.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.23 Location IDs where only the same month was encountered
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Keeping this filter would halve our data but still leave us just under 700 time
    series. We should verify that the time series associated with these location IDs
    do indeed only contain the same month. We’ll take the first location ID as an
    example, but in reality, we’d want to spot-check a few cases. The following code
    examines the time series for the count point with ID 900056, and the output is
    shown in figure 8.24.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '![figure](../Images/8-24.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.24 Time series data for location ID 900056
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Figure 8.24 shows that for all 13 years that traffic was counted at location
    900056, it was always done in May. This makes the measurements more comparable
    year on year. Let’s now export this data to an intermediate file to separate the
    cleaning process from the analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Exporting only complete time series data
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Exporting an intermediate version of your data is a good habit to get into,
    especially if you have a lot of raw data, and cleaning and transforming it takes
    a bit of time.
  prefs: []
  type: TYPE_NORMAL
- en: 'We want to keep the raw version of the data filtered down to just the location
    IDs we have identified. We’ll use the Parquet format as it is compact and preserves
    data types. This file will be the starting point for the analysis in chapter 9\.
    The following code creates this exported file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 8.4.2 Project progress so far
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Before we move on to the analysis portion of the project in chapter 9, let’s
    recap what we have achieved in this chapter, which was the data preparation part
    of the project. Here’s what we know about our data:'
  prefs: []
  type: TYPE_NORMAL
- en: One row represents measurements taken at a single location, in a single hour
    on a particular date in a particular direction. A combination of these columns
    makes a record unique.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For every location, we have a maximum of one unique day of measurements for
    a given calendar year.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of years where measurements were taken varies significantly across
    location IDs. This means there is both inconsistent coverage and gaps in many
    of our time series.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apart from missing roughly half of the road name and length data, there are
    no significant missing values.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To mitigate some of the problems, we have extracted only locations with the
    longest and most complete time series to focus on in part 2 of our analysis. Figure
    8.25 shows the analysis steps we have taken and the decisions we have made so
    far.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/8-25.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.25 The latest diagram of our steps, including investigating coverage
    and handling gaps
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: This diagram documents our process so far, and we will use the output of this
    chapter, a filtered version of the raw traffic data, as the starting point for
    the analysis in chapter 9.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Time series data can seem simple yet contain complexity and hidden value.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding how to manipulate time data broadens your data analysis toolkit.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The granularity of the available time series determines the analysis we can
    perform. For example, daily patterns cannot be determined from monthly data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Time series analysis works best if there are no gaps in the data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If there are gaps, they need to be handled either by smoothing over them or
    estimating what the values in the gaps should be.*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
