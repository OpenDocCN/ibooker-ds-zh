["```py\nimport requests\n\nresponse = requests.get('https://api.github.com/repositories',\n                        headers={'Accept': 'application/vnd.github.v3+json'})\nprint(response.status_code)\n\n```", "```py\n200\n\n```", "```py\nprint (response.encoding)\nprint (response.headers['Content-Type'])\nprint (response.headers['server'])\n\n```", "```py\nutf-8\napplication/json; charset=utf-8\nGitHub.com\n\n```", "```py\nimport json\nprint (json.dumps(response.json()[0], indent=2)[:200])\n\n```", "```py\n{\n  \"id\": 1,\n  \"node_id\": \"MDEwOlJlcG9zaXRvcnkx\",\n  \"name\": \"grit\",\n  \"full_name\": \"mojombo/grit\",\n  \"private\": false,\n  \"owner\": {\n    \"login\": \"mojombo\",\n    \"id\": 1,\n    \"node_id\": \"MDQ6VXNlcjE=\",\n\n```", "```py\nresponse = requests.get('https://api.github.com/search/repositories')\nprint (response.status_code)\n\n```", "```py\n422\n\n```", "```py\nresponse = requests.get('https://api.github.com/search/repositories',\n    params={'q': 'data_science+language:python'},\n    headers={'Accept': 'application/vnd.github.v3.text-match+json'})\nprint(response.status_code)\n\n```", "```py\n200\n\n```", "```py\nfor item in response.json()['items'][:5]:\n    printmd('**' + item['name'] + '**' + ': repository ' +\n            item['text_matches'][0]['property'] + ' - \\\"*' +\n            item['text_matches'][0]['fragment'] + '*\\\" matched with ' + '**' +\n            item['text_matches'][0]['matches'][0]['text'] + '**')\n\n```", "```py\nDataCamp: repository description - \"*DataCamp data-science courses*\" matched with\ndata\n\ndata-science-from-scratch: repository description - \"*code for Data Science From\nScratch book*\" matched with Data Science\n\ndata-science-blogs: repository description - \"*A curated list of data science\nblogs*\" matched with data science\n\ngalaxy: repository description - \"*Data intensive science for everyone.*\" matched\nwith Data\n\ndata-scientist-roadmap: repository description - \"*Tutorial coming with \"data\nscience roadmap\" graphe.*\" matched with data science\n\n```", "```py\nresponse = requests.get(\n    'https://api.github.com/repos/pytorch/pytorch/issues/comments')\nprint('Response Code', response.status_code)\nprint('Number of comments', len(response.json()))\n\n```", "```py\nResponse Code 200\nNumber of comments 30\n\n```", "```py\nresponse.links\n\n```", "```py\n{'next': {'url': 'https://api.github.com/repositories/65600975/issues/\ncomments?page=2',\n  'rel': 'next'},\n 'last': {'url': 'https://api.github.com/repositories/65600975/issues/\ncomments?page=1334',\n  'rel': 'last'}}\n\n```", "```py\ndef get_all_pages(url, params=None, headers=None):\n    output_json = []\n    response = requests.get(url, params=params, headers=headers)\n    if response.status_code == 200:\n        output_json = response.json()\n        if 'next' in response.links:\n            next_url = response.links['next']['url']\n            if next_url is not None:\n                output_json += get_all_pages(next_url, params, headers)\n    return output_json\n\nout = get_all_pages(\n    \"https://api.github.com/repos/pytorch/pytorch/issues/comments\",\n    params={\n        'since': '2020-07-01T10:00:01Z',\n        'sorted': 'created',\n        'direction': 'desc'\n    },\n    headers={'Accept': 'application/vnd.github.v3+json'})\ndf = pd.DataFrame(out)\n\nprint (df['body'].count())\ndf[['id','created_at','body']].sample(1)\n\n```", "```py\n3870\n\n```", "```py\nresponse = requests.head(\n    'https://api.github.com/repos/pytorch/pytorch/issues/comments')\nprint('X-Ratelimit-Limit', response.headers['X-Ratelimit-Limit'])\nprint('X-Ratelimit-Remaining', response.headers['X-Ratelimit-Remaining'])\n\n# Converting UTC time to human-readable format\nimport datetime\nprint(\n    'Rate Limits reset at',\n    datetime.datetime.fromtimestamp(int(\n        response.headers['X-RateLimit-Reset'])).strftime('%c'))\n\n```", "```py\nX-Ratelimit-Limit 60\nX-Ratelimit-Remaining 0\nRate Limits reset at Sun Sep 20 12:46:18 2020\n\n```", "```py\nfrom datetime import datetime\nimport time\n\ndef handle_rate_limits(response):\n    now = datetime.now()\n    reset_time = datetime.fromtimestamp(\n        int(response.headers['X-RateLimit-Reset']))\n    remaining_requests = response.headers['X-Ratelimit-Remaining']\n    remaining_time = (reset_time - now).total_seconds()\n    intervals = remaining_time / (1.0 + int(remaining_requests))\n    print('Sleeping for', intervals)\n    time.sleep(intervals)\n    return True\n\n```", "```py\nfrom requests.adapters import HTTPAdapter\nfrom requests.packages.urllib3.util.retry import Retry\n\nretry_strategy = Retry(\n    total=5,\n    status_forcelist=[500, 503, 504],\n    backoff_factor=1\n)\n\nretry_adapter = HTTPAdapter(max_retries=retry_strategy)\n\nhttp = requests.Session()\nhttp.mount(\"https://\", retry_adapter)\nhttp.mount(\"http://\", retry_adapter)\n\nresponse = http.get('https://api.github.com/search/repositories',\n                   params={'q': 'data_science+language:python'})\n\nfor item in response.json()['items'][:5]:\n    print (item['name'])\n\n```", "```py\nDataCamp\ndata-science-from-scratch\ndata-science-blogs\ngalaxy\ndata-scientist-roadmap\n\n```", "```py\nfrom requests.adapters import HTTPAdapter\nfrom requests.packages.urllib3.util.retry import Retry\n\nretry_strategy = Retry(\n    total=5,\n    status_forcelist=[500, 503, 504],\n    backoff_factor=1\n)\n\nretry_adapter = HTTPAdapter(max_retries=retry_strategy)\n\nhttp = requests.Session()\nhttp.mount(\"https://\", retry_adapter)\nhttp.mount(\"http://\", retry_adapter)\n\ndef get_all_pages(url, param=None, header=None):\n    output_json = []\n    response = http.get(url, params=param, headers=header)\n    if response.status_code == 200:\n        output_json = response.json()\n        if 'next' in response.links:\n            next_url = response.links['next']['url']\n            if (next_url is not None) and (handle_rate_limits(response)):\n                output_json += get_all_pages(next_url, param, header)\n    return output_json\n\n```", "```py\nimport tweepy\n\napp_api_key = 'YOUR_APP_KEY_HERE'\napp_api_secret_key = 'YOUR_APP_SECRET_HERE'\n\nauth = tweepy.AppAuthHandler(app_api_key, app_api_secret_key)\napi = tweepy.API(auth)\n\nprint ('API Host', api.host)\nprint ('API Version', api.api_root)\n\n```", "```py\nAPI Host api.twitter.com\nAPI Version /1.1\n\n```", "```py\nsearch_term = 'cryptocurrency'\n\ntweets = tweepy.Cursor(api.search,\n                       q=search_term,\n                       lang=\"en\").items(100)\n\nretrieved_tweets = [tweet._json for tweet in tweets]\ndf = pd.json_normalize(retrieved_tweets)\n\ndf[['text']].sample(3)\n\n```", "```py\napi = tweepy.API(auth,\n                 wait_on_rate_limit=True,\n                 wait_on_rate_limit_notify=True,\n                 retry_count=5,\n                 retry_delay=10)\n\nsearch_term = 'cryptocurrency OR crypto -filter:retweets'\n\ntweets = tweepy.Cursor(api.search,\n                       q=search_term,\n                       lang=\"en\",\n                       tweet_mode='extended',\n                       count=30).items(12000)\n\nretrieved_tweets = [tweet._json for tweet in tweets]\n\ndf = pd.json_normalize(retrieved_tweets)\nprint('Number of retrieved tweets ', len(df))\ndf[['created_at','full_text','entities.hashtags']].sample(2)\n\n```", "```py\nNumber of retrieved tweets  12000\n```", "```py\ndef extract_entities(entity_list):\n    entities = set()\n    if len(entity_list) != 0:\n        for item in entity_list:\n            for key,value in item.items():\n                if key == 'text':\n                    entities.add(value.lower())\n    return list(entities)\n\ndf['Entities'] = df['entities.hashtags'].apply(extract_entities)\npd.Series(np.concatenate(df['Entities'])).value_counts()[:25].plot(kind='barh')\n\n```", "```py\napi = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n\ntweets = tweepy.Cursor(api.user_timeline,\n                       screen_name='MercedesAMGF1',\n                       lang=\"en\",\n                       tweet_mode='extended',\n                       count=100).items(5000)\n\nretrieved_tweets = [tweet._json for tweet in tweets]\ndf = pd.io.json.json_normalize(retrieved_tweets)\nprint ('Number of retrieved tweets ', len(df))\n\n```", "```py\nNumber of retrieved tweets  3232\n\n```", "```py\ndef get_user_timeline(screen_name):\n    api = tweepy.API(auth,\n                     wait_on_rate_limit=True,\n                     wait_on_rate_limit_notify=True)\n    tweets = tweepy.Cursor(api.user_timeline,\n                           screen_name=screen_name,\n                           lang=\"en\",\n                           tweet_mode='extended',\n                           count=200).items()\n    retrieved_tweets = [tweet._json for tweet in tweets]\n    df = pd.io.json.json_normalize(retrieved_tweets)\n    df = df[~df['retweeted_status.id'].isna()]\n    return df\n\n```", "```py\ndf_mercedes = get_user_timeline('MercedesAMGF1')\nprint ('Number of Tweets from Mercedes', len(df_mercedes))\ndf_ferrari = get_user_timeline('ScuderiaFerrari')\nprint ('Number of Tweets from Ferrari', len(df_ferrari))\n\n```", "```py\nNumber of Tweets from Mercedes 180\nNumber of Tweets from Ferrari 203\n\n```", "```py\nfrom blueprints.exploration import wordcloud\n\nplt.figure()\nwordcloud(df_mercedes['full_text'],\n          max_words=100,\n          stopwords=df_mercedes.head(5).index)\n\nwordcloud(df_ferrari['full_text'],\n          max_words=100,\n          stopwords=df_ferrari.head(5).index)\n\n```", "```py\nfrom datetime import datetime\nimport math\n\nclass FileStreamListener(tweepy.StreamListener):\n\n    def __init__(self, max_tweets=math.inf):\n        self.num_tweets = 0\n        self.TWEETS_FILE_SIZE = 100\n        self.num_files = 0\n        self.tweets = []\n        self.max_tweets = max_tweets\n\n    def on_data(self, data):\n        while (self.num_files * self.TWEETS_FILE_SIZE < self.max_tweets):\n            self.tweets.append(json.loads(data))\n            self.num_tweets += 1\n            if (self.num_tweets < self.TWEETS_FILE_SIZE):\n                return True\n            else:\n                filename = 'Tweets_' + str(datetime.now().time()) + '.txt'\n                print (self.TWEETS_FILE_SIZE, 'Tweets saved to', filename)\n                file = open(filename, \"w\")\n                json.dump(self.tweets, file)\n                file.close()\n                self.num_files += 1\n                self.tweets = []\n                self.num_tweets = 0\n                return True\n        return False\n\n    def on_error(self, status_code):\n        if status_code == 420:\n            print ('Too many requests were made, please stagger requests')\n            return False\n        else:\n            print ('Error {}'.format(status_code))\n            return False\n\n```", "```py\nuser_access_token = 'YOUR_USER_ACCESS_TOKEN_HERE'\nuser_access_secret = 'YOUR_USER_ACCESS_SECRET_HERE'\n\nauth = tweepy.OAuthHandler(app_api_key, app_api_secret_key)\nauth.set_access_token(user_access_token, user_access_secret)\napi = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n\n```", "```py\nfileStreamListener = FileStreamListener(5000)\nfileStream = tweepy.Stream(auth=api.auth,\n                           listener=fileStreamListener,\n                           tweet_mode='extended')\nfileStream.filter(track=['cryptocurrency'])\n\n```", "```py\nimport wikipediaapi\n\nwiki_wiki = wikipediaapi.Wikipedia(\n        language='en',\n        extract_format=wikipediaapi.ExtractFormat.WIKI\n)\n\np_wiki = wiki_wiki.page('Cryptocurrency')\nprint (p_wiki.text[:200], '....')\n\n```", "```py\nA cryptocurrency (or crypto currency) is a digital asset designed to work\nas a medium of exchange wherein individual coin ownership records are stored\nin a ledger existing in a form of computerized da ....\n\n```"]