<html><head></head><body><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 13. Using Text Analytics in Production"><div class="chapter" id="ch-production">
<h1><span class="label">Chapter 13. </span>Using Text Analytics in Production</h1>

<p>We have introduced several blueprints so far and understood their application to multiple <a contenteditable="false" data-type="indexterm" data-primary="text analytics in production" data-secondary="use cases for" id="idm45634171103512"/><a contenteditable="false" data-type="indexterm" data-primary="use cases" data-secondary="for replication of models" data-secondary-sortas="replication of models" id="idm45634171102104"/>use cases. Any analysis or machine learning model is most valuable when it can be used easily by others. In this chapter, we will provide blueprints that will allow you to share the text classifier from one of our earlier chapters and also deploy to a cloud environment allowing anybody to make use of what we’ve built.</p>

<p>Consider that you used one of the blueprints in <a data-type="xref" href="ch10.xhtml#ch-embeddings">Chapter 10</a> in this book to analyze various car models using data from Reddit. If one of your colleagues was interested in doing the same analysis for the motorcycle industry, it should be simple to change the data source and reuse the code. In practice, this can prove much more difficult because your colleague will first have to set up an environment similar to the one you used by installing the same version of Python and all the required packages. It’s possible that they might be working on a different operating system where the installation steps are different. Or consider that the clients to whom you presented the analysis are extremely happy and come back three months later asking you to cover many more industries. Now you have to repeat the same analysis but ensure that the code and environment remain the same. The volume of data for this analysis could be much larger, and your system resources may not be sufficient enough, prompting a move to use cloud computing resources. You would have to go through the installation steps on a cloud provider, and this can quickly become time-consuming.</p>

<section data-type="sect1" data-pdf-bookmark="What You’ll Learn and What We’ll Build"><div class="sect1" id="idm45634171097288">
<h1>What You’ll Learn and What We’ll Build</h1>


<p>Often what <a contenteditable="false" data-type="indexterm" data-primary="text analytics in production" data-secondary="about" id="idm45634171096152"/>happens is that you are able to produce excellent results, but they remain unusable because other colleagues who want to use them are unable to rerun the code and reproduce the results. In this chapter, we will show you some techniques that can ensure that your analysis or algorithm can be easily repeated by anyone else, including yourself at a later stage. What if we are able to make it even easier for others to use the output of our analysis? This removes an additional barrier and increases the accessibility of our results. We will show you how you can deploy your machine learning model as a simple REST API that allows anyone to use the predictions from your model in their own work or applications. Finally, we will show you how to make use of cloud infrastructure for faster runtimes or to serve multiple applications and users. Since most <a contenteditable="false" data-type="indexterm" data-primary="Linux" id="idm45634171093768"/>production servers and services run Linux, this chapter includes a lot of executable commands and instructions that run best in a Linux shell or terminal. However, they should work <a contenteditable="false" data-type="indexterm" data-primary="Windows PowerShell" id="idm45634171092344"/>just as well in Windows PowerShell.</p>
</div></section>

<section data-type="sect1" class="blueprint" data-pdf-bookmark="Blueprint: Using Conda to Create Reproducible Python Environments"><div class="sect1" id="idm45634171090856">
<h1>Blueprint: Using Conda to Create Reproducible Python Environments</h1>

<p>The blueprints introduced in this book use Python and the ecosystem of packages to accomplish several text analytics tasks. As with any programming language, <a href="https://python.org/downloads">Python</a> has frequent updates and many supported versions. In addition, commonly used packages like Pandas, NumPy, and SciPy also have regular release cycles when they upgrade to a new version. While the maintainers try to ensure that newer versions are backward compatible, there is a risk that an analysis you completed last year will no longer be able to run with the latest version of Python. Your blueprint might have used a method that is deprecated in the latest version of a library, and this would make your analysis nonreproducible without knowing the version of the library used.</p>

<p>Let’s suppose that you share a blueprint with your colleague in the form of a Jupyter notebook or a Python module; one of the <a contenteditable="false" data-type="indexterm" data-primary="ModuleNotFoundError" id="idm45634171086472"/>common errors they might face when trying to run is as shown here:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="kn">import</code> <code class="nn">spacy</code>
</pre>

<p><code>Out:</code></p>

<pre data-type="programlisting">
---------------------------------------------------------------------------
ModuleNotFoundError                       Traceback (most recent call last)
&lt;ipython-input-1-76a01d9c502b&gt; in &lt;module&gt;
----&gt; 1 import spacy
ModuleNotFoundError: No module named 'spacy'
</pre>

<p>In most cases, <code>ModuleNotFoundError</code> can be easily resolved by manually installing the required package using the command <code><strong>pip install &lt;module_name&gt;</strong></code>. But imagine having to do this for every nonstandard package! This command also installs the latest version, which might not be the one you originally used. As a result, the <a contenteditable="false" data-type="indexterm" data-primary="Miniconda" id="ch13_term1"/><a contenteditable="false" data-type="indexterm" data-primary="conda for reproducing Python environments" id="ch13_term2"/><a contenteditable="false" data-type="indexterm" data-primary="text analytics in production" data-secondary="with conda for reproducible Python environments" data-secondary-sortas="conda for reproducible Python environments" id="ch13_term4"/>best way to ensure reproducibility is to have a standardized way of sharing the Python environment that was used to run the analysis. We make use of the conda package manager along with the Miniconda distribution of Python to solve this problem.</p>

<div data-type="note" epub:type="note"><h6>Note</h6>
<p>There are several ways to <a contenteditable="false" data-type="indexterm" data-primary="pip Python package installer" id="idm45634171074728"/>solve the problem of creating and sharing Python environments, and conda is just one of them. <a href="https://oreil.ly/Dut6o">pip</a> is the standard Python package installer that is included with Python and is widely used to install Python packages. <a href="https://oreil.ly/k5m6A">venv</a> can be used to create virtual environments where each environment can have its own version of Python and set of installed packages. Conda combines the functionality of a package installer and environment manager and therefore is our preferred option. It’s important to distinguish conda from the Anaconda/Miniconda distributions. These distributions include Python and conda along with essential packages required for working with data. While conda can be installed directly with pip, the easiest way is to install Miniconda, which is a small bootstrap version that contains conda, Python, and some essential packages they depend on.</p>
</div>

<p>First, we must install the Miniconda distribution with the <a href="https://oreil.ly/GZ4b-">following steps</a>. This will create a base installation containing just Python, conda, and some essential packages like <code>pip</code>, <code>zlib</code>, etc. We can now create separate environments for each project that contains only the packages we need and are isolated from other such environments. This is useful since any changes you make such as installing additional packages <span class="keep-together">or upgrading</span> to a different Python version does not impact any other project or application as they use their own environment. We can do so by <a contenteditable="false" data-type="indexterm" data-primary="conda commands" id="ch13_term5"/>using the following <span class="keep-together">command:</span></p>

<pre data-type="programlisting">conda create -n env_name [list_of_packages]</pre>

<p>Executing the previous command will create a new Python environment with the default version that was available when Miniconda was installed the first time. Let’s create our environment called <code>blueprints</code> where we explicitly specify the version of Python and the list of additional packages that we would like to install as follows:</p>

<!-- <pre data-type='programlisting' data-code-language="bash"><code>conda create -n blueprints numpy pandas scikit-learn notebook python=3.8</code>:</pre> -->

<pre data-type="programlisting">
$ conda create -n blueprints numpy pandas scikit-learn notebook python=3.8
Collecting package metadata (current_repodata.json): - done
Solving environment: \ done
 Package Plan
  environment location: /home/user/miniconda3/envs/blueprints
  added / updated specs:
    - notebook
    - numpy
    - pandas
    - python=3.8
    - scikit-learn

The following packages will be downloaded:

    package                    |            build
    ---------------------------|-----------------
    blas-1.0                   |              mkl           6 KB
    intel-openmp-2020.1        |              217         780 KB
    joblib-0.16.0              |             py_0         210 KB
    libgfortran-ng-7.3.0       |       hdf63c60_0        1006 KB
    mkl-2020.1                 |              217       129.0 MB
    mkl-service-2.3.0          |   py37he904b0f_0         218 KB
    mkl_fft-1.1.0              |   py37h23d657b_0         143 KB
    mkl_random-1.1.1           |   py37h0573a6f_0         322 KB
    numpy-1.18.5               |   py37ha1c710e_0           5 KB
    numpy-base-1.18.5          |   py37hde5b4d6_0         4.1 MB
    pandas-1.0.5               |   py37h0573a6f_0         7.8 MB
    pytz-2020.1                |             py_0         184 KB
    scikit-learn-0.23.1        |   py37h423224d_0         5.0 MB
    scipy-1.5.0                |   py37h0b6359f_0        14.4 MB
    threadpoolctl-2.1.0        |     pyh5ca1d4c_0          17 KB
    ------------------------------------------------------------
                                           Total:       163.1 MB

The following NEW packages will be INSTALLED:

  _libgcc_mutex      pkgs/main/linux-64::_libgcc_mutex-0.1-main
  attrs              pkgs/main/noarch::attrs-19.3.0-py_0
  backcall           pkgs/main/noarch::backcall-0.2.0-py_0
  blas               pkgs/main/linux-64::blas-1.0-mkl
  bleach             pkgs/main/noarch::bleach-3.1.5-py_0
  ca-certificates    pkgs/main/linux-64::ca-certificates-2020.6.24-0

(Output truncated)
</pre>

<p>Once the command has been executed, you can activate it by executing <code><strong>conda activate &lt;env_name&gt;</strong></code>, and you will notice that the command prompt is prefixed with the name of the environment. You can further verify that the version of Python is the same as you specified:</p>

<pre data-type="programlisting">
$ conda activate blueprints
(blueprints) $ python --version
Python 3.8
</pre>

<p>You can see the list of all environments on your system by using the command <code><strong>conda env list</strong></code>, as shown next. The output will include the base environment, which is the default environment created with the installation of Miniconda. An asterisk against a particular environment indicates the currently active one, in our case the environment we just created. Please make sure that you continue to use this environment when you work on your blueprint:</p>

<pre data-type="programlisting">
(blueprints) $ conda env list
# conda environments:
#
base                     /home/user/miniconda3
blueprints            *  /home/user/miniconda3/envs/blueprints
</pre>

<p><code>conda</code> ensures that each environment can have its own versions of the same package, but this could come at the cost of increased storage since the same version of each package will be used in more than one environment. This is mitigated to a certain extent with the use of hard links, but it may not work in cases where a package uses <a href="https://oreil.ly/bN8Dl">hard-coded paths</a>. However, we recommend creating another environment when you switch projects. But it is a good practice to remove unused environments using the command <code><strong>conda remove --name &lt;env_name&gt; --all</strong></code>.</p>

<p>The advantage of this approach is that when you want to share the code with someone else, you can specify the environment in which it should run. You can export the environment as a YAML file using the command <code><strong>conda env export &gt; environment.yml</strong></code>. Ensure that you are in the desired environment (by running <code><strong>conda activate &lt;environment_name&gt;</strong></code>) before running this command:</p>

<pre data-type="programlisting">
(blueprints) $ conda env export &gt; environment.yml
(blueprints) $ cat environment.yml
name: blueprints
channels:
  - defaults
dependencies:
  - _libgcc_mutex=0.1=main
  - attrs=19.3.0=py_0
  - backcall=0.2.0=py_0
  - blas=1.0=mkl
  - bleach=3.1.5=py_0
  - ca-certificates=2020.6.24=0
  - certifi=2020.6.20=py37_0
  - decorator=4.4.2=py_0
  - defusedxml=0.6.0=py_0
  - entrypoints=0.3=py37_0
  - importlib-metadata=1.7.0=py37_0
  - importlib_metadata=1.7.0=0
  - intel-openmp=2020.1=217
  - ipykernel=5.3.0=py37h5ca1d4c_0
(output truncated)
</pre>

<p>As shown in the <a contenteditable="false" data-type="indexterm" data-primary="environment.yml file" id="idm45634171029912"/>output, the <code>environment.yml</code> file creates a listing of all the packages and their dependencies used in the environment. This file can be used by anyone to re-create the same environment by running the command <code><strong>conda env create -f environment.yml</strong></code>. However, this method can have cross-platform limitations since the dependencies listed in the YAML file are specific to the platform. So if you were working on a Windows system and exported the YAML file, it may not necessarily work <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch13_term5" id="idm45634171027176"/>on a macOS system.</p>

<!-- <p><strong>Conda Environments Across Platforms</strong></p> -->

<p>This is because some of the dependencies required by a Python package are platform-dependent. For instance, the <a href="https://oreil.ly/ND7_H">Intel MKL optimizations</a> are specific to a certain architecture and can be replaced with the <a href="http://openblas.net">OpenBLAS</a> library. To provide a generic environment file, we can use the command <code><strong>conda env export --from-history &gt; environment.yml</strong></code>, which generates a listing of only the packages that were explicitly requested by you. You can see the following output of running this command, which lists only the packages we installed when creating the environment. Contrast this with the previous environment file that also listed packages like <code>attrs</code> and <code>backcall</code>, which are part of the conda environment but not requested by us. When such a YAML file is used to create an environment on a new platform, the default packages and their platform-specific dependencies will be identified and installed automatically by conda. In addition, the packages that we explicitly specified and their dependencies will be installed:</p>

<pre data-type="programlisting">
(blueprints) $ conda env export --from-history &gt; environment.yml
(blueprints) $ cat environment.yml
name: blueprints
channels:
  - defaults
dependencies:
  - scikit-learn
  - pandas
  - notebook
  - python=3.8
  - numpy
prefix: /home/user/miniconda3/envs/blueprints
</pre>

<p>The disadvantage of using the <code>--from-history</code> option is that the created environment is not a replica of the original environment since the base packages and dependencies are platform specific and hence different. If the platform where this environment is to be used is the same, then we do not recommend <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch13_term1" id="idm45634171019032"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch13_term2" id="idm45634171017656"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch13_term4" id="idm45634171016280"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch13_term5" id="idm45634171014904"/>using this option.</p>
</div></section>

<section data-type="sect1" class="blueprint" data-pdf-bookmark="Blueprint: Using Containers to Create Reproducible Environments"><div class="sect1" id="idm45634171089912">
<h1>Blueprint: Using Containers to Create Reproducible Environments</h1>

<p>While a package manager like conda helps in installing multiple packages and managing dependencies, there are <a contenteditable="false" data-type="indexterm" data-primary="containers for creating reproducible environments" id="ch13_term6"/><a contenteditable="false" data-type="indexterm" data-primary="text analytics in production" data-secondary="with containers for creating reproducible environments" data-secondary-sortas="containers for creating reproducible environments" id="ch13_term7"/>several platform-dependent binaries that can still hinder reproducibility. To make things simpler, we make use of a layer of abstraction called <em>containers</em>. The name is derived from the shipping industry, where standard-sized shipping containers are used to transport all kinds of goods by ships, trucks, and rail. Regardless of the type of items or the mode of transport, the shipping container ensures that anyone adhering to that standard can transport those items. In a similar fashion, we use a <a contenteditable="false" data-type="indexterm" data-primary="Docker containers" id="ch13_term8"/>Docker container to standardize the environment we work in and guarantee that an identical environment is re-created every time regardless of where it runs or who runs it. <a href="https://docker.com">Docker</a> is one of the most popular tools that enables this functionality, and we will make use of it in this blueprint. <a data-type="xref" href="#fig-docker-workflow">Figure 13-1</a> shows a high-level overview of how Docker works.</p>

<figure><div id="fig-docker-workflow" class="figure"><img src="Images/btap_1301.jpg" width="787" height="584"/>
  <h6><span class="label">Figure 13-1. </span>Workflow of Docker.</h6>
  </div></figure>

<p>We need to start by <a contenteditable="false" data-type="indexterm" data-primary="Docker, installation of" id="idm45634171001048"/>installing Docker from the <a href="https://oreil.ly/CJWKF">download link</a>. Once it has been set up, please run <code>sudo docker run hello-world</code> from the command line to test that everything has been set up correctly, and you should see the output as shown. Please note that the Docker daemon binds to a Unix socket that is owned by the root user, hence the need to run all commands with <code>sudo</code>. If you are unable to provide root access, there is an <a href="https://oreil.ly/X7lzt">experimental version</a> of Docker that you can also try:</p>

<pre data-type="programlisting">
$ sudo docker run hello-world

Hello from Docker!
This message shows that your installation appears to be working correctly.

To generate this message, Docker took the following steps:
 1. The Docker client contacted the Docker daemon.
 2. The Docker daemon pulled the "hello-world" image from the Docker Hub.
    (amd64)
 3. The Docker daemon created a new container from that image which runs the
    executable that produces the output you are currently reading.
 4. The Docker daemon streamed that output to the Docker client, which sent it
    to your terminal.

To try something more ambitious, you can run an Ubuntu container with:
 $ docker run -it ubuntu bash

Share images, automate workflows, and more with a free Docker ID:
 https://hub.docker.com/

For more examples and ideas, visit:
 https://docs.docker.com/get-started/
</pre>



<p>We can draw an analogy between building a Docker container and purchasing a car. We start by choosing from one of the preconfigured options. These configurations already have some components selected, such as the type of engine (displacement, fuel-type), safety features, level of equipment, etc. We can customize many of these components, for example, upgrade to a more fuel-efficient engine or add additional components like a navigation system or heated seats. At the end, we decide on our preferred configuration and order the car. In a similar way, we specify the <a contenteditable="false" data-type="indexterm" data-primary="Dockerfile" id="ch13_term9"/><a contenteditable="false" data-type="indexterm" data-primary="Docker images" id="ch13_term18"/>configuration of the environment we want to create in the <em>Dockerfile</em>. These are described in the form of a set of instructions, executed in a sequential manner resulting in the creation of a <em>Docker image</em>. A Docker image is like the preferred car configuration that can be created based on our Dockerfile. All Docker images are extensible, so instead of defining all steps, we could extend an existing Docker image and customize it by adding specific steps that we would like. The final step of running a Docker image results in the creation of a <em>Docker container</em>, which is like the car with your preferred configuration delivered to you. In this case, it is a full-fledged environment including an operating system and additional utilities and packages as specified in the Dockerfile. It runs on the hardware and uses the interfaces provided by the host system but is completely isolated from it. In effect, it is a minimal version of a server running the way you designed it. Each Docker container instantiated from the same image will be the same regardless of which host system it is running on. This is powerful as it allows you to encapsulate your analysis and environment and run it on your laptop, in the cloud, or on your organization’s server and expect the same behavior.</p>



<p>We are going to create a Docker image with the same Python environment as used in our analysis so that anyone else can reproduce our analysis by pulling the image and instantiating it as a container. While we can start by specifying our Docker image from scratch, it would be preferable to start with an existing image and customize certain parts of it to create our version. Such an image is referred to as a <em>parent image</em>. A good place to <a contenteditable="false" data-type="indexterm" data-primary="Docker Hub registry" id="idm45634170987880"/>search for parent images is the <a href="https://hub.docker.com">Docker Hub registry</a>, which is a public repository containing prebuilt Docker images. You will find officially supported images like the <a href="https://oreil.ly/kKLXU">Jupyter Data Science</a> notebook as well as <a href="https://oreil.ly/K5SMy">user-created images</a> like the one we have created for <a data-type="xref" href="ch09.xhtml#ch-summarization">Chapter 9</a> that can be accessed here. Every image in the Docker repository can also be used as is to run containers. You can search for images using the <code><strong>sudo docker search</strong></code> command and add arguments to format results, as shown here where we search for available Miniconda images:</p>

<pre data-type="programlisting">
$ sudo docker search miniconda
NAME                                        STARS
continuumio/miniconda3                      218
continuumio/miniconda                       77
conda/miniconda3                            35
conda/miniconda3-centos7                    7
yamitzky/miniconda-neologd                  3
conda/miniconda2                            2
atavares/miniconda-rocker-geospatial        2
</pre>

<p>We see that there is an image for Miniconda3 that would be a good starting point for our own Dockerfile. Note that all Dockerfiles have to start with the <code>FROM</code> keyword specifying which image they are deriving from. If you are specifying a Dockerfile from the start, then you would use the <code>FROM scratch</code> keyword. The details of the <a href="https://oreil.ly/ddYff">Miniconda image and the Dockerfile</a> show how this image derives from a Debian parent image and only adds additional steps to install and set up the conda package manager. When using a parent Docker image, it’s important to check that it’s from a trusted source. Docker Hub provides additional criteria like “Official Images” that can be helpful in identifying an official source.</p>

<p>Let’s walk through the steps defined in our Dockerfile. We start with the Miniconda3 image and then add a step to create our custom environment. We use the <code>ARG</code> instruction to specify the argument for the name of our conda environment (blueprints). We then <a contenteditable="false" data-type="indexterm" data-primary="environment.yml file" id="idm45634170977608"/>use <code>ADD</code> to copy the <code>environment.yml</code> file from the build context to the image. Finally, we create the conda environment by providing the <code><strong>conda create</strong></code> command as an argument to <code>RUN</code>:</p>

<pre data-type="programlisting">
FROM continuumio/miniconda3

# Add environment.yml to the build context and create the environment
ARG conda_env=blueprints
ADD environment.yml /tmp/environment.yml
RUN conda env create -f /tmp/environment.yml
</pre>

<p>In the next set of steps, we want to ensure that the environment is activated in the container. Therefore, we add it to the end of the <code>.bashrc</code> script, which will always run when the container starts. We also update the <code>PATH</code> environment variable using the <code>ENV</code> instruction to ensure that the conda environment is the version of Python used everywhere within the container:</p>

<pre data-type="programlisting">
# Activating the environment and starting the jupyter notebook
RUN echo "source activate ${conda_env}" &gt; ~/.bashrc
ENV PATH /opt/conda/envs/${conda_env}/bin:$PATH
</pre>

<p>In the final step, we want to automatically start a Jupyter notebook that will allow the users of this Docker container to run the analysis in an interactive fashion. We use the <code>ENTRYPOINT</code> instruction, which is used to configure a container that will run as an executable. There can be only one such instruction in a Dockerfile (if there are multiple, only the last one will be valid), and it will be the last command to run when a container comes up and is typically used to start a server like the Jupyter notebook that we want to run. We specify additional arguments to run the server on the IP address of the container itself (<code>0.0.0.0</code>), on a particular port (<code>8888</code>), as the root user (<code>--allow-root</code>), and not open a browser by default (<code>--no-browser</code>). When the container starts, we don’t want it to open the Jupyter server in its browser. Instead, we will attach the host machine to this container using the specified port and access it via the browser there:</p>

<pre data-type="programlisting">
# Start jupyter server on container
EXPOSE 8888
ENTRYPOINT ["jupyter","notebook","--ip=0.0.0.0", \
      "--port=8888","--allow-root","--no-browser"]
</pre>

<p>We use the <code>docker build</code> command to create the image from our Dockerfile. We specify the name of our image with the <code>-t</code> parameter and add a username followed by the name of the image. This is useful in identifying our image when we want to refer to it later. It is not mandatory to specify a username, but we will see later why this is useful. The Dockerfile to be used while building the image is specified with the <code>-f</code> parameter. If nothing is specified, then Docker will pick the file named <em>Dockerfile</em> in the directory specified by the argument <code>PATH</code>. The <code>PATH</code> argument also specifies where to find the files for the “context” of the build on the Docker daemon. All the files in this directory are packaged with <code>tar</code> and sent to the daemon during the build process. This must include all the files and artifacts that have to be added to the image, e.g., the <code>environment.yml</code> file, which will be copied to the image to create the conda environment.</p>

<pre data-type="programlisting">docker build -t username/docker_project -f Dockerfile [PATH]</pre>

<p>On executing this command, the Docker daemon starts creating an image by running the steps specified in the Dockerfile. Typically, you would execute the command in the same directory that already contains all the files and the Dockerfile as well. We specify the <code>PATH</code> argument using <code>.</code> referring to the current directory:</p>

<pre data-type="programlisting">
$ sudo docker build -t textblueprints/ch13:v1 .
Sending build context to Docker daemon  5.363MB
Step 1/8 : FROM continuumio/miniconda3
 ---&gt; b4adc22212f1
Step 2/8 : ARG conda_env=blueprints
 ---&gt; 959ed0c16483
Step 3/8 : ADD environment.yml /tmp/environment.yml
 ---&gt; 60e039e09fa7
Step 4/8 : RUN conda env create -f /tmp/environment.yml
 ---&gt; Running in 85d2f149820b
Collecting package metadata (repodata.json): ...working... done
Solving environment: ...working... done

Downloading and Extracting Packages

(output truncated)

Removing intermediate container 85d2f149820b
Step 5/8 : RUN echo "source activate ${conda_env}" &gt; ~/.bashrc
 ---&gt; e0ed2b448211
Step 6/8 : ENV PATH /opt/conda/envs/${conda_env}/bin:$PATH
 ---&gt; 7068395ce2cf
Step 7/8 : EXPOSE 8888
 ---&gt; Running in f78ac4aa0569
Removing intermediate container f78ac4aa0569
 ---&gt; 06cfff710f8e
Step 8/8 : ENTRYPOINT ["jupyter","notebook","--ip=0.0.0.0",
                       "--port=8888","--allow-root","--no-browser"]
 ---&gt; Running in 87852de682f4
Removing intermediate container 87852de682f4
 ---&gt; 2b45bb18c071
Successfully built 2b45bb18c071
Successfully tagged textblueprints/ch13:v1
</pre>

<p>After the build <a contenteditable="false" data-type="indexterm" data-primary="Docker commands" id="ch13_term41"/>completes, you can check whether the image was created successfully by running the command <code><strong>sudo docker images</strong></code>. You will notice that <code>continuumio/miniconda3</code> image has been downloaded, and in addition, the image specified with your username and <code>docker_project</code> has also been created. Building a Docker will take longer the first time since the parent images have to be downloaded, but subsequent changes and rebuilds will <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch13_term9" id="idm45634170953240"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch13_term18" id="idm45634170951864"/>be much faster:</p>

<pre data-type="programlisting">
$ sudo docker images
REPOSITORY                          TAG                 IMAGE ID
textblueprints/ch13                 v1                  83a05579afe6
jupyter/minimal-notebook            latest              d94723ae86d1
continuumio/miniconda3              latest              b4adc22212f1
hello-world                         latest              bf756fb1ae65
</pre>

<p>We can create a running instance of this environment, also called <em>container</em>, by <span class="keep-together">running</span>:</p>

<pre data-type="programlisting">docker run -p host_port:container_port username/docker_project:tag_name</pre>

<p>The <code>-p</code> argument allows port forwarding, essentially sending any requests received on the <code>host_port</code> to the <code>container_port</code>. By default, the Jupyter server can only access the files and directories within the container. However, we would like to access the Jupyter notebooks and code files present in a local directory from the Jupyter server running inside the container. We can attach a local directory to the container as a volume by using <code>-v host_volume:container_volume</code>, which will create a new directory within the container pointing to a local directory. This ensures that any changes made to the Jupyter notebooks are not lost when the container shuts down. This is the recommended approach to work with files locally but using a Docker container for the reproducible environment. Let’s start our Docker container by running the following command:</p>

<pre data-type="programlisting"><strong>sudo docker run -p 5000:8888 -v \
/home/user/text-blueprints/ch13/:/work textblueprints/ch13:v1</strong></pre>

<p><code>Out:</code></p>

<pre data-type="programlisting">
[NotebookApp] Writing notebook server cookie secret to
/root/.local/share/jupyter/runtime/notebook_cookie_secret
[NotebookApp] Serving notebooks from local directory: /
[NotebookApp] The Jupyter Notebook is running at:
[NotebookApp] http://aaef990b90a3:8888/?token=xxxxxx
[NotebookApp]  or http://127.0.0.1:8888/?token=xxxxxx
[NotebookApp] Use Control-C to stop this server and shut down all kernels
(twice to skip confirmation).
[NotebookApp]

    To access the notebook, open this file in a browser:
        file:///root/.local/share/jupyter/runtime/nbserver-1-open.html
    Or copy and paste one of these URLs:
        http://aaef990b90a3:8888/?token=xxxxxx
     or http://127.0.0.1:8888/?token=xxxxxx
</pre>

<p>The logs you see now are actually the logs of the Jupyter server starting in port 8888 within the container. Since we have mapped the host port 5000, you can copy the URL and only replace the port number to 5000 to access the Jupyter server. You will also find here a directory called <code>work</code>, which should contain all the files from the local directory that was mapped. You can also check the status of all running containers by running the command <code><strong>sudo docker container ps</strong></code>. We can also specify the name for each running container by using the <code>--name argument</code>, and if this is not used, the Docker daemon will assign a randomly created one, as you see here:</p>

<pre data-type="programlisting">
$ sudo docker container ls
CONTAINER ID        IMAGE                    STATUS              NAMES
862e5b0570fe        textblueprints/ch13:v1   Up About a minute   musing_chaum
</pre>

<p>If you quit the terminal window where you ran this command, then the container will also be shut down. To run it in a detached mode, just add the <code>-d</code> option to the run command. When the container starts, it will print the container ID of the started container, and you can monitor the logs using <code>sudo docker logs &lt;container-id&gt;</code>. We have reproduced the complete environment used to run our analysis in this Docker container, and in the next blueprint, let’s see the best techniques to share it.</p>


<p>The easiest way to share this image with anyone is by <a contenteditable="false" data-type="indexterm" data-primary="Docker Hub registry" id="idm45634170935256"/>pushing this to the Docker Hub registry. You can <a href="https://oreil.ly/vyi-2">sign up</a> for a free account. Docker Hub is a public repository for Docker images, and each image is uniquely identified by the username, the name of the image, and a tag. For example, the <code>miniconda3</code> package that we used as our parent image is identified as <code>continuumio/miniconda3:latest</code>, and any images that you share will be identified with your username. Therefore, when we built our image earlier, the username we specified must have been the same as the one used to log in to Docker Hub. Once you have created your credentials, you can click Create a Repository and choose a name and provide a description for your repository. In our case we created a repository called "<code>ch13</code>" that will contain a Docker image for this chapter. Once done, you can log in using the command <code><strong>sudo docker login</strong></code> and enter your username and password. For added security, please follow the <a href="https://oreil.ly/m95HO">instructions</a> to securely store your password.</p>


<div data-type="note" epub:type="note"><h6>Note</h6>
  <p>By default, during the build process of a Docker image, all of the directories and files present in the <code>PATH</code> argument are part of the build context. In a previous command, we indicated the path to be the current directory using the <code>.</code> symbol. This is not necessary since we need to include only the selected list of files that are needed for the build and later the container. For instance, we need <code>environment.yml</code> but not the Jupyter notebook (<code>.ipynb</code>) file. It’s important to <a contenteditable="false" data-type="indexterm" data-primary=".dockerignore file" data-primary-sortas="dockerignore file" id="idm45634170926776"/>specify the list of excluded files in the <code>.dockerignore</code> file to ensure that unwanted files do not automatically get added to the container. Our <code>.dockerignore</code> file is as shown here:</p>

  <pre data-type="programlisting">
  .git
  .cache
  figures
  **/*.html
  **/*.ipynb
  **/*.css
  </pre>

  <p>Another thing to ensure is that the <code>host_port</code> (specified as 5000 in the blueprint) is open and not used by any other application on your system. Ideally, you must use a port number between 1024–49151 as these are <a href="https://oreil.ly/F-Qps">user ports</a>, but you can also check this easily by running the command <code><strong>sudo ss -tulw</strong></code>, which will provide the list of used ports.</p>
  </div>

<p>The next step is to tag the image that you would like to share with a <code>tag_name</code> to identify what it contains. In our case, we tag the image with v1 to signify that it is the first version for this chapter. We run the command <code>sudo docker tag 2b45bb18c071 textblueprints/ch13:v1</code>, where 2b45bb18c071 is the image ID. We can push our file now with the command <code>sudo docker push textblueprints/ch13</code>. Now anyone who wants to run your project can simply run the command <code>docker pull your_username/docker_project:tag_name</code> to create the same environment as you, irrespective of the system they might be personally working on. As an example, you can start working on blueprints in <a data-type="xref" href="ch09.xhtml#ch-summarization">Chapter 9</a> by simply running the command <code>docker pull textblueprints/ch09:v1</code>. You can <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch13_term41" id="idm45634170916568"/>then attach the volume of the directory containing the cloned repository. Docker Hub is a popular public registry and configured as default with Docker, but each cloud provider also has their own version, and many organizations set up private registries for use within their internal applications and teams.</p>

<p>When working with conda environments with multiple scientific computing packages, <a contenteditable="false" data-type="indexterm" data-primary="Docker images" id="idm45634170914488"/>Docker images can get large and therefore create a strain on bandwidth while pushing to Docker Hub. A much more efficient way is to include the <a contenteditable="false" data-type="indexterm" data-primary="Dockerfile" id="idm45634170913064"/>Dockerfile in the base path of your repository. For example, the GitHub repo containing the code for this chapter contains a Dockerfile, which can be used to create the exact environment required to run the code. This blueprint easily allows you to move an analysis from your local system to a cloud machine with additional resources by re-creating the same working environment. This is especially useful when the size of the data increases or an analysis takes too <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch13_term6" id="idm45634170911352"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch13_term7" id="idm45634170909976"/>long to finish.</p>
</div></section>

<section data-type="sect1" class="blueprint" data-pdf-bookmark="Blueprint: Creating a REST API for Your Text Analytics Model"><div class="sect1" id="idm45634171012488">
<h1>Blueprint: Creating a REST API for Your Text Analytics Model</h1>

<p>Let’s say you used the blueprint provided in <a data-type="xref" href="ch11.xhtml#ch-sentiment">Chapter 11</a> to analyze <a contenteditable="false" data-type="indexterm" data-primary="text analytics in production" data-secondary="use cases for" id="idm45634170904680"/><a contenteditable="false" data-type="indexterm" data-primary="use cases" data-secondary="for replication of models" data-secondary-sortas="replication of models" id="idm45634170903256"/>the sentiment of customer support tickets in your organization. Your company is running a campaign to improve customer satisfaction where they would like to provide vouchers to unhappy customers. A colleague from the tech team reaches out to you for help with automating this campaign. While they can pull the Docker container and reproduce your analysis, they would prefer a simpler method where they provide the text of the support ticket and get a response of whether this is an unhappy customer. By <a contenteditable="false" data-type="indexterm" data-primary="REST (Representational State Transfer) API" data-secondary="creating" id="ch13_term12"/><a contenteditable="false" data-type="indexterm" data-primary="text analytics in production" data-secondary="by creating REST API" data-secondary-sortas="creating REST API for model" id="ch13_term13"/>encapsulating our analysis in a REST API, we can create a simple method that is accessible to anyone without them having to rerun the blueprint. They don’t even necessarily need to know Python since a REST API can be called from any language. In <a data-type="xref" href="ch02.xhtml#ch-api">Chapter 2</a>, we made use of REST APIs provided by popular websites to extract data, whereas in this blueprint we are going to create our own.</p>

<p>We will make use of the following three components to host our REST API:</p>

<ul>
	<li>FastAPI: A fast <a contenteditable="false" data-type="indexterm" data-primary="FastAPI" id="ch13_term11"/>web framework for building APIs</li>
	<li>Gunicorn: A Web <a contenteditable="false" data-type="indexterm" data-primary="Gunicorn WSGI server" id="idm45634170892584"/>Service Gateway Interface server that handles all the incoming requests</li>
	<li>Docker: Extending the Docker container that we used in the <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch13_term8" id="idm45634170890824"/>previous blueprint</li>
</ul>

<p>Let’s create a new folder called <em>app</em> where we will place all the code that we require in order to serve sentiment predictions. It will follow the directory structure and contain files as shown next. <em>main.py</em> is where we will create the FastAPI app and the sentiment prediction method, and <em>preprocessing.py</em> is where our helper functions are included. The <em>models</em> directory contains the trained models we need to use to calculate our predictions, in our case the <code>sentiment_vectorizer</code> and <code>sentiment_classification</code>. Finally, we have the Dockerfile, <em>environment.yml</em>, and <em>start_script.sh</em>, which will be used to deploy our REST API:</p>

<pre data-type="programlisting">
├── app
│   ├── main.py
│   ├── Dockerfile
│   ├── environment.yml
│   ├── models
│   │   ├── sentiment_classification.pickle
│   │   └── sentiment_vectorizer.pickle
│   ├── preprocessing.py
│   └── start_script.sh
</pre>

<p><a href="https://oreil.ly/fastapi">FastAPI</a> is a fast Python framework used to build APIs. It is capable of redirecting requests from a web server to specific functions defined in Python. It also takes care of validating the incoming requests against specified schema and is useful for creating a simple REST API. We will encapsulate the predict function of the model we trained in <a data-type="xref" href="ch11.xhtml#ch-sentiment">Chapter 11</a> in this API. Let’s walk through the code in the file <em>main.py</em> step-by-step and explain how it works. You can install FastAPI by running <code>pip install fastapi</code> and Gunicorn by running <code>pip install gunicorn</code>.</p>

<p>Once FastAPI is installed, we can create an app using the following code:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="kn">from</code> <code class="nn">fastapi</code> <code class="kn">import</code> <code class="n">FastAPI</code>
<code class="n">app</code> <code class="o">=</code> <code class="n">FastAPI</code><code class="p">()</code>
</pre>

<p>The FastAPI library runs this app using the included web server and can route requests received at an endpoint to a method in the Python file. This is specified by adding the <code>@app.post</code> attribute at the start of the function definition. We specify the endpoint to be <em>api/v1/sentiment</em>, the first version of our Sentiment API, which accepts HTTP POST requests. An API can <a contenteditable="false" data-type="indexterm" data-primary="backups" id="idm45634170870696"/><a contenteditable="false" data-type="indexterm" data-primary="copies for traceability" id="idm45634170869688"/>evolve over time with changes to functionality, and it’s useful to separate them into different versions to ensure that users of the older version are not affected:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="k">class</code> <code class="nc">Sentiment</code><code class="p">(</code><code class="n">Enum</code><code class="p">):</code>
    <code class="n">POSITIVE</code> <code class="o">=</code> <code class="mi">1</code>
    <code class="n">NEGATIVE</code> <code class="o">=</code> <code class="mi">0</code>

<code class="nd">@app.post</code><code class="p">(</code><code class="s2">"/api/v1/sentiment"</code><code class="p">,</code> <code class="n">response_model</code><code class="o">=</code><code class="n">Review</code><code class="p">)</code>
<code class="k">def</code> <code class="nf">predict</code><code class="p">(</code><code class="n">review</code><code class="p">:</code> <code class="n">Review</code><code class="p">,</code> <code class="n">model</code> <code class="o">=</code> <code class="n">Depends</code><code class="p">(</code><code class="n">load_model</code><code class="p">())):</code>
    <code class="n">text_clean</code> <code class="o">=</code> <code class="n">preprocessing</code><code class="o">.</code><code class="n">clean</code><code class="p">(</code><code class="n">review</code><code class="o">.</code><code class="n">text</code><code class="p">)</code>
    <code class="n">text_tfidf</code> <code class="o">=</code> <code class="n">vectorizer</code><code class="o">.</code><code class="n">transform</code><code class="p">([</code><code class="n">text_clean</code><code class="p">])</code>
    <code class="n">sentiment</code> <code class="o">=</code> <code class="n">prediction_model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">text_tfidf</code><code class="p">)</code>
    <code class="n">review</code><code class="o">.</code><code class="n">sentiment</code> <code class="o">=</code> <code class="n">Sentiment</code><code class="p">(</code><code class="n">sentiment</code><code class="o">.</code><code class="n">item</code><code class="p">())</code><code class="o">.</code><code class="n">name</code>
    <code class="k">return</code> <code class="n">review</code>
</pre>

<p>The <code>predict</code> method retrieves the text field from the input and performs the preprocessing and vectorization steps. It uses the model we trained earlier to predict the sentiment of the product review. The returned sentiment is specified as an <code>Enum</code> class to restrict the possible return values for the API. The input parameter <code>review</code> is defined as an instance of the class <code>Review</code>. The class is as specified next and contains the text of the review, a mandatory field along with <code>reviewerID</code>, <code>productID</code>, and <code>sentiment</code>. FastAPI uses <a href="https://oreil.ly/eErFf">“type hints”</a> to guess the type of the field <code>(str)</code> and perform the necessary validation. As we will see, FastAPI automatically generates a web documentation for our API following the <a href="https://openapis.org">OpenAPI</a> <span class="keep-together">specification</span> from which the API can be tested directly. We add the <code>schema_extra</code> as an example to act as a guide to developers who want to use the API:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="k">class</code> <code class="nc">Review</code><code class="p">(</code><code class="n">BaseModel</code><code class="p">):</code>
    <code class="n">text</code><code class="p">:</code> <code class="nb">str</code>
    <code class="n">reviewerID</code><code class="p">:</code> <code class="n">Optional</code><code class="p">[</code><code class="nb">str</code><code class="p">]</code> <code class="o">=</code> <code class="bp">None</code>
    <code class="n">asin</code><code class="p">:</code> <code class="n">Optional</code><code class="p">[</code><code class="nb">str</code><code class="p">]</code> <code class="o">=</code> <code class="bp">None</code>
    <code class="n">sentiment</code><code class="p">:</code> <code class="n">Optional</code><code class="p">[</code><code class="nb">str</code><code class="p">]</code> <code class="o">=</code> <code class="bp">None</code>

    <code class="k">class</code> <code class="nc">Config</code><code class="p">:</code>
        <code class="n">schema_extra</code> <code class="o">=</code> <code class="p">{</code>
            <code class="s2">"example"</code><code class="p">:</code> <code class="p">{</code>
                <code class="s2">"text"</code><code class="p">:</code> <code class="s2">"This was a great purchase, saved me much time!"</code><code class="p">,</code>
                <code class="s2">"reviewerID"</code><code class="p">:</code> <code class="s2">"A1VU337W6PKAR3"</code><code class="p">,</code>
                <code class="s2">"productID"</code><code class="p">:</code> <code class="s2">"B00K0TIC56"</code>
            <code class="p">}</code>
        <code class="p">}</code>
</pre>

<p>You would have noticed the use of the <code>Depends</code> keyword in the function definition. This allows us to load dependencies or other resources that are required before the function is called. This is treated as another Python function and is defined here:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="k">def</code> <code class="nf">load_model</code><code class="p">():</code>
    <code class="k">try</code><code class="p">:</code>
        <code class="k">print</code><code class="p">(</code><code class="s1">'Calling Depends Function'</code><code class="p">)</code>
        <code class="k">global</code> <code class="n">prediction_model</code><code class="p">,</code> <code class="n">vectorizer</code>
        <code class="n">prediction_model</code> <code class="o">=</code> <code class="n">pickle</code><code class="o">.</code><code class="n">load</code><code class="p">(</code>
            <code class="nb">open</code><code class="p">(</code><code class="s1">'models/sentiment_classification.pickle'</code><code class="p">,</code> <code class="s1">'rb'</code><code class="p">))</code>
        <code class="n">vectorizer</code> <code class="o">=</code> <code class="n">pickle</code><code class="o">.</code><code class="n">load</code><code class="p">(</code><code class="nb">open</code><code class="p">(</code><code class="s1">'models/tfidf_vectorizer.pickle'</code><code class="p">,</code> <code class="s1">'rb'</code><code class="p">))</code>
        <code class="k">print</code><code class="p">(</code><code class="s1">'Models have been loaded'</code><code class="p">)</code>
    <code class="k">except</code> <code class="ne">Exception</code> <code class="k">as</code> <code class="n">e</code><code class="p">:</code>
        <code class="k">raise</code> <code class="ne">ValueError</code><code class="p">(</code><code class="s1">'No model here'</code><code class="p">)</code>
</pre>

<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Pickle is a <a contenteditable="false" data-type="indexterm" data-primary="pickle serialization framework of Python" id="idm45634170575432"/>Python serialization framework that is one of the common ways in which models can be saved/exported. Other standardized formats include <a href="https://oreil.ly/iyl7W">joblib</a> and <a href="https://onnx.ai">ONNX</a>. Some <a contenteditable="false" data-type="indexterm" data-primary="deep learning" id="idm45634170572600"/>deep learning frameworks use their own export formats. For example, <a contenteditable="false" data-type="indexterm" data-primary="TensorFlow Embedding Projector" id="idm45634170571336"/>TensorFlow uses <code>SavedModel</code>, while <a contenteditable="false" data-type="indexterm" data-primary="PyTorch" id="idm45634170569688"/>PyTorch uses pickle but implements its own <code>save()</code> function. It’s important that you adapt the load and predict functions based on the type of model save/export you have used.</p>
</div>

<p>During development, FastAPI can be run with any web server (like <a href="https://uvicorn.org">uvicorn</a>), but it is recommended to <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch13_term11" id="idm45634170566536"/>use a full-fledged <a contenteditable="false" data-type="indexterm" data-primary="Gunicorn WSGI server" id="ch13_term16"/><a contenteditable="false" data-type="indexterm" data-primary="WSGI (Web Service Gateway Interface) servers" id="idm45634170563624"/>Web Service Gateway Interface (WSGI) server, which is production ready and supports multiple worker threads. We choose to use <a href="https://gunicorn.org">Gunicorn</a> as our WSGI server as it provides us with an HTTP server that can receive requests and redirect to the <span class="keep-together">FastAPI</span> app.</p>

<p class="pagebreak-before">Once installed, it can be run by entering:</p>

<pre data-type="programlisting">gunicorn -w 3 -b :5000 -t 5 -k uvicorn.workers.UvicornWorker main:app</pre>

<p>The <code>-w</code> argument is used to specify the number of worker processes to run, three workers in this case. The <code>-b</code> parameter specifies the port that WSGI server listens on, and the <code>-t</code> indicates a timeout value of five seconds after which the server will kill and restart the app in case it’s not responsive. The <code>-k</code> argument specifies the instance of worker class (<code>uvicorn</code>) that must be called to run the app, which is specified by referring to the Python module (<code>main</code>) and the name (<code>app</code>).</p>

<p>Before deploying our API, we have to <a contenteditable="false" data-type="indexterm" data-primary="environment.yml file" id="idm45634170554776"/>revisit the <em>environment.yml</em> file. In the first blueprint, we described ways to generate and share the <em>environment.yml</em> file to ensure that your analysis is reproducible. However, it is not recommended to follow this method when deploying code to production. While the exported <em>environment.yml</em> file is a starting point, we must inspect it manually and ensure that it does not contain unused packages. It’s also important to specify the exact version number of a package to ensure that package updates do not interfere with your production deployment. We use a <a contenteditable="false" data-type="indexterm" data-primary="Vulture code analysis tool" id="idm45634170551800"/>Python code analysis tool called <a href="https://oreil.ly/fC71i">Vulture</a> that identifies unused packages as well as other dead code fragments. Let’s run this analysis for the <em>app</em> folder:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">vulture</code> <code class="n">app</code><code class="o">/</code>
</pre>

<p><code>Out:</code></p>

<pre data-type="programlisting">
app/main.py:11: unused variable 'POSITIVE' (60% confidence)
app/main.py:12: unused variable 'NEGATIVE' (60% confidence)
app/main.py:16: unused variable 'reviewerID' (60% confidence)
app/main.py:17: unused variable 'asin' (60% confidence)
app/main.py:20: unused class 'Config' (60% confidence)
app/main.py:21: unused variable 'schema_extra' (60% confidence)
app/main.py:40: unused variable 'model' (100% confidence)
app/main.py:44: unused attribute 'sentiment' (60% confidence)
app/preprocessing.py:30: unused import 'spacy' (90% confidence)
app/preprocessing.py:34: unused function 'display_nlp' (60% confidence)
</pre>

<p>Along with the list of potential issues, Vulture also provides a confidence score. Please use the identified issues as pointers to check the use of these imports. In the previous example, we know that the class variables we have defined are used to validate the input to the API and are definitely used. We can see that even though <code>spacy</code> and <code>display_nlp</code> are part of the preprocessing module, they are not used in our app. We can choose to remove them and the corresponding dependencies from the YAML file.</p>

<p class="pagebreak-before">You can also determine the version of each package used in the conda environment by running the <code><strong>conda list</strong></code> command and then use this information to create the final cleaned-up environment YAML file, as shown here:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">name</code><code class="p">:</code> <code class="n">sentiment</code><code class="o">-</code><code class="n">app</code>
<code class="n">channels</code><code class="p">:</code>
  <code class="o">-</code> <code class="n">conda</code><code class="o">-</code><code class="n">forge</code>
<code class="n">dependencies</code><code class="p">:</code>
  <code class="o">-</code> <code class="n">python</code><code class="o">==</code><code class="mf">3.8</code>
  <code class="o">-</code> <code class="n">fastapi</code><code class="o">==</code><code class="mf">0.59</code><code class="o">.</code><code class="mi">0</code>
  <code class="o">-</code> <code class="n">pandas</code><code class="o">==</code><code class="mf">1.0</code><code class="o">.</code><code class="mi">5</code>
  <code class="o">-</code> <code class="n">scikit</code><code class="o">-</code><code class="n">learn</code><code class="o">==</code><code class="mf">0.23</code><code class="o">.</code><code class="mi">2</code>
  <code class="o">-</code> <code class="n">gunicorn</code><code class="o">==</code><code class="mf">20.0</code><code class="o">.</code><code class="mi">4</code>
  <code class="o">-</code> <code class="n">uvicorn</code><code class="o">==</code><code class="mf">0.11</code><code class="o">.</code><code class="mi">3</code>
</pre>

<p>As the final step, we can Dockerize the API so that it’s easier to run the entire app in its own container, which is especially beneficial when we want to host it on a cloud provider, as we will see in the next blueprint. We make two changes in the Dockerfile from the previous blueprint as follows:</p>

<pre data-type="programlisting">
# Copy files required for deploying service to app folder in container
COPY . /app
WORKDIR /app
</pre>

<p>The previous instruction is used to <code>COPY</code> all of the contents of the current <em>app</em> folder to the Docker image, which contains all of the files needed to deploy and run the REST API. The current directory in the container is then changed to the <em>app</em> folder by using the <code>WORKDIR</code> instruction:</p>

<pre data-type="programlisting">
# Start WSGI server on container
EXPOSE 5000
RUN ["chmod", "+x", "start_script.sh"]
ENTRYPOINT [ "/bin/bash", "-c" ]
CMD ["./start_script.sh"]
</pre>

<p>We then provide <a contenteditable="false" data-type="indexterm" data-primary="Docker containers" id="ch13_term19"/>the steps to run the WSGI server by first exposing port 5000 on the container. Next, we enable permissions on the <code>start_script</code> so that the Docker daemon can execute it at container startup. We use a combination of <code>ENTRYPOINT</code> (used to start the bash shell in which the script is to be run) and <code>CMD</code> (used to specify the actual script as an argument to the bash shell), which activates the conda environment and starts the Gunicorn server. Since we are running the server within a Docker container, we make a small change to specify the <em>access-logfile</em> to be written to STDOUT (<code>-</code>) to ensure we can still view them:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="c1">#!/bin/bash</code>
<code class="n">source</code> <code class="n">activate</code> <code class="n">my_env_name</code>
<code class="n">GUNICORN_CMD_ARGS</code><code class="o">=</code><code class="s2">"--access-logfile -"</code> <code class="n">gunicorn</code> <code class="o">-</code><code class="n">w</code> <code class="mi">3</code> <code class="o">-</code><code class="n">b</code> <code class="p">:</code><code class="mi">5000</code> <code class="o">-</code><code class="n">t</code> <code class="mi">5</code> \
          <code class="o">-</code><code class="n">k</code> <code class="n">uvicorn</code><code class="o">.</code><code class="n">workers</code><code class="o">.</code><code class="n">UvicornWorker</code> <code class="n">main</code><code class="p">:</code><code class="n">app</code> <code class="o">-</code>
</pre>

<p>We build the <a contenteditable="false" data-type="indexterm" data-primary="Docker images" id="ch13_term17"/>Docker image and run it following the same steps as in the previous blueprint. This will result in a running Docker container where the Gunicorn WSGI server is <a contenteditable="false" data-type="indexterm" data-primary="FastAPI" id="idm45634170373160"/>running the FastAPI app. We have to make sure that we forward a port from the host system where the container is running:</p>

<pre data-type="programlisting">
$ sudo docker run -p 5000:5000 textblueprints/sentiment-app:v1
    [INFO] Starting gunicorn 20.0.4
    [INFO] Listening at: http://0.0.0.0:5000 (11)
    [INFO] Using worker: sync
    [INFO] Booting worker with pid: 14
</pre>

<p>We can make a call to the container running the API from a different program. In a separate terminal window or IDE, create a test method that calls the API and passes in a sample review to check the response. We make a call to port 5000 with the local IP, which is forwarded to port 5000 of the container from which we receive the response, as shown here:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="kn">import</code> <code class="nn">requests</code>
<code class="kn">import</code> <code class="nn">json</code>

<code class="n">url</code> <code class="o">=</code> <code class="s1">'http://0.0.0.0:5000/api/v1/sentiment'</code>
<code class="n">data</code> <code class="o">=</code> <code class="p">{</code>
    <code class="s1">'text'</code><code class="p">:</code>
    <code class="s1">'I could not ask for a better system for my small greenhouse, </code><code class="se">\</code>
<code class="s1">     easy to set up and nozzles do very well'</code><code class="p">,</code>
    <code class="s1">'reviewerID'</code><code class="p">:</code> <code class="s1">'A1VU337W6PKAR3'</code><code class="p">,</code>
    <code class="s1">'productID'</code><code class="p">:</code> <code class="s1">'B00K0TIC56'</code>
<code class="p">}</code>
<code class="n">input_data</code> <code class="o">=</code> <code class="n">json</code><code class="o">.</code><code class="n">dumps</code><code class="p">(</code><code class="n">data</code><code class="p">)</code>
<code class="n">headers</code> <code class="o">=</code> <code class="p">{</code><code class="s1">'content-type'</code><code class="p">:</code> <code class="s1">'application/json'</code><code class="p">,</code> <code class="s1">'Accept-Charset'</code><code class="p">:</code> <code class="s1">'UTF-8'</code><code class="p">}</code>
<code class="n">r</code> <code class="o">=</code> <code class="n">requests</code><code class="o">.</code><code class="n">post</code><code class="p">(</code><code class="n">url</code><code class="p">,</code> <code class="n">data</code><code class="o">=</code><code class="n">input_data</code><code class="p">,</code> <code class="n">headers</code><code class="o">=</code><code class="n">headers</code><code class="p">)</code>
<code class="k">print</code><code class="p">(</code><code class="n">r</code><code class="o">.</code><code class="n">text</code><code class="p">)</code>
</pre>

<p><code>Out:</code></p>

<pre data-type="programlisting">
{
  "prediction": "POSITIVE"
}
</pre>

<p>We can see that our API has generated the expected <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch13_term16" id="idm45634170272344"/>response. Let’s also check the documentation of this API, which we can find at <a href="http://localhost:5000/docs" class="orm:hideurl"><em>http://localhost:5000/docs</em></a>. It should generate a page as shown in <a data-type="xref" href="#fig-swagger-api">Figure 13-2</a>, and clicking the link for our <code>/api/v1/sentiment</code> method will provide additional details on how the method is to be called and also has the option to try it out. This allows others to provide different text inputs and view the results generated by the API without writing any code.</p>

<p>Docker containers are always started in <a contenteditable="false" data-type="indexterm" data-primary="unprivileged mode of Docker containers" id="idm45634170267624"/>unprivileged mode, meaning that even if there is a terminal error, it would only be restricted to the container without any impact to the host system. As a result, we can run the server as a root user safely within the container without worrying about an impact on the host system.</p>

<figure><div id="fig-swagger-api" class="figure"><img src="Images/btap_1302.jpg" width="824" height="504"/>
  <h6><span class="label">Figure 13-2. </span>API specification and testing provided by FastAPI.</h6>
</div></figure>

<p>You can run a combination of the <code>sudo docker tag</code> and <code>sudo docker push</code> commands discussed earlier to share the REST API. Your colleague could easily pull this Docker image to run the API and use it to identify unhappy customers by providing their support tickets. In the next blueprint, we will run the Docker image on a cloud provider and make it available <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch13_term12" id="idm45634170262840"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch13_term13" id="idm45634170261464"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch13_term17" id="idm45634170260088"/>on the internet.</p>

</div></section>

<section data-type="sect1" class="blueprint" data-pdf-bookmark="Blueprint: Deploying and Scaling Your API Using a Cloud Provider"><div class="sect1" id="idm45634170906920">
<h1>Blueprint: Deploying and Scaling Your API Using a Cloud Provider</h1>

<p>Deploying machine <a contenteditable="false" data-type="indexterm" data-primary="cloud providers" id="ch13_term20"/><a contenteditable="false" data-type="indexterm" data-primary="text analytics in production" data-secondary="by deploying and scaling API" data-secondary-sortas="deploying and scaling API" id="ch13_term22"/><a contenteditable="false" data-type="indexterm" data-primary="REST (Representational State Transfer) API" data-secondary="deploying and scaling" id="ch13_term23"/>learning models and monitoring their performance is a complex task and includes multiple tooling options. This is an area of constant innovation that is continuously looking to make it easier for data scientists and developers. There are several cloud providers and multiple ways to deploy and host your API using one of them. This blueprint introduces a simple way to deploy the Docker container we created in the previous <a contenteditable="false" data-type="indexterm" data-primary="Kubernetes" data-secondary="Kubernetes cluster service" id="ch13_term26"/><a contenteditable="false" data-type="indexterm" data-primary="compute clusters" id="ch13_term37"/>blueprint using <a href="https://oreil.ly/C2KX2">Kubernetes</a>. Kubernetes is an open source technology that provides functionality to deploy and manage Docker containers to any underlying physical or virtual infrastructure. In this blueprint, we will be <a contenteditable="false" data-type="indexterm" data-primary="GCP (Google Cloud Platform)" id="ch13_term24"/>using Google Cloud Platform (GCP), but most major providers have support for Kubernetes. We can deploy the Docker container directly to a cloud service and make the REST API available to anyone. However, we choose to deploy this within a Kubernetes cluster since it gives us the flexibility to scale up and down the deployment easily.</p>

<p>You can sign up for a <a href="https://oreil.ly/H1jQS">free account with GCP</a>. By signing up with a cloud provider, you are renting computing resources from a third-party provider and will be asked to provide your billing details. During this blueprint, we will stay within the free-tier limit, but it’s important to keep a close track of your usage to ensure that you are not charged for some cloud resources that you forgot to shut down! Once you’ve completed the sign-up process, you can check this by visiting the Billing section from the <a href="https://oreil.ly/wX4wd">GCP console</a>. Before using this blueprint, please ensure that you have a Docker image containing the REST API pushed and available in <a contenteditable="false" data-type="indexterm" data-primary="Docker Hub registry" id="idm45634170243160"/>Docker Hub or any other container registry.</p>

<p>Let’s start by understanding how we are going to deploy the REST API, which is illustrated in <a data-type="xref" href="#fig-kubernetes-architecture">Figure 13-3</a>. We will create a scalable compute cluster using GCP. This is nothing but a collection of individual servers that are called <em>nodes</em>. The compute cluster shown has three such nodes but can be scaled when needed. We will use Kubernetes to deploy the REST API to each node of the cluster. Assuming we start with three nodes, this will create three replicas of the Docker container, each running on one node. These containers are still not exposed to the internet, and we make use of Kubernetes to run a load balancer service, which provides a gateway to the internet and also redirects requests to each container depending on its utilization. In addition to simplifying our deployment process, the use of Kubernetes ensures that node failures and traffic spikes can be handled by automatically creating additional instances.</p>

<figure><div id="fig-kubernetes-architecture" class="figure"><img src="Images/btap_1303.jpg" width="1172" height="681"/>
<h6><span class="label">Figure 13-3. </span>Kubernetes architecture diagram.</h6>
</div></figure>

<p class="pagebreak-before">Let’s create a project in GCP that we will use for our deployment. Visit <a href="https://oreil.ly/5mCaQ">Google Cloud</a>, choose the Create Project option on the top right, and create a project with your chosen name (we choose sentiment-rest-api). Once the project has been created, click the navigation menu on the top left and navigate to the service called Kubernetes Engine, as shown in <a data-type="xref" href="#fig-kubernetes-engine">Figure 13-4</a>. You have to click the Enable Billing link and select the payment account that you set up when you signed up. You can also click the Billing tab directly and set it up for your project as well. Assuming you are using the free trial to run this blueprint, you will not be charged. It will take a few minutes before it gets enabled for our project. Once this is complete, we are ready to proceed with our deployment.</p>

<figure><div id="fig-kubernetes-engine" class="figure"><img src="Images/btap_1304.jpg" width="606" height="652"/>
<h6><span class="label">Figure 13-4. </span>Enable Billing in the Kubernetes Engine option in the GCP console.</h6>
</div></figure>

<p>We can continue to work with Google Cloud Platform using the <a href="https://oreil.ly/-eZ-W">web console</a> or the command-line tool. While the functionality offered remains the same, we choose to describe the steps in the blueprint with the help of the command-line interface in the interest of brevity and to enable you to copy the commands. Please install the Google Cloud SDK by following the <a href="https://oreil.ly/G3_js">instructions</a> and then use the Kubernetes command-line tool by <a contenteditable="false" data-type="indexterm" data-primary="Kubernetes" data-secondary="kubectl commands" id="ch13_term28"/>running:</p>

<pre data-type="programlisting">gcloud components install kubectl</pre>

<p>In a new terminal window, we first authenticate our user account <a contenteditable="false" data-type="indexterm" data-primary="gcloud instructions" id="idm45634170227928"/>by running <code><strong>gcloud auth login</strong></code>. This will open the browser and redirect you to the Google authentication page. Once you have completed this, you won’t be asked for this again in this terminal window. We configure the project and compute zone where we would like to deploy our cluster. Use the project that we just created, and pick a location close to you from all the <a href="https://oreil.ly/SnRc8">available options</a>; we chose us-central1-a:</p>

<pre data-type="programlisting">gcloud config set project sentiment-rest-api</pre>
<pre data-type="programlisting">gcloud config set compute/zone us-central1-a</pre>

<p>Our next step is to create a Google Kubernetes Engine compute cluster. This is the compute cluster that we will use to deploy our Docker containers. Let’s create a cluster with three nodes and request a machine of type n1-standard-1. This type of machine comes with 3.75GB of RAM and 1 CPU. We can request a more powerful machine, but for our API this should suffice:</p>

<pre data-type="programlisting">
gcloud container clusters create \  sentiment-app-cluster --num-nodes 3 \
--machine-type n1-standard-1</pre>

<p>Every container cluster in GCP <a contenteditable="false" data-type="indexterm" data-primary="HorizontalPodAutoscaling" id="idm45634170221368"/>comes with <code>HorizontalPodAutoscaling</code>, which takes care of monitoring the CPU utilization and adding machines if required. The requested machines will be provisioned and assigned to the cluster, and once it’s executed, you can verify by checking the running compute instances with <code>gcloud compute instances list</code>:</p>

<pre data-type="programlisting">
$ gcloud compute instances list
NAME                                   ZONE           MACHINE_TYPE   STATUS
gke-sentiment-app-cluste-default-pool  us-central1-a  n1-standard-1  RUNNING
gke-sentiment-app-cluste-default-pool  us-central1-a  n1-standard-1  RUNNING
gke-sentiment-app-cluste-default-pool  us-central1-a  n1-standard-1  RUNNING
</pre>

<p>Now that our <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch13_term24" id="idm45634170179720"/>cluster is up and running, we will <a contenteditable="false" data-type="indexterm" data-primary="Docker images" id="ch13_term29"/>deploy the Docker image we created in the previous blueprint to this cluster with the help of Kubernetes. Our Docker image is available on <a contenteditable="false" data-type="indexterm" data-primary="Docker Hub registry" id="idm45634170176520"/>Docker Hub and is uniquely identified by <code>username/project_name:tag</code>. We give the name of our deployment as <code>sentiment-app</code> by running the following command:</p>

<pre data-type="programlisting">
kubectl create deployment sentiment-app <span class="small-period">--</span>image=textblueprints/sentiment-app:v0<span class="small-period">.</span>1</pre>

<p>Once it has been started, we can confirm that it’s running with the command <code>kubectl get pods</code>, which will show us that we have one pod running. A pod is analogous here to the container; in other words, one pod is equivalent to a running container of the provided image. However, we have a three-node cluster, and we can easily deploy more instances of our Docker image. Let’s scale this to three replicas with the following command:</p>

<pre data-type="programlisting">kubectl scale deployment sentiment-app --replicas=3</pre>

<p>You can verify that the other pods have also started running now. Sometimes there might be a delay as the container is deployed to the nodes in the cluster, and you can find detailed information by using the command <code>kubectl describe pods</code>. By having more than one replica, we enable our REST API to be continuously available even in the case of failures. For instance, let’s say one of the pods goes down because of an error; there would be two instances still serving the API. Kubernetes will also automatically create another pod in case of a failure to maintain the desired state. This is also the case since the REST API is stateless and would need additional failure handling in other scenarios.</p>

<p>While we have deployed and scaled the REST API, we have not made it available to the internet. In this final step, we will <a contenteditable="false" data-type="indexterm" data-primary="LoadBalancer service" id="idm45634170167960"/>add a <code>LoadBalancer</code> service called <code>sentiment-app-loadbalancer</code>, which acts as the HTTP server exposing the REST API to the internet and directing requests to the three pods based on the traffic. It’s important to distinguish between the parameter <code>port</code>, which is the port exposed by the <code>LoadBalancer</code> and the <code>target-port</code>, which is the port exposed by each container:</p>

<pre data-type="programlisting">
kubectl expose deployment sentiment-app --name=sentiment-app-loadbalancer
--type=LoadBalancer --port 5000 --target-port 5000</pre>

<p>If you run the <code>kubectl get service</code> command, it provides a listing of all Kubernetes services that are running, including the <code>sentiment-app-loadbalancer</code>. The parameter to take note of is <code>EXTERNAL-IP</code>, which can be used to access our API. The <code>sentiment-app</code> can be accessed using the link <em>http://[EXTERNAL-IP]:5000/apidocs</em>, which will provide the Swagger documentation, and a request can be made to <em>http://[EXTERNAL-IP]:5000/api/v1/sentiment</em>:</p>

<pre data-type="programlisting">
$ kubectl expose deployment sentiment-app --name=sentiment-app-loadbalancer \
--type=LoadBalancer --port 5000 --target-port 5000
service "sentiment-app-loadbalancer" exposed
$ kubectl get service
NAME                         TYPE           CLUSTER-IP    EXTERNAL-IP
kubernetes                   ClusterIP      10.3.240.1    &lt;none&gt;
sentiment-app-loadbalancer   LoadBalancer   10.3.248.29   34.72.142.113
</pre>

<p>Let’s say you retrained the model and want to make the latest version available via the API. We have to build a new Docker image with a new tag (<code>v0.2</code>) and then set the image to that tag with the command <code>kubectl set image</code>, and Kubernetes will automatically update pods in the cluster in a rolling fashion. This ensures that our REST API will always be available but also deploy the new version using a <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch13_term29" id="idm45634170157384"/>rolling strategy.</p>

<p>When we want to shut down our deployment and cluster, we can run the following commands to first delete the <code>LoadBalancer</code> service and then tear down the cluster. This will also release all the compute instances you were using:</p>

<pre data-type="programlisting">kubectl delete service sentiment-app-loadbalancer</pre>
<pre data-type="programlisting">gcloud container clusters delete sentiment-app-cluster</pre>

<p>This blueprint <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch13_term26" id="idm45634170152840"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch13_term28" id="idm45634170151432"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch13_term37" id="idm45634170150056"/>provides a simple way to deploy and scale your machine learning model using cloud resources and does not cover several other aspects that can be crucial to production deployment. It’s important to keep track of the performance of your model by continuously monitoring parameters such as accuracy and adding triggers for retraining. To ensure the quality of predictions, one must have enough test cases and other quality checks before returning a result from the API. In addition, good software design must provide for authentication, identity management, and security, which should be part of any <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch13_term19" id="idm45634170147944"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch13_term20" id="idm45634170146568"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch13_term22" id="idm45634170145192"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch13_term23" id="idm45634170143816"/>publicly available API.</p>
</div></section>

<section data-type="sect1" class="blueprint" data-pdf-bookmark="Blueprint: Automatically Versioning and Deploying Builds"><div class="sect1" id="idm45634170258328">
<h1>Blueprint: Automatically Versioning and Deploying Builds</h1>

<p>In the previous blueprint, we created the first deployment of our REST API. Consider that you now have access to additional data and retrained your model to achieve a higher level of accuracy. We <a contenteditable="false" data-type="indexterm" data-primary="automatic deploying of REST API updates" id="ch13_term30"/><a contenteditable="false" data-type="indexterm" data-primary="text analytics in production" data-secondary="by automatically versioning and deploying builds" data-secondary-sortas="automatically versioning and deploying builds" id="ch13_term34"/>would like to update our REST API with this new version so that the results of our prediction improve. In this blueprint, we will provide an automated way to deploy updates to your API with the help of GitHub actions. Since the code for this book and also the <a href="https://oreil.ly/SesD8">sentiment-app</a> is hosted on GitHub, it made sense to use GitHub actions, but depending on the environment, you could use other tools, such as <a href="https://oreil.ly/vBS8i">GitLab</a>.</p>

<p>We assume that you have saved the model files after retraining. Let’s check in our new model files and make any additional changes to <code>main.py</code>. You can see these additions on the <a href="https://oreil.ly/ktwYX">Git repository</a>. Once all the changes are checked in, we decide that we are satisfied and ready to deploy this new version. We have to <a contenteditable="false" data-type="indexterm" data-primary="tagging in GitHub" id="idm45634170132792"/>tag the current state as the one that we want to deploy by using the <code>git tag v0.2</code> command. This binds the tag name (<code>v0.2</code>) to the current point in the commit history. Tags should normally follow <a href="https://semver.org"><code>Semantic Versioning</code></a>, where version numbers are assigned in the form MAJOR.MINOR.PATCH and are often used to identify updates to a given software module. Once a tag has been assigned, additional changes can be made but will not be considered to be part of the already-tagged state. It will always point to the original commit. We can push the created tag to the repository by running <code>git push origin tag-name</code>.</p>

<p>Using GitHub actions, we have created a deployment pipeline that uses the event of tagging a repository to trigger the start of the deployment pipeline. This pipeline is defined in the <em>main.yml</em> file located in the folder <em>.github/workflow/</em> and defines the steps to be run each time a new tag is assigned. So whenever we want to release a new version of our API, we can create a new tag and push this to the repository.</p>

<p>Let’s walk through the deployment steps:</p>

<pre data-type="programlisting">
name: sentiment-app-deploy

on:
  push:
    tags:
      - '*'

jobs:
  build:
    name: build
    runs-on: ubuntu-latest
    timeout-minutes: 10
    steps:
</pre>

<p>The file starts with a name to identify the GitHub workflow, and the <code>on</code> keyword specifies the events that trigger the deployment. In this case, we specify that only Git push commands that contain a tag will start this deployment. This ensures that we don’t deploy with each commit and control a deployment to the API by using a tag. We can also choose to build only on specific tags, for example, major version revisions. The <code>jobs</code> specifies the series of steps that must be run and sets up the environment that GitHub uses to perform the actions. The <code>build</code> parameter defines the kind of build machine to be used (<code>ubuntu</code>) and a time-out value for the entire series of steps (set to 10 minutes).</p>

<p>Next, we specify the first set of actions as follows:</p>

<pre data-type="programlisting">
    - name: Checkout
      uses: actions/checkout@v2

    - name: build and push image
      uses: docker/build-push-action@v1
      with:
        username: ${{ secrets.DOCKER_USERNAME }}
        password: ${{ secrets.DOCKER_PASSWORD }}
        repository: sidhusmart/sentiment-app
        tag_with_ref: true
        add_git_labels: true
        push: ${{ startsWith(github.ref, 'refs/tags/') }}

    - name: Get the Tag Name
      id: source_details
      run: |-
        echo ::set-output name=TAG_NAME::${GITHUB_REF#refs/tags/}
</pre>

<p>The first step is typically always checkout, which checks out the latest code on the build machine. The next step is to build the <a contenteditable="false" data-type="indexterm" data-primary="Docker containers" id="ch13_term35"/>Docker container using the latest commit from the tag and push this to the Docker Hub registry. The <code>docker/build-push-action@v1</code> is a GitHub action that is already available in <a href="https://oreil.ly/dHiai">GitHub Marketplace</a>, which we reuse. Notice the use of <a contenteditable="false" data-type="indexterm" data-primary="secrets for GitHub repository" id="idm45634170118104"/>secrets to pass in the user credentials. You can encrypt and store the user credentials that your deployment needs by visiting the Settings &gt; Secrets tab of your GitHub repository, as shown in <a data-type="xref" href="#fig-github-secrets">Figure 13-5</a>. This allows us to maintain security and enable automatic builds without any password prompts. We tag the Docker image with the same tag as the one we used in the Git commit. We add another step to get the tag and set this as an environment variable, <code>TAG_NAME</code>, which will be used while updating the cluster.</p>

<figure><div id="fig-github-secrets" class="figure"><img src="Images/btap_1305.jpg" width="1246" height="749"/>
<h6><span class="label">Figure 13-5. </span>Adding credentials to your repository using secrets.</h6>
</div></figure>

<p>For the deployment steps, we have to connect to our <a contenteditable="false" data-type="indexterm" data-primary="GCP (Google Cloud Platform)" id="idm45634170112712"/>running GCP cluster and update the image that we use for the deployment. First, we <a contenteditable="false" data-type="indexterm" data-primary="gcloud instructions" id="idm45634170111512"/>have to add <code>PROJECT_ID</code>, <code>LOCATION_NAME</code>, <code>CLUSTER_NAME</code>, and <code>GCLOUD_AUTH</code> to the secrets to enable this action. We encode these as secrets to ensure that project details of our cloud deployments are not stored publicly. You can get the <code>GCLOUD_AUTH</code> by using the provided <a href="https://oreil.ly/EDELL">instructions</a> and adding the values in the downloaded key as the secret for this field.</p>

<p>The next steps for deployment include setting up the <code>gcloud</code> utility on the build machine and using this to get the Kubernetes configuration file:</p>

<pre data-type="programlisting">
    # Setup gcloud CLI
    - uses: GoogleCloudPlatform/github-actions/setup-gcloud@master
      with:
        version: '290.0.1'
        service_account_key: ${{ secrets.GCLOUD_AUTH }}
        project_id: ${{ secrets.PROJECT_ID }}

    # Get the GKE credentials so we can deploy to the cluster
    - run: |-
        gcloud container clusters get-credentials ${{ secrets.CLUSTER_NAME }} \
                                           --zone ${{ secrets.LOCATION_ZONE }}
</pre>

<p>Finally, we <a contenteditable="false" data-type="indexterm" data-primary="Kubernetes" data-secondary="Kubernetes cluster service" id="idm45634170104632"/>update the Kubernetes deployment with the latest Docker image. This is where we use the <code>TAG_NAME</code> to identify the latest release that we pushed in the second step. Finally, we add an action to monitor the status of the rollout in our cluster:</p>

<pre data-type="programlisting">
    # Deploy the Docker image to the GKE cluster
    - name: Deploy
      run: |-
        kubectl set image --record deployment.apps/sentiment-app \
                  sentiment-app=textblueprints/sentiment-app:\
                  ${{ steps.source_details.outputs.TAG_NAME }}

    # Verify that deployment completed
    - name: Verify Deployment
      run: |-
        kubectl rollout status deployment.apps/sentiment-app
        kubectl get services -o wide
</pre>

<p>You can follow the various stages of the build pipeline using the Actions tab of your repository, as shown in <a data-type="xref" href="#fig-github-progress">Figure 13-6</a>. At the end of the deployment pipeline, an updated version of the API should be available at the same URL and can also be tested by visiting the API documentation.</p>

<p>This technique works well when code and model files are small enough to be packaged into the Docker image. If we use <a contenteditable="false" data-type="indexterm" data-primary="deep learning" id="idm45634170098952"/>deep learning models, this is often not the case, and creating large Docker containers is not recommended. In such cases, we still use Docker containers to package and deploy our API, but the model files reside on the host system and can be attached to the Kubernetes cluster. For cloud deployments, this makes use of a persistent storage like <a href="https://oreil.ly/OZ4Ru">Google Persistent Disk</a>. In such cases, we can perform model updates by performing a cluster update and changing the <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch13_term30" id="idm45634170096552"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch13_term34" id="idm45634170095176"/>attached volume.</p>

<figure><div id="fig-github-progress" class="figure"><img src="Images/btap_1306.jpg" width="1148" height="595"/>
  <h6><span class="label">Figure 13-6. </span>GitHub deployment workflow initiated by pushing a Git tag.</h6>
</div></figure>

</div></section>

<section data-type="sect1" data-pdf-bookmark="Closing Remarks"><div class="sect1" id="idm45634170142056">
<h1>Closing Remarks</h1>

<p>We introduced <a contenteditable="false" data-type="indexterm" data-primary="text analytics in production" data-secondary="about" id="idm45634170091080"/>a number of blueprints with the aim of allowing you to share the analysis and projects you created using previous chapters in this book. We started by showing you how to create reproducible conda environments that will allow your teammate or a fellow learner to easily reproduce your results. With the help of Docker environments, we make it even easier to share your analysis by creating a complete environment that works regardless of the platform or infrastructure that your collaborators <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch13_term35" id="idm45634170088680"/>are using. If someone would like to integrate the results of your analysis in their product or service, we can encapsulate the machine learning model into a REST API that can be called from any language or platform. Finally, we provided a blueprint to easily create a cloud deployment of your API that can be scaled up or down based on usage. This cloud deployment can be updated easily with a new version of your model or additional functionality. While adding each layer of abstraction, we make the analysis accessible to a different (and broader) audience and reduce the amount of detail <a contenteditable="false" data-type="indexterm" data-primary="text analytics in production" data-secondary="further reading on" id="idm45634170086568"/>that is <a contenteditable="false" data-type="indexterm" data-primary="preprocessing of data" data-see="data preprocessing" id="idm45634170085096"/><a contenteditable="false" data-type="indexterm" data-primary="text preprocessing" data-see="data preprocessing" id="idm45634170083688"/><a contenteditable="false" data-type="indexterm" data-primary="classification" data-see="text classification algorithms" id="idm45634170082312"/><a contenteditable="false" data-type="indexterm" data-primary="explainability" data-see="text classification results, explaining" id="idm45634170080920"/><a contenteditable="false" data-type="indexterm" data-primary="extraction of data" data-see="text data extraction from web; text data extraction with APIs" id="idm45634170079528"/><a contenteditable="false" data-type="indexterm" data-primary="HTML websites, scraping of" data-see="scraping websites" id="idm45634170078104"/><a contenteditable="false" data-type="indexterm" data-primary="linguistic processing" data-see="spaCy, linguistic processing with" id="idm45634170076712"/><a contenteditable="false" data-type="indexterm" data-primary="model training" data-see="training" id="idm45634170075320"/><a contenteditable="false" data-type="indexterm" data-primary="scraping websites" data-secondary="extracting data and" data-see="text data extraction from web" id="idm45634170073944"/><a contenteditable="false" data-type="indexterm" data-primary="semantic relationships" data-see="word embeddings for semantic analysis" id="idm45634170072280"/><a contenteditable="false" data-type="indexterm" data-primary="summarization" data-see="text summarization" id="idm45634170070888"/><a contenteditable="false" data-type="indexterm" data-primary="term frequencies" data-see="TF-IDF (Term-Frequency Inverse Document Frequency) weighting; word frequency, analysis of" id="idm45634170069512"/><a contenteditable="false" data-type="indexterm" data-primary="text data extraction from web" data-secondary="scraping websites and" data-see="scraping websites" id="idm45634170067944"/>exposed.</p>
</div></section>

<section data-type="sect1" data-pdf-bookmark="Further Reading"><div class="sect1" id="idm45634170066264">
<h1>Further Reading</h1>

<ul class="author-date-bib">
  <li>Scully, D, et al. <em>Hidden Technical Debt in Machine Learning Systems</em>. <a href="https://papers.nips.cc/paper/2015/file/86df7dcfd896fcaf2674f757a2463eba-Paper.pdf"><em>https://papers.nips.cc/paper/2015/file/86df7dcfd896fcaf2674f757a2463eba-Paper.pdf</em></a>.</li>
</ul>
</div></section>
</div></section></div>



  </body></html>