<html><head></head><body><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 2. Extracting Textual Insights with APIs"><div class="chapter" id="ch-api">
<h1><span class="label">Chapter 2. </span>Extracting Textual Insights with APIs</h1>

<p>When you want to determine the approach to a research question or start working on a text analytics project, the availability of data is often the first stumbling block. A simple Google search or the more specific <a href="https://oreil.ly/SJoyG">Dataset search</a> will throw up curated datasets, and we will use some of these in subsequent chapters of this book. Depending on <a contenteditable="false" data-type="indexterm" data-primary="use cases" data-secondary="for text data extraction" data-secondary-sortas="text data extraction" id="ch2_term32"/><a contenteditable="false" data-type="indexterm" data-primary="text data extraction with APIs" data-secondary="use cases for" id="ch2_term33"/>your project, such datasets may turn out to be generic and not suitable for your use case. You might have to create your own dataset, and <a contenteditable="false" data-type="indexterm" data-primary="APIs (application programming interfaces)" id="ch2_term1"/><a contenteditable="false" data-type="indexterm" data-primary="text data extraction with APIs" data-secondary="about APIs" id="ch2_term2"/>application programming interfaces (APIs) are one way to extract data programmatically in an automated fashion.</p>

<section data-type="sect1" data-pdf-bookmark="What You’ll Learn and What We’ll Build"><div class="sect1" id="idm45634215128744">
<h1>What You’ll Learn and What We’ll Build</h1>

<p>In this chapter, we will provide an overview of APIs and introduce blueprints to extract data for your project from popular websites like <a href="https://github.com">GitHub</a> and <a href="https://twitter.com">Twitter</a>. You will learn about using authentication tokens, handling pagination, understanding rate limits, and automating data extraction. At the end of this chapter, you will be able to create your own datasets by making API calls to any identified service. While the blueprints are illustrated with specific examples such as GitHub and Twitter, they can be used to work with any API.</p>
</div></section>

<section data-type="sect1" data-pdf-bookmark="Application Programming Interfaces"><div class="sect1" id="idm45634215125224">
<h1>Application Programming Interfaces</h1>

<p>APIs are interfaces that allow software applications or components to communicate with one another without having to know how they are implemented. The API provides a set of definitions and protocols including the kinds of requests that can be made, the data formats to be used, and the expected response. An API is a set of software interfaces that is commonly used by developers while building websites, apps, and services. For example, when you sign up for a new account with almost any <span class="keep-together">service</span>, you will be asked to verify your email address or telephone number with a one-time code or link. Typically, the developer would use the API provided by an authentication service to enable this functionality rather than build the entire flow. This allows decoupling of the core functionality that the service provides and uses APIs to build other necessary, but not unique, features. You can read an intuitive nontechnical introduction to APIs provided by <a href="https://oreil.ly/e9iUI">Zapier</a> for a better understanding.</p>

<p>How are programming APIs connected with data for text analytics projects? In addition to enabling basic functionality such as authentication, common functionality on websites is also offered as APIs, providing us with an alternative way of accessing data. For example, third-party tools make use of APIs to create a post or add comments on social media. We can use these same APIs to read and store this information locally to create our dataset. For instance, say you are an analyst working at a Consumer Packaged Goods firm looking to evaluate the performance of a marketing campaign. You could <a contenteditable="false" data-type="indexterm" data-primary="text data extraction with APIs" data-secondary="from Twitter's Search API " data-secondary-sortas="Twitter's Search API " id="idm45634215068200"/><a contenteditable="false" data-type="indexterm" data-primary="Search API of Twitter" id="idm45634215066472"/><a contenteditable="false" data-type="indexterm" data-primary="Twitter" data-secondary="Search API of " id="idm45634215065368"/>extract data using the <a href="https://oreil.ly/PCJsx">Twitter Search API</a>, filter tweets that contain the campaign tagline or hashtag, and analyze the text to understand people’s reactions. Or consider that you are asked by a training provider to help identify upcoming technology areas for new courses. One approach could be to extract data on questions being asked using the <a href="https://oreil.ly/kMsGs">StackOverflow API</a> and identify emerging topics using text analytics.</p>

<p>Using APIs is the preferred approach over <a contenteditable="false" data-type="indexterm" data-primary="scraping websites" data-secondary="problems with" id="idm45634215061720"/>scraping a website. They are designed to be callable functions, are easy to use, and can be automated. They are specifically recommended when working with data that changes frequently or when it’s critical that the project reflects the latest information. When working with any API, it’s important to take the time and read the documentation carefully. It provides granular information on the specific API call, data formats, and parameters as well as other details like user permissions, rate limits, and so on.</p>

<div data-type="note" epub:type="note" class="pagebreak-after"><h6>Note</h6>
<p>Not all APIs are provided free of charge, and some providers have different plans to support different kinds of customers. For example, the Twitter API has Standard, Premium, and Enterprise versions. The Standard API is a public API (available to anyone with a developer account), while the Premium and Enterprise APIs are only for paying customers. We will use only <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch2_term1" id="idm45634215058024"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch2_term2" id="idm45634215056648"/>public APIs in this chapter.</p>
</div>
</div></section>

<section data-type="sect1" class="blueprint pagebreak-after" data-pdf-bookmark="Blueprint: Extracting Data from an API Using the Requests Module"><div class="sect1" id="idm45634215054888">
<h1>Blueprint: Extracting Data from an API Using the Requests Module</h1>

<p>With the popularity of the web driven by the HTTP standard, a URL is most often the primary specification for an API. We will use the <a contenteditable="false" data-type="indexterm" data-primary="text data extraction with APIs" data-secondary="with requests module" data-secondary-sortas="requests module" id="ch2_term4"/><a contenteditable="false" data-type="indexterm" data-primary="requests library (Python)" id="idm45634215049752"/>requests library that is included in the standard Python distribution as the primary way to access and extract data from an API. To illustrate this blueprint, we will <a contenteditable="false" data-type="indexterm" data-primary="GitHub API" data-secondary="data extraction from" id="ch2_term5"/>use the <a href="https://oreil.ly/oUIG1">GitHub API</a>. GitHub is a popular code hosting platform where several open source projects such as Python, scikit-learn, and TensorFlow, as well as the code for this book, are hosted. Let’s say that you would like to determine the popularity of different programming languages such as Python, Java, and JavaScript. We could extract data from GitHub on the languages used by popular repositories and determine the prevalence of each language. Or consider that your organization is hosting a project on GitHub and wants to ensure that users and contributors adhere to the Code of Conduct. We can extract the issues and comments written by contributors and ensure that offensive language is not used. In <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch2_term32" id="idm45634215045128"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch2_term33" id="idm45634215043752"/>this blueprint, we will read and understand the documentation of an API, make requests, and parse the output and create a dataset that can be used to solve our use case.</p>

<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45634215042056">
<h5>SOAP Versus REST Versus GraphQL</h5>

<p>APIs have existed as a standard for software interfaces and communications for a long time, and the technology used to implement them has changed over the years. Simple Object Access Protocol<a contenteditable="false" data-type="indexterm" data-primary="SOAP (Simple Object Access Protocol)" id="idm45634215040328"/> (SOAP) was one of the earliest methods for different software modules to speak with one another using standard interfaces. SOAP uses a standard messaging format encapsulated using the Extensible Markup Language (XML) and can use any communication protocol (like HTTP, TCP) to transmit the message. The SOAP envelope contained in the XML follows a standard definition including the definition of data types, error codes, and so on. <a contenteditable="false" data-type="indexterm" data-primary="REST (Representational State Transfer) API" data-secondary="about" id="idm45634215038680"/>Representational State Transfer (REST), on the other hand, relies on HTTP as the communication protocol including the use of status codes to determine successful or failed calls. It defines data types much more loosely and uses JSON heavily, though other formats are also supported. SOAP is generally considered an older protocol and is typically used within legacy applications in large enterprises, while REST is a preferred format adopted by several web-based services. <a contenteditable="false" data-type="indexterm" data-primary="GraphQL (Graph Query Language)" id="idm45634215036584"/>Graph Query Language (GraphQL) is a relatively new specification that defines a way to interact with APIs similar to writing SQL queries. One of the drawbacks of the REST architecture is that retrieving a single piece of information might require multiple calls to different resources. This will depend on how the resources are organized; for instance, to determine whether a user’s phone number is active, one would have to make an API call to the <code>/user</code> endpoint to retrieve all details followed by a subsequent call to a different endpoint like <code>/contacts</code> with the phone number to check whether that phone number is active. In GraphQL this would be a single API call with a specific SQL-like query where all active phone numbers of a given user would be retrieved. While GraphQL has gained popularity since it was open sourced by Facebook in 2015, REST APIs are much more common. GitHub, for example, maintains version three of its APIs as a <a href="https://oreil.ly/oUIG1">REST API</a> that we will use in the blueprint, whereas the latest version four is a <a href="https://oreil.ly/ukpla">GraphQL API</a>.</p>
</div></aside>

<p>The first API we want to call is to list all the repositories on GitHub. The entry point to the REST API documentation can be found on <a href="https://oreil.ly/oUIG1">GitHub</a>. You can either search for the specific method (also referred to as the <em>endpoint</em>) or navigate to the <a href="https://oreil.ly/8HM5v">GitHub page</a> to see its details, as shown in <a data-type="xref" href="#fig-repositories-documentation">Figure 2-1</a>.</p>

<figure><div id="fig-repositories-documentation" class="figure"><img src="Images/btap_0201.jpg" width="721" height="355"/>
  <h6><span class="label">Figure 2-1. </span>API documentation for listing public repositories.</h6>
</div></figure>

<p>As stated in the documentation, this is a <code>GET</code> method that will provide you with a list of repositories in the order they were created. Let’s make a call using the <code>requests.get</code> method and view the response status:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="kn">import</code> <code class="nn">requests</code>

<code class="n">response</code> <code class="o">=</code> <code class="n">requests</code><code class="o">.</code><code class="n">get</code><code class="p">(</code><code class="s1">'https://api.github.com/repositories'</code><code class="p">,</code>
                        <code class="n">headers</code><code class="o">=</code><code class="p">{</code><code class="s1">'Accept'</code><code class="p">:</code> <code class="s1">'application/vnd.github.v3+json'</code><code class="p">})</code>
<code class="k">print</code><code class="p">(</code><code class="n">response</code><code class="o">.</code><code class="n">status_code</code><code class="p">)</code>
</pre>

<p><code>Out:</code></p>

<pre data-type="programlisting">
200
</pre>

<p>A <a href="https://httpstatuses.com/200">response code of 200</a> indicates that the call to the API was successful. We can also evaluate the encoding of the response object to ensure that we process it correctly. One of the important elements contained in the <a contenteditable="false" data-type="indexterm" data-primary="response object" id="ch2_term7"/>response object is <a contenteditable="false" data-type="indexterm" data-primary="headers object" id="idm45634215003112"/>the <code>headers</code> object. It is a dictionary that contains more detailed information, such as the name of the server, response timestamp, status, and so on. In the following code, we only extract the type of content and server details that have been returned by the API, but you are encouraged to look at all of the elements of this object. Most of this information is present in the detailed API documentation, but inspecting the response is another way to ensure that you parse the response <span class="keep-together">accurately:</span></p>

<pre data-code-language="python" data-type="programlisting">
<code class="k">print</code> <code class="p">(</code><code class="n">response</code><code class="o">.</code><code class="n">encoding</code><code class="p">)</code>
<code class="k">print</code> <code class="p">(</code><code class="n">response</code><code class="o">.</code><code class="n">headers</code><code class="p">[</code><code class="s1">'Content-Type'</code><code class="p">])</code>
<code class="k">print</code> <code class="p">(</code><code class="n">response</code><code class="o">.</code><code class="n">headers</code><code class="p">[</code><code class="s1">'server'</code><code class="p">])</code>
</pre>

<p><code>Out:</code></p>

<pre data-type="programlisting">
utf-8
application/json; charset=utf-8
GitHub.com
</pre>

<p>Looking at the response parameters, we understand that the response follows a UTF-8 encoding, and the content is returned using the <a contenteditable="false" data-type="indexterm" data-primary="JSON format" id="idm45634214948904"/>JSON format. The content can be directly accessed using the <code>content</code> element, which provides the payload in the form of bytes. Since we already know that the response is a JSON object, we can also use the <code>json()</code> command to read the response. This creates a list object where each element is a repository. We show the first element in the response that identifies the <a href="https://oreil.ly/L9b6L">first GitHub repository that was created</a>. We have limited the output to the first 200 characters for the sake of brevity:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="kn">import</code> <code class="nn">json</code>
<code class="k">print</code> <code class="p">(</code><code class="n">json</code><code class="o">.</code><code class="n">dumps</code><code class="p">(</code><code class="n">response</code><code class="o">.</code><code class="n">json</code><code class="p">()[</code><code class="mi">0</code><code class="p">],</code> <code class="n">indent</code><code class="o">=</code><code class="mi">2</code><code class="p">)[:</code><code class="mi">200</code><code class="p">])</code>
</pre>

<p><code>Out:</code></p>

<pre data-type="programlisting">
{
  "id": 1,
  "node_id": "MDEwOlJlcG9zaXRvcnkx",
  "name": "grit",
  "full_name": "mojombo/grit",
  "private": false,
  "owner": {
    "login": "mojombo",
    "id": 1,
    "node_id": "MDQ6VXNlcjE=",
</pre>

<p class="pagebreak-before">While the previous response contains a list of repositories, it is not helpful when looking for specific programming languages. It might be better to use the Search API, which we will use next:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">response</code> <code class="o">=</code> <code class="n">requests</code><code class="o">.</code><code class="n">get</code><code class="p">(</code><code class="s1">'https://api.github.com/search/repositories'</code><code class="p">)</code>
<code class="k">print</code> <code class="p">(</code><code class="n">response</code><code class="o">.</code><code class="n">status_code</code><code class="p">)</code>
</pre>

<p><code>Out:</code></p>

<pre data-type="programlisting">
422
</pre>

<p>The previous request was not successful as it returned with a <a href="https://httpstatuses.com/422">status code of 422</a>. This code indicates that the request was correct, but the server was not able to process the request. This is because we have not provided any search query parameter as specified in the <a href="https://oreil.ly/5EtSw">documentation</a>. It is important to always check and understand the status before proceeding to view the response. You can view a detailed definition of each status code in the <a href="https://oreil.ly/SG6tf">HTTP specification</a>.</p>

<p>Let’s say that we want to identify GitHub repositories related to data science that are written in Python. We will modify the request by adding a second argument called <code>params</code> with the search terms. The search query needs to be constructed following the rules described in <a href="https://oreil.ly/jNCff">GitHub’s documentation</a>. Based on these rules, our search query is encoded to look for <code>data_science</code>, filter the <code>language</code> by Python (<code>language:python</code>), and combine the two (<code>+</code>). This constructed query is passed as the query argument <code>q</code> to params. We also pass the argument <code>headers</code> containing the <code>Accept</code> parameter where we specify <code>text-match+json</code> so that the response contains the matching metadata and provides the response in JSON format:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">response</code> <code class="o">=</code> <code class="n">requests</code><code class="o">.</code><code class="n">get</code><code class="p">(</code><code class="s1">'https://api.github.com/search/repositories'</code><code class="p">,</code>
    <code class="n">params</code><code class="o">=</code><code class="p">{</code><code class="s1">'q'</code><code class="p">:</code> <code class="s1">'data_science+language:python'</code><code class="p">},</code>
    <code class="n">headers</code><code class="o">=</code><code class="p">{</code><code class="s1">'Accept'</code><code class="p">:</code> <code class="s1">'application/vnd.github.v3.text-match+json'</code><code class="p">})</code>
<code class="k">print</code><code class="p">(</code><code class="n">response</code><code class="o">.</code><code class="n">status_code</code><code class="p">)</code>
</pre>

<p><code>Out:</code></p>

<pre data-type="programlisting">
200
</pre>

<p>As described in the example provided in the API documentation for the <code>/search/repositories</code> endpoint, the response contains a dictionary with <code>total_count</code>, <code>incomplete_results</code>, and <code>items</code>. It is important to note that this response format is different from the <code>/repositories</code> endpoint we saw earlier, and we must parse this structure accordingly. Here we list the names of the top five repositories returned by the search:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="k">for</code> <code class="n">item</code> <code class="ow">in</code> <code class="n">response</code><code class="o">.</code><code class="n">json</code><code class="p">()[</code><code class="s1">'items'</code><code class="p">][:</code><code class="mi">5</code><code class="p">]:</code>
    <code class="n">printmd</code><code class="p">(</code><code class="s1">'**'</code> <code class="o">+</code> <code class="n">item</code><code class="p">[</code><code class="s1">'name'</code><code class="p">]</code> <code class="o">+</code> <code class="s1">'**'</code> <code class="o">+</code> <code class="s1">': repository '</code> <code class="o">+</code>
            <code class="n">item</code><code class="p">[</code><code class="s1">'text_matches'</code><code class="p">][</code><code class="mi">0</code><code class="p">][</code><code class="s1">'property'</code><code class="p">]</code> <code class="o">+</code> <code class="s1">' - </code><code class="se">\"</code><code class="s1">*'</code> <code class="o">+</code>
            <code class="n">item</code><code class="p">[</code><code class="s1">'text_matches'</code><code class="p">][</code><code class="mi">0</code><code class="p">][</code><code class="s1">'fragment'</code><code class="p">]</code> <code class="o">+</code> <code class="s1">'*</code><code class="se">\"</code><code class="s1"> matched with '</code> <code class="o">+</code> <code class="s1">'**'</code> <code class="o">+</code>
            <code class="n">item</code><code class="p">[</code><code class="s1">'text_matches'</code><code class="p">][</code><code class="mi">0</code><code class="p">][</code><code class="s1">'matches'</code><code class="p">][</code><code class="mi">0</code><code class="p">][</code><code class="s1">'text'</code><code class="p">]</code> <code class="o">+</code> <code class="s1">'**'</code><code class="p">)</code>
</pre>

<p><code>Out:</code></p>

<pre data-type="programlisting">
<strong>DataCamp</strong>: repository description - "<em>DataCamp data-science courses</em>" matched with
<strong>data</strong>

<strong>data-science-from-scratch</strong>: repository description - "<em>code for Data Science From
Scratch book</em>" matched with <strong>Data Science</strong>

<strong>data-science-blogs</strong>: repository description - "<em>A curated list of data science
blogs</em>" matched with <strong>data science</strong>

<strong>galaxy</strong>: repository description - "<em>Data intensive science for everyone.</em>" matched
with <strong>Data</strong>

<strong>data-scientist-roadmap</strong>: repository description - "<em>Tutorial coming with "data
science roadmap" graphe.</em>" matched with <strong>data science</strong>
</pre>

<p>We’ve seen how to make requests and parse the response. Let’s consider the <a contenteditable="false" data-type="indexterm" data-primary="use cases" data-secondary="for text data extraction" data-secondary-sortas="text data extraction" id="idm45634214686920"/><a contenteditable="false" data-type="indexterm" data-primary="text data extraction with APIs" data-secondary="use cases for" id="idm45634214685224"/>use case of monitoring the comments in a repository and ensuring that they adhere to community guidelines. We will use the <a href="https://oreil.ly/9l-fy">List Repository Issues</a> endpoint for this. Here we must specify the owner and the repository name to get all of the issue comments, and the response will contain a list of all comments in that repository. Let’s make this request for the PyTorch repository, which is a popular deep learning framework:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">response</code> <code class="o">=</code> <code class="n">requests</code><code class="o">.</code><code class="n">get</code><code class="p">(</code>
    <code class="s1">'https://api.github.com/repos/pytorch/pytorch/issues/comments'</code><code class="p">)</code>
<code class="k">print</code><code class="p">(</code><code class="s1">'Response Code'</code><code class="p">,</code> <code class="n">response</code><code class="o">.</code><code class="n">status_code</code><code class="p">)</code>
<code class="k">print</code><code class="p">(</code><code class="s1">'Number of comments'</code><code class="p">,</code> <code class="nb">len</code><code class="p">(</code><code class="n">response</code><code class="o">.</code><code class="n">json</code><code class="p">()))</code>
</pre>

<p><code>Out:</code></p>

<pre data-type="programlisting">
Response Code 200
Number of comments 30
</pre>

<p>While we see that the response has succeeded, the number of comments returned is only 30. PyTorch is a popular framework with a lot of collaborators and users. Checking the issues page of the repository in a browser would show us that the number of comments is much higher. So, what are we missing?</p>

<section data-type="sect2" data-pdf-bookmark="Pagination"><div class="sect2" id="idm45634214602392">
<h2>Pagination</h2>
<p>This is a <a contenteditable="false" data-type="indexterm" data-primary="text data extraction with APIs" data-secondary="pagination" id="ch2_term9"/>technique used by many APIs to limit the number of elements in the response. The total number of comments in a repository can be large, and attempting to respond with all of them would be time-intensive and costly. As a result, the GitHub API implements the pagination concept where it returns only one page at a time, and in this case each page contains 30 results. The <code>links</code> field in the response object <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch2_term7" id="idm45634214598120"/>provides details on the number of pages in the response.</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">response</code><code class="o">.</code><code class="n">links</code>
</pre>

<p><code>Out:</code></p>

<pre data-type="programlisting">
{'next': {'url': 'https://api.github.com/repositories/65600975/issues/
comments?page=2',
  'rel': 'next'},
 'last': {'url': 'https://api.github.com/repositories/65600975/issues/
comments?page=1334',
  'rel': 'last'}}
</pre>

<p>The <code>next</code> field provides us with a URL to the next page, which would contain the next 30 results, while the <code>last</code> field provides a link to the last page, which provides an indication of how many search results there are in total. The number of 30 results per page is also specified in the documentation and usually can be configured up to a certain maximum value. What does this mean for us? To get all the results, we must implement a function that will parse all the results on one page and then call the next URL until the last page has been reached. This is implemented as a recursive function where we check to see if a <code>next</code> link exists and recursively call the same function. The comments from each page are appended to the <code>output_json</code> object, which is finally returned. To restrict the number of comments that we retrieve, we use a filter parameter to fetch only the comments since July 2020. As per the documentation, the date must be specified using the ISO 8601 format and provided as a parameter using the <code>since</code> keyword:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="k">def</code> <code class="nf">get_all_pages</code><code class="p">(</code><code class="n">url</code><code class="p">,</code> <code class="n">params</code><code class="o">=</code><code class="bp">None</code><code class="p">,</code> <code class="n">headers</code><code class="o">=</code><code class="bp">None</code><code class="p">):</code>
    <code class="n">output_json</code> <code class="o">=</code> <code class="p">[]</code>
    <code class="n">response</code> <code class="o">=</code> <code class="n">requests</code><code class="o">.</code><code class="n">get</code><code class="p">(</code><code class="n">url</code><code class="p">,</code> <code class="n">params</code><code class="o">=</code><code class="n">params</code><code class="p">,</code> <code class="n">headers</code><code class="o">=</code><code class="n">headers</code><code class="p">)</code>
    <code class="k">if</code> <code class="n">response</code><code class="o">.</code><code class="n">status_code</code> <code class="o">==</code> <code class="mi">200</code><code class="p">:</code>
        <code class="n">output_json</code> <code class="o">=</code> <code class="n">response</code><code class="o">.</code><code class="n">json</code><code class="p">()</code>
        <code class="k">if</code> <code class="s1">'next'</code> <code class="ow">in</code> <code class="n">response</code><code class="o">.</code><code class="n">links</code><code class="p">:</code>
            <code class="n">next_url</code> <code class="o">=</code> <code class="n">response</code><code class="o">.</code><code class="n">links</code><code class="p">[</code><code class="s1">'next'</code><code class="p">][</code><code class="s1">'url'</code><code class="p">]</code>
            <code class="k">if</code> <code class="n">next_url</code> <code class="ow">is</code> <code class="ow">not</code> <code class="bp">None</code><code class="p">:</code>
                <code class="n">output_json</code> <code class="o">+=</code> <code class="n">get_all_pages</code><code class="p">(</code><code class="n">next_url</code><code class="p">,</code> <code class="n">params</code><code class="p">,</code> <code class="n">headers</code><code class="p">)</code>
    <code class="k">return</code> <code class="n">output_json</code>


<code class="n">out</code> <code class="o">=</code> <code class="n">get_all_pages</code><code class="p">(</code>
    <code class="s2">"https://api.github.com/repos/pytorch/pytorch/issues/comments"</code><code class="p">,</code>
    <code class="n">params</code><code class="o">=</code><code class="p">{</code>
        <code class="s1">'since'</code><code class="p">:</code> <code class="s1">'2020-07-01T10:00:01Z'</code><code class="p">,</code>
        <code class="s1">'sorted'</code><code class="p">:</code> <code class="s1">'created'</code><code class="p">,</code>
        <code class="s1">'direction'</code><code class="p">:</code> <code class="s1">'desc'</code>
    <code class="p">},</code>
    <code class="n">headers</code><code class="o">=</code><code class="p">{</code><code class="s1">'Accept'</code><code class="p">:</code> <code class="s1">'application/vnd.github.v3+json'</code><code class="p">})</code>
<code class="n">df</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">out</code><code class="p">)</code>

<code class="k">print</code> <code class="p">(</code><code class="n">df</code><code class="p">[</code><code class="s1">'body'</code><code class="p">]</code><code class="o">.</code><code class="n">count</code><code class="p">())</code>
<code class="n">df</code><code class="p">[[</code><code class="s1">'id'</code><code class="p">,</code><code class="s1">'created_at'</code><code class="p">,</code><code class="s1">'body'</code><code class="p">]]</code><code class="o">.</code><code class="n">sample</code><code class="p">(</code><code class="mi">1</code><code class="p">)</code>
</pre>

<p><code>Out:</code></p>

<pre data-type="programlisting">
3870
</pre>

<table class="dataframe">
	<thead>
		<tr>
			<th> </th>
			<th>id</th>
			<th>created_at</th>
			<th>body</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<th>2176</th>
			<td>286601372</td>
			<td>2017-03-15T00:09:46Z</td>
			<td>@soumith are you able to explain what dependency is broken? I can’t find the PR you mentioned.</td>
		</tr>
	</tbody>
</table>

<p>We have captured about 3,800 comments for the PyTorch repository by using the recursive pagination function, and we saw an example of one of these comments in the previous table. The dataset we have created here can be used to apply text analytics blueprints, for example, to identify comments that do not adhere to community guidelines and flag for moderation. It can also be augmented by running it at programmed time intervals to ensure that latest comments are always <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch2_term9" id="idm45634214423800"/>captured.</p>
</div></section>

<!-- make sect2 -->
<section data-type="sect2" data-pdf-bookmark="Rate Limiting"><div class="sect2" id="idm45634214422008">
<h2>Rate Limiting</h2>

<p>One issue that you might have noticed while extracting the comments is that we were <a contenteditable="false" data-type="indexterm" data-primary="rate limiting with data extraction" id="ch2_term11"/><a contenteditable="false" data-type="indexterm" data-primary="text data extraction with APIs" data-secondary="rate limiting" id="ch2_term12"/>able to retrieve only 3,800 comments. However, the actual number of comments is much more than that. This was a result of the API applying a rate limit. To ensure that an API can continue serving all users and avoid load on their infrastructure, providers will often enforce rate limits. The rate limit specifies how many requests can be made to an endpoint in a certain time frame. <a contenteditable="false" data-type="indexterm" data-primary="GitHub API" data-secondary="Rate Limiting policy of" id="idm45634214416712"/>GitHub’s <a href="https://oreil.ly/PH7hm">Rate Limiting policy</a> states the following:</p>

<blockquote><p>For unauthenticated requests, the rate limit allows for up to 60 requests per hour. Unauthenticated requests are associated with the originating IP address, and not the user making requests.</p></blockquote>

<p>The information about usage is contained in the <a contenteditable="false" data-type="indexterm" data-primary="headers object" id="idm45634214413064"/>headers section of the <a contenteditable="false" data-type="indexterm" data-primary="response object" id="idm45634214411832"/>response object. We can make a call to the API to only retrieve the <a contenteditable="false" data-type="indexterm" data-primary="X-Ratelimit header elements" id="idm45634214410568"/>headers by using the <code>head</code> method and then peering into the <code>X-Ratelimit-Limit</code>, <code>X-Ratelimit-Remaining</code>, and <code>X-RateLimit-Reset</code> header elements:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">response</code> <code class="o">=</code> <code class="n">requests</code><code class="o">.</code><code class="n">head</code><code class="p">(</code>
    <code class="s1">'https://api.github.com/repos/pytorch/pytorch/issues/comments'</code><code class="p">)</code>
<code class="k">print</code><code class="p">(</code><code class="s1">'X-Ratelimit-Limit'</code><code class="p">,</code> <code class="n">response</code><code class="o">.</code><code class="n">headers</code><code class="p">[</code><code class="s1">'X-Ratelimit-Limit'</code><code class="p">])</code>
<code class="k">print</code><code class="p">(</code><code class="s1">'X-Ratelimit-Remaining'</code><code class="p">,</code> <code class="n">response</code><code class="o">.</code><code class="n">headers</code><code class="p">[</code><code class="s1">'X-Ratelimit-Remaining'</code><code class="p">])</code>

<code class="c1"># Converting UTC time to human-readable format</code>
<code class="kn">import</code> <code class="nn">datetime</code>
<code class="k">print</code><code class="p">(</code>
    <code class="s1">'Rate Limits reset at'</code><code class="p">,</code>
    <code class="n">datetime</code><code class="o">.</code><code class="n">datetime</code><code class="o">.</code><code class="n">fromtimestamp</code><code class="p">(</code><code class="nb">int</code><code class="p">(</code>
        <code class="n">response</code><code class="o">.</code><code class="n">headers</code><code class="p">[</code><code class="s1">'X-RateLimit-Reset'</code><code class="p">]))</code><code class="o">.</code><code class="n">strftime</code><code class="p">(</code><code class="s1">'</code><code class="si">%c</code><code class="s1">'</code><code class="p">))</code>
</pre>

<p><code>Out:</code></p>

<pre data-type="programlisting">
X-Ratelimit-Limit 60
X-Ratelimit-Remaining 0
Rate Limits reset at Sun Sep 20 12:46:18 2020
</pre>

<p><code>X-Ratelimit-Limit</code> indicates how many requests can be made per unit of time (one hour in this case), <code>X-Ratelimit-Remaining</code> is the number of requests that can still be made without violating the rate limits, and <code>X-RateLimit-Reset</code> indicates the time at which the rate would be reset. It’s possible for different API endpoints to have different rate limits. For example, the GitHub Search API has a <a href="https://oreil.ly/95Fw7">per-minute rate limit</a>. If you exceed the rate limit by making requests that exceed the rate limit, then the API will respond with a status of 403.</p>

<p>While making API calls, we must honor the rate limits and also adjust the way we make our calls to ensure that we do not overload the server. While extracting comments from the repository as in the previous example, we are allowed to make 60 API calls every hour. We can make the requests one after the other, thereby quickly exhausting the limit, which is how our earlier blueprint works. The function <code>handle_rate_limits</code> shown next slows down the requests to ensure they are spaced out over the entire duration. It does so by distributing the remaining requests equally over the remaining time by applying a sleep function. This will ensure that our data extraction blueprint respects the rate limits and spaces the requests so that all the requested data is downloaded:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="kn">from</code> <code class="nn">datetime</code> <code class="kn">import</code> <code class="n">datetime</code>
<code class="kn">import</code> <code class="nn">time</code>

<code class="k">def</code> <code class="nf">handle_rate_limits</code><code class="p">(</code><code class="n">response</code><code class="p">):</code>
    <code class="n">now</code> <code class="o">=</code> <code class="n">datetime</code><code class="o">.</code><code class="n">now</code><code class="p">()</code>
    <code class="n">reset_time</code> <code class="o">=</code> <code class="n">datetime</code><code class="o">.</code><code class="n">fromtimestamp</code><code class="p">(</code>
        <code class="nb">int</code><code class="p">(</code><code class="n">response</code><code class="o">.</code><code class="n">headers</code><code class="p">[</code><code class="s1">'X-RateLimit-Reset'</code><code class="p">]))</code>
    <code class="n">remaining_requests</code> <code class="o">=</code> <code class="n">response</code><code class="o">.</code><code class="n">headers</code><code class="p">[</code><code class="s1">'X-Ratelimit-Remaining'</code><code class="p">]</code>
    <code class="n">remaining_time</code> <code class="o">=</code> <code class="p">(</code><code class="n">reset_time</code> <code class="o">-</code> <code class="n">now</code><code class="p">)</code><code class="o">.</code><code class="n">total_seconds</code><code class="p">()</code>
    <code class="n">intervals</code> <code class="o">=</code> <code class="n">remaining_time</code> <code class="o">/</code> <code class="p">(</code><code class="mf">1.0</code> <code class="o">+</code> <code class="nb">int</code><code class="p">(</code><code class="n">remaining_requests</code><code class="p">))</code>
    <code class="k">print</code><code class="p">(</code><code class="s1">'Sleeping for'</code><code class="p">,</code> <code class="n">intervals</code><code class="p">)</code>
    <code class="n">time</code><code class="o">.</code><code class="n">sleep</code><code class="p">(</code><code class="n">intervals</code><code class="p">)</code>
    <code class="k">return</code> <code class="bp">True</code>
</pre>

<p>Network communication including API calls can fail for several reasons, such as interrupted connections, failed DNS lookups, connection timeouts, and so on. By default, the requests library does not implement any retries, and therefore a nice addition to our blueprint is an implementation of a retry strategy. This will allow API calls to be retried in case of specified failure conditions. It can be <a contenteditable="false" data-type="indexterm" data-primary="HTTPAdapter library" id="idm45634214266664"/>implemented <span class="keep-together">with the</span> <code>HTTPAdapter</code> library that allows more fine-grained control of the underlying HTTP connections being made. Here we initialize an adapter with the retry strategy that specifies five retries for a failed attempt. We also specify that these retries <span class="keep-together">should be made</span> only when the error status codes <a href="https://httpstatuses.com/500">500</a>, <span class="keep-together"><a href="https://httpstatuses.com/503">503</a>, and</span> <a href="https://httpstatuses.com/504">504</a> are received. In addition, we specify the <code>backoff_factor</code><sup><a data-type="noteref" id="idm45634214166328-marker" href="ch02.xhtml#idm45634214166328">1</a></sup> value that determines the exponentially increasing time delay between attempts after the second try to ensure that we don’t hammer <span class="keep-together">the server</span>.</p>

<p>Every request object creates a default <code>Sessions</code> object that manages and persists connection settings across different requests, such as cookies, authentication, and proxies that should be stateless. Up to now we relied on the default <code>Sessions</code> object, but to override the connection behavior with our retry strategy, we have to specify a custom adapter that will enable us to use the retry strategy. This means that we will use the new <code>http Session</code> object to make our requests, as shown in the following code:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="kn">from</code> <code class="nn">requests.adapters</code> <code class="kn">import</code> <code class="n">HTTPAdapter</code>
<code class="kn">from</code> <code class="nn">requests.packages.urllib3.util.retry</code> <code class="kn">import</code> <code class="n">Retry</code>

<code class="n">retry_strategy</code> <code class="o">=</code> <code class="n">Retry</code><code class="p">(</code>
    <code class="n">total</code><code class="o">=</code><code class="mi">5</code><code class="p">,</code>
    <code class="n">status_forcelist</code><code class="o">=</code><code class="p">[</code><code class="mi">500</code><code class="p">,</code> <code class="mi">503</code><code class="p">,</code> <code class="mi">504</code><code class="p">],</code>
    <code class="n">backoff_factor</code><code class="o">=</code><code class="mi">1</code>
<code class="p">)</code>

<code class="n">retry_adapter</code> <code class="o">=</code> <code class="n">HTTPAdapter</code><code class="p">(</code><code class="n">max_retries</code><code class="o">=</code><code class="n">retry_strategy</code><code class="p">)</code>

<code class="n">http</code> <code class="o">=</code> <code class="n">requests</code><code class="o">.</code><code class="n">Session</code><code class="p">()</code>
<code class="n">http</code><code class="o">.</code><code class="n">mount</code><code class="p">(</code><code class="s2">"https://"</code><code class="p">,</code> <code class="n">retry_adapter</code><code class="p">)</code>
<code class="n">http</code><code class="o">.</code><code class="n">mount</code><code class="p">(</code><code class="s2">"http://"</code><code class="p">,</code> <code class="n">retry_adapter</code><code class="p">)</code>

<code class="n">response</code> <code class="o">=</code> <code class="n">http</code><code class="o">.</code><code class="n">get</code><code class="p">(</code><code class="s1">'https://api.github.com/search/repositories'</code><code class="p">,</code>
                   <code class="n">params</code><code class="o">=</code><code class="p">{</code><code class="s1">'q'</code><code class="p">:</code> <code class="s1">'data_science+language:python'</code><code class="p">})</code>

<code class="k">for</code> <code class="n">item</code> <code class="ow">in</code> <code class="n">response</code><code class="o">.</code><code class="n">json</code><code class="p">()[</code><code class="s1">'items'</code><code class="p">][:</code><code class="mi">5</code><code class="p">]:</code>
    <code class="k">print</code> <code class="p">(</code><code class="n">item</code><code class="p">[</code><code class="s1">'name'</code><code class="p">])</code>
</pre>

<p><code>Out:</code></p>

<pre data-type="programlisting">
DataCamp
data-science-from-scratch
data-science-blogs
galaxy
data-scientist-roadmap
</pre>
</div></section> <!--Check this -->

<p>Putting all this together, we can modify the blueprint to <a contenteditable="false" data-type="indexterm" data-primary="text data extraction with APIs" data-secondary="pagination" id="idm45634214027096"/>handle pagination, rate limits, and retries as follows:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="kn">from</code> <code class="nn">requests.adapters</code> <code class="kn">import</code> <code class="n">HTTPAdapter</code>
<code class="kn">from</code> <code class="nn">requests.packages.urllib3.util.retry</code> <code class="kn">import</code> <code class="n">Retry</code>

<code class="n">retry_strategy</code> <code class="o">=</code> <code class="n">Retry</code><code class="p">(</code>
    <code class="n">total</code><code class="o">=</code><code class="mi">5</code><code class="p">,</code>
    <code class="n">status_forcelist</code><code class="o">=</code><code class="p">[</code><code class="mi">500</code><code class="p">,</code> <code class="mi">503</code><code class="p">,</code> <code class="mi">504</code><code class="p">],</code>
    <code class="n">backoff_factor</code><code class="o">=</code><code class="mi">1</code>
<code class="p">)</code>

<code class="n">retry_adapter</code> <code class="o">=</code> <code class="n">HTTPAdapter</code><code class="p">(</code><code class="n">max_retries</code><code class="o">=</code><code class="n">retry_strategy</code><code class="p">)</code>

<code class="n">http</code> <code class="o">=</code> <code class="n">requests</code><code class="o">.</code><code class="n">Session</code><code class="p">()</code>
<code class="n">http</code><code class="o">.</code><code class="n">mount</code><code class="p">(</code><code class="s2">"https://"</code><code class="p">,</code> <code class="n">retry_adapter</code><code class="p">)</code>
<code class="n">http</code><code class="o">.</code><code class="n">mount</code><code class="p">(</code><code class="s2">"http://"</code><code class="p">,</code> <code class="n">retry_adapter</code><code class="p">)</code>

<code class="k">def</code> <code class="nf">get_all_pages</code><code class="p">(</code><code class="n">url</code><code class="p">,</code> <code class="n">param</code><code class="o">=</code><code class="bp">None</code><code class="p">,</code> <code class="n">header</code><code class="o">=</code><code class="bp">None</code><code class="p">):</code>
    <code class="n">output_json</code> <code class="o">=</code> <code class="p">[]</code>
    <code class="n">response</code> <code class="o">=</code> <code class="n">http</code><code class="o">.</code><code class="n">get</code><code class="p">(</code><code class="n">url</code><code class="p">,</code> <code class="n">params</code><code class="o">=</code><code class="n">param</code><code class="p">,</code> <code class="n">headers</code><code class="o">=</code><code class="n">header</code><code class="p">)</code>
    <code class="k">if</code> <code class="n">response</code><code class="o">.</code><code class="n">status_code</code> <code class="o">==</code> <code class="mi">200</code><code class="p">:</code>
        <code class="n">output_json</code> <code class="o">=</code> <code class="n">response</code><code class="o">.</code><code class="n">json</code><code class="p">()</code>
        <code class="k">if</code> <code class="s1">'next'</code> <code class="ow">in</code> <code class="n">response</code><code class="o">.</code><code class="n">links</code><code class="p">:</code>
            <code class="n">next_url</code> <code class="o">=</code> <code class="n">response</code><code class="o">.</code><code class="n">links</code><code class="p">[</code><code class="s1">'next'</code><code class="p">][</code><code class="s1">'url'</code><code class="p">]</code>
            <code class="k">if</code> <code class="p">(</code><code class="n">next_url</code> <code class="ow">is</code> <code class="ow">not</code> <code class="bp">None</code><code class="p">)</code> <code class="ow">and</code> <code class="p">(</code><code class="n">handle_rate_limits</code><code class="p">(</code><code class="n">response</code><code class="p">)):</code>
                <code class="n">output_json</code> <code class="o">+=</code> <code class="n">get_all_pages</code><code class="p">(</code><code class="n">next_url</code><code class="p">,</code> <code class="n">param</code><code class="p">,</code> <code class="n">header</code><code class="p">)</code>
    <code class="k">return</code> <code class="n">output_json</code>
</pre>

<p>If you look closely at the rate limit documentation, you will observe that there are <a contenteditable="false" data-type="indexterm" data-primary="authenticated API requests" id="idm45634214023176"/>different rate limits based on the type of authentication used. All our requests up to now were unauthenticated requests, and the rate limits are much lower. We can identify our data extraction application to GitHub by registering for an account. We can then make authenticated requests to the API that increases the rate limits. This practice ensures that there is no abuse of the API by unidentified users or fraudulent applications, and most API providers do not allow access to an API without a form of authentication.</p>

<p>This blueprint shows you how to extract data from any API using the simple Python requests module and creating your own dataset. This is the fundamental way in which most API requests work and is useful for a one-off analysis and initial exploration of a new data source. <a contenteditable="false" data-type="indexterm" data-primary="text data extraction with APIs" data-secondary="use cases for" id="ch2_term14"/><a contenteditable="false" data-type="indexterm" data-primary="use cases" data-secondary="for text data extraction" data-secondary-sortas="text data extraction" id="ch2_term15"/>Going back to our use case, if you were looking to identify the popular deep-learning frameworks for you to start learning, then this blueprint would be a good choice. Or let’s say that your organization already has a sales forecasting model and you would like to evaluate the benefit of adding financial market news on the accuracy of this model. Assuming there is an API that provides financial news, you can easily create a dataset, apply text analytics blueprints, and test the <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch2_term4" id="idm45634213826984"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch2_term5" id="idm45634213825608"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch2_term11" id="idm45634213824232"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch2_term12" id="idm45634213822856"/>relevance to the model.</p>
</div></section>

<section data-type="sect1" class="blueprint less_space" data-pdf-bookmark="Blueprint: Extracting Twitter Data with Tweepy"><div class="sect1" id="idm45634215053304">
<h1>Blueprint: Extracting Twitter Data with Tweepy</h1>

<p>To make it easier for developers to work with their APIs, many of the popular services provide packages in multiple programming languages or at least have one or more community-supported modules. While the API is officially supported, these packages are well-maintained Python modules that incorporate additional functionality that makes them easy to use. This means you can focus on the kind of data that you would like to extract rather than the technical details of making API calls, authentication, and so on. In this blueprint, we will use one of the community-developed and supported Python modules for <a contenteditable="false" data-type="indexterm" data-primary="Twitter" data-secondary="about" id="ch2_term13"/>Twitter <a contenteditable="false" data-type="indexterm" data-primary="Tweepy for data extraction" id="idm45634213817992"/><a contenteditable="false" data-type="indexterm" data-primary="text data extraction with APIs" data-secondary="Tweepy for " id="idm45634213816840"/>called <a href="https://oreil.ly/yZOU7">Tweepy</a>. Twitter maintains a list of <a href="https://oreil.ly/lwrFM">libraries for different languages</a> that includes several libraries for Python. We chose Tweepy because it’s actively maintained and used by many researchers. While this blueprint uses Tweepy to extract data from the Twitter API, the steps described would be similar for any other API.</p>

<p>We described earlier how you might use Twitter to analyze the effectiveness of a new marketing campaign. Another use case could be to perform text analytics to understand the popularity and sentiment for cryptocurrencies as a way to predict their adoption and value in the economy. Twitter is a social media network where users spontaneously share short messages, often reacting in real time to world events such as major calamities or popular sporting events. The user can also add the geolocation if they want to, and this gives us the ability to understand the most trending current events in a certain city or geographical area. During the government-imposed lockdowns due to COVID-19, several researchers used Twitter data to understand the spread of the virus and the <a href="https://oreil.ly/J7pDT">impact of lockdowns</a> and also used these as predictive variables of <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch2_term14" id="idm45634213811672"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch2_term15" id="idm45634213810296"/>economic health.</p>

<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>Please note that when using a public API like Twitter, you will retrieve data from the public timelines of many users, and it could contain strong, maybe even offensive, language, including profanities. Please be aware of this and ensure that the data is handled appropriately depending on your use-case.</p>
</div>

<section data-type="sect2" data-pdf-bookmark="Obtaining Credentials"><div class="sect2" id="idm45634213807176">
<h2>Obtaining Credentials</h2>
<p>The first <a contenteditable="false" data-type="indexterm" data-primary="text data extraction with APIs" data-secondary="credentials for" id="ch2_term17"/>step when working with any API is authenticating yourself or your application. Twitter requires all users of their API to register as a developer and provide details for why they would like to use the API. This helps them identify you and prevent any unauthorized access. You must <a href="https://oreil.ly/vEnJp">register yourself as a developer</a>. If you do not already have a Twitter account, then you will also be required to create one. You will be asked about your purpose for creating a developer account and additional questions on how you intend to use the Twitter API. <a data-type="xref" href="#fig-twitter-developer-account">Figure 2-2</a> shows some examples of these screens. Please provide detailed responses to ensure that Twitter fully understands your purpose for creating a developer account. For example, in this blueprint we are looking to extract tweets using the API to illustrate how this is done. Since we are only going to use the extraction capability, the question “Will your app use Tweet, Retweet, like, follow, or Direct Message functionality?” is not applicable and can be deselected. You must read and understand each question before proceeding. Note that this requirement will be different for each API and is also subject to change.</p>

<figure><div id="fig-twitter-developer-account" class="figure"><img src="Images/btap_0202.jpg" width="1079" height="404"/>
<h6><span class="label">Figure 2-2. </span>Illustration of sign-up flow for creating a Twitter developer account.</h6>
</div></figure>

<p>Now that you have a developer account, the next step is to create an app. The credentials of the app are used when making API calls, and it’s important to specify the reason for creating the app. You have to provide details like the app name, the purpose for creating the app, and the website URL that is associated with the app. If you will use this app for research and learning purposes, then you could state this in the app description and provide the URL for your university page or GitHub repository associated with your project. Once the app is approved by Twitter, you can navigate to the tab <em>Keys and tokens</em>, as shown in <a data-type="xref" href="#fig-twitter-app-creation">Figure 2-3</a>, where you will find the fields <em>API key</em> and <em>API secret key</em>. Please note that these are the credentials that will be used for authentication when making API calls, and it’s important to not <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch2_term13" id="idm45634213795640"/>reveal them.</p>

<figure><div id="fig-twitter-app-creation" class="figure"><img src="Images/btap_0203.jpg" width="990" height="374"/>
<h6><span class="label">Figure 2-3. </span>Creating a Twitter app and obtaining credentials.</h6>
</div></figure>
</div></section>
<section data-type="sect2" data-pdf-bookmark="Installing and Configuring Tweepy"><div class="sect2" id="idm45634213792552">
<h2>Installing and Configuring Tweepy</h2>
<p>The <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch2_term17" id="idm45634213790792"/><a contenteditable="false" data-type="indexterm" data-primary="text data extraction with APIs" data-secondary="Tweepy for " id="ch2_term18"/><a contenteditable="false" data-type="indexterm" data-primary="Tweepy for data extraction" id="ch2_term19"/>project repository for <a href="https://oreil.ly/OHfnn">Tweepy</a> and <a href="https://oreil.ly/lDDo1">documentation</a> are the best source for all information about using Tweepy. We can install Tweepy by entering <strong><code>pip install tweepy</code></strong> into the terminal. Next, we have to authenticate the app with the <a contenteditable="false" data-type="indexterm" data-primary="Twitter" data-secondary="data extraction from" id="ch2_term20"/>Twitter API, and we do this with the help of the <code>tweepy.AppAuthHandler</code> module to which we pass the API key and API secret key we obtained in the previous step. Finally, we instantiate the <code>tweepy.API</code> class, which will be used to make all subsequent calls to the Twitter API. Once the connection is made, we can confirm the host and version of the API object. Please note that since we are interested in read-only access to public information, we use <a href="https://oreil.ly/4oWbP">application-only authentication</a>:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="kn">import</code> <code class="nn">tweepy</code>

<code class="n">app_api_key</code> <code class="o">=</code> <code class="s1">'YOUR_APP_KEY_HERE'</code>
<code class="n">app_api_secret_key</code> <code class="o">=</code> <code class="s1">'YOUR_APP_SECRET_HERE'</code>


<code class="n">auth</code> <code class="o">=</code> <code class="n">tweepy</code><code class="o">.</code><code class="n">AppAuthHandler</code><code class="p">(</code><code class="n">app_api_key</code><code class="p">,</code> <code class="n">app_api_secret_key</code><code class="p">)</code>
<code class="n">api</code> <code class="o">=</code> <code class="n">tweepy</code><code class="o">.</code><code class="n">API</code><code class="p">(</code><code class="n">auth</code><code class="p">)</code>

<code class="k">print</code> <code class="p">(</code><code class="s1">'API Host'</code><code class="p">,</code> <code class="n">api</code><code class="o">.</code><code class="n">host</code><code class="p">)</code>
<code class="k">print</code> <code class="p">(</code><code class="s1">'API Version'</code><code class="p">,</code> <code class="n">api</code><code class="o">.</code><code class="n">api_root</code><code class="p">)</code>
</pre>

<p><code>Out:</code></p>

<pre data-type="programlisting">
API Host api.twitter.com
API Version /1.1
</pre>
</div></section>
<section data-type="sect2" data-pdf-bookmark="Extracting Data from the Search API"><div class="sect2" id="idm45634213666088">
<h2>Extracting Data from the Search API</h2>
<p>Let’s say we want to analyze the perception of cryptocurrency and determine its popularity. We will use the <a contenteditable="false" data-type="indexterm" data-primary="Twitter" data-secondary="Search API of " id="ch2_term31"/><a contenteditable="false" data-type="indexterm" data-primary="Search API of Twitter" id="ch2_term21"/><a contenteditable="false" data-type="indexterm" data-primary="text data extraction with APIs" data-secondary="from Twitter's Search API " data-secondary-sortas="Twitter's Search API" id="ch2_term22"/>Search API to <a contenteditable="false" data-type="indexterm" data-primary="datasets, examples of" data-secondary="Twitter tweets" id="ch2_term23"/>retrieve all tweets that mention this to create our dataset. The Twitter API also uses <a contenteditable="false" data-type="indexterm" data-primary="text data extraction with APIs" data-secondary="pagination" id="idm45634213657256"/>pagination to return multiple pages of results, but instead of implementing our own way of managing this, we will use <a contenteditable="false" data-type="indexterm" data-primary="Cursor object (Tweepy)" id="idm45634213655656"/>the <code>Cursor</code> object provided by the Tweepy library to iterate through the results. We pass the search query to the API object and additionally specify the language of the tweets to be extracted (English in this case). We choose to retrieve only 100 items and create a <code>DataFrame</code> by loading the results as a JSON object:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">search_term</code> <code class="o">=</code> <code class="s1">'cryptocurrency'</code>

<code class="n">tweets</code> <code class="o">=</code> <code class="n">tweepy</code><code class="o">.</code><code class="n">Cursor</code><code class="p">(</code><code class="n">api</code><code class="o">.</code><code class="n">search</code><code class="p">,</code>
                       <code class="n">q</code><code class="o">=</code><code class="n">search_term</code><code class="p">,</code>
                       <code class="n">lang</code><code class="o">=</code><code class="s2">"en"</code><code class="p">)</code><code class="o">.</code><code class="n">items</code><code class="p">(</code><code class="mi">100</code><code class="p">)</code>

<code class="n">retrieved_tweets</code> <code class="o">=</code> <code class="p">[</code><code class="n">tweet</code><code class="o">.</code><code class="n">_json</code> <code class="k">for</code> <code class="n">tweet</code> <code class="ow">in</code> <code class="n">tweets</code><code class="p">]</code>
<code class="n">df</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">json_normalize</code><code class="p">(</code><code class="n">retrieved_tweets</code><code class="p">)</code>

<code class="n">df</code><code class="p">[[</code><code class="s1">'text'</code><code class="p">]]</code><code class="o">.</code><code class="n">sample</code><code class="p">(</code><code class="mi">3</code><code class="p">)</code>
</pre>

<table class="dataframe">
	<thead>
		<tr>
			<th> </th>
			<th>text</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<th>59</th>
			<td>Hi! I’ve been using OKEx which makes it really easy and safe to buy, sell, and store cryptocurrency (like Bitcoin).… https://t.co/4m0mpyQTSN</td>
		</tr>
		<tr>
			<th>17</th>
			<td class="emoji">Get connected today 📉 #getconnected #bitcointrading #Bitcoin #BitcoinCash #bitcoinmining #cryptocurrency https://t.co/J60bCyFPUI</td>
		</tr>
		<tr>
			<th>22</th>
			<td>RT @stoinkies: We reached over 100 followers!\nGiveaway time!\nFOLLOW +RETWEET + LIKE THIS TWEET = Win 200 Dogecoin!\nEvery participant also g…</td>
		</tr>
	</tbody>
</table>

<p>We have successfully completed the API call and can see the text of the retrieved tweets in the previous table, which already show interesting aspects. For example, we see the use of the word <em>RT</em>, which indicates a retweet (where the user has shared another tweet). We see the usage of emojis, which is a strong characteristic of the medium, and also notice that some tweets are truncated. Twitter actually imposes a limit on the number of characters that each tweet can contain, which was originally 140 characters and later extended to 280. This led to the creation of an <a href="https://oreil.ly/fvl-3">extended tweet object</a>, which we must specify explicitly while retrieving results in Tweepy. Additionally, you must be aware that the standard version of the Twitter Search API provides results only from the last week, and one must sign up for the Premium or Enterprise versions for historical tweets.</p>

<div data-type="note" epub:type="note"><h6>Note</h6>
<p>For each endpoint, Twitter specifies a maximum value of <code>count</code>. This is the maximum number of results that is returned in a single page of the response. For example, the search endpoint specifies a maximum value of <code>count=100</code>, whereas <code>user_timeline</code> has a maximum value of <code>count=200</code>.</p>
</div>

<p>Let’s expand our search to include an additional keyword relevant to the cryptocurrency topic like <code>crypto</code> and filter out retweets for now. This is done by using the <code>filter</code> keyword appended with a minus sign in the search term. We also <a contenteditable="false" data-type="indexterm" data-primary="tweet_mode=extended parameter" id="idm45634213554984"/>specify that we would like to fetch tweets with the <code>tweet_mode=extended</code> parameter, which ensures that we retrieve the full text of all tweets. The <a href="https://oreil.ly/4IGcB">Standard search API</a> searches only a sample of recent Tweets published in the past seven days, but even this could potentially be a large number, and to avoid a large wait time to run the blueprint, we restrict ourselves to 12,000 tweets. We specify the parameter <code>count=30</code>, which is the maximum number of tweets that can be retrieved in one call. Therefore, we must make 400 such calls to obtain our dataset while taking into consideration the <a contenteditable="false" data-type="indexterm" data-primary="rate limiting with data extraction" id="idm45634213551752"/><a contenteditable="false" data-type="indexterm" data-primary="text data extraction with APIs" data-secondary="rate limiting" id="idm45634213550680"/>rate <span class="keep-together">limits</span>. This is within the rate limit of 450 requests every 15 minutes specified by the API. It’s possible that you might exceed this rate limit while experimenting with this blueprint, and therefore <a contenteditable="false" data-type="indexterm" data-primary="wait_on_rate_limit parameter" id="idm45634213548280"/>we enable the automatic wait functionality provided <span class="keep-together">by Tweepy</span> by setting the <code>wait_on_rate_limit</code> parameter. We also set <code>wait_on_rate_limit_notify</code> so that we are notified of such wait times. If you are within the rate limits, the following function should execute in about five minutes:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">api</code> <code class="o">=</code> <code class="n">tweepy</code><code class="o">.</code><code class="n">API</code><code class="p">(</code><code class="n">auth</code><code class="p">,</code>
                 <code class="n">wait_on_rate_limit</code><code class="o">=</code><code class="bp">True</code><code class="p">,</code>
                 <code class="n">wait_on_rate_limit_notify</code><code class="o">=</code><code class="bp">True</code><code class="p">,</code>
                 <code class="n">retry_count</code><code class="o">=</code><code class="mi">5</code><code class="p">,</code>
                 <code class="n">retry_delay</code><code class="o">=</code><code class="mi">10</code><code class="p">)</code>

<code class="n">search_term</code> <code class="o">=</code> <code class="s1">'cryptocurrency OR crypto -filter:retweets'</code>

<code class="n">tweets</code> <code class="o">=</code> <code class="n">tweepy</code><code class="o">.</code><code class="n">Cursor</code><code class="p">(</code><code class="n">api</code><code class="o">.</code><code class="n">search</code><code class="p">,</code>
                       <code class="n">q</code><code class="o">=</code><code class="n">search_term</code><code class="p">,</code>
                       <code class="n">lang</code><code class="o">=</code><code class="s2">"en"</code><code class="p">,</code>
                       <code class="n">tweet_mode</code><code class="o">=</code><code class="s1">'extended'</code><code class="p">,</code>
                       <code class="n">count</code><code class="o">=</code><code class="mi">30</code><code class="p">)</code><code class="o">.</code><code class="n">items</code><code class="p">(</code><code class="mi">12000</code><code class="p">)</code>

<code class="n">retrieved_tweets</code> <code class="o">=</code> <code class="p">[</code><code class="n">tweet</code><code class="o">.</code><code class="n">_json</code> <code class="k">for</code> <code class="n">tweet</code> <code class="ow">in</code> <code class="n">tweets</code><code class="p">]</code>

<code class="n">df</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">json_normalize</code><code class="p">(</code><code class="n">retrieved_tweets</code><code class="p">)</code>
<code class="k">print</code><code class="p">(</code><code class="s1">'Number of retrieved tweets '</code><code class="p">,</code> <code class="nb">len</code><code class="p">(</code><code class="n">df</code><code class="p">))</code>
<code class="n">df</code><code class="p">[[</code><code class="s1">'created_at'</code><code class="p">,</code><code class="s1">'full_text'</code><code class="p">,</code><code class="s1">'entities.hashtags'</code><code class="p">]]</code><code class="o">.</code><code class="n">sample</code><code class="p">(</code><code class="mi">2</code><code class="p">)</code>
</pre>

<p><code>Out:</code></p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">Number</code> <code class="n">of</code> <code class="n">retrieved</code> <code class="n">tweets</code>  <code class="mi">12000</code></pre>

<table class="dataframe">
	<thead>
		<tr>
			<th> </th>
			<th>created_at</th>
			<th>full_text</th>
			<th>entities.hashtags</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<th>10505</th>
			<td>Sat Sep 19 22:30:12 +0000 2020</td>
			<td class="emoji">Milk was created to let liquidity providers (people who have LP tokens) benefit because they can stake LP tokens at SpaceSwap, they get MILK token as a reward as well as 0.3% UniSwap commission.\n\n👇👇👇\nhttps://t.co/M7sGbIDq4W\n#DeFi #cryptocurrency #UniSwap #altcoin</td>
			<td>[{'text’: ‘DeFi', ‘indices’: [224, 229]}, {'text’: ‘cryptocurrency', ‘indices’: [230, 245]}, {'text’: ‘UniSwap', ‘indices’: [246, 254]}, {'text’: ‘altcoin', ‘indices’: [256, 264]}]</td>
		</tr>
		<tr>
			<th>11882</th>
			<td>Sat Sep 19 20:57:45 +0000 2020</td>
			<td>You can EARN dividends from our curation activity. The minimum to participate is 2000 #steem delegation... with delegation there is no risk of losing your principal. We can process the payout in #bitcoin and all major #cryptocurrencies .. #cryptocurrency \nhttps://t.co/4b3iH2AI4S</td>
			<td>[{'text’: ’steem', ‘indices’: [86, 92]}, {'text’: ‘bitcoin', ‘indices’: [195, 203]}, {'text’: ‘cryptocurrencies', ‘indices’: [218, 235]}, {'text’: ‘cryptocurrency', ‘indices’: [239, 254]}]</td>
		</tr>
	</tbody>
</table>

<p>There is a lot of information that the API provides, as shown in the sample of two previous tweets that contain important elements such as the date when the tweet was sent out, the content of the tweet, and so on. Twitter also returns several entities such as <a contenteditable="false" data-type="indexterm" data-primary="hashtags in tweets" id="idm45634213409288"/>hashtags contained within the tweet, and it would be interesting to see which hashtags are used heavily when discussing cryptocurrency:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="k">def</code> <code class="nf">extract_entities</code><code class="p">(</code><code class="n">entity_list</code><code class="p">):</code>
    <code class="n">entities</code> <code class="o">=</code> <code class="nb">set</code><code class="p">()</code>
    <code class="k">if</code> <code class="nb">len</code><code class="p">(</code><code class="n">entity_list</code><code class="p">)</code> <code class="o">!=</code> <code class="mi">0</code><code class="p">:</code>
        <code class="k">for</code> <code class="n">item</code> <code class="ow">in</code> <code class="n">entity_list</code><code class="p">:</code>
            <code class="k">for</code> <code class="n">key</code><code class="p">,</code><code class="n">value</code> <code class="ow">in</code> <code class="n">item</code><code class="o">.</code><code class="n">items</code><code class="p">():</code>
                <code class="k">if</code> <code class="n">key</code> <code class="o">==</code> <code class="s1">'text'</code><code class="p">:</code>
                    <code class="n">entities</code><code class="o">.</code><code class="n">add</code><code class="p">(</code><code class="n">value</code><code class="o">.</code><code class="n">lower</code><code class="p">())</code>
    <code class="k">return</code> <code class="nb">list</code><code class="p">(</code><code class="n">entities</code><code class="p">)</code>

<code class="n">df</code><code class="p">[</code><code class="s1">'Entities'</code><code class="p">]</code> <code class="o">=</code> <code class="n">df</code><code class="p">[</code><code class="s1">'entities.hashtags'</code><code class="p">]</code><code class="o">.</code><code class="n">apply</code><code class="p">(</code><code class="n">extract_entities</code><code class="p">)</code>
<code class="n">pd</code><code class="o">.</code><code class="n">Series</code><code class="p">(</code><code class="n">np</code><code class="o">.</code><code class="n">concatenate</code><code class="p">(</code><code class="n">df</code><code class="p">[</code><code class="s1">'Entities'</code><code class="p">]))</code><code class="o">.</code><code class="n">value_counts</code><code class="p">()[:</code><code class="mi">25</code><code class="p">]</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">kind</code><code class="o">=</code><code class="s1">'barh'</code><code class="p">)</code>
</pre>

<!-- <p><code>Out:</code></p> -->

<p>The preceding code creates the graph shown in <a data-type="xref" href="#fig02in01">Figure 2-4</a>, which shows us the important hashtags being used in conjunction with cryptocurrencies. It includes examples of cryptocurrencies such as <em>bitcoin</em> and <em>ethereum</em> as well as their trading short-codes <em>btc</em> and <em>eth</em>. It also throws up related activities such as <em>trading</em> and <em>airdrops</em>. There are also mentions of entities like <em>fintech</em> and <em>applecash</em>. At a first glance, it already gives you insight into the various terms and entities being discussed, and the presence of trading short-codes indicates that there might be some market information contained in these tweets. While this is a simple count of entities, we can use this dataset to apply more advanced text analytics techniques to determine popular sentiment about cryptocurrencies that derive relationships between entities. Please note that the results may differ depending on when the Twitter search was run and the random <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch2_term21" id="idm45634213251160"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch2_term22" id="idm45634213249784"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch2_term31" id="idm45634213248408"/>selection by the API.</p>

<figure><div id="fig02in01" class="figure"><img src="Images/btap_02in01.jpg" width="1260" height="1256"/>
<h6><span class="label">Figure 2-4. </span>Common hashtags used when discussing cryptocurrency.</h6>
</div></figure>

</div></section>
<section data-type="sect2" data-pdf-bookmark="Extracting Data from a User’s Timeline"><div class="sect2" id="idm45634213665496">
<h2>Extracting Data from a User’s Timeline</h2>

<p>Search is not the only way to interact with Twitter as we can use the API to also <a contenteditable="false" data-type="indexterm" data-primary="text data extraction with APIs" data-secondary="from Twitter user's timeline" data-secondary-sortas="Twitter user's timeline" id="ch2_term24"/><a contenteditable="false" data-type="indexterm" data-primary="Twitter" data-secondary="user's timelines on" id="ch2_term25"/>extract tweets by a specific user or account. This might be a person like a famous celebrity or world leader, or it might be an organization like a sports team. For instance, <a contenteditable="false" data-type="indexterm" data-primary="use cases" data-secondary="for text data extraction" data-secondary-sortas="text data extraction" id="ch2_term26"/><a contenteditable="false" data-type="indexterm" data-primary="text data extraction with APIs" data-secondary="use cases for" id="ch2_term27"/>what if we would like to compare tweets from two popular Formula One teams, Mercedes and Ferrari? We can extract all the tweets that they have sent out and contrast their individual styles and the main themes that they focus on. We provide the screen name for the account (<code>MercedesAMGF1</code>) to retrieve all the tweets sent by this account:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">api</code> <code class="o">=</code> <code class="n">tweepy</code><code class="o">.</code><code class="n">API</code><code class="p">(</code><code class="n">auth</code><code class="p">,</code> <code class="n">wait_on_rate_limit</code><code class="o">=</code><code class="bp">True</code><code class="p">,</code> <code class="n">wait_on_rate_limit_notify</code><code class="o">=</code><code class="bp">True</code><code class="p">)</code>

<code class="n">tweets</code> <code class="o">=</code> <code class="n">tweepy</code><code class="o">.</code><code class="n">Cursor</code><code class="p">(</code><code class="n">api</code><code class="o">.</code><code class="n">user_timeline</code><code class="p">,</code>
                       <code class="n">screen_name</code><code class="o">=</code><code class="s1">'MercedesAMGF1'</code><code class="p">,</code>
                       <code class="n">lang</code><code class="o">=</code><code class="s2">"en"</code><code class="p">,</code>
                       <code class="n">tweet_mode</code><code class="o">=</code><code class="s1">'extended'</code><code class="p">,</code>
                       <code class="n">count</code><code class="o">=</code><code class="mi">100</code><code class="p">)</code><code class="o">.</code><code class="n">items</code><code class="p">(</code><code class="mi">5000</code><code class="p">)</code>

<code class="n">retrieved_tweets</code> <code class="o">=</code> <code class="p">[</code><code class="n">tweet</code><code class="o">.</code><code class="n">_json</code> <code class="k">for</code> <code class="n">tweet</code> <code class="ow">in</code> <code class="n">tweets</code><code class="p">]</code>
<code class="n">df</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">io</code><code class="o">.</code><code class="n">json</code><code class="o">.</code><code class="n">json_normalize</code><code class="p">(</code><code class="n">retrieved_tweets</code><code class="p">)</code>
<code class="k">print</code> <code class="p">(</code><code class="s1">'Number of retrieved tweets '</code><code class="p">,</code> <code class="nb">len</code><code class="p">(</code><code class="n">df</code><code class="p">))</code>
</pre>

<p><code>Out:</code></p>

<pre data-type="programlisting">
Number of retrieved tweets  3232
</pre>

<p>As you can see, though we requested 5,000 tweets, we were able to retrieve only about 3,200 of them. This is a <a href="https://oreil.ly/RaNaQ">restriction placed on the API</a>. <span class="keep-together">Let’s retrieve</span> the tweets for the Ferrari team as well using their screen name <span class="keep-together">(<code>ScuderiaFerrari</code>)</span>:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="k">def</code> <code class="nf">get_user_timeline</code><code class="p">(</code><code class="n">screen_name</code><code class="p">):</code>
    <code class="n">api</code> <code class="o">=</code> <code class="n">tweepy</code><code class="o">.</code><code class="n">API</code><code class="p">(</code><code class="n">auth</code><code class="p">,</code>
                     <code class="n">wait_on_rate_limit</code><code class="o">=</code><code class="bp">True</code><code class="p">,</code>
                     <code class="n">wait_on_rate_limit_notify</code><code class="o">=</code><code class="bp">True</code><code class="p">)</code>
    <code class="n">tweets</code> <code class="o">=</code> <code class="n">tweepy</code><code class="o">.</code><code class="n">Cursor</code><code class="p">(</code><code class="n">api</code><code class="o">.</code><code class="n">user_timeline</code><code class="p">,</code>
                           <code class="n">screen_name</code><code class="o">=</code><code class="n">screen_name</code><code class="p">,</code>
                           <code class="n">lang</code><code class="o">=</code><code class="s2">"en"</code><code class="p">,</code>
                           <code class="n">tweet_mode</code><code class="o">=</code><code class="s1">'extended'</code><code class="p">,</code>
                           <code class="n">count</code><code class="o">=</code><code class="mi">200</code><code class="p">)</code><code class="o">.</code><code class="n">items</code><code class="p">()</code>
    <code class="n">retrieved_tweets</code> <code class="o">=</code> <code class="p">[</code><code class="n">tweet</code><code class="o">.</code><code class="n">_json</code> <code class="k">for</code> <code class="n">tweet</code> <code class="ow">in</code> <code class="n">tweets</code><code class="p">]</code>
    <code class="n">df</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">io</code><code class="o">.</code><code class="n">json</code><code class="o">.</code><code class="n">json_normalize</code><code class="p">(</code><code class="n">retrieved_tweets</code><code class="p">)</code>
    <code class="n">df</code> <code class="o">=</code> <code class="n">df</code><code class="p">[</code><code class="o">~</code><code class="n">df</code><code class="p">[</code><code class="s1">'retweeted_status.id'</code><code class="p">]</code><code class="o">.</code><code class="n">isna</code><code class="p">()]</code>
    <code class="k">return</code> <code class="n">df</code>
</pre>

<pre data-code-language="python" data-type="programlisting">
<code class="n">df_mercedes</code> <code class="o">=</code> <code class="n">get_user_timeline</code><code class="p">(</code><code class="s1">'MercedesAMGF1'</code><code class="p">)</code>
<code class="k">print</code> <code class="p">(</code><code class="s1">'Number of Tweets from Mercedes'</code><code class="p">,</code> <code class="nb">len</code><code class="p">(</code><code class="n">df_mercedes</code><code class="p">))</code>
<code class="n">df_ferrari</code> <code class="o">=</code> <code class="n">get_user_timeline</code><code class="p">(</code><code class="s1">'ScuderiaFerrari'</code><code class="p">)</code>
<code class="k">print</code> <code class="p">(</code><code class="s1">'Number of Tweets from Ferrari'</code><code class="p">,</code> <code class="nb">len</code><code class="p">(</code><code class="n">df_ferrari</code><code class="p">))</code>
</pre>

<p><code>Out:</code></p>

<pre data-type="programlisting">
Number of Tweets from Mercedes 180
Number of Tweets from Ferrari 203
</pre>

<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>One of the quirks of the Tweepy implementation is that in the case of retweets, the <code>full_text</code> column is truncated, and the <code>retweeted_status.full_text</code> column must be used to retrieve all the characters of the tweet. For our use case, retweets are not important, and we filter them by checking if <code>retweeted_status.id</code> is empty. However, depending on the use case, you can add a condition to replace the column <code>full_text</code> with <code>retweeted_status.full_text</code> in the case of retweets.</p>
</div>

<p>When we remove retweets, the number of tweets authored by each team handle significantly drops. We will reuse the word <a contenteditable="false" data-type="indexterm" data-primary="visualization of data" data-secondary="with word clouds " data-secondary-sortas="word clouds " id="idm45634212941432"/><a contenteditable="false" data-type="indexterm" data-primary="word clouds " id="idm45634212939784"/>cloud blueprint from <a data-type="xref" href="ch01.xhtml#ch-exploration">Chapter 1</a> with the function <code>wordcloud</code> to quickly visualize the tweets from each of the two teams and identify the keywords they focus on. Mercedes tweets seem to focus a lot on the races that the team participates in, such as <em>tuscangp</em>, <em>britishgp</em> and <em>race</em>, <em>day</em>. The Ferrari tweets, on the other hand, promote their merchandise, such as <em>ferraristore</em>, and drivers, such as <em>enzofitti</em> and <em>schumachermick</em>:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="kn">from</code> <code class="nn">blueprints.exploration</code> <code class="kn">import</code> <code class="n">wordcloud</code>

<code class="n">plt</code><code class="o">.</code><code class="n">figure</code><code class="p">()</code>
<code class="n">wordcloud</code><code class="p">(</code><code class="n">df_mercedes</code><code class="p">[</code><code class="s1">'full_text'</code><code class="p">],</code>
          <code class="n">max_words</code><code class="o">=</code><code class="mi">100</code><code class="p">,</code>
          <code class="n">stopwords</code><code class="o">=</code><code class="n">df_mercedes</code><code class="o">.</code><code class="n">head</code><code class="p">(</code><code class="mi">5</code><code class="p">)</code><code class="o">.</code><code class="n">index</code><code class="p">)</code>

<code class="n">wordcloud</code><code class="p">(</code><code class="n">df_ferrari</code><code class="p">[</code><code class="s1">'full_text'</code><code class="p">],</code>
          <code class="n">max_words</code><code class="o">=</code><code class="mi">100</code><code class="p">,</code>
          <code class="n">stopwords</code><code class="o">=</code><code class="n">df_ferrari</code><code class="o">.</code><code class="n">head</code><code class="p">(</code><code class="mi">5</code><code class="p">)</code><code class="o">.</code><code class="n">index</code><code class="p">)</code>
</pre>

<p><code>Out:</code></p>

<figure><div class="figure"><img src="Images/btap_02in02.jpg" width="630" height="152"/>
<h6/>
</div></figure>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Extracting Data from the Streaming API"><div class="sect2" id="idm45634213245112">

<h2>Extracting Data from the Streaming API</h2>

<p>Some <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch2_term24" id="idm45634212819048"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch2_term25" id="idm45634212817640"/>APIs provide near real-time data, which might also be <a contenteditable="false" data-type="indexterm" data-primary="Streaming API of Twitter" id="ch2_term28"/><a contenteditable="false" data-type="indexterm" data-primary="text data extraction with APIs" data-secondary="from Twitter's Streaming API" data-secondary-sortas="Twitter's Streaming API" id="ch2_term29"/><a contenteditable="false" data-type="indexterm" data-primary="Twitter" data-secondary="Streaming API of" id="ch2_term30"/>referred to as <em>streaming data</em>. In such a scenario, the API would like to <em>push</em> the data to us rather than waiting for a <em>get</em> request as we have been doing so far. An example of this is the Twitter Streaming API. This API provides us with a sample of the tweets being sent out in real time and can be filtered on several criteria. Since this is a continuous stream of data, we have to handle the data extraction process in a different manner. <a contenteditable="false" data-type="indexterm" data-primary="StreamListener (Tweepy)" id="idm45634212809416"/>Tweepy already provides basic functionality in the <code>StreamListener</code> class that contains the <code>on_data</code> function. This function is called each time a new tweet is pushed by the streaming API, and we can customize it to implement logic that is specific to certain use cases.</p>

<p>Staying with the cryptocurrency use case, let’s suppose that we want to have a continuously updated sentiment measure of different cryptocurrencies to make trading decisions. In this case, we would track real-time tweets mentioning cryptocurrencies and continuously update the popularity score. On the other hand, as researchers, we might be interested in analyzing the reactions of users during key live events such as the Super Bowl or announcement of election results. In such scenarios, we would listen for the entire duration of the event and store the results for subsequent analysis. To <a contenteditable="false" data-type="indexterm" data-primary="FileStreamListener (Tweepy)" id="idm45634212806200"/>keep this blueprint generic, we have created the <code>FileStreamListener</code> class as shown next, which will manage all the actions to be taken on the stream of incoming tweets. For every tweet pushed by the Twitter API, the <code>on_data</code> method is called. In our implementation, we gather incoming tweets into batches of 100 and then write to a file with the timestamp. The choice of 100 can be varied based on the memory available on the system:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="kn">from</code> <code class="nn">datetime</code> <code class="kn">import</code> <code class="n">datetime</code>
<code class="kn">import</code> <code class="nn">math</code>

<code class="k">class</code> <code class="nc">FileStreamListener</code><code class="p">(</code><code class="n">tweepy</code><code class="o">.</code><code class="n">StreamListener</code><code class="p">):</code>

    <code class="k">def</code> <code class="nf-Magic">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">max_tweets</code><code class="o">=</code><code class="n">math</code><code class="o">.</code><code class="n">inf</code><code class="p">):</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">num_tweets</code> <code class="o">=</code> <code class="mi">0</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">TWEETS_FILE_SIZE</code> <code class="o">=</code> <code class="mi">100</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">num_files</code> <code class="o">=</code> <code class="mi">0</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">tweets</code> <code class="o">=</code> <code class="p">[]</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">max_tweets</code> <code class="o">=</code> <code class="n">max_tweets</code>

    <code class="k">def</code> <code class="nf">on_data</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">data</code><code class="p">):</code>
        <code class="k">while</code> <code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">num_files</code> <code class="o">*</code> <code class="bp">self</code><code class="o">.</code><code class="n">TWEETS_FILE_SIZE</code> <code class="o">&lt;</code> <code class="bp">self</code><code class="o">.</code><code class="n">max_tweets</code><code class="p">):</code>
            <code class="bp">self</code><code class="o">.</code><code class="n">tweets</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">json</code><code class="o">.</code><code class="n">loads</code><code class="p">(</code><code class="n">data</code><code class="p">))</code>
            <code class="bp">self</code><code class="o">.</code><code class="n">num_tweets</code> <code class="o">+=</code> <code class="mi">1</code>
            <code class="k">if</code> <code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">num_tweets</code> <code class="o">&lt;</code> <code class="bp">self</code><code class="o">.</code><code class="n">TWEETS_FILE_SIZE</code><code class="p">):</code>
                <code class="k">return</code> <code class="bp">True</code>
            <code class="k">else</code><code class="p">:</code>
                <code class="n">filename</code> <code class="o">=</code> <code class="s1">'Tweets_'</code> <code class="o">+</code> <code class="nb">str</code><code class="p">(</code><code class="n">datetime</code><code class="o">.</code><code class="n">now</code><code class="p">()</code><code class="o">.</code><code class="n">time</code><code class="p">())</code> <code class="o">+</code> <code class="s1">'.txt'</code>
                <code class="k">print</code> <code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">TWEETS_FILE_SIZE</code><code class="p">,</code> <code class="s1">'Tweets saved to'</code><code class="p">,</code> <code class="n">filename</code><code class="p">)</code>
                <code class="nb">file</code> <code class="o">=</code> <code class="nb">open</code><code class="p">(</code><code class="n">filename</code><code class="p">,</code> <code class="s2">"w"</code><code class="p">)</code>
                <code class="n">json</code><code class="o">.</code><code class="n">dump</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">tweets</code><code class="p">,</code> <code class="nb">file</code><code class="p">)</code>
                <code class="nb">file</code><code class="o">.</code><code class="n">close</code><code class="p">()</code>
                <code class="bp">self</code><code class="o">.</code><code class="n">num_files</code> <code class="o">+=</code> <code class="mi">1</code>
                <code class="bp">self</code><code class="o">.</code><code class="n">tweets</code> <code class="o">=</code> <code class="p">[]</code>
                <code class="bp">self</code><code class="o">.</code><code class="n">num_tweets</code> <code class="o">=</code> <code class="mi">0</code>
                <code class="k">return</code> <code class="bp">True</code>
        <code class="k">return</code> <code class="bp">False</code>

    <code class="k">def</code> <code class="nf">on_error</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">status_code</code><code class="p">):</code>
        <code class="k">if</code> <code class="n">status_code</code> <code class="o">==</code> <code class="mi">420</code><code class="p">:</code>
            <code class="k">print</code> <code class="p">(</code><code class="s1">'Too many requests were made, please stagger requests'</code><code class="p">)</code>
            <code class="k">return</code> <code class="bp">False</code>
        <code class="k">else</code><code class="p">:</code>
            <code class="k">print</code> <code class="p">(</code><code class="s1">'Error {}'</code><code class="o">.</code><code class="n">format</code><code class="p">(</code><code class="n">status_code</code><code class="p">))</code>
            <code class="k">return</code> <code class="bp">False</code>
</pre>

<p>To get access to the streaming API, the basic app <a contenteditable="false" data-type="indexterm" data-primary="authenticated API requests" id="idm45634212800936"/>authentication is not enough. We must also provide the user authentication, which can be found on the same page as shown before. This means that the Streaming API requests are made by the app we created on behalf of the user (in this case our own account). This also means that we have to use the <code>OAuthHandler</code> class instead of the <code>AppAuthHandler</code> that we used up <span class="keep-together">to now:</span></p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">user_access_token</code> <code class="o">=</code> <code class="s1">'YOUR_USER_ACCESS_TOKEN_HERE'</code>
<code class="n">user_access_secret</code> <code class="o">=</code> <code class="s1">'YOUR_USER_ACCESS_SECRET_HERE'</code>

<code class="n">auth</code> <code class="o">=</code> <code class="n">tweepy</code><code class="o">.</code><code class="n">OAuthHandler</code><code class="p">(</code><code class="n">app_api_key</code><code class="p">,</code> <code class="n">app_api_secret_key</code><code class="p">)</code>
<code class="n">auth</code><code class="o">.</code><code class="n">set_access_token</code><code class="p">(</code><code class="n">user_access_token</code><code class="p">,</code> <code class="n">user_access_secret</code><code class="p">)</code>
<code class="n">api</code> <code class="o">=</code> <code class="n">tweepy</code><code class="o">.</code><code class="n">API</code><code class="p">(</code><code class="n">auth</code><code class="p">,</code> <code class="n">wait_on_rate_limit</code><code class="o">=</code><code class="bp">True</code><code class="p">,</code> <code class="n">wait_on_rate_limit_notify</code><code class="o">=</code><code class="bp">True</code><code class="p">)</code>
</pre>

<p>When initializing an object of <code>FileStreamListener</code>, we also specify the maximum number of tweets that we would like to extract. This acts like a stopping condition, and if not specified, the process will run as long as it is not terminated by the user or stopped due to a server error. We initialize the Twitter stream by passing in the authentication object (<code>api.auth</code>) and the object that will manage the stream (<code>fileStreamListener</code>). We also ask for the extended tweets to be provided. Once this is done, we can start tracking live tweets from the stream using the filter function and providing keywords that we would like to track:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">fileStreamListener</code> <code class="o">=</code> <code class="n">FileStreamListener</code><code class="p">(</code><code class="mi">5000</code><code class="p">)</code>
<code class="n">fileStream</code> <code class="o">=</code> <code class="n">tweepy</code><code class="o">.</code><code class="n">Stream</code><code class="p">(</code><code class="n">auth</code><code class="o">=</code><code class="n">api</code><code class="o">.</code><code class="n">auth</code><code class="p">,</code>
                           <code class="n">listener</code><code class="o">=</code><code class="n">fileStreamListener</code><code class="p">,</code>
                           <code class="n">tweet_mode</code><code class="o">=</code><code class="s1">'extended'</code><code class="p">)</code>
<code class="n">fileStream</code><code class="o">.</code><code class="n">filter</code><code class="p">(</code><code class="n">track</code><code class="o">=</code><code class="p">[</code><code class="s1">'cryptocurrency'</code><code class="p">])</code>
</pre>

<p>If you would like to run the extractor in a separate thread, you can pass the keyword <code>async=True</code> to the filter function, and this will run continuously in a separate thread. Once it has run for some time and stored tweets, we can read this into a Pandas <span class="keep-together"><code>DataFrame</code></span> as before. When an error occurs, the <code>FileStreamListener</code> does not attempt retries but only prints the error <code>status_code</code>. You are encouraged to implement failure handling and customize the <code>on_data</code> method to suit the <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch2_term26" id="idm45634212503112"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch2_term27" id="idm45634212501704"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch2_term28" id="idm45634212500328"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch2_term29" id="idm45634212498952"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch2_term30" id="idm45634212384456"/>use case.</p>

<p>These blueprints only provide guidance on accessing popular APIs for data extraction. Since each API is different, the functionality provided by the corresponding Python module will also be different. For <a contenteditable="false" data-type="indexterm" data-primary="Wikipedia articles" id="idm45634212382184"/><a contenteditable="false" data-type="indexterm" data-primary="datasets, examples of" data-secondary="Wikipedia articles" id="idm45634212381080"/>instance, <a href="https://oreil.ly/zruJt">Wikipedia</a> is another popular source for extracting <a contenteditable="false" data-type="indexterm" data-primary="wikipediaapi (Python)" id="idm45634212378792"/>text data, and <a href="https://oreil.ly/Eyon3"><code>wikipediaapi</code></a> is one of the supported Python modules for extracting this data. It can be installed by using the command <strong><code>pip install wikipediaapi</code></strong>, and since this is a publicly available data source, the authentication and generation of access tokens is not necessary. You only need to specify the version of Wikipedia (language) and the topic name for which you want to extract data. The following code snippet shows the steps to download the Wikipedia entry for “Cryptocurrency” and shows the initial few lines of this <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch2_term18" id="idm45634212375608"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch2_term19" id="idm45634212374296"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch2_term20" id="idm45634212372920"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch2_term23" id="idm45634212371544"/>article:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="kn">import</code> <code class="nn">wikipediaapi</code>

<code class="n">wiki_wiki</code> <code class="o">=</code> <code class="n">wikipediaapi</code><code class="o">.</code><code class="n">Wikipedia</code><code class="p">(</code>
        <code class="n">language</code><code class="o">=</code><code class="s1">'en'</code><code class="p">,</code>
        <code class="n">extract_format</code><code class="o">=</code><code class="n">wikipediaapi</code><code class="o">.</code><code class="n">ExtractFormat</code><code class="o">.</code><code class="n">WIKI</code>
<code class="p">)</code>

<code class="n">p_wiki</code> <code class="o">=</code> <code class="n">wiki_wiki</code><code class="o">.</code><code class="n">page</code><code class="p">(</code><code class="s1">'Cryptocurrency'</code><code class="p">)</code>
<code class="k">print</code> <code class="p">(</code><code class="n">p_wiki</code><code class="o">.</code><code class="n">text</code><code class="p">[:</code><code class="mi">200</code><code class="p">],</code> <code class="s1">'....'</code><code class="p">)</code>
</pre>

<p><code>Out:</code></p>

<pre data-type="programlisting">
A cryptocurrency (or crypto currency) is a digital asset designed to work
as a medium of exchange wherein individual coin ownership records are stored
in a ledger existing in a form of computerized da ....
</pre>
</div></section>
</div></section>

<section data-type="sect1" data-pdf-bookmark="Closing Remarks"><div class="sect1" id="idm45634215052936">
<h1>Closing Remarks</h1>

<p>In this chapter, we first introduced <a contenteditable="false" data-type="indexterm" data-primary="text data extraction with APIs" data-secondary="about APIs" id="idm45634212289496"/>blueprints that make use of the Python requests library to make <a contenteditable="false" data-type="indexterm" data-primary="APIs (application programming interfaces)" id="idm45634212288024"/>API calls and extract data. We also introduced ways to work with paginated results, rate limits, and retries. These blueprints work for any kind of API and are great if you would like to control and customize several aspects for your data extraction. In the next set of blueprints, we used Tweepy to extract data from the Twitter API. This is an example of a community-developed Python library that supports a popular API and provides tested functionality out of the box. You often don’t have to worry about implementing your own pagination or backoff strategy and is therefore one less thing to worry about. If your use case needs to get data from a popular API, then it is convenient to use such a preexisting package.</p>
</div></section>
<div data-type="footnotes"><p data-type="footnote" id="idm45634214166328"><sup><a href="ch02.xhtml#idm45634214166328-marker">1</a></sup> Delay introduced between subsequent calls defined as <code>time_delay={backoff factor} * (2 ** ({number of total retries} - 1))</code>.</p></div></div></section></div>



  </body></html>