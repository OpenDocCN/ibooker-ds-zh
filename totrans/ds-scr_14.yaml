- en: Chapter 13\. Naive Bayes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is well for the heart to be naive and for the mind not to be.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Anatole France
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A social network isn’t much good if people can’t network. Accordingly, DataSciencester
    has a popular feature that allows members to send messages to other members. And
    while most members are responsible citizens who send only well-received “how’s
    it going?” messages, a few miscreants persistently spam other members about get-rich
    schemes, no-prescription-required pharmaceuticals, and for-profit data science
    credentialing programs. Your users have begun to complain, and so the VP of Messaging
    has asked you to use data science to figure out a way to filter out these spam
    messages.
  prefs: []
  type: TYPE_NORMAL
- en: A Really Dumb Spam Filter
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Imagine a “universe” that consists of receiving a message chosen randomly from
    all possible messages. Let *S* be the event “the message is spam” and *B* be the
    event “the message contains the word *bitcoin*.” Bayes’s theorem tells us that
    the probability that the message is spam conditional on containing the word *bitcoin*
    is:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="upper P left-parenthesis upper S vertical-bar upper B right-parenthesis
    equals left-bracket upper P left-parenthesis upper B vertical-bar upper S right-parenthesis
    upper P left-parenthesis upper S right-parenthesis right-bracket slash left-bracket
    upper P left-parenthesis upper B vertical-bar upper S right-parenthesis upper
    P left-parenthesis upper S right-parenthesis plus upper P left-parenthesis upper
    B vertical-bar normal not-sign upper S right-parenthesis upper P left-parenthesis
    normal not-sign upper S right-parenthesis right-bracket" display="block"><mrow><mi>P</mi>
    <mo>(</mo> <mi>S</mi> <mo>|</mo> <mi>B</mi> <mo>)</mo> <mo>=</mo> <mo>[</mo> <mi>P</mi>
    <mo>(</mo> <mi>B</mi> <mo>|</mo> <mi>S</mi> <mo>)</mo> <mi>P</mi> <mo>(</mo> <mi>S</mi>
    <mo>)</mo> <mo>]</mo> <mo>/</mo> <mo>[</mo> <mi>P</mi> <mo>(</mo> <mi>B</mi> <mo>|</mo>
    <mi>S</mi> <mo>)</mo> <mi>P</mi> <mo>(</mo> <mi>S</mi> <mo>)</mo> <mo>+</mo> <mi>P</mi>
    <mo>(</mo> <mi>B</mi> <mo>|</mo> <mo>¬</mo> <mi>S</mi> <mo>)</mo> <mi>P</mi> <mo>(</mo>
    <mo>¬</mo> <mi>S</mi> <mo>)</mo> <mo>]</mo></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: The numerator is the probability that a message is spam *and* contains *bitcoin*,
    while the denominator is just the probability that a message contains *bitcoin*.
    Hence, you can think of this calculation as simply representing the proportion
    of *bitcoin* messages that are spam.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we have a large collection of messages we know are spam, and a large collection
    of messages we know are not spam, then we can easily estimate *P*(*B*|*S*) and
    *P*(*B*|*¬S*). If we further assume that any message is equally likely to be spam
    or not spam (so that *P*(*S*) = *P*(*¬S*) = 0.5), then:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="left-bracket upper P left-parenthesis upper S vertical-bar upper
    B right-parenthesis equals upper P left-parenthesis upper B vertical-bar upper
    S right-parenthesis slash left-bracket upper P left-parenthesis upper B vertical-bar
    upper S right-parenthesis plus upper P left-parenthesis upper B vertical-bar normal
    not-sign upper S right-parenthesis right-bracket right-bracket" display="block"><mrow><mi>P</mi>
    <mo>(</mo> <mi>S</mi> <mo>|</mo> <mi>B</mi> <mo>)</mo> <mo>=</mo> <mi>P</mi> <mo>(</mo>
    <mi>B</mi> <mo>|</mo> <mi>S</mi> <mo>)</mo> <mo>/</mo> <mo>[</mo> <mi>P</mi> <mo>(</mo>
    <mi>B</mi> <mo>|</mo> <mi>S</mi> <mo>)</mo> <mo>+</mo> <mi>P</mi> <mo>(</mo> <mi>B</mi>
    <mo>|</mo> <mo>¬</mo> <mi>S</mi> <mo>)</mo> <mo>]</mo></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, if 50% of spam messages have the word *bitcoin*, but only 1% of
    nonspam messages do, then the probability that any given *bitcoin*-containing
    email is spam is:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="0.5 slash left-parenthesis 0.5 plus 0.01 right-parenthesis equals
    98 percent-sign" display="block"><mrow><mn>0</mn> <mo>.</mo> <mn>5</mn> <mo>/</mo>
    <mo>(</mo> <mn>0</mn> <mo>.</mo> <mn>5</mn> <mo>+</mo> <mn>0</mn> <mo>.</mo> <mn>01</mn>
    <mo>)</mo> <mo>=</mo> <mn>98</mn> <mo>%</mo></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: A More Sophisticated Spam Filter
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Imagine now that we have a vocabulary of many words, *w*[1] ..., *w*[n]. To
    move this into the realm of probability theory, we’ll write *X*[i] for the event
    “a message contains the word *w*[i].” Also imagine that (through some unspecified-at-this-point
    process) we’ve come up with an estimate *P*(*X*[i]|*S*) for the probability that
    a spam message contains the *i*th word, and a similar estimate *P*(*X*[i]|¬*S*)
    for the probability that a nonspam message contains the *i*th word.
  prefs: []
  type: TYPE_NORMAL
- en: 'The key to Naive Bayes is making the (big) assumption that the presences (or
    absences) of each word are independent of one another, conditional on a message
    being spam or not. Intuitively, this assumption means that knowing whether a certain
    spam message contains the word *bitcoin* gives you no information about whether
    that same message contains the word *rolex*. In math terms, this means that:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="upper P left-parenthesis upper X 1 equals x 1 comma period period
    period comma upper X Subscript n Baseline equals x Subscript n Baseline vertical-bar
    upper S right-parenthesis equals upper P left-parenthesis upper X 1 equals x 1
    vertical-bar upper S right-parenthesis times ellipsis times upper P left-parenthesis
    upper X Subscript n Baseline equals x Subscript n Baseline vertical-bar upper
    S right-parenthesis" display="block"><mrow><mi>P</mi> <mrow><mo>(</mo> <msub><mi>X</mi>
    <mn>1</mn></msub> <mo>=</mo> <msub><mi>x</mi> <mn>1</mn></msub> <mo>,</mo> <mo>.</mo>
    <mo>.</mo> <mo>.</mo> <mo>,</mo> <msub><mi>X</mi> <mi>n</mi></msub> <mo>=</mo>
    <msub><mi>x</mi> <mi>n</mi></msub> <mo>|</mo> <mi>S</mi> <mo>)</mo></mrow> <mo>=</mo>
    <mi>P</mi> <mrow><mo>(</mo> <msub><mi>X</mi> <mn>1</mn></msub> <mo>=</mo> <msub><mi>x</mi>
    <mn>1</mn></msub> <mo>|</mo> <mi>S</mi> <mo>)</mo></mrow> <mo>×</mo> <mo>⋯</mo>
    <mo>×</mo> <mi>P</mi> <mrow><mo>(</mo> <msub><mi>X</mi> <mi>n</mi></msub> <mo>=</mo>
    <msub><mi>x</mi> <mi>n</mi></msub> <mo>|</mo> <mi>S</mi> <mo>)</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'This is an extreme assumption. (There’s a reason the technique has *naive*
    in its name.) Imagine that our vocabulary consists *only* of the words *bitcoin*
    and *rolex*, and that half of all spam messages are for “earn bitcoin” and that
    the other half are for “authentic rolex.” In this case, the Naive Bayes estimate
    that a spam message contains both *bitcoin* and *rolex* is:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="upper P left-parenthesis upper X 1 equals 1 comma upper X 2 equals
    1 vertical-bar upper S right-parenthesis equals upper P left-parenthesis upper
    X 1 equals 1 vertical-bar upper S right-parenthesis upper P left-parenthesis upper
    X 2 equals 1 vertical-bar upper S right-parenthesis equals .5 times .5 equals
    .25" display="block"><mrow><mi>P</mi> <mrow><mo>(</mo> <msub><mi>X</mi> <mn>1</mn></msub>
    <mo>=</mo> <mn>1</mn> <mo>,</mo> <msub><mi>X</mi> <mn>2</mn></msub> <mo>=</mo>
    <mn>1</mn> <mo>|</mo> <mi>S</mi> <mo>)</mo></mrow> <mo>=</mo> <mi>P</mi> <mrow><mo>(</mo>
    <msub><mi>X</mi> <mn>1</mn></msub> <mo>=</mo> <mn>1</mn> <mo>|</mo> <mi>S</mi>
    <mo>)</mo></mrow> <mi>P</mi> <mrow><mo>(</mo> <msub><mi>X</mi> <mn>2</mn></msub>
    <mo>=</mo> <mn>1</mn> <mo>|</mo> <mi>S</mi> <mo>)</mo></mrow> <mo>=</mo> <mo>.</mo>
    <mn>5</mn> <mo>×</mo> <mo>.</mo> <mn>5</mn> <mo>=</mo> <mo>.</mo> <mn>25</mn></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: since we’ve assumed away the knowledge that *bitcoin* and *rolex* actually never
    occur together. Despite the unrealisticness of this assumption, this model often
    performs well and has historically been used in actual spam filters.
  prefs: []
  type: TYPE_NORMAL
- en: 'The same Bayes’s theorem reasoning we used for our “bitcoin-only” spam filter
    tells us that we can calculate the probability a message is spam using the equation:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="upper P left-parenthesis upper S vertical-bar upper X equals
    x right-parenthesis equals upper P left-parenthesis upper X equals x vertical-bar
    upper S right-parenthesis slash left-bracket upper P left-parenthesis upper X
    equals x vertical-bar upper S right-parenthesis plus upper P left-parenthesis
    upper X equals x vertical-bar normal not-sign upper S right-parenthesis right-bracket"
    display="block"><mrow><mi>P</mi> <mo>(</mo> <mi>S</mi> <mo>|</mo> <mi>X</mi> <mo>=</mo>
    <mi>x</mi> <mo>)</mo> <mo>=</mo> <mi>P</mi> <mo>(</mo> <mi>X</mi> <mo>=</mo> <mi>x</mi>
    <mo>|</mo> <mi>S</mi> <mo>)</mo> <mo>/</mo> <mo>[</mo> <mi>P</mi> <mo>(</mo> <mi>X</mi>
    <mo>=</mo> <mi>x</mi> <mo>|</mo> <mi>S</mi> <mo>)</mo> <mo>+</mo> <mi>P</mi> <mo>(</mo>
    <mi>X</mi> <mo>=</mo> <mi>x</mi> <mo>|</mo> <mo>¬</mo> <mi>S</mi> <mo>)</mo> <mo>]</mo></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: The Naive Bayes assumption allows us to compute each of the probabilities on
    the right simply by multiplying together the individual probability estimates
    for each vocabulary word.
  prefs: []
  type: TYPE_NORMAL
- en: 'In practice, you usually want to avoid multiplying lots of probabilities together,
    to prevent a problem called *underflow*, in which computers don’t deal well with
    floating-point numbers that are too close to 0. Recalling from algebra that <math
    alttext="log left-parenthesis a b right-parenthesis equals log a plus log b"><mrow><mo
    form="prefix">log</mo> <mo>(</mo> <mi>a</mi> <mi>b</mi> <mo>)</mo> <mo>=</mo>
    <mo form="prefix">log</mo> <mi>a</mi> <mo>+</mo> <mo form="prefix">log</mo> <mi>b</mi></mrow></math>
    and that <math alttext="exp left-parenthesis log x right-parenthesis equals x"><mrow><mo
    form="prefix">exp</mo><mo>(</mo> <mo form="prefix">log</mo> <mi>x</mi> <mo>)</mo>
    <mo>=</mo> <mi>x</mi></mrow></math> , we usually compute <math alttext="p 1 asterisk
    ellipsis asterisk p Subscript n"><mrow><msub><mi>p</mi> <mn>1</mn></msub> <mo>*</mo>
    <mo>⋯</mo> <mo>*</mo> <msub><mi>p</mi> <mi>n</mi></msub></mrow></math> as the
    equivalent (but floating-point-friendlier):'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="exp left-parenthesis log left-parenthesis p 1 right-parenthesis
    plus ellipsis plus log left-parenthesis p Subscript n Baseline right-parenthesis
    right-parenthesis" display="block"><mrow><mo form="prefix">exp</mo><mo>(</mo>
    <mo form="prefix">log</mo> <mrow><mo>(</mo> <msub><mi>p</mi> <mn>1</mn></msub>
    <mo>)</mo></mrow> <mo>+</mo> <mo>⋯</mo> <mo>+</mo> <mo form="prefix">log</mo>
    <mrow><mo>(</mo> <msub><mi>p</mi> <mi>n</mi></msub> <mo>)</mo></mrow> <mo>)</mo></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: The only challenge left is coming up with estimates for <math><mrow><mi>P</mi>
    <mo>(</mo> <msub><mi>X</mi><mi>i</mi></msub> <mo>|</mo> <mi>S</mi> <mo>)</mo></mrow></math>
    and <math><mrow><mi>P</mi> <mo>(</mo> <msub><mi>X</mi><mi>i</mi></msub> <mo>|</mo>
    <mo>¬</mo> <mi>S</mi> <mo>)</mo></mrow></math> , the probabilities that a spam
    message (or nonspam message) contains the word <math><msub><mi>w</mi> <mi>i</mi></msub></math>
    . If we have a fair number of “training” messages labeled as spam and not spam,
    an obvious first try is to estimate <math><mrow><mi>P</mi> <mo>(</mo> <msub><mi>X</mi>
    <mi>i</mi></msub> <mo>|</mo> <mi>S</mi> <mo>)</mo></mrow></math> simply as the
    fraction of spam messages containing the word <math><msub><mi>w</mi> <mi>i</mi></msub></math>
    .
  prefs: []
  type: TYPE_NORMAL
- en: This causes a big problem, though. Imagine that in our training set the vocabulary
    word *data* only occurs in nonspam messages. Then we’d estimate <math alttext="upper
    P left-parenthesis quotation-mark data quotation-mark vertical-bar upper S right-parenthesis
    equals 0"><mrow><mi>P</mi> <mo>(</mo> <mtext>data</mtext> <mo>|</mo> <mi>S</mi>
    <mo>)</mo> <mo>=</mo> <mn>0</mn></mrow></math> . The result is that our Naive
    Bayes classifier would always assign spam probability 0 to *any* message containing
    the word *data*, even a message like “data on free bitcoin and authentic rolex
    watches.” To avoid this problem, we usually use some kind of smoothing.
  prefs: []
  type: TYPE_NORMAL
- en: 'In particular, we’ll choose a *pseudocount*—*k*—and estimate the probability
    of seeing the *i*th word in a spam message as:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="upper P left-parenthesis upper X Subscript i Baseline vertical-bar
    upper S right-parenthesis equals left-parenthesis k plus number of spams containing
    w Subscript i Baseline right-parenthesis slash left-parenthesis 2 k plus number
    of spams right-parenthesis" display="block"><mrow><mi>P</mi> <mrow><mo>(</mo>
    <msub><mi>X</mi> <mi>i</mi></msub> <mo>|</mo> <mi>S</mi> <mo>)</mo></mrow> <mo>=</mo>
    <mrow><mo>(</mo> <mi>k</mi> <mo>+</mo> <mtext>number</mtext> <mtext>of</mtext>
    <mtext>spams</mtext> <mtext>containing</mtext> <msub><mi>w</mi> <mi>i</mi></msub>
    <mo>)</mo></mrow> <mo>/</mo> <mrow><mo>(</mo> <mn>2</mn> <mi>k</mi> <mo>+</mo>
    <mtext>number</mtext> <mtext>of</mtext> <mtext>spams</mtext> <mo>)</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: We do similarly for <math><mrow><mi>P</mi> <mo>(</mo> <msub><mi>X</mi> <mi>i</mi></msub>
    <mo>|</mo> <mo>¬</mo> <mi>S</mi> <mo>)</mo></mrow></math> . That is, when computing
    the spam probabilities for the *i*th word, we assume we also saw *k* additional
    nonspams containing the word and *k* additional nonspams not containing the word.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if *data* occurs in 0/98 spam messages, and if *k* is 1, we estimate
    *P*(data|*S*) as 1/100 = 0.01, which allows our classifier to still assign some
    nonzero spam probability to messages that contain the word *data*.
  prefs: []
  type: TYPE_NORMAL
- en: Implementation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now we have all the pieces we need to build our classifier. First, let’s create
    a simple function to tokenize messages into distinct words. We’ll first convert
    each message to lowercase, then use `re.findall` to extract “words” consisting
    of letters, numbers, and apostrophes. Finally, we’ll use `set` to get just the
    distinct words:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We’ll also define a type for our training data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: As our classifier needs to keep track of tokens, counts, and labels from the
    training data, we’ll make it a class. Following convention, we refer to nonspam
    emails as *ham* emails.
  prefs: []
  type: TYPE_NORMAL
- en: 'The constructor will take just one parameter, the pseudocount to use when computing
    probabilities. It also initializes an empty set of tokens, counters to track how
    often each token is seen in spam messages and ham messages, and counts of how
    many spam and ham messages it was trained on:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we’ll give it a method to train it on a bunch of messages. First, we
    increment the `spam_messages` and `ham_messages` counts. Then we tokenize each
    message text, and for each token we increment the `token_spam_counts` or `token_ham_counts`
    based on the message type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Ultimately we’ll want to predict *P*(spam | token). As we saw earlier, to apply
    Bayes’s theorem we need to know *P*(token | spam) and *P*(token | ham) for each
    token in the vocabulary. So we’ll create a “private” helper function to compute
    those:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we’re ready to write our `predict` method. As mentioned earlier, rather
    than multiplying together lots of small probabilities, we’ll instead sum up the
    log probabilities:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: And now we have a classifier.
  prefs: []
  type: TYPE_NORMAL
- en: Testing Our Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s make sure our model works by writing some unit tests for it.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'First, let’s check that it got the counts right:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let’s make a prediction. We’ll also (laboriously) go through our Naive
    Bayes logic by hand, and make sure that we get the same result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: This test passes, so it seems like our model is doing what we think it is. If
    you look at the actual probabilities, the two big drivers are that our message
    contains *spam* (which our lone training spam message did) and that it doesn’t
    contain *ham* (which both our training ham messages did).
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s try it on some real data.
  prefs: []
  type: TYPE_NORMAL
- en: Using Our Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A popular (if somewhat old) dataset is the [SpamAssassin public corpus](https://spamassassin.apache.org/old/publiccorpus/).
    We’ll look at the files prefixed with *20021010*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a script that will download and unpack them to the directory of your
    choice (or you can do it manually):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: It’s possible the location of the files will change (this happened between the
    first and second editions of this book), in which case adjust the script accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: 'After downloading the data you should have three folders: *spam*, *easy_ham*,
    and *hard_ham*. Each folder contains many emails, each contained in a single file.
    To keep things *really* simple, we’ll just look at the subject lines of each email.'
  prefs: []
  type: TYPE_NORMAL
- en: 'How do we identify the subject line? When we look through the files, they all
    seem to start with “Subject:”. So we’ll look for that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can split the data into training data and test data, and then we’re
    ready to build a classifier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s generate some predictions and check how our model does:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: This gives 84 true positives (spam classified as “spam”), 25 false positives
    (ham classified as “spam”), 703 true negatives (ham classified as “ham”), and
    44 false negatives (spam classified as “ham”). This means our precision is 84
    / (84 + 25) = 77%, and our recall is 84 / (84 + 44) = 65%, which are not bad numbers
    for such a simple model. (Presumably we’d do better if we looked at more than
    the subject lines.)
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also inspect the model’s innards to see which words are least and most
    indicative of spam:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The spammiest words include things like *sale*, *mortgage*, *money*, and *rates*,
    whereas the hammiest words include things like *spambayes*, *users*, *apt*, and
    *perl*. So that also gives us some intuitive confidence that our model is basically
    doing the right thing.
  prefs: []
  type: TYPE_NORMAL
- en: 'How could we get better performance? One obvious way would be to get more data
    to train on. There are a number of ways to improve the model as well. Here are
    some possibilities that you might try:'
  prefs: []
  type: TYPE_NORMAL
- en: Look at the message content, not just the subject line. You’ll have to be careful
    how you deal with the message headers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Our classifier takes into account every word that appears in the training set,
    even words that appear only once. Modify the classifier to accept an optional
    `min_count` threshold and ignore tokens that don’t appear at least that many times.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The tokenizer has no notion of similar words (e.g., *cheap* and *cheapest*).
    Modify the classifier to take an optional `stemmer` function that converts words
    to *equivalence classes* of words. For example, a really simple stemmer function
    might be:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Creating a good stemmer function is hard. People frequently use the [Porter
    stemmer](http://tartarus.org/martin/PorterStemmer/).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Although our features are all of the form “message contains word <math><msub><mi>w</mi>
    <mi>i</mi></msub></math> ,” there’s no reason why this has to be the case. In
    our implementation, we could add extra features like “message contains a number”
    by creating phony tokens like *contains:number* and modifying the `tokenizer`
    to emit them when appropriate.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For Further Exploration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Paul Graham’s articles [“A Plan for Spam”](http://www.paulgraham.com/spam.html)
    and [“Better Bayesian Filtering”](http://www.paulgraham.com/better.html) are interesting
    and give more insight into the ideas behind building spam filters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[scikit-learn](https://scikit-learn.org/stable/modules/naive_bayes.html) contains
    a `BernoulliNB` model that implements the same Naive Bayes algorithm we implemented
    here, as well as other variations on the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
