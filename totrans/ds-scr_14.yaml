- en: Chapter 13\. Naive Bayes
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第13章. 贝叶斯朴素分类
- en: It is well for the heart to be naive and for the mind not to be.
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 为心灵保持天真，为思想保持成熟。
- en: ''
  id: totrans-2
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Anatole France
  id: totrans-3
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 阿纳托尔·法朗士
- en: A social network isn’t much good if people can’t network. Accordingly, DataSciencester
    has a popular feature that allows members to send messages to other members. And
    while most members are responsible citizens who send only well-received “how’s
    it going?” messages, a few miscreants persistently spam other members about get-rich
    schemes, no-prescription-required pharmaceuticals, and for-profit data science
    credentialing programs. Your users have begun to complain, and so the VP of Messaging
    has asked you to use data science to figure out a way to filter out these spam
    messages.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 如果人们不能进行社交，那么社交网络就没什么用了。因此，DataSciencester拥有一个受欢迎的功能，允许会员发送消息给其他会员。虽然大多数会员是负责任的公民，只发送受欢迎的“最近好吗？”消息，但一些不法分子坚持不懈地向其他成员发送关于致富计划、无需处方的药物和盈利数据科学证书项目的垃圾邮件。您的用户已经开始抱怨，因此消息副总裁要求您使用数据科学找出一种过滤这些垃圾邮件的方法。
- en: A Really Dumb Spam Filter
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一个非常愚蠢的垃圾邮件过滤器
- en: 'Imagine a “universe” that consists of receiving a message chosen randomly from
    all possible messages. Let *S* be the event “the message is spam” and *B* be the
    event “the message contains the word *bitcoin*.” Bayes’s theorem tells us that
    the probability that the message is spam conditional on containing the word *bitcoin*
    is:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 假设一个“宇宙”，由从所有可能消息中随机选择的消息组成。设*S*为事件“消息是垃圾邮件”，*B*为事件“消息包含词*bitcoin*”。贝叶斯定理告诉我们，包含词*bitcoin*的消息是垃圾邮件的条件概率是：
- en: <math alttext="upper P left-parenthesis upper S vertical-bar upper B right-parenthesis
    equals left-bracket upper P left-parenthesis upper B vertical-bar upper S right-parenthesis
    upper P left-parenthesis upper S right-parenthesis right-bracket slash left-bracket
    upper P left-parenthesis upper B vertical-bar upper S right-parenthesis upper
    P left-parenthesis upper S right-parenthesis plus upper P left-parenthesis upper
    B vertical-bar normal not-sign upper S right-parenthesis upper P left-parenthesis
    normal not-sign upper S right-parenthesis right-bracket" display="block"><mrow><mi>P</mi>
    <mo>(</mo> <mi>S</mi> <mo>|</mo> <mi>B</mi> <mo>)</mo> <mo>=</mo> <mo>[</mo> <mi>P</mi>
    <mo>(</mo> <mi>B</mi> <mo>|</mo> <mi>S</mi> <mo>)</mo> <mi>P</mi> <mo>(</mo> <mi>S</mi>
    <mo>)</mo> <mo>]</mo> <mo>/</mo> <mo>[</mo> <mi>P</mi> <mo>(</mo> <mi>B</mi> <mo>|</mo>
    <mi>S</mi> <mo>)</mo> <mi>P</mi> <mo>(</mo> <mi>S</mi> <mo>)</mo> <mo>+</mo> <mi>P</mi>
    <mo>(</mo> <mi>B</mi> <mo>|</mo> <mo>¬</mo> <mi>S</mi> <mo>)</mo> <mi>P</mi> <mo>(</mo>
    <mo>¬</mo> <mi>S</mi> <mo>)</mo> <mo>]</mo></mrow></math>
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper P left-parenthesis upper S vertical-bar upper B right-parenthesis
    equals left-bracket upper P left-parenthesis upper B vertical-bar upper S right-parenthesis
    upper P left-parenthesis upper S right-parenthesis right-bracket slash left-bracket
    upper P left-parenthesis upper B vertical-bar upper S right-parenthesis upper
    P left-parenthesis upper S right-parenthesis plus upper P left-parenthesis upper
    B vertical-bar normal not-sign upper S right-parenthesis upper P left-parenthesis
    normal not-sign upper S right-parenthesis right-bracket" display="block"><mrow><mi>P</mi>
    <mo>(</mo> <mi>S</mi> <mo>|</mo> <mi>B</mi> <mo>)</mo> <mo>=</mo> <mo>[</mo> <mi>P</mi>
    <mo>(</mo> <mi>B</mi> <mo>|</mo> <mi>S</mi> <mo>)</mo> <mi>P</mi> <mo>(</mo> <mi>S</mi>
    <mo>)</mo> <mo>]</mo> <mo>/</mo> <mo>[</mo> <mi>P</mi> <mo>(</mo> <mi>B</mi> <mo>|</mo>
    <mi>S</mi> <mo>)</mo> <mi>P</mi> <mo>(</mo> <mi>S</mi> <mo>)</mo> <mo>+</mo> <mi>P</mi>
    <mo>(</mo> <mi>B</mi> <mo>|</mo> <mo>¬</mo> <mi>S</mi> <mo>)</mo> <mi>P</mi> <mo>(</mo>
    <mo>¬</mo> <mi>S</mi> <mo>)</mo> <mo>]</mo></mrow></math>
- en: The numerator is the probability that a message is spam *and* contains *bitcoin*,
    while the denominator is just the probability that a message contains *bitcoin*.
    Hence, you can think of this calculation as simply representing the proportion
    of *bitcoin* messages that are spam.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 分子是消息是垃圾邮件且包含*bitcoin*的概率，而分母只是消息包含*bitcoin*的概率。因此，您可以将这个计算视为简单地表示为垃圾邮件的*bitcoin*消息的比例。
- en: 'If we have a large collection of messages we know are spam, and a large collection
    of messages we know are not spam, then we can easily estimate *P*(*B*|*S*) and
    *P*(*B*|*¬S*). If we further assume that any message is equally likely to be spam
    or not spam (so that *P*(*S*) = *P*(*¬S*) = 0.5), then:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有大量的已知是垃圾邮件的消息和大量的已知不是垃圾邮件的消息，那么我们可以很容易地估计*P*(*B*|*S*)和*P*(*B*|*¬S*)。如果我们进一步假设任何消息都有相等的可能性是垃圾邮件或不是垃圾邮件（所以*P*(*S*)
    = *P*(*¬S*) = 0.5），那么：
- en: <math alttext="left-bracket upper P left-parenthesis upper S vertical-bar upper
    B right-parenthesis equals upper P left-parenthesis upper B vertical-bar upper
    S right-parenthesis slash left-bracket upper P left-parenthesis upper B vertical-bar
    upper S right-parenthesis plus upper P left-parenthesis upper B vertical-bar normal
    not-sign upper S right-parenthesis right-bracket right-bracket" display="block"><mrow><mi>P</mi>
    <mo>(</mo> <mi>S</mi> <mo>|</mo> <mi>B</mi> <mo>)</mo> <mo>=</mo> <mi>P</mi> <mo>(</mo>
    <mi>B</mi> <mo>|</mo> <mi>S</mi> <mo>)</mo> <mo>/</mo> <mo>[</mo> <mi>P</mi> <mo>(</mo>
    <mi>B</mi> <mo>|</mo> <mi>S</mi> <mo>)</mo> <mo>+</mo> <mi>P</mi> <mo>(</mo> <mi>B</mi>
    <mo>|</mo> <mo>¬</mo> <mi>S</mi> <mo>)</mo> <mo>]</mo></mrow></math>
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="left-bracket upper P left-parenthesis upper S vertical-bar upper
    B right-parenthesis equals upper P left-parenthesis upper B vertical-bar upper
    S right-parenthesis slash left-bracket upper P left-parenthesis upper B vertical-bar
    upper S right-parenthesis plus upper P left-parenthesis upper B vertical-bar normal
    not-sign upper S right-parenthesis right-bracket right-bracket" display="block"><mrow><mi>P</mi>
    <mo>(</mo> <mi>S</mi> <mo>|</mo> <mi>B</mi> <mo>)</mo> <mo>=</mo> <mi>P</mi> <mo>(</mo>
    <mi>B</mi> <mo>|</mo> <mi>S</mi> <mo>)</mo> <mo>/</mo> <mo>[</mo> <mi>P</mi> <mo>(</mo>
    <mi>B</mi> <mo>|</mo> <mi>S</mi> <mo>)</mo> <mo>+</mo> <mi>P</mi> <mo>(</mo> <mi>B</mi>
    <mo>|</mo> <mo>¬</mo> <mi>S</mi> <mo>)</mo> <mo>]</mo></mrow></math>
- en: 'For example, if 50% of spam messages have the word *bitcoin*, but only 1% of
    nonspam messages do, then the probability that any given *bitcoin*-containing
    email is spam is:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果垃圾邮件中有50%的消息包含*bitcoin*这个词，而非垃圾邮件中只有1%的消息包含，那么包含*bitcoin*的任意邮件是垃圾邮件的概率是：
- en: <math alttext="0.5 slash left-parenthesis 0.5 plus 0.01 right-parenthesis equals
    98 percent-sign" display="block"><mrow><mn>0</mn> <mo>.</mo> <mn>5</mn> <mo>/</mo>
    <mo>(</mo> <mn>0</mn> <mo>.</mo> <mn>5</mn> <mo>+</mo> <mn>0</mn> <mo>.</mo> <mn>01</mn>
    <mo>)</mo> <mo>=</mo> <mn>98</mn> <mo>%</mo></mrow></math>
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="0.5 slash left-parenthesis 0.5 plus 0.01 right-parenthesis equals
    98 percent-sign" display="block"><mrow><mn>0</mn> <mo>.</mo> <mn>5</mn> <mo>/</mo>
    <mo>(</mo> <mn>0</mn> <mo>.</mo> <mn>5</mn> <mo>+</mo> <mn>0</mn> <mo>.</mo> <mn>01</mn>
    <mo>)</mo> <mo>=</mo> <mn>98</mn> <mo>%</mo></mrow></math>
- en: A More Sophisticated Spam Filter
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一个更复杂的垃圾邮件过滤器
- en: Imagine now that we have a vocabulary of many words, *w*[1] ..., *w*[n]. To
    move this into the realm of probability theory, we’ll write *X*[i] for the event
    “a message contains the word *w*[i].” Also imagine that (through some unspecified-at-this-point
    process) we’ve come up with an estimate *P*(*X*[i]|*S*) for the probability that
    a spam message contains the *i*th word, and a similar estimate *P*(*X*[i]|¬*S*)
    for the probability that a nonspam message contains the *i*th word.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 现在想象我们有一个包含许多词*w*[1] ... *w*[n]的词汇表。为了将其转化为概率理论的领域，我们将*X*[i]写成事件“消息包含词*w*[i]”。还想象一下（通过某种未指定的过程），我们已经得出了一个估计值*P*(*X*[i]|*S*)，表示垃圾邮件消息包含第*i*个词的概率，以及类似的估计*P*(*X*[i]|¬*S*)，表示非垃圾邮件消息包含第*i*个词的概率。
- en: 'The key to Naive Bayes is making the (big) assumption that the presences (or
    absences) of each word are independent of one another, conditional on a message
    being spam or not. Intuitively, this assumption means that knowing whether a certain
    spam message contains the word *bitcoin* gives you no information about whether
    that same message contains the word *rolex*. In math terms, this means that:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯方法的关键在于做出（大胆的）假设，即每个单词的存在（或不存在）在消息是垃圾邮件与否的条件下是独立的。直观地说，这个假设意味着知道某个垃圾邮件消息是否包含词*bitcoin*并不会告诉你同一消息是否包含词*rolex*。用数学术语来说，这意味着：
- en: <math alttext="upper P left-parenthesis upper X 1 equals x 1 comma period period
    period comma upper X Subscript n Baseline equals x Subscript n Baseline vertical-bar
    upper S right-parenthesis equals upper P left-parenthesis upper X 1 equals x 1
    vertical-bar upper S right-parenthesis times ellipsis times upper P left-parenthesis
    upper X Subscript n Baseline equals x Subscript n Baseline vertical-bar upper
    S right-parenthesis" display="block"><mrow><mi>P</mi> <mrow><mo>(</mo> <msub><mi>X</mi>
    <mn>1</mn></msub> <mo>=</mo> <msub><mi>x</mi> <mn>1</mn></msub> <mo>,</mo> <mo>.</mo>
    <mo>.</mo> <mo>.</mo> <mo>,</mo> <msub><mi>X</mi> <mi>n</mi></msub> <mo>=</mo>
    <msub><mi>x</mi> <mi>n</mi></msub> <mo>|</mo> <mi>S</mi> <mo>)</mo></mrow> <mo>=</mo>
    <mi>P</mi> <mrow><mo>(</mo> <msub><mi>X</mi> <mn>1</mn></msub> <mo>=</mo> <msub><mi>x</mi>
    <mn>1</mn></msub> <mo>|</mo> <mi>S</mi> <mo>)</mo></mrow> <mo>×</mo> <mo>⋯</mo>
    <mo>×</mo> <mi>P</mi> <mrow><mo>(</mo> <msub><mi>X</mi> <mi>n</mi></msub> <mo>=</mo>
    <msub><mi>x</mi> <mi>n</mi></msub> <mo>|</mo> <mi>S</mi> <mo>)</mo></mrow></mrow></math>
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper P left-parenthesis upper X 1 equals x 1 comma period period
    period comma upper X Subscript n Baseline equals x Subscript n Baseline vertical-bar
    upper S right-parenthesis equals upper P left-parenthesis upper X 1 equals x 1
    vertical-bar upper S right-parenthesis times ellipsis times upper P left-parenthesis
    upper X Subscript n Baseline equals x Subscript n Baseline vertical-bar upper
    S right-parenthesis" display="block"><mrow><mi>P</mi> <mrow><mo>(</mo> <msub><mi>X</mi>
    <mn>1</mn></msub> <mo>=</mo> <msub><mi>x</mi> <mn>1</mn></msub> <mo>,</mo> <mo>.</mo>
    <mo>.</mo> <mo>.</mo> <mo>,</mo> <msub><mi>X</mi> <mi>n</mi></msub> <mo>=</mo>
    <msub><mi>x</mi> <mi>n</mi></msub> <mo>|</mo> <mi>S</mi> <mo>)</mo></mrow> <mo>=</mo>
    <mi>P</mi> <mrow><mo>(</mo> <msub><mi>X</mi> <mn>1</mn></msub> <mo>=</mo> <msub><mi>x</mi>
    <mn>1</mn></msub> <mo>|</mo> <mi>S</mi> <mo>)</mo></mrow> <mo>×</mo> <mo>⋯</mo>
    <mo>×</mo> <mi>P</mi> <mrow><mo>(</mo> <msub><mi>X</mi> <mi>n</mi></msub> <mo>=</mo>
    <msub><mi>x</mi> <mi>n</mi></msub> <mo>|</mo> <mi>S</mi> <mo>)</mo></mrow></mrow></math>
- en: 'This is an extreme assumption. (There’s a reason the technique has *naive*
    in its name.) Imagine that our vocabulary consists *only* of the words *bitcoin*
    and *rolex*, and that half of all spam messages are for “earn bitcoin” and that
    the other half are for “authentic rolex.” In this case, the Naive Bayes estimate
    that a spam message contains both *bitcoin* and *rolex* is:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个极端的假设。（这个技术名称中带有*天真*也是有原因的。）假设我们的词汇表*仅仅*包括*比特币*和*劳力士*这两个词，而一半的垃圾邮件是关于“赚取比特币”，另一半是关于“正品劳力士”。在这种情况下，朴素贝叶斯估计一封垃圾邮件包含*比特币*和*劳力士*的概率是：
- en: <math alttext="upper P left-parenthesis upper X 1 equals 1 comma upper X 2 equals
    1 vertical-bar upper S right-parenthesis equals upper P left-parenthesis upper
    X 1 equals 1 vertical-bar upper S right-parenthesis upper P left-parenthesis upper
    X 2 equals 1 vertical-bar upper S right-parenthesis equals .5 times .5 equals
    .25" display="block"><mrow><mi>P</mi> <mrow><mo>(</mo> <msub><mi>X</mi> <mn>1</mn></msub>
    <mo>=</mo> <mn>1</mn> <mo>,</mo> <msub><mi>X</mi> <mn>2</mn></msub> <mo>=</mo>
    <mn>1</mn> <mo>|</mo> <mi>S</mi> <mo>)</mo></mrow> <mo>=</mo> <mi>P</mi> <mrow><mo>(</mo>
    <msub><mi>X</mi> <mn>1</mn></msub> <mo>=</mo> <mn>1</mn> <mo>|</mo> <mi>S</mi>
    <mo>)</mo></mrow> <mi>P</mi> <mrow><mo>(</mo> <msub><mi>X</mi> <mn>2</mn></msub>
    <mo>=</mo> <mn>1</mn> <mo>|</mo> <mi>S</mi> <mo>)</mo></mrow> <mo>=</mo> <mo>.</mo>
    <mn>5</mn> <mo>×</mo> <mo>.</mo> <mn>5</mn> <mo>=</mo> <mo>.</mo> <mn>25</mn></mrow></math>
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper P left-parenthesis upper X 1 equals 1 comma upper X 2 equals
    1 vertical-bar upper S right-parenthesis equals upper P left-parenthesis upper
    X 1 equals 1 vertical-bar upper S right-parenthesis upper P left-parenthesis upper
    X 2 equals 1 vertical-bar upper S right-parenthesis equals .5 times .5 equals
    .25" display="block"><mrow><mi>P</mi> <mrow><mo>(</mo> <msub><mi>X</mi> <mn>1</mn></msub>
    <mo>=</mo> <mn>1</mn> <mo>,</mo> <msub><mi>X</mi> <mn>2</mn></msub> <mo>=</mo>
    <mn>1</mn> <mo>|</mo> <mi>S</mi> <mo>)</mo></mrow> <mo>=</mo> <mi>P</mi> <mrow><mo>(</mo>
    <msub><mi>X</mi> <mn>1</mn></msub> <mo>=</mo> <mn>1</mn> <mo>|</mo> <mi>S</mi>
    <mo>)</mo></mrow> <mi>P</mi> <mrow><mo>(</mo> <msub><mi>X</mi> <mn>2</mn></msub>
    <mo>=</mo> <mn>1</mn> <mo>|</mo> <mi>S</mi> <mo>)</mo></mrow> <mo>=</mo> <mo>.</mo>
    <mn>5</mn> <mo>×</mo> <mo>.</mo> <mn>5</mn> <mo>=</mo> <mo>.</mo> <mn>25</mn></mrow></math>
- en: since we’ve assumed away the knowledge that *bitcoin* and *rolex* actually never
    occur together. Despite the unrealisticness of this assumption, this model often
    performs well and has historically been used in actual spam filters.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们假设了*比特币*和*劳力士*实际上从不会同时出现的知识。尽管这种假设的不现实性，这种模型通常表现良好，并且在实际的垃圾邮件过滤器中历史悠久地使用。
- en: 'The same Bayes’s theorem reasoning we used for our “bitcoin-only” spam filter
    tells us that we can calculate the probability a message is spam using the equation:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们用于“仅比特币”垃圾邮件过滤器的相同贝叶斯定理推理告诉我们可以使用以下方程来计算一封邮件是垃圾邮件的概率：
- en: <math alttext="upper P left-parenthesis upper S vertical-bar upper X equals
    x right-parenthesis equals upper P left-parenthesis upper X equals x vertical-bar
    upper S right-parenthesis slash left-bracket upper P left-parenthesis upper X
    equals x vertical-bar upper S right-parenthesis plus upper P left-parenthesis
    upper X equals x vertical-bar normal not-sign upper S right-parenthesis right-bracket"
    display="block"><mrow><mi>P</mi> <mo>(</mo> <mi>S</mi> <mo>|</mo> <mi>X</mi> <mo>=</mo>
    <mi>x</mi> <mo>)</mo> <mo>=</mo> <mi>P</mi> <mo>(</mo> <mi>X</mi> <mo>=</mo> <mi>x</mi>
    <mo>|</mo> <mi>S</mi> <mo>)</mo> <mo>/</mo> <mo>[</mo> <mi>P</mi> <mo>(</mo> <mi>X</mi>
    <mo>=</mo> <mi>x</mi> <mo>|</mo> <mi>S</mi> <mo>)</mo> <mo>+</mo> <mi>P</mi> <mo>(</mo>
    <mi>X</mi> <mo>=</mo> <mi>x</mi> <mo>|</mo> <mo>¬</mo> <mi>S</mi> <mo>)</mo> <mo>]</mo></mrow></math>
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper P left-parenthesis upper S vertical-bar upper X equals
    x right-parenthesis equals upper P left-parenthesis upper X equals x vertical-bar
    upper S right-parenthesis slash left-bracket upper P left-parenthesis upper X
    equals x vertical-bar upper S right-parenthesis plus upper P left-parenthesis
    upper X equals x vertical-bar normal not-sign upper S right-parenthesis right-bracket"
    display="block"><mrow><mi>P</mi> <mo>(</mo> <mi>S</mi> <mo>|</mo> <mi>X</mi> <mo>=</mo>
    <mi>x</mi> <mo>)</mo> <mo>=</mo> <mi>P</mi> <mo>(</mo> <mi>X</mi> <mo>=</mo> <mi>x</mi>
    <mo>|</mo> <mi>S</mi> <mo>)</mo> <mo>/</mo> <mo>[</mo> <mi>P</mi> <mo>(</mo> <mi>X</mi>
    <mo>=</mo> <mi>x</mi> <mo>|</mo> <mi>S</mi> <mo>)</mo> <mo>+</mo> <mi>P</mi> <mo>(</mo>
    <mi>X</mi> <mo>=</mo> <mi>x</mi> <mo>|</mo> <mo>¬</mo> <mi>S</mi> <mo>)</mo> <mo>]</mo></mrow></math>
- en: The Naive Bayes assumption allows us to compute each of the probabilities on
    the right simply by multiplying together the individual probability estimates
    for each vocabulary word.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 朴素贝叶斯假设允许我们通过简单地将每个词汇单词的概率估计相乘来计算右侧的每个概率。
- en: 'In practice, you usually want to avoid multiplying lots of probabilities together,
    to prevent a problem called *underflow*, in which computers don’t deal well with
    floating-point numbers that are too close to 0. Recalling from algebra that <math
    alttext="log left-parenthesis a b right-parenthesis equals log a plus log b"><mrow><mo
    form="prefix">log</mo> <mo>(</mo> <mi>a</mi> <mi>b</mi> <mo>)</mo> <mo>=</mo>
    <mo form="prefix">log</mo> <mi>a</mi> <mo>+</mo> <mo form="prefix">log</mo> <mi>b</mi></mrow></math>
    and that <math alttext="exp left-parenthesis log x right-parenthesis equals x"><mrow><mo
    form="prefix">exp</mo><mo>(</mo> <mo form="prefix">log</mo> <mi>x</mi> <mo>)</mo>
    <mo>=</mo> <mi>x</mi></mrow></math> , we usually compute <math alttext="p 1 asterisk
    ellipsis asterisk p Subscript n"><mrow><msub><mi>p</mi> <mn>1</mn></msub> <mo>*</mo>
    <mo>⋯</mo> <mo>*</mo> <msub><mi>p</mi> <mi>n</mi></msub></mrow></math> as the
    equivalent (but floating-point-friendlier):'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 实际操作中，通常要避免将大量概率相乘，以防止*下溢*问题，即计算机无法处理太接近0的浮点数。从代数中记得<math alttext="log left-parenthesis
    a b right-parenthesis equals log a plus log b"><mrow><mo form="prefix">log</mo>
    <mo>(</mo> <mi>a</mi> <mi>b</mi> <mo>)</mo> <mo>=</mo> <mo form="prefix">log</mo>
    <mi>a</mi> <mo>+</mo> <mo form="prefix">log</mo> <mi>b</mi></mrow></math> 和<math
    alttext="exp left-parenthesis log x right-parenthesis equals x"><mrow><mo form="prefix">exp</mo><mo>(</mo>
    <mo form="prefix">log</mo> <mi>x</mi> <mo>)</mo> <mo>=</mo> <mi>x</mi></mrow></math>
    ，我们通常将<math alttext="p 1 asterisk ellipsis asterisk p Subscript n"><mrow><msub><mi>p</mi>
    <mn>1</mn></msub> <mo>*</mo> <mo>⋯</mo> <mo>*</mo> <msub><mi>p</mi> <mi>n</mi></msub></mrow></math>
    计算为等效的（但更友好于浮点数的）形式：
- en: <math alttext="exp left-parenthesis log left-parenthesis p 1 right-parenthesis
    plus ellipsis plus log left-parenthesis p Subscript n Baseline right-parenthesis
    right-parenthesis" display="block"><mrow><mo form="prefix">exp</mo><mo>(</mo>
    <mo form="prefix">log</mo> <mrow><mo>(</mo> <msub><mi>p</mi> <mn>1</mn></msub>
    <mo>)</mo></mrow> <mo>+</mo> <mo>⋯</mo> <mo>+</mo> <mo form="prefix">log</mo>
    <mrow><mo>(</mo> <msub><mi>p</mi> <mi>n</mi></msub> <mo>)</mo></mrow> <mo>)</mo></mrow></math>
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="exp left-parenthesis log left-parenthesis p 1 right-parenthesis
    plus ellipsis plus log left-parenthesis p Subscript n Baseline right-parenthesis
    right-parenthesis" display="block"><mrow><mo form="prefix">exp</mo><mo>(</mo>
    <mo form="prefix">log</mo> <mrow><mo>(</mo> <msub><mi>p</mi> <mn>1</mn></msub>
    <mo>)</mo></mrow> <mo>+</mo> <mo>⋯</mo> <mo>+</mo> <mo form="prefix">log</mo>
    <mrow><mo>(</mo> <msub><mi>p</mi> <mi>n</mi></msub> <mo>)</mo></mrow> <mo>)</mo></mrow></math>
- en: The only challenge left is coming up with estimates for <math><mrow><mi>P</mi>
    <mo>(</mo> <msub><mi>X</mi><mi>i</mi></msub> <mo>|</mo> <mi>S</mi> <mo>)</mo></mrow></math>
    and <math><mrow><mi>P</mi> <mo>(</mo> <msub><mi>X</mi><mi>i</mi></msub> <mo>|</mo>
    <mo>¬</mo> <mi>S</mi> <mo>)</mo></mrow></math> , the probabilities that a spam
    message (or nonspam message) contains the word <math><msub><mi>w</mi> <mi>i</mi></msub></math>
    . If we have a fair number of “training” messages labeled as spam and not spam,
    an obvious first try is to estimate <math><mrow><mi>P</mi> <mo>(</mo> <msub><mi>X</mi>
    <mi>i</mi></msub> <mo>|</mo> <mi>S</mi> <mo>)</mo></mrow></math> simply as the
    fraction of spam messages containing the word <math><msub><mi>w</mi> <mi>i</mi></msub></math>
    .
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 唯一剩下的挑战是估计<math><mrow><mi>P</mi> <mo>(</mo> <msub><mi>X</mi><mi>i</mi></msub>
    <mo>|</mo> <mi>S</mi> <mo>)</mo></mrow></math> 和<math><mrow><mi>P</mi> <mo>(</mo>
    <msub><mi>X</mi><mi>i</mi></msub> <mo>|</mo> <mo>¬</mo> <mi>S</mi> <mo>)</mo></mrow></math>
    ，即垃圾邮件（或非垃圾邮件）包含单词<math><msub><mi>w</mi> <mi>i</mi></msub></math> 的概率。如果我们有大量标记为垃圾邮件和非垃圾邮件的“训练”邮件，一个明显的第一次尝试是简单地估计<math><mrow><mi>P</mi>
    <mo>(</mo> <msub><mi>X</mi> <mi>i</mi></msub> <mo>|</mo> <mi>S</mi> <mo>)</mo></mrow></math>
    为仅仅是包含单词<math><msub><mi>w</mi> <mi>i</mi></msub></math> 的垃圾邮件的比例。
- en: This causes a big problem, though. Imagine that in our training set the vocabulary
    word *data* only occurs in nonspam messages. Then we’d estimate <math alttext="upper
    P left-parenthesis quotation-mark data quotation-mark vertical-bar upper S right-parenthesis
    equals 0"><mrow><mi>P</mi> <mo>(</mo> <mtext>data</mtext> <mo>|</mo> <mi>S</mi>
    <mo>)</mo> <mo>=</mo> <mn>0</mn></mrow></math> . The result is that our Naive
    Bayes classifier would always assign spam probability 0 to *any* message containing
    the word *data*, even a message like “data on free bitcoin and authentic rolex
    watches.” To avoid this problem, we usually use some kind of smoothing.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 不过，这会造成一个很大的问题。想象一下，在我们的训练集中，词汇表中的单词*data*只出现在非垃圾邮件中。然后我们会估计<math alttext="upper
    P left-parenthesis quotation-mark data quotation-mark vertical-bar upper S right-parenthesis
    equals 0"><mrow><mi>P</mi> <mo>(</mo> <mtext>data</mtext> <mo>|</mo> <mi>S</mi>
    <mo>)</mo> <mo>=</mo> <mn>0</mn></mrow></math>。结果是，我们的朴素贝叶斯分类器将始终为包含单词*data*的*任何*消息分配垃圾邮件概率0，即使是像“免费比特币和正品劳力士手表数据”这样的消息也是如此。为了避免这个问题，我们通常使用某种平滑技术。
- en: 'In particular, we’ll choose a *pseudocount*—*k*—and estimate the probability
    of seeing the *i*th word in a spam message as:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是，我们将选择一个*伪计数*——*k*——并估计在垃圾邮件中看到第*i*个单词的概率为：
- en: <math alttext="upper P left-parenthesis upper X Subscript i Baseline vertical-bar
    upper S right-parenthesis equals left-parenthesis k plus number of spams containing
    w Subscript i Baseline right-parenthesis slash left-parenthesis 2 k plus number
    of spams right-parenthesis" display="block"><mrow><mi>P</mi> <mrow><mo>(</mo>
    <msub><mi>X</mi> <mi>i</mi></msub> <mo>|</mo> <mi>S</mi> <mo>)</mo></mrow> <mo>=</mo>
    <mrow><mo>(</mo> <mi>k</mi> <mo>+</mo> <mtext>number</mtext> <mtext>of</mtext>
    <mtext>spams</mtext> <mtext>containing</mtext> <msub><mi>w</mi> <mi>i</mi></msub>
    <mo>)</mo></mrow> <mo>/</mo> <mrow><mo>(</mo> <mn>2</mn> <mi>k</mi> <mo>+</mo>
    <mtext>number</mtext> <mtext>of</mtext> <mtext>spams</mtext> <mo>)</mo></mrow></mrow></math>
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper P left-parenthesis upper X Subscript i Baseline vertical-bar
    upper S right-parenthesis equals left-parenthesis k plus number of spams containing
    w Subscript i Baseline right-parenthesis slash left-parenthesis 2 k plus number
    of spams right-parenthesis" display="block"><mrow><mi>P</mi> <mrow><mo>(</mo>
    <msub><mi>X</mi> <mi>i</mi></msub> <mo>|</mo> <mi>S</mi> <mo>)</mo></mrow> <mo>=</mo>
    <mrow><mo>(</mo> <mi>k</mi> <mo>+</mo> <mtext>number</mtext> <mtext>of</mtext>
    <mtext>spams</mtext> <mtext>containing</mtext> <msub><mi>w</mi> <mi>i</mi></msub>
    <mo>)</mo></mrow> <mo>/</mo> <mrow><mo>(</mo> <mn>2</mn> <mi>k</mi> <mo>+</mo>
    <mtext>number</mtext> <mtext>of</mtext> <mtext>spams</mtext> <mo>)</mo></mrow></mrow></math>
- en: We do similarly for <math><mrow><mi>P</mi> <mo>(</mo> <msub><mi>X</mi> <mi>i</mi></msub>
    <mo>|</mo> <mo>¬</mo> <mi>S</mi> <mo>)</mo></mrow></math> . That is, when computing
    the spam probabilities for the *i*th word, we assume we also saw *k* additional
    nonspams containing the word and *k* additional nonspams not containing the word.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对<math><mrow><mi>P</mi> <mo>(</mo> <msub><mi>X</mi> <mi>i</mi></msub> <mo>|</mo>
    <mo>¬</mo> <mi>S</mi> <mo>)</mo></mrow></math>也是类似的。也就是说，当计算第*i*个单词的垃圾邮件概率时，我们假设我们还看到了包含该单词的*k*个额外的非垃圾邮件和*k*个额外的不包含该单词的非垃圾邮件。
- en: For example, if *data* occurs in 0/98 spam messages, and if *k* is 1, we estimate
    *P*(data|*S*) as 1/100 = 0.01, which allows our classifier to still assign some
    nonzero spam probability to messages that contain the word *data*.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果*data*出现在0/98封垃圾邮件中，如果*k*为1，则我们将*P*(data|*S*)估计为1/100 = 0.01，这使得我们的分类器仍然可以为包含单词*data*的消息分配一些非零的垃圾邮件概率。
- en: Implementation
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实施
- en: 'Now we have all the pieces we need to build our classifier. First, let’s create
    a simple function to tokenize messages into distinct words. We’ll first convert
    each message to lowercase, then use `re.findall` to extract “words” consisting
    of letters, numbers, and apostrophes. Finally, we’ll use `set` to get just the
    distinct words:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了构建分类器所需的所有组件。首先，让我们创建一个简单的函数，将消息标记为不同的单词。我们首先将每条消息转换为小写，然后使用`re.findall`提取由字母、数字和撇号组成的“单词”。最后，我们将使用`set`获取不同的单词：
- en: '[PRE0]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We’ll also define a type for our training data:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将为我们的训练数据定义一个类型：
- en: '[PRE1]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: As our classifier needs to keep track of tokens, counts, and labels from the
    training data, we’ll make it a class. Following convention, we refer to nonspam
    emails as *ham* emails.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的分类器需要跟踪来自训练数据的标记、计数和标签，我们将其构建为一个类。按照惯例，我们将非垃圾邮件称为*ham*邮件。
- en: 'The constructor will take just one parameter, the pseudocount to use when computing
    probabilities. It also initializes an empty set of tokens, counters to track how
    often each token is seen in spam messages and ham messages, and counts of how
    many spam and ham messages it was trained on:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 构造函数只需要一个参数，即计算概率时使用的伪计数。它还初始化一个空的标记集合、计数器来跟踪在垃圾邮件和非垃圾邮件中看到每个标记的频率，以及它被训练的垃圾邮件和非垃圾邮件的数量：
- en: '[PRE2]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Next, we’ll give it a method to train it on a bunch of messages. First, we
    increment the `spam_messages` and `ham_messages` counts. Then we tokenize each
    message text, and for each token we increment the `token_spam_counts` or `token_ham_counts`
    based on the message type:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将为其提供一个方法，让它训练一堆消息。首先，我们增加`spam_messages`和`ham_messages`计数。然后我们对每个消息文本进行标记化，对于每个标记，根据消息类型递增`token_spam_counts`或`token_ham_counts`：
- en: '[PRE3]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Ultimately we’ll want to predict *P*(spam | token). As we saw earlier, to apply
    Bayes’s theorem we need to know *P*(token | spam) and *P*(token | ham) for each
    token in the vocabulary. So we’ll create a “private” helper function to compute
    those:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，我们将想要预测*P*(垃圾邮件 | 标记)。正如我们之前看到的，要应用贝叶斯定理，我们需要知道每个词汇表中的标记对于*垃圾邮件*和*非垃圾邮件*的*P*(标记
    | 垃圾邮件)和*P*(标记 | 非垃圾邮件)。因此，我们将创建一个“私有”辅助函数来计算这些：
- en: '[PRE4]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Finally, we’re ready to write our `predict` method. As mentioned earlier, rather
    than multiplying together lots of small probabilities, we’ll instead sum up the
    log probabilities:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们准备编写我们的`predict`方法。如前所述，我们不会将许多小概率相乘，而是将对数概率相加：
- en: '[PRE5]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: And now we have a classifier.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了一个分类器。
- en: Testing Our Model
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测试我们的模型
- en: Let’s make sure our model works by writing some unit tests for it.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'First, let’s check that it got the counts right:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Now let’s make a prediction. We’ll also (laboriously) go through our Naive
    Bayes logic by hand, and make sure that we get the same result:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This test passes, so it seems like our model is doing what we think it is. If
    you look at the actual probabilities, the two big drivers are that our message
    contains *spam* (which our lone training spam message did) and that it doesn’t
    contain *ham* (which both our training ham messages did).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s try it on some real data.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: Using Our Model
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A popular (if somewhat old) dataset is the [SpamAssassin public corpus](https://spamassassin.apache.org/old/publiccorpus/).
    We’ll look at the files prefixed with *20021010*.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a script that will download and unpack them to the directory of your
    choice (or you can do it manually):'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: It’s possible the location of the files will change (this happened between the
    first and second editions of this book), in which case adjust the script accordingly.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: 'After downloading the data you should have three folders: *spam*, *easy_ham*,
    and *hard_ham*. Each folder contains many emails, each contained in a single file.
    To keep things *really* simple, we’ll just look at the subject lines of each email.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: 'How do we identify the subject line? When we look through the files, they all
    seem to start with “Subject:”. So we’ll look for that:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now we can split the data into training data and test data, and then we’re
    ready to build a classifier:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Let’s generate some predictions and check how our model does:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: This gives 84 true positives (spam classified as “spam”), 25 false positives
    (ham classified as “spam”), 703 true negatives (ham classified as “ham”), and
    44 false negatives (spam classified as “ham”). This means our precision is 84
    / (84 + 25) = 77%, and our recall is 84 / (84 + 44) = 65%, which are not bad numbers
    for such a simple model. (Presumably we’d do better if we looked at more than
    the subject lines.)
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also inspect the model’s innards to see which words are least and most
    indicative of spam:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The spammiest words include things like *sale*, *mortgage*, *money*, and *rates*,
    whereas the hammiest words include things like *spambayes*, *users*, *apt*, and
    *perl*. So that also gives us some intuitive confidence that our model is basically
    doing the right thing.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: 'How could we get better performance? One obvious way would be to get more data
    to train on. There are a number of ways to improve the model as well. Here are
    some possibilities that you might try:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: Look at the message content, not just the subject line. You’ll have to be careful
    how you deal with the message headers.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Our classifier takes into account every word that appears in the training set,
    even words that appear only once. Modify the classifier to accept an optional
    `min_count` threshold and ignore tokens that don’t appear at least that many times.
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The tokenizer has no notion of similar words (e.g., *cheap* and *cheapest*).
    Modify the classifier to take an optional `stemmer` function that converts words
    to *equivalence classes* of words. For example, a really simple stemmer function
    might be:'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分词器没有类似词汇的概念（例如 *cheap* 和 *cheapest*）。修改分类器以接受可选的 `stemmer` 函数，将单词转换为 *等价类*
    的单词。例如，一个非常简单的词干提取函数可以是：
- en: '[PRE14]'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Creating a good stemmer function is hard. People frequently use the [Porter
    stemmer](http://tartarus.org/martin/PorterStemmer/).
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 创建一个良好的词干提取函数很难。人们经常使用 [Porter stemmer](http://tartarus.org/martin/PorterStemmer/)。
- en: Although our features are all of the form “message contains word <math><msub><mi>w</mi>
    <mi>i</mi></msub></math> ,” there’s no reason why this has to be the case. In
    our implementation, we could add extra features like “message contains a number”
    by creating phony tokens like *contains:number* and modifying the `tokenizer`
    to emit them when appropriate.
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尽管我们的特征都是形如“消息包含词 <math><msub><mi>w</mi> <mi>i</mi></msub></math>”，但这并非必须如此。在我们的实现中，我们可以通过创建虚假标记，如
    *contains:number*，并在适当时修改 `tokenizer` 以发出它们来添加额外的特征，比如“消息包含数字”。
- en: For Further Exploration
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步探索
- en: Paul Graham’s articles [“A Plan for Spam”](http://www.paulgraham.com/spam.html)
    and [“Better Bayesian Filtering”](http://www.paulgraham.com/better.html) are interesting
    and give more insight into the ideas behind building spam filters.
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 保罗·格雷厄姆的文章 [“一个垃圾邮件过滤计划”](http://www.paulgraham.com/spam.html) 和 [“更好的贝叶斯过滤”](http://www.paulgraham.com/better.html)
    非常有趣，并深入探讨了构建垃圾邮件过滤器背后的思想。
- en: '[scikit-learn](https://scikit-learn.org/stable/modules/naive_bayes.html) contains
    a `BernoulliNB` model that implements the same Naive Bayes algorithm we implemented
    here, as well as other variations on the model.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[scikit-learn](https://scikit-learn.org/stable/modules/naive_bayes.html) 包含一个
    `BernoulliNB` 模型，实现了我们这里实现的相同朴素贝叶斯算法，以及模型的其他变体。'
