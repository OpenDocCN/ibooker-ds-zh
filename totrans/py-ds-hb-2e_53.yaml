- en: 'Chapter 48\. In Depth: Gaussian Mixture Models'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第48章：深入：高斯混合模型
- en: The *k*-means clustering model explored in the previous chapter is simple and
    relatively easy to understand, but its simplicity leads to practical challenges
    in its application. In particular, the nonprobabilistic nature of *k*-means and
    its use of simple distance from cluster center to assign cluster membership leads
    to poor performance for many real-world situations. In this chapter we will take
    a look at Gaussian mixture models, which can be viewed as an extension of the
    ideas behind *k*-means, but can also be a powerful tool for estimation beyond
    simple clustering.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一章中探讨的 *k*-means 聚类模型简单且相对易于理解，但其简单性导致在实际应用中存在实际挑战。特别是，*k*-means 的非概率性质以及其使用简单的距离从聚类中心分配聚类成员导致在许多实际情况下性能不佳。在本章中，我们将介绍高斯混合模型，它可以被视为对
    *k*-means 背后思想的扩展，同时也可以是一种超越简单聚类的强大工具。
- en: 'We begin with the standard imports:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从标准导入开始：
- en: '[PRE0]'
  id: totrans-3
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Motivating Gaussian Mixtures: Weaknesses of k-Means'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 激励高斯混合模型：*k*-means 的弱点
- en: Let’s take a look at some of the weaknesses of *k*-means and think about how
    we might improve the cluster model. As we saw in the previous chapter, given simple,
    well-separated data, *k*-means finds suitable clustering results.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一些 *k*-means 的弱点，并思考如何改进聚类模型。正如我们在前一章中看到的，对于简单而明确分离的数据，*k*-means 能够找到合适的聚类结果。
- en: For example, if we have simple blobs of data, the *k*-means algorithm can quickly
    label those clusters in a way that closely matches what we might do by eye (see
    [Figure 48-1](#fig_0512-gaussian-mixtures_files_in_output_5_0)).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们有简单的数据块，*k*-means 算法可以快速地标记这些聚类，其结果与我们可能通过眼睛观察到的相似（参见 [图48-1](#fig_0512-gaussian-mixtures_files_in_output_5_0)）。
- en: '[PRE1]'
  id: totrans-7
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-8
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![output 5 0](assets/output_5_0.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![output 5 0](assets/output_5_0.png)'
- en: Figure 48-1\. k-means labels for simple data
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图48-1：简单数据的 k-means 标签
- en: 'From an intuitive standpoint, we might expect that the clustering assignment
    for some points is more certain than others: for example, there appears to be
    a very slight overlap between the two middle clusters, such that we might not
    have complete confidence in the cluster assignment of points between them. Unfortunately,
    the *k*-means model has no intrinsic measure of probability or uncertainty of
    cluster assignments (although it may be possible to use a bootstrap approach to
    estimate this uncertainty). For this, we must think about generalizing the model.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 从直觉上讲，我们可能期望某些点的聚类分配比其他点更加确定：例如，在两个中间聚类之间似乎存在非常轻微的重叠，因此我们可能对它们之间的点的聚类分配没有完全的信心。不幸的是，*k*-means
    模型没有内在的概率或聚类分配不确定性的衡量方法（尽管可能可以使用自举方法来估计此不确定性）。为此，我们必须考虑模型的泛化。
- en: 'One way to think about the *k*-means model is that it places a circle (or,
    in higher dimensions, a hypersphere) at the center of each cluster, with a radius
    defined by the most distant point in the cluster. This radius acts as a hard cutoff
    for cluster assignment within the training set: any point outside this circle
    is not considered a member of the cluster. We can visualize this cluster model
    with the following function (see [Figure 48-2](#fig_0512-gaussian-mixtures_files_in_output_8_0)).'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 关于 *k*-means 模型的一种思考方式是，它在每个聚类的中心放置一个圆（或在更高维度中，一个超球体），其半径由聚类中最远的点定义。这个半径作为训练集内聚类分配的硬截止：任何在这个圆外的点都不被视为聚类的成员。我们可以用以下函数可视化这个聚类模型（参见
    [图48-2](#fig_0512-gaussian-mixtures_files_in_output_8_0)）。
- en: '[PRE3]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![output 8 0](assets/output_8_0.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![output 8 0](assets/output_8_0.png)'
- en: Figure 48-2\. The circular clusters implied by the k-means model
  id: totrans-16
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图48-2：k-means 模型暗示的圆形聚类
- en: 'An important observation for *k*-means is that these cluster models *must be
    circular*: *k*-means has no built-in way of accounting for oblong or elliptical
    clusters. So, for example, if we take the same data and transform it, the cluster
    assignments end up becoming muddled, as you can see in [Figure 48-3](#fig_0512-gaussian-mixtures_files_in_output_10_0).'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 *k*-means 的一个重要观察是，这些聚类模型必须是圆形的：*k*-means 没有内建的方法来处理椭圆形或椭圆形聚类。因此，例如，如果我们取同样的数据并对其进行转换，聚类分配最终变得混乱，正如你可以在
    [图48-3](#fig_0512-gaussian-mixtures_files_in_output_10_0) 中看到的。
- en: '[PRE5]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![output 10 0](assets/output_10_0.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![output 10 0](assets/output_10_0.png)'
- en: Figure 48-3\. Poor performance of k-means for noncircular clusters
  id: totrans-20
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图48-3：*k*-means 对于非圆形聚类的性能不佳
- en: 'By eye, we recognize that these transformed clusters are noncircular, and thus
    circular clusters would be a poor fit. Nevertheless, *k*-means is not flexible
    enough to account for this, and tries to force-fit the data into four circular
    clusters. This results in a mixing of cluster assignments where the resulting
    circles overlap: see especially the bottom-right of this plot. One might imagine
    addressing this particular situation by preprocessing the data with PCA (see [Chapter 45](ch45.xhtml#section-0509-principal-component-analysis)),
    but in practice there is no guarantee that such a global operation will circularize
    the individual groups.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 凭眼观察，我们认识到这些转换后的聚类不是圆形的，因此圆形聚类会拟合效果差。然而，*k*-means 不足以解决这个问题，并试图强行将数据拟合为四个圆形聚类。这导致了聚类分配的混合，其中结果的圆形重叠：尤其是在图的右下角可见。可以想象通过使用PCA预处理数据来处理这种情况（参见[第45章](ch45.xhtml#section-0509-principal-component-analysis)），但实际上不能保证这样的全局操作会使各个群体圆形化。
- en: These two disadvantages of *k*-means—its lack of flexibility in cluster shape
    and lack of probabilistic cluster assignment—mean that for many datasets (especially
    low-dimensional datasets) it may not perform as well as you might hope.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '*k*-means 的这两个缺点——在聚类形状上的灵活性不足和缺乏概率聚类分配——意味着对于许多数据集（特别是低维数据集），其性能可能不如人们所期望的那样好。'
- en: 'You might imagine addressing these weaknesses by generalizing the *k*-means
    model: for example, you could measure uncertainty in cluster assignment by comparing
    the distances of each point to *all* cluster centers, rather than focusing on
    just the closest. You might also imagine allowing the cluster boundaries to be
    ellipses rather than circles, so as to account for noncircular clusters. It turns
    out these are two essential components of a different type of clustering model,
    Gaussian mixture models.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过泛化*k*-means模型来解决这些弱点：例如，可以通过比较每个点到*所有*聚类中心的距离来测量聚类分配的不确定性，而不是仅关注最近的距离。您还可以想象允许聚类边界为椭圆而不是圆形，以适应非圆形聚类。事实证明，这些是不同类型聚类模型——高斯混合模型的两个基本组成部分。
- en: 'Generalizing E–M: Gaussian Mixture Models'
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 泛化E-M：高斯混合模型
- en: A Gaussian mixture model (GMM) attempts to find a mixture of multidimensional
    Gaussian probability distributions that best model any input dataset. In the simplest
    case, GMMs can be used for finding clusters in the same manner as *k*-means (see
    [Figure 48-4](#fig_0512-gaussian-mixtures_files_in_output_13_0)).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 高斯混合模型（GMM）试图找到最适合模拟任何输入数据集的多维高斯概率分布混合物。在最简单的情况下，GMM可以像*k*-means一样用于查找聚类（参见[图48-4](#fig_0512-gaussian-mixtures_files_in_output_13_0)）。
- en: '[PRE6]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![output 13 0](assets/output_13_0.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![output 13 0](assets/output_13_0.png)'
- en: Figure 48-4\. Gaussian mixture model labels for the data
  id: totrans-28
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图48-4。数据的高斯混合模型标签
- en: 'But because a GMM contains a probabilistic model under the hood, it is also
    possible to find probabilistic cluster assignments—in Scikit-Learn this is done
    using the `predict_proba` method. This returns a matrix of size `[n_samples, n_clusters]`
    which measures the probability that any point belongs to the given cluster:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 但是因为GMM在幕后包含一个概率模型，因此可以找到概率聚类分配——在Scikit-Learn中，这是通过`predict_proba`方法完成的。这将返回一个大小为`[n_samples,
    n_clusters]`的矩阵，用于测量任何点属于给定聚类的概率：
- en: '[PRE7]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We can visualize this uncertainty by, for example, making the size of each
    point proportional to the certainty of its prediction; looking at [Figure 48-5](#fig_0512-gaussian-mixtures_files_in_output_17_0),
    we can see that it is precisely the points at the boundaries between clusters
    that reflect this uncertainty of cluster assignment:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过将每个点的大小与其预测的确定性成比例来可视化这种不确定性；查看[图48-5](#fig_0512-gaussian-mixtures_files_in_output_17_0)，我们可以看到恰好在聚类边界上的点反映了聚类分配的不确定性：
- en: '[PRE8]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![output 17 0](assets/output_17_0.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![output 17 0](assets/output_17_0.png)'
- en: 'Figure 48-5\. GMM probabilistic labels: probabilities are shown by the size
    of points'
  id: totrans-34
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图48-5。GMM概率标签：点的大小显示概率大小
- en: 'Under the hood, a Gaussian mixture model is very similar to *k*-means: it uses
    an expectation–maximization approach, which qualitatively does the following:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在幕后，高斯混合模型与*k*-means非常相似：它使用期望最大化方法，大致如下：
- en: Choose starting guesses for the location and shape.
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择位置和形状的初始猜测。
- en: 'Repeat until converged:'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 直到收敛为止重复：
- en: '*E-step*: For each point, find weights encoding the probability of membership
    in each cluster.'
  id: totrans-38
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*E步骤*：对于每个点，找到编码每个聚类成员概率的权重。'
- en: '*M-step*: For each cluster, update its location, normalization, and shape based
    on *all* data points, making use of the weights.'
  id: totrans-39
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*M步*：对于每个聚类，根据*所有*数据点更新其位置、归一化和形状，利用权重。'
- en: The result of this is that each cluster is associated not with a hard-edged
    sphere, but with a smooth Gaussian model. Just as in the *k*-means expectation–maximization
    approach, this algorithm can sometimes miss the globally optimal solution, and
    thus in practice multiple random initializations are used.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 其结果是，每个聚类不再关联于硬边界的球体，而是与平滑的高斯模型相关联。就像*k*-means期望最大化方法一样，这种算法有时会错过全局最优解，因此在实践中使用多个随机初始化。
- en: 'Let’s create a function that will help us visualize the locations and shapes
    of the GMM clusters by drawing ellipses based on the GMM output:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个函数，通过根据GMM输出绘制椭圆来帮助我们可视化GMM聚类的位置和形状：
- en: '[PRE9]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: With this in place, we can take a look at what the four-component GMM gives
    us for our initial data (see [Figure 48-6](#fig_0512-gaussian-mixtures_files_in_output_21_0)).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些基础，我们可以看看四分量GMM对我们的初始数据给出了什么结果（参见[图 48-6](#fig_0512-gaussian-mixtures_files_in_output_21_0)）。
- en: '[PRE10]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![output 21 0](assets/output_21_0.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![output 21 0](assets/output_21_0.png)'
- en: Figure 48-6\. The four-component GMM in the presence of circular clusters
  id: totrans-46
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 48-6. 存在圆形聚类的四分量GMM
- en: Similarly, we can use the GMM approach to fit our stretched dataset; allowing
    for a full covariance the model will fit even very oblong, stretched-out clusters,
    as we can see in [Figure 48-7](#fig_0512-gaussian-mixtures_files_in_output_23_0).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，我们可以使用GMM方法拟合我们的伸展数据集；允许完全协方差模型将适合甚至是非常椭圆形、拉伸的聚类，正如我们在[图 48-7](#fig_0512-gaussian-mixtures_files_in_output_23_0)中所看到的。
- en: '[PRE11]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '![output 23 0](assets/output_23_0.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![output 23 0](assets/output_23_0.png)'
- en: Figure 48-7\. The four-component GMM in the presence of noncircular clusters
  id: totrans-50
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 48-7. 存在非圆形聚类的四分量GMM
- en: This makes clear that GMMs address the two main practical issues with *k*-means
    encountered before.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这清楚地表明，GMM解决了之前在*k*-means中遇到的两个主要实际问题。
- en: Choosing the Covariance Type
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择协方差类型
- en: If you look at the details of the preceding fits, you will see that the `covariance_type`
    option was set differently within each. This hyperparameter controls the degrees
    of freedom in the shape of each cluster; it’s essential to set this carefully
    for any given problem. The default is `covariance_type="diag"`, which means that
    the size of the cluster along each dimension can be set independently, with the
    resulting ellipse constrained to align with the axes. `covariance_type="spherical"`
    is a slightly simpler and faster model, which constrains the shape of the cluster
    such that all dimensions are equal. The resulting clustering will have similar
    characteristics to that of *k*-means, though it’s not entirely equivalent. A more
    complicated and computationally expensive model (especially as the number of dimensions
    grows) is to use `covariance_type="full"`, which allows each cluster to be modeled
    as an ellipse with arbitrary orientation. [Figure 48-8](#fig_images_in_0512-covariance-type)
    represents these three choices for a single cluster.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您查看前面拟合的细节，您会发现在每个拟合中设置了`covariance_type`选项。该超参数控制每个聚类形状的自由度；对于任何给定的问题，仔细设置这一点至关重要。默认值是`covariance_type="diag"`，这意味着可以独立设置每个维度上的聚类大小，生成的椭圆受限于与轴对齐。`covariance_type="spherical"`是一个稍微简单且更快的模型，它限制了聚类形状，使得所有维度相等。结果聚类将具有与*k*-means类似的特征，尽管它并非完全等价。一个更复杂和计算开销更大的模型（特别是在维度增长时）是使用`covariance_type="full"`，它允许将每个聚类建模为带有任意方向的椭圆。[图
    48-8](#fig_images_in_0512-covariance-type)表示了这三种选择对单个聚类的影响。
- en: '![05.12 covariance type](assets/05.12-covariance-type.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![05.12 协方差类型](assets/05.12-covariance-type.png)'
- en: Figure 48-8\. Visualization of GMM covariance types^([1](ch48.xhtml#idm45858720757328))
  id: totrans-55
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 48-8. GMM协方差类型可视化^([1](ch48.xhtml#idm45858720757328))
- en: Gaussian Mixture Models as Density Estimation
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高斯混合模型作为密度估计
- en: Though the GMM is often categorized as a clustering algorithm, fundamentally
    it is an algorithm for *density estimation*. That is to say, the result of a GMM
    fit to some data is technically not a clustering model, but a generative probabilistic
    model describing the distribution of the data.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管GMM通常被归类为聚类算法，但从根本上讲，它是一种用于*密度估计*的算法。也就是说，对某些数据进行GMM拟合的结果在技术上不是聚类模型，而是描述数据分布的生成概率模型。
- en: As an example, consider some data generated from Scikit-Learn’s `make_moons`
    function, introduced in [Chapter 47](ch47.xhtml#section-0511-k-means) (see [Figure 48-9](#fig_0512-gaussian-mixtures_files_in_output_28_0)).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 以 Scikit-Learn 的`make_moons`函数生成的数据为例，介绍在[第47章](ch47.xhtml#section-0511-k-means)中。
- en: '[PRE12]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '![output 28 0](assets/output_28_0.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![output 28 0](assets/output_28_0.png)'
- en: Figure 48-9\. GMM applied to clusters with nonlinear boundaries
  id: totrans-61
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图48-9\. GMM应用于具有非线性边界的聚类
- en: If we try to fit this with a two-component GMM viewed as a clustering model,
    the results are not particularly useful (see [Figure 48-10](#fig_0512-gaussian-mixtures_files_in_output_30_0)).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们尝试用一个两组分的GMM作为聚类模型来拟合它，结果并不特别有用（见[图48-10](#fig_0512-gaussian-mixtures_files_in_output_30_0)）。
- en: '[PRE13]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '![output 30 0](assets/output_30_0.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![output 30 0](assets/output_30_0.png)'
- en: Figure 48-10\. Two-component GMM fit to nonlinear clusters
  id: totrans-65
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图48-10\. 对非线性聚类拟合的两组分GMM
- en: But if we instead use many more components and ignore the cluster labels, we
    find a fit that is much closer to the input data (see [Figure 48-11](#fig_0512-gaussian-mixtures_files_in_output_32_0)).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，如果我们使用更多组分并忽略聚类标签，我们会发现拟合结果更接近输入数据（见[图48-11](#fig_0512-gaussian-mixtures_files_in_output_32_0)）。
- en: '[PRE14]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '![output 32 0](assets/output_32_0.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![output 32 0](assets/output_32_0.png)'
- en: Figure 48-11\. Using many GMM clusters to model the distribution of points
  id: totrans-69
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图48-11\. 使用多个GMM组件来建模点分布
- en: Here the mixture of 16 Gaussian components serves not to find separated clusters
    of data, but rather to model the overall *distribution* of the input data. This
    is a generative model of the distribution, meaning that the GMM gives us the recipe
    to generate new random data distributed similarly to our input. For example, here
    are 400 new points drawn from this 16-component GMM fit to our original data (see
    [Figure 48-12](#fig_0512-gaussian-mixtures_files_in_output_34_0)).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的16个高斯分量的混合并不是为了找到数据的分离聚类，而是为了对输入数据的整体*分布*进行建模。这是一个生成模型，意味着GMM给了我们一个生成新随机数据的方法，其分布类似于我们的原始输入数据。例如，这里有400个新点从这个16组分的GMM拟合到我们的原始数据中绘制出来（见[图48-12](#fig_0512-gaussian-mixtures_files_in_output_34_0)）。
- en: '[PRE15]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '![output 34 0](assets/output_34_0.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![output 34 0](assets/output_34_0.png)'
- en: Figure 48-12\. New data drawn from the 16-component GMM
  id: totrans-73
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图48-12\. 从16组分GMM中绘制的新数据
- en: A GMM is convenient as a flexible means of modeling an arbitrary multidimensional
    distribution of data.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: GMM作为一种灵活的方法，方便地对数据的任意多维分布进行建模。
- en: The fact that a GMM is a generative model gives us a natural means of determining
    the optimal number of components for a given dataset. A generative model is inherently
    a probability distribution for the dataset, and so we can simply evaluate the
    *likelihood* of the data under the model, using cross-validation to avoid overfitting.
    Another means of correcting for overfitting is to adjust the model likelihoods
    using some analytic criterion such as the [Akaike information criterion (AIC)](https://oreil.ly/BmH9X)
    or the [Bayesian information criterion (BIC)](https://oreil.ly/Ewivh). Scikit-Learn’s
    `GaussianMixture` estimator actually includes built-in methods that compute both
    of these, so it is very easy to operate using this approach.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: GMM作为生成模型的事实给了我们一种自然的方法来确定给定数据集的最优组件数。生成模型本质上是数据集的概率分布，因此我们可以简单地在模型下评估数据的*似然性*，使用交叉验证来避免过度拟合。另一种校正过度拟合的方法是使用一些分析标准来调整模型的似然性，例如[阿卡奇信息准则（AIC）](https://oreil.ly/BmH9X)或[贝叶斯信息准则（BIC）](https://oreil.ly/Ewivh)。Scikit-Learn的`GaussianMixture`估计器实际上包含内置方法来计算这两者，因此使用这种方法非常容易。
- en: Let’s look at the AIC and BIC versus the number of GMM components for our moons
    dataset (see [Figure 48-13](#fig_0512-gaussian-mixtures_files_in_output_37_0)).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们的moons数据集的GMM组件数对应的AIC和BIC（见[图48-13](#fig_0512-gaussian-mixtures_files_in_output_37_0)）。
- en: '[PRE16]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '![output 37 0](assets/output_37_0.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![output 37 0](assets/output_37_0.png)'
- en: Figure 48-13\. Visualization of AIC and BIC for choosing the number of GMM components
  id: totrans-79
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图48-13\. AIC和BIC的可视化，用于选择GMM组件数
- en: 'The optimal number of clusters is the value that minimizes the AIC or BIC,
    depending on which approximation we wish to use. The AIC tells us that our choice
    of 16 components earlier was probably too many: around 8–12 components would have
    been a better choice. As is typical with this sort of problem, the BIC recommends
    a simpler model.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 最优的聚类数是能够最小化AIC或BIC的值，具体取决于我们希望使用哪种近似方法。AIC告诉我们，我们之前选择的16组分可能太多了：选择大约8-12组分可能更合适。对于这类问题，BIC通常推荐一个更简单的模型。
- en: 'Notice the important point: this choice of number of components measures how
    well a GMM works *as a density estimator*, not how well it works *as a clustering
    algorithm*. I’d encourage you to think of the GMM primarily as a density estimator,
    and use it for clustering only when warranted within simple datasets.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 注意重要的一点：组件数量的选择衡量的是GMM作为密度估计器的工作效果，而不是作为聚类算法的工作效果。我鼓励您主要将GMM视为密度估计器，并仅在简单数据集内合适时用它进行聚类。
- en: 'Example: GMMs for Generating New Data'
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 示例：使用GMM生成新数据
- en: We just saw a simple example of using a GMM as a generative model in order to
    create new samples from the distribution defined by the input data. Here we will
    run with this idea and generate *new handwritten digits* from the standard digits
    corpus that we have used before.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚看到了使用GMM作为生成模型的简单示例，以便从定义为输入数据分布的模型中创建新的样本。在这里，我们将继续这个想法，并从之前使用过的标准数字语料库中生成*新的手写数字*。
- en: 'To start with, let’s load the digits data using Scikit-Learn’s data tools:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们使用Scikit-Learn的数据工具加载数字数据：
- en: '[PRE17]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Next, let’s plot the first 50 of these to recall exactly what we’re looking
    at (see [Figure 48-14](#fig_0512-gaussian-mixtures_files_in_output_42_0)).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们绘制前50个样本，以确切回顾我们正在查看的内容（参见[图 48-14](#fig_0512-gaussian-mixtures_files_in_output_42_0)）。
- en: '[PRE18]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '![output 42 0](assets/output_42_0.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![output 42 0](assets/output_42_0.png)'
- en: Figure 48-14\. Handwritten digits input
  id: totrans-89
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 48-14\. 手写数字输入
- en: 'We have nearly 1,800 digits in 64 dimensions, and we can build a GMM on top
    of these to generate more. GMMs can have difficulty converging in such a high-dimensional
    space, so we will start with an invertible dimensionality reduction algorithm
    on the data. Here we will use a straightforward PCA, asking it to preserve 99%
    of the variance in the projected data:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有将近1,800个64维度的数字样本，我们可以在其上构建一个混合高斯模型（GMM）以生成更多数字。在这么高维度的空间中，GMM可能会有收敛困难，因此我们将从数据中开始使用一个可逆的降维算法。这里我们将使用简单的PCA，要求它在投影数据中保留99%的方差：
- en: '[PRE19]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The result is 41 dimensions, a reduction of nearly 1/3 with almost no information
    loss. Given this projected data, let’s use the AIC to get a gauge for the number
    of GMM components we should use (see [Figure 48-15](#fig_0512-gaussian-mixtures_files_in_output_46_0)).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是41个维度，几乎没有信息损失的减少了近1/3。鉴于这个投影数据，让我们使用AIC来确定我们应该使用多少个GMM组件（参见[图 48-15](#fig_0512-gaussian-mixtures_files_in_output_46_0)）。
- en: '[PRE20]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '![output 46 0](assets/output_46_0.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![output 46 0](assets/output_46_0.png)'
- en: Figure 48-15\. AIC curve for choosing the appropriate number of GMM components
  id: totrans-95
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 48-15\. 选择适当的GMM组件数量的AIC曲线
- en: 'It appears that around 140 components minimizes the AIC; we will use this model.
    Let’s quickly fit this to the data and confirm that it has converged:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来大约使用140个组件可以最小化AIC；我们将使用这个模型。让我们快速将其拟合到数据上并确认它已经收敛：
- en: '[PRE21]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Now we can draw samples of 100 new points within this 41-dimensional projected
    space, using the GMM as a generative model:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以在这个41维度的投影空间内绘制100个新点的样本，使用GMM作为生成模型：
- en: '[PRE22]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Finally, we can use the inverse transform of the PCA object to construct the
    new digits (see [Figure 48-16](#fig_0512-gaussian-mixtures_files_in_output_52_0)).
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以使用PCA对象的逆变换来构造新的数字（参见[图 48-16](#fig_0512-gaussian-mixtures_files_in_output_52_0)）。
- en: '[PRE23]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '![output 52 0](assets/output_52_0.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![output 52 0](assets/output_52_0.png)'
- en: Figure 48-16\. “New” digits randomly drawn from the underlying model of the
    GMM estimator
  id: totrans-103
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 48-16\. 从GMM估计器的基础模型中随机绘制的“新”数字
- en: The results for the most part look like plausible digits from the dataset!
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数结果看起来像数据集中合理的数字！
- en: 'Consider what we’ve done here: given a sampling of handwritten digits, we have
    modeled the distribution of that data in such a way that we can generate brand
    new samples of digits from the data: these are “handwritten digits,” which do
    not individually appear in the original dataset, but rather capture the general
    features of the input data as modeled by the mixture model. Such a generative
    model of digits can prove very useful as a component of a Bayesian generative
    classifier, as we shall see in the next chapter.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑我们在这里所做的：鉴于手写数字的抽样，我们已经模拟了该数据的分布，以便我们可以从数据中生成全新的样本：这些是“手写数字”，它们不会单独出现在原始数据集中，而是捕捉了混合模型建模的输入数据的一般特征。这样的手写数字的生成模型在贝叶斯生成分类器的组成部分中可以非常有用，这一点我们将在下一章看到。
- en: ^([1](ch48.xhtml#idm45858720757328-marker)) Code to produce this figure can
    be found in the [online appendix](https://oreil.ly/MLsk8).
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch48.xhtml#idm45858720757328-marker)) 生成此图的代码可以在[在线附录](https://oreil.ly/MLsk8)中找到。
