- en: 'Chapter 48\. In Depth: Gaussian Mixture Models'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The *k*-means clustering model explored in the previous chapter is simple and
    relatively easy to understand, but its simplicity leads to practical challenges
    in its application. In particular, the nonprobabilistic nature of *k*-means and
    its use of simple distance from cluster center to assign cluster membership leads
    to poor performance for many real-world situations. In this chapter we will take
    a look at Gaussian mixture models, which can be viewed as an extension of the
    ideas behind *k*-means, but can also be a powerful tool for estimation beyond
    simple clustering.
  prefs: []
  type: TYPE_NORMAL
- en: 'We begin with the standard imports:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Motivating Gaussian Mixtures: Weaknesses of k-Means'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s take a look at some of the weaknesses of *k*-means and think about how
    we might improve the cluster model. As we saw in the previous chapter, given simple,
    well-separated data, *k*-means finds suitable clustering results.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if we have simple blobs of data, the *k*-means algorithm can quickly
    label those clusters in a way that closely matches what we might do by eye (see
    [Figure 48-1](#fig_0512-gaussian-mixtures_files_in_output_5_0)).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![output 5 0](assets/output_5_0.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 48-1\. k-means labels for simple data
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'From an intuitive standpoint, we might expect that the clustering assignment
    for some points is more certain than others: for example, there appears to be
    a very slight overlap between the two middle clusters, such that we might not
    have complete confidence in the cluster assignment of points between them. Unfortunately,
    the *k*-means model has no intrinsic measure of probability or uncertainty of
    cluster assignments (although it may be possible to use a bootstrap approach to
    estimate this uncertainty). For this, we must think about generalizing the model.'
  prefs: []
  type: TYPE_NORMAL
- en: 'One way to think about the *k*-means model is that it places a circle (or,
    in higher dimensions, a hypersphere) at the center of each cluster, with a radius
    defined by the most distant point in the cluster. This radius acts as a hard cutoff
    for cluster assignment within the training set: any point outside this circle
    is not considered a member of the cluster. We can visualize this cluster model
    with the following function (see [Figure 48-2](#fig_0512-gaussian-mixtures_files_in_output_8_0)).'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '![output 8 0](assets/output_8_0.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 48-2\. The circular clusters implied by the k-means model
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'An important observation for *k*-means is that these cluster models *must be
    circular*: *k*-means has no built-in way of accounting for oblong or elliptical
    clusters. So, for example, if we take the same data and transform it, the cluster
    assignments end up becoming muddled, as you can see in [Figure 48-3](#fig_0512-gaussian-mixtures_files_in_output_10_0).'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![output 10 0](assets/output_10_0.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 48-3\. Poor performance of k-means for noncircular clusters
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'By eye, we recognize that these transformed clusters are noncircular, and thus
    circular clusters would be a poor fit. Nevertheless, *k*-means is not flexible
    enough to account for this, and tries to force-fit the data into four circular
    clusters. This results in a mixing of cluster assignments where the resulting
    circles overlap: see especially the bottom-right of this plot. One might imagine
    addressing this particular situation by preprocessing the data with PCA (see [Chapter 45](ch45.xhtml#section-0509-principal-component-analysis)),
    but in practice there is no guarantee that such a global operation will circularize
    the individual groups.'
  prefs: []
  type: TYPE_NORMAL
- en: These two disadvantages of *k*-means—its lack of flexibility in cluster shape
    and lack of probabilistic cluster assignment—mean that for many datasets (especially
    low-dimensional datasets) it may not perform as well as you might hope.
  prefs: []
  type: TYPE_NORMAL
- en: 'You might imagine addressing these weaknesses by generalizing the *k*-means
    model: for example, you could measure uncertainty in cluster assignment by comparing
    the distances of each point to *all* cluster centers, rather than focusing on
    just the closest. You might also imagine allowing the cluster boundaries to be
    ellipses rather than circles, so as to account for noncircular clusters. It turns
    out these are two essential components of a different type of clustering model,
    Gaussian mixture models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Generalizing E–M: Gaussian Mixture Models'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A Gaussian mixture model (GMM) attempts to find a mixture of multidimensional
    Gaussian probability distributions that best model any input dataset. In the simplest
    case, GMMs can be used for finding clusters in the same manner as *k*-means (see
    [Figure 48-4](#fig_0512-gaussian-mixtures_files_in_output_13_0)).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '![output 13 0](assets/output_13_0.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 48-4\. Gaussian mixture model labels for the data
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'But because a GMM contains a probabilistic model under the hood, it is also
    possible to find probabilistic cluster assignments—in Scikit-Learn this is done
    using the `predict_proba` method. This returns a matrix of size `[n_samples, n_clusters]`
    which measures the probability that any point belongs to the given cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We can visualize this uncertainty by, for example, making the size of each
    point proportional to the certainty of its prediction; looking at [Figure 48-5](#fig_0512-gaussian-mixtures_files_in_output_17_0),
    we can see that it is precisely the points at the boundaries between clusters
    that reflect this uncertainty of cluster assignment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '![output 17 0](assets/output_17_0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 48-5\. GMM probabilistic labels: probabilities are shown by the size
    of points'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Under the hood, a Gaussian mixture model is very similar to *k*-means: it uses
    an expectation–maximization approach, which qualitatively does the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Choose starting guesses for the location and shape.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Repeat until converged:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*E-step*: For each point, find weights encoding the probability of membership
    in each cluster.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '*M-step*: For each cluster, update its location, normalization, and shape based
    on *all* data points, making use of the weights.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: The result of this is that each cluster is associated not with a hard-edged
    sphere, but with a smooth Gaussian model. Just as in the *k*-means expectation–maximization
    approach, this algorithm can sometimes miss the globally optimal solution, and
    thus in practice multiple random initializations are used.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s create a function that will help us visualize the locations and shapes
    of the GMM clusters by drawing ellipses based on the GMM output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: With this in place, we can take a look at what the four-component GMM gives
    us for our initial data (see [Figure 48-6](#fig_0512-gaussian-mixtures_files_in_output_21_0)).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '![output 21 0](assets/output_21_0.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 48-6\. The four-component GMM in the presence of circular clusters
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Similarly, we can use the GMM approach to fit our stretched dataset; allowing
    for a full covariance the model will fit even very oblong, stretched-out clusters,
    as we can see in [Figure 48-7](#fig_0512-gaussian-mixtures_files_in_output_23_0).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '![output 23 0](assets/output_23_0.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 48-7\. The four-component GMM in the presence of noncircular clusters
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This makes clear that GMMs address the two main practical issues with *k*-means
    encountered before.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the Covariance Type
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you look at the details of the preceding fits, you will see that the `covariance_type`
    option was set differently within each. This hyperparameter controls the degrees
    of freedom in the shape of each cluster; it’s essential to set this carefully
    for any given problem. The default is `covariance_type="diag"`, which means that
    the size of the cluster along each dimension can be set independently, with the
    resulting ellipse constrained to align with the axes. `covariance_type="spherical"`
    is a slightly simpler and faster model, which constrains the shape of the cluster
    such that all dimensions are equal. The resulting clustering will have similar
    characteristics to that of *k*-means, though it’s not entirely equivalent. A more
    complicated and computationally expensive model (especially as the number of dimensions
    grows) is to use `covariance_type="full"`, which allows each cluster to be modeled
    as an ellipse with arbitrary orientation. [Figure 48-8](#fig_images_in_0512-covariance-type)
    represents these three choices for a single cluster.
  prefs: []
  type: TYPE_NORMAL
- en: '![05.12 covariance type](assets/05.12-covariance-type.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 48-8\. Visualization of GMM covariance types^([1](ch48.xhtml#idm45858720757328))
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Gaussian Mixture Models as Density Estimation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Though the GMM is often categorized as a clustering algorithm, fundamentally
    it is an algorithm for *density estimation*. That is to say, the result of a GMM
    fit to some data is technically not a clustering model, but a generative probabilistic
    model describing the distribution of the data.
  prefs: []
  type: TYPE_NORMAL
- en: As an example, consider some data generated from Scikit-Learn’s `make_moons`
    function, introduced in [Chapter 47](ch47.xhtml#section-0511-k-means) (see [Figure 48-9](#fig_0512-gaussian-mixtures_files_in_output_28_0)).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '![output 28 0](assets/output_28_0.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 48-9\. GMM applied to clusters with nonlinear boundaries
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If we try to fit this with a two-component GMM viewed as a clustering model,
    the results are not particularly useful (see [Figure 48-10](#fig_0512-gaussian-mixtures_files_in_output_30_0)).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '![output 30 0](assets/output_30_0.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 48-10\. Two-component GMM fit to nonlinear clusters
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: But if we instead use many more components and ignore the cluster labels, we
    find a fit that is much closer to the input data (see [Figure 48-11](#fig_0512-gaussian-mixtures_files_in_output_32_0)).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '![output 32 0](assets/output_32_0.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 48-11\. Using many GMM clusters to model the distribution of points
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Here the mixture of 16 Gaussian components serves not to find separated clusters
    of data, but rather to model the overall *distribution* of the input data. This
    is a generative model of the distribution, meaning that the GMM gives us the recipe
    to generate new random data distributed similarly to our input. For example, here
    are 400 new points drawn from this 16-component GMM fit to our original data (see
    [Figure 48-12](#fig_0512-gaussian-mixtures_files_in_output_34_0)).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '![output 34 0](assets/output_34_0.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 48-12\. New data drawn from the 16-component GMM
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: A GMM is convenient as a flexible means of modeling an arbitrary multidimensional
    distribution of data.
  prefs: []
  type: TYPE_NORMAL
- en: The fact that a GMM is a generative model gives us a natural means of determining
    the optimal number of components for a given dataset. A generative model is inherently
    a probability distribution for the dataset, and so we can simply evaluate the
    *likelihood* of the data under the model, using cross-validation to avoid overfitting.
    Another means of correcting for overfitting is to adjust the model likelihoods
    using some analytic criterion such as the [Akaike information criterion (AIC)](https://oreil.ly/BmH9X)
    or the [Bayesian information criterion (BIC)](https://oreil.ly/Ewivh). Scikit-Learn’s
    `GaussianMixture` estimator actually includes built-in methods that compute both
    of these, so it is very easy to operate using this approach.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at the AIC and BIC versus the number of GMM components for our moons
    dataset (see [Figure 48-13](#fig_0512-gaussian-mixtures_files_in_output_37_0)).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '![output 37 0](assets/output_37_0.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 48-13\. Visualization of AIC and BIC for choosing the number of GMM components
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The optimal number of clusters is the value that minimizes the AIC or BIC,
    depending on which approximation we wish to use. The AIC tells us that our choice
    of 16 components earlier was probably too many: around 8–12 components would have
    been a better choice. As is typical with this sort of problem, the BIC recommends
    a simpler model.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Notice the important point: this choice of number of components measures how
    well a GMM works *as a density estimator*, not how well it works *as a clustering
    algorithm*. I’d encourage you to think of the GMM primarily as a density estimator,
    and use it for clustering only when warranted within simple datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: GMMs for Generating New Data'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We just saw a simple example of using a GMM as a generative model in order to
    create new samples from the distribution defined by the input data. Here we will
    run with this idea and generate *new handwritten digits* from the standard digits
    corpus that we have used before.
  prefs: []
  type: TYPE_NORMAL
- en: 'To start with, let’s load the digits data using Scikit-Learn’s data tools:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Next, let’s plot the first 50 of these to recall exactly what we’re looking
    at (see [Figure 48-14](#fig_0512-gaussian-mixtures_files_in_output_42_0)).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '![output 42 0](assets/output_42_0.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 48-14\. Handwritten digits input
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'We have nearly 1,800 digits in 64 dimensions, and we can build a GMM on top
    of these to generate more. GMMs can have difficulty converging in such a high-dimensional
    space, so we will start with an invertible dimensionality reduction algorithm
    on the data. Here we will use a straightforward PCA, asking it to preserve 99%
    of the variance in the projected data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The result is 41 dimensions, a reduction of nearly 1/3 with almost no information
    loss. Given this projected data, let’s use the AIC to get a gauge for the number
    of GMM components we should use (see [Figure 48-15](#fig_0512-gaussian-mixtures_files_in_output_46_0)).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '![output 46 0](assets/output_46_0.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 48-15\. AIC curve for choosing the appropriate number of GMM components
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'It appears that around 140 components minimizes the AIC; we will use this model.
    Let’s quickly fit this to the data and confirm that it has converged:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can draw samples of 100 new points within this 41-dimensional projected
    space, using the GMM as a generative model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Finally, we can use the inverse transform of the PCA object to construct the
    new digits (see [Figure 48-16](#fig_0512-gaussian-mixtures_files_in_output_52_0)).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '![output 52 0](assets/output_52_0.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 48-16\. “New” digits randomly drawn from the underlying model of the
    GMM estimator
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The results for the most part look like plausible digits from the dataset!
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider what we’ve done here: given a sampling of handwritten digits, we have
    modeled the distribution of that data in such a way that we can generate brand
    new samples of digits from the data: these are “handwritten digits,” which do
    not individually appear in the original dataset, but rather capture the general
    features of the input data as modeled by the mixture model. Such a generative
    model of digits can prove very useful as a component of a Bayesian generative
    classifier, as we shall see in the next chapter.'
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch48.xhtml#idm45858720757328-marker)) Code to produce this figure can
    be found in the [online appendix](https://oreil.ly/MLsk8).
  prefs: []
  type: TYPE_NORMAL
