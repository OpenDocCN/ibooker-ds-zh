- en: 9 An AI-powered trivia game
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Connecting your app to a Large Language Model (LLM)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Engineering LLM prompts to achieve your desired results
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Structured Outputs to obtain LLM responses in a custom parsable format
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Managing state in a sequential Streamlit app
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using st.data_editor to create editable tables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating software is significantly different from what it used to be just a
    few short years ago. The difference stems from major developments in the field
    of AI (Artificial Intelligence), which—unless you've been living under a rock—you've
    probably heard orf.
  prefs: []
  type: TYPE_NORMAL
- en: I'm talking, of course, about breakthroughs in LLMs or Large Language Models,
    and the tremendously exciting possibilities they open up. By processing and generating
    natural language, LLMs can understand context, answer complex questions, and even
    write software on their own—all with astonishing fluency. Tasks that once required
    domain-specific expertise or painstaking programming can now be achieved with
    just a few well-crafted "prompts".
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we'll dive into how you can harness the power of LLMs in your
    own applications, relying on AI prompts and responses to implement product features
    that would have required highly advanced techniques half a decade ago. Along the
    way, we'll also discuss how to tune your LLM interactions to get the results you
    want, and how to do so without burning a hole in your pocket.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The GitHub repo for this book is [https://github.com/aneevdavis/streamlit-in-action](https://github.com/aneevdavis/streamlit-in-action).
    The chapter_09 folder has this chapter's code and a requirements.txt file with
    exact versions of all the required Python libraries.
  prefs: []
  type: TYPE_NORMAL
- en: '9.1 Fact Frenzy: An AI trivia game'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you watched the game show *Jeopardy!* as a kid—or as an adult for that matter—you're
    going to love this chapter. Having grown up outside the US, I was in my thirties
    before I watched my first episode of the show, but I suffered no dearth of trivia
    shows to obsess over when I was a boy—*Mastermind*, *Bournvita Quiz Contest*,
    and *Kaun Banega Crorepati?* (an Indian take on the original British show *Who
    Wants to Be a Millionaire?*) were all staples of my youth.
  prefs: []
  type: TYPE_NORMAL
- en: Fifteen-year-old me would never forgive adult me if I didn't include at least
    one trivia app in this book. Fortunately, trivia is a great fit for our first
    AI-powered Streamlit app, which will generate questions, evaluate answers, and
    even mix it up with a variety of quizmaster styles, all using artificial intelligence.
  prefs: []
  type: TYPE_NORMAL
- en: 9.1.1 Stating the concept and requirements
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Once again, the first step we'll take is to state the concept of the app we
    want to build.
  prefs: []
  type: TYPE_NORMAL
- en: Concept
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Fact Frenzy, a trivia game that asks users a set of trivia questions and evaluates
    their answers using AI
  prefs: []
  type: TYPE_NORMAL
- en: As we've done several times before by this point in the book, the requirements
    will flesh out this simple idea further.
  prefs: []
  type: TYPE_NORMAL
- en: Requirements
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Fact Frenzy will:'
  prefs: []
  type: TYPE_NORMAL
- en: use an AI model to generate trivia questions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ask these questions to a player
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: allow the user to enter a free-text response to each question
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: use AI to evaluate whether the answer is correct, and to provide the correct
    answer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: keep track of a player's score
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: allow the player to set a difficulty level for questions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: offer a variety of quizmaster speaking styles
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While we *generally* want our requirements to be free of "implementation" language
    (as discussed in Chapter 3), in this case the entire point of our app is to demonstrate
    the use of AI—so we definitely need it to use AI models to perform its functions.
  prefs: []
  type: TYPE_NORMAL
- en: We've also added a fun element in the form of quizmaster speaking styles—or
    in other words, mimicking the style of various people while asking questions,
    something we'd only be able to achieve using AI.
  prefs: []
  type: TYPE_NORMAL
- en: What's out of scope
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'What are we leaving out? While a professional trivia game could be arbitrarily
    complex, in building Fact Frenzy, we want to create a minimal app to get hands-on
    practice with topics we haven''t encountered before. So we won''t focus on any
    of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: persistent storage and retrieval of questions, answers, and scores
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: creating and managing users
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: letting users choose particular categories of questions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Placing the above items out of scope will let us concentrate on things like
    interacting with large language models or LLMs, state management, and of course,
    new Streamlit elements.
  prefs: []
  type: TYPE_NORMAL
- en: 9.1.2 Visualizing the user experience
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To give ourselves a concrete idea of what Fact Frenzy is about, take a look
    at a sketch of our proposed UI, displayed in figure 9.1.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../Images/09__image001.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.1 A UI sketch of Fact Frenzy
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The game window shown in the sketch has a two-column layout. The left column
    has a "New game" button, as well as a couple of settings: Quizmaster and Difficulty.'
  prefs: []
  type: TYPE_NORMAL
- en: Referring to the last requirement identified in the previous section, the Quizmaster
    setting is supposed to use AI to mimic speaking styles of various characters.
    In figure 9.1, the selected value is "Gollum", a character from *The Lord of the
    Rings* who speaks in a distinctive, hissing manner using phrases like "my precious."
  prefs: []
  type: TYPE_NORMAL
- en: The column on the right shows an AI-generated question "spoken" in Gollum's
    voice along with the player's score and a box to enter the answer.
  prefs: []
  type: TYPE_NORMAL
- en: Gimmicks aside, it's a pretty standard question-and-answer trivia game that
    lets the player enter free-form text to answer.
  prefs: []
  type: TYPE_NORMAL
- en: 9.1.3 Brainstorming the implementation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: While we'll "outsource" large parts of our logic to an LLM, we'll still need
    to own the overall game flow. Figure 9.2 shows the design we'll work to implement
    in the rest of the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike some of the other apps we've written where users could take a variety
    of actions at any point, Fact Frenzy is quite linear. As the diagram shows, the
    basic logic runs in a loop—using an LLM to retrieve a trivia question, posing
    it to the player, getting the LLM to evaluate the answer, stating whether the
    provided answer was right or not, and doing it all over again for the next question,
    until the game is done.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../Images/09__image002.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.2 The flow of logic in our AI-based trivia game
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Later on, we'll add a few bells and whistles such as allowing the player to
    set the difficulty level and quizmaster, but figure 9.2 is a pretty good representation
    of the core flow.
  prefs: []
  type: TYPE_NORMAL
- en: 9.2 Using AI to generate trivia questions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At the heart of Fact Frenzy is an AI model that powers the trivia experience.
    This model generates questions, evaluates player responses, and even adds personality
    to the quizmaster. To achieve all this, we use a Large Language Model (LLM)—a
    powerful AI system designed to process and generate human-like text.
  prefs: []
  type: TYPE_NORMAL
- en: LLMs are trained on vast amounts of textual data, making them incredibly versatile.
    They can perform a wide range of tasks from answering factual questions to generating
    poetry, coding, or—importantly for our purposes—role-playing as a quizmaster.
    Popular examples of LLMs include OpenAI's GPT series, Anthropic's Claude, and
    Google's Gemini—all of which leverage cutting-edge machine learning techniques
    to generate coherent, contextually appropriate text.
  prefs: []
  type: TYPE_NORMAL
- en: 9.2.1 Why use an LLM in Fact Frenzy?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'What makes LLMs suitable for use in our trivia game? To answer this, let''s
    take a minute to consider some possible components of such a game and how an LLM
    can support building each one:'
  prefs: []
  type: TYPE_NORMAL
- en: A giant list of questions
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Without an LLM, we would need to maintain a set of trivia questions large and
    diverse enough to keep our app engaging. Since the major LLMs of today are all
    trained on text and data pertaining to history, culture, geography, astronomy,
    and every other category of trivia you can think of, they're instead able to generate
    questions on the fly.
  prefs: []
  type: TYPE_NORMAL
- en: The ability to evaluate answers
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: If we were going the traditional non-LLM route, we would realistically only
    be able to ask multiple-choice questions, as we'd need to match the answer a player
    gives to the actual one accurately. LLMs, on the other hand, can interpret and
    respond to free-text user inputs. This allows us to ask open-ended trivia questions
    and still evaluate player responses correctly.
  prefs: []
  type: TYPE_NORMAL
- en: Entertainment value
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In addition to being able to handle factual information, LLMs can also be creative,
    and provide humor for engagement. Later in the chapter, we'll ask one to mimic
    the style of various characters as quizmasters, giving the game a personality,
    so to speak.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, in Fact Frenzy, the LLM will play the triple role of question generator,
    answer evaluator, and comic relief provider. In the next section, we'll set up
    an account with a top-tier LLM provider—OpenAI—enabling us to start interacting
    with LLMs for the first time.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: While LLMs are undeniably powerful, they are not omniscient. They rely on patterns
    in their training data to generate responses and may occasionally make mistakes,
    especially when evaluating highly nuanced or ambiguous answers. However, for our
    trivia app, these models strike a perfect balance between intelligence, versatility,
    and entertainment.
  prefs: []
  type: TYPE_NORMAL
- en: 9.2.2 Setting up an OpenAI API key
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For this chapter, we'll use an LLM provided by OpenAI, perhaps the best-known
    among the set of new-ish AI firms that have been dominating tech news lately.
  prefs: []
  type: TYPE_NORMAL
- en: As we've done many times in this book so far, we'll need to open an account
    with an external service, and wire our Python code to it.
  prefs: []
  type: TYPE_NORMAL
- en: If you don't already have an OpenAI account (your ChatGPT account counts), go
    to [https://platform.openai.com/](https://platform.openai.com/) and sign up for
    one.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: If you created your OpenAI account recently or just now, you *may* have some
    free credits applied to your account. However, the free tier currently comes with
    relatively severe usage limits, such as only being able to call some models three
    times per minute. While you could *technically* get away with using the free tier
    for this chapter (assuming your account has the free credits in the first place),
    if you plan to do any amount of serious AI development, I recommend upgrading
    to a paid tier. As you'll see, you can get a lot of LLM usage for fairly low prices.
    Though you'll likely need to *buy* $5 in usage credits to get to the lowest paid
    tier, you won't need to *spend* much of that in this chapter. For reference, I
    spent less than 15 cents in credits for all the testing I did while developing
    this lesson. You can learn more about cost optimization in the sidebar named "Cost
    considerations".
  prefs: []
  type: TYPE_NORMAL
- en: Once you're in, go to the Settings page—at the time of writing, you can do this
    by clicking the gear icon at the top right corner of the page—and then click on
    "API keys" in the panel to the left.
  prefs: []
  type: TYPE_NORMAL
- en: Create a new secret key and note it down. You can leave all the defaults unchanged
    (see figure 9.3).
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../Images/09__image003.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.3 Creating a secret key in your OpenAI account
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: You'll only be able to see your key once, so copy-and-paste it somewhere. This
    goes without saying, but keep it safe!
  prefs: []
  type: TYPE_NORMAL
- en: 9.2.3 Calling the OpenAI API in Python
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Before we start any development on the app, let's make sure we can call the
    OpenAI API in Python without any issues.
  prefs: []
  type: TYPE_NORMAL
- en: To begin, install the OpenAI's Python library with `pip install openai`. You
    could technically call the API via HTTP calls using the `requests` module (as
    we did in Chapter 5), but this library is more convenient to use.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once that''s done, open a Python shell and import the `OpenAI` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The `OpenAI` class lets you instantiate a client using an API key to make calls
    to the OpenAI API:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Replace `sk-proj-...` with the actual API key you copied in the previous step.
    With a `client` object created, let''s prepare an instruction to send the LLM:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Each request you send to the LLM is called a *prompt*. A prompt—or at least
    the type we''ll use here—consists of *messages*. In the code above, we''re assembling
    these into a list. Each message takes the form of a dictionary with two keys:
    `role` and `content`.'
  prefs: []
  type: TYPE_NORMAL
- en: '`role` can be one of `user`, `system`, and `assistant`. We''ll explore more
    examples later, but the value of `role` signifies the perspective of the speaker
    in the conversation:'
  prefs: []
  type: TYPE_NORMAL
- en: '`system` represents instructions or context-setting for the model, such as
    rules for how it should behave.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`user` represents the person interacting with the model (you).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`assistant` represents the model''s responses—we''ll discuss this in the next
    chapter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The prompt we''re creating here tells the LLM (in the system message) that
    it should behave like a helpful programming assistant. The actual instruction
    we want the LLM to respond to is in the user message: `Explain what Streamlit
    is in 10 words or fewer`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now make an actual request to the API:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The OpenAI API has several different endpoints—one for turning audio to text,
    one for creating images, and so on. The one we'll be using is the *chat completions*
    endpoint, and it's used for text generation.
  prefs: []
  type: TYPE_NORMAL
- en: Given a list of messages in a conversation, this endpoint is supposed to return
    what comes next—hence the term *completion*. OpenAI has a plethora of models we
    could use, but here we've picked `gpt-4o-mini`, which provides a good balance
    between intelligence, speed, and cost.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: While gpt-4o-mini is currently the most suitable model OpenAI offers for our
    use case, given the speed of developments in the AI space, by the time this book
    goes to print, we may have newer models that are smarter *and* cheaper. Keep an
    eye on OpenAI's pricing page at [https://openai.com/api/pricing/](https://openai.com/api/pricing/)
    to ensure you're using the model that fits best.
  prefs: []
  type: TYPE_NORMAL
- en: 'The above statement should take a few seconds to execute, and will return a
    `ChatCompletion` object. If you like, you can inspect this object by typing just
    `completion` into the shell, but you can access the actual text response we want
    like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: I couldn't have said it better myself! That concludes our first programmatic
    interaction with an LLM. Next, let's build this into our trivia game's code.
  prefs: []
  type: TYPE_NORMAL
- en: 9.2.4 Writing an LLM class
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In Chapter 8, we created a `Database` class that encapsulated the interaction
    that our app could have with an external database. We'll follow the same model
    in this chapter with an `Llm` class that handles all communication with the external
    LLM. This allows us to separate the logistics of talking to the LLM from the rest
    of our app, making it easier to maintain, test, or even swap it out entirely,
    without touching the remaining code.
  prefs: []
  type: TYPE_NORMAL
- en: We've covered the basics of calling the LLM in the prior section, so all that's
    left is to put the logic in a class.
  prefs: []
  type: TYPE_NORMAL
- en: Create a new file, `llm.py` with the content shown in listing 9.1.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.1 llm.py
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: (`chapter_09/in_progress_01/llm.py` in the GitHub repo)
  prefs: []
  type: TYPE_NORMAL
- en: The `Llm` class' `__init__` simply creates a new OpenAI client object using
    an API key that's passed to it, assigning this to `self.client`.
  prefs: []
  type: TYPE_NORMAL
- en: The `ask` method is what logic outside the `Llm` class will interact with, and
    returns the LLM's response to our prompt. Its code is essentially the same as
    what we ran in the Python shell earlier, except that we take in `user_msg` and
    `sys_msg` as arguments and put the creation of the `messages` list in its own
    method, called `construct_messages`.
  prefs: []
  type: TYPE_NORMAL
- en: Since we don't *have* to pass a system role message—the LLM will try to be helpful
    anyway—we give `sys_msg` a default value of None. `construct_messages` takes this
    fact into account while generating the `messages` list. Since this is a utility
    function that doesn't depend on anything else in the object, we make it a static
    method by decorating it with `@staticmethod`.
  prefs: []
  type: TYPE_NORMAL
- en: We'll refine the `Llm` class further along in the chapter, but for now, let's
    move on to writing the code that calls it.
  prefs: []
  type: TYPE_NORMAL
- en: 9.2.5 The Game class
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As in Chapter 8, we'll have a single class—appropriately named `Game`—that contains
    all the backend logic that our app's frontend will call directly. This is somewhat
    analogous to the `Hub` class from Chapter 8, though we'll structure `Game` differently.
  prefs: []
  type: TYPE_NORMAL
- en: For the moment we'll keep it quite simple, as all it needs to do is to pass
    a prompt to our `Llm` class. The initial version of the `Game` class that we'll
    place in `game.py` is shown in listing 9.2.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.2 game.py
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: (`chapter_09/in_progress_01/game.py` in the GitHub repo)
  prefs: []
  type: TYPE_NORMAL
- en: The initialization of a `Game` instance (through `__init__`) involves creating
    an `Llm` object by passing it the API key that we'll presumably get from the calling
    code.
  prefs: []
  type: TYPE_NORMAL
- en: The `ask_llm_for_question` method passes a simple prompt asking the LLM to generate
    a trivia question. Notice that the system message now tells the LLM to behave
    like a quizmaster.
  prefs: []
  type: TYPE_NORMAL
- en: The user message instructs the LLM to ask a question, warning it to not provide
    any choices or reveal the answer.
  prefs: []
  type: TYPE_NORMAL
- en: 9.2.6 Calling the Game class in our app
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We can now write a minimal version of our frontend code to test out everything
    we've done. As usual, our API key needs to be kept secret and safe, so we'll put
    it in a `secrets.toml` file in a new `.streamlit` directory, as seen in listing
    9.3.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.3 .streamlit/secrets.toml
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '#A Replace sk-proj-... with your actual API key.'
  prefs: []
  type: TYPE_NORMAL
- en: We've kept `secrets.toml` quite simple this time around with a non-nested structure—notice
    the absence of a section like `[config]`. The O in TOML does stand for "obvious"
    after all.
  prefs: []
  type: TYPE_NORMAL
- en: Go ahead and create `main.py` (shown in listing 9.4) now.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.4 main.py
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: (`chapter_09/in_progress_01/main.py` in the GitHub repo)
  prefs: []
  type: TYPE_NORMAL
- en: There's nothing fancy here yet; we simply create a `Game` instance called `game`,
    call its `ask_llm_for_question` method to generate a trivia question, and write
    it to the screen.
  prefs: []
  type: TYPE_NORMAL
- en: 'Notice how we''ve combined `st.container` with a border and `st.write` into
    a single statement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Concise and pretty, much like Streamlit itself. Run your app using `streamlit
    run main.py` (as in prior chapters, make sure you first `cd` into the directory
    that contains `main.py` so Streamlit can find the `.streamlit` directory) to see
    something like figure 9.4.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../Images/09__image004.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.4 The app can now successfully call the OpenAI API to obtain a trivia
    question (see chapter_09/in_progress_01 in the GitHub repo for the full code)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Excellent—Our AI quizmaster can now ask the player a question! Next we'll have
    it evaluate the player's answer.
  prefs: []
  type: TYPE_NORMAL
- en: 9.3 Using AI to evaluate answers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The prompt we fed GPT-4o mini is a simple one. Vitally, we didn't have to do
    anything particularly complicated with the output—just display it on the screen
    as-is. As such, we didn't really care about the *format* in which the LLM responded.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, evaluating answers presents a slightly different challenge. When a
    player enters an answer into the app, we need the LLM to tell us two things:'
  prefs: []
  type: TYPE_NORMAL
- en: Whether the answer is correct
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If not, what the correct answer actually is
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If you''ve interacted with AI assistants before, this sounds well within their
    capabilities. But let''s examine a practical challenge: how do you reliably parse
    the LLM''s response?'
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, let''s say we tell the LLM "Hey, you asked this question: <question>,
    and the player said the answer was <answer>. Tell me if that''s right, or if not,
    what the correct answer is".'
  prefs: []
  type: TYPE_NORMAL
- en: 'The LLM does its thing and responds with:'
  prefs: []
  type: TYPE_NORMAL
- en: '"Incorrect, the answer is actually <correct answer>".'
  prefs: []
  type: TYPE_NORMAL
- en: What do we do with this reply? Sure, we could display it on the screen, but
    we probably also need to perform additional actions, like deciding whether or
    not to increment the player's score. Which means that we need to *parse* the response
    to understand whether the answer was right.
  prefs: []
  type: TYPE_NORMAL
- en: How do we do that? A naive approach would be to look for the word "incorrect"
    in the response and mark the answer as being correct or not accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: But what if the LLM's answer is actually "Nope, that's not right, the answer
    is <correct answer>" or even "Hah! It *would* have been correct if they'd said
    <correct answer>. But they didn't, so tough luck."
  prefs: []
  type: TYPE_NORMAL
- en: The point is that there are tons of creative ways the LLM could answer, and
    while we would be able to understand them as humans, we need a simple way to determine
    their meaning in a machine-friendly way.
  prefs: []
  type: TYPE_NORMAL
- en: We could request the LLM in our prompt to include the words "correct" or "incorrect"
    somewhere, but it might still occasionally mess it up. Luckily, there's a better
    approach.
  prefs: []
  type: TYPE_NORMAL
- en: 9.3.1 Structured Outputs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Being able to parse LLM outputs reliably is a natural concern for developers,
    so OpenAI has a solution for this called *Structured Outputs*.
  prefs: []
  type: TYPE_NORMAL
- en: Structured Outputs is a feature that ensures that the model will generate a
    response that adheres to a schema that *you* provide, making it simple to parse
    programmatically.
  prefs: []
  type: TYPE_NORMAL
- en: 'For our use case, this means we can request the LLM to provide two structured
    fields in its response: a boolean field that says whether the provided answer
    is correct, and the actual correct answer.'
  prefs: []
  type: TYPE_NORMAL
- en: Let's create this schema as a class named `AnswerEvaluation` (listing 9.5).
    We'll need the third-party `pydantic` module to get this working, so install that
    first with `pip install pydantic`.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.5 answer_evaluation.py
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: (`chapter_09/in_progress_02/answer_evaluation.py` in the GitHub repo)
  prefs: []
  type: TYPE_NORMAL
- en: '`pydantic` is a data validation library that uses type hints to ensure that
    data conforms to a specified type.'
  prefs: []
  type: TYPE_NORMAL
- en: '`BaseModel`, which we import from `pydantic`, is a class that allows you to
    define a schema and perform data validation. It works well with OpenAI''s Structured
    Outputs.'
  prefs: []
  type: TYPE_NORMAL
- en: The class we're defining, `AnswerEvaluation`, is a *subclass* of `BaseModel`.
    *Subclasses* and *superclasses* are related to the concept of *inheritance* in
    object-oriented programming. Explaining inheritance in detail is beyond the scope
    of this book, but just know that a subclass can *inherit* functionality and attributes
    from its superclass, allowing you to reuse code and build on existing functionality
    without starting from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: In this case, `AnswerEvaluation` (subclass) inherits the features of `BaseModel`
    (superclass), such as data validation, serialization, and type checking, making
    it easy to define and work with structured data.
  prefs: []
  type: TYPE_NORMAL
- en: The body of `AnswerEvaluation` is identical to what you might expect if it were
    a dataclass instead. Indeed, dataclasses are similar to `pydantic`'s `BaseModel`
    except that dataclasses do not provide the complex validations and related functionality
    that `BaseModel` does.
  prefs: []
  type: TYPE_NORMAL
- en: 'Fortunately, we don''t really need to worry about the internals of how this
    works—just note that `AnswerEvaluation` has the two fields we spoke of: a boolean
    `is_correct` and a string, `correct_answer`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let''s modify our `ask` function in the `Llm` class (`llm.py`) to support
    Structured Outputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: (`chapter_09/in_progress_02/llm.py` in the GitHub repo)
  prefs: []
  type: TYPE_NORMAL
- en: Here we've added an argument called `schema` that's `None` by default. If a
    value is provided (`if schema`), we call a different method in our OpenAI client
    (`beta.chat.completions.parse` as opposed to `chat.completions.create` from earlier).
  prefs: []
  type: TYPE_NORMAL
- en: 'The first two parameters we pass to this new method are the same as before,
    but we''ve added a third one: `response_format`, to which we provide the value
    of `schema`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The final value we return is also different: `completion.choices[0].message.parsed`
    rather than `completion.choices[0].message.content`.'
  prefs: []
  type: TYPE_NORMAL
- en: If `schema` is not provided, we simply default to the earlier behavior, thereby
    ensuring that the `ask` method can handle both Structured Outputs and regular
    text.
  prefs: []
  type: TYPE_NORMAL
- en: The value we need to pass to `schema` is a *class—*not an instance of the class,
    but the class *itself*. The value *returned* will then be an *instance* of that
    class, and will therefore follow the schema. As you may already have guessed,
    for our use case, we'll pass the `AnswerEvaluation` class to `schema`.
  prefs: []
  type: TYPE_NORMAL
- en: We'll create this calling code in a bit, but first let's create a new LLM prompt
    asking the model to evaluate a player's answer.
  prefs: []
  type: TYPE_NORMAL
- en: During development, you should expect to have to tweak your prompt many times
    to get better results—in fact, there's a whole field called *prompt engineering*
    that has sprung up in recent years.
  prefs: []
  type: TYPE_NORMAL
- en: Since our prompts are meaningfully different from our code, let's put them in
    a different file where we can edit them without touching the rest of the code.
    We'll name this `prompts.py` and give it the contents in listing 9.6.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.6 prompts.py
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: (`chapter_09/in_progress_02/prompts.py` in the GitHub repo)
  prefs: []
  type: TYPE_NORMAL
- en: Each prompt is structured as a dictionary with keys `system` and `user`, corresponding
    to the system and user messages.
  prefs: []
  type: TYPE_NORMAL
- en: '`QUESTION_PROMPT` is our prompt from earlier, while `ANSWER_PROMPT` is new.
    Notice that its user message (a Python *multi-line string* bounded by `''''''`s
    if you''re wondering about the syntax) contains these lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: We're treating `{question}` and `{answer}` here as variables that we can replace
    with real values when we send the prompt to the LLM later.
  prefs: []
  type: TYPE_NORMAL
- en: Also note the last two lines in this message where we're telling the LLM to
    evaluate the answer for correctness and *also* to provide the correct answer.
    The model is smart enough to interpret this instruction and provide the results
    in our `AnswerEvaluation` schema.
  prefs: []
  type: TYPE_NORMAL
- en: 'Speaking of which, let''s actually write the code that passes this schema along
    with the prompt to fulfill our answer evaluation use case. We''ll do this by modifying
    `game.py`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: (`chapter_09/in_progress_02/game.py` in the GitHub repo)
  prefs: []
  type: TYPE_NORMAL
- en: We've refactored the `ask_llms_for_question` method to use our new `prompts.py`
    module.
  prefs: []
  type: TYPE_NORMAL
- en: But the main change here is the new function, `ask_llm_to_evaluate_answer`,
    which takes in the originally asked `question`, and the `answer` provided by the
    user, plugging these values into the `{question}` and `{answer}` slots in the
    user message we discussed a minute ago.
  prefs: []
  type: TYPE_NORMAL
- en: This time, we pass in `AnswerEvaluation` (imported from `answer_evaluation.py`)
    as a third argument to `self.llm`'s ask method—`schema`, as I hope you recall.
    One interesting aspect to this is that we're passing in `AnswerEvaluation` *itself*
    here—not an *instance* of `AnswerEvaluation`, but the class. In Python, most of
    the constructs in your code are *objects* that you can pass around like this,
    including classes—something that enables powerful and flexible programing patterns.
  prefs: []
  type: TYPE_NORMAL
- en: 'But I digress. Let''s get back to enabling our game to accept and evaluate
    answers from a player. The last step is to make the requisite changes to the frontend
    in `main.py`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: (`chapter_09/in_progress_02/main.py` in the GitHub repo)
  prefs: []
  type: TYPE_NORMAL
- en: You should be able to understand what we're doing here pretty easily. Once we've
    posed the trivia question to the player, we display a text input for their answer,
    as well as a "Submit" button that when clicked, triggers the `ask_llm_to_evaluate_answer`
    method in `game.py`.
  prefs: []
  type: TYPE_NORMAL
- en: The resulting value—stored in `evaluation`—is an instance of the `AnswerEvaluation`
    class. We use its `is_correct` attribute to display the appropriate correct/incorrect
    message, and `evaluation.correct_answer` for the real answer.
  prefs: []
  type: TYPE_NORMAL
- en: Try re-running your app now and supplying an answer to the trivia question.
    Figure 9.5 shows what I got.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../Images/09__image005.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.5 An issue with session state causes our answer to be matched with
    the wrong question (see chapter_09/in_progress_02 in the GitHub repo for the full
    code)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Oops! There are shenanigans afoot! The question I answered "Ottawa" to in the
    left part of figure 9.5 was "What is the capital of Canada?". But when I press
    the "Submit" button, the app displays a different question—"What is the largest
    planet in our solar system?"—and then has the nerve to tell me that "Ottawa" is,
    in fact, *not* the right answer to that.
  prefs: []
  type: TYPE_NORMAL
- en: You'll notice a similar bait-and-switch in your testing. What's going on? Are
    our AI overlords toying with us?
  prefs: []
  type: TYPE_NORMAL
- en: As it turns out, the LLM is entirely innocent of this alleged mischief. The
    issue—as we've seen several times before in this book—is related to session state
    and Streamlit app re-runs. In this particular instance, a click on the "Submit"
    button triggers a re-run from the *top*, meaning that before the code under `if
    st.button("Submit"):` can run, `game.ask_llm_for_question()` is called once again,
    resulting in a *new* question, which is what is passed as the first argument to
    `ask_llm_to_evaluate_answer`, along with "Ottawa" as the second argument pulled
    from the text input.
  prefs: []
  type: TYPE_NORMAL
- en: Well, at the very least, the Structured Outputs part of our code seems to be
    working, since Jupiter is indeed the largest planet in the solar system, and Ottawa
    is incorrect.
  prefs: []
  type: TYPE_NORMAL
- en: However, to get Fact Frenzy to exhibit the behavior we expect, we'll need to
    think through state management quite thoroughly in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: 9.4 Moving through game states
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In chapters prior, Streamlit's rerun-the-whole-app-each-time model meant that
    we had to make extensive use of `st.session_state` to get the app to remember
    values. That holds true here as well.
  prefs: []
  type: TYPE_NORMAL
- en: Fact Frenzy, however, is *sequential* in a way that our previous apps weren't.
    You can think of the desired behavior of our game as moving between various *states*,
    taking a different set of actions and displaying different widgets in each. Figure
    9.6 illustrates this.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../Images/09__image006.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.6 Our game consists of four sequential states
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The diagram identifies four states that the game can be in:'
  prefs: []
  type: TYPE_NORMAL
- en: GET_QUESTION, in which we retrieve a question from the LLM
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ASK_QUESTION, where we pose said question to the player
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: EVALUATE_ANSWER, which involves calling the LLM to evaluate the answer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: STATE_RESULT, where we tell the player whether they were right or not
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each of these actions should only happen while the game is in the state that
    it corresponds to. Additionally, we need to ensure that each LLM request we're
    making only happens once per question—because the alternative messes up our logic
    *and* costs money.
  prefs: []
  type: TYPE_NORMAL
- en: A pattern that we'll find useful in sequential apps such as this one and others
    you may write in the future is to formally retain a state/status attribute in
    the app's central class (`Game` in our case) coupled with methods to modify it,
    and to use conditional logic based on this attribute in the frontend to display
    the right elements on the screen. Figure 9.7 should make what I'm talking about
    clearer.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../Images/09__image007.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.7 Conditional branching in the frontend based on the game's state
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: As you can see from our proposed logic in figure 9.7, the `Game` class will
    have a `status` attribute that indicates the state the game is in. We'll read
    this state in the frontend to branch between the various display elements—whether
    it's a text input for the answer in the `ASK_QUESTION` state, or simply a wait-during-the-LLM-request
    status element during the `GET_QUESTION` and `EVALUATE_ANSWER` states.
  prefs: []
  type: TYPE_NORMAL
- en: The `Game` class also has methods that change its `status` attribute, controlling
    all of this. `obtain_question` would presumably get the question from the LLM,
    but also change the status to `ASK_QUESTION` when it's done. `accept_answer` would
    take in the answer from the player and also switch the status to `EVALUATE_ANSWER`,
    and the `evaluate_answer` method, in addition to getting the LLM to give us the
    result, would shift the status to `STATE_RESULT`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s put all this in action now, starting with the `Game` class:'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.7 game.py (modified)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: (`chapter_09/in_progress_03/game.py` in the GitHub repo)
  prefs: []
  type: TYPE_NORMAL
- en: As discussed, the first big change here is the inclusion of a `self.status`
    attribute that formally indicates the state of the game. We initialize this to
    `GET_QUESTION` as that's the first sequential state we want.
  prefs: []
  type: TYPE_NORMAL
- en: You'll notice we also have three other attributes—`curr_question`, `curr_answer`,
    and `curr_eval`—to hold the current question, answer, and evaluation within the
    `Game` instance. This is a departure from the earlier version of `game.py` where
    we were handling `question` and `answer` as variables outside the class. Keeping
    track of these within the class is better-suited to our new *stateful* approach.
  prefs: []
  type: TYPE_NORMAL
- en: You'll see this reflected in the `ask_llm_to_evaluate_answer` method, where
    we've dispensed with the `question` and `answer` parameters in favor of the `self.curr_question`
    and `self.curr_answer` attributes.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, we've introduced three new methods (also discussed earlier)—`obtain_question`,
    `accept_answer`, and `evaluate_answer`. `obtain_question` and `evaluate_answer`
    are wrappers around `ask_llm_for_question` and `ask_llm_to_evaluate_answer` respectively,
    each merely assigning the result to its associated attribute—`self.curr_question`
    or `self.curr_answer`—before adding a line to move `self.status` to its next value.
  prefs: []
  type: TYPE_NORMAL
- en: '`accept_answer` is even simpler; it just sets `self.answer` to an answer presumably
    provided by the player.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The second half of the set of changes we need to implement the state management
    approach we''ve envisioned lies in `main.py`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: (`chapter_09/in_progress_03/main.py` in the GitHub repo)
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by placing our `Game` instance in `st.session_state`, ensuring that
    we''ll be dealing with the same instance across re-runs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The last line here is for convenience, enabling us to refer to our `Game` instance
    simply as `game`, instead of spelling out `st.session_state.game` each time.
  prefs: []
  type: TYPE_NORMAL
- en: 'The remaining code builds out conditional branching based on `game`''s `status`
    attribute. Let''s briefly consider each such condition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The first branch deals with the `GET_QUESTION` state. The app handles this state
    without any interaction from the user, as it merely obtains the question from
    the LLM. This can take a perceptible amount of time, however, so we display what's
    known as a *status element*.
  prefs: []
  type: TYPE_NORMAL
- en: A status element is simply a widget that gives some indication of what's going
    on to the user while a long-running operation happens in the background. Streamlit
    has several different status elements—`st.spinner`, `st.status`, `st.toast`, `st.progress`—each
    of which has slightly different characteristics.
  prefs: []
  type: TYPE_NORMAL
- en: '`st.spinner`, which we''ve used here, simply displays a spinning circle animation
    (the same one we saw in Chapter 6 when we applied the `show_spinner` parameter
    to `@st.cache_data`) until the background operation has been completed.'
  prefs: []
  type: TYPE_NORMAL
- en: Notice the `st.rerun()` after we've called `game.obtain_question()`. This exists
    because once `obtain_question` (from `game.py`) has changed the status to `ASK_QUESTION`,
    we need the code to run again, so as to enter the *next* conditional branch given
    by `elif game.status == 'ASK_QUESTION':`.
  prefs: []
  type: TYPE_NORMAL
- en: The remaining branches are quite similar. In each case, there is some kind of
    trigger causing the app to move to the next state, followed by a rerun. In the
    `ASK_QUESTION` state, a click on "Submit" calls `game.accept_answer(answer)`,
    which sets `game`'s `curr_answer` attribute and changes the state to `EVALUATE_ANSWER`.
  prefs: []
  type: TYPE_NORMAL
- en: In `EVALUATE_ANSWER`, we call `game.evaluate_answer()` and display another `st.spinner`
    while we wait for it to return, eventually changing the status to `STATE_RESULT`.
  prefs: []
  type: TYPE_NORMAL
- en: After the rerun, we simply display the appropriate messages based on `game.curr_eval`,
    our `AnswerEvaluation` object.
  prefs: []
  type: TYPE_NORMAL
- en: Check out the results now by rerunning the app (figure 9.8)
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../Images/09__image008.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.8 The question-answer mismatch issue has been resolved (see chapter_09/in_progress_03
    in the GitHub repo for the full code)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: This time you'll see that the app matches the question and answer correctly,
    fixing the issue we saw earlier.
  prefs: []
  type: TYPE_NORMAL
- en: '9.5 Game mechanics: Keeping score, a New Game button, and Game Over'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Fact Frenzy currently does the bare minimum we need it to do: it asks a player
    a question, and evaluates the answer, all using AI. It isn''t much of a *game*
    though; there''s no score, and no concept of when the game starts and ends. Let''s
    tackle each of these problems in turn.'
  prefs: []
  type: TYPE_NORMAL
- en: Keeping score
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We want Fact Frenzy to be easy to pick up, so we'll keep our scoring mechanism
    basic; each correct answer fetches one point.
  prefs: []
  type: TYPE_NORMAL
- en: 'This should be fairly trivial to incorporate into the `Game` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: (`chapter_09/in_progress_04/game.py` in the GitHub repo)
  prefs: []
  type: TYPE_NORMAL
- en: We simply add a new `score` attribute to the `Game` instance in `__init__`,
    setting it to 0 to start with.
  prefs: []
  type: TYPE_NORMAL
- en: Later, in the `evaluate_answer` method, we increment this score by 1 if the
    LLM determines that the answer is correct.
  prefs: []
  type: TYPE_NORMAL
- en: Game Over
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Our app doesn''t yet have any concept of when the game is over, so let''s define
    this. Again, we''ll keep the logic simple: we''ll ask a predefined number of questions,
    and the game is done when all of them have been asked and answered.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This involves modifying the `Game` class again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: (`chapter_09/in_progress_04/game.py` in the GitHub repo)
  prefs: []
  type: TYPE_NORMAL
- en: 'Again, this should be pretty easy to follow. We add two more attributes to
    the instance in `__init__`: `num_question_completed` and `max_questions` (set
    to 1 for now since we don''t actually support multiple questions yet—that''s coming
    up in the next section).'
  prefs: []
  type: TYPE_NORMAL
- en: We increment `num_questions_completed` by 1 in `evaluate_answer`, and add a
    new method called `is_over` that returns `True` if the number of questions completed
    matches or exceeds `self.max_questions`.
  prefs: []
  type: TYPE_NORMAL
- en: A New Game button
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: At the moment, Fact Frenzy jumps straight into asking the LLM for a question
    as soon as the page loads. A "New Game" button would let users trigger the start
    of a game or take any other action we may want to add later before we engage the
    LLM.
  prefs: []
  type: TYPE_NORMAL
- en: 'This will primarily affect our frontend code, so let''s update `main.py` thus:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: (`chapter_09/in_progress_04/main.py` in the GitHub repo)
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several changes to highlight here. Firstly, there are two new functions:
    `start_new_game` and `new_game_button`, which we''ll look into in a second.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Since it''s now possible for a game to not have started yet—before the "New
    Game" button is clicked, we allow for game to be `None` if it hasn''t been added
    to `st.session_state` yet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ve also changed the layout of the app to have two columns: a side column
    (`side_col`) and a main one (`main_col`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: This side column could have simply been an `st.sidebar`, but in a later section,
    it'll turn out we need this column to have a higher width than `st.sidebar` offers
    by default.
  prefs: []
  type: TYPE_NORMAL
- en: 'Anyway, `side_col` contains a header introducing Fact Frenzy, and a call to
    `new_game_button`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'In Chapter 8, we used the Material library to display icons. Here, we''ve given
    Fact Frenzy a lightning-bolt icon using a different approach: by pasting an emoji
    into our code. We''re able to do this because emoji are part of the *Unicode*
    standard, which defines a consistent way to represent text and symbols across
    different systems and platforms. Whenever you want to add an emoji, you can search
    for it on a website like `emojipedia.org` and copy it.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `new_game_button` function is defined thus:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: In essence, we're checking if the game is already in progress—`if game and not
    game.is_over()` determines that `game` doesn't have the value `None` and that
    its `is_over` method returns `False`—and displaying a different button according
    to the result.
  prefs: []
  type: TYPE_NORMAL
- en: We vary two characteristics of the button—its text and its type. The text says
    "Restart game" if the game is in progress, and "Start new game" if it's not.
  prefs: []
  type: TYPE_NORMAL
- en: How about the button's `type` parameter? We may have given it a value in previous
    chapters, but let's examine it more thoroughly now. The three values `type` can
    take are `primary`, `secondary`, and `tertiary`—each indicating how prominent
    the button should be. A button with type `primary` has a solid color (usually
    orange) with white text, a `secondary` button—the default if you don't specify
    a type—is white with solid-colored text, while a `tertiary` button is more subtle
    and appears as regular text without a border.
  prefs: []
  type: TYPE_NORMAL
- en: In UI design, it's a good practice to guide the user towards the "correct" or
    most likely action they might want to take at any given point—it makes for a more
    intuitive design. If the game hasn't started yet, the choice that makes the most
    sense is to click the "Start new game" button, so we give it a type of `primary`.
    If the game is in progress, the default action should be to answer the question,
    not to restart the game. Therefore, while we make that possibility available,
    we don't overly emphasize it.
  prefs: []
  type: TYPE_NORMAL
- en: 'These differences in the button are mostly cosmetic, however. In either case,
    a click issues a call to `start_new_game`, which has the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: As before, we create a `Game` instance and assign it to `st.session_state.game`.
    Since the presence of `game` in `st.session_state` changes what should be displayed
    on the screen, we also issue an `st.rerun()`.
  prefs: []
  type: TYPE_NORMAL
- en: By wrapping this logic in a function, we're preventing it from executing by
    default, instead requiring the New Game button to actually be clicked.
  prefs: []
  type: TYPE_NORMAL
- en: The main column of the game—`main_col`—is, of course, where the content is meant
    to be displayed. In this iteration of `main.py`, we've simply moved the widgets
    we had before into `main_col`. There are a few additions worth highlighting though.
  prefs: []
  type: TYPE_NORMAL
- en: 'If `game` is `None`—which means no game has started yet—we want the main column
    to be completely blank so the player''s attention is focused on `side_col`. This
    explains why the code under with `main_col` starts with `if game`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: We've also added a header that just says "Question." We'll update this later
    to show the question number.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we''ve added some logic to handle the case of the game being over
    under the `STATE_RESULT` state:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: This should be quite clear. We use the `is_over` method we defined earlier to
    check if the game is done, and show an appropriate message and the final score
    (`game.score`) if so.
  prefs: []
  type: TYPE_NORMAL
- en: That concludes another iteration of our code. Go ahead and re-run your app to
    get figure 9.9.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../Images/09__image009.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.9 Keeping score, a New Game button, and Game Over (see chapter_09/in_progress_04
    in the GitHub repo for the full code)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Sweet! Fact Frenzy is starting to look pretty slick. Adding support for multiple
    questions is next!
  prefs: []
  type: TYPE_NORMAL
- en: 9.6 Including multiple questions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous section, we introduced key game mechanics to our app—adding
    a score system and defining the game's start and end—to make it feel more like
    a real game.
  prefs: []
  type: TYPE_NORMAL
- en: Fact Frenzy still only asks one question though, so it's not much of one yet.
    It's time to change that. But before we do, let's explore an LLM-related challenge
    we'll face.
  prefs: []
  type: TYPE_NORMAL
- en: 9.6.1 Response variability, or lack thereof
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In many ways, an LLM is like a black box. Unlike a piece of "regular" code that
    tends to be deterministic—meaning that the same input always produces the same
    output—LLMs are *probabilistic*, which means you may get different responses for
    the same input (or similar inputs), based on a set of probabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Depending on what you're trying to achieve, this variability can be a good or
    a bad thing. For example, if you're trying to get an LLM to generate poetry, you
    may want a fairly high amount of creativity or variability in the response, whereas
    if you're evaluating a mathematical equation, you want less.
  prefs: []
  type: TYPE_NORMAL
- en: Vendors like OpenAI generally expose a few controls for this variability, making
    it easier to manage, but often we need to engineer the prompt to extract the behavior
    we want from the model.
  prefs: []
  type: TYPE_NORMAL
- en: For our use case of generating questions, we want relatively high variability.
    If you've played around with our current app for a while, you may have noticed
    that the questions we get from the LLM are often repeated. In my testing, for
    instance, the model had a particular fondness for asking about the only planet
    in the solar system that rotates on its side.
  prefs: []
  type: TYPE_NORMAL
- en: This won't work for us. For one thing, if a single game includes multiple questions,
    all of them *must* be unique. Secondly, even if a particular question isn't asked
    twice in the same game, we don't want it to repeat too often across *different*
    games either.
  prefs: []
  type: TYPE_NORMAL
- en: Let's take a look at a few ways in which we can control variability in the LLM's
    answer.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: One consequence of the fact that LLMs generate text based on probabilistic patterns
    (rather than an understanding of facts) is what we call "hallucinations"—instances
    where the model produces outputs that are plausible-sounding but factually incorrect
    or entirely fabricated. These hallucinations arise because LLMs rely on the statistical
    relationships in their training data, which can sometimes lead to confident but
    misleading responses. Strategies exist to reduce the likelihood of hallucinations,
    such as enabling LLMs to connect to external sources of information, but there's
    no way to guarantee that they won't occur at all. Dealing with hallucinations
    is outside the scope of this chapter-~-we'll tackle supplementing an LLM's knowledge
    base with our own sources in the next one. Just be aware that our app may occasionally
    produce a question or answer that isn't factual. Fortunately, based on my testing,
    these occurrences tend to be infrequent.
  prefs: []
  type: TYPE_NORMAL
- en: Varying temperature and top_p
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The prompts we send to a large language model and the response we get back from
    it both consist of *tokens*. A token may be a single word or it may be part of
    a word.
  prefs: []
  type: TYPE_NORMAL
- en: At its heart, an LLM constructs the response to a prompt step-by-step, or rather,
    token-by-token. In each step, it considers a wide range of possibilities for the
    next token to include in its response, assigning a *probability* of being picked
    (from high school math, a number between 0 and 1) to each token option.
  prefs: []
  type: TYPE_NORMAL
- en: These tokens form what's known as a *probability distribution*—think of it as
    a curve that represents the likelihood of each token being the next one, with
    the more likely tokens plotted at a higher place on the curve than the less likely
    ones.
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI offers two parameters—`temperature` and `top_p`—that can adjust the composition
    of this curve. Figure 9.10 illustrates the effect of varying these parameters
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../Images/09__image010.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.10 How temperature and top_p affect the creativity and predictability
    of an LLM's output
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '`temperature` can take values from 0 to 2, with a higher value serving to make
    the curve flatter, and a lower value making it more pronounced. A higher temperature
    thus tends to "even" out the curve, increasing the probability that some of the
    less likely token options may be picked, which makes the LLM take more "risks"
    and increases the overall creativity of its response.'
  prefs: []
  type: TYPE_NORMAL
- en: '`top_p` can go from 0 to 1\. It represents a cutoff for the cumulative probability
    of the tokens that the model will choose from. To take an example, the LLM may
    determine that the five most likely next tokens in its response and their respective
    probabilities are: "but": 0.6, "then": 0.2, "a": 0.1, "this": 0.06, and "that":
    0.02—with all other tokens having much lower probabilities.'
  prefs: []
  type: TYPE_NORMAL
- en: A `top_p` of 0.8 would mean that the model should only choose between the most
    likely tokens with a combined probability of at least 0.8\. In this case, since
    "but" and "then" together cover a probability of 0.6 + 0.2 >= 0.8, the model discards
    everything else.
  prefs: []
  type: TYPE_NORMAL
- en: A `top_p` of 0.95, on the other hand, would require the model to also consider
    "a" and "this" to cover the required cumulative probability (0.6 + 0.2 + 0.1 +
    0.06 >= 0.95).
  prefs: []
  type: TYPE_NORMAL
- en: A higher `top_p`, therefore, means that the LLM will consider more token options,
    potentially reducing the predictability and coherence of the response, but increasing
    its diversity.
  prefs: []
  type: TYPE_NORMAL
- en: Getting back to our original goal of generating a variety of questions, we probably
    want a moderately high `temperature`—say 0.7 to 0.8—and a relatively high `top_p`
    of, say, 0.9.
  prefs: []
  type: TYPE_NORMAL
- en: Including previous questions in the prompt
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As discussed, while it would be ideal for questions to not repeat across *different*
    games, we must practically *guarantee* that the same question isn't asked twice
    in a *single* game.
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, this is easily achieved—by explicitly telling the LLM which questions
    have been asked so far in the game so it knows to steer clear of those.
  prefs: []
  type: TYPE_NORMAL
- en: For reinforcement, we could even tell the LLM to make sure to never ask the
    same question twice.
  prefs: []
  type: TYPE_NORMAL
- en: Injecting randomness
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Another way to get a wider variety of questions back is to inject some structured
    randomness into the prompt. You may have heard of the word game "Mad Libs" where
    players are provided a story with various parts of speech replaced with blanks.
    Each player then fills in a blank with a word of their choice, with the completed
    story often being hilarious.
  prefs: []
  type: TYPE_NORMAL
- en: We could do something similar here. We could change our prompt to something
    like "Generate a unique trivia question in the category ______ and a topic ______
    within that category. The question should reference a person or thing whose name
    starts with the letter ______".
  prefs: []
  type: TYPE_NORMAL
- en: Within our code, we could then maintain lists of categories, topics, and letters,
    randomly picking one from each list to fill in the blanks before sending the prompt
    to the LLM. If we have say, 10 categories, 10 topics within each category, we
    would have 26 (letters in the alphabet) x 10 x 10 = 2600 unique combinations,
    in addition to the variability that the LLM itself provides.
  prefs: []
  type: TYPE_NORMAL
- en: Or to save us the trouble of maintaining these lists, why not ask the LLM to
    pick a category and topic first? Interestingly, doing this appears to increase
    the diversity of the responses generated.
  prefs: []
  type: TYPE_NORMAL
- en: Yet another way to inject randomness that seems to help is to explicitly provide
    a random *seed* in your prompt. In programming, a random seed is a value (generally
    an integer) that initializes a random number generator. While it's not very clear
    that adding one to the text of your prompt actually causes the LLM to generate
    a random number, in my testing, doing so did increase the variability of responses.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: It's important to note here that modifying your prompt to get the results you
    want is not pure science; often you'll need to experiment with various techniques
    and prompts to identify the approach that works best. You may also see surprising
    results—for instance, AI researchers have found asking an LLM to think through
    its approach to solving a problem step-by-step often improves how well it performs
    the task.
  prefs: []
  type: TYPE_NORMAL
- en: 9.6.2 Implementing multiple questions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that we've reviewed how variability works in LLMs and possible approaches
    to ensure we get different questions each time, let's modify Fact Frenzy so it
    asks the user multiple questions in a game.
  prefs: []
  type: TYPE_NORMAL
- en: Modifying the LLM prompt
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Let's first make the requisite changes to our prompts to put into practice what
    we've learned.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we do that—our `Llm` class doesn''t currently offer a way to change
    the `temperature` and `top_p`, so we should modify its code (in `llm.py`) thus:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: (`chapter_09/in_progress_05/llm.py` in the GitHub repo)
  prefs: []
  type: TYPE_NORMAL
- en: As you can see above, we've refactored the `ask` method in the `Llm` class a
    fair bit. First, it accepts `temperature` and `top_p` as new arguments, both defaulting
    to `None`.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of repeating the `model`, `messages`, `temperature`, and `top_p` arguments
    to the OpenAI client's `beta.chat.completions.parse` or `chat.completions.create`,
    we construct an `llm_args` dictionary that holds the right arguments and their
    values depending on whether each is provided.
  prefs: []
  type: TYPE_NORMAL
- en: 'We then use the `**` dictionary unpacking operator (that we encountered in
    Chapter 7) to pass the arguments to the OpenAI methods. Note that we can combine
    this with the normal way of passing arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Here we pass `reponse_format` in the regular way, but unpack `llm_args` for
    the remaining arguments.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, in `prompts.py`, edit our question-generation prompt so it now reads:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: (`chapter_09/in_progress_05/prompts.py` in the GitHub repo)
  prefs: []
  type: TYPE_NORMAL
- en: 'You''ll see that we''ve incorporated several of the techniques discussed in
    the last section:'
  prefs: []
  type: TYPE_NORMAL
- en: The system prompt requests the LLM to behave like a quizmaster who never asks
    the same question twice.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We ask the LLM to think of a unique category and a topic within it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We add a "seed" variable and ask the LLM to generate the question using that
    seed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At the end of the prompt, for references, we provide the questions that have
    already been asked, so the LLM can avoid those.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We need to accompany these changes with additional ones in the `Game` class.
    Besides the LLM stuff, to enable asking multiple questions in a game, we need
    to be able to repeat the movement from the first game state to the last many times.
    In effect, our state diagram now becomes a *cycle* as opposed to a line, as shown
    in figure 9.11.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../Images/09__image011.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.11 To implement multiple questions, we now cycle through the game states
    in a loop
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'This means we also need another method to move from the `STATE_RESULT` state
    back to `GET_QUESTION`. Let’s add this method along with the rest of the changes
    to `game.py` now:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: (`chapter_09/in_progress_05/game.py` in the GitHub repo)
  prefs: []
  type: TYPE_NORMAL
- en: 'You''ll see that we''ve added a new attribute, `self.questions`, within `__init__`,
    initializing it to an empty list. As you''ve likely guessed, this will hold all
    the questions we get from the LLM. We accomplish this through this addition in
    the `obtain_questions` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Additionally, since we'll finally have more than one question to ask, we've
    changed the value of `self.max_questions` to 5\. Feel free to change this to whatever
    number you like.
  prefs: []
  type: TYPE_NORMAL
- en: We've revamped the `ask_llm_for_question` method entirely, since our user message
    now has a couple of variables we need to provide values for. `{already_asked}`
    can simply be replaced by `self.questions` (with the individual list items separated
    by newlines).
  prefs: []
  type: TYPE_NORMAL
- en: 'For the random seed, we simply use the current timestamp converted to an integer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Since timestamps always go up by definition, the current timestamp is guaranteed
    to be something the LLM has never gotten before from us.
  prefs: []
  type: TYPE_NORMAL
- en: We also now pass `temperature` and `top_p` values to `self.llm.ask` in line
    with our exploration of these parameters.
  prefs: []
  type: TYPE_NORMAL
- en: To enable multiple questions, a newly added `proceed_to_next_question` sets
    the game's status back to `GET_QUESTION`, completing the state cycle in figure
    9.11.
  prefs: []
  type: TYPE_NORMAL
- en: 'The changes required to the frontend are relatively simple. Edit `main.py`
    thus:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: (`chapter_09/in_progress_05/main.py` in the GitHub repo)
  prefs: []
  type: TYPE_NORMAL
- en: 'Firstly, we''ve modified the header of the main column to provide the question
    number (`len(game.questions)`) and the total number of questions (`game.max_questions`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ve also added a subheader to display the score:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: To facilitate the state change from `STATE_RESULT` to `GET_QUESTION`, we've
    added an `else` clause that will—if the game isn't over—display a "Next question"
    button that triggers the `game` object's `proceed_to_next_question()` when clicked.
  prefs: []
  type: TYPE_NORMAL
- en: 'Notice the unfamiliar way in which we''ve written the `st.button` widget:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '`st.button`''s `on_click` parameter lets you specify a function to execute
    when the button is clicked. We *could* also have written this in the way we''ve
    done so far in this book, i.e. as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'The difference lies in when the triggered function is actually executed. Specifically:'
  prefs: []
  type: TYPE_NORMAL
- en: When we use the `if st.button` notation, the button click first triggers a re-run
    of the page, causing everything above the button to be re-rendered again before
    the code under the `if` executes—triggered by the fact that `st.button` evaluates
    to `True` in the re-run. After this code executes, we may need to manually trigger
    *another* re-run as shown above—and as we've been doing throughout this book,
    really—to see any changes in the page caused by it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With the `on_click` notation, the button click causes the function listed under
    `on_click` (called a *callback* by the way) to execute *before* the page is re-run
    and everything above the button is re-rendered. We don't need a manual `st.rerun()`
    in this case, because the re-run triggered by the button-click already takes into
    account the changes made by the callback since it has already executed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So why haven't we been using this method all along? Well, the `if st.button`
    structure tends to be a little easier to grasp for simple apps. Besides, callbacks
    have some restrictions—you can't trigger an app rerun within a callback, for instance.
  prefs: []
  type: TYPE_NORMAL
- en: In any case, you should be able to re-run your app at this point to try out
    a working multi-question game (figure 9.12).
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../Images/09__image012.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.12 A working multi-question trivia game (see chapter_09/in_progress_05
    in the GitHub repo for the full code)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Our trivia game is technically complete now, but let's see if we can make it
    more engaging in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Cost considerations
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: LLMs are an incredible general-purpose tool, but it is important to realize
    that using them in your app—especially in production where it may be accessed
    by hundreds of users—is not free. The last thing you want is to accidentally run
    up a huge bill.
  prefs: []
  type: TYPE_NORMAL
- en: '**How cost is calculated**'
  prefs: []
  type: TYPE_NORMAL
- en: When using an LLM, costs are typically calculated based on the number of *tokens*
    processed. As discussed in section 9.5.1, a "token" represents a chunk of text—a
    word or a part of a word—that the model reads (input) or generates (output).
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI charges people based on the sum of the input and output tokens processed.
    This means that the size of the input prompt and the size of the output text *both*
    contribute to the cost. The pricing also differs by model. At the time of writing,
    the model we've been using in this chapter—gpt-4o-mini—costs 15 cents for every
    1 million tokens processed.
  prefs: []
  type: TYPE_NORMAL
- en: You can use tools like [https://platform.openai.com/tokenizer](https://platform.openai.com/tokenizer)
    to count the tokens in a piece of text and determine costs.
  prefs: []
  type: TYPE_NORMAL
- en: '**Cost optimization strategies**'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several ways in which you could optimize cost while working with
    LLMs. Here are a few ideas:'
  prefs: []
  type: TYPE_NORMAL
- en: Keep your input prompts short and to-the-point to reduce costs associated with
    input tokens.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reduce the size of the output text, either by instructing the LLM to keep its
    response short, or by explicitly restricting the number of tokens processed to
    a maximum value (e.g. by passing a value to the `max_tokens` argument while calling
    the OpenAI endpoint).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Batch together multiple requests to reduce the total number of prompts sent
    to the LLM. For example, instead of providing a list of previously asked questions
    each time we need a new question—as we're doing here—we could simply ask the LLM
    to generate the total number of questions we want in one go.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use less capable but cheaper models for some of your prompts. In our case, we're
    using gpt-4o-mini, which strikes a pretty good balance of cost and intelligence,
    but depending on your application, it may be possible to use even cheaper models
    for less complex tasks. Familiarize yourself with OpenAI's pricing page.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Avoid LLM costs altogether by asking the user to provide their *own* LLM API
    key. For our game, this involves a major hit to user experience as it requires
    players to create an OpenAI account before they can play, but you're guaranteed
    to not have to pay a dime in LLM-related costs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 9.7 Adding AI personalities
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As an informative trivia game, Fact Frenzy works perfectly fine now. The end-to-end
    flow—from starting a new game to cycling through the questions and keeping score
    until the game ends—has been established.
  prefs: []
  type: TYPE_NORMAL
- en: However, it still lacks a certain *je ne sais quoi*—it's rather dry and mechanical.
    Wouldn't it be cool if we could give our game a personality? Fortunately, this
    is exactly the kind of thing that LLMs are great at. We could, for instance, ask
    GPT-4o to mimic the style of various characters while asking questions.
  prefs: []
  type: TYPE_NORMAL
- en: In fact, we could let players choose what character they want their quizmaster
    to take. Sound fun? Let's get to it!
  prefs: []
  type: TYPE_NORMAL
- en: 9.7.1 Adding game settings
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We don't currently have a page or place in the app where players can view or
    change any settings, so we'll tackle that first.
  prefs: []
  type: TYPE_NORMAL
- en: What options do we want the user to be able to set? We've already talked about
    the quizmaster speaking style, so that can be the first. We could also let the
    player pick a difficulty level that suits them.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several different designs we could use for a "settings editor", but
    I want to use this opportunity to introduce a handy Streamlit widget we haven''t
    encountered before: `st.data_editor`.'
  prefs: []
  type: TYPE_NORMAL
- en: st.data_editor
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In Chapters 6 and 7, we learned about Pandas dataframes, which make working
    with tabular data in Python easy. We discovered `st.dataframe`, used to render
    dataframes as a table in a Streamlit app for viewing.
  prefs: []
  type: TYPE_NORMAL
- en: '`st.data_editor` displays dataframes too, but also makes them *editable*, providing
    users with an experience that you might expect in a spreadsheet.'
  prefs: []
  type: TYPE_NORMAL
- en: What does this have to do with adding settings to our app? Well, we could place
    the settings we want in a dataframe, and enable people to edit the dataframe to
    modify a setting.
  prefs: []
  type: TYPE_NORMAL
- en: 'With the two settings we discussed a few paragraphs above, the settings dataframe
    might look like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'If this dataframe is stored in `default_settings`, we might write our `st.data_editor`
    widget like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: This would display the widget shown in figure 9.13.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../Images/09__image013.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.13 A simple output of st.data_editor
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The first argument here is the initial state of the data we want to edit—in
    this case, the default settings.
  prefs: []
  type: TYPE_NORMAL
- en: The `num_rows='fixed'` means that the data editor widget shouldn't allow users
    to add any new rows. This makes sense because we don't want to have multiple rows
    in the dataframe shown above—a single setting can only have one value.
  prefs: []
  type: TYPE_NORMAL
- en: In any run of the app that happens before the user interacts with the data editor,
    `settings` will hold the same value as `default_settings`. Once the user changes
    a setting—for example, they might change Difficulty to `Easy`—`settings` will
    hold the edited dataframe across future re-runs until the user edits it again.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The `key='settings_editor'` parameter adds a widget key to the session state
    for the data editor. While this isn't strictly required for the app to function
    correctly, it protects us from a certain quirk of Streamlit where it forgets the
    values of a widget without an explicit key between re-runs if that widget isn't
    rendered for some reason in a particular run. Adding a widget key doesn't cost
    us anything, so it's safer to provide one to avoid unforeseen bugs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Getting back to our example, we don''t necessarily want users to have to type
    in the name of the quizmaster or a difficulty level; we''d rather have them select
    from a list of options. `st.data_editor` supports this in the form of *column
    configurations*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Here we exert more granular control over the editable data, configuring each
    column in the data through `st.column_config`.
  prefs: []
  type: TYPE_NORMAL
- en: For each of our columns, we use a `SelectBoxColumn` which lets us specify a
    list of options to choose from, and whether a value must be specified (the `required`
    parameter, set to `True` above).
  prefs: []
  type: TYPE_NORMAL
- en: The results are shown in figure 9.14.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../Images/09__image014.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.14 st.data_editor with one of the columns showing a SelectBoxColumn
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '`st.column_config` supports many different column types besides `SelectBoxColumn`,
    such as a `CheckboxColumn` for boolean values, a `DatetimeColumn` that displays
    a date/time picker, and a `LinkColumn` for clickable URLs.'
  prefs: []
  type: TYPE_NORMAL
- en: It also supports non-editable types that can be used with `st.dataframe`, including
    `AreaChartColumn`, `BarChartColumn`, a `ListColumn` for lists, and even a `ProgressColumn`
    for numbers (displayed in a progress bar versus a target).
  prefs: []
  type: TYPE_NORMAL
- en: Creating the settings editor
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Now that we know how `st.data_editor` works, let's go ahead and create a settings
    editor UI. We'll put this in a new file called `settings.py` (shown in listing
    9.8).
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.8 settings.py
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: (`chapter_09/in_progress_06/settings.py` in the GitHub repo)
  prefs: []
  type: TYPE_NORMAL
- en: We place the options for the Quizmaster and Difficulty settings right at the
    top for easy access, listing them under `QM_OPTIONS` and `DIFFICULTY_OPTIONS`.
  prefs: []
  type: TYPE_NORMAL
- en: The Quizmaster options range from an actual quizmaster, the late Alex Trebek
    of *Jeopardy!* fame, to a range of fictional characters like Gollum from *The
    Lord Of The Rings*, and Gruk the Caveman, an entirely made-up figure to let the
    LLM go wild.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ve initialized `default_settings` thus:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Note how this *isn't* a dataframe as we suggested initially—it's a dictionary
    instead, with the name of each setting as a key, and a one-element list containing
    the default option for that setting as the corresponding value.
  prefs: []
  type: TYPE_NORMAL
- en: Interestingly, `st.data_editor` can display things that are not Pandas dataframes.
    This includes native Python types such as dictionaries, lists, and sets. The quality
    of being able to display these types even applies to `st.dataframe`, despite the
    name. In this case, it means we don't actually have to maintain the settings as
    a dataframe; we can use the more readable dictionary form above.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `settings_editor` function renders the actual settings editor UI. We place
    everything within yet another new Streamlit widget called `st.popover`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: '`st.popover` displays a popover widget, which is a small pop-up screen that
    you can trigger by clicking an associated button—in a similar manner to `st.expander`.
    The first argument is the label for the button that triggers the `st.popover`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The contents of the popover are written within the `with st.popover(...)` context
    manager. In this case, we''re displaying the `st.data_editor` widget and returning
    its value, i.e. the edited `settings` dictionary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: This is pretty much the same code that we wrote in the previous section while
    we were discussing `st.data_editor`, though with the addition of a `use_container_width=True`
    argument, which adjusts the width of the popover.
  prefs: []
  type: TYPE_NORMAL
- en: Applying the settings
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'How do we use these settings in Fact Frenzy? Both the Quizmaster and Difficulty
    settings relate to the question text generated by the LLM, so let''s incorporate
    them in the question prompt in `prompts.py`, which becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: (`chapter_09/in_progress_06/prompts.py` in the GitHub repo)
  prefs: []
  type: TYPE_NORMAL
- en: Obviously, there are plenty of ways to work our two new variables into the prompt—the
    above is just one.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll modify `game.py` next:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: (`chapter_09/in_progress_06/game.py` in the GitHub repo)
  prefs: []
  type: TYPE_NORMAL
- en: '`Game`''s `__init__` now accepts a `settings` parameter, which—as you''d expect—is
    in the dictionary format we used in `settings.py`. This is assigned to `self.settings`
    so other methods can access it.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ve added two associated methods: `get_setting` and `modify_settings`. `get_setting`
    deals with getting the value of a given setting, which is slightly tricky because
    each dictionary value is a single-element list (designed that way so it works
    with `st.data_editor`). `get_setting` abstracts away this somewhat unsightly logic
    so we restrict it to one place in the code.'
  prefs: []
  type: TYPE_NORMAL
- en: '`modify_settings` replaces `self.settings` with a given `new_settings` dictionary.
    This will come into play when the user changes a setting.'
  prefs: []
  type: TYPE_NORMAL
- en: Turning to the `ask_llm_for_question` method, we replace the `{quizmaster}`
    and `{difficulty}` variables we added to the prompt with their corresponding values
    from the settings, obtained through `get_setting`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The changes to `main.py` are all that remain now, so let''s make those:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: (`chapter_09/in_progress_06/main.py` in the GitHub repo)
  prefs: []
  type: TYPE_NORMAL
- en: In `start_new_game`, to obtain the initial `Game` instance, we now pass in `default_settings`,
    directly imported from `settings.py`.
  prefs: []
  type: TYPE_NORMAL
- en: The actual settings editor is displayed within the side column (`side_col`),
    right above the New Game button. The return value—recall that this would be the
    `default_settings` dictionary before the user changes the value of any settings,
    and the modified dictionary afterward—is stored in the `settings` variable.
  prefs: []
  type: TYPE_NORMAL
- en: And, finally, in every re-run—provided that we're in a game—we run `game.modify_settings(settings)`
    to pick up any changes the user has made to the settings.
  prefs: []
  type: TYPE_NORMAL
- en: That should do it. Run your app again to see figure 9.15\. Play around with
    the AI quizmaster options and difficulties; it's now more fun to read the questions!
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../Images/09__image015.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.15 A final version of Fact Frenzy with editable settings (see chapter_09/in_progress_06
    in the GitHub repo for the full code)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'That concludes our development of Fact Frenzy, the first—and only—game we''ll
    build in this book. In Chapter 10, we''ll continue our exploration of AI apps
    with a more practical application: a customer support chatbot.'
  prefs: []
  type: TYPE_NORMAL
- en: 9.8 Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A Large Language Model (LLM) is an AI system designed to process and generate
    human-like text.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLMs can perform both creative and analytical tasks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI, one of the most popular LLM providers, allows developers to access its
    GPT series through an API.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `openai` library provides a convenient way to call the OpenAI API in Python.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can pass a conversation to OpenAI API's chat completion endpoint with messages
    tagged as `"system"`, `"assistant"` or `"user"`, causing the model to complete
    the conversation in a logical way.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Structured Outputs is a feature provided by OpenAI that ensures the model will
    generate a response that adheres to a given schema.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A common pattern in linear Streamlit apps is to implement conditional branching
    logic based on a variable that's held in `st.session_state`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can vary parameters such as `temperature` and `top_p` to affect the creativity
    and predictability of responses generated by an LLM.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Injecting randomness into the prompt is a good way to ensure we get different
    responses to similar prompts.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It's important to optimize cost in LLM-based applications. You can do this by
    having the LLM process fewer input and output tokens, reducing the number of prompts,
    using cheaper models, or even having users bear the cost by requiring them to
    supply their own API key.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`st.data_editor` provides a way to create editable tables in Streamlit apps.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`st.column_config` lets you configure columns to be of certain editable and
    non-editable types in `st.data_editor` and `st.dataframe`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`st.popover` displays a small pop-up screen triggered by a click on an associated
    button.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
