<html><head></head><body>
<div id="sbo-rt-content"><section data-pdf-bookmark="Chapter 10. Image Similarity Detection with &#10;Deep Learning and PySpark LSH" data-type="chapter" epub:type="chapter"><div class="chapter" id="image_similarity_detection_with_deep_learning_and_pyspark_lsh">
<h1><span class="label">Chapter 10. </span>Image Similarity Detection with 
<span class="keep-together">Deep Learning</span> and PySpark LSH</h1>
<p>Whether you encounter them on social media<a data-primary="image similarity detection" data-secondary="about" data-type="indexterm" id="idm46507962540352"/><a data-primary="deep learning" data-secondary="about image similarity detection" data-type="indexterm" id="idm46507962539504"/><a data-primary="ImageNet dataset sparking deep learning" data-type="indexterm" id="idm46507962538624"/><a data-primary="deep learning" data-secondary="ImageNet dataset sparking" data-type="indexterm" id="idm46507962537984"/> or e-commerce stores, images are integral to our digital lives. In fact, it was an image dataset—ImageNet—which was a key component for sparking the current deep learning revolution. A remarkable performance by a classification model in the ImageNet 2012 challenge was an important milestone and led to widespread attention. It is no wonder then that you are likely to encounter image data at some point as a data science practitioner.</p>
<p>In this chapter, you will gain experience scaling a deep learning workflow for a visual task, namely, image similarity detection, with PySpark. The task of identifying images that are similar to each other comes intuitively to humans, but it is a complex computational task. At scale, it becomes even more difficult. In this chapter, we will introduce an approximate method for finding similar items called locality sensitive hashing, or LSH, and apply it to images. We’ll use deep learning to convert image data into a numerical vector representation. PySpark’s LSH algorithm will be applied to the resulting vectors, which will allow us to find similar images given a new input image.</p>
<p>On a high level, this example mirrors one of the approaches used by photo sharing apps such as Instagram and Pinterest for image similarity detection. This helps their users make sense of the deluge of visual data that exists on their platforms. This also depicts how a deep learning workflow can benefit from PySpark’s scalability.</p>
<p>We’ll start by briefly introducing PyTorch,<a data-primary="PyTorch" data-secondary="about" data-type="indexterm" id="idm46507962535728"/><a data-primary="deep learning" data-secondary="PyTorch framework" data-tertiary="about" data-type="indexterm" id="idm46507962534752"/><a data-primary="image similarity detection" data-secondary="PyTorch framework" data-tertiary="about" data-type="indexterm" id="idm46507962533536"/> a deep learning framework. It has gained prominence in recent years for its relatively easier learning curve compared to other major low-level deep learning libraries. Then we’ll download and prepare our dataset. The dataset being used for our task is the Cars dataset released in 2013 by Stanford AI Lab. PyTorch will be used for image preprocessing. This will be followed by conversion of our input image data into a vector representation (image embeddings). We’ll then import the resulting embeddings into PySpark and transform them using the LSH algorithm. We’ll finish up by taking a new image and performing a nearest neighbors search using our LSH-transformed dataset to find similar images.</p>
<p>Let’s start by introducing and setting up PyTorch.</p>
<section data-pdf-bookmark="PyTorch" data-type="sect1"><div class="sect1" id="idm46507962531712">
<h1>PyTorch</h1>
<p>PyTorch is a library for building <a data-primary="PyTorch" data-secondary="about" data-type="indexterm" id="idm46507962530384"/><a data-primary="deep learning" data-secondary="PyTorch framework" data-tertiary="about" data-type="indexterm" id="idm46507962529408"/><a data-primary="image similarity detection" data-secondary="PyTorch framework" data-tertiary="about" data-type="indexterm" id="idm46507962528192"/>deep learning projects. It emphasizes flexibility and allows deep learning models to be expressed in idiomatic Python. It found early adopters in the research community. Recently, it has grown into one of the most prominent deep learning tools across a broad range of applications due to its ease of use. Along with TensorFlow, it is the most popular library for deep learning as of now.</p>
<p>PyTorch’s simple and flexible interface enables fast experimentation. You can load data, apply transforms, and build models with a few lines of code. Then, you have the flexibility to write customized training, validation, and test loops and deploy trained models with ease. It is consistently being used in professional contexts for real-world, mission-critical work. <a data-primary="PyTorch" data-secondary="about" data-tertiary="GPU support" data-type="indexterm" id="idm46507962526400"/><a data-primary="deep learning" data-secondary="PyTorch framework" data-tertiary="about GPU support" data-type="indexterm" id="idm46507962525152"/><a data-primary="GPU support by PyTorch" data-type="indexterm" id="idm46507962523936"/><a data-primary="image similarity detection" data-secondary="PyTorch framework" data-tertiary="about GPU support" data-type="indexterm" id="idm46507962523264"/>Being able to use GPUs (graphical processing units) for training resource-intensive models has been a big factor for making deep learning popular. PyTorch provides great GPU support, although we won’t need that for our task.</p>
<section data-pdf-bookmark="Installation" data-type="sect2"><div class="sect2" id="idm46507962521776">
<h2>Installation</h2>
<p>On the <a href="https://oreil.ly/CHkJo">PyTorch website</a>, you can <a data-primary="PyTorch" data-secondary="installing" data-type="indexterm" id="idm46507962519584"/><a data-primary="installing PyTorch" data-type="indexterm" id="idm46507962518576"/><a data-primary="deep learning" data-secondary="PyTorch framework" data-tertiary="installing" data-type="indexterm" id="idm46507962517904"/><a data-primary="image similarity detection" data-secondary="PyTorch framework" data-tertiary="installing" data-type="indexterm" id="idm46507962516688"/>easily obtain the installation instructions based on your system configuration, as shown in <a data-type="xref" href="#pytorch_installation_cpu_support">Figure 10-1</a>.</p>
<figure><div class="figure" id="pytorch_installation_cpu_support">
<img alt="PyTorch installation CPU support" height="383" src="assets/aaps_1001.png" width="1063"/>
<h6><span class="label">Figure 10-1. </span>PyTorch installation, CPU support</h6>
</div></figure>
<p>Execute the provided command and follow the instructions for your configuration:</p>
<pre data-code-language="shell" data-type="programlisting">$ pip3 install torch torchvision</pre>
<p>We will not be relying on a GPU and, hence, will choose CPU as a compute platform. <a data-primary="GPU support by PyTorch" data-secondary="installing with GPU support" data-type="indexterm" id="idm46507962480032"/><a data-primary="PyTorch" data-secondary="installing" data-tertiary="GPU support" data-type="indexterm" id="idm46507962479184"/><a data-primary="installing PyTorch" data-secondary="GPU support" data-type="indexterm" id="idm46507962478096"/><a data-primary="image similarity detection" data-secondary="PyTorch framework" data-tertiary="installing GPU support" data-type="indexterm" id="idm46507962477248"/>If you have a GPU setup that you want to use, choose options accordingly to obtain the required instructions. We will not be needing Torchaudio for this chapter either, so we skip its installation.</p>
</div></section>
</div></section>
<section data-pdf-bookmark="Preparing the Data" data-type="sect1"><div class="sect1" id="idm46507962508896">
<h1>Preparing the Data</h1>
<p>We will be using the<a data-primary="image similarity detection" data-secondary="Stanford Cars dataset" data-type="indexterm" id="idm46507962507104"/><a data-primary="deep learning" data-secondary="Stanford Cars dataset" data-type="indexterm" id="idm46507962506128"/><a data-primary="PyTorch" data-secondary="Stanford Cars dataset" data-type="indexterm" id="idm46507962505184"/><a data-primary="datasets" data-secondary="Stanford Cars" data-type="indexterm" id="idm46507962504240"/><a data-primary="“3D Object Representations for Fine-Grained Categorization” (Krause, Stark, Deng, and Fei-Fei)" data-type="indexterm" id="idm46507962503296"/><a data-primary="Krause, Jonathan" data-type="indexterm" id="idm46507962502432"/><a data-primary="Stark, Michael" data-type="indexterm" id="idm46507962501872"/><a data-primary="Deng, Jia" data-type="indexterm" id="idm46507962501200"/><a data-primary="Fei-Fei, Li" data-type="indexterm" id="idm46507962500528"/> <a href="https://oreil.ly/gxo8Q">Stanford Cars dataset</a>. It was released as part of the ICCV 2013 paper “3D Object Representations for Fine-Grained Categorization” by Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei.</p>
<p>You can download the images from Kaggle or using the source link provided by Stanford AI Lab.</p>
<pre data-code-language="shell" data-type="programlisting">wget http://ai.stanford.edu/~jkrause/car196/car_ims.tgz</pre>
<p>Once it’s downloaded, unzip the train and test image directories and place them in a directory called <em>cars_data</em>:</p>
<pre data-code-language="shell" data-type="programlisting"><code class="nv">data_directory</code> <code class="o">=</code> <code class="s2">"cars_data"</code>
<code class="nv">train_images</code> <code class="o">=</code> <code class="s2">"cars_data/cars_train/cars_train"</code></pre>
<p>You can get a CSV file containing labels for the training dataset <a href="https://oreil.ly/UoHXh">here</a>. Download it, rename it to <em>cars_train_data.csv</em>, and place it in the data directory. Let’s have a look at it:</p>
<pre data-code-language="python" data-type="programlisting"><code class="kn">import</code> <code class="nn">pandas</code> <code class="k">as</code> <code class="nn">pd</code>

<code class="n">train_df</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">read_csv</code><code class="p">(</code><code class="n">data_directory</code><code class="o">+</code><code class="s2">"/cars_train_data.csv"</code><code class="p">)</code>

<code class="n">train_df</code><code class="o">.</code><code class="n">head</code><code class="p">()</code>
<code class="o">...</code>

    <code class="n">Unnamed</code><code class="p">:</code> <code class="mi">0</code> 	<code class="n">x1</code> 	<code class="n">y1</code> 	    <code class="n">x2</code> 	    <code class="n">y2</code> 	    <code class="n">Class</code> 	<code class="n">image</code>
<code class="mi">0</code> 	         <code class="mi">0</code> 	<code class="mi">39</code> 	<code class="mi">116</code> 	<code class="mi">569</code> 	<code class="mi">375</code> 	<code class="mi">14</code> 	    <code class="mf">00001.</code><code class="n">jpg</code>
<code class="mi">1</code> 	         <code class="mi">1</code> 	<code class="mi">36</code> 	<code class="mi">116</code> 	<code class="mi">868</code> 	<code class="mi">587</code> 	<code class="mi">3</code> 	    <code class="mf">00002.</code><code class="n">jpg</code>
<code class="mi">2</code> 	         <code class="mi">2</code> 	<code class="mi">85</code> 	<code class="mi">109</code> 	<code class="mi">601</code> 	<code class="mi">381</code> 	<code class="mi">91</code> 	    <code class="mf">00003.</code><code class="n">jpg</code>
<code class="mi">3</code> 	         <code class="mi">3</code> 	<code class="mi">621</code> 	<code class="mi">393</code>     <code class="mi">1484</code>    <code class="mi">1096</code>    <code class="mi">134</code> 	    <code class="mf">00004.</code><code class="n">jpg</code>
<code class="mi">4</code> 	         <code class="mi">4</code> 	<code class="mi">14</code> 	<code class="mi">36</code>      <code class="mi">133</code>     <code class="mi">99</code>      <code class="mi">106</code> 	    <code class="mf">00005.</code><code class="n">jpg</code></pre>
<p>Ignore all columns other than <code>Class</code> and <code>image</code>. The other columns are related to the original research project that this dataset was derived from and will not be used for our task.</p>
<section data-pdf-bookmark="Resizing Images Using PyTorch" data-type="sect2"><div class="sect2" id="idm46507962343920">
<h2>Resizing Images Using PyTorch</h2>
<p>Before we head further, we’ll need to preprocess our images. Preprocessing data is very common in machine learning since deep learning models (neural networks) expect the input to meet certain requirements.</p>
<p>We need to apply a series of preprocessing steps, called <em>transforms</em>, to convert input images into the proper format for the models. <a data-primary="ResNet-18 architecture" data-secondary="image format required" data-type="indexterm" id="idm46507962367504"/>In our case, we need them to be 224 x 224-pixel JPEG-formatted images, since that is a requirement for the ResNet-18 model that we’ll use in the next section. We perform this transformation using PyTorch’s Torchvision package in the following code:</p>
<pre data-code-language="python" data-type="programlisting"><code class="kn">import</code> <code class="nn">os</code>
<code class="kn">from</code> <code class="nn">PIL</code> <code class="kn">import</code> <code class="n">Image</code>
<code class="kn">from</code> <code class="nn">torchvision</code> <code class="kn">import</code> <code class="n">transforms</code>

<code class="c1"># needed input dimensions for the CNN</code>
<code class="n">input_dim</code> <code class="o">=</code> <code class="p">(</code><code class="mi">224</code><code class="p">,</code><code class="mi">224</code><code class="p">)</code>
<code class="n">input_dir_cnn</code> <code class="o">=</code> <code class="n">data_directory</code> <code class="o">+</code> <code class="s2">"/images/input_images_cnn"</code>

<code class="n">os</code><code class="o">.</code><code class="n">makedirs</code><code class="p">(</code><code class="n">input_dir_cnn</code><code class="p">,</code> <code class="n">exist_ok</code> <code class="o">=</code> <code class="kc">True</code><code class="p">)</code>

<code class="n">transformation_for_cnn_input</code> <code class="o">=</code> <code class="n">transforms</code><code class="o">.</code><code class="n">Compose</code><code class="p">([</code><code class="n">transforms</code><code class="o">.</code><code class="n">Resize</code><code class="p">(</code><code class="n">input_dim</code><code class="p">)])</code>

<code class="k">for</code> <code class="n">image_name</code> <code class="ow">in</code> <code class="n">os</code><code class="o">.</code><code class="n">listdir</code><code class="p">(</code><code class="n">train_images</code><code class="p">):</code>
    <code class="n">I</code> <code class="o">=</code> <code class="n">Image</code><code class="o">.</code><code class="n">open</code><code class="p">(</code><code class="n">os</code><code class="o">.</code><code class="n">path</code><code class="o">.</code><code class="n">join</code><code class="p">(</code><code class="n">train_images</code><code class="p">,</code> <code class="n">image_name</code><code class="p">))</code>
    <code class="n">newI</code> <code class="o">=</code> <code class="n">transformation_for_cnn_input</code><code class="p">(</code><code class="n">I</code><code class="p">)</code>

    <code class="n">newI</code><code class="o">.</code><code class="n">save</code><code class="p">(</code><code class="n">os</code><code class="o">.</code><code class="n">path</code><code class="o">.</code><code class="n">join</code><code class="p">(</code><code class="n">input_dir_cnn</code><code class="p">,</code> <code class="n">image_name</code><code class="p">))</code>

    <code class="n">newI</code><code class="o">.</code><code class="n">close</code><code class="p">()</code>
    <code class="n">I</code><code class="o">.</code><code class="n">close</code><code class="p">()</code></pre>
<p>Here we use a single transformation that resizes the image to fit within the neural networks. However, we can use the <code>Compose</code> transform to define a series of transforms used to preprocess our image too.</p>
<p>Our dataset is in place now. In the next section, we will convert our image data into a vector representation fit for use with PySpark’s LSH algorithm.</p>
</div></section>
</div></section>
<section data-pdf-bookmark="Deep Learning Model for Vector Representation of Images" data-type="sect1"><div class="sect1" id="idm46507962508304">
<h1>Deep Learning Model for Vector Representation of Images</h1>
<p>Convolutional neural networks, or CNNs, <a data-primary="image similarity detection" data-secondary="deep learning model" data-tertiary="about" data-type="indexterm" id="idm46507962154640"/><a data-primary="deep learning" data-secondary="model for vector representation of images" data-tertiary="about" data-type="indexterm" id="idm46507962153488"/><a data-primary="PyTorch" data-secondary="deep learning model" data-tertiary="about" data-type="indexterm" id="idm46507962152240"/><a data-primary="convolutional neural networks (CNNs)" data-type="indexterm" id="idm46507962151024"/><a data-primary="ResNet-18 architecture" data-type="indexterm" id="idm46507962150384"/>are the standard neural network architectures used for prediction when the input observations are images. We won’t be using them for any prediction task but rather for generating a vector representation of images. Specifically, we will use the ResNet-18 architecture.</p>
<p>Residual Network (ResNet) was introduced<a data-primary="“Deep Residual Learning for Image Recognition” (Ren, He, Sun, and Zhang)" data-primary-sortas="Deep Residual Learning" data-type="indexterm" id="idm46507962149200"/><a data-primary="Ren, Shaoqing" data-type="indexterm" id="idm46507962148256"/><a data-primary="He, Kaiming" data-type="indexterm" id="idm46507962147584"/><a data-primary="Sun, Jian" data-type="indexterm" id="idm46507962146912"/><a data-primary="Zhang, Xiangyu" data-type="indexterm" id="idm46507962146240"/> by Shaoqing Ren, Kaiming He, Jian Sun, and Xiangyu Zhang in their 2015 paper “Deep Residual Learning for Image Recognition.” The 18 in ResNet-18 stands for the number of layers that exist in the neural network architecture. Other popular variants of ResNet include 34 and 50 layers. A larger number of layers results in improved performance at the cost of increased computation.</p>
<section data-pdf-bookmark="Image Embeddings" data-type="sect2"><div class="sect2" id="idm46507962145312">
<h2>Image Embeddings</h2>
<p>An <em>image embedding</em> is a representation<a data-primary="image similarity detection" data-secondary="deep learning model" data-tertiary="image embeddings" data-type="indexterm" id="ch10-embed"/><a data-primary="image embedding" data-type="indexterm" id="ch10-embed2"/><a data-primary="deep learning" data-secondary="model for vector representation of images" data-tertiary="image embeddings" data-type="indexterm" id="ch10-embed3"/><a data-primary="PyTorch" data-secondary="deep learning model" data-tertiary="image embeddings" data-type="indexterm" id="ch10-embed4"/><a data-primary="vectors" data-secondary="image embedding" data-type="indexterm" id="ch10-embed5"/><a data-primary="ResNet-18 architecture" data-secondary="image embedding" data-type="indexterm" id="ch10-embed6"/> of an image in a vector space. The basic idea is that if a given image is close to another image, their embedding will also be similar and close in the spatial dimension.</p>
<p>The image in <a data-type="xref" href="#ILSVRC_2012">Figure 10-2</a>, <a href="https://oreil.ly/YRhhT">released by Andrej Karpathy</a>, shows<a data-primary="vectors" data-secondary="image embedding" data-tertiary="example image" data-type="indexterm" id="idm46507962132960"/><a data-primary="image embedding" data-secondary="example image" data-type="indexterm" id="idm46507962131712"/><a data-primary="Karpathy, Andrej" data-type="indexterm" id="idm46507962130768"/> how images can be represented in a lower dimensional space. As an example, you can notice vehicles near the top and birds in the bottom-left space.</p>
<figure><div class="figure" id="ILSVRC_2012">
<img alt="ILSVRC 2012 image embeddings in a 2-D space" height="1001" src="assets/aaps_1002.png" width="1001"/>
<h6><span class="label">Figure 10-2. </span>ILSVRC 2012 image embeddings in a 2-D space</h6>
</div></figure>
<p>We can obtain image embeddings from ResNet-18 by taking the output of its second-to-last, fully connected layer, which has a dimension of 512. Next, we create a class that, provided an image, can return its numeric vector form representation.</p>
<pre data-code-language="python" data-type="programlisting"><code class="kn">import</code><code> </code><code class="nn">torch</code><code>
</code><code class="kn">from</code><code> </code><code class="nn">torchvision</code><code> </code><code class="kn">import</code><code> </code><code class="n">models</code><code>
</code><code>
</code><code class="k">class</code><code> </code><code class="nc">Img2VecResnet18</code><code class="p">(</code><code class="p">)</code><code class="p">:</code><code>
</code><code>    </code><code class="k">def</code><code> </code><code class="fm">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">)</code><code class="p">:</code><code>
</code><code>        </code><code class="bp">self</code><code class="o">.</code><code class="n">device</code><code> </code><code class="o">=</code><code> </code><code class="n">torch</code><code class="o">.</code><code class="n">device</code><code class="p">(</code><code class="s2">"</code><code class="s2">cpu</code><code class="s2">"</code><code class="p">)</code><code>
</code><code>        </code><code class="bp">self</code><code class="o">.</code><code class="n">numberFeatures</code><code> </code><code class="o">=</code><code> </code><code class="mi">512</code><code>
</code><code>        </code><code class="bp">self</code><code class="o">.</code><code class="n">modelName</code><code> </code><code class="o">=</code><code> </code><code class="s2">"</code><code class="s2">resnet-18</code><code class="s2">"</code><code>
</code><code>        </code><code class="bp">self</code><code class="o">.</code><code class="n">model</code><code class="p">,</code><code> </code><code class="bp">self</code><code class="o">.</code><code class="n">featureLayer</code><code> </code><code class="o">=</code><code> </code><code class="bp">self</code><code class="o">.</code><code class="n">getFeatureLayer</code><code class="p">(</code><code class="p">)</code><code>
</code><code>        </code><code class="bp">self</code><code class="o">.</code><code class="n">model</code><code> </code><code class="o">=</code><code> </code><code class="bp">self</code><code class="o">.</code><code class="n">model</code><code class="o">.</code><code class="n">to</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">device</code><code class="p">)</code><code>
</code><code>        </code><code class="bp">self</code><code class="o">.</code><code class="n">model</code><code class="o">.</code><code class="n">eval</code><code class="p">(</code><code class="p">)</code><code>
</code><code>        </code><code class="bp">self</code><code class="o">.</code><code class="n">toTensor</code><code> </code><code class="o">=</code><code> </code><code class="n">transforms</code><code class="o">.</code><code class="n">ToTensor</code><code class="p">(</code><code class="p">)</code><code> </code><a class="co" href="#callout_image_similarity_detection_with___span_class__keep_together__deep_learning__span__and_pyspark_lsh_CO1-1" id="co_image_similarity_detection_with___span_class__keep_together__deep_learning__span__and_pyspark_lsh_CO1-1"><img alt="1" height="12" src="assets/1.png" width="12"/></a><code>
</code><code>        </code><code class="bp">self</code><code class="o">.</code><code class="n">normalize</code><code> </code><code class="o">=</code><code> </code><code class="n">transforms</code><code class="o">.</code><code class="n">Normalize</code><code class="p">(</code><code class="n">mean</code><code class="o">=</code><code class="p">[</code><code class="mf">0.485</code><code class="p">,</code><code> </code><code class="mf">0.456</code><code class="p">,</code><code> </code><code class="mf">0.406</code><code class="p">]</code><code class="p">,</code><code>
</code><code>                                              </code><code class="n">std</code><code class="o">=</code><code class="p">[</code><code class="mf">0.229</code><code class="p">,</code><code> </code><code class="mf">0.224</code><code class="p">,</code><code> </code><code class="mf">0.225</code><code class="p">]</code><code class="p">)</code><code> </code><a class="co" href="#callout_image_similarity_detection_with___span_class__keep_together__deep_learning__span__and_pyspark_lsh_CO1-2" id="co_image_similarity_detection_with___span_class__keep_together__deep_learning__span__and_pyspark_lsh_CO1-2"><img alt="2" height="12" src="assets/2.png" width="12"/></a><code>
</code><code>
</code><code>    </code><code class="k">def</code><code> </code><code class="nf">getFeatureLayer</code><code class="p">(</code><code class="bp">self</code><code class="p">)</code><code class="p">:</code><code>
</code><code>        </code><code class="n">cnnModel</code><code> </code><code class="o">=</code><code> </code><code class="n">models</code><code class="o">.</code><code class="n">resnet18</code><code class="p">(</code><code class="n">pretrained</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code><code>
</code><code>        </code><code class="n">layer</code><code> </code><code class="o">=</code><code> </code><code class="n">cnnModel</code><code class="o">.</code><code class="n">_modules</code><code class="o">.</code><code class="n">get</code><code class="p">(</code><code class="s1">'</code><code class="s1">avgpool</code><code class="s1">'</code><code class="p">)</code><code>
</code><code>        </code><code class="bp">self</code><code class="o">.</code><code class="n">layer_output_size</code><code> </code><code class="o">=</code><code> </code><code class="mi">512</code><code>
</code><code>
</code><code>        </code><code class="k">return</code><code> </code><code class="n">cnnModel</code><code class="p">,</code><code> </code><code class="n">layer</code><code>
</code><code>
</code><code>    </code><code class="k">def</code><code> </code><code class="nf">getVec</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code><code> </code><code class="n">img</code><code class="p">)</code><code class="p">:</code><code>
</code><code>        </code><code class="n">image</code><code> </code><code class="o">=</code><code> </code><code class="bp">self</code><code class="o">.</code><code class="n">normalize</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">toTensor</code><code class="p">(</code><code class="n">img</code><code class="p">)</code><code class="p">)</code><code class="o">.</code><code class="n">unsqueeze</code><code class="p">(</code><code class="mi">0</code><code class="p">)</code><code class="o">.</code><code class="n">to</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">device</code><code class="p">)</code><code>
</code><code>        </code><code class="n">embedding</code><code> </code><code class="o">=</code><code> </code><code class="n">torch</code><code class="o">.</code><code class="n">zeros</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code><code> </code><code class="bp">self</code><code class="o">.</code><code class="n">numberFeatures</code><code class="p">,</code><code> </code><code class="mi">1</code><code class="p">,</code><code> </code><code class="mi">1</code><code class="p">)</code><code>
</code><code>        </code><code class="k">def</code><code> </code><code class="nf">copyData</code><code class="p">(</code><code class="n">m</code><code class="p">,</code><code> </code><code class="n">i</code><code class="p">,</code><code> </code><code class="n">o</code><code class="p">)</code><code class="p">:</code><code> </code><code class="n">embedding</code><code class="o">.</code><code class="n">copy_</code><code class="p">(</code><code class="n">o</code><code class="o">.</code><code class="n">data</code><code class="p">)</code><code>
</code><code>        </code><code class="n">h</code><code> </code><code class="o">=</code><code> </code><code class="bp">self</code><code class="o">.</code><code class="n">featureLayer</code><code class="o">.</code><code class="n">register_forward_hook</code><code class="p">(</code><code class="n">copyData</code><code class="p">)</code><code>
</code><code>        </code><code class="bp">self</code><code class="o">.</code><code class="n">model</code><code class="p">(</code><code class="n">image</code><code class="p">)</code><code>
</code><code>        </code><code class="n">h</code><code class="o">.</code><code class="n">remove</code><code class="p">(</code><code class="p">)</code><code>
</code><code>        </code><code class="k">return</code><code> </code><code class="n">embedding</code><code class="o">.</code><code class="n">numpy</code><code class="p">(</code><code class="p">)</code><code class="p">[</code><code class="mi">0</code><code class="p">,</code><code> </code><code class="p">:</code><code class="p">,</code><code> </code><code class="mi">0</code><code class="p">,</code><code> </code><code class="mi">0</code><code class="p">]</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_image_similarity_detection_with___span_class__keep_together__deep_learning__span__and_pyspark_lsh_CO1-1" id="callout_image_similarity_detection_with___span_class__keep_together__deep_learning__span__and_pyspark_lsh_CO1-1"><img alt="1" height="12" src="assets/1.png" width="12"/></a></dt>
<dd><p>Convert images into the PyTorch tensor format.</p></dd>
<dt><a class="co" href="#co_image_similarity_detection_with___span_class__keep_together__deep_learning__span__and_pyspark_lsh_CO1-2" id="callout_image_similarity_detection_with___span_class__keep_together__deep_learning__span__and_pyspark_lsh_CO1-2"><img alt="2" height="12" src="assets/2.png" width="12"/></a></dt>
<dd><p>Rescale the range of pixel values between 0 and 1. The values for the mean and standard deviation (std) were precomputed based on the data used to train the model. Normalizing the image improves the accuracy of the classifier.</p></dd>
</dl>
<p>We now initialize the <code>Img2VecResnet18</code> class and apply the <code>getVec</code> method to all of the images to obtain their image embeddings.</p>
<pre data-code-language="python" data-type="programlisting"><code class="kn">import</code> <code class="nn">tqdm</code>

<code class="n">img2vec</code> <code class="o">=</code> <code class="n">Img2VecResnet18</code><code class="p">()</code>
<code class="n">allVectors</code> <code class="o">=</code> <code class="p">{}</code>
<code class="k">for</code> <code class="n">image</code> <code class="ow">in</code> <code class="n">tqdm</code><code class="p">(</code><code class="n">os</code><code class="o">.</code><code class="n">listdir</code><code class="p">(</code><code class="n">input_dir_cnn</code><code class="p">)):</code>
    <code class="n">I</code> <code class="o">=</code> <code class="n">Image</code><code class="o">.</code><code class="n">open</code><code class="p">(</code><code class="n">os</code><code class="o">.</code><code class="n">path</code><code class="o">.</code><code class="n">join</code><code class="p">(</code><code class="n">input_dir_cnn</code><code class="p">,</code> <code class="n">image</code><code class="p">))</code>
    <code class="n">vec</code> <code class="o">=</code> <code class="n">img2vec</code><code class="o">.</code><code class="n">getVec</code><code class="p">(</code><code class="n">I</code><code class="p">)</code>
    <code class="n">allVectors</code><code class="p">[</code><code class="n">image</code><code class="p">]</code> <code class="o">=</code> <code class="n">vec</code>
    <code class="n">I</code><code class="o">.</code><code class="n">close</code><code class="p">()</code></pre>
<p>For a larger dataset, you may want to sequentially write the vector output to a file rather than keeping it in memory to avoid an out-of-memory error. The data is manageable here, so we create a dictionary, which we save as a CSV file in the next step:</p>
<pre data-code-language="python" data-type="programlisting"><code class="kn">import</code> <code class="nn">pandas</code> <code class="k">as</code> <code class="nn">pd</code>

<code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">allVectors</code><code class="p">)</code><code class="o">.</code><code class="n">transpose</code><code class="p">()</code><code class="o">.</code>\
    <code class="n">to_csv</code><code class="p">(</code><code class="n">data_folder</code> <code class="o">+</code> <code class="s1">'/input_data_vectors.csv'</code><code class="p">)</code></pre>
<div data-type="note" epub:type="note">
<p>Since we are working locally, we went with the CSV format for saving the vector output. <a data-primary="Parquet data storage format" data-secondary="image-embedded dataframe" data-type="indexterm" id="idm46507961636176"/>However, Parquet format is more appropriate for data of this nature. You could easily save the data in Parquet format by replacing <code>to_csv</code> with <code>to_parquet</code> in the previous code.</p>
</div>
<p>Now that we have the required image embeddings, we can import them into PySpark.<a data-startref="ch10-embed" data-type="indexterm" id="idm46507961633792"/><a data-startref="ch10-embed2" data-type="indexterm" id="idm46507961649952"/><a data-startref="ch10-embed3" data-type="indexterm" id="idm46507961649280"/><a data-startref="ch10-embed4" data-type="indexterm" id="idm46507961648608"/><a data-startref="ch10-embed5" data-type="indexterm" id="idm46507961647936"/><a data-startref="ch10-embed6" data-type="indexterm" id="idm46507961647264"/></p>
</div></section>
<section data-pdf-bookmark="Import Image Embeddings into PySpark" data-type="sect2"><div class="sect2" id="idm46507962144720">
<h2>Import Image Embeddings into PySpark</h2>
<p>Start the PySpark shell:<a data-primary="image embedding" data-secondary="importing into PySpark" data-type="indexterm" id="idm46507961645088"/><a data-primary="image similarity detection" data-secondary="deep learning model" data-tertiary="image embeddings into PySpark" data-type="indexterm" id="idm46507961644112"/><a data-primary="deep learning" data-secondary="model for vector representation of images" data-tertiary="image embeddings into PySpark" data-type="indexterm" id="idm46507961642960"/><a data-primary="PyTorch" data-secondary="deep learning model" data-tertiary="image embeddings into PySpark" data-type="indexterm" id="idm46507961617536"/></p>
<pre data-code-language="shell" data-type="programlisting">$ pyspark --driver-memory 4g</pre>
<p>Import the image embeddings:</p>
<pre data-code-language="python" data-type="programlisting"><code class="n">input_df</code> <code class="o">=</code> <code class="n">spark</code><code class="o">.</code><code class="n">read</code><code class="o">.</code><code class="n">option</code><code class="p">(</code><code class="s1">'inferSchema'</code><code class="p">,</code> <code class="kc">True</code><code class="p">)</code><code class="o">.</code>\
                    <code class="n">csv</code><code class="p">(</code><code class="n">data_directory</code> <code class="o">+</code> <code class="s1">'/input_data_vectors.csv'</code><code class="p">)</code>
<code class="n">input_df</code><code class="o">.</code><code class="n">columns</code>
<code class="o">...</code>

<code class="p">[</code><code class="s1">'_c0'</code><code class="p">,</code>
 <code class="s1">'_c1'</code><code class="p">,</code>
 <code class="s1">'_c2'</code><code class="p">,</code>
 <code class="s1">'_c3'</code><code class="p">,</code>
 <code class="s1">'_c4'</code><code class="p">,</code>
<code class="p">[</code><code class="o">...</code><code class="p">]</code>
<code class="s1">'_c509'</code><code class="p">,</code>
 <code class="s1">'_c510'</code><code class="p">,</code>
 <code class="s1">'_c511'</code><code class="p">,</code>
 <code class="s1">'_c512'</code><code class="p">]</code></pre>
<p>PySpark’s LSH implementation requires<a data-primary="image similarity detection" data-secondary="deep learning model" data-tertiary="vector column required" data-type="indexterm" id="idm46507961606208"/><a data-primary="PyTorch" data-secondary="deep learning model" data-tertiary="vector column required" data-type="indexterm" id="idm46507961555056"/><a data-primary="deep learning" data-secondary="model for vector representation of images" data-tertiary="vector column required" data-type="indexterm" id="idm46507961553904"/><a data-primary="transformations of distributed data" data-secondary="VectorAssembler" data-type="indexterm" id="idm46507961552720"/><a data-primary="VectorAssembler" data-type="indexterm" id="idm46507961551808"/> a vector column as an input. We can create one by combining the relevant columns in our dataframe using the <code>VectorAssembler</code> transform:</p>
<pre data-code-language="python" data-type="programlisting"><code class="kn">from</code> <code class="nn">pyspark.ml.feature</code> <code class="kn">import</code> <code class="n">VectorAssembler</code>

<code class="n">vector_columns</code> <code class="o">=</code> <code class="n">input_df</code><code class="o">.</code><code class="n">columns</code><code class="p">[</code><code class="mi">1</code><code class="p">:]</code>
<code class="n">assembler</code> <code class="o">=</code> <code class="n">VectorAssembler</code><code class="p">(</code><code class="n">inputCols</code><code class="o">=</code><code class="n">vector_columns</code><code class="p">,</code> <code class="n">outputCol</code><code class="o">=</code><code class="s2">"features"</code><code class="p">)</code>

<code class="n">output</code> <code class="o">=</code> <code class="n">assembler</code><code class="o">.</code><code class="n">transform</code><code class="p">(</code><code class="n">input_df</code><code class="p">)</code>
<code class="n">output</code> <code class="o">=</code> <code class="n">output</code><code class="o">.</code><code class="n">select</code><code class="p">(</code><code class="s1">'_c0'</code><code class="p">,</code> <code class="s1">'features'</code><code class="p">)</code>

<code class="n">output</code><code class="o">.</code><code class="n">show</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="n">vertical</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>
<code class="o">...</code>

<code class="o">-</code><code class="n">RECORD</code> <code class="mi">0</code><code class="o">------------------------</code>
 <code class="n">_c0</code>      <code class="o">|</code> <code class="mf">01994.</code><code class="n">jpg</code>
 <code class="n">features</code> <code class="o">|</code> <code class="p">[</code><code class="mf">0.05640895</code><code class="p">,</code><code class="mf">2.709</code><code class="o">...</code>

<code class="o">...</code>

<code class="n">output</code><code class="o">.</code><code class="n">printSchema</code><code class="p">()</code>
<code class="o">...</code>

<code class="n">root</code>
 <code class="o">|--</code> <code class="n">_c0</code><code class="p">:</code> <code class="n">string</code> <code class="p">(</code><code class="n">nullable</code> <code class="o">=</code> <code class="n">true</code><code class="p">)</code>
 <code class="o">|--</code> <code class="n">features</code><code class="p">:</code> <code class="n">vector</code> <code class="p">(</code><code class="n">nullable</code> <code class="o">=</code> <code class="n">true</code><code class="p">)</code></pre>
<p>In the next section, we will use the LSH algorithm to create a way for us to find similar images from our dataset.</p>
</div></section>
</div></section>
<section data-pdf-bookmark="Image Similarity Search Using PySpark LSH" data-type="sect1"><div class="sect1" id="idm46507961478592">
<h1>Image Similarity Search Using PySpark LSH</h1>
<p>Locality sensitive hashing is an important class<a data-primary="locality sensitive hashing (LSH)" data-type="indexterm" id="ch10-LSH"/><a data-primary="LSH (locality sensitive hashing)" data-type="indexterm" id="ch10-LSH2"/><a data-primary="PySpark API" data-secondary="LSH for image similarity" data-type="indexterm" id="idm46507961416288"/><a data-primary="image similarity detection" data-secondary="deep learning model" data-tertiary="image similarity via LSH" data-type="indexterm" id="idm46507961415376"/><a data-primary="deep learning" data-secondary="model for vector representation of images" data-tertiary="image similarity via LSH" data-type="indexterm" id="idm46507961368832"/><a data-primary="PyTorch" data-secondary="deep learning model" data-tertiary="image similarity via LSH" data-type="indexterm" id="idm46507961367744"/> of hashing techniques, which is commonly used in clustering, approximate nearest neighbor search, and outlier detection with large datasets. Locality sensitive functions take two data points and decide whether or not they should be a candidate pair.</p>
<p>The general idea of LSH is to use a family of functions (“LSH families”) to hash data points into buckets so that the data points that are close to each other are in the same buckets with high probability, while data points that are far away from each other are very likely in different buckets. The data points that map to the same buckets are considered a candidate pair.</p>
<p>In PySpark, different LSH families are implemented in separate classes (e.g., <code>MinHash</code> and <code>BucketedRandomProjection</code>), and APIs for feature transformation, approximate similarity join, and approximate nearest neighbor are provided in each class.</p>
<p>We’ll use the BucketedRandomProjection implementation of LSH.<a data-primary="PySpark API" data-secondary="LSH for image similarity" data-tertiary="BucketedRandomProjection" data-type="indexterm" id="idm46507961364464"/><a data-primary="LSH (locality sensitive hashing)" data-secondary="BucketedRandomProjection" data-type="indexterm" id="idm46507961363136"/><a data-primary="locality sensitive hashing (LSH)" data-secondary="BucketedRandomProjection" data-type="indexterm" id="idm46507961362160"/></p>
<p>Let’s first create our model object:</p>
<pre data-code-language="python" data-type="programlisting"><code class="kn">from</code> <code class="nn">pyspark.ml.feature</code> <code class="kn">import</code> <code class="n">BucketedRandomProjectionLSH</code>

<code class="n">brp</code> <code class="o">=</code> <code class="n">BucketedRandomProjectionLSH</code><code class="p">(</code><code class="n">inputCol</code><code class="o">=</code><code class="s2">"features"</code><code class="p">,</code> <code class="n">outputCol</code><code class="o">=</code><code class="s2">"hashes"</code><code class="p">,</code>
                                  <code class="n">numHashTables</code><code class="o">=</code><code class="mi">200</code><code class="p">,</code> <code class="n">bucketLength</code><code class="o">=</code><code class="mf">2.0</code><code class="p">)</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">brp</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">output</code><code class="p">)</code></pre>
<p>In the BucketedRandomProjection LSH implementation, the bucket length can be used to control the average size of hash buckets (and thus the number of buckets). <a data-primary="false positives" data-type="indexterm" id="idm46507961342512"/>A larger bucket length (i.e., fewer buckets) increases the probability of features being hashed to the same bucket (increasing the number of true and false positives).</p>
<p>We now transform the input DataFrame using the newly created LSH model object. The resulting DataFrame will contain a <code>hashes</code> column containing hashed representation of the image embeddings:</p>
<pre data-code-language="python" data-type="programlisting"><code class="n">lsh_df</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">transform</code><code class="p">(</code><code class="n">output</code><code class="p">)</code>
<code class="n">lsh_df</code><code class="o">.</code><code class="n">show</code><code class="p">(</code><code class="mi">5</code><code class="p">)</code>

<code class="o">...</code>
<code class="o">+---------+--------------------+--------------------+</code>
<code class="o">|</code>      <code class="n">_c0</code><code class="o">|</code>            <code class="n">features</code><code class="o">|</code>              <code class="n">hashes</code><code class="o">|</code>
<code class="o">+---------+--------------------+--------------------+</code>
<code class="o">|</code><code class="mf">01994.</code><code class="n">jpg</code><code class="o">|</code><code class="p">[</code><code class="mf">0.05640895</code><code class="p">,</code><code class="mf">2.709</code><code class="o">...|</code><code class="p">[[</code><code class="mf">0.0</code><code class="p">],</code> <code class="p">[</code><code class="o">-</code><code class="mf">2.0</code><code class="p">],</code> <code class="p">[</code><code class="o">...|</code>
<code class="o">|</code><code class="mf">07758.</code><code class="n">jpg</code><code class="o">|</code><code class="p">[</code><code class="mf">2.1690884</code><code class="p">,</code><code class="mf">3.4647</code><code class="o">...|</code><code class="p">[[</code><code class="mf">0.0</code><code class="p">],</code> <code class="p">[</code><code class="o">-</code><code class="mf">1.0</code><code class="p">],</code> <code class="p">[</code><code class="o">...|</code>
<code class="o">|</code><code class="mf">05257.</code><code class="n">jpg</code><code class="o">|</code><code class="p">[</code><code class="mf">0.7666548</code><code class="p">,</code><code class="mf">3.7960</code><code class="o">...|</code><code class="p">[[</code><code class="o">-</code><code class="mf">1.0</code><code class="p">],</code> <code class="p">[</code><code class="o">-</code><code class="mf">1.0</code><code class="p">],</code> <code class="o">...|</code>
<code class="o">|</code><code class="mf">07642.</code><code class="n">jpg</code><code class="o">|</code><code class="p">[</code><code class="mf">0.86353475</code><code class="p">,</code><code class="mf">2.993</code><code class="o">...|</code><code class="p">[[</code><code class="o">-</code><code class="mf">1.0</code><code class="p">],</code> <code class="p">[</code><code class="o">-</code><code class="mf">1.0</code><code class="p">],</code> <code class="o">...|</code>
<code class="o">|</code><code class="mf">00850.</code><code class="n">jpg</code><code class="o">|</code><code class="p">[</code><code class="mf">0.49161428</code><code class="p">,</code><code class="mf">2.172</code><code class="o">...|</code><code class="p">[[</code><code class="o">-</code><code class="mf">1.0</code><code class="p">],</code> <code class="p">[</code><code class="o">-</code><code class="mf">2.0</code><code class="p">],</code> <code class="o">...|</code>
<code class="o">+---------+--------------------+--------------------+</code>
<code class="n">only</code> <code class="n">showing</code> <code class="n">top</code> <code class="mi">5</code> <code class="n">rows</code></pre>
<p>With our LSH-transformed dataset ready, we’ll put our work to the test in the next section.</p>
<section data-pdf-bookmark="Nearest Neighbor Search" data-type="sect2"><div class="sect2" id="idm46507961228224">
<h2>Nearest Neighbor Search</h2>
<p>Let’s try to find a similar image using a new image.<a data-primary="PyTorch" data-secondary="deep learning model" data-tertiary="nearest neighbor search" data-type="indexterm" id="idm46507961151728"/><a data-primary="image similarity detection" data-secondary="deep learning model" data-tertiary="nearest neighbor search" data-type="indexterm" id="idm46507961150480"/><a data-primary="deep learning" data-secondary="model for vector representation of images" data-tertiary="nearest neighbor search" data-type="indexterm" id="idm46507961149296"/> For now, we will pick one from the input dataset itself (<a data-type="xref" href="#random-car">Figure 10-3</a>):</p>
<pre data-code-language="python" data-type="programlisting"><code class="kn">from</code> <code class="nn">IPython.display</code> <code class="kn">import</code> <code class="n">display</code>
<code class="kn">from</code> <code class="nn">PIL</code> <code class="kn">import</code> <code class="n">Image</code>

<code class="n">input_dir_cnn</code> <code class="o">=</code> <code class="n">data_folder</code> <code class="o">+</code> <code class="s2">"/images/input_images_cnn"</code>

<code class="n">test_image</code> <code class="o">=</code> <code class="n">os</code><code class="o">.</code><code class="n">listdir</code><code class="p">(</code><code class="n">input_dir_cnn</code><code class="p">)[</code><code class="mi">0</code><code class="p">]</code>
<code class="n">test_image</code> <code class="o">=</code> <code class="n">os</code><code class="o">.</code><code class="n">path</code><code class="o">.</code><code class="n">join</code><code class="p">(</code><code class="n">input_dir_cnn</code><code class="p">,</code> <code class="n">test_image</code><code class="p">)</code>
<code class="nb">print</code><code class="p">(</code><code class="n">test_image</code><code class="p">)</code>
<code class="n">display</code><code class="p">(</code><code class="n">Image</code><code class="o">.</code><code class="n">open</code><code class="p">(</code><code class="n">test_image</code><code class="p">))</code>
<code class="o">...</code>
<code class="n">cars_data</code><code class="o">/</code><code class="n">images</code><code class="o">/</code><code class="n">input_images_cnn</code><code class="o">/</code><code class="mf">01994.</code><code class="n">jpg</code></pre>
<figure><div class="figure" id="random-car">
<img alt="Randomly picked car image" height="176" src="assets/aaps_1003.png" width="224"/>
<h6><span class="label">Figure 10-3. </span>Randomly picked car image from our dataset</h6>
</div></figure>
<p>First, we’ll need to convert the input image into a vector format using our <code>I⁠m⁠g⁠2⁠V⁠e⁠c​R⁠e⁠s⁠n⁠e⁠t⁠1⁠8</code> class:</p>
<pre data-code-language="python" data-type="programlisting"><code class="n">img2vec</code> <code class="o">=</code> <code class="n">Img2VecResnet18</code><code class="p">()</code>
<code class="n">I</code> <code class="o">=</code> <code class="n">Image</code><code class="o">.</code><code class="n">open</code><code class="p">(</code><code class="n">test_image</code><code class="p">)</code>
<code class="n">test_vec</code> <code class="o">=</code> <code class="n">img2vec</code><code class="o">.</code><code class="n">getVec</code><code class="p">(</code><code class="n">I</code><code class="p">)</code>
<code class="n">I</code><code class="o">.</code><code class="n">close</code><code class="p">()</code>

<code class="nb">print</code><code class="p">(</code><code class="nb">len</code><code class="p">(</code><code class="n">test_vec</code><code class="p">))</code>
<code class="nb">print</code><code class="p">(</code><code class="n">test_vec</code><code class="p">)</code>
<code class="o">...</code>

<code class="mi">512</code>
<code class="p">[</code><code class="mf">5.64089492e-02</code> <code class="mf">2.70972490e+00</code> <code class="mf">2.15519500e+00</code> <code class="mf">1.43926993e-01</code>
 <code class="mf">2.47581363e+00</code> <code class="mf">1.36641121e+00</code> <code class="mf">1.08204508e+00</code> <code class="mf">7.62105465e-01</code>
<code class="p">[</code><code class="o">...</code><code class="p">]</code>
<code class="mf">5.62133253e-01</code> <code class="mf">4.33687061e-01</code> <code class="mf">3.95899676e-02</code> <code class="mf">1.47889364e+00</code>
 <code class="mf">2.89110214e-01</code> <code class="mf">6.61322474e-01</code> <code class="mf">1.84713617e-01</code> <code class="mf">9.42268595e-02</code><code class="p">]</code>
<code class="o">...</code>

<code class="n">test_vector</code> <code class="o">=</code> <code class="n">Vectors</code><code class="o">.</code><code class="n">dense</code><code class="p">(</code><code class="n">test_vec</code><code class="p">)</code></pre>
<p>Now we perform an approximate nearest neighbor search:</p>
<pre data-code-language="python" data-type="programlisting"><code class="nb">print</code><code class="p">(</code><code class="s2">"Approximately searching lsh_df for 5 nearest neighbors </code><code class="se">\</code>
<code class="s2">        of input vector:"</code><code class="p">)</code>
<code class="n">result</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">approxNearestNeighbors</code><code class="p">(</code><code class="n">lsh_df</code><code class="p">,</code> <code class="n">test_vector</code><code class="p">,</code> <code class="mi">5</code><code class="p">)</code>

<code class="n">result</code><code class="o">.</code><code class="n">show</code><code class="p">()</code>
<code class="o">...</code>
<code class="o">+---------+--------------------+--------------------+--------------------+</code>
<code class="o">|</code>      <code class="n">_c0</code><code class="o">|</code>            <code class="n">features</code><code class="o">|</code>              <code class="n">hashes</code><code class="o">|</code>             <code class="n">distCol</code><code class="o">|</code>
<code class="o">+---------+--------------------+--------------------+--------------------+</code>
<code class="o">|</code><code class="mf">01994.</code><code class="n">jpg</code><code class="o">|</code><code class="p">[</code><code class="mf">0.05640895</code><code class="p">,</code><code class="mf">2.709</code><code class="o">...|</code><code class="p">[[</code><code class="mf">0.0</code><code class="p">],</code> <code class="p">[</code><code class="o">-</code><code class="mf">2.0</code><code class="p">],</code> <code class="p">[</code><code class="o">...|</code><code class="mf">3.691941786298668</code><code class="o">...|</code>
<code class="o">|</code><code class="mf">00046.</code><code class="n">jpg</code><code class="o">|</code><code class="p">[</code><code class="mf">0.89430475</code><code class="p">,</code><code class="mf">1.992</code><code class="o">...|</code><code class="p">[[</code><code class="mf">0.0</code><code class="p">],</code> <code class="p">[</code><code class="o">-</code><code class="mf">2.0</code><code class="p">],</code> <code class="p">[</code><code class="o">...|</code>   <code class="mf">10.16105522433224</code><code class="o">|</code>
<code class="o">|</code><code class="mf">04232.</code><code class="n">jpg</code><code class="o">|</code><code class="p">[</code><code class="mf">0.71477133</code><code class="p">,</code><code class="mf">2.582</code><code class="o">...|</code><code class="p">[[</code><code class="o">-</code><code class="mf">1.0</code><code class="p">],</code> <code class="p">[</code><code class="o">-</code><code class="mf">2.0</code><code class="p">],</code> <code class="o">...|</code>  <code class="mf">10.255391011678762</code><code class="o">|</code>
<code class="o">|</code><code class="mf">05146.</code><code class="n">jpg</code><code class="o">|</code><code class="p">[</code><code class="mf">0.36903867</code><code class="p">,</code><code class="mf">3.410</code><code class="o">...|</code><code class="p">[[</code><code class="o">-</code><code class="mf">1.0</code><code class="p">],</code> <code class="p">[</code><code class="o">-</code><code class="mf">2.0</code><code class="p">],</code> <code class="o">...|</code>  <code class="mf">10.264572173322843</code><code class="o">|</code>
<code class="o">|</code><code class="mf">00985.</code><code class="n">jpg</code><code class="o">|</code><code class="p">[</code><code class="mf">0.530428</code><code class="p">,</code><code class="mf">2.87453</code><code class="o">...|</code><code class="p">[[</code><code class="o">-</code><code class="mf">1.0</code><code class="p">],</code> <code class="p">[</code><code class="o">-</code><code class="mf">2.0</code><code class="p">],</code> <code class="o">...|</code>  <code class="mf">10.474841359816633</code><code class="o">|</code>
<code class="o">+---------+--------------------+--------------------+--------------------+</code></pre>
<p>You can check the images in Figures <a data-type="xref" data-xrefstyle="select:labelnumber" href="#result-image1">10-4</a> through <a data-type="xref" data-xrefstyle="select:labelnumber" href="#result-image5">10-8</a> to see that the model gets it somewhat right already:</p>
<pre data-code-language="python" data-type="programlisting"><code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">list</code><code class="p">(</code><code class="n">result</code><code class="o">.</code><code class="n">select</code><code class="p">(</code><code class="s1">'_c0'</code><code class="p">)</code><code class="o">.</code><code class="n">toPandas</code><code class="p">()[</code><code class="s1">'_c0'</code><code class="p">]):</code>
    <code class="n">display</code><code class="p">(</code><code class="n">Image</code><code class="o">.</code><code class="n">open</code><code class="p">(</code><code class="n">os</code><code class="o">.</code><code class="n">path</code><code class="o">.</code><code class="n">join</code><code class="p">(</code><code class="n">input_dir_cnn</code><code class="p">,</code> <code class="n">i</code><code class="p">)))</code></pre>
<figure><div class="figure" id="result-image1">
<img alt="Result image 1" height="176" src="assets/aaps_1003.png" width="224"/>
<h6><span class="label">Figure 10-4. </span>Result image 1</h6>
</div></figure>
<figure><div class="figure">
<img alt="Result image 2" height="176" src="assets/aaps_1005.png" width="224"/>
<h6><span class="label">Figure 10-5. </span>Result image 2</h6>
</div></figure>
<figure><div class="figure">
<img alt="Result image 3" height="178" src="assets/aaps_1006.png" width="224"/>
<h6><span class="label">Figure 10-6. </span>Result image 3</h6>
</div></figure>
<figure><div class="figure">
<img alt="Result image 4" height="181" src="assets/aaps_1007.png" width="224"/>
<h6><span class="label">Figure 10-7. </span>Result image 4</h6>
</div></figure>
<figure><div class="figure" id="result-image5">
<img alt="Result image 5" height="175" src="assets/aaps_1008.png" width="224"/>
<h6><span class="label">Figure 10-8. </span>Result image 5</h6>
</div></figure>
<p>The input image is on top of the list as one would expect.<a data-startref="ch10-LSH" data-type="indexterm" id="idm46507960666416"/><a data-startref="ch10-LSH2" data-type="indexterm" id="idm46507960665808"/></p>
</div></section>
</div></section>
<section data-pdf-bookmark="Where to Go from Here" data-type="sect1"><div class="sect1" id="idm46507961419248">
<h1>Where to Go from Here</h1>
<p>In this chapter, we learned how PySpark can be combined with a modern deep learning framework to scale an image similarity detection workflow.</p>
<p>There are multiple ways to improve this implementation. You can try using a better model or improving the preprocessing to get better quality of embeddings. Further, the LSH model can be tweaked. In a real-life setting, you may need to update the reference dataset consistently to account for new images coming into the system. The simplest way to do this is by running a batch job at periodic intervals to create new LSH models. You can explore all of these depending on your need and interest.</p>
</div></section>
</div></section></div></body></html>