<html><head></head><body>
<div id="sbo-rt-content"><section data-pdf-bookmark="Chapter 5. Anomaly Detection with K-means Clustering" data-type="chapter" epub:type="chapter"><div class="chapter" id="idm46507982499648">
<h1><span class="label">Chapter 5. </span>Anomaly Detection <span class="keep-together">with K-means Clustering</span></h1>
<p>Classification and regression are powerful, well-studied techniques in machine learning. <a data-type="xref" href="ch04.xhtml#making_predictions_with_decision_trees_and_decision_forests">Chapter 4</a> demonstrated using a classifier as a predictor of unknown values. But there was a catch: to predict unknown values for new data, we had to know the target values for many previously seen examples. Classifiers can help only if we, the data scientists, know what we are looking for and can provide plenty of examples where input produced a known output. <a data-primary="classification" data-secondary="supervised learning via" data-type="indexterm" id="idm46507977916896"/><a data-primary="regression" data-secondary="supervised learning via" data-type="indexterm" id="idm46507977916048"/><a data-primary="supervised learning with classification and regression" data-seealso="decision trees" data-type="indexterm" id="idm46507977915200"/>These were collectively known as <em>supervised learning</em> techniques, because their learning process receives the correct output value for each example in the input.</p>
<p>However, sometimes the correct output<a data-primary="unsupervised learning" data-seealso="anomaly detection" data-type="indexterm" id="idm46507977913264"/> is unknown for some or all examples. Consider the problem of dividing up an ecommerce site’s customers by their shopping habits and tastes. The input features are their purchases, clicks, demographic information, and more. The output should be groupings of customers: perhaps one group will represent fashion-conscious buyers, another will turn out to correspond to price-sensitive bargain hunters, and so on.</p>
<p>If you were asked to determine this target label for each new customer, you would quickly run into a problem in applying a supervised learning technique like a classifier: you don’t know a priori who should be considered fashion-conscious, for example. In fact, you’re not even sure if “fashion-conscious” is a meaningful grouping of the site’s customers to begin with!</p>
<p>Fortunately, <em>unsupervised learning</em> techniques can help. These techniques do not learn to predict a target value, because none is available. They can, however, learn structure in data and find groupings of similar inputs, or learn what types of input are likely to occur and what types are not. This chapter will introduce unsupervised learning using clustering implementations in MLlib. <a data-primary="anomaly detection" data-secondary="about" data-type="indexterm" id="idm46507977910912"/><a data-primary="network traffic anomaly detection" data-secondary="about anomaly detection" data-type="indexterm" id="idm46507977909936"/><a data-primary="K-means clustering" data-secondary="about anomaly detection" data-type="indexterm" id="idm46507977909024"/>Specifically, we will use the K-means 
<span class="keep-together">clustering</span> algorithm for identifying anomalies in network traffic data. Anomaly detection is often used to find fraud, detect network attacks, or discover problems in servers or other sensor-equipped machinery. In these cases, it’s important to be able to find new types of anomalies that have never been seen before—new forms of fraud, intrusions, and failure modes for servers. Unsupervised learning techniques are useful in these cases because they can learn what input data normally looks like and therefore detect when new data is unlike past data. Such new data is not necessarily attacks or fraud; it is simply unusual and therefore worth further investigation.</p>
<p>We will start with the basics of the K-means clustering algorithm. This will be followed by an introduction to the KDD Cup 1999 dataset. We’ll then create our first K-means model using PySpark. Then we’ll go over methods for determining a good value of <em>k</em>—number of clusters—when implementing the K-means algorithm. Next, we’ll improve our model by normalizing the input features and using previously discarded categorical features by implementing the one-hot encoding method. We will wrap up by going over the entropy metric and exploring some results from our model.</p>
<section data-pdf-bookmark="K-means Clustering" data-type="sect1"><div class="sect1" id="idm46507977906064">
<h1>K-means Clustering</h1>
<p>The inherent problem of anomaly detection is,<a data-primary="K-means clustering" data-type="indexterm" id="idm46507977904560"/><a data-primary="anomaly detection" data-secondary="K-means clustering" data-type="indexterm" id="idm46507977903856"/><a data-primary="network traffic anomaly detection" data-secondary="K-means clustering" data-type="indexterm" id="idm46507977902912"/> as its name implies, that of finding unusual things. If we already knew what “anomalous” meant for a dataset, we could easily detect anomalies in the data with supervised learning. An algorithm would receive inputs labeled “normal” and “anomaly” and learn to distinguish the two. However, the nature of anomalies is that they are unknown unknowns. Put another way, an anomaly that has been observed and understood is no longer an anomaly.</p>
<p>Clustering is the best-known type<a data-primary="clustering algorithms for unsupervised learning" data-seealso="K-means clustering" data-type="indexterm" id="idm46507977901488"/> of unsupervised learning. Clustering algorithms try to find natural groupings in data. Data points that are like one another but unlike others are likely to represent a meaningful grouping, so clustering algorithms try to put such data into the same cluster.</p>
<p>K-means clustering may be the most widely used clustering algorithm. It attempts to detect <em>k</em> clusters in a dataset, where <em>k</em> is given by the data scientist.<a data-primary="hyperparameters" data-secondary="k for K-means clustering" data-type="indexterm" id="idm46507977898544"/> <em>k</em> is a hyperparameter of the model, and the right value will depend on the dataset. In fact, choosing a good value for <em>k</em> will be a central plot point in this chapter.</p>
<p>What does “like” mean when the dataset contains information such as customer activity? Or transactions? K-means requires a notion of distance between data points. <a data-primary="Euclidean distance for K-means" data-type="indexterm" id="idm46507977896160"/><a data-primary="K-means clustering" data-secondary="Euclidean distance for" data-type="indexterm" id="idm46507977895456"/>It is common to use simple Euclidean distance to measure distance between data points with K-means, and as it happens, this is one of two distance functions supported by Spark MLlib as of this writing, the other one being Cosine. The Euclidean distance is defined for data points whose features are all numeric.<a data-primary="K-means clustering" data-secondary="like points" data-type="indexterm" id="idm46507977894384"/> “Like” points are those whose intervening distance is small.</p>
<p>To K-means, a cluster is simply a point: the center of all the points that make up the cluster. <a data-primary="feature vectors" data-secondary="K-means clustering" data-type="indexterm" id="idm46507977892816"/>These are, in fact, just feature vectors containing all numeric features and can be called vectors. However, it may be more intuitive to think of them as points here, because they are treated as points in a Euclidean space.</p>
<p>This center is called the cluster <em>centroid</em> <a data-primary="centroid of a K-means cluster" data-type="indexterm" id="idm46507977890912"/><a data-primary="K-means clustering" data-secondary="centroid of cluster" data-type="indexterm" id="idm46507977890112"/>and is the arithmetic mean of the points—hence the name K-<em>means</em>. To start, the algorithm picks some data points as the initial cluster centroids. Then each data point is assigned to the nearest centroid. Then for each cluster, a new cluster centroid is computed as the mean of the data points just assigned to that cluster. This process is repeated.</p>
<p>We will now look at a use case that depicts how K-means clustering can help us identify potentially anomalous activity in a network.</p>
</div></section>
<section data-pdf-bookmark="Identifying Anomalous Network Traffic" data-type="sect1"><div class="sect1" id="idm46507977887680">
<h1>Identifying Anomalous Network Traffic</h1>
<p>Cyberattacks are increasingly visible in the news. <a data-primary="K-means clustering" data-secondary="network traffic anomalies" data-tertiary="about" data-type="indexterm" id="idm46507977886144"/><a data-primary="anomaly detection" data-secondary="network traffic anomalies" data-tertiary="about" data-type="indexterm" id="idm46507977884832"/><a data-primary="network traffic anomaly detection" data-secondary="about" data-type="indexterm" id="idm46507977883600"/><a data-primary="cyberattacks" data-see="network traffic anomaly detection" data-type="indexterm" id="idm46507977882640"/>Some attacks attempt to flood a computer with network traffic to crowd out legitimate traffic. But in other cases, attacks attempt to exploit flaws in networking software to gain unauthorized access to a computer. While it’s quite obvious when a computer is being bombarded with traffic, detecting an exploit can be like searching for a needle in an incredibly large haystack of network requests.</p>
<p>Some exploit behaviors follow known patterns. For example, accessing every port on a machine in rapid succession is not something any normal software program should ever need to do. However, it is a typical first step for an attacker looking for services running on the computer that may be exploitable.</p>
<p>If you were to count the number of distinct ports accessed by a remote host in a short time, you would have a feature that probably predicts a port-scanning attack quite well. A handful is probably normal; hundreds indicate an attack. The same goes for detecting other types of attacks from other features of network connections—number of bytes sent and received, TCP errors, and so forth.</p>
<p>But what about those unknown unknowns? The biggest threat may be the one that has never yet been detected and classified. Part of detecting potential network intrusions is detecting anomalies. These are connections that aren’t known to be attacks but do not resemble connections that have been observed in the past.</p>
<p>Here, unsupervised learning techniques like K-means can be used to detect anomalous network connections. K-means can cluster connections based on statistics about each of them. The resulting clusters themselves aren’t interesting per se, but they collectively define types of connections that are like past connections. Anything not close to a cluster could be anomalous. Clusters are interesting insofar as they define regions of normal connections; everything else is unusual and potentially anomalous.</p>
<section data-pdf-bookmark="KDD Cup 1999 Dataset" data-type="sect2"><div class="sect2" id="idm46507977879792">
<h2>KDD Cup 1999 Dataset</h2>
<p>The <a href="https://oreil.ly/UtYd9">KDD Cup</a> was an <a data-primary="anomaly detection" data-secondary="network traffic anomalies" data-tertiary="KDD Cup 1999 dataset" data-type="indexterm" id="idm46507977876848"/><a data-primary="K-means clustering" data-secondary="network traffic anomalies" data-tertiary="KDD Cup 1999 dataset" data-type="indexterm" id="idm46507977875600"/><a data-primary="network traffic anomaly detection" data-secondary="KDD Cup 1999 dataset" data-type="indexterm" id="idm46507977874368"/><a data-primary="datasets" data-secondary="KDD Cup (1999)" data-type="indexterm" id="idm46507977873408"/><a data-primary="KDD Cup 1999 dataset" data-type="indexterm" id="idm46507977872464"/><a data-primary="data mining competition dataset" data-type="indexterm" id="idm46507977871792"/>annual data mining competition organized by a special interest group of the Association for Computing Machinery (ACM). Each year, a machine learning problem was posed, along with a dataset, and researchers were invited to submit a paper detailing their best solution to the problem. In 1999, the topic was network intrusion, and the dataset is <a href="https://oreil.ly/ezBDa">still available</a> at the KDD website. We will need to download the <em>kddcupdata.data.gz</em> and <em>kddcup.info</em> files from the website. The remainder of this chapter will walk through building a system to detect anomalous network traffic using Spark, by learning from this data.</p>
<div data-type="note" epub:type="note">
<p>Don’t use this dataset to build a real network intrusion system! The data did not necessarily reflect real network traffic at the 
<span class="keep-together">time—even</span> if it did, it reflects traffic patterns from more than 20 years ago.</p>
</div>
<p>Fortunately, the organizers had already processed raw network packet data into summary information about individual network connections. The dataset is about 708 MB in size and contains about 4.9 million connections. This is large, if not massive, and is certainly sufficient for our purposes here. For each connection, the dataset contains information such as the number of bytes sent, login attempts, TCP errors, and so on. Each connection is one line of CSV-formatted data, containing 38 features. Feature information and ordering can be found in the <em>kddcup.info</em> file.</p>
<p>Unzip the <em>kddcup.data.gz</em> data file and copy it into your storage. This example, like others, will assume the file is available at <em>data/kddcup.data</em>. Let’s see the data in its raw form:</p>
<pre data-code-language="shell" data-type="programlisting">head -n <code class="m">1</code> data/kddcup.data

...

<code class="m">0</code>,tcp,http,SF,215,45076,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,1,...</pre>
<p>This connection, for example, was a TCP connection to an HTTP service—215 bytes were sent, and 45,706 bytes were received. The user was logged in, and so on.</p>
<p>Many features are counts, like <code>num_file_creations</code> in the 17th column, as listed in the <em>kddcup.info</em> file. <a data-primary="categorical data" data-secondary="binary mapped to numeric" data-type="indexterm" id="idm46507977859936"/>Many features take on the value 0 or 1, indicating the presence or absence of a behavior, like <code>su_attempted</code> in the 15th column. They look like the one-hot encoded categorical features from <a data-type="xref" href="ch04.xhtml#making_predictions_with_decision_trees_and_decision_forests">Chapter 4</a>, but are not grouped and related in the same way.
Each is like a yes/no feature, and is therefore arguably a categorical feature. It is not always valid to translate categorical features as numbers and treat them as if they had an ordering. However, in the special case of a binary categorical feature, in most machine learning algorithms, mapping these to a numeric feature taking on values 0 and 1 will work well.</p>
<p>The rest are ratios like <code>dst_host_srv_rerror_rate</code> in the next-to-last column and take on
values from 0.0 to 1.0, inclusive.</p>
<p>Interestingly, a label is given in the last field. Most connections are labeled <code>normal.</code>, but some have been
identified as examples of various types of network attacks. These would be useful in learning to distinguish
a known attack from a normal connection, but the problem here is anomaly detection and finding potentially new
and unknown attacks. This label will be mostly set aside for our purposes.</p>
</div></section>
</div></section>
<section data-pdf-bookmark="A First Take on Clustering" data-type="sect1"><div class="sect1" id="idm46507977856176">
<h1>A First Take on Clustering</h1>
<p>Open the <code>pyspark-shell</code>, and load the CSV data<a data-primary="anomaly detection" data-secondary="network traffic anomalies" data-tertiary="clustering first take" data-type="indexterm" id="ch05-first"/><a data-primary="K-means clustering" data-secondary="network traffic anomalies" data-tertiary="clustering first take" data-type="indexterm" id="ch05-first2"/><a data-primary="network traffic anomaly detection" data-secondary="clustering first take" data-type="indexterm" id="ch05-first3"/> as a dataframe. It’s a CSV file again, but without header information. It’s necessary to supply column names as given in the accompanying <em>kddcup.info</em> file.</p>
<pre data-code-language="python" data-type="programlisting"><code class="n">data_without_header</code> <code class="o">=</code> <code class="n">spark</code><code class="o">.</code><code class="n">read</code><code class="o">.</code><code class="n">option</code><code class="p">(</code><code class="s2">"inferSchema"</code><code class="p">,</code> <code class="kc">True</code><code class="p">)</code><code class="o">.</code>\
                                  <code class="n">option</code><code class="p">(</code><code class="s2">"header"</code><code class="p">,</code> <code class="kc">False</code><code class="p">)</code><code class="o">.</code>\
                                  <code class="n">csv</code><code class="p">(</code><code class="s2">"data/kddcup.data"</code><code class="p">)</code>

<code class="n">column_names</code> <code class="o">=</code> <code class="p">[</code>  <code class="s2">"duration"</code><code class="p">,</code> <code class="s2">"protocol_type"</code><code class="p">,</code> <code class="s2">"service"</code><code class="p">,</code> <code class="s2">"flag"</code><code class="p">,</code>
  <code class="s2">"src_bytes"</code><code class="p">,</code> <code class="s2">"dst_bytes"</code><code class="p">,</code> <code class="s2">"land"</code><code class="p">,</code> <code class="s2">"wrong_fragment"</code><code class="p">,</code> <code class="s2">"urgent"</code><code class="p">,</code>
  <code class="s2">"hot"</code><code class="p">,</code> <code class="s2">"num_failed_logins"</code><code class="p">,</code> <code class="s2">"logged_in"</code><code class="p">,</code> <code class="s2">"num_compromised"</code><code class="p">,</code>
  <code class="s2">"root_shell"</code><code class="p">,</code> <code class="s2">"su_attempted"</code><code class="p">,</code> <code class="s2">"num_root"</code><code class="p">,</code> <code class="s2">"num_file_creations"</code><code class="p">,</code>
  <code class="s2">"num_shells"</code><code class="p">,</code> <code class="s2">"num_access_files"</code><code class="p">,</code> <code class="s2">"num_outbound_cmds"</code><code class="p">,</code>
  <code class="s2">"is_host_login"</code><code class="p">,</code> <code class="s2">"is_guest_login"</code><code class="p">,</code> <code class="s2">"count"</code><code class="p">,</code> <code class="s2">"srv_count"</code><code class="p">,</code>
  <code class="s2">"serror_rate"</code><code class="p">,</code> <code class="s2">"srv_serror_rate"</code><code class="p">,</code> <code class="s2">"rerror_rate"</code><code class="p">,</code> <code class="s2">"srv_rerror_rate"</code><code class="p">,</code>
  <code class="s2">"same_srv_rate"</code><code class="p">,</code> <code class="s2">"diff_srv_rate"</code><code class="p">,</code> <code class="s2">"srv_diff_host_rate"</code><code class="p">,</code>
  <code class="s2">"dst_host_count"</code><code class="p">,</code> <code class="s2">"dst_host_srv_count"</code><code class="p">,</code>
  <code class="s2">"dst_host_same_srv_rate"</code><code class="p">,</code> <code class="s2">"dst_host_diff_srv_rate"</code><code class="p">,</code>
  <code class="s2">"dst_host_same_src_port_rate"</code><code class="p">,</code> <code class="s2">"dst_host_srv_diff_host_rate"</code><code class="p">,</code>
  <code class="s2">"dst_host_serror_rate"</code><code class="p">,</code> <code class="s2">"dst_host_srv_serror_rate"</code><code class="p">,</code>
  <code class="s2">"dst_host_rerror_rate"</code><code class="p">,</code> <code class="s2">"dst_host_srv_rerror_rate"</code><code class="p">,</code>
  <code class="s2">"label"</code><code class="p">]</code>

<code class="n">data</code> <code class="o">=</code> <code class="n">data_without_header</code><code class="o">.</code><code class="n">toDF</code><code class="p">(</code><code class="o">*</code><code class="n">column_names</code><code class="p">)</code></pre>
<p>Begin by exploring the dataset. What labels are present in the data, and how many are there
of each? The following code simply counts by label and prints the results in descending order by count:</p>
<pre data-code-language="python" data-type="programlisting"><code class="kn">from</code> <code class="nn">pyspark.sql.functions</code> <code class="kn">import</code> <code class="n">col</code>
<code class="n">data</code><code class="o">.</code><code class="n">select</code><code class="p">(</code><code class="s2">"label"</code><code class="p">)</code><code class="o">.</code><code class="n">groupBy</code><code class="p">(</code><code class="s2">"label"</code><code class="p">)</code><code class="o">.</code><code class="n">count</code><code class="p">()</code><code class="o">.</code>\
      <code class="n">orderBy</code><code class="p">(</code><code class="n">col</code><code class="p">(</code><code class="s2">"count"</code><code class="p">)</code><code class="o">.</code><code class="n">desc</code><code class="p">())</code><code class="o">.</code><code class="n">show</code><code class="p">(</code><code class="mi">25</code><code class="p">)</code>

<code class="o">...</code>
<code class="o">+----------------+-------+</code>
<code class="o">|</code>           <code class="n">label</code><code class="o">|</code>  <code class="n">count</code><code class="o">|</code>
<code class="o">+----------------+-------+</code>
<code class="o">|</code>          <code class="n">smurf</code><code class="o">.|</code><code class="mi">2807886</code><code class="o">|</code>
<code class="o">|</code>        <code class="n">neptune</code><code class="o">.|</code><code class="mi">1072017</code><code class="o">|</code>
<code class="o">|</code>         <code class="n">normal</code><code class="o">.|</code> <code class="mi">972781</code><code class="o">|</code>
<code class="o">|</code>          <code class="n">satan</code><code class="o">.|</code>  <code class="mi">15892</code><code class="o">|</code>
<code class="o">...</code>
<code class="o">|</code>            <code class="n">phf</code><code class="o">.|</code>      <code class="mi">4</code><code class="o">|</code>
<code class="o">|</code>           <code class="n">perl</code><code class="o">.|</code>      <code class="mi">3</code><code class="o">|</code>
<code class="o">|</code>            <code class="n">spy</code><code class="o">.|</code>      <code class="mi">2</code><code class="o">|</code>
<code class="o">+----------------+-------+</code></pre>
<p>There are 23 distinct labels, and the most frequent are <code>smurf.</code> and <code>neptune.</code> attacks.</p>
<p>Note that the data contains nonnumeric features.<a data-primary="K-means clustering" data-secondary="network traffic anomalies" data-tertiary="categorical data" data-type="indexterm" id="idm46507977596992"/><a data-primary="anomaly detection" data-secondary="network traffic anomalies" data-tertiary="categorical data" data-type="indexterm" id="idm46507977595776"/><a data-primary="network traffic anomaly detection" data-secondary="categorical data" data-type="indexterm" id="idm46507977594592"/> For example, the second column may be <code>tcp</code>, <code>udp</code>, or <code>icmp</code>, but
K-means clustering requires numeric features. The final label column is also nonnumeric. To begin, these will simply
be ignored.</p>
<p>Aside from this, creating a K-means clustering of the data follows the same pattern as was
seen in <a data-type="xref" href="ch04.xhtml#making_predictions_with_decision_trees_and_decision_forests">Chapter 4</a>.<a data-primary="VectorAssembler" data-type="indexterm" id="idm46507977518768"/><a data-primary="transformations of distributed data" data-secondary="VectorAssembler" data-type="indexterm" id="idm46507977518160"/><a data-primary="feature vectors" data-secondary="VectorAssembler of feature vector" data-type="indexterm" id="idm46507977517312"/><a data-primary="vectors" data-secondary="feature vectors" data-tertiary="VectorAssembler of feature vector" data-type="indexterm" id="idm46507977516464"/><a data-primary="Pipeline" data-secondary="VectorAssembler plus KMeans" data-type="indexterm" id="idm46507977515328"/><a data-primary="MLlib component of Spark" data-secondary="Pipeline" data-tertiary="VectorAssembler plus KMeans" data-type="indexterm" id="idm46507977514368"/> A <code>VectorAssembler</code> creates a feature vector, a <code>KMeans</code> implementation creates a model from the feature vectors, and a <code>Pipeline</code> stitches
it all together. From the resulting model, it’s possible to extract and examine the cluster
centers.</p>
<pre data-code-language="python" data-type="programlisting"><code class="kn">from</code> <code class="nn">pyspark.ml.feature</code> <code class="kn">import</code> <code class="n">VectorAssembler</code>
<code class="kn">from</code> <code class="nn">pyspark.ml.clustering</code> <code class="kn">import</code> <code class="n">KMeans</code><code class="p">,</code> <code class="n">KMeansModel</code>
<code class="kn">from</code> <code class="nn">pyspark.ml</code> <code class="kn">import</code> <code class="n">Pipeline</code>

<code class="n">numeric_only</code> <code class="o">=</code> <code class="n">data</code><code class="o">.</code><code class="n">drop</code><code class="p">(</code><code class="s2">"protocol_type"</code><code class="p">,</code> <code class="s2">"service"</code><code class="p">,</code> <code class="s2">"flag"</code><code class="p">)</code><code class="o">.</code><code class="n">cache</code><code class="p">()</code>

<code class="n">assembler</code> <code class="o">=</code> <code class="n">VectorAssembler</code><code class="p">()</code><code class="o">.</code><code class="n">setInputCols</code><code class="p">(</code><code class="n">numeric_only</code><code class="o">.</code><code class="n">columns</code><code class="p">[:</code><code class="o">-</code><code class="mi">1</code><code class="p">])</code><code class="o">.</code>\
                              <code class="n">setOutputCol</code><code class="p">(</code><code class="s2">"featureVector"</code><code class="p">)</code>

<code class="n">kmeans</code> <code class="o">=</code> <code class="n">KMeans</code><code class="p">()</code><code class="o">.</code><code class="n">setPredictionCol</code><code class="p">(</code><code class="s2">"cluster"</code><code class="p">)</code><code class="o">.</code><code class="n">setFeaturesCol</code><code class="p">(</code><code class="s2">"featureVector"</code><code class="p">)</code>

<code class="n">pipeline</code> <code class="o">=</code> <code class="n">Pipeline</code><code class="p">()</code><code class="o">.</code><code class="n">setStages</code><code class="p">([</code><code class="n">assembler</code><code class="p">,</code> <code class="n">kmeans</code><code class="p">])</code>
<code class="n">pipeline_model</code> <code class="o">=</code> <code class="n">pipeline</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">numeric_only</code><code class="p">)</code>
<code class="n">kmeans_model</code> <code class="o">=</code> <code class="n">pipeline_model</code><code class="o">.</code><code class="n">stages</code><code class="p">[</code><code class="mi">1</code><code class="p">]</code>

<code class="kn">from</code> <code class="nn">pprint</code> <code class="kn">import</code> <code class="n">pprint</code>
<code class="n">pprint</code><code class="p">(</code><code class="n">kmeans_model</code><code class="o">.</code><code class="n">clusterCenters</code><code class="p">())</code>

<code class="o">...</code>
<code class="p">[</code><code class="n">array</code><code class="p">([</code><code class="mf">4.83401949e+01</code><code class="p">,</code> <code class="mf">1.83462155e+03</code><code class="p">,</code> <code class="mf">8.26203190e+02</code><code class="p">,</code> <code class="mf">5.71611720e-06</code><code class="p">,</code>
       <code class="mf">6.48779303e-04</code><code class="p">,</code> <code class="mf">7.96173468e-06</code><code class="o">...</code><code class="p">]),</code>
 <code class="n">array</code><code class="p">([</code><code class="mf">1.0999000e+04</code><code class="p">,</code> <code class="mf">0.0000000e+00</code><code class="p">,</code> <code class="mf">1.3099374e+09</code><code class="p">,</code> <code class="mf">0.0000000e+00</code><code class="p">,</code>
       <code class="mf">0.0000000e+00</code><code class="p">,</code> <code class="mf">0.0000000e+00</code><code class="p">,</code><code class="o">...</code><code class="p">])]</code></pre>
<p>It’s not easy to interpret the numbers intuitively,<a data-primary="centroid of a K-means cluster" data-secondary="KDD Cup 1999 dataset" data-type="indexterm" id="idm46507977508960"/><a data-primary="K-means clustering" data-secondary="centroid of cluster" data-tertiary="KDD Cup 1999 dataset" data-type="indexterm" id="idm46507977508112"/> but each of these represents the center (also known as centroid) of one of the clusters that the model produced. The values are the coordinates
of the centroid in terms of each of the numeric input features.</p>
<p>Two vectors are printed, meaning K-means was fitting <em>k</em>=2 clusters to the data. For a complex dataset that
is known to exhibit at least 23 distinct types of connections, this is almost certainly not enough to accurately
model the distinct groupings within the data.</p>
<p>This is a good opportunity to use the given labels to get an intuitive sense of what went into these two clusters by counting the labels within each cluster.</p>
<pre data-code-language="python" data-type="programlisting"><code class="n">with_cluster</code> <code class="o">=</code> <code class="n">pipeline_model</code><code class="o">.</code><code class="n">transform</code><code class="p">(</code><code class="n">numeric_only</code><code class="p">)</code>

<code class="n">with_cluster</code><code class="o">.</code><code class="n">select</code><code class="p">(</code><code class="s2">"cluster"</code><code class="p">,</code> <code class="s2">"label"</code><code class="p">)</code><code class="o">.</code><code class="n">groupBy</code><code class="p">(</code><code class="s2">"cluster"</code><code class="p">,</code> <code class="s2">"label"</code><code class="p">)</code><code class="o">.</code><code class="n">count</code><code class="p">()</code><code class="o">.</code>\
              <code class="n">orderBy</code><code class="p">(</code><code class="n">col</code><code class="p">(</code><code class="s2">"cluster"</code><code class="p">),</code> <code class="n">col</code><code class="p">(</code><code class="s2">"count"</code><code class="p">)</code><code class="o">.</code><code class="n">desc</code><code class="p">())</code><code class="o">.</code><code class="n">show</code><code class="p">(</code><code class="mi">25</code><code class="p">)</code>

<code class="o">...</code>
<code class="o">+-------+----------------+-------+</code>
<code class="o">|</code><code class="n">cluster</code><code class="o">|</code>           <code class="n">label</code><code class="o">|</code>  <code class="n">count</code><code class="o">|</code>
<code class="o">+-------+----------------+-------+</code>
<code class="o">|</code>      <code class="mi">0</code><code class="o">|</code>          <code class="n">smurf</code><code class="o">.|</code><code class="mi">2807886</code><code class="o">|</code>
<code class="o">|</code>      <code class="mi">0</code><code class="o">|</code>        <code class="n">neptune</code><code class="o">.|</code><code class="mi">1072017</code><code class="o">|</code>
<code class="o">|</code>      <code class="mi">0</code><code class="o">|</code>         <code class="n">normal</code><code class="o">.|</code> <code class="mi">972781</code><code class="o">|</code>
<code class="o">|</code>      <code class="mi">0</code><code class="o">|</code>          <code class="n">satan</code><code class="o">.|</code>  <code class="mi">15892</code><code class="o">|</code>
<code class="o">|</code>      <code class="mi">0</code><code class="o">|</code>        <code class="n">ipsweep</code><code class="o">.|</code>  <code class="mi">12481</code><code class="o">|</code>
<code class="o">...</code>
<code class="o">|</code>      <code class="mi">0</code><code class="o">|</code>            <code class="n">phf</code><code class="o">.|</code>      <code class="mi">4</code><code class="o">|</code>
<code class="o">|</code>      <code class="mi">0</code><code class="o">|</code>           <code class="n">perl</code><code class="o">.|</code>      <code class="mi">3</code><code class="o">|</code>
<code class="o">|</code>      <code class="mi">0</code><code class="o">|</code>            <code class="n">spy</code><code class="o">.|</code>      <code class="mi">2</code><code class="o">|</code>
<code class="o">|</code>      <code class="mi">1</code><code class="o">|</code>      <code class="n">portsweep</code><code class="o">.|</code>      <code class="mi">1</code><code class="o">|</code>
<code class="o">+-------+----------------+-------+</code></pre>
<p>The result shows that the clustering was not at all helpful. Only one data point ended up in cluster 1!<a data-startref="ch05-first" data-type="indexterm" id="idm46507977432304"/><a data-startref="ch05-first2" data-type="indexterm" id="idm46507977431696"/><a data-startref="ch05-first3" data-type="indexterm" id="idm46507977431088"/></p>
</div></section>
<section data-pdf-bookmark="Choosing k" data-type="sect1"><div class="sect1" id="idm46507977855680">
<h1>Choosing k</h1>
<p>Two clusters are plainly insufficient. <a data-primary="anomaly detection" data-secondary="network traffic anomalies" data-tertiary="choosing k" data-type="indexterm" id="ch05-k"/><a data-primary="K-means clustering" data-secondary="network traffic anomalies" data-tertiary="choosing k" data-type="indexterm" id="ch05-k2"/><a data-primary="network traffic anomaly detection" data-secondary="choosing k" data-type="indexterm" id="ch05-k3"/>How many clusters are appropriate for this dataset? It’s clear that there
are 23 distinct patterns in the data, so it seems that <em>k</em> could be at least 23, or likely even more. Typically,
many values of <em>k</em> are tried to find the best one. But what is “best”?</p>
<p>A clustering could be considered good<a data-primary="centroid of a K-means cluster" data-secondary="clustering considered good" data-type="indexterm" id="idm46507977134624"/><a data-primary="K-means clustering" data-secondary="centroid of cluster" data-tertiary="clustering considered good" data-type="indexterm" id="idm46507977133776"/><a data-primary="Euclidean distance for K-means" data-type="indexterm" id="idm46507977132688"/><a data-primary="K-means clustering" data-secondary="Euclidean distance for" data-type="indexterm" id="idm46507977132080"/> if each data point were near its closest <span class="keep-together">centroid</span>, where “near” is defined by the Euclidean distance. This is a simple, common way to evaluate the quality of a clustering, by the mean of these distances over all points, or sometimes, the mean of the distances squared. <a data-primary="K-means clustering" data-secondary="computeCost method" data-type="indexterm" id="idm46507977130352"/>In fact, <code>KMeansModel</code> offers a <code>ClusteringEvaluator</code> method that computes the sum of squared distances and can easily be used to compute the mean squared distance.</p>
<p class="pagebreak-before">It’s simple enough to manually evaluate the clustering cost for several values of <em>k</em>. Note that this code could take 10 minutes or more to run:</p>
<pre data-code-language="python" data-type="programlisting"><code class="kn">from</code><code> </code><code class="nn">pyspark</code><code class="nn">.</code><code class="nn">sql</code><code> </code><code class="kn">import</code><code> </code><code class="n">DataFrame</code><code>
</code><code class="kn">from</code><code> </code><code class="nn">pyspark</code><code class="nn">.</code><code class="nn">ml</code><code class="nn">.</code><code class="nn">evaluation</code><code> </code><code class="kn">import</code><code> </code><code class="n">ClusteringEvaluator</code><code>
</code><code>
</code><code class="kn">from</code><code> </code><code class="nn">random</code><code> </code><code class="kn">import</code><code> </code><code class="n">randint</code><code>
</code><code>
</code><code class="k">def</code><code> </code><code class="nf">clustering_score</code><code class="p">(</code><code class="n">input_data</code><code class="p">,</code><code> </code><code class="n">k</code><code class="p">)</code><code class="p">:</code><code>
</code><code>    </code><code class="n">input_numeric_only</code><code> </code><code class="o">=</code><code> </code><code class="n">input_data</code><code class="o">.</code><code class="n">drop</code><code class="p">(</code><code class="s2">"</code><code class="s2">protocol_type</code><code class="s2">"</code><code class="p">,</code><code> </code><code class="s2">"</code><code class="s2">service</code><code class="s2">"</code><code class="p">,</code><code> </code><code class="s2">"</code><code class="s2">flag</code><code class="s2">"</code><code class="p">)</code><code>
</code><code>    </code><code class="n">assembler</code><code> </code><code class="o">=</code><code> </code><code class="n">VectorAssembler</code><code class="p">(</code><code class="p">)</code><code class="o">.</code><code class="n">setInputCols</code><code class="p">(</code><code class="n">input_numeric_only</code><code class="o">.</code><code class="n">columns</code><code class="p">[</code><code class="p">:</code><code class="o">-</code><code class="mi">1</code><code class="p">]</code><code class="p">)</code><code class="o">.</code><code>\
</code><code>                                  </code><code class="n">setOutputCol</code><code class="p">(</code><code class="s2">"</code><code class="s2">featureVector</code><code class="s2">"</code><code class="p">)</code><code>
</code><code>    </code><code class="n">kmeans</code><code> </code><code class="o">=</code><code> </code><code class="n">KMeans</code><code class="p">(</code><code class="p">)</code><code class="o">.</code><code class="n">setSeed</code><code class="p">(</code><code class="n">randint</code><code class="p">(</code><code class="mi">100</code><code class="p">,</code><code class="mi">100000</code><code class="p">)</code><code class="p">)</code><code class="o">.</code><code class="n">setK</code><code class="p">(</code><code class="n">k</code><code class="p">)</code><code class="o">.</code><code>\
</code><code>                      </code><code class="n">setPredictionCol</code><code class="p">(</code><code class="s2">"</code><code class="s2">cluster</code><code class="s2">"</code><code class="p">)</code><code class="o">.</code><code>\
</code><code>                      </code><code class="n">setFeaturesCol</code><code class="p">(</code><code class="s2">"</code><code class="s2">featureVector</code><code class="s2">"</code><code class="p">)</code><code>
</code><code>    </code><code class="n">pipeline</code><code> </code><code class="o">=</code><code> </code><code class="n">Pipeline</code><code class="p">(</code><code class="p">)</code><code class="o">.</code><code class="n">setStages</code><code class="p">(</code><code class="p">[</code><code class="n">assembler</code><code class="p">,</code><code> </code><code class="n">kmeans</code><code class="p">]</code><code class="p">)</code><code>
</code><code>    </code><code class="n">pipeline_model</code><code> </code><code class="o">=</code><code> </code><code class="n">pipeline</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">input_numeric_only</code><code class="p">)</code><code>
</code><code>
</code><code>    </code><code class="n">evaluator</code><code> </code><code class="o">=</code><code> </code><code class="n">ClusteringEvaluator</code><code class="p">(</code><code class="n">predictionCol</code><code class="o">=</code><code class="s1">'</code><code class="s1">cluster</code><code class="s1">'</code><code class="p">,</code><code>
</code><code>                                    </code><code class="n">featuresCol</code><code class="o">=</code><code class="s2">"</code><code class="s2">featureVector</code><code class="s2">"</code><code class="p">)</code><code>
</code><code>    </code><code class="n">predictions</code><code> </code><code class="o">=</code><code> </code><code class="n">pipeline_model</code><code class="o">.</code><code class="n">transform</code><code class="p">(</code><code class="n">numeric_only</code><code class="p">)</code><code>
</code><code>    </code><code class="n">score</code><code> </code><code class="o">=</code><code> </code><code class="n">evaluator</code><code class="o">.</code><code class="n">evaluate</code><code class="p">(</code><code class="n">predictions</code><code class="p">)</code><code>
</code><code>    </code><code class="k">return</code><code> </code><code class="n">score</code><code>
</code><code>
</code><code class="k">for</code><code> </code><code class="n">k</code><code> </code><code class="ow">in</code><code> </code><code class="nb">list</code><code class="p">(</code><code class="nb">range</code><code class="p">(</code><code class="mi">20</code><code class="p">,</code><code class="mi">100</code><code class="p">,</code><code> </code><code class="mi">20</code><code class="p">)</code><code class="p">)</code><code class="p">:</code><code>
</code><code>    </code><code class="nb">print</code><code class="p">(</code><code class="n">clustering_score</code><code class="p">(</code><code class="n">numeric_only</code><code class="p">,</code><code> </code><code class="n">k</code><code class="p">)</code><code class="p">)</code><code> </code><a class="co" href="#callout_anomaly_detection__span_class__keep_together__with_k_means_clustering__span__CO1-1" id="co_anomaly_detection__span_class__keep_together__with_k_means_clustering__span__CO1-1"><img alt="1" height="12" src="assets/1.png" width="12"/></a><code>
</code><code>
</code><code class="o">.</code><code class="o">.</code><code class="o">.</code><code>
</code><code class="p">(</code><code class="mi">20</code><code class="p">,</code><code class="mf">6.649218115128446E7</code><code class="p">)</code><code>
</code><code class="p">(</code><code class="mi">40</code><code class="p">,</code><code class="mf">2.5031424366033625E7</code><code class="p">)</code><code>
</code><code class="p">(</code><code class="mi">60</code><code class="p">,</code><code class="mf">1.027261913057096E7</code><code class="p">)</code><code>
</code><code class="p">(</code><code class="mi">80</code><code class="p">,</code><code class="mf">1.2514131711109027E7</code><code class="p">)</code><code>
</code><code class="p">(</code><code class="mi">100</code><code class="p">,</code><code class="mf">7235531.565096531</code><code class="p">)</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_anomaly_detection__span_class__keep_together__with_k_means_clustering__span__CO1-1" id="callout_anomaly_detection__span_class__keep_together__with_k_means_clustering__span__CO1-1"><img alt="1" height="12" src="assets/1.png" width="12"/></a></dt>
<dd><p>Scores will be shown here using scientific notation.</p></dd>
</dl>
<p>The printed result shows that the score decreases as <em>k</em> increases. Note that scores are shown in scientific notation; the first value is over 10<sup>7</sup>, not just a bit over 6.</p>
<div data-type="note" epub:type="note">
<p>Again, your values will be somewhat different. The clustering depends on a randomly chosen initial set of centroids.</p>
</div>
<p>However, this much is obvious. As more clusters are added, it should always be possible to put data points closer to the nearest centroid. In fact, if <em>k</em> is chosen to equal the number of data points, the average distance will be 0 because every point will be its own cluster of one!</p>
<p>Worse, in the preceding results, the distance for <em>k</em>=80 is higher than for <em>k</em>=60. This shouldn’t happen because a higher <em>k</em> always permits at least as good a clustering as a lower <em>k</em>. The problem is that K-means is not necessarily able to find the optimal clustering for a given <em>k</em>. Its iterative process can converge from a random starting point to a local minimum, which may be good but is not optimal.</p>
<p>This is still true even when more intelligent methods are used to choose initial centroids.<a data-primary="online resources" data-secondary="K-means++ and K-means||" data-type="indexterm" id="idm46507976923056"/><a data-primary="centroid of a K-means cluster" data-secondary="K-means variant algorithms" data-type="indexterm" id="idm46507976922080"/><a data-primary="K-means clustering" data-secondary="centroid of cluster" data-tertiary="K-means variant algorithms" data-type="indexterm" id="idm46507976921200"/> <a href="https://oreil.ly/zes8d">K-means++ and K-means||</a> are variants of selection algorithms that are more likely to choose diverse, separated centroids and lead more reliably to good clustering. <a data-primary="MLlib component of Spark" data-secondary="K-means clustering" data-type="indexterm" id="idm46507976919040"/>Spark MLlib, in
fact, implements K-means||. <span class="keep-together">However,</span> all
still have an element of randomness in selection and can’t guarantee an optimal clustering.</p>
<p>The random starting set of clusters chosen for <em>k</em>=80 perhaps led to a particularly suboptimal clustering, or it
may have stopped early before it reached its local <span class="keep-together">optimum.</span></p>
<p>We can improve it by running the iteration longer.
The algorithm has a threshold via <code>setTol</code> that controls the minimum amount of cluster centroid movement considered significant; lower values mean the K-means algorithm will let the centroids continue to move longer. Increasing the maximum number of iterations with <code>setMaxIter</code> also prevents it from potentially stopping too early at the cost of possibly
more computation.</p>
<pre data-code-language="python" data-type="programlisting"><code class="k">def</code><code> </code><code class="nf">clustering_score_1</code><code class="p">(</code><code class="n">input_data</code><code class="p">,</code><code> </code><code class="n">k</code><code class="p">)</code><code class="p">:</code><code>
</code><code>    </code><code class="n">input_numeric_only</code><code> </code><code class="o">=</code><code> </code><code class="n">input_data</code><code class="o">.</code><code class="n">drop</code><code class="p">(</code><code class="s2">"</code><code class="s2">protocol_type</code><code class="s2">"</code><code class="p">,</code><code> </code><code class="s2">"</code><code class="s2">service</code><code class="s2">"</code><code class="p">,</code><code> </code><code class="s2">"</code><code class="s2">flag</code><code class="s2">"</code><code class="p">)</code><code>
</code><code>    </code><code class="n">assembler</code><code> </code><code class="o">=</code><code> </code><code class="n">VectorAssembler</code><code class="p">(</code><code class="p">)</code><code class="o">.</code><code>\
</code><code>                  </code><code class="n">setInputCols</code><code class="p">(</code><code class="n">input_numeric_only</code><code class="o">.</code><code class="n">columns</code><code class="p">[</code><code class="p">:</code><code class="o">-</code><code class="mi">1</code><code class="p">]</code><code class="p">)</code><code class="o">.</code><code>\
</code><code>                  </code><code class="n">setOutputCol</code><code class="p">(</code><code class="s2">"</code><code class="s2">featureVector</code><code class="s2">"</code><code class="p">)</code><code>
</code><code>    </code><code class="n">kmeans</code><code> </code><code class="o">=</code><code> </code><code class="n">KMeans</code><code class="p">(</code><code class="p">)</code><code class="o">.</code><code class="n">setSeed</code><code class="p">(</code><code class="n">randint</code><code class="p">(</code><code class="mi">100</code><code class="p">,</code><code class="mi">100000</code><code class="p">)</code><code class="p">)</code><code class="o">.</code><code class="n">setK</code><code class="p">(</code><code class="n">k</code><code class="p">)</code><code class="o">.</code><code class="n">setMaxIter</code><code class="p">(</code><code class="mi">40</code><code class="p">)</code><code class="o">.</code><code>\</code><code> </code><a class="co" href="#callout_anomaly_detection__span_class__keep_together__with_k_means_clustering__span__CO2-1" id="co_anomaly_detection__span_class__keep_together__with_k_means_clustering__span__CO2-1"><img alt="1" height="12" src="assets/1.png" width="12"/></a><code>
</code><code>      </code><code class="n">setTol</code><code class="p">(</code><code class="mf">1.0e-5</code><code class="p">)</code><code class="o">.</code><code>\</code><code> </code><a class="co" href="#callout_anomaly_detection__span_class__keep_together__with_k_means_clustering__span__CO2-2" id="co_anomaly_detection__span_class__keep_together__with_k_means_clustering__span__CO2-2"><img alt="2" height="12" src="assets/2.png" width="12"/></a><code>
</code><code>      </code><code class="n">setPredictionCol</code><code class="p">(</code><code class="s2">"</code><code class="s2">cluster</code><code class="s2">"</code><code class="p">)</code><code class="o">.</code><code class="n">setFeaturesCol</code><code class="p">(</code><code class="s2">"</code><code class="s2">featureVector</code><code class="s2">"</code><code class="p">)</code><code>
</code><code>    </code><code class="n">pipeline</code><code> </code><code class="o">=</code><code> </code><code class="n">Pipeline</code><code class="p">(</code><code class="p">)</code><code class="o">.</code><code class="n">setStages</code><code class="p">(</code><code class="p">[</code><code class="n">assembler</code><code class="p">,</code><code> </code><code class="n">kmeans</code><code class="p">]</code><code class="p">)</code><code>
</code><code>    </code><code class="n">pipeline_model</code><code> </code><code class="o">=</code><code> </code><code class="n">pipeline</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">input_numeric_only</code><code class="p">)</code><code>
</code><code>    </code><code class="c1">#</code><code>
</code><code>    </code><code class="n">evaluator</code><code> </code><code class="o">=</code><code> </code><code class="n">ClusteringEvaluator</code><code class="p">(</code><code class="n">predictionCol</code><code class="o">=</code><code class="s1">'</code><code class="s1">cluster</code><code class="s1">'</code><code class="p">,</code><code>
</code><code>                                    </code><code class="n">featuresCol</code><code class="o">=</code><code class="s2">"</code><code class="s2">featureVector</code><code class="s2">"</code><code class="p">)</code><code>
</code><code>    </code><code class="n">predictions</code><code> </code><code class="o">=</code><code> </code><code class="n">pipeline_model</code><code class="o">.</code><code class="n">transform</code><code class="p">(</code><code class="n">numeric_only</code><code class="p">)</code><code>
</code><code>    </code><code class="n">score</code><code> </code><code class="o">=</code><code> </code><code class="n">evaluator</code><code class="o">.</code><code class="n">evaluate</code><code class="p">(</code><code class="n">predictions</code><code class="p">)</code><code>
</code><code>    </code><code class="c1">#</code><code>
</code><code>    </code><code class="k">return</code><code> </code><code class="n">score</code><code>
</code><code>
</code><code>
</code><code class="k">for</code><code> </code><code class="n">k</code><code> </code><code class="ow">in</code><code> </code><code class="nb">list</code><code class="p">(</code><code class="nb">range</code><code class="p">(</code><code class="mi">20</code><code class="p">,</code><code class="mi">101</code><code class="p">,</code><code> </code><code class="mi">20</code><code class="p">)</code><code class="p">)</code><code class="p">:</code><code>
</code><code>    </code><code class="nb">print</code><code class="p">(</code><code class="n">k</code><code class="p">,</code><code> </code><code class="n">clustering_score_1</code><code class="p">(</code><code class="n">numeric_only</code><code class="p">,</code><code> </code><code class="n">k</code><code class="p">)</code><code class="p">)</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_anomaly_detection__span_class__keep_together__with_k_means_clustering__span__CO2-1" id="callout_anomaly_detection__span_class__keep_together__with_k_means_clustering__span__CO2-1"><img alt="1" height="12" src="assets/1.png" width="12"/></a></dt>
<dd><p>Increase from default 20.</p></dd>
<dt><a class="co" href="#co_anomaly_detection__span_class__keep_together__with_k_means_clustering__span__CO2-2" id="callout_anomaly_detection__span_class__keep_together__with_k_means_clustering__span__CO2-2"><img alt="2" height="12" src="assets/2.png" width="12"/></a></dt>
<dd><p>Decrease from default 1.0e-4.</p></dd>
</dl>
<p>This time, at least the scores decrease consistently:</p>
<pre data-type="programlisting">(20,1.8041795813813403E8)
(40,6.33056876207124E7)
(60,9474961.544965891)
(80,9388117.93747141)
(100,8783628.926311461)</pre>
<p>We want to find a point past which increasing <em>k</em> stops reducing the score much—or an “elbow” in a graph of
<em>k</em> versus score, which is generally decreasing but eventually flattens out. Here, it seems to be
decreasing notably past 100. The right value of <em>k</em> may be past 100.<a data-startref="ch05-k" data-type="indexterm" id="idm46507976574464"/><a data-startref="ch05-k2" data-type="indexterm" id="idm46507976573728"/><a data-startref="ch05-k3" data-type="indexterm" id="idm46507976573056"/></p>
</div></section>
<section data-pdf-bookmark="Visualization with SparkR" data-type="sect1"><div class="sect1" id="idm46507977164144">
<h1>Visualization with SparkR</h1>
<p>At this point, it could be useful to step back<a data-primary="R" data-secondary="visualization of K-means clusters" data-type="indexterm" id="ch05-vis"/><a data-primary="visualization of K-means clusters" data-type="indexterm" id="ch05-vis2"/><a data-primary="K-means clustering" data-secondary="visualization of clusters" data-type="indexterm" id="ch05-vis3"/><a data-primary="network traffic anomaly detection" data-secondary="visualization of clusters" data-type="indexterm" id="ch05-vis4"/><a data-primary="anomaly detection" data-secondary="visualization of clusters" data-type="indexterm" id="ch05-vis5"/> and understand more about the data before clustering again.
In particular, looking at a plot of the data points could be helpful.</p>
<p>Spark itself has no tools for visualization, but the popular open source statistical environment <a href="https://www.r-project.org">R</a> has libraries
for both data exploration and data visualization. <a data-primary="R" data-secondary="SparkR integration with" data-type="indexterm" id="ch05-R"/><a data-primary="SparkR integration with R" data-type="indexterm" id="ch05-R2"/>Furthermore, Spark also provides some basic
integration with R via <a href="https://oreil.ly/XX0Q9">SparkR</a>. This brief section will demonstrate using R and SparkR to
cluster the data and explore the clustering.</p>
<p>SparkR is a variant of the <code>spark-shell</code> used throughout this book and is invoked with the
command <code>sparkR</code>. It runs a local R interpreter, like <code>spark-shell</code> runs a variant of the
Scala shell as a local process. <a data-primary="SparkR integration with R" data-secondary="installing SparkR" data-type="indexterm" id="idm46507976531536"/><a data-primary="installing SparkR" data-type="indexterm" id="idm46507976530688"/><a data-primary="online resources" data-secondary="SparkR installer" data-type="indexterm" id="idm46507976530080"/>The machine that runs <code>sparkR</code> needs a local installation of R,
which is not included with Spark. This can be installed, for example, with <code>sudo apt-get install r-base</code>
on Linux distributions like Ubuntu, or <code>brew install R</code> with <a href="http://brew.sh">Homebrew</a> on macOS.</p>
<p>SparkR is a command-line shell environment, like R. <a data-primary="R" data-secondary="RStudio IDE" data-type="indexterm" id="idm46507976526944"/><a data-primary="RStudio IDE for R" data-type="indexterm" id="idm46507976526096"/><a data-primary="SparkR integration with R" data-secondary="RStudio IDE" data-type="indexterm" id="idm46507976525488"/><a data-primary="online resources" data-secondary="SparkR installer" data-tertiary="RStudio IDE" data-type="indexterm" id="idm46507976524640"/>To view visualizations, it’s necessary to
run these commands within an IDE-like environment that can display images. <a href="https://www.rstudio.com">RStudio</a>
is an IDE for R (and works with SparkR); it runs on a desktop operating system so it will be usable here only if you are experimenting with Spark locally rather than on a cluster.</p>
<p>If you are running Spark locally, <a href="https://oreil.ly/JZGQm">download</a> the free version of RStudio and install it. If not, then most of the rest of this example can still be run with <code>sparkR</code> on a command line—for example, on a cluster—though it won’t be possible to display visualizations this way.</p>
<p>If you’re running via RStudio, launch the IDE and configure <code>SPARK_HOME</code> and <code>JAVA_HOME</code>, if your local environment does not already set them, to point to the Spark and JDK installation directories, respectively:</p>
<pre class="pagebreak-before" data-code-language="rconsole" data-type="programlisting"><code class="go">Sys.setenv(SPARK_HOME = "</code><em><code class="go">/path/to/spark</code></em><code class="go">") </code><a class="co" href="#c01a" id="comarker1a"><img alt="1" height="12" src="assets/1.png" width="12"/></a><code class="go">
</code><code class="go">Sys.setenv(JAVA_HOME = "</code><em><code class="go">/path/to/java</code></em><code class="go">")
</code><code class="go">library(SparkR, lib.loc = c(file.path(Sys.getenv("SPARK_HOME"), "R", "lib")))
</code><code class="go">sparkR.session(master = "local[*]",
</code><code class="go">  sparkConfig = list(spark.driver.memory = "4g"))</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#comarker1a" id="c01a"><img alt="1" height="12" src="assets/1.png" width="12"/></a></dt>
<dd><p>Replace with actual paths, of course.</p></dd>
</dl>
<p>Note that these steps aren’t needed if you are running <code>sparkR</code> on the command line. Instead, it accepts command-line configuration parameters such as <code>--driver-memory</code>, just like <code>spark-shell</code>.</p>
<p>SparkR is an R-language wrapper around the same DataFrame and MLlib APIs that have been demonstrated in this chapter. It’s therefore possible to re-create a K-means simple clustering of the data:</p>
<pre data-code-language="rconsole" data-type="programlisting"><code class="go">clusters_data &lt;- read.df("</code><em><code class="go">/path/to/kddcup.data</code></em><code class="go">", "csv", </code><a class="co" href="#c01b" id="comarker1b"><img alt="1" height="12" src="assets/1.png" width="12"/></a><code class="go">
</code><code class="go">                         inferSchema = "true", header = "false")
</code><code class="go">colnames(clusters_data) &lt;- c( </code><a class="co" href="#c02" id="comarker2"><img alt="2" height="12" src="assets/2.png" width="12"/></a><code class="go">
</code><code class="go">  "duration", "protocol_type", "service", "flag",
</code><code class="go">  "src_bytes", "dst_bytes", "land", "wrong_fragment", "urgent",
</code><code class="go">  "hot", "num_failed_logins", "logged_in", "num_compromised",
</code><code class="go">  "root_shell", "su_attempted", "num_root", "num_file_creations",
</code><code class="go">  "num_shells", "num_access_files", "num_outbound_cmds",
</code><code class="go">  "is_host_login", "is_guest_login", "count", "srv_count",
</code><code class="go">  "serror_rate", "srv_serror_rate", "rerror_rate", "srv_rerror_rate",
</code><code class="go">  "same_srv_rate", "diff_srv_rate", "srv_diff_host_rate",
</code><code class="go">  "dst_host_count", "dst_host_srv_count",
</code><code class="go">  "dst_host_same_srv_rate", "dst_host_diff_srv_rate",
</code><code class="go">  "dst_host_same_src_port_rate", "dst_host_srv_diff_host_rate",
</code><code class="go">  "dst_host_serror_rate", "dst_host_srv_serror_rate",
</code><code class="go">  "dst_host_rerror_rate", "dst_host_srv_rerror_rate",
</code><code class="go">  "label")
</code><code class="go">
</code><code class="go">numeric_only &lt;- cache(drop(clusters_data, </code><a class="co" href="#c03" id="comarker3"><img alt="3" height="12" src="assets/3.png" width="12"/></a><code class="go">
</code><code class="go">                           c("protocol_type", "service", "flag", "label")))
</code><code class="go">
</code><code class="go">kmeans_model &lt;- spark.kmeans(numeric_only, ~ ., </code><a class="co" href="#c04" id="comarker4"><img alt="4" height="12" src="assets/4.png" width="12"/></a><code class="go">
</code><code class="go">                             k = 100, maxIter = 40, initMode = "k-means||")</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#comarker1b" id="c01b"><img alt="1" height="12" src="assets/1.png" width="12"/></a></dt>
<dd><p>Replace with path to <em>kddcup.data</em>.</p></dd>
<dt><a class="co" href="#comarker2" id="c02"><img alt="2" height="12" src="assets/2.png" width="12"/></a></dt>
<dd><p>Name columns.</p></dd>
<dt><a class="co" href="#comarker3" id="c03"><img alt="3" height="12" src="assets/3.png" width="12"/></a></dt>
<dd><p>Drop nonnumeric columns again.</p></dd>
<dt><a class="co" href="#comarker4" id="c04"><img alt="4" height="12" src="assets/4.png" width="12"/></a></dt>
<dd><p><code>~ .</code> means all columns.</p></dd>
</dl>
<p class="pagebreak-before">From here, it’s straightforward to assign a cluster to each data point. The operations above
show usage of the SparkR APIs which naturally correspond to core Spark APIs, but are expressed
as R libraries in R-like syntax. The actual clustering is executed using the same
JVM-based, Scala language implementation in MLlib. These operations are effectively a <em>handle</em>,
or remote control, to distributed operations that are not executing in R.</p>
<p>R has its own rich set of libraries for analysis and its own similar concept of a dataframe. It is sometimes useful, therefore, to pull some data down into the R interpreter to be able to use these native R libraries, which are unrelated to
Spark.</p>
<p>Of course, R and its libraries are not distributed,<a data-primary="R" data-secondary="visualization of K-means clusters" data-tertiary="R not distributed" data-type="indexterm" id="idm46507976398256"/> and so it’s not feasible to pull the whole dataset of 4,898,431 data points into R. However, it’s easy to pull only a sample:</p>
<pre data-code-language="rconsole" data-type="programlisting"><code class="go">clustering &lt;- predict(kmeans_model, numeric_only)
</code><code class="go">clustering_sample &lt;- collect(sample(clustering, FALSE, 0.01)) </code><a class="co" href="#callout_anomaly_detection__span_class__keep_together__with_k_means_clustering__span__CO3-1" id="co_anomaly_detection__span_class__keep_together__with_k_means_clustering__span__CO3-1"><img alt="1" height="12" src="assets/1.png" width="12"/></a><code class="go">
</code><code class="go">
</code><code class="go">str(clustering_sample)
</code><code class="go">
</code><code class="go">...
</code><code class="go">'data.frame': 48984 obs. of  39 variables:
</code><code class="go"> $ duration                   : int  0 0 0 0 0 0 0 0 0 0 ...
</code><code class="go"> $ src_bytes                  : int  181 185 162 254 282 310 212 214 181 ...
</code><code class="go"> $ dst_bytes                  : int  5450 9020 4528 849 424 1981 2917 3404 ...
</code><code class="go"> $ land                       : int  0 0 0 0 0 0 0 0 0 0 ...
</code><code class="go">...
</code><code class="go"> $ prediction                 : int  33 33 33 0 0 0 0 0 33 33 ...</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_anomaly_detection__span_class__keep_together__with_k_means_clustering__span__CO3-1" id="callout_anomaly_detection__span_class__keep_together__with_k_means_clustering__span__CO3-1"><img alt="1" height="12" src="assets/1.png" width="12"/></a></dt>
<dd><p>1% sample without replacement</p></dd>
</dl>
<p><code>clustering_sample</code> is actually a local R dataframe, not a Spark DataFrame, so it can
be manipulated like any other data in R. Above, <code>str</code> shows the structure of the dataframe.</p>
<p>For example, it’s possible to extract the cluster assignment and then show statistics about
the distribution of assignments:</p>
<pre data-code-language="rconsole" data-type="programlisting"><code class="go">clusters &lt;- clustering_sample["prediction"] </code><a class="co" href="#callout_anomaly_detection__span_class__keep_together__with_k_means_clustering__span__CO4-1" id="co_anomaly_detection__span_class__keep_together__with_k_means_clustering__span__CO4-1"><img alt="1" height="12" src="assets/1.png" width="12"/></a><code class="go">
</code><code class="go">data &lt;- data.matrix(within(clustering_sample, rm("prediction"))) </code><a class="co" href="#callout_anomaly_detection__span_class__keep_together__with_k_means_clustering__span__CO4-2" id="co_anomaly_detection__span_class__keep_together__with_k_means_clustering__span__CO4-2"><img alt="2" height="12" src="assets/2.png" width="12"/></a><code class="go">
</code><code class="go">
</code><code class="go">table(clusters)
</code><code class="go">
</code><code class="go">...
</code><code class="go">clusters
</code><code class="go">    0    11    14    18    23    25    28    30    31    33    36    ...
</code><code class="go">47294     3     1     2     2   308   105     1    27  1219    15    ...</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_anomaly_detection__span_class__keep_together__with_k_means_clustering__span__CO4-1" id="callout_anomaly_detection__span_class__keep_together__with_k_means_clustering__span__CO4-1"><img alt="1" height="12" src="assets/1.png" width="12"/></a></dt>
<dd><p>Only the clustering assignment column</p></dd>
<dt><a class="co" href="#co_anomaly_detection__span_class__keep_together__with_k_means_clustering__span__CO4-2" id="callout_anomaly_detection__span_class__keep_together__with_k_means_clustering__span__CO4-2"><img alt="2" height="12" src="assets/2.png" width="12"/></a></dt>
<dd><p>Everything but the clustering assignment</p></dd>
</dl>
<p>For example, this shows that most points fell into cluster 0. Although much more could
be done with this data in R, further coverage of this is beyond the scope of this book.</p>
<p>To visualize the data, a library<a data-primary="visualization of K-means clusters" data-secondary="rgl library" data-type="indexterm" id="idm46507976334992"/><a data-primary="RStudio IDE for R" data-secondary="rgl visualization library requiring" data-type="indexterm" id="idm46507976334048"/><a data-primary="R" data-secondary="RStudio IDE" data-tertiary="rgl visualization library requiring" data-type="indexterm" id="idm46507976333088"/> called <code>rgl</code> is required. It will be functional only if
running this example in RStudio. First, install (once) and load the library:</p>
<pre data-code-language="rconsole" data-type="programlisting"><code class="go">install.packages("rgl")</code>
<code class="go">library(rgl)</code></pre>
<p>Note that R may prompt you to download other packages or compiler tools to complete installation,
because installing the package means compiling its source code.</p>
<p>This dataset is 38-dimensional.<a data-primary="random projection visualization of K-means clusters" data-type="indexterm" id="idm46507976302432"/><a data-primary="visualization of K-means clusters" data-secondary="random projection" data-type="indexterm" id="idm46507976302048"/>
It will have to be projected down into at most three dimensions to visualize it with a <em>random projection</em>:</p>
<pre data-code-language="rconsole" data-type="programlisting"><code class="go">random_projection &lt;- matrix(data = rnorm(3*ncol(data)), ncol = 3) </code><a class="co" href="#callout_anomaly_detection__span_class__keep_together__with_k_means_clustering__span__CO5-1" id="co_anomaly_detection__span_class__keep_together__with_k_means_clustering__span__CO5-1"><img alt="1" height="12" src="assets/1.png" width="12"/></a><code class="go">
</code><code class="go">random_projection_norm &lt;-
</code><code class="go">  random_projection / sqrt(rowSums(random_projection*random_projection))
</code><code class="go">
</code><code class="go">projected_data &lt;- data.frame(data %*% random_projection_norm) </code><a class="co" href="#callout_anomaly_detection__span_class__keep_together__with_k_means_clustering__span__CO5-2" id="co_anomaly_detection__span_class__keep_together__with_k_means_clustering__span__CO5-2"><img alt="2" height="12" src="assets/2.png" width="12"/></a></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_anomaly_detection__span_class__keep_together__with_k_means_clustering__span__CO5-1" id="callout_anomaly_detection__span_class__keep_together__with_k_means_clustering__span__CO5-1"><img alt="1" height="12" src="assets/1.png" width="12"/></a></dt>
<dd><p>Make a random 3-D projection and normalize.</p></dd>
<dt><a class="co" href="#co_anomaly_detection__span_class__keep_together__with_k_means_clustering__span__CO5-2" id="callout_anomaly_detection__span_class__keep_together__with_k_means_clustering__span__CO5-2"><img alt="2" height="12" src="assets/2.png" width="12"/></a></dt>
<dd><p>Project and make a new dataframe.</p></dd>
</dl>
<p>This creates a 3-D dataset out of a 38-D dataset by choosing three random unit vectors and
projecting the data onto them. This is a simplistic, rough-and-ready form of
dimension reduction. Of course, there are more sophisticated dimension reduction algorithms,
like principal component analysis or the singular value decomposition. These are available in R but take much longer to run. For purposes of visualization in this example,
a random projection achieves much the same result, faster.</p>
<p>Finally, the clustered points can be plotted<a data-primary="visualization of K-means clusters" data-secondary="interactive 3-D visualization" data-type="indexterm" id="idm46507976256528"/><a data-primary="interactive 3-D visualization of clusters" data-type="indexterm" id="idm46507976255616"/><a data-primary="3-D visualization of clusters" data-type="indexterm" id="idm46507976239328"/><a data-primary="3-D visualization of clusters" data-primary-sortas="three-D visualization" data-type="indexterm" id="idm46507976238720"/> in an interactive 3-D visualization:</p>
<pre data-code-language="rconsole" data-type="programlisting"><code class="go">num_clusters &lt;- max(clusters)</code>
<code class="go">palette &lt;- rainbow(num_clusters)</code>
<code class="go">colors = sapply(clusters, function(c) palette[c])</code>
<code class="go">plot3d(projected_data, col = colors, size = 10)</code></pre>
<p>Note that this will require running RStudio in an environment that supports the <code>rgl</code> library and graphics.</p>
<p>The resulting visualization in <a data-type="xref" href="#AnomalyDetection_Projection1">Figure 5-1</a> shows data points
in 3-D space. Many points fall on top of
one another, and the result is sparse and hard to interpret. However, the dominant feature
of the visualization is its L shape. The points seem to vary along two distinct dimensions, and little in other
dimensions.</p>
<p>This makes sense because the dataset has two features that are on a much larger scale than the others.
Whereas most features have values between 0 and 1, the bytes-sent and bytes-received features vary from 0 to tens of
thousands. The Euclidean distance between points is therefore almost completely determined by these two features.
It’s almost as if the other features don’t exist! So it’s important to normalize away these differences
in scale to put features on near-equal footing.<a data-startref="ch05-R" data-type="indexterm" id="idm46507976230736"/><a data-startref="ch05-R2" data-type="indexterm" id="idm46507976230128"/><a data-startref="ch05-vis" data-type="indexterm" id="idm46507976229520"/><a data-startref="ch05-vis2" data-type="indexterm" id="idm46507976228912"/><a data-startref="ch05-vis3" data-type="indexterm" id="idm46507976228272"/><a data-startref="ch05-vis4" data-type="indexterm" id="idm46507976227600"/><a data-startref="ch05-vis5" data-type="indexterm" id="idm46507976226928"/></p>
<figure class="width-75"><div class="figure" id="AnomalyDetection_Projection1">
<img alt="aaps 0501" height="902" src="assets/aaps_0501.png" width="1050"/>
<h6><span class="label">Figure 5-1. </span>Random 3-D projection</h6>
</div></figure>
</div></section>
<section data-pdf-bookmark="Feature Normalization" data-type="sect1"><div class="sect1" id="idm46507976571472">
<h1>Feature Normalization</h1>
<p><a data-primary="anomaly detection" data-secondary="visualization of clusters" data-tertiary="feature normalization" data-type="indexterm" id="idm46507976222848"/><a data-primary="feature normalization" data-type="indexterm" id="idm46507976221632"/><a data-primary="K-means clustering" data-secondary="visualization of clusters" data-tertiary="feature normalization" data-type="indexterm" id="idm46507976199744"/><a data-primary="network traffic anomaly detection" data-secondary="visualization of clusters" data-tertiary="feature normalization" data-type="indexterm" id="idm46507976198656"/><a data-primary="normalization of features" data-type="indexterm" id="idm46507976197568"/><a data-primary="datasets" data-secondary="normalization of features" data-type="indexterm" id="idm46507976196960"/><a data-primary="visualization of K-means clusters" data-secondary="feature normalization" data-type="indexterm" id="idm46507976196112"/>We can normalize each feature by converting it to a standard score.
This means subtracting the mean of the feature’s values from each value and dividing by the standard deviation,
as shown in the standard score equation:</p>
<div data-type="equation">
<math alttext="n o r m a l i z e d Subscript i Baseline equals StartFraction f e a t u r e Subscript i Baseline minus mu Subscript i Baseline Over sigma Subscript i Baseline EndFraction" display="block">
<mrow>
<mi>n</mi>
<mi>o</mi>
<mi>r</mi>
<mi>m</mi>
<mi>a</mi>
<mi>l</mi>
<mi>i</mi>
<mi>z</mi>
<mi>e</mi>
<msub><mi>d</mi> <mi>i</mi> </msub>
<mo>=</mo>
<mfrac><mrow><mi>f</mi><mi>e</mi><mi>a</mi><mi>t</mi><mi>u</mi><mi>r</mi><msub><mi>e</mi> <mi>i</mi> </msub><mo>-</mo><msub><mi>μ</mi> <mi>i</mi> </msub></mrow> <msub><mi>σ</mi> <mi>i</mi> </msub></mfrac>
</mrow>
</math>
</div>
<p>In fact, subtracting means has no effect on the clustering because the subtraction effectively shifts all the
data points by the same amount in the same direction. This does not affect interpoint Euclidean distances.</p>
<p><a data-primary="MLlib component of Spark" data-secondary="StandardScaler" data-type="indexterm" id="idm46507976180768"/><a data-primary="StandardScaler" data-type="indexterm" id="idm46507976179792"/>MLlib provides <code>StandardScaler</code>, a component that can perform this kind of <span class="keep-together">standardization</span> and be easily added to the clustering pipeline.</p>
<p>We can run the same test with normalized data on a higher range of <em>k</em>:</p>
<pre data-code-language="python" data-type="programlisting"><code class="kn">from</code> <code class="nn">pyspark.ml.feature</code> <code class="kn">import</code> <code class="n">StandardScaler</code>

<code class="k">def</code> <code class="nf">clustering_score_2</code><code class="p">(</code><code class="n">input_data</code><code class="p">,</code> <code class="n">k</code><code class="p">):</code>
    <code class="n">input_numeric_only</code> <code class="o">=</code> <code class="n">input_data</code><code class="o">.</code><code class="n">drop</code><code class="p">(</code><code class="s2">"protocol_type"</code><code class="p">,</code> <code class="s2">"service"</code><code class="p">,</code> <code class="s2">"flag"</code><code class="p">)</code>
    <code class="n">assembler</code> <code class="o">=</code> <code class="n">VectorAssembler</code><code class="p">()</code><code class="o">.</code>\
                <code class="n">setInputCols</code><code class="p">(</code><code class="n">input_numeric_only</code><code class="o">.</code><code class="n">columns</code><code class="p">[:</code><code class="o">-</code><code class="mi">1</code><code class="p">])</code><code class="o">.</code>\
                <code class="n">setOutputCol</code><code class="p">(</code><code class="s2">"featureVector"</code><code class="p">)</code>
    <code class="n">scaler</code> <code class="o">=</code> <code class="n">StandardScaler</code><code class="p">()</code><code class="o">.</code><code class="n">setInputCol</code><code class="p">(</code><code class="s2">"featureVector"</code><code class="p">)</code><code class="o">.</code>\
                              <code class="n">setOutputCol</code><code class="p">(</code><code class="s2">"scaledFeatureVector"</code><code class="p">)</code><code class="o">.</code>\
                              <code class="n">setWithStd</code><code class="p">(</code><code class="kc">True</code><code class="p">)</code><code class="o">.</code><code class="n">setWithMean</code><code class="p">(</code><code class="kc">False</code><code class="p">)</code>
    <code class="n">kmeans</code> <code class="o">=</code> <code class="n">KMeans</code><code class="p">()</code><code class="o">.</code><code class="n">setSeed</code><code class="p">(</code><code class="n">randint</code><code class="p">(</code><code class="mi">100</code><code class="p">,</code><code class="mi">100000</code><code class="p">))</code><code class="o">.</code>\
                      <code class="n">setK</code><code class="p">(</code><code class="n">k</code><code class="p">)</code><code class="o">.</code><code class="n">setMaxIter</code><code class="p">(</code><code class="mi">40</code><code class="p">)</code><code class="o">.</code>\
                      <code class="n">setTol</code><code class="p">(</code><code class="mf">1.0e-5</code><code class="p">)</code><code class="o">.</code><code class="n">setPredictionCol</code><code class="p">(</code><code class="s2">"cluster"</code><code class="p">)</code><code class="o">.</code>\
                      <code class="n">setFeaturesCol</code><code class="p">(</code><code class="s2">"scaledFeatureVector"</code><code class="p">)</code>
    <code class="n">pipeline</code> <code class="o">=</code> <code class="n">Pipeline</code><code class="p">()</code><code class="o">.</code><code class="n">setStages</code><code class="p">([</code><code class="n">assembler</code><code class="p">,</code> <code class="n">scaler</code><code class="p">,</code> <code class="n">kmeans</code><code class="p">])</code>
    <code class="n">pipeline_model</code> <code class="o">=</code> <code class="n">pipeline</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">input_numeric_only</code><code class="p">)</code>
    <code class="c1">#</code>
    <code class="n">evaluator</code> <code class="o">=</code> <code class="n">ClusteringEvaluator</code><code class="p">(</code><code class="n">predictionCol</code><code class="o">=</code><code class="s1">'cluster'</code><code class="p">,</code>
                                    <code class="n">featuresCol</code><code class="o">=</code><code class="s2">"scaledFeatureVector"</code><code class="p">)</code>
    <code class="n">predictions</code> <code class="o">=</code> <code class="n">pipeline_model</code><code class="o">.</code><code class="n">transform</code><code class="p">(</code><code class="n">numeric_only</code><code class="p">)</code>
    <code class="n">score</code> <code class="o">=</code> <code class="n">evaluator</code><code class="o">.</code><code class="n">evaluate</code><code class="p">(</code><code class="n">predictions</code><code class="p">)</code>
    <code class="c1">#</code>
    <code class="k">return</code> <code class="n">score</code>

<code class="k">for</code> <code class="n">k</code> <code class="ow">in</code> <code class="nb">list</code><code class="p">(</code><code class="nb">range</code><code class="p">(</code><code class="mi">60</code><code class="p">,</code> <code class="mi">271</code><code class="p">,</code> <code class="mi">30</code><code class="p">)):</code>
    <code class="nb">print</code><code class="p">(</code><code class="n">k</code><code class="p">,</code> <code class="n">clustering_score_2</code><code class="p">(</code><code class="n">numeric_only</code><code class="p">,</code> <code class="n">k</code><code class="p">))</code>
<code class="o">...</code>
<code class="p">(</code><code class="mi">60</code><code class="p">,</code><code class="mf">1.2454250178069293</code><code class="p">)</code>
<code class="p">(</code><code class="mi">90</code><code class="p">,</code><code class="mf">0.7767730051608682</code><code class="p">)</code>
<code class="p">(</code><code class="mi">120</code><code class="p">,</code><code class="mf">0.5070473497003614</code><code class="p">)</code>
<code class="p">(</code><code class="mi">150</code><code class="p">,</code><code class="mf">0.4077081720067704</code><code class="p">)</code>
<code class="p">(</code><code class="mi">180</code><code class="p">,</code><code class="mf">0.3344486714980788</code><code class="p">)</code>
<code class="p">(</code><code class="mi">210</code><code class="p">,</code><code class="mf">0.276237617334138</code><code class="p">)</code>
<code class="p">(</code><code class="mi">240</code><code class="p">,</code><code class="mf">0.24571877339169032</code><code class="p">)</code>
<code class="p">(</code><code class="mi">270</code><code class="p">,</code><code class="mf">0.21818167354866858</code><code class="p">)</code></pre>
<p>This has helped put dimensions on more equal footing, and the absolute distances between points (and thus the cost) is much smaller in absolute terms. However, the above output doesn’t yet provide an obvious value of <em>k</em> beyond which increasing it does little to improve the cost.</p>
<p>Another 3-D visualization of the normalized data points reveals a richer structure, as
expected. Some points are spaced in regular, discrete intervals in one direction; these are
likely projections of discrete dimensions in the data, like counts. With 100 clusters,
it’s hard to make out which points come from which clusters. One large cluster seems
to dominate, and many clusters correspond to small, compact subregions (some of
which are omitted from this zoomed detail of the entire 3-D visualization). The result,
shown in <a data-type="xref" href="#AnomalyDetection_Projection2">Figure 5-2</a>, does not necessarily advance the analysis but is an interesting
sanity check.</p>
<figure><div class="figure" id="AnomalyDetection_Projection2">
<img alt="aaps 0502" height="1058" src="assets/aaps_0502.png" width="1278"/>
<h6><span class="label">Figure 5-2. </span>Random 3-D projection, normalized</h6>
</div></figure>
</div></section>
<section data-pdf-bookmark="Categorical Variables" data-type="sect1"><div class="sect1" id="idm46507976223824">
<h1>Categorical Variables</h1>
<p>Normalization was a valuable step forward,<a data-primary="anomaly detection" data-secondary="network traffic anomalies" data-tertiary="categorical data" data-type="indexterm" id="idm46507975921920"/><a data-primary="K-means clustering" data-secondary="network traffic anomalies" data-tertiary="categorical data" data-type="indexterm" id="idm46507975920656"/><a data-primary="network traffic anomaly detection" data-secondary="categorical data" data-type="indexterm" id="idm46507975919424"/><a data-primary="categorical data" data-secondary="network traffic anomaly detection" data-type="indexterm" id="idm46507975918464"/> but more can be done to improve the clustering. In particular, several
features have been left out entirely because they aren’t numeric. This is throwing away valuable information.
Adding them back, in some form, should produce a better-informed clustering.</p>
<p>Earlier, three categorical features were excluded because nonnumeric features can’t be used with the Euclidean
distance function that K-means uses in MLlib. This is the reverse of the issue noted in <a data-type="xref" href="ch04.xhtml#RandomDecisionForests">“Random Forests”</a>,
where numeric features were used to represent categorical values but a categorical feature was desired.</p>
<p><a data-primary="categorical data" data-secondary="binary mapped to numeric" data-type="indexterm" id="idm46507975915424"/><a data-primary="one-hot encoding" data-secondary="numeric treatment of" data-type="indexterm" id="idm46507975914432"/>The categorical features can be translated into several
binary indicator features using one-hot encoding, which can be viewed as numeric dimensions.
For example, the second column contains the protocol type: <code>tcp</code>, <code>udp</code>, or <code>icmp</code>. This feature could be thought
of as <em>three</em> features, as if features “is TCP,” “is UDP,” and “is ICMP” were in the dataset. The single feature value
<code>tcp</code> might become <code>1,0,0</code>; <code>udp</code> might be <code>0,1,0</code>; and so on.</p>
<p>Here again, MLlib provides components<a data-primary="MLlib component of Spark" data-secondary="categorical features into numeric" data-type="indexterm" id="idm46507975858656"/><a data-primary="StringIndexer" data-type="indexterm" id="idm46507975857808"/><a data-primary="OneHotEncoder" data-type="indexterm" id="idm46507975857200"/><a data-primary="one-hot encoding" data-secondary="numeric treatment of" data-tertiary="translating into numeric" data-type="indexterm" id="idm46507975856592"/><a data-primary="Pipeline" data-secondary="StringIndexer and OneHotEncoder" data-type="indexterm" id="idm46507975855504"/><a data-primary="MLlib component of Spark" data-secondary="Pipeline" data-tertiary="StringIndexer and OneHotEncoder" data-type="indexterm" id="idm46507975854656"/> that implement this transformation. In fact, one-hot encoding
string-valued features like <code>protocol_type</code> are actually a two-step process. First, the string values
are converted to integer indices like 0, 1, 2, and so on using <code>StringIndexer</code>. Then, these integer
indices are encoded into a vector with <code>OneHotEncoder</code>. These two steps can be thought of as a
small <code>Pipeline</code> in themselves.</p>
<pre data-code-language="python" data-type="programlisting"><code class="kn">from</code><code> </code><code class="nn">pyspark</code><code class="nn">.</code><code class="nn">ml</code><code class="nn">.</code><code class="nn">feature</code><code> </code><code class="kn">import</code><code> </code><code class="n">OneHotEncoder</code><code class="p">,</code><code> </code><code class="n">StringIndexer</code><code>
</code><code>
</code><code class="k">def</code><code> </code><code class="nf">one_hot_pipeline</code><code class="p">(</code><code class="n">input_col</code><code class="p">)</code><code class="p">:</code><code>
</code><code>    </code><code class="n">indexer</code><code> </code><code class="o">=</code><code> </code><code class="n">StringIndexer</code><code class="p">(</code><code class="p">)</code><code class="o">.</code><code class="n">setInputCol</code><code class="p">(</code><code class="n">input_col</code><code class="p">)</code><code class="o">.</code><code>\
</code><code>                              </code><code class="n">setOutputCol</code><code class="p">(</code><code class="n">input_col</code><code> </code><code class="o">+</code><code> </code><code class="s2">"</code><code class="s2">-_indexed</code><code class="s2">"</code><code class="p">)</code><code>
</code><code>    </code><code class="n">encoder</code><code> </code><code class="o">=</code><code> </code><code class="n">OneHotEncoder</code><code class="p">(</code><code class="p">)</code><code class="o">.</code><code class="n">setInputCol</code><code class="p">(</code><code class="n">input_col</code><code> </code><code class="o">+</code><code> </code><code class="s2">"</code><code class="s2">indexed</code><code class="s2">"</code><code class="p">)</code><code class="o">.</code><code>\
</code><code>                              </code><code class="n">setOutputCol</code><code class="p">(</code><code class="n">input_col</code><code> </code><code class="o">+</code><code> </code><code class="s2">"</code><code class="s2">_vec</code><code class="s2">"</code><code class="p">)</code><code>
</code><code>    </code><code class="n">pipeline</code><code> </code><code class="o">=</code><code> </code><code class="n">Pipeline</code><code class="p">(</code><code class="p">)</code><code class="o">.</code><code class="n">setStages</code><code class="p">(</code><code class="p">[</code><code class="n">indexer</code><code class="p">,</code><code> </code><code class="n">encoder</code><code class="p">]</code><code class="p">)</code><code>
</code><code>    </code><code class="k">return</code><code> </code><code class="n">pipeline</code><code class="p">,</code><code> </code><code class="n">input_col</code><code> </code><code class="o">+</code><code> </code><code class="s2">"</code><code class="s2">_vec</code><code class="s2">"</code><code> </code><a class="co" href="#callout_anomaly_detection__span_class__keep_together__with_k_means_clustering__span__CO6-1" id="co_anomaly_detection__span_class__keep_together__with_k_means_clustering__span__CO6-1"><img alt="1" height="12" src="assets/1.png" width="12"/></a></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_anomaly_detection__span_class__keep_together__with_k_means_clustering__span__CO6-1" id="callout_anomaly_detection__span_class__keep_together__with_k_means_clustering__span__CO6-1"><img alt="1" height="12" src="assets/1.png" width="12"/></a></dt>
<dd><p>Return pipeline and name of output vector column</p></dd>
</dl>
<p>This method produces a <code>Pipeline</code> that can be added as a component in the overall clustering
pipeline; pipelines can be composed. All that is left is to make sure to add the new vector
output columns into <code>VectorAssembler</code>’s output and proceed as before with scaling, clustering,
and evaluation. The source code is omitted for brevity here, but can be found in the repository
accompanying this chapter.</p>
<pre data-type="programlisting">(60,39.739250062068685)
(90,15.814341529964691)
(120,3.5008631362395413)
(150,2.2151974068685547)
(180,1.587330730808905)
(210,1.3626704802348888)
(240,1.1202477806210747)
(270,0.9263659836264369)</pre>
<p>These sample results suggest, possibly, <em>k</em>=180 as a value where the score flattens out a bit. At least the clustering is now using all input features.</p>
</div></section>
<section data-pdf-bookmark="Using Labels with Entropy" data-type="sect1"><div class="sect1" id="idm46507975923024">
<h1>Using Labels with Entropy</h1>
<p>Earlier, we used the given label for each<a data-primary="anomaly detection" data-secondary="network traffic anomalies" data-tertiary="entropy with labels" data-type="indexterm" id="idm46507975755936"/><a data-primary="K-means clustering" data-secondary="network traffic anomalies" data-tertiary="entropy with labels" data-type="indexterm" id="idm46507975754720"/><a data-primary="network traffic anomaly detection" data-secondary="entropy with labels" data-type="indexterm" id="idm46507975753536"/><a data-primary="entropy as impurity metric" data-secondary="labels for anomaly detection" data-type="indexterm" id="idm46507975712592"/><a data-primary="anomaly detection" data-secondary="network traffic anomalies" data-tertiary="choosing k via entropy labels" data-type="indexterm" id="idm46507975711696"/><a data-primary="K-means clustering" data-secondary="network traffic anomalies" data-tertiary="choosing k via entropy labels" data-type="indexterm" id="idm46507975710448"/><a data-primary="network traffic anomaly detection" data-secondary="choosing k via entropy labels" data-type="indexterm" id="idm46507975709200"/> data point to create a quick sanity check
of the quality of the clustering. This notion can be formalized further and used as an alternative means of
evaluating clustering quality and, therefore, of choosing <em>k</em>.</p>
<p>The labels
tell us something about the true nature of each data point. A good clustering, it seems, should agree with these
human-applied labels. It should put together points that share a label frequently and not lump together points of
many different labels. It should produce clusters with relatively homogeneous labels.</p>
<p>You may recall from <a data-type="xref" href="ch04.xhtml#RandomDecisionForests">“Random Forests”</a> that we have metrics for
homogeneity: Gini impurity and entropy. These are functions of the proportions of labels in each cluster and produce a number that is low when the proportions are skewed toward few, or one, label. Entropy will be used here for illustration:</p>
<pre data-code-language="python" data-type="programlisting"><code class="kn">from</code> <code class="nn">math</code> <code class="kn">import</code> <code class="n">log</code>

<code class="k">def</code> <code class="nf">entropy</code><code class="p">(</code><code class="n">counts</code><code class="p">):</code>
    <code class="n">values</code> <code class="o">=</code> <code class="p">[</code><code class="n">c</code> <code class="k">for</code> <code class="n">c</code> <code class="ow">in</code> <code class="n">counts</code> <code class="k">if</code> <code class="p">(</code><code class="n">c</code> <code class="o">&gt;</code> <code class="mi">0</code><code class="p">)]</code>
    <code class="n">n</code> <code class="o">=</code> <code class="nb">sum</code><code class="p">(</code><code class="n">values</code><code class="p">)</code>
    <code class="n">p</code> <code class="o">=</code> <code class="p">[</code><code class="n">v</code><code class="o">/</code><code class="n">n</code> <code class="k">for</code> <code class="n">v</code> <code class="ow">in</code> <code class="n">values</code><code class="p">]</code>
    <code class="k">return</code> <code class="nb">sum</code><code class="p">([</code><code class="o">-</code><code class="mi">1</code><code class="o">*</code><code class="p">(</code><code class="n">p_v</code><code class="p">)</code> <code class="o">*</code> <code class="n">log</code><code class="p">(</code><code class="n">p_v</code><code class="p">)</code> <code class="k">for</code> <code class="n">p_v</code> <code class="ow">in</code> <code class="n">p</code><code class="p">])</code></pre>
<p>A good clustering would have clusters whose collections of labels are homogeneous and so
have low entropy. A weighted average of entropy can therefore be used as a cluster score:</p>
<pre data-code-language="python" data-type="programlisting"><code class="kn">from</code><code> </code><code class="nn">pyspark</code><code class="nn">.</code><code class="nn">sql</code><code> </code><code class="kn">import</code><code> </code><code class="n">functions</code><code> </code><code class="k">as</code><code> </code><code class="n">fun</code><code>
</code><code class="kn">from</code><code> </code><code class="nn">pyspark</code><code class="nn">.</code><code class="nn">sql</code><code> </code><code class="kn">import</code><code> </code><code class="n">Window</code><code>
</code><code>
</code><code class="n">cluster_label</code><code> </code><code class="o">=</code><code> </code><code class="n">pipeline_model</code><code class="o">.</code><code>\
</code><code>                    </code><code class="n">transform</code><code class="p">(</code><code class="n">data</code><code class="p">)</code><code class="o">.</code><code>\
</code><code>                    </code><code class="n">select</code><code class="p">(</code><code class="s2">"</code><code class="s2">cluster</code><code class="s2">"</code><code class="p">,</code><code> </code><code class="s2">"</code><code class="s2">label</code><code class="s2">"</code><code class="p">)</code><code> </code><a class="co" href="#callout_anomaly_detection__span_class__keep_together__with_k_means_clustering__span__CO7-1" id="co_anomaly_detection__span_class__keep_together__with_k_means_clustering__span__CO7-1"><img alt="1" height="12" src="assets/1.png" width="12"/></a><code>
</code><code>
</code><code class="n">df</code><code> </code><code class="o">=</code><code> </code><code class="n">cluster_label</code><code class="o">.</code><code>\
</code><code>        </code><code class="n">groupBy</code><code class="p">(</code><code class="s2">"</code><code class="s2">cluster</code><code class="s2">"</code><code class="p">,</code><code> </code><code class="s2">"</code><code class="s2">label</code><code class="s2">"</code><code class="p">)</code><code class="o">.</code><code>\
</code><code>        </code><code class="n">count</code><code class="p">(</code><code class="p">)</code><code class="o">.</code><code class="n">orderBy</code><code class="p">(</code><code class="s2">"</code><code class="s2">cluster</code><code class="s2">"</code><code class="p">)</code><code> </code><a class="co" href="#callout_anomaly_detection__span_class__keep_together__with_k_means_clustering__span__CO7-2" id="co_anomaly_detection__span_class__keep_together__with_k_means_clustering__span__CO7-2"><img alt="2" height="12" src="assets/2.png" width="12"/></a><code>
</code><code>
</code><code class="n">w</code><code> </code><code class="o">=</code><code> </code><code class="n">Window</code><code class="o">.</code><code class="n">partitionBy</code><code class="p">(</code><code class="s2">"</code><code class="s2">cluster</code><code class="s2">"</code><code class="p">)</code><code>
</code><code>
</code><code class="n">p_col</code><code> </code><code class="o">=</code><code> </code><code class="n">df</code><code class="p">[</code><code class="s1">'</code><code class="s1">count</code><code class="s1">'</code><code class="p">]</code><code> </code><code class="o">/</code><code> </code><code class="n">fun</code><code class="o">.</code><code class="n">sum</code><code class="p">(</code><code class="n">df</code><code class="p">[</code><code class="s1">'</code><code class="s1">count</code><code class="s1">'</code><code class="p">]</code><code class="p">)</code><code class="o">.</code><code class="n">over</code><code class="p">(</code><code class="n">w</code><code class="p">)</code><code>
</code><code class="n">with_p_col</code><code> </code><code class="o">=</code><code> </code><code class="n">df</code><code class="o">.</code><code class="n">withColumn</code><code class="p">(</code><code class="s2">"</code><code class="s2">p_col</code><code class="s2">"</code><code class="p">,</code><code> </code><code class="n">p_col</code><code class="p">)</code><code>
</code><code>
</code><code class="n">result</code><code> </code><code class="o">=</code><code> </code><code class="n">with_p_col</code><code class="o">.</code><code class="n">groupBy</code><code class="p">(</code><code class="s2">"</code><code class="s2">cluster</code><code class="s2">"</code><code class="p">)</code><code class="o">.</code><code>\
</code><code>              </code><code class="n">agg</code><code class="p">(</code><code class="o">-</code><code class="n">fun</code><code class="o">.</code><code class="n">sum</code><code class="p">(</code><code class="n">col</code><code class="p">(</code><code class="s2">"</code><code class="s2">p_col</code><code class="s2">"</code><code class="p">)</code><code> </code><code class="o">*</code><code> </code><code class="n">fun</code><code class="o">.</code><code class="n">log2</code><code class="p">(</code><code class="n">col</code><code class="p">(</code><code class="s2">"</code><code class="s2">p_col</code><code class="s2">"</code><code class="p">)</code><code class="p">)</code><code class="p">)</code><code>\
</code><code>                        </code><code class="o">.</code><code class="n">alias</code><code class="p">(</code><code class="s2">"</code><code class="s2">entropy</code><code class="s2">"</code><code class="p">)</code><code class="p">,</code><code>
</code><code>                    </code><code class="n">fun</code><code class="o">.</code><code class="n">sum</code><code class="p">(</code><code class="n">col</code><code class="p">(</code><code class="s2">"</code><code class="s2">count</code><code class="s2">"</code><code class="p">)</code><code class="p">)</code><code>\
</code><code>                        </code><code class="o">.</code><code class="n">alias</code><code class="p">(</code><code class="s2">"</code><code class="s2">cluster_size</code><code class="s2">"</code><code class="p">)</code><code class="p">)</code><code>
</code><code>
</code><code class="n">result</code><code> </code><code class="o">=</code><code> </code><code class="n">result</code><code class="o">.</code><code class="n">withColumn</code><code class="p">(</code><code class="s1">'</code><code class="s1">weightedClusterEntropy</code><code class="s1">'</code><code class="p">,</code><code>
</code><code>                          </code><code class="n">col</code><code class="p">(</code><code class="s1">'</code><code class="s1">entropy</code><code class="s1">'</code><code class="p">)</code><code> </code><code class="o">*</code><code> </code><code class="n">col</code><code class="p">(</code><code class="s1">'</code><code class="s1">cluster_size</code><code class="s1">'</code><code class="p">)</code><code class="p">)</code><code> </code><a class="co" href="#callout_anomaly_detection__span_class__keep_together__with_k_means_clustering__span__CO7-3" id="co_anomaly_detection__span_class__keep_together__with_k_means_clustering__span__CO7-3"><img alt="3" height="12" src="assets/3.png" width="12"/></a><code>
</code><code>
</code><code class="n">weighted_cluster_entropy_avg</code><code> </code><code class="o">=</code><code> </code><code class="n">result</code><code class="o">.</code><code>\
</code><code>                            </code><code class="n">agg</code><code class="p">(</code><code class="n">fun</code><code class="o">.</code><code class="n">sum</code><code class="p">(</code><code>
</code><code>                              </code><code class="n">col</code><code class="p">(</code><code class="s1">'</code><code class="s1">weightedClusterEntropy</code><code class="s1">'</code><code class="p">)</code><code class="p">)</code><code class="p">)</code><code class="o">.</code><code>\
</code><code>                            </code><code class="n">collect</code><code class="p">(</code><code class="p">)</code><code>
</code><code class="n">weighted_cluster_entropy_avg</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="o">/</code><code class="n">data</code><code class="o">.</code><code class="n">count</code><code class="p">(</code><code class="p">)</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_anomaly_detection__span_class__keep_together__with_k_means_clustering__span__CO7-1" id="callout_anomaly_detection__span_class__keep_together__with_k_means_clustering__span__CO7-1"><img alt="1" height="12" src="assets/1.png" width="12"/></a></dt>
<dd><p>Predict cluster for each datum.</p></dd>
<dt><a class="co" href="#co_anomaly_detection__span_class__keep_together__with_k_means_clustering__span__CO7-2" id="callout_anomaly_detection__span_class__keep_together__with_k_means_clustering__span__CO7-2"><img alt="2" height="12" src="assets/2.png" width="12"/></a></dt>
<dd><p>Count labels, per cluster</p></dd>
<dt><a class="co" href="#co_anomaly_detection__span_class__keep_together__with_k_means_clustering__span__CO7-3" id="callout_anomaly_detection__span_class__keep_together__with_k_means_clustering__span__CO7-3"><img alt="3" height="12" src="assets/3.png" width="12"/></a></dt>
<dd><p>Average entropy weighted by cluster size.</p></dd>
</dl>
<p>As before, this analysis can be used to obtain some idea of a suitable value of <em>k</em>. Entropy will
not necessarily decrease as <em>k</em> increases, so it is possible to look for a local minimum value. Here again,
results suggest <em>k</em>=180 is a reasonable choice because its score is actually lower than 150 and 210:</p>
<pre data-type="programlisting">(60,0.03475331900669869)
(90,0.051512668026335535)
(120,0.02020028911919293)
(150,0.019962563512905682)
(180,0.01110240886325257)
(210,0.01259738444250231)
(240,0.01357435960663116)
(270,0.010119881917660544)</pre>
</div></section>
<section data-pdf-bookmark="Clustering in Action" data-type="sect1"><div class="sect1" id="idm46507975757232">
<h1>Clustering in Action</h1>
<p>Finally, with confidence, we can cluster<a data-primary="anomaly detection" data-secondary="network traffic anomalies" data-tertiary="clustering with normalized dataset" data-type="indexterm" id="idm46507975526400"/><a data-primary="K-means clustering" data-secondary="network traffic anomalies" data-tertiary="clustering with normalized dataset" data-type="indexterm" id="idm46507975558320"/><a data-primary="network traffic anomaly detection" data-secondary="clustering with normalized dataset" data-type="indexterm" id="idm46507975557104"/> the full, normalized dataset with <em>k</em>=180. Again, we can print the labels
for each cluster to get some sense of the resulting clustering. Clusters do seem to be dominated by one type of attack each and contain only a few types:</p>
<pre data-code-language="python" data-type="programlisting"><code class="n">pipeline_model</code><code> </code><code class="o">=</code><code> </code><code class="n">fit_pipeline_4</code><code class="p">(</code><code class="n">data</code><code class="p">,</code><code> </code><code class="mi">180</code><code class="p">)</code><code> </code><a class="co" href="#callout_anomaly_detection__span_class__keep_together__with_k_means_clustering__span__CO8-1" id="co_anomaly_detection__span_class__keep_together__with_k_means_clustering__span__CO8-1"><img alt="1" height="12" src="assets/1.png" width="12"/></a><code>
</code><code class="n">count_by_cluster_label</code><code> </code><code class="o">=</code><code> </code><code class="n">pipeline_model</code><code class="o">.</code><code class="n">transform</code><code class="p">(</code><code class="n">data</code><code class="p">)</code><code class="o">.</code><code>\
</code><code>                                        </code><code class="n">select</code><code class="p">(</code><code class="s2">"</code><code class="s2">cluster</code><code class="s2">"</code><code class="p">,</code><code> </code><code class="s2">"</code><code class="s2">label</code><code class="s2">"</code><code class="p">)</code><code class="o">.</code><code>\
</code><code>                                        </code><code class="n">groupBy</code><code class="p">(</code><code class="s2">"</code><code class="s2">cluster</code><code class="s2">"</code><code class="p">,</code><code> </code><code class="s2">"</code><code class="s2">label</code><code class="s2">"</code><code class="p">)</code><code class="o">.</code><code>\
</code><code>                                        </code><code class="n">count</code><code class="p">(</code><code class="p">)</code><code class="o">.</code><code class="n">orderBy</code><code class="p">(</code><code class="s2">"</code><code class="s2">cluster</code><code class="s2">"</code><code class="p">,</code><code> </code><code class="s2">"</code><code class="s2">label</code><code class="s2">"</code><code class="p">)</code><code>
</code><code class="n">count_by_cluster_label</code><code class="o">.</code><code class="n">show</code><code class="p">(</code><code class="p">)</code><code>
</code><code>
</code><code class="o">.</code><code class="o">.</code><code class="o">.</code><code>
</code><code class="o">+</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">+</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">+</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">+</code><code>
</code><code class="o">|</code><code class="n">cluster</code><code class="o">|</code><code>     </code><code class="n">label</code><code class="o">|</code><code> </code><code class="n">count</code><code class="o">|</code><code>
</code><code class="o">+</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">+</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">+</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">+</code><code>
</code><code class="o">|</code><code>      </code><code class="mi">0</code><code class="o">|</code><code>     </code><code class="n">back</code><code class="o">.</code><code class="o">|</code><code>   </code><code class="mi">324</code><code class="o">|</code><code>
</code><code class="o">|</code><code>      </code><code class="mi">0</code><code class="o">|</code><code>   </code><code class="n">normal</code><code class="o">.</code><code class="o">|</code><code> </code><code class="mi">42921</code><code class="o">|</code><code>
</code><code class="o">|</code><code>      </code><code class="mi">1</code><code class="o">|</code><code>  </code><code class="n">neptune</code><code class="o">.</code><code class="o">|</code><code>  </code><code class="mi">1039</code><code class="o">|</code><code>
</code><code class="o">|</code><code>      </code><code class="mi">1</code><code class="o">|</code><code class="n">portsweep</code><code class="o">.</code><code class="o">|</code><code>     </code><code class="mi">9</code><code class="o">|</code><code>
</code><code class="o">|</code><code>      </code><code class="mi">1</code><code class="o">|</code><code>    </code><code class="n">satan</code><code class="o">.</code><code class="o">|</code><code>     </code><code class="mi">2</code><code class="o">|</code><code>
</code><code class="o">|</code><code>      </code><code class="mi">2</code><code class="o">|</code><code>  </code><code class="n">neptune</code><code class="o">.</code><code class="o">|</code><code class="mi">365375</code><code class="o">|</code><code>
</code><code class="o">|</code><code>      </code><code class="mi">2</code><code class="o">|</code><code class="n">portsweep</code><code class="o">.</code><code class="o">|</code><code>   </code><code class="mi">141</code><code class="o">|</code><code>
</code><code class="o">|</code><code>      </code><code class="mi">3</code><code class="o">|</code><code class="n">portsweep</code><code class="o">.</code><code class="o">|</code><code>     </code><code class="mi">2</code><code class="o">|</code><code>
</code><code class="o">|</code><code>      </code><code class="mi">3</code><code class="o">|</code><code>    </code><code class="n">satan</code><code class="o">.</code><code class="o">|</code><code> </code><code class="mi">10627</code><code class="o">|</code><code>
</code><code class="o">|</code><code>      </code><code class="mi">4</code><code class="o">|</code><code>  </code><code class="n">neptune</code><code class="o">.</code><code class="o">|</code><code>  </code><code class="mi">1033</code><code class="o">|</code><code>
</code><code class="o">|</code><code>      </code><code class="mi">4</code><code class="o">|</code><code class="n">portsweep</code><code class="o">.</code><code class="o">|</code><code>     </code><code class="mi">6</code><code class="o">|</code><code>
</code><code class="o">|</code><code>      </code><code class="mi">4</code><code class="o">|</code><code>    </code><code class="n">satan</code><code class="o">.</code><code class="o">|</code><code>     </code><code class="mi">1</code><code class="o">|</code><code>
</code><code class="o">.</code><code class="o">.</code><code class="o">.</code></pre>
<dl class="calloutlist">
<dt><a class="co" href="#co_anomaly_detection__span_class__keep_together__with_k_means_clustering__span__CO8-1" id="callout_anomaly_detection__span_class__keep_together__with_k_means_clustering__span__CO8-1"><img alt="1" height="12" src="assets/1.png" width="12"/></a></dt>
<dd><p>See accompanying source code for <code>fit_pipeline_4</code> definition.</p></dd>
</dl>
<p>Now we can make an actual anomaly detector.<a data-primary="anomaly detection" data-secondary="network traffic anomalies" data-tertiary="anomaly detector" data-type="indexterm" id="idm46507975050384"/><a data-primary="K-means clustering" data-secondary="network traffic anomalies" data-tertiary="anomaly detector" data-type="indexterm" id="idm46507975049168"/><a data-primary="network traffic anomaly detection" data-secondary="anomaly detector" data-type="indexterm" id="idm46507975047984"/> Anomaly detection amounts to measuring a new data point’s distance
to its nearest centroid. If this distance exceeds some threshold, it is anomalous. This threshold might be chosen
to be the distance of, say, the 100th-farthest data point from among known data:</p>
<pre data-code-language="python" data-type="programlisting"><code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>

<code class="kn">from</code> <code class="nn">pyspark.spark.ml.linalg</code> <code class="kn">import</code> <code class="n">Vector</code><code class="p">,</code> <code class="n">Vectors</code>
<code class="kn">from</code> <code class="nn">pyspark.sql.functions</code> <code class="kn">import</code> <code class="n">udf</code>

<code class="n">k_means_model</code> <code class="o">=</code> <code class="n">pipeline_model</code><code class="o">.</code><code class="n">stages</code><code class="p">[</code><code class="o">-</code><code class="mi">1</code><code class="p">]</code>
<code class="n">centroids</code> <code class="o">=</code> <code class="n">k_means_model</code><code class="o">.</code><code class="n">clusterCenters</code>

<code class="n">clustered</code> <code class="o">=</code> <code class="n">pipeline_model</code><code class="o">.</code><code class="n">transform</code><code class="p">(</code><code class="n">data</code><code class="p">)</code>

<code class="k">def</code> <code class="nf">dist_func</code><code class="p">(</code><code class="n">cluster</code><code class="p">,</code> <code class="n">vec</code><code class="p">):</code>
    <code class="k">return</code> <code class="nb">float</code><code class="p">(</code><code class="n">np</code><code class="o">.</code><code class="n">linalg</code><code class="o">.</code><code class="n">norm</code><code class="p">(</code><code class="n">centroids</code><code class="p">[</code><code class="n">cluster</code><code class="p">]</code> <code class="o">-</code> <code class="n">vec</code><code class="p">))</code>
<code class="n">dist</code> <code class="o">=</code> <code class="n">udf</code><code class="p">(</code><code class="n">dist_func</code><code class="p">)</code>

<code class="n">threshold</code> <code class="o">=</code> <code class="n">clustered</code><code class="o">.</code><code class="n">select</code><code class="p">(</code><code class="s2">"cluster"</code><code class="p">,</code> <code class="s2">"scaledFeatureVector"</code><code class="p">)</code><code class="o">.</code>\
    <code class="n">withColumn</code><code class="p">(</code><code class="s2">"dist_value"</code><code class="p">,</code>
        <code class="n">dist</code><code class="p">(</code><code class="n">col</code><code class="p">(</code><code class="s2">"cluster"</code><code class="p">),</code> <code class="n">col</code><code class="p">(</code><code class="s2">"scaledFeatureVector"</code><code class="p">)))</code><code class="o">.</code>\
    <code class="n">orderBy</code><code class="p">(</code><code class="n">col</code><code class="p">(</code><code class="s2">"dist_value"</code><code class="p">)</code><code class="o">.</code><code class="n">desc</code><code class="p">())</code><code class="o">.</code><code class="n">take</code><code class="p">(</code><code class="mi">100</code><code class="p">)</code></pre>
<p>The final step can be to apply this threshold to all new data points as they arrive. <a data-primary="Spark Streaming for anomaly detection" data-type="indexterm" id="idm46507975031168"/>For example, Spark Streaming
can be used to apply this function to small batches of input data arriving from sources like Kafka
or files in cloud storage. Data points exceeding the threshold might trigger an alert that sends an email
or updates a database.</p>
</div></section>
<section data-pdf-bookmark="Where to Go from Here" data-type="sect1"><div class="sect1" id="idm46507975184656">
<h1>Where to Go from Here</h1>
<p>The <code>KMeansModel</code> is, by itself, the essence of an anomaly detection system. The preceding code demonstrated how
to apply it to data to detect anomalies. This same code could be used within
<a href="https://oreil.ly/UHHBR">Spark Streaming</a> to score new data as it arrives in near real time, and
perhaps trigger an alert or review.</p>
<p>MLlib also includes a variation called <code>StreamingKMeans</code>, <a data-primary="MLlib component of Spark" data-secondary="StreamingKMeans" data-type="indexterm" id="idm46507975181168"/><a data-primary="K-means clustering" data-secondary="StreamingKMeans" data-type="indexterm" id="idm46507975180192"/>which can update a clustering incrementally
as new data arrives in a <code>StreamingKMeansModel</code>. We could use this to continue to learn, approximately,
how new data affects the clustering, and not just to assess new data against existing clusters.
It can be integrated with Spark Streaming as well. However, it has not been updated for the new
DataFrame-based APIs.</p>
<p>This model is only a simplistic one. <a data-primary="Euclidean distance for K-means" data-type="indexterm" id="idm46507975178032"/><a data-primary="K-means clustering" data-secondary="Euclidean distance for" data-type="indexterm" id="idm46507975177360"/>For example, Euclidean distance is used in this example because it is the
only distance function supported by Spark MLlib at this time. In the future, it may be possible to use distance
functions that can better account for the distributions of and correlations between features, <a data-primary="Mahalanobis distance" data-type="indexterm" id="idm46507975176288"/>such as the
<a href="https://oreil.ly/PKG7A">Mahalanobis distance</a>.</p>
<p>There are also more sophisticated<a data-primary="anomaly detection" data-secondary="network traffic anomalies" data-tertiary="choosing k via more sophisticated metrics" data-type="indexterm" id="idm46507975174352"/><a data-primary="K-means clustering" data-secondary="network traffic anomalies" data-tertiary="choosing k via more sophisticated metrics" data-type="indexterm" id="idm46507975172784"/><a data-primary="network traffic anomaly detection" data-secondary="choosing k via more sophisticated metrics" data-type="indexterm" id="idm46507975171632"/>
<a href="https://oreil.ly/9yE9P">cluster-quality evaluation metrics</a>
that could be applied (even without labels) to pick <em>k</em>, <a data-primary="Silhouette coefficient" data-type="indexterm" id="idm46507975169552"/><a data-primary="online resources" data-secondary="Silhouette coefficient" data-type="indexterm" id="idm46507975168816"/>such as the
<a href="https://oreil.ly/LMN1h">Silhouette coefficient</a>. These tend to evaluate
not just closeness of points within one cluster, but closeness of points to other clusters. <a data-primary="anomaly detection" data-secondary="K-means clustering" data-tertiary="alternative models" data-type="indexterm" id="idm46507975166928"/><a data-primary="K-means clustering" data-secondary="alternative models" data-type="indexterm" id="idm46507975165680"/><a data-primary="network traffic anomaly detection" data-secondary="K-means clustering" data-tertiary="alternative models" data-type="indexterm" id="idm46507975164736"/>Finally, different models could be applied
instead of simple K-means clustering; <a data-primary="Gaussian mixture model" data-type="indexterm" id="idm46507975163328"/><a data-primary="online resources" data-secondary="Gaussian mixture model" data-type="indexterm" id="idm46507975162656"/><a data-primary="DBSCAN model" data-type="indexterm" id="idm46507975161712"/><a data-primary="online resources" data-secondary="DBSCAN model" data-type="indexterm" id="idm46507975161040"/>for example, a
<a href="https://oreil.ly/KTgD6">Gaussian mixture model</a>
or <a href="https://oreil.ly/xlshs">DBSCAN</a> could capture more
subtle relationships between data points and the cluster centers. <a data-primary="MLlib component of Spark" data-secondary="Gaussian mixture models" data-type="indexterm" id="idm46507975158560"/><a data-primary="Gaussian mixture model" data-secondary="Spark MLlib implementation" data-type="indexterm" id="idm46507975157568"/>Spark MLlib already implements <a href="https://oreil.ly/LG84u">Gaussian mixture models</a>;
implementations of others may become available in Spark MLlib or other Spark-based libraries in the future.</p>
<p>Of course, clustering isn’t just for anomaly detection. In fact, it’s more often
associated with use cases where the actual clusters matter! For example, clustering can also be used
to group customers according to their behaviors, preferences, and attributes.
Each cluster, by itself, might represent a usefully distinguishable type of customer. This is a
more data-driven way to segment customers rather than leaning on arbitrary, generic divisions like
“age 20–34” and “female.”</p>
</div></section>
</div></section></div></body></html>