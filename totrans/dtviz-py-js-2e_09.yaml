- en: Chapter 6\. Heavyweight Scraping with Scrapy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As your scraping goals get more ambitious, hacking solutions with Beautiful
    Soup and requests can get very messy very fast. Managing the scraped data as requests
    spawn more requests gets tricky, and if your requests are being made synchronously,
    things start to slow down rapidly. A whole load of problems you probably hadn’t
    anticipated start to make themselves known. It’s at this point that you want to
    turn to a powerful, robust library that solves all these problems and more. And
    that’s where Scrapy comes in.
  prefs: []
  type: TYPE_NORMAL
- en: Where Beautiful Soup is a very handy little penknife for fast and dirty scraping,
    Scrapy is a Python library that can do large-scale data scrapes with ease. It
    has all the things you’d expect, like built-in caching (with expiration times),
    asynchronous requests via Python’s Twisted web framework, user-agent randomization,
    and a whole lot more. The price for all this power is a fairly steep learning
    curve, which this chapter is intended to smooth, using a simple example. I think
    Scrapy is a powerful addition to any dataviz toolkit and really opens up possibilities
    for web data collection.
  prefs: []
  type: TYPE_NORMAL
- en: In [“Scraping Data”](ch05.xhtml#get_data_scraping), we managed to scrape a dataset
    containing all the Nobel Prize winners by name, year, and category. We did a speculative
    scrape of the winners’ linked biography pages, which showed that extracting the
    country of nationality was going to be difficult. In this chapter, we’ll set the
    bar on our Nobel Prize data a bit higher and aim to scrape objects of the form
    shown in [Example 6-1](#scrapy_target_JSON).
  prefs: []
  type: TYPE_NORMAL
- en: Example 6-1\. Our targeted Nobel JSON object
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In addition to this data, we’ll aim to scrape prizewinners’ photos (where applicable)
    and some potted biographical data (see [Figure 6-1](#scrapy_targets)). We’ll be
    using the photos and body text to add a little character to our Nobel Prize visualization.
  prefs: []
  type: TYPE_NORMAL
- en: '![dpj2 0601](assets/dpj2_0601.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-1\. Scraping targets for the prizewinners’ pages
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Setting Up Scrapy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Scrapy should be one of the Anaconda packages (see [Chapter 1](ch01.xhtml#chapter_install)),
    so you should already have it on hand. If that’s not the case, then you can install
    it with the following `conda` command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: If you’re not using Anaconda, a quick `pip` install will do the job:^([1](ch06.xhtml#idm45607786166832))
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'With Scrapy installed, you should have access to the `scrapy` command. Unlike
    the vast majority of Python libraries, Scrapy is designed to be driven from the
    command line within the context of a scraping project, defined by configuration
    files, scraping spiders, pipelines, and so on. Let’s generate a fresh project
    for our Nobel Prize scraping, using the `startproject` option. This is going to
    generate a project folder, so make sure you run it from a suitable work directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: As the output of `startproject` says, you’ll want to switch to the *nobel_winners*
    directory in order to start driving Scrapy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take a look at the project’s directory tree:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: As shown, the project directory has a subdirectory with the same name and a
    config file *scrapy.cfg*. The *nobel_winners* subdirectory is a Python module
    (containing an *__init__.py* file) with a few skeleton files and a *spiders* directory,
    which will contain your scrapers.
  prefs: []
  type: TYPE_NORMAL
- en: Establishing the Targets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [“Scraping Data”](ch05.xhtml#get_data_scraping), we tried to scrape the Nobel
    winners’ nationalities from their biography pages but found they were missing
    or inconsistently labeled in many cases (see [Chapter 5](ch05.xhtml#chapter_getting_data)).
    Rather than get the country data indirectly, a little Wikipedia searching shows
    a way through. There is a [page](https://oreil.ly/p6pXm) that lists winners by
    country. The winners are presented in titled, ordered lists (see [Figure 6-2](#scrapy_wiki_list)),
    not in tabular form, which makes recovering our basic name, category, and year
    data a little harder. Also the data organization is not ideal (e.g., the country
    header titles and winner lists aren’t in useful, separate blocks). As we’ll see,
    a few well-structured Scrapy queries will easily net us the data we need.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 6-2](#scrapy_wiki_list) shows the starting page for our first spider
    along with the key elements it will be targeting. A list of country name titles
    (A) is followed by an ordered list (B) of their Nobel Prize–winning citizens.'
  prefs: []
  type: TYPE_NORMAL
- en: '![dpj2 0602](assets/dpj2_0602.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-2\. Scraping Wikipedia’s Nobel Prizes by nationality
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'In order to scrape the list data, we need to fire up our Chrome browser’s DevTools
    (see [“The Elements Tab”](ch04.xhtml#chrome_elements)) and inspect the target
    elements using the Elements tab and its inspector (magnifying glass). [Figure 6-3](#scrapy_wiki_list_source)
    shows the key HTML targets for our first spider: header titles (h2) containing
    a country name and followed by an ordered list (ol) of winners (li).'
  prefs: []
  type: TYPE_NORMAL
- en: '![dpj2 0603](assets/dpj2_0603.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-3\. Finding the HTML targets for the wikilist
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Targeting HTML with Xpaths
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Scrapy uses [xpaths](https://oreil.ly/Y67BF) to define its HTML targets. Xpath
    is a syntax for describing parts of an X(HT)ML document, and while it can get
    rather complicated, the basics are straightforward and will often solve the job
    at hand.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can get the xpath of an HTML element by using Chrome’s Elements tab to
    hover over the source and then right-clicking and selecting Copy XPath. For example,
    in the case of our Nobel Prize wikilist’s country names (h3 in [Figure 6-3](#scrapy_wiki_list_source)),
    selecting the xpath of Argentina (the first country) gives the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We can use the following xpath rules to decode it:'
  prefs: []
  type: TYPE_NORMAL
- en: '`//E`'
  prefs: []
  type: TYPE_NORMAL
- en: Element `<E>` anywhere in the document (e.g., `//img` gets all images on the
    page)
  prefs: []
  type: TYPE_NORMAL
- en: '`//E[@id="foo"]`'
  prefs: []
  type: TYPE_NORMAL
- en: Select element `<E>` with ID `foo`
  prefs: []
  type: TYPE_NORMAL
- en: '`//*[@id="foo"]`'
  prefs: []
  type: TYPE_NORMAL
- en: Select any element with ID `foo`
  prefs: []
  type: TYPE_NORMAL
- en: '`//E/F[1]`'
  prefs: []
  type: TYPE_NORMAL
- en: First child element `<F>` of element `<E>`
  prefs: []
  type: TYPE_NORMAL
- en: '`//E/*[1]`'
  prefs: []
  type: TYPE_NORMAL
- en: First child of element `<E>`
  prefs: []
  type: TYPE_NORMAL
- en: 'Following these rules shows that our Argentinian title `//*[@id="mw-content-text"]/div[1]/h3[1]`
    is the first header (h2) child of the first `div` of the DOM element with ID `mw-content-text`.
    This is equivalent to the following HTML:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Note that unlike Python, the xpaths don’t use a zero-based index but make the
    first member *1*.
  prefs: []
  type: TYPE_NORMAL
- en: Testing Xpaths with the Scrapy Shell
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Getting your xpath targeting right is crucial to good scraping and can involve
    a degree of iteration. Scrapy makes this process much easier by providing a command-line
    shell, which takes a URL and creates a response context in which you can try out
    your xpaths, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we have an IPython-based shell with code-complete and syntax highlighting
    in which to try out our xpath targeting. Let’s grab all the `<h3>` headers on
    the wiki page:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting `h3s` is a [SelectorList](https://oreil.ly/zpbqa), a specialized
    Python `list` object. Let’s see how many headers we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We can grab the first [`Selector` object](https://oreil.ly/uBhdU) and query
    its methods and properties in the Scrapy shell by pressing Tab after appending
    a dot:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'You’ll often use the `extract` method to get the raw result of the xpath selector:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'This shows that our country headers start on the first `<h3>` and contain a
    `span` with class `mw-headline`. We can use the presence of the `mw-headline`
    class as a filter for our country headers and the contents as our country label.
    Let’s try out an xpath, using the selector’s `text` method to extract the text
    from the `mw-headline` span. Note that we use the `xpath` method of the `<h3>`
    selector, which makes the xpath query relative to that element:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The `extract` method returns a list of possible matches, in our case the single
    `'Argentina'` string. By iterating through the `h3s` list, we can now get our
    country names.
  prefs: []
  type: TYPE_NORMAL
- en: 'Assuming we have a country’s `<h3>` header, we now need to get the `<ol>` ordered
    list of Nobel winners following it ([Figure 6-2](#scrapy_wiki_list) B). Handily,
    the xpath `following-sibling` selector can do just that. Let’s grab the first
    ordered list after the Argentina header:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Looking at the truncated data for `ol_arg` shows that we have selected an ordered
    list. Note that even though there’s only one `Selector`, `xpath` still returns
    a `SelectorList`. For convenience, you’ll generally just select the first member
    directly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we’ve got the ordered list, let’s get a list of its member `<li>`
    elements (as of mid 2022):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s examine one of those list elements using `extract`. As a first test,
    we’re looking to scrape the name of the winner and capture the list element’s
    text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Extracting the list element shows a standard pattern: a hyperlinked name to
    the winner’s Wikipedia page followed by a comma-separated winning category and
    year. A robust way to get the winning name is just to select the text of the list
    element’s first `<a>` tag:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'It’s often useful to get all the text in, for example, a list element, stripping
    the various HTML `<a>`, `<span>`, and other tags. `descendant-or-self` gives us
    a handy way of doing this, producing a list of the descendants’ text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'We can get the full text by joining the list elements together:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Note that the first item of `list_text` is the winner’s name, giving us another
    way to access it if, for example, it were missing a hyperlink.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve established the xpaths to our scraping targets (the name and
    link text of the Nobel Prize winners), let’s incorporate them into our first Scrapy
    spider.
  prefs: []
  type: TYPE_NORMAL
- en: Selecting with Relative Xpaths
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As just shown, Scrapy `xpath` selections return lists of selectors which, in
    turn, have their own `xpath` methods. When using the `xpath` method, it’s important
    to be clear about relative and absolute selections. Let’s make the distinction
    clear using the Nobel page’s table of contents as an example.
  prefs: []
  type: TYPE_NORMAL
- en: 'The table of contents has the following structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'We can select the table of contents of the Nobel wiki page using a standard
    `xpath` query on the response, and getting the `div` with ID `toc`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'If we want to get all the country `<li>` list tags, we can use a relative `xpath`
    on the selected `toc` div. Looking at the HTML in [Figure 6-3](#scrapy_wiki_list_source)
    shows that the unordered list `ul` of countries is the first list member of the
    second list item of the table of content’s top list. This list can be selected
    by the following equivalent xpaths, both selecting children of the current `toc`
    selection relatively:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'A common mistake is to use a nonrelative `xpath` selector on the current selection,
    which selects from the whole document, in this case getting all unordered (`<ul>`)
    `<li>` tags:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Errors made from mistaking relative and nonrelative queries crop up a lot in
    the forums, so it’s good to be very aware of the distinction and watch those dots.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Getting the right xpath expression for your target element(s) can be a little
    tricky, and those difficult edge cases can demand a complex nest of clauses. The
    use of a well-written cheat sheet can be a great help here, and thankfully there
    are many good xpath ones. A very nice selection can be found [at devhints.io](https://devhints.io/xpath).
  prefs: []
  type: TYPE_NORMAL
- en: A First Scrapy Spider
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Armed with a little xpath knowledge, let’s produce our first scraper aiming
    to get the country and link text for the winners ([Figure 6-2](#scrapy_wiki_list)
    A and B).
  prefs: []
  type: TYPE_NORMAL
- en: 'Scrapy calls its scrapers *spiders*, each of which is a Python module placed
    in the *spiders* directory of your project. We’ll call our first scraper *nwinner_list_spider.py*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Spiders are subclassed `scrapy.Spider` classes, and any placed in the *spiders*
    directory will be automatically detected by Scrapy and made accessible by name
    to the `scrapy` command.
  prefs: []
  type: TYPE_NORMAL
- en: The basic Scrapy spider shown in [Example 6-2](#scrapy_spider) follows a pattern
    you’ll be using with most of your spiders. First, you subclass a Scrapy `item`
    to create fields for your scraped data (section A in [Example 6-2](#scrapy_spider)).
    You then create a named spider by subclassing `scrapy.Spider` (section B in [Example 6-2](#scrapy_spider)).
    You will use the spider’s name when calling `scrapy` from the command line. Each
    spider has a `parse` method, which deals with the HTTP requests to a list of start
    URLs contained in a `start_url` class attribute. In our case, the start URL is
    the Wikipedia page for Nobel laureates by country.
  prefs: []
  type: TYPE_NORMAL
- en: Example 6-2\. A first Scrapy spider
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_heavyweight_scraping_with_scrapy_CO1-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Gets all the `<h3>` headers on the page, most of which will be our target country
    titles.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_heavyweight_scraping_with_scrapy_CO1-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Where possible, gets the text of the `<h3>` element’s child `<span>` with class
    `mw-headline`.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_heavyweight_scraping_with_scrapy_CO1-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Gets the list of country winners.
  prefs: []
  type: TYPE_NORMAL
- en: The `parse` method in [Example 6-2](#scrapy_spider) receives the response from
    an HTTP request to the Wikipedia Nobel Prize page and yields Scrapy items, which
    are then converted to JSON objects and appended to the output file, a JSON array
    of objects.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s run our first spider to make sure we’re correctly parsing and scraping
    our Nobel data. First, navigate to the *nobel_winners* root directory (containing
    the *scrapy.cfg* file) of the scraping project. Let’s see what scraping spiders
    are available:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'As expected, we have one `nwinners_list` spider sitting in the *spiders* directory.
    To start it scraping, we use the `crawl` command and direct the output to a *nwinners.json*
    file. By default, we get a lot of Python logging information accompanying the
    crawl:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_heavyweight_scraping_with_scrapy_CO2-1)'
  prefs: []
  type: TYPE_NORMAL
- en: We scraped 1,169 Nobel winners from the page.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output of the scrapy `crawl` shows 1,169 items successfully scraped. Let’s
    look at our JSON output file to make sure things have gone according to plan:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, we have an array of JSON objects with the four key fields present
    and correct.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a spider that successfully scrapes the list data for all the
    Nobel winners on the page, let’s start refining it to grab all the data we are
    targeting for our Nobel Prize visualization (see [Example 6-1](#scrapy_target_JSON)
    and [Figure 6-1](#scrapy_targets)).
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s add all the data we plan to scrape as fields to our `scrapy.Item`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'It’s also sensible to simplify the code a bit and use a dedicated function,
    `process_winner_li`, to process the winners’ link text. We’ll pass a link selector
    and country name to it and return a dictionary containing the scraped data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: The `process_winner_li` method is shown in [Example 6-3](#scrapy_process_li).
    A `wdata` dictionary is filled with information extracted from the winner’s `li`
    tag, using a couple of regexes to find the prize year and category.
  prefs: []
  type: TYPE_NORMAL
- en: Example 6-3\. Processing a winner’s list item
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_heavyweight_scraping_with_scrapy_CO3-1)'
  prefs: []
  type: TYPE_NORMAL
- en: To grab the `href` attribute from the list item’s `<a>` tag (`<li><a href=*/wiki…​*>[winner
    name]</a>…​`), we use the xpath attribute referent @.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_heavyweight_scraping_with_scrapy_CO3-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Here, we use `re`, Python’s built-in regex library, to find the four-digit year
    strings in the list item’s text.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_heavyweight_scraping_with_scrapy_CO3-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Another use of the regex library to find the Nobel Prize category in the text.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_heavyweight_scraping_with_scrapy_CO3-4)'
  prefs: []
  type: TYPE_NORMAL
- en: An asterisk following the winner’s name is used to indicate that the country
    is the winner’s by birth—​not nationality—​at the time of the prize (e.g., `"William
    Lawrence Bragg*, Physics, 1915"` in the list for Australia).
  prefs: []
  type: TYPE_NORMAL
- en: '[Example 6-3](#scrapy_process_li) returns all the winners’ data available on
    the main Wikipedia Nobels by Country page—that is, the name, year, category, country
    (country of birth or country of nationality when awarded the prize), and a link
    to the individual winners’ pages. We’ll need to use this last information to get
    those biographical pages and use them to scrape our remaining target data (see
    [Example 6-1](#scrapy_target_JSON) and [Figure 6-1](#scrapy_targets)).'
  prefs: []
  type: TYPE_NORMAL
- en: Scraping the Individual Biography Pages
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The main Wikipedia Nobels by Country page gave us a lot of our target data,
    but the winner’s date of birth, date of death (where applicable), and gender are
    still to be scraped. It is hoped that this information is available, either implicitly
    or explicitly, on their biography pages (for nonorganization winners). Now’s a
    good time to fire up Chrome’s Elements tab and take a look at those pages to work
    out how we’re going to extract the desired data.
  prefs: []
  type: TYPE_NORMAL
- en: We saw in the last chapter ([Chapter 5](ch05.xhtml#chapter_getting_data)) that
    the visible information boxes on individual’s pages are not a reliable source
    of information and are often missing entirely. Until recently,^([3](ch06.xhtml#idm45607784791328))
    a hidden `persondata` table (see [Figure 6-4](#scrapy_persondata)) gave fairly
    reliable access to such information as place of birth, date of death, and the
    like. Unfortunately, this handy resource has been deprecated.^([4](ch06.xhtml#idm45607784789392))
    The good news is that this is part of an attempt to improve the categorization
    of biographical information by giving it a dedicated space in [Wikidata](https://oreil.ly/ICbBi),
    Wikipedia’s central storage for its structured data.
  prefs: []
  type: TYPE_NORMAL
- en: '![dpj2 0604](assets/dpj2_0604.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-4\. A Nobel Prize winner’s hidden `persondata` table
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Examining Wikipedia’s biography pages with Chrome’s Elements tab shows a link
    to the relevant Wikidata item (see [Figure 6-5](#scrapy_wikidata_link)), which
    takes you to the biographical data held at [*https://www.wikidata.org*](https://www.wikidata.org).
    By following this link, we can scrape whatever we find there, which we hope will
    be the bulk of our target data—​significant dates and places (see [Example 6-1](#scrapy_target_JSON)).
  prefs: []
  type: TYPE_NORMAL
- en: '![dpj2 0605](assets/dpj2_0605.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-5\. Hyperlink to the winner’s Wikidata
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Following the link to Wikidata shows a page containing fields for the data we
    are looking for, such as the date of birth of our prize winner. As [Figure 6-6](#scrapy_wikidata)
    shows, the properties are embedded in a nest of computer-generated HTML, with
    related codes, which we can use as a scraping identifier (e.g., date of birth
    has the code `P569`).
  prefs: []
  type: TYPE_NORMAL
- en: '![dpj2 0606](assets/dpj2_0606.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-6\. Biographical properties at Wikidata
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As [Figure 6-7](#scrapy_wikidata_xpath) shows, the actual data we want, in this
    case a date string, is contained in a further nested branch of HTML, within its
    respective property tag. By selecting the `div` and right-clicking, we can store
    the element’s xpath and use that to tell Scrapy how to get the data it contains.
  prefs: []
  type: TYPE_NORMAL
- en: '![dpj2 0607](assets/dpj2_0607.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-7\. Getting the xpath for a Wikidata property
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Now that we have the xpaths necessary to find our scraping targets, let’s put
    it all together and see how Scrapy chains requests, allowing for complex, multipage
    scraping operations.
  prefs: []
  type: TYPE_NORMAL
- en: Chaining Requests and Yielding Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section we’ll see how to chain Scrapy requests, allowing us to follow
    hyperlinks, scraping data as we go. First, let’s enable Scrapy’s page caching.
    While experimenting with xpath targets, we want to limit the number of calls to
    Wikipedia, and it’s good manners to store our fetched pages. Unlike some datasets
    out there, our Nobel Prize winners change but once a year.^([5](ch06.xhtml#idm45607784684416))
  prefs: []
  type: TYPE_NORMAL
- en: Caching Pages
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As you might expect, Scrapy has a [sophisticated caching system](https://oreil.ly/ytYWP)
    that gives you fine-grained control over your page caching (e.g., allowing you
    to choose between database or filesystem storage backends, how long before your
    pages are expired, etc.). It is implemented as [middleware](https://oreil.ly/w8v7c)
    enabled in our project’s `settings.py` module. There are various options available
    but for the purposes of our Nobel scraping, simply setting `HTTPCACHE_ENABLED`
    to `True` will suffice:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Check out the full range of Scrapy middleware [in Scrapy’s documentation](https://oreil.ly/9CMc4).
  prefs: []
  type: TYPE_NORMAL
- en: Having ticked the caching box, let’s see how to chain Scrapy requests.
  prefs: []
  type: TYPE_NORMAL
- en: Yielding Requests
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our existing spider’s `parse` method cycles through the Nobel winners, using
    the `process_winner_li` method to scrape the country, name, year, category, and
    biography-hyperlink fields. We now want to use the biography hyperlinks to generate
    a Scrapy request that will fetch the bio pages and send them to a custom method
    for scraping.
  prefs: []
  type: TYPE_NORMAL
- en: Scrapy implements a Pythonic pattern for chaining requests, using Python’s `yield`
    statement to create a generator,^([6](ch06.xhtml#idm45607784643296)) allowing
    Scrapy to easily consume any extra page requests we make. [Example 6-4](#scrapy_yield)
    shows the pattern in action.
  prefs: []
  type: TYPE_NORMAL
- en: Example 6-4\. Yielding a request with Scrapy
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_heavyweight_scraping_with_scrapy_CO4-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Makes a request to the winner’s biography page, using the link (`wdata[*link*]`)
    scraped from `process_winner_li`.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_heavyweight_scraping_with_scrapy_CO4-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Sets the callback function to handle the response.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_heavyweight_scraping_with_scrapy_CO4-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Creates a Scrapy `Item` to hold our Nobel data and initializes it with the data
    just scraped from `process_winner_li`. This `Item` data is attached to the metadata
    of the request to allow any response access to it.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_heavyweight_scraping_with_scrapy_CO4-4)'
  prefs: []
  type: TYPE_NORMAL
- en: By yielding the request, we make the `parse` method a generator of consumable
    requests.
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](assets/5.png)](#co_heavyweight_scraping_with_scrapy_CO4-5)'
  prefs: []
  type: TYPE_NORMAL
- en: This method handles the callback from our bio-link request. In order to add
    scraped data to our Scrapy `Item`, we first retrieve it from the `response` metadata.
  prefs: []
  type: TYPE_NORMAL
- en: Our investigation of the Wikipedia pages in [“Scraping the Individual Biography
    Pages”](#scrapy_indiv_bios) showed that we need to locate a winner’s Wikidata
    link from their biography page and use it to generate a request. We will then
    scrape the date, place, and gender data from the response.
  prefs: []
  type: TYPE_NORMAL
- en: '[Example 6-5](#scrapy_wikidata_source) shows `parse_bio` and `parse_wikidata`,
    the two methods used to scrape our winners’ biographical data. `parse_bio` uses
    the scraped Wikidata link to request the Wikidata page, yielding the `request`
    as it in turn was yielded in the `parse` method. At the end of the request chain,
    `parse_wikidata` retrieves the item and fills in any of the fields available from
    Wikidata, eventually yielding the item to Scrapy.'
  prefs: []
  type: TYPE_NORMAL
- en: Example 6-5\. Parsing the winners’ biography data
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_heavyweight_scraping_with_scrapy_CO5-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Extracts the link to Wikidata identified in [Figure 6-5](#scrapy_wikidata_link).
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_heavyweight_scraping_with_scrapy_CO5-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Extracts the `wiki_code` from the URL, e.g., [*http://wikidata.org/wiki/Q155525*](http://wikidata.org/wiki/Q155525)
    → Q155525.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_heavyweight_scraping_with_scrapy_CO5-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Uses the Wikidata link to generate a request with our spider’s `parse_wikidata`
    as a callback to deal with the response.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_heavyweight_scraping_with_scrapy_CO5-4)'
  prefs: []
  type: TYPE_NORMAL
- en: These are the property codes we found earlier (see [Figure 6-6](#scrapy_wikidata)),
    with names corresponding to fields in our Scrapy item, `NWinnerItem`. Those with
    a `True` `link` attribute are contained in `<a>` tags.
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](assets/5.png)](#co_heavyweight_scraping_with_scrapy_CO5-5)'
  prefs: []
  type: TYPE_NORMAL
- en: Finally we yield the item, which at this point should have all the target data
    available from Wikipedia.
  prefs: []
  type: TYPE_NORMAL
- en: 'With our request chain in place, let’s check that the spider is scraping our
    required data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Things are looking good. With the exception of the `born_in` field, which is
    dependent on a name in the main Wikipedia Nobel Prize winners list having an asterisk,
    we’re getting all the data we were targeting. This dataset is now ready to be
    cleaned by pandas in the coming chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve scraped our basic biographical data for the Nobel Prize winners,
    let’s go scrape our remaining targets, some biographical body text, and a picture
    of the great man or woman, where available.
  prefs: []
  type: TYPE_NORMAL
- en: Scrapy Pipelines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order to add a little personality to our Nobel Prize visualization, it would
    be good to have a little biographical text and an image of the winner. Wikipedia’s
    biographical pages generally provide these things, so let’s go about scraping
    them.
  prefs: []
  type: TYPE_NORMAL
- en: Up to now, our scraped data has been text strings. In order to scrape images
    in their various formats, we need to use a Scrapy *pipeline*. [Pipelines](https://oreil.ly/maUyE)
    provide a way of postprocessing the items we have scraped, and you can define
    any number of them. You can write your own or take advantage of those already
    provided by Scrapy, such as the `ImagesPipeline` we’ll be using.
  prefs: []
  type: TYPE_NORMAL
- en: 'In its simplest form, a pipeline need only define a `process_item` method.
    This receives the scraped items and the spider object. Let’s write a little pipeline
    to reject genderless Nobel Prize winners (so we can omit prizes given to organizations
    rather than individuals) using our existing `nwinners_full` spider to deliver
    the items. First, we add a `DropNonPersons` pipeline to the `pipelines.py` module
    of our project:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_heavyweight_scraping_with_scrapy_CO6-1)'
  prefs: []
  type: TYPE_NORMAL
- en: If our scraped item failed to find a gender property at Wikidata, it is probably
    an organization such as the Red Cross. Our visualization is focused on individual
    winners, so here we use `DropItem` to remove the item from our output stream.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_heavyweight_scraping_with_scrapy_CO6-2)'
  prefs: []
  type: TYPE_NORMAL
- en: We need to return the item to further pipelines or for saving by Scrapy.
  prefs: []
  type: TYPE_NORMAL
- en: 'As mentioned in the `pipelines.py` header, in order to add this pipeline to
    the spiders of our project, we need to register it in the `settings.py` module
    by adding it to a `dict` of pipelines and setting it to active (`1`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Now that we’ve got the basic workflow for our pipelines, let’s add a useful
    one to our project.
  prefs: []
  type: TYPE_NORMAL
- en: Scraping Text and Images with a Pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We now want to scrape the winners’ biographies and photos (see [Figure 6-1](#scrapy_targets)),
    where available. We can scrape the biographical text using the same method as
    our last spider, but the photos are best dealt with by an image pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: We could easily write our own pipeline to take a scraped image URL, request
    it from Wikipedia, and save to disk, but to do it properly requires a bit of care.
    For example, we would like to avoid reloading an image that was recently downloaded
    or hasn’t changed in the meantime. Some flexibility in specifying where to store
    the images is a useful feature. It would also be good to have the option of converting
    the images into a common format (e.g., JPG or PNG) or of generating thumbnails.
    Luckily, Scrapy provides an `ImagesPipeline` object with all this functionality
    and more. This is one of its [media pipelines](https://oreil.ly/y9vAT), which
    includes a `FilesPipeline` for dealing with general files.
  prefs: []
  type: TYPE_NORMAL
- en: We could add the image and biography-text scraping to our existing `nwinners_full`
    spider, but that’s starting to get a little large, and segregating this character
    data from the more formal categories makes sense. So we’ll create a new spider
    called `nwinners_minibio` that will reuse parts of the previous spider’s `parse`
    method in order to loop through the Nobel winners.
  prefs: []
  type: TYPE_NORMAL
- en: As usual, when creating a Scrapy spider, our first job is to get the xpaths
    for our scraping targets—​in this case, where available that’s the first part
    of the winners’ biographical text and a photograph of them. To do this, we fire
    up Chrome Elements and explore the HTML source of the biography pages looking
    for the targets shown in [Figure 6-8](#scrapy_crick).
  prefs: []
  type: TYPE_NORMAL
- en: '![dpj2 0608](assets/dpj2_0608.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6-8\. The target elements for our biography scraping: the first part
    of the biography (A) marked by a stop point (B), and the winner’s photograph (C)'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Example 6-6\. Scraping the biographical text
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Investigating with Chrome Elements (see [Example 6-6](#scrapy_bio_paras)) shows
    the biographical text ([Figure 6-8](#scrapy_crick) A) is contained in child paragraphs
    of the `div` with class `mw-parser-output`, which is a child of the `div` with
    ID `mw-content-text`. The paragraphs are sandwiched between a `table` with class
    `infobox` and a table-of-contents `div` with ID `toc`. We can use the xpath `following-sibling`
    and `preceding-sibling` operators to craft a selector that captures the target
    paragraphs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_heavyweight_scraping_with_scrapy_CO7-1)'
  prefs: []
  type: TYPE_NORMAL
- en: All paragraphs following the first table in the child `div` of the `div` with
    ID `mw-content-text`.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_heavyweight_scraping_with_scrapy_CO7-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Exclude (not) all paragraphs that have a preceding sibling `div` with ID `toc`.
  prefs: []
  type: TYPE_NORMAL
- en: Testing this with the Scrapy shell shows it consistently captures the Nobel
    winners’ mini-bios.
  prefs: []
  type: TYPE_NORMAL
- en: 'Further exploration of the winners’ pages shows that their photos ([Figure 6-8](#scrapy_crick)
    C) are contained in a table of class `infobox` and are the only image tags (`<img>`)
    in that table:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: The xpath `'//table[contains(@class,"infobox")]//img/@src` will get the source
    address of the image.
  prefs: []
  type: TYPE_NORMAL
- en: 'As with our first spider, we first need to declare a Scrapy `Item` to hold
    our scraped data. We’ll scrape the bio link and name of the winner, which we can
    use as identifiers for the image and text. We also need somewhere to store our
    `image-urls` (though we will only scrape one bio image, I’ll cover the multiple-image
    use case), the resultant images references (a file path), and a `bio_image` field
    to store the particular image we’re interested in:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we reuse the scraping loop over our Nobel Prize winners (see [Example 6-4](#scrapy_yield)
    for details), this time yielding a request to our new `get_mini_bio` method, which
    will scrape the image URLs and bio text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Our `get_mini_bio` method will add any available photo URLs to the `image_urls`
    list and add all paragraphs of the biography up to the `<p></p>` stop point to
    the item’s `mini_bio` field:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_heavyweight_scraping_with_scrapy_CO8-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Targets the first (and only) image in the table of class `infobox` and gets
    its source (`src`) attribute (e.g., `<img src='//upload.wikime⁠dia.org/​.../Max_Perutz.jpg'...`).
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_heavyweight_scraping_with_scrapy_CO8-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Grab our mini-bio paragraphs in a sibling sandwich.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_heavyweight_scraping_with_scrapy_CO8-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Replaces Wikipedia’s internal hrefs (e.g., */wiki/…​*) with the full addresses
    our visualization will need.
  prefs: []
  type: TYPE_NORMAL
- en: With our bio-scraping spider defined, we need to create its complementary pipeline,
    which will take the image URLs scraped and convert them into saved images. We’ll
    use Scrapy’s [images pipeline](https://oreil.ly/MqUuX) for this job.
  prefs: []
  type: TYPE_NORMAL
- en: The `ImagesPipeline` shown in [Example 6-7](#scrapy_images_pipeline) has two
    main methods, `get_media_requests`, which generates the requests for the image
    URLs, and `item_completed`, called after the requests have been consumed.
  prefs: []
  type: TYPE_NORMAL
- en: Example 6-7\. Scraping images with the image pipeline
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_heavyweight_scraping_with_scrapy_CO9-1)'
  prefs: []
  type: TYPE_NORMAL
- en: This takes any image URLs scraped by our *nwinners_minibio* spider and generates
    an HTTP request for their content.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_heavyweight_scraping_with_scrapy_CO9-2)'
  prefs: []
  type: TYPE_NORMAL
- en: After the image URL requests have been made, the results are delivered to the
    `item_completed` method.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_heavyweight_scraping_with_scrapy_CO9-3)'
  prefs: []
  type: TYPE_NORMAL
- en: This Python list comprehension filters the list of result tuples (of form `[(True,
    Image), (False, Image) …​]`) for those that were successful and stores their file
    paths relative to the directory specified by the `IMAGES_STORE` variable in `settings.py`.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_heavyweight_scraping_with_scrapy_CO9-4)'
  prefs: []
  type: TYPE_NORMAL
- en: We use a Scrapy [item adapter](https://oreil.ly/8P6uq), which provides a common
    interface for working with supported item types.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have the spider and pipeline defined, we just need to add the pipeline
    to our `settings.py` module and set the `IMAGES_STORE` variable to the directory
    we want to save the images in:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s run our new spider from the *nobel_winners* root directory of our project,
    and check its output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: The spider is correctly harvesting mini-bios and, using its image pipeline,
    photos of the Nobel winners. The image was stored in `image_urls` and successfully
    processed, loading the JPG file stored in the *images* directory we specified
    with `IMAGE_STORE` with a relative path (`full/a5f763b828006e704cb291411b8b643bfb1886c.jpg`).
    The filename is, conveniently enough, a [SHA1 hash](https://oreil.ly/SlSl2) of
    the image’s URL, which allows the image pipeline to check for existing images,
    enabling it to prevent redundant requests.
  prefs: []
  type: TYPE_NORMAL
- en: 'A quick listing of our images directory shows a nice array of Wikipedia Nobel
    Prize winner images, ready to be used in our web visualization:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: As we’ll see in [Chapter 16](ch16.xhtml#chapter_building_viz), we will be placing
    these in the *static* folder of our web app, ready to be accessed via the winner’s
    `bio_image` field.
  prefs: []
  type: TYPE_NORMAL
- en: With our images and biography text to hand, we’ve successfully scraped all the
    targets we set ourselves at the beginning of the chapter (see [Example 6-1](#scrapy_target_JSON)
    and [Figure 6-1](#scrapy_targets)). Now, it’s time for a quick summary before
    moving on to clean this inevitably dirty data with help from pandas.
  prefs: []
  type: TYPE_NORMAL
- en: Specifying Pipelines with Multiple Spiders
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The pipelines enabled in `settings.py` are applied to all spiders in our Scrapy
    project. Often, if you have a number of spiders, you’ll want to be able to specify
    which pipelines are applied on a spider-by-spider basis. There are a [number of
    ways](https://oreil.ly/62Uzn) to achieve this, but the best I’ve seen is to use
    the spiders’ `custom_settings` class property to set the `ITEM_PIPELINES` dictionary
    instead of setting it in `settings.py`. In the case of our `nwinners_minibio`
    spider, this means adapting the `NWinnerSpiderBio` class like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: Now the `NobelImagesPipeline` pipeline will only be applied while scraping the
    Nobel Prize winners’ biographies.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we produced two Scrapy spiders that managed to grab the simple
    statistical dataset of our Nobel Prize winners plus some biographical text (and,
    where available, a photograph, to add some color to the stats). Scrapy is a powerful
    library that takes care of everything you could need in a full-fledged scraper.
    Although the workflow requires more effort to implement than doing some hacking
    with Beautiful Soup, Scrapy has far more power and comes into its own as your
    scraping ambitions increase. All Scrapy spiders follow the standard recipe demonstrated
    here, and the workflow should become routine after you program a few.
  prefs: []
  type: TYPE_NORMAL
- en: I hope this chapter has conveyed the rather hacky, iterative nature of scraping,
    and some of the quiet satisfaction that can be had when producing relatively clean
    data from the unpromising mound of stuff so often found on the web. The fact is
    that now and for the foreseeable future, the large majority of interesting data
    (the fuel for the art and science of data visualization) is trapped in a form
    that is unusable for the web-based visualizations that this book focuses on. Scraping
    is, in this sense, an emancipating endeavor.
  prefs: []
  type: TYPE_NORMAL
- en: The data we scraped, much of it human-edited, will certainly have some errors—​from
    badly formatted dates to categorical anomalies to missing fields. Making that
    data presentable is the focus of the next pandas-based chapters. But first, we
    need a little introduction to pandas and its building block, NumPy.
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch06.xhtml#idm45607786166832-marker)) See [the Scrapy install docs](https://oreil.ly/LamAt)
    for platform-specific details.
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch06.xhtml#idm45607785183776-marker)) There are some handy online tools
    for testing regexes, some of them programming-language-specific. [Pyregex](http://www.pyregex.com)
    is a good Python one, with a handy cheat sheet included.
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](ch06.xhtml#idm45607784791328-marker)) The author got stung by this removal.
  prefs: []
  type: TYPE_NORMAL
- en: ^([4](ch06.xhtml#idm45607784789392-marker)) See [Wikipedia](https://oreil.ly/pLVcE)
    for an explanation.
  prefs: []
  type: TYPE_NORMAL
- en: ^([5](ch06.xhtml#idm45607784684416-marker)) Strictly speaking, there are edits
    being made continually by the Wikipedia community, but the fundamental details
    should be stable until the next set of prizes.
  prefs: []
  type: TYPE_NORMAL
- en: ^([6](ch06.xhtml#idm45607784643296-marker)) See [Jeff Knupp’s blog, “Everything
    I Know About Python”](https://oreil.ly/qgku4), for a nice rundown of Python generators
    and the use of `yield`.
  prefs: []
  type: TYPE_NORMAL
