- en: Chapter 10\. Working with Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Experts often possess more data than judgment.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Colin Powell
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Working with data is both an art and a science. We’ve mostly been talking about
    the science part, but in this chapter we’ll look at some of the art.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring Your Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After you’ve identified the questions you’re trying to answer and have gotten
    your hands on some data, you might be tempted to dive in and immediately start
    building models and getting answers. But you should resist this urge. Your first
    step should be to *explore* your data.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring One-Dimensional Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The simplest case is when you have a one-dimensional dataset, which is just
    a collection of numbers. For example, these could be the daily average number
    of minutes each user spends on your site, the number of times each of a collection
    of data science tutorial videos was watched, or the number of pages of each of
    the data science books in your data science library.
  prefs: []
  type: TYPE_NORMAL
- en: An obvious first step is to compute a few summary statistics. You’d like to
    know how many data points you have, the smallest, the largest, the mean, and the
    standard deviation.
  prefs: []
  type: TYPE_NORMAL
- en: 'But even these don’t necessarily give you a great understanding. A good next
    step is to create a histogram, in which you group your data into discrete *buckets*
    and count how many points fall into each bucket:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'For example, consider the two following sets of data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Both have means close to 0 and standard deviations close to 58. However, they
    have very different distributions. [Figure 10-1](#histogram_uniform) shows the
    distribution of `uniform`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'while [Figure 10-2](#histogram_normal) shows the distribution of `normal`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![Histogram of uniform](assets/dsf2_1001.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-1\. Histogram of uniform
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In this case the two distributions have a pretty different `max` and `min`,
    but even knowing that wouldn’t have been sufficient to understand *how* they differed.
  prefs: []
  type: TYPE_NORMAL
- en: Two Dimensions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now imagine you have a dataset with two dimensions. Maybe in addition to daily
    minutes you have years of data science experience. Of course you’d want to understand
    each dimension individually. But you probably also want to scatter the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, consider another fake dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: If you were to run `plot_histogram` on `ys1` and `ys2`, you’d get similar-looking
    plots (indeed, both are normally distributed with the same mean and standard deviation).
  prefs: []
  type: TYPE_NORMAL
- en: '![Histogram of normal](assets/dsf2_1002.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-2\. Histogram of normal
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'But each has a very different joint distribution with `xs`, as shown in [Figure 10-3](#scatter_ys1_ys2):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![Scattering two different ys''s](assets/dsf2_1003.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-3\. Scattering two different ys
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'This difference would also be apparent if you looked at the correlations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Many Dimensions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'With many dimensions, you’d like to know how all the dimensions relate to one
    another. A simple approach is to look at the *correlation matrix*, in which the
    entry in row *i* and column *j* is the correlation between the *i*th dimension
    and the *j*th dimension of the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'A more visual approach (if you don’t have too many dimensions) is to make a
    *scatterplot matrix* ([Figure 10-4](#scatterplot_matrix)) showing all the pairwise
    scatterplots. To do that we’ll use `plt.subplots`, which allows us to create subplots
    of our chart. We give it the number of rows and the number of columns, and it
    returns a `figure` object (which we won’t use) and a two-dimensional array of
    `axes` objects (each of which we’ll plot to):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '![Scatterplot matrix](assets/dsf2_1004.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-4\. Scatterplot matrix
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Looking at the scatterplots, you can see that series 1 is very negatively correlated
    with series 0, series 2 is positively correlated with series 1, and series 3 only
    takes on the values 0 and 6, with 0 corresponding to small values of series 2
    and 6 corresponding to large values.
  prefs: []
  type: TYPE_NORMAL
- en: This is a quick way to get a rough sense of which of your variables are correlated
    (unless you spend hours tweaking matplotlib to display things exactly the way
    you want them to, in which case it’s not a quick way).
  prefs: []
  type: TYPE_NORMAL
- en: Using NamedTuples
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One common way of representing data is using `dict`s:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: There are several reasons why this is less than ideal, however. This is a slightly
    inefficient representation (a `dict` involves some overhead), so that if you have
    a lot of stock prices they’ll take up more memory than they have to. For the most
    part, this is a minor consideration.
  prefs: []
  type: TYPE_NORMAL
- en: 'A larger issue is that accessing things by `dict` key is error-prone. The following
    code will run without error and just do the wrong thing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, while we can type-annotate uniform dictionaries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: there’s no helpful way to annotate dictionaries-as-data that have lots of different
    value types. So we also lose the power of type hints.
  prefs: []
  type: TYPE_NORMAL
- en: 'As an alternative, Python includes a `namedtuple` class, which is like a `tuple`
    but with named slots:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Like regular `tuple`s, `namedtuple`s are immutable, which means that you can’t
    modify their values once they’re created. Occasionally this will get in our way,
    but mostly that’s a good thing.
  prefs: []
  type: TYPE_NORMAL
- en: 'You’ll notice that we still haven’t solved the type annotation issue. We do
    that by using the typed variant, `NamedTuple`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: And now your editor can help you out, as shown in [Figure 10-5](#helpful_editor).
  prefs: []
  type: TYPE_NORMAL
- en: '![Helpful editor](assets/dsf2_1005.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-5\. Helpful editor
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Very few people use `NamedTuple` in this way. But they should!
  prefs: []
  type: TYPE_NORMAL
- en: Dataclasses
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Dataclasses are (sort of) a mutable version of `NamedTuple`. (I say “sort of”
    because `NamedTuple`s represent their data compactly as tuples, whereas dataclasses
    are regular Python classes that simply generate some methods for you automatically.)
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Dataclasses are new in Python 3.7. If you’re using an older version, this section
    won’t work for you.
  prefs: []
  type: TYPE_NORMAL
- en: 'The syntax is very similar to `NamedTuple`. But instead of inheriting from
    a base class, we use a decorator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'As mentioned, the big difference is that we can modify a dataclass instance’s
    values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: If we tried to modify a field of the `NamedTuple` version, we’d get an `AttributeError`.
  prefs: []
  type: TYPE_NORMAL
- en: 'This also leaves us susceptible to the kind of errors we were hoping to avoid
    by not using `dict`s:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: We won’t be using dataclasses, but you may encounter them out in the wild.
  prefs: []
  type: TYPE_NORMAL
- en: Cleaning and Munging
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Real-world data is *dirty*. Often you’ll have to do some work on it before you
    can use it. We saw examples of this in [Chapter 9](ch09.html#getting_data). We
    have to convert strings to `float`s or `int`s before we can use them. We have
    to check for missing values and outliers and bad data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Previously, we did that right before using the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'But it’s probably less error-prone to do the parsing in a function that we
    can test:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: What if there’s bad data? A “float” value that doesn’t actually represent a
    number? Maybe you’d rather get a `None` than crash your program?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'For example, if we have comma-delimited stock prices with bad data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'we can now read and return only the valid rows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: and decide what we want to do about the invalid ones. Generally speaking, the
    three options are to get rid of them, to go back to the source and try to fix
    the bad/missing data, or to do nothing and cross our fingers. If there’s one bad
    row out of millions, it’s probably okay to ignore it. But if half your rows have
    bad data, that’s something you need to fix.
  prefs: []
  type: TYPE_NORMAL
- en: A good next step is to check for outliers, using techniques from [“Exploring
    Your Data”](#exploring_your_data) or by ad hoc investigating. For example, did
    you notice that one of the dates in the stocks file had the year 3014? That won’t
    (necessarily) give you an error, but it’s quite plainly wrong, and you’ll get
    screwy results if you don’t catch it. Real-world datasets have missing decimal
    points, extra zeros, typographical errors, and countless other problems that it’s
    your job to catch. (Maybe it’s not officially your job, but who else is going
    to do it?)
  prefs: []
  type: TYPE_NORMAL
- en: Manipulating Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the most important skills of a data scientist is *manipulating data*.
    It’s more of a general approach than a specific technique, so we’ll just work
    through a handful of examples to give you the flavor of it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Imagine we have a bunch of stock price data that looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Let’s start asking questions about this data. Along the way we’ll try to notice
    patterns in what we’re doing and abstract out some tools to make the manipulation
    easier.
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, suppose we want to know the highest-ever closing price for AAPL.
    Let’s break this down into concrete steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Restrict ourselves to AAPL rows.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Grab the `closing_price` from each row.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Take the `max` of those prices.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We can do all three at once using a comprehension:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'More generally, we might want to know the highest-ever closing price for each
    stock in our dataset. One way to do this is:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a `dict` to keep track of highest prices (we’ll use a `defaultdict` that
    returns minus infinity for missing values, since any price will be greater than
    that).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Iterate over our data, updating it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Here’s the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now start to ask more complicated things, like what are the largest
    and smallest one-day percent changes in our dataset. The percent change is `price_today
    / price_yesterday - 1`, which means we need some way of associating today’s price
    and yesterday’s price. One approach is to group the prices by symbol, and then,
    within each group:'
  prefs: []
  type: TYPE_NORMAL
- en: Order the prices by date.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use `zip` to get (previous, current) pairs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Turn the pairs into new “percent change” rows.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let’s start by grouping the prices by symbol:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Since the prices are tuples, they’ll get sorted by their fields in order: first
    by symbol, then by date, then by price. This means that if we have some prices
    all with the same symbol, `sort` will sort them by date (and then by price, which
    does nothing, since we only have one per date), which is what we want.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'which we can use to compute a sequence of day-over-day changes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'and then collect them all:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'At which point it’s easy to find the largest and smallest:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now use this new `all_changes` dataset to find which month is the best
    to invest in tech stocks. We’ll just look at the average daily change by month:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: We’ll be doing these sorts of manipulations throughout the book, usually without
    calling too much explicit attention to them.
  prefs: []
  type: TYPE_NORMAL
- en: Rescaling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Many techniques are sensitive to the *scale* of your data. For example, imagine
    that you have a dataset consisting of the heights and weights of hundreds of data
    scientists, and that you are trying to identify *clusters* of body sizes.
  prefs: []
  type: TYPE_NORMAL
- en: Intuitively, we’d like clusters to represent points near each other, which means
    that we need some notion of distance between points. We already have a Euclidean
    `distance` function, so a natural approach might be to treat (height, weight)
    pairs as points in two-dimensional space. Consider the people listed in [Table 10-1](#heights-and-weights).
  prefs: []
  type: TYPE_NORMAL
- en: Table 10-1\. Heights and weights
  prefs: []
  type: TYPE_NORMAL
- en: '| Person | Height (inches) | Height (centimeters) | Weight (pounds) |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| A | 63 | 160 | 150 |'
  prefs: []
  type: TYPE_TB
- en: '| B | 67 | 170.2 | 160 |'
  prefs: []
  type: TYPE_TB
- en: '| C | 70 | 177.8 | 171 |'
  prefs: []
  type: TYPE_TB
- en: 'If we measure height in inches, then B’s nearest neighbor is A:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'However, if we measure height in centimeters, then B’s nearest neighbor is
    instead C:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Obviously it’s a problem if changing units can change results like this. For
    this reason, when dimensions aren’t comparable with one another, we will sometimes
    *rescale* our data so that each dimension has mean 0 and standard deviation 1\.
    This effectively gets rid of the units, converting each dimension to “standard
    deviations from the mean.”
  prefs: []
  type: TYPE_NORMAL
- en: 'To start with, we’ll need to compute the `mean` and the `standard_deviation`
    for each position:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then use them to create a new dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Of course, let’s write a test to conform that `rescale` does what we think
    it should:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: As always, you need to use your judgment. If you were to take a huge dataset
    of heights and weights and filter it down to only the people with heights between
    69.5 inches and 70.5 inches, it’s quite likely (depending on the question you’re
    trying to answer) that the variation remaining is simply *noise*, and you might
    not want to put its standard deviation on equal footing with other dimensions’
    deviations.
  prefs: []
  type: TYPE_NORMAL
- en: 'An Aside: tqdm'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Frequently we’ll end up doing computations that take a long time. When you’re
    doing such work, you’d like to know that you’re making progress and how long you
    should expect to wait.
  prefs: []
  type: TYPE_NORMAL
- en: One way of doing this is with the `tqdm` library, which generates custom progress
    bars. We’ll use it some throughout the rest of the book, so let’s take this chance
    to learn how it works.
  prefs: []
  type: TYPE_NORMAL
- en: 'To start with, you’ll need to install it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'There are only a few features you need to know about. The first is that an
    iterable wrapped in `tqdm.tqdm` will produce a progress bar:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'which produces an output that looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: In particular, it shows you what fraction of your loop is done (though it can’t
    do this if you use a generator), how long it’s been running, and how long it expects
    to run.
  prefs: []
  type: TYPE_NORMAL
- en: In this case (where we are just wrapping a call to `range`) you can just use
    `tqdm.trange`.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also set the description of the progress bar while it’s running. To
    do that, you need to capture the `tqdm` iterator in a `with` statement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'This adds a description like the following, with a counter that updates as
    new primes are discovered:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Using `tqdm` will occasionally make your code flaky—sometimes the screen redraws
    poorly, and sometimes the loop will simply hang. And if you accidentally wrap
    a `tqdm` loop inside another `tqdm` loop, strange things might happen. Typically
    its benefits outweigh these downsides, though, so we’ll try to use it whenever
    we have slow-running computations.
  prefs: []
  type: TYPE_NORMAL
- en: Dimensionality Reduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sometimes the “actual” (or useful) dimensions of the data might not correspond
    to the dimensions we have. For example, consider the dataset pictured in [Figure 10-6](#scatter_pca_data).
  prefs: []
  type: TYPE_NORMAL
- en: '![Data with the ''wrong'' axes](assets/dsf2_1006.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-6\. Data with the “wrong” axes
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Most of the variation in the data seems to be along a single dimension that
    doesn’t correspond to either the x-axis or the y-axis.
  prefs: []
  type: TYPE_NORMAL
- en: When this is the case, we can use a technique called *principal component analysis*
    (PCA) to extract one or more dimensions that capture as much of the variation
    in the data as possible.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In practice, you wouldn’t use this technique on such a low-dimensional dataset.
    Dimensionality reduction is mostly useful when your dataset has a large number
    of dimensions and you want to find a small subset that captures most of the variation.
    Unfortunately, that case is difficult to illustrate in a two-dimensional book
    format.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a first step, we’ll need to translate the data so that each dimension has
    mean 0:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: (If we don’t do this, our techniques are likely to identify the mean itself
    rather than the variation in the data.)
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 10-7](#pca_data_mean_removed) shows the example data after de-meaning.'
  prefs: []
  type: TYPE_NORMAL
- en: '![PCA data with mean removed.](assets/dsf2_1007.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-7\. Data after de-meaning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Now, given a de-meaned matrix *X*, we can ask which is the direction that captures
    the greatest variance in the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, given a direction `d` (a vector of magnitude 1), each row `x`
    in the matrix extends `dot(x, d)` in the `d` direction. And every nonzero vector
    `w` determines a direction if we rescale it to have magnitude 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Therefore, given a nonzero vector `w`, we can compute the variance of our dataset
    in the direction determined by `w`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'We’d like to find the direction that maximizes this variance. We can do this
    using gradient descent, as soon as we have the gradient function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'And now the first principal component that we have is just the direction that
    maximizes the `directional_variance` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: On the de-meaned dataset, this returns the direction `[0.924, 0.383]`, which
    does appear to capture the primary axis along which our data varies ([Figure 10-8](#pca_data_with_first_principal_component)).
  prefs: []
  type: TYPE_NORMAL
- en: '![PCA data with first component.](assets/dsf2_1008.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-8\. First principal component
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Once we’ve found the direction that’s the first principal component, we can
    project our data onto it to find the values of that component:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'If we want to find further components, we first remove the projections from
    the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: Because this example dataset is only two-dimensional, after we remove the first
    component, what’s left will be effectively one-dimensional ([Figure 10-9](#pca_data_with_first_component_removed)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Data after removing first principal component](assets/dsf2_1009.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-9\. Data after removing the first principal component
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: At that point, we can find the next principal component by repeating the process
    on the result of `remove_projection` ([Figure 10-10](#second_principal_component)).
  prefs: []
  type: TYPE_NORMAL
- en: 'On a higher-dimensional dataset, we can iteratively find as many components
    as we want:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then *transform* our data into the lower-dimensional space spanned by
    the components:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: This technique is valuable for a couple of reasons. First, it can help us clean
    our data by eliminating noise dimensions and consolidating highly correlated dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: '![First two principal components.](assets/dsf2_1010.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-10\. First two principal components
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Second, after extracting a low-dimensional representation of our data, we can
    use a variety of techniques that don’t work as well on high-dimensional data.
    We’ll see examples of such techniques throughout the book.
  prefs: []
  type: TYPE_NORMAL
- en: At the same time, while this technique can help you build better models, it
    can also make those models harder to interpret. It’s easy to understand conclusions
    like “every extra year of experience adds an average of $10k in salary.” It’s
    much harder to make sense of “every increase of 0.1 in the third principal component
    adds an average of $10k in salary.”
  prefs: []
  type: TYPE_NORMAL
- en: For Further Exploration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As mentioned at the end of [Chapter 9](ch09.html#getting_data), [pandas](http://pandas.pydata.org/)
    is probably the primary Python tool for cleaning, munging, manipulating, and working
    with data. All the examples we did by hand in this chapter could be done much
    more simply using pandas. [*Python for Data Analysis*](https://learning.oreilly.com/library/view/python-for-data/9781491957653/)
    (O’Reilly), by Wes McKinney, is probably the best way to learn pandas.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: scikit-learn has a wide variety of [matrix decomposition](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.decomposition)
    functions, including PCA.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
