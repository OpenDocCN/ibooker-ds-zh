<html><head></head><body><section data-pdf-bookmark="Chapter 20. Clustering" data-type="chapter" epub:type="chapter"><div class="chapter" id="clustering">&#13;
<h1><span class="label">Chapter 20. </span>Clustering</h1>&#13;
&#13;
<blockquote data-type="epigraph" epub:type="epigraph">&#13;
    <p>Where we such clusters had</p>&#13;
    <p>As made us nobly wild, not mad</p>&#13;
    <p data-type="attribution">Robert Herrick</p>&#13;
</blockquote>&#13;
&#13;
<p>Most<a data-primary="clustering" data-secondary="unsupervised learning using" data-type="indexterm" id="idm45635724122152"/><a data-primary="unsupervised learning" data-type="indexterm" id="idm45635724121128"/> of the algorithms in this book are what’s known as <em>supervised learning</em> algorithms, in that they start with a set of labeled data and use that as the basis for making predictions about new, unlabeled data.  Clustering, however, is an example of <em>unsupervised learning</em>, in which we work with completely unlabeled data (or in which our data has labels but we ignore them).</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="The Idea" data-type="sect1"><div class="sect1" id="idm45635724119096">&#13;
<h1>The Idea</h1>&#13;
&#13;
<p>Whenever<a data-primary="clustering" data-secondary="concept of" data-type="indexterm" id="idm45635724117768"/> you look at some source of data,&#13;
it’s likely that the data will somehow form <em>clusters</em>.&#13;
A dataset showing where millionaires live probably has clusters&#13;
in places like Beverly Hills and Manhattan.  A dataset showing how&#13;
many hours people work each week probably has a cluster around 40&#13;
(and if it’s taken from a state with laws mandating special benefits&#13;
for people who work at least 20 hours a week, it probably has another&#13;
cluster right around 19).  A dataset of demographics of registered voters&#13;
likely forms a variety of clusters&#13;
(e.g., “soccer moms,” “bored retirees,” “unemployed millennials”)&#13;
that pollsters and political consultants consider relevant.</p>&#13;
&#13;
<p>Unlike some of the problems we’ve looked at, there is generally no “correct” clustering.&#13;
An alternative clustering scheme might group some of the “unemployed millennials” with&#13;
“grad students,” and others with “parents’ basement dwellers.”  Neither scheme is necessarily&#13;
more correct—instead, each is likely more optimal with respect to its own&#13;
“how good are the clusters?” metric.</p>&#13;
&#13;
<p>Furthermore, the clusters won’t label themselves.  You’ll have to do that by&#13;
looking at the data underlying each one.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="The Model" data-type="sect1"><div class="sect1" id="idm45635724113992">&#13;
<h1>The Model</h1>&#13;
&#13;
<p>For<a data-primary="clustering" data-secondary="model for" data-type="indexterm" id="idm45635724112264"/> us, each <code>input</code> will be a vector in <em>d</em>-dimensional space, which, as usual, we will represent as a list of numbers.  Our goal will be to identify clusters of similar inputs and (sometimes) to find a representative value for each cluster.</p>&#13;
&#13;
<p>For example, each input could be a numeric vector that represents the title of a blog post, in which case the goal might be to find clusters of similar posts, perhaps in order to understand what our users are blogging about.  Or imagine that we have a picture containing thousands of <code>(red, green, blue)</code> colors and that we need to screen-print a 10-color version of it.  Clustering can help us choose 10 colors that will minimize the total “color error.”</p>&#13;
&#13;
<p>One<a data-primary="k-means clustering" data-type="indexterm" id="idm45635724108328"/> of the simplest clustering methods is <em>k</em>-means, in which the number of clusters <em>k</em> is chosen in advance, after which the goal is to partition the inputs into sets <math>&#13;
  <mrow>&#13;
    <msub><mi>S</mi> <mn>1</mn> </msub>&#13;
    <mo>,</mo>&#13;
    <mo>...</mo>&#13;
    <mo>,</mo>&#13;
    <msub><mi>S</mi> <mi>k</mi> </msub>&#13;
  </mrow>&#13;
</math> in a way that minimizes the total sum of squared distances from each point to the mean of its assigned cluster.</p>&#13;
&#13;
<p>There are a lot of ways to assign <em>n</em> points to <em>k</em> clusters, which means that finding an optimal clustering is a very hard problem.  We’ll settle for an iterative algorithm that usually finds a good clustering:</p>&#13;
<ol>&#13;
<li>&#13;
<p>Start with a set of <em>k</em>-means, which are points in <em>d</em>-dimensional space.</p>&#13;
</li>&#13;
<li>&#13;
<p>Assign each point to the mean to which it is closest.</p>&#13;
</li>&#13;
<li>&#13;
<p>If no point’s assignment has changed, stop and keep the clusters.</p>&#13;
</li>&#13;
<li>&#13;
<p>If some point’s assignment has changed, recompute the means and return to <span class="keep-together">step 2.</span></p>&#13;
</li>&#13;
&#13;
</ol>&#13;
&#13;
<p>Using the <code>vector_mean</code> function from <a data-type="xref" href="ch04.html#linear_algebra">Chapter 4</a>, it’s pretty simple to create a class that does this.</p>&#13;
&#13;
<p>To start with, we’ll create a helper function that measures how many coordinates two vectors differ in. We’ll use this to track our training progress:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="kn">from</code> <code class="nn">scratch.linear_algebra</code> <code class="kn">import</code> <code class="n">Vector</code>&#13;
&#13;
<code class="k">def</code> <code class="nf">num_differences</code><code class="p">(</code><code class="n">v1</code><code class="p">:</code> <code class="n">Vector</code><code class="p">,</code> <code class="n">v2</code><code class="p">:</code> <code class="n">Vector</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">int</code><code class="p">:</code>&#13;
    <code class="k">assert</code> <code class="nb">len</code><code class="p">(</code><code class="n">v1</code><code class="p">)</code> <code class="o">==</code> <code class="nb">len</code><code class="p">(</code><code class="n">v2</code><code class="p">)</code>&#13;
    <code class="k">return</code> <code class="nb">len</code><code class="p">([</code><code class="n">x1</code> <code class="k">for</code> <code class="n">x1</code><code class="p">,</code> <code class="n">x2</code> <code class="ow">in</code> <code class="nb">zip</code><code class="p">(</code><code class="n">v1</code><code class="p">,</code> <code class="n">v2</code><code class="p">)</code> <code class="k">if</code> <code class="n">x1</code> <code class="o">!=</code> <code class="n">x2</code><code class="p">])</code>&#13;
&#13;
<code class="k">assert</code> <code class="n">num_differences</code><code class="p">([</code><code class="mi">1</code><code class="p">,</code> <code class="mi">2</code><code class="p">,</code> <code class="mi">3</code><code class="p">],</code> <code class="p">[</code><code class="mi">2</code><code class="p">,</code> <code class="mi">1</code><code class="p">,</code> <code class="mi">3</code><code class="p">])</code> <code class="o">==</code> <code class="mi">2</code>&#13;
<code class="k">assert</code> <code class="n">num_differences</code><code class="p">([</code><code class="mi">1</code><code class="p">,</code> <code class="mi">2</code><code class="p">],</code> <code class="p">[</code><code class="mi">1</code><code class="p">,</code> <code class="mi">2</code><code class="p">])</code> <code class="o">==</code> <code class="mi">0</code></pre>&#13;
&#13;
<p>We also need a function that, given some vectors and their assignments to clusters,&#13;
computes the means of the clusters. It may be the case that some cluster has no points&#13;
assigned to it. We can’t take the mean of an empty collection, so in that case we’ll&#13;
just randomly pick one of the points to serve as the “mean” of that cluster:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="kn">from</code> <code class="nn">typing</code> <code class="kn">import</code> <code class="n">List</code>&#13;
<code class="kn">from</code> <code class="nn">scratch.linear_algebra</code> <code class="kn">import</code> <code class="n">vector_mean</code>&#13;
&#13;
<code class="k">def</code> <code class="nf">cluster_means</code><code class="p">(</code><code class="n">k</code><code class="p">:</code> <code class="nb">int</code><code class="p">,</code>&#13;
                  <code class="n">inputs</code><code class="p">:</code> <code class="n">List</code><code class="p">[</code><code class="n">Vector</code><code class="p">],</code>&#13;
                  <code class="n">assignments</code><code class="p">:</code> <code class="n">List</code><code class="p">[</code><code class="nb">int</code><code class="p">])</code> <code class="o">-&gt;</code> <code class="n">List</code><code class="p">[</code><code class="n">Vector</code><code class="p">]:</code>&#13;
    <code class="c1"># clusters[i] contains the inputs whose assignment is i</code>&#13;
    <code class="n">clusters</code> <code class="o">=</code> <code class="p">[[]</code> <code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">k</code><code class="p">)]</code>&#13;
    <code class="k">for</code> <code class="nb">input</code><code class="p">,</code> <code class="n">assignment</code> <code class="ow">in</code> <code class="nb">zip</code><code class="p">(</code><code class="n">inputs</code><code class="p">,</code> <code class="n">assignments</code><code class="p">):</code>&#13;
        <code class="n">clusters</code><code class="p">[</code><code class="n">assignment</code><code class="p">]</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="nb">input</code><code class="p">)</code>&#13;
&#13;
    <code class="c1"># if a cluster is empty, just use a random point</code>&#13;
    <code class="k">return</code> <code class="p">[</code><code class="n">vector_mean</code><code class="p">(</code><code class="n">cluster</code><code class="p">)</code> <code class="k">if</code> <code class="n">cluster</code> <code class="k">else</code> <code class="n">random</code><code class="o">.</code><code class="n">choice</code><code class="p">(</code><code class="n">inputs</code><code class="p">)</code>&#13;
            <code class="k">for</code> <code class="n">cluster</code> <code class="ow">in</code> <code class="n">clusters</code><code class="p">]</code></pre>&#13;
&#13;
<p>And now we’re ready to code up our clusterer. As usual, we’ll use <code>tqdm</code> to track our progress, but here we don’t know how many iterations it will take, so we then use <code>itertools.count</code>, which creates an infinite iterable, and we’ll <code>return</code> out of it when we’re done:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="kn">import</code> <code class="nn">itertools</code>&#13;
<code class="kn">import</code> <code class="nn">random</code>&#13;
<code class="kn">import</code> <code class="nn">tqdm</code>&#13;
<code class="kn">from</code> <code class="nn">scratch.linear_algebra</code> <code class="kn">import</code> <code class="n">squared_distance</code>&#13;
&#13;
<code class="k">class</code> <code class="nc">KMeans</code><code class="p">:</code>&#13;
    <code class="k">def</code> <code class="nf-Magic">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">k</code><code class="p">:</code> <code class="nb">int</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="bp">None</code><code class="p">:</code>&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">k</code> <code class="o">=</code> <code class="n">k</code>                      <code class="c1"># number of clusters</code>&#13;
        <code class="bp">self</code><code class="o">.</code><code class="n">means</code> <code class="o">=</code> <code class="bp">None</code>&#13;
&#13;
    <code class="k">def</code> <code class="nf">classify</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="nb">input</code><code class="p">:</code> <code class="n">Vector</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">int</code><code class="p">:</code>&#13;
        <code class="sd">"""return the index of the cluster closest to the input"""</code>&#13;
        <code class="k">return</code> <code class="nb">min</code><code class="p">(</code><code class="nb">range</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">k</code><code class="p">),</code>&#13;
                   <code class="n">key</code><code class="o">=</code><code class="k">lambda</code> <code class="n">i</code><code class="p">:</code> <code class="n">squared_distance</code><code class="p">(</code><code class="nb">input</code><code class="p">,</code> <code class="bp">self</code><code class="o">.</code><code class="n">means</code><code class="p">[</code><code class="n">i</code><code class="p">]))</code>&#13;
&#13;
    <code class="k">def</code> <code class="nf">train</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">inputs</code><code class="p">:</code> <code class="n">List</code><code class="p">[</code><code class="n">Vector</code><code class="p">])</code> <code class="o">-&gt;</code> <code class="bp">None</code><code class="p">:</code>&#13;
        <code class="c1"># Start with random assignments</code>&#13;
        <code class="n">assignments</code> <code class="o">=</code> <code class="p">[</code><code class="n">random</code><code class="o">.</code><code class="n">randrange</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">k</code><code class="p">)</code> <code class="k">for</code> <code class="n">_</code> <code class="ow">in</code> <code class="n">inputs</code><code class="p">]</code>&#13;
&#13;
        <code class="k">with</code> <code class="n">tqdm</code><code class="o">.</code><code class="n">tqdm</code><code class="p">(</code><code class="n">itertools</code><code class="o">.</code><code class="n">count</code><code class="p">())</code> <code class="k">as</code> <code class="n">t</code><code class="p">:</code>&#13;
            <code class="k">for</code> <code class="n">_</code> <code class="ow">in</code> <code class="n">t</code><code class="p">:</code>&#13;
                <code class="c1"># Compute means and find new assignments</code>&#13;
                <code class="bp">self</code><code class="o">.</code><code class="n">means</code> <code class="o">=</code> <code class="n">cluster_means</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">k</code><code class="p">,</code> <code class="n">inputs</code><code class="p">,</code> <code class="n">assignments</code><code class="p">)</code>&#13;
                <code class="n">new_assignments</code> <code class="o">=</code> <code class="p">[</code><code class="bp">self</code><code class="o">.</code><code class="n">classify</code><code class="p">(</code><code class="nb">input</code><code class="p">)</code> <code class="k">for</code> <code class="nb">input</code> <code class="ow">in</code> <code class="n">inputs</code><code class="p">]</code>&#13;
&#13;
                <code class="c1"># Check how many assignments changed and if we're done</code>&#13;
                <code class="n">num_changed</code> <code class="o">=</code> <code class="n">num_differences</code><code class="p">(</code><code class="n">assignments</code><code class="p">,</code> <code class="n">new_assignments</code><code class="p">)</code>&#13;
                <code class="k">if</code> <code class="n">num_changed</code> <code class="o">==</code> <code class="mi">0</code><code class="p">:</code>&#13;
                    <code class="k">return</code>&#13;
&#13;
                <code class="c1"># Otherwise keep the new assignments, and compute new means</code>&#13;
                <code class="n">assignments</code> <code class="o">=</code> <code class="n">new_assignments</code>&#13;
                <code class="bp">self</code><code class="o">.</code><code class="n">means</code> <code class="o">=</code> <code class="n">cluster_means</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">k</code><code class="p">,</code> <code class="n">inputs</code><code class="p">,</code> <code class="n">assignments</code><code class="p">)</code>&#13;
                <code class="n">t</code><code class="o">.</code><code class="n">set_description</code><code class="p">(</code><code class="n">f</code><code class="s2">"changed: {num_changed} / {len(inputs)}"</code><code class="p">)</code></pre>&#13;
&#13;
<p>Let’s take a look at how this works.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Example: Meetups" data-type="sect1"><div class="sect1" id="idm45635724113368">&#13;
<h1>Example: Meetups</h1>&#13;
&#13;
<p>To<a data-primary="clustering" data-secondary="meetups example" data-type="indexterm" id="idm45635723830792"/><a data-primary="meetups example (clustering)" data-type="indexterm" id="idm45635723604808"/> celebrate DataSciencester’s growth, your VP of User Rewards wants to organize several in-person meetups for your hometown users, complete with beer, pizza, and DataSciencester t-shirts.  You know the locations of all your local users (<a data-type="xref" href="#user_locations">Figure 20-1</a>), and she’d like you to choose meetup locations that make it convenient for everyone to attend.</p>&#13;
&#13;
<figure><div class="figure" id="user_locations">&#13;
<img alt="User locations." src="assets/dsf2_2001.png"/>&#13;
<h6><span class="label">Figure 20-1. </span>The locations of your hometown users</h6>&#13;
</div></figure>&#13;
&#13;
<p>Depending on how you look at it, you probably see two or three clusters.  (It’s easy to do visually because the data is only two-dimensional.  With more dimensions, it would be a lot harder to eyeball.)</p>&#13;
&#13;
<p>Imagine first that she has enough budget for three meetups.  You go to your computer and try this:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="n">random</code><code class="o">.</code><code class="n">seed</code><code class="p">(</code><code class="mi">12</code><code class="p">)</code>                   <code class="c1"># so you get the same results as me</code>&#13;
<code class="n">clusterer</code> <code class="o">=</code> <code class="n">KMeans</code><code class="p">(</code><code class="n">k</code><code class="o">=</code><code class="mi">3</code><code class="p">)</code>&#13;
<code class="n">clusterer</code><code class="o">.</code><code class="n">train</code><code class="p">(</code><code class="n">inputs</code><code class="p">)</code>&#13;
<code class="n">means</code> <code class="o">=</code> <code class="nb">sorted</code><code class="p">(</code><code class="n">clusterer</code><code class="o">.</code><code class="n">means</code><code class="p">)</code>   <code class="c1"># sort for the unit test</code>&#13;
&#13;
<code class="k">assert</code> <code class="nb">len</code><code class="p">(</code><code class="n">means</code><code class="p">)</code> <code class="o">==</code> <code class="mi">3</code>&#13;
&#13;
<code class="c1"># Check that the means are close to what we expect</code>&#13;
<code class="k">assert</code> <code class="n">squared_distance</code><code class="p">(</code><code class="n">means</code><code class="p">[</code><code class="mi">0</code><code class="p">],</code> <code class="p">[</code><code class="o">-</code><code class="mi">44</code><code class="p">,</code> <code class="mi">5</code><code class="p">])</code> <code class="o">&lt;</code> <code class="mi">1</code>&#13;
<code class="k">assert</code> <code class="n">squared_distance</code><code class="p">(</code><code class="n">means</code><code class="p">[</code><code class="mi">1</code><code class="p">],</code> <code class="p">[</code><code class="o">-</code><code class="mi">16</code><code class="p">,</code> <code class="o">-</code><code class="mi">10</code><code class="p">])</code> <code class="o">&lt;</code> <code class="mi">1</code>&#13;
<code class="k">assert</code> <code class="n">squared_distance</code><code class="p">(</code><code class="n">means</code><code class="p">[</code><code class="mi">2</code><code class="p">],</code> <code class="p">[</code><code class="mi">18</code><code class="p">,</code> <code class="mi">20</code><code class="p">])</code> <code class="o">&lt;</code> <code class="mi">1</code></pre>&#13;
&#13;
<p>You find three clusters centered at [–44, 5], [–16, –10], and [18, 20], and you look for meetup venues near those locations (<a data-type="xref" href="#user_locations_3_means">Figure 20-2</a>).</p>&#13;
&#13;
<figure><div class="figure" id="user_locations_3_means">&#13;
<img alt="User locations with 3 means." src="assets/dsf2_2002.png"/>&#13;
<h6><span class="label">Figure 20-2. </span>User locations grouped into three clusters</h6>&#13;
</div></figure>&#13;
&#13;
<p>You show your results to the VP, who informs you that now she only has enough budgeted for <em>two</em> meetups.</p>&#13;
&#13;
<p>“No problem,” you say:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="n">random</code><code class="o">.</code><code class="n">seed</code><code class="p">(</code><code class="mi">0</code><code class="p">)</code>&#13;
<code class="n">clusterer</code> <code class="o">=</code> <code class="n">KMeans</code><code class="p">(</code><code class="n">k</code><code class="o">=</code><code class="mi">2</code><code class="p">)</code>&#13;
<code class="n">clusterer</code><code class="o">.</code><code class="n">train</code><code class="p">(</code><code class="n">inputs</code><code class="p">)</code>&#13;
<code class="n">means</code> <code class="o">=</code> <code class="nb">sorted</code><code class="p">(</code><code class="n">clusterer</code><code class="o">.</code><code class="n">means</code><code class="p">)</code>&#13;
&#13;
<code class="k">assert</code> <code class="nb">len</code><code class="p">(</code><code class="n">means</code><code class="p">)</code> <code class="o">==</code> <code class="mi">2</code>&#13;
<code class="k">assert</code> <code class="n">squared_distance</code><code class="p">(</code><code class="n">means</code><code class="p">[</code><code class="mi">0</code><code class="p">],</code> <code class="p">[</code><code class="o">-</code><code class="mi">26</code><code class="p">,</code> <code class="o">-</code><code class="mi">5</code><code class="p">])</code> <code class="o">&lt;</code> <code class="mi">1</code>&#13;
<code class="k">assert</code> <code class="n">squared_distance</code><code class="p">(</code><code class="n">means</code><code class="p">[</code><code class="mi">1</code><code class="p">],</code> <code class="p">[</code><code class="mi">18</code><code class="p">,</code> <code class="mi">20</code><code class="p">])</code> <code class="o">&lt;</code> <code class="mi">1</code></pre>&#13;
&#13;
<p>As shown in <a data-type="xref" href="#user_locations_2_means">Figure 20-3</a>, one meetup should still be near [18, 20], but now the other should be near [–26, –5].</p>&#13;
&#13;
<figure><div class="figure" id="user_locations_2_means">&#13;
<img alt="User locations with 2 means." src="assets/dsf2_2003.png"/>&#13;
<h6><span class="label">Figure 20-3. </span>User locations grouped into two clusters</h6>&#13;
</div></figure>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Choosing k" data-type="sect1"><div class="sect1" id="idm45635723831800">&#13;
<h1>Choosing k</h1>&#13;
&#13;
<p>In<a data-primary="clustering" data-secondary="choosing k" data-type="indexterm" id="idm45635723319160"/> the previous example, the choice of <em>k</em> was driven by factors outside of our control.  In general, this won’t be the case.  There are various ways to choose a <em>k</em>. One that’s reasonably easy to understand involves plotting the sum of squared errors (between each point and the mean of its cluster) as a function of <em>k</em> and looking at where the graph “bends”:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="kn">from</code> <code class="nn">matplotlib</code> <code class="kn">import</code> <code class="n">pyplot</code> <code class="k">as</code> <code class="n">plt</code>&#13;
&#13;
<code class="k">def</code> <code class="nf">squared_clustering_errors</code><code class="p">(</code><code class="n">inputs</code><code class="p">:</code> <code class="n">List</code><code class="p">[</code><code class="n">Vector</code><code class="p">],</code> <code class="n">k</code><code class="p">:</code> <code class="nb">int</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">float</code><code class="p">:</code>&#13;
    <code class="sd">"""finds the total squared error from k-means clustering the inputs"""</code>&#13;
    <code class="n">clusterer</code> <code class="o">=</code> <code class="n">KMeans</code><code class="p">(</code><code class="n">k</code><code class="p">)</code>&#13;
    <code class="n">clusterer</code><code class="o">.</code><code class="n">train</code><code class="p">(</code><code class="n">inputs</code><code class="p">)</code>&#13;
    <code class="n">means</code> <code class="o">=</code> <code class="n">clusterer</code><code class="o">.</code><code class="n">means</code>&#13;
    <code class="n">assignments</code> <code class="o">=</code> <code class="p">[</code><code class="n">clusterer</code><code class="o">.</code><code class="n">classify</code><code class="p">(</code><code class="nb">input</code><code class="p">)</code> <code class="k">for</code> <code class="nb">input</code> <code class="ow">in</code> <code class="n">inputs</code><code class="p">]</code>&#13;
&#13;
    <code class="k">return</code> <code class="nb">sum</code><code class="p">(</code><code class="n">squared_distance</code><code class="p">(</code><code class="nb">input</code><code class="p">,</code> <code class="n">means</code><code class="p">[</code><code class="n">cluster</code><code class="p">])</code>&#13;
               <code class="k">for</code> <code class="nb">input</code><code class="p">,</code> <code class="n">cluster</code> <code class="ow">in</code> <code class="nb">zip</code><code class="p">(</code><code class="n">inputs</code><code class="p">,</code> <code class="n">assignments</code><code class="p">))</code></pre>&#13;
&#13;
<p>which we can apply to our previous example:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="c1"># now plot from 1 up to len(inputs) clusters</code>&#13;
&#13;
<code class="n">ks</code> <code class="o">=</code> <code class="nb">range</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="nb">len</code><code class="p">(</code><code class="n">inputs</code><code class="p">)</code> <code class="o">+</code> <code class="mi">1</code><code class="p">)</code>&#13;
<code class="n">errors</code> <code class="o">=</code> <code class="p">[</code><code class="n">squared_clustering_errors</code><code class="p">(</code><code class="n">inputs</code><code class="p">,</code> <code class="n">k</code><code class="p">)</code> <code class="k">for</code> <code class="n">k</code> <code class="ow">in</code> <code class="n">ks</code><code class="p">]</code>&#13;
&#13;
<code class="n">plt</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">ks</code><code class="p">,</code> <code class="n">errors</code><code class="p">)</code>&#13;
<code class="n">plt</code><code class="o">.</code><code class="n">xticks</code><code class="p">(</code><code class="n">ks</code><code class="p">)</code>&#13;
<code class="n">plt</code><code class="o">.</code><code class="n">xlabel</code><code class="p">(</code><code class="s2">"k"</code><code class="p">)</code>&#13;
<code class="n">plt</code><code class="o">.</code><code class="n">ylabel</code><code class="p">(</code><code class="s2">"total squared error"</code><code class="p">)</code>&#13;
<code class="n">plt</code><code class="o">.</code><code class="n">title</code><code class="p">(</code><code class="s2">"Total Error vs. # of Clusters"</code><code class="p">)</code>&#13;
<code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">()</code></pre>&#13;
&#13;
<p>Looking at <a data-type="xref" href="#choosing_a_k">Figure 20-4</a>, this method agrees with our original eyeballing that three is the “right” number of clusters.</p>&#13;
&#13;
<figure><div class="figure" id="choosing_a_k">&#13;
<img alt="Choosing a k." src="assets/dsf2_2004.png"/>&#13;
<h6><span class="label">Figure 20-4. </span>Choosing a <em>k</em></h6>&#13;
</div></figure>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Example: Clustering Colors" data-type="sect1"><div class="sect1" id="idm45635723076152">&#13;
<h1>Example: Clustering Colors</h1>&#13;
&#13;
<p>The<a data-primary="clustering" data-secondary="clustering colors example" data-type="indexterm" id="idm45635723074264"/> VP of Swag has designed attractive DataSciencester stickers that&#13;
he’d like you to hand out at meetups.  Unfortunately, your sticker&#13;
printer can print at most five colors per sticker.&#13;
And since the VP of Art is on sabbatical, the VP of Swag asks if there’s some way&#13;
you can take his design and modify it so that it contains only five colors.</p>&#13;
&#13;
<p>Computer images can be represented as two-dimensional arrays of pixels,&#13;
where each pixel is itself a three-dimensional vector <code>(red, green, blue)</code> indicating its color.</p>&#13;
&#13;
<p>Creating a five-color version of the image, then, entails:</p>&#13;
<ol>&#13;
<li>&#13;
<p>Choosing five colors.</p>&#13;
</li>&#13;
<li>&#13;
<p>Assigning one of those colors to each pixel.</p>&#13;
</li>&#13;
&#13;
</ol>&#13;
&#13;
<p>It turns out this is a great task for <em>k</em>-means clustering, which can partition the pixels into five clusters in red-green-blue space.  If we then recolor the pixels in each cluster to the mean color, we’re done.</p>&#13;
&#13;
<p>To start with, we’ll need a way to load an image into Python. We can do this with matplotlib, if we first install the pillow library:</p>&#13;
&#13;
<pre data-type="programlisting">python -m pip install pillow</pre>&#13;
&#13;
<p>Then we can just use <code>matplotlib.image.imread</code>:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="n">image_path</code> <code class="o">=</code> <code class="s-Affix">r</code><code class="s2">"girl_with_book.jpg"</code>    <code class="c1"># wherever your image is</code>&#13;
<code class="kn">import</code> <code class="nn">matplotlib.image</code> <code class="kn">as</code> <code class="nn">mpimg</code>&#13;
<code class="n">img</code> <code class="o">=</code> <code class="n">mpimg</code><code class="o">.</code><code class="n">imread</code><code class="p">(</code><code class="n">image_path</code><code class="p">)</code> <code class="o">/</code> <code class="mi">256</code>  <code class="c1"># rescale to between 0 and 1</code></pre>&#13;
&#13;
<p>Behind the scenes <code>img</code> is a NumPy array,&#13;
but for our purposes, we can treat it as a list of lists of lists.</p>&#13;
&#13;
<p><code>img[i][j]</code> is the pixel in the <em>i</em>th row and <em>j</em>th column,&#13;
and each pixel is a list <code>[red, green, blue]</code> of numbers between 0 and 1&#13;
indicating the <a href="http://en.wikipedia.org/wiki/RGB_color_model">color of that pixel</a>:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="n">top_row</code> <code class="o">=</code> <code class="n">img</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code>&#13;
<code class="n">top_left_pixel</code> <code class="o">=</code> <code class="n">top_row</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code>&#13;
<code class="n">red</code><code class="p">,</code> <code class="n">green</code><code class="p">,</code> <code class="n">blue</code> <code class="o">=</code> <code class="n">top_left_pixel</code></pre>&#13;
&#13;
<p>In particular, we can get a flattened list of all the pixels as:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="c1"># .tolist() converts a NumPy array to a Python list</code>&#13;
<code class="n">pixels</code> <code class="o">=</code> <code class="p">[</code><code class="n">pixel</code><code class="o">.</code><code class="n">tolist</code><code class="p">()</code> <code class="k">for</code> <code class="n">row</code> <code class="ow">in</code> <code class="n">img</code> <code class="k">for</code> <code class="n">pixel</code> <code class="ow">in</code> <code class="n">row</code><code class="p">]</code></pre>&#13;
&#13;
<p>and then feed them to our clusterer:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="n">clusterer</code> <code class="o">=</code> <code class="n">KMeans</code><code class="p">(</code><code class="mi">5</code><code class="p">)</code>&#13;
<code class="n">clusterer</code><code class="o">.</code><code class="n">train</code><code class="p">(</code><code class="n">pixels</code><code class="p">)</code>   <code class="c1"># this might take a while</code></pre>&#13;
&#13;
<p>Once it finishes, we just construct a new image with the same format:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="k">def</code> <code class="nf">recolor</code><code class="p">(</code><code class="n">pixel</code><code class="p">:</code> <code class="n">Vector</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="n">Vector</code><code class="p">:</code>&#13;
    <code class="n">cluster</code> <code class="o">=</code> <code class="n">clusterer</code><code class="o">.</code><code class="n">classify</code><code class="p">(</code><code class="n">pixel</code><code class="p">)</code>        <code class="c1"># index of the closest cluster</code>&#13;
    <code class="k">return</code> <code class="n">clusterer</code><code class="o">.</code><code class="n">means</code><code class="p">[</code><code class="n">cluster</code><code class="p">]</code>            <code class="c1"># mean of the closest cluster</code>&#13;
&#13;
<code class="n">new_img</code> <code class="o">=</code> <code class="p">[[</code><code class="n">recolor</code><code class="p">(</code><code class="n">pixel</code><code class="p">)</code> <code class="k">for</code> <code class="n">pixel</code> <code class="ow">in</code> <code class="n">row</code><code class="p">]</code>   <code class="c1"># recolor this row of pixels</code>&#13;
           <code class="k">for</code> <code class="n">row</code> <code class="ow">in</code> <code class="n">img</code><code class="p">]</code>                     <code class="c1"># for each row in the image</code></pre>&#13;
&#13;
<p>and display it, using <code>plt.imshow</code>:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="n">plt</code><code class="o">.</code><code class="n">imshow</code><code class="p">(</code><code class="n">new_img</code><code class="p">)</code>&#13;
<code class="n">plt</code><code class="o">.</code><code class="n">axis</code><code class="p">(</code><code class="s1">'off'</code><code class="p">)</code>&#13;
<code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">()</code></pre>&#13;
&#13;
<p>It is difficult to show color results in a black-and-white book, but <a data-type="xref" href="#mt_both">Figure 20-5</a> shows grayscale versions&#13;
of a full-color picture and the output of using this process to reduce it to five colors.</p>&#13;
&#13;
<figure><div class="figure" id="mt_both">&#13;
<img alt="Original picture and its 5-means decoloring." src="assets/dsf2_2005.png"/>&#13;
<h6><span class="label">Figure 20-5. </span>Original picture and its 5-means decoloring</h6>&#13;
</div></figure>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Bottom-Up Hierarchical Clustering" data-type="sect1"><div class="sect1" id="idm45635723075208">&#13;
<h1>Bottom-Up Hierarchical Clustering</h1>&#13;
&#13;
<p>An<a data-primary="clustering" data-secondary="bottom-up hierarchical clustering" data-type="indexterm" id="Cbot20"/><a data-primary="bottom-up hierarchical clustering" data-type="indexterm" id="BUHC20"/><a data-primary="hierarchical clustering" data-type="indexterm" id="hier20"/> alternative approach to clustering is to “grow” clusters from the bottom up.  We can do this in the following way:</p>&#13;
<ol>&#13;
<li>&#13;
<p>Make each input its own cluster of one.</p>&#13;
</li>&#13;
<li>&#13;
<p>As long as there are multiple clusters remaining, find the two closest clusters and merge them.</p>&#13;
</li>&#13;
&#13;
</ol>&#13;
&#13;
<p>At the end, we’ll have one giant cluster containing all the inputs.  If we keep track of the merge order, we can re-create any number of clusters by unmerging.  For example, if we want three clusters, we can just undo the last two merges.</p>&#13;
&#13;
<p>We’ll use a really simple representation of clusters.  Our values will live in <em>leaf</em> clusters, which we will represent as <code>NamedTuple</code>s:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="kn">from</code> <code class="nn">typing</code> <code class="kn">import</code> <code class="n">NamedTuple</code><code class="p">,</code> <code class="n">Union</code>&#13;
&#13;
<code class="k">class</code> <code class="nc">Leaf</code><code class="p">(</code><code class="n">NamedTuple</code><code class="p">):</code>&#13;
    <code class="n">value</code><code class="p">:</code> <code class="n">Vector</code>&#13;
&#13;
<code class="n">leaf1</code> <code class="o">=</code> <code class="n">Leaf</code><code class="p">([</code><code class="mi">10</code><code class="p">,</code>  <code class="mi">20</code><code class="p">])</code>&#13;
<code class="n">leaf2</code> <code class="o">=</code> <code class="n">Leaf</code><code class="p">([</code><code class="mi">30</code><code class="p">,</code> <code class="o">-</code><code class="mi">15</code><code class="p">])</code></pre>&#13;
&#13;
<p>We’ll use these to grow <em>merged</em> clusters, which we will also represent as <code>NamedTuple</code>s:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="k">class</code> <code class="nc">Merged</code><code class="p">(</code><code class="n">NamedTuple</code><code class="p">):</code>&#13;
    <code class="n">children</code><code class="p">:</code> <code class="nb">tuple</code>&#13;
    <code class="n">order</code><code class="p">:</code> <code class="nb">int</code>&#13;
&#13;
<code class="n">merged</code> <code class="o">=</code> <code class="n">Merged</code><code class="p">((</code><code class="n">leaf1</code><code class="p">,</code> <code class="n">leaf2</code><code class="p">),</code> <code class="n">order</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>&#13;
&#13;
<code class="n">Cluster</code> <code class="o">=</code> <code class="n">Union</code><code class="p">[</code><code class="n">Leaf</code><code class="p">,</code> <code class="n">Merged</code><code class="p">]</code></pre>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>This is another case where Python’s type annotations have let us down.&#13;
You’d like to type hint <code>Merged.children</code> as <code>Tuple[Cluster, Cluster]</code>&#13;
but <code>mypy</code> doesn’t allow recursive types like that.</p>&#13;
</div>&#13;
&#13;
<p>We’ll talk about merge order in a bit, but first let’s create a helper function&#13;
that recursively returns all the values contained in a (possibly merged) cluster:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="k">def</code> <code class="nf">get_values</code><code class="p">(</code><code class="n">cluster</code><code class="p">:</code> <code class="n">Cluster</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="n">List</code><code class="p">[</code><code class="n">Vector</code><code class="p">]:</code>&#13;
    <code class="k">if</code> <code class="nb">isinstance</code><code class="p">(</code><code class="n">cluster</code><code class="p">,</code> <code class="n">Leaf</code><code class="p">):</code>&#13;
        <code class="k">return</code> <code class="p">[</code><code class="n">cluster</code><code class="o">.</code><code class="n">value</code><code class="p">]</code>&#13;
    <code class="k">else</code><code class="p">:</code>&#13;
        <code class="k">return</code> <code class="p">[</code><code class="n">value</code>&#13;
                <code class="k">for</code> <code class="n">child</code> <code class="ow">in</code> <code class="n">cluster</code><code class="o">.</code><code class="n">children</code>&#13;
                <code class="k">for</code> <code class="n">value</code> <code class="ow">in</code> <code class="n">get_values</code><code class="p">(</code><code class="n">child</code><code class="p">)]</code>&#13;
&#13;
<code class="k">assert</code> <code class="n">get_values</code><code class="p">(</code><code class="n">merged</code><code class="p">)</code> <code class="o">==</code> <code class="p">[[</code><code class="mi">10</code><code class="p">,</code> <code class="mi">20</code><code class="p">],</code> <code class="p">[</code><code class="mi">30</code><code class="p">,</code> <code class="o">-</code><code class="mi">15</code><code class="p">]]</code></pre>&#13;
&#13;
<p>In order to merge the closest clusters, we need some notion of the distance between clusters.  We’ll use the <em>minimum</em> distance between elements of the two clusters, which merges the two clusters that are closest to touching (but will sometimes produce large chain-like clusters that aren’t very tight).  If we wanted tight spherical clusters, we might use the <em>maximum</em> distance instead, as it merges the two clusters that fit in the smallest ball. Both are common choices, as is the <em>average</em> distance:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="kn">from</code> <code class="nn">typing</code> <code class="kn">import</code> <code class="n">Callable</code>&#13;
<code class="kn">from</code> <code class="nn">scratch.linear_algebra</code> <code class="kn">import</code> <code class="n">distance</code>&#13;
&#13;
<code class="k">def</code> <code class="nf">cluster_distance</code><code class="p">(</code><code class="n">cluster1</code><code class="p">:</code> <code class="n">Cluster</code><code class="p">,</code>&#13;
                     <code class="n">cluster2</code><code class="p">:</code> <code class="n">Cluster</code><code class="p">,</code>&#13;
                     <code class="n">distance_agg</code><code class="p">:</code> <code class="n">Callable</code> <code class="o">=</code> <code class="nb">min</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">float</code><code class="p">:</code>&#13;
    <code class="sd">"""</code>&#13;
<code class="sd">    compute all the pairwise distances between cluster1 and cluster2</code>&#13;
<code class="sd">    and apply the aggregation function _distance_agg_ to the resulting list</code>&#13;
<code class="sd">    """</code>&#13;
    <code class="k">return</code> <code class="n">distance_agg</code><code class="p">([</code><code class="n">distance</code><code class="p">(</code><code class="n">v1</code><code class="p">,</code> <code class="n">v2</code><code class="p">)</code>&#13;
                         <code class="k">for</code> <code class="n">v1</code> <code class="ow">in</code> <code class="n">get_values</code><code class="p">(</code><code class="n">cluster1</code><code class="p">)</code>&#13;
                         <code class="k">for</code> <code class="n">v2</code> <code class="ow">in</code> <code class="n">get_values</code><code class="p">(</code><code class="n">cluster2</code><code class="p">)])</code></pre>&#13;
&#13;
<p>We’ll use the merge order slot to track the order in which we did the merging.  Smaller numbers will represent <em>later</em> merges.  This means when we want to unmerge clusters, we do so from lowest merge order to highest.  Since <code>Leaf</code> clusters were never merged, we’ll assign them infinity, the highest possible value. And since they don’t have an <code>.order</code> property, we’ll create a helper function:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="k">def</code> <code class="nf">get_merge_order</code><code class="p">(</code><code class="n">cluster</code><code class="p">:</code> <code class="n">Cluster</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">float</code><code class="p">:</code>&#13;
    <code class="k">if</code> <code class="nb">isinstance</code><code class="p">(</code><code class="n">cluster</code><code class="p">,</code> <code class="n">Leaf</code><code class="p">):</code>&#13;
        <code class="k">return</code> <code class="nb">float</code><code class="p">(</code><code class="s1">'inf'</code><code class="p">)</code>  <code class="c1"># was never merged</code>&#13;
    <code class="k">else</code><code class="p">:</code>&#13;
        <code class="k">return</code> <code class="n">cluster</code><code class="o">.</code><code class="n">order</code></pre>&#13;
&#13;
<p>Similarly, since <code>Leaf</code> clusters don’t have children, we’ll create and add a helper function for that:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="kn">from</code> <code class="nn">typing</code> <code class="kn">import</code> <code class="n">Tuple</code>&#13;
&#13;
<code class="k">def</code> <code class="nf">get_children</code><code class="p">(</code><code class="n">cluster</code><code class="p">:</code> <code class="n">Cluster</code><code class="p">):</code>&#13;
    <code class="k">if</code> <code class="nb">isinstance</code><code class="p">(</code><code class="n">cluster</code><code class="p">,</code> <code class="n">Leaf</code><code class="p">):</code>&#13;
        <code class="k">raise</code> <code class="ne">TypeError</code><code class="p">(</code><code class="s2">"Leaf has no children"</code><code class="p">)</code>&#13;
    <code class="k">else</code><code class="p">:</code>&#13;
        <code class="k">return</code> <code class="n">cluster</code><code class="o">.</code><code class="n">children</code></pre>&#13;
&#13;
<p>Now we’re ready to create the clustering algorithm:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="k">def</code> <code class="nf">bottom_up_cluster</code><code class="p">(</code><code class="n">inputs</code><code class="p">:</code> <code class="n">List</code><code class="p">[</code><code class="n">Vector</code><code class="p">],</code>&#13;
                      <code class="n">distance_agg</code><code class="p">:</code> <code class="n">Callable</code> <code class="o">=</code> <code class="nb">min</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="n">Cluster</code><code class="p">:</code>&#13;
    <code class="c1"># Start with all leaves</code>&#13;
    <code class="n">clusters</code><code class="p">:</code> <code class="n">List</code><code class="p">[</code><code class="n">Cluster</code><code class="p">]</code> <code class="o">=</code> <code class="p">[</code><code class="n">Leaf</code><code class="p">(</code><code class="nb">input</code><code class="p">)</code> <code class="k">for</code> <code class="nb">input</code> <code class="ow">in</code> <code class="n">inputs</code><code class="p">]</code>&#13;
&#13;
    <code class="k">def</code> <code class="nf">pair_distance</code><code class="p">(</code><code class="n">pair</code><code class="p">:</code> <code class="n">Tuple</code><code class="p">[</code><code class="n">Cluster</code><code class="p">,</code> <code class="n">Cluster</code><code class="p">])</code> <code class="o">-&gt;</code> <code class="nb">float</code><code class="p">:</code>&#13;
        <code class="k">return</code> <code class="n">cluster_distance</code><code class="p">(</code><code class="n">pair</code><code class="p">[</code><code class="mi">0</code><code class="p">],</code> <code class="n">pair</code><code class="p">[</code><code class="mi">1</code><code class="p">],</code> <code class="n">distance_agg</code><code class="p">)</code>&#13;
&#13;
    <code class="c1"># as long as we have more than one cluster left...</code>&#13;
    <code class="k">while</code> <code class="nb">len</code><code class="p">(</code><code class="n">clusters</code><code class="p">)</code> <code class="o">&gt;</code> <code class="mi">1</code><code class="p">:</code>&#13;
        <code class="c1"># find the two closest clusters</code>&#13;
        <code class="n">c1</code><code class="p">,</code> <code class="n">c2</code> <code class="o">=</code> <code class="nb">min</code><code class="p">(((</code><code class="n">cluster1</code><code class="p">,</code> <code class="n">cluster2</code><code class="p">)</code>&#13;
                      <code class="k">for</code> <code class="n">i</code><code class="p">,</code> <code class="n">cluster1</code> <code class="ow">in</code> <code class="nb">enumerate</code><code class="p">(</code><code class="n">clusters</code><code class="p">)</code>&#13;
                      <code class="k">for</code> <code class="n">cluster2</code> <code class="ow">in</code> <code class="n">clusters</code><code class="p">[:</code><code class="n">i</code><code class="p">]),</code>&#13;
                      <code class="n">key</code><code class="o">=</code><code class="n">pair_distance</code><code class="p">)</code>&#13;
&#13;
        <code class="c1"># remove them from the list of clusters</code>&#13;
        <code class="n">clusters</code> <code class="o">=</code> <code class="p">[</code><code class="n">c</code> <code class="k">for</code> <code class="n">c</code> <code class="ow">in</code> <code class="n">clusters</code> <code class="k">if</code> <code class="n">c</code> <code class="o">!=</code> <code class="n">c1</code> <code class="ow">and</code> <code class="n">c</code> <code class="o">!=</code> <code class="n">c2</code><code class="p">]</code>&#13;
&#13;
        <code class="c1"># merge them, using merge_order = # of clusters left</code>&#13;
        <code class="n">merged_cluster</code> <code class="o">=</code> <code class="n">Merged</code><code class="p">((</code><code class="n">c1</code><code class="p">,</code> <code class="n">c2</code><code class="p">),</code> <code class="n">order</code><code class="o">=</code><code class="nb">len</code><code class="p">(</code><code class="n">clusters</code><code class="p">))</code>&#13;
&#13;
        <code class="c1"># and add their merge</code>&#13;
        <code class="n">clusters</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">merged_cluster</code><code class="p">)</code>&#13;
&#13;
    <code class="c1"># when there's only one cluster left, return it</code>&#13;
    <code class="k">return</code> <code class="n">clusters</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code></pre>&#13;
&#13;
<p>Its use is very simple:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="n">base_cluster</code> <code class="o">=</code> <code class="n">bottom_up_cluster</code><code class="p">(</code><code class="n">inputs</code><code class="p">)</code></pre>&#13;
&#13;
<p>This produces a clustering that looks as follows:</p>&#13;
&#13;
<pre data-type="programlisting">  0  1  2  3  4  5  6  7  8  9  10 11 12 13 14 15 16 17 18&#13;
──┬──┬─────┬────────────────────────────────┬───────────┬─ [19, 28]&#13;
  │  │     │                                │           └─ [21, 27]&#13;
  │  │     │                                └─ [20, 23]&#13;
  │  │     └─ [26, 13]&#13;
  │  └────────────────────────────────────────────┬─ [11, 15]&#13;
  │                                               └─ [13, 13]&#13;
  └─────┬─────┬──┬───────────┬─────┬─ [-49, 0]&#13;
        │     │  │           │     └─ [-46, 5]&#13;
        │     │  │           └─ [-41, 8]&#13;
        │     │  └─ [-49, 15]&#13;
        │     └─ [-34, 1]&#13;
        └───────────┬──┬──┬─────┬─ [-22, -16]&#13;
                    │  │  │     └─ [-19, -11]&#13;
                    │  │  └─ [-25, -9]&#13;
                    │  └─────────────────┬─────┬─────┬─ [-11, -6]&#13;
                    │                    │     │     └─ [-12, -8]&#13;
                    │                    │     └─ [-14, 5]&#13;
                    │                    └─ [-18, -3]&#13;
                    └─────────────────┬─ [-13, -19]&#13;
                                      └─ [-9, -16]</pre>&#13;
&#13;
<p>The numbers at the top indicate “merge order.”&#13;
Since we had 20 inputs, it took 19 merges to get to this one cluster.&#13;
The first merge created cluster 18 by combining the&#13;
leaves [19, 28] and [21, 27].  And the last merge created cluster 0.</p>&#13;
&#13;
<p>If you wanted only two clusters, you’d split at the first fork (“0”),&#13;
creating one cluster with six points and a second with the rest. For three clusters, you’d continue to the second fork (“1”), which&#13;
indicates to split that first cluster into the cluster with&#13;
([19, 28], [21, 27], [20, 23], [26, 13]) and the cluster with&#13;
([11, 15], [13, 13]). And so on.</p>&#13;
&#13;
<p>Generally, though, we don’t want to be squinting at nasty text representations like this. Instead, let’s write a function that generates any number of clusters by performing the appropriate number of unmerges:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="k">def</code> <code class="nf">generate_clusters</code><code class="p">(</code><code class="n">base_cluster</code><code class="p">:</code> <code class="n">Cluster</code><code class="p">,</code>&#13;
                      <code class="n">num_clusters</code><code class="p">:</code> <code class="nb">int</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="n">List</code><code class="p">[</code><code class="n">Cluster</code><code class="p">]:</code>&#13;
    <code class="c1"># start with a list with just the base cluster</code>&#13;
    <code class="n">clusters</code> <code class="o">=</code> <code class="p">[</code><code class="n">base_cluster</code><code class="p">]</code>&#13;
&#13;
    <code class="c1"># as long as we don't have enough clusters yet...</code>&#13;
    <code class="k">while</code> <code class="nb">len</code><code class="p">(</code><code class="n">clusters</code><code class="p">)</code> <code class="o">&lt;</code> <code class="n">num_clusters</code><code class="p">:</code>&#13;
        <code class="c1"># choose the last-merged of our clusters</code>&#13;
        <code class="n">next_cluster</code> <code class="o">=</code> <code class="nb">min</code><code class="p">(</code><code class="n">clusters</code><code class="p">,</code> <code class="n">key</code><code class="o">=</code><code class="n">get_merge_order</code><code class="p">)</code>&#13;
        <code class="c1"># remove it from the list</code>&#13;
        <code class="n">clusters</code> <code class="o">=</code> <code class="p">[</code><code class="n">c</code> <code class="k">for</code> <code class="n">c</code> <code class="ow">in</code> <code class="n">clusters</code> <code class="k">if</code> <code class="n">c</code> <code class="o">!=</code> <code class="n">next_cluster</code><code class="p">]</code>&#13;
&#13;
        <code class="c1"># and add its children to the list (i.e., unmerge it)</code>&#13;
        <code class="n">clusters</code><code class="o">.</code><code class="n">extend</code><code class="p">(</code><code class="n">get_children</code><code class="p">(</code><code class="n">next_cluster</code><code class="p">))</code>&#13;
&#13;
    <code class="c1"># once we have enough clusters...</code>&#13;
    <code class="k">return</code> <code class="n">clusters</code></pre>&#13;
&#13;
<p>So, for example, if we want to generate three clusters, we can just do:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="n">three_clusters</code> <code class="o">=</code> <code class="p">[</code><code class="n">get_values</code><code class="p">(</code><code class="n">cluster</code><code class="p">)</code>&#13;
                  <code class="k">for</code> <code class="n">cluster</code> <code class="ow">in</code> <code class="n">generate_clusters</code><code class="p">(</code><code class="n">base_cluster</code><code class="p">,</code> <code class="mi">3</code><code class="p">)]</code></pre>&#13;
&#13;
<p>which we can easily plot:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="k">for</code> <code class="n">i</code><code class="p">,</code> <code class="n">cluster</code><code class="p">,</code> <code class="n">marker</code><code class="p">,</code> <code class="n">color</code> <code class="ow">in</code> <code class="nb">zip</code><code class="p">([</code><code class="mi">1</code><code class="p">,</code> <code class="mi">2</code><code class="p">,</code> <code class="mi">3</code><code class="p">],</code>&#13;
                                     <code class="n">three_clusters</code><code class="p">,</code>&#13;
                                     <code class="p">[</code><code class="s1">'D'</code><code class="p">,</code><code class="s1">'o'</code><code class="p">,</code><code class="s1">'*'</code><code class="p">],</code>&#13;
                                     <code class="p">[</code><code class="s1">'r'</code><code class="p">,</code><code class="s1">'g'</code><code class="p">,</code><code class="s1">'b'</code><code class="p">]):</code>&#13;
    <code class="n">xs</code><code class="p">,</code> <code class="n">ys</code> <code class="o">=</code> <code class="nb">zip</code><code class="p">(</code><code class="o">*</code><code class="n">cluster</code><code class="p">)</code>  <code class="c1"># magic unzipping trick</code>&#13;
    <code class="n">plt</code><code class="o">.</code><code class="n">scatter</code><code class="p">(</code><code class="n">xs</code><code class="p">,</code> <code class="n">ys</code><code class="p">,</code> <code class="n">color</code><code class="o">=</code><code class="n">color</code><code class="p">,</code> <code class="n">marker</code><code class="o">=</code><code class="n">marker</code><code class="p">)</code>&#13;
&#13;
    <code class="c1"># put a number at the mean of the cluster</code>&#13;
    <code class="n">x</code><code class="p">,</code> <code class="n">y</code> <code class="o">=</code> <code class="n">vector_mean</code><code class="p">(</code><code class="n">cluster</code><code class="p">)</code>&#13;
    <code class="n">plt</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">x</code><code class="p">,</code> <code class="n">y</code><code class="p">,</code> <code class="n">marker</code><code class="o">=</code><code class="s1">'$'</code> <code class="o">+</code> <code class="nb">str</code><code class="p">(</code><code class="n">i</code><code class="p">)</code> <code class="o">+</code> <code class="s1">'$'</code><code class="p">,</code> <code class="n">color</code><code class="o">=</code><code class="s1">'black'</code><code class="p">)</code>&#13;
&#13;
<code class="n">plt</code><code class="o">.</code><code class="n">title</code><code class="p">(</code><code class="s2">"User Locations -- 3 Bottom-Up Clusters, Min"</code><code class="p">)</code>&#13;
<code class="n">plt</code><code class="o">.</code><code class="n">xlabel</code><code class="p">(</code><code class="s2">"blocks east of city center"</code><code class="p">)</code>&#13;
<code class="n">plt</code><code class="o">.</code><code class="n">ylabel</code><code class="p">(</code><code class="s2">"blocks north of city center"</code><code class="p">)</code>&#13;
<code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">()</code></pre>&#13;
&#13;
<p>This gives very different results than <em>k</em>-means did, as shown in <a data-type="xref" href="#hierarchical_clustering_image">Figure 20-6</a>.</p>&#13;
&#13;
<figure><div class="figure" id="hierarchical_clustering_image">&#13;
<img alt="Three Bottom-Up Clusters Using Min Distance." src="assets/dsf2_2006.png"/>&#13;
<h6><span class="label">Figure 20-6. </span>Three bottom-up clusters using min distance</h6>&#13;
</div></figure>&#13;
&#13;
<p>As mentioned previously, this is because using <code>min</code> in <code>cluster_distance</code> tends to give chain-like clusters.  If we instead use <code>max</code> (which gives tight clusters), it looks the same as the 3-means result (<a data-type="xref" href="#hierarchical_clustering_image_max">Figure 20-7</a>).</p>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>The previous <code>bottom_up_clustering</code> implementation is relatively simple, but also shockingly inefficient. In particular, it recomputes the distance between each pair of inputs at every step. A more efficient implementation might instead precompute the distances between each pair of inputs and then perform a lookup inside <code>cluster_distance</code>.  A <em>really</em> efficient implementation would likely also remember the <code>cluster_distance</code>s from the previous step.<a data-primary="" data-startref="hier20" data-type="indexterm" id="idm45635721766584"/><a data-primary="" data-startref="BUHC20" data-type="indexterm" id="idm45635721765608"/><a data-primary="" data-startref="Cbot20" data-type="indexterm" id="idm45635721764664"/></p>&#13;
</div>&#13;
&#13;
<figure><div class="figure" id="hierarchical_clustering_image_max">&#13;
<img alt="Three Bottom-Up Clusters Using Max Distance." src="assets/dsf2_2007.png"/>&#13;
<h6><span class="label">Figure 20-7. </span>Three bottom-up clusters using max distance</h6>&#13;
</div></figure>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="For Further Exploration" data-type="sect1"><div class="sect1" id="idm45635722795336">&#13;
<h1>For Further Exploration</h1>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>scikit-learn has<a data-primary="clustering" data-secondary="tools for" data-type="indexterm" id="idm45635721759960"/><a data-primary="scikit-learn" data-type="indexterm" id="idm45635721758952"/> an entire module, <a href="http://scikit-learn.org/stable/modules/clustering.html"><code>sklearn.cluster</code></a>, that contains several clustering algorithms including <code>KMeans</code> and the <code>Ward</code> hierarchical clustering algorithm (which uses a different criterion for merging clusters than ours did).</p>&#13;
</li>&#13;
<li>&#13;
<p><a href="http://www.scipy.org/">SciPy</a> has two<a data-primary="SciPy" data-type="indexterm" id="idm45635721754824"/> clustering models: <code>scipy.cluster.vq</code>, which does <em>k</em>-means, and <code>scipy.cluster.hierarchy</code>, which has a variety of hierarchical clustering algorithms.</p>&#13;
</li>&#13;
</ul>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section></body></html>