- en: Chapter 26\. Data Ethics
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第26章 数据伦理
- en: Grub first, then ethics.
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 先吃饭，然后考虑伦理。
- en: ''
  id: totrans-2
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Bertolt Brecht
  id: totrans-3
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 贝托尔特·布莱希特
- en: What Is Data Ethics?
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是数据伦理？
- en: With the use of data comes the misuse of data. This has pretty much always been
    the case, but recently this idea has been reified as “data ethics” and has featured
    somewhat prominently in the news.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 随着数据的使用，数据的滥用也随之而来。这几乎一直如此，但最近这个想法已经被具体化为“数据伦理”，并在新闻中占据了一定的位置。
- en: For instance, in the 2016 election, a company called Cambridge Analytica [improperly
    accessed Facebook data](https://en.wikipedia.org/wiki/Facebook%E2%80%93Cambridge_Analytica_data_scandal)
    and used that for political ad targeting.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在2016年的选举中，一家名为剑桥分析公司的公司[不当获取了Facebook数据](https://en.wikipedia.org/wiki/Facebook%E2%80%93Cambridge_Analytica_data_scandal)，并将其用于政治广告定向投放。
- en: In 2018, an autonomous car being tested by Uber [struck and killed a pedestrian](https://www.nytimes.com/2018/05/24/technology/uber-autonomous-car-ntsb-investigation.html)
    (there was a “safety driver” in the car, but apparently she was not paying attention
    at the time).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在2018年，由Uber测试的自动驾驶汽车[撞死了一名行人](https://www.nytimes.com/2018/05/24/technology/uber-autonomous-car-ntsb-investigation.html)（汽车上有一名“安全驾驶员”，但显然她当时没有注意）。
- en: Algorithms are used [to predict the risk that criminals will reoffend](https://www.themarshallproject.org/2015/08/04/the-new-science-of-sentencing)
    and to sentence them accordingly. Is this more or less fair than allowing judges
    to determine the same?
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 算法被用来[预测罪犯再犯的风险](https://www.themarshallproject.org/2015/08/04/the-new-science-of-sentencing)并据此判刑。这比允许法官做出相同判断更公平吗？
- en: Some airlines [assign families separate seats](https://twitter.com/ShelkeGaneshB/status/1066161967105216512),
    forcing them to pay extra to sit together. Should a data scientist have stepped
    in to prevent this? (Many data scientists in the linked thread seem to believe
    so.)
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 一些航空公司[给家庭分配单独的座位](https://twitter.com/ShelkeGaneshB/status/1066161967105216512)，强迫他们额外付费才能坐在一起。一个数据科学家应该介入阻止这种情况吗？（链接线程中的许多数据科学家似乎认为应该如此。）
- en: “Data ethics” purports to provide answers to these questions, or at least a
    framework for wrestling with them. I’m not so arrogant as to tell you *how* to
    think about these things (and “these things” are changing quickly), so in this
    chapter we’ll just take a quick tour of some of the most relevant issues and (hopefully)
    inspire you to think about them further. (Alas, I am not a good enough philosopher
    to do ethics *from scratch*.)
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: “数据伦理”自称提供了对这些问题的答案，或者至少提供了一个处理这些问题的框架。我并不傲慢到告诉你如何考虑这些事情（而且“这些事情”正在迅速变化），所以在本章中，我们将快速浏览一些最相关的问题，并（希望）激发你进一步思考这些问题。
    （遗憾的是，我不是一个足够好的哲学家，无法从零开始进行伦理思考。）
- en: No, Really, What Is Data Ethics?
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 不，真的，什么是数据伦理？
- en: Well, let’s start with “what is ethics?” If you take the average of every definition
    you can find, you end up with something like *ethics* is a framework for thinking
    about “right” and “wrong” behavior. *Data* ethics, then, is a framework for thinking
    about right and wrong behavior involving data.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，让我们从“什么是伦理学？”开始。如果你总结每一个你能找到的定义的平均值，你最终会得到类似于*伦理学*是一个思考“正确”和“错误”行为的框架。*数据*伦理，因此，是一个思考涉及数据的正确和错误行为的框架。
- en: Some people talk as if “data ethics” is (perhaps implicitly) a set of commandments
    about what you may and may not do. Some of them are hard at work creating manifestos,
    others crafting mandatory pledges to which they hope to make you swear. Still
    others are campaigning for data ethics to be made a mandatory part of the data
    science curriculum—hence this chapter, as a means of hedging my bets in case they
    succeed.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 有些人谈论“数据伦理”似乎是（也许是隐含地）关于你可以做什么和不可以做什么的一套戒律。有些人正在努力创建宣言，其他人正在制定希望你发誓遵守的强制性承诺。还有一些人正在努力让数据伦理成为数据科学课程的强制组成部分——因此本章，作为一种在他们成功的情况下敲定我的赌注的方式。
- en: Note
  id: totrans-14
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注
- en: Curiously, [there is not much data suggesting that ethics courses lead to ethical
    behavior](https://www.washingtonpost.com/news/on-leadership/wp/2014/01/13/can-you-teach-businessmen-to-be-ethical),
    in which case perhaps this campaign is itself data-unethical!
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 令人好奇的是，[没有太多数据表明伦理课程会导致道德行为](https://www.washingtonpost.com/news/on-leadership/wp/2014/01/13/can-you-teach-businessmen-to-be-ethical)，在这种情况下，也许这场运动本身就是数据不道德的表现！
- en: Other people (for example, yours truly) think that reasonable people will frequently
    disagree over subtle matters of right and wrong, and that the important part of
    data ethics is committing to *consider* the ethical consequences of your behaviors.
    This requires *understanding* the sorts of things that many “data ethics” advocates
    don’t approve of, but it doesn’t necessarily require agreeing with their disapproval.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 其他人（例如，诚挚地）认为，合理的人经常会在对错的微妙问题上意见分歧，并且数据伦理的重要部分是承诺*考虑*你的行为的伦理后果。这需要*理解*许多“数据伦理”倡导者不赞同的事情，但不一定需要同意他们的反对意见。
- en: Should I Care About Data Ethics?
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我应该关心数据伦理吗？
- en: You should care about ethics whatever your job. If your job involves data, you
    are free to characterize your caring as “data ethics,” but you should care just
    as much about ethics in the nondata parts of your job.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 无论你的工作是什么，你都应该关注伦理问题。如果你的工作涉及数据，你可以自由地将你的关心称为“数据伦理”，但你也应该同样关心工作中与数据无关的伦理问题。
- en: Perhaps what’s different about technology jobs is that technology *scales*,
    and that decisions made by individuals working on technology problems (whether
    data-related or not) have potentially wide-reaching effects.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 或许技术工作不同之处在于技术*扩展*，个人在解决技术问题时（无论是与数据相关还是其他）做出的决策可能具有潜在的广泛影响。
- en: A tiny change to a news discovery algorithm could be the difference between
    millions of people reading an article and no one reading it.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 改动一点点新闻发现算法可能会导致成百上千的人阅读一篇文章，或者没有人阅读它。
- en: A single flawed algorithm for granting parole that’s used all over the country
    systematically affects millions of people, whereas a flawed-in-its-own-way parole
    board affects only the people who come before it.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 一个单一有缺陷的假释算法在全国范围内使用，系统性地影响数百万人，而一个自身存在缺陷的假释委员会只影响到前来面见它的人。
- en: So yes, in general, you should care about what effects your work has on the
    world. And the broader the effects of your work, the more you need to worry about
    these things.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，是的，总体而言，你应该关心你的工作对世界的影响。而你的工作影响越广泛，你就越需要担心这些事情。
- en: Unfortunately, some of the discourse around data ethics involves people trying
    to force their ethical conclusions on you. Whether you should care about the same
    things *they* care about is really up to you.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，围绕数据伦理的一些讨论涉及到人们试图把他们的伦理结论强加给你。你是否应该关心他们关心的事情，这确实取决于你自己。
- en: Building Bad Data Products
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建糟糕的数据产品
- en: Some “data ethics” issues are the result of building *bad products*.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 一些“数据伦理”问题源于构建*糟糕的产品*。
- en: For example, Microsoft [released a chat bot named Tay](https://en.wikipedia.org/wiki/Tay_(bot))
    that parroted back things tweeted to it, which the internet quickly discovered
    enabled them to get Tay to tweet all sorts of offensive things. It seems unlikely
    that anyone at Microsoft debated the ethicality of releasing a “racist” bot; most
    likely they simply built a bot and failed to think through how it could be abused.
    This is perhaps a low bar, but let’s agree that you should think about how the
    things you build could be abused.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，微软[发布了一个名为Tay的聊天机器人](https://en.wikipedia.org/wiki/Tay_(bot))，它会复述对它发推特的内容，互联网很快发现这使得他们能让Tay发表各种冒犯性的言论。看起来微软没有讨论发布这个“种族主义”机器人的伦理性；很可能他们只是简单地制作了一个机器人，但未能深思其可能被滥用的后果。这可能是一个低门槛，但让我们一致认为你应该考虑你所构建的东西可能如何被滥用。
- en: Another example is that Google Photos at one point [used an image recognition
    algorithm that would sometimes classify pictures of black people as “gorillas”](https://www.theverge.com/2018/1/12/16882408/google-racist-gorillas-photo-recognition-algorithm-ai).
    Again, it is extremely unlikely that anyone at Google *explicitly decided* to
    ship this feature (let alone grappled with the “ethics” of it). Here it seems
    likely the problem is some combination of bad training data, model inaccuracy,
    and the gross offensiveness of the mistake (if the model had occasionally categorized
    mailboxes as fire trucks, probably no one would have cared).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个例子是，Google Photos 曾经使用一个图像识别算法，有时会将黑人的照片分类为“大猩猩”。同样，几乎没有人认为谷歌有*明确决定*发布这一功能（更不用说在“伦理”方面苦苦挣扎了）。在这里，问题很可能是训练数据的问题，模型的不准确性，以及这个错误的极其冒犯性（如果模型偶尔将邮箱分类为消防车，可能没有人会在意）。
- en: 'In this case the solution is less obvious: how can you ensure that your trained
    model won’t make predictions that are in some way offensive? Of course you should
    train (and test) your model on a diverse range of inputs, but can you ever be
    sure that there isn’t *some* input somewhere out there that will make your model
    behave in a way that embarrasses you? This is a hard problem. (Google seems to
    have “solved” it by simply refusing to ever predict “gorilla.”)'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，解决方案不太明显：你如何确保你训练的模型不会做出在某种程度上冒犯性的预测？当然，你应该在各种输入上训练（和测试）你的模型，但你能确保你的模型永远不会出现某种让你感到尴尬的输入吗？这是一个难题。（谷歌似乎通过简单地拒绝预测“大猩猩”来“解决”了这个问题。）
- en: Trading Off Accuracy and Fairness
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 平衡准确性与公平性
- en: Imagine you are building a model that predicts how likely people are to take
    some action. You do a pretty good job ([Table 26-1](#pretty-good-job)).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，你正在建立一个模型，预测人们采取某些行动的可能性。你做得相当不错（[表格 26-1](#pretty-good-job)）。
- en: Table 26-1\. A pretty good job
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 26-1\. 做得相当不错
- en: '| Prediction | People | Actions | % |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| 预测 | 人们 | 行动 | % |'
- en: '| --- | --- | --- | --- |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Unlikely | 125 | 25 | 20% |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| 不可能 | 125 | 25 | 20% |'
- en: '| Likely | 125 | 75 | 60% |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| 可能 | 125 | 75 | 60% |'
- en: Of the people you predict are unlikely to take the action, only 20% of them
    do. Of the people you predict are likely to take the action, 60% of them do. Seems
    not terrible.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 你预测的人中，有20%的人不太可能采取行动。而你预测的人中，有60%的人采取了行动。看起来不太糟糕。
- en: 'Now imagine that the people can be split into two groups: A and B. Some of
    your colleagues are concerned that your model is *unfair* to one of the groups.
    Although the model does not take group membership into account, it does consider
    various other factors that correlate in complicated ways with group membership.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 现在想象一下，人们可以分为两组：A和B。你的一些同事担心你的模型对其中一组是*不公平*的。虽然模型不考虑组别成员资格，但它确实考虑了与组别成员资格相关的各种以复杂方式相关的其他因素。
- en: Indeed, when you break down the predictions by group, you discover surprising
    statistics ([Table 26-2](#surprising-statistics)).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，当你按组别分解预测时，你会发现一些令人惊讶的统计数据（[表格 26-2](#surprising-statistics)）。
- en: Table 26-2\. Surprising statistics
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 26-2\. 令人惊讶的统计数据
- en: '| Group | Prediction | People | Actions | % |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| 组别 | 预测 | 人们 | 行动 | % |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| A | Unlikely | 100 | 20 | 20% |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| A | 不可能 | 100 | 20 | 20% |'
- en: '| A | Likely | 25 | 15 | 60% |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| A | 可能 | 25 | 15 | 60% |'
- en: '| B | Unlikely | 25 | 5 | 20% |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| B | 不可能 | 25 | 5 | 20% |'
- en: '| B | Likely | 100 | 60 | 60% |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| B | 可能 | 100 | 60 | 60% |'
- en: 'Is your model unfair? The data scientists on your team make a variety of arguments:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 你的模型不公平吗？你团队的数据科学家提出了各种论点：
- en: Argument 1
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: Argument 1
- en: Your model classifies 80% of group A as “unlikely” but 80% of group B as “likely.”
    This data scientist complains that the model is treating the two groups unfairly
    in the sense that it is generating vastly different predictions across the two
    groups.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 你的模型将80%的A组分类为“不可能”，但将80%的B组分类为“可能”。这位数据科学家抱怨说，模型在某种程度上不公平地对待了两组，因为它在两组之间生成了截然不同的预测。
- en: Argument 2
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: Argument 2
- en: Regardless of group membership, if we predict “unlikely” you have a 20% chance
    of action, and if we predict “likely” you have a 60% chance of action. This data
    scientist insists that the model is “accurate” in the sense that its predictions
    seem to *mean* the same things no matter which group you belong to.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 无论组别成员资格如何，如果我们预测“不可能”，你有20%的行动机会，如果我们预测“可能”，你有60%的行动机会。这位数据科学家坚持认为，模型在某种意义上是“准确”的，因为它的预测似乎无论你属于哪个组，都*意味着*相同的事情。
- en: Argument 3
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: Argument 3
- en: 40/125 = 32% of group B were falsely labeled “likely,” whereas only 10/125 =
    8% of group A were falsely labeled “likely.” This data scientist (who considers
    a “likely” prediction to be a bad thing) insists that the model unfairly stigmatizes
    group B.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: B组的40/125 = 32%被错误标记为“可能”，而A组的10/125 = 8%被错误标记为“可能”。这位数据科学家（认为“可能”预测是一件坏事）坚持认为模型不公平地污名化了B组。
- en: Argument 4
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: Argument 4
- en: 20/125 = 16% of group A were falsely labeled “unlikely,” whereas only 5/125
    = 4% of group B were falsely labeled “unlikely.” This data scientist (who considers
    an “unlikely” prediction to be a bad thing) insists that the model unfairly stigmatizes
    group A.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 20/125 = 16%的A组被错误标记为“不可能”，而只有5/125 = 4%的B组被错误标记为“不可能”。这位数据科学家（认为“不可能”预测是一件坏事）坚持认为模型不公平地污名化了A组。
- en: Which of these data scientists is correct? Are any of them correct? Perhaps
    it depends on the context.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数据科学家中哪些是正确的？有没有正确的？也许这取决于情境。
- en: Possibly you feel one way if the two groups are “men” and “women” and another
    way if the two groups are “R users” and “Python users.” Or possibly not if it
    turns out that Python users skew male and R users skew female?
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 可能当两组是“男性”和“女性”时，您的感觉会有所不同；当两组是“R 用户”和“Python 用户”时，您的感觉又会有所不同。或者，如果Python用户偏向男性而R用户偏向女性，可能也不会有不同的感觉？
- en: Possibly you feel one way if the model is for predicting whether a DataSciencester
    user will *apply* for a job through the DataSciencester job board and another
    way if the model is predicting whether a user will *pass* such an interview.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 如果模型用于预测DataSciencester用户是否将通过DataSciencester求职板申请工作，您可能会有一种感觉；如果模型用于预测用户是否将*通过*这样的面试，您可能会有另一种感觉。
- en: Possibly your opinion depends on the model itself, what features it takes into
    account, and what data it was trained on.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 可能您的意见取决于模型本身，它考虑了哪些特征以及它训练的数据。
- en: In any event, my point is to impress upon you that there can be a tradeoff between
    “accuracy” and “fairness” (depending, of course, on how you define them) and that
    these tradeoffs don’t always have obvious “right” solutions.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 无论如何，我的观点是要向您强调“准确性”和“公平性”之间可能存在权衡（当然，这取决于您如何定义它们），而这些权衡并不总是有明显的“正确”解决方案。
- en: Collaboration
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 合作
- en: A repressive (by your standards) country’s government officials have finally
    decided to allow citizens to join DataSciencester. However, they insist that the
    users from their country not be allowed to discuss deep learning. Furthermore,
    they want you to report to them the names of any users who even *try* to seek
    out information on deep learning.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 一个压制（按您的标准）国家的政府官员最终决定允许公民加入DataSciencester。然而，他们坚持要求来自他们国家的用户不得讨论深度学习。此外，他们希望您向他们报告任何试图寻找深度学习信息的用户的姓名，即使他们只是*尝试*寻找。
- en: Are this country’s data scientists better off with access to the topic-limited
    (and surveilled) DataSciencester that you’d be allowed to offer? Or are the proposed
    restrictions so awful that they’d be better off with no access at all?
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这个国家的数据科学家是否更适合访问您将被允许提供的主题限制（并受到监视的）DataSciencester？还是建议的限制如此可怕，以至于他们干脆不访问？
- en: Interpretability
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可解释性
- en: The DataSciencester HR department asks you to develop a model predicting which
    employees are most at risk of leaving the company, so that it can intervene and
    try to make them happier. (Attrition rate is an important component of the “10
    Happiest Workplaces” magazine feature that your CEO aspires to appear in.)
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: DataSciencester HR部门要求您开发一个模型，预测哪些员工最有可能离开公司，以便他们可以进行干预并试图让他们更快乐。 （离职率是您的CEO渴望出现在“10个最幸福工作场所”杂志特写中的重要组成部分。）
- en: 'You’ve collected an assortment of historical data and are considering three
    models:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 您收集了一系列历史数据，正在考虑三种模型：
- en: A decision tree
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树
- en: A neural network
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个神经网络
- en: A high-priced “retention expert”
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个高价“留存专家”
- en: One of your data scientists insists that you should just use whichever model
    performs best.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 您的一个数据科学家坚持认为您应该使用表现最好的模型。
- en: A second insists that you not use the neural network model, as only the other
    two can explain their predictions, and that only explanation of the predictions
    can help HR institute widespread changes (as opposed to one-off interventions).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个坚持您不要使用神经网络模型，因为只有其他两个模型能解释它们的预测，而只有预测的解释才能帮助HR实施广泛的变革（而不是一次性的干预）。
- en: A third says that while the “expert” can offer *an* explanation for her predictions,
    there’s no reason to take her at her word that it describes the *real* reasons
    she predicted the way she did.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 第三个说，虽然这位“专家”可以对她的预测提供*一个*解释，但没有理由相信她的解释描述了她预测的*真正*原因。
- en: As with our other examples, there is no absolute best choice here. In some circumstances
    (possibly for legal reasons or if your predictions are somehow life-changing)
    you might prefer a model that performs worse but whose predictions can be explained.
    In others, you might just want the model that predicts best. In still others,
    perhaps there is no interpretable model that performs well.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 和我们的其他例子一样，在这里没有绝对的最佳选择。在某些情况下（可能是出于法律原因或者如果您的预测对生活有重大影响），您可能更喜欢一个性能较差但可以解释其预测的模型。在其他情况下，您可能只想要预测最好的模型。在另一些情况下，也许没有一个可解释的模型表现良好。
- en: Recommendations
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 推荐
- en: As we discussed in [Chapter 23](ch23.html#recommender_systems), a common data
    science application involves recommending things to people. When someone watches
    a YouTube video, YouTube recommends videos they should watch next.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[第23章](ch23.html#recommender_systems)中讨论的那样，一个常见的数据科学应用涉及向人们推荐事物。当有人观看YouTube视频时，YouTube会推荐他们接下来应该观看的视频。
- en: YouTube makes money through advertising and (presumably) wants to recommend
    videos that you are more likely to watch, so that they can show you more advertisements.
    However, it turns out that people like to watch videos about conspiracy theories,
    which tend to feature in the recommendations.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: YouTube通过广告赚钱，（据推测）希望推荐您更有可能观看的视频，以便它们可以向您展示更多广告。然而，事实证明，人们喜欢观看关于阴谋论的视频，这些视频往往出现在推荐中。
- en: Note
  id: totrans-76
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注
- en: At the time I wrote this chapter, if you searched YouTube for “saturn” the third
    result was “Something Is Happening On Saturn… Are THEY Hiding It?” which maybe
    gives you a sense of the kinds of videos I’m talking about.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在我写这一章时，如果您在YouTube上搜索“saturn”，第三个结果是“Something Is Happening On Saturn… Are
    THEY Hiding It?”这也许可以让您感受到我所说的那些视频的类型。
- en: Does YouTube have an obligation not to recommend conspiracy videos? Even if
    that’s what lots of people seem to want to watch?
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: YouTube是否有义务不推荐阴谋视频？即使这是许多人似乎想观看的内容？
- en: A different example is that if you go to google.com (or bing.com) and start
    typing a search, the search engine will offer suggestions to autocomplete your
    search. These suggestions are based (at least in part) on other people’s searches;
    in particular, if other people are searching for unsavory things this may be reflected
    in your suggestions.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个例子是，如果您转到google.com（或bing.com）并开始输入搜索内容，搜索引擎会提供自动完成您搜索的建议。这些建议基于其他人的搜索（至少部分地）；特别是，如果其他人正在搜索不良内容，这可能会反映在您的建议中。
- en: Should a search engine try to affirmatively filter out suggestions it doesn’t
    like? Google (for whatever reason) seems intent on not suggesting things related
    to people’s religion. For example, if you type “mitt romney m” into Bing, the
    first suggestion is “mitt romney mormon” (which is what I would have expected),
    whereas Google refuses to provide that suggestion.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 搜索引擎是否应该积极过滤掉它不喜欢的建议？谷歌（出于某种原因）似乎坚决不提供与人们宗教有关的建议。例如，如果在Bing中键入“mitt romney m”，第一个建议是“mitt
    romney mormon”（这是我预料中的），而谷歌拒绝提供这样的建议。
- en: Indeed, Google explicitly filters out autosuggestions that it considers [“offensive
    or disparaging”](https://blog.google/products/search/google-search-autocomplete/).
    (How it decides what’s offensive or disparaging is left vague.) And yet sometimes
    the truth is offensive. Is protecting people from those suggestions the ethical
    thing to do? Or is it an unethical thing to do? Or is it not a question of ethics
    at all?
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，谷歌明确过滤掉它认为是[“冒犯性或贬低性”的](https://blog.google/products/search/google-search-autocomplete/)自动建议。（它如何决定什么是冒犯性或贬低性是模糊的。）但有时候真相就是冒犯的。保护人们免受这些建议的影响是道德行为吗？还是不道德的行为？或者根本不是道德问题？
- en: Biased Data
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 偏见数据
- en: In [“Word Vectors”](ch21.html#word_vectors) we used a corpus of documents to
    learn vector embeddings for words. These vectors were designed to exhibit *distributional
    similarity*. That is, words that appear in similar contexts should have similar
    vectors. In particular, any biases that exist in the training data will be reflected
    in the word vectors themselves.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在[“词向量”](ch21.html#word_vectors)中，我们使用了一组文档语料库来学习单词的向量嵌入。这些向量被设计为展示*分布式相似性*。也就是说，出现在相似上下文中的词语应该具有相似的向量。特别是，训练数据中存在的任何偏见都将反映在词向量本身中。
- en: For example, if our documents are all about how R users are moral reprobates
    and how Python users are paragons of virtue, most likely the model will learn
    such associations for “Python” and “R.”
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们的文档都是关于R用户是道德败类，Python用户是美德典范，那么模型很可能会学习到“Python”和“R”的这种关联。
- en: More commonly, word vectors are based on some combination of Google News articles,
    Wikipedia, books, and crawled web pages. This means that they’ll learn whatever
    distributional patterns are present in those sources.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 更常见的情况是，词向量基于一些组合：Google新闻文章、维基百科、书籍和爬取的网页。这意味着它们将学习到这些来源中存在的任何分布模式。
- en: For example, if the majority of news articles about software engineers are about
    *male* software engineers, then the learned vector for “software” might lie closer
    to vectors for other “male” words than to the vectors for “female” words.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果关于软件工程师的大多数新闻文章都是关于*男性*软件工程师，那么“软件”的学习向量可能更接近于其他“男性”词语的向量，而不是“女性”词语的向量。
- en: At that point any downstream applications you build using these vectors might
    also exhibit this closeness. Depending on the application, this may or may not
    be a problem for you. In that case there are various techniques that you can try
    to “remove” specific biases, although you’ll probably never get all of them. But
    it’s something you should be aware of.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在那一点上，您使用这些向量构建的任何下游应用程序可能也会表现出这种紧密性。根据应用程序的不同，这可能是个问题，也可能不是。在这种情况下，您可以尝试各种技术来“消除”特定的偏见，尽管您可能永远无法消除所有偏见。但这是您应该注意的问题。
- en: Similarly, as in the “photos” example in [“Building Bad Data Products”](#bad_data_products),
    if you train a model on nonrepresentative data, there’s a strong possibility it
    will perform poorly in the real world, possibly in ways that are offensive or
    embarrassing.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，就像在[“构建糟糕的数据产品”](#bad_data_products)中的“照片”示例一样，如果您在非代表性数据上训练模型，那么它很可能会在真实世界中表现不佳，可能会以冒犯或令人尴尬的方式表现出来。
- en: Along different lines, it’s also possible that your algorithms might codify
    actual biases that exist out in the world. For example, your parole model may
    do a perfect job of predicting which released criminals get rearrested, but if
    those rearrests are themselves the result of biased real-world processes, then
    your model might be perpetuating that bias.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，您的算法可能也会使实际世界中存在的实际偏见被编码。例如，您的假释模型可能完美地预测哪些释放的罪犯会再次被逮捕，但如果这些再次逮捕本身是有偏见的现实世界过程的结果，那么您的模型可能会延续这种偏见。
- en: Data Protection
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据保护
- en: You know a lot about the DataSciencester users. You know what technologies they
    like, who their data scientist friends are, where they work, how much they earn,
    how much time they spend on the site, which job postings they click on, and so
    forth.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 您了解DataSciencester用户的很多信息。您知道他们喜欢什么技术，他们的数据科学家朋友是谁，他们在哪工作，他们赚多少钱，他们在网站上花费多少时间，他们点击哪些职位发布等等。
- en: The VP of Monetization wants to sell this data to advertisers, who are eager
    to market their various “big data” solutions to your users. The Chief Scientist
    wants to share this data with academic researchers, who are keen to publish papers
    about who becomes a data scientist. The VP of Electioneering has plans to provide
    this data to political campaigns, most of whom are eager to recruit their own
    data science organizations. And the VP of Government Affairs would like to use
    this data to answer questions from law enforcement.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 赚钱副总裁想将这些数据卖给广告商，他们渴望向您的用户营销各种“大数据”解决方案。首席科学家想将这些数据与学术研究人员分享，他们热衷于发表关于如何成为数据科学家的论文。竞选副总裁计划将这些数据提供给政治竞选活动，他们中的大多数人渴望招募自己的数据科学组织。政府事务副总裁希望使用这些数据来回答执法部门的问题。
- en: Thanks to a forward-thinking VP of Contracts, your users agreed to terms of
    service that guarantee you the right to do pretty much whatever you want with
    their data.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 由于一位有远见的合同副总裁，您的用户同意了服务条款，几乎允许您对他们的数据做任何想做的事情。
- en: However (as you have now come to expect), various of the data scientists on
    your team raise various objections to these various uses. One thinks it’s wrong
    to hand the data over to advertisers; another worries that academics can’t be
    trusted to safeguard the data responsibly. A third thinks that the company should
    stay out of politics, while the last insists that police can’t be trusted and
    that collaborating with law enforcement will harm innocent people.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 然而（正如您现在预料到的），您团队中的各个数据科学家对这些各种用途提出了各种异议。有人认为将数据交给广告商是错误的；另一些人担心学术界不能信任地负责保护数据。第三个人认为公司应该远离政治，而最后一个人坚持认为警方不可信任，与执法部门合作将伤害无辜人群。
- en: Do any of these data scientists have a point?
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数据科学家中有人有道理吗？
- en: In Summary
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结一下
- en: These are a lot of things to worry about! And there are countless more we haven’t
    mentioned, and still more that will come up in the future but that would never
    occur to us today.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这些都是很多需要担心的事情！而且我们还没有提到的无数其他问题，还会有更多未来会出现但今天我们无法想象的问题。
- en: For Further Exploration
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步探索
- en: There is no shortage of people professing important thoughts about data ethics.
    Searching on Twitter (or your favorite news site) is probably the best way to
    find out about the most current data ethics controversy.
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在谈论数据伦理的重要思想的人并不少。在 Twitter（或者你最喜欢的新闻网站）上搜索可能是了解当前最新数据伦理争议的最佳方式。
- en: 'If you want something slightly more practical, Mike Loukides, Hilary Mason,
    and DJ Patil have written a short ebook, [*Ethics and Data Science*](https://www.oreilly.com/library/view/ethics-and-data/9781492043898/),
    on putting data ethics into practice, which I am honor-bound to recommend on account
    of Mike being the person who agreed to publish *Data Science from Scratch* way
    back in 2014\. (Exercise: is this ethical of me?)'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你想要更实际一点的东西，Mike Loukides、Hilary Mason 和 DJ Patil 编写了一本短篇电子书，[*数据科学与伦理*](https://www.oreilly.com/library/view/ethics-and-data/9781492043898/)，讲述了如何将数据伦理付诸实践，因为
    Mike 在2014年同意出版《*从零开始的数据科学*》，我觉得有义务推荐这本书。（练习：这样做对吗？）
