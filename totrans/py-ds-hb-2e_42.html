<html><head></head><body>
<div id="sbo-rt-content"><section data-pdf-bookmark="Chapter 37. What Is Machine Learning?" data-type="chapter" epub:type="chapter"><div class="chapter" id="section-0501-what-is-machine-learning">
<h1><span class="label">Chapter 37. </span>What Is Machine Learning?</h1>
<p><a data-primary="machine learning" data-secondary="basics" data-type="indexterm" id="ix_ch37-asciidoc0"/>Before we take a look at the details of several machine learning
methods, <a data-primary="machine learning" data-secondary="defined" data-type="indexterm" id="idm45858744764976"/>let’s start by looking at what machine learning is,
and what it isn’t. Machine learning is often categorized as
a subfield of artificial intelligence, but I find that categorization
can be misleading. The study of machine learning certainly arose from
research in this context, but in the data science application of machine
learning methods, it’s more helpful to think of machine
learning as a means of <em>building models of data</em>.</p>
<p>In this context, “learning” enters the fray when we give these models
<em>tunable parameters</em> that can be adapted to observed data; in this way
the program can be considered to be “learning” from the data. Once
these models have been fit to previously seen data, they can be used to
predict and understand aspects of newly observed data. I’ll
leave to the reader the more philosophical digression regarding the
extent to which this type of mathematical, model-based “learning” is
similar to the “learning” exhibited by the human brain.</p>
<p>Understanding the problem setting in machine learning is essential to
using these tools effectively, and so we will start with some broad
categorizations of the types of approaches we’ll discuss
here.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>All of the figures in this chapter are generated based on
actual machine learning computations; the code behind them can be found
in the
<a href="https://oreil.ly/o1Zya">online appendix</a>.</p>
</div>
<section data-pdf-bookmark="Categories of Machine Learning" data-type="sect1"><div class="sect1" id="ch_0501-what-is-machine-learning_categories-of-machine-learning">
<h1>Categories of Machine Learning</h1>
<p><a data-primary="machine learning" data-secondary="categories of" data-type="indexterm" id="idm45858744758528"/>Machine learning can be categorized into two main types: supervised
learning and unsupervised learning.</p>
<p><a data-primary="machine learning" data-secondary="supervised" data-type="indexterm" id="idm45858744757056"/><a data-primary="supervised learning" data-type="indexterm" id="idm45858744755856"/><em>Supervised learning</em> involves somehow modeling the relationship between
measured features of data and some labels associated with the data; once
this model is determined, it can be used to apply labels to new, unknown
data. <a data-primary="classification task" data-secondary="defined" data-type="indexterm" id="idm45858744754800"/><a data-primary="regression task" data-secondary="defined" data-type="indexterm" id="idm45858744753824"/>This is sometimes further subdivided into classification tasks and
regression tasks: in <em>classification</em>, the labels are discrete
categories, while in <em>regression</em>, the labels are continuous quantities.
You will see examples of both types of supervised learning in the
following section.</p>
<p><a data-primary="machine learning" data-secondary="unsupervised" data-type="indexterm" id="idm45858744751648"/><a data-primary="unsupervised learning" data-secondary="defined" data-type="indexterm" id="idm45858744750640"/><em>Unsupervised learning</em> involves modeling the features of a dataset
without reference to any label. These models include tasks such as
<em>clustering</em> and <em>dimensionality reduction.</em> <a data-primary="clustering" data-type="indexterm" id="idm45858744748320"/>Clustering algorithms
identify distinct groups of data, while dimensionality reduction
algorithms search for more succinct representations of the data. You
will also see examples of both types of unsupervised learning in the
following section.</p>
<p><a data-primary="semi-supervised learning" data-type="indexterm" id="idm45858744747232"/>In addition, there are so-called <em>semi-supervised learning</em> methods,
which fall somewhere between supervised learning and unsupervised
learning. Semi-supervised learning methods are often useful when only
incomplete labels are available.</p>
</div></section>
<section data-pdf-bookmark="Qualitative Examples of Machine Learning Applications" data-type="sect1"><div class="sect1" id="ch_0501-what-is-machine-learning_qualitative-examples-of-machine-learning-applications">
<h1>Qualitative Examples of Machine Learning Applications</h1>
<p><a data-primary="machine learning" data-secondary="qualitative examples" data-type="indexterm" id="ix_ch37-asciidoc1"/>To make these ideas more concrete, let’s take a look at a
few very simple examples of a machine learning task. These examples are
meant to give an intuitive, non-quantitative overview of the types of
machine learning tasks we will be looking at in this part of the book.
In later chapters, we will go into more depth regarding the particular
models and how they are used. For a preview of these more technical
aspects, you can find the Python source that generates the figures in the online
<a href="https://oreil.ly/o1Zya">appendix</a>.</p>
<section data-pdf-bookmark="Classification: Predicting Discrete Labels" data-type="sect2"><div class="sect2" id="ch_0501-what-is-machine-learning_classification-predicting-discrete-labels">
<h2>Classification: Predicting Discrete Labels</h2>
<p><a data-primary="classification task" data-secondary="machine learning" data-type="indexterm" id="ix_ch37-asciidoc2"/><a data-primary="labels/labeling" data-secondary="classification task" data-type="indexterm" id="ix_ch37-asciidoc3"/><a data-primary="machine learning" data-secondary="classification task" data-type="indexterm" id="ix_ch37-asciidoc4"/><a data-primary="supervised learning" data-secondary="classification task" data-type="indexterm" id="ix_ch37-asciidoc5"/>We will first take a look at a simple classification task, in which we
are given a set of labeled points and want to use these to classify some
unlabeled points.</p>
<p>Imagine that we have the data shown in <a data-type="xref" href="#fig_images_in_0501-classification-1">Figure 37-1</a>. <a data-primary="feature, data point" data-type="indexterm" id="idm45858744733536"/>This data is two-dimensional: that is, we have two <em>features</em> for each
point, represented by the (x,y) positions of the points on the plane. <a data-primary="class labels (for data point)" data-type="indexterm" id="idm45858744732256"/>In
addition, we have one of two <em>class labels</em> for each point, here
represented by the colors of the points. From these features and labels,
we would like to create a model that will let us decide whether a new
point should be labeled “blue” or “red.”</p>
<figure><div class="figure" id="fig_images_in_0501-classification-1">
<img alt="05.01 classification 1" height="461" src="assets/05.01-classification-1.png" width="600"/>
<h6><span class="label">Figure 37-1. </span>A simple dataset for classification</h6>
</div></figure>
<p>There are a number of possible models for such a classification task,
but we will start with a very simple one. We will make the assumption
that the two groups can be separated by drawing a straight line through
the plane between them, such that points on each side of the line all
fall in the same group. <a data-primary="model (defined)" data-type="indexterm" id="idm45858744728544"/><a data-primary="model parameters (defined)" data-type="indexterm" id="idm45858744727840"/>Here the <em>model</em> is a quantitative version of
the statement “a straight line separates the classes,” while the
<em>model parameters</em> are the particular numbers describing the location
and orientation of that line for our data. The optimal values for these
model parameters are learned from the data (this is the “learning” in
machine learning), which is often called <em>training the model</em>.</p>
<p><a data-type="xref" href="#fig_images_in_0501-classification-2">Figure 37-2</a> shows a visual representation of what the
trained model looks like for this data.</p>
<figure><div class="figure" id="fig_images_in_0501-classification-2">
<img alt="05.01 classification 2" height="461" src="assets/05.01-classification-2.png" width="600"/>
<h6><span class="label">Figure 37-2. </span>A simple classification model</h6>
</div></figure>
<p>Now that this model has been trained, it can be generalized to new,
unlabeled data. In other words, we can take a new set of data, draw this
line through it, and assign labels to the new points based on this model
(see <a data-type="xref" href="#fig_images_in_0501-classification-3">Figure 37-3</a>). This stage is usually called <em>prediction</em>.</p>
<figure><div class="figure" id="fig_images_in_0501-classification-3">
<img alt="05.01 classification 3" height="207" src="assets/05.01-classification-3.png" width="600"/>
<h6><span class="label">Figure 37-3. </span>Applying a classification model to new data</h6>
</div></figure>
<p class="pagebreak-before less_space">This is the basic idea of a classification task in machine learning,
where “classification” indicates that the data has discrete class
labels. At first glance this may seem trivial: it’s easy to
look at our data and draw such a discriminatory line to accomplish this
classification. A benefit of the machine learning approach, however, is
that it can generalize to much larger datasets in many more dimensions. For example, this is similar to the task of automated spam detection for
email. In this case, we might use the following features and labels:</p>
<ul>
<li>
<p><em>feature 1</em>, <em>feature 2</em>, etc. <math alttext="right-arrow">
<mo>→</mo>
</math> normalized counts of
important words or phrases (“Viagra”, “Extended warranty”, etc.)</p>
</li>
<li>
<p><em>label</em> <math alttext="right-arrow">
<mo>→</mo>
</math> “spam” or “not spam”</p>
</li>
</ul>
<p>For the training set, these labels might be determined by individual
inspection of a small representative sample of emails; for the remaining
emails, the label would be determined using the model. For a suitably
trained classification algorithm with enough well-constructed features
(typically thousands or millions of words or phrases), this type of
approach can be very effective. We will see an example of such
text-based classification in <a data-type="xref" href="ch41.xhtml#section-0505-naive-bayes">Chapter 41</a>.</p>
<p>Some important classification algorithms that we will discuss in more
detail are Gaussian naive Bayes (see <a data-type="xref" href="ch41.xhtml#section-0505-naive-bayes">Chapter 41</a>), support vector machines (see
<a data-type="xref" href="ch43.xhtml#section-0507-support-vector-machines">Chapter 43</a>), and random forest classification (see
<a data-type="xref" href="ch44.xhtml#section-0508-random-forests">Chapter 44</a>).<a data-startref="ix_ch37-asciidoc5" data-type="indexterm" id="idm45858744707056"/><a data-startref="ix_ch37-asciidoc4" data-type="indexterm" id="idm45858744706384"/><a data-startref="ix_ch37-asciidoc3" data-type="indexterm" id="idm45858744705776"/><a data-startref="ix_ch37-asciidoc2" data-type="indexterm" id="idm45858744705104"/></p>
</div></section>
<section data-pdf-bookmark="Regression: Predicting Continuous Labels" data-type="sect2"><div class="sect2" id="ch_0501-what-is-machine-learning_regression-predicting-continuous-labels">
<h2>Regression: Predicting Continuous Labels</h2>
<p><a data-primary="labels/labeling" data-secondary="regression task" data-type="indexterm" id="ix_ch37-asciidoc6"/><a data-primary="machine learning" data-secondary="regression task" data-type="indexterm" id="ix_ch37-asciidoc7"/><a data-primary="regression task" data-secondary="machine learning" data-type="indexterm" id="ix_ch37-asciidoc8"/><a data-primary="supervised learning" data-secondary="regression task" data-type="indexterm" id="ix_ch37-asciidoc9"/>In contrast with the discrete labels of a classification algorithm, we
will next look at a simple regression task in which the labels are
continuous quantities.</p>
<p>Consider the data shown in <a data-type="xref" href="#fig_images_in_0501-regression-1">Figure 37-4</a>, which consists of a set
of points each with a continuous label.</p>
<figure class="width-75"><div class="figure" id="fig_images_in_0501-regression-1">
<img alt="05.01 regression 1" height="422" src="assets/05.01-regression-1.png" width="600"/>
<h6><span class="label">Figure 37-4. </span>A simple dataset for regression</h6>
</div></figure>
<p>As with the classification example, we have two-dimensional data: that
is, there are two features describing each data point. The color of each
point represents the continuous label for that point.</p>
<p>There are a number of possible regression models we might use for this
type of data, but here we will use a simple linear regression model to
predict the points. This simple model assumes that if we treat the label
as a third spatial dimension, we can fit a plane to the data. This is a
higher-level generalization of the well-known problem of fitting a line
to data with two coordinates.</p>
<p>We can visualize this setup as shown in <a data-type="xref" href="#fig_images_in_0501-regression-2">Figure 37-5</a>.</p>
<figure class="width-75"><div class="figure" id="fig_images_in_0501-regression-2">
<img alt="05.01 regression 2" height="512" src="assets/05.01-regression-2.png" width="600"/>
<h6><span class="label">Figure 37-5. </span>A three-dimensional view of the regression data</h6>
</div></figure>
<p>Notice that the <em>feature 1–feature 2</em> plane here is the same as in the
two-dimensional plot in <a data-type="xref" href="#fig_images_in_0501-regression-1">Figure 37-4</a>; in this case, however, we have
represented the labels by both color and three-dimensional axis
position. From this view, it seems reasonable that fitting a plane
through this three-dimensional data would allow us to predict the
expected label for any set of input parameters. Returning to the
two-dimensional projection, when we fit such a plane we get the result
shown in <a data-type="xref" href="#fig_images_in_0501-regression-3">Figure 37-6</a>.</p>
<figure class="width-70"><div class="figure" id="fig_images_in_0501-regression-3">
<img alt="05.01 regression 3" height="423" src="assets/05.01-regression-3.png" width="600"/>
<h6><span class="label">Figure 37-6. </span>A representation of the regression model</h6>
</div></figure>
<p>This plane of fit gives us what we need to predict labels for new
points. Visually, we find the results shown in <a data-type="xref" href="#fig_images_in_0501-regression-4">Figure 37-7</a>.</p>
<figure><div class="figure" id="fig_images_in_0501-regression-4">
<img alt="05.01 regression 4" height="207" src="assets/05.01-regression-4.png" width="600"/>
<h6><span class="label">Figure 37-7. </span>Applying the regression model to new data</h6>
</div></figure>
<p>As with the classification example, this task may seem trivial in a low
number of dimensions. But the power of these methods is that they can be
straightforwardly applied and evaluated in the case of data with many,
many features. For example, this is similar to the task of computing the distance to
galaxies observed through a telescope—in this case, we might use the
following features and labels:</p>
<ul>
<li>
<p><em>feature 1</em>, <em>feature 2</em>, etc. <math alttext="right-arrow">
<mo>→</mo>
</math> brightness of each
galaxy at one of several wavelengths or colors</p>
</li>
<li>
<p><em>label</em> <math alttext="right-arrow">
<mo>→</mo>
</math> distance or redshift of the galaxy</p>
</li>
</ul>
<p>The distances for a small number of these galaxies might be determined
through an independent set of (typically more expensive or complex)
observations. Distances to remaining galaxies could then be estimated
using a suitable regression model, without the need to employ the more
expensive observation across the entire set. In astronomy circles, this
is known as the “photometric redshift” problem.</p>
<p>Some important regression algorithms that we will discuss are linear
regression (see <a data-type="xref" href="ch42.xhtml#section-0506-linear-regression">Chapter 42</a>), support vector machines (see
<a data-type="xref" href="ch43.xhtml#section-0507-support-vector-machines">Chapter 43</a>), and random forest regression (see
<a data-type="xref" href="ch44.xhtml#section-0508-random-forests">Chapter 44</a>).<a data-startref="ix_ch37-asciidoc9" data-type="indexterm" id="idm45858744671232"/><a data-startref="ix_ch37-asciidoc8" data-type="indexterm" id="idm45858744670528"/><a data-startref="ix_ch37-asciidoc7" data-type="indexterm" id="idm45858744669856"/><a data-startref="ix_ch37-asciidoc6" data-type="indexterm" id="idm45858744669184"/></p>
</div></section>
<section data-pdf-bookmark="Clustering: Inferring Labels on Unlabeled Data" data-type="sect2"><div class="sect2" id="ch_0501-what-is-machine-learning_clustering-inferring-labels-on-unlabeled-data">
<h2>Clustering: Inferring Labels on Unlabeled Data</h2>
<p><a data-primary="clustering" data-secondary="basics" data-type="indexterm" id="ix_ch37-asciidoc10"/><a data-primary="labels/labeling" data-secondary="clustering" data-type="indexterm" id="ix_ch37-asciidoc11"/><a data-primary="machine learning" data-secondary="clustering" data-type="indexterm" id="ix_ch37-asciidoc12"/><a data-primary="unsupervised learning" data-secondary="clustering" data-type="indexterm" id="ix_ch37-asciidoc13"/>The classification and regression illustrations we just saw are examples
of supervised learning algorithms, in which we are trying to build a
model that will predict labels for new data. Unsupervised learning
involves models that describe data without reference to any known
labels.</p>
<p>One common case of unsupervised learning is “clustering,” in which
data is automatically assigned to some number of discrete groups. For
example, we might have some two-dimensional data like that shown in <a data-type="xref" href="#fig_images_in_0501-clustering-1">Figure 37-8</a>.</p>
<figure class="width-75"><div class="figure" id="fig_images_in_0501-clustering-1">
<img alt="05.01 clustering 1" height="461" src="assets/05.01-clustering-1.png" width="600"/>
<h6><span class="label">Figure 37-8. </span>Example data for clustering</h6>
</div></figure>
<p>By eye, it is clear that each of these points is part of a distinct
group. Given this input, a clustering model will use the intrinsic
structure of the data to determine which points are related. <a data-primary="clustering" data-secondary="k-means" data-type="indexterm" id="idm45858744658176"/><a data-primary="k-means clustering" data-type="indexterm" id="idm45858744657200"/>Using the
very fast and intuitive <em>k</em>-means algorithm (see
<a data-type="xref" href="ch47.xhtml#section-0511-k-means">Chapter 47</a>), we find the
clusters shown in <a data-type="xref" href="#fig_images_in_0501-clustering-2">Figure 37-9</a>.</p>
<figure class="width-75"><div class="figure" id="fig_images_in_0501-clustering-2">
<img alt="05.01 clustering 2" height="461" src="assets/05.01-clustering-2.png" width="600"/>
<h6><span class="label">Figure 37-9. </span>Data labeled with a k-means clustering model</h6>
</div></figure>
<p><em>k</em>-means fits a model consisting of <em>k</em> cluster centers; the optimal
centers are assumed to be those that minimize the distance of each point
from its assigned center. Again, this might seem like a trivial exercise
in two dimensions, but as our data becomes larger and more complex such
clustering algorithms can continue to be employed to extract useful
information from the dataset.</p>
<p>We will discuss the <em>k</em>-means algorithm in more depth in
<a data-type="xref" href="ch47.xhtml#section-0511-k-means">Chapter 47</a>. Other important
clustering algorithms include Gaussian mixture models (see
<a data-type="xref" href="ch48.xhtml#section-0512-gaussian-mixtures">Chapter 48</a>)
and spectral clustering (see
<a href="https://oreil.ly/9FHKO">Scikit-Learn’s
clustering documentation</a>).<a data-startref="ix_ch37-asciidoc13" data-type="indexterm" id="idm45858744647232"/><a data-startref="ix_ch37-asciidoc12" data-type="indexterm" id="idm45858744646528"/><a data-startref="ix_ch37-asciidoc11" data-type="indexterm" id="idm45858744645856"/><a data-startref="ix_ch37-asciidoc10" data-type="indexterm" id="idm45858744645184"/></p>
</div></section>
<section data-pdf-bookmark="Dimensionality Reduction: Inferring Structure of Unlabeled Data" data-type="sect2"><div class="sect2" id="ch_0501-what-is-machine-learning_dimensionality-reduction-inferring-structure-of-unlabeled-data">
<h2>Dimensionality Reduction: Inferring Structure of Unlabeled Data</h2>
<p><a data-primary="dimensionality reduction" data-secondary="machine learning" data-type="indexterm" id="ix_ch37-asciidoc14"/><a data-primary="labels/labeling" data-secondary="dimensionality reduction and" data-type="indexterm" id="ix_ch37-asciidoc15"/><a data-primary="machine learning" data-secondary="dimensionality reduction" data-type="indexterm" id="ix_ch37-asciidoc16"/><a data-primary="unsupervised learning" data-secondary="dimensionality reduction" data-type="indexterm" id="ix_ch37-asciidoc17"/>Dimensionality reduction is another example of an unsupervised
algorithm, in which labels or other information are inferred from the
structure of the dataset itself. Dimensionality reduction is a bit more
abstract than the examples we looked at before, but generally it seeks
to pull out some low-dimensional representation of data that in some way
preserves relevant qualities of the full dataset. Different
dimensionality reduction routines measure these relevant qualities in
different ways, as we will see in
<a data-type="xref" href="ch46.xhtml#section-0510-manifold-learning">Chapter 46</a>.</p>
<p>As an example of this, consider the data shown in <a data-type="xref" href="#fig_images_in_0501-dimesionality-1">Figure 37-10</a>.</p>
<figure class="width-75"><div class="figure" id="fig_images_in_0501-dimesionality-1">
<img alt="05.01 dimesionality 1" height="422" src="assets/05.01-dimesionality-1.png" width="600"/>
<h6><span class="label">Figure 37-10. </span>Example data for dimensionality reduction</h6>
</div></figure>
<p>Visually, it is clear that there is some structure in this data: it is
drawn from a one-dimensional line that is arranged in a spiral within
this two-dimensional space. In a sense, you could say that this data is
“intrinsically” only one-dimensional, though this one-dimensional data
is embedded in two-dimensional space. A suitable dimensionality
reduction model in this case would be sensitive to this nonlinear
embedded structure and be able to detect this lower-dimensionality
representation.</p>
<p><a data-primary="Isomap" data-secondary="dimensionality reduction" data-type="indexterm" id="idm45858744632368"/><a data-type="xref" href="#fig_images_in_0501-dimesionality-2">Figure 37-11</a> shows a visualization of the results of the Isomap
algorithm, a manifold learning algorithm that does exactly this.</p>
<p>Notice that the colors (which represent the extracted one-dimensional
latent variable) change uniformly along the spiral, which indicates that
the algorithm did in fact detect the structure we saw by eye. As with
the previous examples, the power of dimensionality reduction algorithms
becomes clearer in higher-dimensional cases. For example, we might wish
to visualize important relationships within a dataset that has 100 or
1,000 features. Visualizing 1,000-dimensional data is a challenge, and
one way we can make this more manageable is to use a dimensionality
reduction technique to reduce the data to 2 or 3 dimensions.</p>
<p>Some important dimensionality reduction algorithms that we will discuss
are principal component analysis (see
<a data-type="xref" href="ch45.xhtml#section-0509-principal-component-analysis">Chapter 45</a>) and various manifold learning algorithms, including
Isomap and locally linear embedding (see
<a data-type="xref" href="ch46.xhtml#section-0510-manifold-learning">Chapter 46</a>)<a data-startref="ix_ch37-asciidoc17" data-type="indexterm" id="idm45858744627472"/><a data-startref="ix_ch37-asciidoc16" data-type="indexterm" id="idm45858744626768"/><a data-startref="ix_ch37-asciidoc15" data-type="indexterm" id="idm45858744626096"/><a data-startref="ix_ch37-asciidoc14" data-type="indexterm" id="idm45858744625424"/>.<a data-startref="ix_ch37-asciidoc1" data-type="indexterm" id="idm45858744624624"/></p>
<figure class="width-75"><div class="figure" id="fig_images_in_0501-dimesionality-2">
<img alt="05.01 dimesionality 2" height="459" src="assets/05.01-dimesionality-2.png" width="600"/>
<h6><span class="label">Figure 37-11. </span>Data with labels learned via dimensionality reduction</h6>
</div></figure>
</div></section>
</div></section>
<section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="ch_0501-what-is-machine-learning_summary">
<h1>Summary</h1>
<p>Here we have seen a few simple examples of some of the basic types of
machine learning approaches. Needless to say, there are a number of
important practical details that we have glossed over, but this chapter
was designed to give you a basic idea of what types of problems machine
learning approaches can solve.</p>
<p>In short, we saw the following:</p>
<ul>
<li>
<p><em>Supervised learning</em>: Models that can predict labels based on labeled
training data</p>
<ul>
<li>
<p><em>Classification</em>: Models that predict labels as two or more discrete
categories</p>
</li>
<li>
<p><em>Regression</em>: Models that predict continuous labels</p>
</li>
</ul>
</li>
<li>
<p><em>Unsupervised learning</em>: Models that identify structure in unlabeled
data</p>
<ul>
<li>
<p><em>Clustering</em>: Models that detect and identify distinct groups in the
data</p>
</li>
<li>
<p><em>Dimensionality reduction</em>: Models that detect and identify
lower-dimensional structure in higher-dimensional data</p>
</li>
</ul>
</li>
</ul>
<p>In the following chapters, we will go into much greater depth within
these categories, and see some more interesting examples of where these
concepts can be useful.<a data-startref="ix_ch37-asciidoc0" data-type="indexterm" id="idm45858744610208"/></p>
</div></section>
</div></section></div></body></html>