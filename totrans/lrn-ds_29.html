<html><head></head><body><section data-pdf-bookmark="Data Sources" data-type="afterword" epub:type="afterword"><div class="appendix" id="ax-data-source">&#13;
<h1>Data Sources</h1>&#13;
<p>All of the data analyzed in this book are available on the <a href="https://learningds.org">book’s website</a> and <a href="https://github.com/DS-100/textbook">GitHub repository</a>. These datasets are from open repositories and from individuals. We acknowledge them all here, and include, as appropriate, the filename for the data stored in our repository, a description of the resource, a link to the original source, a related publication, and the author(s)/owner(s).</p>&#13;
<p>To begin, we provide the sources for the four case studies in the book. Our analysis of the data in these case studies is based on research articles or, in one case, a blog post. We generally follow the line of inquiry in these sources, simplifying the analyses to match the level of the book.</p>&#13;
<p>Here are the four case studies:</p>&#13;
<dl>&#13;
    <dt><code>seattle_bus_times.csv</code></dt> &#13;
        <dd>Mark Hallenbeck of the <a href="https://oreil.ly/3hZ_A">Washington State Transportation Center</a> provides the Seattle Transit data. Our analysis is based on <a href="https://oreil.ly/kaQv-">“The Waiting Time Paradox, or, Why Is My Bus Always Late?”</a> by Jake VanderPlas.</dd>&#13;
    <dt><code>aqs_06-067-0010.csv</code>, <code>list_of_aqs_sites.csv</code>, <code>matched_pa_aqs.csv</code>, <code>list_of_purpleair_sensors.json</code>, and <code>purpleair_AMTS</code></dt> &#13;
        <dd>The datasets used in the study of air quality monitors are available from Karoline Barkjohn of the Environmental Protection Agency. These were originally acquired by Barkjohn and collaborators from the <a href="https://oreil.ly/Sjku6">US Air Quality System</a> and <a href="https://www2.purpleair.com">PurpleAir</a>. Our analysis is based on <a href="https://oreil.ly/jWuNx">“Development and Application of a United States-Wide Correction for PM 2.5 Data Collected with the PurpleAir Sensor”</a> by Barkjohn, Brett Gantt, and Andrea Clements.</dd>&#13;
    <dt><code>donkeys.csv</code></dt> &#13;
        <dd>Kate Milner collected the data for the Kenyan donkey study on behalf of the UK Donkey Sanctuary. Jonathan Rougier makes the data available in the <a href="https://oreil.ly/oiMNE">paranomo package</a> (follow link to download). Our analysis is based on <a href="https://doi.org/10.1111/j.1740-9713.2014.00768.x">“How to Weigh a Donkey in the Kenyan Countryside”</a> by Milner and Rougier.</dd>&#13;
    <dt><code>fake_news.csv</code></dt> &#13;
        <dd>The hand-classified fake news data are from <a href="https://arxiv.org/abs/1809.01286">“FakeNewsNet: A Data Repository with News Content, Social Context, and Spatiotemporal Information for Studying Fake News on Social Media”</a> by Kai Shu et al.</dd>&#13;
</dl>&#13;
<p>In addition to these case studies, we use another 20-plus datasets as examples throughout the book. We acknowledge the people and organizations that make these datasets available in the order in which they appear in the book:</p>&#13;
<dl>&#13;
    <dt><code>gft.csv</code></dt>&#13;
        <dd>The data on the Google Flu Trends is available from <a href="https://doi.org/10.7910/DVN/24823">Gary King Dataverse</a> and the plot made from these data is based on <a href="https://doi.org/10.1126/science.1248506">“The Parable of Google Flu: Traps in Big Data Analysis”</a> by David Lazer et al.</dd>&#13;
    <dt><code>WikipediaExp.csv</code></dt> &#13;
        <dd>Arnout van de Rijt provides the data for the Wikipedia experiment. These data are analyzed in <a href="https://oreil.ly/BDDSV">“Experimental Study of Informal Rewards in Peer Production”</a> by Michael Restivo and van de Rijt.</dd>&#13;
    <dt><code>co2_mm_mlo.txt</code></dt> &#13;
        <dd>The CO<sub>2</sub> concentrations measured at Mauna Loa by the <a href="https://noaa.gov">National Oceanic and Atmospheric Administration (NOAA)</a> are available from the <a href="https://gml.noaa.gov/obop/mlo">Global Monitoring Laboratory</a>.</dd>&#13;
    <dt><code>pm30.csv</code></dt> &#13;
        <dd>We downloaded these air quality measurements for one day and one sensor from the <a href="https://www2.purpleair.com">PurpleAir map</a>.</dd>&#13;
    <dt><code>babynames.csv</code></dt> &#13;
        <dd>The <a href="https://oreil.ly/DBiky">US Social Security Department</a> provides the names from all Social Security card applications.</dd>&#13;
    <dt><code>DAWN-Data.txt</code></dt> &#13;
        <dd>The <a href="https://oreil.ly/n8NOQ">2011 DAWN survey</a> of drug-related emergency room visits is administered by the <a href="https://samhsa.gov">US Substance Abuse and Mental Health Services Administration</a>.</dd>&#13;
    <dt><code>businesses.csv</code>, <code>inspections.csv</code>, and <code>violations.csv</code></dt> &#13;
        <dd>The data on restaurant inspection scores in San Francisco is from <a href="https://datasf.org">DataSF</a>.</dd>&#13;
    <dt><code>akc.csv</code></dt> &#13;
        <dd>The data on dog breeds come from Information Is Beautiful’s <a href="https://oreil.ly/KjIyv">“Best in Show: The Ultimate Data Dog”</a> visualization and was originally acquired from the <a href="https://akc.org">American Kennel Club</a>.</dd>&#13;
    <dt><code>sfhousing.csv</code></dt> &#13;
        <dd>The housing sale prices for the San Francisco Bay Area were scraped from the <a href="https://oreil.ly/kaziA"><em>San Francisco Chronicle</em></a> real estate pages.</dd>&#13;
    <dt><code>cherryBlossomMen.csv</code></dt> &#13;
        <dd>The run times in the annual <a href="https://cherryblossom.org">Cherry Blossom 10-mile run</a> were scraped from the race results pages.</dd>&#13;
    <dt><code>earnings2020.csv</code></dt> &#13;
        <dd>The weekly earnings data are made available by the <a href="https://oreil.ly/cZG_w">US Bureau of Labor Statistics</a>.</dd>&#13;
    <dt><code>co2_by_country.csv</code></dt> &#13;
        <dd>The annual country CO<sub>2</sub> emissions is available from <a href="https://ourworldindata.org">Our World in Data</a>.</dd>&#13;
    <dt><code>100m_sprint.csv</code></dt> &#13;
        <dd>The times for the 100-meter sprint are from <a href="https://fivethirtyeight.com/">FiveThirtyEight</a> and the figure is based on <a href="https://oreil.ly/ewY7w">“The Fastest Men in the World Are Still Chasing Usain Bolt”</a> by Josh Planos.</dd>&#13;
    <dt><code>stateoftheunion1790-2022.txt</code></dt> &#13;
        <dd>The State of the Union addresses are compiled from the <a href="https://oreil.ly/AnkW8">American Presidency Project</a>.</dd>&#13;
    <dt><code>CDS_ERA5_22-12.nc</code></dt> &#13;
        <dd>We collected these data from the <a href="https://cds.climate.copernicus.eu">Climate Data Store</a>, which is supported by the <a href="https://ecmwf.int">European Centre for Medium-Range Weather Forecasts</a>.</dd>&#13;
    <dt><code>world_record_1500m.csv</code></dt>&#13;
        <dd>The 1,5000-meter world records come from the Wikipedia page <a href="https://oreil.ly/2_P4H">“1,500 Metres World Record Progression”</a>.</dd>&#13;
    <dt><code>the_clash.csv</code></dt> &#13;
        <dd>The Clash songs are available at the <a href="https://oreil.ly/FYP8B">Spotify Web API</a>. The retrieval of the data follows <a href="https://oreil.ly/mWgYl">“Exploring the Spotify API in Python”</a> by Steven Morse.</dd>&#13;
    <dt><code>catalog.xml</code></dt> &#13;
        <dd>The XML plant catalog document is from the <a href="https://oreil.ly/MNw-G">W3Schools plant catalog</a>.</dd>&#13;
    <dt><code>ECB_EU_exchange.csv</code></dt> &#13;
        <dd>The exchange rates are available from the <a href="https://oreil.ly/Wc61c">European Central Bank</a>.</dd>&#13;
    <dt><code>mobility.csv</code></dt> &#13;
        <dd>These data are available at <a href="https://oreil.ly/W_5KH">Opportunity Insights</a>, and our example follows <a href="https://doi.org/10.1093/qje/qju022">“Where Is the Land of Opportunity? The Geography of Intergenerational Mobility in the United States”</a> by Raj Chetty et al.</dd>&#13;
    <dt><code>utilities.csv</code></dt> &#13;
        <dd>Daniel Kaplan’s home energy consumption data is available to <a href="https://oreil.ly/YTAsK">download</a> and appear in his first edition of <a class="orm:hideurl" href="https://dtkaplan.github.io/SM2-bookdown/preface-to-this-electronic-version.html"><em>Statistical Modeling: A Fresh Approach</em></a> (self-pub, CreateSpace).</dd>&#13;
    <dt><code>market-analysis.csv</code></dt> &#13;
        <dd>Stan Lipovetsky provides these data, and they correspond to the data in his paper <a href="https://oreil.ly/UZUJq">“Regressions Regularized by Correlations”</a>.</dd>&#13;
    <dt><code>crabs.data</code></dt> &#13;
        <dd>The crab measurements are from the <a href="https://wildlife.ca.gov">California Department of Fish and Wildlife</a>, available to download from the <a href="https://oreil.ly/mZsQ8">Stat Labs Data repository</a>.</dd>&#13;
    <dt><code>black_spruce.csv</code></dt>&#13;
        <dd>Roy Lawrence Rich collected the wind-damaged tree data for his thesis <a href="https://oreil.ly/Pkw8N">“Large Wind Disturbance in the Boundary Waters Canoe Area Wilderness. Forest Dynamics and Development Changes Associated with the July 4th 1999 Blowdown”</a>. The data are available online in the <a href="https://oreil.ly/6rPOB"><code>alr4</code> package</a>. Our analysis is based on “Logistic Regression” in Weisberg’s <a class="orm:hideurl" href="https://doi.org/10.1002/0471704091"><em>Applied Linear Regression</em></a>.</dd>&#13;
</dl>&#13;
</div></section></body></html>