- en: Chapter 9\. Graph Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Graphs, diagrams, and networks are all around us: Cities and roadmaps, airports
    and connecting flights, electrical networks, the power grid, the world wide web,
    molecular networks, biological networks such as our nervous system, social networks,
    terrorist organization networks, schematic represenations of mathematical models,
    artificial neural networks, and many, many others. They are easily recognizable,
    with distinct nodes representing some entities that we care for, which are then
    connected by directed or undirected edges, idicating the presence of some relationship
    between the connected nodes.'
  prefs: []
  type: TYPE_NORMAL
- en: Data that has a natural graph structure is better understood by a mechanism
    that exploits and preserves that structure, building functions that operate directly
    on graphs (however they are mathematically represented), as opposed to feeding
    graph data into machine learning models that artificially reshape it before analyzing
    it. This inevitably leads to loss of valuable information. This is the same reason
    convolutional neural networks are successful with image data, recurrent neural
    networks are successful with sequential data, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'Graph based models are very attractive for data scientists and engineers: Graph
    structures offer a flexibility that is not afforded in spaces with a fixed underlying
    coordinate system such as in Euclidean spaces, or in relational databases, where
    the data along with its features is forced to adhere to a rigid and predetermined
    form. Moreover, graphs are the natural setting that allows us to investigate the
    relationships between the points in a data set. So far, our machine learning models
    consumed data represented as isolated data points. Graph models, on the other
    hand, consume isolated data points, *along with the connections between them*,
    allowing for deeper understanding and more expressive models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The human brain naturally internalizes graphical structures: It is able to
    model entities and their connections. It is also flexible enough to generate new
    networks, or expand and enhance existing ones, for example, when city planning,
    project planning, or when continuously updating transit networks. Moreover, humans
    can transition from natural language text to graph models and vice versa seamlessly:
    When we read something new, we find it natural to formulate a graphical represenation
    in order to better comprehend it or to illustrate it to other people. Conversely,
    when we see graph schematics, we are able to describe it via natural language.
    There are currently models that generate natural language text based on knowledge
    graphs and vice versa. This is called reasoning over knowledge graphs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point we are pretty comfortable with the building blocks of neural
    networks, along with the types of data and tasks they are usually suited for:'
  prefs: []
  type: TYPE_NORMAL
- en: Multilayer perceptron or fully connected neural network ([Chapter 4](ch04.xhtml#ch04))
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Convolutional layers ([Chapter 5](ch05.xhtml#ch05))
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Recurrent layers ([Chapter 7](ch07.xhtml#ch07))
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Encoder-decoder components ([Chapter 7](ch07.xhtml#ch07))
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Adversarial components and two player zero sum games ([Chapter 8](ch08.xhtml#ch08))
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Variational components ([Chapter 8](ch08.xhtml#ch08))
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The main tasks are mostly: Classification, regression, clustering, coding and
    decoding, or new data generation, where the model learns the joint probability
    distribution of the features of the data.'
  prefs: []
  type: TYPE_NORMAL
- en: We are also familiar with the fact that we can mix and match some of the components
    of neural networks in order to construct new models that are geared towards specific
    tasks. The good news is that graph neural networks use the same exact ingredients,
    so we do not need to go over any new machine learning concepts in this chapter.
    Once we undertsand how to mathematically represent graph data along with its features,
    in a way that can be fed into a neural network, either for analysis or for new
    network (graph) data generation, we are good to go. We will therefore avoid going
    down a maze of surveys for all the graph neural networks out there. Instead, we
    focus on the simple mathematical formulation, popular applications, common tasks
    for graph models, available data sets, and model evaluation methods. Our goal
    is to develop a strong intuition for the workings of the subject. The main challenge
    is, yet again, lowering the dimensionality of the problem in a way that makes
    it amenable to computation and analysis, while preserving the most amount of information.
    In other terms, for a network with millions of users, we cannot expect our models
    to take as input vectors or matrices with millions of dimensions. We need efficient
    representation methods for graph data.
  prefs: []
  type: TYPE_NORMAL
- en: 'For readers aiming to dive deeper and fast track into graph neural networks,
    the 2019 survey paper: [*A Comprehensive Survey On Graph Neural Networks*](https://arxiv.org/pdf/1901.00596.pdf?ref=https://githubhelp.com)
    is an excellent place to start (of course, only after carefully reading this chapter).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Graphs: Nodes, Edges, And Features For Each'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Graphs are naturally well suited to model any problem where the goal is to understand
    a discrete collection of objects (with emphasis on discrete and not continuous)
    through the relationships between them. Graph theory is a relatively young discipline
    in discrete mathematics and computer science with virtually unlimited applications.
    This field is in need of more brains to tackle its many unsolved problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'A graph (see [Figure 9-1](#Fig_nodes_edges)) is made up of:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Nodes or Vertices: Bundled together in a set <math alttext="upper N o d e s
    equals StartSet n o d e 1 comma n o d e 2 comma ellipsis comma n o d e Subscript
    n Baseline EndSet"><mrow><mi>N</mi> <mi>o</mi> <mi>d</mi> <mi>e</mi> <mi>s</mi>
    <mo>=</mo> <mo>{</mo> <mi>n</mi> <mi>o</mi> <mi>d</mi> <msub><mi>e</mi> <mn>1</mn></msub>
    <mo>,</mo> <mi>n</mi> <mi>o</mi> <mi>d</mi> <msub><mi>e</mi> <mn>2</mn></msub>
    <mo>,</mo> <mo>⋯</mo> <mo>,</mo> <mi>n</mi> <mi>o</mi> <mi>d</mi> <msub><mi>e</mi>
    <mi>n</mi></msub> <mo>}</mo></mrow></math> . This can be as little as a handful
    of nodes (or even one node), or as massive as billions of nodes.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Edges: Connecting any two nodes (this can include an edge from a node to itself,
    or multiple edges connecting the same two nodes) in a directed (pointing from
    one node to the other) or undirected way (the edge has no direction from either
    node to the other). The set of edges is <math alttext="upper E d g e s equals
    StartSet e d g e Subscript i j Baseline equals left-parenthesis n o d e Subscript
    i Baseline comma n o d e Subscript j Baseline right-parenthesis such that there
    is an edge pointing from n o d e Subscript i Baseline to n o d e Subscript j Baseline
    EndSet"><mrow><mi>E</mi> <mi>d</mi> <mi>g</mi> <mi>e</mi> <mi>s</mi> <mo>=</mo>
    <mo>{</mo> <mi>e</mi> <mi>d</mi> <mi>g</mi> <msub><mi>e</mi> <mrow><mi>i</mi><mi>j</mi></mrow></msub>
    <mo>=</mo> <mrow><mo>(</mo> <mi>n</mi> <mi>o</mi> <mi>d</mi> <msub><mi>e</mi>
    <mi>i</mi></msub> <mo>,</mo> <mi>n</mi> <mi>o</mi> <mi>d</mi> <msub><mi>e</mi>
    <mi>j</mi></msub> <mo>)</mo></mrow> <mtext>such</mtext> <mtext>that</mtext> <mtext>there</mtext>
    <mtext>is</mtext> <mtext>an</mtext> <mtext>edge</mtext> <mtext>pointing</mtext>
    <mtext>from</mtext> <mi>n</mi> <mi>o</mi> <mi>d</mi> <msub><mi>e</mi> <mi>i</mi></msub>
    <mtext>to</mtext> <mi>n</mi> <mi>o</mi> <mi>d</mi> <msub><mi>e</mi> <mi>j</mi></msub>
    <mo>}</mo></mrow></math> .'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Node features: We can assign to each <math alttext="n o d e Subscript i"><mrow><mi>n</mi>
    <mi>o</mi> <mi>d</mi> <msub><mi>e</mi> <mi>i</mi></msub></mrow></math> a list
    of say *d* features (such as the age, gender and income level of a social media
    user) bundled together in a vector <math alttext="ModifyingAbove f e a t u r e
    s Subscript n o d e Sub Subscript i With right-arrow"><mover accent="true"><mrow><mi>f</mi><mi>e</mi><mi>a</mi><mi>t</mi><mi>u</mi><mi>r</mi><mi>e</mi><msub><mi>s</mi>
    <mrow><mi>n</mi><mi>o</mi><mi>d</mi><msub><mi>e</mi> <mi>i</mi></msub></mrow></msub></mrow>
    <mo>→</mo></mover></math> . We can then bundle all the feature vectors of all
    the *n* nodes of the graph in a matrix <math alttext="upper F e a t u r e s Subscript
    upper N o d e s"><mrow><mi>F</mi> <mi>e</mi> <mi>a</mi> <mi>t</mi> <mi>u</mi>
    <mi>r</mi> <mi>e</mi> <msub><mi>s</mi> <mrow><mi>N</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>s</mi></mrow></msub></mrow></math>
    of size <math alttext="d times n"><mrow><mi>d</mi> <mo>×</mo> <mi>n</mi></mrow></math>
    .'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Edge features: Similarly, we can assign to each <math alttext="e d g e Subscript
    i j"><mrow><mi>e</mi> <mi>d</mi> <mi>g</mi> <msub><mi>e</mi> <mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow></math>
    a list of say *c* features (such as the length of a road, its speed limit, and
    whether it is a toll road or not) bundled together in a vector <math alttext="ModifyingAbove
    f e a t u r e s Subscript e d g e Sub Subscript i j With right-arrow"><mover accent="true"><mrow><mi>f</mi><mi>e</mi><mi>a</mi><mi>t</mi><mi>u</mi><mi>r</mi><mi>e</mi><msub><mi>s</mi>
    <mrow><mi>e</mi><mi>d</mi><mi>g</mi><msub><mi>e</mi> <mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow></msub></mrow>
    <mo>→</mo></mover></math> . We can then bundle all the feature vectors of all
    the *m* edges of the graph in a matrix <math alttext="upper F e a t u r e s Subscript
    upper E d g e s"><mrow><mi>F</mi> <mi>e</mi> <mi>a</mi> <mi>t</mi> <mi>u</mi>
    <mi>r</mi> <mi>e</mi> <msub><mi>s</mi> <mrow><mi>E</mi><mi>d</mi><mi>g</mi><mi>e</mi><mi>s</mi></mrow></msub></mrow></math>
    of size <math alttext="c times m"><mrow><mi>c</mi> <mo>×</mo> <mi>m</mi></mrow></math>
    .'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![300](assets/emai_0901.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-1\. A graph is made up of nodes and directed or undirected edges connecting
    the nodes.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Graph models are powerful because they are flexible and are not necessarily
    forced to adhere to a rigid grid-like structure. We can think of their nodes as
    *floating through space* with no coordinates whatsoever. They are only held together
    by the edges that connect them. However, we need a way to represent their intrinsic
    structure. There are software packages that visualize graphs given their sets
    of nodes and edges, but we cannot do analysis and computations on these pretty
    (and informative) pictures. There are two popular graph represenations that we
    can use as inputs to machine learning models: A graph’s *adjacency matrix* and
    its *incidence matrix*. There are other representations that are useful for graph
    theoretic algorithms, such as *edge listing*, *two linear arrays*, and *successor
    listing*. All of these represenations convey the same information but differ in
    their storage requirements and the efficiency of graph retrieval, search, and
    manipulation. Most graph neural networks take as input the adjacency matrix along
    with the feature matrices for the nodes and the edges. Many times, they must do
    a dimension reduction (called graph represenation or graph embedding) before feeding
    the graph data into a model. Other times, the dimension reduction step is part
    of the model itself.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Adjacency matrix*: One algebraic way to store the structure of a graph on
    a machine and study its properties is through an adjacency matrix, which is an
    <math alttext="n times n"><mrow><mi>n</mi> <mo>×</mo> <mi>n</mi></mrow></math>
    whose entries <math alttext="a d j a c e n c y Subscript i j Baseline equals 1"><mrow><mi>a</mi>
    <mi>d</mi> <mi>j</mi> <mi>a</mi> <mi>c</mi> <mi>e</mi> <mi>n</mi> <mi>c</mi> <msub><mi>y</mi>
    <mrow><mi>i</mi><mi>j</mi></mrow></msub> <mo>=</mo> <mn>1</mn></mrow></math> if
    there is an edge from <math alttext="n o d e Subscript i"><mrow><mi>n</mi> <mi>o</mi>
    <mi>d</mi> <msub><mi>e</mi> <mi>i</mi></msub></mrow></math> <math alttext="n o
    d e Subscript j"><mrow><mi>n</mi> <mi>o</mi> <mi>d</mi> <msub><mi>e</mi> <mi>j</mi></msub></mrow></math>
    and <math alttext="a d j a c e n c y Subscript i j Baseline equals 0"><mrow><mi>a</mi>
    <mi>d</mi> <mi>j</mi> <mi>a</mi> <mi>c</mi> <mi>e</mi> <mi>n</mi> <mi>c</mi> <msub><mi>y</mi>
    <mrow><mi>i</mi><mi>j</mi></mrow></msub> <mo>=</mo> <mn>0</mn></mrow></math> if
    there is no edge from <math alttext="n o d e Subscript i"><mrow><mi>n</mi> <mi>o</mi>
    <mi>d</mi> <msub><mi>e</mi> <mi>i</mi></msub></mrow></math> <math alttext="n o
    d e Subscript j"><mrow><mi>n</mi> <mi>o</mi> <mi>d</mi> <msub><mi>e</mi> <mi>j</mi></msub></mrow></math>
    . Note that this definition is able to accomodate a self edge which is an edge
    from a vertex to itself, but not multiple edges between two distinct nodes, unless
    we decide to include the numbers 2, 3, *etc.* as entries in the adjacency matrix.
    This however can mess up some results that graph theorists have established using
    the adjacency matrix.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Incidence matrix*: This is another algebraic way to store the structure of
    the graph and retain its full information. Here, we list both the nodes and the
    edges, then formulate a matrix whose rows correspond to the vertices and whose
    columns correspond to the edges. An entry <math alttext="i n c i d e n c e Subscript
    i j"><mrow><mi>i</mi> <mi>n</mi> <mi>c</mi> <mi>i</mi> <mi>d</mi> <mi>e</mi> <mi>n</mi>
    <mi>c</mi> <msub><mi>e</mi> <mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow></math>
    of the matrix is 1 if <math alttext="e d g e Subscript j"><mrow><mi>e</mi> <mi>d</mi>
    <mi>g</mi> <msub><mi>e</mi> <mi>j</mi></msub></mrow></math> connects <math alttext="n
    o d e Subscript i"><mrow><mi>n</mi> <mi>o</mi> <mi>d</mi> <msub><mi>e</mi> <mi>i</mi></msub></mrow></math>
    to some other node, and zero otherwise. Note that this definition is able to accomodate
    multiple edges between two distinct nodes, but not a self edge from a node to
    itself. Since many graphs have much more edges than vertices, this matrix tends
    to be very wide and larger in size than the adjacency matrix.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The *Laplacian matrix* is another matrix that is associated with an undirected
    graph. It is an <math alttext="n times n"><mrow><mi>n</mi> <mo>×</mo> <mi>n</mi></mrow></math>
    symmetric matrix where each node has a corresponding row and column. The diagonal
    entries of the Laplacian matrix equal to the degree of each node, and the off
    diagonal entries are zero if there is no edge between nodes corrsponding to that
    entry and -1 if there is an edge between them. This is the discrete analogue of
    the continuous Laplace operator from calculus and partial differential equations
    where the discretization happens at the nodes of the graph. The Laplacian takes
    into account the second derivatives of a continuous (and twice differentiable)
    function, which measure the concavity of a function, or how much its value at
    a point differ from its value at the surrounding points. Similar to the continuous
    Laplacian operator, the Laplacian matrix provides a measure of the extent a graph
    differs at one node from its values at nearby nodes. The Laplacian matrix of a
    graph appears when we investigate random walks on graphs and when we study electrical
    networks and resistances. We will see these later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: We can easily infer simple node and edge statistics from the adjacency and incidence
    matrices, such as the degrees of nodes (the degree of a node is the number of
    edges connected to this node). The degree distribution P(k) reflects the variability
    in the degrees of all the nodes. P(k) is the emperical probability that a node
    has exactly k edges. This is of interest for many networks, such as web connectivity
    and biological networks. For example, if the distribution of nodes of degree *k*
    in a graph follows a power law of the form <math alttext="upper P left-parenthesis
    k right-parenthesis equals k Superscript negative alpha"><mrow><mi>P</mi> <mrow><mo>(</mo>
    <mi>k</mi> <mo>)</mo></mrow> <mo>=</mo> <msup><mi>k</mi> <mrow><mo>-</mo><mi>α</mi></mrow></msup></mrow></math>
    then such graphs have few nodes of high connectivity, or hubs, which are central
    to the network topology, holding it together, along with many nodes with low connectivity,
    which connect to the hubs.
  prefs: []
  type: TYPE_NORMAL
- en: We can also add time dependency, and think of dynamic graphs whose properties
    change as time evolves. Currently, there are models that add time dependency to
    the node and/or edge feature vectors (so each entry of these vectors becomes time
    dependent). For example, for a GPS system that predicts travel routes, the edge
    features connecting one point on the map to the other change with time depending
    on the traffic situation.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a mathematical framework for graph objects along with their
    node and edge features, we can feed these represenative vectors, matrices (and
    labels for supervised models) into machine learning models and do business as
    usual. Most of the time half of the story is having a good representation for
    the objects at hand. The other half of the story is the expressive power of machine
    learning models in general, where we can get good results without encoding (or
    even having to learn) the rules that lead to these results. For the purposes of
    this chapter, this means that we can jump straight into graph neural networks
    *before* learning proper graph theory.
  prefs: []
  type: TYPE_NORMAL
- en: Directed Graphs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For directed graphs, on one hand, we are interested in the same properties as
    undirected graphs, such as their spanning trees, fundamental circuits, cut sets,
    planarity, thickness, and others. On the other hand, directed graphs have their
    own unique properties which are different than undirected graphs, such as strong
    connectedness, arborescence (a directed form of rooted tree), decyclization, and
    others.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: PageRank Algorithm'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[PageRank](https://en.wikipedia.org/wiki/PageRank) is a currently retired algorithm
    (expired in 2019) that Google used to rank web pages in their search engine results.
    It provides a measure for the importance of a webpage based on how many other
    pages link to it. In graph language, the nodes are the webpages and the directed
    edges are the links pointing from one page to another. According to PageRank node
    is important when it has many other webpages pointing to it, that is, when its
    incoming degree is large (see [Figure 9-2](#Fig_PageRank)).'
  prefs: []
  type: TYPE_NORMAL
- en: '![250](assets/emai_0902.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-2\. PageRank gives higher score for pages with more pages pointing
    (or linking) to them ([image source](https://en.wikipedia.org/wiki/PageRank#/media/File:PageRanks-Example.svg)).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As a concrete example involving graphs, adjacency matrix, linear algebra, and
    the web, let’s walk through PageRank algorithm for an absurdly simplified world
    wide web consisting of only four indexed webpages, such as in [Figure 9-3](#Fig_graph_page_rank),
    as opposed to billions.
  prefs: []
  type: TYPE_NORMAL
- en: '![250](assets/emai_0903.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-3\. A fictitious world wide web consisting of only four indexed webpages.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In the graph of [Figure 9-3](#Fig_graph_page_rank), only B links to A;A and
    D link to B; A and D link to C; A, B, and C link to D; A links to B, C, and D;
    B links to A and D; C links to D; and D links to B and C.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s think of a web surfer who starts at some page then randomly clicks on
    a link from that page, then a link from this new page, and so on. This surfer
    simulates a *random walk on the graph of the web*.
  prefs: []
  type: TYPE_NORMAL
- en: In general, on the graph representing the world wide web, such a random surfer
    traverses the graph from a certain node to one of its neighbors (or back to itself
    if there are links pointing back to the page itself). We will encounter the world
    wide web one more time in this chapter and explore the kind questions that we
    like to understand about the nature its graph. We need a matrix for the random
    walk, which for this application we call the *linking matrix*, but in reality
    it is the adjacency matrix weighted by the degree of each vertex. We use this
    random walk matrix, or linking matrix, to understand the long term behavior of
    the random walk on the graph. Random walks on graphs will appear throughout this
    chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Back to the four-page world wide web of [Figure 9-3](#Fig_graph_page_rank).
    If the web surfer is at page A, there is a one third chance the surfer will move
    to page B, one third to chance to move to C, and one third to move to D. Thus,
    the *outward linking* vector of page A is
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign ModifyingAbove l i n k i n g Subscript upper A Baseline
    With right-arrow equals Start 4 By 1 Matrix 1st Row  0 2nd Row  1 slash 3 3rd
    Row  1 slash 3 4th Row  1 slash 3 EndMatrix dollar-sign"><mrow><mover accent="true"><mrow><mi>l</mi><mi>i</mi><mi>n</mi><mi>k</mi><mi>i</mi><mi>n</mi><msub><mi>g</mi>
    <mi>A</mi></msub></mrow> <mo>→</mo></mover> <mo>=</mo> <mfenced close=")" open="("><mtable><mtr><mtd><mn>0</mn></mtd></mtr>
    <mtr><mtd><mrow><mn>1</mn> <mo>/</mo> <mn>3</mn></mrow></mtd></mtr> <mtr><mtd><mrow><mn>1</mn>
    <mo>/</mo> <mn>3</mn></mrow></mtd></mtr> <mtr><mtd><mrow><mn>1</mn> <mo>/</mo>
    <mn>3</mn></mrow></mtd></mtr></mtable></mfenced></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: If the web surfer is at page B, there is a one half chance they will move to
    page A and one half chance they will move to page D. Thus, the outward linking
    vector of page B is
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign ModifyingAbove l i n k i n g Subscript upper B Baseline
    With right-arrow equals Start 4 By 1 Matrix 1st Row  1 slash 2 2nd Row  0 3rd
    Row  0 4th Row  1 slash 2 EndMatrix dollar-sign"><mrow><mover accent="true"><mrow><mi>l</mi><mi>i</mi><mi>n</mi><mi>k</mi><mi>i</mi><mi>n</mi><msub><mi>g</mi>
    <mi>B</mi></msub></mrow> <mo>→</mo></mover> <mo>=</mo> <mfenced close=")" open="("><mtable><mtr><mtd><mrow><mn>1</mn>
    <mo>/</mo> <mn>2</mn></mrow></mtd></mtr> <mtr><mtd><mn>0</mn></mtd></mtr> <mtr><mtd><mn>0</mn></mtd></mtr>
    <mtr><mtd><mrow><mn>1</mn> <mo>/</mo> <mn>2</mn></mrow></mtd></mtr></mtable></mfenced></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, the outward linking vectors of pages C and D are
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign ModifyingAbove l i n k i n g Subscript upper C Baseline
    With right-arrow equals Start 4 By 1 Matrix 1st Row  0 2nd Row  0 3rd Row  0 4th
    Row  1 EndMatrix and ModifyingAbove l i n k i n g Subscript upper D Baseline With
    right-arrow equals Start 4 By 1 Matrix 1st Row  0 2nd Row  1 slash 2 3rd Row  1
    slash 2 4th Row  0 EndMatrix dollar-sign"><mrow><mover accent="true"><mrow><mi>l</mi><mi>i</mi><mi>n</mi><mi>k</mi><mi>i</mi><mi>n</mi><msub><mi>g</mi>
    <mi>C</mi></msub></mrow> <mo>→</mo></mover> <mo>=</mo> <mfenced close=")" open="("><mtable><mtr><mtd><mn>0</mn></mtd></mtr>
    <mtr><mtd><mn>0</mn></mtd></mtr> <mtr><mtd><mn>0</mn></mtd></mtr> <mtr><mtd><mn>1</mn></mtd></mtr></mtable></mfenced>
    <mtext>and</mtext> <mover accent="true"><mrow><mi>l</mi><mi>i</mi><mi>n</mi><mi>k</mi><mi>i</mi><mi>n</mi><msub><mi>g</mi>
    <mi>D</mi></msub></mrow> <mo>→</mo></mover> <mo>=</mo> <mfenced close=")" open="("><mtable><mtr><mtd><mn>0</mn></mtd></mtr>
    <mtr><mtd><mrow><mn>1</mn> <mo>/</mo> <mn>2</mn></mrow></mtd></mtr> <mtr><mtd><mrow><mn>1</mn>
    <mo>/</mo> <mn>2</mn></mrow></mtd></mtr> <mtr><mtd><mn>0</mn></mtd></mtr></mtable></mfenced></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'We bundle the linking vectors of all the webpages together to create a linking
    matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign upper L i n k i n g equals Start 4 By 4 Matrix 1st
    Row 1st Column 0 2nd Column 1 slash 2 3rd Column 0 4th Column 0 2nd Row 1st Column
    1 slash 3 2nd Column 0 3rd Column 0 4th Column 1 slash 2 3rd Row 1st Column 1
    slash 3 2nd Column 0 3rd Column 0 4th Column 1 slash 2 4th Row 1st Column 1 slash
    3 2nd Column 1 slash 2 3rd Column 1 4th Column 0 EndMatrix dollar-sign"><mrow><mi>L</mi>
    <mi>i</mi> <mi>n</mi> <mi>k</mi> <mi>i</mi> <mi>n</mi> <mi>g</mi> <mo>=</mo> <mfenced
    close=")" open="("><mtable><mtr><mtd><mn>0</mn></mtd> <mtd><mrow><mn>1</mn> <mo>/</mo>
    <mn>2</mn></mrow></mtd> <mtd><mn>0</mn></mtd> <mtd><mn>0</mn></mtd></mtr> <mtr><mtd><mrow><mn>1</mn>
    <mo>/</mo> <mn>3</mn></mrow></mtd> <mtd><mn>0</mn></mtd> <mtd><mn>0</mn></mtd>
    <mtd><mrow><mn>1</mn> <mo>/</mo> <mn>2</mn></mrow></mtd></mtr> <mtr><mtd><mrow><mn>1</mn>
    <mo>/</mo> <mn>3</mn></mrow></mtd> <mtd><mn>0</mn></mtd> <mtd><mn>0</mn></mtd>
    <mtd><mrow><mn>1</mn> <mo>/</mo> <mn>2</mn></mrow></mtd></mtr> <mtr><mtd><mrow><mn>1</mn>
    <mo>/</mo> <mn>3</mn></mrow></mtd> <mtd><mrow><mn>1</mn> <mo>/</mo> <mn>2</mn></mrow></mtd>
    <mtd><mn>1</mn></mtd> <mtd><mn>0</mn></mtd></mtr></mtable></mfenced></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that the columns of the linking matrix are the outward linking probabilities
    and the rows of A are the *inward linking* probabilities: How can a surfer end
    up at page A? They can only be at B, and from there there is only a 0.5 probability
    they will end up at A.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we can *rank* page A by adding up the ranks of all the pages pointing to
    A each weighted by the probability a surfer will end up at page A from that page,
    that is, a page with many highly ranked pages pointing to it will also rank high.
    The ranks of all four pages are therefore:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign StartLayout 1st Row 1st Column r a n k Subscript
    upper A 2nd Column equals 0 r a n k Subscript upper A Baseline plus 1 slash 2
    r a n k Subscript upper B Baseline plus 0 r a n k Subscript upper C Baseline plus
    0 r a n k Subscript upper D Baseline 2nd Row 1st Column r a n k Subscript upper
    B 2nd Column equals 1 slash 3 r a n k Subscript upper A Baseline plus 0 r a n
    k Subscript upper B Baseline plus 0 r a n k Subscript upper C Baseline plus 1
    slash 2 r a n k Subscript upper D Baseline 3rd Row 1st Column r a n k Subscript
    upper C 2nd Column equals 1 slash 3 r a n k Subscript upper A Baseline plus 0
    r a n k Subscript upper B Baseline plus 0 r a n k Subscript upper C Baseline plus
    1 slash 2 r a n k Subscript upper D Baseline 4th Row 1st Column r a n k Subscript
    upper D 2nd Column equals 1 slash 3 r a n k Subscript upper A Baseline plus 1
    slash 2 r a n k Subscript upper B Baseline plus 1 r a n k Subscript upper C Baseline
    plus 0 r a n k Subscript upper D EndLayout dollar-sign"><mtable displaystyle="true"><mtr><mtd
    columnalign="right"><mrow><mi>r</mi> <mi>a</mi> <mi>n</mi> <msub><mi>k</mi> <mi>A</mi></msub></mrow></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <mn>0</mn> <mi>r</mi> <mi>a</mi> <mi>n</mi>
    <msub><mi>k</mi> <mi>A</mi></msub> <mo>+</mo> <mn>1</mn> <mo>/</mo> <mn>2</mn>
    <mi>r</mi> <mi>a</mi> <mi>n</mi> <msub><mi>k</mi> <mi>B</mi></msub> <mo>+</mo>
    <mn>0</mn> <mi>r</mi> <mi>a</mi> <mi>n</mi> <msub><mi>k</mi> <mi>C</mi></msub>
    <mo>+</mo> <mn>0</mn> <mi>r</mi> <mi>a</mi> <mi>n</mi> <msub><mi>k</mi> <mi>D</mi></msub></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mrow><mi>r</mi> <mi>a</mi> <mi>n</mi> <msub><mi>k</mi>
    <mi>B</mi></msub></mrow></mtd> <mtd columnalign="left"><mrow><mo>=</mo> <mn>1</mn>
    <mo>/</mo> <mn>3</mn> <mi>r</mi> <mi>a</mi> <mi>n</mi> <msub><mi>k</mi> <mi>A</mi></msub>
    <mo>+</mo> <mn>0</mn> <mi>r</mi> <mi>a</mi> <mi>n</mi> <msub><mi>k</mi> <mi>B</mi></msub>
    <mo>+</mo> <mn>0</mn> <mi>r</mi> <mi>a</mi> <mi>n</mi> <msub><mi>k</mi> <mi>C</mi></msub>
    <mo>+</mo> <mn>1</mn> <mo>/</mo> <mn>2</mn> <mi>r</mi> <mi>a</mi> <mi>n</mi> <msub><mi>k</mi>
    <mi>D</mi></msub></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mrow><mi>r</mi>
    <mi>a</mi> <mi>n</mi> <msub><mi>k</mi> <mi>C</mi></msub></mrow></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <mn>1</mn> <mo>/</mo> <mn>3</mn> <mi>r</mi> <mi>a</mi> <mi>n</mi> <msub><mi>k</mi>
    <mi>A</mi></msub> <mo>+</mo> <mn>0</mn> <mi>r</mi> <mi>a</mi> <mi>n</mi> <msub><mi>k</mi>
    <mi>B</mi></msub> <mo>+</mo> <mn>0</mn> <mi>r</mi> <mi>a</mi> <mi>n</mi> <msub><mi>k</mi>
    <mi>C</mi></msub> <mo>+</mo> <mn>1</mn> <mo>/</mo> <mn>2</mn> <mi>r</mi> <mi>a</mi>
    <mi>n</mi> <msub><mi>k</mi> <mi>D</mi></msub></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mrow><mi>r</mi>
    <mi>a</mi> <mi>n</mi> <msub><mi>k</mi> <mi>D</mi></msub></mrow></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <mn>1</mn> <mo>/</mo> <mn>3</mn> <mi>r</mi> <mi>a</mi> <mi>n</mi> <msub><mi>k</mi>
    <mi>A</mi></msub> <mo>+</mo> <mn>1</mn> <mo>/</mo> <mn>2</mn> <mi>r</mi> <mi>a</mi>
    <mi>n</mi> <msub><mi>k</mi> <mi>B</mi></msub> <mo>+</mo> <mn>1</mn> <mi>r</mi>
    <mi>a</mi> <mi>n</mi> <msub><mi>k</mi> <mi>C</mi></msub> <mo>+</mo> <mn>0</mn>
    <mi>r</mi> <mi>a</mi> <mi>n</mi> <msub><mi>k</mi> <mi>D</mi></msub></mrow></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'To find the numerical value for the rank of each webpage, we have to solve
    the above system of linear equations, which is the territory of linear algebra.
    In matrix vector notation, we write the above system as:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign ModifyingAbove r a n k s With right-arrow equals
    upper L i n k i n g ModifyingAbove r a n k s With right-arrow dollar-sign"><mrow><mover
    accent="true"><mrow><mi>r</mi><mi>a</mi><mi>n</mi><mi>k</mi><mi>s</mi></mrow>
    <mo>→</mo></mover> <mo>=</mo> <mi>L</mi> <mi>i</mi> <mi>n</mi> <mi>k</mi> <mi>i</mi>
    <mi>n</mi> <mi>g</mi> <mover accent="true"><mrow><mi>r</mi><mi>a</mi><mi>n</mi><mi>k</mi><mi>s</mi></mrow>
    <mo>→</mo></mover></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, the vector containing all the ranks of all the webpages is an eigenvector
    of the linking matrix of the graph of the webpages (where the node are the webpages
    and the directed edges are the links between them) with eigenvalue 1\. Recall
    that in reality, the graph of the web is enormous, which means that the linking
    matrix is enormous, and devising efficient ways to find its eigenvectors becomes
    of immediate interest.
  prefs: []
  type: TYPE_NORMAL
- en: 'Computing eigenvectors and eigenvalues of a given matrix is one of the most
    important contributions of numerical linear algebra, with immediate applications
    in many field. A lot of the numerical methods for finding eigenvectors and eigenvalues
    involve repeatedly multiplying a matrix with a vector. When dealing with huge
    matrices, this is expensive and we have to use every trick in the book to make
    the operations cheaper: We take advantage of the sparsity of the matrix (many
    entries are zeros so it is a waste to multiply with these entries *then* discover
    that they are just zeros); we introduce randomization or stochasticity, and venture
    into the fields of high dimensional probability and large random matrices (we
    will get a flavor of these in [Chapter 11](ch11.xhtml#ch11) on Probability). For
    now, we re-emphasize the iterative method we introduced in [Chapter 6](ch06.xhtml#ch06)
    on Singular Value Decompoitions: We start with a random vector <math alttext="ModifyingAbove
    r a n k s With right-arrow Subscript 0"><msub><mover accent="true"><mrow><mi>r</mi><mi>a</mi><mi>n</mi><mi>k</mi><mi>s</mi></mrow>
    <mo>→</mo></mover> <mn>0</mn></msub></math> then produce a sequence of vectors
    iteratively by multiplying by the linking matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign ModifyingAbove r a n k s With right-arrow Subscript
    i plus 1 Baseline equals upper L i n k i n g ModifyingAbove r a n k s With right-arrow
    Subscript i Baseline dollar-sign"><mrow><msub><mover accent="true"><mrow><mi>r</mi><mi>a</mi><mi>n</mi><mi>k</mi><mi>s</mi></mrow>
    <mo>→</mo></mover> <mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub> <mo>=</mo>
    <mi>L</mi> <mi>i</mi> <mi>n</mi> <mi>k</mi> <mi>i</mi> <mi>n</mi> <mi>g</mi> <msub><mover
    accent="true"><mrow><mi>r</mi><mi>a</mi><mi>n</mi><mi>k</mi><mi>s</mi></mrow>
    <mo>→</mo></mover> <mi>i</mi></msub></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: For our four page world wide web, this converges to the vector
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign ModifyingAbove r a n k s With right-arrow equals
    Start 4 By 1 Matrix 1st Row  0.12 2nd Row  0.24 3rd Row  0.24 4th Row  0.4 EndMatrix
    dollar-sign"><mrow><mover accent="true"><mrow><mi>r</mi><mi>a</mi><mi>n</mi><mi>k</mi><mi>s</mi></mrow>
    <mo>→</mo></mover> <mo>=</mo> <mfenced close=")" open="("><mtable><mtr><mtd><mrow><mn>0</mn>
    <mo>.</mo> <mn>12</mn></mrow></mtd></mtr> <mtr><mtd><mrow><mn>0</mn> <mo>.</mo>
    <mn>24</mn></mrow></mtd></mtr> <mtr><mtd><mrow><mn>0</mn> <mo>.</mo> <mn>24</mn></mrow></mtd></mtr>
    <mtr><mtd><mrow><mn>0</mn> <mo>.</mo> <mn>4</mn></mrow></mtd></mtr></mtable></mfenced></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: which means that page D is ranked highest and in a search engine query with
    similar content it will be the first page returned. We can then redraw the diagram
    in [Figure 9-3](#Fig_graph_page_rank) with each circle’s size corresponding to
    the page’s importance.
  prefs: []
  type: TYPE_NORMAL
- en: 'When PageRank algorithm was in use, the real implementation included a damping
    factor *d* which is a number between zero and one, usually around 0.85, which
    takes into account only an 85 percent chance that the web surfer clicks on a link
    from the page they are currently at, and a fifteen percent chance that they start
    at a completely new page which had no links from the page they are currently at.
    This modifies the iterative process to find the rankings of the pages of the web
    in a straight forward way:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign ModifyingAbove r a n k s With right-arrow Subscript
    i plus 1 Baseline equals d left-parenthesis upper L i n k i n g ModifyingAbove
    r a n k s With right-arrow Subscript i Baseline right-parenthesis plus StartFraction
    1 minus d Over total number of pages EndFraction ModifyingAbove o n e s With right-arrow
    dollar-sign"><mrow><msub><mover accent="true"><mrow><mi>r</mi><mi>a</mi><mi>n</mi><mi>k</mi><mi>s</mi></mrow>
    <mo>→</mo></mover> <mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub> <mo>=</mo>
    <mi>d</mi> <mrow><mo>(</mo> <mi>L</mi> <mi>i</mi> <mi>n</mi> <mi>k</mi> <mi>i</mi>
    <mi>n</mi> <mi>g</mi> <msub><mover accent="true"><mrow><mi>r</mi><mi>a</mi><mi>n</mi><mi>k</mi><mi>s</mi></mrow>
    <mo>→</mo></mover> <mi>i</mi></msub> <mo>)</mo></mrow> <mo>+</mo> <mfrac><mrow><mn>1</mn><mo>-</mo><mi>d</mi></mrow>
    <mrow><mtext>total</mtext><mtext>number</mtext><mtext>of</mtext><mtext>pages</mtext></mrow></mfrac>
    <mover accent="true"><mrow><mi>o</mi><mi>n</mi><mi>e</mi><mi>s</mi></mrow> <mo>→</mo></mover></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, if you are wondering whether Google keeps searching the web for new
    webpages and indexing them, and does it keep checking all indexed webpages for
    new links? The answer is yes, and the following excerpts are from [Google’s How
    Search Works](https://developers.google.com/search/docs/advanced/guidelines/how-search-works):
    *Google Search is a fully-automated search engine that uses software known as
    web crawlers that explore the web regularly to find pages to add to our index.
    In fact, the vast majority of pages listed in our results aren’t manually submitted
    for inclusion, but are found and added automatically when our web crawlers explore
    the web. […] There isn’t a central registry of all web pages, so Google must constantly
    look for new and updated pages and add them to its list of known pages. This process
    is called “URL discovery”. Some pages are known because Google has already visited
    them. Other pages are discovered when Google follows a link from a known page
    to a new page: for example, a hub page, such as a category page, links to a new
    blog post. Still other pages are discovered when you submit a list of pages (a
    sitemap) for Google to crawl. […] When a user enters a query, our machines search
    the index for matching pages and return the results we believe are the highest
    quality and most relevant to the user. Relevancy is determined by hundreds of
    factors, which could include information such as the user’s location, language,
    and device (desktop or phone). For example, searching for “bicycle repair shops”
    would show different results to a user in Paris than it would to a user in Hong
    Kong.*'
  prefs: []
  type: TYPE_NORMAL
- en: The more data we collect the more complex searching it becomes. Google rolled
    RankBrain in 2015\. It uses machine learning to vectorize the text on the webpages,
    similar to what we did in [Chapter 7](ch07.xhtml#ch07). This process adds context
    and meaning to the indexed pages, so that the search returns more accurate results.
    The bad thing that this process adds is the much higher dimensions associated
    with meaning vectors. To circumvent the difficulty of checking every vector at
    every dimension before returning the webpages closest to the query, Google uses
    an approximate nearest neighbor algorithm, which helps return excellent results
    in milliseconds, the experience we have now.
  prefs: []
  type: TYPE_NORMAL
- en: Inverting Matrices Using Graphs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Many problems in the applied sciences involve writing a discrete linear system
    <math alttext="upper A ModifyingAbove x With right-arrow equals ModifyingAbove
    b With right-arrow"><mrow><mi>A</mi> <mover accent="true"><mi>x</mi> <mo>→</mo></mover>
    <mo>=</mo> <mover accent="true"><mi>b</mi> <mo>→</mo></mover></mrow></math> and
    solving it, which is equivalent to inverting the matrix A and finding the solution
    <math alttext="ModifyingAbove x With right-arrow equals upper A Superscript negative
    1 Baseline ModifyingAbove b With right-arrow"><mrow><mover accent="true"><mi>x</mi>
    <mo>→</mo></mover> <mo>=</mo> <msup><mi>A</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup>
    <mover accent="true"><mi>b</mi> <mo>→</mo></mover></mrow></math> . But for large
    matrices, this is a computationally expensive operation, along with high storage
    requirements, and poor accuracy. We are always looking for efficient ways to invert
    matrices, sometimes leveraging the special characteristics of the particular matrices
    at hand.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a graph theoretic method that computes the inverse of a matrix
    of a decent size (for example, a hundred rows and a hundred columns):'
  prefs: []
  type: TYPE_NORMAL
- en: Replace each nonzero entry in the matrix A with a 1\. We obtain a binary matrix.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Permute the rows and the corresponding columns of the resulting binary matrix
    to make all diagonal entries 1’s.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We think of the matrix obtained as the adjacency matrix of a directed graph
    (where we delete the self-loops corresponding to 1’s along the diagonal from the
    graph).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The resulting directed graph is partitioned into its fragments.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If a fragment is too large, then we tear it further into smaller fragments by
    removing an appropriate edge.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We invert the smaller matrices.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Apparently this leades to the inverse of the original matrix.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We will not explain why and how, but this method is so cute so it made its way
    into this chpater.
  prefs: []
  type: TYPE_NORMAL
- en: 'Cayley Graphs Of Groups: Pure Algebra And Parallel Computing'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Graphs of groups, also called Cayley graphs or Cayley diagrams, can be helpful
    in designing and analyzing network architectures for parallel computers, routing
    problems, and routing algorithms for interconnected networks. The paper [Processor
    Interconnection Networks From Cayley Graphs](https://www.nsa.gov/portals/75/documents/news-features/declassified-documents/cryptologic-quarterly/Processor_Interconnection.pdf)
    is an interesting and easy read on earlier designs applying Cayley graphs for
    parallel computing networks, and explains how to construct Cayley graphs that
    meet specific design parameters. Cayley graphs have also been applied for [classification
    of data](https://www.sciencedirect.com/science/article/pii/S0012365X08006869).
  prefs: []
  type: TYPE_NORMAL
- en: 'We can represent every group with n elements as a connected directed graph
    of n nodes, where each node corresponds to an element from the group, and each
    edge represents a multiplication by a generator from the group. The edges are
    labeled (or colored) depending on which generator from the group we are multiplying
    by (see [Figure 9-4](#Fig_graph_of_group)). This directed graph uniquely defines
    the group: Each product of elements in the group corresponds to following a sequence
    of directed edges on the graph. For example, the graph of a cyclic group of n
    elements is a directed circuit of n nodes in which every edge represents multiplication
    by one generator of the group.'
  prefs: []
  type: TYPE_NORMAL
- en: From a pure math perspective, Cayley graphs are useful for visualizing and studying
    abstract groups, encoding their full abstract structure and all of their elements
    in a visual diagram. The symmetry of Cayley graphs makes them useful for constructing
    more involved abstract objects. These are central tools for combinatorial and
    geometric group theory. For more Cayley graphs, check [this page](https://mathworld.wolfram.com/CayleyGraph.xhtml).
  prefs: []
  type: TYPE_NORMAL
- en: '![250](assets/emai_0904.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9-4\. Cayley graph for the free group on two generators a and b: Each
    node represents an element of the free group, and each edge represents multiplication
    by a or b. ([image source](https://commons.wikimedia.org/wiki/File:F2_Cayley_Graph.png)).'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Message Passing Within A Graph
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A useful approach for modeling the propagation of information within graphs,
    as well as neatly aggregating the information conveyed in the nodes, edges, and
    the structure of the graph itself into vectors of a certain desired dimension
    is the *message passing framework*. Within this framework, we update every node
    with information from the feature vectors of its neighboring nodes and the edges
    connected to them. A graph neural network performs multiple rounds of message
    passing, each round propagates a single node’s information further. Finally, we
    combine the latent features of each individual node to obtain its unified vector
    representation, and to also represent the whole graph.
  prefs: []
  type: TYPE_NORMAL
- en: More concretely, for a specific node, we choose a function that takes as input
    the node’s feature vector, the feature vector of one of its neighboring nodes
    (those connected to it by an edge), and the feature vector of the edge that connects
    it to this neighboring node, and outputs a new vector that contains within it
    information from the node, the neighbor, and their connecting edge. We apply the
    same function to all of the node’s neighbors, then add the resulting vectors together,
    producing a *message vector*. Finally, we update the feature vector of our node
    by combining its original feature vector with the message vector within an update
    function that we also choose. When we do this for each node in the graph, each
    node’s new feature vector will contain information from itself, all its neighbors,
    and all its connecting edges. Now when we repeat this process one more time, the
    node’s most recent feature vector will contain information from itself, all its
    neighbors *and its neighbor’s neighbors*, and all the corresponding connecting
    edges. Thus, the more message passing rounds we do, the more each node’s feature
    vector contains information from farther nodes within the graph, moving one edge
    separation at a time. The information diffuses successively across the entire
    network.
  prefs: []
  type: TYPE_NORMAL
- en: The Limitless Applications Of Graphs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Applications for graph neural networks, and graph models in general, are ubiquitous
    and so important that I am a bit regretful I did not start the book with graphs.
    In any graph model, we start by answering the following questions:'
  prefs: []
  type: TYPE_NORMAL
- en: What are the nodes?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is the relationship that links two nodes, that establishes directed or
    undirected edge(s) between them?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Should the model include feature vectors for the nodes and/or the edges?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is our model dynamic, where the nodes, edges, and their features evolve with
    time, or is it static in time?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What are we interested in? Classifying (for example cancerous or noncancerous;
    fake news spreader or non fake news spreader)? Generating new graphs (for example
    for drug discovery)? Clustering? Embedding the graph into a lower dimensional
    and structured space?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What kind of the data is available or needed, and is the data organized and/or
    labeled? Does it need preprocessing?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We survey few applications in this section, but there are many more that genuinely
    lend themselves to a graph modeling structure. It is good to read the abstracts
    of the linked publications since they help capture common themes and ways of thinking
    about these models. The following list gives a good idea of the common tasks for
    graph neural networks, which include:'
  prefs: []
  type: TYPE_NORMAL
- en: Node classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Graph classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clustering and community detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generation of new graphs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Influence maximization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Link prediction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Image data as graphs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We might encounter graph neural networks tested on the [MNIST data set for
    handwritten digits](http://yann.lecun.com/exdb/mnist/), which is one of the benchmark
    sets for computer vision. If you wonder how image data (stored as three dimensional
    tensors of pixel intensities across each channel) manages to fit into a graph
    structure, here’s how it works: Each pixel is a node, its features are the respective
    intensities of its three channels (if it is a color image, otherwise it only has
    one feature). The edges connect each pixel to the three, five, or eight pixels
    surrounding it, depending on whether the pixel is located at a corner, edge, or
    in middle of the image.'
  prefs: []
  type: TYPE_NORMAL
- en: Brain Networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of the main pursuits in neuroscience is understanding the network organization
    of the brain. Graph models provide a natural framework and many tools for analyzing
    the complex networks of the brain, both in terms of their anatomy and their functionality.
  prefs: []
  type: TYPE_NORMAL
- en: To create artificial intelligence on par with human intelligence, we must understand
    the human brain on many levels. One aspect is the brain’s network connectivity,
    how connectivity affects the brain’s functionality, and how to replicate that,
    building up from small computational units up to modular components to a fully
    independent and functional system.
  prefs: []
  type: TYPE_NORMAL
- en: 'Human brain anatomical networks, demonstrate short path length (conservation
    of wiring costs) along with high degree cortical *hubs*, that is, high clustering.
    This is on both the cellular scales and on the whole brain scale. In other words,
    the brain network seems to have organized itself in a way that maximizes the efficiency
    of information transfer and minimizes connection cost. The network also demonstrates
    modular and hierarchical topological structures and functionalities. The topological
    structures of the brain networks and their functionalities are interdependent
    over both short and long time scales: The dynamic properties of the networks are
    affected by their structural connectivity, and over a longer timescale, the dynamics
    affect the topological structure of the network.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The most important questions are: What is the relationship between the network
    properties of the brain and its cognitive behavior? What is the relationship betweeen
    the network properties and brain and mental disoders? For example, we can view
    neuropsychiatric disorders as dysconnectivity syndromes, where graph theory can
    help quantify weaknesses, vulnerability to lesions, and abnormalities in the network
    structures. In fact, graph theory has been applied to study the structural and
    functional network properties in schizophrenia, Alzheimer’s, and other disorders.'
  prefs: []
  type: TYPE_NORMAL
- en: Spread Of Disease
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As we have all learned from the Covid19 pandemic, it is of crucial importance
    to be able to forecast disease incidents accurately and reliably, for mitigation
    purposes, quarantine measures, policy, and many other decision factors. A graph
    model can consider either individuals or entire geographic blocks as nodes, and
    contact occurences between these individuals or blocks as edges. Recent models
    for predicting Covid19 spread, for example the article [Combining graph neural
    networks and spatio-temporal disease models to improve the prediction of weekly
    COVID-19 cases in Germany (2022)](https://www.nature.com/articles/s41598-022-07757-5),
    incorporate human mobility data from Facebook, Apple, and Google to model interactions
    between the nodes in their models.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is plenty of data that can be put to good use here: Facebook’s [Data
    For Good](https://dataforgood.facebook.com) resource has a wealth of data on population
    densities, social mobility and travel patterns, social connectedness, and others.
    Google’s [Covid 19 Community Mobility Reports](https://www.google.com/covid19/mobility/index.xhtml?hl=en)
    draw insights from Google Maps and other products into a data set that charts
    movement trends over time by geography, across different categories of places
    such as retail and recreation, groceries and pharmacies, parks, transit stations,
    workplaces, and residential areas. Similarly, Apple’s and Amazon’s mobility data
    serve similar purpose with the goal of aiding efforts to limit the spread of Covid19.'
  prefs: []
  type: TYPE_NORMAL
- en: Spread Of Information
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We can use graphs to model the spread of information, disease, rumors, gossip,
    computer viruses, innovative ideas, or others. Such a model is usually a directed
    graph, where each node corresponds to an individual, and the edges are tagged
    with information about the interaction between individuals. The edge tags, or
    weights, are usually probabilities: The weight <math alttext="w Subscript i j"><msub><mi>w</mi>
    <mrow><mi>i</mi><mi>j</mi></mrow></msub></math> of the edge connecting <math alttext="n
    o d e Subscript i"><mrow><mi>n</mi> <mi>o</mi> <mi>d</mi> <msub><mi>e</mi> <mi>i</mi></msub></mrow></math>
    to <math alttext="n o d e Subscript j"><mrow><mi>n</mi> <mi>o</mi> <mi>d</mi>
    <msub><mi>e</mi> <mi>j</mi></msub></mrow></math> is the probability of a certain
    effect (disease, rumor, computer virus) propagating from <math alttext="n o d
    e Subscript i"><mrow><mi>n</mi> <mi>o</mi> <mi>d</mi> <msub><mi>e</mi> <mi>i</mi></msub></mrow></math>
    to <math alttext="n o d e Subscript j"><mrow><mi>n</mi> <mi>o</mi> <mi>d</mi>
    <msub><mi>e</mi> <mi>j</mi></msub></mrow></math> .'
  prefs: []
  type: TYPE_NORMAL
- en: Detecting And Tracking Fake News Propagation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Graph neural networks perform better in the task of detecting fake news (see
    [Figure 9-5](#Fig_fake_news)) than content based natural language processing approaches.
    The abstract of the 2019 paper, [*Fake News Detection on Social Media using Geometric
    Deep Learning*](https://arxiv.org/pdf/1902.06673.pdf) is informative: *Social
    media are nowadays one of the main news sources for millions of people around
    the globe due to their low cost, easy access, and rapid dissemination. This however
    comes at the cost of dubious trustworthiness and significant risk of exposure
    to ‘fake news’, intentionally written to mislead the readers. Automatically detecting
    fake news poses challenges that defy existing content-based analysis approaches.
    One of the main reasons is that often the interpretation of the news requires
    the knowledge of political or social context or ‘common sense’, which current
    natural language processing algorithms are still missing. Recent studies have
    empirically shown that fake and real news spread differently on social media,
    forming propagation patterns that could be harnessed for the automatic fake news
    detection. Propagation based approaches have multiple advantages compared to their
    content based counterparts, among which is language independence and better resilience
    to adversarial attacks. In this paper, we show a novel automatic fake news detection
    model based on geometric deep learning. The underlying core algorithms are a generalization
    of classical convolutional neural networks to graphs, allowing the fusion of heterogeneous
    data such as content, user profile and activity, social graph, and news propagation.
    Our model was trained and tested on news stories, verified by professional fact
    checking organizations, that were spread on Twitter. Our experiments indicate
    that social network structure and propagation are important features allowing
    highly accurate (92.7% ROC AUC) fake news detection. Second, we observe that fake
    news can be reliably detected at an early stage, after just a few hours of propagation.
    Third, we test the aging of our model on training and testing data separated in
    time. Our results point to the promise of propagation based approaches for fake
    news detection as an alternative or complementary strategy to content based approaches.*'
  prefs: []
  type: TYPE_NORMAL
- en: '![250](assets/emai_0905.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-5\. Nodes that spread fake news are labeled in red color. People who
    think alike cluster together in social networks ([image source](https://arxiv.org/pdf/1902.06673.pdf)).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Web-Scale Recommendation Systems
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Since 2018, Pinterest has been using [PinSage graph convolutional network](https://arxiv.org/pdf/1806.01973.pdf).
    This curates users’ homefeed and makes suggestions for new and relevant pins.
    The authors utilize *random walks on graphs* in their model, which we will discuss
    later in this chapter. Here is the full abstract: *Recent advancements in deep
    neural networks for graph-structured data have led to state-of-the-art performance
    on recommender system benchmarks. However, making these methods practical and
    scalable to web-scale recommendation tasks with billions of items and hundreds
    of millions of users remains a challenge. Here we describe a large-scale deep
    recommendation engine that we developed and deployed at Pinterest. We develop
    a data efficient Graph Convolutional Network (GCN) algorithm PinSage, which combines
    efficient random walks and graph convolutions to generate embeddings of nodes
    (i.e., items) that incorporate both graph structure as well as node feature information.
    Compared to prior GCN approaches, we develop a novel method based on highly efficient
    random walks to structure the convolutions and design a novel training strategy
    that relies on harder-and-harder training examples to improve robustness and convergence
    of the model. We deploy PinSage at Pinterest and train it on 7.5 billion examples
    on a graph with 3 billion nodes representing pins and boards, and 18 billion edges.
    According to offline metrics, user studies and A/B tests, PinSage generates higher-quality
    recommendations than comparable deep learning and graph-based alternatives. To
    our knowledge, this is the largest application of deep graph embed- dings to date
    and paves the way for a new generation of web-scale recommender systems based
    on graph convolutional architectures.*'
  prefs: []
  type: TYPE_NORMAL
- en: Fighting Cancer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the 2019 article, [*HyperFoods: Machine intelligent mapping of cancer-beating
    molecules in foods*](https://www.nature.com/articles/s41598-019-45349-y), the
    authors use protein, gene, and drug interaction data to identify the molecules
    that help prevent and beat cancer. They also map the foods that are the richest
    in cancer beating molecules (see [Figure 9-6](#Fig_HyperFoods)). Again, the authors
    utilize random walks on graphs. Here’s the abstract of the paper: *Recent data
    indicate that up to 30–40% of cancers can be prevented by dietary and lifestyle
    measures alone. Herein, we introduce a unique network based machine learning platform
    to identify putative food based cancer beating molecules. These have been identified
    through their molecular biological network commonality with clinically approved
    anti-cancer therapies. A machine-learning algorithm of random walks on graphs
    (operating within the supercomputing DreamLab platform) was used to simulate drug
    actions on human interactome networks to obtain genome-wide activity profiles
    of 1962 approved drugs (199 of which were classified as “anti-cancer” with their
    primary indications). A supervised approach was employed to predict cancer-beating
    molecules using these ‘learned’ interactome activity profiles. The validated model
    performance predicted anti-cancer therapeutics with classification accuracy of
    84–90%. A comprehensive database of 7962 bioactive molecules within foods was
    fed into the model, which predicted 110 cancer-beating molecules (defined by anti-cancer
    drug likeness threshold of >70%) with expected capacity comparable to clinically
    approved anti-cancer drugs from a variety of chemical classes including flavonoids,
    terpenoids, and polyphenols. This in turn was used to construct a ‘food map’ with
    anti-cancer potential of each ingredient defined by the number of cancer-beating
    molecules found therein. Our analysis underpins the design of next-generation
    cancer preventative and therapeutic nutrition strategies.*'
  prefs: []
  type: TYPE_NORMAL
- en: '![250](assets/emai_0906.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9-6\. HyperFoods: Machine intelligent mapping of cancer-beating molecules
    in foods. The bigger the node the more diverse the set of cancer beating molecules.
    ([image source](https://www.nature.com/articles/s41598-019-45349-y)).'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Biochemical Graphs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We can represent molecules and chemical compounds as graphs where the nodes
    are the atoms the edges are the chemical bonds between them. Data sets from this
    chemoinformatics domain are useful for assessing a classification model’s performance.
    For example, the [NCI-1](https://paperswithcode.com/dataset/nci1) dataset, containing
    around 4100 chemical compounds, is useful for anti-cancer screens where the chemicals
    are labeled as positive or negative to hinder cell lung cancer. Similar labeled
    graph data sets for proteins and other compounds are available on the same website,
    along with the papers that employ them and the performance of different models
    on these data sets.
  prefs: []
  type: TYPE_NORMAL
- en: Molecular Graph Generation For Drug And Protein Structure Discovery
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the last chapter we learned how generative networks such as variational autoencoders
    and adversarial networks learn joint probability distributions from the data in
    order to generate similar looking data for various purposes. Generative networks
    for graphs build on similar ideas, however, they are a bit more involved than
    networks generating images per se. Generative graph networks generate new graphs
    either in a sequential manner, outputting nodes and edges step by step, or in
    a global manner, outputting a whole graph’s adjacency matrix at once. See for
    example the survey [paper on generative graph networks (2020)](https://arxiv.org/pdf/2007.06686.pdf)
    for details on the topic.
  prefs: []
  type: TYPE_NORMAL
- en: Citation Networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In citation networks, the nodes could be the authors and the edges are their
    co-authorships, or the nodes are papers and the (directed) edges are the citations
    between them: Each paper has directed edges pointing to the papers it cites. Features
    for each paper include its abstract, authors, year, venue, title, field of study,
    and others. Tasks include node clustering, node classification, and link prediction.
    Popular data sets for paper citation networks include Cora, Citeseer and Pubmed.
    The [Cora data set](https://sites.google.com/site/semanticbasedregularization/home/software/experiments_on_cora)
    contains around three thousand machine learning publications grouped into seven
    categories. Each paper in citation networks is represented by a one-hot vector
    indicating the presence or absence of a word from a prespecified dictionary, or
    by a term frequency-inverse document frequency (TF-IDF) vector. These data sets
    are updated continuously as more papers join the networks.'
  prefs: []
  type: TYPE_NORMAL
- en: Social Media Networks And Social Influence Prediction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Social media networks such as Facebook, Twitter, Instagram, and Reddit are
    a distinctive feature of our time (after 2010). The [Reddit data set](https://paperswithcode.com/dataset/reddit)
    is an example of the available datasets: This is a graph where the nodes are the
    posts and the edges are between two posts which have comments from the same user.
    The posts are also labeled with the community to which they belong.'
  prefs: []
  type: TYPE_NORMAL
- en: Social media networks and their social influence have a substantial impact on
    our societies, ranging from advertising to winning presidential elections to toppling
    political regimes. One important task for a graph model representing social networks
    is to predict the social influence of the nodes in the network. Here, the nodes
    are the users and their interactions are the edges. Features include users’ gender,
    age, sex, location, activity level, and others. One way to quantify social influence,
    the target variable, is through predicting the actions of a user given the actions
    of their near neighbors in the network. For example, if a user’s friends buy a
    product, what is the probability that they will buy the same product after a given
    period of time? Random walks on graphs help predict the social influence of certain
    nodes in a network.
  prefs: []
  type: TYPE_NORMAL
- en: Sociological Structures
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Social diagrams are directed graphs that represent relationships among individuals
    in a society or among groups of individuals. The nodes are the members of the
    society or the groups, and the directed edges are the relationships between these
    members, such as admiration, association, influence, and others. We are interested
    in the connectedness, separability, size of fragments, and so forth, in these
    social diagrams. One example is from anthropological studies where a number of
    tribes are classified according to their kinship structures.
  prefs: []
  type: TYPE_NORMAL
- en: Bayesian Networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Soon in this chapter we will discuss Bayesian networks. These are probabilistic
    graph models whose goal is one we are very familiar with in the AI field: To learn
    joint probability distribution of the features of a data set. Bayesian networks
    consider this joint probability distribution as a product of single variable distributions
    *conditional only on a nodes’ parents* in a graph representing the relationships
    between the features of the data. That is, the nodes are the feature variables
    and the edges are between the features that *we believe* to be connected. Applications
    include spam filtering, voice recognition, coding and decoding, to name a few.'
  prefs: []
  type: TYPE_NORMAL
- en: Traffic Forecasting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Traffic prediction is the task of predicting traffic volumes using historical
    road maps, road speed, and traffic volume data. There are [benchmark traffic data
    sets](https://paperswithcode.com/task/traffic-prediction) which we can use to
    track progress and compare models. For example, the [METR-LA](https://www.researchgate.net/figure/The-dataset-was-collected-from-the-highway-of-Los-Angeles-County-METR-LA-from-1-March_fig7_344150366)
    is a spatial-temporal graph data set, containing four months of traffic data collected
    by 207 sensors on the highways of LA County. The traffic network is a graph, where
    the nodes are the sensors, and the edges are the road segments between these sensors.
    At a certain time *t*, the features are traffic parameters such as velocity and
    volume. The task of a graph neural network is to predict the features of the graph
    after certain time has elapsed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Other traffic forecasting models employ [Bayesian networks](http://bigeye.au.tsinghua.edu.cn/english/paper/A%20Bayesian%20Network%20Approach%20to.pdf):
    Traffic flows among adjacent road links. The model uses information from adjacent
    road links to analyze the trends of focus links.'
  prefs: []
  type: TYPE_NORMAL
- en: Logistics And Operations Research
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We can model and solve many problems in operations research, such as transportation
    problems and activity networks, using graphs. The graphs involved are usually
    weighted directed graphs. Operations research problems are combinatorial in nature,
    and are always trivial if the network is small. However, for large real world
    networks, the challenge is finding efficient algorithms that can sift through
    the enormous search space and quickly rule out big parts of it out. A large part
    of the research literature deals with estimating the computational complexity
    of such algorithms. This is called combinatorial optmization. Typical problems
    include the *travelling salesman problem*, *supply chain optimization*, *shared
    rides routing and fares*, *job matching*, and others. Some of the graph methods
    and algorithms used for such problems are minimal spanning trees, shortest paths,
    max flow min cuts, and matching in graphs. We will visit operations research examples
    later in this book.
  prefs: []
  type: TYPE_NORMAL
- en: Language Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Graph models are relevant for a variety of natural language tasks. These tasks
    seem different at the surface but many of them boil down to clustering, for which
    graph models are very well suited.
  prefs: []
  type: TYPE_NORMAL
- en: For any application we must first choose what the nodes, the edges, and the
    features for each represent. For natural language, these choices reveal hidden
    structures and regularities in the language and in the language corpora.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of representing a natural language sentence as a sequence of tokens
    for recurrent models or as a vector of tokens for transformers, in graph models,
    we embed sentences in a graph then employ graph deep learning (or graph neural
    networks).
  prefs: []
  type: TYPE_NORMAL
- en: One example from computational linguistics is constructing diagrams for parsing
    language [Figure 9-7](#Fig_parsing).
  prefs: []
  type: TYPE_NORMAL
- en: '![300](assets/emai_0907.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-7\. A parsed sentence.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The nodes are words, n-grams, or phrases, and the edges are the relationships
    between them, which depend on the language’s grammar or syntax (article, noun,
    verb, *etc.*). A language is defined as the set of all strings correctly generated
    from the language’s vocabulary according to its grammar rules. In that sense,
    computer languages are easy to parse (they are built that way), while natural
    languages are much harder to specify completely due to their complex nature.
  prefs: []
  type: TYPE_NORMAL
- en: Parsing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Parsing means converting a stream of input into a structured or formal representation
    so it can be automatically processed. The input to a parser might be sentences,
    words, or even characters. The output is a tree diagram containing information
    about the function of each part of the input. Our brain is a great parser for
    language inputs. Computers parse programming languages.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another example is news clustering or article recommendations. Here, we use
    graph embeddings of text data to determine text similarity: The nodes can be words
    and the edges can be semantic relationships between the words, or just their co-occurences.
    Or the nodes can be words and documents, and the edges can again be semantic or
    co-occurence relationships. Features for nodes and edges can include authors,
    topics, time periods, and others. Clusters emerge naturally in such graphs.'
  prefs: []
  type: TYPE_NORMAL
- en: Another type of parsing that does not depend on the syntax or grammar of a language
    is Abstract Meaning Representation (AMR). It relies instead on semantic representation,
    in the sense that sentences which are similar in meaning should be assigned the
    same abstract meaning represenation, even if they are not identically worded.
    Abstract meaning representation graphs are rooted, labeled, directed, acyclic
    graphs, representing full sentences. These are useful for machine translation
    and natural language understanding. There are packages and libraries for abstract
    meaning representation parsing, visualization, and surface generation as well
    as a publicly available data sets.
  prefs: []
  type: TYPE_NORMAL
- en: 'For other natural language applications, the following survey paper is a nice
    reference that is easy to read and learn more on the subject: [A survey of graphs
    in natural language processing](https://web.eecs.umich.edu/~mihalcea/papers/nastase.jnle15.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: Graph Structure Of The Web
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Since the inception of the *world wide web* (www) in 1989, it has grown enormously
    and has become an indespensible tool for billions of people around the world.
    It allows access to billions webpages, documents, and other resources using an
    internet web browser. With billions of pages linking to each other, it is of great
    interest to investigate the graph structure of the web. Mathematically, this vast
    and expansive graph is fascinating in its own right. But understanding this graph
    is important for more reasons than a beautiful mental exercise, providing insights
    into algorithms for crawling, indexing, and ranking the web (as in PageRank algorithm
    that we saw earlier in this chapter), searching communities, and discovering the
    social and other phenomena that characterize its growth or decay.
  prefs: []
  type: TYPE_NORMAL
- en: 'The world wide web graph has:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Nodes: Webpages, on the scale of billions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Edges: Are directed from one page linking to another page, on the scale of
    hundreds of billions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We are interested in:'
  prefs: []
  type: TYPE_NORMAL
- en: What is the average degree of the nodes?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Degree distributions of the nodes (for both the in degree and out degree, which
    can be very different). Are they power laws? Some other laws?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Connectivity of the graph: What is the percentage of connected pairs?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Average distances between the nodes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is the observed structure of the web dependent or independent of the the particular
    crawl used?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Particular structures of weakly and of strongly connected components.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is there a giant strongly connected component? What is the proportion of nodes
    that can reach or can be reached from this giant component?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Automatically Analyzing Computer Programs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We can use graphs for verification of computer programs, program reasoning,
    reliability theory, fault diagnosis in computers, and studying the structure of
    computer memory. The paper, [Graph Neural Networks on Program Analysis](https://miltos.allamanis.com/publicationfiles/allamanis2021graph/allamanis2021graph.pdf:),
    is one example: *Program analysis aims to determine if a program’s behavior complies
    with some specification. Commonly, program analyses need to be defined and tuned
    by humans. This is a costly process. Recently, machine learning methods have shown
    promise for probabilistically realizing a wide range of program analyses. Given
    the structured nature of programs, and the commonality of graph representations
    in program analysis, graph neural networks (GNN) offer an elegant way to represent,
    learn, and reason about programs and are commonly used in machine learning-based
    program analyses. This article discusses the use of graph neural networks for
    program analysis, highlighting two practical use cases: Variable misuse detection
    and type inference.*'
  prefs: []
  type: TYPE_NORMAL
- en: Data Structures In Computer Science
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A data structure in computer science is a structure that stores, manages, and
    organizes data. There are different data structures and they are usually chosed
    in a way that makes it efficient to access the data (reads, writes, appends, infer
    or store relationships, *etc.*).
  prefs: []
  type: TYPE_NORMAL
- en: Some data structures use graphs to organize data, computational devices in a
    cluster, and represent the flow of data and computation or the communication network.
    There are also graph databases geared towards storing and querying of graph data.
    Other databases transform graph data to more structured formats (such as relational
    formats).
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some examples for graph data structures:'
  prefs: []
  type: TYPE_NORMAL
- en: We have already encountered PageRank algorithm, along with the link structure
    of a website represented as a directed graph, where the nodes are the webpages
    and the edges represented the links from one page to another. A database keeping
    all the webpages along with their link structures can be either graph structured,
    where the graph is stored as is using the linking matrix or adjacency matrix,
    and no transformation necessary, or it can transformed to fit the structure of
    other nongraphical databases.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Binary search trees for organizing files in a database: Binary search trees
    are ordered data structures that are efficient for both random and sequential
    access of records, and for modification of files. The inherent order of a binary
    search tree speeds up search time: We cut the amount of data to sort through by
    half at each level of the tree. It also speeds up insersion time: Unlike an array,
    when we add a node to the binary tree data structure, we create a new piece in
    memory and link to it. This is faster than creating a new large array then inserting
    the data from the smaller array to the new, larger one.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Graph based information retrieval systems: In some information retrieval systems
    we assign a certain number of index terms to each document. We can think of these
    as the documents’ indicators, descriptors, or keywords. These index terms are
    represented will be nodes of a graph. We connect two index terms with an undirected
    edge if these two happen to be closely related, such as the indices *graph* and
    *network*. The resulting *similarity graph* is very large, and is possibly disconnected.
    The maximally connected subgraphs of this graph are its *components*, and they
    naturally classify the documents in this system. For information retrieval, our
    query specifies some index terms, that is, certain nodes of the graph, and the
    sytem returns the maximal complete subgraph that includes the corresponding nodes.
    This gives the complete list of index terms which in turn specify the documents
    we are searching for.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Load Balancing In Distributed Networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The computational world has grown from Moore’s Law to parallel computing to
    cloud computing. In cloud computing, neither our data nor our files nor the machines
    executing our files and doing computations on our data are near us. They are not
    even near each other. As applications become more complex and as network traffic
    increases, we need the software or hardware analogue of a network traffic cop,
    that distributes network traffic across multiple servers, so that no single server
    bears a heavy load, enhancing performance in terms of application response times,
    end user experience, and others. As traffic increases, more applicances, or nodes,
    need to be added to handle the volume. Network traffic distributing needs to be
    done while preserving data security and privacy, and should be able to predict
    traffic bottlenecks before they happen. This is exactly what load balancers do.
    It is not hard to imagine the distributed network as a graph with the nodes as
    the connected servers and appliances. Load balancing is then a traffic flow problem
    on a given graph, and there are a variety of algorithms for allocating the load.
    All algorithms operate on the network’s graph. Some are static, allocating loads
    without updating the network’s current state in terms of loads or malfunctioning
    units, others are dynamic, but requiring constant communication within the network
    about the nodes’ statuses. The following are some algorithms:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Least connection algorithm: This method directs traffic to the server with
    the fewest active connections.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Least response time algorithm: This method directs traffic to the server with
    the fewest active connections and the lowest average response time.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Round Robin algorithm: This algorithm allocates load in a rotation on the servers:
    First it directs traffic to the first available server, then it moves that server
    to the bottom of the queue.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'IP Hash: This method allocates servers based on the IP address of the client.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Artificial Neural Networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Finally, artificial neural networks themselves are graphs where the nodes are
    the computational units and the edges are inputs and outputs of these units. [Figure 9-8](#Fig_neural_networks_graphs)
    summarizes popular artificial neural network models as graphs.
  prefs: []
  type: TYPE_NORMAL
- en: '![250](assets/emai_0908.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-8\. Neural networks as graphs ([image source](https://www.asimovinstitute.org/author/fjodorvanveen/)).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Random Walks On Graphs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A random walk on a graph ([Figure 9-9](#Fig_random_walk_graph)) means exactly
    what it says: A sequence of steps that starts at some node, and at each time step
    chooses a neighboring node (using the adjacency matrix) with probability proportional
    to the weights of the edges, and moves there.'
  prefs: []
  type: TYPE_NORMAL
- en: '![300](assets/emai_0909.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-9\. A random walker on an undirected graph.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'If the edges are unweighted, then the neighboring nodes are all equally likely
    to be chosen for the walk’s move. At any time step, the walk can stay at the same
    node in the case there is a self edge, or when it is a lazy random walk, when
    there is a positive probability that the walker stays at a node instead of moving
    to one of its neigbours. We are interested in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: What is the list of nodes visited by a random walk, in the order they are visited?
    Here, the starting point and the structure of the graph matter in how much a walker
    covers or whether the walk can ever reach certain regions of the graph. In graph
    neural networks, one is interested in learning a representation for a given node
    based on its neighbours’ features. In large graphs where nodes have more neighbors
    than to be computationally feasible, we employ random walks. However, we have
    to be careful that different parts of the graph have different random walk expansion
    speeds, so if we do not take that into account by adjusting the nummber of steps
    of a random walk according to the subgraph structure, we might end up with low
    quality represenations for the nodes, and undesirable outcomes as these go down
    the work pipeline.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the expected behavior of a random walk, that is, the probability distribution
    over the visited nodes after a certain number of steps. We can study basic properties
    of a random walk, such as its long term behavior, by using *the spectrum of the
    graph* which is the set of eigenvalues of its adjacency matrix. In general, the
    spectrum of an operator helps us undertsand what happens when we repeatedly apply
    the operator. Randomly walking on a graph is equivalent to repeatedly applying
    a normalized version of the adjacency matrix to the node of the graph where we
    started. Every time we apply this *random walk matrix*, we walk one step further
    on the graph.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How does a random walk behave on different types of graphs, such as paths, trees,
    two fully connected graphs joined by one edge, infinite graphs, and others?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For a given graph, does the walk ever return to its starting point? If so how
    long do we have to walk until we return?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How long do we have to walk until we reach a specific node?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How long do we have to walk until we visit all the nodes?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How does a random walk *expand*? That is, what is the *influence distribution*
    of certain nodes that belong in certain regions of the graph? What is the size
    of their influence?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Can we design algorithms based on random walks that are able to reach *obscure*
    parts of large graphs?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Random walks and Brownian motion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the limit where the size of steps of a random walk goes to zero, we obtain
    a Brownian motion. Brownian motion usually models the random fluctuations of particles
    suspended in a medium such as a fluid, or price fluctuations of derivatives in
    financial markets. We frequently encounter the term Brownian motion with the term
    *Weiner process*, which is a continuous stochastic process with clear mathematical
    definition on how the motion (of a particle or of a price fluctuation in finance)
    starts (at zero), how the next step is sampled (from the normal distribution and
    with independent increments), and assumptions about its continuity as a function
    of time (it is almost surely continuous). Another term it is associated with is
    *martingale*. We will see these in [Chapter 11](ch11.xhtml#ch11) on Probability.
  prefs: []
  type: TYPE_NORMAL
- en: We did encounter a random walk once in this chapter when discussing PageRank
    algorithm, where a random webpage surfer randomly chooses to move from the page
    she’s at to a neighbouring page on the web. We noticed that the long term behavior
    of the walk is discovered when we repeatedly apply the linking matrix of the graph,
    which is the same as the adjacency matrix normalized by the degrees of each node.
    In the next section we see more uses of random walks for graph neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: We can use random walks (on directed or undirected graphs, weighted or unweighted
    graphs) for community detection and influence maximization in small networks,
    where we would only need the graph’s adjacency matrix (as opposed to node embedding
    into feature vectors then clustering).
  prefs: []
  type: TYPE_NORMAL
- en: Node Representation Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before implementing any graph tasks on a machine, we must be able to reperesent
    the nodes of the graph as vectors that contain information about their position
    in the graph and their features relative to their locality within the graph. A
    node’s represenation vector is usually aggregated from the node’s own features
    and those features of the nodes surounding it.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are different ways to aggregate features, transform them, or even to
    choose which of the neighbouring nodes contribute to the feature represenation
    of a given node. Let’s go over few methods:'
  prefs: []
  type: TYPE_NORMAL
- en: Traditional node representation methods rely on subgraph summary statistics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In other methods, nodes that occur together on short random walks will have
    similar vector representations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Other methods take into account that random walks tend to spread differently
    on different graph substructures, so a node’s represenation method adapt to the
    local substructure that the node belongs in, deciding on an appropriate radius
    on influence for each node depending on the topology of the subgraph it belongs
    to.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yet other methods produce a multi-scale representation by multiplying feature
    vector of a node with powers of a random walk matrice.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Other methods allow for nonlinear aggregation of the feature vectors of a node
    and its neighbours.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is also important to determine how large of a neighbourhood a node draws
    information from (influence distribution), that is, to find the range of nodes
    whose features affect a given node’s representation. This is analogous to sensitivity
    analysis in statistics, but here we need to determine the sensitivity of a node’s
    representation to changes in the features of the nodes surrounding it.
  prefs: []
  type: TYPE_NORMAL
- en: After creating a node represenation vector, we feed it into another machine
    learning model during training, such as a support vector machine model for classification,
    just like feeding other features of the data into the model. For example, We can
    learn the feature vector of every user in a social network, then pass these vectors
    into a classification model along with other features, to predict if the user
    is a fake news spreader or not. However, we do not have to rely on a machine learning
    model downstream to classify nodes. We can do that directly from the graph structural
    data where we can predict a node’s class depending on its association with other
    local nodes. The graph can be only partially labeled, and the task is to predict
    the rest of the labels. Moreover, the node representation step can either be a
    preprocessing step, or one part of an end to end model such as a graph neural
    network.
  prefs: []
  type: TYPE_NORMAL
- en: Tasks For Graph Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After going through the linear algebra formulation of graphs, applications of
    graph models, random walks on graphs, and vector node representations that encode
    node features along with their zones of influence within the graph, we should
    have a good idea about the kind of tasks that graph neural networks can perform.
    Let’s go through some of these.
  prefs: []
  type: TYPE_NORMAL
- en: Node Classification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In articles citation network, such as in Citeseer or Cora, where the nodes are
    academic papers (given as bag-of-words vectors) and the directed edges are the
    citations between the papers, classify each paper into a specific discipline.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the Reddit dataset, where the nodes are comment posts (given as word vectors)
    and undirected edges are between comments posted by the same user, classify each
    post according to the community it belongs to.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Protein-Protein Interaction network dataset contains 24 graphs where the
    nodes are labeled with the gene ontology sets (do not worry about the medical
    technical names, focus on the math instead. This is the nice thing about mathematical
    modeling, it works the same way for all kinds of applications from all kinds of
    fields, which validates it as a potential underlying language of the universe).
    Usually 20 graphs from the Protein-Protein Interaction network dataset are used
    for training, 2 graphs are used for validation and the rest for testing, each
    corresponding to a human tissue. The features associated with the nodes are positional
    gene sets, motif gene sets, and immunological signatures. Classify each node according
    to its gene ontology set.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are wildlife trade monitoring networks such as [Trade.org](https://www.traffic.org/),
    analyzing dynamic wildlife trade trends and using and updating data sets such
    as [CITES Wildlife Trade Database](https://www.kaggle.com/datasets/cites/cites-wildlife-trade-database)
    the [USADA US Department of Agriculture data commons](https://data.nal.usda.gov/dataset/data-united-states-wildlife-and-wildlife-product-imports-2000%E2%80%932014)
    (this data set includes more than million wildlife or wildlife product shipments,
    representing more than 60 biological classes and more than 3.2 billion live organisms).
    One classification task on the trade network is to classify each node, representing
    a trader (a buyer or a seller) as being engaged in illegal trade activity or not.
    The edges in the network would represent a trade transaction between those buyer
    and seller. Features for the nodes would include personal information of the traders,
    bank account numbers, locations, *etc.*, and features for the edges would include
    transaction idenstification numbers, dates, price tags, the species bought or
    sold, among others. If we already have a subset of the traders labeled as illegal
    traders, our model’s task would then be to predict the label of other nodes in
    the network based on their connections with other nodes (and their features) in
    the network.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Node classification examples lend themselves naturally to semi-supervised learning,
    where only few nodes in the data set come with their labels and the task is to
    label the rest of the nodes. Clean labeled data is what we all should be advocating
    for for our systems to be more accurate, reliable, and transparent.
  prefs: []
  type: TYPE_NORMAL
- en: Graph Classification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Sometimes we care for labeling a whole graph as opposed labeling the individual
    nodes of a graph. For example, in the PROTEINS dataset we have a collection of
    chemical compounds each represented as a graph and labeled as either an enzyme
    or not an enzyme. For a graph learning model we would input the nodes, edges,
    their features, the graph structure, and the label for each graph in the dataset,
    creating a whole graph representation or embedding, as opposed to a single node
    representation.
  prefs: []
  type: TYPE_NORMAL
- en: Clustering And Community Detection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Clustering in graphs is an important task which discovers communities or groups
    in networks, such as terrorist organizations. One way is to create node and graph
    representations then feed them into traditional clustering methods such as *k*
    means clustering. Other ways produce node and graph represenations that take into
    account the goal of clustering within their design. These can include encoder
    decoder designs and attention mechanisms similar to methods we encountered in
    previous chapters. Other methods are spectral, which means that they rely on the
    eigen values of the Laplacian matrix of the graph. Note that for nongraph data,
    principal component analysis is one method that we used for clustering which is
    also spectral, relying on the singular values of the data table. Computing eigenvalues
    of anything is always an expensive operation so the goal becomes finding ways
    around having to do it. For graphs we can employ graph theoretic methods such
    as *max flow min cut* (we will see this later in this chapter). Different methods
    have their own sets of strengths and shortcomings, for example some employ time
    proven graph theoretic results but fail to include the node or edge features because
    the theory was not developed with any features in mind, let alone a whole bunch
    of them. The message is to always be honest about what our models account and
    do not account for.
  prefs: []
  type: TYPE_NORMAL
- en: Graph Generation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Graph generation is very important for drug discovery, materials design, and
    other applications. Traditionally, graph generation approaches used hand crafted
    families of random graph models using a simple stochastic generation process.
    These models are well-understood mathematically due to their simple properties.
    However, due to the same reaon, they are limited in their ability to capture real
    world graphs with more complex dependencies, or even the correct statistical properties
    such as heavy tailed distribution for the node degrees that many real world networks
    exhibit. More recent approaches, such as generative graph neural networks, integrate
    graph and node representations with generative models that we went over in the
    previous chapter. These have a greater capacity to learn structural information
    from the data and generate complex graphs such as molecules and compounds.
  prefs: []
  type: TYPE_NORMAL
- en: Influence maximization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Influence maximization is a subfield of network diffusion, where the goal is
    to maximize the diffusion of something, such as information or a vaccine, through
    a network, while only giving the thing to few initial nodes, or seeds. So the
    objective is to locate the few nodes that have an overall maximal influence. Applications
    include information propagation such as job opennings, news, advertisements, and
    vaccinations. Traditional methods for locating the seeds choose the nodes based
    on highest degree, closeness, betweenness, and other graph structure properties.
    Others employ the field of discrete optimization, obtaining good results and proving
    the existence of approximate optimizers. More recent approaches employ graph neural
    networks and adversarial networks, when there are other objectives competing with
    the objective of maximizing a node’s influence, such as reaching a specific portion
    of the population, such as a certain minority group, who is not necessarily strongly
    connected with the natural hubs of the graph.
  prefs: []
  type: TYPE_NORMAL
- en: Link prediction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Given two nodes of a graph, what is the probability that there is an edge linking
    them? Note that proximity in the sense of sharing common neighbours is not necessarily
    an indicator for a link (or an interaction). In a social network, people tend
    to run in the same circles so two people sharing many common friends are likely
    to interact so they are likely connected as well, but in some biological systems,
    such as in studying protein-protein interactions, the opposite is true: Proteins
    sharing more common neighbours are less likely to interact. Therefore, computing
    similarity scores based on basic properties such as graph distance, degrees, common
    neighbours, and others does not always produce correct results. We need neural
    networks to learn node and graph embeddings along with classifying whether two
    nodes are linked or not. One example of such networks is in this paper: [Link
    Prediction with Graph Neural Networks and Knowledge Extraction](http://cs230.stanford.edu/projects_spring_2020/reports/38854344.pdf).'
  prefs: []
  type: TYPE_NORMAL
- en: Dynamic Graph Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Many of the applications we have discussed in this chapter would benefit from
    including time dependency in our graph models, since they are dynamic in nature.
    Examples include traffic forecasting, load balance for distributed networks, simulating
    all kinds of interacting particle systems, and illegal wildlife trade monitoring.
    In dynamic graph models, node and edge features are allowed to evolve with time,
    and some nodes or edges can be added or removed. This modeling captures information
    such as latest trade trends in a market, fluctuations, new criminal activity in
    certain networks, and new routes or connections in transportation systems.
  prefs: []
  type: TYPE_NORMAL
- en: Thinking about how to model dynamic graphs and extract information from them
    is not new, see for example the 1997 article [Dynamic graph models](https://www.sciencedirect.com/science/article/pii/S0895717797000502),
    however, the introduction of deep learning makes knowledge extraction from such
    systems more straightforward. Current approaches for dynamic graphs integrate
    graph convolutions to capture spatial dependencies with recurrent neural networks
    or convolutional neural networks to model temporal dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: The paper [Learning to Simulate Complex Physics with Graph Networks (2020)](https://arxiv.org/pdf/2002.09405.pdf)
    is a great example with wonderful high resolution results that employs a dynamic
    graph neural networks to simulate any system of interacting particles on a much
    larger scale, in terms of both the number of involved particles and the time the
    system is allowed to (numerically) evolve, than what was done before. The particles,
    such as sand or water particles, are the nodes of the graph, with attributes such
    as position, velocity, pressure, external forces, etc., and the edges connect
    the particles that are allowed to interact with each other. The input to the neural
    network is a graph, and the output is a graph with the same nodes and edges but
    with updated attributes of particle positions and properties. The network learns
    the dynamics, or the update rule at each time step, via message passing. The update
    rule depends on the system’s state at the current time step, and on a parametrized
    function whose parameters are optimized for some training objective which depends
    on the specific application, which is the main step in any neural network. The
    prediction targets for supervised learning are the average acceleration for each
    particle.
  prefs: []
  type: TYPE_NORMAL
- en: Bayesian Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Bayesian networks are graphs that are perfectly equipped to deal with uncertainty,
    encoding probabilities in a mathematically sound way. [Figure 9-10](#Fig_Bayes_net_1)
    and [Figure 9-11](#Fig_Bayes_net_2) are examples of two Bayesian networks.
  prefs: []
  type: TYPE_NORMAL
- en: '![300](assets/emai_0910.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-10\. A Bayesian network
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![300](assets/emai_0911.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-11\. Another Bayesian network
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'In a Bayesian network, we have:'
  prefs: []
  type: TYPE_NORMAL
- en: The nodes are variables that we believe our model should include.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The edges are directed, pointing from the *parent* node to the *child* node,
    or from a *higher neuron* to a *lower neuron* in the sense that: We know the probability
    of the child variable conditional on observing the parent variable.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: No cycles are allowed in the graph of the network.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'They rely heavily on Bayes’s rule: If there is an arrow from A to B, then P(B|A)
    is the forward probability and P(A|B) is the inverse probability. Think of this
    as P(evidence|hypothesis) or P(symptoms|disease). We can calculate the inverse
    probability from Bayes rule:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign upper P left-parenthesis upper A vertical-bar upper
    B right-parenthesis equals StartFraction upper P left-parenthesis upper B vertical-bar
    upper A right-parenthesis upper P left-parenthesis upper A right-parenthesis Over
    upper P left-parenthesis upper B right-parenthesis EndFraction period dollar-sign"><mrow><mi>P</mi>
    <mrow><mo>(</mo> <mi>A</mi> <mo>|</mo> <mi>B</mi> <mo>)</mo></mrow> <mo>=</mo>
    <mfrac><mrow><mi>P</mi><mo>(</mo><mi>B</mi><mo>|</mo><mi>A</mi><mo>)</mo><mi>P</mi><mo>(</mo><mi>A</mi><mo>)</mo></mrow>
    <mrow><mi>P</mi><mo>(</mo><mi>B</mi><mo>)</mo></mrow></mfrac> <mo>.</mo></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'If there is no arrow pointing to a variable (if it has no parents), then all
    we need is the *prior* probability of that variable, which we compute from the
    data or from expert knowledge, such as: Thirteen percent of women in the United
    State develop breast cancer.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'If we happen to obtain more data on one of the variables in the model, or more
    *evidence*, then we update the node corresponding to that variable (the conditional
    probability), then propagate that information following the connections in the
    network, updating the conditional probabilities at each node, in two different
    ways, depending on whether the information is propagating from parent to child,
    or from child to parent. The update in each direction is very simple: Comply with
    Bayes’s rule.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A Bayesian network represents is a compactified conditional probability table
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: What a Bayesian network represents is a compactified conditional probability
    table. Usually when we model a real world scenario, each discrete variable can
    assume certain discrete values or categories, and each continuous variable can
    assume any value in a given continuous range. In theory, we can construct a giant
    conditional probability table which gives the probability of each variable assuming
    a certain state given fixed values of the other variables. In reality, even for
    a reasonable small number of variables, this is infeasible and expensive both
    for storage and computations. Moreover, we do not have access to all the information
    required to construct the table. The way Bayesian networks get around this hurdle
    is to allow variables to interact with only few neighbouring variables, so we
    only have to compute the probability of a variable given the states of only those
    directly connected to it in the graph of the network, albeit both forward and
    backward. If new evidence about any variable in the network arrives, then the
    graph structure, together with Bayes’s rule, guides us to update the probabilities
    of all the variables in the network in a systematic, explainable, and transparent
    way. *Sparsifying* the network this way is a feature of Bayesian networks that
    has allows for their success.
  prefs: []
  type: TYPE_NORMAL
- en: 'In summary, a Bayesian network’s graph specifies the joint probability distribution
    of the model’s variables (or the data’s features) as a product of local conditional
    probability distributions, one for each node:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign upper P left-parenthesis x 1 comma x 2 comma ellipsis
    comma x Subscript n Baseline right-parenthesis equals product Underscript i equals
    1 Overscript n Endscripts upper P left-parenthesis x Subscript i Baseline vertical-bar
    parents of x Subscript i Baseline right-parenthesis dollar-sign"><mrow><mi>P</mi>
    <mrow><mo>(</mo> <msub><mi>x</mi> <mn>1</mn></msub> <mo>,</mo> <msub><mi>x</mi>
    <mn>2</mn></msub> <mo>,</mo> <mo>⋯</mo> <mo>,</mo> <msub><mi>x</mi> <mi>n</mi></msub>
    <mo>)</mo></mrow> <mo>=</mo> <msubsup><mo>∏</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>n</mi></msubsup> <mi>P</mi> <mrow><mo>(</mo> <msub><mi>x</mi> <mi>i</mi></msub>
    <mo>|</mo> <mtext>parents</mtext> <mtext>of</mtext> <msub><mi>x</mi> <mi>i</mi></msub>
    <mo>)</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: Making predictions using a Bayesian network
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Once a Bayesian network is set and the conditional probabilities initiated
    (and continuously updated using more data), we can get quick results simply by
    searching these conditional probability distribution tables along Bayes’s rule
    or the product rule, depending on the query such as: What is the probability that
    this email is spam given the words it contains, the sender location, the time
    of the day, the links it includes, the history of interaction between the sender
    and the recepient, and other values of the spam detection variables? What is the
    probability that a patient has breast cancer given her mammogram test result,
    her family history, her symptoms, her blood tests, *etc.*? The best part here
    is that we do not need to consume energy executing large programs or employ large
    clusters of computers to get our results. That is, our phones’ and tablets’ batteries
    will last longer because they don’t have to spend much computational power coding
    and decoding messages, instead, they apply Bayesian networks for using [(*turbo
    code*)](http://authors.library.caltech.edu/6938/1/MCEieeejstc98.pdf) forward error
    correction algorithm.'
  prefs: []
  type: TYPE_NORMAL
- en: Bayesian networks are belief networks, not causal networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In Bayesian networks, though a pointed arrow from parent variable to child
    variable is preferably causal, in general it *is not* causal. All it means is
    that we we can model the probability distribution of a child variable given the
    states of its parent(s), and we can use Bayes rule to find the inverse probability:
    The probability distribution of a parent given the child. This is usually the
    more difficult direction because it is less intuitive and harder to observe. One
    way to think about this is that it is easier to calculate the probability distribution
    of a child’s traits given that we know his parents’ traits, *P(child|mother,father)*,
    even before having the child, than infering the parents’ traits given that we
    know the child’s traits *P(father|child)* and *P(mother|child)*. The Bayesian
    network in this example [Figure 9-12](#Fig_mother_father) has three nodes, mother,
    father, and child, with an edge pointing from the mother to the child, and another
    edge pointing from the father to the child.'
  prefs: []
  type: TYPE_NORMAL
- en: '![300](assets/emai_0912.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-12\. The child variable is a collider in this Bayesian network.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'There is no edge between the mother and the father because there is no reason
    for their traits to be related. Knowing the mother’s traits gives us no information
    about the father’s traits, however, knowing the mother’s traits and the child’s
    traits allows us to know slightly more about the father’s traits, or the distribution
    P(father|mother,child). This means that the mother’s and father’s traits, which
    were originally independent, are conditionally dependent given knowing the child’s
    traits. Thus, a Bayesian network models the dependencies between variables in
    a graph structure, providing a map for how the variables *are believed* to relate
    to each other: Their conditional dependencies and independencies. This is why
    Bayesian networks are also called belief networks.'
  prefs: []
  type: TYPE_NORMAL
- en: Keep this in mind about Bayesian networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s keep the following in mind about Bayesian networks:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Bayesian networks have no causal direction and are limited in answering causal
    questions, or why questions, such as: What caused the onset of a certain disease?
    That said, we will soon learn that we can use a Bayesian network for causal reasoning,
    and to predict the consequences of intervention. Whether used for causal reasoning
    or not, the way we update a Bayesian network, or the way we propagate the *belief*,
    always works in the same way.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If some variables have missing data, Bayesian networks can handle it because
    they are designed in a way that propagates information efficiently from variables
    with abundant information about them to variables with less information.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chains, Forks, and Colliders
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The building blocks of Bayesian networks (with three or more nodes) are three
    types of *junctions*: Chain, fork, and collider, illustrated in [Figure 9-13](#Fig_chain_fork_collider).'
  prefs: []
  type: TYPE_NORMAL
- en: '![300](assets/emai_0913.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-13\. The three types of junctions in a Bayesian network.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '*Chain*: A <math alttext="right-arrow"><mo>→</mo></math> B <math alttext="right-arrow"><mo>→</mo></math>
    C. In this chain, B is a mediator. If we know the value of B, then learning about
    A does not increase or decrease our belief in C. Thus, A and C are conditionally
    independent, given that we know the value of the mediator B. Conditional independence
    allows us, and a machine using a Bayesian network, to focus only on the relevant
    information.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Fork*: B <math alttext="right-arrow"><mo>→</mo></math> A and B <math alttext="right-arrow"><mo>→</mo></math>
    C, that is, B is a common parent or a confounder of A and C. The data will show
    that A and C statistically correlated even though there is no causal relationship
    between them. We can expose this fake correlation by conditioning on the confounder
    B.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Collider*: A <math alttext="right-arrow"><mo>→</mo></math> B and C <math alttext="right-arrow"><mo>→</mo></math>
    B. Colliders are different than chains or forks when we condition on the variable
    in the middle. We saw this in the parents pointing to a child example above. If
    A and C are independent are originally independent, conditioning on B makes them
    dependent! This unexpected and noncausal transfer of information is one characteristic
    of Bayesian networks conditioning: Conditioning on a collider happens to open
    a dependence path between its parents.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Another thing we need to be careful about: When the outcome and mediator are
    confounded. Conditioning on the mediator in this case is different than holding
    it constant.'
  prefs: []
  type: TYPE_NORMAL
- en: Given a data set, how do we set up a Bayesian network for the involved variables?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The graph structure of a Bayesian network can be decided on manually by us,
    or learned from the data by algorithms. Algorithms for Bayesian networks are very
    mature and there are commercial . Once the network’s structure is set in place,
    if a new piece of information about a certain variable in the network arrives,
    it is easy to update the conditional probabilities at each node, by following
    the diagram and propagating the information through the network, updating the
    belief about each variable in the network. The inventor of Bayesian networks,
    [Judea Pearl](https://en.wikipedia.org/wiki/Judea_Pearl) likens this updating
    process to living organic tissue, and to a biological network of neurons, where
    if you excite one neuron, the whole network reacts, propagating the information
    from one neuron to its neighbours.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we can think of neural networks as Bayesian networks.
  prefs: []
  type: TYPE_NORMAL
- en: Models learning patterns from the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It is important to note that Bayesian networks, and any other models learning
    the joint probability distributions of the features of the given data, such as
    the models we encountered in the previous chapter on generative models, and the
    deterministic models in earlier chapters, only detect patterns from the data and
    learn associations, as opposed to learning *what caused* these patterns to begin
    with. For an AI agent to be truly intelligent and reason like humans, they must
    ask the questions *how*, *why*, and *what if* about what they see and what they
    *do*, and they must seek answers, like humans do at a very early age. In fact,
    for humans, there is the age of *why* early in their development: It is the age
    when children drive their parents crazy asking *why* about everything and anything.
    An AI agent should have a *causal model*. This concept is so important for attaining
    general AI. We will visit it in the next section and one more time in [Chapter 11](ch11.xhtml#ch11)
    on Probability.'
  prefs: []
  type: TYPE_NORMAL
- en: Graph Diagrams For Probabilistic Causal Modeling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Since taking our first steps in statistics, all we have heard was: *Correlation
    is not causation*. Then we go on and on about data, more data, and correlations
    in data. Alright then, we get the message, but what about causation? What is it?
    How do we quantify it? As humans we know exactly what *why* means? We conceptualize
    cause and effect intuitively, even as little as eight months olds. I actually
    think we function more on a natural and intuitive level in the world of *why*
    than in the world of association. *Why* then (see?), do our machines, who we expect
    that at some point will be able to reason like us, only function at an association
    and regression level? This is the point that mathematician and philosopher Judea
    Pearl argues for in the field of AI, and in his wonderful book *The Book of Why
    (2020)*. My favorite quote from this book: *Noncausal correlation violates our
    common sense*. The idea is that we need to both articulate and quantify which
    correlations are due to causation and which are due to some other factors.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Pearl builds his mathematical causality models using diagrams (graphs), that
    are similar to Bayesian network graphs, but endowed with a probabilistic reasoning
    scheme based on *the do calculus*, or computing probabilities *given the do* operator
    as opposed to computing probabilities *given the observe* operator, which is very
    familiar from noncausal statistical models. The main point is this:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Observing is not the same as doing*. In math notation, <math alttext="upper
    P r o b left-parenthesis number of bus riders vertical-bar color coded routes
    right-parenthesis"><mrow><mi>P</mi> <mi>r</mi> <mi>o</mi> <mi>b</mi> <mo>(</mo>
    <mtext>number</mtext> <mtext>of</mtext> <mtext>bus</mtext> <mtext>riders</mtext>
    <mo>|</mo> <mtext>color</mtext> <mtext>coded</mtext> <mtext>routes</mtext> <mo>)</mo></mrow></math>
    is not the same as <math alttext="upper P r o b left-parenthesis number of bus
    riders vertical-bar do color coded routes right-parenthesis"><mrow><mi>P</mi>
    <mi>r</mi> <mi>o</mi> <mi>b</mi> <mo>(</mo> <mtext>number</mtext> <mtext>of</mtext>
    <mtext>bus</mtext> <mtext>riders</mtext> <mo>|</mo> <mtext>do</mtext> <mtext>color</mtext>
    <mtext>coded</mtext> <mtext>routes</mtext> <mo>)</mo></mrow></math> .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can infer the first one from data: Look for the ridership numbers given
    that the bus routes in a certain city are color coded. This probability does not
    tell us the *effect* color coded routes has on the number of riders. The second
    probability, *with the do operator*, is different and the data alone, without
    a causal diagram, cannot tell us the answer. The difference is when we invoke
    the do operator, then we are deliberately changing the bus routes to color coded,
    and we want to assess the effect of that change on bus ridership. If the ridership
    increases after this deliberate *doing*, and given that we drew the correct graph
    including the variables and how they talk to each other, then we can assert: Using
    color coded bus routes *caused* the increase in ridership. When we *do* instead
    of *observe*, we manually block all the roads that could naturally lead to colored
    bus routes, such as a change in leadership, or a time of the year, that at the
    same time might affect ridership. We cannot block these roads if we were simply
    observing the data. Moreover, when we use the do operator, we intentionally and
    manually *set the value* of bus routes to color coded (as opposed to numbered,
    *etc.*)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Actually, the bus routes example is not made up. I am currently collaborating
    with the department of public transportation in Harrisonburg, Virginia, with the
    goals of increasing their ridership, improving their efficiency, and optimizing
    their operations given both limited resources and a drastic drop in the city’s
    population when the university is not in session. In 2019, the transportation
    department *deliberately* changed their routes from a number system to a color
    coded system, and at the same time *deliberately* changed their schedules from
    adaptive to the university’s class schedule to fixed schedules. Here is what happened:
    Their ridership increased a whopping 18%. You bet my students who are working
    on the project this summer (2022) will soon be drawing causal diagrams and writing
    probabilities that look like:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="dollar-sign upper P left-parenthesis r i d e r s h i p vertical-bar
    do color coded routes comma do fixed schedules right-parenthesis period dollar-sign"><mrow><mi>P</mi>
    <mo>(</mo> <mi>r</mi> <mi>i</mi> <mi>d</mi> <mi>e</mi> <mi>r</mi> <mi>s</mi> <mi>h</mi>
    <mi>i</mi> <mi>p</mi> <mo>|</mo> <mtext>do</mtext> <mtext>color</mtext> <mtext>coded</mtext>
    <mtext>routes</mtext> <mo>,</mo> <mtext>do</mtext> <mtext>fixed</mtext> <mtext>schedules</mtext>
    <mo>)</mo> <mo>.</mo></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'A machine that is so good at detecting a pattern and acting on it, like a lizard
    observing a bug fly around, learning its pattern, then catching it and eating
    it, has a very different level of *intellect* than a machine that is able to reason
    on two higher levels than mere detection of patterns:'
  prefs: []
  type: TYPE_NORMAL
- en: '*If I deliberately take this action, what will happen to [insert variable here]?*'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*What if I didn’t take this action, would [the variable taking a certain value]
    still have happened?* What if Harrisonburg City did not move to color coded routes
    and to fixed schedules, would the ridership still have increased? What if only
    one of these variables changed instead of both?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The data alone cannot answer these questions. In fact, carefully constructed
    causal diagrams help us tell apart the times when we can use the data alone to
    answer the above questions, and when we cannot answer them *irrespective of how
    much more data we collect*. Until our machines are endowed with graphs representing
    causal reasoning, our mahines have the same level of intellect as lizards. Amazingly,
    humans do all these computations instantaneously, albeit with arriving at wrong
    conclusions many times and arguing with each other about causes and effects for
    decades. We still need math and graphs to settle matters. In fact, the graph guides
    us and tells us which data we must look for and collect, which variables to condition
    on, and which variables to apply the do operator on. This intentional design and
    reasoning is very different than the culture of amassing big volumes of data,
    or aimlessly conditioning on all kinds of variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we know this we can draw diagrams and design models that can help
    us settle all kinds of causal questions: Did I heal because of the doctor’s treatment
    or did I heal because time has passed and life has calmed down? We would still
    need to collect and organize data, but this process will now be intentional and
    guided. A machine endowed with these ways of reasoning: A causal diagram model,
    a long with the (very short) list of valid operations that go along with the causal
    diagram model, will be able to answer queries on all three levels of causation:'
  prefs: []
  type: TYPE_NORMAL
- en: Are variables A and B correlated? Are ridership and labeling of bus routes correlated?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If I set variable A to a specific value, how would variable B change? If I deliberately
    set color coded routes, would ridership increase?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If variable A did not take a certain value, would variable B have changed? If
    I did not change to color coded bus routes, would ridership still have increased?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We still need to learn how to deal with probability expressions that involve
    the *do* operator. We have established that seeing is not the same as doing: *Seeing*
    is in the data, while *doing* is deliberately running an experiment to assess
    the causal effect of a certain variable on another. It more costly than just counting
    proportions seen in the data. Pearl establishes three rules for manipulating probability
    expressions that involve the *do* operator. These help us move from expressions
    with *doing* to others with only *seeing*, where we can get the answers from the
    data. These rules are valuable because they enable us to quantify causal effects
    by *seeing*, by-passing *doing*. We go over these in [Chapter 11](ch11.xhtml#ch11)
    on Probability.'
  prefs: []
  type: TYPE_NORMAL
- en: A Brief History Of Graph Theory
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We cannot leave this chapter without a good overview of graph theory and the
    current state of the field. This area is built on such simple foundations, yet
    it is beautiful, stimulating, and with far reaching applications that it made
    me reassess my whole mathematical career path and try to convert urgently.
  prefs: []
  type: TYPE_NORMAL
- en: 'The vocabulary of graph theory includes: Graphs, nodes, edges, degrees, connectivity,
    trees, spanning trees, circuits, fundamental circuits, vector space of a graph,
    rank and nullity (like in linear algebra), duality, path, walk, Euler line, Hamiltonian
    circuit, cut, network flow, traversing, coloring, enumerating, links, and vulnerability.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The timeline of the development of graph theory is enlightening, with its roots
    in transportation systems, maps and geography, electric circuits, and molecular
    structures in chemistry:'
  prefs: []
  type: TYPE_NORMAL
- en: In 1736, Euler published the first paper in graph theory, solving the [Königsberg
    bridge problem](https://mathworld.wolfram.com/KoenigsbergBridgeProblem.xhtml).
    Then nothing happened in the field for more than a hundred years.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In 1847, Kirchhoff developed the theory of trees while working on electrical
    networks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shortly after, in the 1850’s, Cayley discovered trees as he was trying to enumerate
    the isomers of saturated hydrocarbons <math alttext="upper C Subscript n Baseline
    upper H Subscript 2 n plus 2"><mrow><msub><mi>C</mi> <mi>n</mi></msub> <msub><mi>H</mi>
    <mrow><mn>2</mn><mi>n</mi><mo>+</mo><mn>2</mn></mrow></msub></mrow></math> . Arthur
    Cayley (1821-1895) is one of the founding fathers of graph theory. We find his
    name anywhere where there is graph data. More recently, [CayleyNet](https://arxiv.org/abs/1705.07664)
    uses complex rational functions called Cayley polynomials for a spectral domain
    approach for deep learning on graph data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'During the same time period, in 1850, Sir William Hamilton invented the game
    that became the basis of Hamiltonian circuits, and sold it in Dublin: We have
    a wooden, regular polyhedron with 12 faces and 20 corners, each face is a regular
    pentagon and three edges meeting at each corner. The 20 corners had the names
    of 20 cities, such as London, Rome, New York, Mumbai, Delhi, Paris, and others.
    We have to find a route along the edges of the polyhedron, passing through each
    of the 20 cities exactly once (a Hamiltonian circuit). The solution of this specific
    problem is easy, but until now we have no necessary and sufficient condition for
    the existence of such a route in an arbitrary graph.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Also during the same time period, at a lecture by Möbius (1840’s), a letter
    by De Morgan (1850’s), and in a publication by Cayley in the first volume of the
    Proceedings of the Royal Geographic Society (1879), the most famous problem in
    graph theory (solved in 1970), the four color theorem, came to life. This has
    occupied many mathematicians since then, leading to many interesting discoveries.
    It states that four colors are sufficient for coloring any map on a plane such
    that the countries with common boundaries have different colors. The interesting
    thing is that if we give ourselves more space by moving out of the flat plane,
    for example to the surface of a sphere, then we do have solutions for this conjecture.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unfortunately, nothing happened for another 70 years or so, until the 1920s
    when König wrote the first book on the subject and published it in 1936.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Things changed with the arrival of computers and their increasing ability to
    explore large problems of combinatorial nature. This spurred intense activity
    in both pure and applied graph theory. There are now thousands of papers and dozens
    of books on the subject, with significant contributers such as Claude Berge, Oystein
    Ore, Paul Erdös, William Tutte, and Frank Harary.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Main Considerations In Graph Theory
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let’s organize the main topics in graph theory and aim for a bird’s eye view
    without diving into details:'
  prefs: []
  type: TYPE_NORMAL
- en: Spanning Trees And Shortest Spanning Trees
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: These are of great importance and are used in network routing protocols, shortest
    path algorithms, and search algorithms. A spanning tree of a graph is a subgraph
    that is a tree (any two vertices can be connected using one unique path) including
    all of the vertices of the graph. That is, spanning trees keep the vertices of
    a graph together. The same graph can have many spanning trees.
  prefs: []
  type: TYPE_NORMAL
- en: Cut Sets And Cut Vertices
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can break any connected graph apart, disconnecting it, by cutting through
    enough edges, or sometimes by removing enough vertices. If we are able to find
    these cut sets in a given graph, such as in a communication network, an electrical
    grid, a transportation network, or others, we can cut all communication means
    between its disconnected parts. Usually we are interested in the smallest or minimal
    cut sets, which will accomplish the task of disconnecting the graph by removing
    the least amount of its edges or vertices. This helps us identify the weakest
    links in a network. In contrast to spanning trees, cut sets separate the vertices,
    as opposed to keeping all of them together. Thus, we would rightly expect a close
    relationship between spanning trees and cut sets. Moreover, if the graph represents
    a network with a source of some sort, such as fluid, traffic, electricity, or
    information, and a sink, where each edge allows only a certain amount to flow
    through it, then there is a close relationship between the maximum flow that can
    move from the source to the sink and the cut through the edges of the graph that
    disconnects the source from the sink, with minimal total capacity of the cut edges.
    This is the *max flow min cut theorem*, which states that in a flow network, the
    maximum amount of flow passing from the source to the sink is equal to the total
    weight of the edges in a minimal cut. In mathematics, when a maximization problem
    (max flow) becomes equivalent to a minimization problem (min cut) in a nontrivial
    way (for example by not just flipping the sign of the objective function), it
    signals duality. Indeed, the max flow min cut theorem for graphs is a special
    case of the duality theorem from linear optimization.
  prefs: []
  type: TYPE_NORMAL
- en: Planarity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Is the geometric representation of a graph planar or three dimensional? That
    is, can we draw the vertices of the graph and connect its edges, all in one plane,
    without its edges crossing each other? This is interesting for technological applications
    such as automatic wiring of complex systems, printed circuits, and large-scale
    integrated circuits. For nonplanar graphs, we are interested in properties such
    as the thickness of these graphs and the number of crossings between edges. An
    equivalent condition for a planar graph is the existence of a *dual graph*, where
    the relationship between a graph and its dual becomes clear in the context of
    the vector space of a graph. Linear algebra and graphs come together here, where
    algebraic and combinatoric representations answer questions about geometric figures
    and vice versa. For the planarity question, we only need to consider only simple,
    nonseparable graphs whose vertices all have degree three or more. Moreover, any
    graph with number of edges larger than three times the number of its vertices
    minus six is nonplanar. There are many unsolved problems in this field of study.
  prefs: []
  type: TYPE_NORMAL
- en: Graphs As Vector Spaces
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It is important to understand a graph both as a geometric object and an algebraic
    object, along with the correspondence between the two representations. This is
    the case for graphs. Every graph corresponds to an *e* dimensional vector space
    over the field of integers modulo 2, where *e* is the number of edges of the graph.
    So if the graph only has three edges <math alttext="e d g e 1 comma e d g e 2
    comma e d g e 3"><mrow><mi>e</mi> <mi>d</mi> <mi>g</mi> <msub><mi>e</mi> <mn>1</mn></msub>
    <mo>,</mo> <mi>e</mi> <mi>d</mi> <mi>g</mi> <msub><mi>e</mi> <mn>2</mn></msub>
    <mo>,</mo> <mi>e</mi> <mi>d</mi> <mi>g</mi> <msub><mi>e</mi> <mn>3</mn></msub></mrow></math>
    then it corresponds to the three dimensional vector space containing the vectors
    <math alttext="left-parenthesis 0 comma 0 comma 0 right-parenthesis comma left-parenthesis
    1 comma 0 comma 0 right-parenthesis comma left-parenthesis 0 comma 1 comma 0 right-parenthesis
    comma left-parenthesis 1 comma 1 comma 0 right-parenthesis comma left-parenthesis
    1 comma 0 comma 1 right-parenthesis comma left-parenthesis 0 comma 1 comma 1 right-parenthesis
    comma left-parenthesis 0 comma 0 comma 1 right-parenthesis comma left-parenthesis
    1 comma 1 comma 1 right-parenthesis"><mrow><mo>(</mo> <mn>0</mn> <mo>,</mo> <mn>0</mn>
    <mo>,</mo> <mn>0</mn> <mo>)</mo> <mo>,</mo> <mo>(</mo> <mn>1</mn> <mo>,</mo> <mn>0</mn>
    <mo>,</mo> <mn>0</mn> <mo>)</mo> <mo>,</mo> <mo>(</mo> <mn>0</mn> <mo>,</mo> <mn>1</mn>
    <mo>,</mo> <mn>0</mn> <mo>)</mo> <mo>,</mo> <mo>(</mo> <mn>1</mn> <mo>,</mo> <mn>1</mn>
    <mo>,</mo> <mn>0</mn> <mo>)</mo> <mo>,</mo> <mo>(</mo> <mn>1</mn> <mo>,</mo> <mn>0</mn>
    <mo>,</mo> <mn>1</mn> <mo>)</mo> <mo>,</mo> <mo>(</mo> <mn>0</mn> <mo>,</mo> <mn>1</mn>
    <mo>,</mo> <mn>1</mn> <mo>)</mo> <mo>,</mo> <mo>(</mo> <mn>0</mn> <mo>,</mo> <mn>0</mn>
    <mo>,</mo> <mn>1</mn> <mo>)</mo> <mo>,</mo> <mo>(</mo> <mn>1</mn> <mo>,</mo> <mn>1</mn>
    <mo>,</mo> <mn>1</mn> <mo>)</mo></mrow></math> . Here <math alttext="left-parenthesis
    0 comma 0 comma 0 right-parenthesis"><mrow><mo>(</mo> <mn>0</mn> <mo>,</mo> <mn>0</mn>
    <mo>,</mo> <mn>0</mn> <mo>)</mo></mrow></math> corresponds to the null subgraph
    containing none of the three edges, <math alttext="left-parenthesis 1 comma 1
    comma 1 right-parenthesis"><mrow><mo>(</mo> <mn>1</mn> <mo>,</mo> <mn>1</mn> <mo>,</mo>
    <mn>1</mn> <mo>)</mo></mrow></math> corresponds to the full graph containing all
    three edges, <math alttext="left-parenthesis 0 comma 1 comma 1 right-parenthesis"><mrow><mo>(</mo>
    <mn>0</mn> <mo>,</mo> <mn>1</mn> <mo>,</mo> <mn>1</mn> <mo>)</mo></mrow></math>
    corresponds to the subgraph containing only <math alttext="e d g e 2"><mrow><mi>e</mi>
    <mi>d</mi> <mi>g</mi> <msub><mi>e</mi> <mn>2</mn></msub></mrow></math> , and <math
    alttext="e d g e 3"><mrow><mi>e</mi> <mi>d</mi> <mi>g</mi> <msub><mi>e</mi> <mn>3</mn></msub></mrow></math>
    , and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Field of integers modulo 2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The field of integers modulo 2 only contains the two elements 0 and 1, with
    the operations + and <math alttext="times"><mo>×</mo></math> both happening modulo
    two. These are in fact equivalent to the logical operations *xor* (exclusive or
    operator) and *and* in Boolean logic. A vector space has to be defined over a
    field and has to be closed under multiplication of its vectors by *scalars* from
    that field. In this case, the scalars are only 0 and 1, and the multiplication
    happens modulo 2\. Graphs are therefore nice examples of vector spaces over finite
    fields, which are different than the usual real or complex numbers. The dimension
    of the vector space of a graph is the number of edges *e* of the graph, and the
    total number of vectors in this vector space is 2^(*e*). We can see here how graph
    theory is immediately applicable to switching circuits (with on and off switches),
    digital systems and signals, since all operate in the field of integers modulo
    2.
  prefs: []
  type: TYPE_NORMAL
- en: With this simple correspondence, and backed by the whole field of linear algebra,
    it is natural to try to understand cut sets, circuits, fundamental circuits, spanning
    trees, and other important graph substructures, and relationships among them,
    in the context of vector subspaces, basis, intersections, orthogonality, and dimensions
    of these subspaces.
  prefs: []
  type: TYPE_NORMAL
- en: Realizability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have already used the adjacency matrix and the incidence matrix as matrix
    representations that completely describe a graph. Others matrices decribe important
    features of the graph, such as the circuit matrix, the cut set matrix, and the
    path matrix. Then of course the relevant studies have to do how with how all these
    relate to each other and interact.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another very important topic is that of *realizability*: What conditions must
    a given matrix satisfy so that it is the circuit matrix of some graph?'
  prefs: []
  type: TYPE_NORMAL
- en: Coloring And Matching
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In many situations we are interested in assigning labels, or colors, to the
    nodes, edges of a graph, or even regions in a planar graph. The famous graph coloring
    problem is when the colors we want to assign to each node are such that no neighboring
    vertices get same color, moreover, we want to do this using the minimal amount
    of colors. The smallest number of colors required to color a graph is called its
    *chromatic number*. Related to coloring are topics such as node partitioning,
    covering, and the chromatic polynomial.
  prefs: []
  type: TYPE_NORMAL
- en: A matching is a set of edges so that no two are adjacent. A maximal matching
    is a maximal set of edges where no two are adjacent. Matching in a general graph
    and in a bipartite graph have many applications, such as matching a minimal set
    of classes to satisfy graduation requirements, or matching job assignments to
    employees’ preferences (this ends up being a max flow min cut problem). We can
    use random walk based algorithms to find perfect matchings on large bipartite
    graphs.
  prefs: []
  type: TYPE_NORMAL
- en: Enumeration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Cayley in 1857 was interested in counting the number of isomers of saturated
    hydrocarbon <math alttext="upper C Subscript n Baseline upper H Subscript 2 n
    plus 2"><mrow><msub><mi>C</mi> <mi>n</mi></msub> <msub><mi>H</mi> <mrow><mn>2</mn><mi>n</mi><mo>+</mo><mn>2</mn></mrow></msub></mrow></math>
    , which led him to counting the number of different trees with n nodes, and to
    his contributions to graph theory. There are many types of graphs to be enumerated,
    and many have been their own research papers. Examples include enumerating all
    rooted trees, simple graphs, simple digraphs and others possessing specific properties.
    Enumeration is a huge area in graph theory. One important enumeration technique
    is Pólya’s counting theorem, where one needs to find an appropriate permutation
    group and then obtain its cycle index, which is nontrivial.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithms And Computational Aspects Of Graphs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Algorithms and computer implementations are of tremendous value for anyone
    working with graph modeling. Algorithms exist for traditional graph theoretical
    tasks such as:'
  prefs: []
  type: TYPE_NORMAL
- en: Find out if graph is separable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Find out if a graph is connected.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Find out the components of a graph.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Find the spanning trees of a graph.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Find a set of fundamental circuits.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Find cut sets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Find the shortest path from a given node to another.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Test whether the graph is planar.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Build a graph with specific properties.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Graph neural networks nowadays come with their own open source packages. As
    always for an algorithm to be of any practical use it must efficient: Its running
    time must not increase factorially or even exponentially with the number of nodes
    of the graph. It should be polynomial time, proportional to <math alttext="n Superscript
    k"><msup><mi>n</mi> <mi>k</mi></msup></math> , where *k* is preferably a low number.'
  prefs: []
  type: TYPE_NORMAL
- en: For anyone wishing to enter the field of graph modeling, it is of great use
    to familiarize themselves with both the theory and computational aspects of graphs.
  prefs: []
  type: TYPE_NORMAL
- en: Summary And Looking Ahead
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter was a summary of various aspects of graphical modeling, with emphasis
    on examples, applications, and building intuition. There are many references for
    readers aiming to dive deeper. The main message is not to get lost in the weeds
    (and they are very thick) without an aim or an understanding of the big picture,
    the current state of the field, and how it relates to AI.
  prefs: []
  type: TYPE_NORMAL
- en: We also introduced random walks on graphs, Bayesian networks and probabilistic
    causal models, which shifted our brain even more in the direction of probabilistic
    thinking, the main topic of [Chapter 11](ch11.xhtml#ch11). It was my intention
    all along to go over all kinds of uses for probability in AI before going into
    a math chapter on probability ([Chapter 11](ch11.xhtml#ch11)).
  prefs: []
  type: TYPE_NORMAL
- en: 'We leave this chapter with this very nice read: [Relational inductive biases,
    deep learning, and graph networks](https://arxiv.org/pdf/1806.01261.pdf) which
    makes the case for the deep learning community to adopt graph networks: *We present
    a new building block for the AI toolkit with a strong relational inductive bias—the
    graph network—which generalizes and extends various approaches for neural networks
    that operate on graphs, and provides a straightforward interface for manipulating
    structured knowledge and producing structured behaviors. We discuss how graph
    networks can support relational reasoning and combinatorial generalization, laying
    the foundation for more sophisticated, interpretable, and flexible patterns of
    reasoning.*'
  prefs: []
  type: TYPE_NORMAL
