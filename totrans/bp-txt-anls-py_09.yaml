- en: Chapter 9\. Text Summarization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There is a massive amount of information on the internet on every topic. A Google
    search returns millions of search results containing text, images, videos, and
    so on. Even if we consider only the text content, it’s not possible to read through
    it all. Text summarization methods are able to condense text information to a
    short summary of a few lines or a paragraph and make it digestible to most users.
    Applications of text summarization can be found not just on the internet but also
    in fields like paralegal case summaries, book synopses, etc.
  prefs: []
  type: TYPE_NORMAL
- en: What You’ll Learn and What We’ll Build
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will start with an introduction to text summarization and
    provide an overview of the methods used. We will analyze different types of text
    data and their specific characteristics that are useful in determining the choice
    of summarization method. We will provide blueprints that apply these methods to
    different use cases and analyze their performance. At the end of this chapter,
    you will have a good understanding of different text summarization methods and
    be able to choose the right approach for any application.
  prefs: []
  type: TYPE_NORMAL
- en: Text Summarization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It is likely that you have undertaken a summarization task knowingly or unknowingly
    at some point in life. Examples are telling a friend about a movie you watched
    last night and trying to explain your work to your family. We all like to provide
    a brief summary of our experiences to the rest of the world to share our feelings
    and motivate others. *Text summarization* is defined as the method used for generating
    a concise summary of longer text while still conveying useful information and
    without losing the overall context. This is a method that we are quite familiar
    with: when reading course textbooks, lecture notes, or even this book, many students
    will try to highlight important sentences or make short notes to capture the important
    concepts. Automatic text summarization methods allow us to use computers to do
    this task.'
  prefs: []
  type: TYPE_NORMAL
- en: Summarization methods can be broadly classified into *extraction* and *abstraction*
    methods. In extractive summarization, important phrases or sentences are identified
    in a given body of text and combined to form the summary of the entire text. Such
    methods identify the important parts of text by assigning weights correctly, remove
    sentences that might convey redundant information, rank different parts of the
    text, and combine the most important ones as the summary. These methods select
    a part of the original text as the summary, so while each sentence would be grammatically
    accurate, it may not form a cohesive paragraph.
  prefs: []
  type: TYPE_NORMAL
- en: Abstractive summarization methods, on the other hand, try to paraphrase and
    generate a summary just like a human would. This typically involves the use of
    deep neural networks that are capable of generating phrases and sentences that
    provide a grammatically accurate summary of the text and not just picking out
    important words or sentences. However, the process of training deep neural networks
    requires a lot of training data and addresses multiple subdomains within NLP,
    like natural language generation, semantic segmentation, etc.
  prefs: []
  type: TYPE_NORMAL
- en: Abstractive summarization methods are an area of active research with several
    [approaches](https://oreil.ly/DxXd1) looking to improve the state of the art.
    The [`Transformers` library](https://oreil.ly/JS-x8) from Hugging Face provides
    an implementation that uses a pre-trained model to perform the summarization task.
    We explore the concept of pre-trained models and the Transformers library in more
    detail in [Chapter 11](ch11.xhtml#ch-sentiment). Extractive summarization is preferred
    in many use cases because these methods are simple to implement and fast to run.
    In this chapter, we will focus on blueprints using extractive summarization.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s say you are working with a legal firm that wants to review historical
    cases to help prepare for a current case. Since case proceedings and judgments
    are very long, they want to generate summaries and review the entire case only
    if it’s relevant. Such a summary helps them to quickly look at multiple cases
    and allocate their time efficiently. We can consider this an example of text summarization
    applied to long-form text. Another use case might be a media company that wants
    to send a newsletter to its subscribers every morning highlighting the important
    events of the previous day. Customers don’t appreciate long emails, and therefore
    creating a short summary of each article is important to keep them engaged. In
    this use case, you need to summarize shorter pieces of text. While working on
    these projects, maybe you have to work in a team that uses a chat communication
    tool like Slack or Microsoft Teams. There are shared chat groups (or channels)
    where all team members can communicate with each other. If you are away for a
    few hours in a meeting, it can quickly get flooded with multiple messages and
    discussions. As a user, it’s hard to go through 100+ unread messages, and you
    can’t be sure if you missed something important. In such a situation, it can be
    beneficial to have a way to summarize these missed discussions with the help of
    an automated bot.
  prefs: []
  type: TYPE_NORMAL
- en: 'In each of the use cases, we see a different type of text that we are looking
    to summarize. Let’s briefly present them again:'
  prefs: []
  type: TYPE_NORMAL
- en: Long-form text written in a structured manner, containing paragraphs, and spread
    across multiple pages. Examples include case proceedings, research papers, textbooks,
    etc.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Short-form text such as news articles, and blogs where images, data, and other
    graphical elements might be present.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multiple, short pieces of text in the form of conversations that can contain
    special characters such as emojis and are not very structured. Examples include
    Twitter threads, online discussion forums, and group messaging applications.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each of these types of text data presents information differently, and therefore
    the method used to summarize one may not work for the other. In our blueprints
    we present methods that work for these text types and provide guidance to determine
    the appropriate method.
  prefs: []
  type: TYPE_NORMAL
- en: Extractive Methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'All extractive methods follow these three basic steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Create an intermediate representation of the text.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Score the sentences/phrases based on the chosen representation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Rank and choose sentences to create a summary of the text.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: While most blueprints will follow these steps, the specific method that they
    use to create the intermediate representation or score will vary.
  prefs: []
  type: TYPE_NORMAL
- en: Data Preprocessing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before proceeding to the actual blueprint, we will reuse the blueprint from
    [Chapter 3](ch03.xhtml#ch-scraping) to read a given URL that we would like to
    summarize. In this blueprint we will focus on generating a summary using the text,
    but you can study [Chapter 3](ch03.xhtml#ch-scraping) to get a detailed overview
    of extracting data from a URL. The output of the article has been shortened for
    brevity; to view the entire article, you can follow the URL:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We make use of the `reprlib` package, which allows us to customize the output
    of the print statement. In this case, printing the contents of the full article
    would not make sense. We limit the size of the output to 800 characters, and the
    `reprlib` package reformats the output to show a selected sequence of words from
    the beginning and end of the article.
  prefs: []
  type: TYPE_NORMAL
- en: 'Blueprint: Summarizing Text Using Topic Representation'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let’s first try to summarize the example Reuters article ourselves. Having
    read through it, we could provide the following manually generated summary:'
  prefs: []
  type: TYPE_NORMAL
- en: 5G is the next generation of wireless technology that will rely on denser arrays
    of small antennas to offer data speeds up to 50 or 100 times faster than current
    4G networks. These new networks are supposed to deliver faster data not just to
    phones and computers but to a whole array of sensors in cars, cargo, crop equipment,
    etc. Qualcomm is the dominant player in smartphone communications chips today,
    and the concern is that a takeover by Singapore-based Broadcom could see the firm
    cut research and development spending by Qualcomm or hive off strategically important
    parts of the company to other buyers, including in China. This risked weakening
    Qualcomm, which would boost China over the United States in the 5G race.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: As humans, we understand what the article is conveying and then generate a summary
    of our understanding. However, an algorithm doesn’t have this understanding and
    therefore has to rely on the identification of important topics to determine whether
    a sentence should be included in the summary. In the example article, topics could
    be broad themes like technology, telecommunications, and 5G, but to an algorithm
    this is nothing but a collection of important words. Our first method tries to
    distinguish between important and not-so-important words that allows us to then
    give a higher rank to sentences that contain important words.
  prefs: []
  type: TYPE_NORMAL
- en: Identifying Important Words with TF-IDF Values
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The simplest approach would be to identify important sentences based on an
    aggregate of the TF-IDF values of the words in that sentence. A detailed explanation
    of TF-IDF is provided in [Chapter 5](ch05.xhtml#ch-vectorization), but for this
    blueprint, we apply the TF-IDF vectorization and then aggregate the values to
    a sentence level. We can generate a score for each sentence as a sum of the TF-IDF
    values for each word in that sentence. This would mean that a sentence with a
    high score contains many important words as compared to other sentences in the
    article:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'In this case, there are approximately 20 sentences in the article, and we chose
    to create a condensed summary that is only 10% of the size of the original article
    (approximately two to three sentences). We sum up the TF-IDF values for each sentence
    and use `np.argsort` to sort them. This method sorts the indices of each sentence
    in ascending order, and we reverse the returned indices using `[::-1]`. To ensure
    the same flow of thoughts as presented in the article, we print the chosen summarized
    sentences in the same order in which they appear. We can see the results of our
    generated summary, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: In this method, we create an intermediate representation of the text using TF-IDF
    values, score the sentences based on this, and pick three sentences with the highest
    score. The sentences selected using this method agree with the manual summary
    we wrote earlier and capture the main points covered by the article. Some nuances
    like the importance of Qualcomm in the industry and the specific applications
    of 5G technology are missing. But this method serves as a good blueprint to quickly
    identify important sentences and automatically generate the summary for news articles.
    We wrap this blueprint into a function `tfidf_summary` that is defined in the
    accompanying notebook and reused later in the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: LSA Algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the modern methods used in extractive-based summarization is *latent
    semantic analysis* (LSA). LSA is a general-purpose method that is used for topic
    modeling, document similarity, and other tasks. LSA assumes that words that are
    close in meaning will occur in the same documents. In the LSA algorithm, we first
    represent the entire article in the form of a sentence-term matrix. The concept
    of a document-term matrix has been introduced in [Chapter 8](ch08.xhtml#ch-topicmodels),
    and we can adapt the concept to fit a sentence-term matrix. Each row represents
    a sentence, and each column represents a word. The value of each cell in this
    matrix is the word frequency often scaled as TF-IDF weights. The objective of
    this method is to reduce all the words to a few topics by creating a modified
    representation of the sentence-term matrix. To create the modified representation,
    we apply the method of nonnegative matrix factorization that expresses this matrix
    as the product of two new decomposed matrices that have fewer rows/columns. You
    can refer to [Chapter 8](ch08.xhtml#ch-topicmodels) for a more detailed understanding
    of this method. After the matrix decomposition step, we can generate the summary
    by choosing the top N important topics and then picking the most important sentences
    for each of these topics to form our summary.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of applying LSA from scratch, we make use of the package `sumy`, which
    can be installed using the command `**pip install sumy**`. It provides multiple
    summarization methods within the same library. This library uses an integrated
    stop words list along with the tokenizer and stemmer functionality from NLTK but
    makes this configurable. In addition, it is also able to read input from plain
    text, HTML, and files. This gives us the ability to quickly test different summarization
    methods and change the default configurations to suit specific use cases. For
    now, we will go with the default options, including identifying the top three
    sentences:^([1](ch09.xhtml#idm45634184456856))
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: By analyzing the results, we see that there is a difference in only one sentence
    from the results of the TF-IDF, and that is sentence 2\. While the LSA method
    chose to highlight a sentence that captures the topic about challenges, the TF-IDF
    method chose a sentence that provides more information about 5G. In this scenario,
    the summaries generated by the two methods are not very different, but let’s analyze
    how this method works on a longer article.
  prefs: []
  type: TYPE_NORMAL
- en: 'We wrap this blueprint into a function `lsa_summary`, which is defined in the
    accompanying notebook and can be reused:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Then:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'And finally:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The difference in the chosen summarized sentences becomes more evident here.
    The main topic of the trade war tensions is captured by both methods, but the
    LSA summarizer also highlights important topics such as the apprehensiveness of
    investors and corporate confidence. While the TF-IDF tries to express the same
    idea in its chosen sentences, it does not pick the right sentences and therefore
    fails to convey the idea. There are other topic-based summarization methods, but
    we have chosen to highlight LSA as a simple and widely used method.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: It’s interesting to note that the `sumy` library also provides the implementation
    of one of the oldest methods for automatic text summarization (`LuhnSummarizer`),
    which was created by [Hans Peter Luhn in 1958](https://oreil.ly/j6cQI). This method
    is also based on topic representation by identifying important words using their
    counts and setting thresholds to get rid of extremely frequent and infrequent
    words. You can use this as a baseline method for your summarization experiments
    and compare improvements provided by other methods.
  prefs: []
  type: TYPE_NORMAL
- en: 'Blueprint: Summarizing Text Using an Indicator Representation'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Indicator representation methods aim to create the intermediate representation
    of a sentence by using features of the sentence and its relationship to others
    in the document rather than using only the words in the sentence. [TextRank](https://oreil.ly/yY29h)
    is one of the most popular examples of an indicator-based method. TextRank is
    inspired by PageRank, a “graph-based ranking algorithm that was originally used
    by Google to rank search results. As per the authors of the TextRank paper, graph-based
    algorithms rely on the collective knowledge of web architects rather than individual
    content analysis of web pages,” which leads to improved performance. Applied to
    our context, we will rely on the features of a sentence and the linkages between
    them rather than on topics contained in each sentence.
  prefs: []
  type: TYPE_NORMAL
- en: We will first try to understand how the PageRank algorithm works and then adapt
    the methodology to the text summarization problem. Let’s consider a list of web
    pages—(A, B, C, D, E, and F) and their links to one another. In [Figure 9-1](#fig-pagerank-graph),
    page A contains a link to page D. Page B contains links to A and D and so on.
    We can also represent this in the form of a matrix with rows referring to each
    page and with columns referring to incoming links from other pages. The matrix
    shown in the figure represents our graph with rows representing each node, columns
    referring to incoming links from other nodes, and the value of the cell representing
    the weight of the edge between them. We start with a simple representation (1
    indicates an incoming link, 0 indicates none). We can then normalize these values
    by dividing by the total number of outgoing links for each web page. For example,
    page C has two outgoing links (to pages E and F), and therefore the value of each
    outgoing link is 0.5.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/btap_0901.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-1\. Web page links and corresponding PageRank matrix.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The PageRank for a given page is a weighted sum of the PageRank for all other
    pages that have a link to it. This also means that calculating the PageRank is
    an iterative function where we must start with some assumed values of PageRank
    for each page. If we assume all initial values to be 1 and multiply the matrices
    as shown in [Figure 9-2](#fig-pagerank-results), we arrive at the PageRank for
    each page after one iteration (not taking into consideration the damping factor
    for this illustration).
  prefs: []
  type: TYPE_NORMAL
- en: The [research paper by Brin and Page](https://oreil.ly/WjjFv) showed that after
    repeating this calculation for many iterations the values stabilize, and hence
    we get the PageRank or importance for each page. TextRank adapts the previous
    approach by considering each sentence in the text to be analogous to a page and
    therefore a node in the graph. The weight of the edges between nodes is determined
    by the similarity between sentences, and the authors of TextRank suggest a simple
    approach by counting the number of shared lexical tokens, normalized by the size
    of both sentences. There are other similarity measures such as cosine distance
    and longest common substring that can also be used.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/btap_0902.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-2\. Application of one iteration of PageRank algorithm.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Since the sumy package also provides a TextRank implementation, we will use
    it to generate the summarized sentences for the article on the US recession that
    we saw previously:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: While one of the summarized sentences remains the same, this method has chosen
    to return two other sentences that are probably linked to the main conclusions
    drawn in this article. While these sentences themselves may not seem important,
    the use of a graph-based method resulted in selecting highly linked sentences
    that support the main theme of the article. We wrap this blueprint as a function
    `textrank_summary`, allowing us to reuse it.
  prefs: []
  type: TYPE_NORMAL
- en: 'We also want to see how this method works on the shorter article on 5G technology
    that we looked at previously:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'We see that the results capture the central idea of the Qualcomm acquisition
    but do not contain any mention of 5G as a technology that was selected by the
    LSA method. TextRank generally works better in the case of longer text content
    as it is able to identify the most important sentences using the graph linkages.
    In the case of shorter text content, the graphs are not very large, and therefore
    the wisdom of the network plays a smaller role. Let’s use an example of even longer
    content from Wikipedia to highlight this point further. We will reuse the blueprint
    from [Chapter 2](ch02.xhtml#ch-api) to download the text content of a Wikipedia
    article. In this case, we choose an article that describes a historical event
    or series of events: the Mongol invasion of Europe. And since this is much longer
    text, we choose to summarize about 10 sentences to provide a better summary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Then:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: We illustrate the results as highlighted sentences in the original Wikipedia
    page ([Figure 9-3](#fig-wikipedia-summary)) to show that using the TextRank algorithm
    provides an almost accurate summarization of the article by picking the most important
    sentences from each section of the article. We can compare how this works with
    an LSA method, but we leave this as an exercise to the reader using the previous
    blueprint. Based on our experiences, when we want to summarize a large piece of
    text content, for example, scientific research papers, collection of writings,
    and speeches by world leaders or multiple web pages, then we would choose a graph-based
    method like TextRank.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/btap_0903.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-3\. Wikipedia page with selected summary sentences highlighted.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Measuring the Performance of Text Summarization Methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the blueprints so far, we have seen many methods that produce summaries of
    some given text. Each summary differs from the other in subtle ways, and we have
    to rely on our subjective evaluation. This is certainly a challenge in selecting
    a method that works best for a given use case. In this section, we will introduce
    commonly used accuracy metrics and show how they can be used to empirically select
    the best method for summarization.
  prefs: []
  type: TYPE_NORMAL
- en: We must understand that to automatically evaluate the summary of some given
    text, there must be a reference summary that it can be compared with. Typically,
    this is a summary written by a human and is referred to as the *gold standard*.
    Every automatically generated summary can be compared with the gold standard to
    get an accuracy measure. This also gives us the opportunity to easily compare
    multiple methods and choose the best one. However, we will often run into the
    issue that a human-generated summary may not exist for every use case. In such
    situations, we can choose a proxy measure to be considered as the gold standard.
    An example in the case of a news article would be the headline. While it is written
    by a human, it is a poor proxy as it can be quite short and is not an accurate
    summary but more of a leading statement to draw users. While this may not give
    us the best results, it is still useful to compare the performance of different
    summarization methods.
  prefs: []
  type: TYPE_NORMAL
- en: '*Recall-Oriented* *Understudy for Gisting Evaluation* (ROUGE) is one of the
    most commonly used methods to measure the accuracy of a summary. There are several
    types of ROUGE metrics, but the basic idea is simple. It arrives at the measure
    of accuracy by comparing the number of shared terms between the automatically
    generated summary and the gold standard. ROUGE-N is a metric that measures the
    number of common n-grams (ROUGE-1 compares individual words, ROUGE-2 compares
    bigrams, and so on).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The original [ROUGE paper](https://oreil.ly/Tsncq) compared how many of the
    words that appear in the gold standard also appear in the automatically generated
    summary. This is what we introduced in [Chapter 6](ch06.xhtml#ch-classification)
    as *recall*. So if most of the words present in the gold standard were also present
    in the generated summary, we would achieve a high score. However, this metric
    alone does not tell the whole story. Consider that we generate a verbose summary
    that is long but includes most of the words in the gold standard. This summary
    would have a high score, but it would not be a good summary since it doesn’t provide
    a concise representation. This is why the ROUGE measure has been extended to compare
    the number of shared words to the total number of words in the generated summary
    as well. This indicates the precision: the number of words in the generated summary
    that are actually useful. We can combine these measures to generate the F-score.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see an example of ROUGE for one of our generated summaries. Since we
    do not have a gold standard human-generated summary, we use the headline of the
    article as a proxy for the gold standard. While it is simple to calculate this
    independently, we make use of the Python package called `rouge_scorer` to make
    our life easier. This package implements all the ROUGE measures that we will use
    later, and it can be installed by executing the command `**pip install rouge_scorer**`.
    We make use of a print utility function `print_rouge_score` to present a concise
    view of the scores:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The previous result shows us that the summary generated by TextRank has a high
    recall but low precision. This is an artifact of our gold standard being an extremely
    short headline, which is itself not the best choice but used here for illustration.
    The most important use of our metric is a comparison with another summarization
    method, and in this case, let’s compare this with the LSA-generated summary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The above result shows us that TextRank was the superior method in this case
    because it had a higher precision, while the recall of both methods was the same.
    We can easily extend ROUGE-1 to ROUGE-2, which would compare the number of common
    sequences of two words (bigrams). Another important metric is ROUGE-L, which measures
    the number of common sequences between the reference summary and the generated
    summary by identifying the longest common subsequences. A subsequence of a sentence
    is a new sentence that can be generated from the original sentence with some words
    deleted without changing the relative order of the remaining words. The advantage
    of this metric is that it does not focus on exact sequence matches but in-sequence
    matches that reflect sentence-level word order. Let’s analyze the ROUGE-2 and
    ROUGE-L metrics for the Wikipedia page. Again, we do not have a gold standard,
    and therefore we will use the introductory paragraph as the proxy for our gold
    standard:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Then:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Based on the results, we see that TextRank proves to be more accurate than LSA.
    We can use the same method as shown earlier to see which method works best for
    shorter Wikipedia entries, which we will leave as an exercise for the reader.
    When applying this to your use case, it is important that you choose the right
    summary for comparison. For instance, when working with news articles, instead
    of using the headline, you could look for a summary section contained within the
    article or generate one yourself for a small number of articles. This would allow
    you to have a fair comparison between different methods.
  prefs: []
  type: TYPE_NORMAL
- en: 'Blueprint: Summarizing Text Using Machine Learning'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Many of you might have participated in online discussion forums for topics such
    as travel planning, programming, etc. Users on these platforms communicate in
    the form of threads. Anybody can start a thread, and other members provide their
    responses on this thread. Threads can become long, and the key message might be
    lost. In this blueprint, we will use data extracted from a travel forum used in
    the research paper,^([2](ch09.xhtml#idm45634183488696)) which contains the text
    for all posts in a thread along with the summary for that thread, as shown in
    [Figure 9-4](#fig-thread-illustration).
  prefs: []
  type: TYPE_NORMAL
- en: In this blueprint, we are going to use machine learning to help us automatically
    identify the most important posts across the entire thread that accurately summarize
    it. We will first use the summary by the annotator to create target labels for
    our dataset. We will then generate features that can be useful to determine whether
    a particular post should be in the summary and finally train a model and evaluate
    the accuracy. The task at hand is similar to text classification but performed
    at a post level.
  prefs: []
  type: TYPE_NORMAL
- en: While the forum threads are used to illustrate this blueprint, it can easily
    be used for other use-cases. For example, consider the [CNN and Daily Mail news
    summarization task](https://oreil.ly/T_RNc), [DUC](https://oreil.ly/0Hlov), or
    [SUMMAC](https://oreil.ly/Wg322) datasets. In each of these datasets, you will
    find the text of each article and the highlighted summary sentences. These are
    analogous to the text of each thread and the summary as presented in this blueprint.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/btap_0904.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9-4\. Posts in a thread and the corresponding summary from a travel forum.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Step 1: Creating Target Labels'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The first step is to load the dataset, understand its structure, and create
    target labels using the provided summary. We have performed the initial data preparation
    steps to create a well-formatted `DataFrame`, shown next. Please refer to the
    `Data_Preparation` notebook in the GitHub repo of the book for a detailed look
    at the steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '|   | 170 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Date | 29 September 2009, 1:41 |'
  prefs: []
  type: TYPE_TB
- en: '| Filename | thread41_system20 |'
  prefs: []
  type: TYPE_TB
- en: '| ThreadID | 60763_5_3122150 |'
  prefs: []
  type: TYPE_TB
- en: '| Title | which attractions need to be pre booked? |'
  prefs: []
  type: TYPE_TB
- en: '| postNum | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| text | Hi I am coming to NY in Oct! So excited&quot; Have wanted to visit
    for years. We are planning on doing all the usual stuff so wont list it all but
    wondered which attractions should be pre booked and which can you just turn up
    at> I am plannin on booking ESB but what else? thanks x |'
  prefs: []
  type: TYPE_TB
- en: '| userID | musicqueenLon... |'
  prefs: []
  type: TYPE_TB
- en: '| summary | A woman was planning to travel NYC in October and needed some suggestions
    about attractions in the NYC. She was planning on booking ESB.Someone suggested
    that the TOTR was much better compared to ESB. The other suggestion was to prebook
    the show to avoid wasting time in line.Someone also suggested her New York Party
    Shuttle tours. |'
  prefs: []
  type: TYPE_TB
- en: Each row in this dataset refers to a post in a thread. Each thread is identified
    by a unique `ThreadID`, and it’s possible that multiple rows in the `DataFrame`
    have the same `ThreadID`. The column `Title` refers to the name with which the
    user started the thread. The content of each post is in the `text` column, along
    with additional details like the name of the user who created the post (`userID`),
    the time when the post was created (`Date`), and its position in the thread (`postNum`).
    For this dataset, human-generated summaries for each thread are provided in the
    `summary` column.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will reuse the regular expression cleaning and spaCy pipeline blueprints
    from [Chapter 4](ch04.xhtml#ch-preparation) to remove special formatting, URLs,
    and other punctuation from the posts. We will also generate the lemmatized representation
    of the text, which we will use for prediction. You can find the function definitions
    in the accompanying notebook for this chapter. Since we are making use of the
    spaCy lemmatization function, it might take a couple of minutes to complete execution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Each observation in our dataset contains a post that is part of a thread. If
    we were to apply a train-test split at this level, it is possible that two posts
    belonging to the same thread would end up in the train and test datasets, which
    would lead to inaccurate training. As a result, we use `GroupShuffleSplit` to
    group all posts into their respective threads and then randomly select 80% of
    the threads to create the training dataset, with the rest of the threads forming
    part of the test dataset. This function ensures that posts belonging to the same
    thread are part of the same dataset. The `GroupShuffleSplit` function does not
    actually split the data but provides a set of indices that split the data identified
    by `train_split` and `test_split`. We use these indices to create the two datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Our next step is to determine the target label for each of our posts. The target
    label defines whether a particular post should be included in the summary. We
    determine this by comparing each post to the annotator summary and picking posts
    that are most similar to be included in the summary. There are several metrics
    that can be used to determine the similarity of two sentences, but in our use
    case we are working with short texts and therefore choose the [Jaro-Winkler distance](https://oreil.ly/b5q0B).
    We use the `textdistance` package that also provides implementations of other
    distance metrics. This can be easily installed using the command `**pip install
    textdistance**`. You can also easily modify the blueprint and choose a metric
    based on your use case.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following step, we determine the similarity and rank all the posts within
    a thread based on the chosen metric. We then create our target label named `summaryPost`
    that contains a True or False value indicating whether this post is part of the
    summary. This is based on the rank of the post and the compression factor. We
    choose a compression factor of 30%, which means that we pick the top 30% of all
    posts ordered by their similarity to be included in the summary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '|   | text | summaryPost |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 170 | Hi I am coming to NY in Oct! So excited” Have wanted to visit for years.
    We are planning on doing all the usual stuff so wont list it all but wondered
    which attractions should be pre booked and which can you just turn up at> I am
    plannin on booking ESB but what else? thanks x | True |'
  prefs: []
  type: TYPE_TB
- en: '| 171 | I wouldnt bother doing the ESB if I was you TOTR is much better. What
    other attractions do you have in mind? | False |'
  prefs: []
  type: TYPE_TB
- en: '| 172 | The Statue of Liberty, if you plan on going to the statue itself or
    to Ellis Island (as opposed to taking a boat past): http://www.statuecruises.com/
    Also, we prefer to book shows and plays in advance rather than trying for the
    same-day tickets, as that allows us to avoid wasting time in line. If that sounds
    appealing to you, have a look at http://www.broadwaybox.com/ | True |'
  prefs: []
  type: TYPE_TB
- en: 'As you can see in the previous results for a given thread, the first and third
    posts are tagged as `summaryPost`, but the second post is not considered important
    and would not be included in the summary. Because of the way we defined our target
    label, it is possible in rare situations that very short posts are also included
    in the summary. This might happen when a short post contains the same words as
    the thread title. This is not useful to the summary, and we correct this by setting
    all posts containing 20 words or less to not be included in the summary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Step 2: Adding Features to Assist Model Prediction'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Since we are dealing with forum threads in this blueprint, there are some additional
    features that we can generate to help our model in the prediction. The title of
    the thread conveys the topic succinctly and can be helpful in identifying which
    post should actually be selected in the summary. We cannot directly include the
    title as a feature since it would be the same for each post in the thread, but
    instead we calculate the similarity between the post and the title as one of the
    features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Another useful feature could be the length of the post. Short posts could be
    asking clarifying questions and would not capture the most useful knowledge of
    the thread. Longer posts could indicate that a lot of useful information is being
    shared. The position of where the post appears in the thread could also be a useful
    indicator of whether it should be in the summary. This might vary depending on
    the way in which the forum threads are organized. In the case of the travel forum,
    the posts are chronologically ordered, and the occurrence of the post is given
    by the column `postNum`, which we can readily use as a feature:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'As a final step, let’s create the vectorized representation of the lemmas that
    we extracted earlier using the *TfidfVectorizer*. We then create a new `DataFrame`,
    `train_df_tf`, which contains the vectorized lemmas and the additional features
    that we created earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: This step of adding features can be extended or customized depending on the
    use case. For example, if we are looking to summarize longer text, then the paragraph
    that a sentence belongs to will be important. Normally, each paragraph or section
    tries to capture an idea, and sentence similarity metrics at that level would
    be relevant. If we are looking at generating summaries of scientific papers, then
    the number of citations and the sentences used for those citations have proven
    to be useful. We must also repeat the same feature engineering steps on the test
    dataset, which we show in the accompanying notebook but exclude here.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 3: Build a Machine Learning Model'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that we’ve generated features, we will reuse the text classification blueprint
    from [Chapter 6](ch06.xhtml#ch-classification) but use a `RandomForestClassifier`
    model instead of the SVM model. While building a machine learning model for summarization,
    we might have additional features other than the vectorized text representation.
    Particularly in situations where a combination of numeric and categorical features
    are present, a tree-based classifier might perform better:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s apply this model on the test threads and predict the summary posts. To
    determine the accuracy, we concatenate all identified summary posts and generate
    the ROUGE-1 score by comparing it with the annotator summary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: We see that the mean ROUGE-1 score for all threads in the test set is 0.34,
    which is comparable with extractive summarization scores on other [public summarization
    tasks](https://oreil.ly/SaCk2). You will also notice on the leaderboard that the
    use of pretrained models such as BERT improves the score, and we explore this
    technique in detail in [Chapter 11](ch11.xhtml#ch-sentiment).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s also take a look at one of the summarized results produced by this model
    to understand how useful it might be:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: '|   | postNum | text |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 551 | 4 | Well, you’re really in luck, because there’s a lot going on, including
    the Elmwood Avenue Festival of the Arts (http://www.elmwoodartfest.org), with
    special activities for youngsters, performances (including one by Nikki Hicks,
    one of my favorite local vocalists), and food of all kinds. Elmwood Avenue is
    one of the area’s most colorful and thriving neighborhoods, and very walkable.
    The Buffalo Irish Festival is also going on that weekend in Hamburg, as it happens,
    at the fairgrounds: www.buf... |'
  prefs: []
  type: TYPE_TB
- en: '| 552 | 5 | Depending on your time frame, a quick trip to Niagara Falls would
    be great. It is a 45 minute drive from Hamburg and well worth the investment of
    time. Otherwise you have some beaches in Angola to enjoy. If the girls like to
    shop you have the Galleria, which is a great expansive Mall. If you enjoy a more
    eclectic afternoon, lunch on Elmwood Avenue, a stroll through the Albright Know
    Art gallery, and hitting some of the hip shops would be a cool afternoon. Darien
    Lake Theme Park is 40 minutes... |'
  prefs: []
  type: TYPE_TB
- en: In the previous example, the original thread consisted of nine posts, two of
    which have been picked to summarize the thread, as shown earlier. Reading through
    the summary posts shows that the thread is about activities for youngsters, and
    there are already some specific suggestions, such as Elmwood Avenue, Darien Lake
    Theme Park, etc. Imagine that while scrolling through the forum search results,
    this information is provided on a mouse hover. It gives the user an accurate enough
    summary to decide whether it’s interesting and click through for more details
    or continue looking at other search results. You could also easily reuse this
    blueprint with other datasets as mentioned at the start and customize the distance
    function, introduce additional features, and then train the model.
  prefs: []
  type: TYPE_NORMAL
- en: Closing Remarks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we introduced the concept of text summarization and provided
    blueprints that can be used to generate summaries for different use cases. If
    you are looking to generate summaries from short text such as web pages, blogs,
    and news articles, then the first blueprint based on topic representation using
    the LSA summarizer would be a good choice. If you are working with much larger
    text such as speeches, book chapters, or scientific articles, then the blueprint
    using TextRank would be a better choice. These blueprints are great as the first
    step in your journey toward automatic text summarization as they are simple and
    fast. However, the third blueprint using machine learning provides a more custom
    solution for your specific use case. Provided you have the necessary annotated
    data, this method can be tailored by adding features and optimizing the machine
    learning model to improve performance. For example, your company or product might
    have multiple policy documents that govern user data, terms and conditions, and
    other such processes that you want to summarize for a new user or employee. You
    could start with the third blueprint and customize the second step by adding features
    such as the number of clauses, usage of block letters, presence of bold or underlined
    text, etc., that will help the model summarize the important points in the policy
    documents.
  prefs: []
  type: TYPE_NORMAL
- en: Further Reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Allahyari, Mehdi, et al. “Text Summarization Techniques: A Brief Survey.” [*https://arxiv.org/pdf/1707.02268.pdf*](https://arxiv.org/pdf/1707.02268.pdf).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bhatia, Sumit, et al. “Summarizing Online Forum Discussions—Can Dialog Acts
    of Individual Messages Help?” [*http://sumitbhatia.net/papers/emnlp14.pdf*](http://sumitbhatia.net/papers/emnlp14.pdf).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Collins, Ed, et al. “A Supervised Approach to Extractive Summarisation of Scientific
    Papers.” [*https://arxiv.org/pdf/1706.03946.pdf*](https://arxiv.org/pdf/1706.03946.pdf).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tarnpradab, Sansiri, et al. “Toward Extractive Summarization of Online ForumDiscussions
    via Hierarchical Attention Networks.” [*https://aaai.org/ocs/index.php/FLAIRS/FLAIRS17/paper/viewFile/15500/14945*](https://aaai.org/ocs/index.php/FLAIRS/FLAIRS17/paper/viewFile/15500/14945).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ^([1](ch09.xhtml#idm45634184456856-marker)) You can find more information about
    the package, including the usage guidelines that we used while designing this
    blueprint on [GitHub](https://oreil.ly/I0FMA).
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch09.xhtml#idm45634183488696-marker)) Sansiri Tarnpradab, et al. “Toward
    Extractive Summarization of Online ForumDiscussions via Hierarchical Attention
    Networks.” [*https://arxiv.org/abs/1805.10390*](https://arxiv.org/abs/1805.10390).
    See the [data set (*.zip*)](https://oreil.ly/cqU_O) as well.
  prefs: []
  type: TYPE_NORMAL
