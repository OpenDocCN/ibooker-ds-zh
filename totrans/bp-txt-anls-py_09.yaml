- en: Chapter 9\. Text Summarization
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第9章。文本摘要
- en: There is a massive amount of information on the internet on every topic. A Google
    search returns millions of search results containing text, images, videos, and
    so on. Even if we consider only the text content, it’s not possible to read through
    it all. Text summarization methods are able to condense text information to a
    short summary of a few lines or a paragraph and make it digestible to most users.
    Applications of text summarization can be found not just on the internet but also
    in fields like paralegal case summaries, book synopses, etc.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 互联网上有大量关于每个主题的信息。Google 搜索返回数百万条搜索结果，其中包含文本、图像、视频等内容。即使我们只考虑文本内容，也不可能全部阅读。文本摘要方法能够将文本信息压缩成几行或一个段落的简短摘要，并使大多数用户能够理解。文本摘要的应用不仅限于互联网，还包括类似法律助理案例摘要、书籍梗概等领域。
- en: What You’ll Learn and What We’ll Build
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 您将学到什么以及我们将要构建的内容
- en: In this chapter, we will start with an introduction to text summarization and
    provide an overview of the methods used. We will analyze different types of text
    data and their specific characteristics that are useful in determining the choice
    of summarization method. We will provide blueprints that apply these methods to
    different use cases and analyze their performance. At the end of this chapter,
    you will have a good understanding of different text summarization methods and
    be able to choose the right approach for any application.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将从文本摘要的介绍开始，并概述所使用的方法。我们将分析不同类型的文本数据及其特定特征，这些特征对于确定摘要方法的选择非常有用。我们将提供适用于不同用例的蓝图，并分析它们的性能。在本章末尾，您将对不同的文本摘要方法有很好的理解，并能够为任何应用选择合适的方法。
- en: Text Summarization
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本摘要
- en: 'It is likely that you have undertaken a summarization task knowingly or unknowingly
    at some point in life. Examples are telling a friend about a movie you watched
    last night and trying to explain your work to your family. We all like to provide
    a brief summary of our experiences to the rest of the world to share our feelings
    and motivate others. *Text summarization* is defined as the method used for generating
    a concise summary of longer text while still conveying useful information and
    without losing the overall context. This is a method that we are quite familiar
    with: when reading course textbooks, lecture notes, or even this book, many students
    will try to highlight important sentences or make short notes to capture the important
    concepts. Automatic text summarization methods allow us to use computers to do
    this task.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 很可能在生活中的某个时刻，您有意或无意地进行了摘要任务。例如，告诉朋友您昨晚观看的电影，或尝试向家人解释您的工作。我们都喜欢向世界其他地方提供我们经历的简要总结，以分享我们的感受并激励他人。*文本摘要*被定义为在保留有用信息的同时生成更简洁的长文本摘要的方法，而不会失去整体背景。这是一种我们非常熟悉的方法：在阅读课程教材、讲义笔记甚至本书时，许多学生会尝试突出重要的句子或做简短的笔记来捕捉重要概念。自动文本摘要方法允许我们使用计算机来完成这项任务。
- en: Summarization methods can be broadly classified into *extraction* and *abstraction*
    methods. In extractive summarization, important phrases or sentences are identified
    in a given body of text and combined to form the summary of the entire text. Such
    methods identify the important parts of text by assigning weights correctly, remove
    sentences that might convey redundant information, rank different parts of the
    text, and combine the most important ones as the summary. These methods select
    a part of the original text as the summary, so while each sentence would be grammatically
    accurate, it may not form a cohesive paragraph.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 摘要方法可以大致分为*抽取*和*生成*方法。在抽取式摘要中，会从给定的文本中识别重要短语或句子，并将它们组合成整个文本的摘要。这些方法通过正确分配权重来识别文本的重要部分，删除可能传达冗余信息的句子，对文本的不同部分进行排名，并将最重要的部分组合成摘要。这些方法选择原始文本的一部分作为摘要，因此每个句子在语法上都是准确的，但可能不会形成连贯的段落。
- en: Abstractive summarization methods, on the other hand, try to paraphrase and
    generate a summary just like a human would. This typically involves the use of
    deep neural networks that are capable of generating phrases and sentences that
    provide a grammatically accurate summary of the text and not just picking out
    important words or sentences. However, the process of training deep neural networks
    requires a lot of training data and addresses multiple subdomains within NLP,
    like natural language generation, semantic segmentation, etc.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，抽象总结方法尝试像人类一样转述并生成摘要。这通常涉及使用能够生成提供文本语法正确摘要的短语和句子的深度神经网络。然而，训练深度神经网络的过程需要大量的训练数据，并且涉及NLP的多个子领域，如自然语言生成、语义分割等。
- en: Abstractive summarization methods are an area of active research with several
    [approaches](https://oreil.ly/DxXd1) looking to improve the state of the art.
    The [`Transformers` library](https://oreil.ly/JS-x8) from Hugging Face provides
    an implementation that uses a pre-trained model to perform the summarization task.
    We explore the concept of pre-trained models and the Transformers library in more
    detail in [Chapter 11](ch11.xhtml#ch-sentiment). Extractive summarization is preferred
    in many use cases because these methods are simple to implement and fast to run.
    In this chapter, we will focus on blueprints using extractive summarization.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 抽象总结方法是一个活跃研究领域，有几种[方法](https://oreil.ly/DxXd1)致力于改进现有技术。Hugging Face的[`Transformers`库](https://oreil.ly/JS-x8)提供了一个使用预训练模型执行总结任务的实现。我们将在[第11章](ch11.xhtml#ch-sentiment)详细探讨预训练模型和Transformers库的概念。在许多用例中，萃取式总结更受青睐，因为这些方法实现简单且运行速度快。在本章中，我们将专注于使用萃取式总结的蓝图。
- en: Let’s say you are working with a legal firm that wants to review historical
    cases to help prepare for a current case. Since case proceedings and judgments
    are very long, they want to generate summaries and review the entire case only
    if it’s relevant. Such a summary helps them to quickly look at multiple cases
    and allocate their time efficiently. We can consider this an example of text summarization
    applied to long-form text. Another use case might be a media company that wants
    to send a newsletter to its subscribers every morning highlighting the important
    events of the previous day. Customers don’t appreciate long emails, and therefore
    creating a short summary of each article is important to keep them engaged. In
    this use case, you need to summarize shorter pieces of text. While working on
    these projects, maybe you have to work in a team that uses a chat communication
    tool like Slack or Microsoft Teams. There are shared chat groups (or channels)
    where all team members can communicate with each other. If you are away for a
    few hours in a meeting, it can quickly get flooded with multiple messages and
    discussions. As a user, it’s hard to go through 100+ unread messages, and you
    can’t be sure if you missed something important. In such a situation, it can be
    beneficial to have a way to summarize these missed discussions with the help of
    an automated bot.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您在一家法律公司工作，希望查看历史案例以帮助准备当前案例。由于案件程序和判决非常长，他们希望生成摘要，并仅在相关时查看整个案例。这样的摘要帮助他们快速查看多个案例，并有效分配时间。我们可以将此视为应用于长篇文本的文本总结示例。另一个用例可能是媒体公司每天早晨向订阅者发送新闻简报，重点突出前一天的重要事件。客户不喜欢长邮件，因此创建每篇文章的简短摘要对保持他们的参与至关重要。在这种用例中，您需要总结较短的文本。在处理这些项目时，也许您需要在使用Slack或Microsoft
    Teams等聊天沟通工具的团队中工作。有共享的聊天组（或频道），所有团队成员可以彼此交流。如果您在会议中离开几个小时，聊天信息可能会迅速积累，导致大量未读消息和讨论。作为用户，浏览100多条未读消息很困难，并且无法确定是否错过了重要内容。在这种情况下，通过自动化机器人总结这些错过的讨论可能会有所帮助。
- en: 'In each of the use cases, we see a different type of text that we are looking
    to summarize. Let’s briefly present them again:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个用例中，我们看到不同类型的文本需要总结。让我们简要再次呈现它们：
- en: Long-form text written in a structured manner, containing paragraphs, and spread
    across multiple pages. Examples include case proceedings, research papers, textbooks,
    etc.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结构化撰写的长篇文本，包含段落并分布在多页之间。例如案件程序、研究论文、教科书等。
- en: Short-form text such as news articles, and blogs where images, data, and other
    graphical elements might be present.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 短文本，如新闻文章和博客，其中可能包含图像、数据和其他图形元素。
- en: Multiple, short pieces of text in the form of conversations that can contain
    special characters such as emojis and are not very structured. Examples include
    Twitter threads, online discussion forums, and group messaging applications.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多个短文本片段采用对话形式，可以包含表情符号等特殊字符，结构不是很严谨。例如Twitter的线索、在线讨论论坛和群组消息应用程序。
- en: Each of these types of text data presents information differently, and therefore
    the method used to summarize one may not work for the other. In our blueprints
    we present methods that work for these text types and provide guidance to determine
    the appropriate method.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这些类型的文本数据每种呈现信息方式不同，因此用于一个类型的摘要方法可能不适用于另一种。在我们的蓝图中，我们提出适用于这些文本类型的方法，并提供指导以确定适当的方法。
- en: Extractive Methods
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 抽取方法
- en: 'All extractive methods follow these three basic steps:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 所有抽取方法都遵循这三个基本步骤：
- en: Create an intermediate representation of the text.
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建文本的中间表示。
- en: Score the sentences/phrases based on the chosen representation.
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于选择的表示对句子/短语进行评分。
- en: Rank and choose sentences to create a summary of the text.
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对句子进行排名和选择，以创建文本摘要。
- en: While most blueprints will follow these steps, the specific method that they
    use to create the intermediate representation or score will vary.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然大多数蓝图会按照这些步骤进行，但它们用来创建中间表示或分数的具体方法会有所不同。
- en: Data Preprocessing
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据预处理
- en: 'Before proceeding to the actual blueprint, we will reuse the blueprint from
    [Chapter 3](ch03.xhtml#ch-scraping) to read a given URL that we would like to
    summarize. In this blueprint we will focus on generating a summary using the text,
    but you can study [Chapter 3](ch03.xhtml#ch-scraping) to get a detailed overview
    of extracting data from a URL. The output of the article has been shortened for
    brevity; to view the entire article, you can follow the URL:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续实际蓝图之前，我们将重复使用[第3章](ch03.xhtml#ch-scraping)中的蓝图来读取我们想要总结的给定URL。在这份蓝图中，我们将专注于使用文本生成摘要，但您可以研究[第3章](ch03.xhtml#ch-scraping)以获取从URL提取数据的详细概述。为了简洁起见，文章的输出已经缩短；要查看整篇文章，您可以访问以下URL：
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '`Out:`'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出：`'
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Note
  id: totrans-26
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: We make use of the `reprlib` package, which allows us to customize the output
    of the print statement. In this case, printing the contents of the full article
    would not make sense. We limit the size of the output to 800 characters, and the
    `reprlib` package reformats the output to show a selected sequence of words from
    the beginning and end of the article.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`reprlib`包，该包允许我们自定义打印语句的输出。在这种情况下，打印完整文章的内容是没有意义的。我们限制输出的大小为800个字符，`reprlib`包重新格式化输出，显示文章开头和结尾的选定序列词语。
- en: 'Blueprint: Summarizing Text Using Topic Representation'
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 蓝图：使用主题表示进行文本摘要
- en: 'Let’s first try to summarize the example Reuters article ourselves. Having
    read through it, we could provide the following manually generated summary:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先尝试自己总结一下例子Reuters文章。阅读完之后，我们可以提供以下手动生成的摘要：
- en: 5G is the next generation of wireless technology that will rely on denser arrays
    of small antennas to offer data speeds up to 50 or 100 times faster than current
    4G networks. These new networks are supposed to deliver faster data not just to
    phones and computers but to a whole array of sensors in cars, cargo, crop equipment,
    etc. Qualcomm is the dominant player in smartphone communications chips today,
    and the concern is that a takeover by Singapore-based Broadcom could see the firm
    cut research and development spending by Qualcomm or hive off strategically important
    parts of the company to other buyers, including in China. This risked weakening
    Qualcomm, which would boost China over the United States in the 5G race.
  id: totrans-30
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 5G是下一代无线技术，将依赖更密集的小天线阵列，提供比当前4G网络快50到100倍的数据速度。这些新网络预计不仅将数据传输速度提高到手机和电脑，还将扩展到汽车、货物、农作物设备等各种传感器。高通是今天智能手机通信芯片市场的主导者，人们担心新加坡的博通公司收购高通可能会导致高通削减研发支出或将公司战略重要部分出售给其他买家，包括在中国的买家。这可能会削弱高通，在5G竞赛中促进中国超越美国的风险。
- en: As humans, we understand what the article is conveying and then generate a summary
    of our understanding. However, an algorithm doesn’t have this understanding and
    therefore has to rely on the identification of important topics to determine whether
    a sentence should be included in the summary. In the example article, topics could
    be broad themes like technology, telecommunications, and 5G, but to an algorithm
    this is nothing but a collection of important words. Our first method tries to
    distinguish between important and not-so-important words that allows us to then
    give a higher rank to sentences that contain important words.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 作为人类，我们理解文章传达的内容，然后生成我们理解的摘要。然而，算法没有这种理解，因此必须依赖于识别重要主题来确定是否应将句子包括在摘要中。在示例文章中，主题可能是技术、电信和5G等广泛主题，但对于算法来说，这只是一组重要单词的集合。我们的第一种方法试图区分重要和不那么重要的单词，从而使我们能够将包含重要单词的句子排名较高。
- en: Identifying Important Words with TF-IDF Values
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用TF-IDF值识别重要词语
- en: 'The simplest approach would be to identify important sentences based on an
    aggregate of the TF-IDF values of the words in that sentence. A detailed explanation
    of TF-IDF is provided in [Chapter 5](ch05.xhtml#ch-vectorization), but for this
    blueprint, we apply the TF-IDF vectorization and then aggregate the values to
    a sentence level. We can generate a score for each sentence as a sum of the TF-IDF
    values for each word in that sentence. This would mean that a sentence with a
    high score contains many important words as compared to other sentences in the
    article:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的方法是基于句子中单词的TF-IDF值的总和来识别重要句子。详细解释TF-IDF在[第5章](ch05.xhtml#ch-vectorization)中提供，但对于这个蓝图，我们应用TF-IDF向量化，然后将值聚合到句子级别。我们可以为每个句子生成一个分数，作为该句子中每个单词的TF-IDF值的总和。这意味着得分高的句子包含的重要单词比文章中的其他句子多：
- en: '[PRE2]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'In this case, there are approximately 20 sentences in the article, and we chose
    to create a condensed summary that is only 10% of the size of the original article
    (approximately two to three sentences). We sum up the TF-IDF values for each sentence
    and use `np.argsort` to sort them. This method sorts the indices of each sentence
    in ascending order, and we reverse the returned indices using `[::-1]`. To ensure
    the same flow of thoughts as presented in the article, we print the chosen summarized
    sentences in the same order in which they appear. We can see the results of our
    generated summary, as shown here:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，文章中大约有20句话，我们选择创建一个只有原始文章大小的10%的简要总结（大约两到三句话）。我们对每个句子的TF-IDF值进行求和，并使用`np.argsort`对它们进行排序。这种方法按升序对每个句子的索引进行排序，我们使用`[::-1]`来逆转返回的索引。为了确保与文章中呈现的思路相同，我们按照它们出现的顺序打印所选的摘要句子。我们可以看到我们生成的摘要结果，如下所示：
- en: '[PRE3]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '`Out:`'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出：`'
- en: '[PRE4]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: In this method, we create an intermediate representation of the text using TF-IDF
    values, score the sentences based on this, and pick three sentences with the highest
    score. The sentences selected using this method agree with the manual summary
    we wrote earlier and capture the main points covered by the article. Some nuances
    like the importance of Qualcomm in the industry and the specific applications
    of 5G technology are missing. But this method serves as a good blueprint to quickly
    identify important sentences and automatically generate the summary for news articles.
    We wrap this blueprint into a function `tfidf_summary` that is defined in the
    accompanying notebook and reused later in the chapter.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种方法中，我们使用TF-IDF值创建文本的中间表示，根据这些值对句子进行评分，并选择三个得分最高的句子。使用这种方法选择的句子与我们之前写的手动摘要一致，并捕捉了文章涵盖的主要要点。一些细微差别，比如Qualcomm在行业中的重要性和5G技术的具体应用，被忽略了。但这种方法作为快速识别重要句子并自动生成新闻文章摘要的良好蓝图。我们将这个蓝图封装成一个名为`tfidf_summary`的函数，该函数在附带的笔记本中定义并在本章后面再次使用。
- en: LSA Algorithm
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LSA算法
- en: One of the modern methods used in extractive-based summarization is *latent
    semantic analysis* (LSA). LSA is a general-purpose method that is used for topic
    modeling, document similarity, and other tasks. LSA assumes that words that are
    close in meaning will occur in the same documents. In the LSA algorithm, we first
    represent the entire article in the form of a sentence-term matrix. The concept
    of a document-term matrix has been introduced in [Chapter 8](ch08.xhtml#ch-topicmodels),
    and we can adapt the concept to fit a sentence-term matrix. Each row represents
    a sentence, and each column represents a word. The value of each cell in this
    matrix is the word frequency often scaled as TF-IDF weights. The objective of
    this method is to reduce all the words to a few topics by creating a modified
    representation of the sentence-term matrix. To create the modified representation,
    we apply the method of nonnegative matrix factorization that expresses this matrix
    as the product of two new decomposed matrices that have fewer rows/columns. You
    can refer to [Chapter 8](ch08.xhtml#ch-topicmodels) for a more detailed understanding
    of this method. After the matrix decomposition step, we can generate the summary
    by choosing the top N important topics and then picking the most important sentences
    for each of these topics to form our summary.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于抽取的摘要方法中，使用的一种现代方法是*潜在语义分析*（LSA）。LSA是一种通用方法，用于主题建模、文档相似性和其他任务。LSA假设意思相近的词会出现在同一篇文档中。在LSA算法中，我们首先将整篇文章表示为一个句子-词矩阵。文档-词矩阵的概念已在[第8章](ch08.xhtml#ch-topicmodels)中介绍过，我们可以将该概念调整为适合句子-词矩阵。每行代表一个句子，每列代表一个词。该矩阵中每个单元格的值是词频通常按TF-IDF权重进行缩放。该方法的目标是通过创建句子-词矩阵的修改表示来将所有单词减少到几个主题中。为了创建修改后的表示，我们应用非负矩阵分解的方法，将该矩阵表示为具有较少行/列的两个新分解矩阵的乘积。您可以参考[第8章](ch08.xhtml#ch-topicmodels)更详细地了解这一方法。在矩阵分解步骤之后，我们可以通过选择前N个重要主题生成摘要，然后选择每个主题中最重要的句子来形成我们的摘要。
- en: Instead of applying LSA from scratch, we make use of the package `sumy`, which
    can be installed using the command `**pip install sumy**`. It provides multiple
    summarization methods within the same library. This library uses an integrated
    stop words list along with the tokenizer and stemmer functionality from NLTK but
    makes this configurable. In addition, it is also able to read input from plain
    text, HTML, and files. This gives us the ability to quickly test different summarization
    methods and change the default configurations to suit specific use cases. For
    now, we will go with the default options, including identifying the top three
    sentences:^([1](ch09.xhtml#idm45634184456856))
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不再从头开始应用LSA，而是利用`sumy`包，可以使用命令`**pip install sumy**`进行安装。该库提供了同一库内的多种摘要方法。此库使用一个集成的停用词列表，并结合来自NLTK的分词器和词干处理功能，但可以进行配置。此外，它还能够从纯文本、HTML和文件中读取输入。这使我们能够快速测试不同的摘要方法，并更改默认配置以适应特定的使用案例。目前，我们将使用默认选项，包括识别前三个句子：
- en: '[PRE5]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '`Out:`'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出：`'
- en: '[PRE6]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: By analyzing the results, we see that there is a difference in only one sentence
    from the results of the TF-IDF, and that is sentence 2\. While the LSA method
    chose to highlight a sentence that captures the topic about challenges, the TF-IDF
    method chose a sentence that provides more information about 5G. In this scenario,
    the summaries generated by the two methods are not very different, but let’s analyze
    how this method works on a longer article.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 通过分析结果，我们看到TF-IDF的结果仅有一句与LSA的结果有所不同，即第2句。虽然LSA方法选择突出显示一个关于挑战主题的句子，但TF-IDF方法选择了一个更多关于5G信息的句子。在这种情况下，两种方法生成的摘要并没有非常不同，但让我们分析一下这种方法在更长文章上的工作效果。
- en: 'We wrap this blueprint into a function `lsa_summary`, which is defined in the
    accompanying notebook and can be reused:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将这个蓝图封装成一个函数`lsa_summary`，该函数在附带的笔记本中定义，并可重复使用：
- en: '[PRE7]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '`Out:`'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出：`'
- en: '[PRE8]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Then:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 然后：
- en: '[PRE9]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '`Out:`'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出：`'
- en: '[PRE10]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'And finally:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 最后：
- en: '[PRE11]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '`Out:`'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出：`'
- en: '[PRE12]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The difference in the chosen summarized sentences becomes more evident here.
    The main topic of the trade war tensions is captured by both methods, but the
    LSA summarizer also highlights important topics such as the apprehensiveness of
    investors and corporate confidence. While the TF-IDF tries to express the same
    idea in its chosen sentences, it does not pick the right sentences and therefore
    fails to convey the idea. There are other topic-based summarization methods, but
    we have chosen to highlight LSA as a simple and widely used method.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，选择的摘要句子的差异变得更加明显。贸易战紧张局势的主要话题被两种方法捕捉到，但LSA摘要器还突出了投资者的担忧和企业信心等重要话题。虽然TF-IDF试图在其选择的句子中表达相同的观点，但它没有选择正确的句子，因此未能传达这一观点。还有其他基于主题的摘要方法，但我们选择突出LSA作为一个简单且广泛使用的方法。
- en: Note
  id: totrans-60
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: It’s interesting to note that the `sumy` library also provides the implementation
    of one of the oldest methods for automatic text summarization (`LuhnSummarizer`),
    which was created by [Hans Peter Luhn in 1958](https://oreil.ly/j6cQI). This method
    is also based on topic representation by identifying important words using their
    counts and setting thresholds to get rid of extremely frequent and infrequent
    words. You can use this as a baseline method for your summarization experiments
    and compare improvements provided by other methods.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，`sumy`库还提供了自动文本摘要的一个最古老的方法（`LuhnSummarizer`）的实现，该方法由[Hans Peter Luhn于1958年创造](https://oreil.ly/j6cQI)。这种方法也是基于通过识别重要词汇的计数和设置阈值来表示主题。您可以将其用作文本摘要实验的基准方法，并比较其他方法提供的改进。
- en: 'Blueprint: Summarizing Text Using an Indicator Representation'
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 蓝图：使用指示器表示对文本进行摘要
- en: Indicator representation methods aim to create the intermediate representation
    of a sentence by using features of the sentence and its relationship to others
    in the document rather than using only the words in the sentence. [TextRank](https://oreil.ly/yY29h)
    is one of the most popular examples of an indicator-based method. TextRank is
    inspired by PageRank, a “graph-based ranking algorithm that was originally used
    by Google to rank search results. As per the authors of the TextRank paper, graph-based
    algorithms rely on the collective knowledge of web architects rather than individual
    content analysis of web pages,” which leads to improved performance. Applied to
    our context, we will rely on the features of a sentence and the linkages between
    them rather than on topics contained in each sentence.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 指示器表示方法旨在通过使用句子的特征及其与文档中其他句子的关系来创建句子的中间表示，而不仅仅是使用句子中的单词。[TextRank](https://oreil.ly/yY29h)是指示器方法中最流行的例子之一。TextRank受PageRank启发，是一种“基于图的排名算法，最初由Google用于排名搜索结果。根据TextRank论文的作者，基于图的算法依赖于网页结构的集体知识，而不是单个网页内容的分析”，这导致了改进的性能。在我们的背景下应用，我们将依赖句子的特征和它们之间的链接，而不是依赖每个句子所包含的主题。
- en: We will first try to understand how the PageRank algorithm works and then adapt
    the methodology to the text summarization problem. Let’s consider a list of web
    pages—(A, B, C, D, E, and F) and their links to one another. In [Figure 9-1](#fig-pagerank-graph),
    page A contains a link to page D. Page B contains links to A and D and so on.
    We can also represent this in the form of a matrix with rows referring to each
    page and with columns referring to incoming links from other pages. The matrix
    shown in the figure represents our graph with rows representing each node, columns
    referring to incoming links from other nodes, and the value of the cell representing
    the weight of the edge between them. We start with a simple representation (1
    indicates an incoming link, 0 indicates none). We can then normalize these values
    by dividing by the total number of outgoing links for each web page. For example,
    page C has two outgoing links (to pages E and F), and therefore the value of each
    outgoing link is 0.5.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 首先我们将尝试理解PageRank算法的工作原理，然后将方法应用于文本摘要问题。让我们考虑一个网页列表（A、B、C、D、E和F）及其彼此之间的链接。在[图9-1](#fig-pagerank-graph)中，页面A包含指向页面D的链接。页面B包含指向A和D的链接，依此类推。我们还可以用一个矩阵表示，行表示每个页面，列表示来自其他页面的入链。图中显示的矩阵表示我们的图，行表示每个节点，列表示来自其他节点的入链，单元格的值表示它们之间边的权重。我们从一个简单的表示开始（1表示有入链，0表示没有）。然后我们可以通过将每个网页的出链总数进行除法来归一化这些值。例如，页面C有两个出链（到页面E和F），因此每个出链的值为0.5。
- en: '![](Images/btap_0901.jpg)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/btap_0901.jpg)'
- en: Figure 9-1\. Web page links and corresponding PageRank matrix.
  id: totrans-66
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-1\. 网页链接和相应的PageRank矩阵。
- en: The PageRank for a given page is a weighted sum of the PageRank for all other
    pages that have a link to it. This also means that calculating the PageRank is
    an iterative function where we must start with some assumed values of PageRank
    for each page. If we assume all initial values to be 1 and multiply the matrices
    as shown in [Figure 9-2](#fig-pagerank-results), we arrive at the PageRank for
    each page after one iteration (not taking into consideration the damping factor
    for this illustration).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 对于给定页面的PageRank是所有具有链接的其他页面的PageRank的加权和。这也意味着计算PageRank是一个迭代函数，我们必须从每个页面的一些假设的PageRank初始值开始。如果我们假设所有初始值为1，并按照[图9-2](#fig-pagerank-results)所示的方式进行矩阵乘法，我们可以在一次迭代后得到每个页面的PageRank（不考虑此示例的阻尼因子）。
- en: The [research paper by Brin and Page](https://oreil.ly/WjjFv) showed that after
    repeating this calculation for many iterations the values stabilize, and hence
    we get the PageRank or importance for each page. TextRank adapts the previous
    approach by considering each sentence in the text to be analogous to a page and
    therefore a node in the graph. The weight of the edges between nodes is determined
    by the similarity between sentences, and the authors of TextRank suggest a simple
    approach by counting the number of shared lexical tokens, normalized by the size
    of both sentences. There are other similarity measures such as cosine distance
    and longest common substring that can also be used.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '[Brin和Page的研究论文](https://oreil.ly/WjjFv)表明，重复进行多次迭代计算后，数值稳定，因此我们得到每个页面的PageRank或重要性。TextRank通过将文本中的每个句子视为一个页面和因此图中的一个节点来调整先前的方法。节点之间边的权重由句子之间的相似性决定，TextRank的作者建议通过计算共享词汇标记的数量（归一化为两个句子的大小）来实现简单的方法。还有其他相似度度量，如余弦距离和最长公共子串也可以使用。'
- en: '![](Images/btap_0902.jpg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/btap_0902.jpg)'
- en: Figure 9-2\. Application of one iteration of PageRank algorithm.
  id: totrans-70
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-2\. PageRank算法的一次迭代应用。
- en: 'Since the sumy package also provides a TextRank implementation, we will use
    it to generate the summarized sentences for the article on the US recession that
    we saw previously:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 由于sumy包还提供了TextRank实现，我们将使用它为我们之前看到的关于美国经济衰退的文章生成总结的句子：
- en: '[PRE13]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: While one of the summarized sentences remains the same, this method has chosen
    to return two other sentences that are probably linked to the main conclusions
    drawn in this article. While these sentences themselves may not seem important,
    the use of a graph-based method resulted in selecting highly linked sentences
    that support the main theme of the article. We wrap this blueprint as a function
    `textrank_summary`, allowing us to reuse it.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 当总结句之一保持不变时，这种方法选择返回其他两个可能与本文主要结论相关联的句子。虽然这些句子本身可能并不重要，但使用基于图的方法选择了支持文章主题的高度关联句子。我们将这个蓝图封装成一个函数`textrank_summary`，允许我们进行重复使用。
- en: 'We also want to see how this method works on the shorter article on 5G technology
    that we looked at previously:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还想看看这种方法在我们之前查看过的关于5G技术的较短文章上的运作：
- en: '[PRE15]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '`Out:`'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE16]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We see that the results capture the central idea of the Qualcomm acquisition
    but do not contain any mention of 5G as a technology that was selected by the
    LSA method. TextRank generally works better in the case of longer text content
    as it is able to identify the most important sentences using the graph linkages.
    In the case of shorter text content, the graphs are not very large, and therefore
    the wisdom of the network plays a smaller role. Let’s use an example of even longer
    content from Wikipedia to highlight this point further. We will reuse the blueprint
    from [Chapter 2](ch02.xhtml#ch-api) to download the text content of a Wikipedia
    article. In this case, we choose an article that describes a historical event
    or series of events: the Mongol invasion of Europe. And since this is much longer
    text, we choose to summarize about 10 sentences to provide a better summary:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到，结果捕捉到了高通收购的中心思想，但没有提及LSA方法选择的5G技术。TextRank通常在长文本内容的情况下表现更好，因为它能够使用图链接识别最重要的句子。在较短的文本内容中，图不是很大，因此网络智慧发挥的作用较小。让我们使用来自维基百科的更长内容的例子来进一步说明这一点。我们将重复使用来自[第2章](ch02.xhtml#ch-api)的蓝图，下载维基百科文章的文本内容。在这种情况下，我们选择描述历史事件或事件系列的文章：蒙古入侵欧洲。由于这是更长的文本，我们选择总结大约10句话，以提供更好的总结：
- en: '[PRE17]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '`Out:`'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE18]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Then:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 然后：
- en: '[PRE19]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: We illustrate the results as highlighted sentences in the original Wikipedia
    page ([Figure 9-3](#fig-wikipedia-summary)) to show that using the TextRank algorithm
    provides an almost accurate summarization of the article by picking the most important
    sentences from each section of the article. We can compare how this works with
    an LSA method, but we leave this as an exercise to the reader using the previous
    blueprint. Based on our experiences, when we want to summarize a large piece of
    text content, for example, scientific research papers, collection of writings,
    and speeches by world leaders or multiple web pages, then we would choose a graph-based
    method like TextRank.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将结果展示为原始维基百科页面中的突出显示句子（[Figure 9-3](#fig-wikipedia-summary)），以展示使用TextRank算法通过从文章的每个部分中选择最重要的句子，几乎准确地对文章进行了总结。我们可以比较这与LSA方法的工作，但我们将这留给读者使用先前的蓝图作为练习。根据我们的经验，当我们想要总结大量的文本内容时，例如科学研究论文、作品集以及世界领导人的演讲或多个网页时，我们会选择像TextRank这样基于图的方法。
- en: '![](Images/btap_0903.jpg)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/btap_0903.jpg)'
- en: Figure 9-3\. Wikipedia page with selected summary sentences highlighted.
  id: totrans-87
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: Figure 9-3\. 维基百科页面，突出显示了选定的摘要句子。
- en: Measuring the Performance of Text Summarization Methods
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 衡量文本摘要方法的性能
- en: In the blueprints so far, we have seen many methods that produce summaries of
    some given text. Each summary differs from the other in subtle ways, and we have
    to rely on our subjective evaluation. This is certainly a challenge in selecting
    a method that works best for a given use case. In this section, we will introduce
    commonly used accuracy metrics and show how they can be used to empirically select
    the best method for summarization.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们在蓝图中已经看到了许多方法来生成某段文本的摘要。每个摘要在细微之处都有所不同，我们必须依靠我们的主观评估。在选择最适合特定使用案例的方法方面，这无疑是一个挑战。在本节中，我们将介绍常用的准确度度量标准，并展示它们如何被用来经验性地选择最佳的摘要方法。
- en: We must understand that to automatically evaluate the summary of some given
    text, there must be a reference summary that it can be compared with. Typically,
    this is a summary written by a human and is referred to as the *gold standard*.
    Every automatically generated summary can be compared with the gold standard to
    get an accuracy measure. This also gives us the opportunity to easily compare
    multiple methods and choose the best one. However, we will often run into the
    issue that a human-generated summary may not exist for every use case. In such
    situations, we can choose a proxy measure to be considered as the gold standard.
    An example in the case of a news article would be the headline. While it is written
    by a human, it is a poor proxy as it can be quite short and is not an accurate
    summary but more of a leading statement to draw users. While this may not give
    us the best results, it is still useful to compare the performance of different
    summarization methods.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须理解，要自动评估某段给定文本的摘要，必须有一个可以进行比较的参考摘要。通常，这是由人类编写的摘要，称为*黄金标准*。每个自动生成的摘要都可以与黄金标准进行比较，以获得准确度的度量。这也为我们提供了比较多种方法并选择最佳方法的机会。然而，我们经常会遇到一个问题，即并非每个使用案例都有人类生成的摘要存在。在这种情况下，我们可以选择一个代理度量来视为黄金标准。在新闻文章的案例中，一个例子就是标题。虽然它是由人类编写的，但作为一个代理度量它并不准确，因为它可能非常简短，更像是一个引导性陈述来吸引用户。虽然这可能不会给我们带来最佳结果，但比较不同摘要方法的性能仍然是有用的。
- en: '*Recall-Oriented* *Understudy for Gisting Evaluation* (ROUGE) is one of the
    most commonly used methods to measure the accuracy of a summary. There are several
    types of ROUGE metrics, but the basic idea is simple. It arrives at the measure
    of accuracy by comparing the number of shared terms between the automatically
    generated summary and the gold standard. ROUGE-N is a metric that measures the
    number of common n-grams (ROUGE-1 compares individual words, ROUGE-2 compares
    bigrams, and so on).'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '*用于Gisting评估的召回导向的Understudy*（ROUGE）是最常用的测量摘要准确性的方法之一。有几种类型的ROUGE度量标准，但基本思想很简单。它通过比较自动生成的摘要与黄金标准之间的共享术语数量来得出准确度的度量。ROUGE-N是一种度量标准，用于衡量常见的n-gram（ROUGE-1比较单个词，ROUGE-2比较二元组，依此类推）。'
- en: 'The original [ROUGE paper](https://oreil.ly/Tsncq) compared how many of the
    words that appear in the gold standard also appear in the automatically generated
    summary. This is what we introduced in [Chapter 6](ch06.xhtml#ch-classification)
    as *recall*. So if most of the words present in the gold standard were also present
    in the generated summary, we would achieve a high score. However, this metric
    alone does not tell the whole story. Consider that we generate a verbose summary
    that is long but includes most of the words in the gold standard. This summary
    would have a high score, but it would not be a good summary since it doesn’t provide
    a concise representation. This is why the ROUGE measure has been extended to compare
    the number of shared words to the total number of words in the generated summary
    as well. This indicates the precision: the number of words in the generated summary
    that are actually useful. We can combine these measures to generate the F-score.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 原始的 [ROUGE 论文](https://oreil.ly/Tsncq) 比较了在自动生成的摘要中出现的单词中有多少也出现在金标准中。这就是我们在
    [第 6 章](ch06.xhtml#ch-classification) 中介绍的 *召回率*。因此，如果金标准中大多数单词也出现在生成的摘要中，我们将获得高分。然而，单靠这一指标并不能讲述整个故事。考虑到我们生成了一个冗长但包含金标准中大多数单词的摘要。这个摘要将获得高分，但它不是一个好的摘要，因为它没有提供简洁的表示。这就是为什么
    ROUGE 测量已经扩展到将共享单词的数量与生成的摘要中的总单词数进行比较。这表明了精度：生成摘要中实际有用的单词数。我们可以结合这些措施生成 F 分数。
- en: 'Let’s see an example of ROUGE for one of our generated summaries. Since we
    do not have a gold standard human-generated summary, we use the headline of the
    article as a proxy for the gold standard. While it is simple to calculate this
    independently, we make use of the Python package called `rouge_scorer` to make
    our life easier. This package implements all the ROUGE measures that we will use
    later, and it can be installed by executing the command `**pip install rouge_scorer**`.
    We make use of a print utility function `print_rouge_score` to present a concise
    view of the scores:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个我们生成摘要的 ROUGE 示例。由于我们没有金标准的人工生成摘要，我们使用文章标题作为金标准的代理。虽然这样计算独立简单，但我们利用名为
    `rouge_scorer` 的 Python 包来使我们的生活更轻松。这个包实现了我们后来将使用的所有 ROUGE 测量，并且可以通过执行命令 `**pip
    install rouge_scorer**` 进行安装。我们利用一个打印实用函数 `print_rouge_score` 来展示得分的简洁视图：
- en: '[PRE20]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '`Out:`'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出：`'
- en: '[PRE21]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The previous result shows us that the summary generated by TextRank has a high
    recall but low precision. This is an artifact of our gold standard being an extremely
    short headline, which is itself not the best choice but used here for illustration.
    The most important use of our metric is a comparison with another summarization
    method, and in this case, let’s compare this with the LSA-generated summary:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 先前的结果显示，TextRank 生成的摘要具有高召回率但低精度。这是我们金标准是一个极短标题的结果，本身并不是最佳选择，但在这里用于说明。我们度量标准的最重要用途是与另一种总结方法进行比较，在这种情况下，让我们与
    LSA 生成的摘要进行比较：
- en: '[PRE22]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '`Out:`'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出：`'
- en: '[PRE23]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The above result shows us that TextRank was the superior method in this case
    because it had a higher precision, while the recall of both methods was the same.
    We can easily extend ROUGE-1 to ROUGE-2, which would compare the number of common
    sequences of two words (bigrams). Another important metric is ROUGE-L, which measures
    the number of common sequences between the reference summary and the generated
    summary by identifying the longest common subsequences. A subsequence of a sentence
    is a new sentence that can be generated from the original sentence with some words
    deleted without changing the relative order of the remaining words. The advantage
    of this metric is that it does not focus on exact sequence matches but in-sequence
    matches that reflect sentence-level word order. Let’s analyze the ROUGE-2 and
    ROUGE-L metrics for the Wikipedia page. Again, we do not have a gold standard,
    and therefore we will use the introductory paragraph as the proxy for our gold
    standard:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 上述结果表明，在这种情况下，TextRank 是优越的方法，因为它具有更高的精度，而两种方法的召回率相同。我们可以轻松地扩展 ROUGE-1 到 ROUGE-2，这将比较两个词（二元组）的公共序列的数量。另一个重要的指标是
    ROUGE-L，它通过识别参考摘要与生成摘要之间的最长公共子序列来衡量。句子的子序列是一个新句子，可以从原始句子中删除一些单词而不改变剩余单词的相对顺序。这个指标的优势在于它不专注于精确的序列匹配，而是反映句子级词序的顺序匹配。让我们分析维基百科页面的
    ROUGE-2 和 ROUGE-L 指标。再次强调，我们没有一个金标准，因此我们将使用简介段落作为我们金标准的代理：
- en: '[PRE24]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '`Out:`'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出：`'
- en: '[PRE25]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Then:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 然后：
- en: '[PRE26]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '`Out:`'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出：`'
- en: '[PRE27]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Based on the results, we see that TextRank proves to be more accurate than LSA.
    We can use the same method as shown earlier to see which method works best for
    shorter Wikipedia entries, which we will leave as an exercise for the reader.
    When applying this to your use case, it is important that you choose the right
    summary for comparison. For instance, when working with news articles, instead
    of using the headline, you could look for a summary section contained within the
    article or generate one yourself for a small number of articles. This would allow
    you to have a fair comparison between different methods.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 根据结果，我们看到TextRank比LSA更准确。我们可以使用与前面展示的相同方法来查看哪种方法对较短的维基百科条目效果最好，这将留给读者作为练习。当应用到您的用例时，重要的是选择正确的摘要进行比较。例如，在处理新闻文章时，您可以查找文章内包含的摘要部分，而不是使用标题，或者为少数文章生成自己的摘要。这样可以在不同方法之间进行公平比较。
- en: 'Blueprint: Summarizing Text Using Machine Learning'
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 蓝图：使用机器学习进行文本总结
- en: Many of you might have participated in online discussion forums for topics such
    as travel planning, programming, etc. Users on these platforms communicate in
    the form of threads. Anybody can start a thread, and other members provide their
    responses on this thread. Threads can become long, and the key message might be
    lost. In this blueprint, we will use data extracted from a travel forum used in
    the research paper,^([2](ch09.xhtml#idm45634183488696)) which contains the text
    for all posts in a thread along with the summary for that thread, as shown in
    [Figure 9-4](#fig-thread-illustration).
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 许多人可能参与了关于旅行规划、编程等主题的在线讨论论坛。在这些平台上，用户以线程的形式进行交流。任何人都可以开始一个线程，其他成员则在该线程上提供他们的回应。线程可能会变得很长，关键信息可能会丢失。在这个蓝图中，我们将使用从研究论文中提取的数据，^([2](ch09.xhtml#idm45634183488696))
    这些数据包含了一个线程中所有帖子的文本以及该线程的摘要，如[图9-4](#fig-thread-illustration)所示。
- en: In this blueprint, we are going to use machine learning to help us automatically
    identify the most important posts across the entire thread that accurately summarize
    it. We will first use the summary by the annotator to create target labels for
    our dataset. We will then generate features that can be useful to determine whether
    a particular post should be in the summary and finally train a model and evaluate
    the accuracy. The task at hand is similar to text classification but performed
    at a post level.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个蓝图中，我们将使用机器学习来帮助我们自动识别整个线程中最重要的帖子，这些帖子准确地总结了整个线程。我们首先使用注释者的摘要为我们的数据集创建目标标签。然后生成能够确定特定帖子是否应该出现在摘要中的特征，并最终训练一个模型并评估其准确性。手头的任务类似于文本分类，但是在帖子级别上执行。
- en: While the forum threads are used to illustrate this blueprint, it can easily
    be used for other use-cases. For example, consider the [CNN and Daily Mail news
    summarization task](https://oreil.ly/T_RNc), [DUC](https://oreil.ly/0Hlov), or
    [SUMMAC](https://oreil.ly/Wg322) datasets. In each of these datasets, you will
    find the text of each article and the highlighted summary sentences. These are
    analogous to the text of each thread and the summary as presented in this blueprint.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然论坛线程用于说明这个蓝图，但它也可以轻松地用于其他用例。例如，考虑[CNN和每日邮报新闻摘要任务](https://oreil.ly/T_RNc)，[DUC](https://oreil.ly/0Hlov)，或[SUMMAC](https://oreil.ly/Wg322)数据集。在这些数据集中，你会找到每篇文章的文本和突出显示的摘要句子。这些与本蓝图中呈现的每个线程的文本和摘要类似。
- en: '![](Images/btap_0904.jpg)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/btap_0904.jpg)'
- en: Figure 9-4\. Posts in a thread and the corresponding summary from a travel forum.
  id: totrans-115
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-4。一个线程中的帖子及其来自旅行论坛的对应摘要。
- en: 'Step 1: Creating Target Labels'
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第1步：创建目标标签
- en: 'The first step is to load the dataset, understand its structure, and create
    target labels using the provided summary. We have performed the initial data preparation
    steps to create a well-formatted `DataFrame`, shown next. Please refer to the
    `Data_Preparation` notebook in the GitHub repo of the book for a detailed look
    at the steps:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是加载数据集，了解其结构，并使用提供的摘要创建目标标签。我们已经执行了初始的数据准备步骤，创建了一个格式良好的`DataFrame`，如下所示。请参阅书籍的GitHub仓库中的`Data_Preparation`笔记本，详细了解这些步骤：
- en: '[PRE28]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '|   | 170 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '|   | 170 |'
- en: '| --- | --- |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Date | 29 September 2009, 1:41 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| 日期 | 2009年9月29日，1:41 |'
- en: '| Filename | thread41_system20 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| 文件名 | thread41_system20 |'
- en: '| ThreadID | 60763_5_3122150 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| 线程ID | 60763_5_3122150 |'
- en: '| Title | which attractions need to be pre booked? |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| 标题 | 需要预订哪些景点？ |'
- en: '| postNum | 1 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| 帖子编号 | 1 |'
- en: '| text | Hi I am coming to NY in Oct! So excited&quot; Have wanted to visit
    for years. We are planning on doing all the usual stuff so wont list it all but
    wondered which attractions should be pre booked and which can you just turn up
    at> I am plannin on booking ESB but what else? thanks x |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| text | Hi I am coming to NY in Oct! So excited&quot; Have wanted to visit
    for years. We are planning on doing all the usual stuff so wont list it all but
    wondered which attractions should be pre booked and which can you just turn up
    at> I am plannin on booking ESB but what else? thanks x |'
- en: '| userID | musicqueenLon... |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| 用户ID | musicqueenLon... |'
- en: '| summary | A woman was planning to travel NYC in October and needed some suggestions
    about attractions in the NYC. She was planning on booking ESB.Someone suggested
    that the TOTR was much better compared to ESB. The other suggestion was to prebook
    the show to avoid wasting time in line.Someone also suggested her New York Party
    Shuttle tours. |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| summary | A woman was planning to travel NYC in October and needed some suggestions
    about attractions in the NYC. She was planning on booking ESB.Someone suggested
    that the TOTR was much better compared to ESB. The other suggestion was to prebook
    the show to avoid wasting time in line.Someone also suggested her New York Party
    Shuttle tours. |'
- en: Each row in this dataset refers to a post in a thread. Each thread is identified
    by a unique `ThreadID`, and it’s possible that multiple rows in the `DataFrame`
    have the same `ThreadID`. The column `Title` refers to the name with which the
    user started the thread. The content of each post is in the `text` column, along
    with additional details like the name of the user who created the post (`userID`),
    the time when the post was created (`Date`), and its position in the thread (`postNum`).
    For this dataset, human-generated summaries for each thread are provided in the
    `summary` column.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据集中的每一行都指的是主题中的一个帖子。每个主题由一个唯一的 `ThreadID` 标识，`DataFrame` 中可能有多行具有相同的 `ThreadID`。`Title`
    列指的是用户开始主题时使用的名称。每个帖子的内容都在 `text` 列中，还包括其他细节，比如创建帖子的用户的姓名（`userID`）、帖子创建时间（`Date`）以及在主题中的位置（`postNum`）。对于这个数据集，每个主题都提供了人工生成的摘要，位于
    `summary` 列中。
- en: 'We will reuse the regular expression cleaning and spaCy pipeline blueprints
    from [Chapter 4](ch04.xhtml#ch-preparation) to remove special formatting, URLs,
    and other punctuation from the posts. We will also generate the lemmatized representation
    of the text, which we will use for prediction. You can find the function definitions
    in the accompanying notebook for this chapter. Since we are making use of the
    spaCy lemmatization function, it might take a couple of minutes to complete execution:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将重用[第四章](ch04.xhtml#ch-preparation)中的正则表达式清理和 spaCy 流水线蓝图，以删除帖子中的特殊格式、URL
    和其他标点符号。我们还将生成文本的词形还原表示，用于预测。你可以在本章的附带笔记本中找到函数定义。由于我们正在使用 spaCy 的词形还原功能，执行可能需要几分钟才能完成：
- en: '[PRE29]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Each observation in our dataset contains a post that is part of a thread. If
    we were to apply a train-test split at this level, it is possible that two posts
    belonging to the same thread would end up in the train and test datasets, which
    would lead to inaccurate training. As a result, we use `GroupShuffleSplit` to
    group all posts into their respective threads and then randomly select 80% of
    the threads to create the training dataset, with the rest of the threads forming
    part of the test dataset. This function ensures that posts belonging to the same
    thread are part of the same dataset. The `GroupShuffleSplit` function does not
    actually split the data but provides a set of indices that split the data identified
    by `train_split` and `test_split`. We use these indices to create the two datasets:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们数据集中的每个观测都包含一个帖子，该帖子是主题的一部分。如果我们在这个层面应用训练-测试分割，那么可能会导致两个属于同一主题的帖子分别进入训练集和测试集，这将导致训练不准确。因此，我们使用
    `GroupShuffleSplit` 将所有帖子分组到它们各自的主题中，然后随机选择 80% 的主题来创建训练数据集，其余的主题组成测试数据集。这个函数确保属于同一主题的帖子属于同一数据集。`GroupShuffleSplit`
    函数实际上并不分割数据，而是提供了一组索引，这些索引标识了由 `train_split` 和 `test_split` 确定的数据的分割。我们使用这些索引来创建这两个数据集：
- en: '[PRE30]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '[PRE31]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '`Out:`'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出：`'
- en: '[PRE32]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Our next step is to determine the target label for each of our posts. The target
    label defines whether a particular post should be included in the summary. We
    determine this by comparing each post to the annotator summary and picking posts
    that are most similar to be included in the summary. There are several metrics
    that can be used to determine the similarity of two sentences, but in our use
    case we are working with short texts and therefore choose the [Jaro-Winkler distance](https://oreil.ly/b5q0B).
    We use the `textdistance` package that also provides implementations of other
    distance metrics. This can be easily installed using the command `**pip install
    textdistance**`. You can also easily modify the blueprint and choose a metric
    based on your use case.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的下一步是确定每篇文章的目标标签。目标标签定义了是否应将特定文章包含在摘要中。我们通过将每篇文章与注释员摘要进行比较，并选择最相似的文章来确定这一点。有几种度量标准可用于确定两个句子的相似性，但在我们的用例中，我们处理短文本，因此选择了[Jaro-Winkler距离](https://oreil.ly/b5q0B)。我们使用`textdistance`包，该包还提供其他距离度量的实现。您可以使用命令`**pip
    install textdistance**`轻松安装它。您还可以轻松修改蓝图，并根据您的用例选择度量标准。
- en: 'In the following step, we determine the similarity and rank all the posts within
    a thread based on the chosen metric. We then create our target label named `summaryPost`
    that contains a True or False value indicating whether this post is part of the
    summary. This is based on the rank of the post and the compression factor. We
    choose a compression factor of 30%, which means that we pick the top 30% of all
    posts ordered by their similarity to be included in the summary:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的步骤中，我们根据所选择的度量标准确定相似性并对主题中的所有帖子进行排序。然后，我们创建名为`summaryPost`的目标标签，其中包含一个True或False值，指示此帖子是否属于摘要。这是基于帖子的排名和压缩因子。我们选择了30%的压缩因子，这意味着我们选择按相似性排序的所有帖子中的前30%来包含在摘要中：
- en: '[PRE33]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[PRE34]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '`Out:`'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出:`'
- en: '|   | text | summaryPost |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '|   | text | summaryPost |'
- en: '| --- | --- | --- |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 170 | Hi I am coming to NY in Oct! So excited” Have wanted to visit for years.
    We are planning on doing all the usual stuff so wont list it all but wondered
    which attractions should be pre booked and which can you just turn up at> I am
    plannin on booking ESB but what else? thanks x | True |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| 170 | 嗨，我十月份要去纽约！好兴奋！多年来一直想去参观。我们计划做所有传统的事情，所以不会列出所有的事情，但想知道哪些景点应该提前预订，哪些可以直接到场？我打算预订帝国大厦，还有什么？谢谢
    x | True |'
- en: '| 171 | I wouldnt bother doing the ESB if I was you TOTR is much better. What
    other attractions do you have in mind? | False |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| 171 | 如果我是你，我不会去帝国大厦，TOPR要好得多。你还有哪些景点考虑？ | False |'
- en: '| 172 | The Statue of Liberty, if you plan on going to the statue itself or
    to Ellis Island (as opposed to taking a boat past): http://www.statuecruises.com/
    Also, we prefer to book shows and plays in advance rather than trying for the
    same-day tickets, as that allows us to avoid wasting time in line. If that sounds
    appealing to you, have a look at http://www.broadwaybox.com/ | True |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| 172 | 自由女神像，如果您计划去雕像本身或埃利斯岛（而不是乘船经过）：http://www.statuecruises.com/ 另外，我们更喜欢提前预订演出和戏剧，而不是尝试购买当天票，因为这样可以避免排队浪费时间。如果这听起来对您有吸引力，请看看http://www.broadwaybox.com/
    | True |'
- en: 'As you can see in the previous results for a given thread, the first and third
    posts are tagged as `summaryPost`, but the second post is not considered important
    and would not be included in the summary. Because of the way we defined our target
    label, it is possible in rare situations that very short posts are also included
    in the summary. This might happen when a short post contains the same words as
    the thread title. This is not useful to the summary, and we correct this by setting
    all posts containing 20 words or less to not be included in the summary:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在前面的结果中看到的，对于给定的主题，第一和第三篇文章被标记为`summaryPost`，但第二篇文章不重要，不会被包含在摘要中。由于我们定义了目标标签的方式，很少情况下可能会将非常短的帖子包含在摘要中。当一个短帖子包含与主题标题相同的词时，可能会发生这种情况。这对摘要没有用，我们通过将所有包含20个词或更少的帖子设置为不包含在摘要中来进行修正：
- en: '[PRE35]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Step 2: Adding Features to Assist Model Prediction'
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤2：添加帮助模型预测的特征
- en: 'Since we are dealing with forum threads in this blueprint, there are some additional
    features that we can generate to help our model in the prediction. The title of
    the thread conveys the topic succinctly and can be helpful in identifying which
    post should actually be selected in the summary. We cannot directly include the
    title as a feature since it would be the same for each post in the thread, but
    instead we calculate the similarity between the post and the title as one of the
    features:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们在这个蓝图中处理的是论坛主题，我们可以生成一些额外的特征来帮助我们的模型进行预测。主题的标题简洁地传达了主题，并且在识别应该在摘要中实际选择的帖子时可能会有所帮助。我们不能直接将标题包含为一个特征，因为对于主题中的每个帖子来说它都是相同的，但是我们可以计算帖子与标题之间的相似度作为其中一个特征：
- en: '[PRE36]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Another useful feature could be the length of the post. Short posts could be
    asking clarifying questions and would not capture the most useful knowledge of
    the thread. Longer posts could indicate that a lot of useful information is being
    shared. The position of where the post appears in the thread could also be a useful
    indicator of whether it should be in the summary. This might vary depending on
    the way in which the forum threads are organized. In the case of the travel forum,
    the posts are chronologically ordered, and the occurrence of the post is given
    by the column `postNum`, which we can readily use as a feature:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个有用的特征可能是帖子的长度。短帖子可能是在询问澄清问题，不会捕捉到主题的最有用的知识。长帖子可能表明正在分享大量有用信息。帖子在主题中的位置也可能是一个有用的指标，用于确定是否应该将其包含在摘要中。这可能会根据论坛主题的组织方式而有所不同。在旅行论坛的情况下，帖子是按时间顺序排序的，帖子的发生是通过列`postNum`给出的，我们可以直接将其用作一个特征：
- en: '[PRE37]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'As a final step, let’s create the vectorized representation of the lemmas that
    we extracted earlier using the *TfidfVectorizer*. We then create a new `DataFrame`,
    `train_df_tf`, which contains the vectorized lemmas and the additional features
    that we created earlier:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 作为最后一步，让我们使用*TfidfVectorizer*创建我们之前提取的词元的向量化表示。然后，我们创建一个新的`DataFrame`，`train_df_tf`，其中包含向量化的词元和我们之前创建的附加特征：
- en: '[PRE38]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[PRE39]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: This step of adding features can be extended or customized depending on the
    use case. For example, if we are looking to summarize longer text, then the paragraph
    that a sentence belongs to will be important. Normally, each paragraph or section
    tries to capture an idea, and sentence similarity metrics at that level would
    be relevant. If we are looking at generating summaries of scientific papers, then
    the number of citations and the sentences used for those citations have proven
    to be useful. We must also repeat the same feature engineering steps on the test
    dataset, which we show in the accompanying notebook but exclude here.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 添加特征的这一步骤可以根据使用情况进行扩展或定制。例如，如果我们想要总结更长的文本，那么一个句子所属的段落将是重要的。通常，每个段落或部分都试图捕捉一个思想，并且在该水平上使用的句子相似性度量将是相关的。如果我们试图生成科学论文的摘要，那么引用次数和用于这些引用的句子已被证明是有用的。我们还必须在测试数据集上重复相同的特征工程步骤，我们在附带的笔记本中展示了这一点，但在这里排除了。
- en: 'Step 3: Build a Machine Learning Model'
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤3：构建机器学习模型
- en: 'Now that we’ve generated features, we will reuse the text classification blueprint
    from [Chapter 6](ch06.xhtml#ch-classification) but use a `RandomForestClassifier`
    model instead of the SVM model. While building a machine learning model for summarization,
    we might have additional features other than the vectorized text representation.
    Particularly in situations where a combination of numeric and categorical features
    are present, a tree-based classifier might perform better:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经生成了特征，我们将重用[第6章](ch06.xhtml#ch-classification)中的文本分类蓝图，但是使用`RandomForestClassifier`模型代替SVM模型。在构建用于摘要的机器学习模型时，我们可能有除了向量化的文本表示之外的其他特征。特别是在存在数字和分类特征的组合的情况下，基于树的分类器可能会表现得更好：
- en: '[PRE40]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '`Out:`'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出:`'
- en: '[PRE41]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Let’s apply this model on the test threads and predict the summary posts. To
    determine the accuracy, we concatenate all identified summary posts and generate
    the ROUGE-1 score by comparing it with the annotator summary:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在测试主题上应用这个模型，并预测摘要帖子。为了确定准确性，我们连接所有识别的摘要帖子，并通过与注释摘要进行比较生成ROUGE-1分数：
- en: '[PRE42]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '[PRE43]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '`Out:`'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出:`'
- en: '[PRE44]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: We see that the mean ROUGE-1 score for all threads in the test set is 0.34,
    which is comparable with extractive summarization scores on other [public summarization
    tasks](https://oreil.ly/SaCk2). You will also notice on the leaderboard that the
    use of pretrained models such as BERT improves the score, and we explore this
    technique in detail in [Chapter 11](ch11.xhtml#ch-sentiment).
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到，测试集中所有主题的平均 ROUGE-1 分数为 0.34，与其他[公共摘要任务](https://oreil.ly/SaCk2)上的抽取式摘要分数相当。您还会注意到排行榜上使用预训练模型如
    BERT 改善了分数，我们在[第 11 章](ch11.xhtml#ch-sentiment)中详细探讨了这一技术。
- en: '[PRE45]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '`Out:`'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE46]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Let’s also take a look at one of the summarized results produced by this model
    to understand how useful it might be:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们也来看看由这个模型生成的一个摘要结果，以了解它可能有多有用：
- en: '[PRE47]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '`Out:`'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE48]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '|   | postNum | text |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '|   | postNum | text |'
- en: '| --- | --- | --- |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 551 | 4 | Well, you’re really in luck, because there’s a lot going on, including
    the Elmwood Avenue Festival of the Arts (http://www.elmwoodartfest.org), with
    special activities for youngsters, performances (including one by Nikki Hicks,
    one of my favorite local vocalists), and food of all kinds. Elmwood Avenue is
    one of the area’s most colorful and thriving neighborhoods, and very walkable.
    The Buffalo Irish Festival is also going on that weekend in Hamburg, as it happens,
    at the fairgrounds: www.buf... |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| 551 | 4 | 看来你真的很幸运，因为有很多事情可以做，包括艾尔姆伍德艺术节（http://www.elmwoodartfest.org），为年轻人准备的特别活动，表演（包括我最喜欢的本地歌手之一尼基·希克斯的表演），以及各种美食。艾尔姆伍德大道是该地区最丰富多彩且充满活力的社区之一，非常适合步行。布法罗爱尔兰文化节也将在汉堡的周末举行，正好在展览会场地：www.buf...
    |'
- en: '| 552 | 5 | Depending on your time frame, a quick trip to Niagara Falls would
    be great. It is a 45 minute drive from Hamburg and well worth the investment of
    time. Otherwise you have some beaches in Angola to enjoy. If the girls like to
    shop you have the Galleria, which is a great expansive Mall. If you enjoy a more
    eclectic afternoon, lunch on Elmwood Avenue, a stroll through the Albright Know
    Art gallery, and hitting some of the hip shops would be a cool afternoon. Darien
    Lake Theme Park is 40 minutes... |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| 552 | 5 | 根据您的时间安排，快速到尼亚加拉大瀑布旅行是一个很好的选择。从汉堡开车45分钟，非常值得投资时间。否则，您可以去安哥拉的一些海滩享受时光。如果女孩们喜欢购物，您可以去加勒利亚购物中心，这是一个非常大的商场。如果您喜欢一个更有特色的下午，可以在艾尔布赖特诺艺术画廊午餐，漫步艾尔姆伍德大道，然后逛逛一些时尚店铺，这将是一个很酷的下午。达里恩湖主题公园距离...'
- en: In the previous example, the original thread consisted of nine posts, two of
    which have been picked to summarize the thread, as shown earlier. Reading through
    the summary posts shows that the thread is about activities for youngsters, and
    there are already some specific suggestions, such as Elmwood Avenue, Darien Lake
    Theme Park, etc. Imagine that while scrolling through the forum search results,
    this information is provided on a mouse hover. It gives the user an accurate enough
    summary to decide whether it’s interesting and click through for more details
    or continue looking at other search results. You could also easily reuse this
    blueprint with other datasets as mentioned at the start and customize the distance
    function, introduce additional features, and then train the model.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，原始主题包括九个帖子，其中两个被选出来总结主题，如前所示。阅读总结帖子显示，主题是关于年轻人的活动，已经有一些具体建议，比如艾尔姆伍德大道，达里恩湖主题公园等。想象一下，在浏览论坛搜索结果时，鼠标悬停时提供这些信息。这为用户提供了足够准确的摘要，以决定是否有趣并单击获取更多详细信息或继续查看其他搜索结果。您还可以轻松地将此蓝图与其他数据集重新使用，如开头所述，并自定义距离函数，引入附加功能，然后训练模型。
- en: Closing Remarks
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结尾语
- en: In this chapter, we introduced the concept of text summarization and provided
    blueprints that can be used to generate summaries for different use cases. If
    you are looking to generate summaries from short text such as web pages, blogs,
    and news articles, then the first blueprint based on topic representation using
    the LSA summarizer would be a good choice. If you are working with much larger
    text such as speeches, book chapters, or scientific articles, then the blueprint
    using TextRank would be a better choice. These blueprints are great as the first
    step in your journey toward automatic text summarization as they are simple and
    fast. However, the third blueprint using machine learning provides a more custom
    solution for your specific use case. Provided you have the necessary annotated
    data, this method can be tailored by adding features and optimizing the machine
    learning model to improve performance. For example, your company or product might
    have multiple policy documents that govern user data, terms and conditions, and
    other such processes that you want to summarize for a new user or employee. You
    could start with the third blueprint and customize the second step by adding features
    such as the number of clauses, usage of block letters, presence of bold or underlined
    text, etc., that will help the model summarize the important points in the policy
    documents.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了文本摘要的概念，并提供了可用于为不同用例生成摘要的蓝图。如果您希望从诸如网页、博客和新闻文章等短文本生成摘要，则基于LSA摘要器的主题表示的第一个蓝图将是一个不错的选择。如果您处理的文本更大，例如演讲稿、书籍章节或科学文章，则使用TextRank的蓝图将是一个更好的选择。这些蓝图作为您迈向自动文本摘要的第一步非常棒，因为它们简单又快速。然而，使用机器学习的第三个蓝图为您的特定用例提供了更定制的解决方案。只要您拥有必要的标注数据，就可以通过添加特征和优化机器学习模型来定制此方法以提高性能。例如，您的公司或产品可能有多个管理用户数据、条款和条件以及其他流程的政策文件，您希望为新用户或员工总结这些文件的重要内容。您可以从第三个蓝图开始，并通过添加特征（例如从句数量、使用块字母、是否存在粗体或下划线文本等）来定制第二步，以帮助模型总结政策文件中的重要要点。
- en: Further Reading
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'Allahyari, Mehdi, et al. “Text Summarization Techniques: A Brief Survey.” [*https://arxiv.org/pdf/1707.02268.pdf*](https://arxiv.org/pdf/1707.02268.pdf).'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Allahyari, Mehdi等人提出了一份关于文本摘要技术的简要调查。[*https://arxiv.org/pdf/1707.02268.pdf*](https://arxiv.org/pdf/1707.02268.pdf)。
- en: Bhatia, Sumit, et al. “Summarizing Online Forum Discussions—Can Dialog Acts
    of Individual Messages Help?” [*http://sumitbhatia.net/papers/emnlp14.pdf*](http://sumitbhatia.net/papers/emnlp14.pdf).
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bhatia, Sumit等人提出了一种总结在线论坛讨论的方法——个体消息的对话行为是否有助于？[*http://sumitbhatia.net/papers/emnlp14.pdf*](http://sumitbhatia.net/papers/emnlp14.pdf)。
- en: Collins, Ed, et al. “A Supervised Approach to Extractive Summarisation of Scientific
    Papers.” [*https://arxiv.org/pdf/1706.03946.pdf*](https://arxiv.org/pdf/1706.03946.pdf).
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Collins, Ed等人提出了一种对科学论文进行提取式摘要的监督方法。[*https://arxiv.org/pdf/1706.03946.pdf*](https://arxiv.org/pdf/1706.03946.pdf)。
- en: Tarnpradab, Sansiri, et al. “Toward Extractive Summarization of Online ForumDiscussions
    via Hierarchical Attention Networks.” [*https://aaai.org/ocs/index.php/FLAIRS/FLAIRS17/paper/viewFile/15500/14945*](https://aaai.org/ocs/index.php/FLAIRS/FLAIRS17/paper/viewFile/15500/14945).
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tarnpradab, Sansiri等人提出了一种通过层次注意力网络实现在线论坛讨论摘要的方法。[*https://aaai.org/ocs/index.php/FLAIRS/FLAIRS17/paper/viewFile/15500/14945*](https://aaai.org/ocs/index.php/FLAIRS/FLAIRS17/paper/viewFile/15500/14945)。
- en: ^([1](ch09.xhtml#idm45634184456856-marker)) You can find more information about
    the package, including the usage guidelines that we used while designing this
    blueprint on [GitHub](https://oreil.ly/I0FMA).
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch09.xhtml#idm45634184456856-marker)) 您可以在[GitHub](https://oreil.ly/I0FMA)上找到有关该软件包的更多信息，包括我们在设计此蓝图时使用的使用指南。
- en: ^([2](ch09.xhtml#idm45634183488696-marker)) Sansiri Tarnpradab, et al. “Toward
    Extractive Summarization of Online ForumDiscussions via Hierarchical Attention
    Networks.” [*https://arxiv.org/abs/1805.10390*](https://arxiv.org/abs/1805.10390).
    See the [data set (*.zip*)](https://oreil.ly/cqU_O) as well.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch09.xhtml#idm45634183488696-marker)) Sansiri Tarnpradab等人提出了一种通过层次注意力网络实现在线论坛讨论摘要的方法。[*https://arxiv.org/abs/1805.10390*](https://arxiv.org/abs/1805.10390)。也可以查看[数据集（*.zip*）](https://oreil.ly/cqU_O)。
