["```py\nIn [1]: %matplotlib inline\n        import numpy as np\n        import matplotlib.pyplot as plt\n        plt.style.use('seaborn-whitegrid')\n        from scipy import stats\n```", "```py\nIn [2]: from sklearn.datasets import make_blobs\n        X, y = make_blobs(n_samples=50, centers=2,\n                          random_state=0, cluster_std=0.60)\n        plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn');\n```", "```py\nIn [3]: xfit = np.linspace(-1, 3.5)\n        plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n        plt.plot([0.6], [2.1], 'x', color='red', markeredgewidth=2, markersize=10)\n\n        for m, b in [(1, 0.65), (0.5, 1.6), (-0.2, 2.9)]:\n            plt.plot(xfit, m * xfit + b, '-k')\n\n        plt.xlim(-1, 3.5);\n```", "```py\nIn [4]: xfit = np.linspace(-1, 3.5)\n        plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n\n        for m, b, d in [(1, 0.65, 0.33), (0.5, 1.6, 0.55), (-0.2, 2.9, 0.2)]:\n            yfit = m * xfit + b\n            plt.plot(xfit, yfit, '-k')\n            plt.fill_between(xfit, yfit - d, yfit + d, edgecolor='none',\n                             color='lightgray', alpha=0.5)\n\n        plt.xlim(-1, 3.5);\n```", "```py\nIn [5]: from sklearn.svm import SVC # \"Support vector classifier\"\n        model = SVC(kernel='linear', C=1E10)\n        model.fit(X, y)\nOut[5]: SVC(C=10000000000.0, kernel='linear')\n```", "```py\nIn [6]: def plot_svc_decision_function(model, ax=None, plot_support=True):\n            \"\"\"Plot the decision function for a 2D SVC\"\"\"\n            if ax is None:\n                ax = plt.gca()\n            xlim = ax.get_xlim()\n            ylim = ax.get_ylim()\n\n            # create grid to evaluate model\n            x = np.linspace(xlim[0], xlim[1], 30)\n            y = np.linspace(ylim[0], ylim[1], 30)\n            Y, X = np.meshgrid(y, x)\n            xy = np.vstack([X.ravel(), Y.ravel()]).T\n            P = model.decision_function(xy).reshape(X.shape)\n\n            # plot decision boundary and margins\n            ax.contour(X, Y, P, colors='k',\n                       levels=[-1, 0, 1], alpha=0.5,\n                       linestyles=['--', '-', '--'])\n\n            # plot support vectors\n            if plot_support:\n                ax.scatter(model.support_vectors_[:, 0],\n                           model.support_vectors_[:, 1],\n                           s=300, linewidth=1, edgecolors='black',\n                           facecolors='none');\n            ax.set_xlim(xlim)\n            ax.set_ylim(ylim)\n```", "```py\nIn [7]: plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n        plot_svc_decision_function(model);\n```", "```py\nIn [8]: model.support_vectors_\nOut[8]: array([[0.44359863, 3.11530945],\n               [2.33812285, 3.43116792],\n               [2.06156753, 1.96918596]])\n```", "```py\nIn [9]: def plot_svm(N=10, ax=None):\n            X, y = make_blobs(n_samples=200, centers=2,\n                              random_state=0, cluster_std=0.60)\n            X = X[:N]\n            y = y[:N]\n            model = SVC(kernel='linear', C=1E10)\n            model.fit(X, y)\n\n            ax = ax or plt.gca()\n            ax.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n            ax.set_xlim(-1, 4)\n            ax.set_ylim(-1, 6)\n            plot_svc_decision_function(model, ax)\n\n        fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n        fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\n        for axi, N in zip(ax, [60, 120]):\n            plot_svm(N, axi)\n            axi.set_title('N = {0}'.format(N))\n```", "```py\nIn [10]: from ipywidgets import interact, fixed\n         interact(plot_svm, N=(10, 200), ax=fixed(None));\nOut[10]: interactive(children=(IntSlider(value=10, description='N', max=200, min=10),\n          > Output()), _dom_classes=('widget-...\n```", "```py\nIn [11]: from sklearn.datasets import make_circles\n         X, y = make_circles(100, factor=.1, noise=.1)\n\n         clf = SVC(kernel='linear').fit(X, y)\n\n         plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n         plot_svc_decision_function(clf, plot_support=False);\n```", "```py\nIn [12]: r = np.exp(-(X ** 2).sum(1))\n```", "```py\nIn [13]: from mpl_toolkits import mplot3d\n\n         ax = plt.subplot(projection='3d')\n         ax.scatter3D(X[:, 0], X[:, 1], r, c=y, s=50, cmap='autumn')\n         ax.view_init(elev=20, azim=30)\n         ax.set_xlabel('x')\n         ax.set_ylabel('y')\n         ax.set_zlabel('r');\n```", "```py\nIn [14]: clf = SVC(kernel='rbf', C=1E6)\n         clf.fit(X, y)\nOut[14]: SVC(C=1000000.0)\n```", "```py\nIn [15]: plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n         plot_svc_decision_function(clf)\n         plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1],\n                     s=300, lw=1, facecolors='none');\n```", "```py\nIn [16]: X, y = make_blobs(n_samples=100, centers=2,\n                           random_state=0, cluster_std=1.2)\n         plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn');\n```", "```py\nIn [17]: X, y = make_blobs(n_samples=100, centers=2,\n                           random_state=0, cluster_std=0.8)\n\n         fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n         fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\n\n         for axi, C in zip(ax, [10.0, 0.1]):\n             model = SVC(kernel='linear', C=C).fit(X, y)\n             axi.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n             plot_svc_decision_function(model, axi)\n             axi.scatter(model.support_vectors_[:, 0],\n                         model.support_vectors_[:, 1],\n                         s=300, lw=1, facecolors='none');\n             axi.set_title('C = {0:.1f}'.format(C), size=14)\n```", "```py\nIn [18]: from sklearn.datasets import fetch_lfw_people\n         faces = fetch_lfw_people(min_faces_per_person=60)\n         print(faces.target_names)\n         print(faces.images.shape)\nOut[18]: ['Ariel Sharon' 'Colin Powell' 'Donald Rumsfeld' 'George W Bush'\n          'Gerhard Schroeder' 'Hugo Chavez' 'Junichiro Koizumi' 'Tony Blair']\n         (1348, 62, 47)\n```", "```py\nIn [19]: fig, ax = plt.subplots(3, 5, figsize=(8, 6))\n         for i, axi in enumerate(ax.flat):\n             axi.imshow(faces.images[i], cmap='bone')\n             axi.set(xticks=[], yticks=[],\n                     xlabel=faces.target_names[faces.target[i]])\n```", "```py\nIn [20]: from sklearn.svm import SVC\n         from sklearn.decomposition import PCA\n         from sklearn.pipeline import make_pipeline\n\n         pca = PCA(n_components=150, whiten=True,\n                   svd_solver='randomized', random_state=42)\n         svc = SVC(kernel='rbf', class_weight='balanced')\n         model = make_pipeline(pca, svc)\n```", "```py\nIn [21]: from sklearn.model_selection import train_test_split\n         Xtrain, Xtest, ytrain, ytest = train_test_split(faces.data, faces.target,\n                                                         random_state=42)\n```", "```py\nIn [22]: from sklearn.model_selection import GridSearchCV\n         param_grid = {'svc__C': [1, 5, 10, 50],\n                       'svc__gamma': [0.0001, 0.0005, 0.001, 0.005]}\n         grid = GridSearchCV(model, param_grid)\n\n         %time grid.fit(Xtrain, ytrain)\n         print(grid.best_params_)\nOut[22]: CPU times: user 1min 19s, sys: 8.56 s, total: 1min 27s\n         Wall time: 36.2 s\n         {'svc__C': 10, 'svc__gamma': 0.001}\n```", "```py\nIn [23]: model = grid.best_estimator_\n         yfit = model.predict(Xtest)\n```", "```py\nIn [24]: fig, ax = plt.subplots(4, 6)\n         for i, axi in enumerate(ax.flat):\n             axi.imshow(Xtest[i].reshape(62, 47), cmap='bone')\n             axi.set(xticks=[], yticks=[])\n             axi.set_ylabel(faces.target_names[yfit[i]].split()[-1],\n                            color='black' if yfit[i] == ytest[i] else 'red')\n         fig.suptitle('Predicted Names; Incorrect Labels in Red', size=14);\n```", "```py\nIn [25]: from sklearn.metrics import classification_report\n         print(classification_report(ytest, yfit,\n                                     target_names=faces.target_names))\nOut[25]:                    precision    recall  f1-score   support\n\n              Ariel Sharon       0.65      0.73      0.69        15\n              Colin Powell       0.80      0.87      0.83        68\n           Donald Rumsfeld       0.74      0.84      0.79        31\n             George W Bush       0.92      0.83      0.88       126\n         Gerhard Schroeder       0.86      0.83      0.84        23\n               Hugo Chavez       0.93      0.70      0.80        20\n         Junichiro Koizumi       0.92      1.00      0.96        12\n                Tony Blair       0.85      0.95      0.90        42\n\n                  accuracy                           0.85       337\n                 macro avg       0.83      0.84      0.84       337\n              weighted avg       0.86      0.85      0.85       337\n```", "```py\nIn [26]: from sklearn.metrics import confusion_matrix\n         import seaborn as sns\n         mat = confusion_matrix(ytest, yfit)\n         sns.heatmap(mat.T, square=True, annot=True, fmt='d',\n                     cbar=False, cmap='Blues',\n                     xticklabels=faces.target_names,\n                     yticklabels=faces.target_names)\n         plt.xlabel('true label')\n         plt.ylabel('predicted label');\n```"]