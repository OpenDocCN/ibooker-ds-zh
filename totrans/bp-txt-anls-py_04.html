<html><head></head><body><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 4. Preparing Textual Data for Statistics and Machine Learning"><div class="chapter" id="ch-preparation">
<h1><span class="label">Chapter 4. </span>Preparing Textual Data for Statistics and Machine Learning</h1>

<p>Technically, any text document is just a sequence of characters. To build models on the content, we need to transform a text into a sequence of words or, more generally, meaningful sequences of characters called <em>tokens</em>. But that alone is not sufficient. Think of the word sequence <em>New York</em>, which should be treated as a single named-entity. Correctly identifying such word sequences as compound structures requires sophisticated linguistic processing.</p>

<p>Data preparation or <a contenteditable="false" data-type="indexterm" data-primary="data preprocessing" data-secondary="pipelines for" id="ch4_term1"/><a contenteditable="false" data-type="indexterm" data-primary="pipelines" id="ch4_term2"/><a contenteditable="false" data-type="indexterm" data-primary="text data preparation" data-secondary="about" id="ch4_term3"/>data preprocessing in general involves not only the transformation of data into a form that can serve as the basis for analysis but also the removal of disturbing noise. What’s noise and what isn’t always depends on the analysis you are going to perform. When working with text, noise comes in different flavors. The raw data may include HTML tags or special characters that should be removed in most cases. But frequent words carrying little meaning, the so-called <em>stop words</em>, introduce noise into machine learning and data analysis because they make it harder to detect patterns.</p>

<section data-type="sect1" data-pdf-bookmark="What You’ll Learn and What We’ll Build"><div class="sect1" id="idm45634203405624">
<h1>What You’ll Learn and What We’ll Build</h1>

<p>In this chapter, we will develop blueprints for a text preprocessing pipeline. The pipeline will take the raw text as input, clean it, transform it, and extract the basic features of textual content. We start with regular expressions for data cleaning and tokenization and then focus on <a contenteditable="false" data-type="indexterm" data-primary="spaCy, linguistic processing with" data-secondary="about" id="idm45634203404024"/>linguistic processing with <em>spaCy</em>. spaCy is a powerful NLP library with a modern API and state-of-the-art models. For some operations we will make use of <em>textacy</em>, a library that provides some nice add-on functionality especially for data preprocessing. We will also point to NLTK and other libraries whenever it appears helpful.</p>

<p>After studying this chapter, you will know the required and optional steps of data preparation. You will know how to use regular expressions for data cleaning and how to use spaCy for feature extraction. With the provided blueprints you will be able to quickly set up a data preparation pipeline for your own project.</p>
</div></section>

<section data-type="sect1" data-pdf-bookmark="A Data Preprocessing Pipeline"><div class="sect1" id="idm45634203400392">
<h1>A Data Preprocessing Pipeline</h1>
<p>Data preprocessing usually involves a sequence of steps. Often, this sequence is called a <em>pipeline</em> because you feed raw data into the pipeline and get the transformed and preprocessed data out of it. In <a data-type="xref" href="ch01.xhtml#ch-exploration">Chapter 1</a> we already built a simple data processing pipeline including tokenization and stop word removal. We will use the term <em>pipeline</em> in this chapter as a general term for a sequence of processing steps. <a data-type="xref" href="#fig-pipeline">Figure 4-1</a> gives an overview of the blueprints we are going to build for the preprocessing pipeline in this chapter.</p><figure><div id="fig-pipeline" class="figure">
<img src="Images/btap_0401.jpg" width="1444" height="426"/>
<h6><span class="label">Figure 4-1. </span>A pipeline with typical preprocessing steps for textual data.</h6></div></figure>

<p>The first <a contenteditable="false" data-type="indexterm" data-primary="data cleaning" id="idm45634203393464"/><a contenteditable="false" data-type="indexterm" data-primary="text data preparation" data-secondary="cleaning processes for" id="idm45634203392328"/>major block of operations in our pipeline is <em>data cleaning</em>. We start by identifying and removing noise in text like HTML tags and nonprintable characters. <a contenteditable="false" data-type="indexterm" data-primary="normalization of text" id="idm45634203390296"/><a contenteditable="false" data-type="indexterm" data-primary="character normalization" id="idm45634203389192"/>During <em>character normalization</em>, special characters such as accents and hyphens are transformed into a standard representation. Finally, we can mask or remove identifiers like URLs or email addresses if they are not relevant for the analysis or if there are privacy issues. Now the text is clean enough to start linguistic processing.</p>

<p>Here, <em>tokenization</em> splits a document into a list of separate tokens like words and punctuation <a contenteditable="false" data-type="indexterm" data-primary="part-of-speech tags" id="idm45634203386232"/>characters. <em>Part-of-speech (POS) tagging</em> is the process of determining the word class, whether it’s a noun, a verb, an <a contenteditable="false" data-type="indexterm" data-primary="lemmatization" id="idm45634203384552"/>article, etc. <em>Lemmatization</em> maps inflected words to their uninflected root, the lemma (e.g., “are” → “be”). The <a contenteditable="false" data-type="indexterm" data-primary="spaCy, linguistic processing with" data-secondary="for named-entity recognition" data-secondary-sortas="named-entity recognition" id="idm45634203382792"/><a contenteditable="false" data-type="indexterm" data-primary="named-entity recognition (NER)" id="idm45634203381144"/>target of <em>named-entity recognition</em> is the identification of references to people, organizations, locations, etc., in the text.</p>

<p>In the end, we want to create a database with preprared data ready for analysis and machine learning. Thus, the required preparation steps vary from project to project. It’s up to you to decide which of the <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch4_term1" id="idm45634203378856"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch4_term2" id="idm45634203377384"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch4_term3" id="idm45634203376008"/>following blueprints you need to include in your problem-specific pipeline. </p>

</div></section>

<section data-type="sect1" data-pdf-bookmark="Introducing the Dataset: Reddit Self-Posts"><div class="sect1" id="idm45634203374376">
<h1>Introducing the Dataset: Reddit Self-Posts</h1>
<p>The preparation <a contenteditable="false" data-type="indexterm" data-primary="datasets, examples of" data-secondary="Reddit Self-Posts" id="ch4_term4"/><a contenteditable="false" data-type="indexterm" data-primary="Reddit Self-Posts " id="ch4_term5"/><a contenteditable="false" data-type="indexterm" data-primary="text data preparation" data-secondary="with Reddit Self-Posts" data-secondary-sortas="Reddit Self-Posts" id="ch4_term6"/>of textual data is particularly challenging when you work with <a contenteditable="false" data-type="indexterm" data-primary="user-generated content (UGC)" id="idm45634203367512"/>user-generated content (UGC). In contrast to well-redacted text from professional reports, news, and blogs, user contributions in social media usually are short and contain lots of abbreviations, hashtags, emojis, and typos. Thus, we will use the <a href="https://oreil.ly/0pnqO">Reddit Self-Posts</a> dataset, which is <a contenteditable="false" data-type="indexterm" data-primary="datasets, examples of" data-secondary="from Kaggle" data-secondary-sortas="Kaggle" id="idm45634203365288"/>hosted on Kaggle. The complete dataset contains roughly 1 million user posts with title and content, arranged in 1,013 different subreddits, each of which has 1,000 records. We will use a subset of only 20,000 posts contained in the autos category. The dataset we prepare in this chapter is the basis for the analysis of word embeddings in <a data-type="xref" href="ch10.xhtml#ch-embeddings">Chapter 10</a>.</p>

<section data-type="sect2" data-pdf-bookmark="Loading Data Into Pandas"><div class="sect2" id="idm45634203362168">
<h2>Loading Data Into Pandas</h2>
<p>The original dataset consists of two separate CSV files, one with the posts and the other one with some metadata for the subreddits, including category information. Both <a contenteditable="false" data-type="indexterm" data-primary="Pandas library" data-secondary="dataframes in" id="idm45634203360648"/><a contenteditable="false" data-type="indexterm" data-primary="DataFrame (Pandas)" id="idm45634203359176"/>files are loaded into a Pandas <code>DataFrame</code> by <code>pd.read_csv()</code> and then joined into a single <code>DataFrame</code>.</p>

<pre data-code-language="python" data-type="programlisting">
<code class="kn">import</code> <code class="nn">pandas</code> <code class="kn">as</code> <code class="nn">pd</code>

<code class="n">posts_file</code> <code class="o">=</code> <code class="s2">"rspct.tsv.gz"</code>
<code class="n">posts_df</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">read_csv</code><code class="p">(</code><code class="n">posts_file</code><code class="p">,</code> <code class="n">sep</code><code class="o">=</code><code class="s1">'</code><code class="se">\t</code><code class="s1">'</code><code class="p">)</code>

<code class="n">subred_file</code> <code class="o">=</code> <code class="s2">"subreddit_info.csv.gz"</code>
<code class="n">subred_df</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">read_csv</code><code class="p">(</code><code class="n">subred_file</code><code class="p">)</code><code class="o">.</code><code class="n">set_index</code><code class="p">([</code><code class="s1">'subreddit'</code><code class="p">])</code>

<code class="n">df</code> <code class="o">=</code> <code class="n">posts_df</code><code class="o">.</code><code class="n">join</code><code class="p">(</code><code class="n">subred_df</code><code class="p">,</code> <code class="n">on</code><code class="o">=</code><code class="s1">'subreddit'</code><code class="p">)</code>
</pre>
</div></section>

<section data-type="sect2" class="blueprint" data-pdf-bookmark="Blueprint: Standardizing Attribute Names"><div class="sect2" id="idm45634203354568">
<h2>Blueprint: Standardizing Attribute Names</h2>
<p>Before we start working with the data, we <a contenteditable="false" data-type="indexterm" data-primary="text data preparation" data-secondary="standardizing attribute names for" id="idm45634203298824"/>will change the dataset-specific column names to more generic names. We recommend always naming the main <code>DataFrame</code> <code>df</code>, and naming the column with the text to analyze <code>text</code>. Such naming conventions for common variables and attribute names make it easier to reuse the code of the blueprints in different projects.</p>

<p>Let’s take a look at the columns list of this dataset:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="k">print</code><code class="p">(</code><code class="n">df</code><code class="o">.</code><code class="n">columns</code><code class="p">)</code>
</pre>

<p><code>Out:</code></p>

<pre data-type="programlisting">
Index(['id', 'subreddit', 'title', 'selftext', 'category_1', 'category_2',
       'category_3', 'in_data', 'reason_for_exclusion'],
      dtype='object')
</pre>

<p>For column renaming and selection, we define a dictionary <code>column_mapping</code> where each entry defines a mapping from the current column name to a new name. Columns mapped to <code>None</code> and unmentioned columns are dropped. A dictionary is perfect documentation for such a transformation and easy to reuse. This dictionary is then used to select and rename the columns that we want to keep.</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">column_mapping</code> <code class="o">=</code> <code class="p">{</code>
    <code class="s1">'id'</code><code class="p">:</code> <code class="s1">'id'</code><code class="p">,</code>
    <code class="s1">'subreddit'</code><code class="p">:</code> <code class="s1">'subreddit'</code><code class="p">,</code>
    <code class="s1">'title'</code><code class="p">:</code> <code class="s1">'title'</code><code class="p">,</code>
    <code class="s1">'selftext'</code><code class="p">:</code> <code class="s1">'text'</code><code class="p">,</code>
    <code class="s1">'category_1'</code><code class="p">:</code> <code class="s1">'category'</code><code class="p">,</code>
    <code class="s1">'category_2'</code><code class="p">:</code> <code class="s1">'subcategory'</code><code class="p">,</code>
    <code class="s1">'category_3'</code><code class="p">:</code> <code class="bp">None</code><code class="p">,</code> <code class="c1"># no data</code>
    <code class="s1">'in_data'</code><code class="p">:</code> <code class="bp">None</code><code class="p">,</code> <code class="c1"># not needed</code>
    <code class="s1">'reason_for_exclusion'</code><code class="p">:</code> <code class="bp">None</code> <code class="c1"># not needed</code>
<code class="p">}</code>

<code class="c1"># define remaining columns</code>
<code class="n">columns</code> <code class="o">=</code> <code class="p">[</code><code class="n">c</code> <code class="k">for</code> <code class="n">c</code> <code class="ow">in</code> <code class="n">column_mapping</code><code class="o">.</code><code class="n">keys</code><code class="p">()</code> <code class="k">if</code> <code class="n">column_mapping</code><code class="p">[</code><code class="n">c</code><code class="p">]</code> <code class="o">!=</code> <code class="bp">None</code><code class="p">]</code>

<code class="c1"># select and rename those columns</code>
<code class="n">df</code> <code class="o">=</code> <code class="n">df</code><code class="p">[</code><code class="n">columns</code><code class="p">]</code><code class="o">.</code><code class="n">rename</code><code class="p">(</code><code class="n">columns</code><code class="o">=</code><code class="n">column_mapping</code><code class="p">)</code>
</pre>

<p>As already mentioned, we limit the data to the autos category:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">df</code> <code class="o">=</code> <code class="n">df</code><code class="p">[</code><code class="n">df</code><code class="p">[</code><code class="s1">'category'</code><code class="p">]</code> <code class="o">==</code> <code class="s1">'autos'</code><code class="p">]</code>
</pre>

<p>Let’s take a brief look at a sample record to get a first impression of the data:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">df</code><code class="o">.</code><code class="n">sample</code><code class="p">(</code><code class="mi">1</code><code class="p">)</code><code class="o">.</code><code class="n">T</code>
</pre>



<table class="dataframe tex2jax_ignore">
  <thead>
    <tr>
      <th/>
      <th>14356</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>id</th>
      <td>7jc2k4</td>
    </tr>
    <tr>
      <th>subreddit</th>
      <td>volt</td>
    </tr>
    <tr>
      <th>title</th>
      <td>Dashcam for 2017 volt</td>
    </tr>
    <tr>
      <th>text</th>
      <td>Hello.&lt;lb&gt;I’m looking into getting a dashcam. &lt;lb&gt;Does anyone have any recommendations? &lt;lb&gt;&lt;lb&gt;I’m generally looking for a rechargeable one so that I don’t have to route wires down to the cigarette lighter. &lt;lb&gt;Unless there are instructions on how to wire it properly without wires showing. &lt;lb&gt;&lt;lb&gt;&lt;lb&gt;Thanks!</td>
    </tr>
    <tr>
      <th>category</th>
      <td>autos</td>
    </tr>
    <tr>
      <th>subcategory</th>
      <td>chevrolet</td>
    </tr>
  </tbody>
</table>

</div></section>

<section data-type="sect2" data-pdf-bookmark="Saving and Loading a DataFrame"><div class="sect2" id="idm45634203300040">
<h2>Saving and Loading a DataFrame</h2>
<p>After <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch4_term4" id="idm45634203060808"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch4_term5" id="idm45634203059080"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch4_term6" id="idm45634203057704"/>each step of data preparation, it is <a contenteditable="false" data-type="indexterm" data-primary="Pandas library" data-secondary="dataframes in" id="idm45634203056200"/><a contenteditable="false" data-type="indexterm" data-primary="DataFrame (Pandas)" id="idm45634203054824"/><a contenteditable="false" data-type="indexterm" data-primary="copies for traceability" id="idm45634203037352"/><a contenteditable="false" data-type="indexterm" data-primary="backups" id="idm45634203036248"/><a contenteditable="false" data-type="indexterm" data-primary="pickle serialization framework of Python" id="idm45634203035144"/><a contenteditable="false" data-type="indexterm" data-primary="text data preparation" data-secondary="saving dataframes during" id="idm45634203033944"/>helpful to write the respective <code>DataFrame</code> to disk as a checkpoint. Pandas directly supports a number of <a href="https://oreil.ly/VaXTx">serialization options</a>. Text-based formats like CSV or <a contenteditable="false" data-type="indexterm" data-primary="JSON format" id="idm45634203031368"/>JSON can be imported into most other tools easily. However, information about data types is lost (CSV) or only saved rudimentarily (JSON). The standard <a contenteditable="false" data-type="indexterm" data-primary="serialization format of Python" id="idm45634203029960"/>serialization format of Python, <code>pickle</code>, is supported by Pandas and therefore a viable option. It is fast and preserves all information but can only be processed by Python. “Pickling” a data frame is easy; you just need to specify the filename:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">df</code><code class="o">.</code><code class="n">to_pickle</code><code class="p">(</code><code class="s2">"reddit_dataframe.pkl"</code><code class="p">)</code>
</pre>

<p>We prefer, however, storing dataframes in <a contenteditable="false" data-type="indexterm" data-primary="SQL databases" id="idm45634203025128"/>SQL databases because they give you all the advantages of SQL, including filters, joins, and easy access from many tools. But in contrast to <code>pickle</code>, only SQL data types are supported. Columns containing objects or lists, for example, cannot simply be saved this way and need to be serialized <span class="keep-together">manually</span>.</p>

<p>In our examples, we will use <a contenteditable="false" data-type="indexterm" data-primary="SQLite for dataframes" id="idm45634203020888"/>SQLite to persist data frames. SQLite is well integrated with Python. Moreover, it’s just a library and does not require a server, so the files are self-contained and can be exchanged between different team members easily. For more power and safety, we recommend a server-based SQL database.</p>

<p>We use <code>pd.to_sql()</code> to save our <code>DataFrame</code> as table <code>posts</code> into an SQLite database. The <code>DataFrame</code> index is not stored, and any existing data is overwritten:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="kn">import</code> <code class="nn">sqlite3</code>

<code class="n">db_name</code> <code class="o">=</code> <code class="s2">"reddit-selfposts.db"</code>
<code class="n">con</code> <code class="o">=</code> <code class="n">sqlite3</code><code class="o">.</code><code class="n">connect</code><code class="p">(</code><code class="n">db_name</code><code class="p">)</code>
<code class="n">df</code><code class="o">.</code><code class="n">to_sql</code><code class="p">(</code><code class="s2">"posts"</code><code class="p">,</code> <code class="n">con</code><code class="p">,</code> <code class="n">index</code><code class="o">=</code><code class="bp">False</code><code class="p">,</code> <code class="n">if_exists</code><code class="o">=</code><code class="s2">"replace"</code><code class="p">)</code>
<code class="n">con</code><code class="o">.</code><code class="n">close</code><code class="p">()</code>
</pre>

<p>The <code>DataFrame</code> can be easily restored with <code>pd.read_sql()</code>:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">con</code> <code class="o">=</code> <code class="n">sqlite3</code><code class="o">.</code><code class="n">connect</code><code class="p">(</code><code class="n">db_name</code><code class="p">)</code>
<code class="n">df</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">read_sql</code><code class="p">(</code><code class="s2">"select * from posts"</code><code class="p">,</code> <code class="n">con</code><code class="p">)</code>
<code class="n">con</code><code class="o">.</code><code class="n">close</code><code class="p">()</code>
</pre>
</div></section>
</div></section>

<section data-type="sect1" class="pagebreak-before less_space" data-pdf-bookmark="Cleaning Text Data"><div class="sect1" id="ch4-cleaning">
<h1>Cleaning Text Data</h1>
<p>When working with <a contenteditable="false" data-type="indexterm" data-primary="cleaning text data" id="ch4_term7"/><a contenteditable="false" data-type="indexterm" data-primary="data cleaning" id="ch4_term8"/><a contenteditable="false" data-type="indexterm" data-primary="text data preparation" data-secondary="cleaning processes for" id="ch4_term10"/>user requests or comments as opposed to well-edited articles, you usually have to deal with a number of quality issues:</p>
<dl>
<dt>Special formatting and program code</dt>
<dd> The text may <a contenteditable="false" data-type="indexterm" data-primary="text data preparation" data-secondary="noise identification and removal for" id="ch4_term11"/>still contain special characters, HTML entities, Markdown tags, and things like that. These artifacts should be cleaned in advance because they complicate tokenization and introduce noise.</dd>
<dt>Salutations, signatures, addresses, etc.</dt>
<dd> Personal communication often contains meaningless polite phrases and salutations by name that are usually irrelevant for the analysis.</dd>
<dt>Replies</dt>
<dd> If your text contains answers repeating the question text, you need to delete the duplicate questions. Keeping them will distort any model and statistics.</dd>
</dl>
<p>In this section, we will <a contenteditable="false" data-type="indexterm" data-primary="regular expressions" id="ch4_term12"/>demonstrate how to use regular expressions to identify and remove unwanted patterns in the data. Check out the following sidebar for some more details on regular expressions in Python.</p>

<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45634202942120">
<h5>Regular Expressions</h5>

<p>Regular expressions are an essential tool for text data preparation. They can be used not only for tokenization and data cleaning but also for the identification and treatment of email addresses, salutations, program code, and more. </p>

<p>Python <a contenteditable="false" data-type="indexterm" data-primary="re library (Python)" id="idm45634202939880"/>has the standard library <code>re</code> for regular expressions and the <a contenteditable="false" data-type="indexterm" data-primary="regex library (Python)" id="idm45634202938200"/>newer, backward-compatible library <a href="https://oreil.ly/x2dYf">regex</a> that offers support for POSIX character classes and some more flexibility. </p>

<p>A good overview about the available meta-characters like <code>^</code> as well as character classes like <code>\w</code> is available at <a href="https://oreil.ly/pZgG1">W3Schools</a>. There is also a number of interactive websites to develop and test regular expressions, e.g., <a href="https://regex101.com"><em>https://regex101.com</em></a> (make sure to set the flavor to Python). </p>

<p>In many packages, you will find the precompiled regular expressions like this:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">RE_BRACKET</code> <code class="o">=</code> <code class="n">re</code><code class="o">.</code><code class="n">compile</code><code class="p">(</code><code class="s1">'\[[^\[\]]*\]'</code><code class="p">)</code>
<code class="n">text</code> <code class="o">=</code> <code class="n">RE_BRACKET</code><code class="o">.</code><code class="n">sub</code><code class="p">(</code><code class="s1">' '</code><code class="p">,</code> <code class="n">text</code><code class="p">)</code></pre>

<p>Precompilation was originally a mechanism to improve performance, but modern Python automatically caches compiled versions of regular expressions. However, it still gives some benefit for frequently accessed expressions and improves readability.</p></div></aside>

<p>Take a look at the following text example from the Reddit dataset:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">text</code> <code class="o">=</code> <code class="s2">"""</code>
<code class="s2">After viewing the [PINKIEPOOL Trailer](https://www.youtu.be/watch?v=ieHRoHUg)</code>
<code class="s2">it got me thinking about the best match ups.</code>
<code class="s2">&lt;lb&gt;Here's my take:&lt;lb&gt;&lt;lb&gt;[](/sp)[](/ppseesyou) Deadpool&lt;lb&gt;[](/sp)[](/ajsly)</code>
<code class="s2">Captain America&lt;lb&gt;"""</code>
</pre>

<p>It will definitely improve the results if this text gets some cleaning and polishing. Some tags are just artifacts from web scraping, so we will get rid of them. And as we are not interested in the URLs and other links, we will discard them as well.</p>


<section data-type="sect2" class="blueprint" data-pdf-bookmark="Blueprint: Identify Noise with Regular Expressions"><div class="sect2" id="idm45634202837544">
<h2>Blueprint: Identify Noise with Regular Expressions</h2>
<p>The identification <a contenteditable="false" data-type="indexterm" data-primary="noise in text data, identifying and removing" id="ch4_term13"/>of quality problems in a big dataset can be tricky. Of course, you can and should take a look at a sample of the data. But <span class="keep-together">the probability</span> is high that you won’t find all the issues. It is better to <span class="keep-together">define rough</span> patterns indicating likely problems and check the complete dataset <span class="keep-together">programmatically</span>. </p>

<p>The following function can help you to identify noise in textual data. By <em>noise</em> we mean everything that’s not plain text and may therefore disturb further analysis. The function uses a regular expression to search for a number of suspicious characters and returns their share of all characters as a score for impurity. Very short texts (less than <code>min_len</code> characters) are ignored because here a single special character would lead to a significant impurity and distort the result.</p>

<pre data-code-language="python" data-type="programlisting">
<code class="kn">import</code> <code class="nn">re</code>

<code class="n">RE_SUSPICIOUS</code> <code class="o">=</code> <code class="n">re</code><code class="o">.</code><code class="n">compile</code><code class="p">(</code><code class="s-Affix">r</code><code class="s1">'[&amp;#&lt;&gt;{}\[\]</code><code class="se">\\</code><code class="s1">]'</code><code class="p">)</code>

<code class="k">def</code> <code class="nf">impurity</code><code class="p">(</code><code class="n">text</code><code class="p">,</code> <code class="n">min_len</code><code class="o">=</code><code class="mi">10</code><code class="p">):</code>
    <code class="sd">"""returns the share of suspicious characters in a text"""</code>
    <code class="k">if</code> <code class="n">text</code> <code class="o">==</code> <code class="bp">None</code> <code class="ow">or</code> <code class="nb">len</code><code class="p">(</code><code class="n">text</code><code class="p">)</code> <code class="o">&lt;</code> <code class="n">min_len</code><code class="p">:</code>
        <code class="k">return</code> <code class="mi">0</code>
    <code class="k">else</code><code class="p">:</code>
        <code class="k">return</code> <code class="nb">len</code><code class="p">(</code><code class="n">RE_SUSPICIOUS</code><code class="o">.</code><code class="n">findall</code><code class="p">(</code><code class="n">text</code><code class="p">))</code><code class="o">/</code><code class="nb">len</code><code class="p">(</code><code class="n">text</code><code class="p">)</code>

<code class="k">print</code><code class="p">(</code><code class="n">impurity</code><code class="p">(</code><code class="n">text</code><code class="p">))</code>
</pre>

<p><code>Out:</code></p>

<pre data-type="programlisting">
0.09009009009009009
</pre>

<p>You almost never find these characters in well-redacted text, so the scores in general should be very small. For the previous example text, about 9% of the characters <span class="keep-together">are “suspicious”</span> according to our definition. The search pattern may of course <span class="keep-together">need adaption</span> for corpora containing hashtags or similar tokens containing special <span class="keep-together">characters</span>. However, it doesn’t need to be perfect; it just needs to be good enough to indicate potential quality issues. </p>

<p>For the Reddit data, we can get the most “impure” records with the following two statements. Note that we use <a contenteditable="false" data-type="indexterm" data-primary="apply function" id="idm45634202717432"/>Pandas <code>apply()</code> instead of the similar <code>map()</code> because it allows us to forward additional parameters like <code>min_len</code> to the applied function.<sup><a data-type="noteref" id="idm45634202714920-marker" href="ch04.xhtml#idm45634202714920">1</a></sup> </p>

<pre data-code-language="python" data-type="programlisting">
<code class="c1"># add new column to data frame</code>
<code class="n">df</code><code class="p">[</code><code class="s1">'impurity'</code><code class="p">]</code> <code class="o">=</code> <code class="n">df</code><code class="p">[</code><code class="s1">'text'</code><code class="p">]</code><code class="o">.</code><code class="n">apply</code><code class="p">(</code><code class="n">impurity</code><code class="p">,</code> <code class="n">min_len</code><code class="o">=</code><code class="mi">10</code><code class="p">)</code>

<code class="c1"># get the top 3 records</code>
<code class="n">df</code><code class="p">[[</code><code class="s1">'text'</code><code class="p">,</code> <code class="s1">'impurity'</code><code class="p">]]</code><code class="o">.</code><code class="n">sort_values</code><code class="p">(</code><code class="n">by</code><code class="o">=</code><code class="s1">'impurity'</code><code class="p">,</code> <code class="n">ascending</code><code class="o">=</code><code class="bp">False</code><code class="p">)</code><code class="o">.</code><code class="n">head</code><code class="p">(</code><code class="mi">3</code><code class="p">)</code>
</pre>



<table class="dataframe tex2jax_ignore">
  <thead>
    <tr>
      <th/>
      <th>text</th>
      <th>impurity</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>19682</th>
      <td>Looking at buying a 335i with 39k miles and 11 months left on the CPO warranty. I asked the deal...</td>
      <td>0.21</td>
    </tr>
    <tr>
      <th>12357</th>
      <td>I’m looking to lease an a4 premium plus automatic with the nav package.&lt;lb&gt;&lt;lb&gt;Vehicle Price:&lt;ta...</td>
      <td>0.17</td>
    </tr>
    <tr>
      <th>2730</th>
      <td>Breakdown below:&lt;lb&gt;&lt;lb&gt;Elantra GT&lt;lb&gt;&lt;lb&gt;2.0L 4-cylinder&lt;lb&gt;&lt;lb&gt;6-speed Manual Transmission&lt;lb&gt;...</td>
      <td>0.14</td>
    </tr>
  </tbody>
</table>


<p>Obviously, there are many tags like <code>&lt;lb&gt;</code> (linebreak) and <code>&lt;tab&gt;</code> included. Let’s check if there are others by utilizing our <a contenteditable="false" data-type="indexterm" data-primary="count_words function" id="idm45634202625208"/>word count blueprint from <a data-type="xref" href="ch01.xhtml#ch-exploration">Chapter 1</a> in combination with a simple regex tokenizer for such tags:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="kn">from</code> <code class="nn">blueprints.exploration</code> <code class="kn">import</code> <code class="n">count_words</code>
<code class="n">count_words</code><code class="p">(</code><code class="n">df</code><code class="p">,</code> <code class="n">column</code><code class="o">=</code><code class="s1">'text'</code><code class="p">,</code> <code class="n">preprocess</code><code class="o">=</code><code class="k">lambda</code> <code class="n">t</code><code class="p">:</code> <code class="n">re</code><code class="o">.</code><code class="n">findall</code><code class="p">(</code><code class="s-Affix">r</code><code class="s1">'&lt;[\w/]*&gt;'</code><code class="p">,</code> <code class="n">t</code><code class="p">))</code>
</pre>



<table class="dataframe tex2jax_ignore">
  <thead>
    <tr>
      <th>freq</th>
      <th>token</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>&lt;lb&gt;</th>
      <td>100729</td>
    </tr>
    <tr>
      <th>&lt;tab&gt;</th>
      <td>642</td>
    </tr>
  </tbody>
</table>


<p>Now we know that although these two tags are common, they are the only ones.</p>
</div></section>

<section data-type="sect2" class="blueprint" data-pdf-bookmark="Blueprint: Removing Noise with Regular Expressions"><div class="sect2" id="ch04removenoiseregex">
<h2>Blueprint: Removing Noise with Regular Expressions</h2>
<p>Our approach to data cleaning consists of defining a set of regular expressions and identifying problematic patterns and corresponding substitution rules.<sup><a data-type="noteref" id="idm45634202565272-marker" href="ch04.xhtml#idm45634202565272">2</a></sup> The blueprint function first substitutes all HTML escapes (e.g., <code>&amp;amp;</code>) by their plain-text representation and then replaces certain patterns by spaces. Finally, sequences of whitespaces are pruned:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="kn">import</code> <code class="nn">html</code>

<code class="k">def</code> <code class="nf">clean</code><code class="p">(</code><code class="n">text</code><code class="p">):</code>
    <code class="c1"># convert html escapes like &amp;amp; to characters.</code>
    <code class="n">text</code> <code class="o">=</code> <code class="n">html</code><code class="o">.</code><code class="n">unescape</code><code class="p">(</code><code class="n">text</code><code class="p">)</code>
    <code class="c1"># tags like &lt;tab&gt;</code>
    <code class="n">text</code> <code class="o">=</code> <code class="n">re</code><code class="o">.</code><code class="n">sub</code><code class="p">(</code><code class="s-Affix">r</code><code class="s1">'&lt;[^&lt;&gt;]*&gt;'</code><code class="p">,</code> <code class="s1">' '</code><code class="p">,</code> <code class="n">text</code><code class="p">)</code>
    <code class="c1"># markdown URLs like [Some text](https://....)</code>
    <code class="n">text</code> <code class="o">=</code> <code class="n">re</code><code class="o">.</code><code class="n">sub</code><code class="p">(</code><code class="s-Affix">r</code><code class="s1">'\[([^\[\]]*)\]\([^\(\)]*\)'</code><code class="p">,</code> <code class="s-Affix">r</code><code class="s1">'\1'</code><code class="p">,</code> <code class="n">text</code><code class="p">)</code>
    <code class="c1"># text or code in brackets like [0]</code>
    <code class="n">text</code> <code class="o">=</code> <code class="n">re</code><code class="o">.</code><code class="n">sub</code><code class="p">(</code><code class="s-Affix">r</code><code class="s1">'\[[^\[\]]*\]'</code><code class="p">,</code> <code class="s1">' '</code><code class="p">,</code> <code class="n">text</code><code class="p">)</code>
    <code class="c1"># standalone sequences of specials, matches &amp;# but not #cool</code>
    <code class="n">text</code> <code class="o">=</code> <code class="n">re</code><code class="o">.</code><code class="n">sub</code><code class="p">(</code><code class="s-Affix">r</code><code class="s1">'(?:^|\s)[&amp;#&lt;&gt;{}\[\]+|</code><code class="se">\\</code><code class="s1">:-]{1,}(?:\s|$)'</code><code class="p">,</code> <code class="s1">' '</code><code class="p">,</code> <code class="n">text</code><code class="p">)</code>
    <code class="c1"># standalone sequences of hyphens like --- or ==</code>
    <code class="n">text</code> <code class="o">=</code> <code class="n">re</code><code class="o">.</code><code class="n">sub</code><code class="p">(</code><code class="s-Affix">r</code><code class="s1">'(?:^|\s)[\-=\+]{2,}(?:\s|$)'</code><code class="p">,</code> <code class="s1">' '</code><code class="p">,</code> <code class="n">text</code><code class="p">)</code>
    <code class="c1"># sequences of white spaces</code>
    <code class="n">text</code> <code class="o">=</code> <code class="n">re</code><code class="o">.</code><code class="n">sub</code><code class="p">(</code><code class="s-Affix">r</code><code class="s1">'\s+'</code><code class="p">,</code> <code class="s1">' '</code><code class="p">,</code> <code class="n">text</code><code class="p">)</code>
    <code class="k">return</code> <code class="n">text</code><code class="o">.</code><code class="n">strip</code><code class="p">()</code>
</pre>

<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>Be careful: if your regular expressions are not defined precisely enough, you can accidentally delete valuable information during this process without noticing it! The repeaters <code>+</code> and <code>*</code> can be especially dangerous because they match unbounded sequences of characters and can remove large portions of the text.</p></div>

<p>Let’s apply the <code>clean</code> function to the earlier sample text and check the result:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">clean_text</code> <code class="o">=</code> <code class="n">clean</code><code class="p">(</code><code class="n">text</code><code class="p">)</code>
<code class="k">print</code><code class="p">(</code><code class="n">clean_text</code><code class="p">)</code>
<code class="k">print</code><code class="p">(</code><code class="s2">"Impurity:"</code><code class="p">,</code> <code class="n">impurity</code><code class="p">(</code><code class="n">clean_text</code><code class="p">))</code>
</pre>

<p><code>Out:</code></p>

<pre data-type="programlisting">
After viewing the PINKIEPOOL Trailer it got me thinking about the best
match ups. Here's my take: Deadpool Captain America
Impurity: 0.0
</pre>

<p>That looks pretty good. Once you have treated the first patterns, you should check the impurity of the cleaned text again and add further cleaning steps if necessary:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">df</code><code class="p">[</code><code class="s1">'clean_text'</code><code class="p">]</code> <code class="o">=</code> <code class="n">df</code><code class="p">[</code><code class="s1">'text'</code><code class="p">]</code><code class="o">.</code><code class="n">map</code><code class="p">(</code><code class="n">clean</code><code class="p">)</code>
<code class="n">df</code><code class="p">[</code><code class="s1">'impurity'</code><code class="p">]</code>   <code class="o">=</code> <code class="n">df</code><code class="p">[</code><code class="s1">'clean_text'</code><code class="p">]</code><code class="o">.</code><code class="n">apply</code><code class="p">(</code><code class="n">impurity</code><code class="p">,</code> <code class="n">min_len</code><code class="o">=</code><code class="mi">20</code><code class="p">)</code>
</pre>

<pre data-code-language="python" data-type="programlisting">
<code class="n">df</code><code class="p">[[</code><code class="s1">'clean_text'</code><code class="p">,</code> <code class="s1">'impurity'</code><code class="p">]]</code><code class="o">.</code><code class="n">sort_values</code><code class="p">(</code><code class="n">by</code><code class="o">=</code><code class="s1">'impurity'</code><code class="p">,</code> <code class="n">ascending</code><code class="o">=</code><code class="bp">False</code><code class="p">)</code> \
                              <code class="o">.</code><code class="n">head</code><code class="p">(</code><code class="mi">3</code><code class="p">)</code>
</pre>

<table class="dataframe tex2jax_ignore">
  <thead>
    <tr>
      <th/>
      <th>clean_text</th>
      <th>impurity</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>14058</th>
      <td>Mustang 2018, 2019, or 2020? Must Haves!! 1. Have a Credit score of 780\+ for the best low interest rates! 2. Join a Credit Union to finance the vehicle! 3. Or Find a Lender to finance the vehicle...</td>
      <td>0.03</td>
    </tr>
    <tr>
      <th>18934</th>
      <td>At the dealership, they offered an option for foot-well illumination, but I cannot find any reference to this online. Has anyone gotten it? How does it look? Anyone have pictures. Not sure if this...</td>
      <td>0.03</td>
    </tr>
    <tr>
      <th>16505</th>
      <td>I am looking at four Caymans, all are in a similar price range. The major differences are the miles, the years, and one isn’t a S. https://www.cargurus.com/Cars/inventorylisting/viewDetailsFilterV...</td>
      <td>0.02</td>
    </tr>
  </tbody>
</table>


<p>Even the dirtiest records, according to our regular expression, look pretty clean now. But besides those rough patterns we were searching for, there are also more subtle variations of characters that can <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch4_term11" id="idm45634202228664"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch4_term12" id="idm45634202227288"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch4_term13" id="idm45634202225912"/>cause problems.</p>
</div></section>

<section data-type="sect2" class="blueprint" data-pdf-bookmark="Blueprint: Character Normalization with textacy"><div class="sect2" id="idm45634202567064">
<h2>Blueprint: Character Normalization with textacy</h2>
<p>Take a look at the <a contenteditable="false" data-type="indexterm" data-primary="text data preparation" data-secondary="normalization of characters and terms" id="ch4_term14"/><a contenteditable="false" data-type="indexterm" data-primary="normalization of text" id="ch4_term15"/><a contenteditable="false" data-type="indexterm" data-primary="textacy library" id="ch4_term16"/>following sentence, which contains typical issues related to variants of letters and quote characters:</p>

<pre data-type="programlisting">
text = "The café “Saint-Raphaël” is loca-\nted on Côte dʼAzur."
</pre>

<p>Accented <a contenteditable="false" data-type="indexterm" data-primary="character normalization" id="idm45634202216696"/>characters can be a problem because people do not consistently use them. For example, the tokens <code>Saint-Raphaël</code> and <code>Saint-Raphael</code> will not be recognized as identical. In addition, texts often contain words separated by a hyphen due to the automatic line breaks. Fancy Unicode hyphens and apostrophes like the ones used in the text can be a problem for tokenization. For all of these issues it makes sense to normalize the text and replace accents and fancy characters with ASCII equivalents. </p>

<p>We will <a contenteditable="false" data-type="indexterm" data-primary="spaCy, linguistic processing with" data-secondary="textacy library and" id="idm45634202197112"/>use <a href="https://textacy.readthedocs.io"><em>textacy</em></a> for that purpose. textacy is an NLP library built to work with spaCy. It leaves the linguistic part to spaCy and focuses on pre- and postprocessing. Thus, its preprocessing module comprises a nice collection of functions to normalize characters and to treat common patterns such as URLs, email addresses, telephone numbers, and so on, which we will use <a contenteditable="false" data-type="indexterm" data-primary="textacy functions" id="idm45634202194392"/>next. <a data-type="xref" href="#tab-textacy-prep">Table 4-1</a> shows a selection of <a contenteditable="false" data-type="indexterm" data-primary="text data preparation" data-secondary="textacy preprocessing functions for" id="idm45634202192200"/>textacy’s <a contenteditable="false" data-type="indexterm" data-primary="replace functions in textacy" id="ch4_term21"/>preprocessing functions. All of these functions work on plain text, completely independent from spaCy.</p>

<table id="tab-textacy-prep"><caption><span class="label">Table 4-1. </span>Subset of textacy’s preprocessing functions</caption><thead>
<tr>
<th>Function</th>

<th>Description</th></tr>
</thead>
<tbody>
<tr>
  <td><code>normalize_hyphenated_words</code></td>
  <td>Reassembles words that were separated by a line break</td></tr>
<tr>
  <td><code>normalize_quotation_marks</code></td>
  <td>Replaces all kind of fancy quotation marks with an ASCII equivalent</td></tr>
<tr>
  <td><code>normalize_unicode</code></td>
  <td>Unifies different codes of accented characters in Unicode</td></tr>
<tr>
  <td><code>remove_accents</code></td>
  <td>Replaces accented characters with ASCII, if possible, or drops them</td></tr>
<tr>
  <td><code>replace_urls</code></td>
  <td>Similar for URLs like https://xyz.com</td></tr>
<tr>
  <td><code>replace_emails</code></td>
  <td>Replaces emails with _EMAIL_</td></tr>
<tr>
  <td><code>replace_hashtags</code></td>
  <td>Similar for tags like #sunshine</td></tr>
<tr>
  <td><code>replace_numbers</code></td>
  <td>Similar for numbers like 1235</td></tr>
<tr>
  <td><code>replace_phone_numbers</code></td>
  <td>Similar for telephone numbers +1 800 456-6553</td></tr>
<tr>
  <td><code>replace_user_handles</code></td>
  <td>Similar for user handles like @pete</td></tr>
<tr>
  <td><code>replace_emojis</code></td>
  <td>Replaces smileys etc. with _EMOJI_</td></tr>
</tbody>
</table>

<p>Our blueprint function shown here standardizes fancy hyphens and quotes and removes accents with the help of textacy:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="kn">import</code> <code class="nn">textacy.preprocessing</code> <code class="kn">as</code> <code class="nn">tprep</code>

<code class="k">def</code> <code class="nf">normalize</code><code class="p">(</code><code class="n">text</code><code class="p">):</code>
    <code class="n">text</code> <code class="o">=</code> <code class="n">tprep</code><code class="o">.</code><code class="n">normalize_hyphenated_words</code><code class="p">(</code><code class="n">text</code><code class="p">)</code>
    <code class="n">text</code> <code class="o">=</code> <code class="n">tprep</code><code class="o">.</code><code class="n">normalize_quotation_marks</code><code class="p">(</code><code class="n">text</code><code class="p">)</code>
    <code class="n">text</code> <code class="o">=</code> <code class="n">tprep</code><code class="o">.</code><code class="n">normalize_unicode</code><code class="p">(</code><code class="n">text</code><code class="p">)</code>
    <code class="n">text</code> <code class="o">=</code> <code class="n">tprep</code><code class="o">.</code><code class="n">remove_accents</code><code class="p">(</code><code class="n">text</code><code class="p">)</code>
    <code class="k">return</code> <code class="n">text</code>
</pre>

<p>When this is applied to the earlier example sentence, we get the following result:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="k">print</code><code class="p">(</code><code class="n">normalize</code><code class="p">(</code><code class="n">text</code><code class="p">))</code>
</pre>

<p><code>Out:</code></p>

<pre data-type="programlisting">
The cafe "Saint-Raphael" is located on Cote d'Azur.
</pre>

<div data-type="note" epub:type="note"><h6>Note</h6>
<p>As Unicode normalization has many facets, you can check out other libraries. <a href="https://oreil.ly/eqKI-"><em>unidecode</em></a>, for example, does an excellent <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch4_term14" id="idm45634202112360"/>job here.</p></div>
</div></section>

<section data-type="sect2" class="blueprint" data-pdf-bookmark="Blueprint: Pattern-Based Data Masking with textacy"><div class="sect2" id="idm45634202223816">
<h2>Blueprint: Pattern-Based Data Masking with textacy</h2>
<p>Text, in particular <a contenteditable="false" data-type="indexterm" data-primary="data masking" id="ch4_term19"/><a contenteditable="false" data-type="indexterm" data-primary="text data preparation" data-secondary="data masking for" id="ch4_term20"/>content written by users, often contains not only ordinary words but also several kinds of identifiers, such as URLs, email addresses, or phone numbers. Sometimes we are interested especially in those items, for example, to analyze the most frequently mentioned URLs. In many cases, though, it may be better to remove or mask this information, either because it is not relevant or for privacy reasons.</p>

<p>textacy has some convenient <code>replace</code> functions for data masking (see <a data-type="xref" href="#tab-textacy-prep">Table 4-1</a>). Most of the functions are based on <a contenteditable="false" data-type="indexterm" data-primary="regular expressions" id="idm45634202109272"/>regular expressions, which are easily accessible via the <a href="https://oreil.ly/Ly6Ce">open source code</a>. Thus, whenever you need to treat any of these items, textacy has a regular expression for it that you can directly use or adapt to your needs. Let’s illustrate this with a simple call to find the most frequently used URLs in the corpus:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="kn">from</code> <code class="nn">textacy.preprocessing.resources</code> <code class="kn">import</code> <code class="n">RE_URL</code>

<code class="n">count_words</code><code class="p">(</code><code class="n">df</code><code class="p">,</code> <code class="n">column</code><code class="o">=</code><code class="s1">'clean_text'</code><code class="p">,</code> <code class="n">preprocess</code><code class="o">=</code><code class="n">RE_URL</code><code class="o">.</code><code class="n">findall</code><code class="p">)</code><code class="o">.</code><code class="n">head</code><code class="p">(</code><code class="mi">3</code><code class="p">)</code>
</pre>



<table class="dataframe tex2jax_ignore">
  <thead>
    <tr>
      <th>token</th>
      <th>freq</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>www.getlowered.com</td>
      <td>3</td>
    </tr>
    <tr>
      <td>http://www.ecolamautomotive.com/#!2/kv7fq</td>
      <td>2</td>
    </tr>
    <tr>
      <td>https://www.reddit.com/r/Jeep/comments/4ux232/just_ordered_an_android_head_unit_joying_jeep/</td>
      <td>2</td>
    </tr>
  </tbody>
</table>


<p>For the analysis we want to perform with this dataset (in <a data-type="xref" href="ch10.xhtml#ch-embeddings">Chapter 10</a>), we are not interested in those URLs. They rather represent a disturbing artifact. Thus, we will substitute all URLs in our text with <code>replace_urls</code>, which is in fact just a call to <code>RE_URL.sub</code>. The default substitution for all of textacy’s replace functions is a generic tag enclosed by underscores like <code>_URL_</code>. You can choose your own substitution by specifying the <code>replace_with</code> parameter. Often it makes sense to not completely remove those items because it leaves the structure of the sentences intact. The following call illustrates the functionality:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="kn">from</code> <code class="nn">textacy.preprocessing.replace</code> <code class="kn">import</code> <code class="n">replace_urls</code>

<code class="n">text</code> <code class="o">=</code> <code class="s2">"Check out https://spacy.io/usage/spacy-101"</code>

<code class="c1"># using default substitution _URL_</code>
<code class="k">print</code><code class="p">(</code><code class="n">replace_urls</code><code class="p">(</code><code class="n">text</code><code class="p">))</code>
</pre>

<p><code>Out:</code></p>

<pre data-type="programlisting">
Check out _URL_
</pre>

<p>To finalize <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch4_term21" id="idm45634202019224"/>data cleaning, we apply the normalization and data masking functions to our data:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">df</code><code class="p">[</code><code class="s1">'clean_text'</code><code class="p">]</code> <code class="o">=</code> <code class="n">df</code><code class="p">[</code><code class="s1">'clean_text'</code><code class="p">]</code><code class="o">.</code><code class="n">map</code><code class="p">(</code><code class="n">replace_urls</code><code class="p">)</code>
<code class="n">df</code><code class="p">[</code><code class="s1">'clean_text'</code><code class="p">]</code> <code class="o">=</code> <code class="n">df</code><code class="p">[</code><code class="s1">'clean_text'</code><code class="p">]</code><code class="o">.</code><code class="n">map</code><code class="p">(</code><code class="n">normalize</code><code class="p">)</code>
</pre>

<p>Data cleaning is like cleaning your house. You’ll always find some dirty corners, and you won’t ever get your house totally clean. So you stop cleaning when it is <em>sufficiently</em> clean. That’s what we assume for our data at the moment. Later in the process, if analysis results are suffering from remaining noise, we may need to get back to data cleaning.</p>

<p>We finally rename the text columns so that <code>clean_text</code> becomes <code>text</code>, drop the impurity column, and store the new version of the <code>DataFrame</code> in the <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch4_term15" id="idm45634201922232"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch4_term16" id="idm45634201920824"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch4_term19" id="idm45634201919448"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch4_term20" id="idm45634201918072"/>database.</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">df</code><code class="o">.</code><code class="n">rename</code><code class="p">(</code><code class="n">columns</code><code class="o">=</code><code class="p">{</code><code class="s1">'text'</code><code class="p">:</code> <code class="s1">'raw_text'</code><code class="p">,</code> <code class="s1">'clean_text'</code><code class="p">:</code> <code class="s1">'text'</code><code class="p">},</code> <code class="n">inplace</code><code class="o">=</code><code class="bp">True</code><code class="p">)</code>
<code class="n">df</code><code class="o">.</code><code class="n">drop</code><code class="p">(</code><code class="n">columns</code><code class="o">=</code><code class="p">[</code><code class="s1">'impurity'</code><code class="p">],</code> <code class="n">inplace</code><code class="o">=</code><code class="bp">True</code><code class="p">)</code>

<code class="n">con</code> <code class="o">=</code> <code class="n">sqlite3</code><code class="o">.</code><code class="n">connect</code><code class="p">(</code><code class="n">db_name</code><code class="p">)</code>
<code class="n">df</code><code class="o">.</code><code class="n">to_sql</code><code class="p">(</code><code class="s2">"posts_cleaned"</code><code class="p">,</code> <code class="n">con</code><code class="p">,</code> <code class="n">index</code><code class="o">=</code><code class="bp">False</code><code class="p">,</code> <code class="n">if_exists</code><code class="o">=</code><code class="s2">"replace"</code><code class="p">)</code>
<code class="n">con</code><code class="o">.</code><code class="n">close</code><code class="p">()</code>
</pre>
</div></section>
</div></section>

<section data-type="sect1" data-pdf-bookmark="Tokenization"><div class="sect1" id="idm45634202896856">
<h1>Tokenization</h1>
<p>We already <a contenteditable="false" data-type="indexterm" data-primary="text data preparation" data-secondary="tokenizers for" id="ch4_term22"/><a contenteditable="false" data-type="indexterm" data-primary="tokenization/tokens" data-secondary="in data preparation" data-secondary-sortas="data preparation" id="ch4_term23"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch4_term7" id="idm45634201823928"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch4_term8" id="idm45634201822552"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch4_term10" id="idm45634201821176"/>introduced a regex tokenizer in <a data-type="xref" href="ch01.xhtml#ch-exploration">Chapter 1</a>, which used a simple rule. In practice, however, tokenization can be quite complex if we want to treat everything correctly. Consider the following piece of text as an example:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">text</code> <code class="o">=</code> <code class="s2">"""</code>
<code class="s2">2019-08-10 23:32: @pete/@louis - I don't have a well-designed</code>
<code class="s2">solution for today's problem. The code of module AC68 should be -1.</code>
<code class="s2">Have to think a bit... #goodnight ;-) 😩😬"""</code>
</pre>

<p>Obviously, the rules to define word and sentence boundaries are not that simple. So <a contenteditable="false" data-type="indexterm" data-primary="tokenization/tokens" data-secondary="defined" id="idm45634201816152"/>what exactly is a token? Unfortunately, there is no clear definition. We could say that a token is a linguistic unit that is semantically useful for analysis. This definition implies that tokenization is application dependent to some degree. For example, in many cases we can simply discard punctuation characters, but not if we want to keep emoticons like <code>:-)</code> for sentiment analysis. The same is true for tokens containing numbers or hashtags. Even though most tokenizers, including those used in <a contenteditable="false" data-type="indexterm" data-primary="NLTK library" id="idm45634201813912"/><a contenteditable="false" data-type="indexterm" data-primary="tokenization/tokens" data-secondary="with NLTK" data-secondary-sortas="NLTK" id="idm45634201812808"/>NLTK and <a contenteditable="false" data-type="indexterm" data-primary="tokenization/tokens" data-secondary="in spaCy linguistic processing" data-secondary-sortas="spaCy linguistic processing" id="idm45634201810824"/><a contenteditable="false" data-type="indexterm" data-primary="spaCy, linguistic processing with" data-secondary="tokenizers for" id="idm45634201809208"/>spaCy, are based on regular expressions, they apply quite complex and sometimes language-specific rules. </p>

<p>We will first develop our own blueprint for tokenization-based regular expressions before we briefly introduce NLTK’s tokenizers. Tokenization in spaCy will be covered in the next section of this chapter as part of spaCy’s integrated process.</p>

<section data-type="sect2" class="blueprint" data-pdf-bookmark="Blueprint: Tokenization with Regular Expressions"><div class="sect2" id="idm45634201806936">
<h2>Blueprint: Tokenization with Regular Expressions</h2>

<p>Useful functions <a contenteditable="false" data-type="indexterm" data-primary="tokenization/tokens" data-secondary="with regular expressions" data-secondary-sortas="regular expressions" id="ch4_term24"/><a contenteditable="false" data-type="indexterm" data-primary="regular expressions" id="ch4_term25"/><a contenteditable="false" data-type="indexterm" data-primary="text data preparation" data-secondary="with regular expressions" data-secondary-sortas="regular expressions" id="ch4_term26"/>for tokenization are <code>re.split()</code> and <code>re.findall()</code>. The first one splits a string at matching expressions, while the latter extracts all character sequences matching a certain pattern.
For example, in <a data-type="xref" href="ch01.xhtml#ch-exploration">Chapter 1</a> we <a contenteditable="false" data-type="indexterm" data-primary="regex library (Python)" id="idm45634201795656"/>used the <code>regex</code> library with the POSIX pattern <code>[\w-]*\p{L}[\w-]*</code> to find sequences of alphanumeric characters with at least one letter. The <a contenteditable="false" data-type="indexterm" data-primary="CountVectorizer" id="idm45634201793528"/><a contenteditable="false" data-type="indexterm" data-primary="scikit-learn" data-secondary="CountVectorizer of" id="idm45634201740680"/>scikit-learn <span class="keep-together"><code>CountVectorizer</code></span> uses the pattern <code>\w\w+</code> for its default tokenization. It matches all <span class="keep-together">sequences</span> of two or more alphanumeric characters. Applied to our sample sentence, this yields the following result:<sup><a data-type="noteref" id="idm45634201737000-marker" href="ch04.xhtml#idm45634201737000">3</a></sup> </p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">tokens</code> <code class="o">=</code> <code class="n">re</code><code class="o">.</code><code class="n">findall</code><code class="p">(</code><code class="s-Affix">r</code><code class="s1">'\w\w+'</code><code class="p">,</code> <code class="n">text</code><code class="p">)</code>
<code class="k">print</code><code class="p">(</code><code class="o">*</code><code class="n">tokens</code><code class="p">,</code> <code class="n">sep</code><code class="o">=</code><code class="s1">'|'</code><code class="p">)</code>
</pre>

<p><code>Out:</code></p>

<pre data-type="programlisting">
2019|08|10|23|32|pete|louis|don|have|well|designed|solution|for|today
problem|The|code|of|module|AC68|should|be|Have|to|think|bit|goodnight
</pre>

<p>Unfortunately, all special characters and the emojis are lost. To improve the result, <span class="keep-together">we add</span> some additional expressions for the emojis and create a reusable regular expression <code>RE_TOKEN</code>. <a contenteditable="false" data-type="indexterm" data-primary="VERBOSE option" id="idm45634201725208"/>The <code>VERBOSE</code> option allows readable formatting of complex expressions. The following <code>tokenize</code> function and the example illustrate the use:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">RE_TOKEN</code> <code class="o">=</code> <code class="n">re</code><code class="o">.</code><code class="n">compile</code><code class="p">(</code><code class="s-Affix">r</code><code class="s2">"""</code>
<code class="s2">               ( [#]?[@\w'’\.\-\:]*\w     # words, hashtags and email addresses</code>
<code class="s2">               | [:;&lt;]\-?[\)\(3]          # coarse pattern for basic text emojis</code>
<code class="s2">               | [\U0001F100-\U0001FFFF]  # coarse code range for unicode emojis</code>
<code class="s2">               )</code>
<code class="s2">               """</code><code class="p">,</code> <code class="n">re</code><code class="o">.</code><code class="n">VERBOSE</code><code class="p">)</code>

<code class="k">def</code> <code class="nf">tokenize</code><code class="p">(</code><code class="n">text</code><code class="p">):</code>
    <code class="k">return</code> <code class="n">RE_TOKEN</code><code class="o">.</code><code class="n">findall</code><code class="p">(</code><code class="n">text</code><code class="p">)</code>

<code class="n">tokens</code> <code class="o">=</code> <code class="n">tokenize</code><code class="p">(</code><code class="n">text</code><code class="p">)</code>
<code class="k">print</code><code class="p">(</code><code class="o">*</code><code class="n">tokens</code><code class="p">,</code> <code class="n">sep</code><code class="o">=</code><code class="s1">'|'</code><code class="p">)</code>
</pre>

<p><code>Out:</code></p>

<pre data-type="programlisting">
2019-08-10|23:32|@pete|@louis|I|don't|have|a|well-designed|solution
for|today's|problem|The|code|of|module|AC68|should|be|-1|Have|to|think
a|bit|#goodnight|;-)|😩|😬
</pre>

<p>This expression should yield reasonably good results on most user-generated content. It can be used to quickly tokenize text for data exploration, as explained in <a data-type="xref" href="ch01.xhtml#ch-exploration">Chapter 1</a>. It’s also a good alternative for the default tokenization of the scikit-learn vectorizers, which will be introduced in the next chapter.</p>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Tokenization with NLTK"><div class="sect2" id="idm45634201806312">
<h2>Tokenization with NLTK</h2>
<p>Let’s take a <a contenteditable="false" data-type="indexterm" data-primary="NLTK library" id="idm45634201632552"/><a contenteditable="false" data-type="indexterm" data-primary="tokenization/tokens" data-secondary="with NLTK" data-secondary-sortas="NLTK" id="idm45634201631416"/>brief look at NLTK’s tokenizers, as NLTK is frequently used for tokenization. The <a contenteditable="false" data-type="indexterm" data-primary="word_tokenize function" id="idm45634201629640"/>standard NLTK tokenizer can be called by the shortcut <code>word_tokenize</code>. It produces the following result on our sample text:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="kn">import</code> <code class="nn">nltk</code>

<code class="n">tokens</code> <code class="o">=</code> <code class="n">nltk</code><code class="o">.</code><code class="n">tokenize</code><code class="o">.</code><code class="n">word_tokenize</code><code class="p">(</code><code class="n">text</code><code class="p">)</code>
<code class="k">print</code><code class="p">(</code><code class="o">*</code><code class="n">tokens</code><code class="p">,</code> <code class="n">sep</code><code class="o">=</code><code class="s1">'|'</code><code class="p">)</code>
</pre>

<p><code>Out:</code></p>

<pre data-type="programlisting">
2019-08-10|23:32|:|@|pete/|@|louis|-|I|do|n't|have|a|well-designed
solution|for|today|'s|problem|.|The|code|of|module|AC68|should|be|-1|.
Have|to|think|a|bit|...|#|goodnight|;|-|)||😩😬</pre>

<p>The function internally uses the <code>TreebankWordTokenizer</code> in combination with the <code>PunktSentenceTokenizer</code>. It works well for standard text but has its flaws with hashtags or text emojis. NLTK also provides a <code>RegexpTokenizer</code>, which is basically a wrapper for <code>re.findall()</code> with some added convenience functionality. Besides that, there are other regular-expression-based tokenizers in NLTK, like the <code>TweetTokenizer</code> or the multilingual <code>ToktokTokenizer</code>, which you can check out in the notebook on <a href="https://oreil.ly/zLTEl">GitHub</a> for this chapter.</p>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Recommendations for Tokenization"><div class="sect2" id="idm45634201568216">
<h2>Recommendations for Tokenization</h2>
<p>You <a contenteditable="false" data-type="indexterm" data-primary="tokenization/tokens" data-secondary="recommendations for" id="idm45634201566648"/>will probably need to use custom regular expressions if you aim for high precision on domain-specific token patterns. Fortunately, you can find regular expressions for many common patterns in open source libraries and adapt them to your needs.<sup><a data-type="noteref" id="idm45634201564856-marker" href="ch04.xhtml#idm45634201564856">4</a></sup> </p>

<p>In general, you should be aware of the following problematic cases in your application and define how to treat them:<sup><a data-type="noteref" id="idm45634201561544-marker" href="ch04.xhtml#idm45634201561544">5</a></sup> </p>
<ul>
<li>Tokens containing periods, such as <code>Dr.</code>, <code>Mrs.</code>, <code>U.</code><code/>, <code>xyz.com</code></li>
<li>Hyphens, like in <code>rule-based</code> </li>
<li>Clitics (connected word abbreviations), like in <code>couldn't</code>, <code>we've</code> or <code>je t'aime</code></li>
<li>Numerical expressions, such as telephone numbers (<code>(123) 456-7890</code>) or dates (<code>August 7th, 2019</code>)</li>
<li>Emojis, hashtags, email addresses, or URLs</li>
</ul>
<p>The tokenizers in common libraries differ especially with regard <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch4_term22" id="idm45634201533512"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch4_term23" id="idm45634201532136"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch4_term24" id="idm45634201530760"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch4_term25" id="idm45634201529384"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch4_term26" id="idm45634201528008"/>to those tokens.</p>
</div></section>
</div></section>

<section data-type="sect1" data-pdf-bookmark="Linguistic Processing with spaCy"><div class="sect1" id="ch4-spacy">
<h1>Linguistic Processing with spaCy</h1>
<p>spaCy is a <a contenteditable="false" data-type="indexterm" data-primary="spaCy, linguistic processing with" data-secondary="NLP pipeline for" id="ch4_term28"/><a contenteditable="false" data-type="indexterm" data-primary="text data preparation" data-secondary="NLP spaCy pipeline for" id="ch4_term47"/><a contenteditable="false" data-type="indexterm" data-primary="data preprocessing" data-secondary="pipelines for" id="ch4_term48"/><a contenteditable="false" data-type="indexterm" data-primary="pipelines" id="ch4_term49"/>powerful library for linguistic data processing. It provides an integrated pipeline of processing components, by default a tokenizer, a part-of-speech tagger, a dependency parser, and a named-entity recognizer (see <a data-type="xref" href="#fig-spacy-pipe">Figure 4-2</a>). Tokenization is based on complex language-dependent rules and regular expressions, while all subsequent steps use pretrained neural models.</p>

<figure><div id="fig-spacy-pipe" class="figure">
<img src="Images/btap_0402.jpg" width="1367" height="199"/>
<h6><span class="label">Figure 4-2. </span>spaCy’s NLP pipeline.</h6></div></figure>

<p>The philosophy of spaCy is that the original text is retained throughout the process. Instead of transforming it, spaCy adds layers of information. The main object to represent the processed text is a <code>Doc</code> object, which itself <a contenteditable="false" data-type="indexterm" data-primary="Token objects" id="idm45634201513848"/>contains a list of <code>Token</code> objects. Any <a contenteditable="false" data-type="indexterm" data-primary="Span objects" id="idm45634201512136"/>range selection of tokens creates a <code>Span</code>. Each of these object types has properties that are determined step-by-step.</p>

<p>In this section, we explain <a contenteditable="false" data-type="indexterm" data-primary="spaCy, linguistic processing with" data-secondary="tokenizers for" id="ch4_term29"/><a contenteditable="false" data-type="indexterm" data-primary="tokenization/tokens" data-secondary="in spaCy linguistic processing" data-secondary-sortas="spaCy linguistic processing" id="ch4_term30"/>how to process a document with spaCy, how to work with tokens and their attributes, how to use part-of-speech tags, and how to extract named entities. We will dive even deeper into spaCy’s more advanced concepts in <a data-type="xref" href="ch12.xhtml#ch-knowledge">Chapter 12</a>, where we write our own pipeline components, create custom attributes, and work with the dependency tree generated by the parser for knowledge extraction.</p>

<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>For the development of the examples in this book, we used spaCy version 2.3.2. If you already use spaCy 3.0, which is still under development at the time of writing, your results may look slightly different.</p></div>

<section data-type="sect2" data-pdf-bookmark="Instantiating a Pipeline"><div class="sect2" id="idm45634201503640">
<h2>Instantiating a Pipeline</h2>
<p>Let’s get started with spaCy. As a first step we need to instantiate an object of spaCy’s <code>Language</code> class by calling <code>spacy.load()</code> along with the name of the model file to use.<sup><a data-type="noteref" id="idm45634201500904-marker" href="ch04.xhtml#idm45634201500904">6</a></sup> We will use the small English language model <code>en_core_web_sm</code> in this chapter. The variable <a contenteditable="false" data-type="indexterm" data-primary="Language objects" id="idm45634201498840"/>for the <code>Language</code> object is usually called <code>nlp</code>:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="kn">import</code> <code class="nn">spacy</code>
<code class="n">nlp</code> <code class="o">=</code> <code class="n">spacy</code><code class="o">.</code><code class="n">load</code><code class="p">(</code><code class="s1">'en_core_web_sm'</code><code class="p">)</code>
</pre>

<p>This <code>Language</code> object now contains the shared vocabulary, the model, and the processing pipeline. You can check the pipeline components via this property of the object: </p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">nlp</code><code class="o">.</code><code class="n">pipeline</code>
</pre>

<p><code>Out:</code></p>

<pre data-type="programlisting">
[('tagger', &lt;spacy.pipeline.pipes.Tagger at 0x7fbd766f84c0&gt;),
 ('parser', &lt;spacy.pipeline.pipes.DependencyParser at 0x7fbd813184c0&gt;),
 ('ner', &lt;spacy.pipeline.pipes.EntityRecognizer at 0x7fbd81318400&gt;)]
</pre>

<p>The default pipeline consists of a tagger, parser, and named-entity recognizer (<code>ner</code>), all of which are language-dependent. The tokenizer is not explicitly listed because this step is always necessary. </p>

<p>spaCy’s tokenizer is <a contenteditable="false" data-type="indexterm" data-primary="execution time" id="idm45634201469736"/><a contenteditable="false" data-type="indexterm" data-primary="spaCy, linguistic processing with" data-secondary="execution time with" id="idm45634201468600"/><a contenteditable="false" data-type="indexterm" data-primary="text data preparation" data-secondary="execution time of" id="idm45634201443368"/><a contenteditable="false" data-type="indexterm" data-primary="time, execution" id="idm45634201441992"/>pretty fast, but all other steps are based on neural models and consume a significant amount of time. Compared to other libraries, though, spaCy’s models are among the fastest. Processing the whole pipeline takes about 10–20 times as long as just tokenization, where each step is taking a similar share of the total time. If tokenization of 1,000 documents takes, for example, one second, tagging, parsing, and NER may each take an additional five seconds. This may become a problem if you process big datasets. So, it’s better to switch off the parts that you don’t need. </p>

<p>Often you will only need the tokenizer and the part-of-speech tagger. In this case, you should disable the parser and named-entity recognition like this:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">nlp</code> <code class="o">=</code> <code class="n">spacy</code><code class="o">.</code><code class="n">load</code><code class="p">(</code><code class="s2">"en_core_web_sm"</code><code class="p">,</code> <code class="n">disable</code><code class="o">=</code><code class="p">[</code><code class="s2">"parser"</code><code class="p">,</code> <code class="s2">"ner"</code><code class="p">])</code>
</pre>

<p>If you just want the tokenizer and nothing else, you can also simply call <code>nlp.make_doc</code> on a text.</p>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Processing Text"><div class="sect2" id="idm45634201503048">
<h2>Processing Text</h2>
<p>The pipeline <a contenteditable="false" data-type="indexterm" data-primary="spaCy, linguistic processing with" data-secondary="text to doc object in" id="ch4_term31"/>is executed by calling the <code>nlp</code> object. The call returns an object of type <code>spacy.tokens.doc.Doc</code>, a container to access the tokens, spans (ranges of tokens), and their linguistic annotations.</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">nlp</code> <code class="o">=</code> <code class="n">spacy</code><code class="o">.</code><code class="n">load</code><code class="p">(</code><code class="s2">"en_core_web_sm"</code><code class="p">)</code>
<code class="n">text</code> <code class="o">=</code> <code class="s2">"My best friend Ryan Peters likes fancy adventure games."</code>
<code class="n">doc</code> <code class="o">=</code> <code class="n">nlp</code><code class="p">(</code><code class="n">text</code><code class="p">)</code>
</pre>

<p>spaCy is object-oriented as well as nondestructive. The original text is always retained. When you print the <code>doc</code> object, it uses <code>doc.text</code>, the property containing the original text. But <code>doc</code> is also a container object for the tokens, and you can use it as an iterator for them: </p>

<pre data-code-language="python" data-type="programlisting">
<code class="k">for</code> <code class="n">token</code> <code class="ow">in</code> <code class="n">doc</code><code class="p">:</code>
    <code class="k">print</code><code class="p">(</code><code class="n">token</code><code class="p">,</code> <code class="n">end</code><code class="o">=</code><code class="s2">"|"</code><code class="p">)</code>
</pre>

<p><code>Out:</code></p>

<pre data-type="programlisting">
My|best|friend|Ryan|Peters|likes|fancy|adventure|games|.|
</pre>

<p>Each <a contenteditable="false" data-type="indexterm" data-primary="Token objects" id="idm45634201360968"/>token is actually an object of spaCy’s class <code>Token</code>. Tokens, as well as docs, have a number of interesting properties for language <a contenteditable="false" data-type="indexterm" data-primary="part-of-speech tags" id="idm45634201359288"/><a contenteditable="false" data-type="indexterm" data-primary="dependency parser" id="idm45634201358184"/><a contenteditable="false" data-type="indexterm" data-primary="named-entity recognizer" id="idm45634201357080"/>processing. <a data-type="xref" href="#tab-spacy-attributes">Table 4-2</a> shows which of these properties are created by each pipeline component.<sup><a data-type="noteref" id="idm45634201354856-marker" href="ch04.xhtml#idm45634201354856">7</a></sup> </p>

<table id="tab-spacy-attributes"><caption><span class="label">Table 4-2. </span>Selection of attributes created by spaCy’s built-in pipeline</caption><thead>
<tr>
<th>Component</th>

<th>Creates</th></tr>
</thead>
<tbody>
<tr>
<td>Tokenizer</td>

<td><code>Token.is_punct</code>, <code>Token.is_alpha</code>, <code>Token.like_email</code>, <code>Token.like_url</code></td></tr>
<tr>
<td>Part-of-speech tagger</td>

<td><code>Token.pos_</code></td></tr>
<tr>
<td>Dependency parser</td>

<td><code>Token.dep_</code>, <code>Token.head</code>, <code>Doc.sents</code>, <code>Doc.noun_chunks</code></td></tr>
<tr>
<td>Named-entity recognizer</td>

<td><code>Doc.ents</code>, <code>Token.ent_iob_</code>, <code>Token.ent_type_</code></td></tr>
</tbody>
</table>

<p>We  provide a small utility function, <code>display_nlp</code>, to generate a table containing the tokens and their attributes. Internally, we create a <code>DataFrame</code> for this and use the token position in the document as an index. Punctuation characters are skipped by default in this function. <a data-type="xref" href="#tab-nlp-result">Table 4-3</a> shows the output of this function for our example sentence:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="k">def</code> <code class="nf">display_nlp</code><code class="p">(</code><code class="n">doc</code><code class="p">,</code> <code class="n">include_punct</code><code class="o">=</code><code class="bp">False</code><code class="p">):</code>
    <code class="sd">"""Generate data frame for visualization of spaCy tokens."""</code>
    <code class="n">rows</code> <code class="o">=</code> <code class="p">[]</code>
    <code class="k">for</code> <code class="n">i</code><code class="p">,</code> <code class="n">t</code> <code class="ow">in</code> <code class="nb">enumerate</code><code class="p">(</code><code class="n">doc</code><code class="p">):</code>
        <code class="k">if</code> <code class="ow">not</code> <code class="n">t</code><code class="o">.</code><code class="n">is_punct</code> <code class="ow">or</code> <code class="n">include_punct</code><code class="p">:</code>
            <code class="n">row</code> <code class="o">=</code> <code class="p">{</code><code class="s1">'token'</code><code class="p">:</code> <code class="n">i</code><code class="p">,</code>  <code class="s1">'text'</code><code class="p">:</code> <code class="n">t</code><code class="o">.</code><code class="n">text</code><code class="p">,</code> <code class="s1">'lemma_'</code><code class="p">:</code> <code class="n">t</code><code class="o">.</code><code class="n">lemma_</code><code class="p">,</code>
                   <code class="s1">'is_stop'</code><code class="p">:</code> <code class="n">t</code><code class="o">.</code><code class="n">is_stop</code><code class="p">,</code> <code class="s1">'is_alpha'</code><code class="p">:</code> <code class="n">t</code><code class="o">.</code><code class="n">is_alpha</code><code class="p">,</code>
                   <code class="s1">'pos_'</code><code class="p">:</code> <code class="n">t</code><code class="o">.</code><code class="n">pos_</code><code class="p">,</code> <code class="s1">'dep_'</code><code class="p">:</code> <code class="n">t</code><code class="o">.</code><code class="n">dep_</code><code class="p">,</code>
                   <code class="s1">'ent_type_'</code><code class="p">:</code> <code class="n">t</code><code class="o">.</code><code class="n">ent_type_</code><code class="p">,</code> <code class="s1">'ent_iob_'</code><code class="p">:</code> <code class="n">t</code><code class="o">.</code><code class="n">ent_iob_</code><code class="p">}</code>
            <code class="n">rows</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">row</code><code class="p">)</code>

    <code class="n">df</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">rows</code><code class="p">)</code><code class="o">.</code><code class="n">set_index</code><code class="p">(</code><code class="s1">'token'</code><code class="p">)</code>
    <code class="n">df</code><code class="o">.</code><code class="n">index</code><code class="o">.</code><code class="n">name</code> <code class="o">=</code> <code class="bp">None</code>
    <code class="k">return</code> <code class="n">df</code>
</pre>



<table class="dataframe tex2jax_ignore" id="tab-nlp-result">
<caption><span class="label">Table 4-3. </span>Result of spaCy’s document processing as generated by display_nlp</caption>
  <thead>
    <tr>
      <th/>
      <th>text</th>
      <th>lemma_</th>
      <th>is_stop</th>
      <th>is_alpha</th>
      <th>pos_</th>
      <th>dep_</th>
      <th>ent_type_</th>
      <th>ent_iob_</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>My</td>
      <td>-PRON-</td>
      <td>True</td>
      <td>True</td>
      <td>DET</td>
      <td>poss</td>
      <td/>
      <td>O</td>
    </tr>
    <tr>
      <th>1</th>
      <td>best</td>
      <td>good</td>
      <td>False</td>
      <td>True</td>
      <td>ADJ</td>
      <td>amod</td>
      <td/>
      <td>O</td>
    </tr>
    <tr>
      <th>2</th>
      <td>friend</td>
      <td>friend</td>
      <td>False</td>
      <td>True</td>
      <td>NOUN</td>
      <td>nsubj</td>
      <td/>
      <td>O</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Ryan</td>
      <td>Ryan</td>
      <td>False</td>
      <td>True</td>
      <td>PROPN</td>
      <td>compound</td>
      <td>PERSON</td>
      <td>B</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Peters</td>
      <td>Peters</td>
      <td>False</td>
      <td>True</td>
      <td>PROPN</td>
      <td>appos</td>
      <td>PERSON</td>
      <td>I</td>
    </tr>
    <tr>
      <th>5</th>
      <td>likes</td>
      <td>like</td>
      <td>False</td>
      <td>True</td>
      <td>VERB</td>
      <td>ROOT</td>
      <td/>
      <td>O</td>
    </tr>
    <tr>
      <th>6</th>
      <td>fancy</td>
      <td>fancy</td>
      <td>False</td>
      <td>True</td>
      <td>ADJ</td>
      <td>amod</td>
      <td/>
      <td>O</td>
    </tr>
    <tr>
      <th>7</th>
      <td>adventure</td>
      <td>adventure</td>
      <td>False</td>
      <td>True</td>
      <td>NOUN</td>
      <td>compound</td>
      <td/>
      <td>O</td>
    </tr>
    <tr>
      <th>8</th>
      <td>games</td>
      <td>game</td>
      <td>False</td>
      <td>True</td>
      <td>NOUN</td>
      <td>dobj</td>
      <td/>
      <td>O</td>
    </tr>
  </tbody>
</table>


<p>For each token, you find the lemma, some descriptive flags, the part-of-speech tag, the dependency tag (not used here, but in <a data-type="xref" href="ch12.xhtml#ch-knowledge">Chapter 12</a>), and possibly some information about the entity type. The <code>is_&lt;something&gt;</code> flags are created based on rules, but all part-of-speech, dependency, and named-entity attributes are based on <a contenteditable="false" data-type="indexterm" data-primary="neural networks" id="idm45634201154136"/>neural network models. So, there is always some degree of uncertainty in this information. The corpora used for training contain a mixture of news articles and online articles. The predictions of the model are fairly accurate if your data has similar linguistic characteristics. But if your data is very different—if you are working with Twitter data or <span class="keep-together">IT service</span> desk tickets, for example—you should be aware that this information is <span class="keep-together">unreliable</span>. </p>

<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>spaCy uses the <a contenteditable="false" data-type="indexterm" data-primary="spaCy, linguistic processing with" data-secondary="underscore convention in" id="idm45634201150024"/>convention that token attributes with an underscore like <code>pos_</code> yield the readable textual representation. <code>pos</code> without an underscore returns spaCy’s numeric identifier of a part-of-speech tag.<sup><a data-type="noteref" id="idm45634201147656-marker" href="ch04.xhtml#idm45634201147656">8</a></sup> The numeric identifiers can be imported as constants, e.g., <code>spacy.symbols.VERB</code>. Make sure not to <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch4_term28" id="idm45634201145592"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch4_term31" id="idm45634201144184"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch4_term47" id="idm45634201142808"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch4_term48" id="idm45634201141432"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch4_term49" id="idm45634201059048"/>mix them up!</p></div>
</div></section>

<section data-type="sect2" class="blueprint" data-pdf-bookmark="Blueprint: Customizing Tokenization"><div class="sect2" id="idm45634201414536">
<h2>Blueprint: Customizing Tokenization</h2>
<p>Tokenization is the <a contenteditable="false" data-type="indexterm" data-primary="spaCy, linguistic processing with" data-secondary="for customizing tokenization" data-secondary-sortas="customizing tokenization" id="ch4_term32"/>first step in the pipeline, and everything depends on the correct tokens. spaCy’s tokenizer does a good job in most cases, but it splits on hash signs, hyphens, and underscores, which is sometimes not what you want. Therefore, it may be necessary to adjust its behavior. Let’s look at the following text as an example:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">text</code> <code class="o">=</code> <code class="s2">"@Pete: choose low-carb #food #eat-smart. _url_ ;-) 😋👍"</code>
<code class="n">doc</code> <code class="o">=</code> <code class="n">nlp</code><code class="p">(</code><code class="n">text</code><code class="p">)</code>

<code class="k">for</code> <code class="n">token</code> <code class="ow">in</code> <code class="n">doc</code><code class="p">:</code>
    <code class="k">print</code><code class="p">(</code><code class="n">token</code><code class="p">,</code> <code class="n">end</code><code class="o">=</code><code class="s2">"|"</code><code class="p">)</code>
</pre>

<p class="pagebreak-before"><code>Out:</code></p>

<pre data-type="programlisting">
@Pete|:|choose|low|-|carb|#|food|#|eat|-|smart|.|_|url|_|;-)|😋|👍|
</pre>

<p>spaCy’s tokenizer is completely rule-based. First, it splits the text on whitespace characters. Then it uses <a contenteditable="false" data-type="indexterm" data-primary="prefixes in text analysis" id="idm45634201028536"/>prefix, <a contenteditable="false" data-type="indexterm" data-primary="suffixes in text analysis" id="idm45634201027336"/>suffix, and infix splitting rules defined by regular expressions to further split the remaining tokens. Exception rules are used to handle language-specific exceptions like <em>can’t</em>, which should be split into <em>ca</em> and <em>n’t</em> with lemmas <em>can</em> and <em>not</em>.<sup><a data-type="noteref" id="idm45634201023736-marker" href="ch04.xhtml#idm45634201023736">9</a></sup> </p>

<p>As you can see in the example, spaCy’s English tokenizer contains an infix rule for splits at hyphens. In addition, it has a prefix rule to split off characters like <code>#</code> or <code>_</code>. It works well for tokens prefixed with <code>@</code> and emojis, though.</p>

<p>One option is to merge tokens in a postprocessing step using <code>doc.retokenize</code>. However, that will not fix any miscalculated part-of-speech tags and syntactical dependencies because these rely on tokenization. So it may be better to change the tokenization rules and create correct tokens in the first place.</p>

<p>The best approach for this is to create your own variant of the tokenizer with individual rules for infix, prefix, and suffix splitting.<sup><a data-type="noteref" id="idm45634201018552-marker" href="ch04.xhtml#idm45634201018552">10</a></sup> The following function creates a tokenizer object with individual rules in a “minimally invasive” way: we just drop the respective patterns from spaCy’s default rules but retain the major part of the logic:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="kn">from</code> <code class="nn">spacy.tokenizer</code> <code class="kn">import</code> <code class="n">Tokenizer</code>
<code class="kn">from</code> <code class="nn">spacy.util</code> <code class="kn">import</code> <code class="n">compile_prefix_regex</code><code class="p">,</code> \
                       <code class="n">compile_infix_regex</code><code class="p">,</code> <code class="n">compile_suffix_regex</code>

<code class="k">def</code> <code class="nf">custom_tokenizer</code><code class="p">(</code><code class="n">nlp</code><code class="p">):</code>

    <code class="c1"># use default patterns except the ones matched by re.search</code>
    <code class="n">prefixes</code> <code class="o">=</code> <code class="p">[</code><code class="n">pattern</code> <code class="k">for</code> <code class="n">pattern</code> <code class="ow">in</code> <code class="n">nlp</code><code class="o">.</code><code class="n">Defaults</code><code class="o">.</code><code class="n">prefixes</code>
                <code class="k">if</code> <code class="n">pattern</code> <code class="ow">not</code> <code class="ow">in</code> <code class="p">[</code><code class="s1">'-'</code><code class="p">,</code> <code class="s1">'_'</code><code class="p">,</code> <code class="s1">'#'</code><code class="p">]]</code>
    <code class="n">suffixes</code> <code class="o">=</code> <code class="p">[</code><code class="n">pattern</code> <code class="k">for</code> <code class="n">pattern</code> <code class="ow">in</code> <code class="n">nlp</code><code class="o">.</code><code class="n">Defaults</code><code class="o">.</code><code class="n">suffixes</code>
                <code class="k">if</code> <code class="n">pattern</code> <code class="ow">not</code> <code class="ow">in</code> <code class="p">[</code><code class="s1">'_'</code><code class="p">]]</code>
    <code class="n">infixes</code>  <code class="o">=</code> <code class="p">[</code><code class="n">pattern</code> <code class="k">for</code> <code class="n">pattern</code> <code class="ow">in</code> <code class="n">nlp</code><code class="o">.</code><code class="n">Defaults</code><code class="o">.</code><code class="n">infixes</code>
                <code class="k">if</code> <code class="ow">not</code> <code class="n">re</code><code class="o">.</code><code class="n">search</code><code class="p">(</code><code class="n">pattern</code><code class="p">,</code> <code class="s1">'xx-xx'</code><code class="p">)]</code>

    <code class="k">return</code> <code class="n">Tokenizer</code><code class="p">(</code><code class="n">vocab</code>          <code class="o">=</code> <code class="n">nlp</code><code class="o">.</code><code class="n">vocab</code><code class="p">,</code>
                     <code class="n">rules</code>          <code class="o">=</code> <code class="n">nlp</code><code class="o">.</code><code class="n">Defaults</code><code class="o">.</code><code class="n">tokenizer_exceptions</code><code class="p">,</code>
                     <code class="n">prefix_search</code>  <code class="o">=</code> <code class="n">compile_prefix_regex</code><code class="p">(</code><code class="n">prefixes</code><code class="p">)</code><code class="o">.</code><code class="n">search</code><code class="p">,</code>
                     <code class="n">suffix_search</code>  <code class="o">=</code> <code class="n">compile_suffix_regex</code><code class="p">(</code><code class="n">suffixes</code><code class="p">)</code><code class="o">.</code><code class="n">search</code><code class="p">,</code>
                     <code class="n">infix_finditer</code> <code class="o">=</code> <code class="n">compile_infix_regex</code><code class="p">(</code><code class="n">infixes</code><code class="p">)</code><code class="o">.</code><code class="n">finditer</code><code class="p">,</code>
                     <code class="n">token_match</code>    <code class="o">=</code> <code class="n">nlp</code><code class="o">.</code><code class="n">Defaults</code><code class="o">.</code><code class="n">token_match</code><code class="p">)</code>

<code class="n">nlp</code> <code class="o">=</code> <code class="n">spacy</code><code class="o">.</code><code class="n">load</code><code class="p">(</code><code class="s1">'en_core_web_sm'</code><code class="p">)</code>
<code class="n">nlp</code><code class="o">.</code><code class="n">tokenizer</code> <code class="o">=</code> <code class="n">custom_tokenizer</code><code class="p">(</code><code class="n">nlp</code><code class="p">)</code>

<code class="n">doc</code> <code class="o">=</code> <code class="n">nlp</code><code class="p">(</code><code class="n">text</code><code class="p">)</code>
<code class="k">for</code> <code class="n">token</code> <code class="ow">in</code> <code class="n">doc</code><code class="p">:</code>
    <code class="k">print</code><code class="p">(</code><code class="n">token</code><code class="p">,</code> <code class="n">end</code><code class="o">=</code><code class="s2">"|"</code><code class="p">)</code>
</pre>

<p><code>Out:</code></p>

<pre data-type="programlisting">
@Pete|:|choose|low-carb|#food|#eat-smart|.|_url_|;-)|😋|👍|
</pre>

<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>Be careful <a contenteditable="false" data-type="indexterm" data-primary="tokenization/tokens" data-secondary="recommendations for" id="idm45634201012056"/>with tokenization modifications because their effects can be subtle, and fixing a set of cases can break another set of cases. For example, with our modification, tokens like <code>Chicago-based</code> won’t be split anymore. In addition, there are several Unicode characters for hyphens and dashes that could cause problems if they have not <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch4_term32" id="idm45634200824264"/>been normalized.</p></div>
</div></section>

<section data-type="sect2" class="blueprint" data-pdf-bookmark="Blueprint: Working with Stop Words"><div class="sect2" id="idm45634201057080">
<h2>Blueprint: Working with Stop Words</h2>
<p>spaCy uses language-specific <a contenteditable="false" data-type="indexterm" data-primary="spaCy, linguistic processing with" data-secondary="for stop words" data-secondary-sortas="stop words" id="idm45634200820824"/><a contenteditable="false" data-type="indexterm" data-primary="stop words" id="idm45634200819112"/><a contenteditable="false" data-type="indexterm" data-primary="words" data-secondary="stop words" id="idm45634200818008"/><a contenteditable="false" data-type="indexterm" data-primary="text data preparation" data-secondary="with stop words" data-secondary-sortas="stop words" id="idm45634200816632"/>stop word lists to set the <code>is_stop</code> property for each token directly after tokenization. Thus, filtering stop words (and similarly punctuation tokens) is easy:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">text</code> <code class="o">=</code> <code class="s2">"Dear Ryan, we need to sit down and talk. Regards, Pete"</code>
<code class="n">doc</code> <code class="o">=</code> <code class="n">nlp</code><code class="p">(</code><code class="n">text</code><code class="p">)</code>

<code class="n">non_stop</code> <code class="o">=</code> <code class="p">[</code><code class="n">t</code> <code class="k">for</code> <code class="n">t</code> <code class="ow">in</code> <code class="n">doc</code> <code class="k">if</code> <code class="ow">not</code> <code class="n">t</code><code class="o">.</code><code class="n">is_stop</code> <code class="ow">and</code> <code class="ow">not</code> <code class="n">t</code><code class="o">.</code><code class="n">is_punct</code><code class="p">]</code>
<code class="k">print</code><code class="p">(</code><code class="n">non_stop</code><code class="p">)</code>
</pre>

<p><code>Out:</code></p>

<pre data-type="programlisting">
[Dear, Ryan, need, sit, talk, Regards, Pete]
</pre>

<p>The list of English stop words with more than 300 entries can be accessed by importing <code>spacy.lang.en.STOP_WORDS</code>. When an <code>nlp</code> object is created, this list is loaded and stored under <code>nlp.Defaults.stop_words</code>.
We can modify spaCy’s default behavior by setting the <code>is_stop</code> property of the respective words in spaCy’s vocabulary:<sup><a data-type="noteref" id="idm45634200775880-marker" href="ch04.xhtml#idm45634200775880">11</a></sup> </p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">nlp</code> <code class="o">=</code> <code class="n">spacy</code><code class="o">.</code><code class="n">load</code><code class="p">(</code><code class="s1">'en_core_web_sm'</code><code class="p">)</code>
<code class="n">nlp</code><code class="o">.</code><code class="n">vocab</code><code class="p">[</code><code class="s1">'down'</code><code class="p">]</code><code class="o">.</code><code class="n">is_stop</code> <code class="o">=</code> <code class="bp">False</code>
<code class="n">nlp</code><code class="o">.</code><code class="n">vocab</code><code class="p">[</code><code class="s1">'Dear'</code><code class="p">]</code><code class="o">.</code><code class="n">is_stop</code> <code class="o">=</code> <code class="bp">True</code>
<code class="n">nlp</code><code class="o">.</code><code class="n">vocab</code><code class="p">[</code><code class="s1">'Regards'</code><code class="p">]</code><code class="o">.</code><code class="n">is_stop</code> <code class="o">=</code> <code class="bp">True</code>
</pre>

<p>If we rerun the previous example, we get the following result:</p>

<pre data-type="programlisting">
[Ryan, need, sit, down, talk, Pete]
</pre>
</div></section>

<section data-type="sect2" class="blueprint" data-pdf-bookmark="Blueprint: Extracting Lemmas Based on Part of Speech"><div class="sect2" id="idm45634200822328">
<h2>Blueprint: Extracting Lemmas Based on Part of Speech</h2>
<p><em>Lemmatization</em> is the <a contenteditable="false" data-type="indexterm" data-primary="part-of-speech tags" id="ch4_term33"/><a contenteditable="false" data-type="indexterm" data-primary="spaCy, linguistic processing with" data-secondary="for lemmas with part-of-speech, extracting" data-secondary-sortas="lemmas with part-of-speech, extracting" id="ch4_term34"/><a contenteditable="false" data-type="indexterm" data-primary="lemmatization" id="ch4_term35"/><a contenteditable="false" data-type="indexterm" data-primary="text data preparation" data-secondary="lemmatization for" id="ch4_term50"/>mapping of a word to its uninflected root.
Treating words like <em>housing</em>, <em>housed</em>, and <em>house</em> as the same has many advantages for statistics, machine learning, and information retrieval. It can not only improve the quality of the models but also decrease training time and model size because the vocabulary is much smaller if only uninflected forms are kept. In addition, it is often helpful to restrict the types of the words used to certain categories, such as nouns, verbs, and adjectives. Those word types are called <em>part-of-speech tags</em>.</p>

<p>Let’s first take a closer look at lemmatization. The lemma of a token or span can be accessed by the <code>lemma_</code> property, as illustrated in the following example:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">text</code> <code class="o">=</code> <code class="s2">"My best friend Ryan Peters likes fancy adventure games."</code>
<code class="n">doc</code> <code class="o">=</code> <code class="n">nlp</code><code class="p">(</code><code class="n">text</code><code class="p">)</code>

<code class="k">print</code><code class="p">(</code><code class="o">*</code><code class="p">[</code><code class="n">t</code><code class="o">.</code><code class="n">lemma_</code> <code class="k">for</code> <code class="n">t</code> <code class="ow">in</code> <code class="n">doc</code><code class="p">],</code> <code class="n">sep</code><code class="o">=</code><code class="s1">'|'</code><code class="p">)</code>
</pre>

<p><code>Out:</code></p>

<pre data-type="programlisting">
-PRON-|good|friend|Ryan|Peters|like|fancy|adventure|game|.
</pre>

<p>The correct assignment of the lemma requires a lookup dictionary and knowledge about the part of speech of a word. For example, the lemma of the noun <em>meeting</em> is <em>meeting</em>, while the lemma of the verb is <em>meet</em>. In English, spaCy is able to make this distinction. In most other languages, however, lemmatization is purely dictionary-based, ignoring the part-of-speech dependency. Note that personal pronouns like <em>I</em>, <em>me</em>, <em>you</em>, and <em>her</em> always get the lemma <code>-PRON-</code> in spaCy. </p>

<p>The other token attribute we will use in this blueprint is the part-of-speech tag.
<a data-type="xref" href="#tab-nlp-result">Table 4-3</a> shows that each token in a spaCy doc has two part-of-speech attributes: <code>pos_</code> and <code>tag_</code>. <code>tag_</code> is the tag from the tagset used to train the model. For spaCy’s English models, which have been trained on the OntoNotes 5 corpus, this is the Penn Treebank tagset. For a German model, this would be the Stuttgart-Tübingen tagset. The <code>pos_</code> attribute contains the simplified tag of the <a contenteditable="false" data-type="indexterm" data-primary="universal part-of-speech tagset" id="idm45634200546184"/>universal part-of-speech tagset.<sup><a data-type="noteref" id="idm45634200544984-marker" href="ch04.xhtml#idm45634200544984">12</a></sup> We recommend using this attribute as the values will remain stable across different models.
<a data-type="xref" href="#tab-pos-tags">Table 4-4</a> shows the complete tag set descriptions.</p>

<table id="tab-pos-tags"><caption><span class="label">Table 4-4. </span>Universal part-of-speech tags</caption><thead>
<tr>
<th>Tag</th>

<th>Description</th>

<th>Examples</th></tr>
</thead>
<tbody>
<tr>
<td>ADJ</td>
<td>Adjectives (describe nouns)</td>
<td>big, green, African</td></tr>
<tr>
<td>ADP</td>
<td>Adpositions (prepositions and postpositions)</td>
<td>in, on</td></tr>
<tr>
<td>ADV</td>
<td>Adverbs (modify verbs or adjectives)</td>
<td>very, exactly, always</td></tr>
<tr>
<td>AUX</td>
<td>Auxiliary (accompanies verb)</td>
<td>can (do), is (doing)</td></tr>
<tr>
<td>CCONJ</td>
<td>Connecting conjunction</td>
<td>and, or, but</td></tr>
<tr>
<td>DET</td>
<td>Determiner (with regard to nouns)</td>
<td>the, a, all (things), your (idea)</td></tr>
<tr>
<td>INTJ</td>
<td>Interjection (independent word, exclamation, expression of emotion)</td>
<td>hi, yeah</td></tr>
<tr>
<td>NOUN</td>
<td>Nouns (common and proper)</td>
<td>house, computer</td></tr>
<tr>
<td>NUM</td>
<td>Cardinal numbers</td>
<td>nine, 9, IX</td></tr>
<tr>
<td>PROPN</td>
<td>Proper noun, name, or part of a name</td>
<td>Peter, Berlin</td></tr>
<tr>
<td>PRON</td>
<td>Pronoun, substitute for noun</td>
<td>I, you, myself, who</td></tr>
<tr>
<td>PART</td>
<td>Particle (makes sense only with other word)</td>
<td/></tr>
<tr>
<td>PUNCT</td>
<td>Punctuation characters</td>
<td>, . ;</td></tr>
<tr>
<td>SCONJ</td>
<td>Subordinating conjunction</td>
<td>before, since, if</td></tr>
<tr>
<td>SYM</td>
<td>Symbols (word-like)</td>
<td>$, ©</td></tr>
<tr>
<td>VERB</td>
<td>Verbs (all tenses and modes)</td>
<td>go, went, thinking</td></tr>
<tr>
<td>X</td>
<td>Anything that cannot be assigned</td>
<td>grlmpf</td></tr>
</tbody>
</table>

<p>Part-of-speech tags are an excellent alternative to <a contenteditable="false" data-type="indexterm" data-primary="stop words" id="idm45634200492248"/><a contenteditable="false" data-type="indexterm" data-primary="words" data-secondary="stop words" id="idm45634200491144"/><a contenteditable="false" data-type="indexterm" data-primary="text data preparation" data-secondary="with stop words" data-secondary-sortas="stop words" id="idm45634200489768"/>stop words as word filters.
In <a contenteditable="false" data-type="indexterm" data-primary="function words" id="idm45634200487944"/><a contenteditable="false" data-type="indexterm" data-primary="words" data-secondary="function vs. content" id="idm45634200486840"/>linguistics, pronouns, prepositions, conjunctions, and determiners are called <em>function words</em> because their main function is to create grammatical relationships within a sentence. Nouns, verbs, adjectives, and adverbs are content words, and the meaning of a sentence depends mainly on them.</p>

<p>Often, we are interested only in content words. Thus, instead of using a stop word list, we can use part-of-speech tags to select the word types we are interested in and discard the rest. For example, a list containing only  the nouns and proper nouns in a <code>doc</code> can be generated like this:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">text</code> <code class="o">=</code> <code class="s2">"My best friend Ryan Peters likes fancy adventure games."</code>
<code class="n">doc</code> <code class="o">=</code> <code class="n">nlp</code><code class="p">(</code><code class="n">text</code><code class="p">)</code>

<code class="n">nouns</code> <code class="o">=</code> <code class="p">[</code><code class="n">t</code> <code class="k">for</code> <code class="n">t</code> <code class="ow">in</code> <code class="n">doc</code> <code class="k">if</code> <code class="n">t</code><code class="o">.</code><code class="n">pos_</code> <code class="ow">in</code> <code class="p">[</code><code class="s1">'NOUN'</code><code class="p">,</code> <code class="s1">'PROPN'</code><code class="p">]]</code>
<code class="k">print</code><code class="p">(</code><code class="n">nouns</code><code class="p">)</code>
</pre>

<p><code>Out:</code></p>

<pre data-type="programlisting">
[friend, Ryan, Peters, adventure, games]
</pre>

<p>We could easily define a more general filter function for this purpose, but <a contenteditable="false" data-type="indexterm" data-primary="textacy library" id="ch4_term36"/><a contenteditable="false" data-type="indexterm" data-primary="extract.words function (textacy)" id="idm45634200445944"/>textacy’s <code>extract.words</code> function conveniently provides this functionality. It also allows us to filter on part of speech and additional token properties such as <code>is_punct</code> or <code>is_stop</code>. Thus, the filter function allows both part-of-speech selection and stop word filtering. Internally it works just like we illustrated for the noun filter shown previously.</p>

<p>The following example shows how to extract tokens for adjectives and nouns from the sample sentence:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="kn">import</code> <code class="nn">textacy</code>

<code class="n">tokens</code> <code class="o">=</code> <code class="n">textacy</code><code class="o">.</code><code class="n">extract</code><code class="o">.</code><code class="n">words</code><code class="p">(</code><code class="n">doc</code><code class="p">,</code>
            <code class="n">filter_stops</code> <code class="o">=</code> <code class="bp">True</code><code class="p">,</code>           <code class="c1"># default True, no stopwords</code>
            <code class="n">filter_punct</code> <code class="o">=</code> <code class="bp">True</code><code class="p">,</code>           <code class="c1"># default True, no punctuation</code>
            <code class="n">filter_nums</code> <code class="o">=</code> <code class="bp">True</code><code class="p">,</code>            <code class="c1"># default False, no numbers</code>
            <code class="n">include_pos</code> <code class="o">=</code> <code class="p">[</code><code class="s1">'ADJ'</code><code class="p">,</code> <code class="s1">'NOUN'</code><code class="p">],</code> <code class="c1"># default None = include all</code>
            <code class="n">exclude_pos</code> <code class="o">=</code> <code class="bp">None</code><code class="p">,</code>            <code class="c1"># default None = exclude none</code>
            <code class="n">min_freq</code> <code class="o">=</code> <code class="mi">1</code><code class="p">)</code>                  <code class="c1"># minimum frequency of words</code>

<code class="k">print</code><code class="p">(</code><code class="o">*</code><code class="p">[</code><code class="n">t</code> <code class="k">for</code> <code class="n">t</code> <code class="ow">in</code> <code class="n">tokens</code><code class="p">],</code> <code class="n">sep</code><code class="o">=</code><code class="s1">'|'</code><code class="p">)</code>
</pre>

<p><code>Out:</code></p>

<pre data-type="programlisting">
best|friend|fancy|adventure|games
</pre>

<p>Our blueprint function to extract a filtered list of word lemmas is finally just a tiny wrapper around that function. By forwarding the keyword arguments (<code>**kwargs</code>), this function accepts the same parameters as textacy’s <code>extract.words</code>.</p>

<pre data-code-language="python" data-type="programlisting">
<code class="k">def</code> <code class="nf">extract_lemmas</code><code class="p">(</code><code class="n">doc</code><code class="p">,</code> <code class="o">**</code><code class="n">kwargs</code><code class="p">):</code>
    <code class="k">return</code> <code class="p">[</code><code class="n">t</code><code class="o">.</code><code class="n">lemma_</code> <code class="k">for</code> <code class="n">t</code> <code class="ow">in</code> <code class="n">textacy</code><code class="o">.</code><code class="n">extract</code><code class="o">.</code><code class="n">words</code><code class="p">(</code><code class="n">doc</code><code class="p">,</code> <code class="o">**</code><code class="n">kwargs</code><code class="p">)]</code>

<code class="n">lemmas</code> <code class="o">=</code> <code class="n">extract_lemmas</code><code class="p">(</code><code class="n">doc</code><code class="p">,</code> <code class="n">include_pos</code><code class="o">=</code><code class="p">[</code><code class="s1">'ADJ'</code><code class="p">,</code> <code class="s1">'NOUN'</code><code class="p">])</code>
<code class="k">print</code><code class="p">(</code><code class="o">*</code><code class="n">lemmas</code><code class="p">,</code> <code class="n">sep</code><code class="o">=</code><code class="s1">'|'</code><code class="p">)</code>
</pre>

<p><code>Out:</code></p>

<pre data-type="programlisting">
good|friend|fancy|adventure|game
</pre>

<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Using lemmas instead of inflected words is often a good idea, but not always. For example, it can have a negative effect on sentiment analysis where “good” and “best” make a difference.</p></div>
</div></section>

<section data-type="sect2" class="blueprint pagebreak-before less_space" data-pdf-bookmark="Blueprint: Extracting Noun Phrases"><div class="sect2" id="idm45634200623768">
<h2>Blueprint: Extracting Noun Phrases</h2>
<p>In <a data-type="xref" href="ch01.xhtml#ch-exploration">Chapter 1</a> we illustrated <a contenteditable="false" data-type="indexterm" data-primary="spaCy, linguistic processing with" data-secondary="for noun phrases, extracting" data-secondary-sortas="noun phrases, extracting" id="idm45634200251352"/><a contenteditable="false" data-type="indexterm" data-primary="noun phrases, extracting" id="idm45634200249720"/><a contenteditable="false" data-type="indexterm" data-primary="n-grams" id="idm45634200248600"/>how to use n-grams for analysis. n-grams are simple enumerations of subsequences of <em>n</em> words in a sentence. For example, the sentence we used earlier contains the following <a contenteditable="false" data-type="indexterm" data-primary="bigrams" id="idm45634200246952"/>bigrams:</p>

<pre data-type="programlisting">
My_best|best_friend|friend_Ryan|Ryan_Peters|Peters_likes|likes_fancy
fancy_adventure|adventure_games
</pre>

<p>Many of those bigrams are not very useful for analysis, for example, <code>likes_fancy</code> or <code>my_best</code>. It would be even worse for trigrams. But how can we detect word sequences that have real meaning? One way is to apply <a contenteditable="false" data-type="indexterm" data-primary="rule-based matcher (spaCy)" id="idm45634200243368"/><a contenteditable="false" data-type="indexterm" data-primary="pattern-matching" id="idm45634200242296"/><a contenteditable="false" data-type="indexterm" data-primary="spaCy, linguistic processing with" data-secondary="for matching patterns" data-secondary-sortas="matching patterns" id="idm45634200241192"/>pattern-matching on the part-of-speech tags. spaCy has a quite powerful <a href="https://oreil.ly/VjOJK">rule-based matcher</a>, and textacy has a convenient wrapper for <a href="https://oreil.ly/Xz70U">pattern-based phrase extraction</a>. The following pattern extracts sequences of nouns with a preceding <span class="keep-together">adjective</span>:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">text</code> <code class="o">=</code> <code class="s2">"My best friend Ryan Peters likes fancy adventure games."</code>
<code class="n">doc</code> <code class="o">=</code> <code class="n">nlp</code><code class="p">(</code><code class="n">text</code><code class="p">)</code>

<code class="n">patterns</code> <code class="o">=</code> <code class="p">[</code><code class="s2">"POS:ADJ POS:NOUN:+"</code><code class="p">]</code>
<code class="n">spans</code> <code class="o">=</code> <code class="n">textacy</code><code class="o">.</code><code class="n">extract</code><code class="o">.</code><code class="n">matches</code><code class="p">(</code><code class="n">doc</code><code class="p">,</code> <code class="n">patterns</code><code class="o">=</code><code class="n">patterns</code><code class="p">)</code>
<code class="k">print</code><code class="p">(</code><code class="o">*</code><code class="p">[</code><code class="n">s</code><code class="o">.</code><code class="n">lemma_</code> <code class="k">for</code> <code class="n">s</code> <code class="ow">in</code> <code class="n">spans</code><code class="p">],</code> <code class="n">sep</code><code class="o">=</code><code class="s1">'|'</code><code class="p">)</code>
</pre>

<p><code>Out:</code></p>

<pre data-type="programlisting">
good friend|fancy adventure|fancy adventure game
</pre>

<p>Alternatively, you could use spaCy’s <code>doc.noun_chunks</code> for noun phrase extraction. However, as the returned chunks can also include pronouns and determiners, this function is less suited for feature extraction:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="k">print</code><code class="p">(</code><code class="o">*</code><code class="n">doc</code><code class="o">.</code><code class="n">noun_chunks</code><code class="p">,</code> <code class="n">sep</code><code class="o">=</code><code class="s1">'|'</code><code class="p">)</code>
</pre>

<p><code>Out:</code></p>

<pre data-type="programlisting">
My best friend|Ryan Peters|fancy adventure games
</pre>

<p>Thus, we define our blueprint for noun phrase extraction based on part-of-speech patterns. The function takes a <code>doc</code>, a list of part-of-speech tags, and a separator character to join the words of the noun phrase. The constructed pattern searches for sequences of nouns that are preceded by a token with one of the specified part-of-speech tags. Returned are the lemmas. Our example extracts all phrases consisting of an adjective or a noun followed by a <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch4_term33" id="idm45634200100600"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch4_term34" id="idm45634200164024"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch4_term35" id="idm45634200162648"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch4_term50" id="idm45634200161272"/>sequence of nouns:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="k">def</code> <code class="nf">extract_noun_phrases</code><code class="p">(</code><code class="n">doc</code><code class="p">,</code> <code class="n">preceding_pos</code><code class="o">=</code><code class="p">[</code><code class="s1">'NOUN'</code><code class="p">],</code> <code class="n">sep</code><code class="o">=</code><code class="s1">'_'</code><code class="p">):</code>
    <code class="n">patterns</code> <code class="o">=</code> <code class="p">[]</code>
    <code class="k">for</code> <code class="n">pos</code> <code class="ow">in</code> <code class="n">preceding_pos</code><code class="p">:</code>
        <code class="n">patterns</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">f</code><code class="s2">"POS:{pos} POS:NOUN:+"</code><code class="p">)</code>
    <code class="n">spans</code> <code class="o">=</code> <code class="n">textacy</code><code class="o">.</code><code class="n">extract</code><code class="o">.</code><code class="n">matches</code><code class="p">(</code><code class="n">doc</code><code class="p">,</code> <code class="n">patterns</code><code class="o">=</code><code class="n">patterns</code><code class="p">)</code>
    <code class="k">return</code> <code class="p">[</code><code class="n">sep</code><code class="o">.</code><code class="n">join</code><code class="p">([</code><code class="n">t</code><code class="o">.</code><code class="n">lemma_</code> <code class="k">for</code> <code class="n">t</code> <code class="ow">in</code> <code class="n">s</code><code class="p">])</code> <code class="k">for</code> <code class="n">s</code> <code class="ow">in</code> <code class="n">spans</code><code class="p">]</code>

<code class="k">print</code><code class="p">(</code><code class="o">*</code><code class="n">extract_noun_phrases</code><code class="p">(</code><code class="n">doc</code><code class="p">,</code> <code class="p">[</code><code class="s1">'ADJ'</code><code class="p">,</code> <code class="s1">'NOUN'</code><code class="p">]),</code> <code class="n">sep</code><code class="o">=</code><code class="s1">'|'</code><code class="p">)</code>
</pre>

<p><code>Out:</code></p>

<pre data-type="programlisting">
good_friend|fancy_adventure|fancy_adventure_game|adventure_game
</pre>
</div></section>

<section data-type="sect2" class="blueprint" data-pdf-bookmark="Blueprint: Extracting Named Entities"><div class="sect2" id="idm45634200254008">
<h2>Blueprint: Extracting Named Entities</h2>
<p><em>Named-entity</em> <a contenteditable="false" data-type="indexterm" data-primary="named-entity recognition (NER)" id="idm45634200011576"/><a contenteditable="false" data-type="indexterm" data-primary="spaCy, linguistic processing with" data-secondary="for named-entity recognition" data-secondary-sortas="named-entity recognition" id="idm45634200010472"/><a contenteditable="false" data-type="indexterm" data-primary="text data preparation" data-secondary="named entities in" id="idm45634200008920"/><em>recognition</em> refers to the process of detecting entities such as people, locations, or organizations in text. <a contenteditable="false" data-type="indexterm" data-primary="Span objects" id="idm45634200007016"/>Each entity can consist of one or more tokens, like <em>San Francisco</em>. Therefore, named entities are represented by <code>Span</code> objects.
As with noun phrases, it can be helpful to retrieve a list of named entities for further analysis. </p>

<p>If you look again at <a data-type="xref" href="#tab-nlp-result">Table 4-3</a>, you see the token attributes for named-entity recognition, <code>ent_type_</code> and <code>ent_iob_</code>. <code>ent_iob_</code> contains the information if a token begins an entity (<code>B</code>), is inside an entity (<code>I</code>), or is outside (<code>O</code>). Instead of iterating through the tokens, we can also access the named entities directly with <code>doc.ents</code>. Here, the property for the entity type is called <code>label_</code>. Let’s illustrate this with an example:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">text</code> <code class="o">=</code> <code class="s2">"James O'Neill, chairman of World Cargo Inc, lives in San Francisco."</code>
<code class="n">doc</code> <code class="o">=</code> <code class="n">nlp</code><code class="p">(</code><code class="n">text</code><code class="p">)</code>

<code class="k">for</code> <code class="n">ent</code> <code class="ow">in</code> <code class="n">doc</code><code class="o">.</code><code class="n">ents</code><code class="p">:</code>
    <code class="k">print</code><code class="p">(</code><code class="n">f</code><code class="s2">"({ent.text}, {ent.label_})"</code><code class="p">,</code> <code class="n">end</code><code class="o">=</code><code class="s2">" "</code><code class="p">)</code>
</pre>

<p><code>Out:</code></p>

<pre data-type="programlisting">
(James O'Neill, PERSON) (World Cargo Inc, ORG) (San Francisco, GPE)
</pre>

<p>spaCy’s <code>displacy</code> module <a contenteditable="false" data-type="indexterm" data-primary="displacy (spaCy)" id="idm45634199914664"/>also provides <a contenteditable="false" data-type="indexterm" data-primary="spaCy, linguistic processing with" data-secondary="visualizations for" id="idm45634199913400"/><a contenteditable="false" data-type="indexterm" data-primary="visualization of data" data-secondary="with spaCy displacy module" data-secondary-sortas="spaCy displacy module" id="idm45634199912024"/>visualization for the named-entity recognition, which makes the result much more readable and visually supports the identification of misclassified entities:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="kn">from</code> <code class="nn">spacy</code> <code class="kn">import</code> <code class="n">displacy</code>

<code class="n">displacy</code><code class="o">.</code><code class="n">render</code><code class="p">(</code><code class="n">doc</code><code class="p">,</code> <code class="n">style</code><code class="o">=</code><code class="s1">'ent'</code><code class="p">)</code>
</pre>

<figure><div class="figure">
<img src="Images/btap_04in01.jpg" width="991" height="57"/>
<h6/>
</div></figure>

<p>The named entities were identified correctly as a person, an organization, and a geo-political entity (GPE). But be aware that the accuracy for named-entity recognition may not be very good if your corpus is missing a clear grammatical structure. Check out <a data-type="xref" href="ch12.xhtml#ch12-ner">“Named-Entity Recognition”</a> for a detailed <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch4_term29" id="idm45634199881032"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch4_term30" id="idm45634199879624"/>discussion.</p>

<p>For the extraction of named entities of certain types, we again make use of one of textacy’s convenient functions:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="k">def</code> <code class="nf">extract_entities</code><code class="p">(</code><code class="n">doc</code><code class="p">,</code> <code class="n">include_types</code><code class="o">=</code><code class="bp">None</code><code class="p">,</code> <code class="n">sep</code><code class="o">=</code><code class="s1">'_'</code><code class="p">):</code>

    <code class="n">ents</code> <code class="o">=</code> <code class="n">textacy</code><code class="o">.</code><code class="n">extract</code><code class="o">.</code><code class="n">entities</code><code class="p">(</code><code class="n">doc</code><code class="p">,</code>
             <code class="n">include_types</code><code class="o">=</code><code class="n">include_types</code><code class="p">,</code>
             <code class="n">exclude_types</code><code class="o">=</code><code class="bp">None</code><code class="p">,</code>
             <code class="n">drop_determiners</code><code class="o">=</code><code class="bp">True</code><code class="p">,</code>
             <code class="n">min_freq</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>

    <code class="k">return</code> <code class="p">[</code><code class="n">sep</code><code class="o">.</code><code class="n">join</code><code class="p">([</code><code class="n">t</code><code class="o">.</code><code class="n">lemma_</code> <code class="k">for</code> <code class="n">t</code> <code class="ow">in</code> <code class="n">e</code><code class="p">])</code><code class="o">+</code><code class="s1">'/'</code><code class="o">+</code><code class="n">e</code><code class="o">.</code><code class="n">label_</code> <code class="k">for</code> <code class="n">e</code> <code class="ow">in</code> <code class="n">ents</code><code class="p">]</code>
</pre>

<p>With this function we can, for example, <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch4_term36" id="idm45634199875416"/>retrieve the named entities of types <code>PERSON</code> and <code>GPE</code> (geo-political entity) like this:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="k">print</code><code class="p">(</code><code class="n">extract_entities</code><code class="p">(</code><code class="n">doc</code><code class="p">,</code> <code class="p">[</code><code class="s1">'PERSON'</code><code class="p">,</code> <code class="s1">'GPE'</code><code class="p">]))</code>
</pre>

<p><code>Out:</code></p>

<pre data-type="programlisting">
["James_O'Neill/PERSON", 'San_Francisco/GPE']
</pre>

</div></section>
</div></section>

<section data-type="sect1" data-pdf-bookmark="Feature Extraction on a Large Dataset"><div class="sect1" id="ch04largedatasetfeatureextract">
<h1>Feature Extraction on a Large Dataset</h1>
<p>Now that we know the tools spaCy provides, we can <a contenteditable="false" data-type="indexterm" data-primary="feature extraction with spaCy" id="ch4_term37"/><a contenteditable="false" data-type="indexterm" data-primary="spaCy, linguistic processing with" data-secondary="for feature extraction on large dataset" data-secondary-sortas="feature extraction on large dataset" id="ch4_term38"/><a contenteditable="false" data-type="indexterm" data-primary="text data preparation" data-secondary="feature extraction with spaCy for" id="ch4_term39"/>finally build our linguistic feature extractor. <a data-type="xref" href="#fig-feature-extraction">Figure 4-3</a> illustrates what we are going to do. In the end, we want to create a dataset that can be used as input to statistical analysis and various machine learning algorithms. Once extracted, we will persist the preprocessed data “ready to use” in a database.</p>

<figure class="pagebreak-after"><div id="fig-feature-extraction" class="figure">
<img src="Images/btap_0403.jpg" width="1174" height="321"/>
<h6><span class="label">Figure 4-3. </span>Feature extraction from text with spaCy.</h6></div></figure>


<section data-type="sect2" class="blueprint pagebreak-after" data-pdf-bookmark="Blueprint: Creating One Function to Get It All"><div class="sect2" id="idm45634199762488">
<h2>Blueprint: Creating One Function to Get It All</h2>
<p>This blueprint function combines all the extraction functions from the previous section. It neatly puts everything we want to extract in one place in the code so that the subsequent steps do not need to be adjusted if you add or change something here:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="k">def</code> <code class="nf">extract_nlp</code><code class="p">(</code><code class="n">doc</code><code class="p">):</code>
    <code class="k">return</code> <code class="p">{</code>
    <code class="s1">'lemmas'</code>          <code class="p">:</code> <code class="n">extract_lemmas</code><code class="p">(</code><code class="n">doc</code><code class="p">,</code>
                                     <code class="n">exclude_pos</code> <code class="o">=</code> <code class="p">[</code><code class="s1">'PART'</code><code class="p">,</code> <code class="s1">'PUNCT'</code><code class="p">,</code>
                                        <code class="s1">'DET'</code><code class="p">,</code> <code class="s1">'PRON'</code><code class="p">,</code> <code class="s1">'SYM'</code><code class="p">,</code> <code class="s1">'SPACE'</code><code class="p">],</code>
                                     <code class="n">filter_stops</code> <code class="o">=</code> <code class="bp">False</code><code class="p">),</code>
    <code class="s1">'adjs_verbs'</code>      <code class="p">:</code> <code class="n">extract_lemmas</code><code class="p">(</code><code class="n">doc</code><code class="p">,</code> <code class="n">include_pos</code> <code class="o">=</code> <code class="p">[</code><code class="s1">'ADJ'</code><code class="p">,</code> <code class="s1">'VERB'</code><code class="p">]),</code>
    <code class="s1">'nouns'</code>           <code class="p">:</code> <code class="n">extract_lemmas</code><code class="p">(</code><code class="n">doc</code><code class="p">,</code> <code class="n">include_pos</code> <code class="o">=</code> <code class="p">[</code><code class="s1">'NOUN'</code><code class="p">,</code> <code class="s1">'PROPN'</code><code class="p">]),</code>
    <code class="s1">'noun_phrases'</code>    <code class="p">:</code> <code class="n">extract_noun_phrases</code><code class="p">(</code><code class="n">doc</code><code class="p">,</code> <code class="p">[</code><code class="s1">'NOUN'</code><code class="p">]),</code>
    <code class="s1">'adj_noun_phrases'</code><code class="p">:</code> <code class="n">extract_noun_phrases</code><code class="p">(</code><code class="n">doc</code><code class="p">,</code> <code class="p">[</code><code class="s1">'ADJ'</code><code class="p">]),</code>
    <code class="s1">'entities'</code>        <code class="p">:</code> <code class="n">extract_entities</code><code class="p">(</code><code class="n">doc</code><code class="p">,</code> <code class="p">[</code><code class="s1">'PERSON'</code><code class="p">,</code> <code class="s1">'ORG'</code><code class="p">,</code> <code class="s1">'GPE'</code><code class="p">,</code> <code class="s1">'LOC'</code><code class="p">])</code>
    <code class="p">}</code>
</pre>

<p>The function returns a dictionary with everything we want to extract, as shown in this example:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">text</code> <code class="o">=</code> <code class="s2">"My best friend Ryan Peters likes fancy adventure games."</code>
<code class="n">doc</code> <code class="o">=</code> <code class="n">nlp</code><code class="p">(</code><code class="n">text</code><code class="p">)</code>
<code class="k">for</code> <code class="n">col</code><code class="p">,</code> <code class="n">values</code> <code class="ow">in</code> <code class="n">extract_nlp</code><code class="p">(</code><code class="n">doc</code><code class="p">)</code><code class="o">.</code><code class="n">items</code><code class="p">():</code>
    <code class="k">print</code><code class="p">(</code><code class="n">f</code><code class="s2">"{col}: {values}"</code><code class="p">)</code>
</pre>

<p><code>Out:</code></p>

<pre data-type="programlisting">
lemmas: ['good', 'friend', 'Ryan', 'Peters', 'like', 'fancy', 'adventure', \
         'game']
adjs_verbs: ['good', 'like', 'fancy']
nouns: ['friend', 'Ryan', 'Peters', 'adventure', 'game']
noun_phrases: ['adventure_game']
adj_noun_phrases: ['good_friend', 'fancy_adventure', 'fancy_adventure_game']
entities: ['Ryan_Peters/PERSON']
</pre>

<p>The list of returned column names is needed for the next steps. Instead of hard-coding it, we just call <code>extract_nlp</code> with an empty document to retrieve the list:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">nlp_columns</code> <code class="o">=</code> <code class="nb">list</code><code class="p">(</code><code class="n">extract_nlp</code><code class="p">(</code><code class="n">nlp</code><code class="o">.</code><code class="n">make_doc</code><code class="p">(</code><code class="s1">''</code><code class="p">))</code><code class="o">.</code><code class="n">keys</code><code class="p">())</code>
<code class="k">print</code><code class="p">(</code><code class="n">nlp_columns</code><code class="p">)</code>
</pre>

<p><code>Out:</code></p>

<pre data-type="programlisting">
['lemmas', 'adjs_verbs', 'nouns', 'noun_phrases', 'adj_noun_phrases', 'entities']
</pre>
</div></section>

<section data-type="sect2" class="blueprint" data-pdf-bookmark="Blueprint: Using spaCy on a Large Dataset"><div class="sect2" id="idm45634199761896">
<h2>Blueprint: Using spaCy on a Large Dataset</h2>
<p>Now we can use this function to extract features from <a contenteditable="false" data-type="indexterm" data-primary="text data preparation" data-secondary="with large dataset" data-secondary-sortas="large dataset" id="ch4_term40"/>all the records of a dataset. We take the cleaned texts that we created and saved at the beginning of this chapter and add the titles:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">db_name</code> <code class="o">=</code> <code class="s2">"reddit-selfposts.db"</code>
<code class="n">con</code> <code class="o">=</code> <code class="n">sqlite3</code><code class="o">.</code><code class="n">connect</code><code class="p">(</code><code class="n">db_name</code><code class="p">)</code>
<code class="n">df</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">read_sql</code><code class="p">(</code><code class="s2">"select * from posts_cleaned"</code><code class="p">,</code> <code class="n">con</code><code class="p">)</code>
<code class="n">con</code><code class="o">.</code><code class="n">close</code><code class="p">()</code>

<code class="n">df</code><code class="p">[</code><code class="s1">'text'</code><code class="p">]</code> <code class="o">=</code> <code class="n">df</code><code class="p">[</code><code class="s1">'title'</code><code class="p">]</code> <code class="o">+</code> <code class="s1">': '</code> <code class="o">+</code> <code class="n">df</code><code class="p">[</code><code class="s1">'text'</code><code class="p">]</code>
</pre>

<p>Before we start NLP processing, we initialize the new <code>DataFrame</code> columns we want to fill with values:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="k">for</code> <code class="n">col</code> <code class="ow">in</code> <code class="n">nlp_columns</code><code class="p">:</code>
    <code class="n">df</code><code class="p">[</code><code class="n">col</code><code class="p">]</code> <code class="o">=</code> <code class="bp">None</code>
</pre>

<p>spaCy’s neural models benefit from <a contenteditable="false" data-type="indexterm" data-primary="GPUs (graphics processing units)" id="idm45634199360120"/>running on GPU. Thus, we try to load the model on the GPU before we start:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="k">if</code> <code class="n">spacy</code><code class="o">.</code><code class="n">prefer_gpu</code><code class="p">():</code>
    <code class="k">print</code><code class="p">(</code><code class="s2">"Working on GPU."</code><code class="p">)</code>
<code class="k">else</code><code class="p">:</code>
    <code class="k">print</code><code class="p">(</code><code class="s2">"No GPU found, working on CPU."</code><code class="p">)</code>
</pre>

<p>Now we have to decide which model and which of the pipeline components to use. Remember to disable unneccesary components to improve runtime! We stick to the small English model with the default pipeline and use our custom tokenizer that splits on hyphens:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">nlp</code> <code class="o">=</code> <code class="n">spacy</code><code class="o">.</code><code class="n">load</code><code class="p">(</code><code class="s1">'en_core_web_sm'</code><code class="p">,</code> <code class="n">disable</code><code class="o">=</code><code class="p">[])</code>
<code class="n">nlp</code><code class="o">.</code><code class="n">tokenizer</code> <code class="o">=</code> <code class="n">custom_tokenizer</code><code class="p">(</code><code class="n">nlp</code><code class="p">)</code> <code class="c1"># optional</code>
</pre>

<p>When processing larger datasets, it is recommended to use spaCy’s batch processing for a <a contenteditable="false" data-type="indexterm" data-primary="execution time" id="ch4_term42"/><a contenteditable="false" data-type="indexterm" data-primary="spaCy, linguistic processing with" data-secondary="execution time with" id="ch4_term43"/><a contenteditable="false" data-type="indexterm" data-primary="text data preparation" data-secondary="execution time of" id="ch4_term44"/><a contenteditable="false" data-type="indexterm" data-primary="time, execution" id="ch4_term45"/>significant performance gain (roughly factor 2 on our dataset). The function <code>nlp.pipe</code> takes an iterable of texts, processes them internally as a batch, and yields a list of processed <code>Doc</code> objects in the same order as the input data.</p>

<p>To use it, we first have to define a batch size. Then we can loop over the batches and call <code>nlp.pipe</code>.</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">batch_size</code> <code class="o">=</code> <code class="mi">50</code>

<code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="nb">len</code><code class="p">(</code><code class="n">df</code><code class="p">),</code> <code class="n">batch_size</code><code class="p">):</code>
    <code class="n">docs</code> <code class="o">=</code> <code class="n">nlp</code><code class="o">.</code><code class="n">pipe</code><code class="p">(</code><code class="n">df</code><code class="p">[</code><code class="s1">'text'</code><code class="p">][</code><code class="n">i</code><code class="p">:</code><code class="n">i</code><code class="o">+</code><code class="n">batch_size</code><code class="p">])</code>

    <code class="k">for</code> <code class="n">j</code><code class="p">,</code> <code class="n">doc</code> <code class="ow">in</code> <code class="nb">enumerate</code><code class="p">(</code><code class="n">docs</code><code class="p">):</code>
        <code class="k">for</code> <code class="n">col</code><code class="p">,</code> <code class="n">values</code> <code class="ow">in</code> <code class="n">extract_nlp</code><code class="p">(</code><code class="n">doc</code><code class="p">)</code><code class="o">.</code><code class="n">items</code><code class="p">():</code>
            <code class="n">df</code><code class="p">[</code><code class="n">col</code><code class="p">]</code><code class="o">.</code><code class="n">iloc</code><code class="p">[</code><code class="n">i</code><code class="o">+</code><code class="n">j</code><code class="p">]</code> <code class="o">=</code> <code class="n">values</code>
</pre>

<p>In the inner loop we extract the features from the processed <code>doc</code> and write the values back into the <code>DataFrame</code>. The whole process takes about six to eight minutes on the dataset without using a GPU and about three to four minutes with the GPU on <a contenteditable="false" data-type="indexterm" data-primary="Google Colab" id="idm45634199201192"/>Colab.</p>

<p>The newly created columns are perfectly suited for frequency analysis with the functions from <a data-type="xref" href="ch01.xhtml#ch-exploration">Chapter 1</a>. Let’s check for the most frequently mentioned noun phrases in the autos category:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">count_words</code><code class="p">(</code><code class="n">df</code><code class="p">,</code> <code class="s1">'noun_phrases'</code><code class="p">)</code><code class="o">.</code><code class="n">head</code><code class="p">(</code><code class="mi">10</code><code class="p">)</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">kind</code><code class="o">=</code><code class="s1">'barh'</code><code class="p">)</code><code class="o">.</code><code class="n">invert_yaxis</code><code class="p">()</code>
</pre>
<p><code>Out:</code></p>
<figure><div class="figure">
<img src="Images/btap_04in02.jpg" width="1621" height="497"/>
<h6/>
</div></figure>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Persisting the Result"><div class="sect2" id="idm45634199492984">
<h2>Persisting the Result</h2>
<p>Finally, <a contenteditable="false" data-type="indexterm" data-primary="text data preparation" data-secondary="saving dataframes during" id="idm45634199182088"/>we <a contenteditable="false" data-type="indexterm" data-primary="SQLite for dataframes" id="idm45634199180584"/>save the complete <code>DataFrame</code> to SQLite. To do so, we need to serialize the extracted lists to space-separated strings, as lists are not supported by most databases:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">df</code><code class="p">[</code><code class="n">nlp_columns</code><code class="p">]</code> <code class="o">=</code> <code class="n">df</code><code class="p">[</code><code class="n">nlp_columns</code><code class="p">]</code><code class="o">.</code><code class="n">applymap</code><code class="p">(</code><code class="k">lambda</code> <code class="n">items</code><code class="p">:</code> <code class="s1">' '</code><code class="o">.</code><code class="n">join</code><code class="p">(</code><code class="n">items</code><code class="p">))</code>

<code class="n">con</code> <code class="o">=</code> <code class="n">sqlite3</code><code class="o">.</code><code class="n">connect</code><code class="p">(</code><code class="n">db_name</code><code class="p">)</code>
<code class="n">df</code><code class="o">.</code><code class="n">to_sql</code><code class="p">(</code><code class="s2">"posts_nlp"</code><code class="p">,</code> <code class="n">con</code><code class="p">,</code> <code class="n">index</code><code class="o">=</code><code class="bp">False</code><code class="p">,</code> <code class="n">if_exists</code><code class="o">=</code><code class="s2">"replace"</code><code class="p">)</code>
<code class="n">con</code><code class="o">.</code><code class="n">close</code><code class="p">()</code>
</pre>

<p>The resulting table provides a solid and ready-to-use basis for further analyses. In fact, we will use this data again in <a data-type="xref" href="ch10.xhtml#ch-embeddings">Chapter 10</a> to train word embeddings on the extracted lemmas. Of course, the preprocessing steps depend on what you are going to do with the data. Working with <a contenteditable="false" data-type="indexterm" data-primary="word frequency, analysis of" data-secondary="data for" id="idm45634199060216"/><a contenteditable="false" data-type="indexterm" data-primary="bag-of-words models" id="idm45634199058936"/>sets of words like those produced by our blueprint is perfect for any kind of statistical analysis on word frequencies and machine <span class="keep-together">learning</span> based on a bag-of-words vectorization. You will need to adapt the steps for algorithms that rely on knowledge about word sequences. </p>
</div></section>

<section data-type="sect2" data-pdf-bookmark="A Note on Execution Time"><div class="sect2" id="idm45634199056472">
<h2>A Note on Execution Time</h2>
<p>Complete linguistic processing is <em>really</em> time-consuming. In fact, processing <a contenteditable="false" data-type="indexterm" data-primary="datasets, examples of" data-secondary="Reddit Self-Posts" id="idm45634199054536"/><a contenteditable="false" data-type="indexterm" data-primary="Reddit Self-Posts " id="idm45634199053160"/>just the 20,000 Reddit posts with spaCy takes several minutes. <a contenteditable="false" data-type="indexterm" data-primary="regex library (Python)" id="idm45634199051928"/>A simple regexp tokenizer, in contrast, takes only a few seconds to tokenize all records on the same machine. It’s the tagging, <a contenteditable="false" data-type="indexterm" data-primary="dependency parser" id="idm45634199050552"/>parsing, and <a contenteditable="false" data-type="indexterm" data-primary="named-entity recognition (NER)" id="idm45634199049320"/><a contenteditable="false" data-type="indexterm" data-primary="spaCy, linguistic processing with" data-secondary="for named-entity recognition" data-secondary-sortas="named-entity recognition" id="idm45634199048168"/><a contenteditable="false" data-type="indexterm" data-primary="text data preparation" data-secondary="named entities in" id="idm45634199046472"/>named-entity recognition that’s expensive, even though spaCy is really fast compared to other libraries. So if you don’t need named entities, you should definitely disable the parser and name-entity recognition to save more than 60% of the runtime.</p>

<p>Processing the data in batches with <code>nlp.pipe</code> and <a contenteditable="false" data-type="indexterm" data-primary="GPUs (graphics processing units)" id="idm45634199043768"/>using GPUs is one way to speed up data processing for spaCy. But data preparation in general is also a perfect candidate for parallelization. One <a contenteditable="false" data-type="indexterm" data-primary="multiprocessing library (Python)" id="idm45634199042376"/>option to parallelize tasks in Python is using the library <a href="https://oreil.ly/hoqxv"><code>multiprocessing</code></a>.
Especially for the parallelization of operations on dataframes, there are some scalable alternatives to Pandas worth checking, namely <a href="https://dask.org">Dask</a>, <a href="https://oreil.ly/BPMLh">Modin</a>, and <a href="https://oreil.ly/hb66b">Vaex</a>. <a href="https://oreil.ly/-PCJa">pandarallel</a> is a library that adds parallel apply operators directly to Pandas.</p>

<p>In any case, it is helpful to watch the progress and get a runtime estimate. As already <a contenteditable="false" data-type="indexterm" data-primary="tqdm library" id="idm45634199036680"/>mentioned in <a data-type="xref" href="ch01.xhtml#ch-exploration">Chapter 1</a>, the <em>tqdm</em> library is a great tool for that purpose because it provides <a href="https://oreil.ly/Rbh_-">progress bars</a> for iterators and dataframe operations. Our notebooks on GitHub use tqdm whenever <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch4_term37" id="idm45634199033320"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch4_term38" id="idm45634199031944"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch4_term39" id="idm45634199030568"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch4_term40" id="idm45634199029192"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch4_term42" id="idm45634199027816"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch4_term43" id="idm45634198997784"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch4_term44" id="idm45634198996408"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch4_term45" id="idm45634198995032"/>possible.</p>

</div></section>
</div></section>

<section data-type="sect1" data-pdf-bookmark="There Is More"><div class="sect1" id="idm45634198993400">
<h1>There Is More</h1>
<p>We started out with data cleaning and went through a whole pipeline of linguistic processing. Still, there some aspects that we didn’t cover in detail but that may be helpful or even necessary in your projects.</p>

<section data-type="sect2" data-pdf-bookmark="Language Detection"><div class="sect2" id="idm45634198991448">
<h2>Language Detection</h2>
<p>Many corpora contain text in <a contenteditable="false" data-type="indexterm" data-primary="text data preparation" data-secondary="language detection for" id="idm45634198989880"/><a contenteditable="false" data-type="indexterm" data-primary="language detection" id="idm45634198988504"/><a contenteditable="false" data-type="indexterm" data-primary="pretrained models" data-secondary="for language detection" id="idm45634198987400"/>different languages. Whenever you are working with a multilingual corpus, you have to decide on one of these options:</p>
<ul>
<li>Ignore other languages if they represent a negligible minority, and treat every text as if it were of the corpus’s major language, e.g., English.</li>
<li>Translate all texts to the main language, for example, by using Google Translate.</li>
<li>Identify the language and do language-dependent preprocessing in the next steps.</li>
</ul>
<p>There are good libraries for language detection. Our recommendation is <a contenteditable="false" data-type="indexterm" data-primary="FastText" id="idm45634198983496"/>Facebook’s <a href="https://oreil.ly/6QhAj">fastText library</a>. fastText provides a pretrained model that identifies 176 languages really fast and accurately. We provide an additional blueprint for language detection with fastText in the <a href="https://oreil.ly/c3dsK">GitHub repository</a> for this chapter.</p>

<p>textacy’s <code>make_spacy_doc</code> function <a contenteditable="false" data-type="indexterm" data-primary="textacy library" id="idm45634198979768"/>allows you to automatically load the respective language model for linguistic processing if available. By default, it uses a language detection model based on <a href="https://oreil.ly/mJLfx">Google’s Compact Language Detector v3</a>, but you could also hook in any language detection function (for example, fastText).</p>

</div></section>

<section data-type="sect2" data-pdf-bookmark="Spell-Checking"><div class="sect2" id="idm45634198977512">
<h2>Spell-Checking</h2>
<p>User-generated <a contenteditable="false" data-type="indexterm" data-primary="text data preparation" data-secondary="spell-checking for" id="idm45634198976328"/><a contenteditable="false" data-type="indexterm" data-primary="spell-checking" id="idm45634198974920"/><a contenteditable="false" data-type="indexterm" data-primary="spelling discrepancies" id="idm45634198973816"/>content suffers from a lot of misspellings. It would be great if a spell-checker could automatically correct these errors. <a href="https://oreil.ly/puo2S">SymSpell</a> is a popular spell-checker with a <a href="https://oreil.ly/yNs_k">Python port</a>. However, as you know from your smartphone, automatic spelling correction may itself introduce funny artifacts. So, you should definitely check whether the quality really improves.</p>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Token Normalization"><div class="sect2" id="idm45634198970616">
<h2>Token Normalization</h2>
<p>Often, there are different spellings for identical terms or variations of terms that you want to treat and especially count identically. In this case, it is useful to <a contenteditable="false" data-type="indexterm" data-primary="normalization of text" id="idm45634198969112"/><a contenteditable="false" data-type="indexterm" data-primary="text data preparation" data-secondary="normalization of characters and terms" id="idm45634198968008"/><a contenteditable="false" data-type="indexterm" data-primary="tokenization/tokens" data-secondary="normalization in" id="idm45634198966568"/>normalize these terms and map them to a common standard. Here are some examples:</p>
<ul>
<li>U.S.A. or U.S. → USA</li>
<li>dot-com bubble → dotcom bubble</li>
<li>München → Munich</li>
</ul>
<p>You could use <a contenteditable="false" data-type="indexterm" data-primary="spaCy, linguistic processing with" data-secondary="for matching patterns" data-secondary-sortas="matching patterns" id="idm45634198962888"/><a contenteditable="false" data-type="indexterm" data-primary="rule-based matcher (spaCy)" id="idm45634198961144"/><a contenteditable="false" data-type="indexterm" data-primary="pattern-matching" id="idm45634198960024"/>spaCy’s phrase matcher to integrate this kind of normalization as a post-processing step into its pipeline. If you don’t use spaCy, you can use a simple Python dictionary to map different spellings to their normalized forms.</p>
</div></section>
</div></section>

<section data-type="sect1" data-pdf-bookmark="Closing Remarks and Recommendations"><div class="sect1" id="idm45634198958296">
<h1>Closing Remarks and Recommendations</h1>
<p>“Garbage in, garbage out” is a frequently cited problem in data projects. This is <a contenteditable="false" data-type="indexterm" data-primary="text data preparation" data-secondary="about" id="idm45634198957112"/>especially true for textual data, which is inherently noisy. Therefore, <a contenteditable="false" data-type="indexterm" data-primary="data cleaning" id="idm45634198955512"/><a contenteditable="false" data-type="indexterm" data-primary="cleaning text data" id="idm45634198954312"/><a contenteditable="false" data-type="indexterm" data-primary="text data preparation" data-secondary="cleaning processes for" id="idm45634198953208"/>data cleaning is one of the most important tasks in any text analysis project. Spend enough effort to ensure high data quality and check it systematically. We have shown many solutions to identify and resolve quality issues in this section.</p>

<p>The second prerequisite for reliable analyses and robust models is normalization.
Many machine learning algorithms for text are based on the <a contenteditable="false" data-type="indexterm" data-primary="bag-of-words models" id="idm45634198950872"/>bag-of-words model, which generates a notion of similarity between documents based on word frequencies. In general, you are better off with lemmatized text when you do text classification, topic modeling, or clustering based on TF-IDF. You should avoid or use only sparingly those kinds of normalization or stop word removal for more complex machine learning tasks such as text summarization, machine translation, or question answering where the model needs to reflect the variety of the language.</p>

</div></section>

<div data-type="footnotes"><p data-type="footnote" id="idm45634202714920"><sup><a href="ch04.xhtml#idm45634202714920-marker">1</a></sup> The Pandas operations <code>map</code> and <code>apply</code> were explained in <a data-type="xref" href="ch01.xhtml#ch1-pipeline">“Blueprint: Building a Simple Text Preprocessing Pipeline”</a>.</p><p data-type="footnote" id="idm45634202565272"><sup><a href="ch04.xhtml#idm45634202565272-marker">2</a></sup> Libraries specialized in HTML data cleaning such as Beautiful Soup were introduced in <a data-type="xref" href="ch03.xhtml#ch-scraping">Chapter 3</a>.</p><p data-type="footnote" id="idm45634201737000"><sup><a href="ch04.xhtml#idm45634201737000-marker">3</a></sup> The asterisk operator (*) unpacks the list into separate arguments for <code>print</code>.</p><p data-type="footnote" id="idm45634201564856"><sup><a href="ch04.xhtml#idm45634201564856-marker">4</a></sup> For example, check out <a href="https://oreil.ly/R45_t">NLTK’s tweet tokenizer</a> for regular expressions for text emoticons and URLs, or see textacy’s <span class="keep-together"><a href="https://oreil.ly/i0HhJ">compile regexes</a></span>.</p><p data-type="footnote" id="idm45634201561544"><sup><a href="ch04.xhtml#idm45634201561544-marker">5</a></sup> A good overview is <a href="https://oreil.ly/LyGvt">“The Art of Tokenization” by Craig Trim</a>.</p><p data-type="footnote" id="idm45634201500904"><sup><a href="ch04.xhtml#idm45634201500904-marker">6</a></sup> See <a href="https://oreil.ly/spaCy">spaCy’s website</a> for a list of available models.</p><p data-type="footnote" id="idm45634201354856"><sup><a href="ch04.xhtml#idm45634201354856-marker">7</a></sup> See <a href="https://oreil.ly/cvNhV">spaCy’s API</a> for a complete list.</p><p data-type="footnote" id="idm45634201147656"><sup><a href="ch04.xhtml#idm45634201147656-marker">8</a></sup> See <a href="https://oreil.ly/EpmEI">spaCy’s API</a> for a complete list of attributes.</p><p data-type="footnote" id="idm45634201023736"><sup><a href="ch04.xhtml#idm45634201023736-marker">9</a></sup> See <a href="https://oreil.ly/HMWja">spaCy’s tokenization usage docs</a> for details and an illustrative example.</p><p data-type="footnote" id="idm45634201018552"><sup><a href="ch04.xhtml#idm45634201018552-marker">10</a></sup> See <a href="https://oreil.ly/45yU4">spaCy’s tokenizer usage docs</a> for details.</p><p data-type="footnote" id="idm45634200775880"><sup><a href="ch04.xhtml#idm45634200775880-marker">11</a></sup> Modifying the stop word list this way will probably become deprecated with spaCy 3.0. Instead, it is recommended to create a modified subclass of the respective language class. See the <a href="https://oreil.ly/CV2Cz">GitHub notebook</a> for this chapter for details.</p><p data-type="footnote" id="idm45634200544984"><sup><a href="ch04.xhtml#idm45634200544984-marker">12</a></sup> See <a href="https://oreil.ly/lAKtm">Universal Part-of-speech tags</a> for more.</p></div></div></section></div>



  </body></html>