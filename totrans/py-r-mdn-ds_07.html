<html><head></head><body><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 4. Data Format Context"><div class="chapter" id="ch05">
<h1><span class="label">Chapter 4. </span>Data Format Context</h1>


<p class="byline">Boyan Angelov</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45127452083816">
<h5>A note for Early Release readers</h5>
<p>With Early Release ebooks, you get books in their earliest form—the author’s raw and unedited content as they write—so you can take advantage of these technologies long before the official release of these titles.</p>
</div></aside>

<p>In this chapter we’ll review tools in Python and R for importing and processing data in a variety of formats. We’ll cover a selection of packages, compare and contrast them, and highlight the properties that make them effective. At the end of this tour, you’ll be able to select packages with confidence. Each section illustrates the tool’s capabilities with a specific mini-case study, based on tasks that a data scientist encounters daily. If you’re transitioning your work from one language to another, or simply want to find out how to get started quickly using complete, well-maintained and context-specific packages this chapter will guide you.</p>

<p>Before we dive in, remember that the open-source ecosystem is constantly changing. New developments, such as <a href="https://www.tensorflow.org/tutorials/text/transformer">transformer models</a> and <a href="https://robotics.sciencemag.org/content/4/37/eaay7120">xAI</a>, seem to emerge every other week. These often aim at lowering the learning curve and increasing developer productivity. This explosion of diversity also applies to related packages, resulting in a nearly constant flow of new and (hopefully) better tools. If you have a very specific problem, there’s probably a package already available for you, so you don’t have to reinvent the wheel. Tool selection can be overwhelming, but at the same time this variety of options can improve the quality and speed of your data science work.</p>

<p>The package selection in this chapter can appear limited in view, hence it is essential to clarify our selection criteria. So what qualities should we look for in a good tool?</p>

<ul>
<li>
<p>It should be <strong>open source</strong>: there is a large number of valuable commercial tools available, but we firmly believe that open source tools have a great advantage. They tend to be easier to extend and understand what their inner workings are, and are more popular.</p>
</li>
<li>
<p>It should be <strong>feature-complete</strong>: the package should include a comprehensive set of functions that help the user do their fundamental work without resorting to other tools.</p>
</li>
<li>
<p>It should be <strong>well-maintained</strong>: one of the drawbacks of using Open Source Software (OSS) is that sometimes packages have a short lifecycle, and their maintenance is abandoned (so called “abandonware”). We want to use packages which are actively worked on, so we can feel confident they are up-to-date.</p>
</li>
</ul>

<p>Let’s begin with a definition. What is a “data format”? There are <a href="https://en.wikipedia.org/wiki/Data_format">several answers</a> available. Possible candidates are <em>data type</em>, <em>recording format</em> and <em>file format</em>. <em>Data type</em> is related to data stored in databases or types in programming languages (for example integer, float or string). The <em>recording format</em> is how data is stored in a physical medium, such as CD or DVD. And finally, what we’re after, the <em>file format</em>, i.e. how information is <em>prepared for a computing purpose</em>.</p>

<p>With that definition in hand, one might still wonder why should we dedicate an entire chapter to focus just on file formats? You have probably been exposed to them in another context, such as saving a PowerPoint slide deck with a <code>.ppt</code> or <code>.pptx</code> extension (and wondering which one is better). The problem here goes much further beyond basic tool compatibility. The way information is stored influences the complete downstream data science process. For example, if our end goal is to perform advanced analytics and the information is stored in a text format, we have to pay attention to factors such as character encoding (a notorious problem, especially for Python<sup><a data-type="noteref" id="idm45127452048648-marker" href="ch04.xhtml#idm45127452048648">1</a></sup>). For such data to be effectively processed, it also needs to go through several steps<sup><a data-type="noteref" id="idm45127452046808-marker" href="ch04.xhtml#idm45127452046808">2</a></sup>, such as <a href="https://en.wikipedia.org/wiki/Tokenization">tokenization</a> and <a href="https://en.wikipedia.org/wiki/Stop_word">stop word</a> removal. Those same steps are not applicable to image data, even though we may have the same end goal in mind, e.g. classification. In that case other processing techniques are more suitable, such as resizing and scaling. These differences in data processing pipelines are shown on <a data-type="xref" href="#pipelines_diff">???</a>. To summarize: the data format is the most significant factor influencing what you can, and cannot do with it.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>We now use the word “pipeline” for the first time in this context, so let’s use the opportunity to define it. You have probably heard the expression that “data is the new oil”. This expression goes beyond a simple marketing strategy and represents a useful way to think about data. There are surprisingly many parallels between how oil and data are processed. You can imagine that the initial data that the business collects is the rawest form - probably of limited use initially. It then undergoes a sequence of steps, called data processing, before it’s used in some application (i.e. for training an ML model or feeding a dashboard). In oil processing this would be called refinement and enrichment - making the data usable for a business purpose. Pipelines transport the different oil types (raw, refined) through the system to its final state. The same term can be used in data science and engineering to describe the infrastructure and technology required to process and deliver data.</p>
</div>
<ol id="pipelines_diff">
<li>
<p>difference between common data format pipelines. The green color indicates the shared steps between the workflows.
image::img/pipelines_diff.jpg[""]</p>
</li>

</ol>

<p>Infrastructure and performance also need to be taken into consideration when working with a specific data format. For example, with image data, you’ll need more storage availability. For time-series data you might need to use a particular database, such as <a href="https://www.influxdata.com/products/influxdb-overview/">Influx DB</a>. And finally, in terms of performance, image classification is often solved using deep learning methods based on Convolutional Neural Networks (CNNs) which may require a Graphics Processing Unit (GPU). Without it, model training can be very slow and become a bottleneck both for your development work and a potential production deployment.</p>

<p>Now that we covered the reasons to carefully consider which packages to use, we’ll have a look at the possible data formats. This overview is presented in <a data-type="xref" href="#data-format-zoo-table">Table 4-1</a> (note that those tools are mainly designed for small to medium size datasets). Admittedly, we are just scratching the surface on what’s out there, and there are a few notable omissions (such as audio and video). Here, we’ll focusing on the most widely used formats.</p>
<table id="data-format-zoo-table">
<caption><span class="label">Table 4-1. </span>An overview of data formats and popular Python and R packages used to process data stored in them.</caption>
<thead>
<tr>
<th>Data type</th>
<th>Python package</th>
<th>R package</th>
</tr>
</thead>
<tbody>
<tr>
<td><p>Tabular</p></td>
<td><p><code>pandas</code></p></td>
<td><p><code>readr</code>, <code>rio</code></p></td>
</tr>
<tr>
<td><p>Image</p></td>
<td><p><code>open-cv</code>, <code>scikit-image</code>, <code>PIL</code></p></td>
<td><p><code>magickr</code>, <code>imager</code>, <code>EBImage</code></p></td>
</tr>
<tr>
<td><p>Text</p></td>
<td><p><code>nltk</code>, <code>spaCy</code></p></td>
<td><p><code>tidytext</code>, <code>stringr</code></p></td>
</tr>
<tr>
<td><p>Time series</p></td>
<td><p><code>prophet</code>, <code>sktime</code></p></td>
<td><p><code>prophet</code>, <code>ts</code>, <code>zoo</code></p></td>
</tr>
<tr>
<td><p>Spatial</p></td>
<td><p><code>gdal</code>, <code>geopandas</code>, <code>pysal</code></p></td>
<td><p><code>rgdal</code>, <code>sp</code>, <code>sf</code>, <code>raster</code></p></td>
</tr>
</tbody>
</table>

<p>This table is by no means exhaustive, and we are certain new tools will appear soon, but those are the workhorses fulfilling our selection criteria. Let’s get them to work in the following sections, and see which ones are the best for the job!</p>






<section data-type="sect1" data-pdf-bookmark="External versus base packages"><div class="sect1" id="idm45127452012488">
<h1>External versus base packages</h1>

<p>In <a data-type="xref" href="ch02.xhtml#ch03">Chapter 2</a> and <a data-type="xref" href="ch03.xhtml#ch04">Chapter 3</a>, we introduced packages very early in the learning process. In Python we used <code>pandas</code> right at the outset and we also transitioned to the <code>tidyverse</code> in R relatively quickly. This allowed us to be productive much faster than if we went down the rabbit holes of archaic language features that you’re unlikely to need as a beginner<sup><a data-type="noteref" id="idm45127452007560-marker" href="ch04.xhtml#idm45127452007560">3</a></sup>. A programming language’s utility is defined by the availability and quality of it’s third-party packages, as opposed to the core features of the language itself.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>This is not to say that the aren’t a lot of things that you can accomplish with just base R (as you’ll see in some of the upcoming examples), but taking advantage of the open-source ecosystem is a fundamental skill to increase your productivity and avoid reinventing the wheel.</p>
</div>
<div data-type="warning" epub:type="warning"><h1>Go back and learn the basics</h1>
<p>There is a danger in overusing third-party packages, and you have to be aware of when the right time to go back to the basics is. Otherwise you might fall victim to a false sense of security, and become reliant on the training wheels provided by tools such as <code>pandas</code>. This might lead to difficulties when dealing with more specific real-world challenges.</p>
</div>

<p>Let’s now see this package vs. base language concept plays out in practice by going into detail with a topic we’re already familiar with: tabular data<sup><a data-type="noteref" id="idm45127452002248-marker" href="ch04.xhtml#idm45127452002248">4</a></sup>. There are at least two ways to do this in Python. First, using <code>pandas</code>:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">pandas</code> <code class="kn">as</code> <code class="nn">pd</code>

<code class="n">data</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">read_csv</code><code class="p">(</code><code class="err">“</code><code class="n">dataset</code><code class="o">.</code><code class="n">csv</code><code class="err">”</code><code class="p">)</code></pre>

<p>Second, with the built-in <code>csv</code> module:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">import</code><code> </code><code class="nn">csv</code><code>
</code><code>
</code><code class="k">with</code><code> </code><code class="nb">open</code><code class="p">(</code><code class="err">“</code><code class="n">dataset</code><code class="o">.</code><code class="n">csv</code><code class="err">”</code><code class="p">,</code><code> </code><code class="err">“</code><code class="n">r</code><code class="err">”</code><code class="p">)</code><code> </code><code class="k">as</code><code> </code><code class="nb">file</code><code class="p">:</code><code> </code><a class="co" id="co_data_format_context_CO1-1" href="#callout_data_format_context_CO1-1"><img src="Images/1.png" alt="1" width="12" height="12"/></a><code>
</code><code>	</code><code class="n">reader</code><code> </code><code class="o">=</code><code> </code><code class="n">csv</code><code class="o">.</code><code class="n">reader</code><code class="p">(</code><code class="nb">file</code><code class="p">,</code><code> </code><code class="n">delimiter</code><code class="o">=</code><code class="err">“</code><code class="p">,</code><code class="err">”</code><code class="p">)</code><code>
</code><code class="k">for</code><code> </code><code class="n">row</code><code> </code><code class="ow">in</code><code> </code><code class="n">reader</code><code class="p">:</code><code> </code><a class="co" id="co_data_format_context_CO1-2" href="#callout_data_format_context_CO1-2"><img src="Images/2.png" alt="2" width="12" height="12"/></a><code>
</code><code>	</code><code class="k">print</code><code class="p">(</code><code class="n">row</code><code class="p">)</code></pre>
<dl class="calloutlist">
<dt><a class="co" id="callout_data_format_context_CO1-1" href="#co_data_format_context_CO1-1"><img src="Images/1.png" alt="1" width="12" height="12"/></a></dt>
<dd><p>Note how you need to specify the <a href="https://docs.python.org/3/tutorial/inputoutput.html#reading-and-writing-files">file mode</a>, in this case <code>"r"</code> (standing for “read”). This is to make sure the file is not overwritten by accident, hinting at a more general-purpose oriented language.</p></dd>
<dt><a class="co" id="callout_data_format_context_CO1-2" href="#co_data_format_context_CO1-2"><img src="Images/2.png" alt="2" width="12" height="12"/></a></dt>
<dd><p>Using a loop to read a file might seem strange to a beginner, but it’s making the process explicit.</p></dd>
</dl>

<p>This example tells us that <code>pd.read_csv()</code> in <code>pandas</code> provides a more concise, convenient and intuitive way to import data. It is also less explicit than vanilla Python and not essential. <code>pd.read_csv()</code> is, in essence, a <em>convenience wrapper</em> of existing functionality — good for us!</p>

<p>Here we see that packages serve two functions. First, as we have come to expect, they provide <em>new</em> functionality. Second, they are also convenience wrappers for existing standard functions, which make our lives easier.</p>

<p>This is brilliantly demonstrated in R’s <code>rio</code> package<sup><a data-type="noteref" id="idm45127451851368-marker" href="ch04.xhtml#idm45127451851368">5</a></sup>. <code>rio</code> stands for “R input/output” and it does just was it says<sup><a data-type="noteref" id="idm45127451849368-marker" href="ch04.xhtml#idm45127451849368">6</a></sup>. Here, the single function <code>import()</code> uses the file’s filename extension to select the best function in a collection of packages for importing. This works on Excel, SPSS, stata, SAS or many other formats commonly seen.</p>

<p>Another R tidyverse package, <code>vroom</code> allows for fast import of tabular data, and can read an entire directory of files in one command, with the use of <code>map()</code> functions or <code>for loops</code>.</p>

<p>Finally, the <code>data.table</code> package, which is often neglected at the expense of promoting the tidyverse, provides the exceptional <code>fread()</code> which can import very large files at a fraction of what base R or <code>readr</code> offer.</p>

<p>The usefulness of learning how to use a third-party packages becomes more apparent when we try to perform more complex tasks, as we’ll see next when processing other data formats.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45127451844280">
<h5>Data Science from scratch</h5>
<p>Writing software from scratch is a great way to understand how things work under the hood. It’s a recommended step in learning, especially after getting used to the tools at higher abstraction levels (such as <code>scikit-learn</code>). Excellent reading material on this topic is provided in the O’Reilly book <a href="https://learning.oreilly.com/library/view/data-science-from/9781492041122/">Data Science from Scratch</a>.</p>
</div></aside>

<p>Now that we can appreciate the advantages of packages, we’ll demonstrate some of their capabilities. For this we’ll work on several different real-world use cases, listed in <a data-type="xref" href="#case-study-table">Table 4-2</a>. We won’t focus on minute implementation details, but instead cover the elements that expose their benefits (and shortcomings) for the tasks at hand. Since the focus in this chapter is on data formats, and <a data-type="xref" href="ch05.xhtml#ch06">Chapter 5</a> is all about workflows, all these case studies are about data processing (as illustrated previously in <a data-type="xref" href="#pipelines_diff">???</a>).</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>For pedagogic purposes we have omitted parts of the code. If you’d like to follow along, executable code is available in the <a href="https://github.com/moderndatadesign/PyR4MDS">book repository</a>.</p>
</div>
<table id="case-study-table">
<caption><span class="label">Table 4-2. </span>An overview of the different use-cases</caption>
<thead>
<tr>
<th>Data format</th>
<th>Use case</th>
</tr>
</thead>
<tbody>
<tr>
<td><p>image</p></td>
<td><p><a href="https://www.kaggle.com/kbhartiya83/swimming-pool-and-car-detection">Swimming pool and car Detection</a></p></td>
</tr>
<tr>
<td><p>text</p></td>
<td><p><a href="http://jmcauley.ucsd.edu/data/amazon/">Amazon Music reviews processing</a></p></td>
</tr>
<tr>
<td><p>time series</p></td>
<td><p><a href="https://raw.githubusercontent.com/jbrownlee/Datasets/master/daily-min-temperatures.csv">Daily Australian Temperatures</a></p></td>
</tr>
<tr>
<td><p>spatial</p></td>
<td><p><a href="https://www.gbif.org/"><em>Loxodonta africana</em> species distribution data</a></p></td>
</tr>
</tbody>
</table>

<p>Further information on how to download and process these data is available in the official <a href="https://github.com/moderndatadesign/PyR4MDS">repository</a> for the book.</p>
</div></section>













<section data-type="sect1" data-pdf-bookmark="Image data"><div class="sect1" id="idm45127452011576">
<h1>Image data</h1>

<p>Images pose a unique set of challenges for data scientists. We’ll demonstrate the optimal methodology by covering the challenge of aerial image processing - a domain of growing importance in agriculture, biodiversity conservation, urban planning and climate change research. Our mini use-case uses data from Kaggle, collected to help the detection of swimming pools and cars. For more information on the dataset, you can use the URL in <a data-type="xref" href="#case-study-table">Table 4-2</a>.</p>








<section data-type="sect2" data-pdf-bookmark="OpenCV and scikit-image"><div class="sect2" id="idm45127451822520">
<h2>OpenCV and scikit-image</h2>

<p>As we mentioned at the beginning of the chapter, downstream purpose influences data processing heavily. Since aerial data is often used to train machine learning algorithms, our focus will be on preparatory tasks.</p>

<p>The <a href="https://opencv.org/">OpenCV</a> package is one of the most common ways to work with image data in Python. It contains all the necessary tools for image loading, manipulation and storage. The “CV” in the name stands for Computer Vision - the field of machine learning that focuses on image data. Another handy tool that we’ll use is <code>scikit-image</code>. As its naming suggests, it’s very much related to <a href="https://scikit-learn.org/stable/">scikit-learn</a><sup><a data-type="noteref" id="idm45127451817912-marker" href="ch04.xhtml#idm45127451817912">7</a></sup>.</p>

<p>Here are the steps of our task (refer to <a data-type="xref" href="#case-study-table">Table 4-2</a>):</p>
<ol>
<li>
<p>Resize the image to a specific size</p>
</li>
<li>
<p>Convert the image to black and white</p>
</li>
<li>
<p>Augment the data by rotating the image</p>
</li>

</ol>

<p>For an ML algorithm to learn successfully from data, the input has to be cleaned (data munging), standardized (i.e., scaling) and filtered (feature engineering)<sup><a data-type="noteref" id="idm45127451810536-marker" href="ch04.xhtml#idm45127451810536">8</a></sup>. You can imagine gathering a dataset of images (for example, by scraping<sup><a data-type="noteref" id="idm45127451809800-marker" href="ch04.xhtml#idm45127451809800">9</a></sup> data from Google Images). They will differ in some way or another - such as size and/or color. Steps 1 and 2 in our task list help us deal with that. Step 3 is handy for ML applications. The performance (i.e., classification accuracy, or Area Under the Curve(AUC)) of ML algorithms depends mostly on the amount of training data, which is often in little supply. To get around this, without resorting to obtaining more data<sup><a data-type="noteref" id="idm45127451808600-marker" href="ch04.xhtml#idm45127451808600">10</a></sup>, data scientists have discovered that playing around with the data already available, such as rotating and cropping, can introduce new data points. Those can then be used to train the model again and improve performance. This process is formally known as data augmentation<sup><a data-type="noteref" id="idm45127451807656-marker" href="ch04.xhtml#idm45127451807656">11</a></sup>.</p>

<p>Enough talk - let’s start by importing the data! Remember, if you want to follow along, check the complete code at the book’s <a href="https://github.com/moderndatadesign/PyR4MDS">repository</a>.</p>

<pre id="plot_image" data-type="programlisting" data-code-language="python"><code class="kn">import</code><code> </code><code class="nn">cv2</code><code> </code><a class="co" id="co_data_format_context_CO2-1" href="#callout_data_format_context_CO2-1"><img src="Images/1.png" alt="1" width="12" height="12"/></a><code>
</code><code class="n">single_image</code><code> </code><code class="o">=</code><code> </code><code class="n">cv2</code><code class="o">.</code><code class="n">imread</code><code class="p">(</code><code class="s2">"</code><code class="s2">img_01.jpg</code><code class="s2">"</code><code class="p">)</code><code>
</code><code>
</code><code class="n">plt</code><code class="o">.</code><code class="n">imshow</code><code class="p">(</code><code class="n">single_image</code><code class="p">)</code><code>
</code><code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">(</code><code class="p">)</code></pre>
<dl class="calloutlist">
<dt><a class="co" id="callout_data_format_context_CO2-1" href="#co_data_format_context_CO2-1"><img src="Images/1.png" alt="1" width="12" height="12"/></a></dt>
<dd><p>Using <code>cv2</code> might seem confusing since the package is named <code>OpenCV.</code> <code>cv2</code> is used as a short-hand name. The same naming pattern is used for <code>scikit-image</code>, where the import statement is shortened to <code>skimage</code>.</p></dd>
</dl>

<figure><div id="raw_img_python" class="figure">
<img src="Images/prds_0401.png" alt="" width="432" height="288"/>
<h6><span class="label">Figure 4-1. </span>Raw image plot in Python with <code>matplotlib</code>.</h6>
</div></figure>

<p>So in what object type did <code>cv2</code> store the data? We can check with <code>type</code>:</p>

<pre data-type="programlisting" data-code-language="python"><code class="k">print</code><code class="p">(</code><code class="nb">type</code><code class="p">(</code><code class="n">single_image</code><code class="p">))</code>
<code class="n">numpy</code><code class="o">.</code><code class="n">ndarray</code></pre>

<p>Here we can observe an important feature that already provides advantages to using Python for CV tasks as opposed to R. The image is directly stored as a <code>numpy</code> multidimensional array (<code>nd</code> stands for n-dimensions), making it accessible to a variety of other tools available in the wider Python ecosystem. Because this is built on the <code>pyData</code> stack, it’s well-supported. Is this true for R? Let’s have a look at the <code>magick</code> package:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">library</code><code class="p">(</code><code class="n">magick</code><code class="p">)</code>
<code class="n">single_image</code> <code class="o">&lt;-</code> <code class="n">image_read</code><code class="p">(</code><code class="s1">'img_01.jpg'</code><code class="p">)</code>
<code class="n">class</code><code class="p">(</code><code class="n">single_image</code><code class="p">)</code>
<code class="p">[</code><code class="mi">1</code><code class="p">]</code> <code class="s2">"magick-image"</code></pre>

<p>The <code>magick-image</code> class is only accessible to functions from the <code>magick</code> package, or other closely related tools, but not the powerful base R methods (such as the ones shown in <a data-type="xref" href="ch02.xhtml#ch03">Chapter 2</a>, with the notable exception of <code>plot()</code>). Those different approaches in how various open source packages support each other is illustrated in <a data-type="xref" href="#package_design">Figure 4-2</a>, and is a common thread throughout the examples in this chapter.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>There is at least one exception to this rule - the <code>EBImage</code> package, a part of <a href="https://bioconductor.org/">Bioconductor</a>. By using it you can get access to the image in its raw array form, and then use other tools on top of that. The drawback here is that it’s a part of a domain-specific package, and it might not be easy to see how it works in a standard CV pipeline.</p>
</div>

<figure><div id="package_design" class="figure">
<img src="Images/prds_0402.png" alt="" width="1351" height="739"/>
<h6><span class="label">Figure 4-2. </span>The two types of package design hierarchies as they are used during a data lifecycle (bottom to top). The left pattern shows a suboptimal structure, where users are forced to adopt and use purpose-specific tools at the first level which limits their flexibility and productivity. The pattern on the right shows a better structure, where there are standard tools for the initial phases of the data lineage, enabling a variety of tools downstream.</h6>
</div></figure>

<p>Note that in the previous step (where we loaded the raw image in Python), we also used one of the most popular plotting tool - <code>matplotlib</code> (data visualization is covered in <a data-type="xref" href="ch05.xhtml#ch06">Chapter 5</a>), so we again took advantage of this better design pattern.</p>

<p>Now that we know that the image data is stored as a <code>numpy</code> <code>ndarray</code>, we can use <code>numpy</code>’s methods. What’s the size of the image? For this we can try the <code>.shape</code> method of <code>ndarray</code>:</p>

<pre data-type="programlisting" data-code-language="python"><code class="k">print</code><code class="p">(</code><code class="n">single_image</code><code class="o">.</code><code class="n">shape</code><code class="p">)</code>
<code class="mi">224</code> <code class="mi">224</code> <code class="mi">3</code></pre>

<p>It worked indeed! The first two output values correspond to the image <code>height</code> and <code>width</code> respectively, and the third one to the number of channels in the image - three in this case ((r)ed, (g)reen and (b)lue). Now let’s continue and deliver on the first standardization step - image resizing. Here we’ll use <code>cv2</code> for the first time:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">single_image</code> <code class="o">=</code> <code class="n">cv2</code><code class="o">.</code><code class="n">resize</code><code class="p">(</code><code class="n">single_image</code><code class="p">,(</code><code class="mi">150</code><code class="p">,</code> <code class="mi">150</code><code class="p">))</code>
<code class="k">print</code><code class="p">(</code><code class="n">single_image</code><code class="o">.</code><code class="n">shape</code><code class="p">)</code>
<code class="p">(</code><code class="mi">150</code><code class="p">,</code> <code class="mi">150</code><code class="p">,</code> <code class="mi">3</code><code class="p">)</code></pre>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>If you gain experience working with such fundamental tools in both languages, you’ll be able to test your ideas quickly, even without knowing whether those methods exist. If the tools you use are designed well (as in the better design in <a data-type="xref" href="#package_design">Figure 4-2</a>), often they will work as expected!</p>
</div>

<p>Perfect, it worked like a charm! The next step is to convert the image to black and white. For this, we’ll also use <code>cv2</code>:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">gray_image</code> <code class="o">=</code> <code class="n">cv2</code><code class="o">.</code><code class="n">cvtColor</code><code class="p">(</code><code class="n">single_image</code><code class="p">,</code> <code class="n">cv2</code><code class="o">.</code><code class="n">COLOR_RGB2GRAY</code><code class="p">)</code>
<code class="k">print</code><code class="p">(</code><code class="n">gray_image</code><code class="o">.</code><code class="n">shape</code><code class="p">)</code>
<code class="p">(</code><code class="mi">150</code><code class="p">,</code> <code class="mi">150</code><code class="p">)</code></pre>

<p>The colors are greenish and not grey. This default option chooses a color scheme that makes the contrast more easily discernible for a human eye than black and white. When you look at the shape of the <code>numpy</code> <code>ndarray</code> you can see that the channel number has disappeared - there is just one now. Now let’s complete our task and do a simple data augmentation step and flip the image horizontally. Here we’re again taking advantage that the data is stored as a <code>numpy</code> array. We’ll use a function directly from <code>numpy</code>, without relying on the other CV libraries (<code>OpenCV</code> or <code>scikit-image</code>):</p>

<pre id="flip_example" data-type="programlisting" data-code-language="python"><code class="n">flipped_image</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">fliplr</code><code class="p">(</code><code class="n">gray_image</code><code class="p">)</code></pre>

<p>The results are shown on <a data-type="xref" href="#flipped_image">Figure 4-3</a>.</p>

<figure><div id="flipped_image" class="figure">
<img src="Images/prds_0403.png" alt="" width="432" height="288"/>
<h6><span class="label">Figure 4-3. </span>Plot of an image flipped by using <code>numpy</code> functions.</h6>
</div></figure>

<p>We can use <code>scikit-image</code> for further image manipulation tasks such as rotation, and even this different package will work as expected on our data format:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">skimage</code> <code class="kn">import</code> <code class="n">transform</code>
<code class="n">rotated_image</code> <code class="o">=</code> <code class="n">transform</code><code class="o">.</code><code class="n">rotate</code><code class="p">(</code><code class="n">single_image</code><code class="p">,</code> <code class="n">angle</code><code class="o">=</code><code class="mi">45</code><code class="p">)</code></pre>

<p>The data standardization and augmentation steps we went through illustrate how the less complex package design (<a data-type="xref" href="#package_design">Figure 4-2</a>) makes us more productive. We can drive the point home by showing a negative example for the third step, this time in R. For that, we’ll have to rely on the <code>adimpro</code> package:</p>

<pre data-type="programlisting" data-code-language="r"><code class="nf">library</code><code class="p">(</code><code class="n">adimpro</code><code class="p">)</code>
<code class="nf">rotate.image</code><code class="p">(</code><code class="n">single_image</code><code class="p">,</code> <code class="n">angle</code> <code class="o">=</code> <code class="m">90</code><code class="p">,</code> <code class="n">compress</code><code class="o">=</code><code class="kc">NULL</code><code class="p">)</code></pre>

<p>Whenever we load yet another package, we are decreasing the quality, readability, and reusability of our code. This issue is primarily due to possible unknown bugs, a steeper learning curve, or a potential lack of consistent and thorough documentation for that third-party package. A quick check on the status of <code>adimpro</code> on <a href="https://cran.r-project.org/web/packages/adimpro/index.html">CRAN</a> reveals that the last time it was updated was in November 2019<sup><a data-type="noteref" id="idm45127451457144-marker" href="ch04.xhtml#idm45127451457144">12</a></sup>. This is why using tools such as <code>OpenCV</code>, which work on image data by taking advantage of the <code>PyData</code> stack<sup><a data-type="noteref" id="idm45127451455800-marker" href="ch04.xhtml#idm45127451455800">13</a></sup>, such as <code>numpy</code> is preferred.</p>

<p>A less complex, modular, and abstract enough package design goes a long way to make data scientists productive and happy in using their tools. They are then free to focus on actual work and not dealing with complex documentation or a multitude of abandonware packages. These considerations make Python the clear winner in importing and processing image data, but is this the case for the other formats?</p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Text data"><div class="sect1" id="idm45127451821576">
<h1>Text data</h1>

<p>The analysis of text data is often used interchangeably with the term Natural Language Processing (NLP). This, in turn, is a subfield of ML. Hence it’s not surprising to see that Python-based tools also dominate it. The inherently compute-intensive nature of working with text data is one good reason to why that’s the case. Another one is that dealing with larger datasets can be a more significant challenge in R<sup><a data-type="noteref" id="idm45127451424392-marker" href="ch04.xhtml#idm45127451424392">14</a></sup> then in Python (this topic is covered further in <a data-type="xref" href="ch05.xhtml#ch06">Chapter 5</a>). And it is a Big Data problem. The amount of text data has proliferated in recent years with the rise of services on the internet and social media giants such as Twitter and Facebook. Such organizations have also invested heavily in the technology and related open-source, due to the fact that a large chunk of data available to them is in text format.</p>

<p>Similarly to the image data case, we’ll start by designing a standard NLP task. It should contain the most fundamental elements of an NLP pipeline. For a dataset, we selected texts from the Amazon Product Reviews Dataset (<a data-type="xref" href="#case-study-table">Table 4-2</a>), and we have to prepare it for an advanced analytics use case, such as text classification, sentiment analysis, or topic modeling. The steps needed for completion are the following:</p>
<ol>
<li>
<p>Tokenize the data</p>
</li>
<li>
<p>Remove stop-words</p>
</li>
<li>
<p>Tag the Parts of Speech (PoS)</p>
</li>

</ol>

<p>We’ll also go through more advanced methods (such as word embeddings) in <code>spaCy</code> to demonstrate what the Python packages are capable of, and at the same time, provide a few R examples for comparison.</p>








<section data-type="sect2" data-pdf-bookmark="NLTK and spaCy"><div class="sect2" id="idm45127451416616">
<h2>NLTK and spaCy</h2>

<p>So what are the most common tools in Python? The most popular one is often referred to as the swiss-army knife of NLP - the Natural Language Toolkit (NLTK)<sup><a data-type="noteref" id="idm45127451414392-marker" href="ch04.xhtml#idm45127451414392">15</a></sup>. It contains a good selection of tools covering the whole pipeline. It also has excellent documentation and a relatively low learning curve for its API.</p>
<div data-type="tip"><h1>NLTK Book</h1>
<p>The NLTK authors have also written one of the most accessible books on working with text data - the NLTK Book, currently in version 3. It’s available to read online for free <a href="https://www.nltk.org/book/">on the official website</a>. It can serve as an excellent reference manual, so if you want to dive deeper into some of the topics we cover in this section, go ahead and have a look!</p>
</div>

<p>As a data scientist, one of the first steps in a project is to look at the raw data. Here’s one example review, along with its data type:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">example_review</code> <code class="o">=</code> <code class="n">reviews</code><code class="p">[</code><code class="s2">"reviewText"</code><code class="p">]</code><code class="o">.</code><code class="n">sample</code><code class="p">()</code>
<code class="k">print</code><code class="p">(</code><code class="n">example_review</code><code class="p">)</code>
<code class="k">print</code><code class="p">(</code><code class="nb">type</code><code class="p">(</code><code class="n">example_review</code><code class="p">))</code></pre>

<pre data-type="programlisting">I just recently purchased her ''Paint The Sky With Stars''
 CD and was so impressed that I bought 3 of her previously
 released CD's and plan to buy all her music.  She is
 truely talented and her music is very unique with a
 combination of modern classical and pop with a hint of
 an Angelic tone. I still recommend you buy this CD. Anybody
  who has an appreciation for music will certainly enjoy her music.

str</pre>

<p>This here is important - the data is stored in a fundamental data type in Python - <code>str</code> (string). Similar to the image data being stored as a multidimensional <code>numpy</code> array, many other tools can have access to it. For example, suppose we were to use a tool that efficiently searches and replaces parts of a string, such as <a href="https://github.com/vi3k6i5/flashtext">flashtext</a>. In that case, we’d be able to use it here without formatting issues, and the need to coerce<sup><a data-type="noteref" id="idm45127451365400-marker" href="ch04.xhtml#idm45127451365400">16</a></sup> the data type.</p>

<p>Now we can take the first step in our mini case study - <em>tokenization</em>. It will split the reviews into components, such as words or sentences:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">sentences</code> <code class="o">=</code> <code class="n">nltk</code><code class="o">.</code><code class="n">sent_tokenize</code><code class="p">(</code><code class="n">example_review</code><code class="p">)</code>
<code class="k">print</code><code class="p">(</code><code class="n">sentences</code><code class="p">)</code></pre>

<pre data-type="programlisting">["I just recently purchased her ''Paint The Sky With Stars''
CD and was so impressed that I bought 3 of her
previously released CD's and plan to buy all her music.",
 'She is truely talented and her music is very unique with
  a combination of modern classical and pop with a hint of an Angelic tone.',
 'I still recommend you buy this CD.',
 'Anybody who has an appreciation for music will certainly enjoy her music.']</pre>

<p>Easy enough! For illustration purposes, would it be that hard to attempt this relatively simple task in R, with some functions from <code>tidytext</code>?</p>

<pre data-type="programlisting" data-code-language="r"><code class="n">tidy_reviews</code> <code class="o">&lt;-</code> <code class="n">amazon_reviews</code> <code class="o">%&gt;%</code>
  <code class="nf">unnest_tokens</code><code class="p">(</code><code class="n">word</code><code class="p">,</code> <code class="n">reviewText</code><code class="p">)</code> <code class="o">%&gt;%</code>
  <code class="nf">mutate</code><code class="p">(</code><code class="n">word</code> <code class="o">=</code> <code class="nf">lemmatize_strings</code><code class="p">(</code><code class="n">word</code><code class="p">,</code> <code class="n">dictionary</code> <code class="o">=</code> <code class="n">lexicon</code><code class="o">::</code><code class="n">hash_lemmas</code><code class="p">))</code></pre>

<p>This is one of the most well-documented methods to use. One issue with this is that it relies heavily on the “tidy data” concept, and also on the pipeline chaining concept from <code>dplyr</code> (we covered both in <a data-type="xref" href="ch02.xhtml#ch03">Chapter 2</a>). These concepts are specific to R, and to use <code>tidytext</code> successfully, you would have to learn them first, instead of directly jumping to processing your data. The second issue is the output of this procedure - a new <code>data.frame</code> containing the data in a processed column. While this might be what we need in the end, this skips a few intermediate steps and is several layers of abstraction higher than what we did with <code>nltk</code>. Lowering this abstraction and working in a more modular fashion (such as processing a single text field first) adheres to software development best practices, such as DRY (“Do not repeat yourself”) and separation of concerns.</p>

<p>The second step of our small NLP data processing pipeline is removing stop words<sup><a data-type="noteref" id="idm45127451276088-marker" href="ch04.xhtml#idm45127451276088">17</a></sup>.</p>

<pre data-type="programlisting" data-code-language="r"><code class="n">tidy_reviews</code> <code class="o">&lt;-</code> <code class="n">tidy_reviews</code> <code class="o">%&gt;%</code>
  <code class="nf">anti_join</code><code class="p">(</code><code class="n">stop_words</code><code class="p">)</code></pre>

<p>This code suffers from the same issues, along with a new confusing function - <code>anti_join</code>. Let’s compare to the simple list comprehension (more information on this in <a data-type="xref" href="ch03.xhtml#ch04">Chapter 3</a>) step in <code>nltk</code>:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">english_stop_words</code> <code class="o">=</code> <code class="nb">set</code><code class="p">(</code><code class="n">stopwords</code><code class="o">.</code><code class="n">words</code><code class="p">(</code><code class="s2">"english"</code><code class="p">))</code>
<code class="n">cleaned_words</code> <code class="o">=</code> <code class="p">[</code><code class="n">word</code> <code class="k">for</code> <code class="n">word</code> <code class="ow">in</code> <code class="n">words</code> <code class="k">if</code> <code class="n">word</code> <code class="ow">not</code> <code class="ow">in</code> <code class="n">english_stop_words</code><code class="p">]</code></pre>

<p><code>english_stop_words</code> is just a <code>list,</code> and then the only thing we do is loop through every word in another <code>list</code> (<code>words</code>) and remove it <em>if</em> it’s present in both. This is easier to understand. There’s no relying on advanced concepts or functions that are not directly related. This code is also at the right level of abstraction. Small code chunks can be used more flexibly as parts of a larger text processing pipeline function. A similar “meta” processing function in R can become bloated -  slow to execute and hard to read.</p>

<p>While <code>nltk</code> allows for such fundamental tasks, we’ll now have a look at a more advanced package - <code>spaCy</code>. We’ll use this for the third and final step in our case study - Part of Speech (PoS) tagging<sup><a data-type="noteref" id="idm45127451178120-marker" href="ch04.xhtml#idm45127451178120">18</a></sup>.</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">import</code><code> </code><code class="nn">spacy</code><code>
</code><code>
</code><code class="n">nlp</code><code> </code><code class="o">=</code><code> </code><code class="n">spacy</code><code class="o">.</code><code class="n">load</code><code class="p">(</code><code class="s2">"</code><code class="s2">en_core_web_sm</code><code class="s2">"</code><code class="p">)</code><code> </code><a class="co" id="co_data_format_context_CO3-1" href="#callout_data_format_context_CO3-1"><img src="Images/1.png" alt="1" width="12" height="12"/></a><code>
</code><code>
</code><code class="n">doc</code><code> </code><code class="o">=</code><code> </code><code class="n">nlp</code><code class="p">(</code><code class="n">example_review</code><code class="p">)</code><code> </code><a class="co" id="co_data_format_context_CO3-2" href="#callout_data_format_context_CO3-2"><img src="Images/2.png" alt="2" width="12" height="12"/></a><code>
</code><code class="k">print</code><code class="p">(</code><code class="nb">type</code><code class="p">(</code><code class="n">doc</code><code class="p">)</code><code class="p">)</code></pre>

<pre data-type="programlisting">spacy.tokens.doc.Doc</pre>
<dl class="calloutlist">
<dt><a class="co" id="callout_data_format_context_CO3-1" href="#co_data_format_context_CO3-1"><img src="Images/1.png" alt="1" width="12" height="12"/></a></dt>
<dd><p>Here we are loading all the advanced functionality we need through one function.</p></dd>
<dt><a class="co" id="callout_data_format_context_CO3-2" href="#co_data_format_context_CO3-2"><img src="Images/2.png" alt="2" width="12" height="12"/></a></dt>
<dd><p>We take one example review and feed it to a <code>spaCy</code> model, resulting in the <code>spacy.tokens.doc.Doc</code> type, not a <code>str</code>. This object can then be used for all kinds of other operations:</p></dd>
</dl>

<pre data-type="programlisting" data-code-language="python"><code class="k">for</code> <code class="n">token</code> <code class="ow">in</code> <code class="n">doc</code><code class="p">:</code>
    <code class="k">print</code><code class="p">(</code><code class="n">token</code><code class="o">.</code><code class="n">text</code><code class="p">,</code> <code class="n">token</code><code class="o">.</code><code class="n">pos_</code><code class="p">)</code></pre>

<p>The data is already tokenized on loading. Not only that - all the PoS tags are marked already!</p>

<p>The data processing steps that we covered are relatively basic. How about some newer and more advanced NLP methods? We can take word embeddings for example. This is one of the more advanced text vectorization<sup><a data-type="noteref" id="idm45127451081640-marker" href="ch04.xhtml#idm45127451081640">19</a></sup> methods, where each vector represents the meaning of a word based on its context. For that, we can already use the same <code>nlp</code> object from the <code>spaCy</code> code:</p>

<pre data-type="programlisting" data-code-language="python"><code class="k">for</code> <code class="n">token</code> <code class="ow">in</code> <code class="n">doc</code><code class="p">:</code>
    <code class="k">print</code><code class="p">(</code><code class="n">token</code><code class="o">.</code><code class="n">text</code><code class="p">,</code> <code class="n">token</code><code class="o">.</code><code class="n">has_vector</code><code class="p">,</code> <code class="n">token</code><code class="o">.</code><code class="n">vector_norm</code><code class="p">,</code> <code class="n">token</code><code class="o">.</code><code class="n">is_oov</code><code class="p">)</code></pre>

<pre data-type="programlisting">for token in doc:...
I True 21.885008 True
just True 22.404057 True
recently True 23.668447 True
purchased True 23.86188 True
her True 21.763712 True
' True 18.825636 True</pre>

<p>It’s a welcome surprise to see that those abilities are already built-in into one of the most popular Python NLP packages. On this level of NLP methods, we can see that there’s almost no alternative available in R (or even other languages for that matter). Many analogous solutions in R rely on wrapper code around a Python backend (which can defeat the purpose of using the R language)<sup><a data-type="noteref" id="idm45127451027384-marker" href="ch04.xhtml#idm45127451027384">20</a></sup>. This pattern is often seen in the book, especially in <a data-type="xref" href="ch05.xhtml#ch06">Chapter 5</a>. The same is also true for some other advanced methods such as transformer models<sup><a data-type="noteref" id="idm45127451025304-marker" href="ch04.xhtml#idm45127451025304">21</a></sup>.</p>

<p>For round two Python is again the winner. The capabilities of <code>nltk</code>, <code>spaCy</code> and other associated packages make it an excellent choice for NLP work!</p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Time series data"><div class="sect1" id="idm45127451415672">
<h1>Time series data</h1>

<p>The time-series format is used to store any data with an associated temporal dimension. It could be as simple as shampoo sales from a local grocery store, with a timestamp, or millions of data points from a sensor network measuring humidity in an agricultural field.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>There are some exceptions to the domination of R for the analysis of time-series data. The recent developments in deep learning methods, in particular, Long Short Term Memory networks (LSTM) have proved to be very successful for time series prediction. As is the case for other deep learning methods (more on this in <a data-type="xref" href="ch05.xhtml#ch06">Chapter 5</a>), this is an area better supported by Python tools.</p>
</div>








<section data-type="sect2" data-pdf-bookmark="Base R"><div class="sect2" id="idm45127451018776">
<h2>Base R</h2>

<p>There are quite a few different packages that an R user can use to analyze time-series data, including <code>xts</code>, and <code>zoo</code>, but we’ll be focusing on base R functions as a start. After this, we’ll have a look at one more modern package to illustrate more advanced functionality - <a href="https://facebook.github.io/prophet/">Facebook’s Prophet</a>.</p>

<p>Weather data is both widely available and relatively easy to interpret, so for our case study, we’ll analyze the daily minimum temperature in Australia (<a data-type="xref" href="#case-study-table">Table 4-2</a>). To do a time series analysis, we need to go through the following steps:</p>
<ol>
<li>
<p>Load the data into an appropriate format</p>
</li>
<li>
<p>Plot the data</p>
</li>
<li>
<p>Remove noise and seasonal effects and extract trend</p>
</li>

</ol>

<p>Then we would be able to proceed with more advanced analysis. Imagine we have loaded the data from a <code>.csv</code> file into a <code>data.frame</code> object in R. Nothing out of the ordinary here. Still, differently from most Python packages, R requires data coercion into a specific object type. In this case, we need to transform the <code>data.frame</code> into a <code>ts</code> (which stands for time series).</p>

<pre data-type="programlisting" data-code-language="r"><code class="n">df_ts</code> <code class="o">&lt;-</code> <code class="nf">ts</code><code class="p">(</code><code class="n">ts_data_raw</code><code class="o">$</code><code class="n">Temp</code><code class="p">,</code> <code class="n">start</code><code class="o">=</code><code class="nf">c</code><code class="p">(</code><code class="m">1981</code><code class="p">,</code> <code class="m">01</code><code class="p">,</code> <code class="m">01</code><code class="p">),</code>
            <code class="n">end</code><code class="o">=</code><code class="nf">c</code><code class="p">(</code><code class="m">1990</code><code class="p">,</code> <code class="m">12</code><code class="p">,</code> <code class="m">31</code><code class="p">),</code> <code class="n">frequency</code><code class="o">=</code><code class="m">365</code><code class="p">)</code>
<code class="nf">class</code><code class="p">(</code><code class="n">df_ts</code><code class="p">)</code></pre>

<p>So why would we prefer that to <code>pandas</code>? Well, even after you manage to convert the raw data into a time series <code>pd.DataFrame</code>, you’ll encounter a new concept - <code>DataFrame</code> indexing (see <a data-type="xref" href="#ts_index">Figure 4-4</a>). To be efficient in data munging, you’ll need to understand how this works first!</p>

<figure><div id="ts_index" class="figure">
<img src="Images/prds_0404.png" alt="" width="2947" height="1357"/>
<h6><span class="label">Figure 4-4. </span>The time series index in <code>pandas</code>.</h6>
</div></figure>

<p>This indexing concept can be confusing, so let’s now look at what the alternative is in R and whether that’s better. With the <code>df_ts</code> time series object, there are already a few useful things we can do. It’s also a good starting point when you are working with more advanced time series packages in R, since the coercion of a <code>ts</code> object into <code>xts</code> or <code>zoo</code>, should throw no errors (this once again is an example of the good object design we covered in <a data-type="xref" href="#package_design">Figure 4-2</a>). The first thing you can try to do is <code>plot</code> the object, which often yields good results in R:</p>

<pre data-type="programlisting" data-code-language="r"><code class="nf">plot</code><code class="p">(</code><code class="n">df_ts</code><code class="p">)</code></pre>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Calling the <code>plot</code> function does not simply use a standard function that can plot all kinds of different objects in R (this is what you would expect). It calls a particular method that is associated with the data object (more on the difference between functions and methods is available in <a data-type="xref" href="ch02.xhtml#ch03">Chapter 2</a>). A lot of complexity is hidden behind this simple function call!</p>
</div>

<figure><div id="ts_plot" class="figure">
<img src="Images/prds_0405.png" alt="" width="795" height="345"/>
<h6><span class="label">Figure 4-5. </span>Plot of a time-series (<code>ts</code>) object in base R.</h6>
</div></figure>

<p>The results from <code>plot(df_ts)</code> on <a data-type="xref" href="#ts_plot">Figure 4-5</a> are already useful. The dates on the x-axis are recognized, and a <code>line</code> plot is chosen instead of the default <code>points</code> Plot. The most prevalent issue in analyzing time-series data (and most ML data for that matter) is dealing with noise. The difference between this data format and others is that there are a few different noise sources, and different patterns can be cleaned. This is achieved by a technique called decomposition, for which we have the built-in and well-named function <code>decompose</code>:</p>

<pre data-type="programlisting" data-code-language="r"><code class="n">decomposed_ts</code> <code class="o">&lt;-</code> <code class="nf">decompose</code><code class="p">(</code><code class="n">df_ts</code><code class="p">)</code>
<code class="nf">plot</code><code class="p">(</code><code class="n">decomposed_ts</code><code class="p">)</code></pre>

<p>The results can be seen on <a data-type="xref" href="#ts_decompose">Figure 4-6</a>.</p>

<figure><div id="ts_decompose" class="figure">
<img src="Images/prds_0406.png" alt="" width="500" height="400"/>
<h6><span class="label">Figure 4-6. </span>Plot of decomposed time-series in base R.</h6>
</div></figure>

<p>We can see what the random noise is and also what is a seasonal and overall pattern. We achieved all this with just one function call in base R! In Python, we would need to use the <code>statsmodels</code> package to achieve the same.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="prophet"><div class="sect2" id="idm45127451018184">
<h2>prophet</h2>

<p>For analyzing time-series data, we also have another exciting package example. It’s simultaneously developed for both R and Python (similar to the <code>lime</code> explainable ML tool) - <a href="https://facebook.github.io/prophet/">Facebook Prophet</a>. This example can help us compare the differences in API design. <code>prophet</code> is a package whose main strength lies in the flexibility for a domain user to adjust to their particular need, ease of use of the API, and focus on production readiness. These factors make it a good choice for prototyping time series work and using it in a data product. Let’s have a look; our data is stored as a <code>pandas</code> <code>DataFrame</code> in <code>df</code>:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code><code> </code><code class="nn">fbprophet</code><code> </code><code class="kn">import</code><code> </code><code class="n">Prophet</code><code>
</code><code>
</code><code class="n">m</code><code> </code><code class="o">=</code><code> </code><code class="n">Prophet</code><code class="p">(</code><code class="p">)</code><code>
</code><code class="n">m</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">df</code><code class="p">)</code><code> </code><a class="co" id="co_data_format_context_CO4-1" href="#callout_data_format_context_CO4-1"><img src="Images/1.png" alt="1" width="12" height="12"/></a><code>
</code><code>
</code><code class="n">future</code><code> </code><code class="o">=</code><code> </code><code class="n">m</code><code class="o">.</code><code class="n">make_future_dataframe</code><code class="p">(</code><code class="n">periods</code><code class="o">=</code><code class="mi">365</code><code class="p">)</code><code> </code><a class="co" id="co_data_format_context_CO4-2" href="#callout_data_format_context_CO4-2"><img src="Images/2.png" alt="2" width="12" height="12"/></a><code>
</code><code class="n">future</code><code class="o">.</code><code class="n">tail</code><code class="p">(</code><code class="p">)</code></pre>
<dl class="calloutlist">
<dt><a class="co" id="callout_data_format_context_CO4-1" href="#co_data_format_context_CO4-1"><img src="Images/1.png" alt="1" width="12" height="12"/></a></dt>
<dd><p>Here we see the same <code>fit</code> API pattern again, borrowed from <code>scikit-learn</code>.</p></dd>
<dt><a class="co" id="callout_data_format_context_CO4-2" href="#co_data_format_context_CO4-2"><img src="Images/2.png" alt="2" width="12" height="12"/></a></dt>
<dd><p>This step creates a new empty <code>Data.Frame</code> that stores our predictions later.</p></dd>
</dl>

<pre data-type="programlisting" data-code-language="r"><code class="nf">library</code><code class="p">(</code><code class="n">prophet</code><code class="p">)</code>

<code class="n">m</code> <code class="o">&lt;-</code> <code class="nf">prophet</code><code class="p">(</code><code class="n">df</code><code class="p">)</code>

<code class="n">future</code> <code class="o">&lt;-</code> <code class="nf">make_future_dataframe</code><code class="p">(</code><code class="n">m</code><code class="p">,</code> <code class="n">periods</code> <code class="o">=</code> <code class="m">365</code><code class="p">)</code>
<code class="nf">tail</code><code class="p">(</code><code class="n">future</code><code class="p">)</code></pre>

<p>Both are simple enough and contain the same amount of steps - this is an excellent example of a consistent API design (more on this in <a data-type="xref" href="ch05.xhtml#ch06">Chapter 5</a>).</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>It’s an interesting and helpful idea to offer a consistent user experience across languages, but we do not predict it’ll be widely implemented. Few organizations possess the resources to do such work, which can be limiting since compromises have to be made in software design choices.</p>
</div>

<p>At this point, you can appreciate that knowing both languages would give you a significant advantage in daily work. If you were exposed only to the Python package ecosystem, you would probably try to find similar tools for analyzing time-series and missing out on the incredible opportunities that base R and related R packages provide.</p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Spatial data"><div class="sect1" id="idm45127450739832">
<h1>Spatial data</h1>

<p>The analysis of spatial data is one of the most promising areas in modern machine learning and has a rich history. New tools have been developed in recent years, but R has had the upper hand for a long time, despite some recent Python advances. As in the previous sections, we’ll look at a practical example to see the packages in action.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>There are several formats of spatial data available. In this subsection, we are focusing on the analysis of <em>raster</em> data. For other formats there are some interesting tools available in Python, such as <a href="https://geopandas.org/">GeoPandas</a>, but this is out of scope for this chapter.</p>
</div>

<p>Our task is to process occurrence<sup><a data-type="noteref" id="idm45127450735128-marker" href="ch04.xhtml#idm45127450735128">22</a></sup> and environmental data for <em>Loxodonta africana</em> (African elephant) make it suitable for spatial predictions. Such data processing is typical in Species Distribution Modeling (SDM), where the predictions are used to construct habitat suitability maps used for conservation. This case study is more advanced than the previous ones, and a lot of the steps hide some complexity where the packages are doing the heavy lifting. The steps are as follows:</p>
<ol>
<li>
<p>Obtain environmental raster data</p>
</li>
<li>
<p>Cut the raster to fit the area of interest</p>
</li>
<li>
<p>Deal with spatial autocorrelation with sampling methods</p>
</li>

</ol>








<section data-type="sect2" data-pdf-bookmark="raster"><div class="sect2" id="idm45127450730344">
<h2>raster</h2>

<p>To solve this problem as a first step, we need to process raster data<sup><a data-type="noteref" id="idm45127450728616-marker" href="ch04.xhtml#idm45127450728616">23</a></sup>. This is, in a way, very similar to standard image data, but still different in processing steps. For this R has the excellent <code>raster</code> package available (the alternative is Python’s <code>gdal</code> and R’s <code>rgdal</code>, which in our opinion, are trickier to use).</p>

<pre data-type="programlisting" data-code-language="r"><code class="nf">library</code><code class="p">(</code><code class="n">raster</code><code class="p">)</code>
<code class="n">climate_variables</code> <code class="o">&lt;-</code> <code class="nf">getData</code><code class="p">(</code><code class="n">name</code> <code class="o">=</code> <code class="s">"worldclim"</code><code class="p">,</code> <code class="n">var</code> <code class="o">=</code> <code class="s">"bio"</code><code class="p">,</code> <code class="n">res</code> <code class="o">=</code> <code class="m">10</code><code class="p">)</code></pre>

<p><code>raster</code> allows us to download most of the common useful spatial environmental datasets, including the bioclimactic data<sup><a data-type="noteref" id="idm45127450683720-marker" href="ch04.xhtml#idm45127450683720">24</a></sup>.</p>

<pre data-type="programlisting" data-code-language="r"><code class="n">e</code> <code class="o">&lt;-</code> <code class="nf">extent</code><code class="p">(</code><code class="n">xmin</code><code class="p">,</code> <code class="n">xmax</code><code class="p">,</code> <code class="n">ymin</code><code class="p">,</code> <code class="n">ymax</code><code class="p">)</code>
<code class="n">coords_absence</code> <code class="o">&lt;-</code> <code class="n">dismo</code><code class="o">::</code><code class="nf">randomPoints</code><code class="p">(</code><code class="n">climate_variables</code><code class="p">,</code> <code class="m">10000</code><code class="p">,</code> <code class="n">ext</code> <code class="o">=</code> <code class="n">e</code><code class="p">)</code>
<code class="n">points_absence</code> <code class="o">&lt;-</code> <code class="n">sp</code><code class="o">::</code><code class="nf">SpatialPoints</code><code class="p">(</code><code class="n">coords_absence</code><code class="p">,</code>
                                    <code class="n">proj4string</code> <code class="o">=</code> <code class="n">climate_variables</code><code class="o">@</code><code class="n">crs</code><code class="p">)</code>
<code class="n">env_absence</code> <code class="o">&lt;-</code> <code class="n">raster</code><code class="o">::</code><code class="nf">extract</code><code class="p">(</code><code class="n">climate_variables</code><code class="p">,</code> <code class="n">points_absence</code><code class="p">)</code></pre>

<p>Here we use the handy <code>extent</code> function to crop (cut) the raster data - we are only interested in a subsection of all those environmental layers surrounding the occurrence data. Here we use the longitude and latitude coordinates to draw this rectangle. As a next step, to have a classification problem, we are randomly sampling data points from the raster data (those are called “pseudo absences). You could imagine that those are the <code>0</code>’s in our classification task, and the occurrences (observations) are the <code>1</code>’s - the target variable. We then convert the pseudo-absences to <code>spatial points</code>, and finally extract the climate data for them as well. In the <code>SpatialPoints</code> function, you can also see how we specify the geographic projection system, one of the fundamental concepts when analyzing spatial data.</p>

<p>One of the most common issues when working in ML is correlations within the data. The fundamental assumption for a correct dataset is that the individual observations in the data are <em>independent</em> of each other to get accurate statistical results. This issue is always present in spatial data due to its very nature. This issue is called <em>spatial autocorrelation</em>. There are several packages available for sampling from the data to mitigate this risk to deal with this. One such package is <code>ENMeval</code>:</p>

<pre data-type="programlisting" data-code-language="r"><code class="nf">library</code><code class="p">(</code><code class="n">ENMeval</code><code class="p">)</code>
<code class="n">check1</code> <code class="o">&lt;-</code> <code class="nf">get.checkerboard1</code><code class="p">(</code><code class="n">occs</code><code class="p">,</code> <code class="n">envs</code><code class="p">,</code> <code class="n">bg</code><code class="p">,</code> <code class="n">aggregation.factor</code><code class="o">=</code><code class="m">5</code><code class="p">)</code></pre>

<p>The <code>get.checkerboard1</code> function samples the data in an evenly distributed manner, similar to taking equal points from each square from a black and white chessboard. We can then take this resampled data and successfully train an ML model without worrying about spatial autocorrelation. As a final step, we can take those predictions and create the habitat suitability map, shown on (<a data-type="xref" href="#sdm_map">???</a>).</p>

<pre data-type="programlisting" data-code-language="r"><code class="n">raster_prediction</code> <code class="o">&lt;-</code> <code class="nf">predict</code><code class="p">(</code><code class="n">predictors</code><code class="p">,</code> <code class="n">model</code><code class="p">)</code>
<code class="nf">plot</code><code class="p">(</code><code class="n">raster_prediction</code><code class="p">)</code></pre>
<ol id="sdm_map">
<li>
<p>Plot of a <code>raster</code> object prediction in R, resulting in a habitat suitability map.
image::img/sdm_map.png[""]</p>
</li>

</ol>

<p>When you’re working with spatial raster data, the better package design is provided by R. The fundamental tools such as <code>raster</code> provide a consistent foundation for more advanced application specific ones such as <code>ENMeval</code> and <code>dismo</code>, without the need to worry about complex transformation or error-prone type coercion.</p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Final thoughts"><div class="sect1" id="idm45127450729720">
<h1>Final thoughts</h1>

<p>In this chapter we went through the different common data formats, and what are the best packages to process them so they are ready for advanced tasks. In each case study, we demonstrated a good package design and how that can make a data scientist more productive. We have seen that for more ML-focused tasks, such as CV and NLP, Python is providing the better user experience and lower learning curve. In contrast, for more time series prediction and spatial analysis, R has the upper hand. Those selection choices are shown on <a data-type="xref" href="#decision_tree">Figure 4-7</a>.</p>

<figure><div id="decision_tree" class="figure">
<img src="Images/prds_0407.png" alt="" width="2344" height="1706"/>
<h6><span class="label">Figure 4-7. </span>Decision tree for package selection.</h6>
</div></figure>

<p>What the best tools have in common is the better package design (<a data-type="xref" href="#package_design">Figure 4-2</a>). You should always use the optimal tool for the job and pay attention to the complexity, documentation, and performance of the tools you use!</p>
</div></section>







<div data-type="footnotes"><p data-type="footnote" id="idm45127452048648"><sup><a href="ch04.xhtml#idm45127452048648-marker">1</a></sup> For a more thorough explanation on this have a look here: <a href="https://realpython.com/python-encodings-guide/"><em class="hyperlink">https://realpython.com/python-encodings-guide/</em></a>.</p><p data-type="footnote" id="idm45127452046808"><sup><a href="ch04.xhtml#idm45127452046808-marker">2</a></sup> This is commonly referred to as “data lineage”.</p><p data-type="footnote" id="idm45127452007560"><sup><a href="ch04.xhtml#idm45127452007560-marker">3</a></sup> Who else didn’t learn what <code>if __name__ == "__main__"</code> does in Python?</p><p data-type="footnote" id="idm45127452002248"><sup><a href="ch04.xhtml#idm45127452002248-marker">4</a></sup> One table from the data, stored in a single file.</p><p data-type="footnote" id="idm45127451851368"><sup><a href="ch04.xhtml#idm45127451851368-marker">5</a></sup> Not to forget <code>tidyr</code>, which was discussed in <a data-type="xref" href="ch02.xhtml#ch03">Chapter 2</a></p><p data-type="footnote" id="idm45127451849368"><sup><a href="ch04.xhtml#idm45127451849368-marker">6</a></sup> We did mention that statisticians are very literal, right?</p><p data-type="footnote" id="idm45127451817912"><sup><a href="ch04.xhtml#idm45127451817912-marker">7</a></sup> This consistency is a common thread in the chapters in <a data-type="xref" href="part03.xhtml#p03">Part III</a> and is addressed additionally in <a data-type="xref" href="ch05.xhtml#ch06">Chapter 5</a>.</p><p data-type="footnote" id="idm45127451810536"><sup><a href="ch04.xhtml#idm45127451810536-marker">8</a></sup> Remember - garbage in, garbage out.</p><p data-type="footnote" id="idm45127451809800"><sup><a href="ch04.xhtml#idm45127451809800-marker">9</a></sup> Using code to go through the content of a web page, download and store it in a machine-readable format.</p><p data-type="footnote" id="idm45127451808600"><sup><a href="ch04.xhtml#idm45127451808600-marker">10</a></sup> Which can be expensive, or even impossible in some cases.</p><p data-type="footnote" id="idm45127451807656"><sup><a href="ch04.xhtml#idm45127451807656-marker">11</a></sup> If you want to learn more on data augmentation of images have a look at <a href="https://www.tensorflow.org/tutorials/images/data_augmentation">this</a> tutorial.</p><p data-type="footnote" id="idm45127451457144"><sup><a href="ch04.xhtml#idm45127451457144-marker">12</a></sup> At the time of writing.</p><p data-type="footnote" id="idm45127451455800"><sup><a href="ch04.xhtml#idm45127451455800-marker">13</a></sup> Not to be confused with the conference of the same name, the PyData stack refers to <code>NumPy</code>, <code>SciPy</code>, <code>Pandas</code>, <code>IPython</code> and <code>matplotlib</code>.</p><p data-type="footnote" id="idm45127451424392"><sup><a href="ch04.xhtml#idm45127451424392-marker">14</a></sup> The R community has also rallied to the call and improved the tooling in recent times, but it still arguably lags behind its Python counterparts.</p><p data-type="footnote" id="idm45127451414392"><sup><a href="ch04.xhtml#idm45127451414392-marker">15</a></sup> To learn more about NLTK have a look at the official book available <a href="https://www.nltk.org/book/">here</a>.</p><p data-type="footnote" id="idm45127451365400"><sup><a href="ch04.xhtml#idm45127451365400-marker">16</a></sup> Data type coercion is the conversion of one data type to another.</p><p data-type="footnote" id="idm45127451276088"><sup><a href="ch04.xhtml#idm45127451276088-marker">17</a></sup> This is a common step in NLP. Some examples of stop words are “the”, “a” and “this”. These need to be removed since they rarely offer useful information for ML algorithms.</p><p data-type="footnote" id="idm45127451178120"><sup><a href="ch04.xhtml#idm45127451178120-marker">18</a></sup> The process of labeling words in with the PoS they belong to.</p><p data-type="footnote" id="idm45127451081640"><sup><a href="ch04.xhtml#idm45127451081640-marker">19</a></sup> Converting text into numbers for ingestion by a ML algorithm.</p><p data-type="footnote" id="idm45127451027384"><sup><a href="ch04.xhtml#idm45127451027384-marker">20</a></sup> Such as trying to create custom embeddings. Check the RStudio blog <a href="https://blogs.rstudio.com/ai/posts/2017-12-22-word-embeddings-with-keras/">here</a> for more information.</p><p data-type="footnote" id="idm45127451025304"><sup><a href="ch04.xhtml#idm45127451025304-marker">21</a></sup> You can read more about that <a href="https://blogs.rstudio.com/ai/posts/2020-07-30-state-of-the-art-nlp-models-from-r/">here</a>.</p><p data-type="footnote" id="idm45127450735128"><sup><a href="ch04.xhtml#idm45127450735128-marker">22</a></sup> Location-tagged observations of the animal in the wild.</p><p data-type="footnote" id="idm45127450728616"><sup><a href="ch04.xhtml#idm45127450728616-marker">23</a></sup> Data representing cells, where the cell value represents some information.</p><p data-type="footnote" id="idm45127450683720"><sup><a href="ch04.xhtml#idm45127450683720-marker">24</a></sup> Environmental features that have been determined by ecologists to be highly predictive of species distributions, i.e. humidity and temperature.</p></div></div></section></div></body></html>