<html><head></head><body><section data-pdf-bookmark="Chapter 11. Machine Learning" data-type="chapter" epub:type="chapter"><div class="chapter" id="machine_learning">&#13;
<h1><span class="label">Chapter 11. </span>Machine Learning</h1>&#13;
&#13;
<blockquote data-type="epigraph" epub:type="epigraph">&#13;
    <p>I am always ready to learn although I do not always like being taught.</p>&#13;
    <p data-type="attribution">Winston Churchill</p>&#13;
</blockquote>&#13;
&#13;
<p>Many people imagine that data science is mostly machine learning and that data scientists mostly build and train and tweak machine learning models all day long. (Then again, many of those people don’t actually know what machine learning <em>is</em>.) In fact, data science is mostly turning business problems into data problems and collecting data and understanding data and cleaning data and formatting data, after which machine learning is almost an afterthought. Even so, it’s an interesting and essential afterthought that you pretty much have to know about in order to do data science.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Modeling" data-type="sect1"><div class="sect1" id="idm45635743785928">&#13;
<h1>Modeling</h1>&#13;
&#13;
<p>Before<a data-primary="machine learning" data-secondary="modeling" data-type="indexterm" id="idm45635743784360"/><a data-primary="modeling" data-type="indexterm" id="idm45635743783352"/><a data-primary="predictive models" data-secondary="definition of modeling" data-type="indexterm" id="idm45635743782680"/> we can talk about machine learning, we need to talk about <em>models</em>.</p>&#13;
&#13;
<p>What is a model?  It’s simply a specification of a mathematical (or probabilistic) relationship that exists between different variables.</p>&#13;
&#13;
<p>For instance, if you’re trying to raise money for your social networking site,&#13;
you might<a data-primary="business models" data-type="indexterm" id="idm45635743780088"/> build a <em>business model</em> (likely in a spreadsheet)&#13;
that takes inputs like “number of users,” “ad revenue per user,” and “number of employees” and outputs your annual profit for the next several years.  A cookbook recipe entails a model that relates inputs like “number of eaters” and “hungriness” to quantities of ingredients needed.  And if you’ve ever watched poker on television, you know that each player’s “win probability” is estimated in real time based on a model that takes into account the cards that have been revealed so far and the distribution of cards in the deck.</p>&#13;
&#13;
<p>The business model is probably based on simple mathematical relationships: profit is revenue minus expenses, revenue is units sold times average price, and so on.  The recipe model is probably based on trial and error—someone went in a kitchen and tried different combinations of ingredients until they found one they liked.  And the poker model is based on probability theory, the rules of poker, and some reasonably innocuous assumptions about the random process by which cards are dealt.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="What Is Machine Learning?" data-type="sect1"><div class="sect1" id="idm45635743777192">&#13;
<h1>What Is Machine Learning?</h1>&#13;
&#13;
<p>Everyone<a data-primary="machine learning" data-secondary="definition of term" data-type="indexterm" id="idm45635743775592"/> has her own exact definition, but we’ll use <em>machine learning</em> to refer to creating and using models that are <em>learned from data</em>.  In<a data-primary="predictive models" data-secondary="machine learning and" data-type="indexterm" id="idm45635743773656"/><a data-primary="data mining" data-type="indexterm" id="idm45635743772648"/> other contexts this might be called <em>predictive modeling</em> or <em>data mining</em>, but we will stick with machine learning.  Typically, our goal will be to use existing data to develop models that we can use to <em>predict</em> various outcomes for new data, such as:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Whether an email message is spam or not</p>&#13;
</li>&#13;
<li>&#13;
<p>Whether a credit card transaction is fraudulent</p>&#13;
</li>&#13;
<li>&#13;
<p>Which advertisement a shopper is most likely to click on</p>&#13;
</li>&#13;
<li>&#13;
<p>Which football team is going to win the Super Bowl</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>We’ll<a data-primary="predictive models" data-secondary="types of models" data-type="indexterm" id="idm45635743765848"/><a data-primary="supervised models" data-type="indexterm" id="idm45635743764840"/><a data-primary="unsupervised models" data-type="indexterm" id="idm45635743764168"/><a data-primary="semisupervised models" data-type="indexterm" id="idm45635743763496"/> look at both <em>supervised</em> models (in which there is a set of data labeled with the correct answers to learn from) and <em>unsupervised</em> models (in which there are no such labels).  There are various other types, like <em>semisupervised</em> (in which only some of the data are labeled), <em>online</em> (in which the model needs to continuously adjust to newly arriving data), and<a data-primary="reinforcement models" data-type="indexterm" id="idm45635743760856"/> <em>reinforcement</em> (in which, after making a series of predictions, the model gets a signal indicating how well it did) that we won’t cover in this book.</p>&#13;
&#13;
<p>Now, in<a data-primary="parameterized models" data-type="indexterm" id="idm45635743759160"/> even the simplest situation there are entire universes of models that might describe the relationship we’re interested in.  In most cases we will ourselves choose a <em>parameterized</em> family of models and then use data to learn parameters that are in some way optimal.</p>&#13;
&#13;
<p>For instance, we might assume that a person’s height is (roughly) a linear function of his weight and then use data to learn what that linear function is.  Or we might assume that a decision tree is a good way to diagnose what diseases our patients have and then use data to learn the “optimal” such tree. Throughout the rest of the book, we’ll be investigating different families of models that we can learn.</p>&#13;
&#13;
<p>But before we can do that, we need to better understand the fundamentals of machine learning.  For the rest of the chapter, we’ll discuss some of those basic concepts, before we move on to the models themselves.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Overfitting and Underfitting" data-type="sect1"><div class="sect1" id="overfitting">&#13;
<h1>Overfitting and Underfitting</h1>&#13;
&#13;
<p>A<a data-primary="machine learning" data-secondary="overfitting and underfitting" data-type="indexterm" id="idm45635743716968"/><a data-primary="overfitting and underfitting" data-type="indexterm" id="idm45635743715944"/><a data-primary="underfitting and overfitting" data-type="indexterm" id="idm45635743715256"/> common danger in machine learning is <em>overfitting</em>—producing a model that performs well on the data you train it on but generalizes poorly to any new data. This could involve learning <em>noise</em> in the data.&#13;
Or it could involve learning to identify specific inputs rather than whatever factors are actually predictive for the desired output.</p>&#13;
&#13;
<p>The other side of this is <em>underfitting</em>—producing a model that&#13;
doesn’t perform well even on the training data, although typically&#13;
when this happens you decide your model isn’t good enough&#13;
and keep looking for a better one.</p>&#13;
&#13;
<p>In <a data-type="xref" href="#overfitting_and_underfitting">Figure 11-1</a>, I’ve fit three polynomials to a sample of data. (Don’t worry about how; we’ll get to that in later chapters.)</p>&#13;
&#13;
<figure><div class="figure" id="overfitting_and_underfitting">&#13;
<img alt="Overfitting and Underfitting." src="assets/dsf2_1101.png"/>&#13;
<h6><span class="label">Figure 11-1. </span>Overfitting and underfitting</h6>&#13;
</div></figure>&#13;
&#13;
<p>The horizontal line shows the best fit degree 0 (i.e., constant) polynomial. It severely <em>underfits</em> the training data. The best fit degree 9 (i.e., 10-parameter) polynomial goes through every training data point exactly, but it very severely <em>overfits</em>; if we were to pick a few more data points, it would quite likely miss them by a lot. And the degree 1 line strikes a nice balance; it’s pretty close to every point, and—if these data are representative—the line will likely be close to new data points as well.</p>&#13;
&#13;
<p>Clearly, models that are too complex lead to overfitting&#13;
and don’t generalize well beyond the data they were trained on.&#13;
So how do we make sure our models aren’t too complex?  The most fundamental approach&#13;
involves using different data to train the model and to test the model.</p>&#13;
&#13;
<p>The simplest way to do this is to split the dataset, so that (for example) two-thirds of it is used to train the model, after which we measure the model’s performance on the remaining third:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="kn">import</code> <code class="nn">random</code>&#13;
<code class="kn">from</code> <code class="nn">typing</code> <code class="kn">import</code> <code class="n">TypeVar</code><code class="p">,</code> <code class="n">List</code><code class="p">,</code> <code class="n">Tuple</code>&#13;
<code class="n">X</code> <code class="o">=</code> <code class="n">TypeVar</code><code class="p">(</code><code class="s1">'X'</code><code class="p">)</code>  <code class="c1"># generic type to represent a data point</code>&#13;
&#13;
<code class="k">def</code> <code class="nf">split_data</code><code class="p">(</code><code class="n">data</code><code class="p">:</code> <code class="n">List</code><code class="p">[</code><code class="n">X</code><code class="p">],</code> <code class="n">prob</code><code class="p">:</code> <code class="nb">float</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="n">Tuple</code><code class="p">[</code><code class="n">List</code><code class="p">[</code><code class="n">X</code><code class="p">],</code> <code class="n">List</code><code class="p">[</code><code class="n">X</code><code class="p">]]:</code>&#13;
    <code class="sd">"""Split data into fractions [prob, 1 - prob]"""</code>&#13;
    <code class="n">data</code> <code class="o">=</code> <code class="n">data</code><code class="p">[:]</code>                    <code class="c1"># Make a shallow copy</code>&#13;
    <code class="n">random</code><code class="o">.</code><code class="n">shuffle</code><code class="p">(</code><code class="n">data</code><code class="p">)</code>              <code class="c1"># because shuffle modifies the list.</code>&#13;
    <code class="n">cut</code> <code class="o">=</code> <code class="nb">int</code><code class="p">(</code><code class="nb">len</code><code class="p">(</code><code class="n">data</code><code class="p">)</code> <code class="o">*</code> <code class="n">prob</code><code class="p">)</code>       <code class="c1"># Use prob to find a cutoff</code>&#13;
    <code class="k">return</code> <code class="n">data</code><code class="p">[:</code><code class="n">cut</code><code class="p">],</code> <code class="n">data</code><code class="p">[</code><code class="n">cut</code><code class="p">:]</code>     <code class="c1"># and split the shuffled list there.</code>&#13;
&#13;
<code class="n">data</code> <code class="o">=</code> <code class="p">[</code><code class="n">n</code> <code class="k">for</code> <code class="n">n</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">1000</code><code class="p">)]</code>&#13;
<code class="n">train</code><code class="p">,</code> <code class="n">test</code> <code class="o">=</code> <code class="n">split_data</code><code class="p">(</code><code class="n">data</code><code class="p">,</code> <code class="mf">0.75</code><code class="p">)</code>&#13;
&#13;
<code class="c1"># The proportions should be correct</code>&#13;
<code class="k">assert</code> <code class="nb">len</code><code class="p">(</code><code class="n">train</code><code class="p">)</code> <code class="o">==</code> <code class="mi">750</code>&#13;
<code class="k">assert</code> <code class="nb">len</code><code class="p">(</code><code class="n">test</code><code class="p">)</code> <code class="o">==</code> <code class="mi">250</code>&#13;
&#13;
<code class="c1"># And the original data should be preserved (in some order)</code>&#13;
<code class="k">assert</code> <code class="nb">sorted</code><code class="p">(</code><code class="n">train</code> <code class="o">+</code> <code class="n">test</code><code class="p">)</code> <code class="o">==</code> <code class="n">data</code></pre>&#13;
&#13;
<p>Often, we’ll have paired input variables and output variables.&#13;
In that case, we need to make sure to put corresponding values together in either the training data or the test data:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="n">Y</code> <code class="o">=</code> <code class="n">TypeVar</code><code class="p">(</code><code class="s1">'Y'</code><code class="p">)</code>  <code class="c1"># generic type to represent output variables</code>&#13;
&#13;
<code class="k">def</code> <code class="nf">train_test_split</code><code class="p">(</code><code class="n">xs</code><code class="p">:</code> <code class="n">List</code><code class="p">[</code><code class="n">X</code><code class="p">],</code>&#13;
                     <code class="n">ys</code><code class="p">:</code> <code class="n">List</code><code class="p">[</code><code class="n">Y</code><code class="p">],</code>&#13;
                     <code class="n">test_pct</code><code class="p">:</code> <code class="nb">float</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="n">Tuple</code><code class="p">[</code><code class="n">List</code><code class="p">[</code><code class="n">X</code><code class="p">],</code> <code class="n">List</code><code class="p">[</code><code class="n">X</code><code class="p">],</code> <code class="n">List</code><code class="p">[</code><code class="n">Y</code><code class="p">],</code>&#13;
                                                                 <code class="n">List</code><code class="p">[</code><code class="n">Y</code><code class="p">]]:</code>&#13;
    <code class="c1"># Generate the indices and split them</code>&#13;
    <code class="n">idxs</code> <code class="o">=</code> <code class="p">[</code><code class="n">i</code> <code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="nb">len</code><code class="p">(</code><code class="n">xs</code><code class="p">))]</code>&#13;
    <code class="n">train_idxs</code><code class="p">,</code> <code class="n">test_idxs</code> <code class="o">=</code> <code class="n">split_data</code><code class="p">(</code><code class="n">idxs</code><code class="p">,</code> <code class="mi">1</code> <code class="o">-</code> <code class="n">test_pct</code><code class="p">)</code>&#13;
&#13;
    <code class="k">return</code> <code class="p">([</code><code class="n">xs</code><code class="p">[</code><code class="n">i</code><code class="p">]</code> <code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="n">train_idxs</code><code class="p">],</code>  <code class="c1"># x_train</code>&#13;
            <code class="p">[</code><code class="n">xs</code><code class="p">[</code><code class="n">i</code><code class="p">]</code> <code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="n">test_idxs</code><code class="p">],</code>   <code class="c1"># x_test</code>&#13;
            <code class="p">[</code><code class="n">ys</code><code class="p">[</code><code class="n">i</code><code class="p">]</code> <code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="n">train_idxs</code><code class="p">],</code>  <code class="c1"># y_train</code>&#13;
            <code class="p">[</code><code class="n">ys</code><code class="p">[</code><code class="n">i</code><code class="p">]</code> <code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="n">test_idxs</code><code class="p">])</code>   <code class="c1"># y_test</code></pre>&#13;
&#13;
<p>As always, we want to make sure our code works right:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="n">xs</code> <code class="o">=</code> <code class="p">[</code><code class="n">x</code> <code class="k">for</code> <code class="n">x</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">1000</code><code class="p">)]</code>  <code class="c1"># xs are 1 ... 1000</code>&#13;
<code class="n">ys</code> <code class="o">=</code> <code class="p">[</code><code class="mi">2</code> <code class="o">*</code> <code class="n">x</code> <code class="k">for</code> <code class="n">x</code> <code class="ow">in</code> <code class="n">xs</code><code class="p">]</code>       <code class="c1"># each y_i is twice x_i</code>&#13;
<code class="n">x_train</code><code class="p">,</code> <code class="n">x_test</code><code class="p">,</code> <code class="n">y_train</code><code class="p">,</code> <code class="n">y_test</code> <code class="o">=</code> <code class="n">train_test_split</code><code class="p">(</code><code class="n">xs</code><code class="p">,</code> <code class="n">ys</code><code class="p">,</code> <code class="mf">0.25</code><code class="p">)</code>&#13;
&#13;
<code class="c1"># Check that the proportions are correct</code>&#13;
<code class="k">assert</code> <code class="nb">len</code><code class="p">(</code><code class="n">x_train</code><code class="p">)</code> <code class="o">==</code> <code class="nb">len</code><code class="p">(</code><code class="n">y_train</code><code class="p">)</code> <code class="o">==</code> <code class="mi">750</code>&#13;
<code class="k">assert</code> <code class="nb">len</code><code class="p">(</code><code class="n">x_test</code><code class="p">)</code> <code class="o">==</code> <code class="nb">len</code><code class="p">(</code><code class="n">y_test</code><code class="p">)</code> <code class="o">==</code> <code class="mi">250</code>&#13;
&#13;
<code class="c1"># Check that the corresponding data points are paired correctly</code>&#13;
<code class="k">assert</code> <code class="nb">all</code><code class="p">(</code><code class="n">y</code> <code class="o">==</code> <code class="mi">2</code> <code class="o">*</code> <code class="n">x</code> <code class="k">for</code> <code class="n">x</code><code class="p">,</code> <code class="n">y</code> <code class="ow">in</code> <code class="nb">zip</code><code class="p">(</code><code class="n">x_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">))</code>&#13;
<code class="k">assert</code> <code class="nb">all</code><code class="p">(</code><code class="n">y</code> <code class="o">==</code> <code class="mi">2</code> <code class="o">*</code> <code class="n">x</code> <code class="k">for</code> <code class="n">x</code><code class="p">,</code> <code class="n">y</code> <code class="ow">in</code> <code class="nb">zip</code><code class="p">(</code><code class="n">x_test</code><code class="p">,</code> <code class="n">y_test</code><code class="p">))</code></pre>&#13;
&#13;
<p>After which you can do something like:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="n">model</code> <code class="o">=</code> <code class="n">SomeKindOfModel</code><code class="p">()</code>&#13;
<code class="n">x_train</code><code class="p">,</code> <code class="n">x_test</code><code class="p">,</code> <code class="n">y_train</code><code class="p">,</code> <code class="n">y_test</code> <code class="o">=</code> <code class="n">train_test_split</code><code class="p">(</code><code class="n">xs</code><code class="p">,</code> <code class="n">ys</code><code class="p">,</code> <code class="mf">0.33</code><code class="p">)</code>&#13;
<code class="n">model</code><code class="o">.</code><code class="n">train</code><code class="p">(</code><code class="n">x_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code>&#13;
<code class="n">performance</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">test</code><code class="p">(</code><code class="n">x_test</code><code class="p">,</code> <code class="n">y_test</code><code class="p">)</code></pre>&#13;
&#13;
<p>If the model was overfit to the training data,&#13;
then it will hopefully perform really poorly on the (completely separate) test data.&#13;
Said differently, if it performs well on the test data,&#13;
then you can be more confident that it’s <em>fitting</em> rather than <em>overfitting</em>.</p>&#13;
&#13;
<p>However, there are a couple of ways this can go wrong.</p>&#13;
&#13;
<p>The first is if there are common patterns in the test and training data that wouldn’t generalize to a larger dataset.</p>&#13;
&#13;
<p>For example, imagine that your dataset consists of user activity, with one row per user per week.&#13;
In such a case, most users will appear in both the training data and the test data,&#13;
and certain models might learn to <em>identify</em> users rather than discover relationships&#13;
involving <em>attributes</em>.  This isn’t a huge worry, although it did happen to me once.</p>&#13;
&#13;
<p>A bigger problem is if you use the test/train split not just to judge a model&#13;
but also to <em>choose</em> from among many models.  In that case, although each&#13;
individual model may not be overfit, “choosing a model that performs best on the test set”&#13;
is a meta-training that makes the test set function as a second training set.  (Of course the model&#13;
that performed best on the test set is going to perform well on the test set.)</p>&#13;
&#13;
<p>In<a data-primary="training sets" data-type="indexterm" id="idm45635743203736"/><a data-primary="validation sets" data-type="indexterm" id="idm45635743203000"/><a data-primary="test sets" data-type="indexterm" id="idm45635743202328"/> such a situation, you should split the data into three parts: a training set for building models, a <em>validation</em> set for choosing among trained models, and a test set for judging the final model.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Correctness" data-type="sect1"><div class="sect1" id="idm45635743718072">&#13;
<h1>Correctness</h1>&#13;
&#13;
<p>When<a data-primary="machine learning" data-secondary="correctness" data-type="indexterm" id="idm45635743199272"/><a data-primary="correctness" data-type="indexterm" id="idm45635743198264"/> I’m not doing data science, I dabble in medicine.  And in my spare time I’ve come up with a cheap, noninvasive test that can be given to a newborn baby that predicts—with greater than 98% accuracy—whether the newborn will ever develop leukemia.  My lawyer has convinced me the test is unpatentable, so I’ll share with you the details here: predict leukemia if and only if the baby is named Luke (which sounds sort of like “leukemia”).</p>&#13;
&#13;
<p>As<a data-primary="accuracy" data-type="indexterm" id="idm45635743196616"/> we’ll see, this test is indeed more than 98% accurate.  Nonetheless, it’s an incredibly stupid test, and a good illustration of why we don’t typically use “accuracy” to measure how good a (binary classification) model is.</p>&#13;
&#13;
<p>Imagine<a data-primary="binary judgments" data-type="indexterm" id="idm45635743195112"/> building a model to make a <em>binary</em> judgment. Is this email spam?  Should we hire this candidate?  Is this air traveler secretly a terrorist?</p>&#13;
&#13;
<p>Given<a data-primary="type 1/type 2 errors" data-type="indexterm" id="idm45635743193320"/><a data-primary="true positives/true negatives" data-type="indexterm" id="idm45635743192584"/><a data-primary="false negatives/false positives" data-type="indexterm" id="idm45635743191944"/> a set of labeled data and such a predictive model, every data point lies in one of four categories:</p>&#13;
<dl>&#13;
<dt>True positive</dt>&#13;
<dd>&#13;
<p>“This message is spam, and we correctly predicted spam.”</p>&#13;
</dd>&#13;
<dt>False positive (Type 1 error)</dt>&#13;
<dd>&#13;
<p>“This message is not spam, but we predicted spam.”</p>&#13;
</dd>&#13;
<dt>False negative (Type 2 error)</dt>&#13;
<dd>&#13;
<p>“This message is spam, but we predicted not spam.”</p>&#13;
</dd>&#13;
<dt>True negative</dt>&#13;
<dd>&#13;
<p>“This message is not spam, and we correctly predicted not spam.”</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>We<a data-primary="confusion matrix" data-type="indexterm" id="idm45635743184888"/> often represent these as counts in a <em>confusion matrix</em>:</p>&#13;
<table>&#13;
&#13;
<thead>&#13;
<tr>&#13;
<th/>&#13;
<th>Spam</th>&#13;
<th>Not spam</th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr>&#13;
<td><p>Predict “spam”</p></td>&#13;
<td><p>True positive</p></td>&#13;
<td><p>False positive</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>Predict “not spam”</p></td>&#13;
<td><p>False negative</p></td>&#13;
<td><p>True negative</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
&#13;
<p>Let’s see how my leukemia test fits into this framework.  These days approximately <a href="https://www.babycenter.com/baby-names-luke-2918.htm">5 babies out of 1,000 are named Luke</a>. And the lifetime prevalence of leukemia is about 1.4%, or <a href="https://seer.cancer.gov/statfacts/html/leuks.html">14 out of every 1,000 people</a>.</p>&#13;
&#13;
<p>If we believe these two factors are independent and apply my “Luke is for leukemia” test to 1 million people, we’d expect to see a confusion matrix like:</p>&#13;
<table>&#13;
&#13;
<thead>&#13;
<tr>&#13;
<th/>&#13;
<th>Leukemia</th>&#13;
<th>No leukemia</th>&#13;
<th>Total</th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr>&#13;
<td><p>“Luke”</p></td>&#13;
<td><p>70</p></td>&#13;
<td><p>4,930</p></td>&#13;
<td><p>5,000</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>Not “Luke”</p></td>&#13;
<td><p>13,930</p></td>&#13;
<td><p>981,070</p></td>&#13;
<td><p>995,000</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>Total</p></td>&#13;
<td><p>14,000</p></td>&#13;
<td><p>986,000</p></td>&#13;
<td><p>1,000,000</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
&#13;
<p>We can then use these to compute various statistics about model performance.  For example, <em>accuracy</em> is defined as the fraction of correct predictions:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="k">def</code> <code class="nf">accuracy</code><code class="p">(</code><code class="n">tp</code><code class="p">:</code> <code class="nb">int</code><code class="p">,</code> <code class="n">fp</code><code class="p">:</code> <code class="nb">int</code><code class="p">,</code> <code class="n">fn</code><code class="p">:</code> <code class="nb">int</code><code class="p">,</code> <code class="n">tn</code><code class="p">:</code> <code class="nb">int</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">float</code><code class="p">:</code>&#13;
    <code class="n">correct</code> <code class="o">=</code> <code class="n">tp</code> <code class="o">+</code> <code class="n">tn</code>&#13;
    <code class="n">total</code> <code class="o">=</code> <code class="n">tp</code> <code class="o">+</code> <code class="n">fp</code> <code class="o">+</code> <code class="n">fn</code> <code class="o">+</code> <code class="n">tn</code>&#13;
    <code class="k">return</code> <code class="n">correct</code> <code class="o">/</code> <code class="n">total</code>&#13;
&#13;
<code class="k">assert</code> <code class="n">accuracy</code><code class="p">(</code><code class="mi">70</code><code class="p">,</code> <code class="mi">4930</code><code class="p">,</code> <code class="mi">13930</code><code class="p">,</code> <code class="mi">981070</code><code class="p">)</code> <code class="o">==</code> <code class="mf">0.98114</code></pre>&#13;
&#13;
<p>That seems like a pretty impressive number.  But clearly this is not a good test, which means that we probably shouldn’t put a lot of credence in raw accuracy.</p>&#13;
&#13;
<p>It’s common<a data-primary="precision" data-type="indexterm" id="idm45635743077160"/><a data-primary="recall" data-type="indexterm" id="idm45635743018856"/> to look at the combination of <em>precision</em> and <em>recall</em>.  Precision measures how accurate our <em>positive</em> predictions were:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="k">def</code> <code class="nf">precision</code><code class="p">(</code><code class="n">tp</code><code class="p">:</code> <code class="nb">int</code><code class="p">,</code> <code class="n">fp</code><code class="p">:</code> <code class="nb">int</code><code class="p">,</code> <code class="n">fn</code><code class="p">:</code> <code class="nb">int</code><code class="p">,</code> <code class="n">tn</code><code class="p">:</code> <code class="nb">int</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">float</code><code class="p">:</code>&#13;
    <code class="k">return</code> <code class="n">tp</code> <code class="o">/</code> <code class="p">(</code><code class="n">tp</code> <code class="o">+</code> <code class="n">fp</code><code class="p">)</code>&#13;
&#13;
<code class="k">assert</code> <code class="n">precision</code><code class="p">(</code><code class="mi">70</code><code class="p">,</code> <code class="mi">4930</code><code class="p">,</code> <code class="mi">13930</code><code class="p">,</code> <code class="mi">981070</code><code class="p">)</code> <code class="o">==</code> <code class="mf">0.014</code></pre>&#13;
&#13;
<p>And recall measures what fraction of the positives our model identified:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="k">def</code> <code class="nf">recall</code><code class="p">(</code><code class="n">tp</code><code class="p">:</code> <code class="nb">int</code><code class="p">,</code> <code class="n">fp</code><code class="p">:</code> <code class="nb">int</code><code class="p">,</code> <code class="n">fn</code><code class="p">:</code> <code class="nb">int</code><code class="p">,</code> <code class="n">tn</code><code class="p">:</code> <code class="nb">int</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">float</code><code class="p">:</code>&#13;
    <code class="k">return</code> <code class="n">tp</code> <code class="o">/</code> <code class="p">(</code><code class="n">tp</code> <code class="o">+</code> <code class="n">fn</code><code class="p">)</code>&#13;
&#13;
<code class="k">assert</code> <code class="n">recall</code><code class="p">(</code><code class="mi">70</code><code class="p">,</code> <code class="mi">4930</code><code class="p">,</code> <code class="mi">13930</code><code class="p">,</code> <code class="mi">981070</code><code class="p">)</code> <code class="o">==</code> <code class="mf">0.005</code></pre>&#13;
&#13;
<p>These are both terrible numbers, reflecting that this is a terrible model.</p>&#13;
&#13;
<p>Sometimes<a data-primary="F1 scores" data-type="indexterm" id="idm45635742859144"/> precision and recall are combined into the <em>F1 score</em>, which is defined as:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="k">def</code> <code class="nf">f1_score</code><code class="p">(</code><code class="n">tp</code><code class="p">:</code> <code class="nb">int</code><code class="p">,</code> <code class="n">fp</code><code class="p">:</code> <code class="nb">int</code><code class="p">,</code> <code class="n">fn</code><code class="p">:</code> <code class="nb">int</code><code class="p">,</code> <code class="n">tn</code><code class="p">:</code> <code class="nb">int</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">float</code><code class="p">:</code>&#13;
    <code class="n">p</code> <code class="o">=</code> <code class="n">precision</code><code class="p">(</code><code class="n">tp</code><code class="p">,</code> <code class="n">fp</code><code class="p">,</code> <code class="n">fn</code><code class="p">,</code> <code class="n">tn</code><code class="p">)</code>&#13;
    <code class="n">r</code> <code class="o">=</code> <code class="n">recall</code><code class="p">(</code><code class="n">tp</code><code class="p">,</code> <code class="n">fp</code><code class="p">,</code> <code class="n">fn</code><code class="p">,</code> <code class="n">tn</code><code class="p">)</code>&#13;
&#13;
    <code class="k">return</code> <code class="mi">2</code> <code class="o">*</code> <code class="n">p</code> <code class="o">*</code> <code class="n">r</code> <code class="o">/</code> <code class="p">(</code><code class="n">p</code> <code class="o">+</code> <code class="n">r</code><code class="p">)</code></pre>&#13;
&#13;
<p>This<a data-primary="harmonic mean" data-type="indexterm" id="idm45635742856632"/> is the <a href="http://en.wikipedia.org/wiki/Harmonic_mean"><em>harmonic mean</em></a> of precision and recall and necessarily lies between them.</p>&#13;
&#13;
<p>Usually the choice of a model involves a tradeoff between precision and recall.  A model that predicts “yes” when it’s even a little bit confident will probably have a high recall but a low precision; a model that predicts “yes” only when it’s extremely confident is likely to have a low recall and a high precision.</p>&#13;
&#13;
<p>Alternatively, you can think of this as a tradeoff between false positives and false negatives.  Saying “yes” too often will give you lots of false positives; saying “no” too often will give you lots of false negatives.</p>&#13;
&#13;
<p>Imagine that there were 10 risk factors for leukemia, and that the more of them you had the more likely you were to develop leukemia.  In that case you can imagine a continuum of tests: “predict leukemia if at least one risk factor,” “predict leukemia if at least two risk factors,” and so on.  As you increase the threshold, you increase the test’s precision (since people with more risk factors are more likely to develop the disease), and you decrease the test’s recall (since fewer and fewer of the eventual disease-sufferers will meet the threshold).  In cases like this, choosing the right threshold is a matter of finding the right tradeoff.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="The Bias-Variance Tradeoff" data-type="sect1"><div class="sect1" id="idm45635743200120">&#13;
<h1>The Bias-Variance Tradeoff</h1>&#13;
&#13;
<p>Another<a data-primary="machine learning" data-secondary="bias-variance tradeoff" data-type="indexterm" id="idm45635742774056"/><a data-primary="bias-variance tradeoff" data-type="indexterm" id="idm45635742773048"/><a data-primary="variance" data-type="indexterm" id="idm45635742772376"/> way of thinking about the overfitting problem&#13;
is as a tradeoff between bias and variance.</p>&#13;
&#13;
<p>Both are measures of what would happen if you were to retrain your model&#13;
many times on different sets of training data (from the same larger population).</p>&#13;
&#13;
<p>For example, the degree 0 model in <a data-type="xref" href="#overfitting">“Overfitting and Underfitting”</a> will make a lot of mistakes&#13;
for pretty much any training set (drawn from the same population),&#13;
which means that it has a high <em>bias</em>.&#13;
However, any two randomly chosen training sets should give pretty similar models&#13;
(since any two randomly chosen training sets should have pretty similar average values).&#13;
So we say that it has a low <em>variance</em>.  High bias and low variance typically correspond to underfitting.</p>&#13;
&#13;
<p>On the other hand, the degree 9 model&#13;
fit the training set perfectly.  It has very low bias&#13;
but very high variance (since any two training sets would likely give rise to very&#13;
different models).  This corresponds to overfitting.</p>&#13;
&#13;
<p>Thinking about model problems this way can help you figure out what to do when&#13;
your model doesn’t work so well.</p>&#13;
&#13;
<p>If your model has high bias&#13;
(which means it performs poorly even on your training data), one thing to try is adding more features. Going from the degree 0 model in <a data-type="xref" href="#overfitting">“Overfitting and Underfitting”</a>&#13;
to the degree 1 model was a big improvement.</p>&#13;
&#13;
<p>If your model has high variance, you can similarly <em>remove</em> features.  But another solution is to obtain more data (if you can).</p>&#13;
&#13;
<p>In <a data-type="xref" href="#overfit_underfit_more_data">Figure 11-2</a>, we fit a degree 9 polynomial to different size samples. The model fit based on 10 data points is all over the place, as we saw before. If we instead train on 100 data points, there’s much less overfitting. And the model trained from 1,000 data points looks very similar to the degree 1 model. Holding model complexity constant, the more data you have, the harder it is to overfit. On the other hand, more data won’t help with bias. If your model doesn’t use enough features to capture regularities in the data, throwing more data at it won’t help.</p>&#13;
&#13;
<figure><div class="figure" id="overfit_underfit_more_data">&#13;
<img alt="Reducing Variance With More Data." src="assets/dsf2_1102.png"/>&#13;
<h6><span class="label">Figure 11-2. </span>Reducing variance with more data</h6>&#13;
</div></figure>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Feature Extraction and Selection" data-type="sect1"><div class="sect1" id="feature_extraction">&#13;
<h1>Feature Extraction and Selection</h1>&#13;
&#13;
<p>As<a data-primary="machine learning" data-secondary="feature extraction and selection" data-type="indexterm" id="idm45635742759432"/><a data-primary="feature extraction and selection" data-type="indexterm" id="idm45635742758360"/> has been mentioned, when your data doesn’t have enough features, your model is likely to underfit.&#13;
And when your data has too many features, it’s easy to overfit. But what are features, and where do they come from?</p>&#13;
&#13;
<p><em>Features</em> are whatever inputs we provide to our model.</p>&#13;
&#13;
<p>In the simplest case, features are simply given to you.  If you want to&#13;
predict someone’s salary based on her years of experience,&#13;
then years of experience is the only feature you have. (Although, as we saw in <a data-type="xref" href="#overfitting">“Overfitting and Underfitting”</a>, you might also consider adding years of experience squared, cubed, and so on if that helps you build a better model.)</p>&#13;
&#13;
<p>Things become more interesting as your data becomes more complicated.&#13;
Imagine trying to build a<a data-primary="spam filter example" data-type="indexterm" id="idm45635742754536"/> spam filter to predict whether an email&#13;
is junk or not.  Most models won’t know what to do with a raw email,&#13;
which is just a collection of text.  You’ll have to extract features. For example:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Does the email contain the word <em>Viagra</em>?</p>&#13;
</li>&#13;
<li>&#13;
<p>How many times does the letter <em>d</em> appear?</p>&#13;
</li>&#13;
<li>&#13;
<p>What was the domain of the sender?</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>The answer to a question like the first question here is simply a yes or no, which we typically encode as a 1 or 0.&#13;
The second is a number.  And the third is a choice from&#13;
a discrete set of options.</p>&#13;
&#13;
<p>Pretty much always, we’ll extract features from our data that fall into one of these three categories. What’s more, the types of features we have constrain the types of models we can use.</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>The Naive Bayes classifier we’ll build in <a data-type="xref" href="ch13.html#naive_bayes">Chapter 13</a>&#13;
is suited to yes-or-no features, like the first one in the preceding list.</p>&#13;
</li>&#13;
<li>&#13;
<p>Regression models, which we’ll study in Chapters <a data-type="xref" data-xrefstyle="select:labelnumber" href="ch14.html#simple_linear_regression">14</a> and&#13;
<a data-type="xref" data-xrefstyle="select:labelnumber" href="ch16.html#logistic_regression">16</a>, require numeric features (which could include dummy variables that are 0s and 1s).</p>&#13;
</li>&#13;
<li>&#13;
<p>And decision trees, which we’ll look at in <a data-type="xref" href="ch17.html#decision_trees">Chapter 17</a>,&#13;
can deal with numeric or categorical data.</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>Although in the spam filter example we looked for ways to create features, sometimes we’ll instead look for ways to remove features.</p>&#13;
&#13;
<p>For example, your inputs might be vectors of several hundred numbers.&#13;
Depending on the situation, it might be appropriate to&#13;
distill these down to a handful of important dimensions&#13;
(as in <a data-type="xref" href="ch10.html#principal_component_analysis">“Dimensionality Reduction”</a>)&#13;
and use only that small number of features.&#13;
Or it might be appropriate to use a technique&#13;
(like regularization, which we’ll look at in <a data-type="xref" href="ch15.html#regularization">“Regularization”</a>)&#13;
that penalizes models the more features they use.</p>&#13;
&#13;
<p>How do we choose features?&#13;
That’s where<a data-primary="domain expertise" data-type="indexterm" id="idm45635742737336"/> a combination of <em>experience</em> and <em>domain expertise</em> comes into play.&#13;
If you’ve received lots of emails,&#13;
then you probably have a sense that the presence&#13;
of certain words might be a good indicator of spamminess.&#13;
And you might also get the sense that the number of <em>d</em>s&#13;
is likely not a good indicator of spamminess.  But in general you’ll have to try different things, which is part of the fun.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section class="pagebreak-before" data-pdf-bookmark="For Further Exploration" data-type="sect1"><div class="sect1" id="idm45635742760664">&#13;
<h1>For Further Exploration</h1>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Keep reading! The next several chapters are about different families of machine learning models.</p>&#13;
</li>&#13;
<li>&#13;
<p>The<a data-primary="machine learning" data-secondary="resources for learning about" data-type="indexterm" id="idm45635742688280"/><a data-primary="Coursera" data-type="indexterm" id="idm45635742687304"/> Coursera <a href="https://www.coursera.org/course/ml">Machine Learning</a> course is the original MOOC and is a good place to get a deeper understanding of the basics of machine learning.</p>&#13;
</li>&#13;
<li>&#13;
<p><em>The Elements of Statistical Learning</em>, by Jerome H. Friedman, Robert Tibshirani, and Trevor Hastie (Springer), is a somewhat canonical textbook that can be <a href="http://stanford.io/1ycOXbo">downloaded online for free</a>.  But be warned: it’s <em>very</em> mathy.</p>&#13;
</li>&#13;
</ul>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section></body></html>