- en: Chapter 8\. Gradient Descent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Those who boast of their descent, brag on what they owe to others.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Seneca
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Frequently when doing data science, we’ll be trying to the find the best model
    for a certain situation. And usually “best” will mean something like “minimizes
    the error of its predictions” or “maximizes the likelihood of the data.” In other
    words, it will represent the solution to some sort of optimization problem.
  prefs: []
  type: TYPE_NORMAL
- en: This means we’ll need to solve a number of optimization problems. And in particular,
    we’ll need to solve them from scratch. Our approach will be a technique called
    *gradient descent*, which lends itself pretty well to a from-scratch treatment.
    You might not find it super-exciting in and of itself, but it will enable us to
    do exciting things throughout the book, so bear with me.
  prefs: []
  type: TYPE_NORMAL
- en: The Idea Behind Gradient Descent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Suppose we have some function `f` that takes as input a vector of real numbers
    and outputs a single real number. One simple such function is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: We’ll frequently need to maximize or minimize such functions. That is, we need
    to find the input `v` that produces the largest (or smallest) possible value.
  prefs: []
  type: TYPE_NORMAL
- en: For functions like ours, the *gradient* (if you remember your calculus, this
    is the vector of partial derivatives) gives the input direction in which the function
    most quickly increases. (If you don’t remember your calculus, take my word for
    it or look it up on the internet.)
  prefs: []
  type: TYPE_NORMAL
- en: Accordingly, one approach to maximizing a function is to pick a random starting
    point, compute the gradient, take a small step in the direction of the gradient
    (i.e., the direction that causes the function to increase the most), and repeat
    with the new starting point. Similarly, you can try to minimize a function by
    taking small steps in the *opposite* direction, as shown in [Figure 8-1](#gradient_descent_image).
  prefs: []
  type: TYPE_NORMAL
- en: '![Finding a minimum using gradient descent.](assets/dsf2_0801.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-1\. Finding a minimum using gradient descent
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If a function has a unique global minimum, this procedure is likely to find
    it. If a function has multiple (local) minima, this procedure might “find” the
    wrong one of them, in which case you might rerun the procedure from different
    starting points. If a function has no minimum, then it’s possible the procedure
    might go on forever.
  prefs: []
  type: TYPE_NORMAL
- en: Estimating the Gradient
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If `f` is a function of one variable, its derivative at a point `x` measures
    how `f(x)` changes when we make a very small change to `x`. The derivative is
    defined as the limit of the difference quotients:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: as `h` approaches zero.
  prefs: []
  type: TYPE_NORMAL
- en: (Many a would-be calculus student has been stymied by the mathematical definition
    of limit, which is beautiful but can seem somewhat forbidding. Here we’ll cheat
    and simply say that “limit” means what you think it means.)
  prefs: []
  type: TYPE_NORMAL
- en: The derivative is the slope of the tangent line at <math><mrow><mo>(</mo> <mi>x</mi>
    <mo>,</mo> <mi>f</mi> <mo>(</mo> <mi>x</mi> <mo>)</mo> <mo>)</mo></mrow></math>
    , while the difference quotient is the slope of the not-quite-tangent line that
    runs through <math><mrow><mo>(</mo> <mi>x</mi> <mo>+</mo> <mi>h</mi> <mo>,</mo>
    <mi>f</mi> <mo>(</mo> <mi>x</mi> <mo>+</mo> <mi>h</mi> <mo>)</mo> <mo>)</mo></mrow></math>
    . As *h* gets smaller and smaller, the not-quite-tangent line gets closer and
    closer to the tangent line ([Figure 8-2](#difference_quotient)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Difference quotient as approximation to derivative.](assets/dsf2_0802.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-2\. Approximating a derivative with a difference quotient
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'For many functions it’s easy to exactly calculate derivatives. For example,
    the `square` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'has the derivative:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: which is easy for us to check by explicitly computing the difference quotient
    and taking the limit. (Doing so requires nothing more than high school algebra.)
  prefs: []
  type: TYPE_NORMAL
- en: 'What if you couldn’t (or didn’t want to) find the gradient? Although we can’t
    take limits in Python, we can estimate derivatives by evaluating the difference
    quotient for a very small `e`. [Figure 8-3](#difference_quotient_goodness) shows
    the results of one such estimation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '![Difference quotient is a good approximation.](assets/dsf2_0803.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-3\. Goodness of difference quotient approximation
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: When `f` is a function of many variables, it has multiple *partial derivatives*,
    each indicating how `f` changes when we make small changes in just one of the
    input variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'We calculate its *i*th partial derivative by treating it as a function of just
    its *i*th variable, holding the other variables fixed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'after which we can estimate the gradient the same way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: A major drawback to this “estimate using difference quotients” approach is that
    it’s computationally expensive. If `v` has length *n*, `estimate_gradient` has
    to evaluate `f` on 2*n* different inputs. If you’re repeatedly estimating gradients,
    you’re doing a whole lot of extra work. In everything we do, we’ll use math to
    calculate our gradient functions explicitly.
  prefs: []
  type: TYPE_NORMAL
- en: Using the Gradient
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It’s easy to see that the `sum_of_squares` function is smallest when its input
    `v` is a vector of zeros. But imagine we didn’t know that. Let’s use gradients
    to find the minimum among all three-dimensional vectors. We’ll just pick a random
    starting point and then take tiny steps in the opposite direction of the gradient
    until we reach a point where the gradient is very small:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: If you run this, you’ll find that it always ends up with a `v` that’s very close
    to `[0,0,0]`. The more epochs you run it for, the closer it will get.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the Right Step Size
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Although the rationale for moving against the gradient is clear, how far to
    move is not. Indeed, choosing the right step size is more of an art than a science.
    Popular options include:'
  prefs: []
  type: TYPE_NORMAL
- en: Using a fixed step size
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gradually shrinking the step size over time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At each step, choosing the step size that minimizes the value of the objective
    function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The last approach sounds great but is, in practice, a costly computation. To
    keep things simple, we’ll mostly just use a fixed step size. The step size that
    “works” depends on the problem—too small, and your gradient descent will take
    forever; too big, and you’ll take giant steps that might make the function you
    care about get larger or even be undefined. So we’ll need to experiment.
  prefs: []
  type: TYPE_NORMAL
- en: Using Gradient Descent to Fit Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this book, we’ll be using gradient descent to fit parameterized models to
    data. In the usual case, we’ll have some dataset and some (hypothesized) model
    for the data that depends (in a differentiable way) on one or more parameters.
    We’ll also have a *loss* function that measures how well the model fits our data.
    (Smaller is better.)
  prefs: []
  type: TYPE_NORMAL
- en: 'If we think of our data as being fixed, then our loss function tells us how
    good or bad any particular model parameters are. This means we can use gradient
    descent to find the model parameters that make the loss as small as possible.
    Let’s look at a simple example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: In this case we *know* the parameters of the linear relationship between `x`
    and `y`, but imagine we’d like to learn them from the data. We’ll use gradient
    descent to find the slope and intercept that minimize the average squared error.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll start off with a function that determines the gradient based on the error
    from a single data point:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Let’s think about what that gradient means. Imagine for some `x` our prediction
    is too large. In that case the `error` is positive. The second gradient term,
    `2 * error`, is positive, which reflects the fact that small increases in the
    intercept will make the (already too large) prediction even larger, which will
    cause the squared error (for this `x`) to get even bigger.
  prefs: []
  type: TYPE_NORMAL
- en: The first gradient term, `2 * error * x`, has the same sign as `x`. Sure enough,
    if `x` is positive, small increases in the slope will again make the prediction
    (and hence the error) larger. If `x` is negative, though, small increases in the
    slope will make the prediction (and hence the error) smaller.
  prefs: []
  type: TYPE_NORMAL
- en: Now, that computation was for a single data point. For the whole dataset we’ll
    look at the *mean squared error*. And the gradient of the mean squared error is
    just the mean of the individual gradients.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, here’s what we’re going to do:'
  prefs: []
  type: TYPE_NORMAL
- en: Start with a random value for `theta`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the mean of the gradients.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Adjust `theta` in that direction.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'After a lot of *epochs* (what we call each pass through the dataset), we should
    learn something like the correct parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Minibatch and Stochastic Gradient Descent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One drawback of the preceding approach is that we had to evaluate the gradients
    on the entire dataset before we could take a gradient step and update our parameters.
    In this case it was fine, because our dataset was only 100 pairs and the gradient
    computation was cheap.
  prefs: []
  type: TYPE_NORMAL
- en: Your models, however, will frequently have large datasets and expensive gradient
    computations. In that case you’ll want to take gradient steps more often.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can do this using a technique called *minibatch gradient descent*, in which
    we compute the gradient (and take a gradient step) based on a “minibatch” sampled
    from the larger dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The `TypeVar(T)` allows us to create a “generic” function. It says that our
    `dataset` can be a list of any single type—`str`s, `int`s, `list`s, whatever—but
    whatever that type is, the outputs will be batches of it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we can solve our problem again using minibatches:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Another variation is *stochastic gradient descent*, in which you take gradient
    steps based on one training example at a time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: On this problem, stochastic gradient descent finds the optimal parameters in
    a much smaller number of epochs. But there are always tradeoffs. Basing gradient
    steps on small minibatches (or on single data points) allows you to take more
    of them, but the gradient for a single point might lie in a very different direction
    from the gradient for the dataset as a whole.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, if we weren’t doing our linear algebra from scratch, there would
    be performance gains from “vectorizing” our computations across batches rather
    than computing the gradient one point at a time.
  prefs: []
  type: TYPE_NORMAL
- en: Throughout the book, we’ll play around to find optimal batch sizes and step
    sizes.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The terminology for the various flavors of gradient descent is not uniform.
    The “compute the gradient for the whole dataset” approach is often called *batch
    gradient descent*, and some people say *stochastic gradient descent* when referring
    to the minibatch version (of which the one-point-at-a-time version is a special
    case).
  prefs: []
  type: TYPE_NORMAL
- en: For Further Exploration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Keep reading! We’ll be using gradient descent to solve problems throughout the
    rest of the book.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At this point, you’re undoubtedly sick of me recommending that you read textbooks.
    If it’s any consolation, [*Active Calculus 1.0*](https://scholarworks.gvsu.edu/books/10/),
    by Matthew Boelkins, David Austin, and Steven Schlicker (Grand Valley State University
    Libraries), seems nicer than the calculus textbooks I learned from.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sebastian Ruder has an [epic blog post](http://ruder.io/optimizing-gradient-descent/index.html)
    comparing gradient descent and its many variants.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
