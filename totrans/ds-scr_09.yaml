- en: Chapter 8\. Gradient Descent
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第8章\. 梯度下降
- en: Those who boast of their descent, brag on what they owe to others.
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 那些夸口自己的衰退的人，吹嘘他们欠别人的东西。
- en: ''
  id: totrans-2
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Seneca
  id: totrans-3
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 塞内加
- en: Frequently when doing data science, we’ll be trying to the find the best model
    for a certain situation. And usually “best” will mean something like “minimizes
    the error of its predictions” or “maximizes the likelihood of the data.” In other
    words, it will represent the solution to some sort of optimization problem.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在做数据科学时，我们经常会试图找到某种情况下的最佳模型。通常，“最佳”意味着“最小化预测误差”或“最大化数据的可能性”。换句话说，它将代表某种优化问题的解决方案。
- en: This means we’ll need to solve a number of optimization problems. And in particular,
    we’ll need to solve them from scratch. Our approach will be a technique called
    *gradient descent*, which lends itself pretty well to a from-scratch treatment.
    You might not find it super-exciting in and of itself, but it will enable us to
    do exciting things throughout the book, so bear with me.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着我们需要解决许多优化问题。特别是，我们需要从头开始解决它们。我们的方法将是一种称为*梯度下降*的技术，它非常适合从头开始的处理。你可能不会觉得它本身特别令人兴奋，但它将使我们能够在整本书中做一些令人兴奋的事情，所以请忍耐一下。
- en: The Idea Behind Gradient Descent
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 梯度下降背后的思想
- en: 'Suppose we have some function `f` that takes as input a vector of real numbers
    and outputs a single real number. One simple such function is:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个函数`f`，它以一个实数向量作为输入并输出一个实数。一个简单的这样的函数是：
- en: '[PRE0]'
  id: totrans-8
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We’ll frequently need to maximize or minimize such functions. That is, we need
    to find the input `v` that produces the largest (or smallest) possible value.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们经常需要最大化或最小化这样的函数。也就是说，我们需要找到产生最大（或最小）可能值的输入`v`。
- en: For functions like ours, the *gradient* (if you remember your calculus, this
    is the vector of partial derivatives) gives the input direction in which the function
    most quickly increases. (If you don’t remember your calculus, take my word for
    it or look it up on the internet.)
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 对于像我们这样的函数，*梯度*（如果你记得你的微积分，这是偏导数的向量）给出了函数增加最快的输入方向。（如果你不记得你的微积分，相信我或者去互联网上查一查。）
- en: Accordingly, one approach to maximizing a function is to pick a random starting
    point, compute the gradient, take a small step in the direction of the gradient
    (i.e., the direction that causes the function to increase the most), and repeat
    with the new starting point. Similarly, you can try to minimize a function by
    taking small steps in the *opposite* direction, as shown in [Figure 8-1](#gradient_descent_image).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 相应地，最大化一个函数的一种方法是选择一个随机起点，计算梯度，沿着梯度的方向迈出一小步（即导致函数增加最多的方向），然后用新的起点重复这个过程。类似地，你可以尝试通过向*相反*方向迈小步来最小化一个函数，如[图 8-1](#gradient_descent_image)所示。
- en: '![Finding a minimum using gradient descent.](assets/dsf2_0801.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![使用梯度下降找到最小值。](assets/dsf2_0801.png)'
- en: Figure 8-1\. Finding a minimum using gradient descent
  id: totrans-13
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8-1\. 使用梯度下降找到最小值
- en: Note
  id: totrans-14
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: If a function has a unique global minimum, this procedure is likely to find
    it. If a function has multiple (local) minima, this procedure might “find” the
    wrong one of them, in which case you might rerun the procedure from different
    starting points. If a function has no minimum, then it’s possible the procedure
    might go on forever.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个函数有一个唯一的全局最小值，这个过程很可能会找到它。如果一个函数有多个（局部）最小值，这个过程可能会“找到”它们中的错误之一，这种情况下你可能需要从不同的起点重新运行该过程。如果一个函数没有最小值，那么可能这个过程会永远进行下去。
- en: Estimating the Gradient
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 估计梯度
- en: 'If `f` is a function of one variable, its derivative at a point `x` measures
    how `f(x)` changes when we make a very small change to `x`. The derivative is
    defined as the limit of the difference quotients:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 如果`f`是一个关于一个变量的函数，在点`x`处的导数测量了当我们对`x`做一个非常小的变化时`f(x)`如何改变。导数被定义为差商的极限：
- en: '[PRE1]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: as `h` approaches zero.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 当`h`趋近于零时。
- en: (Many a would-be calculus student has been stymied by the mathematical definition
    of limit, which is beautiful but can seem somewhat forbidding. Here we’ll cheat
    and simply say that “limit” means what you think it means.)
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: （许多想要学习微积分的人都被极限的数学定义所困扰，这是美丽的但可能看起来有些令人生畏的。在这里，我们将作弊并简单地说，“极限”意味着你认为的意思。）
- en: The derivative is the slope of the tangent line at <math><mrow><mo>(</mo> <mi>x</mi>
    <mo>,</mo> <mi>f</mi> <mo>(</mo> <mi>x</mi> <mo>)</mo> <mo>)</mo></mrow></math>
    , while the difference quotient is the slope of the not-quite-tangent line that
    runs through <math><mrow><mo>(</mo> <mi>x</mi> <mo>+</mo> <mi>h</mi> <mo>,</mo>
    <mi>f</mi> <mo>(</mo> <mi>x</mi> <mo>+</mo> <mi>h</mi> <mo>)</mo> <mo>)</mo></mrow></math>
    . As *h* gets smaller and smaller, the not-quite-tangent line gets closer and
    closer to the tangent line ([Figure 8-2](#difference_quotient)).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 导数是切线在 <math><mrow><mo>(</mo> <mi>x</mi> <mo>,</mo> <mi>f</mi> <mo>(</mo> <mi>x</mi>
    <mo>)</mo> <mo>)</mo></mrow></math> 处的斜率，而差商是穿过 <math><mrow><mo>(</mo> <mi>x</mi>
    <mo>+</mo> <mi>h</mi> <mo>,</mo> <mi>f</mi> <mo>(</mo> <mi>x</mi> <mo>+</mo> <mi>h</mi>
    <mo>)</mo> <mo>)</mo></mrow></math> 的不完全切线的斜率。随着 *h* 变得越来越小，这个不完全切线越来越接近切线 ([图 8-2](#difference_quotient))。
- en: '![Difference quotient as approximation to derivative.](assets/dsf2_0802.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![差商作为导数的近似。](assets/dsf2_0802.png)'
- en: Figure 8-2\. Approximating a derivative with a difference quotient
  id: totrans-23
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8-2\. 使用差商近似导数
- en: 'For many functions it’s easy to exactly calculate derivatives. For example,
    the `square` function:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 对于许多函数来说，精确计算导数是很容易的。例如，`square` 函数：
- en: '[PRE2]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'has the derivative:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 具有导数：
- en: '[PRE3]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: which is easy for us to check by explicitly computing the difference quotient
    and taking the limit. (Doing so requires nothing more than high school algebra.)
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们来说很容易检查的是，通过明确计算差商并取极限。(这只需要高中代数就可以做到。)
- en: 'What if you couldn’t (or didn’t want to) find the gradient? Although we can’t
    take limits in Python, we can estimate derivatives by evaluating the difference
    quotient for a very small `e`. [Figure 8-3](#difference_quotient_goodness) shows
    the results of one such estimation:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不能（或不想）找到梯度怎么办？尽管我们不能在 Python 中取极限，但我们可以通过评估一个非常小的 `e` 的差商来估计导数。[图 8-3](#difference_quotient_goodness)
    显示了这样一个估计的结果：
- en: '[PRE4]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![Difference quotient is a good approximation.](assets/dsf2_0803.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![差商是一个良好的近似。](assets/dsf2_0803.png)'
- en: Figure 8-3\. Goodness of difference quotient approximation
  id: totrans-32
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 8-3\. 差商近似的好处
- en: When `f` is a function of many variables, it has multiple *partial derivatives*,
    each indicating how `f` changes when we make small changes in just one of the
    input variables.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 当 `f` 是多变量函数时，它具有多个 *偏导数*，每个偏导数指示当我们只对其中一个输入变量进行微小变化时 `f` 如何变化。
- en: 'We calculate its *i*th partial derivative by treating it as a function of just
    its *i*th variable, holding the other variables fixed:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过将其视为仅其第 *i* 个变量的函数，并固定其他变量来计算其 *i*th 偏导数：
- en: '[PRE5]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'after which we can estimate the gradient the same way:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以以同样的方式估计梯度：
- en: '[PRE6]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Note
  id: totrans-38
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: A major drawback to this “estimate using difference quotients” approach is that
    it’s computationally expensive. If `v` has length *n*, `estimate_gradient` has
    to evaluate `f` on 2*n* different inputs. If you’re repeatedly estimating gradients,
    you’re doing a whole lot of extra work. In everything we do, we’ll use math to
    calculate our gradient functions explicitly.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这种“使用差商估计”的方法的一个主要缺点是计算成本高昂。如果 `v` 的长度为 *n*，`estimate_gradient` 必须在 2*n* 个不同的输入上评估
    `f`。如果你反复估计梯度，那么你需要做大量额外的工作。在我们所做的一切中，我们将使用数学来显式计算我们的梯度函数。
- en: Using the Gradient
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用梯度
- en: 'It’s easy to see that the `sum_of_squares` function is smallest when its input
    `v` is a vector of zeros. But imagine we didn’t know that. Let’s use gradients
    to find the minimum among all three-dimensional vectors. We’ll just pick a random
    starting point and then take tiny steps in the opposite direction of the gradient
    until we reach a point where the gradient is very small:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 很容易看出，当其输入 `v` 是一个零向量时，`sum_of_squares` 函数最小。但想象一下，如果我们不知道这一点。让我们使用梯度来找到所有三维向量中的最小值。我们只需选择一个随机起点，然后沿着梯度的相反方向迈出微小的步骤，直到我们到达梯度非常小的点：
- en: '[PRE7]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: If you run this, you’ll find that it always ends up with a `v` that’s very close
    to `[0,0,0]`. The more epochs you run it for, the closer it will get.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你运行这个程序，你会发现它总是最终得到一个非常接近 `[0,0,0]` 的 `v`。你运行的周期越多，它就会越接近。
- en: Choosing the Right Step Size
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择合适的步长
- en: 'Although the rationale for moving against the gradient is clear, how far to
    move is not. Indeed, choosing the right step size is more of an art than a science.
    Popular options include:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管反对梯度的理由很明确，但移动多远并不清楚。事实上，选择正确的步长更多地是一门艺术而不是一门科学。流行的选择包括：
- en: Using a fixed step size
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用固定步长
- en: Gradually shrinking the step size over time
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随着时间逐渐缩小步长
- en: At each step, choosing the step size that minimizes the value of the objective
    function
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在每一步中，选择使目标函数值最小化的步长。
- en: The last approach sounds great but is, in practice, a costly computation. To
    keep things simple, we’ll mostly just use a fixed step size. The step size that
    “works” depends on the problem—too small, and your gradient descent will take
    forever; too big, and you’ll take giant steps that might make the function you
    care about get larger or even be undefined. So we’ll need to experiment.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一种方法听起来很好，但在实践中是一种昂贵的计算。为了保持简单，我们将主要使用固定步长。“有效”的步长取决于问题——太小，你的梯度下降将永远不会结束；太大，你将采取巨大的步骤，可能会使你关心的函数变得更大甚至未定义。因此，我们需要进行实验。
- en: Using Gradient Descent to Fit Models
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用梯度下降拟合模型
- en: In this book, we’ll be using gradient descent to fit parameterized models to
    data. In the usual case, we’ll have some dataset and some (hypothesized) model
    for the data that depends (in a differentiable way) on one or more parameters.
    We’ll also have a *loss* function that measures how well the model fits our data.
    (Smaller is better.)
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在这本书中，我们将使用梯度下降来拟合数据的参数化模型。通常情况下，我们会有一些数据集和一些（假设的）依赖于一个或多个参数的数据模型（以可微分的方式）。我们还会有一个*损失*函数，用于衡量模型拟合数据的好坏程度（越小越好）。
- en: 'If we think of our data as being fixed, then our loss function tells us how
    good or bad any particular model parameters are. This means we can use gradient
    descent to find the model parameters that make the loss as small as possible.
    Let’s look at a simple example:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将数据视为固定的，那么我们的损失函数告诉我们任何特定模型参数的好坏程度。这意味着我们可以使用梯度下降找到使损失尽可能小的模型参数。让我们看一个简单的例子：
- en: '[PRE9]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: In this case we *know* the parameters of the linear relationship between `x`
    and `y`, but imagine we’d like to learn them from the data. We’ll use gradient
    descent to find the slope and intercept that minimize the average squared error.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们*知道*线性关系的参数`x`和`y`，但想象一下我们希望从数据中学习这些参数。我们将使用梯度下降来找到最小化平均平方误差的斜率和截距。
- en: 'We’ll start off with a function that determines the gradient based on the error
    from a single data point:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将以一个基于单个数据点误差的梯度确定梯度的函数开始：
- en: '[PRE10]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Let’s think about what that gradient means. Imagine for some `x` our prediction
    is too large. In that case the `error` is positive. The second gradient term,
    `2 * error`, is positive, which reflects the fact that small increases in the
    intercept will make the (already too large) prediction even larger, which will
    cause the squared error (for this `x`) to get even bigger.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一下梯度的含义。假设对于某个`x`，我们的预测值过大。在这种情况下，`error`是正的。第二个梯度项，`2 * error`，是正的，这反映了截距的小增加会使（已经过大的）预测值变得更大，进而导致该`x`的平方误差变得更大。
- en: The first gradient term, `2 * error * x`, has the same sign as `x`. Sure enough,
    if `x` is positive, small increases in the slope will again make the prediction
    (and hence the error) larger. If `x` is negative, though, small increases in the
    slope will make the prediction (and hence the error) smaller.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个梯度项`2 * error * x`与`x`的符号相同。确实，如果`x`为正，那么斜率的小增加会再次使预测（因此误差）变大。但是如果`x`为负，斜率的小增加会使预测（因此误差）变小。
- en: Now, that computation was for a single data point. For the whole dataset we’ll
    look at the *mean squared error*. And the gradient of the mean squared error is
    just the mean of the individual gradients.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，这个计算是针对单个数据点的。对于整个数据集，我们将看*均方误差*。均方误差的梯度就是单个梯度的均值。
- en: 'So, here’s what we’re going to do:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们要做的是：
- en: Start with a random value for `theta`.
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从一个随机值`theta`开始。
- en: Compute the mean of the gradients.
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算梯度的均值。
- en: Adjust `theta` in that direction.
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在那个方向上调整`theta`。
- en: Repeat.
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复。
- en: 'After a lot of *epochs* (what we call each pass through the dataset), we should
    learn something like the correct parameters:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 经过许多*epochs*（我们称每次通过数据集的迭代），我们应该学到一些正确的参数：
- en: '[PRE11]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Minibatch and Stochastic Gradient Descent
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 小批量和随机梯度下降
- en: One drawback of the preceding approach is that we had to evaluate the gradients
    on the entire dataset before we could take a gradient step and update our parameters.
    In this case it was fine, because our dataset was only 100 pairs and the gradient
    computation was cheap.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 前述方法的一个缺点是，我们必须在可以采取梯度步骤并更新参数之前对整个数据集进行梯度评估。在这种情况下，这是可以接受的，因为我们的数据集只有100对，梯度计算是廉价的。
- en: Your models, however, will frequently have large datasets and expensive gradient
    computations. In that case you’ll want to take gradient steps more often.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，您的模型通常会有大型数据集和昂贵的梯度计算。在这种情况下，您会希望更频繁地进行梯度步骤。
- en: 'We can do this using a technique called *minibatch gradient descent*, in which
    we compute the gradient (and take a gradient step) based on a “minibatch” sampled
    from the larger dataset:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用一种称为*小批量梯度下降*的技术来实现这一点，通过从更大的数据集中抽取“小批量”来计算梯度（并执行梯度步骤）：
- en: '[PRE12]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Note
  id: totrans-73
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The `TypeVar(T)` allows us to create a “generic” function. It says that our
    `dataset` can be a list of any single type—`str`s, `int`s, `list`s, whatever—but
    whatever that type is, the outputs will be batches of it.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '`TypeVar(T)`允许我们创建一个“泛型”函数。它表示我们的`dataset`可以是任何一种类型的列表——`str`、`int`、`list`等等——但无论是哪种类型，输出都将是它们的批次。'
- en: 'Now we can solve our problem again using minibatches:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以再次使用小批量来解决我们的问题：
- en: '[PRE13]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Another variation is *stochastic gradient descent*, in which you take gradient
    steps based on one training example at a time:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种变体是*随机梯度下降*，在这种方法中，您基于一个训练样本一次进行梯度步骤：
- en: '[PRE14]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: On this problem, stochastic gradient descent finds the optimal parameters in
    a much smaller number of epochs. But there are always tradeoffs. Basing gradient
    steps on small minibatches (or on single data points) allows you to take more
    of them, but the gradient for a single point might lie in a very different direction
    from the gradient for the dataset as a whole.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个问题上，随机梯度下降在更少的迭代次数内找到了最优参数。但总是存在权衡。基于小型小批量（或单个数据点）进行梯度步骤可以让您执行更多次梯度步骤，但单个点的梯度可能与整个数据集的梯度方向差异很大。
- en: In addition, if we weren’t doing our linear algebra from scratch, there would
    be performance gains from “vectorizing” our computations across batches rather
    than computing the gradient one point at a time.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，如果我们不是从头开始进行线性代数运算，通过“向量化”我们在批次中的计算而不是逐点计算梯度，会有性能提升。
- en: Throughout the book, we’ll play around to find optimal batch sizes and step
    sizes.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在整本书中，我们将尝试找到最佳的批量大小和步长。
- en: Note
  id: totrans-82
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The terminology for the various flavors of gradient descent is not uniform.
    The “compute the gradient for the whole dataset” approach is often called *batch
    gradient descent*, and some people say *stochastic gradient descent* when referring
    to the minibatch version (of which the one-point-at-a-time version is a special
    case).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 关于各种梯度下降方法的术语并不统一。通常称为*批量梯度下降*的方法是“对整个数据集计算梯度”，有些人在提到小批量版本时称为*随机梯度下降*（其中逐点版本是一种特例）。
- en: For Further Exploration
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步探索
- en: Keep reading! We’ll be using gradient descent to solve problems throughout the
    rest of the book.
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 继续阅读！我们将在本书的其余部分使用梯度下降来解决问题。
- en: At this point, you’re undoubtedly sick of me recommending that you read textbooks.
    If it’s any consolation, [*Active Calculus 1.0*](https://scholarworks.gvsu.edu/books/10/),
    by Matthew Boelkins, David Austin, and Steven Schlicker (Grand Valley State University
    Libraries), seems nicer than the calculus textbooks I learned from.
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此时，您无疑对我建议您阅读教科书感到厌倦。如果能稍微安慰一下的话，[*Active Calculus 1.0*](https://scholarworks.gvsu.edu/books/10/)，由Matthew
    Boelkins、David Austin和Steven Schlicker（Grand Valley State University Libraries）编写的书，似乎比我学习的微积分教科书更好。
- en: Sebastian Ruder has an [epic blog post](http://ruder.io/optimizing-gradient-descent/index.html)
    comparing gradient descent and its many variants.
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sebastian Ruder 在他的 [epic 博客文章](http://ruder.io/optimizing-gradient-descent/index.html)
    中比较了梯度下降及其许多变体。
