["```py\n# a basic example of downloading data from the web with Python,\n# using the requests library\n#\n# the source data we are downloading will come from the following URLs:\n# http://feeds.bbci.co.uk/news/science_and_environment/rss.xml\n# https://gbfs.citibikenyc.com/gbfs/en/station_status.json\n\n# the `requests` library lets us write Python code that acts like\n# a web browser\nimport requests\n\n# our chosen XML filename\nXMLfilename = \"BBC_RSS.xml\"\n\n# open a new, writable file for our XML output\nxml_output_file = open(XMLfilename,\"w\")\n\n# use the requests library's \"get\" recipe to access the contents of our\n# target URL and store it in our `xml_data` variable\nxml_data=requests.get('http://feeds.bbci.co.uk/news/science_and_environment/rss.xml')\n\n# the requests library's `get()` function puts contents of the web page\n# in a property `text`\n# we'll `write` that directly to our `xml_output_file`\nxml_output_file.write(xml_data.text)\n\n# close our xml_output_file\nxml_output_file.close()\n\n# our chosen JSON filename\nJSONfilename = \"citibikenyc_station_status.json\"\n\n# open a new, writable file for our JSON output\njson_output_file = open(JSONfilename,\"w\")\n\n# use the `requests` library's `get()` recipe to access the contents of our\n# target URL and store it in our `json_data` variable\njson_data = requests.get('https://gbfs.citibikenyc.com/gbfs/en/station_status.json')\n\n# `get()` the contents of the web page and write its `text`\n# directly to `json_output_file`\njson_output_file.write(json_data.text)\n\n# close our json_output_file\njson_output_file.close()\n```", "```py\nq=weather+sebastopol\n```", "```py\nq=weather+san+francisco\n```", "```py\nhttps://www.google.com/search?q=weather+san+francisco\n```", "```py\nhttps://www.google.com/search?q=weather+san+francisco&as_filetype=xml\n```", "```py\nhttps://api.stlouisfed.org/fred/series/observations?series_id=U6RATE&file_type=json\n```", "```py\n{\"error_code\":400,\"error_message\":\"Bad Request.  Variable api_key is not set.\nRead https:\\/\\/research.stlouisfed.org\\/docs\\/api\\/api_key.html for more\ninformation.\"}\n```", "```py\nhttps://api.stlouisfed.org/fred/series/observations?series_id=U6RATE&file_type=json&\napi_key=*YOUR_API_KEY_HERE*\n\n```", "```py\n{\"realtime_start\":\"2021-02-03\",\"realtime_end\":\"2021-02-03\",\"observation_start\":\n\"1600-01-01\",\"observation_end\":\"9999-12-31\",\"units\":\"lin\",\"output_type\":1,\n\"file_type\":\"json\",\"order_by\":\"observation_date\",\"sort_order\":\"asc\",\"count\":324,\n\"offset\":0,\"limit\":100000,\"observations\":[{\"realtime_start\":\"2021-02-03\",\n\"realtime_end\":\"2021-02-03\",\"date\":\"1994-01-01\",\"value\":\"11.7\"},\n...\n{\"realtime_start\":\"2021-02-03\",\"realtime_end\":\"2021-02-03\",\"date\":\"2020-12-01\",\n\"value\":\"11.7\"}]}\n```", "```py\nmy_api_key = \"*`your_api_key_surrounded_by_double_quotes`*\"\n```", "```py\n# import the requests library\nimport requests\n\n# import our API key\nfrom FRED_credentials import my_api_key ![1](assets/1.png)\n\n# specify the FRED endpoint we want to use\nFRED_endpoint = \"https://api.stlouisfed.org/fred/series/observations?\"\n\n# also specify the query parameters and their values\nFRED_parameters = \"series_id=U6RATE&file_type=json\"\n\n# construct the complete URL for our API request, adding our API key to the end\ncomplete_data_URL = FRED_endpoint + FRED_parameters +\"&api_key=\"+my_api_key\n\n# open a new, writable file with our chosen filename\nFRED_output_file = open(\"FRED_API_data.json\",\"w\")\n\n# use the `requests` library's `get()` recipe to access the contents of our\n# target URL and store it in our `FRED_data` variable\nFRED_data = requests.get(complete_data_URL)\n\n# `get()` the contents of the web page and write its `text`\n# directly to `FRED_output_file`\nFRED_output_file.write(FRED_data.text)\n\n# close our FRED_output_file\nFRED_output_file.close()\n```", "```py\n# ignoring all credentials files\n**credentials*\n```", "```py\ngit status\n```", "```py\ngit status --ignored\n```", "```py\nmy_Twitter_key = \"*`your_api_key_surrounded_by_double_quotes`*\"\nmy_Twitter_secret = \"*`your_api_key_secret_surrounded_by_double_quotes`*\"\n```", "```py\ngit status --ignored\n```", "```py\nmy_Twitter_key = \"your_api_key_surrounded_by_double_quotes\"\nmy_Twitter_secret = \"your_api_key_secret_surrounded_by_double_quotes\"\n\n# import the base64 encoding library\nimport base64 ![1](assets/1.png)\n\n# first, combine the API Key and API Key Secret into a single string\n# adding a colon between them\ncombined_key_string = my_Twitter_key+':'+my_Twitter_secret\n\n# use the `encode()` method on that combined string,\n# specifying the ASCII format (see: https://en.wikipedia.org/wiki/ASCII)\nencoded_combined_key = combined_key_string.encode('ascii')\n\n# encode the ASCII-formatted string to base64\nb64_encoded_combined_key = base64.b64encode(encoded_combined_key)\n\n# _decode_ the encoded string back to ASCII,\n# so that it's ready to send over the internet\nauth_ready_key = b64_encoded_combined_key.decode('ascii')\n```", "```py\n# import the encoded key from our credentials file\nfrom Twitter_credentials import auth_ready_key\n\n# include the requests library in order to get data from the web\nimport requests\n\n# specify the Twitter endpoint that we'll use to retrieve\n# our access token or \"bearer\" token\nauth_url = 'https://api.twitter.com/oauth2/token'\n\n# add our `auth_ready_key` to a template `dict` object provided\n# in the Twitter API documentation\nauth_headers = {\n    'Authorization': 'Basic '+auth_ready_key,\n    'Content-Type': 'application/x-www-form-urlencoded;charset=UTF-8'\n} ![1](assets/1.png)\n\n# another `dict` describes what we're asking for\nauth_data = {\n    'grant_type': 'client_credentials'\n} ![2](assets/2.png)\n\n# make our complete request to the authorization endpoint, and store\n# the results in the `auth_resp` variable\nauth_resp = requests.post(auth_url, headers=auth_headers, data=auth_data)\n\n# pull the access token out of the json-formatted data\n# that the authorization endpoint sent back to us\naccess_token = auth_resp.json()['access_token']\n```", "```py\n# now that we have an access/bearer token, we're ready to request some data!\n\n# we'll create a new dict that includes this token\nsearch_headers = {\n    'Authorization': 'Bearer ' + access_token\n}\n\n# this is the Twitter search API endpoint for version 1.1 of the API\nsearch_url  = 'https://api.twitter.com/1.1/search/tweets.json'\n\n# create a new dict that includes our search query parameters\nsearch_params = {\n    'q': 'Python',\n    'result_type': 'recent',\n    'count': 4\n} ![1](assets/1.png)\n\n# send our data request and store the results in `search_resp`\nsearch_resp = requests.get(search_url, headers=search_headers, params=search_params)\n\n# parse the response into a JSON object\nTwitter_data = search_resp.json()\n\n# open an output file where we can save the results\nTwitter_output_file = open(\"Twitter_search_results.json\", \"w\")\n\n# write the returned Twitter data to our output file\nTwitter_output_file.write(str(Twitter_data)) ![2](assets/2.png)\n\n# close the output file\nTwitter_output_file.close()\n\n# loop through our results and print the text of the Twitter status\nfor a_Tweet in Twitter_data['statuses']: ![3](assets/3.png)\n    print(a_Tweet['text'] + '\\n')\n```", "```py\n# include the requests library in order to get data from the web\nimport requests\n\n# specify the URL of the web page we're downloading\n# this one contains a linked list of all the NYC MTA turnstile data files\n# going back to 2010\nmta_turnstiles_index_url = \"http://web.mta.info/developers/turnstile.html\"\n\n# create some header information for our web page request\nheaders = {\n    'User-Agent': 'Mozilla/5.0 (X11; CrOS x86_64 13597.66.0) ' + \\\n                  'AppleWebKit/537.36 (KHTML, like Gecko) ' + \\\n                  'Chrome/88.0.4324.109 Safari/537.36',\n    'From': 'YOUR NAME HERE - youremailaddress@emailprovider.som'\n} ![1](assets/1.png)\n\n# send a `get()` request for the URL, along with our informational headers\nmta_web_page = requests.get(mta_turnstiles_index_url, headers=headers)\n\n# open up a writable local file where we can save the contents of the web page\nmta_turnstiles_output_file = open(\"MTA_turnstiles_index.html\",\"w\")\n\n# write the `text` web page to our output file\nmta_turnstiles_output_file.write(mta_web_page.text)\n\n# close our output file!\nmta_turnstiles_output_file.close()\n```", "```py\n<div class=\"span-84 last\">\n```", "```py\npip install beautifulsoup4\n```", "```py\n# import the Beautiful Soup recipe from the bs4 library\nfrom bs4 import BeautifulSoup\n\n# open the saved copy of our MTA turnstiles web page\n# (original here: http://web.mta.info/developers/turnstile.html)\nmta_web_page = open(\"MTA_turnstiles_index.html\", \"r\")\n\n# define the base URL for the data files\nbase_url = \"http://web.mta.info/developers/\" ![1](assets/1.png)\n\n# the `BeautifulSoup` recipe takes the contents of our web page and another\n# \"ingredient\", which tells it what kind of code it is working with\n# in this case, it's HTML\nsoup = BeautifulSoup(mta_web_page, \"html.parser\")\n\n# using the \"find\" recipe, we can pass a tag type and class name as\n# \"ingredients\" to zero in on the content we want.\ndata_files_section = soup.find(\"div\", class_=\"span-84 last\") ![2](assets/2.png)\n\n# within that div, we can now just look for all the \"anchor\" (`a`) tags\nall_data_links = data_files_section.find_all(\"a\")\n\n# need to open a file to write our extracted links to\nmta_data_list = open(\"MTA_data_index.csv\",\"w\")\n\n# the `find_all()` recipe returns a list of everything it matches\nfor a_link in all_data_links:\n\n    # combine our base URL with the contents of each \"href\" (link) property,\n    # and store it in `complete_link`\n    complete_link = base_url+a_link[\"href\"]\n\n    # write this completed link to our output file, manually adding a\n    # newline `\\n` character to the end, so each link will be on its own row\n    mta_data_list.write(complete_link+\"\\n\")\n\n# once we've written all the links to our file, close it!\nmta_data_list.close()\n```", "```py\n# include the requests library in order to get data from the web\nimport requests\n\n# import the `os` Python library so we can create a new folder\n# in which to store our downloaded data files\nimport os\n\n# import the `time` library\nimport time ![1](assets/1.png)\n\n# open the file where we stored our list of links\nmta_data_links = open(\"MTA_data_index.csv\",\"r\")\n\n# create a folder name so that we can keep the data organized\nfolder_name = \"turnstile_data\"\n\n# add our header information\nheaders = {\n    'User-Agent': 'Mozilla/5.0 (X11; CrOS x86_64 13597.66.0) ' + \\\n                  'AppleWebKit/537.36 (KHTML, like Gecko) ' + \\\n                  'Chrome/88.0.4324.109 Safari/537.36',\n    'From': 'YOUR NAME HERE - youremailaddress@emailprovider.som'\n}\n\n# the built-in `readlines()` function converts our data file to a\n# list, where each line is an item\nmta_links_list = mta_data_links.readlines()\n\n# confirm there isn't already a folder with our chosen name\nif os.path.isdir(folder_name) == False:\n\n    # create a new folder with that name\n    target_folder = os.mkdir(folder_name)\n\n# only download the precise number of files we need\nfor i in range(0,4):\n\n    # use the built-in `strip()` method to remove the newline (`\\n`)\n    # character at the end of each row/link\n    data_url = (mta_links_list[i]).strip()\n\n    # create a unique output filename based on the url\n    data_filename = data_url.split(\"/\")[-1] ![2](assets/2.png)\n\n    # make our request for the data\n    turnstile_data_file = requests.get(data_url, headers=headers)\n\n    # open a new, writable file inside our target folder\n    # using the appropriate filename\n    local_data_file = open(os.path.join(folder_name,data_filename), \"w\")\n\n    # save the contents of the downloaded file to that new file\n    local_data_file.write(turnstile_data_file.text)\n\n    # close the local file\n    local_data_file.close()\n\n    # `sleep()` for two seconds before moving on to the next item in the loop\n    time.sleep(2)\n```"]