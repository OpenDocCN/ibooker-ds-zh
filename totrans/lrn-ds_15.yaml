- en: 'Chapter 12\. Case Study: How Accurate Are Air Quality Measurements?'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: California is prone to wildfires, so much so that its residents (like the authors
    of this book) sometimes say that California is “always on fire.” In 2020, 40 separate
    fires covered the state in smoke, forced thousands of people to evacuate, and
    caused more than $12 billion in damages ([Figure 12-1](#fig-ca-fires)).
  prefs: []
  type: TYPE_NORMAL
- en: '![ca-fires](assets/leds_1201.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12-1\. Satellite image from August 2020 showing smoke covering California
    (image from [Wikipedia](https://oreil.ly/CrDld) licensed under CC BY-SA 3.0 IGO)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In places like California, people use air quality measurements to learn what
    kinds of protective measures they need to take. Depending on conditions, people
    may wish to wear a mask, use air filters, or avoid going outside altogether.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the US, one important source of air quality information is the [Air Quality
    System](https://www.epa.gov/aqs) (AQS), run by the US government. AQS places high-quality
    sensors at locations across the US and makes their data available to the public.
    These sensors are carefully calibrated to strict standards—in fact, the AQS sensors
    are generally seen as the gold standard for accuracy. However, they have a few
    downsides. The sensors are expensive: typically between $15,000 and $40,000 each.
    This means that there are fewer sensors, and they are farther apart. Someone living
    far away from a sensor might not be able to access AQS data for their personal
    use. Also, AQS sensors do not provide real-time data. Since the data undergo extensive
    calibration, they are only released hourly and have a time lag of one to two hours.
    In essence, the AQS sensors are accurate but not timely.'
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, [PurpleAir](https://www2.purpleair.com) sensors, which we introduced
    in [Chapter 3](ch03.html#ch-theory-datadesign), sell for about $250 and can be
    easily installed at home. With the lower price point, thousands of people across
    the US have purchased these sensors for personal use. The sensors can connect
    to a home WiFi network so that air quality can be easily monitored, and they can
    report data back to PurpleAir. In 2020, thousands of owners of PurpleAir sensors
    made their sensors’ measurements publicly available. Compared to the AQS sensors,
    PurpleAir sensors are timelier. They report measurements every two minutes rather
    than every hour. Since there are more deployed PurpleAir sensors, more people
    live close enough to a sensor to make use of the data. However, PurpleAir sensors
    are less accurate. To make the sensors affordable, PurpleAir uses a simpler method
    to count particles in the air. This means that PurpleAir measurements can report
    that air quality is worse than it really is (see [Josh Hug’s blog post](https://oreil.ly/ZH5aj)).
    In essence, PurpleAir sensors tend to be timely but less accurate.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we plan to use the AQS sensor measurements to improve the PurpleAir
    measurements. It’s a big task, and we follow the analysis first developed by [Karoline
    Barkjohn, Brett Gantt, and Andrea Clements](https://oreil.ly/XPxZu) from the US
    Environmental Protection Agency. Barkjohn and her colleagues’ work was so successful
    that, as of this writing, the official US government maps, like the AirNow [Fire
    and Smoke](https://fire.airnow.gov) map, include both AQS and PurpleAir sensors
    and apply Barkjohn’s correction to the PurpleAir data.
  prefs: []
  type: TYPE_NORMAL
- en: Our work follows the data science lifecycle, beginning with considering the
    question and the scope of the available data. Much of our effort is spent cleaning
    and wrangling the data into shape for analysis, but we also carry out an exploratory
    data analysis and build a model for generalization. We begin by considering the
    question and the design and scope of the data.
  prefs: []
  type: TYPE_NORMAL
- en: Question, Design, and Scope
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Ideally, measures of air quality should be both *accurate* and *timely*. Inaccurate
    or biased measurements can mean people do not take air quality as seriously as
    they should. Delayed alerts can expose people to harmful air. The context provided
    in the introduction about the popularity of inexpensive air quality sensors got
    us wondering about their quality and usefulness.
  prefs: []
  type: TYPE_NORMAL
- en: 'Two different kinds of instruments measure a natural phenomenon—the amount
    of particulate matter in the air. The AQS sensor has the advantage of small measurement
    error and negligible bias (see [Chapter 2](ch02.html#ch-data-scope)). On the other
    hand, the PurpleAir instrument is less accurate; the measurements have greater
    variability and are also biased. Our initial question is: can we use the AQS measurements
    to make the PurpleAir measurements better?'
  prefs: []
  type: TYPE_NORMAL
- en: We are in the situation where we have a lot of data available to us. We have
    access to a small number of high-quality measurements from AQS, and we can get
    data from thousands of PurpleAir sensors. To narrow the focus of our question,
    we consider how we might use these two sources of data to improve PurpleAir measurements.
  prefs: []
  type: TYPE_NORMAL
- en: The data from these two sources includes the locations of the sensors. So we
    can try to pair them up, finding a PurpleAir sensor close to each AQS sensor.
    If they’re close, then these sensors are essentially measuring the same air. We
    can treat the AQS sensors as the ground truth (because they are so accurate) and
    study the variation in the PurpleAir measurements given the true air quality.
  prefs: []
  type: TYPE_NORMAL
- en: Even though there are relatively few pairs of collocated AQS and PurpleAir sensors,
    it seems reasonable to generalize any relationship we find to other PurpleAir
    sensors. If there’s a simple relationship between AQS and PurpleAir measurements,
    then we can use this relationship to adjust measurements from any PurpleAir sensor
    so that they are more accurate.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have narrowed down our question quite a bit: can we model the relationship
    between PurpleAir sensor readings and neighboring AQS sensor readings? If yes,
    then hopefully we can use the model to improve PurpleAir readings. Spoiler alert:
    indeed we can!'
  prefs: []
  type: TYPE_NORMAL
- en: This case study nicely integrates the concepts introduced in this part of the
    book. It gives us an opportunity to see how data scientists wrangle, explore,
    and visualize data in a real-world setting. In particular, we see how a large,
    less-accurate dataset can amplify the usefulness of a small, accurate dataset.
    Combining large and small datasets like this is particularly exciting to data
    scientists and applies broadly to other domains ranging from social science to
    medicine.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we begin our wrangling by finding the pairs of AQS and
    PurpleAir sensors that are near each other. We focus specifically on readings
    for PM2.5 particles, which are particles that are smaller than 2.5 micrometers
    in diameter. These particles are small enough to be inhaled into the lungs, pose
    the greatest risk to health, and are especially common in wood smoke.
  prefs: []
  type: TYPE_NORMAL
- en: Finding Collocated Sensors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our analysis begins by finding collocated pairs of AQS and PurpleAir sensors—sensors
    that are placed essentially next to each other. This step is important because
    it lets us reduce the effects of other variables that might cause differences
    in sensor readings. Consider what would happen if we compared an AQS sensor placed
    in a park with a PurpleAir sensor placed along a busy freeway. The two sensors
    would have different readings, in part because the sensors are exposed to different
    environments. Ensuring that sensors are truly collocated lets us claim the differences
    in sensor readings are due to how the sensors are built and to small, localized
    air fluctuations, rather than other potential confounding variables.
  prefs: []
  type: TYPE_NORMAL
- en: Barkjohn’s analysis conducted by the EPA group found pairs of AQS and PurpleAir
    sensors that are installed within 50 meters of each other. The group contacted
    each AQS site to see whether the PurpleAir sensor was also maintained there. This
    extra effort gave them confidence that their sensor pairs were truly collocated.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we explore and clean location data from AQS and PurpleAir.
    Then we perform a join of sorts to construct a list of potentially collocated
    sensors. We won’t contact AQS sites ourselves; instead, we proceed in later sections
    with Barkjohn’s list of confirmed collocated sensors.
  prefs: []
  type: TYPE_NORMAL
- en: 'We downloaded a list of AQS and PurpleAir sensors and saved the data in the
    files *data/list_of_aqs_sites.csv* and *data/list_of_purpleair_sensors.json*.
    Let’s begin by reading these files into `pandas DataFrames`. First, we check file
    sizes to see whether they are reasonable to load into memory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Both files are relatively small. Let’s start with the list of AQS sites.
  prefs: []
  type: TYPE_NORMAL
- en: Wrangling the List of AQS Sites
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have filtered the [AQS map of sites](https://oreil.ly/EkZcB) to show only
    the AQS sites that measure PM2.5, and then downloaded the list of sites as a CSV
    file using the map’s web app. Now we can load it into a `pandas DataFrame`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'There are 28 columns in the table. Let’s check the column names:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'To find out which columns are most useful for us, we reference the [data dictionary](https://oreil.ly/GvMPI)
    that the AQS provides on its website. There we confirm that the data table contains
    information about the AQS sites. So we might expect the granularity corresponds
    to an AQS site, meaning each row represents a single site and the column labeled
    `AQS_Site_ID` is the primary key. We can confirm this with a count of records
    for each ID:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'It looks like some sites appear multiple times in this dataframe. Unfortunately,
    this means that the granularity is finer than the individual site level. To figure
    out why sites are duplicated, let’s take a closer look at the rows for one duplicated
    site:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We select a few columns to examine based on their names—those that sound like
    they might shed some light on the reason for duplicates:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '|   | POC | Monitor_Start_Date | Last_Sample_Date | Sample_Collection_Method
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **458** | 1 | 1/27/1999 | 8/31/2021 | R & P Model 2025 PM-2.5 Sequential
    Air Sampler... |'
  prefs: []
  type: TYPE_TB
- en: '| **459** | 2 | 2/9/2013 | 8/26/2021 | R & P Model 2025 PM-2.5 Sequential Air
    Sampler... |'
  prefs: []
  type: TYPE_TB
- en: '| **460** | 3 | 1/1/2019 | 9/30/2021 | Teledyne T640 at 5.0 LPM |'
  prefs: []
  type: TYPE_TB
- en: '| **461** | 4 | 1/1/2019 | 9/30/2021 | Teledyne T640 at 5.0 LPM |'
  prefs: []
  type: TYPE_TB
- en: 'The `POC` column looks to be useful for distinguishing between rows in the
    table. The data dictionary states this about the column:'
  prefs: []
  type: TYPE_NORMAL
- en: This is the “Parameter Occurrence Code” used to distinguish different instruments
    that measure the same parameter at the same site.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: So, the site `19-163-0015` has four instruments that all measure PM2.5\. The
    granularity of the dataframe is at the level of a single instrument.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since our aim is to match AQS and PurpleAir sensors, we can adjust the granularity
    by selecting one instrument from each AQS site. To do this, we group rows according
    to site ID, then take the first row in each group:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Now the number of rows matches the number of unique IDs.
  prefs: []
  type: TYPE_NORMAL
- en: 'To match AQS sites with PurpleAir sensors, we only need the site ID, latitude,
    and longitude. So we further adjust the structure and keep only those columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Now the `aqs_sites` dataframe is ready, and we move to the PurpleAir sites.
  prefs: []
  type: TYPE_NORMAL
- en: Wrangling the List of PurpleAir Sites
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Unlike the AQS sites, the file containing PurpleAir sensor data comes in a
    JSON format. We address this format in more detail in [Chapter 14](ch14.html#ch-web).
    For now, we use shell tools (see [Chapter 8](ch08.html#ch-files)) to peek at the
    file contents:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'From the first few lines of the file, we can guess that the data are stored
    in the `"data"` key and the column labels in the `"fields"` key. We can use Python’s
    `json` library to read in the file as a Python `dict`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'We can create a dataframe from the values in `data` and label the columns with
    the content of `fields`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '|   | ID | pm | pm_cf_1 | pm_atm | ... | Voc | Ozone1 | Adc | CH |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **0** | 20 | 0.0 | 0.0 | 0.0 | ... | NaN | NaN | 0.01 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| **1** | 47 | NaN | NaN | NaN | ... | NaN | 0.72 | 0.72 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| **2** | 53 | 0.0 | 0.0 | 0.0 | ... | NaN | NaN | 0.00 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| **3** | 74 | 0.0 | 0.0 | 0.0 | ... | NaN | NaN | 0.05 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| **4** | 77 | 9.8 | 9.8 | 9.8 | ... | NaN | NaN | 0.01 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Like the AQS data, there are many more columns in this dataframe than we need:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: In this case, we can guess that the columns we’re most interested in are the
    sensor IDs (`ID`), sensor labels (`Label`), latitude (`Lat`), and longitude (`Lon`).
    But we did consult the data dictionary on the PurpleAir website to double-check.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let’s check the `ID` column for duplicates, as we did for the AQS data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Since the `value_counts()` method lists the counts in descending order, we
    can see that every ID was included only once. So we have verified the granularity
    is at the individual sensor level. Next, we keep only the columns needed to match
    sensor locations from the two sources:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Notice there are tens of thousands more PurpleAir sensors than AQS sensors.
    Our next task is to find the PurpleAir sensor close to each AQS sensor.
  prefs: []
  type: TYPE_NORMAL
- en: Matching AQS and PurpleAir Sensors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Our goal is to match sensors in the two dataframes by finding a PurpleAir sensor
    near each AQS instrument. We consider near to mean within 50 meters. This kind
    of matching is a bit more challenging than the joins we’ve seen thus far. For
    instance, the naive approach to use the `merge` method of `pandas` fails us:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '|   | site_id | lat | lon | id | label |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **0** | 06-111-1004 | 34.45 | -119.23 | 48393 | VCAPCD OJ |'
  prefs: []
  type: TYPE_TB
- en: We cannot simply match instruments with the exact same latitude and longitude;
    we need to find the PurpleAir sites that are close enough to the AQS instrument.
  prefs: []
  type: TYPE_NORMAL
- en: 'To figure out how far apart two locations are, we use a basic approximation:
    `111,111` meters in the north-south direction roughly equals one degree of latitude,
    and `111,111 * cos(latitude)` in the east-west direction corresponds to one degree
    of longitude.^([1](ch12.html#id1449)) So we can find the latitude and longitude
    ranges that correspond to 25 meters in each direction (to make a 50-meter-by-50-meter
    rectangle around each point):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'To simplify even more, we use the median latitude for the AQS sites:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can match coordinates to within the `offset_in_lat` and `offset_in_lon`.
    Doing this in SQL is much easier than in `pandas`, so we push the tables into
    a temporary SQLite database, then run a query to read the tables back into a dataframe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '|   | aqs_id | pa_id | pa_label | aqs_lat | aqs_lon | pa_lat | pa_lon |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **0** | 06-019-0011 | 6568 | IMPROVE_FRES2 | 36.79 | -119.77 | 36.79 | -119.77
    |'
  prefs: []
  type: TYPE_TB
- en: '| **1** | 06-019-0011 | 13485 | AMTS_Fresno | 36.79 | -119.77 | 36.79 | -119.77
    |'
  prefs: []
  type: TYPE_TB
- en: '| **2** | 06-019-0011 | 44427 | Fresno CARB CCAC | 36.79 | -119.77 | 36.79
    | -119.77 |'
  prefs: []
  type: TYPE_TB
- en: '| **...** | ... | ... | ... | ... | ... | ... | ... |'
  prefs: []
  type: TYPE_TB
- en: '| **146** | 53-061-1007 | 3659 | Marysville 7th | 48.05 | -122.17 | 48.05 |
    -122.17 |'
  prefs: []
  type: TYPE_TB
- en: '| **147** | 53-063-0021 | 54603 | Augusta 1 SRCAA | 47.67 | -117.36 | 47.67
    | -117.36 |'
  prefs: []
  type: TYPE_TB
- en: '| **148** | 56-021-0100 | 50045 | WDEQ-AQD Cheyenne NCore | 41.18 | -104.78
    | 41.18 | -104.78 |'
  prefs: []
  type: TYPE_TB
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: We’ve achieved our goal—we matched 149 AQS sites with PurpleAir sensors. Our
    wrangling of the locations is complete, and we turn to the task of wrangling and
    cleaning the sensor measurements. We start with the measurements taken from an
    AQS site.
  prefs: []
  type: TYPE_NORMAL
- en: Wrangling and Cleaning AQS Sensor Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have located sensors that are near each other, we are ready to wrangle
    and clean the files that contain the measurement data for these sites. We demonstrate
    the tasks involved with one AQS instrument and its matching PurpleAir sensor.
    We picked a pair located in Sacramento, California. The AQS sensor ID is `06-067-0010`,
    and the PurpleAir sensor name is `AMTS_TESTINGA`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The AQS provides a website and [API](https://oreil.ly/tl_nc) to download sensor
    data. We downloaded the daily measurements from May 20, 2018, to December 29,
    2019, into the *data/aqs_06-067-0010.csv* file. Let’s begin by loading this file
    into a dataframe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: From the [data dictionary](https://oreil.ly/e1PjI), we find out that the column
    called `arithmetic_mean` corresponds to the actual PM2.5 measurements. Some AQS
    sensors take a measurement every hour. For our analysis, we downloaded the 24-hour
    averages (the arithmetic mean) of the hourly sensor measurements.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s carry out some quality checks and clean the data where necessary. We
    focus on checks related to scope and quality of values:'
  prefs: []
  type: TYPE_NORMAL
- en: Check and correct the granularity of the data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Remove unneeded columns.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Check values in the `date_local` column.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Check values in the `arithmetic_mean` column.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For the sake of brevity, we’ve chosen a few important quality checks that specifically
    reinforce ideas we’ve covered in data wrangling, EDA, and visualization.
  prefs: []
  type: TYPE_NORMAL
- en: Checking Granularity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We would like each row of our data to correspond to a single date with an average
    PM2.5 reading for that date. As we saw earlier, a simple way to check is to see
    whether there are repeat values in the `date_local` column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Indeed, there are 12 rows for each date, so the granularity is *not* at the
    individual date level.
  prefs: []
  type: TYPE_NORMAL
- en: 'From the data dictionary, we learn that there are multiple standards for computing
    the final measurements from the raw sensor data. The `pollutant_standard` column
    contains the name of each standard. The `event_type` column marks whether data
    measured during “exceptional events” are included in the measurement. Let’s check
    how different these average values are by calculating the range of 12 measurements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'For all 189 dates, the max PM2.5–min PM2.5 is 0\. This means that we can simply
    take the first PM2.5 measurement for each date:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'This data-cleaning step gives us the desired granularity: each row represents
    a single date, with an average PM2.5 measurement for that date. Next, we further
    modify the structure of the dataframe and drop unneeded columns.'
  prefs: []
  type: TYPE_NORMAL
- en: Removing Unneeded Columns
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We plan to match the PM2.5 measurements in the AQS dataframe with the PurpleAir
    PM2.5 measurements for each date. To simplify the structure, we can drop all but
    the date and PM2.5 columns. We also rename the PM2.5 column so that it’s easier
    to understand:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: '|   | date_local | pm25 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **0** | 2018-05-20 | 6.5 |'
  prefs: []
  type: TYPE_TB
- en: '| **1** | 2018-05-23 | 2.3 |'
  prefs: []
  type: TYPE_TB
- en: '| **2** | 2018-05-29 | 11.8 |'
  prefs: []
  type: TYPE_TB
- en: '| **3** | 2018-06-01 | 6.0 |'
  prefs: []
  type: TYPE_TB
- en: '| **4** | 2018-06-04 | 8.0 |'
  prefs: []
  type: TYPE_TB
- en: Now that we have the desired shape for our data table, we turn to checking the
    data values.
  prefs: []
  type: TYPE_NORMAL
- en: Checking the Validity of Dates
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s take a closer look at the dates. We have already seen that there are
    gaps when there are no PM2.5 readings, so we expect there are missing dates. Let’s
    parse the dates as timestamp objects to make it easier to figure out which dates
    are missing. As we did in [Chapter 9](ch09.html#ch-wrangling), we check the format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'The dates are represented as YYYY-MM-DD, so we describe the format in the Python
    representation `''%Y-%m-%d''`. To parse the dates, we use the `pd.to_datetime()`
    function, and we reassign the `date_local` column as `pd.TimeStamp`s:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: The method runs without erroring, indicating that all the strings matched the
    format.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Just because the dates can be parsed doesn’t mean that the dates are immediately
    ready to use for further analysis. For instance, the string `9999-01-31` can be
    parsed into a `pd.TimeStamp`, but the date isn’t valid.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that the dates have been converted to timestamps, we can calculate how
    many dates are missing. We find the number of days between the earliest and latest
    dates—this corresponds to the maximum number of measurements we could have recorded:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: Subtracting timestamps gives `Timedelta` objects, which as we see have a few
    useful properties. There are many dates missing from the data. However, when we
    combine these data for this sensor with other sensors, we expect to have enough
    data to fit a model.
  prefs: []
  type: TYPE_NORMAL
- en: Our final wrangling step is to check the quality of the PM2.5 measurements.
  prefs: []
  type: TYPE_NORMAL
- en: Checking the Quality of PM2.5 Measurements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Particulate matter is measured in micrograms per cubic meter of air (µg/m³).
    (There are 1 million micrograms in 1 gram, and 1 pound is equal to about 450 grams.)
    The [EPA has set a standard](https://oreil.ly/XqVqG) of 35 µg/m³ for a daily average
    of PM2.5 and 12 µg/m³ for an annual average. We can use this information to make
    a few basic checks on the PM2.5 measurements. First, PM2.5 can’t go below 0\.
    Second, we can look for abnormally high PM2.5 values and see whether they correspond
    to major events like a wildfire.
  prefs: []
  type: TYPE_NORMAL
- en: 'One visual way to perform these checks is to plot the PM2.5 measurement against
    the date:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: '![](assets/leds_12in01.png)'
  prefs: []
  type: TYPE_IMG
- en: We see that the PM2.5 measurements don’t go below 0 and are typically lower
    than the EPA level. We also found a large spike in PM2.5 around mid-November of
    2018\. This sensor is located in Sacramento, so we can check if there was a fire
    around that area.
  prefs: []
  type: TYPE_NORMAL
- en: Indeed, November 8, 2018, marks the start of the Camp Fire, the “deadliest and
    most destructive wildfire in California history” (see the [Camp Fire page](https://oreil.ly/tqxtH)
    managed by the US Census Bureau). The fire started just 80 miles north of Sacramento,
    so this AQS sensor captured the dramatic spike in PM2.5.
  prefs: []
  type: TYPE_NORMAL
- en: We’ve cleaned and explored the data for one AQS sensor. In the next section,
    we do the same for its collocated PurpleAir sensor.
  prefs: []
  type: TYPE_NORMAL
- en: Wrangling PurpleAir Sensor Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous section, we analyzed data from AQS site `06-067-0010`. The
    matching PurpleAir sensor is named `AMTS_TESTINGA`, and we’ve used the PurpleAir
    website to download the data for this sensor into the *data/purpleair_AMTS* folder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'There are four CSV files. Their names are quite long, and the beginning of
    each is identical. The data dictionary for the PurpleAir data says that each sensor
    has two separate instruments, A and B, that each record data. Note that the PurpleAir
    site we used to collect these data and the accompanying data dictionary has been
    downgraded. The data are now available through a REST API. The [site that documents
    the API](https://oreil.ly/WSciR) also contains information about the fields. (The
    topic of REST is covered in [Chapter 14](ch14.html#ch-web).) Let’s examine the
    later portions of the filenames:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: We can see that the first two CSV files correspond to instrument A and the last
    two to B. Having two instruments is useful for data cleaning; if A and B disagree
    about a measurement, we might question the integrity of the measurement and decide
    to remove it.
  prefs: []
  type: TYPE_NORMAL
- en: 'The data dictionary also mentions that each instrument records Primary and
    Secondary data. The Primary data contains the fields we’re interested in: PM2.5,
    temperature, and humidity. The Secondary data contains data for other particle
    sizes, like PM1.0 and PM10\. So we work only with the Primary files.'
  prefs: []
  type: TYPE_NORMAL
- en: Our tasks are similar to those of the previous section, with the addition of
    addressing readings from two instruments.
  prefs: []
  type: TYPE_NORMAL
- en: 'We begin by loading in the data. When CSV files have long names, we can assign
    the filenames into a Python variable to more easily load the files:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s look at the columns to see which ones we need:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'Although we’re interested in PM2.5, it appears there are two columns that contain
    PM2.5 data: `PM2.5_CF1_ug/m3` and `PM2.5_ATM_ug/m3`. We investigate the difference
    between these two columns to find that PurpleAir sensors use two different methods
    to convert a raw laser recording into a PM2.5 number. These two calculations correspond
    to the CF1 and ATM columns. Barkjohn found that using CF1 produced better results
    than ATM, so we keep that column, along with the date, temperature, and relative
    humidity:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: '|   | timestamp | PM25cf1 | TempF | RH |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **0** | 2018-05-20 00:00:35 UTC | 1.23 | 83.0 | 32.0 |'
  prefs: []
  type: TYPE_TB
- en: '| **1** | 2018-05-20 00:01:55 UTC | 1.94 | 83.0 | 32.0 |'
  prefs: []
  type: TYPE_TB
- en: '| **2** | 2018-05-20 00:03:15 UTC | 1.80 | 83.0 | 32.0 |'
  prefs: []
  type: TYPE_TB
- en: '| **3** | 2018-05-20 00:04:35 UTC | 1.64 | 83.0 | 32.0 |'
  prefs: []
  type: TYPE_TB
- en: '| **4** | 2018-05-20 00:05:55 UTC | 1.33 | 83.0 | 32.0 |'
  prefs: []
  type: TYPE_TB
- en: Next we check granularity.
  prefs: []
  type: TYPE_NORMAL
- en: Checking the Granularity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In order for the granularity of these measurements to match the AQS data, we
    want one average PM2.5 for each date (a 24-hour period). PurpleAir states that
    sensors take measurements every two minutes. Let’s double-check the granularity
    of the raw measurements before we aggregate them to 24-hour periods.
  prefs: []
  type: TYPE_NORMAL
- en: 'To do this we convert the column containing the date information from strings
    to `pd.TimeStamp` objects. The format of the date is different than the AQS format,
    which we describe as `''%Y-%m-%d %X %Z''`. As we soon see, `pandas` has special
    support for dataframes with an index of timestamps:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: '|   | PM25cf1 | TempF | RH |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| timestamp |   |   |   |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **2018-05-20 00:00:35+00:00** | 1.23 | 83.0 | 32.0 |'
  prefs: []
  type: TYPE_TB
- en: '| **2018-05-20 00:01:55+00:00** | 1.94 | 83.0 | 32.0 |'
  prefs: []
  type: TYPE_TB
- en: 'Timestamps are tricky—notice that the original timestamps were given in the
    UTC time zone. However, the AQS data were averaged according to the *local time
    in California*, which is either seven or eight hours behind UTC time, depending
    on whether daylight saving time is in effect. This means we need to change the
    time zone of the PurpleAir timestamps to match the local time zone. The `df.tz_convert()`
    method operates on the index of the dataframe, which is one reason why we set
    the index of `pa` to the timestamps:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: '|   | PM25cf1 | TempF | RH |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| timestamp |   |   |   |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **2018-05-19 17:00:35-07:00** | 1.23 | 83.0 | 32.0 |'
  prefs: []
  type: TYPE_TB
- en: '| **2018-05-19 17:01:55-07:00** | 1.94 | 83.0 | 32.0 |'
  prefs: []
  type: TYPE_TB
- en: If we compare the first two rows of this version of the dataframe to the previous
    one, we see that the time has changed to indicate the seven-hour difference from
    UTC.
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing timestamps can help us check the granularity of the data.
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing timestamps
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'One way to visualize timestamps is to count how many appear in each 24-hour
    period, then plot those counts over time. To group time-series data in `pandas`,
    we can use the `df.resample()` method. This method works on dataframes that have
    an index of timestamps. It behaves like `df.groupby()`, except that we can specify
    how we want the timestamps to be grouped—we can group into dates, weeks, months,
    and many more options (the `D` argument tells `resample` to aggregate timestamps
    into individual dates):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'We see that the number of measurements in a day varies widely. A line plot
    of these counts gives us a better sense of these variations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: '![](assets/leds_12in02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This is a fascinating plot. We see clear gaps in the data where there are no
    measurements. It appears that significant portions of data in July 2018 and September
    2019 are missing. Even when the sensor appears to be working, the number of measurements
    per day is slightly different. For instance, the plot is “bumpy” between August
    and October 2018, where dates have a varying number of measurements. We need to
    decide what we want to do with missing data. But perhaps more urgently: there
    are strange “steps” in the plot. Some dates have around 1,000 readings, some around
    2,000, some around 700, and some around 1,400\. If a sensor takes measurements
    every two minutes, there should be a maximum of 720 measurements per day. For
    a perfect sensor, the plot would display a flat line at 720 measurements. This
    is clearly not the case. Let’s investigate.'
  prefs: []
  type: TYPE_NORMAL
- en: Checking the sampling rate
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Deeper digging reveals that although PurpleAir sensors currently record data
    every 120 seconds, this was not always the case. Before May 30, 2019, sensors
    recorded data every 80 seconds, or 1,080 points a day. The change in sampling
    rate does explain the drop on May 30, 2019\. Let’s next look at the time periods
    where there were many more points than expected. This could mean that some measurements
    were duplicated in the data. We can check this by looking at the measurements
    for one day, say, January 1, 2019\. We pass a string into `.loc` to filter timestamps
    for that date:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'There are almost double the 1,080 expected readings. Let’s check to see if
    readings are duplicated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'Each timestamp appears exactly twice, and we can verify that all duplicated
    dates contain the same PM2.5 reading. Since this is also true for both temperature
    and humidity, we drop the duplicate rows from the dataframe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: 'To check, we remake the line plot of the number of records for a day, and this
    time we shade the regions where the counts are supposed to be contained:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: '![](assets/leds_12in03.png)'
  prefs: []
  type: TYPE_IMG
- en: After dropping duplicate dates, the plot of measurements per day looks much
    more consistent with the counts we expect. Careful readers will see two spikes
    above the maximum measurements around November of each year when daylight saving
    time is no longer in effect. When clocks are rolled back one hour, that day has
    25 hours instead of the usual 24 hours. Timestamps are tricky!
  prefs: []
  type: TYPE_NORMAL
- en: But there are still missing measurements, and we need to decide what to do about
    them.
  prefs: []
  type: TYPE_NORMAL
- en: Handling Missing Values
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The plan is to create 24-hour averages of the measurements, but we don’t want
    to use days when there are not enough measurements. We follow Barkjohn’s analysis
    and only keep a 24-hour average if there are at least 90% of the possible points
    for that day. Remember that before May 30, 2019, there are 1,080 possible points
    in a day, and after that there are 720 possible points. We calculate the minimum
    number of measurements needed to keep per day:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can determine which of the days have enough measurements to keep:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: 'We’re ready to average together the readings for each day and then remove the
    days without enough readings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: '|   | PM25cf1 | TempF | RH |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| timestamp |   |   |   |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **2018-05-20 00:00:00-07:00** | 2.48 | 83.35 | 28.72 |'
  prefs: []
  type: TYPE_TB
- en: '| **2018-05-21 00:00:00-07:00** | 3.00 | 83.25 | 29.91 |'
  prefs: []
  type: TYPE_TB
- en: Now we have the average daily PM2.5 readings for instrument A, and we need to
    repeat on instrument B the data wrangling we just performed on instrument A. Fortunately,
    we can reuse the same pipeline. For brevity, we don’t include that wrangling here.
    But we need to decide what to do if the PM2.5 averages differ. Barkjohn dropped
    rows if the PM2.5 values for A and B differed by more than 61%, or by more than
    5 µg m⁻³. For this pair of sensors, that leads to dropping 12 of the 500+ rows.
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see, it takes a lot of work to prepare and clean these data: we
    handled missing data, aggregated the readings for each instrument, averaged the
    readings together from the two instruments, and removed rows where they disagreed.
    This work has given us a set of PM2.5 readings that we are more confident in.
    We know that each PM2.5 value in the final dataframe is the daily average from
    two separate instruments that generated consistent and complete readings.'
  prefs: []
  type: TYPE_NORMAL
- en: To fully replicate Barkjohn’s analysis, we would need to repeat this process
    over all the PurpleAir sensors. Then we would repeat the AQS cleaning procedure
    on all the AQS sensors. Finally, we would merge the PurpleAir and AQS data together.
    This procedure produces daily average readings for each collocated sensor pair.
    For brevity, we omit this code. Instead, we proceed with the final steps of the
    analysis using the group’s dataset. We begin with an EDA with an eye toward modeling.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring PurpleAir and AQS Measurements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s explore the cleaned dataset of matched AQS and PurpleAir PM2.5 readings
    and look for insights that might help us in modeling. Our main interest is in
    the relationship between the two sources of air quality measurements. But we want
    to keep in mind the scope of the data, like how these data are situated in time
    and place. We learned from our data cleaning that we are working with daily averages
    of PM2.5 for a couple of years and that we have data from dozens of locations
    across the US.
  prefs: []
  type: TYPE_NORMAL
- en: 'First we review the entire cleaned dataframe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: '|   | date | id | region | pm25aqs | pm25pa | temp | rh | dew |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **0** | 2019-05-17 | AK1 | Alaska | 6.7 | 8.62 | 18.03 | 38.56 | 3.63 |'
  prefs: []
  type: TYPE_TB
- en: '| **1** | 2019-05-18 | AK1 | Alaska | 3.8 | 3.49 | 16.12 | 49.40 | 5.44 |'
  prefs: []
  type: TYPE_TB
- en: '| **2** | 2019-05-21 | AK1 | Alaska | 4.0 | 3.80 | 19.90 | 29.97 | 1.73 |'
  prefs: []
  type: TYPE_TB
- en: '| **...** | ... | ... | ... | ... | ... | ... | ... | ... |'
  prefs: []
  type: TYPE_TB
- en: '| **12427** | 2019-02-20 | WI6 | North | 15.6 | 25.30 | 1.71 | 65.78 | -4.08
    |'
  prefs: []
  type: TYPE_TB
- en: '| **12428** | 2019-03-04 | WI6 | North | 14.0 | 8.21 | -14.38 | 48.21 | -23.02
    |'
  prefs: []
  type: TYPE_TB
- en: '| **12429** | 2019-03-22 | WI6 | North | 5.8 | 9.44 | 5.08 | 52.20 | -4.02
    |'
  prefs: []
  type: TYPE_TB
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: 'We include an explanation for each of the columns in our dataframe in the following
    table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Column | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| date | Date of the observation |'
  prefs: []
  type: TYPE_TB
- en: '| id | A unique label for a site, formatted as the US state abbreviation with
    a number (we performed data cleaning for site ID `CA1`) |'
  prefs: []
  type: TYPE_TB
- en: '| region | The name of the region, which corresponds to a group of sites (the
    `CA1` site is located in the `West` region) |'
  prefs: []
  type: TYPE_TB
- en: '| pm25aqs | The PM2.5 measurement from the AQS sensor |'
  prefs: []
  type: TYPE_TB
- en: '| pm25pa | The PM2.5 measurement from the PurpleAir sensor |'
  prefs: []
  type: TYPE_TB
- en: '| temp | Temperature, in Celsius |'
  prefs: []
  type: TYPE_TB
- en: '| rh | Relative humidity, ranging from 0% to 100% |'
  prefs: []
  type: TYPE_TB
- en: '| dew | The dew point (a higher dew point means more moisture is in the air)
    |'
  prefs: []
  type: TYPE_TB
- en: 'Let’s start with making a few simple visualizations to gain insight. Since
    the scope involves measurements over time at particular locations, we can choose
    one location with many measurements and make a line plot of the weekly average
    air quality. To choose, let’s find the sites with many records:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: 'The location labeled `NC4` has nearly 700 observations. To smooth the line
    plot a bit, let’s plot weekly averages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: '![](assets/leds_12in04.png)'
  prefs: []
  type: TYPE_IMG
- en: We see that most PM2.5 values for the AQS sensor (solid line) range between
    5.0 and 15.0 µg m⁻³. The PurpleAir sensor follows the up-and-down pattern of the
    AQS sensor, which is reassuring. But the measurements are consistently higher
    than AQS and, in some cases, quite a bit higher, which tells us that a correction
    might be helpful.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let’s consider the distributions of the PM2.5 readings for the two sensors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: '![](assets/leds_12in05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Both distributions are skewed right, which often happens when there’s a lower
    bound on values (in this case, 0). A better way to compare these two distributions
    is with a quantile–quantile plot (see [Chapter 10](ch10.html#ch-eda)). With a
    q–q plot it can be easier to compare means, spreads, and tails:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: '![](assets/leds_12in06.png)'
  prefs: []
  type: TYPE_IMG
- en: The quantile–quantile plot is roughly linear. We overlaid a dashed line with
    a slope of 2.2; it lines up the quantiles well, which indicates the spread of
    the PurpleAir measurements is about twice that of AQS.
  prefs: []
  type: TYPE_NORMAL
- en: 'What we can’t see in the q–q plot or the side-by-side histograms is how the
    sensor readings vary together. Let’s look at this next. First, we take a look
    at the distribution of difference between the two readings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: '![](assets/leds_12in07.png)'
  prefs: []
  type: TYPE_IMG
- en: If the instruments are in perfect agreement, we will see a spike at 0\. If the
    instruments are in agreement and there is a measurement error with no bias, we
    expect to see a distribution centered at 0\. Instead, we see that 90% of the time,
    the PurpleAir measurement is larger than the AQS 24-hour average, and about 25%
    of the time it is more than 10 µg/m³ higher, which is a lot given the AQS averages
    tend to be between 5 µg/m³ and 10 µg/m³.
  prefs: []
  type: TYPE_NORMAL
- en: 'A scatterplot can give us additional insight into the relationship between
    the measurements from these two instruments. Since we are interested in finding
    a general relationship, regardless of time and location, we include all of our
    average readings in the plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs: []
  type: TYPE_PRE
- en: '![](assets/leds_12in08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'While the relationship looks linear, all but a handful of readings are in the
    bottom-left corner of the plot. Let’s remake the scatterplot and zoom in on the
    bulk of the data to get a better look. We also add a smooth curve to the plot
    to help us see the relationship better:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE98]'
  prefs: []
  type: TYPE_PRE
- en: '![](assets/leds_12in09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The relationship looks roughly linear, but there is a slight bend in the curve
    for small values of AQS. When the air is very clean, the PurpleAir sensor doesn’t
    pick up as much particulate matter and so is more accurate. Also, we can see that
    the curve should go through the point (0, 0). Despite the slight bend in the relationship,
    the linear association (correlation) between these two measurements is high:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE100]'
  prefs: []
  type: TYPE_PRE
- en: Before starting this analysis, we expected that PurpleAir measurements would
    generally overestimate the PM2.5\. And indeed, this is reflected in the scatterplot,
    but we also see that there appears to be a strong linear relationship between
    the measurements from these two instruments that will be helpful in calibrating
    the PurpleAir sensor.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a Model to Correct PurpleAir Measurements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we’ve explored the relationship between PM2.5 readings from AQS and
    PurpleAir sensors, we’re ready for the final step of the analysis: creating a
    model that corrects PurpleAir measurements. Barkjohn’s original analysis fits
    many models to the data in order to find the most appropriate one. In this section,
    we fit a simple linear model using the techniques from [Chapter 4](ch04.html#ch-modeling).
    We also briefly describe the final model Barkjohn chose for real-world use. Since
    these models use methods that we introduce later in the book, we won’t explain
    the technical details very deeply here. Instead, we encourage you to revisit this
    section after reading [Chapter 15](ch15.html#ch-linear).'
  prefs: []
  type: TYPE_NORMAL
- en: First, let’s go over our modeling goals. We want to create a model that predicts
    PM2.5 as accurately as possible. To do this, we build a model that adjusts PurpleAir
    measurements based on AQS measurements. We treat the AQS measurements as the true
    PM2.5 values because they are taken from carefully calibrated instruments and
    are actively used by the US government for decision making. So we have reason
    to trust the AQS PM2.5 values as being precise and close to the truth.
  prefs: []
  type: TYPE_NORMAL
- en: After we build the model that adjusts the PurpleAir measurements using AQS,
    we then flip the model around and use it to predict the true air quality in the
    future from PurpleAir measurements when we don’t have a nearby AQS instrument.
    This is a *calibration* scenario. Since the AQS measurements are close to the
    truth, we fit the more variable PurpleAir measurements to them; this is the calibration
    procedure. Then we use the calibration curve to correct future PurpleAir measurements.
    This two-step process is encapsulated in the upcoming simple linear model and
    its flipped form.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we fit a line to predict a PurpleAir (PA) measurement from the ground
    truth, as recorded by an AQS instrument:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mtext>PA</mtext> <mo>≈</mo> <mi>b</mi> <mo>+</mo> <mi>m</mi>
    <mtext>AQS</mtext></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we flip the line around to use a PA measurement to predict the air quality:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mtext>True air quality</mtext> <mo>≈</mo> <mo>−</mo>
    <mi>b</mi> <mrow><mo>/</mo></mrow> <mi>m</mi> <mo>+</mo> <mn>1</mn> <mrow><mo>/</mo></mrow>
    <mi>m</mi> <mtext>PA</mtext></math>
  prefs: []
  type: TYPE_NORMAL
- en: The scatterplot and histograms that we made during our exploratory data analysis
    suggest that the PurpleAir measurements are more variable, which supports the
    calibration approach. And we saw that the PurpleAir measurements are about twice
    as high as the AQS measurements, which suggests that <math><mi>m</mi></math> may
    be close to 2 and <math><mn>1</mn> <mrow><mo>/</mo></mrow> <mi>m</mi></math> close
    to <math><mn>1</mn> <mrow><mo>/</mo></mrow> <mn>2</mn></math> .
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let’s fit the model. Following the notion from [Chapter 4](ch04.html#ch-modeling),
    we choose a loss function and minimize the average error. Recall that a *loss
    function* measures how far away our model is from the actual data. We use squared
    loss, which in this case is <math><mo stretchy="false">[</mo> <mi>P</mi> <mi>A</mi>
    <mo>−</mo> <mo stretchy="false">(</mo> <mi>b</mi> <mo>+</mo> <mi>m</mi> <mi>A</mi>
    <mi>Q</mi> <mi>S</mi> <mo stretchy="false">)</mo> <msup><mo stretchy="false">]</mo>
    <mn>2</mn></msup></math> . And to fit the model to our data, we minimize the average
    squared loss over our data:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mtable columnalign="right" displaystyle="true" rowspacing="3pt"><mtr><mtd><mfrac><mn>1</mn>
    <mi>n</mi></mfrac> <munderover><mo>∑</mo> <mrow><mi>i</mi> <mo>=</mo> <mn>1</mn></mrow>
    <mrow><mi>n</mi></mrow></munderover> <mo stretchy="false">[</mo> <mi>P</mi> <msub><mi>A</mi>
    <mi>i</mi></msub> <mo>−</mo> <mo stretchy="false">(</mo> <mi>b</mi> <mo>+</mo>
    <mi>m</mi> <mi>A</mi> <mi>Q</mi> <msub><mi>S</mi> <mi>i</mi></msub> <msup><mo
    stretchy="false">]</mo> <mn>2</mn></msup></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'We use the linear modeling functionality provided by `scikit-learn` to do this
    (again, don’t worry about these details for now):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs: []
  type: TYPE_PRE
- en: 'By inverting the line, we get the estimate:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE103]'
  prefs: []
  type: TYPE_PRE
- en: This is close to what we expected. The adjustment to PurpleAir measurements
    is about <math><mn>1</mn> <mrow><mo>/</mo></mrow> <mn>2</mn></math> .
  prefs: []
  type: TYPE_NORMAL
- en: 'The model that Barkjohn settled on incorporated the relative humidity:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mtable columnalign="right" displaystyle="true" rowspacing="3pt"><mtr><mtd><mtext>PA</mtext>
    <mo>≈</mo> <mi>b</mi> <mo>+</mo> <msub><mi>m</mi> <mn>1</mn></msub> <mtext>AQS</mtext>
    <mo>+</mo> <msub><mi>m</mi> <mn>2</mn></msub> <mtext>RH</mtext></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'This is an example of a multivariable linear regression model—it uses more
    than one variable to make predictions. We can fit it by minimizing the average
    squared error over the data:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mtable columnalign="right" displaystyle="true" rowspacing="3pt"><mtr><mtd><mfrac><mn>1</mn>
    <mi>n</mi></mfrac> <munderover><mo>∑</mo> <mrow><mi>i</mi> <mo>=</mo> <mn>1</mn></mrow>
    <mrow><mi>n</mi></mrow></munderover> <mo stretchy="false">[</mo> <mi>P</mi> <msub><mi>A</mi>
    <mi>i</mi></msub> <mo>−</mo> <mo stretchy="false">(</mo> <mi>b</mi> <mo>+</mo>
    <msub><mi>m</mi> <mn>1</mn></msub> <mi>A</mi> <mi>Q</mi> <msub><mi>S</mi> <mi>i</mi></msub>
    <mo>+</mo> <msub><mi>m</mi> <mn>2</mn></msub> <mi>R</mi> <msub><mi>H</mi> <mi>i</mi></msub>
    <msup><mo stretchy="false">]</mo> <mn>2</mn></msup></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Then we invert the calibration to find the prediction model using the following
    equation:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mtable columnalign="right left" columnspacing="0em" displaystyle="true"
    rowspacing="3pt"><mtr><mtd><mtext>True air quality</mtext></mtd> <mtd><mo>≈</mo>
    <mo>−</mo> <mfrac><mi>b</mi> <msub><mi>m</mi> <mn>1</mn></msub></mfrac> <mo>+</mo>
    <mfrac><mn>1</mn> <msub><mi>m</mi> <mn>1</mn></msub></mfrac> <mtext>PA</mtext>
    <mo>−</mo> <mfrac><msub><mi>m</mi> <mn>2</mn></msub> <msub><mi>m</mi> <mn>1</mn></msub></mfrac>
    <mtext>RH</mtext></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'We fit this model and check the coefficients:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE104]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE105]'
  prefs: []
  type: TYPE_PRE
- en: In Chapters [15](ch15.html#ch-linear) and [16](ch16.html#ch-risk), we will learn
    how to compare these two models by examining things like the size of and patterns
    in prediction errors. For now, we note that the model that incorporates relative
    humidity performs the best.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we replicated Barkjohn’s analysis. We created a model that
    corrects PurpleAir measurements so that they closely match AQS measurements. The
    accuracy of this model enables the PurpleAir sensors to be included on official
    US government maps, like the AirNow Fire and Smoke map. Importantly, this model
    gives people timely *and* accurate measurements of air quality.
  prefs: []
  type: TYPE_NORMAL
- en: We saw how crowdsourced, open data can be improved with data from precise, rigorously
    maintained, government-monitored equipment. In the process, we focused on cleaning
    and merging data from multiple sources, but we also fit models to adjust and improve
    air quality measurements.
  prefs: []
  type: TYPE_NORMAL
- en: For this case study, we applied many concepts covered in this part of the book.
    As you saw, wrangling files and data tables into a form we can analyze is a large
    and important part of data science. We used file wrangling and the notions of
    granularity from [Chapter 8](ch08.html#ch-files) to prepare two sources for merging.
    We got them into structures where we could match neighboring air quality sensors.
    This “grungy” part of data science was essential to widening the reach of data
    from rigorously maintained, precise government-monitored equipment by augmenting
    it with crowdsourced, open data.
  prefs: []
  type: TYPE_NORMAL
- en: This preparation process involved intensive, careful examination, cleaning,
    and improvement of the data to ensure their compatibility across the two sources
    and their trustworthiness in our analysis. Concepts from [Chapter 9](ch09.html#ch-wrangling)
    helped us work with time data effectively and find and correct numerous issues
    like missing data points and even duplicated data values.
  prefs: []
  type: TYPE_NORMAL
- en: File and data wrangling, exploratory data analysis, and visualization are major
    parts of many analyses. While fitting models may seem to be the most exciting
    part of data science, getting to know and trust the data is crucial and often
    leads to important insights in the modeling phase. Topics related to modeling
    make up most of the rest of this book. However, before we begin, we cover two
    more topics related to data wrangling. In the next chapter, we show how to create
    analyzable data from text, and in the following chapter we examine other formats
    for source files that we mentioned in [Chapter 8](ch08.html#ch-files).
  prefs: []
  type: TYPE_NORMAL
- en: Before you head to the next chapter, take stock of what you’ve learned so far.
    Pat yourself on the back—you’ve already come a long way! The principles and techniques
    we’ve covered here are useful for nearly every type of data analysis, and you
    can readily start applying them toward analyses of your own.
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch12.html#id1449-marker)) This estimation works by assuming that the Earth
    is perfectly spherical. Then, one degree of latitude is the radius of the Earth
    in meters. Plugging in the average radius of the Earth gives 111,111 meters per
    degree of latitude. Longitude is the same, but the radius of each “ring” around
    the Earth decreases as we get closer to the poles, so we adjust by a factor of
    <math><mrow><mo form="prefix">cos</mo> <mo>(</mo> <mtext>lat</mtext> <mo>)</mo></mrow></math>
    . It turns out that the Earth isn’t perfectly spherical, so these estimations
    can’t be used for precise calculations, like landing a rocket. But for our purposes,
    they do just fine.
  prefs: []
  type: TYPE_NORMAL
