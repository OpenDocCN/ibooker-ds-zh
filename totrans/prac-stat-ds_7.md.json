["```py\noil_px <- sp500_px[, c('CVX', 'XOM')]\npca <- princomp(oil_px)\npca$loadings\n\nLoadings:\n    Comp.1 Comp.2\nCVX -0.747  0.665\nXOM -0.665 -0.747\n\n               Comp.1 Comp.2\nSS loadings       1.0    1.0\nProportion Var    0.5    0.5\nCumulative Var    0.5    1.0\n```", "```py\npcs = PCA(n_components=2)\npcs.fit(oil_px)\nloadings = pd.DataFrame(pcs.components_, columns=oil_px.columns)\nloadings\n```", "```py\nloadings <- pca$loadings\nggplot(data=oil_px, aes(x=CVX, y=XOM)) +\n  geom_point(alpha=.3) +\n  stat_ellipse(type='norm', level=.99) +\n  geom_abline(intercept = 0, slope = loadings[2,1]/loadings[1,1]) +\n  geom_abline(intercept = 0, slope = loadings[2,2]/loadings[1,2])\n```", "```py\ndef abline(slope, intercept, ax):\n    \"\"\"Calculate coordinates of a line based on slope and intercept\"\"\"\n    x_vals = np.array(ax.get_xlim())\n    return (x_vals, intercept + slope * x_vals)\n\nax = oil_px.plot.scatter(x='XOM', y='CVX', alpha=0.3, figsize=(4, 4))\nax.set_xlim(-3, 3)\nax.set_ylim(-3, 3)\nax.plot(*abline(loadings.loc[0, 'CVX'] / loadings.loc[0, 'XOM'], 0, ax),\n        '--', color='C1')\nax.plot(*abline(loadings.loc[1, 'CVX'] / loadings.loc[1, 'XOM'], 0, ax),\n        '--', color='C1')\n```", "```py\nsyms <- c( 'AAPL', 'MSFT', 'CSCO', 'INTC', 'CVX', 'XOM',\n   'SLB', 'COP', 'JPM', 'WFC', 'USB', 'AXP', 'WMT', 'TGT', 'HD', 'COST')\ntop_sp <- sp500_px[row.names(sp500_px)>='2005-01-01', syms]\nsp_pca <- princomp(top_sp)\nscreeplot(sp_pca)\n```", "```py\nsyms = sorted(['AAPL', 'MSFT', 'CSCO', 'INTC', 'CVX', 'XOM', 'SLB', 'COP',\n               'JPM', 'WFC', 'USB', 'AXP', 'WMT', 'TGT', 'HD', 'COST'])\ntop_sp = sp500_px.loc[sp500_px.index >= '2011-01-01', syms]\n\nsp_pca = PCA()\nsp_pca.fit(top_sp)\n\nexplained_variance = pd.DataFrame(sp_pca.explained_variance_)\nax = explained_variance.head(10).plot.bar(legend=False, figsize=(4, 4))\nax.set_xlabel('Component')\n```", "```py\nlibrary(tidyr)\nloadings <- sp_pca$loadings[,1:5]\nloadings$Symbol <- row.names(loadings)\nloadings <- gather(loadings, 'Component', 'Weight', -Symbol)\nggplot(loadings, aes(x=Symbol, y=Weight)) +\n  geom_bar(stat='identity') +\n  facet_grid(Component ~ ., scales='free_y')\n```", "```py\nloadings = pd.DataFrame(sp_pca.components_[0:5, :], columns=top_sp.columns)\nmaxPC = 1.01 * np.max(np.max(np.abs(loadings.loc[0:5, :])))\n\nf, axes = plt.subplots(5, 1, figsize=(5, 5), sharex=True)\nfor i, ax in enumerate(axes):\n    pc_loadings = loadings.loc[i, :]\n    colors = ['C0' if l > 0 else 'C1' for l in pc_loadings]\n    ax.axhline(color='#888888')\n    pc_loadings.plot.bar(ax=ax, color=colors)\n    ax.set_ylabel(f'PC{i+1}')\n    ax.set_ylim(-maxPC, maxPC)\n```", "```py\nca_analysis <- ca(housetasks)\nplot(ca_analysis)\n```", "```py\nca = prince.CA(n_components=2)\nca = ca.fit(housetasks)\n\nca.plot_coordinates(housetasks, figsize=(6, 6))\n```", "```py\ndf <- sp500_px[row.names(sp500_px)>='2011-01-01', c('XOM', 'CVX')]\nkm <- kmeans(df, centers=4)\n```", "```py\ndf = sp500_px.loc[sp500_px.index >= '2011-01-01', ['XOM', 'CVX']]\nkmeans = KMeans(n_clusters=4).fit(df)\n```", "```py\n> df$cluster <- factor(km$cluster)\n> head(df)\n                  XOM        CVX cluster\n2011-01-03 0.73680496  0.2406809       2\n2011-01-04 0.16866845 -0.5845157       1\n2011-01-05 0.02663055  0.4469854       2\n2011-01-06 0.24855834 -0.9197513       1\n2011-01-07 0.33732892  0.1805111       2\n2011-01-10 0.00000000 -0.4641675       1\n```", "```py\ndf['cluster'] = kmeans.labels_\ndf.head()\n```", "```py\n> centers <- data.frame(cluster=factor(1:4), km$centers)\n> centers\n  cluster        XOM        CVX\n1       1 -0.3284864 -0.5669135\n2       2  0.2410159  0.3342130\n3       3 -1.1439800 -1.7502975\n4       4  0.9568628  1.3708892\n```", "```py\ncenters = pd.DataFrame(kmeans.cluster_centers_, columns=['XOM', 'CVX'])\ncenters\n```", "```py\nggplot(data=df, aes(x=XOM, y=CVX, color=cluster, shape=cluster)) +\n  geom_point(alpha=.3) +\n  geom_point(data=centers,  aes(x=XOM, y=CVX), size=3, stroke=2)\n```", "```py\nfig, ax = plt.subplots(figsize=(4, 4))\nax = sns.scatterplot(x='XOM', y='CVX', hue='cluster', style='cluster',\n                     ax=ax, data=df)\nax.set_xlim(-3, 3)\nax.set_ylim(-3, 3)\ncenters.plot.scatter(x='XOM', y='CVX', ax=ax, s=50, color='black')\n```", "```py\nsyms <- c( 'AAPL', 'MSFT', 'CSCO', 'INTC', 'CVX', 'XOM', 'SLB', 'COP',\n           'JPM', 'WFC', 'USB', 'AXP', 'WMT', 'TGT', 'HD', 'COST')\ndf <- sp500_px[row.names(sp500_px) >= '2011-01-01', syms]\nkm <- kmeans(df, centers=5, nstart=10)\n```", "```py\nsyms = sorted(['AAPL', 'MSFT', 'CSCO', 'INTC', 'CVX', 'XOM', 'SLB', 'COP',\n               'JPM', 'WFC', 'USB', 'AXP', 'WMT', 'TGT', 'HD', 'COST'])\ntop_sp = sp500_px.loc[sp500_px.index >= '2011-01-01', syms]\nkmeans = KMeans(n_clusters=5).fit(top_sp)\n```", "```py\nkm$size\n[1] 106 186 285 288 266\n```", "```py\nfrom collections import Counter\nCounter(kmeans.labels_)\n\nCounter({4: 302, 2: 272, 0: 288, 3: 158, 1: 111})\n```", "```py\ncenters <- as.data.frame(t(centers))\nnames(centers) <- paste(\"Cluster\", 1:5)\ncenters$Symbol <- row.names(centers)\ncenters <- gather(centers, 'Cluster', 'Mean', -Symbol)\ncenters$Color = centers$Mean > 0\nggplot(centers, aes(x=Symbol, y=Mean, fill=Color)) +\n  geom_bar(stat='identity', position='identity', width=.75) +\n  facet_grid(Cluster ~ ., scales='free_y')\n```", "```py\ncenters = pd.DataFrame(kmeans.cluster_centers_, columns=syms)\n\nf, axes = plt.subplots(5, 1, figsize=(5, 5), sharex=True)\nfor i, ax in enumerate(axes):\n    center = centers.loc[i, :]\n    maxPC = 1.01 * np.max(np.max(np.abs(center)))\n    colors = ['C0' if l > 0 else 'C1' for l in center]\n    ax.axhline(color='#888888')\n    center.plot.bar(ax=ax, color=colors)\n    ax.set_ylabel(f'Cluster {i + 1}')\n    ax.set_ylim(-maxPC, maxPC)\n```", "```py\npct_var <- data.frame(pct_var = 0,\n                      num_clusters = 2:14)\ntotalss <- kmeans(df, centers=14, nstart=50, iter.max=100)$totss\nfor (i in 2:14) {\n  kmCluster <- kmeans(df, centers=i, nstart=50, iter.max=100)\n  pct_var[i-1, 'pct_var'] <- kmCluster$betweenss / totalss\n}\n```", "```py\ninertia = []\nfor n_clusters in range(2, 14):\n    kmeans = KMeans(n_clusters=n_clusters, random_state=0).fit(top_sp)\n    inertia.append(kmeans.inertia_ / n_clusters)\n\ninertias = pd.DataFrame({'n_clusters': range(2, 14), 'inertia': inertia})\nax = inertias.plot(x='n_clusters', y='inertia')\nplt.xlabel('Number of clusters(k)')\nplt.ylabel('Average Within-Cluster Squared Distances')\nplt.ylim((0, 1.1 * inertias.inertia.max()))\nax.legend().set_visible(False)\n```", "```py\nsyms1 <- c('GOOGL', 'AMZN', 'AAPL', 'MSFT', 'CSCO', 'INTC', 'CVX', 'XOM', 'SLB',\n           'COP', 'JPM', 'WFC', 'USB', 'AXP', 'WMT', 'TGT', 'HD', 'COST')\n# take transpose: to cluster companies, we need the stocks along the rows\ndf <- t(sp500_px[row.names(sp500_px) >= '2011-01-01', syms1])\nd <- dist(df)\nhcl <- hclust(d)\n```", "```py\nsyms1 = ['AAPL', 'AMZN', 'AXP', 'COP', 'COST', 'CSCO', 'CVX', 'GOOGL', 'HD',\n         'INTC', 'JPM', 'MSFT', 'SLB', 'TGT', 'USB', 'WFC', 'WMT', 'XOM']\ndf = sp500_px.loc[sp500_px.index >= '2011-01-01', syms1].transpose()\n\nZ = linkage(df, method='complete')\n```", "```py\nplot(hcl)\n```", "```py\nfig, ax = plt.subplots(figsize=(5, 5))\ndendrogram(Z, labels=df.index, ax=ax, color_threshold=0)\nplt.xticks(rotation=90)\nax.set_ylabel('distance')\n```", "```py\ncutree(hcl, k=4)\nGOOGL  AMZN  AAPL  MSFT  CSCO  INTC   CVX   XOM   SLB   COP   JPM   WFC\n    1     2     3     3     3     3     4     4     4     4     3     3\n  USB   AXP   WMT   TGT    HD  COST\n    3     3     3     3     3     3\n```", "```py\nmemb = fcluster(Z, 4, criterion='maxclust')\nmemb = pd.Series(memb, index=df.index)\nfor key, item in memb.groupby(memb):\n    print(f\"{key} : {', '.join(item.index)}\")\n```", "```py\n> library(mclust)\n> df <- sp500_px[row.names(sp500_px) >= '2011-01-01', c('XOM', 'CVX')]\n> mcl <- Mclust(df)\n> summary(mcl)\nMclust VEE (ellipsoidal, equal shape and orientation) model with 2 components:\n\n log.likelihood    n df       BIC       ICL\n      -2255.134 1131  9 -4573.546 -5076.856\n\nClustering table:\n  1   2\n963 168\n```", "```py\ndf = sp500_px.loc[sp500_px.index >= '2011-01-01', ['XOM', 'CVX']]\nmclust = GaussianMixture(n_components=2).fit(df)\nmclust.bic(df)\n```", "```py\ncluster <- factor(predict(mcl)$classification)\nggplot(data=df, aes(x=XOM, y=CVX, color=cluster, shape=cluster)) +\n  geom_point(alpha=.8)\n```", "```py\nfig, ax = plt.subplots(figsize=(4, 4))\ncolors = [f'C{c}' for c in mclust.predict(df)]\ndf.plot.scatter(x='XOM', y='CVX', c=colors, alpha=0.5, ax=ax)\nax.set_xlim(-3, 3)\nax.set_ylim(-3, 3)\n```", "```py\n> summary(mcl, parameters=TRUE)$mean\n          [,1]        [,2]\nXOM 0.05783847 -0.04374944\nCVX 0.07363239 -0.21175715\n> summary(mcl, parameters=TRUE)$variance\n, , 1\n          XOM       CVX\nXOM 0.3002049 0.3060989\nCVX 0.3060989 0.5496727\n, , 2\n\n         XOM      CVX\nXOM 1.046318 1.066860\nCVX 1.066860 1.915799\n```", "```py\nprint('Mean')\nprint(mclust.means_)\nprint('Covariances')\nprint(mclust.covariances_)\n```", "```py\nplot(mcl, what='BIC', ask=FALSE)\n```", "```py\nresults = []\ncovariance_types = ['full', 'tied', 'diag', 'spherical']\nfor n_components in range(1, 9):\n    for covariance_type in covariance_types:\n        mclust = GaussianMixture(n_components=n_components, warm_start=True,\n                                 covariance_type=covariance_type) ![1](Images/1.png)\n        mclust.fit(df)\n        results.append({\n            'bic': mclust.bic(df),\n            'n_components': n_components,\n            'covariance_type': covariance_type,\n        })\n\nresults = pd.DataFrame(results)\n\ncolors = ['C0', 'C1', 'C2', 'C3']\nstyles = ['C0-','C1:','C0-.', 'C1--']\n\nfig, ax = plt.subplots(figsize=(4, 4))\nfor i, covariance_type in enumerate(covariance_types):\n    subset = results.loc[results.covariance_type == covariance_type, :]\n    subset.plot(x='n_components', y='bic', ax=ax, label=covariance_type,\n                kind='line', style=styles[i])\n```", "```py\ndefaults <- loan_data[loan_data$outcome=='default',]\ndf <- defaults[, c('loan_amnt', 'annual_inc', 'revol_bal', 'open_acc',\n                   'dti', 'revol_util')]\nkm <- kmeans(df, centers=4, nstart=10)\ncenters <- data.frame(size=km$size, km$centers)\nround(centers, digits=2)\n\n   size loan_amnt annual_inc revol_bal open_acc   dti revol_util\n1    52  22570.19  489783.40  85161.35    13.33  6.91      59.65\n2  1192  21856.38  165473.54  38935.88    12.61 13.48      63.67\n3 13902  10606.48   42500.30  10280.52     9.59 17.71      58.11\n4  7525  18282.25   83458.11  19653.82    11.66 16.77      62.27\n```", "```py\ndefaults = loan_data.loc[loan_data['outcome'] == 'default',]\ncolumns = ['loan_amnt', 'annual_inc', 'revol_bal', 'open_acc',\n           'dti', 'revol_util']\n\ndf = defaults[columns]\nkmeans = KMeans(n_clusters=4, random_state=1).fit(df)\ncounts = Counter(kmeans.labels_)\n\ncenters = pd.DataFrame(kmeans.cluster_centers_, columns=columns)\ncenters['size'] = [counts[i] for i in range(4)]\ncenters\n```", "```py\ndf0 <- scale(df)\nkm0 <- kmeans(df0, centers=4, nstart=10)\ncenters0 <- scale(km0$centers, center=FALSE,\n                 scale=1 / attr(df0, 'scaled:scale'))\ncenters0 <- scale(centers0, center=-attr(df0, 'scaled:center'), scale=FALSE)\ncenters0 <- data.frame(size=km0$size, centers0)\nround(centers0, digits=2)\n\n  size loan_amnt annual_inc revol_bal open_acc   dti revol_util\n1 7355  10467.65   51134.87  11523.31     7.48 15.78      77.73\n2 5309  10363.43   53523.09   6038.26     8.68 11.32      30.70\n3 3713  25894.07  116185.91  32797.67    12.41 16.22      66.14\n4 6294  13361.61   55596.65  16375.27    14.25 24.23      59.61\n```", "```py\nscaler = preprocessing.StandardScaler()\ndf0 = scaler.fit_transform(df * 1.0)\n\nkmeans = KMeans(n_clusters=4, random_state=1).fit(df0)\ncounts = Counter(kmeans.labels_)\n\ncenters = pd.DataFrame(scaler.inverse_transform(kmeans.cluster_centers_),\n                       columns=columns)\ncenters['size'] = [counts[i] for i in range(4)]\ncenters\n```", "```py\nsyms <- c('GOOGL', 'AMZN', 'AAPL', 'MSFT', 'CSCO', 'INTC', 'CVX', 'XOM',\n          'SLB', 'COP', 'JPM', 'WFC', 'USB', 'AXP', 'WMT', 'TGT', 'HD', 'COST')\ntop_sp1 <- sp500_px[row.names(sp500_px) >= '2005-01-01', syms]\nsp_pca1 <- princomp(top_sp1)\nscreeplot(sp_pca1)\n```", "```py\nsyms = ['GOOGL', 'AMZN', 'AAPL', 'MSFT', 'CSCO', 'INTC', 'CVX', 'XOM',\n        'SLB', 'COP', 'JPM', 'WFC', 'USB', 'AXP', 'WMT', 'TGT', 'HD', 'COST']\ntop_sp1 = sp500_px.loc[sp500_px.index >= '2005-01-01', syms]\n\nsp_pca1 = PCA()\nsp_pca1.fit(top_sp1)\n\nexplained_variance = pd.DataFrame(sp_pca1.explained_variance_)\nax = explained_variance.head(10).plot.bar(legend=False, figsize=(4, 4))\nax.set_xlabel('Component')\n```", "```py\nround(sp_pca1$loadings[,1:2], 3)\n      Comp.1 Comp.2\nGOOGL  0.781  0.609\nAMZN   0.593 -0.792\nAAPL   0.078  0.004\nMSFT   0.029  0.002\nCSCO   0.017 -0.001\nINTC   0.020 -0.001\nCVX    0.068 -0.021\nXOM    0.053 -0.005\n...\n```", "```py\nloadings = pd.DataFrame(sp_pca1.components_[0:2, :], columns=top_sp1.columns)\nloadings.transpose()\n```", "```py\n> x <- loan_data[1:5, c('dti', 'payment_inc_ratio', 'home_', 'purpose_')]\n> x\n# A tibble: 5 Ã— 4\n    dti payment_inc_ratio   home            purpose\n  <dbl>             <dbl> <fctr>             <fctr>\n1  1.00           2.39320   RENT                car\n2  5.55           4.57170    OWN     small_business\n3 18.08           9.71600   RENT              other\n4 10.08          12.21520   RENT debt_consolidation\n5  7.06           3.90888   RENT              other\n```", "```py\nlibrary(cluster)\ndaisy(x, metric='gower')\nDissimilarities :\n          1         2         3         4\n2 0.6220479\n3 0.6863877 0.8143398\n4 0.6329040 0.7608561 0.4307083\n5 0.3772789 0.5389727 0.3091088 0.5056250\n\nMetric :  mixed ;  Types = I, I, N, N\nNumber of objects : 5\n```", "```py\ndf <- defaults[sample(nrow(defaults), 250),\n               c('dti', 'payment_inc_ratio', 'home', 'purpose')]\nd = daisy(df, metric='gower')\nhcl <- hclust(d)\ndnd <- as.dendrogram(hcl)\nplot(dnd, leaflab='none')\n```", "```py\ndnd_cut <- cut(dnd, h=0.5)\ndf[labels(dnd_cut$lower[[1]]),]\n        dti payment_inc_ratio home_           purpose_\n44532 21.22           8.37694   OWN debt_consolidation\n39826 22.59           6.22827   OWN debt_consolidation\n13282 31.00           9.64200   OWN debt_consolidation\n31510 26.21          11.94380   OWN debt_consolidation\n6693  26.96           9.45600   OWN debt_consolidation\n7356  25.81           9.39257   OWN debt_consolidation\n9278  21.00          14.71850   OWN debt_consolidation\n13520 29.00          18.86670   OWN debt_consolidation\n14668 25.75          17.53440   OWN debt_consolidation\n19975 22.70          17.12170   OWN debt_consolidation\n23492 22.68          18.50250   OWN debt_consolidation\n```", "```py\ndf <- model.matrix(~ -1 + dti + payment_inc_ratio + home_ + pub_rec_zero,\n                   data=defaults)\ndf0 <- scale(df)\nkm0 <- kmeans(df0, centers=4, nstart=10)\ncenters0 <- scale(km0$centers, center=FALSE,\n                 scale=1/attr(df0, 'scaled:scale'))\nround(scale(centers0, center=-attr(df0, 'scaled:center'), scale=FALSE), 2)\n\n    dti payment_inc_ratio home_MORTGAGE home_OWN home_RENT pub_rec_zero\n1 17.20              9.27          0.00        1      0.00         0.92\n2 16.99              9.11          0.00        0      1.00         1.00\n3 16.50              8.06          0.52        0      0.48         0.00\n4 17.46              8.42          1.00        0      0.00         1.00\n```", "```py\ncolumns = ['dti', 'payment_inc_ratio', 'home_', 'pub_rec_zero']\ndf = pd.get_dummies(defaults[columns])\n\nscaler = preprocessing.StandardScaler()\ndf0 = scaler.fit_transform(df * 1.0)\nkmeans = KMeans(n_clusters=4, random_state=1).fit(df0)\ncenters = pd.DataFrame(scaler.inverse_transform(kmeans.cluster_centers_),\n                       columns=df.columns)\ncenters\n```"]