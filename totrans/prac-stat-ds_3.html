<html><head></head><body><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 3. Statistical Experiments and Significance Testing"><div class="chapter" id="Experiments">
<h1><span class="label">Chapter 3. </span>Statistical Experiments and <span class="keep-together">Significance Testing</span></h1>


<p>Design of experiments is a cornerstone of the practice of statistics, with applications in virtually all areas of research.<a data-type="indexterm" data-primary="statistical experiments and significance testing" id="ix_statexp"/>
The goal is to design an experiment in order to confirm or reject a hypothesis. Data scientists often need to conduct continual experiments, particularly regarding user interface and product marketing.
This chapter reviews traditional experimental design and discusses some common challenges in data science.
It also covers some oft-cited concepts in statistical inference and explains their meaning and relevance (or lack of relevance) to data science.</p>

<p>Whenever you<a data-type="indexterm" data-primary="statistical inference, classical inference pipeline" id="idm46522861357128"/> see references to statistical significance, t-tests, or p-values, it is typically in the context of the classical statistical inference “pipeline” (see <a data-type="xref" href="#fig0301">Figure 3-1</a>).
This process starts with a hypothesis (“drug A is better than the existing standard drug,” or “price A is more profitable than the existing price B”).
An experiment (it might be an A/B test) is designed to test the hypothesis—designed in such a way that it hopefully will deliver conclusive results.<a data-type="indexterm" data-primary="inference" id="idm46522861354920"/> The data is collected and analyzed, and then a conclusion is drawn.
The term <em>inference</em> reflects the intention to apply the experiment results, which involve a limited set of data, to a larger process or <span class="keep-together">population</span>.</p>

<figure><div id="fig0301" class="figure">
<img src="Images/psd2_0301.png" alt="images/Inference-pipeline.png" width="1105" height="149"/>
<h6><span class="label">Figure 3-1. </span>The classical statistical inference pipeline</h6>
</div></figure>






<section data-type="sect1" data-pdf-bookmark="A/B Testing"><div class="sect1" id="A-B-test">
<h1>A/B Testing</h1>

<p>An A/B test is an experiment with two groups to establish which of two treatments, products, procedures, or the like is superior.<a data-type="indexterm" data-primary="statistical experiments and significance testing" data-secondary="A/B testing" id="ix_statexpAB"/><a data-type="indexterm" data-primary="A/B testing" id="ix_ABtst"/>
Often one of the two treatments is the standard existing treatment, or no treatment.
If a standard (or no) treatment is used, it is called the <em>control</em>. A typical hypothesis is that a new treatment is better than the control.<a data-type="indexterm" data-primary="control group" id="idm46522861325704"/></p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46522861324872">
<h5>Key Terms for A/B Testing</h5><dl>
<dt class="horizontal"><strong><em>Treatment</em></strong></dt>
<dd>
<p>Something (drug, price, web headline) to which a subject is exposed.<a data-type="indexterm" data-primary="treatment" id="idm46522861321768"/></p>
</dd>
<dt class="horizontal"><strong><em>Treatment group</em></strong></dt>
<dd>
<p>A group of subjects exposed to a specific treatment.</p>
</dd>
<dt class="horizontal"><strong><em>Control group</em></strong></dt>
<dd>
<p>A group of subjects exposed to no (or standard) treatment.<a data-type="indexterm" data-primary="treatment group" id="idm46522861317416"/><a data-type="indexterm" data-primary="randomization" id="idm46522861316712"/></p>
</dd>
<dt class="horizontal"><strong><em>Randomization</em></strong></dt>
<dd>
<p>The process of randomly assigning subjects to treatments.</p>
</dd>
<dt class="horizontal"><strong><em>Subjects</em></strong></dt>
<dd>
<p>The items (web visitors, patients, etc.) that are exposed to treatments.<a data-type="indexterm" data-primary="subjects" id="idm46522861312392"/></p>
</dd>
<dt class="horizontal"><strong><em>Test statistic</em></strong></dt>
<dd>
<p>The metric used to measure the effect of the treatment.</p>
</dd>
</dl>
</div></aside>

<p>A/B tests are common in web design and marketing, since results are so readily measured.
Some examples<a data-type="indexterm" data-primary="A/B testing" data-secondary="examples of" id="idm46522861309320"/> of A/B testing include:</p>

<ul>
<li>
<p>Testing two soil treatments to determine which produces better seed germination</p>
</li>
<li>
<p>Testing two therapies to determine which suppresses cancer more effectively</p>
</li>
<li>
<p>Testing two prices to determine which yields more net profit</p>
</li>
<li>
<p>Testing two web headlines to determine which produces more clicks (<a data-type="xref" href="#fig0302">Figure 3-2</a>)</p>
</li>
<li>
<p>Testing two web ads to determine which generates more conversions</p>
</li>
</ul>

<figure><div id="fig0302" class="figure">
<img src="Images/psd2_0302.png" alt="images/Web-test-A-B.png" width="655" height="662"/>
<h6><span class="label">Figure 3-2. </span>Marketers continually test one web presentation against another</h6>
</div></figure>

<p>A proper A/B test has <em>subjects</em> that can be assigned to one treatment or another.  The subject might be a person, a plant seed, a web visitor; the key is that the subject is exposed to the treatment. Ideally, subjects are <em>randomized</em> (assigned randomly) to treatments.  In this way, you know that any difference between the treatment groups is due to one of two things:</p>

<ul>
<li>
<p>The effect of the different treatments</p>
</li>
<li>
<p>Luck of the draw in which subjects are assigned to which treatments (i.e., the random assignment may have resulted in the naturally better-performing subjects being concentrated in A or B)</p>
</li>
</ul>

<p class="pagebreak-before">You also need to pay attention to the <em>test statistic</em> or metric you use to compare group A to group B.<a data-type="indexterm" data-primary="test statistic" id="idm46522861295192"/>
Perhaps the most common metric in data science is a binary variable: click or no-click, buy or don’t buy, fraud or no fraud, and so on.  Those results would be summed up in a 2×2 table.  <a data-type="xref" href="#two_by_two">Table 3-1</a> is a 2×2 table for an actual price test (see <a data-type="xref" href="#Significance">“Statistical Significance and p-Values”</a> for further discussion of these results).</p>
<table id="two_by_two">
<caption><span class="label">Table 3-1. </span>2×2 table for ecommerce experiment results</caption>
<thead>
<tr>
<th>Outcome</th>
<th>Price A</th>
<th>Price B</th>
</tr>
</thead>
<tbody>
<tr>
<td><p>Conversion</p></td>
<td><p>200</p></td>
<td><p>182</p></td>
</tr>
<tr>
<td><p>No conversion</p></td>
<td><p>23,539</p></td>
<td><p>22,406</p></td>
</tr>
</tbody>
</table>

<p>If the metric is a continuous variable (purchase amount, profit, etc.) or a count (e.g., days in hospital, pages visited), the result might be displayed differently.  If one were interested not in conversion but in revenue per page view, the results of the price test in <a data-type="xref" href="#two_by_two">Table 3-1</a> might look like this in typical default software output:</p>
<blockquote>
<p>Revenue/page view with price A:  mean = 3.87, SD = 51.10</p>

<p>Revenue/page view with price B:  mean = 4.11, SD = 62.98</p></blockquote>

<p>“SD” refers to the <a data-type="indexterm" data-primary="standard deviation" data-secondary="in A/B testing" id="idm46522861280936"/>standard deviation of the values within each group.</p>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>Just because statistical software—including <em>R</em> and <em>Python</em>—generates output by default does not mean that all the output is useful or relevant.
You can see that the preceding standard deviations are not that useful; on their face they suggest that numerous values might be negative, when negative revenue is not feasible. This data consists of a small set of relatively high values (page views with conversions) and a huge number of 0-values (page views with no conversion).
It is difficult to sum up the variability of such data with a single number, though the mean absolute deviation from the mean (7.68 for A and 8.15 for B) is more reasonable than the standard deviation.</p>
</div>








<section data-type="sect2" data-pdf-bookmark="Why Have a Control Group?"><div class="sect2" id="idm46522861276936">
<h2>Why Have a Control Group?</h2>

<p>Why not skip the control group and just run an experiment applying the treatment of interest to only one group, and compare the outcome to prior experience?<a data-type="indexterm" data-primary="A/B testing" data-secondary="benefits of using a control group" id="idm46522861275176"/><a data-type="indexterm" data-primary="control group" data-secondary="benefits of using" id="idm46522861274136"/></p>

<p>Without a control group, there is no assurance that “all other things are equal” and that any difference is really due to the treatment (or to chance).
When you have a control group, it is subject to the same conditions (except for the treatment of <span class="keep-together">interest</span>) as the treatment group.
If you simply make a comparison to “baseline” or prior experience, other factors, besides the treatment, might differ.</p>
<div data-type="note" epub:type="note"><h1>Blinding in studies</h1>
<p>A <em>blind study</em> is one in which the subjects are unaware of whether they are getting treatment A or treatment B.<a data-type="indexterm" data-primary="blind studies" id="idm46522861269832"/>
Awareness of receiving a particular treatment can affect response.
A <em>double-blind</em> study is one in which the investigators and facilitators (e.g., doctors and nurses in a medical study) also are unaware which subjects are getting which treatment.<a data-type="indexterm" data-primary="double blind studies" id="idm46522861268360"/>
Blinding is not possible when the nature of the treatment is transparent—for example, cognitive therapy from a computer versus a psychologist.</p>
</div>

<p>A/B testing in data science is typically used in a web context.<a data-type="indexterm" data-primary="web testing" data-secondary="A/B testing in data science" id="idm46522861266856"/>
Treatments might be the design of a web page, the price of a product, the wording of a headline, or some other item.<a data-type="indexterm" data-primary="data science" data-secondary="A/B testing in" id="idm46522861265528"/> Some thought is required to preserve the principles of randomization.
Typically the subject in the experiment is the web visitor, and the outcomes we are interested in measuring are clicks, purchases, visit duration, number of pages visited, whether a particular page is visited, and the like.
In a standard A/B experiment, you need to decide on one metric ahead of time.
Multiple behavior metrics might be collected and be of interest, but if the experiment is expected to lead to a decision between treatment A and treatment B, a single metric, or <em>test statistic</em>, needs to be established beforehand.
Selecting a test statistic <em>after</em> the experiment is conducted opens the door to researcher bias.<a data-type="indexterm" data-primary="test statistic" id="idm46522861263016"/></p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Why Just A/B?  Why Not C, D,…?"><div class="sect2" id="idm46522861262104">
<h2>Why Just A/B?  Why Not C, D,…?</h2>

<p>A/B tests are popular in the marketing and ecommerce worlds, but are far from the only type of statistical experiment.
Additional treatments can be included.
Subjects might have repeated measurements taken.
Pharmaceutical trials where subjects are scarce, expensive, and acquired over time are sometimes designed with multiple opportunities to stop the experiment and reach a conclusion.<a data-type="indexterm" data-primary="A/B testing" data-secondary="traditional, shortcoming of" id="idm46522861260360"/></p>

<p>Traditional statistical experimental designs focus on answering a static question about the efficacy of specified treatments.
Data scientists are less interested in the question:</p>
<blockquote>
<p>Is the difference between price A and price B statistically significant?</p></blockquote>

<p>than in the question:</p>
<blockquote>
<p>Which, out of multiple possible prices, is best?</p></blockquote>

<p>For this, a relatively new type of experimental <a data-type="indexterm" data-primary="multi-arm bandits" id="idm46522861256344"/>design is used: the <em>multi-arm bandit</em> (see <a data-type="xref" href="#bandits">“Multi-Arm Bandit Algorithm”</a>).</p>
<div data-type="warning" epub:type="warning"><h1>Getting Permission</h1>
<p>In scientific and medical research involving human subjects, it is typically necessary to get their permission, as well as obtain the approval of an institutional review board.<a data-type="indexterm" data-primary="A/B testing" data-secondary="importance of obtaining permissions" id="idm46522861252744"/><a data-type="indexterm" data-primary="permissions for scientific and medical testing" id="idm46522861251752"/>
Experiments in business that are done as a part of ongoing operations almost never do this.
In most cases (e.g., pricing experiments, or experiments about which headline to show or which offer should be made), this practice is widely accepted.
Facebook, however, ran afoul of this general acceptance in 2014 when it experimented with the emotional tone in users’ newsfeeds.
Facebook used sentiment analysis to classify newsfeed posts as positive or negative, and then altered the positive/negative balance in what it showed users.
Some randomly selected users experienced more positive posts, while others experienced more negative posts.
Facebook found that the users who experienced a more positive newsfeed were more likely to post positively themselves, and vice versa.
The magnitude of the effect was small, however, and Facebook faced much criticism for conducting the experiment without users’ knowledge.
Some users speculated that Facebook might have pushed some extremely depressed users over the edge if they got the negative version of their feed.</p>
</div>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46522861249528">
<h5>Key Ideas</h5>
<ul>
<li>
<p>Subjects are assigned to two (or more) groups that are treated exactly alike, except that the treatment under study differs from one group to another.</p>
</li>
<li>
<p>Ideally, subjects are assigned randomly to the groups.</p>
</li>
</ul>
</div></aside>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Further Reading"><div class="sect2" id="idm46522861246056">
<h2>Further Reading</h2>

<ul>
<li>
<p>Two-group comparisons (A/B tests) are a staple of traditional statistics, and just about any introductory statistics text will have extensive coverage of design principles and inference procedures.  For a discussion that places A/B tests in more of a data science context and uses resampling, see <em>Introductory Statistics and Analytics: A Resampling Perspective</em> by Peter Bruce (Wiley, 2014).</p>
</li>
<li>
<p>For web testing, the logistical aspects of testing can be just as challenging as the statistical ones.  A good place to start is the <a href="https://oreil.ly/mAbqF">Google Analytics help section on experiments</a>.</p>
</li>
<li>
<p>Beware advice found in the ubiquitous guides to A/B testing that you see on the web, such as these words in one such guide: “Wait for about 1,000 total visitors and make sure you run the test for a week.”  Such general rules of thumb are not statistically meaningful; see <a data-type="xref" href="#PowerSampleSize">“Power and Sample Size”</a> for more detail.<a data-type="indexterm" data-primary="statistical experiments and significance testing" data-secondary="A/B testing" data-startref="ix_statexpAB" id="idm46522861239560"/><a data-type="indexterm" data-primary="A/B testing" data-startref="ix_ABtst" id="idm46522861238216"/></p>
</li>
</ul>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Hypothesis Tests"><div class="sect1" id="Hypothesis_test">
<h1>Hypothesis Tests</h1>

<p>Hypothesis tests, also called <em>significance tests</em>, are ubiquitous in the traditional statistical analysis of published research.<a data-type="indexterm" data-primary="statistical experiments and significance testing" data-secondary="hypothesis tests" id="ix_statexphyp"/><a data-type="indexterm" data-primary="hypothesis tests" id="ix_hyptst"/><a data-type="indexterm" data-primary="significance tests" data-seealso="hypothesis tests" id="idm46522861232344"/>
Their purpose is to help you learn whether random chance might be responsible for an observed effect.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46522861231032">
<h5>Key Terms for Hypothesis Tests</h5><dl>
<dt class="horizontal"><strong><em>Null hypothesis</em></strong></dt>
<dd>
<p>The hypothesis that chance is to blame.<a data-type="indexterm" data-primary="null hypothesis" id="idm46522861228040"/></p>
</dd>
<dt class="horizontal"><strong><em>Alternative hypothesis</em></strong></dt>
<dd>
<p>Counterpoint to the null (what you hope to prove).<a data-type="indexterm" data-primary="alternative hypothesis" id="idm46522861225528"/></p>
</dd>
<dt class="horizontal"><strong><em>One-way test</em></strong></dt>
<dd>
<p>Hypothesis test that counts chance results only in one direction.<a data-type="indexterm" data-primary="one-way tests" id="idm46522861223016"/></p>
</dd>
<dt class="horizontal"><strong><em>Two-way test</em></strong></dt>
<dd>
<p>Hypothesis test that counts chance results in two directions.<a data-type="indexterm" data-primary="two-way tests" id="idm46522861220504"/></p>
</dd>
</dl>
</div></aside>

<p>An A/B test (see <a data-type="xref" href="#A-B-test">“A/B Testing”</a>) is typically constructed with a hypothesis in mind.<a data-type="indexterm" data-primary="A/B testing" data-secondary="hypotheses in" id="idm46522861218152"/>
For example, the hypothesis might be that price B produces higher profit.
Why do we need a hypothesis?
Why not just look at the outcome of the experiment and go with whichever treatment does better?</p>

<p>The answer lies in the tendency of the human mind to underestimate the scope of natural random behavior.<a data-type="indexterm" data-primary="randomness, underestimating and misinterpreting" id="idm46522861216360"/>
One manifestation of this is the failure to anticipate extreme events, or so-called “black swans” (see <a data-type="xref" href="ch02.xhtml#LongTailedData">“Long-Tailed Distributions”</a>).
Another manifestation is the tendency to misinterpret random events as having patterns of some significance.<a data-type="indexterm" data-primary="significance tests" data-secondary="underestimating and misinterpreting random events in" id="idm46522861214376"/><a data-type="indexterm" data-primary="hypothesis tests" data-secondary="misinterpreting randomness" id="idm46522861213400"/>
Statistical hypothesis testing was invented as a way to protect researchers from being fooled by random chance.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46522861212056">
<h5>Misinterpreting Randomness</h5>
<p>You can observe the human tendency to underestimate randomness in this experiment.  Ask several friends to invent a series of 50 coin flips: have them write down a series of random Hs and Ts.  Then ask them to actually flip a coin 50 times and write down the results.  Have them put the real coin flip results in one pile, and the made-up results in another.  It is easy to tell which results are real: the real ones will have longer runs of Hs or Ts.  In a set of 50 <em>real</em> coin flips, it is not at all unusual to see five or six Hs or Ts in a row.  However, when most of us are inventing random coin flips and we have gotten three or four Hs in a row, we tell ourselves that, for the series to look random, we had better switch to T.</p>

<p>The other side of this coin, so to speak, is that when we <em>do</em> see the real-world equivalent of six Hs in a row (e.g., when one headline outperforms another by 10%), we are inclined to attribute it to something real, not just to chance.</p>
</div></aside>

<p>In a properly designed A/B test, you collect data on treatments A and B in such a way that any observed difference between A and B must be due to either:</p>

<ul>
<li>
<p>Random chance in assignment of subjects</p>
</li>
<li>
<p>A true difference between A and B</p>
</li>
</ul>

<p>A statistical hypothesis test is further analysis of an A/B test, or any randomized experiment, to assess whether random chance is a reasonable explanation for the observed difference between groups A and B.</p>








<section data-type="sect2" data-pdf-bookmark="The Null Hypothesis"><div class="sect2" id="Null_hypothesis">
<h2>The Null Hypothesis</h2>

<p>Hypothesis tests use the following logic:  “Given the human tendency to react to unusual but <a data-type="indexterm" data-primary="null hypothesis" id="idm46522861203048"/><a data-type="indexterm" data-primary="hypothesis tests" data-secondary="null hypothesis" id="idm46522861202344"/>random behavior and interpret it as something meaningful and real, in our experiments we will require proof that the difference between groups is more extreme than what chance might reasonably produce.”
This involves a baseline assumption that the treatments are equivalent, and any difference between the groups is due to chance.  This baseline assumption is termed the <em>null hypothesis</em>.
Our hope, then, is that we can in fact prove the null hypothesis <em>wrong</em> and show that the outcomes for groups A and B are more different than what chance might produce.</p>

<p>One way to do this is via a resampling permutation procedure, in which we shuffle together the results from groups A and B and then repeatedly deal out the data in groups of similar sizes, and then observe how often we get a difference as extreme as the observed difference. The combined shuffled results from groups A and B, and the procedure of resampling from them, embodies the null hypothesis of groups A and B being equivalent and interchangeable and is termed the null model.      See <a data-type="xref" href="#Resampling">“Resampling”</a> for more detail.</p>
</div></section>













<section data-type="sect2" class="pagebreak-before less_space" data-pdf-bookmark="Alternative Hypothesis"><div class="sect2" id="idm46522861197880">
<h2>Alternative Hypothesis</h2>

<p>Hypothesis tests by their nature involve not just a null hypothesis but also an offsetting alternative hypothesis.<a data-type="indexterm" data-primary="null hypothesis" data-secondary="using alternative hypothesis with" id="idm46522861195912"/><a data-type="indexterm" data-primary="alternative hypothesis" id="idm46522861194920"/><a data-type="indexterm" data-primary="hypothesis tests" data-secondary="alternative hypothesis" id="idm46522861194248"/>  Here are some examples:</p>

<ul>
<li>
<p>Null = “no difference between the means of group A and group B”; alternative = “A is different from B” (could be bigger or smaller)</p>
</li>
<li>
<p>Null = “A <math alttext="less-than-or-equal-to">
  <mo>≤</mo>
</math> B”; alternative = “A &gt; B”</p>
</li>
<li>
<p>Null = “B is not X% greater than A”; alternative = “B is X% greater than A”</p>
</li>
</ul>

<p>Taken together, the null and alternative hypotheses must account for all possibilities.  The nature of the null hypothesis determines the structure of the hypothesis test.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="One-Way Versus Two-Way Hypothesis Tests"><div class="sect2" id="directional">
<h2>One-Way Versus Two-Way Hypothesis Tests</h2>

<p>Often in an A/B test, you are testing a new option (say, B) against an established default option (A), and <a data-type="indexterm" data-primary="hypothesis tests" data-secondary="one-way and two-way tests" id="idm46522861186072"/><a data-type="indexterm" data-primary="one-way tests" id="idm46522861185032"/>the presumption is that you will stick with the default option unless the new option proves itself definitively better.
In such a case, you want a hypothesis test to protect you from being fooled by chance in the direction favoring B.
You don’t care about being fooled by chance in the other direction, because you would be sticking with A unless B proves definitively better.
So you want a <em>directional</em> alternative hypothesis (B is better than A).
In such a case, you use a <em>one-way</em> (or one-tail) hypothesis test.
This means that extreme chance results in only one direction count toward the p-value.</p>

<p>If you want a hypothesis test to protect you from being fooled by chance in either direction, the alternative hypothesis is <em>bidirectional</em> (A is different from B; could be bigger or smaller).  In such a case, you use a <em>two-way</em> (or two-tail) hypothesis.<a data-type="indexterm" data-primary="bidirectional alternative hypothesis" id="idm46522861181496"/><a data-type="indexterm" data-primary="two-way tests" id="idm46522861180728"/>
This means that extreme chance results in either direction count toward the p-value.</p>

<p>A one-tail hypothesis test often fits the nature of A/B decision making, in which a decision is required and one option is typically assigned “default” status unless the other proves better.
Software, however, including <em>R</em> and <code>scipy</code> in <em>Python</em>, typically provides a two-tail test in its default output, and many statisticians opt for the more conservative two-tail test just to avoid argument.
One-tail versus two-tail is a confusing subject, and not that relevant for data science, where the precision of p-value calculations is not terribly important.</p>
<aside data-type="sidebar" epub:type="sidebar" class="pagebreak-before less_space"><div class="sidebar" id="idm46522861177544">
<h5>Key Ideas</h5>
<ul>
<li>
<p>A null hypothesis is a logical construct embodying the notion that nothing special has happened, and any effect you observe is due to random chance.</p>
</li>
<li>
<p>The hypothesis test assumes that the null hypothesis is true, creates a “null model” (a probability model), and tests whether the effect you observe is a reasonable outcome of that model.</p>
</li>
</ul>
</div></aside>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Further Reading"><div class="sect2" id="idm46522861173544">
<h2>Further Reading</h2>

<ul>
<li>
<p><em>The Drunkard’s Walk</em> by Leonard Mlodinow (Pantheon, 2008) is a readable survey of the ways in which “randomness rules our lives.”</p>
</li>
<li>
<p>David Freedman, Robert Pisani, and Roger Purves’s classic statistics text <em>Statistics</em>, 4th ed. (W. W. Norton, 2007), has excellent nonmathematical treatments of most statistics topics, including hypothesis testing.</p>
</li>
<li>
<p><em>Introductory Statistics and Analytics: A Resampling Perspective</em> by Peter Bruce (Wiley, 2014) develops hypothesis testing concepts using resampling.<a data-type="indexterm" data-primary="statistical experiments and significance testing" data-secondary="hypothesis tests" data-startref="ix_statexphyp" id="idm46522861168696"/><a data-type="indexterm" data-primary="hypothesis tests" data-startref="ix_hyptst" id="idm46522861167304"/></p>
</li>
</ul>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Resampling"><div class="sect1" id="Resampling">
<h1>Resampling</h1>

<p><em>Resampling</em> in statistics means to repeatedly sample values from observed data, with a general goal of assessing random variability in a statistic.<a data-type="indexterm" data-primary="statistical experiments and significance testing" data-secondary="resampling" id="ix_statexpres"/><a data-type="indexterm" data-primary="resampling" id="ix_resmpl"/>
It can also<a data-type="indexterm" data-primary="bagging" id="idm46522861161048"/><a data-type="indexterm" data-primary="machine learning" data-secondary="use of resampling to improve models" id="idm46522861160344"/> be used to assess and improve the accuracy of some machine-learning models (e.g., the predictions from decision tree models built on multiple bootstrapped data sets can be averaged in a process known as <em>bagging</em>—see <a data-type="xref" href="ch06.xhtml#Bagging">“Bagging and the Random Forest”</a>).</p>

<p>There are two main types of resampling procedures: the <em>bootstrap</em> and <em>permutation</em> tests.<a data-type="indexterm" data-primary="bootstrap" data-secondary="in resampling" data-secondary-sortas="resampling" id="idm46522861156456"/><a data-type="indexterm" data-primary="permutation tests" id="idm46522861155176"/><a data-type="indexterm" data-primary="resampling" data-secondary="bootstrap and permutation tests" id="idm46522861154504"/> The bootstrap is used to assess the reliability of an estimate; it was discussed in the previous chapter (see <a data-type="xref" href="ch02.xhtml#bootstrap">“The Bootstrap”</a>).
Permutation tests are used to test hypotheses, typically involving two or more groups, and we discuss those in this section.</p>
<aside data-type="sidebar" epub:type="sidebar" class="pagebreak-before less_space"><div class="sidebar" id="idm46522861152392">
<h5>Key Terms for Resampling</h5><dl>
<dt class="horizontal"><strong><em>Permutation test</em></strong></dt>
<dd>
<p>The procedure of combining two or more samples together and randomly (or exhaustively) reallocating the observations to resamples.</p>
<dl>
<dt><em>Synonyms</em></dt>
<dd>
<p>Randomization test, random permutation test, exact test</p>
</dd>
</dl>
</dd>
<dt class="horizontal"><strong><em>Resampling</em></strong></dt>
<dd>
<p>Drawing additional samples (“resamples”) from an observed data set.<a data-type="indexterm" data-primary="sampling" data-secondary="with and without replacement" data-secondary-sortas="replacement" id="idm46522861145224"/><a data-type="indexterm" data-primary="replacement (in sampling)" id="idm46522861143960"/></p>
</dd>
<dt class="horizontal"><strong><em>With or without replacement</em></strong></dt>
<dd>
<p>In sampling, whether or not an item is returned to the sample before the next draw.</p>
</dd>
</dl>
</div></aside>








<section data-type="sect2" data-pdf-bookmark="Permutation Test"><div class="sect2" id="Permutation">
<h2>Permutation Test</h2>

<p>In a <em>permutation</em> procedure, two or more samples are involved, typically the groups in an A/B or other hypothesis test.<a data-type="indexterm" data-primary="resampling" data-secondary="permutation tests" id="ix_resmplprm"/><a data-type="indexterm" data-primary="permutation tests" id="ix_permtst"/>
<em>Permute</em> means to change the order of a set of values.
The first step in a <em>permutation test</em> of a hypothesis is to combine the results from groups A and B (and, if used, C, D,…).
This is the logical embodiment of the null hypothesis that the treatments to which the groups were exposed do not differ.
We then test that hypothesis by randomly drawing groups from this combined set and seeing how much they differ from one another.
The permutation procedure is as <span class="keep-together">follows</span>:</p>
<ol>
<li>
<p>Combine the results from the different groups into a single data set.</p>
</li>
<li>
<p>Shuffle the combined data and then randomly draw (without replacement) a resample of the same size as group A (clearly it will contain some data from the other groups).</p>
</li>
<li>
<p>From the remaining data, randomly draw (without replacement) a resample of the same size as group B.</p>
</li>
<li>
<p>Do the same for groups C, D, and so on.  You have now collected one set of resamples that mirror the sizes of the original samples.</p>
</li>
<li>
<p>Whatever statistic or estimate was calculated for the original samples (e.g., difference in group proportions), calculate it now for the resamples, and record; this constitutes one permutation iteration.</p>
</li>
<li>
<p>Repeat the previous steps <em>R</em> times to yield a permutation distribution of the test statistic.</p>
</li>

</ol>

<p>Now go back to the observed difference between groups and compare it to the set of permuted differences.
If the observed difference lies well within the set of permuted differences, then we have not proven anything—the observed difference is within the range of what chance might produce.
However, if the observed difference<a data-type="indexterm" data-primary="statistical significance" id="idm46522861126616"/> lies outside most of the permutation distribution, then we conclude that chance is <em>not</em> responsible.
In technical terms, the difference is <em>statistically significant</em>. (See <a data-type="xref" href="#Significance">“Statistical Significance and p-Values”</a>.)</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Example: Web Stickiness"><div class="sect2" id="PermutationExample">
<h2>Example: Web Stickiness</h2>

<p>A company selling a relatively high-value service wants to test which of two web presentations does a better selling job.<a data-type="indexterm" data-primary="resampling" data-secondary="permutation tests" data-tertiary="web stickiness example" id="ix_resmplprmex"/><a data-type="indexterm" data-primary="web testing" data-secondary="web stickiness example" id="ix_webtstex"/><a data-type="indexterm" data-primary="permutation tests" data-secondary="web stickiness example" id="ix_permtstex"/>
Due to the high value of the service being sold, sales are infrequent and the sales cycle is lengthy; it would take too long to accumulate enough sales to know which presentation is superior.
So the company decides to measure the results with a proxy variable, using the detailed interior page that describes the service.</p>
<div data-type="tip"><h6>Tip</h6>
<p>A <em>proxy</em> variable is one that stands in for the true variable of interest, which may be unavailable, too costly, or too time-consuming to measure.
In climate research, for example, the oxygen content of ancient ice cores is used as a proxy for temperature.<a data-type="indexterm" data-primary="proxy variables" id="idm46522861115384"/>
It is useful to have at least <em>some</em> data on the true variable of interest, so the strength of its association with the proxy can be assessed.</p>
</div>

<p>One potential proxy variable for our company is the number of clicks on the detailed landing page.
A better one is how long people spend on the page.
It is reasonable to think that a web presentation (page) that holds people’s attention longer will lead to more sales.
Hence, our metric is average session time, comparing page A to page B.</p>

<p>Due to the fact that this is an interior, special-purpose page, it does not receive a huge number of visitors.
Also note that Google Analytics, which is how we measure session time, cannot measure session time for the last session a person visits.<a data-type="indexterm" data-primary="Google Analytics" id="idm46522861112472"/>
Instead of deleting that session from the data, though, Google Analytics records it as a zero, so the data requires additional processing to remove those sessions.
The result is a total of 36 sessions for the two different presentations, 21 for page A and 15 for page B.
Using <code>ggplot</code>, we can visually compare the session times using side-by-side boxplots:</p>

<pre data-type="programlisting" data-code-language="r"><code class="nf">ggplot</code><code class="p">(</code><code class="n">session_times</code><code class="p">,</code> <code class="nf">aes</code><code class="p">(</code><code class="n">x</code><code class="o">=</code><code class="n">Page</code><code class="p">,</code> <code class="n">y</code><code class="o">=</code><code class="n">Time</code><code class="p">))</code> <code class="o">+</code>
  <code class="nf">geom_boxplot</code><code class="p">()</code></pre>

<p class="pagebreak-before">The <code>pandas</code> <code>boxplot</code> command uses the keyword argument <code>by</code> to create the figure:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">ax</code> <code class="o">=</code> <code class="n">session_times</code><code class="o">.</code><code class="n">boxplot</code><code class="p">(</code><code class="n">by</code><code class="o">=</code><code class="s1">'Page'</code><code class="p">,</code> <code class="n">column</code><code class="o">=</code><code class="s1">'Time'</code><code class="p">)</code>
<code class="n">ax</code><code class="o">.</code><code class="n">set_xlabel</code><code class="p">(</code><code class="s1">''</code><code class="p">)</code>
<code class="n">ax</code><code class="o">.</code><code class="n">set_ylabel</code><code class="p">(</code><code class="s1">'Time (in seconds)'</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">suptitle</code><code class="p">(</code><code class="s1">''</code><code class="p">)</code></pre>

<p>The boxplot, shown in <a data-type="xref" href="#SessionTimesBoxplot">Figure 3-3</a>, indicates that page B leads to longer sessions than page A.
The means for each group can be computed in <em>R</em> as follows:</p>

<pre data-type="programlisting" data-code-language="r"><code class="n">mean_a</code> <code class="o">&lt;-</code> <code class="nf">mean</code><code class="p">(</code><code class="n">session_times</code><code class="p">[</code><code class="n">session_times</code><code class="p">[</code><code class="s">'Page'</code><code class="p">]</code> <code class="o">==</code> <code class="s">'Page A'</code><code class="p">,</code> <code class="s">'Time'</code><code class="p">])</code>
<code class="n">mean_b</code> <code class="o">&lt;-</code> <code class="nf">mean</code><code class="p">(</code><code class="n">session_times</code><code class="p">[</code><code class="n">session_times</code><code class="p">[</code><code class="s">'Page'</code><code class="p">]</code> <code class="o">==</code> <code class="s">'Page B'</code><code class="p">,</code> <code class="s">'Time'</code><code class="p">])</code>
<code class="n">mean_b</code> <code class="o">-</code> <code class="n">mean_a</code>
<code class="p">[</code><code class="m">1</code><code class="p">]</code> <code class="m">35.66667</code></pre>

<p>In <em>Python</em>, we filter the <code>pandas</code> data frame first by page and then determine the mean of the <code>Time</code> column:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">mean_a</code> <code class="o">=</code> <code class="n">session_times</code><code class="p">[</code><code class="n">session_times</code><code class="o">.</code><code class="n">Page</code> <code class="o">==</code> <code class="s1">'Page A'</code><code class="p">]</code><code class="o">.</code><code class="n">Time</code><code class="o">.</code><code class="n">mean</code><code class="p">()</code>
<code class="n">mean_b</code> <code class="o">=</code> <code class="n">session_times</code><code class="p">[</code><code class="n">session_times</code><code class="o">.</code><code class="n">Page</code> <code class="o">==</code> <code class="s1">'Page B'</code><code class="p">]</code><code class="o">.</code><code class="n">Time</code><code class="o">.</code><code class="n">mean</code><code class="p">()</code>
<code class="n">mean_b</code> <code class="o">-</code> <code class="n">mean_a</code></pre>

<p>Page B has session times that are greater than those of page A by 35.67 seconds, on average.
The question is whether this difference is within the range of what random chance might produce, i.e., is statistically significant.
One way to answer this is to apply a permutation test—combine all the session times together and then repeatedly shuffle and divide them into groups of 21 (recall that <math alttext="n Subscript upper A Baseline equals 21">
  <mrow>
    <msub><mi>n</mi> <mi>A</mi> </msub>
    <mo>=</mo>
    <mn>21</mn>
  </mrow>
</math> for page A) and 15 (<math alttext="n Subscript upper B Baseline equals 15">
  <mrow>
    <msub><mi>n</mi> <mi>B</mi> </msub>
    <mo>=</mo>
    <mn>15</mn>
  </mrow>
</math> for page B).</p>

<p>To apply a permutation test, we need a function to randomly assign the 36 session times to a group of 21 (page A) and a group of 15 (page B). The <em>R</em> version of this function is:</p>

<pre data-type="programlisting" data-code-language="r"><code class="n">perm_fun</code> <code class="o">&lt;-</code> <code class="nf">function</code><code class="p">(</code><code class="n">x</code><code class="p">,</code> <code class="n">nA</code><code class="p">,</code> <code class="n">nB</code><code class="p">)</code>
<code class="p">{</code>
  <code class="n">n</code> <code class="o">&lt;-</code> <code class="n">nA</code> <code class="o">+</code> <code class="n">nB</code>
  <code class="n">idx_b</code> <code class="o">&lt;-</code> <code class="nf">sample</code><code class="p">(</code><code class="m">1</code><code class="o">:</code><code class="n">n</code><code class="p">,</code> <code class="n">nB</code><code class="p">)</code>
  <code class="n">idx_a</code> <code class="o">&lt;-</code> <code class="nf">setdiff</code><code class="p">(</code><code class="m">1</code><code class="o">:</code><code class="n">n</code><code class="p">,</code> <code class="n">idx_b</code><code class="p">)</code>
  <code class="n">mean_diff</code> <code class="o">&lt;-</code> <code class="nf">mean</code><code class="p">(</code><code class="n">x</code><code class="p">[</code><code class="n">idx_b</code><code class="p">])</code> <code class="o">-</code> <code class="nf">mean</code><code class="p">(</code><code class="n">x</code><code class="p">[</code><code class="n">idx_a</code><code class="p">])</code>
  <code class="nf">return</code><code class="p">(</code><code class="n">mean_diff</code><code class="p">)</code>
<code class="p">}</code></pre>

<p>The <em>Python</em> version of this permutation test is the following:</p>

<pre data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">perm_fun</code><code class="p">(</code><code class="n">x</code><code class="p">,</code> <code class="n">nA</code><code class="p">,</code> <code class="n">nB</code><code class="p">):</code>
    <code class="n">n</code> <code class="o">=</code> <code class="n">nA</code> <code class="o">+</code> <code class="n">nB</code>
    <code class="n">idx_B</code> <code class="o">=</code> <code class="nb">set</code><code class="p">(</code><code class="n">random</code><code class="o">.</code><code class="n">sample</code><code class="p">(</code><code class="nb">range</code><code class="p">(</code><code class="n">n</code><code class="p">),</code> <code class="n">nB</code><code class="p">))</code>
    <code class="n">idx_A</code> <code class="o">=</code> <code class="nb">set</code><code class="p">(</code><code class="nb">range</code><code class="p">(</code><code class="n">n</code><code class="p">))</code> <code class="o">-</code> <code class="n">idx_B</code>
    <code class="k">return</code> <code class="n">x</code><code class="o">.</code><code class="n">loc</code><code class="p">[</code><code class="n">idx_B</code><code class="p">]</code><code class="o">.</code><code class="n">mean</code><code class="p">()</code> <code class="o">-</code> <code class="n">x</code><code class="o">.</code><code class="n">loc</code><code class="p">[</code><code class="n">idx_A</code><code class="p">]</code><code class="o">.</code><code class="n">mean</code><code class="p">()</code></pre>

<figure class="width-75"><div id="SessionTimesBoxplot" class="figure">
<img src="Images/psd2_0303.png" alt="Session times for different presentations of a web page." width="1157" height="1156"/>
<h6><span class="label">Figure 3-3. </span>Session times for web pages A and B</h6>
</div></figure>

<p>This function works by sampling (without replacement) <math alttext="n Subscript upper B">
  <msub><mi>n</mi> <mi>B</mi> </msub>
</math> indices and assigning them to the B group; the remaining <math alttext="n Subscript upper A">
  <msub><mi>n</mi> <mi>A</mi> </msub>
</math> indices are assigned to group A. The difference between the two means is returned.
Calling this function <em>R</em> = 1,000 times and specifying <math alttext="n Subscript upper A Baseline equals 21">
  <mrow>
    <msub><mi>n</mi> <mi>A</mi> </msub>
    <mo>=</mo>
    <mn>21</mn>
  </mrow>
</math> and <math alttext="n Subscript upper B Baseline equals 15">
  <mrow>
    <msub><mi>n</mi> <mi>B</mi> </msub>
    <mo>=</mo>
    <mn>15</mn>
  </mrow>
</math> leads to a distribution of differences in the session times that can be plotted as a histogram. In <em>R</em> this is done as follows using the <code>hist</code> function:</p>

<pre data-type="programlisting" data-code-language="r"><code class="n">perm_diffs</code> <code class="o">&lt;-</code> <code class="nf">rep</code><code class="p">(</code><code class="m">0</code><code class="p">,</code> <code class="m">1000</code><code class="p">)</code>
<code class="nf">for </code><code class="p">(</code><code class="n">i</code> <code class="n">in</code> <code class="m">1</code><code class="o">:</code><code class="m">1000</code><code class="p">)</code> <code class="p">{</code>
  <code class="n">perm_diffs</code><code class="p">[</code><code class="n">i</code><code class="p">]</code> <code class="o">=</code> <code class="nf">perm_fun</code><code class="p">(</code><code class="n">session_times</code><code class="p">[,</code> <code class="s">'Time'</code><code class="p">],</code> <code class="m">21</code><code class="p">,</code> <code class="m">15</code><code class="p">)</code>
<code class="p">}</code>
<code class="nf">hist</code><code class="p">(</code><code class="n">perm_diffs</code><code class="p">,</code> <code class="n">xlab</code><code class="o">=</code><code class="s">'Session time differences (in seconds)'</code><code class="p">)</code>
<code class="nf">abline</code><code class="p">(</code><code class="n">v</code><code class="o">=</code><code class="n">mean_b</code> <code class="o">-</code> <code class="n">mean_a</code><code class="p">)</code></pre>

<p>In <em>Python</em>, we can create a similar graph using <code>matplotlib</code>:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">perm_diffs</code> <code class="o">=</code> <code class="p">[</code><code class="n">perm_fun</code><code class="p">(</code><code class="n">session_times</code><code class="o">.</code><code class="n">Time</code><code class="p">,</code> <code class="n">nA</code><code class="p">,</code> <code class="n">nB</code><code class="p">)</code> <code class="k">for</code> <code class="n">_</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">1000</code><code class="p">)]</code>

<code class="n">fig</code><code class="p">,</code> <code class="n">ax</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">(</code><code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">5</code><code class="p">,</code> <code class="mi">5</code><code class="p">))</code>
<code class="n">ax</code><code class="o">.</code><code class="n">hist</code><code class="p">(</code><code class="n">perm_diffs</code><code class="p">,</code> <code class="n">bins</code><code class="o">=</code><code class="mi">11</code><code class="p">,</code> <code class="n">rwidth</code><code class="o">=</code><code class="mf">0.9</code><code class="p">)</code>
<code class="n">ax</code><code class="o">.</code><code class="n">axvline</code><code class="p">(</code><code class="n">x</code> <code class="o">=</code> <code class="n">mean_b</code> <code class="o">-</code> <code class="n">mean_a</code><code class="p">,</code> <code class="n">color</code><code class="o">=</code><code class="s1">'black'</code><code class="p">,</code> <code class="n">lw</code><code class="o">=</code><code class="mi">2</code><code class="p">)</code>
<code class="n">ax</code><code class="o">.</code><code class="n">text</code><code class="p">(</code><code class="mi">50</code><code class="p">,</code> <code class="mi">190</code><code class="p">,</code> <code class="s1">'Observed</code><code class="se">\n</code><code class="s1">difference'</code><code class="p">,</code> <code class="n">bbox</code><code class="o">=</code><code class="p">{</code><code class="s1">'facecolor'</code><code class="p">:</code><code class="s1">'white'</code><code class="p">})</code>
<code class="n">ax</code><code class="o">.</code><code class="n">set_xlabel</code><code class="p">(</code><code class="s1">'Session time differences (in seconds)'</code><code class="p">)</code>
<code class="n">ax</code><code class="o">.</code><code class="n">set_ylabel</code><code class="p">(</code><code class="s1">'Frequency'</code><code class="p">)</code></pre>

<p>The histogram, in <a data-type="xref" href="#SessionTimesPerm">Figure 3-4</a> shows that mean difference of random permutations often exceeds the observed difference in session times (the vertical line). For our results, this happens in 12.6% of the cases:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">mean</code><code class="p">(</code><code class="n">perm_diffs</code> <code class="o">&gt;</code> <code class="p">(</code><code class="n">mean_b</code> <code class="o">-</code> <code class="n">mean_a</code><code class="p">))</code>
<code class="o">---</code>
<code class="mf">0.126</code></pre>

<p>As the simulation uses random numbers, the percentage will vary. For example, in the <em>Python</em> version, we got 12.1%:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">np</code><code class="o">.</code><code class="n">mean</code><code class="p">(</code><code class="n">perm_diffs</code> <code class="o">&gt;</code> <code class="n">mean_b</code> <code class="o">-</code> <code class="n">mean_a</code><code class="p">)</code>
<code class="o">---</code>
<code class="mf">0.121</code></pre>

<p>This suggests that the observed difference in session time between page A and page B is well within the range of chance variation and thus is not statistically significant.</p>

<figure class="width-75"><div id="SessionTimesPerm" class="figure">
<img src="Images/psd2_0304.png" alt="Histogram of the difference in session times from the permutation procedure." width="1180" height="1123"/>
<h6><span class="label">Figure 3-4. </span>Frequency distribution for session time differences between pages A and B; the vertical line shows the observed difference</h6>
</div></figure>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Exhaustive and Bootstrap Permutation Tests"><div class="sect2" id="idm46522861122952">
<h2>Exhaustive and Bootstrap Permutation Tests</h2>

<p>In addition to the preceding random<a data-type="indexterm" data-primary="resampling" data-secondary="permutation tests" data-tertiary="web stickiness example" data-startref="ix_resmplprmex" id="idm46522860436648"/><a data-type="indexterm" data-primary="web testing" data-secondary="web stickiness example" data-startref="ix_webtstex" id="idm46522860435128"/><a data-type="indexterm" data-primary="permutation tests" data-secondary="web stickiness example" data-startref="ix_permtstex" id="idm46522860433912"/> shuffling procedure, also called a <em>random permutation test</em> or a <em>randomization test</em>, there are <a data-type="indexterm" data-primary="resampling" data-secondary="permutation tests" data-tertiary="exhaustive and bootstrap tests" id="idm46522860431736"/><a data-type="indexterm" data-primary="permutation tests" data-secondary="exhaustive and bootstrap" id="idm46522860430488"/><a data-type="indexterm" data-primary="exhaustive permutation tests" id="idm46522860429576"/><a data-type="indexterm" data-primary="bootstrap" data-secondary="bootstrap and permutation tests" id="idm46522860428936"/>two variants of the permutation test:</p>

<ul>
<li>
<p>An <em>exhaustive permutation test</em></p>
</li>
<li>
<p>A <em>bootstrap permutation test</em></p>
</li>
</ul>

<p>In an exhaustive permutation test, instead of just randomly shuffling and dividing the data, we actually figure out all the possible ways it could be divided.
This is practical only for relatively small sample sizes.
With a large number of repeated shufflings, the random permutation test results approximate those of the exhaustive permutation test, and approach them in the limit.<a data-type="indexterm" data-primary="exact tests" id="idm46522860471128"/>
Exhaustive permutation tests are also sometimes called <em>exact tests</em>, due to their statistical property of guaranteeing that the null model will not test as “significant” more than the alpha level of the test (see <a data-type="xref" href="#Significance">“Statistical Significance and p-Values”</a>).</p>

<p>In a bootstrap permutation test, the draws outlined in steps 2 and 3 of the random permutation test are made <em>with replacement</em> instead of without replacement.<a data-type="indexterm" data-primary="replacement (in sampling)" data-secondary="in bootstrap permutation tests" data-secondary-sortas="bootstrap" id="idm46522860467912"/><a data-type="indexterm" data-primary="sampling" data-secondary="with and without replacement" data-secondary-sortas="replacement" id="idm46522860466584"/>
In this way the resampling procedure models not just the random element in the assignment of treatment to subject but also the random element in the selection of subjects from a population.
Both procedures are encountered in statistics, and the distinction between them is somewhat convoluted and not of consequence in the practice of data science.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Permutation Tests:  The Bottom Line for Data Science"><div class="sect2" id="idm46522860464728">
<h2>Permutation Tests:  The Bottom Line for Data Science</h2>

<p>Permutation tests are useful heuristic procedures for exploring the role of random variation.<a data-type="indexterm" data-primary="resampling" data-secondary="permutation tests" data-tertiary="value for data science" id="idm46522860463352"/><a data-type="indexterm" data-primary="permutation tests" data-secondary="value for data science" id="idm46522860462104"/><a data-type="indexterm" data-primary="data science" data-secondary="permutation tests, value of" id="idm46522860461160"/>
They are relatively easy to code, interpret, and explain, and they offer a useful detour around the formalism and “false determinism” of formula-based statistics, in which the precision of formula “answers” tends to imply unwarranted certainty.</p>

<p>One virtue of resampling, in contrast to formula approaches, is that it comes much closer to a one-size-fits-all approach to inference.  Data can be numeric or binary.  Sample sizes can be the same or different.  Assumptions about normally distributed data are not needed.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46522860354024">
<h5>Key Ideas</h5>
<ul>
<li>
<p>In a permutation test, multiple samples are combined and then shuffled.</p>
</li>
<li>
<p>The shuffled values are then divided into resamples, and the statistic of interest is calculated.</p>
</li>
<li>
<p>This process is then repeated, and the resampled statistic is tabulated.</p>
</li>
<li>
<p>Comparing the observed value of the statistic to the resampled distribution allows you to judge whether an observed difference between samples might occur by chance.<a data-type="indexterm" data-primary="resampling" data-secondary="permutation tests" data-startref="ix_resmplprm" id="idm46522860349176"/><a data-type="indexterm" data-primary="permutation tests" data-startref="ix_permtst" id="idm46522860347928"/></p>
</li>
</ul>
</div></aside>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Further Reading"><div class="sect2" id="idm46522860346344">
<h2>Further Reading</h2>

<ul>
<li>
<p><em>Randomization Tests</em>, 4th ed., by Eugene Edgington and Patrick Onghena (Chapman &amp; Hall/CRC Press, 2007)—but don’t get too drawn into the thicket of <span class="keep-together">nonrandom sampling</span></p>
</li>
<li>
<p><em>Introductory Statistics and Analytics: A Resampling Perspective</em> by Peter Bruce (Wiley, 2014)<a data-type="indexterm" data-primary="statistical experiments and significance testing" data-secondary="resampling" data-startref="ix_statexpres" id="idm46522860342264"/><a data-type="indexterm" data-primary="resampling" data-startref="ix_resmpl" id="idm46522860340952"/></p>
</li>
</ul>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Statistical Significance and p-Values"><div class="sect1" id="Significance">
<h1>Statistical Significance and p-Values</h1>

<p>Statistical significance is how statisticians measure whether an experiment (or even a study of existing data) yields a result more extreme than what chance might produce.<a data-type="indexterm" data-primary="statistical experiments and significance testing" data-secondary="statistical significance and p-values" id="ix_statexsig"/><a data-type="indexterm" data-primary="p-values" id="ix_pval"/>
If the result is beyond the realm of chance variation, it is said to be statistically <span class="keep-together">significant</span>.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46522860334312">
<h5>Key Terms for Statistical Significance and p-Values</h5><dl>
<dt class="horizontal"><strong><em>p-value</em></strong></dt>
<dd>
<p>Given a chance model that embodies the null hypothesis, the p-value is the probability of obtaining results as unusual or extreme as the observed results.<a data-type="indexterm" data-primary="alpha" id="idm46522860331128"/></p>
</dd>
<dt class="horizontal"><strong><em>Alpha</em></strong></dt>
<dd>
<p>The probability threshold of “unusualness” that chance results must surpass for actual outcomes to be deemed statistically significant.</p>
</dd>
<dt class="horizontal"><strong><em>Type 1 error</em></strong></dt>
<dd>
<p>Mistakenly concluding an effect is real (when it is due to chance).<a data-type="indexterm" data-primary="type 1 errors" id="idm46522860326616"/></p>
</dd>
<dt class="horizontal"><strong><em>Type 2 error</em></strong></dt>
<dd>
<p>Mistakenly concluding an effect is due to chance (when it is real).<a data-type="indexterm" data-primary="type 2 errors" id="idm46522860324104"/></p>
</dd>
</dl>
</div></aside>

<p class="pagebreak-before">Consider in <a data-type="xref" href="#dataframe2">Table 3-2</a> the results of the web test shown earlier.</p>
<table id="dataframe2">
<caption><span class="label">Table 3-2. </span>2×2 table for ecommerce experiment results</caption>
<thead>
<tr>
<th>Outcome</th>
<th>Price A</th>
<th>Price B</th>
</tr>
</thead>
<tbody>
<tr>
<td><p>Conversion</p></td>
<td><p>200</p></td>
<td><p>182</p></td>
</tr>
<tr>
<td><p>No conversion</p></td>
<td><p>23,539</p></td>
<td><p>22,406</p></td>
</tr>
</tbody>
</table>

<p>Price A converts almost 5% better than price B (0.8425% = 200/(23539+200)*100, versus 0.8057% = 182/(22406+182)*100—a difference of 0.0368 percentage points), big enough to be meaningful in a high-volume business.
We have over 45,000 data points here, and it is tempting to consider this as “big data,” not requiring tests of statistical significance (needed mainly to account for sampling variability in small samples).
However, the conversion rates are so low (less than 1%) that the actual meaningful values—the conversions—are only in the 100s, and the sample size needed is really determined by these conversions.
We can<a data-type="indexterm" data-primary="chance variation" id="idm46522860312280"/> test whether the difference in conversions between prices A and B is within the range of <em>chance variation</em>, using a resampling procedure.<a data-type="indexterm" data-primary="null hypothesis" id="idm46522860310952"/>  By chance variation, we mean the random variation produced by a probability model that embodies the null hypothesis that there is no difference between the rates (see <a data-type="xref" href="#Null_hypothesis">“The Null Hypothesis”</a>).
The following permutation procedure asks, “If the two prices share the same conversion rate, could chance variation produce a difference as big as 5%?”</p>
<ol>
<li>
<p>Put cards labeled 1 and 0 in a box: this represents the supposed shared conversion rate of 382 ones and 45,945 zeros = 0.008246 = 0.8246%.</p>
</li>
<li>
<p>Shuffle and draw out a resample of size 23,739 (same <em>n</em> as price A), and record how many 1s.</p>
</li>
<li>
<p>Record the number of 1s in the remaining 22,588 (same <em>n</em> as price B).</p>
</li>
<li>
<p>Record the difference in proportion of 1s.</p>
</li>
<li>
<p>Repeat steps 2–4.</p>
</li>
<li>
<p>How often was the difference &gt;= 0.0368?</p>
</li>

</ol>

<p class="pagebreak-before">Reusing <a data-type="indexterm" data-primary="permutation tests" id="idm46522860301272"/>the function <code>perm_fun</code> defined in <a data-type="xref" href="#PermutationExample">“Example: Web Stickiness”</a>,
we can create a histogram of randomly permuted differences in conversion rate in <em>R</em>:</p>

<pre data-type="programlisting" data-code-language="r"><code class="n">obs_pct_diff</code> <code class="o">&lt;-</code> <code class="m">100</code> <code class="o">*</code> <code class="p">(</code><code class="m">200</code> <code class="o">/</code> <code class="m">23739</code> <code class="o">-</code> <code class="m">182</code> <code class="o">/</code> <code class="m">22588</code><code class="p">)</code>
<code class="n">conversion</code> <code class="o">&lt;-</code> <code class="nf">c</code><code class="p">(</code><code class="nf">rep</code><code class="p">(</code><code class="m">0</code><code class="p">,</code> <code class="m">45945</code><code class="p">),</code> <code class="nf">rep</code><code class="p">(</code><code class="m">1</code><code class="p">,</code> <code class="m">382</code><code class="p">))</code>
<code class="n">perm_diffs</code> <code class="o">&lt;-</code> <code class="nf">rep</code><code class="p">(</code><code class="m">0</code><code class="p">,</code> <code class="m">1000</code><code class="p">)</code>
<code class="nf">for </code><code class="p">(</code><code class="n">i</code> <code class="n">in</code> <code class="m">1</code><code class="o">:</code><code class="m">1000</code><code class="p">)</code> <code class="p">{</code>
  <code class="n">perm_diffs</code><code class="p">[</code><code class="n">i</code><code class="p">]</code> <code class="o">=</code> <code class="m">100</code> <code class="o">*</code> <code class="nf">perm_fun</code><code class="p">(</code><code class="n">conversion</code><code class="p">,</code> <code class="m">23739</code><code class="p">,</code> <code class="m">22588</code><code class="p">)</code>
<code class="p">}</code>
<code class="nf">hist</code><code class="p">(</code><code class="n">perm_diffs</code><code class="p">,</code> <code class="n">xlab</code><code class="o">=</code><code class="s">'Conversion rate (percent)'</code><code class="p">,</code> <code class="n">main</code><code class="o">=</code><code class="s">''</code><code class="p">)</code>
<code class="nf">abline</code><code class="p">(</code><code class="n">v</code><code class="o">=</code><code class="n">obs_pct_diff</code><code class="p">)</code></pre>

<p>The corresponding <em>Python</em> code is:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">obs_pct_diff</code> <code class="o">=</code> <code class="mi">100</code> <code class="o">*</code> <code class="p">(</code><code class="mi">200</code> <code class="o">/</code> <code class="mi">23739</code> <code class="o">-</code> <code class="mi">182</code> <code class="o">/</code> <code class="mi">22588</code><code class="p">)</code>
<code class="k">print</code><code class="p">(</code><code class="n">f</code><code class="s1">'Observed difference: {obs_pct_diff:.4f}%'</code><code class="p">)</code>
<code class="n">conversion</code> <code class="o">=</code> <code class="p">[</code><code class="mi">0</code><code class="p">]</code> <code class="o">*</code> <code class="mi">45945</code>
<code class="n">conversion</code><code class="o">.</code><code class="n">extend</code><code class="p">([</code><code class="mi">1</code><code class="p">]</code> <code class="o">*</code> <code class="mi">382</code><code class="p">)</code>
<code class="n">conversion</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">Series</code><code class="p">(</code><code class="n">conversion</code><code class="p">)</code>

<code class="n">perm_diffs</code> <code class="o">=</code> <code class="p">[</code><code class="mi">100</code> <code class="o">*</code> <code class="n">perm_fun</code><code class="p">(</code><code class="n">conversion</code><code class="p">,</code> <code class="mi">23739</code><code class="p">,</code> <code class="mi">22588</code><code class="p">)</code>
              <code class="k">for</code> <code class="n">_</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">1000</code><code class="p">)]</code>

<code class="n">fig</code><code class="p">,</code> <code class="n">ax</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">(</code><code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">5</code><code class="p">,</code> <code class="mi">5</code><code class="p">))</code>
<code class="n">ax</code><code class="o">.</code><code class="n">hist</code><code class="p">(</code><code class="n">perm_diffs</code><code class="p">,</code> <code class="n">bins</code><code class="o">=</code><code class="mi">11</code><code class="p">,</code> <code class="n">rwidth</code><code class="o">=</code><code class="mf">0.9</code><code class="p">)</code>
<code class="n">ax</code><code class="o">.</code><code class="n">axvline</code><code class="p">(</code><code class="n">x</code><code class="o">=</code><code class="n">obs_pct_diff</code><code class="p">,</code> <code class="n">color</code><code class="o">=</code><code class="s1">'black'</code><code class="p">,</code> <code class="n">lw</code><code class="o">=</code><code class="mi">2</code><code class="p">)</code>
<code class="n">ax</code><code class="o">.</code><code class="n">text</code><code class="p">(</code><code class="mf">0.06</code><code class="p">,</code> <code class="mi">200</code><code class="p">,</code> <code class="s1">'Observed</code><code class="se">\n</code><code class="s1">difference'</code><code class="p">,</code> <code class="n">bbox</code><code class="o">=</code><code class="p">{</code><code class="s1">'facecolor'</code><code class="p">:</code><code class="s1">'white'</code><code class="p">})</code>
<code class="n">ax</code><code class="o">.</code><code class="n">set_xlabel</code><code class="p">(</code><code class="s1">'Conversion rate (percent)'</code><code class="p">)</code>
<code class="n">ax</code><code class="o">.</code><code class="n">set_ylabel</code><code class="p">(</code><code class="s1">'Frequency'</code><code class="p">)</code></pre>

<p>See the histogram of 1,000 resampled results in <a data-type="xref" href="#conversion-rates-histogram">Figure 3-5</a>: as it happens, in this case the observed difference of 0.0368% is well within the range of chance variation.</p>

<figure class="width-75"><div id="conversion-rates-histogram" class="figure">
<img src="Images/psd2_0305.png" alt="Histogram of the difference in conversion rates from the permutation procedure." width="1143" height="1154"/>
<h6><span class="label">Figure 3-5. </span>Frequency distribution for the difference in conversion rates between prices A and B</h6>
</div></figure>








<section data-type="sect2" data-pdf-bookmark="p-Value"><div class="sect2" id="p-value">
<h2>p-Value</h2>

<p>Simply looking at the graph is not a very precise way to measure statistical significance, so of more interest is the <em>p-value</em>.<a data-type="indexterm" data-primary="statistical experiments and significance testing" data-secondary="statistical significance and p-values" data-tertiary="p-values" id="idm46522860084136"/>
This is the frequency with which the chance model produces a result more extreme than the observed result.
We can estimate a p-value from our permutation test by taking the proportion of times that the permutation <a data-type="indexterm" data-primary="permutation tests" data-secondary="estimating p-values from" id="idm46522860082568"/>test produces a difference equal to or greater than  the observed difference:</p>

<pre data-type="programlisting" data-code-language="r"><code class="nf">mean</code><code class="p">(</code><code class="n">perm_diffs</code> <code class="o">&gt;</code> <code class="n">obs_pct_diff</code><code class="p">)</code>
<code class="p">[</code><code class="m">1</code><code class="p">]</code> <code class="m">0.308</code></pre>

<pre data-type="programlisting" data-code-language="python"><code class="n">np</code><code class="o">.</code><code class="n">mean</code><code class="p">([</code><code class="n">diff</code> <code class="o">&gt;</code> <code class="n">obs_pct_diff</code> <code class="k">for</code> <code class="n">diff</code> <code class="ow">in</code> <code class="n">perm_diffs</code><code class="p">])</code></pre>

<p>Here, both <em>R</em> and <em>Python</em> use the fact that true is interpreted as 1 and false as 0.</p>

<p>The  p-value is 0.308, which means that we would expect to achieve a result as extreme as this, or a more extreme result, by random chance over 30% of the time.</p>

<p>In this case, we didn’t need to use a permutation test to get a p-value.
Since we have a binomial distribution, we can approximate the p-value.
In <em>R</em> code, we do this using the function <code>prop.test</code>:</p>

<pre data-type="programlisting" data-code-language="r"><code class="o">&gt;</code> <code class="nf">prop.test</code><code class="p">(</code><code class="n">x</code><code class="o">=</code><code class="nf">c</code><code class="p">(</code><code class="m">200</code><code class="p">,</code> <code class="m">182</code><code class="p">),</code> <code class="n">n</code><code class="o">=</code><code class="nf">c</code><code class="p">(</code><code class="m">23739</code><code class="p">,</code> <code class="m">22588</code><code class="p">),</code> <code class="n">alternative</code><code class="o">=</code><code class="s">'greater'</code><code class="p">)</code>

	<code class="m">2</code><code class="o">-</code><code class="n">sample</code> <code class="n">test</code> <code class="n">for</code> <code class="n">equality</code> <code class="n">of</code> <code class="n">proportions</code> <code class="n">with</code> <code class="n">continuity</code> <code class="n">correction</code>

<code class="n">data</code><code class="o">:</code>  <code class="nf">c</code><code class="p">(</code><code class="m">200</code><code class="p">,</code> <code class="m">182</code><code class="p">)</code> <code class="n">out</code> <code class="n">of</code> <code class="nf">c</code><code class="p">(</code><code class="m">23739</code><code class="p">,</code> <code class="m">22588</code><code class="p">)</code>
<code class="n">X</code><code class="o">-</code><code class="n">squared</code> <code class="o">=</code> <code class="m">0.14893</code><code class="p">,</code> <code class="n">df</code> <code class="o">=</code> <code class="m">1</code><code class="p">,</code> <code class="n">p</code><code class="o">-</code><code class="n">value</code> <code class="o">=</code> <code class="m">0.3498</code>
<code class="n">alternative</code> <code class="n">hypothesis</code><code class="o">:</code> <code class="n">greater</code>
<code class="m">95</code> <code class="n">percent</code> <code class="n">confidence</code> <code class="n">interval</code><code class="o">:</code>
 <code class="m">-0.001057439</code>  <code class="m">1.000000000</code>
<code class="n">sample</code> <code class="n">estimates</code><code class="o">:</code>
     <code class="n">prop</code> <code class="m">1</code>      <code class="n">prop</code> <code class="m">2</code>
<code class="m">0.008424955</code> <code class="m">0.008057376</code></pre>

<p>The argument <code>x</code> is the number of successes for each group, and the argument <code>n</code> is the number of trials.</p>

<p>The method <code>scipy.stats.chi2_contingency</code> takes the values as shown in <a data-type="xref" href="#dataframe2">Table 3-2</a>:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">survivors</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">([[</code><code class="mi">200</code><code class="p">,</code> <code class="mi">23739</code> <code class="o">-</code> <code class="mi">200</code><code class="p">],</code> <code class="p">[</code><code class="mi">182</code><code class="p">,</code> <code class="mi">22588</code> <code class="o">-</code> <code class="mi">182</code><code class="p">]])</code>
<code class="n">chi2</code><code class="p">,</code> <code class="n">p_value</code><code class="p">,</code> <code class="n">df</code><code class="p">,</code> <code class="n">_</code> <code class="o">=</code> <code class="n">stats</code><code class="o">.</code><code class="n">chi2_contingency</code><code class="p">(</code><code class="n">survivors</code><code class="p">)</code>

<code class="k">print</code><code class="p">(</code><code class="n">f</code><code class="s1">'p-value for single sided test: {p_value / 2:.4f}'</code><code class="p">)</code></pre>

<p>The normal approximation yields a p-value of 0.3498, which is close to the p-value obtained from the permutation test.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Alpha"><div class="sect2" id="idm46522860086072">
<h2>Alpha</h2>

<p>Statisticians frown on the practice of leaving it to the researcher’s discretion <a data-type="indexterm" data-primary="alpha" id="idm46522858503144"/><a data-type="indexterm" data-primary="p-values" data-secondary="alpha" id="idm46522858502440"/><a data-type="indexterm" data-primary="statistical experiments and significance testing" data-secondary="statistical significance and p-values" data-tertiary="alpha" id="idm46522858501496"/>to determine whether a result is “too unusual” to happen by chance.
Rather, a threshold is specified in advance, as in “more extreme than 5% of the chance (null hypothesis) results”; this threshold is known as <em>alpha</em>. Typical alpha levels are 5% and 1%.
Any chosen level is an arbitrary decision—there is nothing about the process that will guarantee correct decisions x% of the time.
This is because the probability question being answered is <em>not</em> “What is the probability that this happened by chance?” but rather “Given a chance model, what is the probability of a result this extreme?”
We then deduce backward about the appropriateness of the chance model, but that judgment does not carry a probability.
This point has been the subject of much confusion.</p>










<section data-type="sect3" data-pdf-bookmark="p-value controversy"><div class="sect3" id="idm46522858575288">
<h3>p-value controversy</h3>

<p>Considerable controversy has surrounded the use of the p-value in recent years.<a data-type="indexterm" data-primary="p-values" data-secondary="controversy over use of" id="idm46522858573912"/><a data-type="indexterm" data-primary="statistical experiments and significance testing" data-secondary="statistical significance and p-values" data-tertiary="controversy over p-values" id="idm46522858572936"/>
One psychology journal has gone so far as to “ban” the use of p-values in submitted papers on the grounds that publication decisions based solely on the p-value were resulting in the publication of poor research.
Too many researchers, only dimly aware of what a p-value really means, root around in the data, and among different possible hypotheses to test, until they find a combination that yields a significant p-value and, hence, a paper suitable for publication.</p>

<p>The real problem is that people want more meaning from the p-value than it contains.  Here’s what we would <em>like</em> the p-value to convey:</p>
<blockquote>
<p>The probability that the result is due to chance.</p></blockquote>

<p>We hope for a low value, so we can conclude that we’ve proved something. This is how many journal editors were interpreting the p-value. But here’s what the p-value <em>actually</em> represents:</p>
<blockquote>
<p>The probability that, <em>given a chance model</em>, results as extreme as the observed results could occur.</p></blockquote>

<p>The difference is subtle but real.
A significant p-value does not carry you quite as far along the road to “proof” as it seems to promise.
The logical foundation for the conclusion “statistically significant” is somewhat weaker when the real meaning of the p-value is understood.</p>

<p>In March 2016, the American Statistical Association, after much internal deliberation, revealed the extent of misunderstanding about p-values when it issued a cautionary statement regarding their use.<a data-type="indexterm" data-primary="American Statistical Association (ASA), statement on use of p-values" id="idm46522858565912"/> The <a href="https://oreil.ly/WVfYU">ASA statement</a> stressed six principles for researchers and journal editors:</p>
<blockquote><ol>
<li>
<p>P-values can indicate how incompatible the data are with a specified statistical model.</p>
</li>
<li>
<p>P-values do not measure the probability that the studied hypothesis is true, or the probability that the data were produced by random chance alone.</p>
</li>
<li>
<p>Scientific conclusions and business or policy decisions should not be based only on whether a p-value passes a specific threshold.</p>
</li>
<li>
<p>Proper inference requires  full reporting and transparency.</p>
</li>
<li>
<p>A p-value, or statistical significance, does not measure the size of an effect or the importance of a result.</p>
</li>
<li>
<p>By itself, a p-value does not provide a good measure of evidence regarding a model or hypothesis.</p>
</li>

</ol></blockquote>
</div></section>













<section data-type="sect3" class="pagebreak-before less_space" data-pdf-bookmark="Practical significance"><div class="sect3" id="idm46522858557784">
<h3>Practical significance</h3>

<p>Even if a result is statistically significant, that does not mean it has practical significance.<a data-type="indexterm" data-primary="practical significance versus statistical significance" id="idm46522858556136"/><a data-type="indexterm" data-primary="statistical significance" data-secondary="practical significance versus" id="idm46522858555336"/><a data-type="indexterm" data-primary="p-values" data-secondary="practical significance and" id="idm46522858554360"/>  A small difference that has no practical meaning can be statistically significant if it arose from large enough samples.  Large samples ensure that small, non-meaningful effects can nonetheless be big enough to rule out chance as an explanation.  Ruling out chance does not magically render important a result that is, in its essence, unimportant.</p>
</div></section>



</div></section>













<section data-type="sect2" data-pdf-bookmark="Type 1 and Type 2 Errors"><div class="sect2" id="idm46522858552520">
<h2>Type 1 and Type 2 Errors</h2>

<p>In assessing <a data-type="indexterm" data-primary="p-values" data-secondary="type 1 and type 2 errors" id="idm46522858551016"/><a data-type="indexterm" data-primary="statistical experiments and significance testing" data-secondary="statistical significance and p-values" data-tertiary="type 1 and type 2 errors" id="idm46522858549944"/>statistical significance, two types of error are possible:</p>

<ul>
<li>
<p>A Type 1 error, in which you mistakenly conclude an effect is real, when it is really just due to chance<a data-type="indexterm" data-primary="type 1 errors" id="idm46522858547368"/><a data-type="indexterm" data-primary="type 2 errors" id="idm46522858546664"/></p>
</li>
<li>
<p>A Type 2 error, in which you mistakenly conclude that an effect is not real (i.e., due to chance), when it actually is real</p>
</li>
</ul>

<p>Actually, a Type 2 error is not so much an error as a judgment that the sample size is too small to detect the effect.  When a p-value falls short of statistical significance (e.g., it exceeds 5%), what we are really saying is “effect not proven.”  It could be that a larger sample would yield a smaller p-value.</p>

<p>The basic function of significance tests (also called <em>hypothesis tests</em>) is to protect against being fooled by random chance; thus they are typically structured to minimize Type 1 errors.<a data-type="indexterm" data-primary="hypothesis tests" data-secondary="structured to minimize type 1 errors" id="idm46522858543048"/></p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Data Science and p-Values"><div class="sect2" id="idm46522858541928">
<h2>Data Science and p-Values</h2>

<p>The work that data scientists do is typically not destined for publication in scientific journals, so the debate over the value of a p-value is somewhat academic.<a data-type="indexterm" data-primary="p-values" data-secondary="data science and" id="idm46522858540488"/><a data-type="indexterm" data-primary="statistical experiments and significance testing" data-secondary="statistical significance and p-values" data-tertiary="data science and p-values" id="idm46522858539512"/><a data-type="indexterm" data-primary="data science" data-secondary="p-values and" id="idm46522858480840"/>
For a data scientist, a p-value is a useful metric in situations where you want to know whether a model result that appears interesting and useful is within the range of normal chance variability.
As a decision tool in an experiment, a p-value should not be considered controlling, but merely another point of information bearing on a decision.
For example, p-values are sometimes used as intermediate inputs in some statistical or machine learning models—a feature might be included in or excluded from a model depending on its p-value.</p>
<aside data-type="sidebar" epub:type="sidebar" class="pagebreak-before less_space"><div class="sidebar" id="idm46522858479112">
<h5>Key Ideas</h5>
<ul>
<li>
<p>Significance tests are used to determine whether an observed effect is within the range of chance variation for a null hypothesis model.</p>
</li>
<li>
<p>The p-value is the probability that results as extreme as the observed results might occur, given a null hypothesis model.</p>
</li>
<li>
<p>The alpha value is the threshold of “unusualness” in a null hypothesis chance model.</p>
</li>
<li>
<p>Significance testing has been much more relevant for formal reporting of research than for data science (but has been fading recently, even for the former).</p>
</li>
</ul>
</div></aside>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Further Reading"><div class="sect2" id="idm46522858473064">
<h2>Further Reading</h2>

<ul>
<li>
<p>Stephen Stigler, “Fisher and the 5% Level,” <em>Chance</em> 21, no. 4 (2008): 12. This article is a short commentary on Ronald Fisher’s 1925 book <em>Statistical Methods for Research Workers</em> (Oliver &amp; Boyd), and on Fisher’s emphasis on the 5% level of significance.</p>
</li>
<li>
<p>See also <a data-type="xref" href="#Hypothesis_test">“Hypothesis Tests”</a> and the further reading mentioned there.<a data-type="indexterm" data-primary="p-values" data-startref="ix_pval" id="idm46522858468328"/><a data-type="indexterm" data-primary="statistical experiments and significance testing" data-secondary="statistical significance and p-values" data-startref="ix_statexsig" id="idm46522858467384"/></p>
</li>
</ul>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="t-Tests"><div class="sect1" id="tTest">
<h1>t-Tests</h1>

<p>There are numerous types of significance tests, depending on whether the data comprises count data or measured data, how many samples there are, and what’s being measured.<a data-type="indexterm" data-primary="statistical experiments and significance testing" data-secondary="t-tests" id="ix_statexpttst"/><a data-type="indexterm" data-primary="t-tests" id="ix_ttst"/>
A very<a data-type="indexterm" data-primary="Student's t-distribution" id="idm46522858461224"/> common one is the <em>t-test</em>, named after Student’s t-distribution, originally developed by W. S. Gosset to approximate the distribution of a single sample mean (see <a data-type="xref" href="ch02.xhtml#t-distribution">“Student’s t-Distribution”</a>).</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46522858458840">
<h5>Key Terms for t-Tests</h5><dl>
<dt class="horizontal"><strong><em>Test statistic</em></strong></dt>
<dd>
<p>A metric for the difference or effect of interest.</p>
</dd>
<dt class="horizontal"><strong><em>t-statistic</em></strong></dt>
<dd>
<p>A standardized version of common test statistics such as means.<a data-type="indexterm" data-primary="t-statistic" id="idm46522858453992"/><a data-type="indexterm" data-primary="t-distribution" id="idm46522858453288"/></p>
</dd>
<dt class="horizontal"><strong><em>t-distribution</em></strong></dt>
<dd>
<p>A reference distribution (in this case derived from the null hypothesis), to which the observed t-statistic can be compared.</p>
</dd>
</dl>
</div></aside>

<p>All significance tests require<a data-type="indexterm" data-primary="test statistic" id="idm46522858449992"/> that you specify a <em>test statistic</em> to measure the effect you are interested in and help you determine whether that observed effect lies within the range of normal chance variation.<a data-type="indexterm" data-primary="test statistic" id="idm46522858448584"/>
In a resampling test (see the discussion of permutation in <a data-type="xref" href="#Permutation">“Permutation Test”</a>), the scale of the data does not matter.  You create the reference (null hypothesis) distribution from the data itself and use the test statistic as is.</p>

<p>In the 1920s and 1930s, when statistical hypothesis testing was being developed, it was not feasible to randomly shuffle data thousands of times to do a resampling test.
Statisticians found that a good approximation to the permutation (shuffled) distribution was the t-test, based on Gosset’s t-distribution.
It is used for the very common two-sample comparison—A/B test—in which the data is numeric.
But in order for the t-distribution to be used without regard to scale, a standardized form of the test statistic must be used.</p>

<p>A classic statistics text would at this stage show various formulas that incorporate Gosset’s distribution and demonstrate how to standardize your data to compare it to the standard t-distribution.
These formulas are not shown here because all statistical software, as well as <em>R</em> and <em>Python</em>, includes commands that embody the formula.
In <em>R</em>, the function is <code>t.test</code>:</p>

<pre data-type="programlisting" data-code-language="r"><code class="o">&gt;</code> <code class="nf">t.test</code><code class="p">(</code><code class="n">Time</code> <code class="o">~</code> <code class="n">Page</code><code class="p">,</code> <code class="n">data</code><code class="o">=</code><code class="n">session_times</code><code class="p">,</code> <code class="n">alternative</code><code class="o">=</code><code class="s">'less'</code><code class="p">)</code>

	<code class="n">Welch</code> <code class="n">Two</code> <code class="n">Sample</code> <code class="n">t</code><code class="o">-</code><code class="n">test</code>

<code class="n">data</code><code class="o">:</code>  <code class="n">Time</code> <code class="n">by</code> <code class="n">Page</code>
<code class="n">t</code> <code class="o">=</code> <code class="m">-1.0983</code><code class="p">,</code> <code class="n">df</code> <code class="o">=</code> <code class="m">27.693</code><code class="p">,</code> <code class="n">p</code><code class="o">-</code><code class="n">value</code> <code class="o">=</code> <code class="m">0.1408</code>
<code class="n">alternative</code> <code class="n">hypothesis</code><code class="o">:</code> <code class="n">true</code> <code class="n">difference</code> <code class="n">in</code> <code class="n">means</code> <code class="n">is</code> <code class="n">less</code> <code class="n">than</code> <code class="m">0</code>
<code class="m">95</code> <code class="n">percent</code> <code class="n">confidence</code> <code class="n">interval</code><code class="o">:</code>
     <code class="o">-</code><code class="kc">Inf</code> <code class="m">19.59674</code>
<code class="n">sample</code> <code class="n">estimates</code><code class="o">:</code>
<code class="n">mean</code> <code class="n">in</code> <code class="n">group</code> <code class="n">Page</code> <code class="n">A</code> <code class="n">mean</code> <code class="n">in</code> <code class="n">group</code> <code class="n">Page</code> <code class="n">B</code>
            <code class="m">126.3333</code>             <code class="m">162.0000</code></pre>

<p>The function <code>scipy.stats.ttest_ind</code> can be used in <em>Python</em>:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">res</code> <code class="o">=</code> <code class="n">stats</code><code class="o">.</code><code class="n">ttest_ind</code><code class="p">(</code><code class="n">session_times</code><code class="p">[</code><code class="n">session_times</code><code class="o">.</code><code class="n">Page</code> <code class="o">==</code> <code class="s1">'Page A'</code><code class="p">]</code><code class="o">.</code><code class="n">Time</code><code class="p">,</code>
                      <code class="n">session_times</code><code class="p">[</code><code class="n">session_times</code><code class="o">.</code><code class="n">Page</code> <code class="o">==</code> <code class="s1">'Page B'</code><code class="p">]</code><code class="o">.</code><code class="n">Time</code><code class="p">,</code>
                      <code class="n">equal_var</code><code class="o">=</code><code class="bp">False</code><code class="p">)</code>
<code class="k">print</code><code class="p">(</code><code class="n">f</code><code class="s1">'p-value for single sided test: {res.pvalue / 2:.4f}'</code><code class="p">)</code></pre>

<p>The alternative hypothesis is that the session time mean for page A is less than that for page B.
The p-value of 0.1408 is fairly close to the permutation test p-values of 0.121 and 0.126 (see <a data-type="xref" href="#PermutationExample">“Example: Web Stickiness”</a>).</p>

<p>In a resampling mode, we structure the solution to reflect the observed data and the hypothesis to be tested, not worrying about whether the data is numeric or binary, whether or not sample sizes are balanced, sample variances, or a variety of other factors.
In the formula world, many variations present themselves, and they can be bewildering.
Statisticians need to navigate that world and learn its map, but data scientists do not—they are typically not in the business of sweating the details of hypothesis tests and confidence intervals the way a researcher preparing a paper for presentation might.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46522858239192">
<h5>Key Ideas</h5>
<ul>
<li>
<p>Before the advent of computers, resampling tests were not practical, and statisticians used standard reference distributions.</p>
</li>
<li>
<p>A test statistic could then be standardized and compared to the reference <span class="keep-together">distribution</span>.</p>
</li>
<li>
<p>One such widely used standardized statistic is the t-statistic.</p>
</li>
</ul>
</div></aside>








<section data-type="sect2" data-pdf-bookmark="Further Reading"><div class="sect2" id="idm46522858338808">
<h2>Further Reading</h2>

<ul>
<li>
<p>Any introductory statistics text will have illustrations of the t-statistic and its uses; two good ones are <em>Statistics</em>, 4th ed., by David Freedman, Robert Pisani, and Roger Purves (W. W. Norton, 2007), and <em>The Basic Practice of Statistics</em>, 8th ed., by David S. Moore, William I. Notz, and Michael A. Fligner (W. H. Freeman, 2017).</p>
</li>
<li>
<p>For a treatment of both the t-test and resampling procedures in parallel, see <em>Introductory Statistics and Analytics: A Resampling Perspective</em> by Peter Bruce (Wiley, 2014) or <em>Statistics: Unlocking the Power of Data</em>, 2nd ed., by Robin Lock and four other Lock family members (Wiley, 2016).<a data-type="indexterm" data-primary="statistical experiments and significance testing" data-secondary="t-tests" data-startref="ix_statexpttst" id="idm46522858334056"/><a data-type="indexterm" data-primary="t-tests" data-startref="ix_ttst" id="idm46522858332808"/></p>
</li>
</ul>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Multiple Testing"><div class="sect1" id="MultipleTesting">
<h1>Multiple Testing</h1>

<p>As we’ve mentioned previously, there is a saying in statistics: “Torture the data long enough, and it will confess.”
This means<a data-type="indexterm" data-primary="statistical experiments and significance testing" data-secondary="multiple testing" id="ix_statexmlti"/><a data-type="indexterm" data-primary="multiple testing" id="ix_mltitst"/> that if you look at the data through enough different perspectives and ask enough questions, you almost invariably will find a statistically significant effect.</p>

<p>For example, if you have 20 predictor variables and one outcome variable, all <em>randomly</em> generated, the odds are pretty good that at least one predictor will (falsely) turn out to be statistically significant if you do a series of 20 significance tests at the alpha = 0.05 level. As previously discussed, this is called a <em>Type 1 error</em>. You can calculate this probability by first finding the probability that all will <em>correctly</em> test nonsignificant at the 0.05 level. The probability that <em>one</em> will correctly test nonsignificant is 0.95, so the probability that all 20 will correctly test nonsignificant is 0.95 × 0.95 × 0.95…, or 0.95<sup>20</sup> = 0.36.<sup><a data-type="noteref" id="idm46522858323800-marker" href="ch03.xhtml#idm46522858323800">1</a></sup> The probability that at least one predictor will (falsely) test significant is the flip side of this probability, or 1 – (<em>probability that all will be nonsignificant</em>) = 0.64. This is known as <em>alpha inflation</em>.</p>

<p>This issue is related to the problem of overfitting in data mining, or “fitting the model to the noise.”  The more variables you add, or the more models you run, the greater the probability that something will emerge as “significant” just by chance.<a data-type="indexterm" data-primary="machine learning" data-secondary="overfitting risk, mitigating" id="idm46522858320856"/></p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46522858319736">
<h5>Key Terms for Multiple Testing</h5><dl>
<dt class="horizontal"><strong><em>Type 1 error</em></strong></dt>
<dd>
<p>Mistakenly concluding that an effect is statistically significant.<a data-type="indexterm" data-primary="type 1 errors" id="idm46522858316664"/></p>
</dd>
<dt class="horizontal"><strong><em>False discovery rate</em></strong></dt>
<dd>
<p>Across multiple tests, the rate of making a Type 1 error.<a data-type="indexterm" data-primary="false discovery rate" id="idm46522858314072"/><a data-type="indexterm" data-primary="alpha" data-secondary="alpha inflation" id="idm46522858313288"/></p>
</dd>
<dt class="horizontal"><strong><em>Alpha inflation</em></strong></dt>
<dd>
<p>The multiple testing phenomenon, in which <em>alpha</em>, the probability of making a Type 1 error, increases as you conduct more tests.</p>
</dd>
<dt class="horizontal"><strong><em>Adjustment of p-values</em></strong></dt>
<dd>
<p>Accounting for doing multiple tests on the same data.<a data-type="indexterm" data-primary="adjustment of p-values" id="idm46522858308216"/><a data-type="indexterm" data-primary="p-values" data-secondary="adjustment of" id="idm46522858307512"/></p>
</dd>
<dt class="horizontal"><strong><em>Overfitting</em></strong></dt>
<dd>
<p>Fitting the noise.<a data-type="indexterm" data-primary="overfitting" id="idm46522858304760"/></p>
</dd>
</dl>
</div></aside>

<p>In supervised learning tasks, a holdout set where models are assessed on data that the model has not seen before mitigates this risk.
In statistical and machine learning tasks not involving a labeled holdout set, the risk of reaching conclusions based on statistical noise persists.</p>

<p>In statistics, there are some procedures intended to deal with this problem in very specific circumstances.
For example, if you are comparing results across multiple treatment groups, you might ask multiple questions. So, for treatments A–C, you might ask:</p>

<ul>
<li>
<p>Is A different from B?</p>
</li>
<li>
<p>Is B different from C?</p>
</li>
<li>
<p>Is A different from C?</p>
</li>
</ul>

<p>Or, in a clinical trial, you might want to look at results from a therapy at multiple stages.
In each case, you are asking multiple questions, and with each question, you are increasing the chance of being fooled by chance.
Adjustment procedures in statistics can compensate for this by setting the bar for statistical significance more stringently than it would be set for a single hypothesis test.
These adjustment procedures typically involve “dividing up the alpha” according to the number of tests.  This results in a smaller alpha (i.e., a more stringent bar for statistical significance) for each test.
One such procedure, the Bonferroni adjustment, simply divides the alpha by the number of comparisons.<a data-type="indexterm" data-primary="Tukey's HSD (honest significance difference)" id="idm46522858210648"/>  Another, used in comparing multiple group means, is Tukey’s “honest significant difference,” or <em>Tukey’s HSD</em>.  <a data-type="indexterm" data-primary="t-distribution" id="idm46522858209208"/>This test applies to the maximum difference among group means, comparing it to a benchmark based on the <em>t-distribution</em> (roughly equivalent to shuffling all the values together, dealing out resampled groups of the same sizes as the original groups, and finding the maximum difference among the resampled group means).</p>

<p>However, the problem of multiple comparisons goes beyond these highly structured cases and is related to the phenomenon of repeated data “dredging” that gives rise to the saying about torturing the data.
Put another way, given sufficiently complex data, if you haven’t found something interesting, you simply haven’t looked long and hard enough.
More data is available now than ever before, and the number of journal articles published nearly doubled between 2002 and 2010.
This gives rise to lots of opportunities to find something interesting in the data, including multiplicity issues such as:</p>

<ul>
<li>
<p>Checking for multiple pairwise differences across groups</p>
</li>
<li>
<p>Looking at multiple subgroup results (“we found no significant treatment effect overall, but we did find an effect for unmarried women younger than 30”)</p>
</li>
<li>
<p>Trying lots of statistical models</p>
</li>
<li>
<p>Including lots of variables in models</p>
</li>
<li>
<p>Asking a number of different questions (i.e., different possible outcomes)</p>
</li>
</ul>
<div data-type="note" epub:type="note"><h1>False Discovery Rate</h1>
<p>The term <em>false discovery rate</em> was originally used to describe the rate at which a given set of hypothesis tests would falsely identify a significant effect.<a data-type="indexterm" data-primary="false discovery rate" id="idm46522858199656"/><a data-type="indexterm" data-primary="hypothesis tests" data-secondary="false discovery rate" id="idm46522858198952"/>  It became particularly useful with the advent of genomic research, in which massive numbers of statistical tests might be conducted as part of a gene sequencing project.  In these cases, the term applies to the testing protocol, and a single false “discovery” refers to the outcome of a hypothesis test (e.g., between two samples).  Researchers sought to set the parameters of the testing process to control the false discovery rate at a specified level.  The term has also been used for classification in data mining; it is the misclassification rate within the class 1 predictions. Or, put another way, it is the probability that a “discovery” (labeling a record as a “1”) is false. Here we typically are dealing with the case where 0s are abundant and 1s are interesting and rare (see <a data-type="xref" href="ch05.xhtml#Classification">Chapter 5</a> and <a data-type="xref" href="ch05.xhtml#RareClassProblem">“The Rare Class Problem”</a>).</p>
</div>

<p>For a variety of reasons, including especially this general issue of “multiplicity,” more research does not necessarily mean better research. For example, the pharmaceutical company Bayer found in 2011 that when it tried to replicate 67 scientific studies, it could fully replicate only 14 of them.
Nearly two-thirds could not be replicated at all.</p>

<p>In any case, the adjustment procedures for highly defined and structured statistical tests are too specific and inflexible to be of general use to data scientists.<a data-type="indexterm" data-primary="data science" data-secondary="multiplicity and" id="idm46522858193928"/>
The bottom line for data scientists on multiplicity is:</p>

<ul>
<li>
<p>For predictive modeling, the risk of getting an illusory model whose apparent efficacy is largely a product of random chance is mitigated by cross-validation (see <a data-type="xref" href="ch04.xhtml#CrossValidation">“Cross-Validation”</a>) and use of a holdout sample.</p>
</li>
<li>
<p>For other procedures without a labeled holdout set to check the model, you must rely on:</p>

<ul>
<li>
<p>Awareness that the more you query and manipulate the data, the greater the role that chance might play.</p>
</li>
<li>
<p>Resampling and simulation heuristics to provide random chance benchmarks against which observed results can be compared.</p>
</li>
</ul>
</li>
</ul>
<aside data-type="sidebar" epub:type="sidebar" class="pagebreak-before less_space"><div class="sidebar" id="idm46522858186952">
<h5>Key Ideas</h5>
<ul>
<li>
<p>Multiplicity in a research study or data mining project (multiple comparisons, many variables, many models, etc.) increases the risk of concluding that something is significant just by chance.</p>
</li>
<li>
<p>For situations involving multiple statistical comparisons (i.e., multiple tests of significance), there are statistical adjustment procedures.</p>
</li>
<li>
<p>In a data mining situation, use of a holdout sample with labeled outcome variables can help avoid misleading results.</p>
</li>
</ul>
</div></aside>








<section data-type="sect2" data-pdf-bookmark="Further Reading"><div class="sect2" id="idm46522858181912">
<h2>Further Reading</h2>

<ul>
<li>
<p>For a short exposition of one procedure (Dunnett’s test) to adjust for multiple comparisons, see David Lane’s <a href="https://oreil.ly/hd_62">online statistics text</a>.</p>
</li>
<li>
<p>Megan Goldman offers a <a href="https://oreil.ly/Dt4Vi">slightly longer treatment of the Bonferroni adjustment procedure</a>.</p>
</li>
<li>
<p>For an in-depth treatment of more flexible statistical procedures for adjusting p-values, see <em>Resampling-Based Multiple Testing</em> by Peter Westfall and Stanley Young (Wiley, 1993).</p>
</li>
<li>
<p>For a discussion of data partitioning and the use of holdout samples in predictive modeling, see Chapter 2 of <em>Data Mining for Business Analytics</em>, by Galit Shmueli, Peter Bruce, Nitin Patel, Peter Gedeck, Inbal Yahav, and Kenneth Lichtendahl (Wiley, 2007–2020, with editions for <em>R</em>, <em>Python</em>, Excel, and JMP).<a data-type="indexterm" data-primary="statistical experiments and significance testing" data-secondary="multiple testing" data-startref="ix_statexmlti" id="idm46522858173624"/><a data-type="indexterm" data-primary="multiple testing" data-startref="ix_mltitst" id="idm46522858172248"/></p>
</li>
</ul>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Degrees of Freedom"><div class="sect1" id="DOF">
<h1>Degrees of Freedom</h1>

<p>In the documentation and settings for many statistical tests and probability distributions, you will<a data-type="indexterm" data-primary="statistical experiments and significance testing" data-secondary="degrees of freedom" id="ix_statexdof"/><a data-type="indexterm" data-primary="degrees of freedom" id="ix_degfree"/> see a reference to “degrees of freedom.”
The concept is applied to statistics calculated from sample data, and refers to the number of values free to vary.
For example, if you know the mean for a sample of 10 values, there are 9 degrees of freedom (once you know 9 of the sample values, the 10th can be calculated and is not free to vary).
The degrees of freedom parameter, as applied to many probability distributions, affects the shape of the distribution.</p>

<p>The number of degrees of freedom is an input to many statistical tests.
For example, degrees of freedom is the name given to the <em>n</em> – 1 denominator seen in the calculations for variance and standard deviation.<a data-type="indexterm" data-primary="n or n – 1, dividing by in variance or standard deviation formula" id="idm46522858165128"/>
Why does it matter?  When you use a sample to estimate the variance for a population, you will end up with an estimate that is slightly biased downward if you use <em>n</em> in the denominator.
If you use <em>n</em> – 1 in the denominator, the estimate will be free of that bias.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46522858163176">
<h5>Key Terms for Degrees of Freedom</h5><dl>
<dt class="horizontal"><strong><em>n or sample size</em></strong></dt>
<dd>
<p>The number of observations (also called <em>rows</em> or <em>records</em>) in the data.<a data-type="indexterm" data-primary="n or sample size" id="idm46522858159432"/></p>
</dd>
<dt class="horizontal"><strong><em>d.f.</em></strong></dt>
<dd>
<p>Degrees of freedom.<a data-type="indexterm" data-primary="d.f. (degrees of freedom)" data-seealso="degrees of freedom" id="idm46522858156888"/></p>
</dd>
</dl>
</div></aside>

<p>A large share of a traditional statistics course or text is consumed by  various standard tests of hypotheses (t-test, F-test, etc.).
When sample statistics are standardized for use in traditional statistical formulas, degrees of freedom is part of the standardization calculation to ensure that your standardized data matches the appropriate reference distribution (t-distribution, F-distribution, etc.).</p>

<p>Is it important for data science?  Not really, at least in the context of significance testing.
For one thing, formal statistical tests are used only sparingly in data science.
For another, the data size is usually large enough that it rarely makes a real difference for a data scientist whether, for example, the denominator has <em>n</em> or <em>n</em> – 1. (As <em>n</em> gets large, the bias that would come from using <em>n</em> in the denominator disappears.)</p>

<p>There is one context, though, in which it is relevant: the use of factored variables in regression (including logistic regression).<a data-type="indexterm" data-primary="regression" data-secondary="factor variables in" id="idm46522858151512"/><a data-type="indexterm" data-primary="factor variables" id="idm46522858150536"/>
Some regression algorithms choke if exactly redundant predictor variables are present.
This most commonly occurs when factoring categorical variables into binary indicators (dummies).
Consider the variable “day of week.”
Although there are seven days of the week, there are only six degrees of freedom in specifying day of week.
For example, once you know that day of week is not Monday through Saturday, you know it must be Sunday.<a data-type="indexterm" data-primary="multicollinearity errors" id="idm46522858149288"/>
Inclusion of the Mon–Sat indicators thus means that <em>also</em> including Sunday would cause the regression to fail, due to a <em>multicollinearity</em> error.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46522858147352">
<h5>Key Ideas</h5>
<ul>
<li>
<p>The number of degrees of freedom (d.f.) forms part of the calculation to standardize test statistics so they can be compared to reference distributions (t-distribution, F-distribution, etc.).</p>
</li>
<li>
<p>The concept of degrees of freedom lies behind the factoring of categorical variables into <em>n</em> – 1 indicator or dummy variables when doing a regression (to avoid multicollinearity).</p>
</li>
</ul>
</div></aside>








<section data-type="sect2" data-pdf-bookmark="Further Reading"><div class="sect2" id="idm46522858143128">
<h2>Further Reading</h2>

<p>There are <a href="https://oreil.ly/VJyts">several web tutorials on degrees of freedom</a>.<a data-type="indexterm" data-primary="statistical experiments and significance testing" data-secondary="degrees of freedom" data-startref="ix_statexdof" id="idm46522858141016"/><a data-type="indexterm" data-primary="degrees of freedom" data-startref="ix_degfree" id="idm46522858139672"/></p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="ANOVA"><div class="sect1" id="ANOVA">
<h1>ANOVA</h1>

<p>Suppose that, instead of an A/B test, we had a comparison of multiple groups, say A/B/C/D, each with numeric data.<a data-type="indexterm" data-primary="variance" data-secondary="analysis of (ANOVA)" id="ix_varAN"/><a data-type="indexterm" data-primary="statistical experiments and significance testing" data-secondary="analysis of variance (ANOVA)" id="ix_statexANOVA"/><a data-type="indexterm" data-primary="analysis of variance (ANOVA)" id="ix_ANOVA"/>
The statistical procedure that tests for a statistically significant difference among the groups is called <em>analysis of variance</em>, or <em>ANOVA</em>.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46522858131560">
<h5>Key Terms for ANOVA</h5><dl>
<dt class="horizontal"><strong><em>Pairwise comparison</em></strong></dt>
<dd>
<p>A hypothesis test (e.g., of means) between two groups among multiple groups.<a data-type="indexterm" data-primary="pairwise comparisons" id="idm46522858128360"/></p>
</dd>
<dt class="horizontal"><strong><em>Omnibus test</em></strong></dt>
<dd>
<p>A single hypothesis test of the overall variance among multiple group means.<a data-type="indexterm" data-primary="omnibus tests" id="idm46522858125848"/></p>
</dd>
<dt class="horizontal"><strong><em>Decomposition of variance</em></strong></dt>
<dd>
<p>Separation of components contributing to an individual value (e.g., from the overall average, from a treatment mean, and from a residual error).<a data-type="indexterm" data-primary="decomposition of variance" id="idm46522858123208"/></p>
</dd>
<dt class="horizontal"><strong><em>F-statistic</em></strong></dt>
<dd>
<p>A standardized statistic that measures the extent to which differences among group means exceed what might be expected in a chance model.<a data-type="indexterm" data-primary="F-statistic" id="idm46522858120520"/><a data-type="indexterm" data-primary="sum of squares (SS)" id="idm46522858119816"/></p>
</dd>
<dt class="horizontal"><strong><em>SS</em></strong></dt>
<dd>
<p>“Sum of squares,” referring to deviations from some average value.</p>
</dd>
</dl>
</div></aside>

<p><a data-type="xref" href="#FourSessionsData">Table 3-3</a> shows the stickiness of four web pages, defined as the number of seconds a visitor spent on the page.<a data-type="indexterm" data-primary="SS" data-see="sum of squares" id="idm46522858115976"/><a data-type="indexterm" data-primary="web testing" data-secondary="web stickiness example" id="idm46522858115032"/>  The four pages are switched out so that each web visitor receives one at random.
There are a total of five visitors for each page, and in <a data-type="xref" href="#FourSessionsData">Table 3-3</a>, each column is an independent set of data.
The first viewer for page 1 has no connection to the first viewer for page 2.
Note that in a web test like this, we cannot fully implement the classic randomized sampling design in which each visitor is selected at random from some huge population.
We must take the visitors as they come.
Visitors may systematically differ depending on time of day, time of week, season of the year, conditions of their internet, what device they are using, and so on.
These factors should be considered as potential bias when the experiment results are reviewed.</p>
<table id="FourSessionsData" class="pagebreak-before less_space">
<caption><span class="label">Table 3-3. </span>Stickiness (in seconds) of four web pages</caption>
<thead>
<tr>
<th/>
<th>Page 1</th>
<th>Page 2</th>
<th>Page 3</th>
<th>Page 4</th>
</tr>
</thead>
<tbody>
<tr>
<td/>
<td><p>164</p></td>
<td><p>178</p></td>
<td><p>175</p></td>
<td><p>155</p></td>
</tr>
<tr>
<td/>
<td><p>172</p></td>
<td><p>191</p></td>
<td><p>193</p></td>
<td><p>166</p></td>
</tr>
<tr>
<td/>
<td><p>177</p></td>
<td><p>182</p></td>
<td><p>171</p></td>
<td><p>164</p></td>
</tr>
<tr>
<td/>
<td><p>156</p></td>
<td><p>185</p></td>
<td><p>163</p></td>
<td><p>170</p></td>
</tr>
<tr>
<td/>
<td><p>195</p></td>
<td><p>177</p></td>
<td><p>176</p></td>
<td><p>168</p></td>
</tr>
<tr>
<td><p>Average</p></td>
<td><p>172</p></td>
<td><p>185</p></td>
<td><p>176</p></td>
<td><p>162</p></td>
</tr>
<tr>
<td><p>Grand average</p></td>
<td/>
<td/>
<td/>
<td><p>173.75</p></td>
</tr>
</tbody>
</table>

<p>Now we have a conundrum (see <a data-type="xref" href="#FourGroups">Figure 3-6</a>). When we were comparing just two groups, it was a simple matter; we merely looked at the difference between the means of each group. With four means, there are six possible comparisons between groups:</p>

<ul>
<li>
<p>Page 1 compared to page 2</p>
</li>
<li>
<p>Page 1 compared to page 3</p>
</li>
<li>
<p>Page 1 compared to page 4</p>
</li>
<li>
<p>Page 2 compared to page 3</p>
</li>
<li>
<p>Page 2 compared to page 4</p>
</li>
<li>
<p>Page 3 compared to page 4</p>
</li>
</ul>

<p>The more <a data-type="indexterm" data-primary="pairwise comparisons" id="idm46522858077896"/>such <em>pairwise</em> comparisons we make, the greater the potential for being fooled by random chance (see <a data-type="xref" href="#MultipleTesting">“Multiple Testing”</a>).
Instead of worrying about all the different comparisons between individual pages we could possibly make, we can do a single overall
test that addresses the question, “Could all the pages have the same underlying stickiness, and the differences among them be due to the random way in which a common set of session times got allocated among the four pages?”</p>

<figure class="width-75"><div id="FourGroups" class="figure">
<img src="Images/psd2_0306.png" alt="Session times across four different web pages." width="1157" height="1156"/>
<h6><span class="label">Figure 3-6. </span>Boxplots of the four groups show considerable differences among them</h6>
</div></figure>

<p>The procedure used to test this is ANOVA.
The basis for it can be seen in the following<a data-type="indexterm" data-primary="resampling" data-secondary="permutation tests" id="idm46522858072680"/> resampling procedure (specified here for the A/B/C/D test of web page <span class="keep-together">stickiness</span>):</p>
<ol>
<li>
<p>Combine all the data together in a single box.</p>
</li>
<li>
<p>Shuffle and draw out four resamples of five values each.</p>
</li>
<li>
<p>Record the mean of each of the four groups.</p>
</li>
<li>
<p>Record the variance among the four group means.</p>
</li>
<li>
<p>Repeat steps 2–4 many (say, 1,000) times.</p>
</li>

</ol>

<p>What proportion of the time did the resampled variance exceed the observed variance?  This is the p-value.</p>

<p>This type of permutation test<a data-type="indexterm" data-primary="permutation tests" data-secondary="for ANOVA" data-secondary-sortas="ANOVA" id="idm46522858064712"/> is a bit more involved than the type used in <a data-type="xref" href="#Permutation">“Permutation Test”</a>.
Fortunately, the <code>aovp</code> function in the <code>lmPerm</code> package computes a permutation test for this case:</p>

<pre data-type="programlisting" data-code-language="r"><code class="o">&gt;</code> <code class="nf">library</code><code class="p">(</code><code class="n">lmPerm</code><code class="p">)</code>
<code class="o">&gt;</code> <code class="nf">summary</code><code class="p">(</code><code class="nf">aovp</code><code class="p">(</code><code class="n">Time</code> <code class="o">~</code> <code class="n">Page</code><code class="p">,</code> <code class="n">data</code><code class="o">=</code><code class="n">four_sessions</code><code class="p">))</code>
<code class="p">[</code><code class="m">1</code><code class="p">]</code> <code class="s">"Settings:  unique SS "</code>
<code class="n">Component</code> <code class="m">1</code> <code class="o">:</code>
            <code class="n">Df</code> <code class="n">R</code> <code class="n">Sum</code> <code class="n">Sq</code> <code class="n">R</code> <code class="n">Mean</code> <code class="n">Sq</code> <code class="n">Iter</code> <code class="nf">Pr</code><code class="p">(</code><code class="n">Prob</code><code class="p">)</code>
<code class="n">Page</code>         <code class="m">3</code>    <code class="m">831.4</code>    <code class="m">277.13</code> <code class="m">3104</code>  <code class="m">0.09278</code> <code class="n">.</code>
<code class="n">Residuals</code>   <code class="m">16</code>   <code class="m">1618.4</code>    <code class="m">101.15</code>
<code class="o">---</code>
<code class="n">Signif.</code> <code class="n">codes</code><code class="o">:</code>  <code class="m">0</code> <code class="s">'***'</code> <code class="m">0.001</code> <code class="s">'**'</code> <code class="m">0.01</code> <code class="s">'*'</code> <code class="m">0.05</code> <code class="s">'.'</code> <code class="m">0.1</code> <code class="s">' '</code> <code class="m">1</code></pre>

<p>The p-value, given by <code>Pr(Prob)</code>, is 0.09278.<a data-type="indexterm" data-primary="p-values" id="idm46522858058664"/>  In other words, given the same underlying stickiness, 9.3% of the time the response rate among four pages might differ as much as was actually observed, just by chance.  This degree of improbability falls short of the traditional statistical threshold of 5%, so we conclude that the difference among the four pages could have arisen by chance.</p>

<p>The column <code>Iter</code> lists the number of iterations taken in the permutation test.
The other columns correspond to a traditional ANOVA table and are described next.</p>

<p>In <em>Python</em>, we can compute the permutation test using the following code:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">observed_variance</code> <code class="o">=</code> <code class="n">four_sessions</code><code class="o">.</code><code class="n">groupby</code><code class="p">(</code><code class="s1">'Page'</code><code class="p">)</code><code class="o">.</code><code class="n">mean</code><code class="p">()</code><code class="o">.</code><code class="n">var</code><code class="p">()[</code><code class="mi">0</code><code class="p">]</code>
<code class="k">print</code><code class="p">(</code><code class="s1">'Observed means:'</code><code class="p">,</code> <code class="n">four_sessions</code><code class="o">.</code><code class="n">groupby</code><code class="p">(</code><code class="s1">'Page'</code><code class="p">)</code><code class="o">.</code><code class="n">mean</code><code class="p">()</code><code class="o">.</code><code class="n">values</code><code class="o">.</code><code class="n">ravel</code><code class="p">())</code>
<code class="k">print</code><code class="p">(</code><code class="s1">'Variance:'</code><code class="p">,</code> <code class="n">observed_variance</code><code class="p">)</code>

<code class="k">def</code> <code class="nf">perm_test</code><code class="p">(</code><code class="n">df</code><code class="p">):</code>
    <code class="n">df</code> <code class="o">=</code> <code class="n">df</code><code class="o">.</code><code class="n">copy</code><code class="p">()</code>
    <code class="n">df</code><code class="p">[</code><code class="s1">'Time'</code><code class="p">]</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">permutation</code><code class="p">(</code><code class="n">df</code><code class="p">[</code><code class="s1">'Time'</code><code class="p">]</code><code class="o">.</code><code class="n">values</code><code class="p">)</code>
    <code class="k">return</code> <code class="n">df</code><code class="o">.</code><code class="n">groupby</code><code class="p">(</code><code class="s1">'Page'</code><code class="p">)</code><code class="o">.</code><code class="n">mean</code><code class="p">()</code><code class="o">.</code><code class="n">var</code><code class="p">()[</code><code class="mi">0</code><code class="p">]</code>

<code class="n">perm_variance</code> <code class="o">=</code> <code class="p">[</code><code class="n">perm_test</code><code class="p">(</code><code class="n">four_sessions</code><code class="p">)</code> <code class="k">for</code> <code class="n">_</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">3000</code><code class="p">)]</code>
<code class="k">print</code><code class="p">(</code><code class="s1">'Pr(Prob)'</code><code class="p">,</code> <code class="n">np</code><code class="o">.</code><code class="n">mean</code><code class="p">([</code><code class="n">var</code> <code class="o">&gt;</code> <code class="n">observed_variance</code> <code class="k">for</code> <code class="n">var</code> <code class="ow">in</code> <code class="n">perm_variance</code><code class="p">]))</code></pre>








<section data-type="sect2" data-pdf-bookmark="F-Statistic"><div class="sect2" id="FStatistic">
<h2>F-Statistic</h2>

<p>Just like the t-test can be used instead of a permutation test for comparing the mean of <a data-type="indexterm" data-primary="F-statistic" id="idm46522857861112"/><a data-type="indexterm" data-primary="analysis of variance (ANOVA)" data-secondary="F-statistic" id="idm46522857860408"/>two groups,
there is a statistical test for ANOVA based on the  <em>F-statistic</em>.
The F-statistic is based on the ratio of the variance across group means (i.e., the treatment effect) to the variance due to residual error.
The higher this ratio, the more statistically significant the result.
If the data follows a normal distribution,
then statistical theory dictates that the statistic should have a certain distribution.
Based on this, it is possible to compute a p-value.</p>

<p class="pagebreak-before">In <em>R</em>, we can compute an <em>ANOVA table</em> using the <code>aov</code> function:</p>

<pre data-type="programlisting" data-code-language="r"><code class="o">&gt;</code> <code class="nf">summary</code><code class="p">(</code><code class="nf">aov</code><code class="p">(</code><code class="n">Time</code> <code class="o">~</code> <code class="n">Page</code><code class="p">,</code> <code class="n">data</code><code class="o">=</code><code class="n">four_sessions</code><code class="p">))</code>
            <code class="n">Df</code> <code class="n">Sum</code> <code class="n">Sq</code> <code class="n">Mean</code> <code class="n">Sq</code> <code class="bp">F</code> <code class="n">value</code> <code class="nf">Pr</code><code class="p">(</code><code class="o">&gt;</code><code class="bp">F</code><code class="p">)</code>
<code class="n">Page</code>         <code class="m">3</code>  <code class="m">831.4</code>   <code class="m">277.1</code>    <code class="m">2.74</code> <code class="m">0.0776</code> <code class="n">.</code>
<code class="n">Residuals</code>   <code class="m">16</code> <code class="m">1618.4</code>   <code class="m">101.2</code>
<code class="o">---</code>
<code class="n">Signif.</code> <code class="n">codes</code><code class="o">:</code>  <code class="m">0</code> ‘<code class="o">***</code>’ <code class="m">0.001</code> ‘<code class="o">**</code>’ <code class="m">0.01</code> ‘<code class="o">*</code>’ <code class="m">0.05</code> ‘<code class="n">.’</code> <code class="m">0.1</code> ‘ ’ <code class="m">1</code></pre>

<p>The <code>statsmodels</code> package provides an ANOVA implementation in <em>Python</em>:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">model</code> <code class="o">=</code> <code class="n">smf</code><code class="o">.</code><code class="n">ols</code><code class="p">(</code><code class="s1">'Time ~ Page'</code><code class="p">,</code> <code class="n">data</code><code class="o">=</code><code class="n">four_sessions</code><code class="p">)</code><code class="o">.</code><code class="n">fit</code><code class="p">()</code>

<code class="n">aov_table</code> <code class="o">=</code> <code class="n">sm</code><code class="o">.</code><code class="n">stats</code><code class="o">.</code><code class="n">anova_lm</code><code class="p">(</code><code class="n">model</code><code class="p">)</code>
<code class="n">aov_table</code></pre>

<p>The output from the <em>Python</em> code is almost identical to that from <em>R</em>.</p>

<p><code>Df</code> is “degrees of freedom,” <code>Sum Sq</code> is “sum of squares,” <code>Mean Sq</code> is “mean squares” (short for mean-squared deviations), and <code>F value</code> is the F-statistic.<a data-type="indexterm" data-primary="sum of squares (SS)" id="idm46522857821384"/><a data-type="indexterm" data-primary="degrees of freedom" id="idm46522857820760"/>
For the grand average, sum of squares is the departure of the grand average from 0, squared, times 20 (the number of observations).
The degrees of freedom for the grand average is 1, by definition.</p>

<p>For the treatment means, the degrees of freedom is 3 (once three values are set, and then the grand average is set, the other treatment mean cannot vary).
Sum of squares for the treatment means is the sum of squared departures between the treatment means and the grand average.</p>

<p>For the residuals, degrees of freedom is 20 (all observations can vary), and SS is the sum of squared difference between the individual observations and the treatment means.
Mean squares (MS) is the sum of squares divided by the degrees of freedom.</p>

<p>The F-statistic is MS(treatment)/MS(error).
The F value thus depends only on this ratio and can be compared to a standard F-distribution to determine whether the differences among treatment means are greater than would be expected  in random chance variation.</p>
<div data-type="note" epub:type="note"><h1>Decomposition of Variance</h1>
<p>Observed values in a data set can be considered sums of different components.
For any observed<a data-type="indexterm" data-primary="analysis of variance (ANOVA)" data-secondary="decomposition of variance" id="idm46522857816280"/><a data-type="indexterm" data-primary="decomposition of variance" id="idm46522857815320"/> data value within a data set, we can break it down into the grand average, the treatment effect, and the residual error. We call this a “decomposition of variance”:</p>
<ol>
<li>
<p>Start with grand average (173.75 for web page stickiness data).</p>
</li>
<li>
<p>Add treatment effect, which might be negative (independent variable = web page).</p>
</li>
<li>
<p>Add residual error, which might be negative.</p>
</li>

</ol>

<p>Thus the decomposition of the variance for the top-left value in the A/B/C/D test table is as follows:</p>
<ol>
<li>
<p>Start with grand average: 173.75.</p>
</li>
<li>
<p>Add treatment (group) effect: –1.75 (172 – 173.75).</p>
</li>
<li>
<p>Add residual: –8 (164 – 172).</p>
</li>
<li>
<p>Equals: 164.</p>
</li>

</ol>
</div>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Two-Way ANOVA"><div class="sect2" id="idm46522857987016">
<h2>Two-Way ANOVA</h2>

<p>The A/B/C/D test just described is a “one-way” ANOVA, in which we have one factor (group) that is varying.<a data-type="indexterm" data-primary="one-way ANOVA" id="idm46522857801544"/><a data-type="indexterm" data-primary="analysis of variance (ANOVA)" data-secondary="two-way ANOVA" id="idm46522857800840"/><a data-type="indexterm" data-primary="two-way ANOVA" id="idm46522857799880"/>
We could have a second factor involved—say, “weekend versus weekday”—with data collected on each combination (group A weekend, group A weekday, group B weekend, etc.).
This would be a “two-way ANOVA,” and we would handle it in similar fashion to the one-way ANOVA by identifying the “interaction effect.”
After identifying the grand average effect and the treatment effect, we then separate the weekend and weekday observations for each group and find the difference between the averages for those subsets and the treatment average.</p>

<p>You can see that ANOVA and then two-way ANOVA are<a data-type="indexterm" data-primary="regression" data-secondary="ANOVA as first step toward statistical model" id="idm46522857798072"/> the first steps on the road toward a full statistical model, such as regression and logistic regression, in which multiple factors and their effects can be modeled (see <a data-type="xref" href="ch04.xhtml#Regression">Chapter 4</a>).</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46522857795752">
<h5>Key Ideas</h5>
<ul>
<li>
<p>ANOVA is a statistical procedure for analyzing the results of an experiment with multiple groups.</p>
</li>
<li>
<p>It is the extension of similar procedures for the A/B test, used to assess whether the overall variation among groups is within the range of chance variation.</p>
</li>
<li>
<p>A useful outcome of ANOVA is the identification of variance components associated with group treatments, interaction effects, and errors.</p>
</li>
</ul>
</div></aside>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Further Reading"><div class="sect2" id="idm46522857615960">
<h2>Further Reading</h2>

<ul>
<li>
<p><em>Introductory Statistics and Analytics: A Resampling Perspective</em> by Peter Bruce (Wiley, 2014) has a chapter on ANOVA.</p>
</li>
<li>
<p><em>Introduction to Design and Analysis of Experiments</em> by George Cobb (Wiley, 2008) is a comprehensive and readable treatment of its subject.<a data-type="indexterm" data-primary="variance" data-secondary="analysis of (ANOVA)" data-startref="ix_varAN" id="idm46522857612552"/><a data-type="indexterm" data-primary="statistical experiments and significance testing" data-secondary="analysis of variance (ANOVA)" data-startref="ix_statexANOVA" id="idm46522857611336"/><a data-type="indexterm" data-primary="analysis of variance (ANOVA)" data-startref="ix_ANOVA" id="idm46522857610008"/></p>
</li>
</ul>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Chi-Square Test"><div class="sect1" id="chi-square">
<h1>Chi-Square Test</h1>

<p>Web testing often goes beyond A/B testing and tests multiple treatments at once.<a data-type="indexterm" data-primary="statistical experiments and significance testing" data-secondary="chi-square test" id="ix_statexchi"/><a data-type="indexterm" data-primary="chi-square test" id="ix_chisq"/>
The chi-square test is used with count data to test how well it fits some expected distribution.
The most common use of the <em>chi-square</em> statistic in statistical practice is with <math alttext="r times c">
  <mrow>
    <mi>r</mi>
    <mo>×</mo>
    <mi>c</mi>
  </mrow>
</math> contingency tables, to assess whether the null hypothesis of independence among variables is reasonable (see also <a data-type="xref" href="ch02.xhtml#chi-square-dist">“Chi-Square Distribution”</a>).</p>

<p>The chi-square test was <a href="https://oreil.ly/cEubO">originally developed by Karl Pearson in 1900</a>. The <a data-type="indexterm" data-primary="Pearson's chi-square test" id="idm46522857599544"/>term <em>chi</em> comes from the Greek letter Χ used by Pearson in the article.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46522857598104">
<h5>Key Terms for Chi-Square Test</h5><dl>
<dt class="horizontal"><strong><em>Chi-square statistic</em></strong></dt>
<dd>
<p>A measure of the extent to which some observed data departs from expectation.<a data-type="indexterm" data-primary="chi-square statistic" id="idm46522857595272"/></p>
</dd>
<dt class="horizontal"><strong><em>Expectation</em> or <em>expected</em></strong></dt>
<dd>
<p>How we would expect the data to turn out under some assumption, typically the null hypothesis.<a data-type="indexterm" data-primary="expectation or expected" id="idm46522857592200"/></p>
</dd>
</dl>
</div></aside>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p><math alttext="r times c">
  <mrow>
    <mi>r</mi>
    <mo>×</mo>
    <mi>c</mi>
  </mrow>
</math> means “rows by columns”—a 2 × 3 table has two rows and three columns.</p>
</div>








<section data-type="sect2" data-pdf-bookmark="Chi-Square Test: A Resampling Approach"><div class="sect2" id="idm46522857587416">
<h2>Chi-Square Test: A Resampling Approach</h2>

<p>Suppose you are <a data-type="indexterm" data-primary="resampling" data-secondary="using in chi-square test" id="idm46522857585672"/><a data-type="indexterm" data-primary="chi-square test" data-secondary="resampling approach" id="idm46522857584600"/><a data-type="indexterm" data-primary="web testing" data-secondary="click rate for three different headlines" id="idm46522857583656"/>testing three different headlines—A, B, and C—and you run them each on 1,000 visitors, with the results shown in <a data-type="xref" href="#table0304">Table 3-4</a>.</p>
<table id="table0304">
<caption><span class="label">Table 3-4. </span>Web testing results for three different headlines</caption>
<thead>
<tr>
<th/>
<th>Headline A</th>
<th>Headline B</th>
<th>Headline C</th>
</tr>
</thead>
<tbody>
<tr>
<td><p>Click</p></td>
<td><p>14</p></td>
<td><p>8</p></td>
<td><p>12</p></td>
</tr>
<tr>
<td><p>No-click</p></td>
<td><p>986</p></td>
<td><p>992</p></td>
<td><p>988</p></td>
</tr>
</tbody>
</table>

<p>The headlines certainly appear to differ. Headline A returns nearly twice the click rate of B.
The actual numbers are small, though.
A resampling procedure can test whether the click rates differ to an extent greater than chance might cause.<a data-type="indexterm" data-primary="null hypothesis" data-secondary="in click rate testing for web headlines" id="idm46522857571336"/>
For this test, we need to have the “expected” distribution of clicks, and in this case, that would be under the null hypothesis assumption that all three headlines share the same click rate, for an overall click rate of 34/3,000.
Under this assumption, our contingency table would look like <a data-type="xref" href="#table0305">Table 3-5</a>.</p>
<table id="table0305">
<caption><span class="label">Table 3-5. </span>Expected if all three headlines have the same click rate (null hypothesis)</caption>
<thead>
<tr>
<th/>
<th>Headline A</th>
<th>Headline B</th>
<th>Headline C</th>
</tr>
</thead>
<tbody>
<tr>
<td><p>Click</p></td>
<td><p>11.33</p></td>
<td><p>11.33</p></td>
<td><p>11.33</p></td>
</tr>
<tr>
<td><p>No-click</p></td>
<td><p>988.67</p></td>
<td><p>988.67</p></td>
<td><p>988.67</p></td>
</tr>
</tbody>
</table>

<p>The <em>Pearson residual</em> is <a data-type="indexterm" data-primary="Pearson residuals" id="idm46522857558616"/>defined as:</p>
<div data-type="equation">
<math display="block">
  <mrow>
    <mi>R</mi>
    <mo>=</mo>
    <mfrac><mrow><mtext>Observed</mtext><mo>-</mo><mtext>Expected</mtext></mrow> <msqrt><mtext>Expected</mtext></msqrt></mfrac>
  </mrow>
</math>
</div>

<p><em>R</em> measures the extent to which the actual counts differ from these expected counts (see <a data-type="xref" href="#table0306">Table 3-6</a>).</p>
<table id="table0306">
<caption><span class="label">Table 3-6. </span>Pearson residuals</caption>
<thead>
<tr>
<th/>
<th>Headline A</th>
<th>Headline B</th>
<th>Headline C</th>
</tr>
</thead>
<tbody>
<tr>
<td><p>Click</p></td>
<td><p>0.792</p></td>
<td><p>–0.990</p></td>
<td><p>0.198</p></td>
</tr>
<tr>
<td><p>No-click</p></td>
<td><p>–0.085</p></td>
<td><p>0.106</p></td>
<td><p>–0.021</p></td>
</tr>
</tbody>
</table>

<p>The chi-square statistic is <a data-type="indexterm" data-primary="chi-square statistic" id="idm46522857541400"/>defined as the sum of the squared Pearson residuals:</p>
<div data-type="equation">
<math display="block">
 <mrow>
   <mi>Χ</mi>
   <mo>=</mo>
   <munderover><mo>∑</mo> <mi>i</mi> <mi>r</mi> </munderover>
   <munderover><mo>∑</mo> <mi>j</mi> <mi>c</mi> </munderover>
   <msup><mi>R</mi> <mn>2</mn> </msup>
 </mrow>
</math>
</div>

<p>where <em>r</em> and <em>c</em> are the number of rows and columns, respectively.
The chi-square statistic for this example is 1.666.
Is that more than could reasonably occur in a chance model?</p>

<p>We can test with <a data-type="indexterm" data-primary="resampling" data-secondary="using in chi-square test" id="idm46522857531192"/>this resampling algorithm:</p>
<ol>
<li>
<p>Constitute a box with 34 ones (clicks) and 2,966 zeros (no clicks).</p>
</li>
<li>
<p>Shuffle, take three separate samples of 1,000, and count the clicks in each.</p>
</li>
<li>
<p>Find the squared differences between the shuffled counts and the expected counts and sum them.</p>
</li>
<li>
<p>Repeat steps 2 and 3, say, 1,000 times.</p>
</li>
<li>
<p>How often does the resampled sum of squared deviations exceed the observed?  That’s the p-value.</p>
</li>

</ol>

<p>The function <code>chisq.test</code> can be used to compute a resampled chi-square statistic in <em>R</em>.
For the click data, the chi-square test is:</p>

<pre data-type="programlisting" data-code-language="r"><code class="o">&gt;</code> <code class="nf">chisq.test</code><code class="p">(</code><code class="n">clicks</code><code class="p">,</code> <code class="n">simulate.p.value</code><code class="o">=</code><code class="kc">TRUE</code><code class="p">)</code>

	<code class="n">Pearson</code><code class="s">'</code><code class="err">s Chi-squared test with simulated p-value (based on 2000 replicates)</code>

<code class="n">data</code><code class="o">:</code>  <code class="n">clicks</code>
<code class="n">X</code><code class="o">-</code><code class="n">squared</code> <code class="o">=</code> <code class="m">1.6659</code><code class="p">,</code> <code class="n">df</code> <code class="o">=</code> <code class="kc">NA</code><code class="p">,</code> <code class="n">p</code><code class="o">-</code><code class="n">value</code> <code class="o">=</code> <code class="m">0.4853</code></pre>

<p>The test shows that this result could easily have been obtained by randomness.</p>

<p>To<a data-type="indexterm" data-primary="permutation tests" data-secondary="for chi-square test" data-secondary-sortas="chi" id="idm46522857492104"/> run a permutation test in <em>Python</em>, use the following implementation:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">box</code> <code class="o">=</code> <code class="p">[</code><code class="mi">1</code><code class="p">]</code> <code class="o">*</code> <code class="mi">34</code>
<code class="n">box</code><code class="o">.</code><code class="n">extend</code><code class="p">([</code><code class="mi">0</code><code class="p">]</code> <code class="o">*</code> <code class="mi">2966</code><code class="p">)</code>
<code class="n">random</code><code class="o">.</code><code class="n">shuffle</code><code class="p">(</code><code class="n">box</code><code class="p">)</code>

<code class="k">def</code> <code class="nf">chi2</code><code class="p">(</code><code class="n">observed</code><code class="p">,</code> <code class="n">expected</code><code class="p">):</code>
    <code class="n">pearson_residuals</code> <code class="o">=</code> <code class="p">[]</code>
    <code class="k">for</code> <code class="n">row</code><code class="p">,</code> <code class="n">expect</code> <code class="ow">in</code> <code class="nb">zip</code><code class="p">(</code><code class="n">observed</code><code class="p">,</code> <code class="n">expected</code><code class="p">):</code>
        <code class="n">pearson_residuals</code><code class="o">.</code><code class="n">append</code><code class="p">([(</code><code class="n">observe</code> <code class="o">-</code> <code class="n">expect</code><code class="p">)</code> <code class="o">**</code> <code class="mi">2</code> <code class="o">/</code> <code class="n">expect</code>
                                  <code class="k">for</code> <code class="n">observe</code> <code class="ow">in</code> <code class="n">row</code><code class="p">])</code>
    <code class="c1"># return sum of squares</code>
    <code class="k">return</code> <code class="n">np</code><code class="o">.</code><code class="n">sum</code><code class="p">(</code><code class="n">pearson_residuals</code><code class="p">)</code>

<code class="n">expected_clicks</code> <code class="o">=</code> <code class="mi">34</code> <code class="o">/</code> <code class="mi">3</code>
<code class="n">expected_noclicks</code> <code class="o">=</code> <code class="mi">1000</code> <code class="o">-</code> <code class="n">expected_clicks</code>
<code class="n">expected</code> <code class="o">=</code> <code class="p">[</code><code class="mi">34</code> <code class="o">/</code> <code class="mi">3</code><code class="p">,</code> <code class="mi">1000</code> <code class="o">-</code> <code class="mi">34</code> <code class="o">/</code> <code class="mi">3</code><code class="p">]</code>
<code class="n">chi2observed</code> <code class="o">=</code> <code class="n">chi2</code><code class="p">(</code><code class="n">clicks</code><code class="o">.</code><code class="n">values</code><code class="p">,</code> <code class="n">expected</code><code class="p">)</code>

<code class="k">def</code> <code class="nf">perm_fun</code><code class="p">(</code><code class="n">box</code><code class="p">):</code>
    <code class="n">sample_clicks</code> <code class="o">=</code> <code class="p">[</code><code class="nb">sum</code><code class="p">(</code><code class="n">random</code><code class="o">.</code><code class="n">sample</code><code class="p">(</code><code class="n">box</code><code class="p">,</code> <code class="mi">1000</code><code class="p">)),</code>
                     <code class="nb">sum</code><code class="p">(</code><code class="n">random</code><code class="o">.</code><code class="n">sample</code><code class="p">(</code><code class="n">box</code><code class="p">,</code> <code class="mi">1000</code><code class="p">)),</code>
                     <code class="nb">sum</code><code class="p">(</code><code class="n">random</code><code class="o">.</code><code class="n">sample</code><code class="p">(</code><code class="n">box</code><code class="p">,</code> <code class="mi">1000</code><code class="p">))]</code>
    <code class="n">sample_noclicks</code> <code class="o">=</code> <code class="p">[</code><code class="mi">1000</code> <code class="o">-</code> <code class="n">n</code> <code class="k">for</code> <code class="n">n</code> <code class="ow">in</code> <code class="n">sample_clicks</code><code class="p">]</code>
    <code class="k">return</code> <code class="n">chi2</code><code class="p">([</code><code class="n">sample_clicks</code><code class="p">,</code> <code class="n">sample_noclicks</code><code class="p">],</code> <code class="n">expected</code><code class="p">)</code>

<code class="n">perm_chi2</code> <code class="o">=</code> <code class="p">[</code><code class="n">perm_fun</code><code class="p">(</code><code class="n">box</code><code class="p">)</code> <code class="k">for</code> <code class="n">_</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">2000</code><code class="p">)]</code>

<code class="n">resampled_p_value</code> <code class="o">=</code> <code class="nb">sum</code><code class="p">(</code><code class="n">perm_chi2</code> <code class="o">&gt;</code> <code class="n">chi2observed</code><code class="p">)</code> <code class="o">/</code> <code class="nb">len</code><code class="p">(</code><code class="n">perm_chi2</code><code class="p">)</code>
<code class="k">print</code><code class="p">(</code><code class="n">f</code><code class="s1">'Observed chi2: {chi2observed:.4f}'</code><code class="p">)</code>
<code class="k">print</code><code class="p">(</code><code class="n">f</code><code class="s1">'Resampled p-value: {resampled_p_value:.4f}'</code><code class="p">)</code></pre>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Chi-Square Test: Statistical Theory"><div class="sect2" id="idm46522857586792">
<h2>Chi-Square Test: Statistical Theory</h2>

<p>Asymptotic statistical theory <a data-type="indexterm" data-primary="chi-square test" data-secondary="statistical theory" id="idm46522857486472"/>shows that the distribution of the chi-square statistic can be approximated  <a data-type="indexterm" data-primary="chi-square distribution" id="idm46522857215176"/>by a <em>chi-square distribution</em> (see <a data-type="xref" href="ch02.xhtml#chi-square-dist">“Chi-Square Distribution”</a>).
The appropriate standard chi-square <a data-type="indexterm" data-primary="degrees of freedom" data-secondary="for chi-square distribution" id="idm46522857213048"/>distribution is determined by the <em>degrees of freedom</em> (see <a data-type="xref" href="#DOF">“Degrees of Freedom”</a>).
For a contingency table, the degrees of freedom are  related to the number of rows (<em>r</em>) and columns (<em>c</em>) as <span class="keep-together">follows</span>:</p>
<div data-type="equation">
<math display="block">
  <mrow>
    <mtext>degrees</mtext>
    <mspace width="4.pt"/>
    <mtext>of</mtext>
    <mspace width="4.pt"/>
    <mtext>freedom</mtext>
    <mo>=</mo>
    <mo>(</mo>
    <mi>r</mi>
    <mo>-</mo>
    <mn>1</mn>
    <mo>)</mo>
    <mo>×</mo>
    <mo>(</mo>
    <mi>c</mi>
    <mo>-</mo>
    <mn>1</mn>
    <mo>)</mo>
  </mrow>
</math>
</div>

<p>The chi-square distribution is typically skewed, with a long tail to the right; see <a data-type="xref" href="#ChiSquareDist">Figure 3-7</a> for the distribution with 1, 2, 5, and 20 degrees of freedom.
The further out on the chi-square distribution the observed statistic is, the lower the p-value.</p>

<p>The function <code>chisq.test</code> can  be used to compute the p-value using the chi-square distribution as a reference:</p>

<pre data-type="programlisting" data-code-language="r"><code class="o">&gt;</code> <code class="nf">chisq.test</code><code class="p">(</code><code class="n">clicks</code><code class="p">,</code> <code class="n">simulate.p.value</code><code class="o">=</code><code class="kc">FALSE</code><code class="p">)</code>

	<code class="n">Pearson</code><code class="s">'</code><code class="err">s Chi-squared test</code>

<code class="n">data</code><code class="o">:</code>  <code class="n">clicks</code>
<code class="n">X</code><code class="o">-</code><code class="n">squared</code> <code class="o">=</code> <code class="m">1.6659</code><code class="p">,</code> <code class="n">df</code> <code class="o">=</code> <code class="m">2</code><code class="p">,</code> <code class="n">p</code><code class="o">-</code><code class="n">value</code> <code class="o">=</code> <code class="m">0.4348</code></pre>

<p>In <em>Python</em>, use the function <code>scipy.stats.chi2_contingency</code>:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">chisq</code><code class="p">,</code> <code class="n">pvalue</code><code class="p">,</code> <code class="n">df</code><code class="p">,</code> <code class="n">expected</code> <code class="o">=</code> <code class="n">stats</code><code class="o">.</code><code class="n">chi2_contingency</code><code class="p">(</code><code class="n">clicks</code><code class="p">)</code>
<code class="k">print</code><code class="p">(</code><code class="n">f</code><code class="s1">'Observed chi2: {chi2observed:.4f}'</code><code class="p">)</code>
<code class="k">print</code><code class="p">(</code><code class="n">f</code><code class="s1">'p-value: {pvalue:.4f}'</code><code class="p">)</code></pre>

<p>The p-value is a little less than the resampling p-value; this is because the chi-square distribution is only an approximation of the actual distribution of the statistic.<a data-type="indexterm" data-primary="p-values" data-secondary="chi-square distribution and" id="idm46522857149224"/></p>

<figure><div id="ChiSquareDist" class="figure">
<img src="Images/psd2_0307.png" alt="Chi-square distribution for 1, 2, 5 and 20 degrees of freedom" width="1156" height="706"/>
<h6><span class="label">Figure 3-7. </span>Chi-square distribution with various degrees of freedom</h6>
</div></figure>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Fisher’s Exact Test"><div class="sect2" id="idm46522857487672">
<h2>Fisher’s Exact Test</h2>

<p>The chi-square distribution is a good approximation of the shuffled resampling test just described, except when counts are extremely low (single digits, especially five or fewer).<a data-type="indexterm" data-primary="exact tests" data-secondary="Fisher's exact test" id="ix_exacttst"/><a data-type="indexterm" data-primary="chi-square test" data-secondary="Fisher's exact test" id="ix_chisqFish"/><a data-type="indexterm" data-primary="Fisher's exact test" id="ix_Fishex"/>
In such cases, the resampling procedure will yield more accurate p-values.
In fact, most statistical software has a procedure to actually enumerate <em>all</em> the possible rearrangements (permutations) that can occur, tabulate their frequencies, and determine exactly how extreme the observed result is.
This is called <em>Fisher’s exact test</em> after the great statistician R. A. Fisher.
<em>R</em> code for Fisher’s exact test is simple in its basic form:</p>

<pre data-type="programlisting" data-code-language="r"><code class="o">&gt;</code> <code class="nf">fisher.test</code><code class="p">(</code><code class="n">clicks</code><code class="p">)</code>

	<code class="n">Fisher</code><code class="s">'</code><code class="err">s Exact Test for Count Data</code>

<code class="n">data</code><code class="o">:</code>  <code class="n">clicks</code>
<code class="n">p</code><code class="o">-</code><code class="n">value</code> <code class="o">=</code> <code class="m">0.4824</code>
<code class="n">alternative</code> <code class="n">hypothesis</code><code class="o">:</code> <code class="n">two.sided</code></pre>

<p>The p-value is very close to the p-value of 0.4853 obtained using the resampling method.</p>

<p>Where some counts are very low but others are quite high (e.g., the denominator in a conversion rate), it may be necessary to do a shuffled permutation test instead of a full exact test, due to the difficulty of calculating all possible permutations.
The <span class="keep-together">preceding</span> <em>R</em> function has several arguments that control whether to use this <span class="keep-together">approximation</span> (<code>simulate.p.value=TRUE or FALSE</code>), how many iterations should be used (<code>B=...</code>), and a computational constraint (<code>workspace=...</code>) that limits how far calculations for the <em>exact</em> result should go.</p>

<p>There is no implementation of Fisher’s exact test easily available in <em>Python</em>.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46522857021576">
<h5>Detecting Scientific Fraud</h5>
<p>An interesting example is provided by the case of Tufts University researcher Thereza Imanishi-Kari, who was accused in 1991 of fabricating data in her research.<a data-type="indexterm" data-primary="scientific fraud, detecting" id="idm46522857019832"/>
Congressman John Dingell became involved, and the case eventually led to the resignation of her colleague, David Baltimore, from the presidency of Rockefeller University.</p>

<p>One element in the case rested on statistical evidence regarding the expected distribution of digits in her laboratory data, where each observation had many digits.
Investigators focused on the <em>interior</em> digits (ignoring the first digit and last digit of a number), which would be expected to follow a <em>uniform random</em> distribution.<a data-type="indexterm" data-primary="uniform random distribution" id="idm46522857017256"/>
That is, they would occur randomly, with each digit having equal probability of occurring (the lead digit might be predominantly one value, and the final digits might be affected by rounding).  <a data-type="xref" href="#table0307">Table 3-7</a> lists the frequencies of interior digits from the actual data in the case.</p>
<table id="table0307">
<caption><span class="label">Table 3-7. </span>Frequency of interior digits in laboratory data</caption>
<thead>
<tr>
<th>Digit</th>
<th>Frequency</th>
</tr>
</thead>
<tbody>
<tr>
<td><p>0</p></td>
<td><p>14</p></td>
</tr>
<tr>
<td><p>1</p></td>
<td><p>71</p></td>
</tr>
<tr>
<td><p>2</p></td>
<td><p>7</p></td>
</tr>
<tr>
<td><p>3</p></td>
<td><p>65</p></td>
</tr>
<tr>
<td><p>4</p></td>
<td><p>23</p></td>
</tr>
<tr>
<td><p>5</p></td>
<td><p>19</p></td>
</tr>
<tr>
<td><p>6</p></td>
<td><p>12</p></td>
</tr>
<tr>
<td><p>7</p></td>
<td><p>45</p></td>
</tr>
<tr>
<td><p>8</p></td>
<td><p>53</p></td>
</tr>
<tr>
<td><p>9</p></td>
<td><p>6</p></td>
</tr>
</tbody>
</table>

<p>The distribution of the 315 digits, shown in <a data-type="xref" href="#ImanishiHist">Figure 3-8</a>, certainly looks nonrandom.</p>

<p>Investigators calculated the departure from expectation (31.5—that’s how often each digit would occur in a strictly uniform distribution) and used a chi-square test (a resampling procedure could equally have been used) to show that the actual distribution was well beyond the range of normal chance variation, indicating the data might have been fabricated.  (Note:  Imanishi-Kari was ultimately exonerated after a lengthy proceeding.)</p>

<figure><div id="ImanishiHist" class="figure">
<img src="Images/psd2_0308.png" alt="A histogram of the interior digits from data in the case involving Imanishi-Kari." width="1156" height="1156"/>
<h6><span class="label">Figure 3-8. </span>Frequency histogram for Imanishi-Kari lab data</h6>
</div></figure>
</div></aside>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Relevance for Data Science"><div class="sect2" id="idm46522856969528">
<h2>Relevance for Data Science</h2>

<p>The chi-square test, or <a data-type="indexterm" data-primary="exact tests" data-secondary="Fisher's exact test" data-startref="ix_exacttst" id="idm46522856967992"/><a data-type="indexterm" data-primary="chi-square test" data-secondary="Fisher's exact test" data-startref="ix_chisqFish" id="idm46522856966744"/><a data-type="indexterm" data-primary="Fisher's exact test" data-startref="ix_Fishex" id="idm46522856965528"/><a data-type="indexterm" data-primary="chi-square test" data-secondary="relevance for data science" id="idm46522856964584"/><a data-type="indexterm" data-primary="Fisher's exact test" data-secondary="relevance for data science" id="idm46522856963624"/><a data-type="indexterm" data-primary="data science" data-secondary="relevance of chi-square tests" id="idm46522856962664"/>Fisher’s exact test, is used when you want to know whether an effect is for real or might be the product of chance.  In most classical statistical applications of the chi-square test, its role is to establish statistical significance, which is typically needed before a study or an experiment can be published.  This is not so important for data scientists.
In most data science experiments, whether A/B or A/B/C…, the goal is not simply to establish statistical significance but rather to arrive at the best treatment.
For this purpose, multi-armed bandits (see <a data-type="xref" href="#bandits">“Multi-Arm Bandit Algorithm”</a>) offer a more complete solution.</p>

<p>One data science application of the chi-square test, especially Fisher’s exact version, is in determining appropriate sample sizes for web experiments.
These experiments often have very low click rates, and despite thousands of exposures, count rates might be too small to yield definitive conclusions in an experiment.
In such cases, Fisher’s exact test, the chi-square test, and other tests can be useful as a component of power and sample size calculations (see <a data-type="xref" href="#PowerSampleSize">“Power and Sample Size”</a>).</p>

<p>Chi-square tests are used widely in research by investigators in search of the elusive statistically significant p-value that will allow publication.<a data-type="indexterm" data-primary="feature selection" data-secondary="chi-square tests in" id="idm46522856957928"/>
Chi-square tests, or similar resampling simulations, are used in data science applications more as a filter to determine whether an effect or a feature is worthy of further consideration than as a formal test of significance.
For example, they are used in spatial statistics and mapping to determine whether spatial data conforms to a specified null distribution (e.g., are crimes concentrated in a certain area to a greater degree than random chance would allow?).
They can also be used in automated feature selection in machine learning, to assess class prevalence across features and identify features where the prevalence of a certain class is unusually high or low, in a way that is not compatible with random variation.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46522856955960">
<h5>Key Ideas</h5>
<ul>
<li>
<p>A common procedure in statistics is to test whether observed data counts are consistent with an assumption of independence (e.g., propensity to buy a particular item is independent of gender).</p>
</li>
<li>
<p>The chi-square distribution is the reference distribution (which embodies the assumption of independence) to which the observed calculated chi-square statistic must be compared.</p>
</li>
</ul>
</div></aside>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Further Reading"><div class="sect2" id="idm46522856952248">
<h2>Further Reading</h2>

<ul>
<li>
<p>R. A. Fisher’s famous “Lady Tasting Tea” example from the beginning of the 20th century remains a simple and effective illustration of his exact test. Google “Lady Tasting Tea,” and you will find a number of good writeups.</p>
</li>
<li>
<p>Stat Trek offers a <a href="https://oreil.ly/77DUf">good tutorial on the chi-square test</a>.<a data-type="indexterm" data-primary="statistical experiments and significance testing" data-secondary="chi-square test" data-startref="ix_statexchi" id="idm46522856948504"/><a data-type="indexterm" data-primary="chi-square test" data-startref="ix_chisq" id="idm46522856947160"/></p>
</li>
</ul>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Multi-Arm Bandit Algorithm"><div class="sect1" id="bandits">
<h1>Multi-Arm Bandit Algorithm</h1>

<p>Multi-arm bandits <a data-type="indexterm" data-primary="statistical experiments and significance testing" data-secondary="multi-arm bandits" id="ix_statexMAB"/><a data-type="indexterm" data-primary="multi-arm bandits" id="ix_mltiarm"/>offer an approach to testing, especially web testing, that allows explicit optimization and more rapid decision making than the traditional statistical approach to designing experiments.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46522856941192">
<h5>Key Terms for Multi-Arm Bandits</h5><dl>
<dt class="horizontal"><strong><em>Multi-arm bandit</em></strong></dt>
<dd>
<p>An imaginary slot machine with multiple arms for the customer to choose from, each with different payoffs, here taken to be an analogy for a multitreatment experiment.<a data-type="indexterm" data-primary="arms (multi-arm bandits)" id="idm46522856938024"/></p>
</dd>
<dt class="horizontal"><strong><em>Arm</em></strong></dt>
<dd>
<p>A treatment in an experiment (e.g., “headline A in a web test”).</p>
</dd>
<dt class="horizontal"><strong><em>Win</em></strong></dt>
<dd>
<p>The<a data-type="indexterm" data-primary="wins" id="idm46522856933656"/> experimental analog of a win at the slot machine (e.g., “customer clicks on the link”).</p>
</dd>
</dl>
</div></aside>

<p>A traditional A/B test involves data collected in an experiment, according to a specified design, to answer a specific question such as, “Which is better, treatment A or treatment B?”
The presumption is that once we get an answer to that question, the experimenting is over and we proceed to act on the results.</p>

<p>You can probably perceive several difficulties with that approach.
First, our answer may be inconclusive: “effect not proven.”
In other words, the results from the experiment may suggest an effect, but if there is an effect, we don’t have a big enough sample to prove it (to the satisfaction of the traditional statistical standards).
What decision do we take?
Second, we might want to begin taking advantage of results that come in prior to the conclusion of the experiment.
Third, we might want the right to change our minds or to try something different based on additional data that comes in after the experiment is over.
The traditional approach to experiments and hypothesis tests dates from the 1920s and is rather inflexible.
The advent of computer power and software has enabled more powerful flexible approaches.
Moreover, data science (and business in general) is not so worried about statistical significance, but concerned more with optimizing overall effort and results.<a data-type="indexterm" data-primary="bandit algorithms" data-seealso="multi-arm bandits" id="idm46522856930280"/></p>

<p>Bandit algorithms, which are very popular in web testing, allow you to test multiple treatments at once and reach conclusions faster than traditional statistical designs.<a data-type="indexterm" data-primary="web testing" data-secondary="popularity of bandit algorithms in" id="idm46522856928728"/>  They take their name from slot machines used in gambling, also termed one-armed bandits (since they are configured in such a way that they extract money from the gambler in a steady flow).<a data-type="indexterm" data-primary="slot machines used in gambling" id="idm46522856927448"/>
If you imagine a slot machine with more than one arm, each arm paying out at a different rate, you would have a multi-armed bandit, which is the full name for this algorithm.</p>

<p>Your goal is to win as much money as possible and, more specifically, to identify and settle on the winning arm sooner rather than later.
The challenge is that you don’t know at what overall rate the arms pay out—you only know the results of individual pulls on the arms.
Suppose each “win” is for the same amount, no matter which arm.
What differs is the probability of a win.
Suppose further that you initially try each arm 50 times and get the following results:</p>
<ul class="simplelist">
  <li>Arm A:  10 wins out of 50</li>
  <li>Arm B:  2 win out of 50</li>
  <li>Arm C:  4 wins out of 50</li>
</ul>

<p>One extreme approach is to say, “Looks like arm A is a winner—let’s quit trying the other arms and stick with A.”
This takes full advantage of the information from the initial trial.
If A is truly superior, we get the benefit of that early on.
On the other hand, if B or C is truly better, we lose any opportunity to discover that.
Another extreme approach is to say, “This all looks to be within the realm of chance—let’s keep pulling them all equally.”
This gives maximum opportunity for alternates to A to show themselves.
However, in the process, we are deploying what seem to be inferior treatments.
How long do we permit that?
Bandit algorithms take a hybrid approach:
we start pulling A more often, to take advantage of its apparent superiority, but we don’t abandon B and C.
We just pull them less often.
If A continues to outperform, we continue to shift resources (pulls) away from B and C and pull A more often.
If, on the other hand, C starts to do better, and A starts to do worse, we can shift pulls from A back to C.
If one of them turns out to be superior to A and this was hidden in the initial trial due to chance, it now has an opportunity to emerge with further <span class="keep-together">testing</span>.</p>

<p>Now think of applying this to web testing.  Instead of multiple slot machine arms, you might have multiple offers, headlines, colors, and so on being tested on a website.
Customers either click (a “win” for the merchant) or don’t click.
Initially, the offers are shown randomly and equally.
If, however, one offer starts to outperform the others, it can be shown (“pulled”) more often.
But what should the parameters of the algorithm that modifies the pull rates be?
What “pull rates” should we change to, and when should we change?</p>

<p>Here is <a data-type="indexterm" data-primary="A/B testing" data-secondary="epsilon-greedy algorithm for" id="idm46522856919768"/><a data-type="indexterm" data-primary="epsilon-greedy algorithm for A/B test" id="idm46522856918696"/>one simple algorithm, the epsilon-greedy algorithm for an A/B test:</p>
<ol>
<li>
<p>Generate a uniformly distributed random number between 0 and 1.</p>
</li>
<li>
<p>If the number lies between 0 and epsilon (where epsilon is a number between 0 and 1, typically fairly small), flip a fair coin (50/50 probability), and:</p>
<ol>
<li>
<p>If the coin is heads, show offer A.</p>
</li>
<li>
<p>If the coin is tails, show offer B.</p>
</li>

</ol>
</li>
<li>
<p>If the number is ≥ epsilon, show whichever offer has had the highest response rate to date.</p>
</li>

</ol>

<p>Epsilon is the single parameter that governs this algorithm.
If epsilon is 1, we end up with a standard simple A/B experiment (random allocation between A and B for each subject).
If epsilon is 0, we end up with a purely <em>greedy</em> algorithm—one that chooses the best available immediate option (a local optimum).<a data-type="indexterm" data-primary="greedy algorithms" id="idm46522856911288"/>  It seeks no further experimentation, simply assigning subjects (web visitors) to the best-performing treatment.</p>

<p>A more <a data-type="indexterm" data-primary="Thompson's sampling" id="idm46522856909720"/>sophisticated algorithm uses “Thompson’s sampling.”
This procedure “samples” (pulls a bandit arm) at each stage to maximize the probability of choosing <span class="keep-together">the best arm.</span>
Of course you don’t know which is the best arm—that’s the whole <span class="keep-together">problem!—</span>but as you observe the payoff with each successive draw, you gain more information.<a data-type="indexterm" data-primary="beta distribution" id="idm46522856907336"/><a data-type="indexterm" data-primary="Bayesian approach in Thompson's sampling" id="idm46522856906632"/>
Thompson’s sampling uses a Bayesian approach: some prior distribution of rewards is assumed initially, using what is called a <em>beta distribution</em> (this is a common mechanism for specifying prior information in a Bayesian problem).
As information accumulates from each draw, this information can be updated, allowing the selection of the next draw to be better optimized as far as choosing the right arm.</p>

<p>Bandit algorithms can efficiently handle 3+ treatments and move toward optimal selection of the “best.”
For traditional statistical testing procedures, the complexity of decision making for 3+ treatments far outstrips that of the traditional A/B test, and the advantage of bandit algorithms is much greater.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46522856904264">
<h5>Key Ideas</h5>
<ul>
<li>
<p>Traditional A/B tests envision a random sampling process, which can lead to excessive exposure to the inferior treatment.</p>
</li>
<li>
<p>Multi-arm bandits, in contrast, alter the sampling process to incorporate information learned during the experiment and reduce the frequency of the inferior treatment.<a data-type="indexterm" data-primary="A/B testing" data-secondary="multi-arm bandits versus" id="idm46522856901112"/></p>
</li>
<li>
<p>They also facilitate efficient treatment of more than two treatments.</p>
</li>
<li>
<p>There are different algorithms for shifting sampling probability away from the inferior treatment(s) and to the (presumed) superior one.</p>
</li>
</ul>
</div></aside>








<section data-type="sect2" data-pdf-bookmark="Further Reading"><div class="sect2" id="idm46522856897704">
<h2>Further Reading</h2>

<ul>
<li>
<p>An excellent short treatment of multi-arm bandit algorithms is found in <em>Bandit Algorithms for Website Optimization</em>, by John Myles White (O’Reilly, 2012).
White includes <em>Python</em> code, as well as the results of simulations to assess the performance of bandits.</p>
</li>
<li>
<p>For more (somewhat technical) information about Thompson sampling, see <a href="https://oreil.ly/OgWrG">“Analysis of Thompson Sampling for the Multi-armed Bandit Problem”</a> by Shipra Agrawal and Navin Goyal.<a data-type="indexterm" data-primary="statistical experiments and significance testing" data-secondary="multi-arm bandits" data-startref="ix_statexMAB" id="idm46522856893384"/><a data-type="indexterm" data-primary="multi-arm bandits" data-startref="ix_mltiarm" id="idm46522856891992"/></p>
</li>
</ul>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Power and Sample Size"><div class="sect1" id="PowerSampleSize">
<h1>Power and Sample Size</h1>

<p>If you run a web test, how do you decide how long it should run (i.e., how many impressions per treatment are needed)?<a data-type="indexterm" data-primary="statistical experiments and significance testing" data-secondary="power and sample size" id="ix_statexPSS"/><a data-type="indexterm" data-primary="power and sample size" id="ix_powsmpl"/><a data-type="indexterm" data-primary="samples" data-secondary="sample size, power and" id="ix_smplsze"/>
Despite what you may read in many guides to web testing, there is no good general guidance—it depends, mainly, on the frequency with which the desired goal is attained.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46522856884744">
<h5>Key Terms for Power and Sample Size</h5><dl>
<dt class="horizontal"><strong><em>Effect size</em></strong></dt>
<dd>
<p>The minimum size of the effect that you hope to be able to detect in a statistical test, such as “a 20% improvement in click rates.”</p>
</dd>
<dt class="horizontal"><strong><em>Power</em></strong></dt>
<dd>
<p>The probability of detecting a given effect size with a given sample size.<a data-type="indexterm" data-primary="effect size" id="idm46522856879752"/><a data-type="indexterm" data-primary="significance level" id="idm46522856879048"/></p>
</dd>
<dt class="horizontal"><strong><em>Significance level</em></strong></dt>
<dd>
<p>The statistical significance level at which the test will be conducted.</p>
</dd>
</dl>
</div></aside>

<p>One step in statistical calculations for sample size is to ask “Will a hypothesis test actually reveal a difference between treatments A and B?”
The outcome of a hypothesis test—the p-value—depends on what the real difference is between treatment A and treatment B.
It also depends on the luck of the draw—who gets selected for the groups in the experiment.
But it makes sense that the bigger the actual difference between treatments A and B, the greater the probability that our experiment will reveal it; and the smaller the difference, the more data will be needed to detect it.
To distinguish between a .350 hitter and a .200 hitter in baseball, not that many at-bats are needed.
To distinguish between a .300 hitter and a .280 hitter, a good many more at-bats will be needed.<a data-type="indexterm" data-primary="power and sample size" data-secondary="power" id="idm46522856875096"/></p>

<p><em>Power</em> is the probability of detecting a specified <em>effect size</em> with specified sample characteristics (size and variability).
For example, we might say (hypothetically) that the probability of distinguishing between a .330 hitter and a .200 hitter in 25 at-bats is 0.75.
The effect size here is a difference of .130.
And “detecting” means that a hypothesis test will reject the null hypothesis of “no difference” and conclude there is a real effect.
So the experiment of 25 at-bats (<em>n</em> = 25) for two hitters, with an effect size of 0.130, has (hypothetical) power of 0.75, or 75%.</p>

<p>You can see that there are several moving parts here, and it is easy to get tangled up in the numerous statistical assumptions and formulas that will be needed (to specify sample variability, effect size, sample size, alpha-level for the hypothesis test, etc., and to calculate power).
Indeed, there is special-purpose statistical software to calculate power.
Most data scientists will not need to go through all the formal steps needed to report power, for example, in a published paper.
However, they may face occasions where they want to collect some data for an A/B test, and collecting or processing the data involves some cost.
In that case, knowing approximately how much data to collect can help avoid the situation where you collect data at some effort, and the result ends up being inconclusive.
Here’s a fairly intuitive alternative approach:</p>
<ol>
<li>
<p>Start with some hypothetical data that represents your best guess about the data that will result (perhaps based on prior data)—for example, a box with 20 ones and 80 zeros to represent a .200 hitter, or a box with some observations of “time spent on website.”</p>
</li>
<li>
<p>Create a second sample simply by adding the desired effect size to the first sample—for example, a second box with 33 ones and 67 zeros, or a second box with 25 seconds added to each initial “time spent on website.”</p>
</li>
<li>
<p>Draw a bootstrap sample of size <em>n</em> from each box.</p>
</li>
<li>
<p>Conduct a permutation (or formula-based) hypothesis test on the two bootstrap samples and record whether the difference between them is statistically <span class="keep-together">significant</span>.</p>
</li>
<li>
<p>Repeat the preceding two steps many times and determine how often the difference was significant—that’s the estimated power.</p>
</li>

</ol>








<section data-type="sect2" data-pdf-bookmark="Sample Size"><div class="sect2" id="idm46522856863784">
<h2>Sample Size</h2>

<p>The most common use of power calculations is to estimate how big a sample you will need.<a data-type="indexterm" data-primary="power and sample size" data-secondary="sample size" id="idm46522856861944"/></p>

<p>For example, suppose you are looking at click-through rates (clicks as a percentage of exposures), and testing a new ad against an existing ad.
How many clicks do you need to accumulate in the study?
If you are interested only in results that show a huge difference (say, a 50% difference), a relatively small sample might do the trick.
If, on the other hand, even a minor difference would be of interest, then a much larger sample is needed.
A standard approach is to establish a policy that a new ad must do better than an existing ad by some percentage, say, 10%; otherwise, the existing ad will remain in place.
This goal, the “effect size,” then drives the sample size.</p>

<p>For example, suppose current click-through rates are about 1.1%, and you are seeking a 10% boost to 1.21%.
So we have two boxes: box A with 1.1% ones (say, 110 ones and 9,890 zeros), and box B with 1.21% ones (say, 121 ones and 9,879 zeros).
For starters, let’s try 300 draws from each box (this would be like 300 “impressions” for each ad).
Suppose our first draw yields the following:</p>
<ul class="simplelist">
  <li>Box A:  3 ones</li>
  <li>Box B:  5 ones</li>
</ul>

<p>Right away we can see that any hypothesis test would reveal this difference (5 versus 3) to be well within the range of chance variation.
This combination of sample size (<span class="keep-together"><em>n</em> = 300</span> in each group) and effect size (10% difference) is too small for any hypothesis test to reliably show a difference.</p>

<p>So we can try increasing the sample size (let’s try 2,000 impressions), and require a larger improvement (50% instead of 10%).</p>

<p>For example, suppose current click-through rates are still 1.1%, but we are now seeking a 50% boost to 1.65%.
So we have two boxes: box A still with 1.1% ones (say, 110 ones and 9,890 zeros), and box B with 1.65% ones (say, 165 ones and 9,868 zeros).
Now we’ll try 2,000 draws from each box.
Suppose our first draw yields the following:</p>
<ul class="simplelist">
  <li>Box A:  19 ones</li>
  <li>Box B:  34 ones</li>
</ul>

<p>A significance test on this difference (34–19) shows it still registers as “not significant” (though much closer to significance than the earlier difference of 5–3).
To calculate power, we would need to repeat the previous procedure many times, or use statistical software that can calculate power, but our initial draw suggests to us that even detecting a 50% improvement will require several thousand ad impressions.</p>

<p>In summary, for calculating power or required sample<a data-type="indexterm" data-primary="power and sample size" data-secondary="calculating, components in" id="idm46522856851512"/> size, there are four moving parts:</p>

<ul>
<li>
<p>Sample size</p>
</li>
<li>
<p>Effect size you want to detect<a data-type="indexterm" data-primary="effect size" id="idm46522856848424"/></p>
</li>
<li>
<p>Significance level (alpha) at which the test will be conducted</p>
</li>
<li>
<p>Power</p>
</li>
</ul>

<p>Specify any <a data-type="indexterm" data-primary="significance level" id="idm46522856845016"/>three of them, and the fourth can be calculated.  Most commonly, you would want to calculate sample size, so you must specify the other three.  With <em>R</em> and <em>Python</em>, you also have to specify the alternative hypothesis as “greater” or “larger” to get a one-sided test; see <a data-type="xref" href="#directional">“One-Way Versus Two-Way Hypothesis Tests”</a> for more discussion of one-way versus two-way tests.
Here is <em>R</em> code for a test involving two proportions, where both samples are the same size (this uses the <code>pwr</code> package):</p>

<pre data-type="programlisting" data-code-language="r"><code class="n">effect_size</code> <code class="o">=</code> <code class="nf">ES.h</code><code class="p">(</code><code class="n">p1</code><code class="o">=</code><code class="m">0.0121</code><code class="p">,</code> <code class="n">p2</code><code class="o">=</code><code class="m">0.011</code><code class="p">)</code>
<code class="nf">pwr.2p.test</code><code class="p">(</code><code class="n">h</code><code class="o">=</code><code class="n">effect_size</code><code class="p">,</code> <code class="n">sig.level</code><code class="o">=</code><code class="m">0.05</code><code class="p">,</code> <code class="n">power</code><code class="o">=</code><code class="m">0.8</code><code class="p">,</code> <code class="n">alternative</code><code class="o">=</code><code class="s">'</code><code class="err">greater’)</code>
<code class="o">--</code>
     <code class="n">Difference</code> <code class="n">of</code> <code class="n">proportion</code> <code class="n">power</code> <code class="n">calculation</code> <code class="n">for</code> <code class="n">binomial</code> <code class="nf">distribution</code>
<code class="nf">                                                       </code><code class="p">(</code><code class="n">arcsine</code> <code class="n">transformation</code><code class="p">)</code>

              <code class="n">h</code> <code class="o">=</code> <code class="m">0.01029785</code>
              <code class="n">n</code> <code class="o">=</code> <code class="m">116601.7</code>
      <code class="n">sig.level</code> <code class="o">=</code> <code class="m">0.05</code>
          <code class="n">power</code> <code class="o">=</code> <code class="m">0.8</code>
    <code class="n">alternative</code> <code class="o">=</code> <code class="n">greater</code>

<code class="n">NOTE</code><code class="o">:</code> <code class="n">same</code> <code class="n">sample</code> <code class="n">sizes</code></pre>

<p>The function <code>ES.h</code> calculates the effect size. We see that if we want a power of 80%, we require a sample size of almost 120,000 impressions. If we are seeking a 50% boost (<code>p1=0.0165</code>), the sample size is reduced to 5,500 impressions.</p>

<p>The <code>statsmodels</code> package contains several methods for power calculation. Here, we use
<code>proportion_effectsize</code> to calculate the effect size and <code>TTestIndPower</code> to solve for
the sample size:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">effect_size</code> <code class="o">=</code> <code class="n">sm</code><code class="o">.</code><code class="n">stats</code><code class="o">.</code><code class="n">proportion_effectsize</code><code class="p">(</code><code class="mf">0.0121</code><code class="p">,</code> <code class="mf">0.011</code><code class="p">)</code>
<code class="n">analysis</code> <code class="o">=</code> <code class="n">sm</code><code class="o">.</code><code class="n">stats</code><code class="o">.</code><code class="n">TTestIndPower</code><code class="p">()</code>
<code class="n">result</code> <code class="o">=</code> <code class="n">analysis</code><code class="o">.</code><code class="n">solve_power</code><code class="p">(</code><code class="n">effect_size</code><code class="o">=</code><code class="n">effect_size</code><code class="p">,</code>
                              <code class="n">alpha</code><code class="o">=</code><code class="mf">0.05</code><code class="p">,</code> <code class="n">power</code><code class="o">=</code><code class="mf">0.8</code><code class="p">,</code> <code class="n">alternative</code><code class="o">=</code><code class="s1">'larger'</code><code class="p">)</code>
<code class="k">print</code><code class="p">(</code><code class="s1">'Sample Size: </code><code class="si">%.3f</code><code class="s1">'</code> <code class="o">%</code> <code class="n">result</code><code class="p">)</code>
<code class="o">--</code>
<code class="n">Sample</code> <code class="n">Size</code><code class="p">:</code> <code class="mf">116602.393</code></pre>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46522856772744">
<h5>Key Ideas</h5>
<ul>
<li>
<p>Finding out how big a sample size you need requires thinking ahead to the statistical test you plan to conduct.</p>
</li>
<li>
<p>You must specify the minimum size of the effect that you want to detect.</p>
</li>
<li>
<p>You must also specify the required probability of detecting that effect size (power).</p>
</li>
<li>
<p>Finally, you must specify the significance level (alpha) at which the test will be conducted.</p>
</li>
</ul>
</div></aside>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Further Reading"><div class="sect2" id="idm46522856863160">
<h2>Further Reading</h2>

<ul>
<li>
<p><em>Sample Size Determination and Power</em> by Thomas Ryan (Wiley, 2013) is a comprehensive and readable review of this subject.</p>
</li>
<li>
<p>Steve Simon, a statistical consultant, has written a <a href="https://oreil.ly/18mtp">very engaging narrative-style post on the subject</a>.</p>
</li>
</ul>
</div></section>





</div></section>













<section data-type="sect1" class="pagebreak-before less_space" data-pdf-bookmark="Summary"><div class="sect1" id="idm46522856890232">
<h1>Summary</h1>

<p>The principles of experimental<a data-type="indexterm" data-primary="samples" data-secondary="sample size, power and" data-startref="ix_smplsze" id="idm46522856662072"/><a data-type="indexterm" data-primary="power and sample size" data-startref="ix_powsmpl" id="idm46522856660824"/><a data-type="indexterm" data-primary="statistical experiments and significance testing" data-secondary="power and sample size" data-startref="ix_statexPSS" id="idm46522856659880"/> design—randomization of subjects into two or more groups receiving different treatments—allow us to draw valid conclusions about how well the treatments work.
It is best to include a control treatment of “making no change.”
The subject of formal statistical inference—hypothesis testing, p-values, <span class="keep-together">t-tests</span>, and much more along these lines—occupies much time and space in a traditional statistics course or text, and the formality is mostly unneeded from a data science perspective.
However, it remains important to recognize the role that random variation can play in fooling the human brain.
Intuitive resampling procedures (permutation and bootstrap) allow data scientists to gauge the extent to which chance variation can play a role in their data analysis.<a data-type="indexterm" data-primary="statistical experiments and significance testing" data-startref="ix_statexp" id="idm46522856656984"/></p>
</div></section>







<div data-type="footnotes"><p data-type="footnote" id="idm46522858323800"><sup><a href="ch03.xhtml#idm46522858323800-marker">1</a></sup> The multiplication rule states that the probability of <em>n</em> independent events all happening is the product of the individual probabilities.  For example, if you and I each flip a coin once, the probability that your coin and my coin will both land heads is 0.5 × 0.5 = 0.25.</p></div></div></section></div>



  </body></html>