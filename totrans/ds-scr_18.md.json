["```py\nfrom typing import List\nimport math\n\ndef entropy(class_probabilities: List[float]) -> float:\n    \"\"\"Given a list of class probabilities, compute the entropy\"\"\"\n    return sum(-p * math.log(p, 2)\n               for p in class_probabilities\n               if p > 0)                     # ignore zero probabilities\n\nassert entropy([1.0]) == 0\nassert entropy([0.5, 0.5]) == 1\nassert 0.81 < entropy([0.25, 0.75]) < 0.82\n```", "```py\nfrom typing import Any\nfrom collections import Counter\n\ndef class_probabilities(labels: List[Any]) -> List[float]:\n    total_count = len(labels)\n    return [count / total_count\n            for count in Counter(labels).values()]\n\ndef data_entropy(labels: List[Any]) -> float:\n    return entropy(class_probabilities(labels))\n\nassert data_entropy(['a']) == 0\nassert data_entropy([True, False]) == 1\nassert data_entropy([3, 4, 4, 4]) == entropy([0.25, 0.75])\n```", "```py\ndef partition_entropy(subsets: List[List[Any]]) -> float:\n    \"\"\"Returns the entropy from this partition of data into subsets\"\"\"\n    total_count = sum(len(subset) for subset in subsets)\n\n    return sum(data_entropy(subset) * len(subset) / total_count\n               for subset in subsets)\n```", "```py\nfrom typing import NamedTuple, Optional\n\nclass Candidate(NamedTuple):\n    level: str\n    lang: str\n    tweets: bool\n    phd: bool\n    did_well: Optional[bool] = None  # allow unlabeled data\n\n                  #  level     lang     tweets  phd  did_well\ninputs = [Candidate('Senior', 'Java',   False, False, False),\n          Candidate('Senior', 'Java',   False, True,  False),\n          Candidate('Mid',    'Python', False, False, True),\n          Candidate('Junior', 'Python', False, False, True),\n          Candidate('Junior', 'R',      True,  False, True),\n          Candidate('Junior', 'R',      True,  True,  False),\n          Candidate('Mid',    'R',      True,  True,  True),\n          Candidate('Senior', 'Python', False, False, False),\n          Candidate('Senior', 'R',      True,  False, True),\n          Candidate('Junior', 'Python', True,  False, True),\n          Candidate('Senior', 'Python', True,  True,  True),\n          Candidate('Mid',    'Python', False, True,  True),\n          Candidate('Mid',    'Java',   True,  False, True),\n          Candidate('Junior', 'Python', False, True,  False)\n         ]\n```", "```py\nfrom typing import Dict, TypeVar\nfrom collections import defaultdict\n\nT = TypeVar('T')  # generic type for inputs\n\ndef partition_by(inputs: List[T], attribute: str) -> Dict[Any, List[T]]:\n    \"\"\"Partition the inputs into lists based on the specified attribute.\"\"\"\n    partitions: Dict[Any, List[T]] = defaultdict(list)\n    for input in inputs:\n        key = getattr(input, attribute)  # value of the specified attribute\n        partitions[key].append(input)    # add input to the correct partition\n    return partitions\n```", "```py\ndef partition_entropy_by(inputs: List[Any],\n                         attribute: str,\n                         label_attribute: str) -> float:\n    \"\"\"Compute the entropy corresponding to the given partition\"\"\"\n    # partitions consist of our inputs\n    partitions = partition_by(inputs, attribute)\n\n    # but partition_entropy needs just the class labels\n    labels = [[getattr(input, label_attribute) for input in partition]\n              for partition in partitions.values()]\n\n    return partition_entropy(labels)\n```", "```py\nfor key in ['level','lang','tweets','phd']:\n    print(key, partition_entropy_by(inputs, key, 'did_well'))\n\nassert 0.69 < partition_entropy_by(inputs, 'level', 'did_well')  < 0.70\nassert 0.86 < partition_entropy_by(inputs, 'lang', 'did_well')   < 0.87\nassert 0.78 < partition_entropy_by(inputs, 'tweets', 'did_well') < 0.79\nassert 0.89 < partition_entropy_by(inputs, 'phd', 'did_well')    < 0.90\n```", "```py\nsenior_inputs = [input for input in inputs if input.level == 'Senior']\n\nassert 0.4 == partition_entropy_by(senior_inputs, 'lang', 'did_well')\nassert 0.0 == partition_entropy_by(senior_inputs, 'tweets', 'did_well')\nassert 0.95 < partition_entropy_by(senior_inputs, 'phd', 'did_well') < 0.96\n```", "```py\n    from typing import NamedTuple, Union, Any\n\n    class Leaf(NamedTuple):\n        value: Any\n\n    class Split(NamedTuple):\n        attribute: str\n        subtrees: dict\n        default_value: Any = None\n\n    DecisionTree = Union[Leaf, Split]\n    ```", "```py\nhiring_tree = Split('level', {   # first, consider \"level\"\n    'Junior': Split('phd', {     # if level is \"Junior\", next look at \"phd\"\n        False: Leaf(True),       #   if \"phd\" is False, predict True\n        True: Leaf(False)        #   if \"phd\" is True, predict False\n    }),\n    'Mid': Leaf(True),           # if level is \"Mid\", just predict True\n    'Senior': Split('tweets', {  # if level is \"Senior\", look at \"tweets\"\n        False: Leaf(False),      #   if \"tweets\" is False, predict False\n        True: Leaf(True)         #   if \"tweets\" is True, predict True\n    })\n})\n```", "```py\ndef classify(tree: DecisionTree, input: Any) -> Any:\n    \"\"\"classify the input using the given decision tree\"\"\"\n\n    # If this is a leaf node, return its value\n    if isinstance(tree, Leaf):\n        return tree.value\n\n    # Otherwise this tree consists of an attribute to split on\n    # and a dictionary whose keys are values of that attribute\n    # and whose values are subtrees to consider next\n    subtree_key = getattr(input, tree.attribute)\n\n    if subtree_key not in tree.subtrees:   # If no subtree for key,\n        return tree.default_value          # return the default value.\n\n    subtree = tree.subtrees[subtree_key]   # Choose the appropriate subtree\n    return classify(subtree, input)        # and use it to classify the input.\n```", "```py\ndef build_tree_id3(inputs: List[Any],\n                   split_attributes: List[str],\n                   target_attribute: str) -> DecisionTree:\n    # Count target labels\n    label_counts = Counter(getattr(input, target_attribute)\n                           for input in inputs)\n    most_common_label = label_counts.most_common(1)[0][0]\n\n    # If there's a unique label, predict it\n    if len(label_counts) == 1:\n        return Leaf(most_common_label)\n\n    # If no split attributes left, return the majority label\n    if not split_attributes:\n        return Leaf(most_common_label)\n\n    # Otherwise split by the best attribute\n\n    def split_entropy(attribute: str) -> float:\n        \"\"\"Helper function for finding the best attribute\"\"\"\n        return partition_entropy_by(inputs, attribute, target_attribute)\n\n    best_attribute = min(split_attributes, key=split_entropy)\n\n    partitions = partition_by(inputs, best_attribute)\n    new_attributes = [a for a in split_attributes if a != best_attribute]\n\n    # Recursively build the subtrees\n    subtrees = {attribute_value : build_tree_id3(subset,\n                                                 new_attributes,\n                                                 target_attribute)\n                for attribute_value, subset in partitions.items()}\n\n    return Split(best_attribute, subtrees, default_value=most_common_label)\n```", "```py\ntree = build_tree_id3(inputs,\n                      ['level', 'lang', 'tweets', 'phd'],\n                      'did_well')\n\n# Should predict True\nassert classify(tree, Candidate(\"Junior\", \"Java\", True, False))\n\n# Should predict False\nassert not classify(tree, Candidate(\"Junior\", \"Java\", True, True))\n```", "```py\n# Should predict True\nassert classify(tree, Candidate(\"Intern\", \"Java\", True, True))\n```", "```py\n    # if there are already few enough split candidates, look at all of them\n    if len(split_candidates) <= self.num_split_candidates:\n        sampled_split_candidates = split_candidates\n    # otherwise pick a random sample\n    else:\n        sampled_split_candidates = random.sample(split_candidates,\n                                                 self.num_split_candidates)\n\n    # now choose the best attribute only from those candidates\n    best_attribute = min(sampled_split_candidates, key=split_entropy)\n\n    partitions = partition_by(inputs, best_attribute)\n```"]