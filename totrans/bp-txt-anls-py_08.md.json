["```py\nimport pandas as pd\ndebates = pd.read_csv(\"un-general-debates.csv\")\ndebates.info()\n\n```", "```py\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 7507 entries, 0 to 7506\nData columns (total 4 columns):\nsession    7507 non-null int64\nyear       7507 non-null int64\ncountry    7507 non-null object\ntext       7507 non-null object\ndtypes: int64(2), object(2)\nmemory usage: 234.7+ KB\n\n```", "```py\nprint(repr(df.iloc[2666][\"text\"][0:200]))\nprint(repr(df.iloc[4729][\"text\"][0:200]))\n\n```", "```py\n'\\ufeffIt is indeed a pleasure for me and the members of my delegation to\nextend to Ambassador Garba our sincere congratulations on his election to the\npresidency of the forty-fourth session of the General '\n'\\ufeffOn behalf of the State of Kuwait, it\\ngives me pleasure to congratulate\nMr. Han Seung-soo,\\nand his friendly country, the Republic of Korea, on\nhis\\nelection as President of the fifty-sixth session of t'\n\n```", "```py\nimport re\ndf[\"paragraphs\"] = df[\"text\"].map(lambda text: re.split('[.?!]\\s*\\n', text))\ndf[\"number_of_paragraphs\"] = df[\"paragraphs\"].map(len)\n\n```", "```py\n%matplotlib inline\ndebates.groupby('year').agg({'number_of_paragraphs': 'mean'}).plot.bar()\n\n```", "```py\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom spacy.lang.en.stop_words import STOP_WORDS as stopwords\n\n```", "```py\ntfidf_text = TfidfVectorizer(stop_words=stopwords, min_df=5, max_df=0.7)\nvectors_text = tfidf_text.fit_transform(debates['text'])\nvectors_text.shape\n\n```", "```py\n(7507, 24611)\n\n```", "```py\n# flatten the paragraphs keeping the years\nparagraph_df = pd.DataFrame([{ \"text\": paragraph, \"year\": year } \n                               for paragraphs, year in \\\n                               zip(df[\"paragraphs\"], df[\"year\"]) \n                                    for paragraph in paragraphs if paragraph])\n\ntfidf_para_vectorizer = TfidfVectorizer(stop_words=stopwords, min_df=5,\n                                        max_df=0.7)\ntfidf_para_vectors = tfidf_para_vectorizer.fit_transform(paragraph_df[\"text\"])\ntfidf_para_vectors.shape\n\n```", "```py\n(282210, 25165)\n\n```", "```py\nfrom sklearn.decomposition import NMF\n\nnmf_text_model = NMF(n_components=10, random_state=42)\nW_text_matrix = nmf_text_model.fit_transform(tfidf_text_vectors)\nH_text_matrix = nmf_text_model.components_\n\n```", "```py\ndef display_topics(model, features, no_top_words=5):\n    for topic, word_vector in enumerate(model.components_):\n        total = word_vector.sum()\n        largest = word_vector.argsort()[::-1] # invert sort order\n        print(\"\\nTopic %02d\" % topic)\n        for i in range(0, no_top_words):\n            print(\" %s (%2.2f)\" % (features[largest[i]],\n                  word_vector[largest[i]]*100.0/total))\n\n```", "```py\ndisplay_topics(nmf_text_model, tfidf_text_vectorizer.get_feature_names())\n```", "```py\nW_text_matrix.sum(axis=0)/W_text_matrix.sum()*100.0\n```", "```py\narray([11.13926287, 17.07197914, 13.64509781, 10.18184685, 11.43081404,\n        5.94072639,  7.89602474,  4.17282682, 11.83871081,  6.68271054])\n\n```", "```py\nnmf_para_model = NMF(n_components=10, random_state=42)\nW_para_matrix = nmf_para_model.fit_transform(tfidf_para_vectors)\nH_para_matrix = nmf_para_model.components_\n\n```", "```py\ndisplay_topics(nmf_para_model, tfidf_para_vectorizer.get_feature_names())\n```", "```py\nfrom sklearn.decomposition import TruncatedSVD\n\nsvd_para_model = TruncatedSVD(n_components = 10, random_state=42)\nW_svd_para_matrix = svd_para_model.fit_transform(tfidf_para_vectors)\nH_svd_para_matrix = svd_para_model.components_\n\n```", "```py\ndisplay_topics(svd_para_model, tfidf_para_vectorizer.get_feature_names())\n```", "```py\nsvd_para.singular_values_\n\n```", "```py\narray([68.21400653, 39.20120165, 36.36831431, 33.44682727, 31.76183677,\n       30.59557993, 29.14061799, 27.40264054, 26.85684195, 25.90408013])\n\n```", "```py\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ncount_para_vectorizer = CountVectorizer(stop_words=stopwords, min_df=5,\n                        max_df=0.7)\ncount_para_vectors = count_para_vectorizer.fit_transform(paragraph_df[\"text\"])\n\n```", "```py\nfrom sklearn.decomposition import LatentDirichletAllocation\n\nlda_para_model = LatentDirichletAllocation(n_components = 10, random_state=42)\nW_lda_para_matrix = lda_para_model.fit_transform(count_para_vectors)\nH_lda_para_matrix = lda_para_model.components_\n\n```", "```py\ndisplay_topics(lda_para_model, tfidf_para.get_feature_names())\n\n```", "```py\nimport pyLDAvis.sklearn\n\nlda_display = pyLDAvis.sklearn.prepare(lda_para_model, count_para_vectors,\n                            count_para_vectorizer, sort_topics=False)\npyLDAvis.display(lda_display)\n\n```", "```py\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\n\ndef wordcloud_topics(model, features, no_top_words=40):\n    for topic, words in enumerate(model.components_):\n        size = {}\n        largest = words.argsort()[::-1] # invert sort order\n        for i in range(0, no_top_words):\n            size[features[largest[i]]] = abs(words[largest[i]])\n        wc = WordCloud(background_color=\"white\", max_words=100,\n                       width=960, height=540)\n        wc.generate_from_frequencies(size)\n        plt.figure(figsize=(12,12))\n        plt.imshow(wc, interpolation='bilinear')\n        plt.axis(\"off\")\n        # if you don't want to save the topic model, comment the next line\n        plt.savefig(f'topic{topic}.png')\n\n```", "```py\nwordcloud_topics(nmf_para_model, tfidf_para_vectorizer.get_feature_names())\nwordcloud_topics(lda_para_model, count_para_vectorizer.get_feature_names())\n\n```", "```py\nimport numpy as np\nbefore_1990 = np.array(paragraph_df[\"year\"] < 1990)\nafter_1990 = ~ before_1990\n\n```", "```py\nW_para_matrix_early = nmf_para_model.transform(tfidf_para_vectors[before_1990])\nW_para_matrix_late  = nmf_para_model.transform(tfidf_para_vectors[after_1990])\nprint(W_para_matrix_early.sum(axis=0)/W_para_matrix_early.sum()*100.0)\nprint(W_para_matrix_late.sum(axis=0)/W_para_matrix_late.sum()*100.0)\n```", "```py\n['9.34', '10.43', '12.18', '12.18', '7.82', '6.05', '12.10', '5.85', '17.36',\n '6.69']\n['7.48', '8.34', '9.75', '9.75', '6.26', '4.84', '9.68', '4.68', '13.90',\n '5.36']\n\n```", "```py\nyear_data = []\nyears = np.unique(paragraph_years)\nfor year in tqdm(years):\n    W_year = nmf_para_model.transform(tfidf_para_vectors[paragraph_years \\\n                                      == year])\n    year_data.append([year] + list(W_year.sum(axis=0)/W_year.sum()*100.0))\n\n```", "```py\ntopic_names = []\nvoc = tfidf_para_vectorizer.get_feature_names()\nfor topic in nmf_para_model.components_:\n    important = topic.argsort()\n    top_word = voc[important[-1]] + \" \" + voc[important[-2]]\n    topic_names.append(\"Topic \" + top_word)\n\n```", "```py\ndf_year = pd.DataFrame(year_data,\n               columns=[\"year\"] + topic_names).set_index(\"year\")\ndf_year.plot.area()\n\n```", "```py\n# create tokenized documents\ngensim_paragraphs = [[w for w in re.findall(r'\\b\\w\\w+\\b' , paragraph.lower())\n                          if w not in stopwords]\n                             for paragraph in paragraph_df[\"text\"]]\n```", "```py\nfrom gensim.corpora import Dictionary\ndict_gensim_para = Dictionary(gensim_paragraphs)\n\n```", "```py\ndict_gensim_para.filter_extremes(no_below=5, no_above=0.7)\n\n```", "```py\nbow_gensim_para = [dict_gensim_para.doc2bow(paragraph) \\\n                    for paragraph in gensim_paragraphs]\n\n```", "```py\nfrom gensim.models import TfidfModel\ntfidf_gensim_para = TfidfModel(bow_gensim_para)\nvectors_gensim_para = tfidf_gensim_para[bow_gensim_para]\n\n```", "```py\nfrom gensim.models.nmf import Nmf\nnmf_gensim_para = Nmf(vectors_gensim_para, num_topics=10,\n                      id2word=dict_gensim_para, kappa=0.1, eval_every=5)\n\n```", "```py\ndisplay_topics_gensim(nmf_gensim_para)\n\n```", "```py\nfrom gensim.models.coherencemodel import CoherenceModel\n\nnmf_gensim_para_coherence = CoherenceModel(model=nmf_gensim_para,\n                                           texts=gensim_paragraphs,\n                                           dictionary=dict_gensim_para,\n                                           coherence='c_v')\nnmf_gensim_para_coherence_score = nmf_gensim_para_coherence.get_coherence()\nprint(nmf_gensim_para_coherence_score)\n\n```", "```py\n0.6500661701098243\n\n```", "```py\nfrom gensim.models import LdaModel\nlda_gensim_para = LdaModel(corpus=bow_gensim_para, id2word=dict_gensim_para,\n    chunksize=2000, alpha='auto', eta='auto', iterations=400, num_topics=10, \n    passes=20, eval_every=None, random_state=42)\n```", "```py\ndisplay_topics_gensim(lda_gensim_para)\n\n```", "```py\nfrom gensim.models.coherencemodel import CoherenceModel\n\nlda_gensim_para_coherence = CoherenceModel(model=lda_gensim_para,\n    texts=gensim_paragraphs, dictionary=dict_gensim_para, coherence='c_v')\nlda_gensim_para_coherence_score = lda_gensim_para_coherence.get_coherence()\nprint(lda_gensim_para_coherence_score)\n\n```", "```py\n0.5444930496493174\n\n```", "```py\nnmf_gensim_para_coherence = CoherenceModel(model=nmf_gensim_para,\n    texts=gensim_paragraphs, dictionary=dict_gensim_para, coherence='c_v')\nnmf_gensim_para_coherence_score = nmf_gensim_para_coherence.get_coherence()\nprint(nmf_gensim_para_coherence_score)\n\n```", "```py\n0.6505110480127619\n\n```", "```py\ntop_topics = lda_gensim_para.top_topics(vectors_gensim_para, topn=5)\navg_topic_coherence = sum([t[1] for t in top_topics]) / len(top_topics)\nprint('Average topic coherence: %.4f.' % avg_topic_coherence)\n\n```", "```py\nAverage topic coherence: -2.4709.\n\n```", "```py\n[(t[1], \" \".join([w[1] for w in t[0]])) for t in top_topics]\n\n```", "```py\n[(-1.5361194241843663, 'general assembly session president secretary'),\n (-1.7014902754187737, 'nations united human security rights'),\n (-1.8485895463251694, 'country people government national support'),\n (-1.9729985026779555, 'peace conflict region people state'),\n (-1.9743434414778658, 'world years today peace time'),\n (-2.0202823396586433, 'international community efforts new global'),\n (-2.7269347656599225, 'development countries economic sustainable 2015'),\n (-2.9089975883502706, 'climate convention pacific environmental sea'),\n (-3.8680684770508753, 'africa african continent terrorist crimes'),\n (-4.1515707817343195, 'south sudan china asia somalia')]\n\n```", "```py\nfrom gensim.models.ldamulticore import LdaMulticore\nlda_para_model_n = []\nfor n in tqdm(range(5, 21)):\n    lda_model = LdaMulticore(corpus=bow_gensim_para, id2word=dict_gensim_para,\n                             chunksize=2000, eta='auto', iterations=400,\n                             num_topics=n, passes=20, eval_every=None,\n                             random_state=42)\n    lda_coherence = CoherenceModel(model=lda_model, texts=gensim_paragraphs,\n                                   dictionary=dict_gensim_para, coherence='c_v')\n    lda_para_model_n.append((n, lda_model, lda_coherence.get_coherence()))\n\n```", "```py\npd.DataFrame(lda_para_model_n, columns=[\"n\", \"model\", \\\n    \"coherence\"]).set_index(\"n\")[[\"coherence\"]].plot(figsize=(16,9))\n```", "```py\ndisplay_topics_gensim(lda_para_model_n[12][1])\n```", "```py\nfrom gensim.models import HdpModel\nhdp_gensim_para = HdpModel(corpus=bow_gensim_para, id2word=dict_gensim_para)\n\n```", "```py\nhdp_gensim_para.print_topics(num_words=10)\n\n```", "```py\nfrom sklearn.cluster import KMeans\nk_means_text = KMeans(n_clusters=10, random_state=42)\nk_means_text.fit(tfidf_para_vectors)\n\n```", "```py\nKMeans(n_clusters=10, random_state=42)\n\n```", "```py\nnp.unique(k_means_para.labels_, return_counts=True)\n```", "```py\n(array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=int32),\narray([133370,  41705,  12396,   9142,  12674,  21080,  19727,  10563,\n         10437,  11116]))\n```", "```py\nsizes = []\nfor i in range(10):\n    sizes.append({\"cluster\": i, \"size\": np.sum(k_means_para.labels_==i)})\npd.DataFrame(sizes).set_index(\"cluster\").plot.bar(figsize=(16,9))\n\n```", "```py\ndef wordcloud_clusters(model, vectors, features, no_top_words=40):\n    for cluster in np.unique(model.labels_):\n        size = {}\n        words = vectors[model.labels_ == cluster].sum(axis=0).A[0]\n        largest = words.argsort()[::-1] # invert sort order\n        for i in range(0, no_top_words):\n            size[features[largest[i]]] = abs(words[largest[i]])\n        wc = WordCloud(background_color=\"white\", max_words=100,\n                       width=960, height=540)\n        wc.generate_from_frequencies(size)\n        plt.figure(figsize=(12,12))\n        plt.imshow(wc, interpolation='bilinear')\n        plt.axis(\"off\")\n        # if you don't want to save the topic model, comment the next line\n        plt.savefig(f'cluster{cluster}.png')\n\nwordcloud_clusters(k_means_para, tfidf_para_vectors,\n                   tfidf_para_vectorizer.get_feature_names())\n```"]