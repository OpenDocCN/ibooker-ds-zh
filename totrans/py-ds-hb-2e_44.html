<html><head></head><body>
<div id="sbo-rt-content"><section data-pdf-bookmark="Chapter 39. Hyperparameters and Model Validation" data-type="chapter" epub:type="chapter"><div class="chapter" id="section-0503-hyperparameters-and-model-validation">
<h1><span class="label">Chapter 39. </span>Hyperparameters and Model Validation</h1>
<p><a data-primary="machine learning" data-secondary="hyperparameters and model validation" data-type="indexterm" id="ix_ch39-asciidoc0"/><a data-primary="model validation" data-type="indexterm" id="ix_ch39-asciidoc1"/>In the previous chapter, we saw the basic recipe for applying a
supervised machine learning model:</p>
<ol>
<li>
<p>Choose a class of model.</p>
</li>
<li>
<p>Choose model hyperparameters.</p>
</li>
<li>
<p>Fit the model to the training data.</p>
</li>
<li>
<p>Use the model to predict labels for new data.</p>
</li>
</ol>
<p>The first two pieces of this—the choice of model and choice of
hyperparameters—are perhaps the most important part of using these tools
and techniques effectively. In order to make informed choices, we need a
way to <em>validate</em> that our model and our hyperparameters are a good fit
to the data. While this may sound simple, there are some pitfalls that
you must avoid to do this effectively.</p>
<section data-pdf-bookmark="Thinking About Model Validation" data-type="sect1"><div class="sect1" id="ch_0503-hyperparameters-and-model-validation_thinking-about-model-validation">
<h1>Thinking About Model Validation</h1>
<p>In principle, model validation is very simple: after choosing a model
and its hyperparameters, we can estimate how effective it is by applying
it to some of the training data and comparing the predictions to the
known values.</p>
<p>This section will first show a naive approach to model validation and
why it fails, before exploring the use of holdout sets and
cross-validation for more robust model evaluation.</p>
<section class="pagebreak-before less_space" data-pdf-bookmark="Model Validation the Wrong Way" data-type="sect2"><div class="sect2" id="ch_0503-hyperparameters-and-model-validation_model-validation-the-wrong-way">
<h2>Model Validation the Wrong Way</h2>
<p><a data-primary="model validation" data-secondary="naive approach to" data-type="indexterm" id="idm45858742369472"/>Let’s start with the naive approach to validation using the
Iris dataset, which we saw in the previous chapter. We will start by
loading the data:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">1</code><code class="p">]:</code> <code class="kn">from</code> <code class="nn">sklearn.datasets</code> <code class="kn">import</code> <code class="n">load_iris</code>
        <code class="n">iris</code> <code class="o">=</code> <code class="n">load_iris</code><code class="p">()</code>
        <code class="n">X</code> <code class="o">=</code> <code class="n">iris</code><code class="o">.</code><code class="n">data</code>
        <code class="n">y</code> <code class="o">=</code> <code class="n">iris</code><code class="o">.</code><code class="n">target</code></pre>
<p>Next, we choose a model and hyperparameters. Here we’ll use
a <em>k</em>-nearest neighbors classifier with <code>n_neighbors=1</code>. This is a very
simple and intuitive model that says “the label of an unknown point is
the same as the label of its closest training point”:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">2</code><code class="p">]:</code> <code class="kn">from</code> <code class="nn">sklearn.neighbors</code> <code class="kn">import</code> <code class="n">KNeighborsClassifier</code>
        <code class="n">model</code> <code class="o">=</code> <code class="n">KNeighborsClassifier</code><code class="p">(</code><code class="n">n_neighbors</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code></pre>
<p>Then we train the model, and use it to predict labels for data whose
labels we already know:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">3</code><code class="p">]:</code> <code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code>
        <code class="n">y_model</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X</code><code class="p">)</code></pre>
<p>Finally, we compute the fraction of correctly labeled points:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">4</code><code class="p">]:</code> <code class="kn">from</code> <code class="nn">sklearn.metrics</code> <code class="kn">import</code> <code class="n">accuracy_score</code>
        <code class="n">accuracy_score</code><code class="p">(</code><code class="n">y</code><code class="p">,</code> <code class="n">y_model</code><code class="p">)</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">4</code><code class="p">]:</code> <code class="mf">1.0</code></pre>
<p>We see an accuracy score of 1.0, which indicates that 100% of points
were correctly labeled by our model! But is this truly measuring the
expected accuracy? Have we really come upon a model that we expect to be
correct 100% of the time?</p>
<p>As you may have gathered, the answer is no. In fact, this approach
contains a fundamental flaw: <em>it trains and evaluates the model on the
same data</em>. Furthermore, this nearest neighbor model is an
<em>instance-based</em> estimator that simply stores the training data, and
predicts labels by comparing new data to these stored points: except in
contrived cases, it will get 100% accuracy every time!</p>
</div></section>
<section data-pdf-bookmark="Model Validation the Right Way: Holdout Sets" data-type="sect2"><div class="sect2" id="ch_0503-hyperparameters-and-model-validation_model-validation-the-right-way-holdout-sets">
<h2>Model Validation the Right Way: Holdout Sets</h2>
<p><a data-primary="holdout sets" data-type="indexterm" id="idm45858742203088"/><a data-primary="model validation" data-secondary="holdout sets" data-type="indexterm" id="idm45858742202496"/>So what can be done? A better sense of a model’s performance
can be found by using what’s known as a <em>holdout set</em>: that
is, we hold back some subset of the data from the training of the model,
and then use this holdout set to check the model’s
performance. This splitting can be done using the <code>train_test_split</code>
utility in Scikit-Learn:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">5</code><code class="p">]:</code> <code class="kn">from</code> <code class="nn">sklearn.model_selection</code> <code class="kn">import</code> <code class="n">train_test_split</code>
        <code class="c1"># split the data with 50% in each set</code>
        <code class="n">X1</code><code class="p">,</code> <code class="n">X2</code><code class="p">,</code> <code class="n">y1</code><code class="p">,</code> <code class="n">y2</code> <code class="o">=</code> <code class="n">train_test_split</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">0</code><code class="p">,</code>
                                          <code class="n">train_size</code><code class="o">=</code><code class="mf">0.5</code><code class="p">)</code>

        <code class="c1"># fit the model on one set of data</code>
        <code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X1</code><code class="p">,</code> <code class="n">y1</code><code class="p">)</code>

        <code class="c1"># evaluate the model on the second set of data</code>
        <code class="n">y2_model</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X2</code><code class="p">)</code>
        <code class="n">accuracy_score</code><code class="p">(</code><code class="n">y2</code><code class="p">,</code> <code class="n">y2_model</code><code class="p">)</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">5</code><code class="p">]:</code> <code class="mf">0.9066666666666666</code></pre>
<p>We see here a more reasonable result: the one-nearest-neighbor
classifier is about 90% accurate on this holdout set. The holdout set is
similar to unknown data, because the model has not “seen” it before.</p>
</div></section>
<section data-pdf-bookmark="Model Validation via Cross-Validation" data-type="sect2"><div class="sect2" id="ch_0503-hyperparameters-and-model-validation_model-validation-via-cross-validation">
<h2>Model Validation via Cross-Validation</h2>
<p><a data-primary="cross-validation" data-type="indexterm" id="ix_ch39-asciidoc2"/>One <a data-primary="model validation" data-secondary="cross-validation" data-type="indexterm" id="ix_ch39-asciidoc3"/>disadvantage of using a holdout set for model validation is that we
have lost a portion of our data to the model training. In the preceding
case, half the dataset does not contribute to the training of the model!
This is not optimal, especially if the initial set of training data is
small.</p>
<p>One way to address this is to use <em>cross-validation</em>; that is, to do a
sequence of fits where each subset of the data is used both as a
training set and as a validation set. Visually, it might look something
like <a data-type="xref" href="#fig_images_in_0503-2-fold-cv">Figure 39-1</a>.</p>
<figure><div class="figure" id="fig_images_in_0503-2-fold-cv">
<img alt="05.03 2 fold CV" height="232" src="assets/05.03-2-fold-CV.png" width="600"/>
<h6><span class="label">Figure 39-1. </span>Visualization of two-fold cross-validation<sup><a data-type="noteref" href="ch39.xhtml#idm45858742132032" id="idm45858742132032-marker">1</a></sup></h6>
</div></figure>
<p>Here we do two validation trials, alternately using each half of the
data as a holdout set. Using the split data from earlier, we could
implement it like this:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">6</code><code class="p">]:</code> <code class="n">y2_model</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X1</code><code class="p">,</code> <code class="n">y1</code><code class="p">)</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X2</code><code class="p">)</code>
        <code class="n">y1_model</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X2</code><code class="p">,</code> <code class="n">y2</code><code class="p">)</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X1</code><code class="p">)</code>
        <code class="n">accuracy_score</code><code class="p">(</code><code class="n">y1</code><code class="p">,</code> <code class="n">y1_model</code><code class="p">),</code> <code class="n">accuracy_score</code><code class="p">(</code><code class="n">y2</code><code class="p">,</code> <code class="n">y2_model</code><code class="p">)</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">6</code><code class="p">]:</code> <code class="p">(</code><code class="mf">0.96</code><code class="p">,</code> <code class="mf">0.9066666666666666</code><code class="p">)</code></pre>
<p>What comes out are two accuracy scores, which we could combine (by, say,
taking the mean) to get a better measure of the global model
performance. <a data-primary="two-fold cross-validation" data-type="indexterm" id="idm45858742048112"/>This particular form of cross-validation is a <em>two-fold
cross-validation</em>—that is, one in which we have split the data into two
sets and used each in turn as a validation set.</p>
<p>We could expand on this idea to use even more trials, and more folds in
the data—for example, <a data-type="xref" href="#fig_images_in_0503-5-fold-cv">Figure 39-2</a> shows a visual depiction of
five-fold cross-validation.</p>
<figure><div class="figure" id="fig_images_in_0503-5-fold-cv">
<img alt="05.03 5 fold CV" height="278" src="assets/05.03-5-fold-CV.png" width="600"/>
<h6><span class="label">Figure 39-2. </span>Visualization of five-fold cross-validation<sup><a data-type="noteref" href="ch39.xhtml#idm45858742031984" id="idm45858742031984-marker">2</a></sup></h6>
</div></figure>
<p>Here we split the data into five groups and use each in turn to
evaluate the model fit on the other four-fifths of the data. This would
be rather tedious to do by hand, but we can use
Scikit-Learn’s <code>cross_val_score</code> convenience routine to do
it succinctly:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">7</code><code class="p">]:</code> <code class="kn">from</code> <code class="nn">sklearn.model_selection</code> <code class="kn">import</code> <code class="n">cross_val_score</code>
        <code class="n">cross_val_score</code><code class="p">(</code><code class="n">model</code><code class="p">,</code> <code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">,</code> <code class="n">cv</code><code class="o">=</code><code class="mi">5</code><code class="p">)</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">7</code><code class="p">]:</code> <code class="n">array</code><code class="p">([</code><code class="mf">0.96666667</code><code class="p">,</code> <code class="mf">0.96666667</code><code class="p">,</code> <code class="mf">0.93333333</code><code class="p">,</code> <code class="mf">0.93333333</code><code class="p">,</code> <code class="mf">1.</code>        <code class="p">])</code></pre>
<p>Repeating the validation across different subsets of the data gives us
an even better idea of the performance of the algorithm.</p>
<p>Scikit-Learn implements a number of cross-validation schemes that are
useful in particular situations; these are implemented via iterators in
the <code>model_selection</code> module. For example, we might wish to go to the
extreme case in which our number of folds is equal to the number of data
points: that is, we train on all points but one in each trial. This type
of cross-validation is known as <em>leave-one-out</em> cross validation, and
can be used as follows:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">8</code><code class="p">]:</code> <code class="kn">from</code> <code class="nn">sklearn.model_selection</code> <code class="kn">import</code> <code class="n">LeaveOneOut</code>
        <code class="n">scores</code> <code class="o">=</code> <code class="n">cross_val_score</code><code class="p">(</code><code class="n">model</code><code class="p">,</code> <code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">,</code> <code class="n">cv</code><code class="o">=</code><code class="n">LeaveOneOut</code><code class="p">())</code>
        <code class="n">scores</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">8</code><code class="p">]:</code> <code class="n">array</code><code class="p">([</code><code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code>
               <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code>
               <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code>
               <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code>
               <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">0.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">0.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">0.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code>
               <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code>
               <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">0.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code>
               <code class="mf">0.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">0.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code>
               <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">,</code> <code class="mf">1.</code><code class="p">])</code></pre>
<p>Because we have 150 samples, the leave-one-out cross-validation yields
scores for 150 trials, and each score indicates either a successful
(1.0) or an unsuccessful (0.0) prediction. Taking the mean of these
gives an estimate of the error rate:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">9</code><code class="p">]:</code> <code class="n">scores</code><code class="o">.</code><code class="n">mean</code><code class="p">()</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">9</code><code class="p">]:</code> <code class="mf">0.96</code></pre>
<p>Other cross-validation schemes can be used similarly. For a description
of what is available in Scikit-Learn, use IPython to explore the
<code>sklearn.model_selection</code> submodule, or take a look at
Scikit-Learn’s
<a href="https://oreil.ly/rITkn">cross-validation
documentation</a>.<a data-startref="ix_ch39-asciidoc3" data-type="indexterm" id="idm45858741649968"/></p>
</div></section>
</div></section>
<section data-pdf-bookmark="Selecting the Best Model" data-type="sect1"><div class="sect1" id="ch_0503-hyperparameters-and-model-validation_selecting-the-best-model">
<h1>Selecting the Best Model</h1>
<p><a data-primary="model selection" data-type="indexterm" id="ix_ch39-asciidoc4"/><a data-primary="model validation" data-secondary="model selection" data-type="indexterm" id="ix_ch39-asciidoc5"/>Now that we’ve explored the basics of validation and
cross-validation, we will go into a little more depth regarding model
selection and selection of hyperparameters. These issues are some of the
most important aspects of the practice of machine learning, but I find
that this information is often glossed over in introductory machine
learning tutorials.</p>
<p>Of core importance is the following question: <em>if our estimator is
underperforming, how should we move forward?</em> There are several possible
answers:</p>
<ul>
<li>
<p>Use a more complicated/more flexible model.</p>
</li>
<li>
<p>Use a less complicated/less flexible model.</p>
</li>
<li>
<p>Gather more training samples.</p>
</li>
<li>
<p>Gather more data to add features to each sample.</p>
</li>
</ul>
<p>The answer to this question is often counterintuitive. In particular,
sometimes using a more complicated model will give worse results, and
adding more training samples may not improve your results! The ability
to determine what steps will improve your model is what separates the
successful machine learning practitioners from the unsuccessful.</p>
<section class="pagebreak-before less_space" data-pdf-bookmark="The Bias-Variance Trade-off" data-type="sect2"><div class="sect2" id="ch_0503-hyperparameters-and-model-validation_the-bias-variance-trade-off">
<h2>The Bias-Variance Trade-off</h2>
<p><a data-primary="bias–variance trade-off" data-secondary="model selection and" data-type="indexterm" id="ix_ch39-asciidoc6"/><a data-primary="model selection" data-secondary="bias–variance trade-off" data-type="indexterm" id="ix_ch39-asciidoc7"/><a data-primary="model validation" data-secondary="bias–variance trade-off" data-type="indexterm" id="ix_ch39-asciidoc8"/><a data-primary="variance, in bias–variance trade-off" data-type="indexterm" id="ix_ch39-asciidoc9"/>Fundamentally, finding “the best model” is about finding a sweet spot
in the trade-off between <em>bias</em> and <em>variance</em>. Consider <a data-type="xref" href="#fig_images_in_0503-bias-variance">Figure 39-3</a>, which presents two regression fits to the same dataset.</p>
<figure><div class="figure" id="fig_images_in_0503-bias-variance">
<img alt="05.03 bias variance" height="204" src="assets/05.03-bias-variance.png" width="600"/>
<h6><span class="label">Figure 39-3. </span>High-bias and high-variance regression models<sup><a data-type="noteref" href="ch39.xhtml#idm45858741405920" id="idm45858741405920-marker">3</a></sup></h6>
</div></figure>
<p>It is clear that neither of these models is a particularly good fit to
the data, but they fail in different ways.</p>
<p>The model on the left attempts to find a straight-line fit through the
data. Because in this case a straight line cannot accurately split the
data, the straight-line model will never be able to describe this
dataset well. <a data-primary="underfitting" data-type="indexterm" id="idm45858741403520"/>Such a model is said to <em>underfit</em> the data: that is, it
does not have enough flexibility to suitably account for all the
features in the data. Another way of saying this is that the model has
high bias.</p>
<p>The model on the right attempts to fit a high-order polynomial through
the data. Here the model fit has enough flexibility to nearly perfectly
account for the fine features in the data, but even though it very
accurately describes the training data, its precise form seems to be
more reflective of the particular noise properties of the data than of
the intrinsic properties of whatever process generated that data. Such a
model is said to <em>overfit</em> the data: that is, it has so much flexibility
that the model ends up accounting for random errors as well as the
underlying data distribution. Another way of saying this is that the
model has high variance.</p>
<p>To look at this in another light, consider what happens if we use these
two models to predict the <em>y</em>-values for some new data. In the plots in
<a data-type="xref" href="#fig_images_in_0503-bias-variance-2">Figure 39-4</a>, the red/lighter points indicate data that is
omitted from the training set.</p>
<figure><div class="figure" id="fig_images_in_0503-bias-variance-2">
<img alt="05.03 bias variance 2" height="204" src="assets/05.03-bias-variance-2.png" width="600"/>
<h6><span class="label">Figure 39-4. </span>Training and validation scores in high-bias and high-variance models<sup><a data-type="noteref" href="ch39.xhtml#idm45858741397728" id="idm45858741397728-marker">4</a></sup></h6>
</div></figure>
<p><a data-primary="coefficient of determination" data-type="indexterm" id="idm45858741396000"/>The score here is the <math alttext="upper R squared">
<msup><mi>R</mi> <mn>2</mn> </msup>
</math> score, or
<a href="https://oreil.ly/2AtV8">coefficient
of determination</a>, which measures how well a model performs relative to
a simple mean of the target values. <math alttext="upper R squared equals 1">
<mrow>
<msup><mi>R</mi> <mn>2</mn> </msup>
<mo>=</mo>
<mn>1</mn>
</mrow>
</math> indicates a
perfect match, <math alttext="upper R squared equals 0">
<mrow>
<msup><mi>R</mi> <mn>2</mn> </msup>
<mo>=</mo>
<mn>0</mn>
</mrow>
</math> indicates the model does no better
than simply taking the mean of the data, and negative values mean even
worse models. From the scores associated with these two models, we can
make an observation that holds more generally:</p>
<ul>
<li>
<p>For high-bias models, the performance of the model on the validation
set is similar to the performance on the training set.</p>
</li>
<li>
<p>For high-variance models, the performance of the model on the
validation set is far worse than the performance on the training set.</p>
</li>
</ul>
<p>If we imagine that we have some ability to tune the model complexity, we
would expect the training score and validation score to behave as
illustrated in <a data-type="xref" href="#fig_images_in_0503-validation-curve">Figure 39-5</a>, often called a <em>validation curve</em>, and we see
the following features:</p>
<ul>
<li>
<p>The training score is everywhere higher than the validation score.
This is generally the case: the model will be a better fit to data it
has seen than to data it has not seen.</p>
</li>
<li>
<p>For very low model complexity (a high-bias model), the training data
is underfit, which means that the model is a poor predictor both for the
training data and for any previously unseen data.</p>
</li>
<li>
<p>For very high model complexity (a high-variance model), the training
data is overfit, which means that the model predicts the training data
very well, but fails for any previously unseen data.</p>
</li>
<li>
<p>For some intermediate value, the validation curve has a maximum. This
level of complexity indicates a suitable trade-off between bias and
variance.</p>
</li>
</ul>
<p>The means of tuning the model complexity varies from model to model;
when we discuss individual models in depth in later chapters, we will
see how each model allows for such tuning.<a data-startref="ix_ch39-asciidoc9" data-type="indexterm" id="idm45858741377216"/><a data-startref="ix_ch39-asciidoc8" data-type="indexterm" id="idm45858741376512"/><a data-startref="ix_ch39-asciidoc7" data-type="indexterm" id="idm45858741375840"/><a data-startref="ix_ch39-asciidoc6" data-type="indexterm" id="idm45858741375168"/></p>
<figure><div class="figure" id="fig_images_in_0503-validation-curve">
<img alt="05.03 validation curve" height="483" src="assets/05.03-validation-curve.png" width="600"/>
<h6><span class="label">Figure 39-5. </span>A schematic of the relationship between model complexity, training score, and validation score<sup><a data-type="noteref" href="ch39.xhtml#idm45858741372496" id="idm45858741372496-marker">5</a></sup></h6>
</div></figure>
</div></section>
<section data-pdf-bookmark="Validation Curves in Scikit-Learn" data-type="sect2"><div class="sect2" id="ch_0503-hyperparameters-and-model-validation_validation-curves-in-scikit-learn">
<h2>Validation Curves in Scikit-Learn</h2>
<p><a data-primary="model selection" data-secondary="validation curves in Scikit-Learn" data-type="indexterm" id="ix_ch39-asciidoc10"/><a data-primary="model validation" data-secondary="validation curves" data-type="indexterm" id="ix_ch39-asciidoc11"/><a data-primary="validation curves" data-type="indexterm" id="ix_ch39-asciidoc12"/>Let’s look at an example of using cross-validation to
compute the validation curve for a class of models. <a data-primary="polynomial regression model" data-type="indexterm" id="idm45858741364976"/>Here we will use a
<em>polynomial regression</em> model, a generalized linear model in
which the degree of the polynomial is a tunable parameter. For example,
a degree-1 polynomial fits a straight line to the data; for model
parameters <math alttext="a">
<mi>a</mi>
</math> and <math alttext="b">
<mi>b</mi>
</math>:</p>
<div data-type="equation">
<math alttext="y equals a x plus b" display="block">
<mrow>
<mi>y</mi>
<mo>=</mo>
<mi>a</mi>
<mi>x</mi>
<mo>+</mo>
<mi>b</mi>
</mrow>
</math>
</div>
<p>A degree-3 polynomial fits a cubic curve to the data; for model
parameters <math alttext="a comma b comma c comma d">
<mrow>
<mi>a</mi>
<mo>,</mo>
<mi>b</mi>
<mo>,</mo>
<mi>c</mi>
<mo>,</mo>
<mi>d</mi>
</mrow>
</math>:</p>
<div data-type="equation">
<math alttext="y equals a x cubed plus b x squared plus c x plus d" display="block">
<mrow>
<mi>y</mi>
<mo>=</mo>
<mi>a</mi>
<msup><mi>x</mi> <mn>3</mn> </msup>
<mo>+</mo>
<mi>b</mi>
<msup><mi>x</mi> <mn>2</mn> </msup>
<mo>+</mo>
<mi>c</mi>
<mi>x</mi>
<mo>+</mo>
<mi>d</mi>
</mrow>
</math>
</div>
<p class="pagebreak-before less_space">We can generalize this to any number of polynomial features. In
Scikit-Learn, we can implement this with a linear regression classifier
combined with the polynomial preprocessor. <a data-primary="pipelines" data-type="indexterm" id="idm45858741341264"/>We will use a <em>pipeline</em> to
string these operations together (we will discuss polynomial features
and pipelines more fully in <a data-type="xref" href="ch40.xhtml#section-0504-feature-engineering">Chapter 40</a>):</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">10</code><code class="p">]:</code> <code class="kn">from</code> <code class="nn">sklearn.preprocessing</code> <code class="kn">import</code> <code class="n">PolynomialFeatures</code>
         <code class="kn">from</code> <code class="nn">sklearn.linear_model</code> <code class="kn">import</code> <code class="n">LinearRegression</code>
         <code class="kn">from</code> <code class="nn">sklearn.pipeline</code> <code class="kn">import</code> <code class="n">make_pipeline</code>

         <code class="k">def</code> <code class="nf">PolynomialRegression</code><code class="p">(</code><code class="n">degree</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code> <code class="o">**</code><code class="n">kwargs</code><code class="p">):</code>
             <code class="k">return</code> <code class="n">make_pipeline</code><code class="p">(</code><code class="n">PolynomialFeatures</code><code class="p">(</code><code class="n">degree</code><code class="p">),</code>
                                  <code class="n">LinearRegression</code><code class="p">(</code><code class="o">**</code><code class="n">kwargs</code><code class="p">))</code></pre>
<p>Now let’s create some data to which we will fit our model:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">11</code><code class="p">]:</code> <code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>

         <code class="k">def</code> <code class="nf">make_data</code><code class="p">(</code><code class="n">N</code><code class="p">,</code> <code class="n">err</code><code class="o">=</code><code class="mf">1.0</code><code class="p">,</code> <code class="n">rseed</code><code class="o">=</code><code class="mi">1</code><code class="p">):</code>
             <code class="c1"># randomly sample the data</code>
             <code class="n">rng</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">RandomState</code><code class="p">(</code><code class="n">rseed</code><code class="p">)</code>
             <code class="n">X</code> <code class="o">=</code> <code class="n">rng</code><code class="o">.</code><code class="n">rand</code><code class="p">(</code><code class="n">N</code><code class="p">,</code> <code class="mi">1</code><code class="p">)</code> <code class="o">**</code> <code class="mi">2</code>
             <code class="n">y</code> <code class="o">=</code> <code class="mi">10</code> <code class="o">-</code> <code class="mf">1.</code> <code class="o">/</code> <code class="p">(</code><code class="n">X</code><code class="o">.</code><code class="n">ravel</code><code class="p">()</code> <code class="o">+</code> <code class="mf">0.1</code><code class="p">)</code>
             <code class="k">if</code> <code class="n">err</code> <code class="o">&gt;</code> <code class="mi">0</code><code class="p">:</code>
                 <code class="n">y</code> <code class="o">+=</code> <code class="n">err</code> <code class="o">*</code> <code class="n">rng</code><code class="o">.</code><code class="n">randn</code><code class="p">(</code><code class="n">N</code><code class="p">)</code>
             <code class="k">return</code> <code class="n">X</code><code class="p">,</code> <code class="n">y</code>

         <code class="n">X</code><code class="p">,</code> <code class="n">y</code> <code class="o">=</code> <code class="n">make_data</code><code class="p">(</code><code class="mi">40</code><code class="p">)</code></pre>
<p>We can now visualize our data, along with polynomial fits of several
degrees (see <a data-type="xref" href="#fig_0503-hyperparameters-and-model-validation_files_in_output_33_0">Figure 39-6</a>).</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">12</code><code class="p">]:</code> <code class="o">%</code><code class="k">matplotlib</code> inline
         <code class="kn">import</code> <code class="nn">matplotlib.pyplot</code> <code class="k">as</code> <code class="nn">plt</code>
         <code class="n">plt</code><code class="o">.</code><code class="n">style</code><code class="o">.</code><code class="n">use</code><code class="p">(</code><code class="s1">'seaborn-whitegrid'</code><code class="p">)</code>

         <code class="n">X_test</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">linspace</code><code class="p">(</code><code class="o">-</code><code class="mf">0.1</code><code class="p">,</code> <code class="mf">1.1</code><code class="p">,</code> <code class="mi">500</code><code class="p">)[:,</code> <code class="kc">None</code><code class="p">]</code>

         <code class="n">plt</code><code class="o">.</code><code class="n">scatter</code><code class="p">(</code><code class="n">X</code><code class="o">.</code><code class="n">ravel</code><code class="p">(),</code> <code class="n">y</code><code class="p">,</code> <code class="n">color</code><code class="o">=</code><code class="s1">'black'</code><code class="p">)</code>
         <code class="n">axis</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">axis</code><code class="p">()</code>
         <code class="k">for</code> <code class="n">degree</code> <code class="ow">in</code> <code class="p">[</code><code class="mi">1</code><code class="p">,</code> <code class="mi">3</code><code class="p">,</code> <code class="mi">5</code><code class="p">]:</code>
             <code class="n">y_test</code> <code class="o">=</code> <code class="n">PolynomialRegression</code><code class="p">(</code><code class="n">degree</code><code class="p">)</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X_test</code><code class="p">)</code>
             <code class="n">plt</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">X_test</code><code class="o">.</code><code class="n">ravel</code><code class="p">(),</code> <code class="n">y_test</code><code class="p">,</code> <code class="n">label</code><code class="o">=</code><code class="s1">'degree=</code><code class="si">{0}</code><code class="s1">'</code><code class="o">.</code><code class="n">format</code><code class="p">(</code><code class="n">degree</code><code class="p">))</code>
         <code class="n">plt</code><code class="o">.</code><code class="n">xlim</code><code class="p">(</code><code class="o">-</code><code class="mf">0.1</code><code class="p">,</code> <code class="mf">1.0</code><code class="p">)</code>
         <code class="n">plt</code><code class="o">.</code><code class="n">ylim</code><code class="p">(</code><code class="o">-</code><code class="mi">2</code><code class="p">,</code> <code class="mi">12</code><code class="p">)</code>
         <code class="n">plt</code><code class="o">.</code><code class="n">legend</code><code class="p">(</code><code class="n">loc</code><code class="o">=</code><code class="s1">'best'</code><code class="p">);</code></pre>
<p>The knob controlling model complexity in this case is the degree of the
polynomial, which can be any nonnegative integer. A useful question to
answer is this: what degree of polynomial provides a suitable trade-off
between bias (underfitting) and variance (overfitting)?</p>
<figure><div class="figure" id="fig_0503-hyperparameters-and-model-validation_files_in_output_33_0">
<img alt="output 33 0" height="392" src="assets/output_33_0.png" width="600"/>
<h6><span class="label">Figure 39-6. </span>Three different polynomial models fit to a dataset<sup><a data-type="noteref" href="ch39.xhtml#idm45858741065984" id="idm45858741065984-marker">6</a></sup></h6>
</div></figure>
<p>We can make progress in this by visualizing the validation curve for
this particular data and model; this can be done straightforwardly using
the <code>validation_curve</code> convenience routine provided by Scikit-Learn.
Given a model, data, parameter name, and a range to explore, this
function will automatically compute both the training score and the
validation score across the range (see <a data-type="xref" href="#fig_0503-hyperparameters-and-model-validation_files_in_output_35_0">Figure 39-7</a>).</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">13</code><code class="p">]:</code> <code class="kn">from</code> <code class="nn">sklearn.model_selection</code> <code class="kn">import</code> <code class="n">validation_curve</code>
         <code class="n">degree</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">arange</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="mi">21</code><code class="p">)</code>
         <code class="n">train_score</code><code class="p">,</code> <code class="n">val_score</code> <code class="o">=</code> <code class="n">validation_curve</code><code class="p">(</code>
             <code class="n">PolynomialRegression</code><code class="p">(),</code> <code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">,</code>
             <code class="n">param_name</code><code class="o">=</code><code class="s1">'polynomialfeatures__degree'</code><code class="p">,</code>
             <code class="n">param_range</code><code class="o">=</code><code class="n">degree</code><code class="p">,</code> <code class="n">cv</code><code class="o">=</code><code class="mi">7</code><code class="p">)</code>

         <code class="n">plt</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">degree</code><code class="p">,</code> <code class="n">np</code><code class="o">.</code><code class="n">median</code><code class="p">(</code><code class="n">train_score</code><code class="p">,</code> <code class="mi">1</code><code class="p">),</code>
                  <code class="n">color</code><code class="o">=</code><code class="s1">'blue'</code><code class="p">,</code> <code class="n">label</code><code class="o">=</code><code class="s1">'training score'</code><code class="p">)</code>
         <code class="n">plt</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">degree</code><code class="p">,</code> <code class="n">np</code><code class="o">.</code><code class="n">median</code><code class="p">(</code><code class="n">val_score</code><code class="p">,</code> <code class="mi">1</code><code class="p">),</code>
                  <code class="n">color</code><code class="o">=</code><code class="s1">'red'</code><code class="p">,</code> <code class="n">label</code><code class="o">=</code><code class="s1">'validation score'</code><code class="p">)</code>
         <code class="n">plt</code><code class="o">.</code><code class="n">legend</code><code class="p">(</code><code class="n">loc</code><code class="o">=</code><code class="s1">'best'</code><code class="p">)</code>
         <code class="n">plt</code><code class="o">.</code><code class="n">ylim</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="mi">1</code><code class="p">)</code>
         <code class="n">plt</code><code class="o">.</code><code class="n">xlabel</code><code class="p">(</code><code class="s1">'degree'</code><code class="p">)</code>
         <code class="n">plt</code><code class="o">.</code><code class="n">ylabel</code><code class="p">(</code><code class="s1">'score'</code><code class="p">);</code></pre>
<figure><div class="figure" id="fig_0503-hyperparameters-and-model-validation_files_in_output_35_0">
<img alt="output 35 0" height="392" src="assets/output_35_0.png" width="600"/>
<h6><span class="label">Figure 39-7. </span>The validation curves for the data in <a data-type="xref" href="#fig_0503-hyperparameters-and-model-validation_files_in_output_40_0">Figure 39-9</a></h6>
</div></figure>
<p>This shows precisely the qualitative behavior we expect: the training
score is everywhere higher than the validation score, the training score
is monotonically improving with increased model complexity, and the
validation score reaches a maximum before dropping off as the model
becomes overfit.</p>
<p>From the validation curve, we can determine that the optimal trade-off
between bias and variance is found for a third-order polynomial. We can
compute and display this fit over the original data as follows (see <a data-type="xref" href="#fig_0503-hyperparameters-and-model-validation_files_in_output_37_0">Figure 39-8</a>).</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">14</code><code class="p">]:</code> <code class="n">plt</code><code class="o">.</code><code class="n">scatter</code><code class="p">(</code><code class="n">X</code><code class="o">.</code><code class="n">ravel</code><code class="p">(),</code> <code class="n">y</code><code class="p">)</code>
         <code class="n">lim</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">axis</code><code class="p">()</code>
         <code class="n">y_test</code> <code class="o">=</code> <code class="n">PolynomialRegression</code><code class="p">(</code><code class="mi">3</code><code class="p">)</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X_test</code><code class="p">)</code>
         <code class="n">plt</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">X_test</code><code class="o">.</code><code class="n">ravel</code><code class="p">(),</code> <code class="n">y_test</code><code class="p">);</code>
         <code class="n">plt</code><code class="o">.</code><code class="n">axis</code><code class="p">(</code><code class="n">lim</code><code class="p">);</code></pre>
<figure><div class="figure" id="fig_0503-hyperparameters-and-model-validation_files_in_output_37_0">
<img alt="output 37 0" height="413" src="assets/output_37_0.png" width="600"/>
<h6><span class="label">Figure 39-8. </span>The cross-validated optimal model for the data in <a data-type="xref" href="#fig_0503-hyperparameters-and-model-validation_files_in_output_33_0">Figure 39-6</a></h6>
</div></figure>
<p>Notice that finding this optimal model did not actually require us to
compute the training score, but examining the relationship between the
training score and validation score can give us useful insight into the
performance of the model<a data-startref="ix_ch39-asciidoc12" data-type="indexterm" id="idm45858740696304"/><a data-startref="ix_ch39-asciidoc11" data-type="indexterm" id="idm45858740695600"/><a data-startref="ix_ch39-asciidoc10" data-type="indexterm" id="idm45858740694928"/>.<a data-startref="ix_ch39-asciidoc5" data-type="indexterm" id="idm45858740694128"/><a data-startref="ix_ch39-asciidoc4" data-type="indexterm" id="idm45858740693424"/></p>
</div></section>
</div></section>
<section data-pdf-bookmark="Learning Curves" data-type="sect1"><div class="sect1" id="ch_0503-hyperparameters-and-model-validation_learning-curves">
<h1>Learning Curves</h1>
<p><a data-primary="learning curves" data-secondary="computing" data-type="indexterm" id="ix_ch39-asciidoc13"/><a data-primary="model validation" data-secondary="learning curves" data-type="indexterm" id="ix_ch39-asciidoc14"/>One important aspect of model complexity is that the optimal model will
generally depend on the size of your training data. For example,
let’s generate a new dataset with five times as many points
(see <a data-type="xref" href="#fig_0503-hyperparameters-and-model-validation_files_in_output_40_0">Figure 39-9</a>).</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">15</code><code class="p">]:</code> <code class="n">X2</code><code class="p">,</code> <code class="n">y2</code> <code class="o">=</code> <code class="n">make_data</code><code class="p">(</code><code class="mi">200</code><code class="p">)</code>
         <code class="n">plt</code><code class="o">.</code><code class="n">scatter</code><code class="p">(</code><code class="n">X2</code><code class="o">.</code><code class="n">ravel</code><code class="p">(),</code> <code class="n">y2</code><code class="p">);</code></pre>
<figure><div class="figure" id="fig_0503-hyperparameters-and-model-validation_files_in_output_40_0">
<img alt="output 40 0" height="234" src="assets/output_40_0.png" width="600"/>
<h6><span class="label">Figure 39-9. </span>Data to demonstrate learning curves</h6>
</div></figure>
<p>Now let’s duplicate the preceding code to plot the
validation curve for this larger dataset; for reference,
we’ll overplot the previous results as well (see <a data-type="xref" href="#fig_0503-hyperparameters-and-model-validation_files_in_output_42_0">Figure 39-10</a>).</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">16</code><code class="p">]:</code> <code class="n">degree</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">arange</code><code class="p">(</code><code class="mi">21</code><code class="p">)</code>
         <code class="n">train_score2</code><code class="p">,</code> <code class="n">val_score2</code> <code class="o">=</code> <code class="n">validation_curve</code><code class="p">(</code>
             <code class="n">PolynomialRegression</code><code class="p">(),</code> <code class="n">X2</code><code class="p">,</code> <code class="n">y2</code><code class="p">,</code>
             <code class="n">param_name</code><code class="o">=</code><code class="s1">'polynomialfeatures__degree'</code><code class="p">,</code>
             <code class="n">param_range</code><code class="o">=</code><code class="n">degree</code><code class="p">,</code> <code class="n">cv</code><code class="o">=</code><code class="mi">7</code><code class="p">)</code>

         <code class="n">plt</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">degree</code><code class="p">,</code> <code class="n">np</code><code class="o">.</code><code class="n">median</code><code class="p">(</code><code class="n">train_score2</code><code class="p">,</code> <code class="mi">1</code><code class="p">),</code>
                  <code class="n">color</code><code class="o">=</code><code class="s1">'blue'</code><code class="p">,</code> <code class="n">label</code><code class="o">=</code><code class="s1">'training score'</code><code class="p">)</code>
         <code class="n">plt</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">degree</code><code class="p">,</code> <code class="n">np</code><code class="o">.</code><code class="n">median</code><code class="p">(</code><code class="n">val_score2</code><code class="p">,</code> <code class="mi">1</code><code class="p">),</code>
                  <code class="n">color</code><code class="o">=</code><code class="s1">'red'</code><code class="p">,</code> <code class="n">label</code><code class="o">=</code><code class="s1">'validation score'</code><code class="p">)</code>
         <code class="n">plt</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">degree</code><code class="p">,</code> <code class="n">np</code><code class="o">.</code><code class="n">median</code><code class="p">(</code><code class="n">train_score</code><code class="p">,</code> <code class="mi">1</code><code class="p">),</code>
                  <code class="n">color</code><code class="o">=</code><code class="s1">'blue'</code><code class="p">,</code> <code class="n">alpha</code><code class="o">=</code><code class="mf">0.3</code><code class="p">,</code> <code class="n">linestyle</code><code class="o">=</code><code class="s1">'dashed'</code><code class="p">)</code>
         <code class="n">plt</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">degree</code><code class="p">,</code> <code class="n">np</code><code class="o">.</code><code class="n">median</code><code class="p">(</code><code class="n">val_score</code><code class="p">,</code> <code class="mi">1</code><code class="p">),</code>
                  <code class="n">color</code><code class="o">=</code><code class="s1">'red'</code><code class="p">,</code> <code class="n">alpha</code><code class="o">=</code><code class="mf">0.3</code><code class="p">,</code> <code class="n">linestyle</code><code class="o">=</code><code class="s1">'dashed'</code><code class="p">)</code>
         <code class="n">plt</code><code class="o">.</code><code class="n">legend</code><code class="p">(</code><code class="n">loc</code><code class="o">=</code><code class="s1">'lower center'</code><code class="p">)</code>
         <code class="n">plt</code><code class="o">.</code><code class="n">ylim</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="mi">1</code><code class="p">)</code>
         <code class="n">plt</code><code class="o">.</code><code class="n">xlabel</code><code class="p">(</code><code class="s1">'degree'</code><code class="p">)</code>
         <code class="n">plt</code><code class="o">.</code><code class="n">ylabel</code><code class="p">(</code><code class="s1">'score'</code><code class="p">);</code></pre>
<p>The solid lines show the new results, while the fainter dashed lines
show the results on the previous smaller dataset. It is clear from the
validation curve that the larger dataset can support a much more
complicated model: the peak here is probably around a degree of 6, but
even a degree-20 model isn’t seriously overfitting the data—the
validation and training scores remain very close.</p>
<figure><div class="figure" id="fig_0503-hyperparameters-and-model-validation_files_in_output_42_0">
<img alt="output 42 0" height="293" src="assets/output_42_0.png" width="600"/>
<h6><span class="label">Figure 39-10. </span>Learning curves for the polynomial model fit to data in <a data-type="xref" href="#fig_0503-hyperparameters-and-model-validation_files_in_output_40_0">Figure 39-9</a><sup><a data-type="noteref" href="ch39.xhtml#idm45858740554592" id="idm45858740554592-marker">7</a></sup></h6>
</div></figure>
<p>So, the behavior of the validation curve has not one but two important
inputs: the model complexity and the number of training points. We can
gain further insight by exploring the behavior of the model as a
function of the number of training points, which we can do by using
increasingly larger subsets of the data to fit our model. A plot of the
training/validation score with respect to the size of the training set
is sometimes known as a <em>learning curve</em>.</p>
<p>The general behavior we would expect from a learning curve is this:</p>
<ul>
<li>
<p><a data-primary="overfitting" data-type="indexterm" id="idm45858740551072"/>A model of a given complexity will <em>overfit</em> a small dataset: this
means the training score will be relatively high, while the validation
score will be relatively low.</p>
</li>
<li>
<p><a data-primary="underfitting" data-type="indexterm" id="idm45858740549120"/>A model of a given complexity will <em>underfit</em> a large dataset: this
means that the training score will decrease, but the validation score
will increase.</p>
</li>
<li>
<p>A model will never, except by chance, give a better score to the
validation set than the training set: this means the curves should keep
getting closer together but never cross.</p>
</li>
</ul>
<p>With these features in mind, we would expect a learning curve to look
qualitatively like that shown in <a data-type="xref" href="#fig_images_in_0503-learning-curve">Figure 39-11</a>.</p>
<figure><div class="figure" id="fig_images_in_0503-learning-curve">
<img alt="05.03 learning curve" height="484" src="assets/05.03-learning-curve.png" width="600"/>
<h6><span class="label">Figure 39-11. </span>Schematic showing the typical interpretation of learning curves<sup><a data-type="noteref" href="ch39.xhtml#idm45858740543504" id="idm45858740543504-marker">8</a></sup></h6>
</div></figure>
<p>The notable feature of the learning curve is the convergence to a
particular score as the number of training samples grows. In particular,
once you have enough points that a particular model has converged,
<em>adding more training data will not help you!</em> The only way to increase
model performance in this case is to use another (often more complex)
model.</p>
<p><a data-primary="learning curves" data-secondary="Scikit-Learn" data-type="indexterm" id="idm45858740540928"/><a data-primary="Scikit-Learn package" data-secondary="learning curves in" data-type="indexterm" id="idm45858740539952"/>Scikit-Learn offers a convenient utility for computing such learning
curves from your models; here we will compute a learning curve for our
original dataset with a second-order polynomial model and a ninth-order
polynomial (see <a data-type="xref" href="#fig_0503-hyperparameters-and-model-validation_files_in_output_47_0">Figure 39-12</a>).</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">17</code><code class="p">]:</code> <code class="kn">from</code> <code class="nn">sklearn.model_selection</code> <code class="kn">import</code> <code class="n">learning_curve</code>

         <code class="n">fig</code><code class="p">,</code> <code class="n">ax</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="mi">2</code><code class="p">,</code> <code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">16</code><code class="p">,</code> <code class="mi">6</code><code class="p">))</code>
         <code class="n">fig</code><code class="o">.</code><code class="n">subplots_adjust</code><code class="p">(</code><code class="n">left</code><code class="o">=</code><code class="mf">0.0625</code><code class="p">,</code> <code class="n">right</code><code class="o">=</code><code class="mf">0.95</code><code class="p">,</code> <code class="n">wspace</code><code class="o">=</code><code class="mf">0.1</code><code class="p">)</code>

         <code class="k">for</code> <code class="n">i</code><code class="p">,</code> <code class="n">degree</code> <code class="ow">in</code> <code class="nb">enumerate</code><code class="p">([</code><code class="mi">2</code><code class="p">,</code> <code class="mi">9</code><code class="p">]):</code>
             <code class="n">N</code><code class="p">,</code> <code class="n">train_lc</code><code class="p">,</code> <code class="n">val_lc</code> <code class="o">=</code> <code class="n">learning_curve</code><code class="p">(</code>
                 <code class="n">PolynomialRegression</code><code class="p">(</code><code class="n">degree</code><code class="p">),</code> <code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">,</code> <code class="n">cv</code><code class="o">=</code><code class="mi">7</code><code class="p">,</code>
                 <code class="n">train_sizes</code><code class="o">=</code><code class="n">np</code><code class="o">.</code><code class="n">linspace</code><code class="p">(</code><code class="mf">0.3</code><code class="p">,</code> <code class="mi">1</code><code class="p">,</code> <code class="mi">25</code><code class="p">))</code>

             <code class="n">ax</code><code class="p">[</code><code class="n">i</code><code class="p">]</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">N</code><code class="p">,</code> <code class="n">np</code><code class="o">.</code><code class="n">mean</code><code class="p">(</code><code class="n">train_lc</code><code class="p">,</code> <code class="mi">1</code><code class="p">),</code>
                        <code class="n">color</code><code class="o">=</code><code class="s1">'blue'</code><code class="p">,</code> <code class="n">label</code><code class="o">=</code><code class="s1">'training score'</code><code class="p">)</code>
             <code class="n">ax</code><code class="p">[</code><code class="n">i</code><code class="p">]</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">N</code><code class="p">,</code> <code class="n">np</code><code class="o">.</code><code class="n">mean</code><code class="p">(</code><code class="n">val_lc</code><code class="p">,</code> <code class="mi">1</code><code class="p">),</code>
                        <code class="n">color</code><code class="o">=</code><code class="s1">'red'</code><code class="p">,</code> <code class="n">label</code><code class="o">=</code><code class="s1">'validation score'</code><code class="p">)</code>
             <code class="n">ax</code><code class="p">[</code><code class="n">i</code><code class="p">]</code><code class="o">.</code><code class="n">hlines</code><code class="p">(</code><code class="n">np</code><code class="o">.</code><code class="n">mean</code><code class="p">([</code><code class="n">train_lc</code><code class="p">[</code><code class="o">-</code><code class="mi">1</code><code class="p">],</code> <code class="n">val_lc</code><code class="p">[</code><code class="o">-</code><code class="mi">1</code><code class="p">]]),</code> <code class="n">N</code><code class="p">[</code><code class="mi">0</code><code class="p">],</code>
                          <code class="n">N</code><code class="p">[</code><code class="o">-</code><code class="mi">1</code><code class="p">],</code> <code class="n">color</code><code class="o">=</code><code class="s1">'gray'</code><code class="p">,</code> <code class="n">linestyle</code><code class="o">=</code><code class="s1">'dashed'</code><code class="p">)</code>

             <code class="n">ax</code><code class="p">[</code><code class="n">i</code><code class="p">]</code><code class="o">.</code><code class="n">set_ylim</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="mi">1</code><code class="p">)</code>
             <code class="n">ax</code><code class="p">[</code><code class="n">i</code><code class="p">]</code><code class="o">.</code><code class="n">set_xlim</code><code class="p">(</code><code class="n">N</code><code class="p">[</code><code class="mi">0</code><code class="p">],</code> <code class="n">N</code><code class="p">[</code><code class="o">-</code><code class="mi">1</code><code class="p">])</code>
             <code class="n">ax</code><code class="p">[</code><code class="n">i</code><code class="p">]</code><code class="o">.</code><code class="n">set_xlabel</code><code class="p">(</code><code class="s1">'training size'</code><code class="p">)</code>
             <code class="n">ax</code><code class="p">[</code><code class="n">i</code><code class="p">]</code><code class="o">.</code><code class="n">set_ylabel</code><code class="p">(</code><code class="s1">'score'</code><code class="p">)</code>
             <code class="n">ax</code><code class="p">[</code><code class="n">i</code><code class="p">]</code><code class="o">.</code><code class="n">set_title</code><code class="p">(</code><code class="s1">'degree = </code><code class="si">{0}</code><code class="s1">'</code><code class="o">.</code><code class="n">format</code><code class="p">(</code><code class="n">degree</code><code class="p">),</code> <code class="n">size</code><code class="o">=</code><code class="mi">14</code><code class="p">)</code>
             <code class="n">ax</code><code class="p">[</code><code class="n">i</code><code class="p">]</code><code class="o">.</code><code class="n">legend</code><code class="p">(</code><code class="n">loc</code><code class="o">=</code><code class="s1">'best'</code><code class="p">)</code></pre>
<figure><div class="figure" id="fig_0503-hyperparameters-and-model-validation_files_in_output_47_0">
<img alt="output 47 0" height="210" src="assets/output_47_0.png" width="600"/>
<h6><span class="label">Figure 39-12. </span>Learning curves for a low-complexity model (left) and a high-complexity model (right)<sup><a data-type="noteref" href="ch39.xhtml#idm45858740396064" id="idm45858740396064-marker">9</a></sup></h6>
</div></figure>
<p>This is a valuable diagnostic, because it gives us a visual depiction of
how our model responds to increasing amounts of training data. In
particular, when the learning curve has already converged (i.e., when
the training and validation curves are already close to each other)
<em>adding more training data will not significantly improve the fit!</em> This
situation is seen in the left panel, with the learning curve for the
degree-2 model.</p>
<p>The only way to increase the converged score is to use a different
(usually more complicated) model. We see this in the right panel: by
moving to a much more complicated model, we increase the score of
convergence (indicated by the dashed line), but at the expense of higher
model variance (indicated by the difference between the training and
validation scores). If we were to add even more data points, the
learning curve for the more complicated model would eventually converge.</p>
<p>Plotting a learning curve for your particular choice of model and
dataset can help you to make this type of decision about how to move
forward in improving your analysis.<a data-startref="ix_ch39-asciidoc14" data-type="indexterm" id="idm45858740156592"/><a data-startref="ix_ch39-asciidoc13" data-type="indexterm" id="idm45858740155888"/></p>
<p>The solid lines show the new results, while the fainter dashed lines
show the results on the previous smaller dataset. It is clear from the
validation curve that the larger dataset can support a much more
complicated model: the peak here is probably around a degree of 6, but
even a degree-20 model isn’t seriously overfitting the data—the
validation and training scores remain very close.</p>
</div></section>
<section data-pdf-bookmark="Validation in Practice: Grid Search" data-type="sect1"><div class="sect1" id="ch_0503-hyperparameters-and-model-validation_validation-in-practice-grid-search">
<h1>Validation in Practice: Grid Search</h1>
<p><a data-primary="model validation" data-secondary="grid search example" data-type="indexterm" id="ix_ch39-asciidoc15"/>The preceding discussion is meant to give you some intuition into the
trade-off between bias and variance, and its dependence on model
complexity and training set size. In practice, models generally have
more than one knob to turn, meaning plots of validation and learning
curves change from lines to multidimensional surfaces. In these cases,
such visualizations are difficult, and we would rather simply find the
particular model that maximizes the validation score.</p>
<p>Scikit-Learn provides some tools to make this kind of search more
convenient: here we’ll consider the use of grid search to
find the optimal polynomial model. We will explore a two-dimensional
grid of model features, namely the polynomial degree and the flag
telling us whether to fit the intercept. This can be set up using
Scikit-Learn’s <code>GridSearchCV</code> meta-estimator:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">18</code><code class="p">]:</code> <code class="kn">from</code> <code class="nn">sklearn.model_selection</code> <code class="kn">import</code> <code class="n">GridSearchCV</code>

         <code class="n">param_grid</code> <code class="o">=</code> <code class="p">{</code><code class="s1">'polynomialfeatures__degree'</code><code class="p">:</code> <code class="n">np</code><code class="o">.</code><code class="n">arange</code><code class="p">(</code><code class="mi">21</code><code class="p">),</code>
                       <code class="s1">'linearregression__fit_intercept'</code><code class="p">:</code> <code class="p">[</code><code class="kc">True</code><code class="p">,</code> <code class="kc">False</code><code class="p">]}</code>

         <code class="n">grid</code> <code class="o">=</code> <code class="n">GridSearchCV</code><code class="p">(</code><code class="n">PolynomialRegression</code><code class="p">(),</code> <code class="n">param_grid</code><code class="p">,</code> <code class="n">cv</code><code class="o">=</code><code class="mi">7</code><code class="p">)</code></pre>
<p>Notice that like a normal estimator, this has not yet been applied to
any data. Calling the <code>fit</code> method will fit the model at each grid
point, keeping track of the scores along the way:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">19</code><code class="p">]:</code> <code class="n">grid</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">);</code></pre>
<p>Now that the model is fit, we can ask for the best parameters as
follows:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">20</code><code class="p">]:</code> <code class="n">grid</code><code class="o">.</code><code class="n">best_params_</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">20</code><code class="p">]:</code> <code class="p">{</code><code class="s1">'linearregression__fit_intercept'</code><code class="p">:</code> <code class="kc">False</code><code class="p">,</code> <code class="s1">'polynomialfeatures__degree'</code><code class="p">:</code> <code class="mi">4</code><code class="p">}</code></pre>
<p>Finally, if we wish, we can use the best model and show the fit to our
data using code from before (see <a data-type="xref" href="#fig_0503-hyperparameters-and-model-validation_files_in_output_56_0">Figure 39-13</a>).</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">21</code><code class="p">]:</code> <code class="n">model</code> <code class="o">=</code> <code class="n">grid</code><code class="o">.</code><code class="n">best_estimator_</code>

         <code class="n">plt</code><code class="o">.</code><code class="n">scatter</code><code class="p">(</code><code class="n">X</code><code class="o">.</code><code class="n">ravel</code><code class="p">(),</code> <code class="n">y</code><code class="p">)</code>
         <code class="n">lim</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">axis</code><code class="p">()</code>
         <code class="n">y_test</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X_test</code><code class="p">)</code>
         <code class="n">plt</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">X_test</code><code class="o">.</code><code class="n">ravel</code><code class="p">(),</code> <code class="n">y_test</code><code class="p">);</code>
         <code class="n">plt</code><code class="o">.</code><code class="n">axis</code><code class="p">(</code><code class="n">lim</code><code class="p">);</code></pre>
<figure><div class="figure" id="fig_0503-hyperparameters-and-model-validation_files_in_output_56_0">
<img alt="output 56 0" height="395" src="assets/output_56_0.png" width="600"/>
<h6><span class="label">Figure 39-13. </span>The best-fit model determined via an automatic grid search</h6>
</div></figure>
<p>Other options in <code>GridSearchCV</code> include the ability to specify a custom
scoring function, to parallelize the computations, to do randomized
searches, and more. For more information, see the examples in Chapters
<a href="ch49.xhtml#section-0513-kernel-density-estimation">49</a> and <a href="ch50.xhtml#section-0514-image-features">50</a>, or refer to Scikit-Learn’s
<a href="https://oreil.ly/xft8j">grid search
documentation</a><a data-startref="ix_ch39-asciidoc15" data-type="indexterm" id="idm45858739819072"/>.<a data-startref="ix_ch39-asciidoc2" data-type="indexterm" id="idm45858739818272"/></p>
</div></section>
<section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="ch_0503-hyperparameters-and-model-validation_summary">
<h1>Summary</h1>
<p>In this chapter we began to explore the concept of model validation and
hyperparameter optimization, focusing on intuitive aspects of the
bias–variance trade-off and how it comes into play when fitting models
to data. In particular, we found that the use of a validation set or
cross-validation approach is vital when tuning parameters in order to
avoid overfitting for more complex/flexible models.<a data-startref="ix_ch39-asciidoc1" data-type="indexterm" id="idm45858739816224"/><a data-startref="ix_ch39-asciidoc0" data-type="indexterm" id="idm45858739815520"/></p>
<p>In later chapters, we will discuss the details of particularly useful
models, what tuning is available for these models, and how these free
parameters affect model complexity. Keep the lessons of this chapter in
mind as you read on and learn about these machine learning approaches!</p>
</div></section>
<div data-type="footnotes"><p data-type="footnote" id="idm45858742132032"><sup><a href="ch39.xhtml#idm45858742132032-marker">1</a></sup> Code to produce this figure can be found in the <a href="https://oreil.ly/jv0wb">online appendix</a>.</p><p data-type="footnote" id="idm45858742031984"><sup><a href="ch39.xhtml#idm45858742031984-marker">2</a></sup> Code to produce this figure can be found in the <a href="https://oreil.ly/2BP2o">online appendix</a>.</p><p data-type="footnote" id="idm45858741405920"><sup><a href="ch39.xhtml#idm45858741405920-marker">3</a></sup> Code to produce this figure can be found in the <a href="https://oreil.ly/j9G96">online appendix</a>.</p><p data-type="footnote" id="idm45858741397728"><sup><a href="ch39.xhtml#idm45858741397728-marker">4</a></sup> Code to produce this figure can be found in the <a href="https://oreil.ly/YfwRC">online appendix</a>.</p><p data-type="footnote" id="idm45858741372496"><sup><a href="ch39.xhtml#idm45858741372496-marker">5</a></sup> Code to produce this figure can be found in the <a href="https://oreil.ly/4AK15">online appendix</a>.</p><p data-type="footnote" id="idm45858741065984"><sup><a href="ch39.xhtml#idm45858741065984-marker">6</a></sup> A full-color version of this figure can be found on <a href="https://oreil.ly/PDSH_GitHub">GitHub</a>.</p><p data-type="footnote" id="idm45858740554592"><sup><a href="ch39.xhtml#idm45858740554592-marker">7</a></sup> A full-color version of this figure can be found on <a href="https://oreil.ly/PDSH_GitHub">GitHub</a>.</p><p data-type="footnote" id="idm45858740543504"><sup><a href="ch39.xhtml#idm45858740543504-marker">8</a></sup> Code to produce this figure can be found in the <a href="https://oreil.ly/omZ1c">online appendix</a>.</p><p data-type="footnote" id="idm45858740396064"><sup><a href="ch39.xhtml#idm45858740396064-marker">9</a></sup> A full-size version of this figure can be found on <a href="https://oreil.ly/PDSH_GitHub">GitHub</a>.</p></div></div></section></div></body></html>