<html><head></head><body><section data-pdf-bookmark="Chapter 8. Gradient Descent" data-type="chapter" epub:type="chapter"><div class="chapter" id="gradient_descent">&#13;
<h1><span class="label">Chapter 8. </span>Gradient Descent</h1>&#13;
&#13;
<blockquote data-type="epigraph" epub:type="epigraph">&#13;
    <p>Those who boast of their descent, brag on what they owe to others.</p>&#13;
    <p data-type="attribution">Seneca</p>&#13;
</blockquote>&#13;
&#13;
<p>Frequently when doing data science, we’ll be trying to the find the best model for a certain situation.  And usually “best” will mean something like “minimizes the error of its predictions” or “maximizes the likelihood of the data.”  In other words, it will represent the solution to some sort of optimization problem.</p>&#13;
&#13;
<p>This means we’ll need to solve a number of optimization problems.  And in particular, we’ll need to solve them from scratch.  Our approach will be a technique called <em>gradient descent</em>, which lends itself pretty well to a from-scratch treatment.  You might not find it super-exciting in and of itself, but it will enable us to do exciting things throughout the book, so bear with me.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="The Idea Behind Gradient Descent" data-type="sect1"><div class="sect1" id="idm45635752925048">&#13;
<h1>The Idea Behind Gradient Descent</h1>&#13;
&#13;
<p>Suppose<a data-primary="gradient descent" data-secondary="concept of" data-type="indexterm" id="idm45635752923464"/> we have some function <code>f</code> that takes as input a vector of real numbers and outputs a single real number.  One simple such function is:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="kn">from</code> <code class="nn">scratch.linear_algebra</code> <code class="kn">import</code> <code class="n">Vector</code><code class="p">,</code> <code class="n">dot</code>&#13;
&#13;
<code class="k">def</code> <code class="nf">sum_of_squares</code><code class="p">(</code><code class="n">v</code><code class="p">:</code> <code class="n">Vector</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">float</code><code class="p">:</code>&#13;
    <code class="sd">"""Computes the sum of squared elements in v"""</code>&#13;
    <code class="k">return</code> <code class="n">dot</code><code class="p">(</code><code class="n">v</code><code class="p">,</code> <code class="n">v</code><code class="p">)</code></pre>&#13;
&#13;
<p>We’ll frequently need to maximize or minimize such functions.  That is, we need to find the input <code>v</code> that produces the largest (or smallest) possible value.</p>&#13;
&#13;
<p>For functions like ours, the <em>gradient</em> (if you remember your calculus, this is the vector of partial derivatives) gives the input direction in which the function most quickly increases. (If you don’t remember your calculus, take my word for it or look it up on the internet.)</p>&#13;
&#13;
<p>Accordingly, one approach to maximizing a function is to pick a random starting point, compute the gradient, take a small step in the direction of the gradient (i.e., the direction that causes the function to increase the most), and repeat with the new starting point.  Similarly, you can try to minimize a function by taking small steps in the <em>opposite</em> direction, as shown in <a data-type="xref" href="#gradient_descent_image">Figure 8-1</a>.</p>&#13;
&#13;
<figure><div class="figure" id="gradient_descent_image">&#13;
<img alt="Finding a minimum using gradient descent." src="assets/dsf2_0801.png"/>&#13;
<h6><span class="label">Figure 8-1. </span>Finding a minimum using gradient descent</h6>&#13;
</div></figure>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>If a function has a unique global minimum, this procedure is likely to find it. If a function has multiple (local) minima, this procedure might “find” the wrong one of them, in which case you might rerun the procedure from different starting points. If a function has no minimum, then it’s possible the procedure might go on forever.</p>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Estimating the Gradient" data-type="sect1"><div class="sect1" id="idm45635752861256">&#13;
<h1>Estimating the Gradient</h1>&#13;
&#13;
<p>If <code>f</code> is a function<a data-primary="gradient descent" data-secondary="estimating the gradient" data-type="indexterm" id="idm45635752849304"/> of one variable, its derivative at a point <code>x</code> measures how <code>f(x)</code> changes when we make a very small change to <code>x</code>. The derivative is defined as the limit of the difference quotients:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="kn">from</code> <code class="nn">typing</code> <code class="kn">import</code> <code class="n">Callable</code>&#13;
&#13;
<code class="k">def</code> <code class="nf">difference_quotient</code><code class="p">(</code><code class="n">f</code><code class="p">:</code> <code class="n">Callable</code><code class="p">[[</code><code class="nb">float</code><code class="p">],</code> <code class="nb">float</code><code class="p">],</code>&#13;
                        <code class="n">x</code><code class="p">:</code> <code class="nb">float</code><code class="p">,</code>&#13;
                        <code class="n">h</code><code class="p">:</code> <code class="nb">float</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">float</code><code class="p">:</code>&#13;
    <code class="k">return</code> <code class="p">(</code><code class="n">f</code><code class="p">(</code><code class="n">x</code> <code class="o">+</code> <code class="n">h</code><code class="p">)</code> <code class="o">-</code> <code class="n">f</code><code class="p">(</code><code class="n">x</code><code class="p">))</code> <code class="o">/</code> <code class="n">h</code></pre>&#13;
&#13;
<p>as <code>h</code> approaches zero.</p>&#13;
&#13;
<p>(Many a would-be calculus student has been stymied by the mathematical definition of limit, which is beautiful but can seem somewhat forbidding. Here we’ll cheat and simply say that “limit” means what you think it means.)</p>&#13;
&#13;
<p>The derivative is the slope of the tangent line at &#13;
<math>&#13;
  <mrow>&#13;
    <mo>(</mo>&#13;
    <mi>x</mi>&#13;
    <mo>,</mo>&#13;
    <mi>f</mi>&#13;
    <mo>(</mo>&#13;
    <mi>x</mi>&#13;
    <mo>)</mo>&#13;
    <mo>)</mo>&#13;
  </mrow>&#13;
</math>, while the difference quotient is the slope of the not-quite-tangent line that runs through <math>&#13;
  <mrow>&#13;
    <mo>(</mo>&#13;
    <mi>x</mi>&#13;
    <mo>+</mo>&#13;
    <mi>h</mi>&#13;
    <mo>,</mo>&#13;
    <mi>f</mi>&#13;
    <mo>(</mo>&#13;
    <mi>x</mi>&#13;
    <mo>+</mo>&#13;
    <mi>h</mi>&#13;
    <mo>)</mo>&#13;
    <mo>)</mo>&#13;
  </mrow>&#13;
</math>.  As <em>h</em> gets smaller and smaller, the not-quite-tangent line gets closer and closer to the tangent line (<a data-type="xref" href="#difference_quotient">Figure 8-2</a>).</p>&#13;
&#13;
<figure><div class="figure" id="difference_quotient">&#13;
<img alt="Difference quotient as approximation to derivative." src="assets/dsf2_0802.png"/>&#13;
<h6><span class="label">Figure 8-2. </span>Approximating a derivative with a difference quotient</h6>&#13;
</div></figure>&#13;
&#13;
<p>For many functions it’s easy to exactly calculate derivatives.  For example, the <code>square</code> function:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="k">def</code> <code class="nf">square</code><code class="p">(</code><code class="n">x</code><code class="p">:</code> <code class="nb">float</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">float</code><code class="p">:</code>&#13;
    <code class="k">return</code> <code class="n">x</code> <code class="o">*</code> <code class="n">x</code></pre>&#13;
&#13;
<p>has the derivative:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="k">def</code> <code class="nf">derivative</code><code class="p">(</code><code class="n">x</code><code class="p">:</code> <code class="nb">float</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">float</code><code class="p">:</code>&#13;
    <code class="k">return</code> <code class="mi">2</code> <code class="o">*</code> <code class="n">x</code></pre>&#13;
&#13;
<p>which is easy for us to check by explicitly computing the difference quotient and taking the limit. (Doing so requires nothing more than high school algebra.)</p>&#13;
&#13;
<p>What if you couldn’t (or didn’t want to) find the gradient?  Although we can’t take limits in Python, we can estimate derivatives by evaluating the difference quotient for a very small <code>e</code>. <a data-type="xref" href="#difference_quotient_goodness">Figure 8-3</a> shows the results of one such estimation:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="n">xs</code> <code class="o">=</code> <code class="nb">range</code><code class="p">(</code><code class="o">-</code><code class="mi">10</code><code class="p">,</code> <code class="mi">11</code><code class="p">)</code>&#13;
<code class="n">actuals</code> <code class="o">=</code> <code class="p">[</code><code class="n">derivative</code><code class="p">(</code><code class="n">x</code><code class="p">)</code> <code class="k">for</code> <code class="n">x</code> <code class="ow">in</code> <code class="n">xs</code><code class="p">]</code>&#13;
<code class="n">estimates</code> <code class="o">=</code> <code class="p">[</code><code class="n">difference_quotient</code><code class="p">(</code><code class="n">square</code><code class="p">,</code> <code class="n">x</code><code class="p">,</code> <code class="n">h</code><code class="o">=</code><code class="mf">0.001</code><code class="p">)</code> <code class="k">for</code> <code class="n">x</code> <code class="ow">in</code> <code class="n">xs</code><code class="p">]</code>&#13;
&#13;
<code class="c1"># plot to show they're basically the same</code>&#13;
<code class="kn">import</code> <code class="nn">matplotlib.pyplot</code> <code class="kn">as</code> <code class="nn">plt</code>&#13;
<code class="n">plt</code><code class="o">.</code><code class="n">title</code><code class="p">(</code><code class="s2">"Actual Derivatives vs. Estimates"</code><code class="p">)</code>&#13;
<code class="n">plt</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">xs</code><code class="p">,</code> <code class="n">actuals</code><code class="p">,</code> <code class="s1">'rx'</code><code class="p">,</code> <code class="n">label</code><code class="o">=</code><code class="s1">'Actual'</code><code class="p">)</code>       <code class="c1"># red  x</code>&#13;
<code class="n">plt</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">xs</code><code class="p">,</code> <code class="n">estimates</code><code class="p">,</code> <code class="s1">'b+'</code><code class="p">,</code> <code class="n">label</code><code class="o">=</code><code class="s1">'Estimate'</code><code class="p">)</code>   <code class="c1"># blue +</code>&#13;
<code class="n">plt</code><code class="o">.</code><code class="n">legend</code><code class="p">(</code><code class="n">loc</code><code class="o">=</code><code class="mi">9</code><code class="p">)</code>&#13;
<code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">()</code></pre>&#13;
&#13;
<figure><div class="figure" id="difference_quotient_goodness">&#13;
<img alt="Difference quotient is a good approximation." src="assets/dsf2_0803.png"/>&#13;
<h6><span class="label">Figure 8-3. </span>Goodness of difference quotient approximation</h6>&#13;
</div></figure>&#13;
&#13;
<p>When <code>f</code> is<a data-primary="partial derivatives" data-type="indexterm" id="idm45635752553816"/> a function of many variables, it has multiple <em>partial derivatives</em>,&#13;
each indicating how <code>f</code> changes when we make small changes in just one of the input variables.</p>&#13;
&#13;
<p>We calculate its <em>i</em>th partial derivative by treating it&#13;
as a function of just its <em>i</em>th variable, holding the other variables fixed:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="k">def</code> <code class="nf">partial_difference_quotient</code><code class="p">(</code><code class="n">f</code><code class="p">:</code> <code class="n">Callable</code><code class="p">[[</code><code class="n">Vector</code><code class="p">],</code> <code class="nb">float</code><code class="p">],</code>&#13;
                                <code class="n">v</code><code class="p">:</code> <code class="n">Vector</code><code class="p">,</code>&#13;
                                <code class="n">i</code><code class="p">:</code> <code class="nb">int</code><code class="p">,</code>&#13;
                                <code class="n">h</code><code class="p">:</code> <code class="nb">float</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">float</code><code class="p">:</code>&#13;
    <code class="sd">"""Returns the i-th partial difference quotient of f at v"""</code>&#13;
    <code class="n">w</code> <code class="o">=</code> <code class="p">[</code><code class="n">v_j</code> <code class="o">+</code> <code class="p">(</code><code class="n">h</code> <code class="k">if</code> <code class="n">j</code> <code class="o">==</code> <code class="n">i</code> <code class="k">else</code> <code class="mi">0</code><code class="p">)</code>    <code class="c1"># add h to just the ith element of v</code>&#13;
         <code class="k">for</code> <code class="n">j</code><code class="p">,</code> <code class="n">v_j</code> <code class="ow">in</code> <code class="nb">enumerate</code><code class="p">(</code><code class="n">v</code><code class="p">)]</code>&#13;
&#13;
    <code class="k">return</code> <code class="p">(</code><code class="n">f</code><code class="p">(</code><code class="n">w</code><code class="p">)</code> <code class="o">-</code> <code class="n">f</code><code class="p">(</code><code class="n">v</code><code class="p">))</code> <code class="o">/</code> <code class="n">h</code></pre>&#13;
&#13;
<p>after which we can estimate the gradient the same way:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="k">def</code> <code class="nf">estimate_gradient</code><code class="p">(</code><code class="n">f</code><code class="p">:</code> <code class="n">Callable</code><code class="p">[[</code><code class="n">Vector</code><code class="p">],</code> <code class="nb">float</code><code class="p">],</code>&#13;
                      <code class="n">v</code><code class="p">:</code> <code class="n">Vector</code><code class="p">,</code>&#13;
                      <code class="n">h</code><code class="p">:</code> <code class="nb">float</code> <code class="o">=</code> <code class="mf">0.0001</code><code class="p">):</code>&#13;
    <code class="k">return</code> <code class="p">[</code><code class="n">partial_difference_quotient</code><code class="p">(</code><code class="n">f</code><code class="p">,</code> <code class="n">v</code><code class="p">,</code> <code class="n">i</code><code class="p">,</code> <code class="n">h</code><code class="p">)</code>&#13;
            <code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="nb">len</code><code class="p">(</code><code class="n">v</code><code class="p">))]</code></pre>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>A major drawback to this “estimate using difference quotients” approach is&#13;
that it’s computationally expensive.&#13;
If <code>v</code> has length <em>n</em>, <code>estimate_gradient</code> has to evaluate <code>f</code> on 2<em>n</em> different inputs.&#13;
If you’re repeatedly estimating gradients, you’re doing a whole lot of extra work.&#13;
In everything we do, we’ll use math to calculate our gradient functions explicitly.</p>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Using the Gradient" data-type="sect1"><div class="sect1" id="idm45635752850952">&#13;
<h1>Using the Gradient</h1>&#13;
&#13;
<p>It’s<a data-primary="gradient descent" data-secondary="using the gradient" data-type="indexterm" id="idm45635752314984"/> easy to see that the <code>sum_of_squares</code> function is smallest when its input <code>v</code> is a vector of zeros.&#13;
But imagine we didn’t know that.&#13;
Let’s use gradients to find the minimum among all three-dimensional vectors.&#13;
We’ll just pick a random starting point and then take tiny steps in the opposite direction of the gradient&#13;
until we reach a point where the gradient is very small:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="kn">import</code> <code class="nn">random</code>&#13;
<code class="kn">from</code> <code class="nn">scratch.linear_algebra</code> <code class="kn">import</code> <code class="n">distance</code><code class="p">,</code> <code class="n">add</code><code class="p">,</code> <code class="n">scalar_multiply</code>&#13;
&#13;
<code class="k">def</code> <code class="nf">gradient_step</code><code class="p">(</code><code class="n">v</code><code class="p">:</code> <code class="n">Vector</code><code class="p">,</code> <code class="n">gradient</code><code class="p">:</code> <code class="n">Vector</code><code class="p">,</code> <code class="n">step_size</code><code class="p">:</code> <code class="nb">float</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="n">Vector</code><code class="p">:</code>&#13;
    <code class="sd">"""Moves `step_size` in the `gradient` direction from `v`"""</code>&#13;
    <code class="k">assert</code> <code class="nb">len</code><code class="p">(</code><code class="n">v</code><code class="p">)</code> <code class="o">==</code> <code class="nb">len</code><code class="p">(</code><code class="n">gradient</code><code class="p">)</code>&#13;
    <code class="n">step</code> <code class="o">=</code> <code class="n">scalar_multiply</code><code class="p">(</code><code class="n">step_size</code><code class="p">,</code> <code class="n">gradient</code><code class="p">)</code>&#13;
    <code class="k">return</code> <code class="n">add</code><code class="p">(</code><code class="n">v</code><code class="p">,</code> <code class="n">step</code><code class="p">)</code>&#13;
&#13;
<code class="k">def</code> <code class="nf">sum_of_squares_gradient</code><code class="p">(</code><code class="n">v</code><code class="p">:</code> <code class="n">Vector</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="n">Vector</code><code class="p">:</code>&#13;
    <code class="k">return</code> <code class="p">[</code><code class="mi">2</code> <code class="o">*</code> <code class="n">v_i</code> <code class="k">for</code> <code class="n">v_i</code> <code class="ow">in</code> <code class="n">v</code><code class="p">]</code></pre>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="c1"># pick a random starting point</code>&#13;
<code class="n">v</code> <code class="o">=</code> <code class="p">[</code><code class="n">random</code><code class="o">.</code><code class="n">uniform</code><code class="p">(</code><code class="o">-</code><code class="mi">10</code><code class="p">,</code> <code class="mi">10</code><code class="p">)</code> <code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">3</code><code class="p">)]</code>&#13;
&#13;
<code class="k">for</code> <code class="n">epoch</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">1000</code><code class="p">):</code>&#13;
    <code class="n">grad</code> <code class="o">=</code> <code class="n">sum_of_squares_gradient</code><code class="p">(</code><code class="n">v</code><code class="p">)</code>    <code class="c1"># compute the gradient at v</code>&#13;
    <code class="n">v</code> <code class="o">=</code> <code class="n">gradient_step</code><code class="p">(</code><code class="n">v</code><code class="p">,</code> <code class="n">grad</code><code class="p">,</code> <code class="o">-</code><code class="mf">0.01</code><code class="p">)</code>    <code class="c1"># take a negative gradient step</code>&#13;
    <code class="k">print</code><code class="p">(</code><code class="n">epoch</code><code class="p">,</code> <code class="n">v</code><code class="p">)</code>&#13;
&#13;
<code class="k">assert</code> <code class="n">distance</code><code class="p">(</code><code class="n">v</code><code class="p">,</code> <code class="p">[</code><code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">])</code> <code class="o">&lt;</code> <code class="mf">0.001</code>    <code class="c1"># v should be close to 0</code></pre>&#13;
&#13;
<p>If you run this, you’ll find that it always ends up with a <code>v</code> that’s very close to <code>[0,0,0]</code>.  The more epochs you run it for, the closer it will get.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Choosing the Right Step Size" data-type="sect1"><div class="sect1" id="idm45635752118424">&#13;
<h1>Choosing the Right Step Size</h1>&#13;
&#13;
<p>Although<a data-primary="gradient descent" data-secondary="choosing step size" data-type="indexterm" id="idm45635752117176"/> the rationale for moving against the gradient is clear, how far to move is not. Indeed, choosing the right step size is more of an art than a science.  Popular options include:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Using a fixed step size</p>&#13;
</li>&#13;
<li>&#13;
<p>Gradually shrinking the step size over time</p>&#13;
</li>&#13;
<li>&#13;
<p>At each step, choosing the step size that minimizes the value of the objective function</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>The last approach sounds great but is, in practice, a costly computation.&#13;
To keep things simple, we’ll mostly just use a fixed step size.&#13;
The step size that “works” depends on the problem—too small, and&#13;
your gradient descent will take forever; too big, and you’ll take giant&#13;
steps that might make the function you care about get larger or even&#13;
be undefined. So we’ll need to experiment.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Using Gradient Descent to Fit Models" data-type="sect1"><div class="sect1" id="idm45635752111720">&#13;
<h1>Using Gradient Descent to Fit Models</h1>&#13;
&#13;
<p>In<a data-primary="gradient descent" data-secondary="using to fit models" data-type="indexterm" id="idm45635752109976"/><a data-primary="loss functions" data-type="indexterm" id="idm45635752108968"/> this book, we’ll be using gradient descent to fit parameterized models to data.&#13;
In the usual case, we’ll have some dataset and some (hypothesized) model for the data&#13;
that depends (in a differentiable way) on one or more parameters.&#13;
We’ll also have a <em>loss</em> function that measures how well the model&#13;
fits our data. (Smaller is better.)</p>&#13;
&#13;
<p>If we think of our data as being fixed, then our loss function&#13;
tells us how good or bad any particular model parameters are. This means we can use gradient descent to find the model parameters that make the loss as small as possible. Let’s look at a simple example:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="c1"># x ranges from -50 to 49, y is always 20 * x + 5</code>&#13;
<code class="n">inputs</code> <code class="o">=</code> <code class="p">[(</code><code class="n">x</code><code class="p">,</code> <code class="mi">20</code> <code class="o">*</code> <code class="n">x</code> <code class="o">+</code> <code class="mi">5</code><code class="p">)</code> <code class="k">for</code> <code class="n">x</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="o">-</code><code class="mi">50</code><code class="p">,</code> <code class="mi">50</code><code class="p">)]</code></pre>&#13;
&#13;
<p>In this case we <em>know</em> the parameters of the linear relationship between&#13;
<code>x</code> and <code>y</code>, but imagine we’d like to learn them from the data. We’ll&#13;
use gradient descent to find the slope and intercept that minimize&#13;
the average squared error.</p>&#13;
&#13;
<p>We’ll start off with a function that&#13;
determines the gradient based on the error from a single data point:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="k">def</code> <code class="nf">linear_gradient</code><code class="p">(</code><code class="n">x</code><code class="p">:</code> <code class="nb">float</code><code class="p">,</code> <code class="n">y</code><code class="p">:</code> <code class="nb">float</code><code class="p">,</code> <code class="n">theta</code><code class="p">:</code> <code class="n">Vector</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="n">Vector</code><code class="p">:</code>&#13;
    <code class="n">slope</code><code class="p">,</code> <code class="n">intercept</code> <code class="o">=</code> <code class="n">theta</code>&#13;
    <code class="n">predicted</code> <code class="o">=</code> <code class="n">slope</code> <code class="o">*</code> <code class="n">x</code> <code class="o">+</code> <code class="n">intercept</code>    <code class="c1"># The prediction of the model.</code>&#13;
    <code class="n">error</code> <code class="o">=</code> <code class="p">(</code><code class="n">predicted</code> <code class="o">-</code> <code class="n">y</code><code class="p">)</code>              <code class="c1"># error is (predicted - actual).</code>&#13;
    <code class="n">squared_error</code> <code class="o">=</code> <code class="n">error</code> <code class="o">**</code> <code class="mi">2</code>           <code class="c1"># We'll minimize squared error</code>&#13;
    <code class="n">grad</code> <code class="o">=</code> <code class="p">[</code><code class="mi">2</code> <code class="o">*</code> <code class="n">error</code> <code class="o">*</code> <code class="n">x</code><code class="p">,</code> <code class="mi">2</code> <code class="o">*</code> <code class="n">error</code><code class="p">]</code>    <code class="c1"># using its gradient.</code>&#13;
    <code class="k">return</code> <code class="n">grad</code></pre>&#13;
&#13;
<p>Let’s think about what that gradient means. Imagine for some <code>x</code> our prediction is too large. In that case the <code>error</code> is positive. The second gradient term, <code>2 * error</code>, is positive, which reflects the fact that small increases in the intercept will make the (already too large) prediction even larger, which will cause the squared error (for this <code>x</code>) to get even bigger.</p>&#13;
&#13;
<p>The first gradient term, <code>2 * error * x</code>, has the same sign as <code>x</code>.&#13;
Sure enough, if <code>x</code> is positive, small increases in the slope&#13;
will again make the prediction (and hence the error) larger. If <code>x</code> is negative, though, small increases in the slope&#13;
will make the prediction (and hence the error) smaller.</p>&#13;
&#13;
<p>Now, that computation<a data-primary="mean squared error" data-type="indexterm" id="idm45635751948360"/> was for a single data point.&#13;
For the whole dataset we’ll look at the <em>mean squared error</em>.&#13;
And the gradient of the mean squared error is just the mean of the individual gradients.</p>&#13;
&#13;
<p>So, here’s what we’re going to do:</p>&#13;
<ol>&#13;
<li>&#13;
<p>Start with a random value for <code>theta</code>.</p>&#13;
</li>&#13;
<li>&#13;
<p>Compute the mean of the gradients.</p>&#13;
</li>&#13;
<li>&#13;
<p>Adjust <code>theta</code> in that direction.</p>&#13;
</li>&#13;
<li>&#13;
<p>Repeat.</p>&#13;
</li>&#13;
&#13;
</ol>&#13;
&#13;
<p>After a lot of <em>epochs</em> (what we call each pass through the dataset),&#13;
we should learn something like the correct parameters:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="kn">from</code> <code class="nn">scratch.linear_algebra</code> <code class="kn">import</code> <code class="n">vector_mean</code>&#13;
&#13;
<code class="c1"># Start with random values for slope and intercept</code>&#13;
<code class="n">theta</code> <code class="o">=</code> <code class="p">[</code><code class="n">random</code><code class="o">.</code><code class="n">uniform</code><code class="p">(</code><code class="o">-</code><code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">),</code> <code class="n">random</code><code class="o">.</code><code class="n">uniform</code><code class="p">(</code><code class="o">-</code><code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">)]</code>&#13;
&#13;
<code class="n">learning_rate</code> <code class="o">=</code> <code class="mf">0.001</code>&#13;
&#13;
<code class="k">for</code> <code class="n">epoch</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">5000</code><code class="p">):</code>&#13;
    <code class="c1"># Compute the mean of the gradients</code>&#13;
    <code class="n">grad</code> <code class="o">=</code> <code class="n">vector_mean</code><code class="p">([</code><code class="n">linear_gradient</code><code class="p">(</code><code class="n">x</code><code class="p">,</code> <code class="n">y</code><code class="p">,</code> <code class="n">theta</code><code class="p">)</code> <code class="k">for</code> <code class="n">x</code><code class="p">,</code> <code class="n">y</code> <code class="ow">in</code> <code class="n">inputs</code><code class="p">])</code>&#13;
    <code class="c1"># Take a step in that direction</code>&#13;
    <code class="n">theta</code> <code class="o">=</code> <code class="n">gradient_step</code><code class="p">(</code><code class="n">theta</code><code class="p">,</code> <code class="n">grad</code><code class="p">,</code> <code class="o">-</code><code class="n">learning_rate</code><code class="p">)</code>&#13;
    <code class="k">print</code><code class="p">(</code><code class="n">epoch</code><code class="p">,</code> <code class="n">theta</code><code class="p">)</code>&#13;
&#13;
<code class="n">slope</code><code class="p">,</code> <code class="n">intercept</code> <code class="o">=</code> <code class="n">theta</code>&#13;
<code class="k">assert</code> <code class="mf">19.9</code> <code class="o">&lt;</code> <code class="n">slope</code> <code class="o">&lt;</code> <code class="mf">20.1</code><code class="p">,</code>   <code class="s2">"slope should be about 20"</code>&#13;
<code class="k">assert</code> <code class="mf">4.9</code> <code class="o">&lt;</code> <code class="n">intercept</code> <code class="o">&lt;</code> <code class="mf">5.1</code><code class="p">,</code> <code class="s2">"intercept should be about 5"</code></pre>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Minibatch and Stochastic Gradient Descent" data-type="sect1"><div class="sect1" id="idm45635752111096">&#13;
<h1>Minibatch and Stochastic Gradient Descent</h1>&#13;
&#13;
<p>One<a data-primary="gradient descent" data-secondary="minibatch and stochastic gradient descent" data-type="indexterm" id="idm45635751816920"/> drawback of the preceding approach is that we had to&#13;
evaluate the gradients on the entire dataset before we could&#13;
take a gradient step and update our parameters. In this case&#13;
it was fine, because our dataset was only 100 pairs and the&#13;
gradient computation was cheap.</p>&#13;
&#13;
<p>Your models, however, will frequently have large datasets&#13;
and expensive gradient computations. In that case you’ll want to take&#13;
gradient steps more often.</p>&#13;
&#13;
<p>We<a data-primary="minibatch gradient descent" data-type="indexterm" id="idm45635751814568"/> can do this using a technique called <em>minibatch gradient descent</em>,&#13;
in which we compute the gradient (and take a gradient step)&#13;
based on a “minibatch” sampled from the larger dataset:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="kn">from</code> <code class="nn">typing</code> <code class="kn">import</code> <code class="n">TypeVar</code><code class="p">,</code> <code class="n">List</code><code class="p">,</code> <code class="n">Iterator</code>&#13;
&#13;
<code class="n">T</code> <code class="o">=</code> <code class="n">TypeVar</code><code class="p">(</code><code class="s1">'T'</code><code class="p">)</code>  <code class="c1"># this allows us to type "generic" functions</code>&#13;
&#13;
<code class="k">def</code> <code class="nf">minibatches</code><code class="p">(</code><code class="n">dataset</code><code class="p">:</code> <code class="n">List</code><code class="p">[</code><code class="n">T</code><code class="p">],</code>&#13;
                <code class="n">batch_size</code><code class="p">:</code> <code class="nb">int</code><code class="p">,</code>&#13;
                <code class="n">shuffle</code><code class="p">:</code> <code class="nb">bool</code> <code class="o">=</code> <code class="bp">True</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="n">Iterator</code><code class="p">[</code><code class="n">List</code><code class="p">[</code><code class="n">T</code><code class="p">]]:</code>&#13;
    <code class="sd">"""Generates `batch_size`-sized minibatches from the dataset"""</code>&#13;
    <code class="c1"># start indexes 0, batch_size, 2 * batch_size, ...</code>&#13;
    <code class="n">batch_starts</code> <code class="o">=</code> <code class="p">[</code><code class="n">start</code> <code class="k">for</code> <code class="n">start</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="nb">len</code><code class="p">(</code><code class="n">dataset</code><code class="p">),</code> <code class="n">batch_size</code><code class="p">)]</code>&#13;
&#13;
    <code class="k">if</code> <code class="n">shuffle</code><code class="p">:</code> <code class="n">random</code><code class="o">.</code><code class="n">shuffle</code><code class="p">(</code><code class="n">batch_starts</code><code class="p">)</code>  <code class="c1"># shuffle the batches</code>&#13;
&#13;
    <code class="k">for</code> <code class="n">start</code> <code class="ow">in</code> <code class="n">batch_starts</code><code class="p">:</code>&#13;
        <code class="n">end</code> <code class="o">=</code> <code class="n">start</code> <code class="o">+</code> <code class="n">batch_size</code>&#13;
        <code class="k">yield</code> <code class="n">dataset</code><code class="p">[</code><code class="n">start</code><code class="p">:</code><code class="n">end</code><code class="p">]</code></pre>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>The <code>TypeVar(T)</code> allows us to create a “generic” function. It says that our <code>dataset</code> can be a list of any single type—<code>str</code>s, <code>int</code>s, <code>list</code>s, whatever—but whatever that type is, the outputs will be batches of it.</p>&#13;
</div>&#13;
&#13;
<p>Now we can solve our problem again using minibatches:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="n">theta</code> <code class="o">=</code> <code class="p">[</code><code class="n">random</code><code class="o">.</code><code class="n">uniform</code><code class="p">(</code><code class="o">-</code><code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">),</code> <code class="n">random</code><code class="o">.</code><code class="n">uniform</code><code class="p">(</code><code class="o">-</code><code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">)]</code>&#13;
&#13;
<code class="k">for</code> <code class="n">epoch</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">1000</code><code class="p">):</code>&#13;
    <code class="k">for</code> <code class="n">batch</code> <code class="ow">in</code> <code class="n">minibatches</code><code class="p">(</code><code class="n">inputs</code><code class="p">,</code> <code class="n">batch_size</code><code class="o">=</code><code class="mi">20</code><code class="p">):</code>&#13;
        <code class="n">grad</code> <code class="o">=</code> <code class="n">vector_mean</code><code class="p">([</code><code class="n">linear_gradient</code><code class="p">(</code><code class="n">x</code><code class="p">,</code> <code class="n">y</code><code class="p">,</code> <code class="n">theta</code><code class="p">)</code> <code class="k">for</code> <code class="n">x</code><code class="p">,</code> <code class="n">y</code> <code class="ow">in</code> <code class="n">batch</code><code class="p">])</code>&#13;
        <code class="n">theta</code> <code class="o">=</code> <code class="n">gradient_step</code><code class="p">(</code><code class="n">theta</code><code class="p">,</code> <code class="n">grad</code><code class="p">,</code> <code class="o">-</code><code class="n">learning_rate</code><code class="p">)</code>&#13;
    <code class="k">print</code><code class="p">(</code><code class="n">epoch</code><code class="p">,</code> <code class="n">theta</code><code class="p">)</code>&#13;
&#13;
<code class="n">slope</code><code class="p">,</code> <code class="n">intercept</code> <code class="o">=</code> <code class="n">theta</code>&#13;
<code class="k">assert</code> <code class="mf">19.9</code> <code class="o">&lt;</code> <code class="n">slope</code> <code class="o">&lt;</code> <code class="mf">20.1</code><code class="p">,</code>   <code class="s2">"slope should be about 20"</code>&#13;
<code class="k">assert</code> <code class="mf">4.9</code> <code class="o">&lt;</code> <code class="n">intercept</code> <code class="o">&lt;</code> <code class="mf">5.1</code><code class="p">,</code> <code class="s2">"intercept should be about 5"</code></pre>&#13;
&#13;
<p>Another<a data-primary="stochastic gradient descent" data-type="indexterm" id="idm45635751666904"/> variation is <em>stochastic gradient descent</em>, in which you take gradient&#13;
steps based on one training example at a time:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="n">theta</code> <code class="o">=</code> <code class="p">[</code><code class="n">random</code><code class="o">.</code><code class="n">uniform</code><code class="p">(</code><code class="o">-</code><code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">),</code> <code class="n">random</code><code class="o">.</code><code class="n">uniform</code><code class="p">(</code><code class="o">-</code><code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">)]</code>&#13;
&#13;
<code class="k">for</code> <code class="n">epoch</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">100</code><code class="p">):</code>&#13;
    <code class="k">for</code> <code class="n">x</code><code class="p">,</code> <code class="n">y</code> <code class="ow">in</code> <code class="n">inputs</code><code class="p">:</code>&#13;
        <code class="n">grad</code> <code class="o">=</code> <code class="n">linear_gradient</code><code class="p">(</code><code class="n">x</code><code class="p">,</code> <code class="n">y</code><code class="p">,</code> <code class="n">theta</code><code class="p">)</code>&#13;
        <code class="n">theta</code> <code class="o">=</code> <code class="n">gradient_step</code><code class="p">(</code><code class="n">theta</code><code class="p">,</code> <code class="n">grad</code><code class="p">,</code> <code class="o">-</code><code class="n">learning_rate</code><code class="p">)</code>&#13;
    <code class="k">print</code><code class="p">(</code><code class="n">epoch</code><code class="p">,</code> <code class="n">theta</code><code class="p">)</code>&#13;
&#13;
<code class="n">slope</code><code class="p">,</code> <code class="n">intercept</code> <code class="o">=</code> <code class="n">theta</code>&#13;
<code class="k">assert</code> <code class="mf">19.9</code> <code class="o">&lt;</code> <code class="n">slope</code> <code class="o">&lt;</code> <code class="mf">20.1</code><code class="p">,</code>   <code class="s2">"slope should be about 20"</code>&#13;
<code class="k">assert</code> <code class="mf">4.9</code> <code class="o">&lt;</code> <code class="n">intercept</code> <code class="o">&lt;</code> <code class="mf">5.1</code><code class="p">,</code> <code class="s2">"intercept should be about 5"</code></pre>&#13;
&#13;
<p>On this problem, stochastic gradient descent finds the optimal parameters in a much smaller&#13;
number of epochs. But there are always tradeoffs. Basing gradient steps on small minibatches (or on single data points) allows you to take more of them,&#13;
but the gradient for a single point might lie in a very&#13;
different direction from the gradient for the dataset as a whole.</p>&#13;
&#13;
<p>In addition, if we weren’t doing our linear algebra from scratch,&#13;
there would be performance gains from “vectorizing” our computations&#13;
across batches rather than computing the gradient one point at a time.</p>&#13;
&#13;
<p>Throughout the book, we’ll play around to find optimal batch sizes and step sizes.</p>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>The terminology for the various flavors of gradient descent is not uniform.&#13;
The “compute the gradient for the whole dataset” approach is often<a data-primary="batch gradient descent" data-type="indexterm" id="idm45635751384104"/> called&#13;
<em>batch gradient descent</em>, and some people say <em>stochastic gradient descent</em> when referring to the minibatch version (of which the one-point-at-a-time version is a special case).</p>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="For Further Exploration" data-type="sect1"><div class="sect1" id="idm45635751938232">&#13;
<h1>For Further Exploration</h1>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Keep reading! We’ll be using gradient descent to solve problems throughout the rest of the book.</p>&#13;
</li>&#13;
<li>&#13;
<p>At<a data-primary="gradient descent" data-secondary="resources for learning" data-type="indexterm" id="idm45635751379256"/> this point, you’re undoubtedly sick of me recommending that you read textbooks.  If it’s any consolation, <a href="https://scholarworks.gvsu.edu/books/10/"><em>Active Calculus 1.0</em></a>, by Matthew Boelkins, David Austin, and Steven Schlicker (Grand Valley State University Libraries), seems nicer than the calculus textbooks I learned from.</p>&#13;
</li>&#13;
<li>&#13;
<p>Sebastian Ruder has an <a href="http://ruder.io/optimizing-gradient-descent/index.html">epic blog post</a> comparing gradient descent and its many variants.</p>&#13;
</li>&#13;
</ul>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section></body></html>