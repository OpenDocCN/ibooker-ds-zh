- en: Chapter 5\. Linear Regression
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第5章 线性回归
- en: One of the most practical techniques in data analysis is fitting a line through
    observed data points to show a relationship between two or more variables. A *regression*
    attempts to fit a function to observed data to make predictions on new data. A
    *linear regression* fits a straight line to observed data, attempting to demonstrate
    a linear relationship between variables and make predictions on new data yet to
    be observed.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 数据分析中最实用的技术之一是通过观察数据点拟合一条直线，以展示两个或更多变量之间的关系。*回归*试图将一个函数拟合到观察数据中，以对新数据进行预测。*线性回归*将一条直线拟合到观察数据中，试图展示变量之间的线性关系，并对尚未观察到的新数据进行预测。
- en: It might make more sense to see a picture rather than read a description of
    linear regression. There is an example of a linear regression in [Figure 5-1](#kVAPNnFMvc).
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 看一张线性回归的图片可能比阅读描述更有意义。在[图5-1](#kVAPNnFMvc)中有一个线性回归的例子。
- en: Linear regression is a workhorse of data science and statistics and not only
    applies concepts we learned in previous chapters but sets up new foundations for
    later topics like neural networks ([Chapter 7](ch07.xhtml#ch07)) and logistic
    regression ([Chapter 6](ch06.xhtml#ch06)). This relatively simple technique has
    been around for more than two hundred years and contemporarily is branded as a
    form of machine learning.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归是数据科学和统计学的中流砥柱，不仅应用了我们在前几章学到的概念，还为后续主题如神经网络（[第7章](ch07.xhtml#ch07)）和逻辑回归（[第6章](ch06.xhtml#ch06)）奠定了新的基础。这种相对简单的技术已经存在了两百多年，当代被称为一种机器学习形式。
- en: Machine learning practitioners often take a different approach to validation,
    starting with a train-test split of the data. Statisticians are more likely to
    use metrics like prediction intervals and correlation for statistical significance.
    We will cover both schools of thought so readers can bridge the ever-widening
    gap between the two disciplines, and thus find themselves best equipped to wear
    both hats.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习从业者通常采用不同的验证方法，从数据的训练-测试分割开始。统计学家更有可能使用像预测区间和相关性这样的指标来进行统计显著性分析。我们将涵盖这两种思维方式，以便读者能够弥合这两个学科之间日益扩大的鸿沟，从而最好地装备自己。
- en: '![emds 0501](Images/emds_0501.png)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
  zh: '![emds 0501](Images/emds_0501.png)'
- en: Figure 5-1\. Example of a linear regression, which fits a line to observed data
  id: totrans-6
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-1 线性回归的示例，将一条直线拟合到观察数据中
- en: A Basic Linear Regression
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一个基本的线性回归
- en: I want to study the relationship between the age of a dog and the number of
    veterinary visits it had. In a fabricated sample we have 10 random dogs. I am
    a fan of understanding complex techniques with simple datasets (real or otherwise),
    so we understand the strengths and limitations of the technique without complex
    data muddying the water. Let’s plot this dataset as shown in [Figure 5-2](#UiqIepMEBG).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我想研究狗的年龄与它看兽医的次数之间的关系。在一个虚构的样本中，我们有10只随机的狗。我喜欢用简单的数据集（真实或其他）来理解复杂的技术，这样我们就能了解技术的优势和局限性，而不会被复杂的数据搞混。让我们将这个数据集绘制成[图5-2](#UiqIepMEBG)所示。
- en: '![emds 0502](Images/emds_0502.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![emds 0502](Images/emds_0502.png)'
- en: Figure 5-2\. Plotting a sample of 10 dogs with their age and number of vet visits
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-2 绘制了10只狗的样本，显示它们的年龄和看兽医的次数
- en: We can clearly see there is a *linear correlation* here, meaning when one of
    these variables increases/decreases, the other increases/decreases in a roughly
    proportional amount. We could draw a line through these points to show a correlation
    like this in [Figure 5-3](#KsuHHebEQo).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以清楚地看到这里存在着*线性相关性*，意味着当这些变量中的一个增加/减少时，另一个也以大致相同的比例增加/减少。我们可以在[图5-3](#KsuHHebEQo)中画一条线来展示这样的相关性。
- en: '![emds 0503](Images/emds_0503.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![emds 0503](Images/emds_0503.png)'
- en: Figure 5-3\. Fitting a line through our data
  id: totrans-13
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-3 拟合我们数据的一条线
- en: I will show how to calculate this fitted line later in this chapter. We will
    also explore how to calculate the quality of this fitted line. For now, let’s
    focus on the benefits of performing a linear regression. It allows us to make
    predictions on data we have not seen before. I do not have a dog in my sample
    that is 8.5 years old, but I can look at this line and estimate the dog will have
    21 veterinary visits in its life. I just look at the line where *x* = 8.5 and
    I see that *y* = 21.218 as shown in [Figure 5-4](#pWiNSOLfvI). Another benefit
    is we can analyze variables for possible relationships and hypothesize that correlated
    variables are causal to one another.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我将在本章后面展示如何计算这条拟合线。我们还将探讨如何计算这条拟合线的质量。现在，让我们专注于执行线性回归的好处。它使我们能够对我们以前没有见过的数据进行预测。我的样本中没有一只8.5岁的狗，但我可以看着这条线估计这只狗一生中会有21次兽医就诊。我只需看看当*x*
    = 8.5时，*y* = 21.218，如[图5-4](#pWiNSOLfvI)所示。另一个好处是我们可以分析可能存在关系的变量，并假设相关的变量之间是因果关系。
- en: Now what are the downsides of a linear regression? I cannot expect that every
    outcome is going to fall *exactly* on that line. After all, real-world data is
    noisy and never perfect and will not follow a straight line. It may not remotely
    follow a straight line at all! There is going to be error around that line, where
    the point will fall above or below the line. We will cover this mathematically
    when we talk about p-values, statistical significance, and prediction intervals,
    which describes how reliable our linear regression is. Another catch is we should
    not use the linear regression to make predictions outside the range of data we
    have, meaning we should not make predictions where *x* < 0 and *x* > 10 because
    we do not have data outside those values.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 现在线性回归的缺点是什么？我不能期望每个结果都会*完全*落在那条线上。毕竟，现实世界的数据是嘈杂的，从不完美，也不会遵循一条直线。它可能根本不会遵循一条直线！在那条线周围会有误差，点会在线的上方或下方。当我们谈论p值、统计显著性和预测区间时，我们将在数学上涵盖这一点，这些内容描述了我们的线性回归有多可靠。另一个问题是我们不应该使用线性回归来预测超出我们拥有数据范围之外的情况，这意味着我们不应该在*x*
    < 0和*x* > 10的情况下进行预测，因为我们没有这些值之外的数据。
- en: '![emds 0504](Images/emds_0504.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![emds 0504](Images/emds_0504.png)'
- en: Figure 5-4\. Making a prediction using a linear regression, seeing that an 8.5-year-old
    dog is predicted to have about 21.2 vet visits
  id: totrans-17
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-4。使用线性回归进行预测，看到一个8.5岁的狗预测将有约21.2次兽医就诊
- en: Don’t Forget Sampling Bias!
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 不要忘记抽样偏差！
- en: We should question this data and how it was sampled to detect bias. Was this
    at a single veterinary clinic? Multiple random clinics? Is there self-selection
    bias by using veterinary data, only polling dogs that visit the vet? If the dogs
    were sampled in the same geography, can that sway the data? Perhaps dogs in hot
    desert climates go to vets more for heat exhaustion and snake bites, and this
    would inflate our veterinary visits in our sample.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该质疑这些数据以及它们是如何抽样的，以便检测偏见。这是在单个兽医诊所吗？多个随机诊所？通过使用兽医数据是否存在自我选择偏见，只调查拜访兽医的狗？如果这些狗是在相同的地理位置抽样的，那会不会影响数据？也许在炎热的沙漠气候中的狗更容易因中暑和被蛇咬而去看兽医，这会使我们样本中的兽医就诊次数增加。
- en: As discussed in [Chapter 3](ch03.xhtml#ch03), it has become fashionable to make
    data an oracle for truth. However data is simply a sample from a population, and
    we need to practice discernment on how well represented our sample is. Be just
    as interested (if not more) in where the data comes from and not just what the
    data says.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 正如在[第3章](ch03.xhtml#ch03)中讨论的那样，将数据视为真理的神谕已经变得时髦。然而，数据只是从一个总体中抽取的样本，我们需要对我们的样本有多好地代表性进行判断。对数据的来源同样感兴趣（如果不是更多），而不仅仅是数据所说的内容。
- en: Basic Linear Regression with SciPy
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用SciPy进行基本线性回归
- en: We have a lot to learn regarding linear regression in this chapter, but let’s
    start out with some code to execute what we know so far.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们有很多关于线性回归要学习的内容，但让我们从一些代码开始执行我们已经了解的内容。
- en: There are plenty of platforms to perform a linear regression, from Excel to
    Python and R. But we will stick with Python in this book, starting with scikit-learn
    to do the work for us. I will show how to build a linear regression “from scratch”
    later in this chapter so we grasp important concepts like gradient descent and
    least squares.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 有很多平台可以执行线性回归，从Excel到Python和R。但在本书中，我们将坚持使用Python，从scikit-learn开始为我们完成工作。我将在本章后面展示如何“从头开始”构建线性回归，以便我们掌握像梯度下降和最小二乘这样的重要概念。
- en: '[Example 5-1](#bhaTvtcODA) is how we use scikit-learn to perform a basic, unvalidated
    linear regression on the sample of 10 dogs. We pull in [this data using Pandas](https://oreil.ly/xCvwR),
    convert it into NumPy arrays, perform linear regression using scikit-learn, and
    use Plotly to display it in a chart.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例5-1](#bhaTvtcODA)是我们如何使用scikit-learn对这10只狗进行基本的、未经验证的线性回归的样本。我们使用Pandas获取[这些数据](https://oreil.ly/xCvwR)，将其转换为NumPy数组，使用scikit-learn进行线性回归，并使用Plotly在图表中显示它。'
- en: Example 5-1\. Using scikit-learn to do a linear regression
  id: totrans-25
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例5-1\. 使用scikit-learn进行线性回归
- en: '[PRE0]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: First we import the data from [this CSV on GitHub](https://bit.ly/3cIH97A).
    We separate the two columns into *X* and *Y* datasets using Pandas. We then `fit()`
    the `LinearRegression` model to the input *X* data and the output *Y* data. We
    can then get the *m* and *b* coefficients that describe our fitted linear function.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们从[GitHub上的这个CSV](https://bit.ly/3cIH97A)导入数据。我们使用Pandas将两列分离为*X*和*Y*数据集。然后，我们将`LinearRegression`模型拟合到输入的*X*数据和输出的*Y*数据。然后我们可以得到描述我们拟合线性函数的*m*和*b*系数。
- en: In the plot, sure enough you will get a fitted line running through these points
    shown in [Figure 5-5](#psauNoJchU).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在图中，您将确实看到一条拟合线穿过这些点，如[图5-5](#psauNoJchU)所示。
- en: '![emds 0505](Images/emds_0505.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![emds 0505](Images/emds_0505.png)'
- en: Figure 5-5\. SciPy will fit a regression line to your data
  id: totrans-30
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-5\. SciPy将拟合一条回归线到您的数据
- en: What decides the best fit line to these points? Let’s discuss that next.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 是什么决定了最佳拟合线到这些点？让我们接下来讨论这个问题。
- en: Residuals and Squared Errors
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 残差和平方误差
- en: 'How do statistics tools like scikit-learn come up with a line that fits to
    these points? It comes down to two questions that are fundamental to machine learning
    training:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 统计工具如scikit-learn如何得出适合这些点的线？这归结为机器学习训练中的两个基本问题：
- en: What defines a “best fit”?
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么定义了“最佳拟合”？
- en: How do we get to that “best fit”?
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们如何得到那个“最佳拟合”呢？
- en: 'The first question has a pretty established answer: we minimize the squares,
    or more specifically the sum of the squared residuals. Let’s break that down.
    Draw any line through the points. The *residual* is the numeric difference between
    the line and the points, as shown in [Figure 5-6](#TIOfQJflet).'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个问题有一个相当确定的答案：我们最小化平方，或更具体地说是平方残差的和。让我们来详细解释一下。画出任何一条穿过点的线。*残差*是线和点之间的数值差异，如[图5-6](#TIOfQJflet)所示。
- en: '![emds 0506](Images/emds_0506.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![emds 0506](Images/emds_0506.png)'
- en: Figure 5-6\. The residuals are the differences between the line and the points
  id: totrans-38
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-6\. 残差是线和点之间的差异
- en: Points above the line will have a positive residual, and points below the line
    will have a negative residual. In other words, it is the subtracted difference
    between the predicted y-values (derived from the line) and the actual y-values
    (which came from the data). Another name for residuals are *errors*, because they
    reflect how wrong our line is in predicting the data.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 线上方的点将具有正残差，而线下方的点将具有负残差。换句话说，它是预测y值（来自线）与实际y值（来自数据）之间的减去差异。残差的另一个名称是*误差*，因为它反映了我们的线在预测数据方面有多么错误。
- en: Let’s calculate these differences between these 10 points and the line *y* =
    1.93939*x* + 4.73333 in [Example 5-2](#ENfFANvoCk) and the residuals for each
    point in [Example 5-3](#PLhHARzaLr).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们计算这10个点与线*y* = 1.93939*x* + 4.73333之间的差异，以及[示例5-2](#ENfFANvoCk)中每个点的残差和[示例5-3](#PLhHARzaLr)。
- en: Example 5-2\. Calculating the residuals for a given line and data
  id: totrans-41
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 5-2\. 计算给定线和数据的残差
- en: '[PRE1]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Example 5-3\. The residuals for each point
  id: totrans-43
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 5-3\. 每个点的残差
- en: '[PRE2]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: If we are fitting a straight line through our 10 data points, we likely want
    to minimize these residuals in total so there is the least gap possible between
    the line and points. But how do we measure the “total”? The best approach is to
    take the *sum of squares*, which simply squares each residual, or multiplies each
    residual by itself, and sums them. We take each actual y-value and subtract from
    it the predicted y-value taken from the line, then square and sum all those differences.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们要通过我们的10个数据点拟合一条直线，我们很可能希望尽可能地减小这些残差，使线和点之间的间隙尽可能小。但是我们如何衡量“总体”呢？最好的方法是采用*平方和*，简单地对每个残差进行平方，或者将每个残差相乘，然后将它们求和。我们取每个实际的y值，并从中减去从线上取得的预测y值，然后对所有这些差异进行平方和。
- en: A visual way to think of it is shown in [Figure 5-7](#ODRNFmUtdw), where we
    overlay a square on each residual and each side is the length of the residual.
    We sum the area of all these squares, and later we will learn how to find the
    minimum sum we can achieve by identifying the best *m* and *b*.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 一个直观的思考方式如[图 5-7](#ODRNFmUtdw)所示，我们在每个残差上叠加一个正方形，每条边的长度都是残差。我们将所有这些正方形的面积相加，稍后我们将学习如何通过确定最佳*m*和*b*来找到我们可以实现的最小和。
- en: '![emds 0507](Images/emds_0507.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![emds 0507](Images/emds_0507.png)'
- en: Figure 5-7\. Visualizing the sum of squares, which would be the sum of all areas
    where each square has a side length equal to the residual
  id: totrans-48
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-7\. 可视化平方和，即所有正方形的面积之和，其中每个正方形的边长等于残差
- en: Let’s modify our code in [Example 5-4](#eJAucETrRq) to find the sum of squares.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们修改我们在[示例 5-4](#eJAucETrRq)中的代码来找到平方和。
- en: Example 5-4\. Calculating the sum of squares for a given line and data
  id: totrans-50
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 5-4\. 计算给定直线和数据的平方和
- en: '[PRE3]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Next question: how do we find the *m* and *b* values that will produce the
    minimum sum of squares, without using a library like scikit-learn? Let’s look
    at that next.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个问题是：如何找到能产生最小平方和的*m*和*b*值，而不使用像scikit-learn这样的库？让我们接着看。
- en: Finding the Best Fit Line
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 寻找最佳拟合直线
- en: 'We now have a way to measure the quality of a given line against the data points:
    the sum of squares. The lower we can make that number, the better the fit. Now
    how do we find the right *m* and *b* values that create the *least* sum of squares?'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有一种方法来衡量给定直线与数据点的质量：平方和。我们能够使这个数字越低，拟合就越好。那么如何找到能产生*最小*平方和的正确*m*和*b*值呢？
- en: 'There are a couple of search algorithms we can employ, which try to find the
    right set of values to solve a given problem. You can try a *brute force* approach,
    generating random *m* and *b* values millions of times and choosing the ones that
    produce the least sum of squares. This will not work well because it will take
    an endless amount of time to find even a decent approximation. We will need something
    a little more guided. I will curate five techniques you can use: closed form,
    matrix inversion, matrix decomposition, gradient descent, and stochastic gradient
    descent. There are other search algorithms like hill climbing that could be used
    (and are covered in [Appendix A](app01.xhtml#appendix)), but we will stick with
    what’s common.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以采用几种搜索算法，试图找到解决给定问题的正确值集。你可以尝试*蛮力*方法，随机生成*m*和*b*值数百万次，并选择产生最小平方和的值。这种方法效果不佳，因为即使找到一个体面的近似值也需要无尽的时间。我们需要一些更有指导性的东西。我将为你整理五种技术：闭式方程、矩阵求逆、矩阵分解、梯度下降和随机梯度下降。还有其他搜索算法，比如爬山算法（在[附录A](app01.xhtml#appendix)中有介绍），但我们将坚持使用常见的方法。
- en: Closed Form Equation
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 闭式方程
- en: Some readers may ask if there is a formula (called a *closed form equation*)
    to fit a linear regression by exact calculation. The answer is yes, but only for
    a simple linear regression with one input variable. This luxury does not exist
    for many machine learning problems with several input variables and a large amount
    of data. We can use linear algebra techniques to scale up, and we will talk about
    this shortly. We will also take the opportunity to learn about search algorithms
    like stochastic gradient descent.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 一些读者可能会问是否有一个公式（称为*闭式方程*）通过精确计算来拟合线性回归。答案是肯定的，但仅适用于只有一个输入变量的简单线性回归。对于具有多个输入变量和大量数据的许多机器学习问题，这种奢侈是不存在的。我们可以使用线性代数技术进行扩展，我们很快将讨论这一点。我们还将借此机会学习诸如随机梯度下降之类的搜索算法。
- en: For a simple linear regression with only one input and one output variable,
    here are the closed form equations to calculate *m* and *b*. [Example 5-5](#SMKENihWel)
    shows how you can do these calculations in Python.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 对于只有一个输入和一个输出变量的简单线性回归，以下是计算*m*和*b*的闭式方程。[示例 5-5](#SMKENihWel)展示了如何在Python中进行这些计算。
- en: <math display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mi>m</mi>
    <mo>=</mo> <mfrac><mrow><mi>n</mi><mo>∑</mo><mrow><mi>x</mi><mi>y</mi></mrow><mo>-</mo><mo>∑</mo><mi>x</mi><mo>∑</mo><mi>y</mi></mrow>
    <mrow><mi>n</mi><mo>∑</mo><msup><mi>x</mi> <mn>2</mn></msup> <mo>-</mo><msup><mrow><mo>(</mo><mo>∑</mo><mi>x</mi><mo>)</mo></mrow>
    <mn>2</mn></msup></mrow></mfrac></mrow></mtd></mtr></mtable></math> <math display="block"><mtable
    displaystyle="true"><mtr><mtd columnalign="right"><mrow><mi>b</mi> <mo>=</mo>
    <mfrac><mrow><mo>∑</mo><mi>y</mi></mrow> <mi>n</mi></mfrac> <mo>-</mo> <mi>m</mi>
    <mfrac><mrow><mo>∑</mo><mi>x</mi></mrow> <mi>n</mi></mfrac></mrow></mtd></mtr></mtable></math>
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mi>m</mi>
    <mo>=</mo> <mfrac><mrow><mi>n</mi><mo>∑</mo><mrow><mi>x</mi><mi>y</mi></mrow><mo>-</mo><mo>∑</mo><mi>x</mi><mo>∑</mo><mi>y</mi></mrow>
    <mrow><mi>n</mi><mo>∑</mo><msup><mi>x</mi> <mn>2</mn></msup> <mo>-</mo><msup><mrow><mo>(</mo><mo>∑</mo><mi>x</mi><mo>)</mo></mrow>
    <mn>2</mn></msup></mrow></mfrac></mrow></mtd></mtr></mtable></math> <math display="block"><mtable
    displaystyle="true"><mtr><mtd columnalign="right"><mrow><mi>b</mi> <mo>=</mo>
    <mfrac><mrow><mo>∑</mo><mi>y</mi></mrow> <mi>n</mi></mfrac> <mo>-</mo> <mi>m</mi>
    <mfrac><mrow><mo>∑</mo><mi>x</mi></mrow> <mi>n</mi></mfrac></mrow></mtd></mtr></mtable></math>
- en: Example 5-5\. Calculating m and b for a simple linear regression
  id: totrans-60
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 5-5\. 计算简单线性回归的*m*和*b*
- en: '[PRE4]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: These equations to calculate *m* and *b* are derived from calculus, and we will
    do some calculus work with SymPy later in this chapter if you have the itch to
    discover where formulas come from. For now, you can plug in the number of data
    points *n* as well as iterate the x- and y-values to do the operations just described.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这些用于计算*m*和*b*的方程式是从微积分中推导出来的，如果您有兴趣发现公式的来源，我们稍后在本章中将使用SymPy进行一些微积分工作。目前，您可以插入数据点数*n*，并迭代x和y值来执行刚才描述的操作。
- en: Going forward, we will learn approaches that are more oriented to contemporary
    techniques that cope with larger amounts of data. Closed form equations tend not
    to scale well.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 今后，我们将学习更适用于处理大量数据的现代技术。闭式方程式往往不适用于大规模应用。
- en: Computational Complexity
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算复杂度
- en: 'The reason the closed form equations do not scale well with larger datasets
    is due to a computer science concept called *computational complexity*, which
    measures how long an algorithm takes as a problem size grows. This might be worth
    getting familiar with; here are two great YouTube videos on the topic:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 闭式方程式不适用于较大数据集的原因是由于计算机科学中称为*计算复杂度*的概念，它衡量算法在问题规模增长时所需的时间。这可能值得熟悉一下；以下是关于这个主题的两个很棒的YouTube视频：
- en: '[“P vs. NP and the Computational Complexity Zoo”](https://oreil.ly/TzQBl)'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“P vs. NP和计算复杂度动物园”](https://oreil.ly/TzQBl)'
- en: '[“What Is Big O Notation?”](https://oreil.ly/EjcSR)'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“大O符号是什么？”](https://oreil.ly/EjcSR)'
- en: Inverse Matrix Techniques
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 逆矩阵技术
- en: Going forward, I will sometimes alternate the coefficients *m* and *b* with
    different names, <math alttext="beta 1"><msub><mi>β</mi> <mn>1</mn></msub></math>
    and <math alttext="beta 0"><msub><mi>β</mi> <mn>0</mn></msub></math> , respectively.
    This is the convention you will see more often in the professional world, so it
    might be a good time to graduate.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 今后，我有时会用不同的名称交替使用系数*m*和*b*，分别为<math alttext="beta 1"><msub><mi>β</mi> <mn>1</mn></msub></math>和<math
    alttext="beta 0"><msub><mi>β</mi> <mn>0</mn></msub></math>，这是您在专业世界中经常看到的惯例，所以现在可能是一个毕业的好时机。
- en: While we dedicated an entire chapter to linear algebra in [Chapter 4](ch04.xhtml#ch04),
    applying it can be a bit overwhelming when you are new to math and data science.
    This is why most examples in this book will use plain Python or scikit-learn.
    However, I will sprinkle in linear algebra where it makes sense, just to show
    how linear algebra is useful. If you find this section overwhelming, feel free
    to move on to the rest of the chapter and come back later.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们在[第4章](ch04.xhtml#ch04)中专门致力于线性代数，但在您刚接触数学和数据科学时，应用它可能有点令人不知所措。这就是为什么本书中的大多数示例将使用纯Python或scikit-learn。但是，在合适的情况下，我会加入线性代数，以展示线性代数的实用性。如果您觉得这一部分令人不知所措，请随时继续阅读本章的其余部分，稍后再回来。
- en: 'We can use transposed and inverse matrices, which we covered in [Chapter 4](ch04.xhtml#ch04),
    to fit a linear regression. Next, we calculate a vector of coefficients <math
    alttext="b"><mi>b</mi></math> given a matrix of input variable values <math alttext="upper
    X"><mi>X</mi></math> and a vector of output variable values <math alttext="y"><mi>y</mi></math>
    . Without going down a rabbit hole of calculus and linear algebra proofs, here
    is the formula:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用转置和逆矩阵，我们在[第4章](ch04.xhtml#ch04)中介绍过，来拟合线性回归。接下来，我们根据输入变量值矩阵<math alttext="upper
    X"><mi>X</mi></math>和输出变量值向量<math alttext="y"><mi>y</mi></math>计算系数向量<math alttext="b"><mi>b</mi></math>。在不深入微积分和线性代数证明的兔子洞中，这是公式：
- en: <math alttext="b equals left-parenthesis upper X Superscript upper T Baseline
    dot upper X right-parenthesis Superscript negative 1 Baseline dot upper X Superscript
    upper T Baseline dot y" display="block"><mrow><mi>b</mi> <mo>=</mo> <msup><mrow><mo>(</mo><msup><mi>X</mi>
    <mi>T</mi></msup> <mo>·</mo><mi>X</mi><mo>)</mo></mrow> <mrow><mo>-</mo><mn>1</mn></mrow></msup>
    <mo>·</mo> <msup><mi>X</mi> <mi>T</mi></msup> <mo>·</mo> <mi>y</mi></mrow></math>
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="b equals left-parenthesis upper X Superscript upper T Baseline
    dot upper X right-parenthesis Superscript negative 1 Baseline dot upper X Superscript
    upper T Baseline dot y" display="block"><mrow><mi>b</mi> <mo>=</mo> <msup><mrow><mo>(</mo><msup><mi>X</mi>
    <mi>T</mi></msup> <mo>·</mo><mi>X</mi><mo>)</mo></mrow> <mrow><mo>-</mo><mn>1</mn></mrow></msup>
    <mo>·</mo> <msup><mi>X</mi> <mi>T</mi></msup> <mo>·</mo> <mi>y</mi></mrow></math>
- en: You will notice transposed and inverse operations are performed on the matrix
    <math alttext="upper X"><mi>X</mi></math> and combined with matrix multiplication.
    Here is how we perform this operation in NumPy in [Example 5-6](#sfoTRMoiPH) to
    get our coefficients *m* and *b*.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到在矩阵<math alttext="upper X"><mi>X</mi></math>上执行了转置和逆操作，并与矩阵乘法结合。这是我们在NumPy中执行此操作的方式，在[例子 5-6](#sfoTRMoiPH)中得到我们的系数*m*和*b*。
- en: Example 5-6\. Using inverse and transposed matrices to fit a linear regression
  id: totrans-74
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 例5-6\. 使用逆矩阵和转置矩阵拟合线性回归
- en: '[PRE5]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: It is not intuitive, but note we have to stack a “column” of 1s next to our
    *X* column. The reason is this will generate the intercept <math alttext="beta
    0"><msub><mi>β</mi> <mn>0</mn></msub></math> coefficient. Since this column is
    all 1s, it effectively generates the intercept and not just a slope <math alttext="beta
    1"><msub><mi>β</mi> <mn>1</mn></msub></math> .
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这并不直观，但请注意我们必须在*X*列旁边堆叠一个“列”为1的列。原因是这将生成截距<math alttext="beta 0"><msub><mi>β</mi>
    <mn>0</mn></msub></math>系数。由于这一列全为1，它实际上生成了截距而不仅仅是一个斜率<math alttext="beta 1"><msub><mi>β</mi>
    <mn>1</mn></msub></math>。
- en: 'When you have a lot of data with a lot of dimensions, computers can start to
    choke and produce unstable results. This is a use case for matrix decomposition,
    which we learned about in [Chapter 4](ch04.xhtml#ch04) on linear algebra. In this
    specific case, we take our matrix *X*, append an additional column of 1s to generate
    the intercept <math alttext="beta 0"><msub><mi>β</mi> <mn>0</mn></msub></math>
    just like before, and then decompose it into two component matrices *Q* and *R*:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 当你有大量数据和大量维度时，计算机可能开始崩溃并产生不稳定的结果。这是矩阵分解的一个用例，我们在线性代数的[第4章](ch04.xhtml#ch04)中学到了。在这种特定情况下，我们取我们的矩阵*X*，附加一个额外的列1来生成截距<math
    alttext="beta 0"><msub><mi>β</mi> <mn>0</mn></msub></math>就像以前一样，然后将其分解为两个组件矩阵*Q*和*R*：
- en: <math alttext="upper X equals upper Q dot upper R" display="block"><mrow><mi>X</mi>
    <mo>=</mo> <mi>Q</mi> <mo>·</mo> <mi>R</mi></mrow></math>
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper X equals upper Q dot upper R" display="block"><mrow><mi>X</mi>
    <mo>=</mo> <mi>Q</mi> <mo>·</mo> <mi>R</mi></mrow></math>
- en: 'Avoiding more calculus rabbit holes, here is how we use *Q* and *R* to find
    the beta coefficient values in the matrix form *b*:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 避免更多的微积分兔子洞，这里是我们如何使用*Q*和*R*来在矩阵形式*b*中找到beta系数值：
- en: <math alttext="b equals upper R Superscript negative 1 Baseline dot upper Q
    Superscript upper T Baseline dot y" display="block"><mrow><mi>b</mi> <mo>=</mo>
    <msup><mi>R</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup> <mo>·</mo> <msup><mi>Q</mi>
    <mi>T</mi></msup> <mo>·</mo> <mi>y</mi></mrow></math>
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="b equals upper R Superscript negative 1 Baseline dot upper Q
    Superscript upper T Baseline dot y" display="block"><mrow><mi>b</mi> <mo>=</mo>
    <msup><mi>R</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup> <mo>·</mo> <msup><mi>Q</mi>
    <mi>T</mi></msup> <mo>·</mo> <mi>y</mi></mrow></math>
- en: And [Example 5-7](#wFWnrEMADG) shows how we use the preceding *QR* decomposition
    formula in Python using NumPy to perform a linear regression.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 而[例子 5-7](#wFWnrEMADG)展示了我们如何在Python中使用NumPy使用前述*QR*分解公式执行线性回归。
- en: Example 5-7\. Using QR decomposition to perform a linear regression
  id: totrans-82
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 例5-7\. 使用QR分解执行线性回归
- en: '[PRE6]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Typically, *QR* decomposition is the method used by many scientific libraries
    for linear regression because it copes with large amounts of data more easily
    and is more stable. What do I mean by *stable*? [*Numerical stability*](https://oreil.ly/A4BWJ)
    is how well an algorithm keeps errors minimized, rather than amplifying errors
    in approximations. Remember that computers work only to so many decimal places
    and have to approximate, so it becomes important our algorithms do not deteriorate
    with compounding errors in those approximations.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，*QR*分解是许多科学库用于线性回归的方法，因为它更容易处理大量数据，并且更稳定。我所说的*稳定*是什么意思？[*数值稳定性*](https://oreil.ly/A4BWJ)是算法保持错误最小化的能力，而不是在近似中放大错误。请记住，计算机只能工作到某个小数位数，并且必须进行近似，因此我们的算法不应随着这些近似中的复合错误而恶化变得重要。
- en: Overwhelmed?
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 感到不知所措吗？
- en: If you find these linear algebra examples of linear regression overwhelming,
    do not worry! I just wanted to provide exposure to a practical use case for linear
    algebra. Going forward, we will focus on other techniques you can use.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你觉得这些线性代数示例中的线性回归让人不知所措，不要担心！我只是想提供一个线性代数实际用例的曝光。接下来，我们将专注于其他你可以使用的技术。
- en: Gradient Descent
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 梯度下降
- en: '*Gradient descent* is an optimization technique that uses derivatives and iterations
    to minimize/maximize a set of parameters against an objective. To learn about
    gradient descent, let’s do a quick thought experiment and then apply it on a simple
    example.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '*梯度下降*是一种优化技术，利用导数和迭代来最小化/最大化一组参数以达到目标。要了解梯度下降，让我们进行一个快速的思想实验，然后在一个简单的例子中应用它。'
- en: A Thought Experiment on Gradient Descent
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 关于梯度下降的思想实验
- en: Imagine you are in a mountain range at night and given a flashlight. You are
    trying to get to the lowest point of the mountain range. You can see the slope
    around you before you even take a step. You step in directions where the slope
    visibly goes downward. You take bigger steps for bigger slopes, and smaller steps
    for smaller slopes. Ultimately, you will find yourself at a low point where the
    slope is flat, a value of 0\. Sounds pretty good, right? This approach with the
    flashlight is known as *gradient descent*, where we step in directions where the
    slope goes downward.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，你在一个山脉中夜晚拿着手电筒。你试图到达山脉的最低点。在你迈出一步之前，你可以看到你周围的斜坡。你朝着斜坡明显向下的方向迈步。对于更大的斜坡，你迈出更大的步伐，对于更小的斜坡，你迈出更小的步伐。最终，你会发现自己在一个斜率为0的低点，一个值为0。听起来不错，对吧？这种使用手电筒的方法被称为*梯度下降*，我们朝着斜坡向下的方向迈步。
- en: 'In machine learning, we often think of all possible sum of square losses we
    will encounter with different parameters as a mountainous landscape. We want to
    minimize our loss, and we navigate the loss landscape to do it. To solve this
    problem, gradient descent has an attractive feature: the partial derivative is
    that flashlight, allowing us to see the slopes for every parameter (in this case
    *m* and *b*, or <math alttext="beta 0"><msub><mi>β</mi> <mn>0</mn></msub></math>
    and <math alttext="beta 1"><msub><mi>β</mi> <mn>1</mn></msub></math> ). We step
    in directions for *m* and *b* where the slope goes downward. We take bigger steps
    for bigger slopes and smaller steps for smaller slopes. We can simply calculate
    the length of this step by taking a fraction of the slope. This fraction is known
    as our *learning rate*. The higher the learning rate, the faster it will run at
    the cost of accuracy. But the lower the learning rate, the longer it will take
    to train and require more iterations.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，我们经常将我们将遇到的所有可能的平方损失总和视为多山的地形。我们想要最小化我们的损失，并且我们通过导航损失地形来实现这一点。为了解决这个问题，梯度下降有一个吸引人的特点：偏导数就像是那盏手电筒，让我们能够看到每个参数（在这种情况下是*m*和*b*，或者<math
    alttext="beta 0"><msub><mi>β</mi> <mn>0</mn></msub></math>和<math alttext="beta
    1"><msub><mi>β</mi> <mn>1</mn></msub></math>）的斜率。我们朝着*m*和*b*的斜率向下的方向迈步。对于更大的斜率，我们迈出更大的步伐，对于更小的斜率，我们迈出更小的步伐。我们可以通过取斜率的一部分来简单地计算这一步的长度。这一部分被称为我们的*学习率*。学习率越高，它运行得越快，但精度会受到影响。但学习率越低，训练所需的时间就越长，需要更多的迭代。
- en: Deciding a learning rate is like choosing between an ant, a human, or a giant
    to step down the slope. An ant (small learning rate) will take tiny steps and
    take an unacceptably long time to get to the bottom but will do so precisely.
    A giant (large learning rate) may keep stepping over the minimum to the point
    he may never reach it no matter how many steps he takes. The human (moderate learning
    rate) probably has most balanced step size, having the right trade between speed
    and accuracy in arriving at the minimum.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 决定学习率就像在选择蚂蚁、人类或巨人来踏下斜坡。蚂蚁（小学习率）会迈出微小的步伐，花费不可接受的长时间才能到达底部，但会准确无误地到达。巨人（大学习率）可能会一直跨过最小值，以至于无论走多少步都可能永远无法到达。人类（适度学习率）可能具有最平衡的步幅，在速度和准确性之间找到正确的平衡，以到达最小值。
- en: Let’s Walk Before We Run
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 先学会走再学会跑
- en: For the function <math alttext="f left-parenthesis x right-parenthesis equals
    left-parenthesis x minus 3 right-parenthesis squared plus 4"><mrow><mi>f</mi>
    <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>=</mo> <msup><mrow><mo>(</mo><mi>x</mi><mo>-</mo><mn>3</mn><mo>)</mo></mrow>
    <mn>2</mn></msup> <mo>+</mo> <mn>4</mn></mrow></math> , let’s find the x-value
    that produces the lowest point of that function. While we could solve this algebraically,
    let’s use gradient descent to do it.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 对于函数<math alttext="f left-parenthesis x right-parenthesis equals left-parenthesis
    x minus 3 right-parenthesis squared plus 4"><mrow><mi>f</mi> <mrow><mo>(</mo>
    <mi>x</mi> <mo>)</mo></mrow> <mo>=</mo> <msup><mrow><mo>(</mo><mi>x</mi><mo>-</mo><mn>3</mn><mo>)</mo></mrow>
    <mn>2</mn></msup> <mo>+</mo> <mn>4</mn></mrow></math>，让我们找到产生该函数最低点的x值。虽然我们可以通过代数方法解决这个问题，但让我们使用梯度下降来做。
- en: Here is visually what we are trying to do. As shown in [Figure 5-8](#PbWEqDgfnq),
    we want to “step” *x* toward the minimum where the slope is 0.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们试图做的可视化效果。如[图 5-8](#PbWEqDgfnq)所示，我们希望“步进”*x*朝向斜率为0的最小值。
- en: '![emds 0508](Images/emds_0508.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![emds 0508](Images/emds_0508.png)'
- en: Figure 5-8\. Stepping toward the local minimum where the slope approaches 0
  id: totrans-97
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-8\. 朝向斜率接近0的局部最小值迈进
- en: In [Example 5-8](#dOSpRhPBkR), the function `f(x)` and its derivative with respect
    to *x* is `dx_f(x)`. Recall we covered in [Chapter 1](ch01.xhtml#ch01) how to
    use SymPy to calculate derivatives. After finding the derivative, we then proceed
    to perform gradient descent.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在[示例 5-8](#dOSpRhPBkR)中，函数`f(x)`及其对*x*的导数为`dx_f(x)`。回想一下，我们在[第1章](ch01.xhtml#ch01)中讨论了如何使用SymPy计算导数。找到导数后，我们继续执行梯度下降。
- en: Example 5-8\. Using gradient descent to find the minimum of a parabola
  id: totrans-99
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 5-8\. 使用梯度下降找到抛物线的最小值
- en: '[PRE7]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: If we graph the function (as shown in [Figure 5-8](#PbWEqDgfnq)), we should
    see the lowest point of the function is clearly where *x* = 3, and the preceding
    code should get very close to that. The learning rate is used to take a fraction
    of the slope and subtract it from the x-value on each iteration. Bigger slopes
    will result in bigger steps, and smaller slopes will result in smaller steps.
    After enough iterations, *x* will end up at the lowest point of the function (or
    close enough to it) where the slope is 0.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们绘制函数（如[图 5-8](#PbWEqDgfnq)所示），我们应该看到函数的最低点明显在*x* = 3处，前面的代码应该非常接近这个点。学习率用于在每次迭代中取斜率的一部分并从x值中减去它。较大的斜率将导致较大的步长，而较小的斜率将导致较小的步长。经过足够的迭代，*x*将最终到达函数的最低点（或足够接近），其中斜率为0。
- en: Gradient Descent and Linear Regression
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 梯度下降和线性回归
- en: 'You now might be wondering how we use this for linear regression. Well, it’s
    the same idea except our “variables” are *m* and *b* (or <math alttext="beta 0"><msub><mi>β</mi>
    <mn>0</mn></msub></math> and <math alttext="beta 1"><msub><mi>β</mi> <mn>1</mn></msub></math>
    ) rather than *x*. Here’s why: in a simple linear regression we already know the
    x- and y-values because those are provided as the training data. The “variables”
    we need to solve are actually the parameters *m* and *b*, so we can find the best
    fit line that will then accept an *x* variable to predict a new y-value.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你可能想知道我们如何将其用于线性回归。嗯，这个想法是一样的，只是我们的“变量”是*m*和*b*（或<math alttext="beta 0"><msub><mi>β</mi>
    <mn>0</mn></msub></math>和<math alttext="beta 1"><msub><mi>β</mi> <mn>1</mn></msub></math>）而不是*x*。原因在于：在简单线性回归中，我们已经知道x和y值，因为这些值作为训练数据提供。我们需要解决的“变量”实际上是参数*m*和*b*，因此我们可以找到最佳拟合线，然后接受一个*x*变量来预测一个新的y值。
- en: How do we calculate the slopes for *m* and *b*? We need the partial derivatives
    for each of these. What function are we taking the derivative of? Remember we
    are trying to minimize loss and that will be the sum of squares. So we need to
    find the derivatives of our sum of squares function with respect to *m* and *b*.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何计算*m*和*b*的斜率？我们需要这两者的偏导数。我们要对哪个函数求导？记住我们试图最小化损失，这将是平方和。因此，我们需要找到我们的平方和函数对*m*和*b*的导数。
- en: 'I implement these two partial derivatives for *m* and *b* as shown in [Example 5-9](#SnshvwHMPb).
    We will learn how to do this shortly in SymPy. I then perform gradient descent
    to find *m* and *b*: 100,000 iterations with a learning rate of .001 will be sufficient.
    Note that the smaller you make that learning rate, the slower it will be and the
    more iterations you will need. But if you make it too high, it will run fast but
    have a poor approximation. When someone says a machine learning algorithm is “learning”
    or “training,” it really is just fitting a regression like this.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我像[示例 5-9](#SnshvwHMPb)中所示实现了这两个*m*和*b*的偏导数。我们很快将学习如何在SymPy中执行此操作。然后我执行梯度下降来找到*m*和*b*：100,000次迭代，学习率为0.001就足够了。请注意，您将学习率设得越小，速度就越慢，需要的迭代次数就越多。但如果设得太高，它将运行得很快，但近似度较差。当有人说机器学习算法正在“学习”或“训练”时，实际上就是在拟合这样一个回归。
- en: Example 5-9\. Performing gradient descent for a linear regression
  id: totrans-106
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 5-9\. 执行线性回归的梯度下降
- en: '[PRE8]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Well, not bad! That approximation got close to our closed form equation solution.
    But what’s the catch? Just because we found the “best fit line” by minimizing
    sum of squares, that does not mean our linear regression is any good. Does minimizing
    the sum of squares guarantee a great model to make predictions? Not exactly. Now
    that I showed you how to fit a linear regression, let’s take a step back, revisit
    the big picture, and determine whether a given linear regression is the right
    way to make predictions in the first place. But before we do that, here’s one
    more detour showing the SymPy solution.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯，不错！那个近似值接近我们的闭式方程解。但有什么问题吗？仅仅因为我们通过最小化平方和找到了“最佳拟合直线”，这并不意味着我们的线性回归就很好。最小化平方和是否保证了一个很好的模型来进行预测？并不完全是这样。现在我向你展示了如何拟合线性回归，让我们退一步，重新审视全局，确定给定的线性回归是否是首选的预测方式。但在我们这样做之前，这里有一个展示
    SymPy 解决方案的更多绕路。
- en: Gradient Descent for Linear Regression Using SymPy
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 SymPy 进行线性回归的梯度下降
- en: If you want the SymPy code that came up with these two derivatives for the sum
    of squares function, for *m* and *b* respectively, here is the code in [Example 5-10](#mliLfkeFPI).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要得到这两个关于平方和函数的导数的 SymPy 代码，分别为 *m* 和 *b*，这里是 [示例 5-10](#mliLfkeFPI) 中的代码。
- en: Example 5-10\. Calculating partial derivatives for m and b
  id: totrans-111
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 5-10\. 计算 *m* 和 *b* 的偏导数
- en: '[PRE9]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: You will see the two derivatives for *m* and *b*, respectively, printed. Note
    the `Sum()` function will iterate and add items together (in this case all the
    data points), and we treat *x* and *y* as functions that look up a value for a
    given point at index *i*.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 你会看到分别为 *m* 和 *b* 的两个导数被打印出来。请注意 `Sum()` 函数将迭代并将项相加（在这种情况下是所有数据点），我们将 *x* 和
    *y* 视为查找给定索引 *i* 处值的函数。
- en: 'In mathematical notation, where *e*(*x*) represents the sum of squares loss
    function, here are the partial derivatives for *m* and *b*:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在数学符号中，其中 *e*(*x*) 代表平方和损失函数，这里是 *m* 和 *b* 的偏导数：
- en: <math display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mi>e</mi>
    <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>=</mo> <munderover><mo>∑</mo>
    <mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow> <mi>n</mi></munderover> <mrow><mo>(</mo>
    <mrow><mo>(</mo> <mi>m</mi> <msub><mi>x</mi> <mi>i</mi></msub> <mo>+</mo> <mi>b</mi>
    <mo>)</mo></mrow> <mo>-</mo> <msub><mi>y</mi> <mi>i</mi></msub></mrow> <msup><mrow><mo>)</mo></mrow>
    <mn>2</mn></msup></mrow></mtd></mtr></mtable></math> <math display="block"><mtable
    displaystyle="true"><mtr><mtd columnalign="right"><mrow><mfrac><mi>d</mi> <mrow><mi>d</mi><mi>m</mi></mrow></mfrac>
    <mi>e</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>=</mo> <munderover><mo>∑</mo>
    <mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow> <mi>n</mi></munderover> <mrow><mn>2</mn>
    <mrow><mo>(</mo> <mi>b</mi> <mo>+</mo> <mi>m</mi> <msub><mi>x</mi> <mi>i</mi></msub>
    <mo>-</mo> <msub><mi>y</mi> <mi>i</mi></msub> <mo>)</mo></mrow> <msub><mi>x</mi>
    <mi>i</mi></msub></mrow></mrow></mtd></mtr></mtable></math> <math display="block"><mtable
    displaystyle="true"><mtr><mtd columnalign="right"><mrow><mfrac><mi>d</mi> <mrow><mi>d</mi><mi>b</mi></mrow></mfrac>
    <mi>e</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>=</mo> <munderover><mo>∑</mo>
    <mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow> <mi>n</mi></munderover> <mrow><mo>(</mo>
    <mn>2</mn> <mi>b</mi> <mo>+</mo> <mn>2</mn> <mi>m</mi> <msub><mi>x</mi> <mi>i</mi></msub>
    <mo>-</mo> <mn>2</mn> <msub><mi>y</mi> <mi>i</mi></msub> <mo>)</mo></mrow></mrow></mtd></mtr></mtable></math>
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mi>e</mi>
    <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>=</mo> <munderover><mo>∑</mo>
    <mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow> <mi>n</mi></munderover> <mrow><mo>(</mo>
    <mrow><mo>(</mo> <mi>m</mi> <msub><mi>x</mi> <mi>i</mi></msub> <mo>+</mo> <mi>b</mi>
    <mo>)</mo></mrow> <mo>-</mo> <msub><mi>y</mi> <mi>i</mi></msub></mrow> <msup><mrow><mo>)</mo></mrow>
    <mn>2</mn></msup></mrow></mtd></mtr></mtable></math> <math display="block"><mtable
    displaystyle="true"><mtr><mtd columnalign="right"><mrow><mfrac><mi>d</mi> <mrow><mi>d</mi><mi>m</mi></mrow></mfrac>
    <mi>e</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>=</mo> <munderover><mo>∑</mo>
    <mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow> <mi>n</mi></munderover> <mrow><mn>2</mn>
    <mrow><mo>(</mo> <mi>b</mi> <mo>+</mo> <mi>m</mi> <msub><mi>x</mi> <mi>i</mi></msub>
    <mo>-</mo> <msub><mi>y</mi> <mi>i</mi></msub> <mo>)</mo></mrow> <msub><mi>x</mi>
    <mi>i</mi></mrow></mrow></mrow></mtd></mtr></mtable></math> <math display="block"><mtable
    displaystyle="true"><mtr><mtd columnalign="right"><mrow><mfrac><mi>d</mi> <mrow><mi>d</mi><mi>b</mi></mrow></mfrac>
    <mi>e</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>=</mo> <munderover><mo>∑</mo>
    <mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow> <mi>n</mi></munderover> <mrow><mo>(</mo>
    <mn>2</mn> <mi>b</mi> <mo>+</mo> <mn>2</mn> <mi>m</mi> <msub><mi>x</mi> <mi>i</mi></msub>
    <mo>-</mo> <mn>2</mn> <msub><mi>y</mi> <mi>i</mi></msub> <mo>)</mo></mrow></mrow></mtd></mtr></mtable></math>
- en: If you want to apply our dataset and execute a linear regression using gradient
    descent, you will have to perform a few additional steps as shown in [Example 5-11](#akiLhwlcJj).
    We will need to substitute for the `n`, `x(i)` and `y(i)` values, iterating all
    of our data points for the `d_m` and `d_b` derivative functions. That should leave
    only the `m` and `b` variables, which we will search for the optimal values using
    gradient descent.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想应用我们的数据集并使用梯度下降执行线性回归，你将需要执行一些额外的步骤，如[示例 5-11](#akiLhwlcJj)所示。我们需要替换`n`、`x(i)`和`y(i)`的值，迭代所有数据点以计算`d_m`和`d_b`的导数函数。这样就只剩下`m`和`b`变量，我们将使用梯度下降寻找最优值。
- en: Example 5-11\. Solving linear regression using SymP
  id: totrans-117
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 5-11\. 使用SymPy解决线性回归
- en: '[PRE10]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: As shown in [Example 5-11](#akiLhwlcJj), it is a good idea to call `lambdify()`
    on both of our partial derivative functions to convert them from SymPy to an optimized
    Python function. This will cause computations to perform much more quickly when
    we do gradient descent. The resulting Python functions are backed by NumPy, SciPy,
    or whatever numerical libraries SymPy detects you have available. After that,
    we can perform gradient descent.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 如[示例 5-11](#akiLhwlcJj)所示，对我们的偏导数函数都调用`lambdify()`是个好主意，将它们从SymPy转换为优化的Python函数。这将使计算在执行梯度下降时更快。生成的Python函数由NumPy、SciPy或SymPy检测到的其他数值库支持。之后，我们可以执行梯度下降。
- en: Finally, if you are curious about what the loss function looks like for this
    simple linear regression, [Example 5-12](#hbwp3lnb3m) shows the SymPy code that
    plugs the `x`, `y`, and `n` values into our loss function and then plots `m` and
    `b` as the input variables. Our gradient descent algorithm gets us to the lowest
    point in this loss landscape shown in [Figure 5-9](#fHieSkPkkg).
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，如果你对这个简单线性回归的损失函数感兴趣，[示例 5-12](#hbwp3lnb3m)展示了SymPy代码，将`x`、`y`和`n`的值代入我们的损失函数，然后将`m`和`b`作为输入变量绘制出来。我们的梯度下降算法将我们带到了损失景观中的最低点，如[图
    5-9](#fHieSkPkkg)所示。
- en: Example 5-12\. Plotting the loss function for linear regression
  id: totrans-121
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 5-12\. 绘制线性回归的损失函数
- en: '[PRE11]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '![emds 0509](Images/emds_0509.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![emds 0509](Images/emds_0509.png)'
- en: Figure 5-9\. The loss landscape for a simple linear regression
  id: totrans-124
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-9\. 简单线性回归的损失景观
- en: Overfitting and Variance
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 过拟合和方差
- en: 'Riddle me this: if we truly wanted to minimize loss, as in reduce the sum of
    squares to 0, what would we do? Are there options other than linear regression?
    One conclusion you may arrive at is simply fit a curve that touches all the points.
    Heck, why not just connect the points in segments and use that to make predictions
    as shown in [Figure 5-10](#qHprwhCkIQ)? That gives us a loss of 0!'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 你猜猜看：如果我们真的想最小化损失，即将平方和减少到0，我们会怎么做？除了线性回归还有其他选择吗？你可能得出的一个结论就是简单地拟合一个触及所有点的曲线。嘿，为什么不只是连接点并用它来做预测，如[图
    5-10](#qHprwhCkIQ)所示？这样就得到了0的损失！
- en: Shoot, why did we go through all that trouble with linear regression and not
    do this instead? Well, remember our big-picture objective is not to minimize the
    sum of squares but to make accurate predictions on new data. This connect-the-dots
    model is severely *overfit*, meaning it shaped the regression to the training
    data too exactly to the point it will predict poorly on new data. This simple
    connect-the-dots model is sensitive to outliers that are far away from the rest
    of the points, meaning it will have high *variance* in predictions. While the
    points in this example are relatively close to a line, this problem will be a
    lot worse with other datasets with more spread and outliers. Because overfitting
    increases variance, predictions are going to be all over the place!
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 真糟糕，为什么我们要费力进行线性回归而不是做这个呢？嗯，记住我们的大局目标不是最小化平方和，而是在新数据上做出准确的预测。这种连接点模型严重*过拟合*，意味着它将回归形状调整得太精确到预测新数据时表现糟糕。这种简单的连接点模型对远离其他点的异常值敏感，意味着它在预测中具有很高的*方差*。虽然这个例子中的点相对接近一条直线，但在其他具有更广泛分布和异常值的数据集中，这个问题会更严重。因为过拟合增加了方差，预测结果将会到处都是！
- en: '![emds 0510](Images/emds_0510.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![emds 0510](Images/emds_0510.png)'
- en: Figure 5-10\. Performing a regression by simply connecting the points, resulting
    in zero loss
  id: totrans-129
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-10\. 通过简单连接点执行回归，导致损失为零
- en: Overfitting Is Memorization
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 过拟合就是记忆
- en: When you hear someone say a regression “memorized” the data rather than generalizing
    it, they are talking about overfitting.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 当有人说回归“记住”了数据而不是泛化它时，他们在谈论过拟合。
- en: As you can guess, we want to find effective generalizations in our model rather
    than memorize data. Otherwise, our regression simply turns into a database where
    we look up values.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所猜测的，我们希望在模型中找到有效的泛化，而不是记忆数据。否则，我们的回归模型简单地变成了一个数据库，我们只是查找数值。
- en: This is why in machine learning you will find bias is added to the model, and
    linear regression is considered a highly biased model. This is not the same as
    bias in the data, which we talked about extensively in [Chapter 3](ch03.xhtml#ch03).
    *Bias in a model* means we prioritize a method (e.g., maintaining a straight line)
    as opposed to bending and fitting to exactly what the data says. A biased model
    leaves some wiggle room hoping to minimize loss on new data for better predictions,
    as opposed to minimizing loss on data it was trained on. I guess you could say
    adding bias to a model counteracts overfitting with *underfitting*, or fitting
    less to the training data.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，你会发现模型中添加了偏差，而线性回归被认为是一个高度偏置的模型。这与数据中的偏差不同，我们在[第三章](ch03.xhtml#ch03)中有详细讨论。*模型中的偏差*意味着我们优先考虑一种方法（例如，保持一条直线），而不是弯曲和完全适应数据。一个有偏差的模型留有一些余地，希望在新数据上最小化损失以获得更好的预测，而不是在训练数据上最小化损失。我想你可以说，向模型添加偏差可以抵消*过拟合*，或者说对训练数据拟合较少。
- en: As you can imagine, this is a balancing act because it is two contradictory
    objectives. In machine learning, we basically are saying, “I want to fit a regression
    to my data, but I don’t want to fit it *too much*. I need some wiggle room for
    predictions on new data that will be different.”
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以想象，这是一个平衡的过程，因为这是两个相互矛盾的目标。在机器学习中，我们基本上是在说，“我想要将回归拟合到我的数据，但我不想拟合得*太多*。我需要一些余地来预测新数据的不同之处。”
- en: Lasso and Ridge Regression
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 套索回归和岭回归
- en: Two somewhat popular variants of linear regression are lasso regression and
    ridge regression. Ridge regression adds a further bias to a linear regression
    in the form of a penalty, therefore causing it to fit less to the data. Lasso
    regression will attempt to marginalize noisy variables, making it useful when
    you want to automatically remove variables that might be irrelevant.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归的两个比较流行的变体是套索回归和岭回归。岭回归在线性回归中添加了进一步的偏差，以一种惩罚的形式，因此导致它对数据拟合较少。套索回归将尝试边缘化嘈杂的变量，这在你想要自动删除可能不相关的变量时非常有用。
- en: Still, we cannot just apply a linear regression to some data, make some predictions
    with it, and assume all is OK. A linear regression can overfit even with the bias
    of a straight line. Therefore, we need to check and mitigate for both overfitting
    and underfitting to find the sweet spot between the two. That is, unless there
    is not one at all, in which case you should abandon the model altogether.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们不能仅仅将线性回归应用于一些数据，进行一些预测，并假设一切都没问题。即使是一条直线的线性回归也可能过拟合。因此，我们需要检查和缓解过拟合和欠拟合，以找到两者之间的平衡点。除非根本没有平衡点，否则你应该完全放弃该模型。
- en: Stochastic Gradient Descent
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机梯度下降
- en: In a machine learning context, you are unlikely to do gradient descent in practice
    like we did earlier, where we trained on all training data (called *batch gradient
    descent*). In practice, you are more likely to perform *stochastic gradient descent*,
    which will train on only one sample of the dataset on each iteration. In *mini-batch
    gradient descent*, multiple samples of the dataset are used (e.g., 10 or 100 data
    points) on each iteration.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习的背景下，你不太可能像之前那样在实践中进行梯度下降，我们在所有训练数据上进行训练（称为*批量梯度下降*）。在实践中，你更有可能执行*随机梯度下降*，它将在每次迭代中仅对数据集的一个样本进行训练。在*小批量梯度下降*中，会使用数据集的多个样本（例如，10或100个数据点）进行每次迭代。
- en: Why use only part of the data on each iteration? Machine learning practitioners
    cite a few benefits. First, it reduces computation significantly, as each iteration
    does not have to traverse the entire training dataset but only part of it. The
    second benefit is it reduces overfitting. Exposing the training algorithm to only
    part of the data on each iteration keeps changing the loss landscape so it does
    not settle in the loss minimum. After all, minimizing the loss is what causes
    overfitting and so we introduce some randomness to create a little bit of underfitting
    (but hopefully not too much).
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么每次迭代只使用部分数据？机器学习从业者引用了一些好处。首先，它显著减少了计算量，因为每次迭代不必遍历整个训练数据集，而只需部分数据。第二个好处是减少过拟合。每次迭代只暴露训练算法于部分数据，使损失景观不断变化，因此不会稳定在损失最小值。毕竟，最小化损失是导致过拟合的原因，因此我们引入一些随机性来创建一点欠拟合（但希望不要太多）。
- en: Of course, our approximation becomes loose so we have to be careful. This is
    why we will talk about train/test splits shortly, as well as other metrics to
    evaluate our linear regression’s reliability.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我们的近似变得松散，所以我们必须小心。这就是为什么我们很快会谈论训练/测试拆分，以及其他评估我们线性回归可靠性的指标。
- en: '[Example 5-13](#qgiQkdFNOc) shows how to perform stochastic gradient descent
    in Python. If you change the sample size to be more than 1, it will perform mini-batch
    gradient descent.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例 5-13](#qgiQkdFNOc)展示了如何在Python中执行随机梯度下降。如果将样本大小改为大于1，它将执行小批量梯度下降。'
- en: Example 5-13\. Performing stochastic gradient descent for a linear regression
  id: totrans-143
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 5-13。执行线性回归的随机梯度下降
- en: '[PRE12]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: When I ran this, I got a linear regression of *y* = 1.9382830354181135*x* +
    4.753408787648379\. Obviously, your results are going to be different, and because
    of stochastic gradient descent we really aren’t going to converge toward a specific
    minimum but will end up in a broader neighborhood.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 当我运行这个时，我得到了一个线性回归 *y* = 1.9382830354181135*x* + 4.753408787648379。显然，你的结果会有所不同，由于随机梯度下降，我们实际上不会收敛到特定的最小值，而是会停留在一个更广泛的邻域。
- en: Is Randomness Bad?
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机性是坏事吗？
- en: If this randomness feels uncomfortable where you get a different answer every
    time you run a piece of code, welcome to the world of machine learning, optimization,
    and stochastic algorithms! Many algorithms that do approximations are random-based,
    and while some are extremely useful, others can be sloppy and perform poorly,
    as you might expect.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这种随机性让你感到不舒服，每次运行一段代码都会得到不同的答案，那么欢迎来到机器学习、优化和随机算法的世界！许多进行近似的算法都是基于随机性的，虽然有些非常有用，但有些可能效果不佳，正如你所预期的那样。
- en: A lot of people look to machine learning and AI as some tool that gives objective
    and precise answers, but that cannot be farther from the truth. Machine learning
    produces approximations with a degree of uncertainty, often without ground truth
    once in production. Machine learning can be misused if one is not aware of how
    it works, and it is remiss not to acknowledge its nondeterministic and approximate
    nature.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 很多人把机器学习和人工智能看作是一种能够给出客观和精确答案的工具，但事实并非如此。机器学习产生的是带有一定不确定性的近似值，通常在生产中没有基本事实。如果不了解它的工作原理，机器学习可能会被滥用，不承认其非确定性和近似性质是不妥的。
- en: While randomness can create some powerful tools, it can also be abused. Be careful
    to not use seed values and randomness to p-hack a “good” result, and put effort
    into analyzing your data and model.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然随机性可以创造一些强大的工具，但也可能被滥用。要小心不要使用种子值和随机性来 p-hack 一个“好”结果，并努力分析你的数据和模型。
- en: The Correlation Coefficient
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 相关系数
- en: Take a look at this scatterplot in [Figure 5-11](#juLpgQHUdt) alongside its
    linear regression. Why might a linear regression not work too well here?
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 看看这个散点图 [图 5-11](#juLpgQHUdt) 以及它的线性回归。为什么线性回归在这里效果不太好？
- en: '![emds 0511](Images/emds_0511.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![emds 0511](Images/emds_0511.png)'
- en: Figure 5-11\. A scatterplot of data with high variance
  id: totrans-153
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-11。具有高方差的数据的散点图
- en: The problem here is that the data has high variance. If the data is extremely
    spread out, it is going to drive up the variance to the point predictions become
    less accurate and useful, resulting in large residuals. Of course we can introduce
    a more biased model, such as linear regression, to not bend and respond to the
    variance so easily. However, the underfitting is also going to undermine our predictions
    because the data is so spread out. We need to numerically measure how “off” our
    predictions are.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的问题是数据具有很高的方差。如果数据极为分散，它将使方差增加到使预测变得不太准确和有用的程度，导致大的残差。当然，我们可以引入更偏向的模型，如线性回归，以不那么容易弯曲和响应方差。然而，欠拟合也会削弱我们的预测，因为数据如此分散。我们需要数值化地衡量我们的预测有多“偏离”。
- en: So how do you measure these residuals in aggregate? How do you also get a sense
    for how bad the variance in the data is? Let me introduce you to the *correlation
    coefficient*, also called the *Pearson correlation*, which measures the strength
    of the relationship between two variables as a value between –1 and 1\. A correlation
    coefficient closer to 0 indicates there is no correlation. A correlation coefficient
    closer to 1 indicates a strong *positive correlation*, meaning when one variable
    increases, the other proportionally increases. If it is closer to –1 then it indicates
    a strong *negative correlation*, which means as one variable increases the other
    proportionally decreases.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 那么如何对这些残差进行整体测量呢？你又如何了解数据中方差的糟糕程度呢？让我向你介绍*相关系数*，也称为*皮尔逊相关系数*，它以-1到1之间的值来衡量两个变量之间关系的强度。相关系数越接近0，表示没有相关性。相关系数越接近1，表示强*正相关*，意味着一个变量增加时，另一个变量成比例增加。如果接近-1，则表示强*负相关*，这意味着一个变量增加时，另一个成比例减少。
- en: Note the correlation coefficient is often denoted as *r*. The highly scattered
    data in [Figure 5-11](#juLpgQHUdt) has a correlation coefficient of 0.1201\. Since
    it is much closer to 0 than 1, we can infer the data has little correlation.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，相关系数通常表示为*r*。在[图5-11](#juLpgQHUdt)中高度分散的数据具有相关系数0.1201。由于它比1更接近0，我们可以推断数据之间关系很小。
- en: Here are four other scatterplots in [Figure 5-12](#fiaNFJsgpS) showing their
    correlation coefficients. Notice that the more the points follow a line, the stronger
    the correlation. More dispersed points result in weaker correlations.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是另外四个散点图，显示它们的相关系数。请注意，点越接近一条线，相关性越强。点更分散会导致相关性较弱。
- en: '![emds 0512](Images/emds_0512.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![emds 0512](Images/emds_0512.png)'
- en: Figure 5-12\. Correlation coefficients for four scatterplots
  id: totrans-159
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-12。四个散点图的相关系数
- en: As you can imagine, the correlation coefficient is useful for seeing if there
    is a possible relationship between two variables. If there is a strong positive-negative
    relationship, it will be useful in our linear regression. If there is not a relationship,
    they may just add noise and hurt model accuracy.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 可以想象，相关系数对于查看两个变量之间是否存在可能的关系是有用的。如果存在强正负关系，它将对我们的线性回归有所帮助。如果没有关系，它们可能只会添加噪音并损害模型的准确性。
- en: How do we use Python to calculate the correlation coefficient? Let’s use the
    simple [10-point dataset](https://bit.ly/2KF29Bd) we used earlier. A quick and
    easy way to analyze correlations for all pairs of variables is using Pandas’s
    `corr()` function. This makes it easy to see the correlation coefficient between
    every pair of variables in a dataset, which in this case will only be `x` and
    `y`. This is known as a *correlation matrix*. Take a look in [Example 5-14](#kCbTrtCRSC).
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何使用Python计算相关系数？让我们使用之前使用的简单[10点数据集](https://bit.ly/2KF29Bd)。分析所有变量对之间的相关性的快速简单方法是使用Pandas的`corr()`函数。这使得轻松查看数据集中每对变量之间的相关系数，这种情况下只会是`x`和`y`。这被称为*相关矩阵*。在[示例5-14](#kCbTrtCRSC)中查看。
- en: Example 5-14\. Using Pandas to see the correlation coefficient between every
    pair of variables
  id: totrans-162
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例5-14。使用Pandas查看每对变量之间的相关系数
- en: '[PRE13]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: As you can see, the correlation coefficient `0.957586` between `x` and `y` indicates
    a strong positive correlation between the two variables. You can ignore the parts
    of the matrix where `x` or `y` is set to itself and has a value of `1.0`. Obviously,
    when `x` or `y` is set to itself, the correlation will be perfect at 1.0, because
    the values match themselves exactly. When you have more than two variables, the
    correlation matrix will show a larger grid because there are more variables to
    pair and compare.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，`x`和`y`之间的相关系数`0.957586`表明这两个变量之间存在强烈的正相关性。您可以忽略矩阵中`x`或`y`设置为自身且值为`1.0`的部分。显然，当`x`或`y`设置为自身时，相关性将完美地为1.0，因为值与自身完全匹配。当您有两个以上的变量时，相关性矩阵将显示更大的网格，因为有更多的变量进行配对和比较。
- en: If you change the code to use a different dataset with a lot of variance, where
    the data is spread out, you will see that the correlation coefficient decreases.
    This again indicates a weaker correlation.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您更改代码以使用具有大量变化的不同数据集，其中数据分散，您将看到相关系数下降。这再次表明了较弱的相关性。
- en: Statistical Significance
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 统计显著性
- en: 'Here is another aspect to a linear regression you must consider: is my data
    correlation coincidental? In [Chapter 3](ch03.xhtml#ch03) we studied hypothesis
    testing and p-values, and we are going to extend those ideas here with a linear
    regression.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 这里还有线性回归的另一个方面需要考虑：我的数据相关性是否巧合？在[第3章](ch03.xhtml#ch03)中，我们研究了假设检验和p值，我们将在这里用线性回归扩展这些想法。
- en: 'Let’s start with a fundamental question: is it possible I see a linear relationship
    in my data due to random chance? How can we be 95% sure the correlation between
    these two variables is significant and not coincidental? If this sounds like a
    hypothesis test from [Chapter 3](ch03.xhtml#ch03), it’s because it is! We need
    to not just express the correlation coefficient but also quantify how confident
    we are that the correlation coefficient did not occur by chance.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从一个基本问题开始：我是否可能由于随机机会在我的数据中看到线性关系？我们如何能够确信这两个变量之间的相关性是显著的而不是巧合的95%？如果这听起来像[第3章](ch03.xhtml#ch03)中的假设检验，那是因为它就是！我们不仅需要表达相关系数，还需要量化我们对相关系数不是偶然发生的信心。
- en: 'Rather than estimating a mean like we did in [Chapter 3](ch03.xhtml#ch03) with
    the drug-testing example, we are estimating the population correlation coefficient
    based on a sample. We denote the population correlation coefficient with the Greek
    symbol <math alttext="rho"><mi>ρ</mi></math> (Rho) while our sample correlation
    coefficient is *r*. Just like we did in [Chapter 3](ch03.xhtml#ch03), we will
    have a null hypothesis <math alttext="upper H 0"><msub><mi>H</mi> <mn>0</mn></msub></math>
    and alternative hypothesis <math alttext="upper H 1"><msub><mi>H</mi> <mn>1</mn></msub></math>
    :'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们在[第3章](ch03.xhtml#ch03)中使用药物测试示例中所做的估计均值不同，我们正在基于样本估计总体相关系数。我们用希腊字母符号<math
    alttext="rho"><mi>ρ</mi></math>（Rho）表示总体相关系数，而我们的样本相关系数是*r*。就像我们在[第3章](ch03.xhtml#ch03)中所做的那样，我们将有一个零假设<math
    alttext="upper H 0"><msub><mi>H</mi> <mn>0</mn></msub></math>和备择假设<math alttext="upper
    H 1"><msub><mi>H</mi> <mn>1</mn></msub></math>：
- en: <math display="block"><mrow><msub><mi>H</mi> <mn>0</mn></msub> <mo>:</mo> <mi>ρ</mi>
    <mo>=</mo> <mn>0</mn> <mtext>(implies</mtext> <mtext>no</mtext> <mtext>relationship)</mtext></mrow></math>
    <math display="block"><mrow><msub><mi>H</mi> <mn>1</mn></msub> <mo>:</mo> <mi>ρ</mi>
    <mo>≠</mo> <mn>0</mn> <mtext>(relationship</mtext> <mtext>is</mtext> <mtext>present)</mtext></mrow></math>
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><msub><mi>H</mi> <mn>0</mn></msub> <mo>:</mo> <mi>ρ</mi>
    <mo>=</mo> <mn>0</mn> <mtext>(意味着</mtext> <mtext>没有</mtext> <mtext>关系)</mtext></mrow></math>
    <math display="block"><mrow><msub><mi>H</mi> <mn>1</mn></msub> <mo>:</mo> <mi>ρ</mi>
    <mo>≠</mo> <mn>0</mn> <mtext>(关系</mtext> <mtext>存在)</mtext></mrow></math>
- en: Our null hypothesis <math alttext="upper H 0"><msub><mi>H</mi> <mn>0</mn></msub></math>
    is that there is no relationship between two variables, or more technically, the
    correlation coefficient is 0\. The alternative hypothesis <math alttext="upper
    H 1"><msub><mi>H</mi> <mn>1</mn></msub></math> is there is a relationship, and
    it can be a positive or negative correlation. This is why the alternative hypothesis
    is defined as <math alttext="rho not-equals 0"><mrow><mi>ρ</mi> <mo>≠</mo> <mn>0</mn></mrow></math>
    to support both a positive and negative correlation.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的零假设<math alttext="upper H 0"><msub><mi>H</mi> <mn>0</mn></msub></math>是两个变量之间没有关系，或更技术性地说，相关系数为0。备择假设<math
    alttext="upper H 1"><msub><mi>H</mi> <mn>1</mn></msub></math>是存在关系，可以是正相关或负相关。这就是为什么备择假设被定义为<math
    alttext="rho not-equals 0"><mrow><mi>ρ</mi> <mo>≠</mo> <mn>0</mn></mrow></math>，以支持正相关和负相关。
- en: Let’s return to our dataset of 10 points as shown in [Figure 5-13](#MOVwTkVqGF).
    How likely is it we would see these data points by chance? And they happen to
    produce what looks like a linear relationship?
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到我们的包含10个点的数据集，如[图5-13](#MOVwTkVqGF)所示。我们看到这些数据点是多大概率是偶然看到的？它们恰好产生了看起来是线性关系？
- en: '![emds 0513](Images/emds_0513.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![emds 0513](Images/emds_0513.png)'
- en: Figure 5-13\. How likely would we see this data, which seems to have a linear
    correlation, by random chance?
  id: totrans-174
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-13\. 这些数据看起来具有线性相关性，我们有多大可能性是随机机会看到的？
- en: We already calculated the correlation coefficient for this dataset in [Example 5-14](#kCbTrtCRSC),
    which is 0.957586\. That’s a strong and compelling positive correlation. But again,
    we need to evaluate if this was by random luck. Let’s pursue our hypothesis test
    with 95% confidence using a two-tailed test, exploring if there is a relationship
    between these two variables.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在[示例5-14](#kCbTrtCRSC)中计算了这个数据集的相关系数为0.957586。这是一个强有力的正相关。但是，我们需要评估这是否是由于随机运气。让我们以95%的置信度进行双尾检验，探讨这两个变量之间是否存在关系。
- en: We talked about the T-distribution in [Chapter 3](ch03.xhtml#ch03), which has
    fatter tails to capture more variance and uncertainty. We use a T-distribution
    rather than a normal distribution to do hypothesis testing with linear regression.
    First, let’s plot a T-distribution with a 95% critical value range as shown in
    [Figure 5-14](#DeDuaGoMhK). We account for the fact there are 10 records in our
    sample and therefore we have 9 degrees of freedom (10 – 1 = 9).
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第3章](ch03.xhtml#ch03)中讨论了T分布，它有更厚的尾部以捕捉更多的方差和不确定性。我们使用T分布而不是正态分布进行线性回归的假设检验。首先，让我们绘制一个T分布，95%的临界值范围如[图5-14](#DeDuaGoMhK)所示。考虑到我们的样本中有10条记录，因此我们有9个自由度（10-1=9）。
- en: '![emds 0514](Images/emds_0514.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![emds 0514](Images/emds_0514.png)'
- en: Figure 5-14\. A T-distribution with 9 degrees of freedom, as there are 10 records
    and we subtract 1
  id: totrans-178
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-14\. 9个自由度的T分布，因为有10条记录，我们减去1
- en: The critical value is approximately ±2.262, and we can calculate that in Python
    as shown in [Example 5-16](#siDtwtJwel). This captures 95% of the center area
    of our T-distribution.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 临界值约为±2.262，我们可以在Python中计算如[示例5-16](#siDtwtJwel)所示。这捕捉了我们T分布中心区域的95%。
- en: Example 5-16\. Calculating the critical value from a T-distribution
  id: totrans-180
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例5-16\. 从T分布计算临界值
- en: '[PRE14]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'If our test value happens to fall outside this range of (–2.262, 2.262), then
    we can reject our null hypothesis. To calculate the test value *t*, we need to
    use the following formula. Again *r* is the correlation coefficient and *n* is
    the sample size:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们的检验值恰好落在（-2.262，2.262）的范围之外，那么我们可以拒绝我们的零假设。要计算检验值*t*，我们需要使用以下公式。再次，*r*是相关系数，*n*是样本大小：
- en: <math display="block"><mrow><mi>t</mi> <mo>=</mo> <mfrac><mi>r</mi> <msqrt><mfrac><mrow><mn>1</mn><mo>-</mo><msup><mi>r</mi>
    <mn>2</mn></msup></mrow> <mrow><mi>n</mi><mo>-</mo><mn>2</mn></mrow></mfrac></msqrt></mfrac></mrow></math><math
    display="block"><mrow><mi>t</mi> <mo>=</mo> <mfrac><mrow><mn>.957586</mn></mrow>
    <msqrt><mfrac><mrow><mn>1</mn><mo>-</mo><msup><mn>.957586</mn> <mn>2</mn></msup></mrow>
    <mrow><mn>10</mn><mo>-</mo><mn>2</mn></mrow></mfrac></msqrt></mfrac> <mo>=</mo>
    <mn>9.339956</mn></mrow></math>
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mi>t</mi> <mo>=</mo> <mfrac><mi>r</mi> <msqrt><mfrac><mrow><mn>1</mn><mo>-</mo><msup><mi>r</mi>
    <mn>2</mn></msup></mrow> <mrow><mi>n</mi><mo>-</mo><mn>2</mn></mrow></mfrac></msqrt></mfrac></mrow></math><math
    display="block"><mrow><mi>t</mi> <mo>=</mo> <mfrac><mrow><mn>.957586</mn></mrow>
    <msqrt><mfrac><mrow><mn>1</mn><mo>-</mo><msup><mn>.957586</mn> <mn>2</mn></msup></mrow>
    <mrow><mn>10</mn><mo>-</mo><mn>2</mn></mrow></mfrac></msqrt></mfrac> <mo>=</mo>
    <mn>9.339956</mn></mrow></math>
- en: Let’s put the whole test together in Python as shown in [Example 5-17](#kLcFTduBrG).
    If our test value falls outside the critical range of 95% confidence, we accept
    that our correlation was not by chance.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在Python中将整个测试放在一起，如[示例5-17](#kLcFTduBrG)所示。如果我们的检验值落在95%置信度的临界范围之外，我们接受我们的相关性不是偶然的。
- en: Example 5-17\. Testing significance for linear-looking data
  id: totrans-185
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例5-17\. 测试看起来线性的数据的显著性
- en: '[PRE15]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The test value here is approximately 9.39956, which is definitely outside the
    range of (–2.262, 2.262) so we can reject the null hypothesis and say our correlation
    is real. That’s because the p-value is remarkably significant: .000005976\. This
    is well below our .05 threshold, so this is virtually not coincidence: there is
    a correlation. It makes sense the p-value is so small because the points strongly
    resemble a line. It is highly unlikely these points randomly arranged themselves
    near a line this closely by chance.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的检验值约为9.39956，明显超出了（-2.262，2.262）的范围，因此我们可以拒绝零假设，并说我们的相关性是真实的。这是因为p值非常显著：0.000005976。这远低于我们的0.05阈值，因此这几乎不是巧合：存在相关性。p值如此之小是有道理的，因为这些点强烈地类似于一条线。这些点随机地如此靠近一条线的可能性极小。
- en: '[Figure 5-15](#qFBEPbEweP) shows some other datasets with their correlation
    coefficients and p-values. Analyze each one of them. Which one is likely the most
    useful for predictions? What are the problems with the other ones?'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '[图5-15](#qFBEPbEweP)展示了一些其他数据集及其相关系数和p值。分析每一个。哪一个可能对预测最有用？其他数据集存在什么问题？'
- en: '![emds 0515](Images/emds_0515.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![emds 0515](Images/emds_0515.png)'
- en: Figure 5-15\. Different datasets and their correlation coefficients and p-values
  id: totrans-190
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-15。不同数据集及其相关系数和p值
- en: 'Now that you’ve had a chance to work through the analysis on the datasets from
    [Figure 5-15](#qFBEPbEweP), let’s walk through the findings. The left figure has
    a high positive correlation, but it only has three points. The lack of data drives
    up the p-value significantly to 0.34913 and increases the likelihood the data
    happened by chance. This makes sense because having just three data points makes
    it likely to see a linear pattern, but it’s not much better than having two points
    that will simply connect a line between them. This brings up an important rule:
    having more data will decrease your p-value, especially if that data gravitates
    toward a line.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你有机会对来自[图5-15](#qFBEPbEweP)的数据集进行分析后，让我们来看看结果。左侧图具有很高的正相关性，但只有三个数据点。数据不足显著提高了p值，达到0.34913，并增加了数据发生偶然性的可能性。这是有道理的，因为只有三个数据点很可能会看到一个线性模式，但这并不比只有两个点好，这两个点只会连接一条直线。这提出了一个重要的规则：拥有更多数据将降低你的p值，特别是如果这些数据趋向于一条线。
- en: The second figure is what we just covered. It has only 10 data points, but it
    forms a linear pattern so nicely, we not only have a strong positive correlation
    but also an extremely low p-value. When you have a p-value this low, you can bet
    you are measuring an engineered and tightly controlled process, not something
    sociological or natural.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 第二幅图就是我们刚刚讨论的内容。它只有10个数据点，但形成了一个线性模式，我们不仅有很强的正相关性，而且p值极低。当p值如此之低时，你可以确定你正在测量一个经过精心设计和严格控制的过程，而不是某种社会学或自然现象。
- en: The right two images in [Figure 5-15](#qFBEPbEweP) fail to identify a linear
    relationship. Their correlation coefficient is close to 0, indicating no correlation,
    and the p-values unsurprisingly indicate randomness played a role.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '[图5-15](#qFBEPbEweP)中右侧的两幅图未能确定线性关系。它们的相关系数接近于0，表明没有相关性，而p值不出所料地表明随机性起了作用。'
- en: 'The rule is this: the more data you have that consistently resembles a line,
    the more significant the p-value for your correlation will be. The more scattered
    or sparse the data, the more the p-value will increase and thus indicate your
    correlation occurred by random chance.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 规则是这样的：拥有更多数据且一致地类似于一条线，你的相关性的p值就会更显著。数据越分散或稀疏，p值就会增加，从而表明你的相关性是由随机机会引起的。
- en: Coefficient of Determination
  id: totrans-195
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 决定系数
- en: Let’s learn an important metric you will see a lot in statistics and machine
    learning regressions. The *coefficient of determination*, called <math alttext="r
    squared"><msup><mi>r</mi> <mn>2</mn></msup></math> , measures how much variation
    in one variable is explainable by the variation of the other variable. It is also
    the square of the correlation coefficient <math alttext="r"><mi>r</mi></math>
    . As <math alttext="r"><mi>r</mi></math> approaches a perfect correlation (–1
    or 1), <math alttext="r squared"><msup><mi>r</mi> <mn>2</mn></msup></math> approaches
    1\. Essentially, <math alttext="r squared"><msup><mi>r</mi> <mn>2</mn></msup></math>
    shows how much two variables interact with each other.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们学习一个在统计学和机器学习回归中经常见到的重要指标。*决定系数*，称为<math alttext="r squared"><msup><mi>r</mi>
    <mn>2</mn></msup></math>，衡量一个变量的变异有多少是由另一个变量的变异解释的。它也是相关系数<math alttext="r"><mi>r</mi></math>的平方。当<math
    alttext="r"><mi>r</mi></math>接近完美相关（-1或1）时，<math alttext="r squared"><msup><mi>r</mi>
    <mn>2</mn></msup></math>接近1。基本上，<math alttext="r squared"><msup><mi>r</mi> <mn>2</mn></msup></math>显示了两个变量相互作用的程度。
- en: Let’s continue looking at our data from [Figure 5-13](#MOVwTkVqGF). In [Example 5-18](#MgPOHfCDiB),
    take our dataframe code from earlier that calculates the correlation coefficient
    and then simply square it. That will multiply each correlation coefficient by
    itself.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续查看我们从[图5-13](#MOVwTkVqGF)中的数据。在[示例5-18](#MgPOHfCDiB)中，使用我们之前计算相关系数的数据框代码，然后简单地对其进行平方。这将使每个相关系数相互乘以自己。
- en: Example 5-18\. Creating a correlation matrix in Pandas
  id: totrans-198
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例5-18。在Pandas中创建相关性矩阵
- en: '[PRE16]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: A coefficient of determination of 0.916971 is interpreted as 91.6971% of the
    variation in *x* is explained by *y* (and vice versa), and the remaining 8.3029%
    is noise caused by other uncaptured variables; 0.916971 is a pretty good coefficient
    of determination, showing that *x* and *y* explain each other’s variance. But
    there could be other variables at play making up that remaining 0.083029\. Remember,
    correlation does not equal causation, so there could be other variables contributing
    to the relationship we are seeing.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 决定系数为0.916971被解释为*x*的变异的91.6971%由*y*（反之亦然）解释，剩下的8.3029%是由其他未捕获的变量引起的噪音；0.916971是一个相当不错的决定系数，显示*x*和*y*解释彼此的方差。但可能有其他变量在起作用，占据了剩下的0.083029。记住，相关性不等于因果关系，因此可能有其他变量导致我们看到的关系。
- en: Correlation Is Not Causation!
  id: totrans-201
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 相关性不代表因果关系！
- en: It is important to note that while we put a lot of emphasis on measuring correlation
    and building metrics around it, please remember *correlation is not causation!*
    You probably have heard that mantra before, but I want to expand on why statisticians
    say it.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，虽然我们非常强调测量相关性并围绕其构建指标，请记住*相关性不代表因果关系*！你可能以前听过这句口头禅，但我想扩展一下统计学家为什么这么说。
- en: Just because we see a correlation between *x* and *y* does not mean *x* causes
    *y*. It could actually be *y* causes *x*! Or maybe there is a third uncaptured
    variable *z* that is causing *x* and *y*. It could be that *x* and *y* do not
    cause each other at all and the correlation is just coincidental, hence why it
    is important we measure the statistical significance.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 仅仅因为我们看到*x*和*y*之间的相关性，并不意味着*x*导致*y*。实际上可能是*y*导致*x*！或者可能存在第三个未捕获的变量*z*导致*x*和*y*。也可能*x*和*y*根本不相互导致，相关性只是巧合，因此我们测量统计显著性非常重要。
- en: Now I have a more pressing question for you. Can computers discern between correlation
    and causation? The answer is a resounding “NO!” Computers have concept of correlation
    but not causation. Let’s say I load a dataset to scikit-learn showing gallons
    of water consumed and my water bill. My computer, or any program including scikit-learn,
    does not have any notion whether more water usage causes a higher bill or a higher
    bill causes more water usage. An AI system could easily conclude the latter, as
    nonsensical as that is. This is why many machine learning projects require a human
    in the loop to inject common sense.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我有一个更紧迫的问题要问你。计算机能区分相关性和因果关系吗？答案是“绝对不！”计算机有相关性的概念，但没有因果关系。假设我加载一个数据集到scikit-learn，显示消耗的水量和我的水费。我的计算机，或包括scikit-learn在内的任何程序，都不知道更多的用水量是否导致更高的账单，或更高的账单是否导致更多的用水量。人工智能系统很容易得出后者的结论，尽管这是荒谬的。这就是为什么许多机器学习项目需要一个人来注入常识。
- en: In computer vision, this happens too. Computer vision often uses a regression
    on the numeric pixels to predict a category. If I train a computer vision system
    to recognize cows using pictures of cows, it can easily make correlations with
    the field rather than the cows. Therefore, if I show a picture of an empty field,
    it will label the grass as cows! This again is because computers have no concept
    of causality (the cow shape should cause the label “cow”) and instead get lost
    in correlations we are not interested in.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算机视觉中，这也会发生。计算机视觉通常会对数字像素进行回归以预测一个类别。如果我训练一个计算机视觉系统来识别牛，使用的是牛的图片，它可能会轻易地将场地与牛进行关联。因此，如果我展示一张空旷的场地的图片，它会将草地标记为牛！这同样是因为计算机没有因果关系的概念（牛的形状应该导致标签“牛”），而是陷入了我们不感兴趣的相关性中。
- en: Standard Error of the Estimate
  id: totrans-206
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 估计的标准误差
- en: 'One way to measure the overall error of a linear regression is the *SSE*, or
    *sum of squared error*. We learned about this earlier where we squared each residual
    and summed them. If <math alttext="ModifyingAbove y With caret"><mover accent="true"><mi>y</mi>
    <mo>^</mo></mover></math> (pronounced “y-hat”) is each predicted value from the
    line and <math alttext="y"><mi>y</mi></math> represents each actual y-value from
    the data, here is the calculation:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 衡量线性回归整体误差的一种方法是*SSE*，或者*平方误差和*。我们之前学过这个概念，其中我们对每个残差进行平方并求和。如果<math alttext="ModifyingAbove
    y With caret"><mover accent="true"><mi>y</mi> <mo>^</mo></mover></math>（读作“y-hat”）是线上的每个预测值，而<math
    alttext="y"><mi>y</mi></math>代表数据中的每个实际y值，这里是计算公式：
- en: <math alttext="upper S upper S upper E equals sigma-summation left-parenthesis
    y minus ModifyingAbove y With caret right-parenthesis squared" display="block"><mrow><mi>S</mi>
    <mi>S</mi> <mi>E</mi> <mo>=</mo> <mo>∑</mo> <msup><mrow><mo>(</mo><mi>y</mi><mo>-</mo><mover
    accent="true"><mi>y</mi> <mo>^</mo></mover><mo>)</mo></mrow> <mn>2</mn></msup></mrow></math>
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper S upper S upper E equals sigma-summation left-parenthesis
    y minus ModifyingAbove y With caret right-parenthesis squared" display="block"><mrow><mi>S</mi>
    <mi>S</mi> <mi>E</mi> <mo>=</mo> <mo>∑</mo> <msup><mrow><mo>(</mo><mi>y</mi><mo>-</mo><mover
    accent="true"><mi>y</mi> <mo>^</mo></mover><mo>)</mo></mrow> <mn>2</mn></msup></mrow></math>
- en: However, all of these squared values are hard to interpret so we can use some
    square root logic to scale things back into their original units. We will also
    average all of them, and this is what the *standard error of the estimate ( <math
    alttext="upper S Subscript e"><msub><mi>S</mi> <mi>e</mi></msub></math> )* does.
    If *n* is the number of data points, [Example 5-19](#OmfMsdJioT) shows how we
    calculate the standard error <math alttext="upper S Subscript e"><msub><mi>S</mi>
    <mi>e</mi></msub></math> in Python.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，所有这些平方值很难解释，所以我们可以使用一些平方根逻辑将事物重新缩放到它们的原始单位。我们还将所有这些值求平均，这就是*估计的标准误差（<math
    alttext="upper S Subscript e"><msub><mi>S</mi> <mi>e</mi></msub></math>)*的作用。如果*n*是数据点的数量，[示例5-19](#OmfMsdJioT)展示了我们如何在Python中计算标准误差<math
    alttext="upper S Subscript e"><msub><mi>S</mi> <mi>e</mi></msub></math>。
- en: <math alttext="upper S Subscript e Baseline equals StartFraction sigma-summation
    left-parenthesis y minus ModifyingAbove y With caret right-parenthesis squared
    Over n minus 2 EndFraction" display="block"><mrow><msub><mi>S</mi> <mi>e</mi></msub>
    <mo>=</mo> <mfrac><mrow><mo>∑</mo><msup><mrow><mo>(</mo><mi>y</mi><mo>-</mo><mover
    accent="true"><mi>y</mi> <mo>^</mo></mover><mo>)</mo></mrow> <mn>2</mn></msup></mrow>
    <mrow><mi>n</mi><mo>-</mo><mn>2</mn></mrow></mfrac></mrow></math>
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper S Subscript e Baseline equals StartFraction sigma-summation
    left-parenthesis y minus ModifyingAbove y With caret right-parenthesis squared
    Over n minus 2 EndFraction" display="block"><mrow><msub><mi>S</mi> <mi>e</mi></msub>
    <mo>=</mo> <mfrac><mrow><mo>∑</mo><msup><mrow><mo>(</mo><mi>y</mi><mo>-</mo><mover
    accent="true"><mi>y</mi> <mo>^</mo></mover><mo>)</mo></mrow> <mn>2</mn></msup></mrow>
    <mrow><mi>n</mi><mo>-</mo><mn>2</mn></mrow></mfrac></mrow></math>
- en: Example 5-19\. Calculating the standard error of the estimate
  id: totrans-211
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 5-19。计算估计的标准误差
- en: '[PRE17]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Why <math alttext="n minus 2"><mrow><mi>n</mi> <mo>-</mo> <mn>2</mn></mrow></math>
    instead of <math alttext="n minus 1"><mrow><mi>n</mi> <mo>-</mo> <mn>1</mn></mrow></math>
    , like we did in so many variance calculations in [Chapter 3](ch03.xhtml#ch03)?
    Without going too deep into mathematical proofs, this is because a linear regression
    has two variables, not just one, so we have to increase the uncertainty by one
    more in our degrees of freedom.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么是<math alttext="n minus 2"><mrow><mi>n</mi> <mo>-</mo> <mn>2</mn></mrow></math>而不是像我们在[第三章](ch03.xhtml#ch03)中的许多方差计算中所做的<math
    alttext="n minus 1"><mrow><mi>n</mi> <mo>-</mo> <mn>1</mn></mrow></math>？不深入数学证明，这是因为线性回归有两个变量，而不只是一个，所以我们必须在自由度中再增加一个不确定性。
- en: You will notice the standard error of the estimate looks remarkably similar
    to the standard deviation we studied in [Chapter 3](ch03.xhtml#ch03). This is
    not by accident. That is because it is the standard deviation for a linear regression.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到估计的标准误差看起来与我们在[第三章](ch03.xhtml#ch03)中学习的标准差非常相似。这并非偶然。这是因为它是线性回归的标准差。
- en: Prediction Intervals
  id: totrans-215
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预测区间
- en: As mentioned earlier, our data in a linear regression is a sample from a population.
    Therefore, our regression is only as good as our sample. Our linear regression
    line also has a normal distribution running along it. Effectively, this makes
    each predicted y-value a sample statistic just like the mean. As a matter of fact,
    the “mean” is shifting along the line.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 正如前面提到的，线性回归中的数据是从一个总体中取样得到的。因此，我们的回归结果只能和我们的样本一样好。我们的线性回归线也沿着正态分布运行。实际上，这使得每个预测的y值都像均值一样是一个样本统计量。事实上，“均值”沿着这条线移动。
- en: Remember when we talked statistics in [Chapter 2](ch02.xhtml#ch02) about variance
    and standard deviation? The concepts apply here too. With a linear regression,
    we hope that data follows a normal distribution in a linear fashion. A regression
    line serves as the shifting “mean” of our bell curve, and the spread of the data
    around the line reflects the variance/standard deviation, as shown in [Figure 5-16](#GQrKIjbjFm).
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 还记得我们在[第2章](ch02.xhtml#ch02)中讨论方差和标准差吗？这些概念在这里也适用。通过线性回归，我们希望数据以线性方式遵循正态分布。回归线充当我们钟形曲线的“均值”，数据围绕该线的分布反映了方差/标准差，如[图5-16](#GQrKIjbjFm)所示。
- en: '![emds 0516](Images/emds_0516.png)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![emds 0516](Images/emds_0516.png)'
- en: Figure 5-16\. A linear regression assumes a normal distribution is following
    the line
  id: totrans-219
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-16。线性回归假设正态分布遵循该线
- en: When we have a normal distribution following a linear regression line, we have
    not just one variable but a second one steering a distribution as well. There
    is a confidence interval around each *y* prediction, and this is known as a *prediction
    interval*.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们有一个正态分布遵循线性回归线时，我们不仅有一个变量，还有第二个变量引导着分布。每个*y*预测周围都有一个置信区间，这被称为*预测区间*。
- en: Let’s bring back some context with our veterinary example, estimating the age
    of a dog and number of vet visits. I want to know the prediction interval for
    number of vet visits with 95% confidence for a dog that is 8.5 years old. What
    this prediction interval looks like is shown in [Figure 5-17](#ASbMImvBJq). We
    are 95% confident that an 8.5 year old dog will have between 16.462 and 25.966
    veterinary visits.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过兽医示例重新带回一些背景，估计一只狗的年龄和兽医访问次数。我想知道一个8.5岁狗的兽医访问次数的95%置信度的预测区间。这个预测区间的样子如[图5-17](#ASbMImvBJq)所示。我们有95%的信心，一个8.5岁的狗将有16.462到25.966次兽医访问。
- en: '![emds 0517](Images/emds_0517.png)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![emds 0517](Images/emds_0517.png)'
- en: Figure 5-17\. A prediction interval for a dog that is 8.5 years old with 95%
    confidence
  id: totrans-223
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-17。一个8.5岁狗的95%置信度的预测区间
- en: 'How do we calculate this? We need to get the margin of error and plus/minus
    it around the predicted y-value. It’s a beastly equation that involves a critical
    value from the T-distribution as well as the standard error of the estimate. Let’s
    take a look:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何计算这个？我们需要得到误差边界，并在预测的y值周围加减。这是一个涉及T分布的临界值以及估计标准误差的庞大方程。让我们来看一下：
- en: <math alttext="upper E equals t Subscript .025 Baseline asterisk upper S Subscript
    e Baseline asterisk StartRoot 1 plus StartFraction 1 Over n EndFraction plus StartFraction
    n left-parenthesis x 0 plus x overbar right-parenthesis squared Over n left-parenthesis
    sigma-summation x squared right-parenthesis minus left-parenthesis sigma-summation
    x right-parenthesis squared EndFraction EndRoot" display="block"><mrow><mi>E</mi>
    <mo>=</mo> <msub><mi>t</mi> <mtext>.025</mtext></msub> <mo>*</mo> <msub><mi>S</mi>
    <mi>e</mi></msub> <mo>*</mo> <msqrt><mrow><mn>1</mn> <mo>+</mo> <mfrac><mn>1</mn>
    <mi>n</mi></mfrac> <mo>+</mo> <mfrac><mrow><mi>n</mi><msup><mrow><mo>(</mo><msub><mi>x</mi>
    <mn>0</mn></msub> <mo>+</mo><mover accent="true"><mi>x</mi> <mo>¯</mo></mover><mo>)</mo></mrow>
    <mn>2</mn></msup></mrow> <mrow><mi>n</mi><mrow><mo>(</mo><mo>∑</mo><msup><mi>x</mi>
    <mn>2</mn></msup> <mo>)</mo></mrow><mo>-</mo><msup><mrow><mo>(</mo><mo>∑</mo><mi>x</mi><mo>)</mo></mrow>
    <mn>2</mn></msup></mrow></mfrac></mrow></msqrt></mrow></math>
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper E equals t Subscript .025 Baseline asterisk upper S Subscript
    e Baseline asterisk StartRoot 1 plus StartFraction 1 Over n EndFraction plus StartFraction
    n left-parenthesis x 0 plus x overbar right-parenthesis squared Over n left-parenthesis
    sigma-summation x squared right-parenthesis minus left-parenthesis sigma-summation
    x right-parenthesis squared EndFraction EndRoot" display="block"><mrow><mi>E</mi>
    <mo>=</mo> <msub><mi>t</mi> <mtext>.025</mtext></msub> <mo>*</mo> <msub><mi>S</mi>
    <mi>e</mi></msub> <mo>*</mo> <msqrt><mrow><mn>1</mn> <mo>+</mo> <mfrac><mn>1</mn>
    <mi>n</mi></mfrac> <mo>+</mo> <mfrac><mrow><mi>n</mi><msup><mrow><mo>(</mo><msub><mi>x</mi>
    <mn>0</mn></msub> <mo>+</mo><mover accent="true"><mi>x</mi> <mo>¯</mo></mover><mo>)</mo></mrow>
    <mn>2</mn></msup></mrow> <mrow><mi>n</mi><mrow><mo>(</mo><mo>∑</mo><msup><mi>x</mi>
    <mn>2</mn></msup> <mo>)</mo></mrow><mo>-</mo><msup><mrow><mo>(</mo><mo>∑</mo><mi>x</mi><mo>)</mo></mrow>
    <mn>2</mn></msup></mrow></mfrac></mrow></msqrt></mrow></math>
- en: The x-value we are interested in is specified as <math alttext="x 0"><msub><mi>x</mi>
    <mn>0</mn></msub></math> , which in this case is 8.5\. Here is how we solve this
    in Python, shown in [Example 5-20](#DaFlrHUbDE).
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 我们感兴趣的x值被指定为<math alttext="x 0"><msub><mi>x</mi> <mn>0</mn></msub></math>，在这种情况下是8.5。这是我们如何在Python中解决这个问题的，如[示例5-20](#DaFlrHUbDE)所示。
- en: Example 5-20\. Calculating a prediction interval of vet visits for a dog that’s
    8.5 years old
  id: totrans-227
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例5-20。计算一只8.5岁狗的兽医访问预测区间
- en: '[PRE18]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Oy vey! That’s a lot of calculation, and unfortunately, SciPy and other mainstream
    data science libraries do not do this. But if you are inclined to statistical
    analysis, this is very useful information. We not only create a prediction based
    on a linear regression (e.g., a dog that’s 8.5 years old will have 21.2145 vet
    visits), but we also are actually able to say something much less absolute: there’s
    a 95% probability an 8.5 year old dog will visit the vet between 16.46 and 25.96
    times. Brilliant, right? And it’s a much safer claim because it captures a range
    rather than a single value, and thus accounts for uncertainty.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 哎呀！这是很多计算，不幸的是，SciPy和其他主流数据科学库都不会做这个。但如果你倾向于统计分析，这是非常有用的信息。我们不仅基于线性回归创建预测（例如，一只8.5岁的狗将有21.2145次兽医访问），而且实际上能够说出一些远非绝对的东西：一个8.5岁的狗会在16.46到25.96次之间访问兽医的概率为95%。很棒，对吧？这是一个更安全的说法，因为它涵盖了一个范围而不是一个单一值，因此考虑了不确定性。
- en: Train/Test Splits
  id: totrans-230
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练/测试分割
- en: This analysis I just did with the correlation coefficient, statistical significance,
    and coefficient of determination unfortunately is not always done by practitioners.
    Sometimes they are dealing with so much data they do not have the time or technical
    ability to do so. For example a 128 × 128 pixel image is at least 16,384 variables.
    Do you have time to do statistical analysis on each of those pixel variables?
    Probably not! Unfortunately, this leads many data scientists to not learn these
    statistical metrics at all.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 我刚刚进行的这个分析，包括相关系数、统计显著性和决定系数，不幸的是并不总是由从业者完成。有时候他们处理的数据太多，没有时间或技术能力这样做。例如，一个128×128像素的图像至少有16,384个变量。你有时间对每个像素变量进行统计分析吗？可能没有！不幸的是，这导致许多数据科学家根本不学习这些统计指标。
- en: In an [obscure online forum](http://disq.us/p/1jas3zg), I once read a post saying
    statistical regression is a scalpel, while machine learning is a chainsaw. When
    operating with massive amounts of data and variables, you cannot sift through
    all of that with a scalpel. You have to resort to a chainsaw and while you lose
    explainability and precision, you at least can scale to make broader predictions
    on more data. That being said, statistical concerns like sampling bias and overfitting
    do not go away. But there are a few practices that can be employed for quick validation.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个[不知名的在线论坛](http://disq.us/p/1jas3zg)上，我曾经看到一篇帖子说统计回归是手术刀，而机器学习是电锯。当处理大量数据和变量时，你无法用手术刀筛选所有这些。你必须求助于电锯，虽然你会失去可解释性和精度，但至少可以扩展到更多数据上进行更广泛的预测。话虽如此，抽样偏差和过拟合等统计问题并没有消失。但有一些实践方法可以用于快速验证。
- en: Why Are There No Confidence Intervals and P-Values in scikit-learn?
  id: totrans-233
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么 scikit-learn 中没有置信区间和P值？
- en: Scikit-learn does not support confidence intervals and P-values, as these two
    techniques are open problems for higher-dimensional data. This only emphasizes
    the gap between statisticians and machine learning practitioners. As one of the
    maintainers of scikit-learn, Gael Varoquaux, said, “In general computing correct
    p-values requires assumptions on the data that are not met by the data used in
    machine learning (no multicollinearity, enough data compared to the dimensionality)….P-values
    are something that are expected to be well checked (they are a guard in medical
    research). Implementing them is asking for trouble….We can give p-values only
    in very narrow settings [with few variables].”
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-learn 不支持置信区间和P值，因为这两种技术对于高维数据是一个悬而未决的问题。这只强调了统计学家和机器学习从业者之间的差距。正如 scikit-learn
    的一位维护者 Gael Varoquaux 所说，“通常计算正确的P值需要对数据做出假设，而这些假设并不符合机器学习中使用的数据（没有多重共线性，与维度相比有足够的数据）....P值是一种期望得到很好检查的东西（在医学研究中是一种保护）。实现它们会带来麻烦....我们只能在非常狭窄的情况下给出P值[有少量变量]。”
- en: 'If you want to go down the rabbit hole, there are some interesting discussions
    on GitHub:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想深入了解，GitHub 上有一些有趣的讨论：
- en: '[*https://github.com/scikit-learn/scikit-learn/issues/6773*](https://github.com/scikit-learn/scikit-learn/issues/6773)'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*https://github.com/scikit-learn/scikit-learn/issues/6773*](https://github.com/scikit-learn/scikit-learn/issues/6773)'
- en: '[*https://github.com/scikit-learn/scikit-learn/issues/16802*](https://github.com/scikit-learn/scikit-learn/issues/16802)'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*https://github.com/scikit-learn/scikit-learn/issues/16802*](https://github.com/scikit-learn/scikit-learn/issues/16802)'
- en: As mentioned before, [statsmodel](https://oreil.ly/8oEHo) is a library that
    provides helpful tools for statistical analysis. Just know it will likely not
    scale to larger-dimensional models because of the aforementioned reasons.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，[statsmodel](https://oreil.ly/8oEHo) 是一个为统计分析提供有用工具的库。只是要知道，由于前述原因，它可能不会适用于更大维度的模型。
- en: A basic technique machine learning practitioners use to mitigate overfitting
    is a practice called the *train/test split*, where typically 1/3 of the data is
    set aside for testing and the other 2/3 is used for training (other ratios can
    be used as well). The *training dataset* is used to fit the linear regression,
    while the *testing dataset* is used to measure the linear regression’s performance
    on data it has not seen before. This technique is generally used for all supervised
    machine learning, including logistic regression and neural networks. [Figure 5-18](#bpLCiplUVe)
    shows a visualization of how we break up our data into 2/3 for training and 1/3
    for testing.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习从业者用于减少过拟合的基本技术之一是一种称为*训练/测试分割*的实践，通常将1/3的数据用于测试，另外的2/3用于训练（也可以使用其他比例）。*训练数据集*用于拟合线性回归，而*测试数据集*用于衡量线性回归在之前未见数据上的表现。这种技术通常用于所有监督学习，包括逻辑回归和神经网络。[图 5-18](#bpLCiplUVe)
    显示了我们如何将数据分割为2/3用于训练和1/3用于测试的可视化。
- en: This Is a Small Dataset
  id: totrans-240
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 这是一个小数据集
- en: As we will learn later, there are other ways to split a training/testing dataset
    than 2/3 and 1/3\. If you have a dataset this small, you will probably be better
    off with 9/10 and 1/10 paired with cross-validation, or even just leave-one-out
    cross-validation. See [“Do Train/Test Splits Have to be Thirds?”](#Do_Training_Test_Splits)
    to learn more.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们将在后面学到的，有其他方法可以将训练/测试数据集分割为2/3和1/3。如果你有一个这么小的数据集，你可能最好使用9/10和1/10与交叉验证配对，或者甚至只使用留一交叉验证。查看[“训练/测试分割是否必须是三分之一？”](#Do_Training_Test_Splits)
    以了解更多。
- en: '![emds 0518](Images/emds_0518.png)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![emds 0518](Images/emds_0518.png)'
- en: Figure 5-18\. Splitting into training/testing data—the line is fitted to the
    training data (dark blue) using least squares, while the testing data (light red)
    is analyzed afterward to see how off the predictions are on data that was not
    seen before
  id: totrans-243
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-18\. 将数据分割为训练/测试数据—使用最小二乘法将线拟合到训练数据（深蓝色），然后分析测试数据（浅红色）以查看预测在之前未见数据上的偏差
- en: '[Example 5-21](#OivsevRKQr) shows how to perform a train/test split using scikit-learn,
    where 1/3 of the data is set aside for testing and the other 2/3 is used for training.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例 5-21](#OivsevRKQr) 展示了如何使用scikit-learn执行训练/测试分割，其中1/3的数据用于测试，另外的2/3用于训练。'
- en: Training Is Fitting a Regression
  id: totrans-245
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练即拟合回归
- en: Remember that “fitting” a regression is synonymous with “training.” The latter
    word is used by machine learning practitioners.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，“拟合”回归与“训练”是同义词。后者是机器学习从业者使用的词语。
- en: Example 5-21\. Doing a train/test split on linear regression
  id: totrans-247
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例5-21\. 在线性回归上进行训练/测试分割
- en: '[PRE19]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Notice that the `train_test_split()` will take our dataset (*X* and *Y* columns),
    shuffle it, and then return our training and testing datasets based on our testing-dataset
    size. We use the `LinearRegression`’s `fit()` function to fit on the training
    datasets `X_train` and `Y_train`. Then we use the `score()` function on the testing
    datasets `X_test` and `Y_test` to evaluate the <math alttext="r squared"><msup><mi>r</mi>
    <mn>2</mn></msup></math> , giving us a sense how the regression performs on data
    it has not seen before. The higher the <math alttext="r squared"><msup><mi>r</mi>
    <mn>2</mn></msup></math> is for our testing dataset, the better. Having that higher
    number indicates the regression performs well on data it has not seen before.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，`train_test_split()`将获取我们的数据集（*X* 和 *Y* 列），对其进行洗牌，然后根据我们的测试数据集大小返回我们的训练和测试数据集。我们使用`LinearRegression`的`fit()`函数来拟合训练数据集`X_train`和`Y_train`。然后我们使用`score()`函数在测试数据集`X_test`和`Y_test`上评估<math
    alttext="r squared"><msup><mi>r</mi> <mn>2</mn></msup></math>，从而让我们了解回归在之前未见过的数据上的表现。测试数据集的<math
    alttext="r squared"><msup><mi>r</mi> <mn>2</mn></msup></math>值越高，表示回归在之前未见过的数据上表现越好。具有更高数值的数字表示回归在之前未见过的数据上表现良好。
- en: We can also alternate the testing dataset across each 1/3 fold. This is known
    as *cross-validation* and is often considered the gold standard of validation
    techniques. [Figure 5-20](#HgfhjgkCrp) shows how each 1/3 of the data takes a
    turn being the testing dataset.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以在每个1/3折叠中交替使用测试数据集。这被称为*交叉验证*，通常被认为是验证技术的黄金标准。[图 5-20](#HgfhjgkCrp) 显示了数据的每个1/3轮流成为测试数据集。
- en: '![emds 0520](Images/emds_0520.png)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: '![emds 0520](Images/emds_0520.png)'
- en: Figure 5-20\. A visualization of cross-validation with three folds
  id: totrans-252
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-20\. 三折交叉验证的可视化
- en: The code in [Example 5-22](#kgNpJaUubi) shows a cross-validation performed across
    three folds, and then the scoring metric (in this case the mean sum of squares
    [MSE]) is averaged alongside its standard deviation to show how consistently each
    test performed.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例 5-22](#kgNpJaUubi) 中的代码展示了跨三个折叠进行的交叉验证，然后评分指标（在本例中为均方和 [MSE]）与其标准偏差一起平均，以展示每个测试的一致性表现。'
- en: Example 5-22\. Using three-fold cross-validation for a linear regression
  id: totrans-254
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 5-22\. 使用三折交叉验证进行线性回归
- en: '[PRE20]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: When you get concerned about variance in your model, one thing you can do, rather
    than a simple train/test split or cross-validation, is use *random-fold validation*
    to repeatedly shuffle and train/test split your data an unlimited number of times
    and aggregate the testing results. In [Example 5-23](#vIhFvqDmvc) there are 10
    iterations of randomly sampling 1/3 of the data for testing and the other 2/3
    for training. Those 10 testing results are then averaged alongside their standard
    deviations to see how consistently the testing datasets perform.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 当你开始关注模型中的方差时，你可以采用*随机折叠验证*，而不是简单的训练/测试拆分或交叉验证，重复地对数据进行洗牌和训练/测试拆分无限次，并汇总测试结果。在[示例
    5-23](#vIhFvqDmvc) 中，有 10 次随机抽取数据的 1/3 进行测试，其余 2/3 进行训练。然后将这 10 个测试结果与它们的标准偏差平均，以查看测试数据集的表现一致性。
- en: What’s the catch? It’s computationally very expensive as we are training the
    regression many times.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 有什么问题？这在计算上非常昂贵，因为我们要多次训练回归。
- en: Example 5-23\. Using a random-fold validation for a linear regression
  id: totrans-258
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 5-23\. 使用随机折叠验证进行线性回归
- en: '[PRE21]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: So when you are crunched for time or your data is too voluminous to statistically
    analyze, a train/test split is going to provide a way to measure how well your
    linear regression will perform on data it has not seen before.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，当你时间紧迫或数据量过大无法进行统计分析时，训练/测试拆分将提供一种衡量线性回归在未见过的数据上表现如何的方法。
- en: Train/Test Splits Are Not Guarantees
  id: totrans-261
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练/测试拆分并不保证结果
- en: It is important to note that just because you apply machine learning best practices
    of splitting your training and testing data, it does not mean your model is going
    to perform well. You can easily overtune your model and p-hack your way into a
    good test result, only to find it does not work well out in the real world. This
    is why holding out another dataset called the *validation set* is sometimes necessary,
    especially if you are comparing different models or configurations. That way,
    your tweaks on the training data to get better performance on the testing data
    do not leak info into the training. You can use the validation dataset as one
    last stopgap to see if p-hacking caused you to overfit to your testing dataset.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，仅仅因为你应用了机器学习的最佳实践，将训练和测试数据拆分，这并不意味着你的模型会表现良好。你很容易过度调整模型，并通过一些手段获得良好的测试结果，但最终发现在现实世界中并不奏效。这就是为什么有时候需要保留另一个数据集，称为*验证集*，特别是当你在比较不同模型或配置时。这样，你对训练数据的调整以获得更好的测试数据性能不会泄漏信息到训练中。你可以使用验证数据集作为最后一道防线，查看是否过度调整导致你对测试数据过拟合。
- en: Even then, your whole dataset (including training, testing, and validation)
    could have been biased to begin with, and no split is going to mitigate that.
    Andrew Ng discussed this as a large problem with machine learning during his [Q&A
    with DeepLearning.AI and Stanford HAI](https://oreil.ly/x23SJ). He walked through
    an example showing why machine learning has not replaced radiologists.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 即使如此，你的整个数据集（包括训练、测试和验证）可能一开始就存在偏差，没有任何拆分可以减轻这种情况。Andrew Ng 在他与 [DeepLearning.AI
    和 Stanford HAI 的问答环节](https://oreil.ly/x23SJ) 中讨论了这个问题，他通过一个例子说明了为什么机器学习尚未取代放射科医生。
- en: Multiple Linear Regression
  id: totrans-264
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多元线性回归
- en: We put almost exclusive focus on doing linear regression on one input variable
    and one output variable in this chapter. However, the concepts we learned here
    should largely apply to multivariable linear regression. Metrics like <math alttext="r
    squared"><msup><mi>r</mi> <mn>2</mn></msup></math> , standard error, and confidence
    intervals can be used but it gets harder with more variables. [Example 5-24](#cICPINJumN)
    is an example of a linear regression with two input variables and one output variable
    using scikit-learn.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们几乎完全专注于对一个输入变量和一个输出变量进行线性回归。然而，我们在这里学到的概念应该基本适用于多变量线性回归。像<math alttext="r
    squared"><msup><mi>r</mi> <mn>2</mn></msup></math>、标准误差和置信区间等指标可以使用，但随着变量的增加变得更加困难。[示例
    5-24](#cICPINJumN) 是一个使用 scikit-learn 进行的具有两个输入变量和一个输出变量的线性回归示例。
- en: Example 5-24\. A linear regression with two input variables
  id: totrans-266
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 5-24。具有两个输入变量的线性回归
- en: '[PRE22]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: There is a degree of precariousness when a model becomes so inundated with variables
    it starts to lose explainability, and this is when machine learning practices
    start to come in and treat the model as as black box. I hope that you are convinced
    statistical concerns do not go away, and data becomes increasingly sparse the
    more variables you add. But if you step back and analyze the relationships between
    each pair of variables using a correlation matrix, and seek understanding on how
    each pair of variables interact, it will help your efforts to create a productive
    machine learning model.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 当模型变得充斥着变量以至于开始失去可解释性时，就会出现一定程度的不稳定性，这时机器学习实践开始将模型视为黑匣子。希望你相信统计问题并没有消失，随着添加的变量越来越多，数据变得越来越稀疏。但是，如果你退后一步，使用相关矩阵分析每对变量之间的关系，并寻求理解每对变量是如何相互作用的，这将有助于你努力创建一个高效的机器学习模型。
- en: Conclusion
  id: totrans-269
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: We covered a lot in this chapter. We attempted to go beyond a cursory understanding
    of linear regression and making train/test splits our only validation. I wanted
    to show you both the scalpel (statistics) and the chainsaw (machine learning)
    so you can judge which is best for a given problem you encounter. There are a
    lot of metrics and analysis methods available in linear regression alone, and
    we covered a number of them to understand whether a linear regression is reliable
    for predictions. You may find yourself in a position to do regressions as broad
    approximations or meticulously analyze and comb your data using statistical tools.
    Which approach you use is situational, and if you want to learn more about statistical
    tools available for Python, check out the [statsmodel library](https://oreil.ly/8oEHo).
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中我们涵盖了很多内容。我们试图超越对线性回归的肤浅理解，并且不仅仅将训练/测试分割作为我们唯一的验证方式。我想向你展示割刀（统计学）和电锯（机器学习），这样你可以判断哪种对于你遇到的问题更好。仅在线性回归中就有许多指标和分析方法可用，我们涵盖了其中一些以了解线性回归是否可靠用于预测。你可能会发现自己处于一种情况，要么做出广泛的近似回归，要么使用统计工具仔细分析和整理数据。你使用哪种方法取决于情况，如果你想了解更多关于Python可用的统计工具，请查看[statsmodel库](https://oreil.ly/8oEHo)。
- en: In [Chapter 6](ch06.xhtml#ch06) covering logistic regression, we will revisit
    the <math alttext="r squared"><msup><mi>r</mi> <mn>2</mn></msup></math> and statistical
    significance. I hope that this chapter convinced you there are ways to analyze
    data meaningfully, and the investment can make the difference in a successful
    project.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第6章](ch06.xhtml#ch06)中涵盖逻辑回归时，我们将重新审视<math alttext="r squared"><msup><mi>r</mi>
    <mn>2</mn></msup></math>和统计显著性。希望这一章能让你相信有方法可以有意义地分析数据，并且这种投资可以在成功的项目中产生差异。
- en: Exercises
  id: totrans-272
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习
- en: A dataset of two variables, *x* and *y*, is provided [here](https://bit.ly/3C8JzrM).
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 提供了一个包含两个变量*x*和*y*的数据集[这里](https://bit.ly/3C8JzrM)。
- en: Perform a simple linear regression to find the *m* and *b* values that minimizes
    the loss (sum of squares).
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行简单的线性回归，找到最小化损失（平方和）的*m*和*b*值。
- en: Calculate the correlation coefficient and statistical significance of this data
    (at 95% confidence). Is the correlation useful?
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算这些数据的相关系数和统计显著性（95%置信度）。相关性是否有用？
- en: If I predict where *x* = 50, what is the 95% prediction interval for the predicted
    value of *y*?
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我预测*x*=50，那么*y*的预测值的95%预测区间是多少？
- en: Start your regression over and do a train/test split. Feel free to experiment
    with cross-validation and random-fold validation. Does the linear regression perform
    well and consistently on the testing data? Why or why not?
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重新开始你的回归，并进行训练/测试分割。随意尝试交叉验证和随机折叠验证。线性回归在测试数据上表现良好且一致吗？为什么？
- en: Answers are in [Appendix B](app02.xhtml#exercise_answers).
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 答案在[附录B](app02.xhtml#exercise_answers)中。
