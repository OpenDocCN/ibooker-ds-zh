- en: Chapter 5\. Linear Regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the most practical techniques in data analysis is fitting a line through
    observed data points to show a relationship between two or more variables. A *regression*
    attempts to fit a function to observed data to make predictions on new data. A
    *linear regression* fits a straight line to observed data, attempting to demonstrate
    a linear relationship between variables and make predictions on new data yet to
    be observed.
  prefs: []
  type: TYPE_NORMAL
- en: It might make more sense to see a picture rather than read a description of
    linear regression. There is an example of a linear regression in [Figure 5-1](#kVAPNnFMvc).
  prefs: []
  type: TYPE_NORMAL
- en: Linear regression is a workhorse of data science and statistics and not only
    applies concepts we learned in previous chapters but sets up new foundations for
    later topics like neural networks ([Chapter 7](ch07.xhtml#ch07)) and logistic
    regression ([Chapter 6](ch06.xhtml#ch06)). This relatively simple technique has
    been around for more than two hundred years and contemporarily is branded as a
    form of machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning practitioners often take a different approach to validation,
    starting with a train-test split of the data. Statisticians are more likely to
    use metrics like prediction intervals and correlation for statistical significance.
    We will cover both schools of thought so readers can bridge the ever-widening
    gap between the two disciplines, and thus find themselves best equipped to wear
    both hats.
  prefs: []
  type: TYPE_NORMAL
- en: '![emds 0501](Images/emds_0501.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-1\. Example of a linear regression, which fits a line to observed data
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: A Basic Linear Regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I want to study the relationship between the age of a dog and the number of
    veterinary visits it had. In a fabricated sample we have 10 random dogs. I am
    a fan of understanding complex techniques with simple datasets (real or otherwise),
    so we understand the strengths and limitations of the technique without complex
    data muddying the water. Let’s plot this dataset as shown in [Figure 5-2](#UiqIepMEBG).
  prefs: []
  type: TYPE_NORMAL
- en: '![emds 0502](Images/emds_0502.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-2\. Plotting a sample of 10 dogs with their age and number of vet visits
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We can clearly see there is a *linear correlation* here, meaning when one of
    these variables increases/decreases, the other increases/decreases in a roughly
    proportional amount. We could draw a line through these points to show a correlation
    like this in [Figure 5-3](#KsuHHebEQo).
  prefs: []
  type: TYPE_NORMAL
- en: '![emds 0503](Images/emds_0503.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-3\. Fitting a line through our data
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: I will show how to calculate this fitted line later in this chapter. We will
    also explore how to calculate the quality of this fitted line. For now, let’s
    focus on the benefits of performing a linear regression. It allows us to make
    predictions on data we have not seen before. I do not have a dog in my sample
    that is 8.5 years old, but I can look at this line and estimate the dog will have
    21 veterinary visits in its life. I just look at the line where *x* = 8.5 and
    I see that *y* = 21.218 as shown in [Figure 5-4](#pWiNSOLfvI). Another benefit
    is we can analyze variables for possible relationships and hypothesize that correlated
    variables are causal to one another.
  prefs: []
  type: TYPE_NORMAL
- en: Now what are the downsides of a linear regression? I cannot expect that every
    outcome is going to fall *exactly* on that line. After all, real-world data is
    noisy and never perfect and will not follow a straight line. It may not remotely
    follow a straight line at all! There is going to be error around that line, where
    the point will fall above or below the line. We will cover this mathematically
    when we talk about p-values, statistical significance, and prediction intervals,
    which describes how reliable our linear regression is. Another catch is we should
    not use the linear regression to make predictions outside the range of data we
    have, meaning we should not make predictions where *x* < 0 and *x* > 10 because
    we do not have data outside those values.
  prefs: []
  type: TYPE_NORMAL
- en: '![emds 0504](Images/emds_0504.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-4\. Making a prediction using a linear regression, seeing that an 8.5-year-old
    dog is predicted to have about 21.2 vet visits
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Don’t Forget Sampling Bias!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We should question this data and how it was sampled to detect bias. Was this
    at a single veterinary clinic? Multiple random clinics? Is there self-selection
    bias by using veterinary data, only polling dogs that visit the vet? If the dogs
    were sampled in the same geography, can that sway the data? Perhaps dogs in hot
    desert climates go to vets more for heat exhaustion and snake bites, and this
    would inflate our veterinary visits in our sample.
  prefs: []
  type: TYPE_NORMAL
- en: As discussed in [Chapter 3](ch03.xhtml#ch03), it has become fashionable to make
    data an oracle for truth. However data is simply a sample from a population, and
    we need to practice discernment on how well represented our sample is. Be just
    as interested (if not more) in where the data comes from and not just what the
    data says.
  prefs: []
  type: TYPE_NORMAL
- en: Basic Linear Regression with SciPy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We have a lot to learn regarding linear regression in this chapter, but let’s
    start out with some code to execute what we know so far.
  prefs: []
  type: TYPE_NORMAL
- en: There are plenty of platforms to perform a linear regression, from Excel to
    Python and R. But we will stick with Python in this book, starting with scikit-learn
    to do the work for us. I will show how to build a linear regression “from scratch”
    later in this chapter so we grasp important concepts like gradient descent and
    least squares.
  prefs: []
  type: TYPE_NORMAL
- en: '[Example 5-1](#bhaTvtcODA) is how we use scikit-learn to perform a basic, unvalidated
    linear regression on the sample of 10 dogs. We pull in [this data using Pandas](https://oreil.ly/xCvwR),
    convert it into NumPy arrays, perform linear regression using scikit-learn, and
    use Plotly to display it in a chart.'
  prefs: []
  type: TYPE_NORMAL
- en: Example 5-1\. Using scikit-learn to do a linear regression
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: First we import the data from [this CSV on GitHub](https://bit.ly/3cIH97A).
    We separate the two columns into *X* and *Y* datasets using Pandas. We then `fit()`
    the `LinearRegression` model to the input *X* data and the output *Y* data. We
    can then get the *m* and *b* coefficients that describe our fitted linear function.
  prefs: []
  type: TYPE_NORMAL
- en: In the plot, sure enough you will get a fitted line running through these points
    shown in [Figure 5-5](#psauNoJchU).
  prefs: []
  type: TYPE_NORMAL
- en: '![emds 0505](Images/emds_0505.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-5\. SciPy will fit a regression line to your data
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: What decides the best fit line to these points? Let’s discuss that next.
  prefs: []
  type: TYPE_NORMAL
- en: Residuals and Squared Errors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'How do statistics tools like scikit-learn come up with a line that fits to
    these points? It comes down to two questions that are fundamental to machine learning
    training:'
  prefs: []
  type: TYPE_NORMAL
- en: What defines a “best fit”?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do we get to that “best fit”?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The first question has a pretty established answer: we minimize the squares,
    or more specifically the sum of the squared residuals. Let’s break that down.
    Draw any line through the points. The *residual* is the numeric difference between
    the line and the points, as shown in [Figure 5-6](#TIOfQJflet).'
  prefs: []
  type: TYPE_NORMAL
- en: '![emds 0506](Images/emds_0506.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-6\. The residuals are the differences between the line and the points
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Points above the line will have a positive residual, and points below the line
    will have a negative residual. In other words, it is the subtracted difference
    between the predicted y-values (derived from the line) and the actual y-values
    (which came from the data). Another name for residuals are *errors*, because they
    reflect how wrong our line is in predicting the data.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s calculate these differences between these 10 points and the line *y* =
    1.93939*x* + 4.73333 in [Example 5-2](#ENfFANvoCk) and the residuals for each
    point in [Example 5-3](#PLhHARzaLr).
  prefs: []
  type: TYPE_NORMAL
- en: Example 5-2\. Calculating the residuals for a given line and data
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Example 5-3\. The residuals for each point
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: If we are fitting a straight line through our 10 data points, we likely want
    to minimize these residuals in total so there is the least gap possible between
    the line and points. But how do we measure the “total”? The best approach is to
    take the *sum of squares*, which simply squares each residual, or multiplies each
    residual by itself, and sums them. We take each actual y-value and subtract from
    it the predicted y-value taken from the line, then square and sum all those differences.
  prefs: []
  type: TYPE_NORMAL
- en: A visual way to think of it is shown in [Figure 5-7](#ODRNFmUtdw), where we
    overlay a square on each residual and each side is the length of the residual.
    We sum the area of all these squares, and later we will learn how to find the
    minimum sum we can achieve by identifying the best *m* and *b*.
  prefs: []
  type: TYPE_NORMAL
- en: '![emds 0507](Images/emds_0507.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-7\. Visualizing the sum of squares, which would be the sum of all areas
    where each square has a side length equal to the residual
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Let’s modify our code in [Example 5-4](#eJAucETrRq) to find the sum of squares.
  prefs: []
  type: TYPE_NORMAL
- en: Example 5-4\. Calculating the sum of squares for a given line and data
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Next question: how do we find the *m* and *b* values that will produce the
    minimum sum of squares, without using a library like scikit-learn? Let’s look
    at that next.'
  prefs: []
  type: TYPE_NORMAL
- en: Finding the Best Fit Line
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We now have a way to measure the quality of a given line against the data points:
    the sum of squares. The lower we can make that number, the better the fit. Now
    how do we find the right *m* and *b* values that create the *least* sum of squares?'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a couple of search algorithms we can employ, which try to find the
    right set of values to solve a given problem. You can try a *brute force* approach,
    generating random *m* and *b* values millions of times and choosing the ones that
    produce the least sum of squares. This will not work well because it will take
    an endless amount of time to find even a decent approximation. We will need something
    a little more guided. I will curate five techniques you can use: closed form,
    matrix inversion, matrix decomposition, gradient descent, and stochastic gradient
    descent. There are other search algorithms like hill climbing that could be used
    (and are covered in [Appendix A](app01.xhtml#appendix)), but we will stick with
    what’s common.'
  prefs: []
  type: TYPE_NORMAL
- en: Closed Form Equation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Some readers may ask if there is a formula (called a *closed form equation*)
    to fit a linear regression by exact calculation. The answer is yes, but only for
    a simple linear regression with one input variable. This luxury does not exist
    for many machine learning problems with several input variables and a large amount
    of data. We can use linear algebra techniques to scale up, and we will talk about
    this shortly. We will also take the opportunity to learn about search algorithms
    like stochastic gradient descent.
  prefs: []
  type: TYPE_NORMAL
- en: For a simple linear regression with only one input and one output variable,
    here are the closed form equations to calculate *m* and *b*. [Example 5-5](#SMKENihWel)
    shows how you can do these calculations in Python.
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mi>m</mi>
    <mo>=</mo> <mfrac><mrow><mi>n</mi><mo>∑</mo><mrow><mi>x</mi><mi>y</mi></mrow><mo>-</mo><mo>∑</mo><mi>x</mi><mo>∑</mo><mi>y</mi></mrow>
    <mrow><mi>n</mi><mo>∑</mo><msup><mi>x</mi> <mn>2</mn></msup> <mo>-</mo><msup><mrow><mo>(</mo><mo>∑</mo><mi>x</mi><mo>)</mo></mrow>
    <mn>2</mn></msup></mrow></mfrac></mrow></mtd></mtr></mtable></math> <math display="block"><mtable
    displaystyle="true"><mtr><mtd columnalign="right"><mrow><mi>b</mi> <mo>=</mo>
    <mfrac><mrow><mo>∑</mo><mi>y</mi></mrow> <mi>n</mi></mfrac> <mo>-</mo> <mi>m</mi>
    <mfrac><mrow><mo>∑</mo><mi>x</mi></mrow> <mi>n</mi></mfrac></mrow></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: Example 5-5\. Calculating m and b for a simple linear regression
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: These equations to calculate *m* and *b* are derived from calculus, and we will
    do some calculus work with SymPy later in this chapter if you have the itch to
    discover where formulas come from. For now, you can plug in the number of data
    points *n* as well as iterate the x- and y-values to do the operations just described.
  prefs: []
  type: TYPE_NORMAL
- en: Going forward, we will learn approaches that are more oriented to contemporary
    techniques that cope with larger amounts of data. Closed form equations tend not
    to scale well.
  prefs: []
  type: TYPE_NORMAL
- en: Computational Complexity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The reason the closed form equations do not scale well with larger datasets
    is due to a computer science concept called *computational complexity*, which
    measures how long an algorithm takes as a problem size grows. This might be worth
    getting familiar with; here are two great YouTube videos on the topic:'
  prefs: []
  type: TYPE_NORMAL
- en: '[“P vs. NP and the Computational Complexity Zoo”](https://oreil.ly/TzQBl)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[“What Is Big O Notation?”](https://oreil.ly/EjcSR)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inverse Matrix Techniques
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Going forward, I will sometimes alternate the coefficients *m* and *b* with
    different names, <math alttext="beta 1"><msub><mi>β</mi> <mn>1</mn></msub></math>
    and <math alttext="beta 0"><msub><mi>β</mi> <mn>0</mn></msub></math> , respectively.
    This is the convention you will see more often in the professional world, so it
    might be a good time to graduate.
  prefs: []
  type: TYPE_NORMAL
- en: While we dedicated an entire chapter to linear algebra in [Chapter 4](ch04.xhtml#ch04),
    applying it can be a bit overwhelming when you are new to math and data science.
    This is why most examples in this book will use plain Python or scikit-learn.
    However, I will sprinkle in linear algebra where it makes sense, just to show
    how linear algebra is useful. If you find this section overwhelming, feel free
    to move on to the rest of the chapter and come back later.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use transposed and inverse matrices, which we covered in [Chapter 4](ch04.xhtml#ch04),
    to fit a linear regression. Next, we calculate a vector of coefficients <math
    alttext="b"><mi>b</mi></math> given a matrix of input variable values <math alttext="upper
    X"><mi>X</mi></math> and a vector of output variable values <math alttext="y"><mi>y</mi></math>
    . Without going down a rabbit hole of calculus and linear algebra proofs, here
    is the formula:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="b equals left-parenthesis upper X Superscript upper T Baseline
    dot upper X right-parenthesis Superscript negative 1 Baseline dot upper X Superscript
    upper T Baseline dot y" display="block"><mrow><mi>b</mi> <mo>=</mo> <msup><mrow><mo>(</mo><msup><mi>X</mi>
    <mi>T</mi></msup> <mo>·</mo><mi>X</mi><mo>)</mo></mrow> <mrow><mo>-</mo><mn>1</mn></mrow></msup>
    <mo>·</mo> <msup><mi>X</mi> <mi>T</mi></msup> <mo>·</mo> <mi>y</mi></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: You will notice transposed and inverse operations are performed on the matrix
    <math alttext="upper X"><mi>X</mi></math> and combined with matrix multiplication.
    Here is how we perform this operation in NumPy in [Example 5-6](#sfoTRMoiPH) to
    get our coefficients *m* and *b*.
  prefs: []
  type: TYPE_NORMAL
- en: Example 5-6\. Using inverse and transposed matrices to fit a linear regression
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: It is not intuitive, but note we have to stack a “column” of 1s next to our
    *X* column. The reason is this will generate the intercept <math alttext="beta
    0"><msub><mi>β</mi> <mn>0</mn></msub></math> coefficient. Since this column is
    all 1s, it effectively generates the intercept and not just a slope <math alttext="beta
    1"><msub><mi>β</mi> <mn>1</mn></msub></math> .
  prefs: []
  type: TYPE_NORMAL
- en: 'When you have a lot of data with a lot of dimensions, computers can start to
    choke and produce unstable results. This is a use case for matrix decomposition,
    which we learned about in [Chapter 4](ch04.xhtml#ch04) on linear algebra. In this
    specific case, we take our matrix *X*, append an additional column of 1s to generate
    the intercept <math alttext="beta 0"><msub><mi>β</mi> <mn>0</mn></msub></math>
    just like before, and then decompose it into two component matrices *Q* and *R*:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="upper X equals upper Q dot upper R" display="block"><mrow><mi>X</mi>
    <mo>=</mo> <mi>Q</mi> <mo>·</mo> <mi>R</mi></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Avoiding more calculus rabbit holes, here is how we use *Q* and *R* to find
    the beta coefficient values in the matrix form *b*:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="b equals upper R Superscript negative 1 Baseline dot upper Q
    Superscript upper T Baseline dot y" display="block"><mrow><mi>b</mi> <mo>=</mo>
    <msup><mi>R</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup> <mo>·</mo> <msup><mi>Q</mi>
    <mi>T</mi></msup> <mo>·</mo> <mi>y</mi></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: And [Example 5-7](#wFWnrEMADG) shows how we use the preceding *QR* decomposition
    formula in Python using NumPy to perform a linear regression.
  prefs: []
  type: TYPE_NORMAL
- en: Example 5-7\. Using QR decomposition to perform a linear regression
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Typically, *QR* decomposition is the method used by many scientific libraries
    for linear regression because it copes with large amounts of data more easily
    and is more stable. What do I mean by *stable*? [*Numerical stability*](https://oreil.ly/A4BWJ)
    is how well an algorithm keeps errors minimized, rather than amplifying errors
    in approximations. Remember that computers work only to so many decimal places
    and have to approximate, so it becomes important our algorithms do not deteriorate
    with compounding errors in those approximations.
  prefs: []
  type: TYPE_NORMAL
- en: Overwhelmed?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you find these linear algebra examples of linear regression overwhelming,
    do not worry! I just wanted to provide exposure to a practical use case for linear
    algebra. Going forward, we will focus on other techniques you can use.
  prefs: []
  type: TYPE_NORMAL
- en: Gradient Descent
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Gradient descent* is an optimization technique that uses derivatives and iterations
    to minimize/maximize a set of parameters against an objective. To learn about
    gradient descent, let’s do a quick thought experiment and then apply it on a simple
    example.'
  prefs: []
  type: TYPE_NORMAL
- en: A Thought Experiment on Gradient Descent
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Imagine you are in a mountain range at night and given a flashlight. You are
    trying to get to the lowest point of the mountain range. You can see the slope
    around you before you even take a step. You step in directions where the slope
    visibly goes downward. You take bigger steps for bigger slopes, and smaller steps
    for smaller slopes. Ultimately, you will find yourself at a low point where the
    slope is flat, a value of 0\. Sounds pretty good, right? This approach with the
    flashlight is known as *gradient descent*, where we step in directions where the
    slope goes downward.
  prefs: []
  type: TYPE_NORMAL
- en: 'In machine learning, we often think of all possible sum of square losses we
    will encounter with different parameters as a mountainous landscape. We want to
    minimize our loss, and we navigate the loss landscape to do it. To solve this
    problem, gradient descent has an attractive feature: the partial derivative is
    that flashlight, allowing us to see the slopes for every parameter (in this case
    *m* and *b*, or <math alttext="beta 0"><msub><mi>β</mi> <mn>0</mn></msub></math>
    and <math alttext="beta 1"><msub><mi>β</mi> <mn>1</mn></msub></math> ). We step
    in directions for *m* and *b* where the slope goes downward. We take bigger steps
    for bigger slopes and smaller steps for smaller slopes. We can simply calculate
    the length of this step by taking a fraction of the slope. This fraction is known
    as our *learning rate*. The higher the learning rate, the faster it will run at
    the cost of accuracy. But the lower the learning rate, the longer it will take
    to train and require more iterations.'
  prefs: []
  type: TYPE_NORMAL
- en: Deciding a learning rate is like choosing between an ant, a human, or a giant
    to step down the slope. An ant (small learning rate) will take tiny steps and
    take an unacceptably long time to get to the bottom but will do so precisely.
    A giant (large learning rate) may keep stepping over the minimum to the point
    he may never reach it no matter how many steps he takes. The human (moderate learning
    rate) probably has most balanced step size, having the right trade between speed
    and accuracy in arriving at the minimum.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s Walk Before We Run
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For the function <math alttext="f left-parenthesis x right-parenthesis equals
    left-parenthesis x minus 3 right-parenthesis squared plus 4"><mrow><mi>f</mi>
    <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>=</mo> <msup><mrow><mo>(</mo><mi>x</mi><mo>-</mo><mn>3</mn><mo>)</mo></mrow>
    <mn>2</mn></msup> <mo>+</mo> <mn>4</mn></mrow></math> , let’s find the x-value
    that produces the lowest point of that function. While we could solve this algebraically,
    let’s use gradient descent to do it.
  prefs: []
  type: TYPE_NORMAL
- en: Here is visually what we are trying to do. As shown in [Figure 5-8](#PbWEqDgfnq),
    we want to “step” *x* toward the minimum where the slope is 0.
  prefs: []
  type: TYPE_NORMAL
- en: '![emds 0508](Images/emds_0508.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-8\. Stepping toward the local minimum where the slope approaches 0
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In [Example 5-8](#dOSpRhPBkR), the function `f(x)` and its derivative with respect
    to *x* is `dx_f(x)`. Recall we covered in [Chapter 1](ch01.xhtml#ch01) how to
    use SymPy to calculate derivatives. After finding the derivative, we then proceed
    to perform gradient descent.
  prefs: []
  type: TYPE_NORMAL
- en: Example 5-8\. Using gradient descent to find the minimum of a parabola
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: If we graph the function (as shown in [Figure 5-8](#PbWEqDgfnq)), we should
    see the lowest point of the function is clearly where *x* = 3, and the preceding
    code should get very close to that. The learning rate is used to take a fraction
    of the slope and subtract it from the x-value on each iteration. Bigger slopes
    will result in bigger steps, and smaller slopes will result in smaller steps.
    After enough iterations, *x* will end up at the lowest point of the function (or
    close enough to it) where the slope is 0.
  prefs: []
  type: TYPE_NORMAL
- en: Gradient Descent and Linear Regression
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You now might be wondering how we use this for linear regression. Well, it’s
    the same idea except our “variables” are *m* and *b* (or <math alttext="beta 0"><msub><mi>β</mi>
    <mn>0</mn></msub></math> and <math alttext="beta 1"><msub><mi>β</mi> <mn>1</mn></msub></math>
    ) rather than *x*. Here’s why: in a simple linear regression we already know the
    x- and y-values because those are provided as the training data. The “variables”
    we need to solve are actually the parameters *m* and *b*, so we can find the best
    fit line that will then accept an *x* variable to predict a new y-value.'
  prefs: []
  type: TYPE_NORMAL
- en: How do we calculate the slopes for *m* and *b*? We need the partial derivatives
    for each of these. What function are we taking the derivative of? Remember we
    are trying to minimize loss and that will be the sum of squares. So we need to
    find the derivatives of our sum of squares function with respect to *m* and *b*.
  prefs: []
  type: TYPE_NORMAL
- en: 'I implement these two partial derivatives for *m* and *b* as shown in [Example 5-9](#SnshvwHMPb).
    We will learn how to do this shortly in SymPy. I then perform gradient descent
    to find *m* and *b*: 100,000 iterations with a learning rate of .001 will be sufficient.
    Note that the smaller you make that learning rate, the slower it will be and the
    more iterations you will need. But if you make it too high, it will run fast but
    have a poor approximation. When someone says a machine learning algorithm is “learning”
    or “training,” it really is just fitting a regression like this.'
  prefs: []
  type: TYPE_NORMAL
- en: Example 5-9\. Performing gradient descent for a linear regression
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Well, not bad! That approximation got close to our closed form equation solution.
    But what’s the catch? Just because we found the “best fit line” by minimizing
    sum of squares, that does not mean our linear regression is any good. Does minimizing
    the sum of squares guarantee a great model to make predictions? Not exactly. Now
    that I showed you how to fit a linear regression, let’s take a step back, revisit
    the big picture, and determine whether a given linear regression is the right
    way to make predictions in the first place. But before we do that, here’s one
    more detour showing the SymPy solution.
  prefs: []
  type: TYPE_NORMAL
- en: Gradient Descent for Linear Regression Using SymPy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you want the SymPy code that came up with these two derivatives for the sum
    of squares function, for *m* and *b* respectively, here is the code in [Example 5-10](#mliLfkeFPI).
  prefs: []
  type: TYPE_NORMAL
- en: Example 5-10\. Calculating partial derivatives for m and b
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: You will see the two derivatives for *m* and *b*, respectively, printed. Note
    the `Sum()` function will iterate and add items together (in this case all the
    data points), and we treat *x* and *y* as functions that look up a value for a
    given point at index *i*.
  prefs: []
  type: TYPE_NORMAL
- en: 'In mathematical notation, where *e*(*x*) represents the sum of squares loss
    function, here are the partial derivatives for *m* and *b*:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mi>e</mi>
    <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>=</mo> <munderover><mo>∑</mo>
    <mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow> <mi>n</mi></munderover> <mrow><mo>(</mo>
    <mrow><mo>(</mo> <mi>m</mi> <msub><mi>x</mi> <mi>i</mi></msub> <mo>+</mo> <mi>b</mi>
    <mo>)</mo></mrow> <mo>-</mo> <msub><mi>y</mi> <mi>i</mi></msub></mrow> <msup><mrow><mo>)</mo></mrow>
    <mn>2</mn></msup></mrow></mtd></mtr></mtable></math> <math display="block"><mtable
    displaystyle="true"><mtr><mtd columnalign="right"><mrow><mfrac><mi>d</mi> <mrow><mi>d</mi><mi>m</mi></mrow></mfrac>
    <mi>e</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>=</mo> <munderover><mo>∑</mo>
    <mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow> <mi>n</mi></munderover> <mrow><mn>2</mn>
    <mrow><mo>(</mo> <mi>b</mi> <mo>+</mo> <mi>m</mi> <msub><mi>x</mi> <mi>i</mi></msub>
    <mo>-</mo> <msub><mi>y</mi> <mi>i</mi></msub> <mo>)</mo></mrow> <msub><mi>x</mi>
    <mi>i</mi></msub></mrow></mrow></mtd></mtr></mtable></math> <math display="block"><mtable
    displaystyle="true"><mtr><mtd columnalign="right"><mrow><mfrac><mi>d</mi> <mrow><mi>d</mi><mi>b</mi></mrow></mfrac>
    <mi>e</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>=</mo> <munderover><mo>∑</mo>
    <mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow> <mi>n</mi></munderover> <mrow><mo>(</mo>
    <mn>2</mn> <mi>b</mi> <mo>+</mo> <mn>2</mn> <mi>m</mi> <msub><mi>x</mi> <mi>i</mi></msub>
    <mo>-</mo> <mn>2</mn> <msub><mi>y</mi> <mi>i</mi></msub> <mo>)</mo></mrow></mrow></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: If you want to apply our dataset and execute a linear regression using gradient
    descent, you will have to perform a few additional steps as shown in [Example 5-11](#akiLhwlcJj).
    We will need to substitute for the `n`, `x(i)` and `y(i)` values, iterating all
    of our data points for the `d_m` and `d_b` derivative functions. That should leave
    only the `m` and `b` variables, which we will search for the optimal values using
    gradient descent.
  prefs: []
  type: TYPE_NORMAL
- en: Example 5-11\. Solving linear regression using SymP
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: As shown in [Example 5-11](#akiLhwlcJj), it is a good idea to call `lambdify()`
    on both of our partial derivative functions to convert them from SymPy to an optimized
    Python function. This will cause computations to perform much more quickly when
    we do gradient descent. The resulting Python functions are backed by NumPy, SciPy,
    or whatever numerical libraries SymPy detects you have available. After that,
    we can perform gradient descent.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, if you are curious about what the loss function looks like for this
    simple linear regression, [Example 5-12](#hbwp3lnb3m) shows the SymPy code that
    plugs the `x`, `y`, and `n` values into our loss function and then plots `m` and
    `b` as the input variables. Our gradient descent algorithm gets us to the lowest
    point in this loss landscape shown in [Figure 5-9](#fHieSkPkkg).
  prefs: []
  type: TYPE_NORMAL
- en: Example 5-12\. Plotting the loss function for linear regression
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '![emds 0509](Images/emds_0509.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-9\. The loss landscape for a simple linear regression
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Overfitting and Variance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Riddle me this: if we truly wanted to minimize loss, as in reduce the sum of
    squares to 0, what would we do? Are there options other than linear regression?
    One conclusion you may arrive at is simply fit a curve that touches all the points.
    Heck, why not just connect the points in segments and use that to make predictions
    as shown in [Figure 5-10](#qHprwhCkIQ)? That gives us a loss of 0!'
  prefs: []
  type: TYPE_NORMAL
- en: Shoot, why did we go through all that trouble with linear regression and not
    do this instead? Well, remember our big-picture objective is not to minimize the
    sum of squares but to make accurate predictions on new data. This connect-the-dots
    model is severely *overfit*, meaning it shaped the regression to the training
    data too exactly to the point it will predict poorly on new data. This simple
    connect-the-dots model is sensitive to outliers that are far away from the rest
    of the points, meaning it will have high *variance* in predictions. While the
    points in this example are relatively close to a line, this problem will be a
    lot worse with other datasets with more spread and outliers. Because overfitting
    increases variance, predictions are going to be all over the place!
  prefs: []
  type: TYPE_NORMAL
- en: '![emds 0510](Images/emds_0510.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-10\. Performing a regression by simply connecting the points, resulting
    in zero loss
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Overfitting Is Memorization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When you hear someone say a regression “memorized” the data rather than generalizing
    it, they are talking about overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: As you can guess, we want to find effective generalizations in our model rather
    than memorize data. Otherwise, our regression simply turns into a database where
    we look up values.
  prefs: []
  type: TYPE_NORMAL
- en: This is why in machine learning you will find bias is added to the model, and
    linear regression is considered a highly biased model. This is not the same as
    bias in the data, which we talked about extensively in [Chapter 3](ch03.xhtml#ch03).
    *Bias in a model* means we prioritize a method (e.g., maintaining a straight line)
    as opposed to bending and fitting to exactly what the data says. A biased model
    leaves some wiggle room hoping to minimize loss on new data for better predictions,
    as opposed to minimizing loss on data it was trained on. I guess you could say
    adding bias to a model counteracts overfitting with *underfitting*, or fitting
    less to the training data.
  prefs: []
  type: TYPE_NORMAL
- en: As you can imagine, this is a balancing act because it is two contradictory
    objectives. In machine learning, we basically are saying, “I want to fit a regression
    to my data, but I don’t want to fit it *too much*. I need some wiggle room for
    predictions on new data that will be different.”
  prefs: []
  type: TYPE_NORMAL
- en: Lasso and Ridge Regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Two somewhat popular variants of linear regression are lasso regression and
    ridge regression. Ridge regression adds a further bias to a linear regression
    in the form of a penalty, therefore causing it to fit less to the data. Lasso
    regression will attempt to marginalize noisy variables, making it useful when
    you want to automatically remove variables that might be irrelevant.
  prefs: []
  type: TYPE_NORMAL
- en: Still, we cannot just apply a linear regression to some data, make some predictions
    with it, and assume all is OK. A linear regression can overfit even with the bias
    of a straight line. Therefore, we need to check and mitigate for both overfitting
    and underfitting to find the sweet spot between the two. That is, unless there
    is not one at all, in which case you should abandon the model altogether.
  prefs: []
  type: TYPE_NORMAL
- en: Stochastic Gradient Descent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In a machine learning context, you are unlikely to do gradient descent in practice
    like we did earlier, where we trained on all training data (called *batch gradient
    descent*). In practice, you are more likely to perform *stochastic gradient descent*,
    which will train on only one sample of the dataset on each iteration. In *mini-batch
    gradient descent*, multiple samples of the dataset are used (e.g., 10 or 100 data
    points) on each iteration.
  prefs: []
  type: TYPE_NORMAL
- en: Why use only part of the data on each iteration? Machine learning practitioners
    cite a few benefits. First, it reduces computation significantly, as each iteration
    does not have to traverse the entire training dataset but only part of it. The
    second benefit is it reduces overfitting. Exposing the training algorithm to only
    part of the data on each iteration keeps changing the loss landscape so it does
    not settle in the loss minimum. After all, minimizing the loss is what causes
    overfitting and so we introduce some randomness to create a little bit of underfitting
    (but hopefully not too much).
  prefs: []
  type: TYPE_NORMAL
- en: Of course, our approximation becomes loose so we have to be careful. This is
    why we will talk about train/test splits shortly, as well as other metrics to
    evaluate our linear regression’s reliability.
  prefs: []
  type: TYPE_NORMAL
- en: '[Example 5-13](#qgiQkdFNOc) shows how to perform stochastic gradient descent
    in Python. If you change the sample size to be more than 1, it will perform mini-batch
    gradient descent.'
  prefs: []
  type: TYPE_NORMAL
- en: Example 5-13\. Performing stochastic gradient descent for a linear regression
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: When I ran this, I got a linear regression of *y* = 1.9382830354181135*x* +
    4.753408787648379\. Obviously, your results are going to be different, and because
    of stochastic gradient descent we really aren’t going to converge toward a specific
    minimum but will end up in a broader neighborhood.
  prefs: []
  type: TYPE_NORMAL
- en: Is Randomness Bad?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If this randomness feels uncomfortable where you get a different answer every
    time you run a piece of code, welcome to the world of machine learning, optimization,
    and stochastic algorithms! Many algorithms that do approximations are random-based,
    and while some are extremely useful, others can be sloppy and perform poorly,
    as you might expect.
  prefs: []
  type: TYPE_NORMAL
- en: A lot of people look to machine learning and AI as some tool that gives objective
    and precise answers, but that cannot be farther from the truth. Machine learning
    produces approximations with a degree of uncertainty, often without ground truth
    once in production. Machine learning can be misused if one is not aware of how
    it works, and it is remiss not to acknowledge its nondeterministic and approximate
    nature.
  prefs: []
  type: TYPE_NORMAL
- en: While randomness can create some powerful tools, it can also be abused. Be careful
    to not use seed values and randomness to p-hack a “good” result, and put effort
    into analyzing your data and model.
  prefs: []
  type: TYPE_NORMAL
- en: The Correlation Coefficient
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Take a look at this scatterplot in [Figure 5-11](#juLpgQHUdt) alongside its
    linear regression. Why might a linear regression not work too well here?
  prefs: []
  type: TYPE_NORMAL
- en: '![emds 0511](Images/emds_0511.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-11\. A scatterplot of data with high variance
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The problem here is that the data has high variance. If the data is extremely
    spread out, it is going to drive up the variance to the point predictions become
    less accurate and useful, resulting in large residuals. Of course we can introduce
    a more biased model, such as linear regression, to not bend and respond to the
    variance so easily. However, the underfitting is also going to undermine our predictions
    because the data is so spread out. We need to numerically measure how “off” our
    predictions are.
  prefs: []
  type: TYPE_NORMAL
- en: So how do you measure these residuals in aggregate? How do you also get a sense
    for how bad the variance in the data is? Let me introduce you to the *correlation
    coefficient*, also called the *Pearson correlation*, which measures the strength
    of the relationship between two variables as a value between –1 and 1\. A correlation
    coefficient closer to 0 indicates there is no correlation. A correlation coefficient
    closer to 1 indicates a strong *positive correlation*, meaning when one variable
    increases, the other proportionally increases. If it is closer to –1 then it indicates
    a strong *negative correlation*, which means as one variable increases the other
    proportionally decreases.
  prefs: []
  type: TYPE_NORMAL
- en: Note the correlation coefficient is often denoted as *r*. The highly scattered
    data in [Figure 5-11](#juLpgQHUdt) has a correlation coefficient of 0.1201\. Since
    it is much closer to 0 than 1, we can infer the data has little correlation.
  prefs: []
  type: TYPE_NORMAL
- en: Here are four other scatterplots in [Figure 5-12](#fiaNFJsgpS) showing their
    correlation coefficients. Notice that the more the points follow a line, the stronger
    the correlation. More dispersed points result in weaker correlations.
  prefs: []
  type: TYPE_NORMAL
- en: '![emds 0512](Images/emds_0512.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-12\. Correlation coefficients for four scatterplots
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As you can imagine, the correlation coefficient is useful for seeing if there
    is a possible relationship between two variables. If there is a strong positive-negative
    relationship, it will be useful in our linear regression. If there is not a relationship,
    they may just add noise and hurt model accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: How do we use Python to calculate the correlation coefficient? Let’s use the
    simple [10-point dataset](https://bit.ly/2KF29Bd) we used earlier. A quick and
    easy way to analyze correlations for all pairs of variables is using Pandas’s
    `corr()` function. This makes it easy to see the correlation coefficient between
    every pair of variables in a dataset, which in this case will only be `x` and
    `y`. This is known as a *correlation matrix*. Take a look in [Example 5-14](#kCbTrtCRSC).
  prefs: []
  type: TYPE_NORMAL
- en: Example 5-14\. Using Pandas to see the correlation coefficient between every
    pair of variables
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the correlation coefficient `0.957586` between `x` and `y` indicates
    a strong positive correlation between the two variables. You can ignore the parts
    of the matrix where `x` or `y` is set to itself and has a value of `1.0`. Obviously,
    when `x` or `y` is set to itself, the correlation will be perfect at 1.0, because
    the values match themselves exactly. When you have more than two variables, the
    correlation matrix will show a larger grid because there are more variables to
    pair and compare.
  prefs: []
  type: TYPE_NORMAL
- en: If you change the code to use a different dataset with a lot of variance, where
    the data is spread out, you will see that the correlation coefficient decreases.
    This again indicates a weaker correlation.
  prefs: []
  type: TYPE_NORMAL
- en: Statistical Significance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here is another aspect to a linear regression you must consider: is my data
    correlation coincidental? In [Chapter 3](ch03.xhtml#ch03) we studied hypothesis
    testing and p-values, and we are going to extend those ideas here with a linear
    regression.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start with a fundamental question: is it possible I see a linear relationship
    in my data due to random chance? How can we be 95% sure the correlation between
    these two variables is significant and not coincidental? If this sounds like a
    hypothesis test from [Chapter 3](ch03.xhtml#ch03), it’s because it is! We need
    to not just express the correlation coefficient but also quantify how confident
    we are that the correlation coefficient did not occur by chance.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Rather than estimating a mean like we did in [Chapter 3](ch03.xhtml#ch03) with
    the drug-testing example, we are estimating the population correlation coefficient
    based on a sample. We denote the population correlation coefficient with the Greek
    symbol <math alttext="rho"><mi>ρ</mi></math> (Rho) while our sample correlation
    coefficient is *r*. Just like we did in [Chapter 3](ch03.xhtml#ch03), we will
    have a null hypothesis <math alttext="upper H 0"><msub><mi>H</mi> <mn>0</mn></msub></math>
    and alternative hypothesis <math alttext="upper H 1"><msub><mi>H</mi> <mn>1</mn></msub></math>
    :'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><msub><mi>H</mi> <mn>0</mn></msub> <mo>:</mo> <mi>ρ</mi>
    <mo>=</mo> <mn>0</mn> <mtext>(implies</mtext> <mtext>no</mtext> <mtext>relationship)</mtext></mrow></math>
    <math display="block"><mrow><msub><mi>H</mi> <mn>1</mn></msub> <mo>:</mo> <mi>ρ</mi>
    <mo>≠</mo> <mn>0</mn> <mtext>(relationship</mtext> <mtext>is</mtext> <mtext>present)</mtext></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: Our null hypothesis <math alttext="upper H 0"><msub><mi>H</mi> <mn>0</mn></msub></math>
    is that there is no relationship between two variables, or more technically, the
    correlation coefficient is 0\. The alternative hypothesis <math alttext="upper
    H 1"><msub><mi>H</mi> <mn>1</mn></msub></math> is there is a relationship, and
    it can be a positive or negative correlation. This is why the alternative hypothesis
    is defined as <math alttext="rho not-equals 0"><mrow><mi>ρ</mi> <mo>≠</mo> <mn>0</mn></mrow></math>
    to support both a positive and negative correlation.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s return to our dataset of 10 points as shown in [Figure 5-13](#MOVwTkVqGF).
    How likely is it we would see these data points by chance? And they happen to
    produce what looks like a linear relationship?
  prefs: []
  type: TYPE_NORMAL
- en: '![emds 0513](Images/emds_0513.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-13\. How likely would we see this data, which seems to have a linear
    correlation, by random chance?
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We already calculated the correlation coefficient for this dataset in [Example 5-14](#kCbTrtCRSC),
    which is 0.957586\. That’s a strong and compelling positive correlation. But again,
    we need to evaluate if this was by random luck. Let’s pursue our hypothesis test
    with 95% confidence using a two-tailed test, exploring if there is a relationship
    between these two variables.
  prefs: []
  type: TYPE_NORMAL
- en: We talked about the T-distribution in [Chapter 3](ch03.xhtml#ch03), which has
    fatter tails to capture more variance and uncertainty. We use a T-distribution
    rather than a normal distribution to do hypothesis testing with linear regression.
    First, let’s plot a T-distribution with a 95% critical value range as shown in
    [Figure 5-14](#DeDuaGoMhK). We account for the fact there are 10 records in our
    sample and therefore we have 9 degrees of freedom (10 – 1 = 9).
  prefs: []
  type: TYPE_NORMAL
- en: '![emds 0514](Images/emds_0514.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-14\. A T-distribution with 9 degrees of freedom, as there are 10 records
    and we subtract 1
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The critical value is approximately ±2.262, and we can calculate that in Python
    as shown in [Example 5-16](#siDtwtJwel). This captures 95% of the center area
    of our T-distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Example 5-16\. Calculating the critical value from a T-distribution
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'If our test value happens to fall outside this range of (–2.262, 2.262), then
    we can reject our null hypothesis. To calculate the test value *t*, we need to
    use the following formula. Again *r* is the correlation coefficient and *n* is
    the sample size:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mi>t</mi> <mo>=</mo> <mfrac><mi>r</mi> <msqrt><mfrac><mrow><mn>1</mn><mo>-</mo><msup><mi>r</mi>
    <mn>2</mn></msup></mrow> <mrow><mi>n</mi><mo>-</mo><mn>2</mn></mrow></mfrac></msqrt></mfrac></mrow></math><math
    display="block"><mrow><mi>t</mi> <mo>=</mo> <mfrac><mrow><mn>.957586</mn></mrow>
    <msqrt><mfrac><mrow><mn>1</mn><mo>-</mo><msup><mn>.957586</mn> <mn>2</mn></msup></mrow>
    <mrow><mn>10</mn><mo>-</mo><mn>2</mn></mrow></mfrac></msqrt></mfrac> <mo>=</mo>
    <mn>9.339956</mn></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: Let’s put the whole test together in Python as shown in [Example 5-17](#kLcFTduBrG).
    If our test value falls outside the critical range of 95% confidence, we accept
    that our correlation was not by chance.
  prefs: []
  type: TYPE_NORMAL
- en: Example 5-17\. Testing significance for linear-looking data
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The test value here is approximately 9.39956, which is definitely outside the
    range of (–2.262, 2.262) so we can reject the null hypothesis and say our correlation
    is real. That’s because the p-value is remarkably significant: .000005976\. This
    is well below our .05 threshold, so this is virtually not coincidence: there is
    a correlation. It makes sense the p-value is so small because the points strongly
    resemble a line. It is highly unlikely these points randomly arranged themselves
    near a line this closely by chance.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 5-15](#qFBEPbEweP) shows some other datasets with their correlation
    coefficients and p-values. Analyze each one of them. Which one is likely the most
    useful for predictions? What are the problems with the other ones?'
  prefs: []
  type: TYPE_NORMAL
- en: '![emds 0515](Images/emds_0515.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-15\. Different datasets and their correlation coefficients and p-values
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Now that you’ve had a chance to work through the analysis on the datasets from
    [Figure 5-15](#qFBEPbEweP), let’s walk through the findings. The left figure has
    a high positive correlation, but it only has three points. The lack of data drives
    up the p-value significantly to 0.34913 and increases the likelihood the data
    happened by chance. This makes sense because having just three data points makes
    it likely to see a linear pattern, but it’s not much better than having two points
    that will simply connect a line between them. This brings up an important rule:
    having more data will decrease your p-value, especially if that data gravitates
    toward a line.'
  prefs: []
  type: TYPE_NORMAL
- en: The second figure is what we just covered. It has only 10 data points, but it
    forms a linear pattern so nicely, we not only have a strong positive correlation
    but also an extremely low p-value. When you have a p-value this low, you can bet
    you are measuring an engineered and tightly controlled process, not something
    sociological or natural.
  prefs: []
  type: TYPE_NORMAL
- en: The right two images in [Figure 5-15](#qFBEPbEweP) fail to identify a linear
    relationship. Their correlation coefficient is close to 0, indicating no correlation,
    and the p-values unsurprisingly indicate randomness played a role.
  prefs: []
  type: TYPE_NORMAL
- en: 'The rule is this: the more data you have that consistently resembles a line,
    the more significant the p-value for your correlation will be. The more scattered
    or sparse the data, the more the p-value will increase and thus indicate your
    correlation occurred by random chance.'
  prefs: []
  type: TYPE_NORMAL
- en: Coefficient of Determination
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s learn an important metric you will see a lot in statistics and machine
    learning regressions. The *coefficient of determination*, called <math alttext="r
    squared"><msup><mi>r</mi> <mn>2</mn></msup></math> , measures how much variation
    in one variable is explainable by the variation of the other variable. It is also
    the square of the correlation coefficient <math alttext="r"><mi>r</mi></math>
    . As <math alttext="r"><mi>r</mi></math> approaches a perfect correlation (–1
    or 1), <math alttext="r squared"><msup><mi>r</mi> <mn>2</mn></msup></math> approaches
    1\. Essentially, <math alttext="r squared"><msup><mi>r</mi> <mn>2</mn></msup></math>
    shows how much two variables interact with each other.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s continue looking at our data from [Figure 5-13](#MOVwTkVqGF). In [Example 5-18](#MgPOHfCDiB),
    take our dataframe code from earlier that calculates the correlation coefficient
    and then simply square it. That will multiply each correlation coefficient by
    itself.
  prefs: []
  type: TYPE_NORMAL
- en: Example 5-18\. Creating a correlation matrix in Pandas
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: A coefficient of determination of 0.916971 is interpreted as 91.6971% of the
    variation in *x* is explained by *y* (and vice versa), and the remaining 8.3029%
    is noise caused by other uncaptured variables; 0.916971 is a pretty good coefficient
    of determination, showing that *x* and *y* explain each other’s variance. But
    there could be other variables at play making up that remaining 0.083029\. Remember,
    correlation does not equal causation, so there could be other variables contributing
    to the relationship we are seeing.
  prefs: []
  type: TYPE_NORMAL
- en: Correlation Is Not Causation!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is important to note that while we put a lot of emphasis on measuring correlation
    and building metrics around it, please remember *correlation is not causation!*
    You probably have heard that mantra before, but I want to expand on why statisticians
    say it.
  prefs: []
  type: TYPE_NORMAL
- en: Just because we see a correlation between *x* and *y* does not mean *x* causes
    *y*. It could actually be *y* causes *x*! Or maybe there is a third uncaptured
    variable *z* that is causing *x* and *y*. It could be that *x* and *y* do not
    cause each other at all and the correlation is just coincidental, hence why it
    is important we measure the statistical significance.
  prefs: []
  type: TYPE_NORMAL
- en: Now I have a more pressing question for you. Can computers discern between correlation
    and causation? The answer is a resounding “NO!” Computers have concept of correlation
    but not causation. Let’s say I load a dataset to scikit-learn showing gallons
    of water consumed and my water bill. My computer, or any program including scikit-learn,
    does not have any notion whether more water usage causes a higher bill or a higher
    bill causes more water usage. An AI system could easily conclude the latter, as
    nonsensical as that is. This is why many machine learning projects require a human
    in the loop to inject common sense.
  prefs: []
  type: TYPE_NORMAL
- en: In computer vision, this happens too. Computer vision often uses a regression
    on the numeric pixels to predict a category. If I train a computer vision system
    to recognize cows using pictures of cows, it can easily make correlations with
    the field rather than the cows. Therefore, if I show a picture of an empty field,
    it will label the grass as cows! This again is because computers have no concept
    of causality (the cow shape should cause the label “cow”) and instead get lost
    in correlations we are not interested in.
  prefs: []
  type: TYPE_NORMAL
- en: Standard Error of the Estimate
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One way to measure the overall error of a linear regression is the *SSE*, or
    *sum of squared error*. We learned about this earlier where we squared each residual
    and summed them. If <math alttext="ModifyingAbove y With caret"><mover accent="true"><mi>y</mi>
    <mo>^</mo></mover></math> (pronounced “y-hat”) is each predicted value from the
    line and <math alttext="y"><mi>y</mi></math> represents each actual y-value from
    the data, here is the calculation:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="upper S upper S upper E equals sigma-summation left-parenthesis
    y minus ModifyingAbove y With caret right-parenthesis squared" display="block"><mrow><mi>S</mi>
    <mi>S</mi> <mi>E</mi> <mo>=</mo> <mo>∑</mo> <msup><mrow><mo>(</mo><mi>y</mi><mo>-</mo><mover
    accent="true"><mi>y</mi> <mo>^</mo></mover><mo>)</mo></mrow> <mn>2</mn></msup></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: However, all of these squared values are hard to interpret so we can use some
    square root logic to scale things back into their original units. We will also
    average all of them, and this is what the *standard error of the estimate ( <math
    alttext="upper S Subscript e"><msub><mi>S</mi> <mi>e</mi></msub></math> )* does.
    If *n* is the number of data points, [Example 5-19](#OmfMsdJioT) shows how we
    calculate the standard error <math alttext="upper S Subscript e"><msub><mi>S</mi>
    <mi>e</mi></msub></math> in Python.
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="upper S Subscript e Baseline equals StartFraction sigma-summation
    left-parenthesis y minus ModifyingAbove y With caret right-parenthesis squared
    Over n minus 2 EndFraction" display="block"><mrow><msub><mi>S</mi> <mi>e</mi></msub>
    <mo>=</mo> <mfrac><mrow><mo>∑</mo><msup><mrow><mo>(</mo><mi>y</mi><mo>-</mo><mover
    accent="true"><mi>y</mi> <mo>^</mo></mover><mo>)</mo></mrow> <mn>2</mn></msup></mrow>
    <mrow><mi>n</mi><mo>-</mo><mn>2</mn></mrow></mfrac></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: Example 5-19\. Calculating the standard error of the estimate
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Why <math alttext="n minus 2"><mrow><mi>n</mi> <mo>-</mo> <mn>2</mn></mrow></math>
    instead of <math alttext="n minus 1"><mrow><mi>n</mi> <mo>-</mo> <mn>1</mn></mrow></math>
    , like we did in so many variance calculations in [Chapter 3](ch03.xhtml#ch03)?
    Without going too deep into mathematical proofs, this is because a linear regression
    has two variables, not just one, so we have to increase the uncertainty by one
    more in our degrees of freedom.
  prefs: []
  type: TYPE_NORMAL
- en: You will notice the standard error of the estimate looks remarkably similar
    to the standard deviation we studied in [Chapter 3](ch03.xhtml#ch03). This is
    not by accident. That is because it is the standard deviation for a linear regression.
  prefs: []
  type: TYPE_NORMAL
- en: Prediction Intervals
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As mentioned earlier, our data in a linear regression is a sample from a population.
    Therefore, our regression is only as good as our sample. Our linear regression
    line also has a normal distribution running along it. Effectively, this makes
    each predicted y-value a sample statistic just like the mean. As a matter of fact,
    the “mean” is shifting along the line.
  prefs: []
  type: TYPE_NORMAL
- en: Remember when we talked statistics in [Chapter 2](ch02.xhtml#ch02) about variance
    and standard deviation? The concepts apply here too. With a linear regression,
    we hope that data follows a normal distribution in a linear fashion. A regression
    line serves as the shifting “mean” of our bell curve, and the spread of the data
    around the line reflects the variance/standard deviation, as shown in [Figure 5-16](#GQrKIjbjFm).
  prefs: []
  type: TYPE_NORMAL
- en: '![emds 0516](Images/emds_0516.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-16\. A linear regression assumes a normal distribution is following
    the line
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: When we have a normal distribution following a linear regression line, we have
    not just one variable but a second one steering a distribution as well. There
    is a confidence interval around each *y* prediction, and this is known as a *prediction
    interval*.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s bring back some context with our veterinary example, estimating the age
    of a dog and number of vet visits. I want to know the prediction interval for
    number of vet visits with 95% confidence for a dog that is 8.5 years old. What
    this prediction interval looks like is shown in [Figure 5-17](#ASbMImvBJq). We
    are 95% confident that an 8.5 year old dog will have between 16.462 and 25.966
    veterinary visits.
  prefs: []
  type: TYPE_NORMAL
- en: '![emds 0517](Images/emds_0517.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-17\. A prediction interval for a dog that is 8.5 years old with 95%
    confidence
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'How do we calculate this? We need to get the margin of error and plus/minus
    it around the predicted y-value. It’s a beastly equation that involves a critical
    value from the T-distribution as well as the standard error of the estimate. Let’s
    take a look:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="upper E equals t Subscript .025 Baseline asterisk upper S Subscript
    e Baseline asterisk StartRoot 1 plus StartFraction 1 Over n EndFraction plus StartFraction
    n left-parenthesis x 0 plus x overbar right-parenthesis squared Over n left-parenthesis
    sigma-summation x squared right-parenthesis minus left-parenthesis sigma-summation
    x right-parenthesis squared EndFraction EndRoot" display="block"><mrow><mi>E</mi>
    <mo>=</mo> <msub><mi>t</mi> <mtext>.025</mtext></msub> <mo>*</mo> <msub><mi>S</mi>
    <mi>e</mi></msub> <mo>*</mo> <msqrt><mrow><mn>1</mn> <mo>+</mo> <mfrac><mn>1</mn>
    <mi>n</mi></mfrac> <mo>+</mo> <mfrac><mrow><mi>n</mi><msup><mrow><mo>(</mo><msub><mi>x</mi>
    <mn>0</mn></msub> <mo>+</mo><mover accent="true"><mi>x</mi> <mo>¯</mo></mover><mo>)</mo></mrow>
    <mn>2</mn></msup></mrow> <mrow><mi>n</mi><mrow><mo>(</mo><mo>∑</mo><msup><mi>x</mi>
    <mn>2</mn></msup> <mo>)</mo></mrow><mo>-</mo><msup><mrow><mo>(</mo><mo>∑</mo><mi>x</mi><mo>)</mo></mrow>
    <mn>2</mn></msup></mrow></mfrac></mrow></msqrt></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: The x-value we are interested in is specified as <math alttext="x 0"><msub><mi>x</mi>
    <mn>0</mn></msub></math> , which in this case is 8.5\. Here is how we solve this
    in Python, shown in [Example 5-20](#DaFlrHUbDE).
  prefs: []
  type: TYPE_NORMAL
- en: Example 5-20\. Calculating a prediction interval of vet visits for a dog that’s
    8.5 years old
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Oy vey! That’s a lot of calculation, and unfortunately, SciPy and other mainstream
    data science libraries do not do this. But if you are inclined to statistical
    analysis, this is very useful information. We not only create a prediction based
    on a linear regression (e.g., a dog that’s 8.5 years old will have 21.2145 vet
    visits), but we also are actually able to say something much less absolute: there’s
    a 95% probability an 8.5 year old dog will visit the vet between 16.46 and 25.96
    times. Brilliant, right? And it’s a much safer claim because it captures a range
    rather than a single value, and thus accounts for uncertainty.'
  prefs: []
  type: TYPE_NORMAL
- en: Train/Test Splits
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This analysis I just did with the correlation coefficient, statistical significance,
    and coefficient of determination unfortunately is not always done by practitioners.
    Sometimes they are dealing with so much data they do not have the time or technical
    ability to do so. For example a 128 × 128 pixel image is at least 16,384 variables.
    Do you have time to do statistical analysis on each of those pixel variables?
    Probably not! Unfortunately, this leads many data scientists to not learn these
    statistical metrics at all.
  prefs: []
  type: TYPE_NORMAL
- en: In an [obscure online forum](http://disq.us/p/1jas3zg), I once read a post saying
    statistical regression is a scalpel, while machine learning is a chainsaw. When
    operating with massive amounts of data and variables, you cannot sift through
    all of that with a scalpel. You have to resort to a chainsaw and while you lose
    explainability and precision, you at least can scale to make broader predictions
    on more data. That being said, statistical concerns like sampling bias and overfitting
    do not go away. But there are a few practices that can be employed for quick validation.
  prefs: []
  type: TYPE_NORMAL
- en: Why Are There No Confidence Intervals and P-Values in scikit-learn?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Scikit-learn does not support confidence intervals and P-values, as these two
    techniques are open problems for higher-dimensional data. This only emphasizes
    the gap between statisticians and machine learning practitioners. As one of the
    maintainers of scikit-learn, Gael Varoquaux, said, “In general computing correct
    p-values requires assumptions on the data that are not met by the data used in
    machine learning (no multicollinearity, enough data compared to the dimensionality)….P-values
    are something that are expected to be well checked (they are a guard in medical
    research). Implementing them is asking for trouble….We can give p-values only
    in very narrow settings [with few variables].”
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to go down the rabbit hole, there are some interesting discussions
    on GitHub:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*https://github.com/scikit-learn/scikit-learn/issues/6773*](https://github.com/scikit-learn/scikit-learn/issues/6773)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*https://github.com/scikit-learn/scikit-learn/issues/16802*](https://github.com/scikit-learn/scikit-learn/issues/16802)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As mentioned before, [statsmodel](https://oreil.ly/8oEHo) is a library that
    provides helpful tools for statistical analysis. Just know it will likely not
    scale to larger-dimensional models because of the aforementioned reasons.
  prefs: []
  type: TYPE_NORMAL
- en: A basic technique machine learning practitioners use to mitigate overfitting
    is a practice called the *train/test split*, where typically 1/3 of the data is
    set aside for testing and the other 2/3 is used for training (other ratios can
    be used as well). The *training dataset* is used to fit the linear regression,
    while the *testing dataset* is used to measure the linear regression’s performance
    on data it has not seen before. This technique is generally used for all supervised
    machine learning, including logistic regression and neural networks. [Figure 5-18](#bpLCiplUVe)
    shows a visualization of how we break up our data into 2/3 for training and 1/3
    for testing.
  prefs: []
  type: TYPE_NORMAL
- en: This Is a Small Dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we will learn later, there are other ways to split a training/testing dataset
    than 2/3 and 1/3\. If you have a dataset this small, you will probably be better
    off with 9/10 and 1/10 paired with cross-validation, or even just leave-one-out
    cross-validation. See [“Do Train/Test Splits Have to be Thirds?”](#Do_Training_Test_Splits)
    to learn more.
  prefs: []
  type: TYPE_NORMAL
- en: '![emds 0518](Images/emds_0518.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-18\. Splitting into training/testing data—the line is fitted to the
    training data (dark blue) using least squares, while the testing data (light red)
    is analyzed afterward to see how off the predictions are on data that was not
    seen before
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '[Example 5-21](#OivsevRKQr) shows how to perform a train/test split using scikit-learn,
    where 1/3 of the data is set aside for testing and the other 2/3 is used for training.'
  prefs: []
  type: TYPE_NORMAL
- en: Training Is Fitting a Regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Remember that “fitting” a regression is synonymous with “training.” The latter
    word is used by machine learning practitioners.
  prefs: []
  type: TYPE_NORMAL
- en: Example 5-21\. Doing a train/test split on linear regression
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Notice that the `train_test_split()` will take our dataset (*X* and *Y* columns),
    shuffle it, and then return our training and testing datasets based on our testing-dataset
    size. We use the `LinearRegression`’s `fit()` function to fit on the training
    datasets `X_train` and `Y_train`. Then we use the `score()` function on the testing
    datasets `X_test` and `Y_test` to evaluate the <math alttext="r squared"><msup><mi>r</mi>
    <mn>2</mn></msup></math> , giving us a sense how the regression performs on data
    it has not seen before. The higher the <math alttext="r squared"><msup><mi>r</mi>
    <mn>2</mn></msup></math> is for our testing dataset, the better. Having that higher
    number indicates the regression performs well on data it has not seen before.
  prefs: []
  type: TYPE_NORMAL
- en: We can also alternate the testing dataset across each 1/3 fold. This is known
    as *cross-validation* and is often considered the gold standard of validation
    techniques. [Figure 5-20](#HgfhjgkCrp) shows how each 1/3 of the data takes a
    turn being the testing dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '![emds 0520](Images/emds_0520.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-20\. A visualization of cross-validation with three folds
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The code in [Example 5-22](#kgNpJaUubi) shows a cross-validation performed across
    three folds, and then the scoring metric (in this case the mean sum of squares
    [MSE]) is averaged alongside its standard deviation to show how consistently each
    test performed.
  prefs: []
  type: TYPE_NORMAL
- en: Example 5-22\. Using three-fold cross-validation for a linear regression
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: When you get concerned about variance in your model, one thing you can do, rather
    than a simple train/test split or cross-validation, is use *random-fold validation*
    to repeatedly shuffle and train/test split your data an unlimited number of times
    and aggregate the testing results. In [Example 5-23](#vIhFvqDmvc) there are 10
    iterations of randomly sampling 1/3 of the data for testing and the other 2/3
    for training. Those 10 testing results are then averaged alongside their standard
    deviations to see how consistently the testing datasets perform.
  prefs: []
  type: TYPE_NORMAL
- en: What’s the catch? It’s computationally very expensive as we are training the
    regression many times.
  prefs: []
  type: TYPE_NORMAL
- en: Example 5-23\. Using a random-fold validation for a linear regression
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: So when you are crunched for time or your data is too voluminous to statistically
    analyze, a train/test split is going to provide a way to measure how well your
    linear regression will perform on data it has not seen before.
  prefs: []
  type: TYPE_NORMAL
- en: Train/Test Splits Are Not Guarantees
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is important to note that just because you apply machine learning best practices
    of splitting your training and testing data, it does not mean your model is going
    to perform well. You can easily overtune your model and p-hack your way into a
    good test result, only to find it does not work well out in the real world. This
    is why holding out another dataset called the *validation set* is sometimes necessary,
    especially if you are comparing different models or configurations. That way,
    your tweaks on the training data to get better performance on the testing data
    do not leak info into the training. You can use the validation dataset as one
    last stopgap to see if p-hacking caused you to overfit to your testing dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Even then, your whole dataset (including training, testing, and validation)
    could have been biased to begin with, and no split is going to mitigate that.
    Andrew Ng discussed this as a large problem with machine learning during his [Q&A
    with DeepLearning.AI and Stanford HAI](https://oreil.ly/x23SJ). He walked through
    an example showing why machine learning has not replaced radiologists.
  prefs: []
  type: TYPE_NORMAL
- en: Multiple Linear Regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We put almost exclusive focus on doing linear regression on one input variable
    and one output variable in this chapter. However, the concepts we learned here
    should largely apply to multivariable linear regression. Metrics like <math alttext="r
    squared"><msup><mi>r</mi> <mn>2</mn></msup></math> , standard error, and confidence
    intervals can be used but it gets harder with more variables. [Example 5-24](#cICPINJumN)
    is an example of a linear regression with two input variables and one output variable
    using scikit-learn.
  prefs: []
  type: TYPE_NORMAL
- en: Example 5-24\. A linear regression with two input variables
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: There is a degree of precariousness when a model becomes so inundated with variables
    it starts to lose explainability, and this is when machine learning practices
    start to come in and treat the model as as black box. I hope that you are convinced
    statistical concerns do not go away, and data becomes increasingly sparse the
    more variables you add. But if you step back and analyze the relationships between
    each pair of variables using a correlation matrix, and seek understanding on how
    each pair of variables interact, it will help your efforts to create a productive
    machine learning model.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We covered a lot in this chapter. We attempted to go beyond a cursory understanding
    of linear regression and making train/test splits our only validation. I wanted
    to show you both the scalpel (statistics) and the chainsaw (machine learning)
    so you can judge which is best for a given problem you encounter. There are a
    lot of metrics and analysis methods available in linear regression alone, and
    we covered a number of them to understand whether a linear regression is reliable
    for predictions. You may find yourself in a position to do regressions as broad
    approximations or meticulously analyze and comb your data using statistical tools.
    Which approach you use is situational, and if you want to learn more about statistical
    tools available for Python, check out the [statsmodel library](https://oreil.ly/8oEHo).
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 6](ch06.xhtml#ch06) covering logistic regression, we will revisit
    the <math alttext="r squared"><msup><mi>r</mi> <mn>2</mn></msup></math> and statistical
    significance. I hope that this chapter convinced you there are ways to analyze
    data meaningfully, and the investment can make the difference in a successful
    project.
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A dataset of two variables, *x* and *y*, is provided [here](https://bit.ly/3C8JzrM).
  prefs: []
  type: TYPE_NORMAL
- en: Perform a simple linear regression to find the *m* and *b* values that minimizes
    the loss (sum of squares).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the correlation coefficient and statistical significance of this data
    (at 95% confidence). Is the correlation useful?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If I predict where *x* = 50, what is the 95% prediction interval for the predicted
    value of *y*?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Start your regression over and do a train/test split. Feel free to experiment
    with cross-validation and random-fold validation. Does the linear regression perform
    well and consistently on the testing data? Why or why not?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Answers are in [Appendix B](app02.xhtml#exercise_answers).
  prefs: []
  type: TYPE_NORMAL
