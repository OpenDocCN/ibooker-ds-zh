<html><head></head><body>
<div id="sbo-rt-content"><section data-pdf-bookmark="Chapter 44. In Depth: Decision Trees &#10;and Random Forests" data-type="chapter" epub:type="chapter"><div class="chapter" id="section-0508-random-forests">
<h1><span class="label">Chapter 44. </span>In Depth: Decision Trees 
<span class="keep-together">and Random Forests</span></h1>
<p><a data-primary="machine learning" data-secondary="decision trees and random forests" data-type="indexterm" id="ix_ch44-asciidoc0"/>Previously we have looked in depth at a simple generative classifier
(naive Bayes; see <a data-type="xref" href="ch41.xhtml#section-0505-naive-bayes">Chapter 41</a>) and a powerful discriminative classifier (support
vector machines; see <a data-type="xref" href="ch43.xhtml#section-0507-support-vector-machines">Chapter 43</a>). Here we’ll take a look at another
powerful algorithm: a nonparametric algorithm called <em>random forests</em>.
<a data-primary="ensemble estimator/method" data-type="indexterm" id="idm45858730747616"/>Random forests are an example of an <em>ensemble</em> method, meaning one that
relies on aggregating the results of a set of simpler estimators. The
somewhat surprising result with such ensemble methods is that the sum
can be greater than the parts: that is, the predictive accuracy of a
majority vote among a number of estimators can end up being better than
that of any of the individual estimators doing the voting! We will see
examples of this in the following sections.</p>
<p>We begin with the standard imports:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">1</code><code class="p">]:</code> <code class="o">%</code><code class="k">matplotlib</code> inline
        <code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>
        <code class="kn">import</code> <code class="nn">matplotlib.pyplot</code> <code class="k">as</code> <code class="nn">plt</code>
        <code class="n">plt</code><code class="o">.</code><code class="n">style</code><code class="o">.</code><code class="n">use</code><code class="p">(</code><code class="s1">'seaborn-whitegrid'</code><code class="p">)</code></pre>
<section data-pdf-bookmark="Motivating Random Forests: Decision Trees" data-type="sect1"><div class="sect1" id="ch_0508-random-forests_motivating-random-forests-decision-trees">
<h1>Motivating Random Forests: Decision Trees</h1>
<p><a data-primary="ensemble learner" data-type="indexterm" id="idm45858730718832"/>Random <a data-primary="decision trees" data-type="indexterm" id="ix_ch44-asciidoc1"/><a data-primary="random forests" data-secondary="motivating with decision trees" data-type="indexterm" id="ix_ch44-asciidoc2"/>forests are an example of an ensemble learner built on decision
trees. For this reason, we’ll start by discussing decision
trees themselves.</p>
<p class="pagebreak-before less_space">Decision trees are extremely intuitive ways to classify or label
objects: you simply ask a series of questions designed to zero in on the
classification. For example, if you wanted to build a decision tree to
classify animals you come across while on a hike, you might construct
the one shown in <a data-type="xref" href="#fig_images_in_0508-decision-tree">Figure 44-1</a>.</p>
<figure><div class="figure" id="fig_images_in_0508-decision-tree">
<img alt="05.08 decision tree" height="204" src="assets/05.08-decision-tree.png" width="600"/>
<h6><span class="label">Figure 44-1. </span>An example of a binary decision tree<sup><a data-type="noteref" href="ch44.xhtml#idm45858730712544" id="idm45858730712544-marker">1</a></sup></h6>
</div></figure>
<p>The binary splitting makes this extremely efficient: in a
well-constructed tree, each question will cut the number of options by
approximately half, very quickly narrowing the options even among a
large number of classes. The trick, of course, comes in deciding which
questions to ask at each step. In machine learning implementations of
decision trees, the questions generally take the form of axis-aligned
splits in the data: that is, each node in the tree splits the data into
two groups using a cutoff value within one of the features.
Let’s now look at an example of this.</p>
<section data-pdf-bookmark="Creating a Decision Tree" data-type="sect2"><div class="sect2" id="ch_0508-random-forests_creating-a-decision-tree">
<h2>Creating a Decision Tree</h2>
<p><a data-primary="decision trees" data-secondary="creating" data-type="indexterm" id="ix_ch44-asciidoc3"/>Consider the following two-dimensional data, which has one of four class
labels (see <a data-type="xref" href="#fig_0508-random-forests_files_in_output_8_0">Figure 44-2</a>).</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">2</code><code class="p">]:</code> <code class="kn">from</code> <code class="nn">sklearn.datasets</code> <code class="kn">import</code> <code class="n">make_blobs</code>

        <code class="n">X</code><code class="p">,</code> <code class="n">y</code> <code class="o">=</code> <code class="n">make_blobs</code><code class="p">(</code><code class="n">n_samples</code><code class="o">=</code><code class="mi">300</code><code class="p">,</code> <code class="n">centers</code><code class="o">=</code><code class="mi">4</code><code class="p">,</code>
                          <code class="n">random_state</code><code class="o">=</code><code class="mi">0</code><code class="p">,</code> <code class="n">cluster_std</code><code class="o">=</code><code class="mf">1.0</code><code class="p">)</code>
        <code class="n">plt</code><code class="o">.</code><code class="n">scatter</code><code class="p">(</code><code class="n">X</code><code class="p">[:,</code> <code class="mi">0</code><code class="p">],</code> <code class="n">X</code><code class="p">[:,</code> <code class="mi">1</code><code class="p">],</code> <code class="n">c</code><code class="o">=</code><code class="n">y</code><code class="p">,</code> <code class="n">s</code><code class="o">=</code><code class="mi">50</code><code class="p">,</code> <code class="n">cmap</code><code class="o">=</code><code class="s1">'rainbow'</code><code class="p">);</code></pre>
<figure><div class="figure" id="fig_0508-random-forests_files_in_output_8_0">
<img alt="output 8 0" height="392" src="assets/output_8_0.png" width="600"/>
<h6><span class="label">Figure 44-2. </span>Data for the decision tree classifier</h6>
</div></figure>
<p>A simple decision tree built on this data will iteratively split the
data along one or the other axis according to some quantitative
criterion, and at each level assign the label of the new region
according to a majority vote of points within it. <a data-type="xref" href="#fig_images_in_0508-decision-tree-levels">Figure 44-3</a>
presents a visualization of the first four levels of a decision tree
classifier for this data.</p>
<figure><div class="figure" id="fig_images_in_0508-decision-tree-levels">
<img alt="05.08 decision tree levels" height="96" src="assets/05.08-decision-tree-levels.png" width="600"/>
<h6><span class="label">Figure 44-3. </span>Visualization of how the decision tree splits the data<sup><a data-type="noteref" href="ch44.xhtml#idm45858730648048" id="idm45858730648048-marker">2</a></sup></h6>
</div></figure>
<p>Notice that after the first split, every point in the upper branch
remains unchanged, so there is no need to further subdivide this branch.
Except for nodes that contain all of one color, at each level <em>every</em>
region is again split along one of the two features.</p>
<p>This process of fitting a decision tree to our data can be done in
Scikit-Learn with the <code>DecisionTreeClassifier</code> estimator:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">3</code><code class="p">]:</code> <code class="kn">from</code> <code class="nn">sklearn.tree</code> <code class="kn">import</code> <code class="n">DecisionTreeClassifier</code>
        <code class="n">tree</code> <code class="o">=</code> <code class="n">DecisionTreeClassifier</code><code class="p">()</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code></pre>
<p class="pagebreak-before less_space">Let’s write a utility function to help us visualize the
output of the classifier:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">4</code><code class="p">]:</code> <code class="k">def</code> <code class="nf">visualize_classifier</code><code class="p">(</code><code class="n">model</code><code class="p">,</code> <code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">,</code> <code class="n">ax</code><code class="o">=</code><code class="kc">None</code><code class="p">,</code> <code class="n">cmap</code><code class="o">=</code><code class="s1">'rainbow'</code><code class="p">):</code>
            <code class="n">ax</code> <code class="o">=</code> <code class="n">ax</code> <code class="ow">or</code> <code class="n">plt</code><code class="o">.</code><code class="n">gca</code><code class="p">()</code>

            <code class="c1"># Plot the training points</code>
            <code class="n">ax</code><code class="o">.</code><code class="n">scatter</code><code class="p">(</code><code class="n">X</code><code class="p">[:,</code> <code class="mi">0</code><code class="p">],</code> <code class="n">X</code><code class="p">[:,</code> <code class="mi">1</code><code class="p">],</code> <code class="n">c</code><code class="o">=</code><code class="n">y</code><code class="p">,</code> <code class="n">s</code><code class="o">=</code><code class="mi">30</code><code class="p">,</code> <code class="n">cmap</code><code class="o">=</code><code class="n">cmap</code><code class="p">,</code>
                       <code class="n">clim</code><code class="o">=</code><code class="p">(</code><code class="n">y</code><code class="o">.</code><code class="n">min</code><code class="p">(),</code> <code class="n">y</code><code class="o">.</code><code class="n">max</code><code class="p">()),</code> <code class="n">zorder</code><code class="o">=</code><code class="mi">3</code><code class="p">)</code>
            <code class="n">ax</code><code class="o">.</code><code class="n">axis</code><code class="p">(</code><code class="s1">'tight'</code><code class="p">)</code>
            <code class="n">ax</code><code class="o">.</code><code class="n">axis</code><code class="p">(</code><code class="s1">'off'</code><code class="p">)</code>
            <code class="n">xlim</code> <code class="o">=</code> <code class="n">ax</code><code class="o">.</code><code class="n">get_xlim</code><code class="p">()</code>
            <code class="n">ylim</code> <code class="o">=</code> <code class="n">ax</code><code class="o">.</code><code class="n">get_ylim</code><code class="p">()</code>

            <code class="c1"># fit the estimator</code>
            <code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code>
            <code class="n">xx</code><code class="p">,</code> <code class="n">yy</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">meshgrid</code><code class="p">(</code><code class="n">np</code><code class="o">.</code><code class="n">linspace</code><code class="p">(</code><code class="o">*</code><code class="n">xlim</code><code class="p">,</code> <code class="n">num</code><code class="o">=</code><code class="mi">200</code><code class="p">),</code>
                                 <code class="n">np</code><code class="o">.</code><code class="n">linspace</code><code class="p">(</code><code class="o">*</code><code class="n">ylim</code><code class="p">,</code> <code class="n">num</code><code class="o">=</code><code class="mi">200</code><code class="p">))</code>
            <code class="n">Z</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">np</code><code class="o">.</code><code class="n">c_</code><code class="p">[</code><code class="n">xx</code><code class="o">.</code><code class="n">ravel</code><code class="p">(),</code> <code class="n">yy</code><code class="o">.</code><code class="n">ravel</code><code class="p">()])</code><code class="o">.</code><code class="n">reshape</code><code class="p">(</code><code class="n">xx</code><code class="o">.</code><code class="n">shape</code><code class="p">)</code>

            <code class="c1"># Create a color plot with the results</code>
            <code class="n">n_classes</code> <code class="o">=</code> <code class="nb">len</code><code class="p">(</code><code class="n">np</code><code class="o">.</code><code class="n">unique</code><code class="p">(</code><code class="n">y</code><code class="p">))</code>
            <code class="n">contours</code> <code class="o">=</code> <code class="n">ax</code><code class="o">.</code><code class="n">contourf</code><code class="p">(</code><code class="n">xx</code><code class="p">,</code> <code class="n">yy</code><code class="p">,</code> <code class="n">Z</code><code class="p">,</code> <code class="n">alpha</code><code class="o">=</code><code class="mf">0.3</code><code class="p">,</code>
                                   <code class="n">levels</code><code class="o">=</code><code class="n">np</code><code class="o">.</code><code class="n">arange</code><code class="p">(</code><code class="n">n_classes</code> <code class="o">+</code> <code class="mi">1</code><code class="p">)</code> <code class="o">-</code> <code class="mf">0.5</code><code class="p">,</code>
                                   <code class="n">cmap</code><code class="o">=</code><code class="n">cmap</code><code class="p">,</code> <code class="n">zorder</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>

            <code class="n">ax</code><code class="o">.</code><code class="n">set</code><code class="p">(</code><code class="n">xlim</code><code class="o">=</code><code class="n">xlim</code><code class="p">,</code> <code class="n">ylim</code><code class="o">=</code><code class="n">ylim</code><code class="p">)</code></pre>
<p>Now we can examine what the decision tree classification looks like (see
<a data-type="xref" href="#fig_0508-random-forests_files_in_output_17_0">Figure 44-4</a>).</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">5</code><code class="p">]:</code> <code class="n">visualize_classifier</code><code class="p">(</code><code class="n">DecisionTreeClassifier</code><code class="p">(),</code> <code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code></pre>
<figure class="width-80"><div class="figure" id="fig_0508-random-forests_files_in_output_17_0">
<img alt="output 17 0" height="401" src="assets/output_17_0.png" width="600"/>
<h6><span class="label">Figure 44-4. </span>Visualization of a decision tree classification</h6>
</div></figure>
<p>If you’re running this notebook live, you can use the helper
script included in the online
<a href="https://oreil.ly/etDrN">appendix</a>
to bring up an interactive visualization of the decision tree building
process:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">6</code><code class="p">]:</code> <code class="c1"># helpers_05_08 is found in the online appendix</code>
        <code class="kn">import</code> <code class="nn">helpers_05_08</code>
        <code class="n">helpers_05_08</code><code class="o">.</code><code class="n">plot_tree_interactive</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">);</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">6</code><code class="p">]:</code> <code class="n">interactive</code><code class="p">(</code><code class="n">children</code><code class="o">=</code><code class="p">(</code><code class="n">Dropdown</code><code class="p">(</code><code class="n">description</code><code class="o">=</code><code class="s1">'depth'</code><code class="p">,</code> <code class="n">index</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code> <code class="n">options</code><code class="o">=</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="mi">5</code><code class="p">),</code>
         <code class="o">&gt;</code> <code class="n">value</code><code class="o">=</code><code class="mi">5</code><code class="p">),</code> <code class="n">Output</code><code class="p">()),</code> <code class="n">_dom_classes</code><code class="o">...</code></pre>
<p>Notice that as the depth increases, we tend to get very strangely shaped
classification regions; for example, at a depth of five, there is a tall
and skinny purple region between the yellow and blue regions.
It’s clear that this is less a result of the true, intrinsic
data distribution, and more a result of the particular sampling or noise
properties of the data. That is, this decision tree, even at only five
levels deep, is clearly overfitting our data.<a data-startref="ix_ch44-asciidoc3" data-type="indexterm" id="idm45858730191840"/></p>
</div></section>
<section data-pdf-bookmark="Decision Trees and Overfitting" data-type="sect2"><div class="sect2" id="ch_0508-random-forests_decision-trees-and-overfitting">
<h2>Decision Trees and Overfitting</h2>
<p><a data-primary="decision trees" data-secondary="overfitting" data-type="indexterm" id="idm45858730176656"/><a data-primary="overfitting" data-type="indexterm" id="idm45858730175680"/>Such overfitting turns out to be a general property of decision trees:
it is very easy to go too deep in the tree, and thus to fit details of
the particular data rather than the overall properties of the
distributions it is drawn from. Another way to see this overfitting is
to look at models trained on different subsets of the data—for example, in
<a data-type="xref" href="#fig_images_in_0508-decision-tree-overfitting">Figure 44-5</a> we train two different trees, each on half of the
original data.</p>
<figure><div class="figure" id="fig_images_in_0508-decision-tree-overfitting">
<img alt="05.08 decision tree overfitting" height="191" src="assets/05.08-decision-tree-overfitting.png" width="600"/>
<h6><span class="label">Figure 44-5. </span>An example of two randomized decision trees<sup><a data-type="noteref" href="ch44.xhtml#idm45858730171984" id="idm45858730171984-marker">3</a></sup></h6>
</div></figure>
<p>It is clear that in some places the two trees produce consistent results
(e.g., in the four corners), while in other places the two trees give
very different classifications (e.g., in the regions between any two
clusters). The key observation is that the inconsistencies tend to
happen where the classification is less certain, and thus by using
information from <em>both</em> of these trees, we might come up with a better
result!</p>
<p>If you are running this notebook live, the following function will allow
you to interactively display the fits of trees trained on a random
subset of the data:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">7</code><code class="p">]:</code> <code class="c1"># helpers_05_08 is found in the online appendix</code>
        <code class="kn">import</code> <code class="nn">helpers_05_08</code>
        <code class="n">helpers_05_08</code><code class="o">.</code><code class="n">randomized_tree_interactive</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">7</code><code class="p">]:</code> <code class="n">interactive</code><code class="p">(</code><code class="n">children</code><code class="o">=</code><code class="p">(</code><code class="n">Dropdown</code><code class="p">(</code><code class="n">description</code><code class="o">=</code><code class="s1">'random_state'</code><code class="p">,</code> <code class="n">options</code><code class="o">=</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="mi">100</code><code class="p">),</code>
         <code class="o">&gt;</code> <code class="n">value</code><code class="o">=</code><code class="mi">0</code><code class="p">),</code> <code class="n">Output</code><code class="p">()),</code> <code class="n">_dom_classes</code><code class="o">...</code></pre>
<p>Just as using information from two trees improves our results, we might
expect that using information from many trees would improve our results
even further.<a data-startref="ix_ch44-asciidoc2" data-type="indexterm" id="idm45858730094544"/><a data-startref="ix_ch44-asciidoc1" data-type="indexterm" id="idm45858730088400"/></p>
</div></section>
</div></section>
<section data-pdf-bookmark="Ensembles of Estimators: Random Forests" data-type="sect1"><div class="sect1" id="ch_0508-random-forests_ensembles-of-estimators-random-forests">
<h1>Ensembles of Estimators: Random Forests</h1>
<p><a data-primary="random forests" data-type="indexterm" id="ix_ch44-asciidoc4"/><a data-primary="random forests" data-secondary="ensembles of estimators" data-type="indexterm" id="ix_ch44-asciidoc5"/>This <a data-primary="random forests" data-secondary="basics" data-type="indexterm" id="ix_ch44-asciidoc6"/>notion—that multiple <a data-primary="bagging" data-type="indexterm" id="idm45858730036832"/>overfitting estimators can be combined to
reduce the effect of this overfitting—is what underlies an ensemble
method called <em>bagging</em>. Bagging makes use of an ensemble (a grab bag,
perhaps) of parallel estimators, each of which overfits the data, and
averages the results to find a better classification. <a data-primary="random forests" data-secondary="defined" data-type="indexterm" id="idm45858730035712"/>An ensemble of
randomized decision trees is known as a <em>random forest</em>.</p>
<p>This type of bagging classification can be done manually using
Scikit-Learn’s <code>BaggingClassifier</code> meta-estimator, as shown
here (see <a data-type="xref" href="#fig_0508-random-forests_files_in_output_28_0">Figure 44-6</a>).</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">8</code><code class="p">]:</code> <code class="kn">from</code> <code class="nn">sklearn.tree</code> <code class="kn">import</code> <code class="n">DecisionTreeClassifier</code>
        <code class="kn">from</code> <code class="nn">sklearn.ensemble</code> <code class="kn">import</code> <code class="n">BaggingClassifier</code>

        <code class="n">tree</code> <code class="o">=</code> <code class="n">DecisionTreeClassifier</code><code class="p">()</code>
        <code class="n">bag</code> <code class="o">=</code> <code class="n">BaggingClassifier</code><code class="p">(</code><code class="n">tree</code><code class="p">,</code> <code class="n">n_estimators</code><code class="o">=</code><code class="mi">100</code><code class="p">,</code> <code class="n">max_samples</code><code class="o">=</code><code class="mf">0.8</code><code class="p">,</code>
                                <code class="n">random_state</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>

        <code class="n">bag</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code>
        <code class="n">visualize_classifier</code><code class="p">(</code><code class="n">bag</code><code class="p">,</code> <code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code></pre>
<p>In this example, we have randomized the data by fitting each estimator
with a random subset of 80% of the training points. In practice,
decision trees are more effectively randomized by injecting some
stochasticity in how the splits are chosen: this way all the data
contributes to the fit each time, but the results of the fit still have
the desired randomness. For example, when determining which feature to
split on, the randomized tree might select from among the top several
features. You can read more technical details about these randomization
strategies in the
<a href="https://oreil.ly/4jrv4">Scikit-Learn
documentation</a> and references within.</p>
<figure class="width-75"><div class="figure" id="fig_0508-random-forests_files_in_output_28_0">
<img alt="output 28 0" height="685" src="assets/output_28_0.png" width="600"/>
<h6><span class="label">Figure 44-6. </span>Decision boundaries for an ensemble of random decision trees</h6>
</div></figure>
<p>In Scikit-Learn, such an optimized ensemble of randomized decision trees
is implemented in the <code>RandomForestClassifier</code> estimator, which takes
care of all the randomization automatically. All you need to do is
select a number of estimators, and it will very quickly—in parallel, if
desired—fit the ensemble of trees (see <a data-type="xref" href="#fig_0508-random-forests_files_in_output_30_0">Figure 44-7</a>).</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">9</code><code class="p">]:</code> <code class="kn">from</code> <code class="nn">sklearn.ensemble</code> <code class="kn">import</code> <code class="n">RandomForestClassifier</code>

        <code class="n">model</code> <code class="o">=</code> <code class="n">RandomForestClassifier</code><code class="p">(</code><code class="n">n_estimators</code><code class="o">=</code><code class="mi">100</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code>
        <code class="n">visualize_classifier</code><code class="p">(</code><code class="n">model</code><code class="p">,</code> <code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">);</code></pre>
<figure class="width-75"><div class="figure" id="fig_0508-random-forests_files_in_output_30_0">
<img alt="output 30 0" height="377" src="assets/output_30_0.png" width="600"/>
<h6><span class="label">Figure 44-7. </span>Decision boundaries for a random forest, which is an optimized ensemble of decision trees</h6>
</div></figure>
<p>We see that by averaging over one hundred randomly perturbed models, we end up
with an overall model that is much closer to our intuition about how the
parameter space should be split.<a data-startref="ix_ch44-asciidoc6" data-type="indexterm" id="idm45858729916880"/></p>
</div></section>
<section data-pdf-bookmark="Random Forest Regression" data-type="sect1"><div class="sect1" id="ch_0508-random-forests_random-forest-regression">
<h1>Random Forest Regression</h1>
<p><a data-primary="random forests" data-secondary="regression" data-type="indexterm" id="ix_ch44-asciidoc7"/><a data-primary="regression" data-seealso="specific forms, e.g., linear regression" data-type="indexterm" id="ix_ch44-asciidoc8"/>In the previous section we considered random forests within the context
of classification. Random forests can also be made to work in the case
of regression (that is, with continuous rather than categorical
variables). The estimator to use for this is the
<code>RandomForestRegressor</code>, and the syntax is very similar to what we saw
earlier.</p>
<p>Consider the following data, drawn from the combination of a fast and
slow oscillation (see <a data-type="xref" href="#fig_0508-random-forests_files_in_output_33_0">Figure 44-8</a>).</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">10</code><code class="p">]:</code> <code class="n">rng</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">RandomState</code><code class="p">(</code><code class="mi">42</code><code class="p">)</code>
         <code class="n">x</code> <code class="o">=</code> <code class="mi">10</code> <code class="o">*</code> <code class="n">rng</code><code class="o">.</code><code class="n">rand</code><code class="p">(</code><code class="mi">200</code><code class="p">)</code>

         <code class="k">def</code> <code class="nf">model</code><code class="p">(</code><code class="n">x</code><code class="p">,</code> <code class="n">sigma</code><code class="o">=</code><code class="mf">0.3</code><code class="p">):</code>
             <code class="n">fast_oscillation</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">sin</code><code class="p">(</code><code class="mi">5</code> <code class="o">*</code> <code class="n">x</code><code class="p">)</code>
             <code class="n">slow_oscillation</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">sin</code><code class="p">(</code><code class="mf">0.5</code> <code class="o">*</code> <code class="n">x</code><code class="p">)</code>
             <code class="n">noise</code> <code class="o">=</code> <code class="n">sigma</code> <code class="o">*</code> <code class="n">rng</code><code class="o">.</code><code class="n">randn</code><code class="p">(</code><code class="nb">len</code><code class="p">(</code><code class="n">x</code><code class="p">))</code>

             <code class="k">return</code> <code class="n">slow_oscillation</code> <code class="o">+</code> <code class="n">fast_oscillation</code> <code class="o">+</code> <code class="n">noise</code>

         <code class="n">y</code> <code class="o">=</code> <code class="n">model</code><code class="p">(</code><code class="n">x</code><code class="p">)</code>
         <code class="n">plt</code><code class="o">.</code><code class="n">errorbar</code><code class="p">(</code><code class="n">x</code><code class="p">,</code> <code class="n">y</code><code class="p">,</code> <code class="mf">0.3</code><code class="p">,</code> <code class="n">fmt</code><code class="o">=</code><code class="s1">'o'</code><code class="p">);</code></pre>
<figure><div class="figure" id="fig_0508-random-forests_files_in_output_33_0">
<img alt="output 33 0" height="392" src="assets/output_33_0.png" width="600"/>
<h6><span class="label">Figure 44-8. </span>Data for random forest regression</h6>
</div></figure>
<p>Using the random forest regressor, we can find the best-fit curve (<a data-type="xref" href="#fig_0508-random-forests_files_in_output_35_0">Figure 44-9</a>).</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">11</code><code class="p">]:</code> <code class="kn">from</code> <code class="nn">sklearn.ensemble</code> <code class="kn">import</code> <code class="n">RandomForestRegressor</code>
         <code class="n">forest</code> <code class="o">=</code> <code class="n">RandomForestRegressor</code><code class="p">(</code><code class="mi">200</code><code class="p">)</code>
         <code class="n">forest</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">x</code><code class="p">[:,</code> <code class="kc">None</code><code class="p">],</code> <code class="n">y</code><code class="p">)</code>

         <code class="n">xfit</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">linspace</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="mi">10</code><code class="p">,</code> <code class="mi">1000</code><code class="p">)</code>
         <code class="n">yfit</code> <code class="o">=</code> <code class="n">forest</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">xfit</code><code class="p">[:,</code> <code class="kc">None</code><code class="p">])</code>
         <code class="n">ytrue</code> <code class="o">=</code> <code class="n">model</code><code class="p">(</code><code class="n">xfit</code><code class="p">,</code> <code class="n">sigma</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code>

         <code class="n">plt</code><code class="o">.</code><code class="n">errorbar</code><code class="p">(</code><code class="n">x</code><code class="p">,</code> <code class="n">y</code><code class="p">,</code> <code class="mf">0.3</code><code class="p">,</code> <code class="n">fmt</code><code class="o">=</code><code class="s1">'o'</code><code class="p">,</code> <code class="n">alpha</code><code class="o">=</code><code class="mf">0.5</code><code class="p">)</code>
         <code class="n">plt</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">xfit</code><code class="p">,</code> <code class="n">yfit</code><code class="p">,</code> <code class="s1">'-r'</code><code class="p">);</code>
         <code class="n">plt</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">xfit</code><code class="p">,</code> <code class="n">ytrue</code><code class="p">,</code> <code class="s1">'-k'</code><code class="p">,</code> <code class="n">alpha</code><code class="o">=</code><code class="mf">0.5</code><code class="p">);</code></pre>
<figure><div class="figure" id="fig_0508-random-forests_files_in_output_35_0">
<img alt="output 35 0" height="392" src="assets/output_35_0.png" width="600"/>
<h6><span class="label">Figure 44-9. </span>Random forest model fit to the data</h6>
</div></figure>
<p>Here the true model is shown in the smooth gray curve, while the random
forest model is shown by the jagged red curve. The nonparametric random
forest model is flexible enough to fit the multiperiod data, without us
needing to specifying a multi-period model!<a data-startref="ix_ch44-asciidoc8" data-type="indexterm" id="idm45858729673776"/><a data-startref="ix_ch44-asciidoc7" data-type="indexterm" id="idm45858729673072"/></p>
</div></section>
<section data-pdf-bookmark="Example: Random Forest for Classifying Digits" data-type="sect1"><div class="sect1" id="ch_0508-random-forests_example-random-forest-for-classifying-digits">
<h1>Example: Random Forest for Classifying Digits</h1>
<p><a data-primary="optical character recognition" data-secondary="random forests for classifying digits" data-type="indexterm" id="ix_ch44-asciidoc9"/><a data-primary="random forests" data-secondary="classifying digits with" data-type="indexterm" id="ix_ch44-asciidoc10"/>In <a data-type="xref" href="ch38.xhtml#section-0502-introducing-scikit-learn">Chapter 38</a>, we worked through an example using the digits dataset
included with Scikit-Learn. Let’s use that again here to see
how the random forest classifier can be applied in this context:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">12</code><code class="p">]:</code> <code class="kn">from</code> <code class="nn">sklearn.datasets</code> <code class="kn">import</code> <code class="n">load_digits</code>
         <code class="n">digits</code> <code class="o">=</code> <code class="n">load_digits</code><code class="p">()</code>
         <code class="n">digits</code><code class="o">.</code><code class="n">keys</code><code class="p">()</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">12</code><code class="p">]:</code> <code class="n">dict_keys</code><code class="p">([</code><code class="s1">'data'</code><code class="p">,</code> <code class="s1">'target'</code><code class="p">,</code> <code class="s1">'frame'</code><code class="p">,</code> <code class="s1">'feature_names'</code><code class="p">,</code> <code class="s1">'target_names'</code><code class="p">,</code>
          <code class="o">&gt;</code> <code class="s1">'images'</code><code class="p">,</code> <code class="s1">'DESCR'</code><code class="p">])</code></pre>
<p>To remind us what we’re looking at, we’ll
visualize the first few data points (see <a data-type="xref" href="#fig_0508-random-forests_files_in_output_40_0">Figure 44-10</a>).</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">13</code><code class="p">]:</code> <code class="c1"># set up the figure</code>
         <code class="n">fig</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">figure</code><code class="p">(</code><code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">6</code><code class="p">,</code> <code class="mi">6</code><code class="p">))</code>  <code class="c1"># figure size in inches</code>
         <code class="n">fig</code><code class="o">.</code><code class="n">subplots_adjust</code><code class="p">(</code><code class="n">left</code><code class="o">=</code><code class="mi">0</code><code class="p">,</code> <code class="n">right</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code> <code class="n">bottom</code><code class="o">=</code><code class="mi">0</code><code class="p">,</code> <code class="n">top</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code>
                             <code class="n">hspace</code><code class="o">=</code><code class="mf">0.05</code><code class="p">,</code> <code class="n">wspace</code><code class="o">=</code><code class="mf">0.05</code><code class="p">)</code>

         <code class="c1"># plot the digits: each image is 8x8 pixels</code>
         <code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">64</code><code class="p">):</code>
             <code class="n">ax</code> <code class="o">=</code> <code class="n">fig</code><code class="o">.</code><code class="n">add_subplot</code><code class="p">(</code><code class="mi">8</code><code class="p">,</code> <code class="mi">8</code><code class="p">,</code> <code class="n">i</code> <code class="o">+</code> <code class="mi">1</code><code class="p">,</code> <code class="n">xticks</code><code class="o">=</code><code class="p">[],</code> <code class="n">yticks</code><code class="o">=</code><code class="p">[])</code>
             <code class="n">ax</code><code class="o">.</code><code class="n">imshow</code><code class="p">(</code><code class="n">digits</code><code class="o">.</code><code class="n">images</code><code class="p">[</code><code class="n">i</code><code class="p">],</code> <code class="n">cmap</code><code class="o">=</code><code class="n">plt</code><code class="o">.</code><code class="n">cm</code><code class="o">.</code><code class="n">binary</code><code class="p">,</code> <code class="n">interpolation</code><code class="o">=</code><code class="s1">'nearest'</code><code class="p">)</code>

             <code class="c1"># label the image with the target value</code>
             <code class="n">ax</code><code class="o">.</code><code class="n">text</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="mi">7</code><code class="p">,</code> <code class="nb">str</code><code class="p">(</code><code class="n">digits</code><code class="o">.</code><code class="n">target</code><code class="p">[</code><code class="n">i</code><code class="p">]))</code></pre>
<figure class="width-80"><div class="figure" id="fig_0508-random-forests_files_in_output_40_0">
<img alt="output 40 0" height="234" src="assets/output_40_0.png" width="600"/>
<h6><span class="label">Figure 44-10. </span>Representation of the digits data</h6>
</div></figure>
<p class="pagebreak-before less_space">We can classify the digits using a random forest as follows:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">14</code><code class="p">]:</code> <code class="kn">from</code> <code class="nn">sklearn.model_selection</code> <code class="kn">import</code> <code class="n">train_test_split</code>

         <code class="n">Xtrain</code><code class="p">,</code> <code class="n">Xtest</code><code class="p">,</code> <code class="n">ytrain</code><code class="p">,</code> <code class="n">ytest</code> <code class="o">=</code> <code class="n">train_test_split</code><code class="p">(</code><code class="n">digits</code><code class="o">.</code><code class="n">data</code><code class="p">,</code> <code class="n">digits</code><code class="o">.</code><code class="n">target</code><code class="p">,</code>
                                                         <code class="n">random_state</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code>
         <code class="n">model</code> <code class="o">=</code> <code class="n">RandomForestClassifier</code><code class="p">(</code><code class="n">n_estimators</code><code class="o">=</code><code class="mi">1000</code><code class="p">)</code>
         <code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">Xtrain</code><code class="p">,</code> <code class="n">ytrain</code><code class="p">)</code>
         <code class="n">ypred</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">Xtest</code><code class="p">)</code></pre>
<p>Let’s look at the classification report for this classifier:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">15</code><code class="p">]:</code> <code class="kn">from</code> <code class="nn">sklearn</code> <code class="kn">import</code> <code class="n">metrics</code>
         <code class="nb">print</code><code class="p">(</code><code class="n">metrics</code><code class="o">.</code><code class="n">classification_report</code><code class="p">(</code><code class="n">ypred</code><code class="p">,</code> <code class="n">ytest</code><code class="p">))</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">15</code><code class="p">]:</code>               <code class="n">precision</code>    <code class="n">recall</code>  <code class="n">f1</code><code class="o">-</code><code class="n">score</code>   <code class="n">support</code>

                    <code class="mi">0</code>       <code class="mf">1.00</code>      <code class="mf">0.97</code>      <code class="mf">0.99</code>        <code class="mi">38</code>
                    <code class="mi">1</code>       <code class="mf">0.98</code>      <code class="mf">0.98</code>      <code class="mf">0.98</code>        <code class="mi">43</code>
                    <code class="mi">2</code>       <code class="mf">0.95</code>      <code class="mf">1.00</code>      <code class="mf">0.98</code>        <code class="mi">42</code>
                    <code class="mi">3</code>       <code class="mf">0.98</code>      <code class="mf">0.96</code>      <code class="mf">0.97</code>        <code class="mi">46</code>
                    <code class="mi">4</code>       <code class="mf">0.97</code>      <code class="mf">1.00</code>      <code class="mf">0.99</code>        <code class="mi">37</code>
                    <code class="mi">5</code>       <code class="mf">0.98</code>      <code class="mf">0.96</code>      <code class="mf">0.97</code>        <code class="mi">49</code>
                    <code class="mi">6</code>       <code class="mf">1.00</code>      <code class="mf">1.00</code>      <code class="mf">1.00</code>        <code class="mi">52</code>
                    <code class="mi">7</code>       <code class="mf">1.00</code>      <code class="mf">0.96</code>      <code class="mf">0.98</code>        <code class="mi">50</code>
                    <code class="mi">8</code>       <code class="mf">0.94</code>      <code class="mf">0.98</code>      <code class="mf">0.96</code>        <code class="mi">46</code>
                    <code class="mi">9</code>       <code class="mf">0.98</code>      <code class="mf">0.98</code>      <code class="mf">0.98</code>        <code class="mi">47</code>

             <code class="n">accuracy</code>                           <code class="mf">0.98</code>       <code class="mi">450</code>
            <code class="n">macro</code> <code class="n">avg</code>       <code class="mf">0.98</code>      <code class="mf">0.98</code>      <code class="mf">0.98</code>       <code class="mi">450</code>
         <code class="n">weighted</code> <code class="n">avg</code>       <code class="mf">0.98</code>      <code class="mf">0.98</code>      <code class="mf">0.98</code>       <code class="mi">450</code></pre>
<p>And for good measure, plot the confusion matrix (see <a data-type="xref" href="#fig_0508-random-forests_files_in_output_46_0">Figure 44-11</a>).</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">16</code><code class="p">]:</code> <code class="kn">from</code> <code class="nn">sklearn.metrics</code> <code class="kn">import</code> <code class="n">confusion_matrix</code>
         <code class="kn">import</code> <code class="nn">seaborn</code> <code class="k">as</code> <code class="nn">sns</code>
         <code class="n">mat</code> <code class="o">=</code> <code class="n">confusion_matrix</code><code class="p">(</code><code class="n">ytest</code><code class="p">,</code> <code class="n">ypred</code><code class="p">)</code>
         <code class="n">sns</code><code class="o">.</code><code class="n">heatmap</code><code class="p">(</code><code class="n">mat</code><code class="o">.</code><code class="n">T</code><code class="p">,</code> <code class="n">square</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code> <code class="n">annot</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code> <code class="n">fmt</code><code class="o">=</code><code class="s1">'d'</code><code class="p">,</code>
                     <code class="n">cbar</code><code class="o">=</code><code class="kc">False</code><code class="p">,</code> <code class="n">cmap</code><code class="o">=</code><code class="s1">'Blues'</code><code class="p">)</code>
         <code class="n">plt</code><code class="o">.</code><code class="n">xlabel</code><code class="p">(</code><code class="s1">'true label'</code><code class="p">)</code>
         <code class="n">plt</code><code class="o">.</code><code class="n">ylabel</code><code class="p">(</code><code class="s1">'predicted label'</code><code class="p">);</code></pre>
<p>We find that a simple, untuned random forest results in a quite accurate
classification of the digits data.<a data-startref="ix_ch44-asciidoc10" data-type="indexterm" id="idm45858729083248"/><a data-startref="ix_ch44-asciidoc9" data-type="indexterm" id="idm45858729082640"/></p>
<figure><div class="figure" id="fig_0508-random-forests_files_in_output_46_0">
<img alt="output 46 0" height="363" src="assets/output_46_0.png" width="600"/>
<h6><span class="label">Figure 44-11. </span>Confusion matrix for digit classification with random forests</h6>
</div></figure>
</div></section>
<section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="ch_0508-random-forests_summary">
<h1>Summary</h1>
<p><a data-primary="random forests" data-secondary="advantages/disadvantages" data-type="indexterm" id="idm45858729057136"/>This chapter provided a brief introduction to the concept of ensemble
estimators, and in particular the random forest, an ensemble of
randomized decision trees. Random forests are a powerful method with
several advantages:</p>
<ul>
<li>
<p>Both training and prediction are very fast, because of the simplicity
of the underlying decision trees. In addition, both tasks can be
straightforwardly parallelized, because the individual trees are
entirely independent entities.</p>
</li>
<li>
<p>The multiple trees allow for a probabilistic classification: a
majority vote among estimators gives an estimate of the probability
(accessed in Scikit-Learn with the <code>predict_proba</code> method).</p>
</li>
<li>
<p>The nonparametric model is extremely flexible and can thus perform
well on tasks that are underfit by other estimators.</p>
</li>
</ul>
<p>A primary disadvantage of random forests is that the results are not
easily interpretable: that is, if you would like to draw conclusions
about the <em>meaning</em> of the classification model, random forests may not
be the best choice<a data-startref="ix_ch44-asciidoc5" data-type="indexterm" id="idm45858729005712"/><a data-startref="ix_ch44-asciidoc4" data-type="indexterm" id="idm45858729005104"/>.<a data-startref="ix_ch44-asciidoc0" data-type="indexterm" id="idm45858729004368"/></p>
</div></section>
<div data-type="footnotes"><p data-type="footnote" id="idm45858730712544"><sup><a href="ch44.xhtml#idm45858730712544-marker">1</a></sup> Code to produce this figure can be found in the <a href="https://oreil.ly/xP9ZI">online appendix</a>.</p><p data-type="footnote" id="idm45858730648048"><sup><a href="ch44.xhtml#idm45858730648048-marker">2</a></sup> Code to produce this figure can be found in the <a href="https://oreil.ly/H4WFg">online appendix</a>.</p><p data-type="footnote" id="idm45858730171984"><sup><a href="ch44.xhtml#idm45858730171984-marker">3</a></sup> Code to produce this figure can be found in the <a href="https://oreil.ly/PessV">online appendix</a>.</p></div></div></section></div></body></html>