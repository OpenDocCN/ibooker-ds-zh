<html><head></head><body><section data-pdf-bookmark="Chapter 26. Data Ethics" data-type="chapter" epub:type="chapter"><div class="chapter" id="data_ethics">&#13;
<h1><span class="label">Chapter 26. </span>Data Ethics</h1>&#13;
&#13;
<blockquote data-type="epigraph" epub:type="epigraph">&#13;
    <p>Grub first, then ethics.</p>&#13;
    <p data-type="attribution">Bertolt Brecht</p>&#13;
</blockquote>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="What Is Data Ethics?" data-type="sect1"><div class="sect1" id="what_is_data_ethics">&#13;
<h1>What Is Data Ethics?</h1>&#13;
&#13;
<p>With<a data-primary="data ethics" data-secondary="examples of data misuse" data-type="indexterm" id="idm45635703589304"/> the use of data comes the misuse of data.&#13;
This has pretty much always been the case,&#13;
but recently this idea has been reified as “data ethics”&#13;
and has featured somewhat prominently in the news.</p>&#13;
&#13;
<p>For instance, in the 2016 election, a company called Cambridge Analytica&#13;
<a href="https://en.wikipedia.org/wiki/Facebook%E2%80%93Cambridge_Analytica_data_scandal">improperly accessed Facebook data</a>&#13;
and used that for political ad targeting.</p>&#13;
&#13;
<p>In 2018, an autonomous car being tested by Uber&#13;
<a href="https://www.nytimes.com/2018/05/24/technology/uber-autonomous-car-ntsb-investigation.html">struck and killed a pedestrian</a>&#13;
(there was a “safety driver” in the car, but apparently she was not paying attention at the time).</p>&#13;
&#13;
<p>Algorithms are used&#13;
<a href="https://www.themarshallproject.org/2015/08/04/the-new-science-of-sentencing">to predict the risk that criminals will reoffend</a>&#13;
and to sentence them accordingly. Is this more or less fair than allowing judges to determine the same?</p>&#13;
&#13;
<p>Some airlines&#13;
<a href="https://twitter.com/ShelkeGaneshB/status/1066161967105216512">assign families separate seats</a>,&#13;
forcing them to pay extra to sit together. Should a data scientist have stepped in to prevent this?&#13;
(Many data scientists in the linked thread seem to believe so.)</p>&#13;
&#13;
<p>“Data ethics” purports to provide answers to these questions,&#13;
or at least a framework for wrestling with them.&#13;
I’m not so arrogant as to tell you <em>how</em> to think about these things&#13;
(and “these things” are changing quickly), so in this chapter&#13;
we’ll just take a quick tour of some of the most relevant issues and (hopefully)&#13;
inspire you to think about them further.&#13;
(Alas, I am not a good enough philosopher to do ethics <em>from scratch</em>.)</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="No, Really, What Is Data Ethics?" data-type="sect1"><div class="sect1" id="idm45635703581000">&#13;
<h1>No, Really, What Is Data Ethics?</h1>&#13;
&#13;
<p>Well, let’s start<a data-primary="data ethics" data-secondary="definition of term" data-type="indexterm" id="idm45635703579432"/> with “what is ethics?”&#13;
If<a data-primary="ethics" data-seealso="data ethics" data-type="indexterm" id="idm45635703578248"/> you take the average of every definition you can find,&#13;
you end up with something like <em>ethics</em> is a framework for thinking about&#13;
“right” and “wrong” behavior. <em>Data</em> ethics, then, is a framework&#13;
for thinking about right and wrong behavior involving data.</p>&#13;
&#13;
<p>Some people talk as if “data ethics” is (perhaps implicitly) a set of commandments about what you may and may not do. Some of them are hard at work creating manifestos, others crafting mandatory pledges&#13;
to which they hope to make you swear. Still others are campaigning&#13;
for data ethics to be made a mandatory part of the data science curriculum—hence this chapter, as a means of hedging my bets in case they succeed.</p>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>Curiously,&#13;
<a href="https://www.washingtonpost.com/news/on-leadership/wp/2014/01/13/can-you-teach-businessmen-to-be-ethical">there is not much data suggesting that ethics courses lead to ethical behavior</a>,&#13;
in which case perhaps this campaign is itself data-unethical!</p>&#13;
</div>&#13;
&#13;
<p>Other people (for example, yours truly) think that reasonable people&#13;
will frequently disagree over subtle matters of right and wrong,&#13;
and that the important part of data ethics is committing to&#13;
<em>consider</em> the ethical consequences of your behaviors.&#13;
This requires <em>understanding</em> the sorts of things that many&#13;
“data ethics” advocates don’t approve of, but it doesn’t necessarily&#13;
require agreeing with their disapproval.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Should I Care About Data Ethics?" data-type="sect1"><div class="sect1" id="idm45635703571368">&#13;
<h1>Should I Care About Data Ethics?</h1>&#13;
&#13;
<p>You<a data-primary="data ethics" data-secondary="wide-reaching effects of data science" data-type="indexterm" id="idm45635703569944"/> should care about ethics whatever your job.&#13;
If your job involves data, you are free to characterize your caring as “data ethics,” but you should care just as much about ethics in the nondata parts of your job.</p>&#13;
&#13;
<p>Perhaps what’s different about technology jobs is that technology <em>scales</em>,&#13;
and that decisions made by individuals working on technology&#13;
problems (whether data-related or not) have potentially&#13;
wide-reaching effects.</p>&#13;
&#13;
<p>A tiny change to a news discovery&#13;
algorithm could be the difference between millions of people&#13;
reading an article and no one reading it.</p>&#13;
&#13;
<p>A single flawed algorithm for granting parole that’s used all over the country systematically affects millions of people, whereas a flawed-in-its-own-way parole board affects only the people who come before it.</p>&#13;
&#13;
<p>So yes, in general, you should care about what effects your work has on the world.&#13;
And the broader the effects of your work, the more you need to worry about these things.</p>&#13;
&#13;
<p>Unfortunately, some of the discourse around data ethics involves people&#13;
trying to force their ethical conclusions on you. Whether you should care&#13;
about the same things <em>they</em> care about is really up to you.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Building Bad Data Products" data-type="sect1"><div class="sect1" id="bad_data_products">&#13;
<h1>Building Bad Data Products</h1>&#13;
&#13;
<p>Some “data ethics” issues<a data-primary="data ethics" data-secondary="issues resulting from bad products" data-type="indexterm" id="idm45635703562840"/> are the result of building <em>bad products</em>.</p>&#13;
&#13;
<p>For example, Microsoft&#13;
<a href="https://en.wikipedia.org/wiki/Tay_(bot)">released a chat bot named Tay</a> that parroted back things tweeted to it,&#13;
which the internet quickly discovered enabled them to get Tay&#13;
to tweet all sorts of offensive things.&#13;
It seems unlikely that anyone at Microsoft&#13;
debated the ethicality of releasing a “racist” bot;&#13;
most likely they simply built a bot and failed to think through&#13;
how it could be abused. This is perhaps a low bar, but let’s agree&#13;
that you should think about how the things you build could be abused.</p>&#13;
&#13;
<p>Another<a data-primary="data ethics" data-secondary="offensive predictions" data-type="indexterm" id="idm45635703559208"/><a data-primary="predictive models" data-secondary="guarding against potentially offensive predictions" data-type="indexterm" id="idm45635703558200"/> example is that Google Photos at one point&#13;
<a href="https://www.theverge.com/2018/1/12/16882408/google-racist-gorillas-photo-recognition-algorithm-ai">used an image recognition algorithm that would sometimes classify pictures of black people as “gorillas”</a>.&#13;
Again, it is extremely unlikely that anyone at Google <em>explicitly decided</em> to ship this feature&#13;
(let alone grappled with the “ethics” of it).&#13;
Here it seems likely the problem is some combination of bad training data,&#13;
model inaccuracy,&#13;
and the gross offensiveness of the mistake&#13;
(if the model had occasionally categorized mailboxes as fire trucks,&#13;
 probably no one would have cared).</p>&#13;
&#13;
<p>In this case the solution is less obvious: how can you ensure that your trained model won’t make predictions that are in some way offensive?&#13;
Of course you should train (and test) your model on a diverse range of inputs, but can you ever be sure that there isn’t <em>some</em> input somewhere out there that will make your model behave in a way that embarrasses you? This is a hard problem. (Google seems to have “solved” it by simply refusing to ever predict “gorilla.”)</p>&#13;
&#13;
<p> </p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Trading Off Accuracy and Fairness" data-type="sect1"><div class="sect1" id="idm45635703553512">&#13;
<h1>Trading Off Accuracy and Fairness</h1>&#13;
&#13;
<p>Imagine<a data-primary="data ethics" data-secondary="tradeoffs between accuracy and fairness" data-type="indexterm" id="idm45635703551912"/><a data-primary="predictive models" data-secondary="tradeoffs between accuracy and fairness" data-type="indexterm" id="idm45635703550888"/> you are building a model that predicts how likely people are to take some action. You do a pretty good job (<a data-type="xref" href="#pretty-good-job">Table 26-1</a>).</p>&#13;
<table id="pretty-good-job">&#13;
<caption><span class="label">Table 26-1. </span>A pretty good job</caption>&#13;
<thead>&#13;
<tr>&#13;
<th>Prediction</th>&#13;
<th>People</th>&#13;
<th>Actions</th>&#13;
<th>%</th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr>&#13;
<td><p>Unlikely</p></td>&#13;
<td><p>125</p></td>&#13;
<td><p>25</p></td>&#13;
<td><p>20%</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>Likely</p></td>&#13;
<td><p>125</p></td>&#13;
<td><p>75</p></td>&#13;
<td><p>60%</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
&#13;
<p>Of the people you predict are unlikely to take the action, only 20% of them do. Of the people you predict are likely to take the action, 60% of them do. Seems not terrible.</p>&#13;
&#13;
<p>Now imagine that the people can be split into two groups: A and B.&#13;
Some of your colleagues are concerned that your model is <em>unfair</em> to one of the groups.&#13;
Although the model does not take group membership into account, it does consider&#13;
various other factors that correlate in complicated ways with group membership.</p>&#13;
&#13;
<p>Indeed, when you break down the predictions by group, you discover surprising statistics (<a data-type="xref" href="#surprising-statistics">Table 26-2</a>).</p>&#13;
<table id="surprising-statistics">&#13;
<caption><span class="label">Table 26-2. </span>Surprising statistics</caption>&#13;
<thead>&#13;
<tr>&#13;
<th>Group</th>&#13;
<th>Prediction</th>&#13;
<th>People</th>&#13;
<th>Actions</th>&#13;
<th>%</th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr>&#13;
<td><p>A</p></td>&#13;
<td><p>Unlikely</p></td>&#13;
<td><p>100</p></td>&#13;
<td><p>20</p></td>&#13;
<td><p>20%</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>A</p></td>&#13;
<td><p>Likely</p></td>&#13;
<td><p>25</p></td>&#13;
<td><p>15</p></td>&#13;
<td><p>60%</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>B</p></td>&#13;
<td><p>Unlikely</p></td>&#13;
<td><p>25</p></td>&#13;
<td><p>5</p></td>&#13;
<td><p>20%</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>B</p></td>&#13;
<td><p>Likely</p></td>&#13;
<td><p>100</p></td>&#13;
<td><p>60</p></td>&#13;
<td><p>60%</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
&#13;
<p>Is your model unfair? The data scientists on your team make a variety of arguments:</p>&#13;
<dl>&#13;
<dt>Argument 1</dt>&#13;
<dd>&#13;
<p>Your model classifies 80% of group A as “unlikely” but 80% of group B as “likely.”&#13;
This data scientist complains that the model is treating the two groups unfairly&#13;
in the sense that it is generating vastly different predictions across the two groups.</p>&#13;
</dd>&#13;
<dt>Argument 2</dt>&#13;
<dd>&#13;
<p>Regardless of group membership, if we predict “unlikely” you have a 20% chance of action, and&#13;
if we predict “likely” you have a 60% chance of action.&#13;
This data scientist insists that the model is “accurate”&#13;
in the sense that its predictions seem to <em>mean</em> the same things&#13;
no matter which group you belong to.</p>&#13;
</dd>&#13;
<dt>Argument 3</dt>&#13;
<dd>&#13;
<p>40/125 = 32% of group B were falsely labeled “likely,” whereas only 10/125 = 8% of group A were falsely labeled “likely.”&#13;
This data scientist (who considers a “likely” prediction to be a bad thing)&#13;
insists that the model unfairly stigmatizes group B.</p>&#13;
</dd>&#13;
<dt>Argument 4</dt>&#13;
<dd>&#13;
<p>20/125 = 16% of group A were falsely labeled “unlikely,”&#13;
whereas only 5/125 = 4% of group B were falsely labeled “unlikely.”&#13;
This data scientist (who considers an “unlikely” prediction to be a bad thing)&#13;
insists that the model unfairly stigmatizes group A.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>Which of these data scientists is correct? Are any of them correct? Perhaps it depends on the context.</p>&#13;
&#13;
<p>Possibly you feel one way if the two groups are “men” and “women” and another way if the two groups&#13;
are “R users” and “Python users.” Or possibly not if it turns out that Python users skew male and R users skew female?</p>&#13;
&#13;
<p>Possibly you feel one way if the model is for predicting whether a DataSciencester user will&#13;
<em>apply</em> for a job through the DataSciencester job board and another way&#13;
if the model is predicting whether a user will <em>pass</em> such an interview.</p>&#13;
&#13;
<p>Possibly your opinion depends on the model itself,&#13;
what features it takes into account,&#13;
and what data it was trained on.</p>&#13;
&#13;
<p>In any event,&#13;
my point is to impress upon you that&#13;
there can be a tradeoff between “accuracy” and “fairness”&#13;
(depending, of course, on how you define them)&#13;
and that these tradeoffs don’t always have obvious “right” solutions.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Collaboration" data-type="sect1"><div class="sect1" id="idm45635703552856">&#13;
<h1>Collaboration</h1>&#13;
&#13;
<p>A<a data-primary="data ethics" data-secondary="government restrictions" data-type="indexterm" id="idm45635703454984"/> repressive (by your standards) country’s government officials have finally decided to&#13;
allow citizens to join DataSciencester. However, they insist&#13;
that the users from their country not be allowed to discuss deep learning.&#13;
Furthermore, they want you to report to them the names of any users&#13;
who even <em>try</em> to seek out information on deep learning.</p>&#13;
&#13;
<p>Are this country’s data scientists better off with access&#13;
to the topic-limited (and surveilled) DataSciencester that&#13;
you’d be allowed to offer? Or are the proposed restrictions&#13;
so awful that they’d be better off with no access at all?</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Interpretability" data-type="sect1"><div class="sect1" id="idm45635703452152">&#13;
<h1>Interpretability</h1>&#13;
&#13;
<p>The<a data-primary="data ethics" data-secondary="model selection" data-type="indexterm" id="idm45635703450648"/> DataSciencester HR department asks you to develop a model predicting&#13;
which employees are most at risk of leaving the company, so that it can&#13;
intervene and try to make them happier.&#13;
(Attrition rate is an important component of the&#13;
 “10 Happiest Workplaces” magazine feature that your CEO&#13;
 aspires to appear in.)</p>&#13;
&#13;
<p> </p>&#13;
&#13;
<p>You’ve collected an assortment of historical data and are considering three models:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>A decision tree</p>&#13;
</li>&#13;
<li>&#13;
<p>A neural network</p>&#13;
</li>&#13;
<li>&#13;
<p>A high-priced “retention expert”</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>One of your data scientists insists that&#13;
you should just use whichever model performs best.</p>&#13;
&#13;
<p>A second insists that you not use the neural network model, as only the other two can explain their predictions, and that only explanation of the predictions can help HR institute widespread changes (as opposed to one-off interventions).</p>&#13;
&#13;
<p>A third says that while the “expert”&#13;
can offer <em>an</em> explanation for her predictions,&#13;
there’s no reason to take her at her word that it describes&#13;
the <em>real</em> reasons she predicted the way she did.</p>&#13;
&#13;
<p>As with our other examples, there is no absolute best choice here.&#13;
In some circumstances&#13;
(possibly for legal reasons or if your predictions are somehow life-changing)&#13;
you might prefer a model that performs worse but whose predictions can be explained.&#13;
In others, you might just want the model that predicts best.&#13;
In still others, perhaps there is no interpretable model that performs well.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Recommendations" data-type="sect1"><div class="sect1" id="idm45635703441672">&#13;
<h1>Recommendations</h1>&#13;
&#13;
<p>As<a data-primary="data ethics" data-secondary="censorship" data-type="indexterm" id="idm45635703440168"/> we discussed in <a data-type="xref" href="ch23.html#recommender_systems">Chapter 23</a>, a common data science application&#13;
involves recommending things to people. When someone watches a YouTube video,&#13;
YouTube recommends videos they should watch next.</p>&#13;
&#13;
<p>YouTube makes money through advertising and (presumably) wants to recommend videos&#13;
that you are more likely to watch, so that they can show you more advertisements.&#13;
However, it turns out that people like to watch videos about conspiracy theories,&#13;
which tend to feature in the recommendations.</p>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>At the time I wrote this chapter, if you searched YouTube for “saturn”&#13;
the third result was “Something Is Happening On Saturn… Are THEY Hiding It?”&#13;
which maybe gives you a sense of the kinds of videos I’m talking about.</p>&#13;
</div>&#13;
&#13;
<p>Does YouTube have an obligation not to recommend conspiracy videos?&#13;
Even if that’s what lots of people seem to want to watch?</p>&#13;
&#13;
<p>A different example is that&#13;
if you go to google.com (or bing.com) and start typing a search,&#13;
the search engine will offer suggestions to autocomplete your search.&#13;
These suggestions are based (at least in part) on other people’s searches;&#13;
in particular, if other people are searching for unsavory things&#13;
this may be reflected in your suggestions.</p>&#13;
&#13;
<p>Should a search engine try to affirmatively filter out suggestions it doesn’t like?&#13;
Google (for whatever reason) seems intent on not suggesting things related to people’s&#13;
religion. For example, if you type “mitt romney m” into Bing,&#13;
the first suggestion is “mitt romney mormon” (which is what I would have expected),&#13;
whereas Google refuses to provide that suggestion.</p>&#13;
&#13;
<p>Indeed, Google explicitly filters out autosuggestions that it considers&#13;
<a href="https://blog.google/products/search/google-search-autocomplete/">“offensive or disparaging”</a>.&#13;
(How it decides what’s offensive or disparaging is left vague.)&#13;
And yet sometimes the truth is offensive.&#13;
Is protecting people from those suggestions the ethical thing to do?&#13;
Or is it an unethical thing to do?&#13;
Or is it not a question of ethics at all?</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Biased Data" data-type="sect1"><div class="sect1" id="idm45635703432248">&#13;
<h1>Biased Data</h1>&#13;
&#13;
<p>In <a data-type="xref" href="ch21.html#word_vectors">“Word Vectors”</a> we<a data-primary="data ethics" data-secondary="biased data" data-type="indexterm" id="idm45635703429944"/><a data-primary="biased data" data-type="indexterm" id="idm45635703428968"/> used a corpus of documents&#13;
to learn vector embeddings for words. These vectors&#13;
were<a data-primary="distributional similarity" data-type="indexterm" id="idm45635703428168"/> designed to exhibit <em>distributional similarity</em>.&#13;
That is, words that appear in similar contexts should have similar vectors.&#13;
In particular, any biases that exist in the training data will&#13;
be reflected in the word vectors themselves.</p>&#13;
&#13;
<p>For example, if our documents are all about how R users are moral reprobates&#13;
and how Python users are paragons of virtue, most likely the model&#13;
will learn such associations for “Python” and “R.”</p>&#13;
&#13;
<p>More commonly, word vectors are based on some combination of&#13;
Google News articles, Wikipedia, books, and crawled web pages.&#13;
This means that they’ll learn whatever distributional patterns&#13;
are present in those sources.</p>&#13;
&#13;
<p>For example, if the majority of news articles about software engineers&#13;
are about <em>male</em> software engineers, then the learned vector for&#13;
“software” might lie closer to vectors for other “male” words&#13;
than to the vectors for “female” words.</p>&#13;
&#13;
<p>At that point any downstream applications you build using these vectors&#13;
might also exhibit this closeness. Depending on the application,&#13;
this may or may not be a problem for you.&#13;
In that case there are various&#13;
techniques that you can try to “remove” specific biases,&#13;
although you’ll probably never get all of them.&#13;
But it’s something you should be aware of.</p>&#13;
&#13;
<p>Similarly, as<a data-primary="nonrepresentative data" data-type="indexterm" id="idm45635703423096"/> in the “photos” example in <a data-type="xref" href="#bad_data_products">“Building Bad Data Products”</a>,&#13;
if you train a model on nonrepresentative data,&#13;
there’s a strong possibility it will perform poorly in the real world,&#13;
possibly in ways that are offensive or embarrassing.</p>&#13;
&#13;
<p>Along different lines, it’s also possible that your algorithms&#13;
might codify actual biases that exist out in the world.&#13;
For example, your parole model may do a perfect job&#13;
of predicting which released criminals get rearrested,&#13;
but if those rearrests are themselves the result of biased real-world processes,&#13;
then your model might be perpetuating that bias.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Data Protection" data-type="sect1"><div class="sect1" id="idm45635703420360">&#13;
<h1>Data Protection</h1>&#13;
&#13;
<p>You<a data-primary="data ethics" data-secondary="privacy" data-type="indexterm" id="idm45635703418856"/> know a lot about the DataSciencester users.&#13;
You know what technologies they like,&#13;
who their data scientist friends are,&#13;
where they work,&#13;
how much they earn,&#13;
how much time they spend on the site,&#13;
which job postings they click on, and so forth.</p>&#13;
&#13;
<p>The VP of Monetization wants to sell this data to advertisers,&#13;
who are eager to market their various “big data” solutions to your users.&#13;
The Chief Scientist wants to share this data with academic researchers,&#13;
who are keen to publish papers about who becomes a data scientist.&#13;
The VP of Electioneering has plans to provide this data to political campaigns,&#13;
most of whom are eager to recruit their own data science organizations.&#13;
And the VP of Government Affairs would like to use this data to answer questions from law enforcement.</p>&#13;
&#13;
<p>Thanks to a forward-thinking VP of Contracts,&#13;
your users agreed to terms of service that guarantee you&#13;
the right to do pretty much whatever you want with their data.</p>&#13;
&#13;
<p>However (as you have now come to expect), various of the data scientists&#13;
on your team raise various objections to these various uses.&#13;
One thinks it’s wrong to hand the data over to advertisers;&#13;
another worries that academics can’t be trusted to safeguard the data responsibly.&#13;
A third thinks that the company should stay out of politics,&#13;
while the last insists that police can’t be trusted and that&#13;
collaborating with law enforcement will harm innocent people.</p>&#13;
&#13;
<p>Do any of these data scientists have a point?</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="In Summary" data-type="sect1"><div class="sect1" id="idm45635703414440">&#13;
<h1>In Summary</h1>&#13;
&#13;
<p>These are a lot of things to worry about!&#13;
And there are countless more we haven’t mentioned,&#13;
and still more that will come up in the future&#13;
but that would never occur to us today.</p>&#13;
&#13;
<p> &#13;
 </p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="For Further Exploration" data-type="sect1"><div class="sect1" id="idm45635703412248">&#13;
<h1>For Further Exploration</h1>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>There<a data-primary="data ethics" data-secondary="resources for learning about" data-type="indexterm" id="idm45635703410056"/> is no shortage of people professing important thoughts about data ethics.&#13;
Searching on Twitter (or your favorite news site) is probably the best way to&#13;
find out about the most current data ethics controversy.</p>&#13;
</li>&#13;
<li>&#13;
<p>If you want something slightly more practical,&#13;
Mike Loukides, Hilary Mason, and DJ Patil have written a short ebook, <a href="https://www.oreilly.com/library/view/ethics-and-data/9781492043898/"><em>Ethics and Data Science</em></a>, on putting data ethics into practice,&#13;
which I am honor-bound to recommend on account of Mike being the person who agreed to publish&#13;
<em>Data Science from Scratch</em> way back in 2014. (Exercise: is this ethical of me?)</p>&#13;
</li>&#13;
</ul>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section></body></html>