["```py\nIn [1]: %matplotlib inline\n        import numpy as np\n        import matplotlib.pyplot as plt\n        plt.style.use('seaborn-whitegrid')\n```", "```py\nIn [2]: rng = np.random.RandomState(1)\n        X = np.dot(rng.rand(2, 2), rng.randn(2, 200)).T\n        plt.scatter(X[:, 0], X[:, 1])\n        plt.axis('equal');\n```", "```py\nIn [3]: from sklearn.decomposition import PCA\n        pca = PCA(n_components=2)\n        pca.fit(X)\nOut[3]: PCA(n_components=2)\n```", "```py\nIn [4]: print(pca.components_)\nOut[4]: [[-0.94446029 -0.32862557]\n         [-0.32862557  0.94446029]]\n```", "```py\nIn [5]: print(pca.explained_variance_)\nOut[5]: [0.7625315 0.0184779]\n```", "```py\nIn [6]: def draw_vector(v0, v1, ax=None):\n            ax = ax or plt.gca()\n            arrowprops=dict(arrowstyle='->', linewidth=2,\n                            shrinkA=0, shrinkB=0)\n            ax.annotate('', v1, v0, arrowprops=arrowprops)\n\n        # plot data\n        plt.scatter(X[:, 0], X[:, 1], alpha=0.2)\n        for length, vector in zip(pca.explained_variance_, pca.components_):\n            v = vector * 3 * np.sqrt(length)\n            draw_vector(pca.mean_, pca.mean_ + v)\n        plt.axis('equal');\n```", "```py\nIn [7]: pca = PCA(n_components=1)\n        pca.fit(X)\n        X_pca = pca.transform(X)\n        print(\"original shape:   \", X.shape)\n        print(\"transformed shape:\", X_pca.shape)\nOut[7]: original shape:    (200, 2)\n        transformed shape: (200, 1)\n```", "```py\nIn [8]: X_new = pca.inverse_transform(X_pca)\n        plt.scatter(X[:, 0], X[:, 1], alpha=0.2)\n        plt.scatter(X_new[:, 0], X_new[:, 1], alpha=0.8)\n        plt.axis('equal');\n```", "```py\nIn [9]: from sklearn.datasets import load_digits\n        digits = load_digits()\n        digits.data.shape\nOut[9]: (1797, 64)\n```", "```py\nIn [10]: pca = PCA(2)  # project from 64 to 2 dimensions\n         projected = pca.fit_transform(digits.data)\n         print(digits.data.shape)\n         print(projected.shape)\nOut[10]: (1797, 64)\n         (1797, 2)\n```", "```py\nIn [11]: plt.scatter(projected[:, 0], projected[:, 1],\n                     c=digits.target, edgecolor='none', alpha=0.5,\n                     cmap=plt.cm.get_cmap('rainbow', 10))\n         plt.xlabel('component 1')\n         plt.ylabel('component 2')\n         plt.colorbar();\n```", "```py\nIn [12]: pca = PCA().fit(digits.data)\n         plt.plot(np.cumsum(pca.explained_variance_ratio_))\n         plt.xlabel('number of components')\n         plt.ylabel('cumulative explained variance');\n```", "```py\nIn [13]: def plot_digits(data):\n             fig, axes = plt.subplots(4, 10, figsize=(10, 4),\n                                      subplot_kw={'xticks':[], 'yticks':[]},\n                                      gridspec_kw=dict(hspace=0.1, wspace=0.1))\n             for i, ax in enumerate(axes.flat):\n                 ax.imshow(data[i].reshape(8, 8),\n                           cmap='binary', interpolation='nearest',\n                           clim=(0, 16))\n         plot_digits(digits.data)\n```", "```py\nIn [14]: rng = np.random.default_rng(42)\n         rng.normal(10, 2)\nOut[14]: 10.609434159508863\n```", "```py\nIn [15]: rng = np.random.default_rng(42)\n         noisy = rng.normal(digits.data, 4)\n         plot_digits(noisy)\n```", "```py\nIn [16]: pca = PCA(0.50).fit(noisy)\n         pca.n_components_\nOut[16]: 12\n```", "```py\nIn [17]: components = pca.transform(noisy)\n         filtered = pca.inverse_transform(components)\n         plot_digits(filtered)\n```", "```py\nIn [18]: from sklearn.datasets import fetch_lfw_people\n         faces = fetch_lfw_people(min_faces_per_person=60)\n         print(faces.target_names)\n         print(faces.images.shape)\nOut[18]: ['Ariel Sharon' 'Colin Powell' 'Donald Rumsfeld' 'George W Bush'\n          'Gerhard Schroeder' 'Hugo Chavez' 'Junichiro Koizumi' 'Tony Blair']\n         (1348, 62, 47)\n```", "```py\nIn [19]: pca = PCA(150, svd_solver='randomized', random_state=42)\n         pca.fit(faces.data)\nOut[19]: PCA(n_components=150, random_state=42, svd_solver='randomized')\n```", "```py\nIn [20]: fig, axes = plt.subplots(3, 8, figsize=(9, 4),\n                                  subplot_kw={'xticks':[], 'yticks':[]},\n                                  gridspec_kw=dict(hspace=0.1, wspace=0.1))\n         for i, ax in enumerate(axes.flat):\n             ax.imshow(pca.components_[i].reshape(62, 47), cmap='bone')\n```", "```py\nIn [21]: plt.plot(np.cumsum(pca.explained_variance_ratio_))\n         plt.xlabel('number of components')\n         plt.ylabel('cumulative explained variance');\n```", "```py\nIn [22]: # Compute the components and projected faces\n         pca = pca.fit(faces.data)\n         components = pca.transform(faces.data)\n         projected = pca.inverse_transform(components)\n```", "```py\nIn [23]: # Plot the results\n         fig, ax = plt.subplots(2, 10, figsize=(10, 2.5),\n                                subplot_kw={'xticks':[], 'yticks':[]},\n                                gridspec_kw=dict(hspace=0.1, wspace=0.1))\n         for i in range(10):\n             ax[0, i].imshow(faces.data[i].reshape(62, 47), cmap='binary_r')\n             ax[1, i].imshow(projected[i].reshape(62, 47), cmap='binary_r')\n\n         ax[0, 0].set_ylabel('full-dim\\ninput')\n         ax[1, 0].set_ylabel('150-dim\\nreconstruction');\n```"]