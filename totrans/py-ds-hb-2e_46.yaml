- en: 'Chapter 41\. In Depth: Naive Bayes Classification'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The previous four chapters have given a general overview of the concepts of
    machine learning. In the rest of [Part V](part05.xhtml#section-0500-machine-learning),
    we will be taking a closer look first at four algorithms for supervised learning,
    and then at four algorithms for unsupervised learning. We start here with our
    first supervised method, naive Bayes classification.
  prefs: []
  type: TYPE_NORMAL
- en: Naive Bayes models are a group of extremely fast and simple classification algorithms
    that are often suitable for very high-dimensional datasets. Because they are so
    fast and have so few tunable parameters, they end up being useful as a quick-and-dirty
    baseline for a classification problem. This chapter will provide an intuitive
    explanation of how naive Bayes classifiers work, followed by a few examples of
    them in action on some datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Bayesian Classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Naive Bayes classifiers are built on Bayesian classification methods. These
    rely on Bayes’s theorem, which is an equation describing the relationship of conditional
    probabilities of statistical quantities. In Bayesian classification, we’re interested
    in finding the probability of a label <math alttext="upper L"><mi>L</mi></math>
    given some observed features, which we can write as <math alttext="upper P left-parenthesis
    upper L vertical-bar normal f normal e normal a normal t normal u normal r normal
    e normal s right-parenthesis"><mrow><mi>P</mi> <mo>(</mo> <mi>L</mi> <mo>|</mo>
    <mi>features</mi> <mo>)</mo></mrow></math> . Bayes’s theorem tells us how to express
    this in terms of quantities we can compute more directly:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="upper P left-parenthesis upper L vertical-bar normal f normal
    e normal a normal t normal u normal r normal e normal s right-parenthesis equals
    StartFraction upper P left-parenthesis normal f normal e normal a normal t normal
    u normal r normal e normal s vertical-bar upper L right-parenthesis upper P left-parenthesis
    upper L right-parenthesis Over upper P left-parenthesis normal f normal e normal
    a normal t normal u normal r normal e normal s right-parenthesis EndFraction"
    display="block"><mrow><mi>P</mi> <mrow><mo>(</mo> <mi>L</mi> <mo>|</mo> <mi>features</mi>
    <mo>)</mo></mrow> <mo>=</mo> <mfrac><mrow><mi>P</mi><mo>(</mo> <mi>features</mi>
    <mo>|</mo><mi>L</mi><mo>)</mo><mi>P</mi><mo>(</mo><mi>L</mi><mo>)</mo></mrow>
    <mrow><mi>P</mi><mo>(</mo> <mi>features</mi> <mo>)</mo></mrow></mfrac></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'If we are trying to decide between two labels—let’s call them <math alttext="upper
    L 1"><msub><mi>L</mi> <mn>1</mn></msub></math> and <math alttext="upper L 2"><msub><mi>L</mi>
    <mn>2</mn></msub></math> —then one way to make this decision is to compute the
    ratio of the posterior probabilities for each label:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="StartFraction upper P left-parenthesis upper L 1 vertical-bar
    normal f normal e normal a normal t normal u normal r normal e normal s right-parenthesis
    Over upper P left-parenthesis upper L 2 vertical-bar normal f normal e normal
    a normal t normal u normal r normal e normal s right-parenthesis EndFraction equals
    StartFraction upper P left-parenthesis normal f normal e normal a normal t normal
    u normal r normal e normal s vertical-bar upper L 1 right-parenthesis Over upper
    P left-parenthesis normal f normal e normal a normal t normal u normal r normal
    e normal s vertical-bar upper L 2 right-parenthesis EndFraction StartFraction
    upper P left-parenthesis upper L 1 right-parenthesis Over upper P left-parenthesis
    upper L 2 right-parenthesis EndFraction" display="block"><mrow><mfrac><mrow><mi>P</mi><mo>(</mo><msub><mi>L</mi>
    <mn>1</mn></msub> <mo>|</mo> <mi>features</mi> <mo>)</mo></mrow> <mrow><mi>P</mi><mo>(</mo><msub><mi>L</mi>
    <mn>2</mn></msub> <mo>|</mo> <mi>features</mi> <mo>)</mo></mrow></mfrac> <mo>=</mo>
    <mfrac><mrow><mi>P</mi><mo>(</mo> <mi>features</mi> <mo>|</mo><msub><mi>L</mi>
    <mn>1</mn></msub> <mo>)</mo></mrow> <mrow><mi>P</mi><mo>(</mo> <mi>features</mi>
    <mo>|</mo><msub><mi>L</mi> <mn>2</mn></msub> <mo>)</mo></mrow></mfrac> <mfrac><mrow><mi>P</mi><mo>(</mo><msub><mi>L</mi>
    <mn>1</mn></msub> <mo>)</mo></mrow> <mrow><mi>P</mi><mo>(</mo><msub><mi>L</mi>
    <mn>2</mn></msub> <mo>)</mo></mrow></mfrac></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: All we need now is some model by which we can compute <math alttext="upper P
    left-parenthesis normal f normal e normal a normal t normal u normal r normal
    e normal s vertical-bar upper L Subscript i Baseline right-parenthesis"><mrow><mi>P</mi>
    <mo>(</mo> <mi>features</mi> <mo>|</mo> <msub><mi>L</mi> <mi>i</mi></msub> <mo>)</mo></mrow></math>
    for each label. Such a model is called a *generative model* because it specifies
    the hypothetical random process that generates the data. Specifying this generative
    model for each label is the main piece of the training of such a Bayesian classifier.
    The general version of such a training step is a very difficult task, but we can
    make it simpler through the use of some simplifying assumptions about the form
    of this model.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is where the “naive” in “naive Bayes” comes in: if we make very naive
    assumptions about the generative model for each label, we can find a rough approximation
    of the generative model for each class, and then proceed with the Bayesian classification.
    Different types of naive Bayes classifiers rest on different naive assumptions
    about the data, and we will examine a few of these in the following sections.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We begin with the standard imports:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Gaussian Naive Bayes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Perhaps the easiest naive Bayes classifier to understand is Gaussian naive
    Bayes. With this classifier, the assumption is that *data from each label is drawn
    from a simple Gaussian distribution*. Imagine that we have the following data,
    shown in [Figure 41-1](#fig_0505-naive-bayes_files_in_output_5_0):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![output 5 0](assets/output_5_0.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 41-1\. Data for Gaussian naive Bayes classification^([1](ch41.xhtml#idm45858738121616))
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The simplest Gaussian model is to assume that the data is described by a Gaussian
    distribution with no covariance between dimensions. This model can be fit by computing
    the mean and standard deviation of the points within each label, which is all
    we need to define such a distribution. The result of this naive Gaussian assumption
    is shown in [Figure 41-2](#fig_images_in_0505-gaussian-nb).
  prefs: []
  type: TYPE_NORMAL
- en: '![05.05 gaussian NB](assets/05.05-gaussian-NB.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 41-2\. Visualization of the Gaussian naive Bayes model^([2](ch41.xhtml#idm45858738117008))
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The ellipses here represent the Gaussian generative model for each label, with
    larger probability toward the center of the ellipses. With this generative model
    in place for each class, we have a simple recipe to compute the likelihood <math
    alttext="upper P left-parenthesis normal f normal e normal a normal t normal u
    normal r normal e normal s vertical-bar upper L 1 right-parenthesis"><mrow><mi>P</mi>
    <mo>(</mo> <mi>features</mi> <mo>|</mo> <msub><mi>L</mi> <mn>1</mn></msub> <mo>)</mo></mrow></math>
    for any data point, and thus we can quickly compute the posterior ratio and determine
    which label is the most probable for a given point.
  prefs: []
  type: TYPE_NORMAL
- en: 'This procedure is implemented in Scikit-Learn’s `sklearn.naive_bayes.GaussianNB`
    estimator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s generate some new data and predict the label:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Now we can plot this new data to get an idea of where the decision boundary
    is (see [Figure 41-3](#fig_0505-naive-bayes_files_in_output_13_0)).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '![output 13 0](assets/output_13_0.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 41-3\. Visualization of the Gaussian naive Bayes classification
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We see a slightly curved boundary in the classifications—in general, the boundary
    produced by a Gaussian naive Bayes model will be quadratic.
  prefs: []
  type: TYPE_NORMAL
- en: 'A nice aspect of this Bayesian formalism is that it naturally allows for probabilistic
    classification, which we can compute using the `predict_proba` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The columns give the posterior probabilities of the first and second labels,
    respectively. If you are looking for estimates of uncertainty in your classification,
    Bayesian approaches like this can be a good place to start.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, the final classification will only be as good as the model assumptions
    that lead to it, which is why Gaussian naive Bayes often does not produce very
    good results. Still, in many cases—especially as the number of features becomes
    large—this assumption is not detrimental enough to prevent Gaussian naive Bayes
    from being a reliable method.
  prefs: []
  type: TYPE_NORMAL
- en: Multinomial Naive Bayes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Gaussian assumption just described is by no means the only simple assumption
    that could be used to specify the generative distribution for each label. Another
    useful example is multinomial naive Bayes, where the features are assumed to be
    generated from a simple multinomial distribution. The multinomial distribution
    describes the probability of observing counts among a number of categories, and
    thus multinomial naive Bayes is most appropriate for features that represent counts
    or count rates.
  prefs: []
  type: TYPE_NORMAL
- en: The idea is precisely the same as before, except that instead of modeling the
    data distribution with the best-fit Gaussian, we model it with a best-fit multinomial
    distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: Classifying Text'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One place where multinomial naive Bayes is often used is in text classification,
    where the features are related to word counts or frequencies within the documents
    to be classified. We discussed the extraction of such features from text in [Chapter 40](ch40.xhtml#section-0504-feature-engineering);
    here we will use the sparse word count features from the 20 Newsgroups corpus
    made available through Scikit-Learn to show how we might classify these short
    documents into categories.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s download the data and take a look at the target names:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'For simplicity here, we will select just a few of these categories and download
    the training and testing sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is a representative entry from the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to use this data for machine learning, we need to be able to convert
    the content of each string into a vector of numbers. For this we will use the
    TF-IDF vectorizer (introduced in [Chapter 40](ch40.xhtml#section-0504-feature-engineering)),
    and create a pipeline that attaches it to a multinomial naive Bayes classifier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'With this pipeline, we can apply the model to the training data and predict
    labels for the test data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have predicted the labels for the test data, we can evaluate them
    to learn about the performance of the estimator. For example, let’s take a look
    at the confusion matrix between the true and predicted labels for the test data
    (see [Figure 41-4](#fig_0505-naive-bayes_files_in_output_29_0)).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Evidently, even this very simple classifier can successfully separate space
    discussions from computer discussions, but it gets confused between discussions
    about religion and discussions about Christianity. This is perhaps to be expected!
  prefs: []
  type: TYPE_NORMAL
- en: 'The cool thing here is that we now have the tools to determine the category
    for *any* string, using the `predict` method of this pipeline. Here’s a utility
    function that will return the prediction for a single string:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s try it out:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '![output 29 0](assets/output_29_0.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 41-4\. Confusion matrix for the multinomial naive Bayes text classifier
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Remember that this is nothing more sophisticated than a simple probability model
    for the (weighted) frequency of each word in the string; nevertheless, the result
    is striking. Even a very naive algorithm, when used carefully and trained on a
    large set of high-dimensional data, can be surprisingly effective.
  prefs: []
  type: TYPE_NORMAL
- en: When to Use Naive Bayes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Because naive Bayes classifiers make such stringent assumptions about data,
    they will generally not perform as well as more complicated models. That said,
    they have several advantages:'
  prefs: []
  type: TYPE_NORMAL
- en: They are fast for both training and prediction.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They provide straightforward probabilistic prediction.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They are often easily interpretable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They have few (if any) tunable parameters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These advantages mean a naive Bayes classifier is often a good choice as an
    initial baseline classification. If it performs suitably, then congratulations:
    you have a very fast, very interpretable classifier for your problem. If it does
    not perform well, then you can begin exploring more sophisticated models, with
    some baseline knowledge of how well they should perform.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Naive Bayes classifiers tend to perform especially well in the following situations:'
  prefs: []
  type: TYPE_NORMAL
- en: When the naive assumptions actually match the data (very rare in practice)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For very well-separated categories, when model complexity is less important
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For very high-dimensional data, when model complexity is less important
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The last two points seem distinct, but they actually are related: as the dimensionality
    of a dataset grows, it is much less likely for any two points to be found close
    together (after all, they must be close in *every single dimension* to be close
    overall). This means that clusters in high dimensions tend to be more separated,
    on average, than clusters in low dimensions, assuming the new dimensions actually
    add information. For this reason, simplistic classifiers like the ones discussed
    here tend to work as well or better than more complicated classifiers as the dimensionality
    grows: once you have enough data, even a simple model can be very powerful.'
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch41.xhtml#idm45858738121616-marker)) A full-color version of this figure
    can be found on [GitHub](https://oreil.ly/PDSH_GitHub).
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch41.xhtml#idm45858738117008-marker)) Code to produce this figure can
    be found in the [online appendix](https://oreil.ly/o0ENq).
  prefs: []
  type: TYPE_NORMAL
