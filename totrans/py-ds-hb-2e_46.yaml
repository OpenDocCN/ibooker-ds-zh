- en: 'Chapter 41\. In Depth: Naive Bayes Classification'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第41章 深入：朴素贝叶斯分类
- en: The previous four chapters have given a general overview of the concepts of
    machine learning. In the rest of [Part V](part05.xhtml#section-0500-machine-learning),
    we will be taking a closer look first at four algorithms for supervised learning,
    and then at four algorithms for unsupervised learning. We start here with our
    first supervised method, naive Bayes classification.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 前四章概述了机器学习的概念。在[第五部分](part05.xhtml#section-0500-machine-learning)的其余部分，我们将首先更详细地查看四种监督学习算法，然后是四种无监督学习算法。我们从第一个监督方法朴素贝叶斯分类开始。
- en: Naive Bayes models are a group of extremely fast and simple classification algorithms
    that are often suitable for very high-dimensional datasets. Because they are so
    fast and have so few tunable parameters, they end up being useful as a quick-and-dirty
    baseline for a classification problem. This chapter will provide an intuitive
    explanation of how naive Bayes classifiers work, followed by a few examples of
    them in action on some datasets.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 朴素贝叶斯模型是一组极快速且简单的分类算法，通常适用于非常高维度的数据集。因为它们速度快、可调参数少，所以它们通常用作分类问题的快速基准线。本章将提供朴素贝叶斯分类器工作原理的直观解释，并在一些数据集上展示它们的几个例子。
- en: Bayesian Classification
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 贝叶斯分类
- en: 'Naive Bayes classifiers are built on Bayesian classification methods. These
    rely on Bayes’s theorem, which is an equation describing the relationship of conditional
    probabilities of statistical quantities. In Bayesian classification, we’re interested
    in finding the probability of a label <math alttext="upper L"><mi>L</mi></math>
    given some observed features, which we can write as <math alttext="upper P left-parenthesis
    upper L vertical-bar normal f normal e normal a normal t normal u normal r normal
    e normal s right-parenthesis"><mrow><mi>P</mi> <mo>(</mo> <mi>L</mi> <mo>|</mo>
    <mi>features</mi> <mo>)</mo></mrow></math> . Bayes’s theorem tells us how to express
    this in terms of quantities we can compute more directly:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 朴素贝叶斯分类器是建立在贝叶斯分类方法之上的。这些方法依赖于贝叶斯定理，该定理描述了统计量的条件概率关系。在贝叶斯分类中，我们感兴趣的是找到给定一些观察特征的标签<math
    alttext="upper L"><mi>L</mi></math>的概率，可以写作<math alttext="upper P left-parenthesis
    upper L vertical-bar normal f normal e normal a normal t normal u normal r normal
    e normal s right-parenthesis"><mrow><mi>P</mi> <mo>(</mo> <mi>L</mi> <mo>|</mo>
    <mi>features</mi> <mo>)</mo></mrow></math>。贝叶斯定理告诉我们如何用我们可以更直接计算的量来表达这一点：
- en: <math alttext="upper P left-parenthesis upper L vertical-bar normal f normal
    e normal a normal t normal u normal r normal e normal s right-parenthesis equals
    StartFraction upper P left-parenthesis normal f normal e normal a normal t normal
    u normal r normal e normal s vertical-bar upper L right-parenthesis upper P left-parenthesis
    upper L right-parenthesis Over upper P left-parenthesis normal f normal e normal
    a normal t normal u normal r normal e normal s right-parenthesis EndFraction"
    display="block"><mrow><mi>P</mi> <mrow><mo>(</mo> <mi>L</mi> <mo>|</mo> <mi>features</mi>
    <mo>)</mo></mrow> <mo>=</mo> <mfrac><mrow><mi>P</mi><mo>(</mo> <mi>features</mi>
    <mo>|</mo><mi>L</mi><mo>)</mo><mi>P</mi><mo>(</mo><mi>L</mi><mo>)</mo></mrow>
    <mrow><mi>P</mi><mo>(</mo> <mi>features</mi> <mo>)</mo></mrow></mfrac></mrow></math>
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper P left-parenthesis upper L vertical-bar normal f normal
    e normal a normal t normal u normal r normal e normal s right-parenthesis equals
    StartFraction upper P left-parenthesis normal f normal e normal a normal t normal
    u normal r normal e normal s vertical-bar upper L right-parenthesis upper P left-parenthesis
    upper L right-parenthesis Over upper P left-parenthesis normal f normal e normal
    a normal t normal u normal r normal e normal s right-parenthesis EndFraction"
    display="block"><mrow><mi>P</mi> <mrow><mo>(</mo> <mi>L</mi> <mo>|</mo> <mi>features</mi>
    <mo>)</mo></mrow> <mo>=</mo> <mfrac><mrow><mi>P</mi><mo>(</mo> <mi>features</mi>
    <mo>|</mo><mi>L</mi><mo>)</mo><mi>P</mi><mo>(</mo><mi>L</mi><mo>)</mo></mrow>
    <mrow><mi>P</mi><mo>(</mo> <mi>features</mi> <mo>)</mo></mrow></mfrac></mrow></math>
- en: 'If we are trying to decide between two labels—let’s call them <math alttext="upper
    L 1"><msub><mi>L</mi> <mn>1</mn></msub></math> and <math alttext="upper L 2"><msub><mi>L</mi>
    <mn>2</mn></msub></math> —then one way to make this decision is to compute the
    ratio of the posterior probabilities for each label:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们试图在两个标签之间做出决策——让我们称它们为<math alttext="upper L 1"><msub><mi>L</mi> <mn>1</mn></msub></math>和<math
    alttext="upper L 2"><msub><mi>L</mi> <mn>2</mn></msub></math>——那么做出这个决定的一种方法是计算每个标签的后验概率的比率：
- en: <math alttext="StartFraction upper P left-parenthesis upper L 1 vertical-bar
    normal f normal e normal a normal t normal u normal r normal e normal s right-parenthesis
    Over upper P left-parenthesis upper L 2 vertical-bar normal f normal e normal
    a normal t normal u normal r normal e normal s right-parenthesis EndFraction equals
    StartFraction upper P left-parenthesis normal f normal e normal a normal t normal
    u normal r normal e normal s vertical-bar upper L 1 right-parenthesis Over upper
    P left-parenthesis normal f normal e normal a normal t normal u normal r normal
    e normal s vertical-bar upper L 2 right-parenthesis EndFraction StartFraction
    upper P left-parenthesis upper L 1 right-parenthesis Over upper P left-parenthesis
    upper L 2 right-parenthesis EndFraction" display="block"><mrow><mfrac><mrow><mi>P</mi><mo>(</mo><msub><mi>L</mi>
    <mn>1</mn></msub> <mo>|</mo> <mi>features</mi> <mo>)</mo></mrow> <mrow><mi>P</mi><mo>(</mo><msub><mi>L</mi>
    <mn>2</mn></msub> <mo>|</mo> <mi>features</mi> <mo>)</mo></mrow></mfrac> <mo>=</mo>
    <mfrac><mrow><mi>P</mi><mo>(</mo> <mi>features</mi> <mo>|</mo><msub><mi>L</mi>
    <mn>1</mn></msub> <mo>)</mo></mrow> <mrow><mi>P</mi><mo>(</mo> <mi>features</mi>
    <mo>|</mo><msub><mi>L</mi> <mn>2</mn></msub> <mo>)</mo></mrow></mfrac> <mfrac><mrow><mi>P</mi><mo>(</mo><msub><mi>L</mi>
    <mn>1</mn></msub> <mo>)</mo></mrow> <mrow><mi>P</mi><mo>(</mo><msub><mi>L</mi>
    <mn>2</mn></msub> <mo>)</mo></mrow></mfrac></mrow></math>
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="StartFraction upper P left-parenthesis upper L 1 vertical-bar
    normal f normal e normal a normal t normal u normal r normal e normal s right-parenthesis
    Over upper P left-parenthesis upper L 2 vertical-bar normal f normal e normal
    a normal t normal u normal r normal e normal s right-parenthesis EndFraction equals
    StartFraction upper P left-parenthesis normal f normal e normal a normal t normal
    u normal r normal e normal s vertical-bar upper L 1 right-parenthesis Over upper
    P left-parenthesis normal f normal e normal a normal t normal u normal r normal
    e normal s vertical-bar upper L 2 right-parenthesis EndFraction StartFraction
    upper P left-parenthesis upper L 1 right-parenthesis Over upper P left-parenthesis
    upper L 2 right-parenthesis EndFraction" display="block"><mrow><mfrac><mrow><mi>P</mi><mo>(</mo><msub><mi>L</mi>
    <mn>1</mn></msub> <mo>|</mo> <mi>features</mi> <mo>)</mo></mrow> <mrow><mi>P</mi><mo>(</mo><msub><mi>L</mi>
    <mn>2</mn></msub> <mo>|</mo> <mi>features</mi> <mo>)</mo></mrow></mfrac> <mo>=</mo>
    <mfrac><mrow><mi>P</mi><mo>(</mo> <mi>features</mi> <mo>|</mo><msub><mi>L</mi>
    <mn>1</mn></msub> <mo>)</mo></mrow> <mrow><mi>P</mi><mo>(</mo> <mi>features</mi>
    <mo>|</mo><msub><mi>L</mi> <mn>2</mn></msub> <mo>)</mo></mrow></mfrac> <mfrac><mrow><mi>P</mi><mo>(</mo><msub><mi>L</mi>
    <mn>1</mn></msub> <mo>)</mo></mrow> <mrow><mi>P</mi><mo>(</mo><msub><mi>L</mi>
    <mn>2</mn></msub> <mo>)</mo></mrow></mfrac></mrow></math>
- en: All we need now is some model by which we can compute <math alttext="upper P
    left-parenthesis normal f normal e normal a normal t normal u normal r normal
    e normal s vertical-bar upper L Subscript i Baseline right-parenthesis"><mrow><mi>P</mi>
    <mo>(</mo> <mi>features</mi> <mo>|</mo> <msub><mi>L</mi> <mi>i</mi></msub> <mo>)</mo></mrow></math>
    for each label. Such a model is called a *generative model* because it specifies
    the hypothetical random process that generates the data. Specifying this generative
    model for each label is the main piece of the training of such a Bayesian classifier.
    The general version of such a training step is a very difficult task, but we can
    make it simpler through the use of some simplifying assumptions about the form
    of this model.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们所需的只是一些模型，通过这些模型我们可以计算每个标签<math alttext="upper P left-parenthesis normal
    f normal e normal a normal t normal u normal r normal e normal s vertical-bar
    upper L Subscript i Baseline right-parenthesis"><mrow><mi>P</mi> <mo>(</mo> <mi>features</mi>
    <mo>|</mo> <msub><mi>L</mi> <mi>i</mi></msub> <mo>)</mo></mrow></math>。这样的模型被称为*生成模型*，因为它指定了生成数据的假设随机过程。为每个标签指定这种生成模型是这样一个贝叶斯分类器训练的主要部分。对于这样一个训练步骤的一般版本来说，这是一个非常困难的任务，但是我们可以通过对这个模型形式做一些简化的假设来简化它。
- en: 'This is where the “naive” in “naive Bayes” comes in: if we make very naive
    assumptions about the generative model for each label, we can find a rough approximation
    of the generative model for each class, and then proceed with the Bayesian classification.
    Different types of naive Bayes classifiers rest on different naive assumptions
    about the data, and we will examine a few of these in the following sections.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是“朴素贝叶斯”中的“朴素”所在：如果我们对每个标签的生成模型作出非常朴素的假设，我们可以找到每个类别的生成模型的粗略近似，然后继续贝叶斯分类。不同类型的朴素贝叶斯分类器基于关于数据的不同朴素假设，我们将在接下来的几节中讨论其中一些。
- en: 'We begin with the standard imports:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从标准导入开始：
- en: '[PRE0]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Gaussian Naive Bayes
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高斯朴素贝叶斯
- en: 'Perhaps the easiest naive Bayes classifier to understand is Gaussian naive
    Bayes. With this classifier, the assumption is that *data from each label is drawn
    from a simple Gaussian distribution*. Imagine that we have the following data,
    shown in [Figure 41-1](#fig_0505-naive-bayes_files_in_output_5_0):'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 或许最容易理解的朴素贝叶斯分类器是高斯朴素贝叶斯。使用这个分类器，假设*每个标签的数据都来自简单的高斯分布*。想象一下我们有以下数据，显示在[图 41-1](#fig_0505-naive-bayes_files_in_output_5_0)中：
- en: '[PRE1]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![output 5 0](assets/output_5_0.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![output 5 0](assets/output_5_0.png)'
- en: Figure 41-1\. Data for Gaussian naive Bayes classification^([1](ch41.xhtml#idm45858738121616))
  id: totrans-16
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 41-1\. 高斯朴素贝叶斯分类数据^([1](ch41.xhtml#idm45858738121616))
- en: The simplest Gaussian model is to assume that the data is described by a Gaussian
    distribution with no covariance between dimensions. This model can be fit by computing
    the mean and standard deviation of the points within each label, which is all
    we need to define such a distribution. The result of this naive Gaussian assumption
    is shown in [Figure 41-2](#fig_images_in_0505-gaussian-nb).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的高斯模型假设数据由没有各维度间协方差的高斯分布描述。这个模型可以通过计算每个标签内点的均值和标准差来拟合，这是我们定义这种分布所需的所有内容。这种朴素高斯假设的结果显示在[图
    41-2](#fig_images_in_0505-gaussian-nb)中。
- en: '![05.05 gaussian NB](assets/05.05-gaussian-NB.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![05.05 gaussian NB](assets/05.05-gaussian-NB.png)'
- en: Figure 41-2\. Visualization of the Gaussian naive Bayes model^([2](ch41.xhtml#idm45858738117008))
  id: totrans-19
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 41-2\. 高斯朴素贝叶斯模型可视化^([2](ch41.xhtml#idm45858738117008))
- en: The ellipses here represent the Gaussian generative model for each label, with
    larger probability toward the center of the ellipses. With this generative model
    in place for each class, we have a simple recipe to compute the likelihood <math
    alttext="upper P left-parenthesis normal f normal e normal a normal t normal u
    normal r normal e normal s vertical-bar upper L 1 right-parenthesis"><mrow><mi>P</mi>
    <mo>(</mo> <mi>features</mi> <mo>|</mo> <msub><mi>L</mi> <mn>1</mn></msub> <mo>)</mo></mrow></math>
    for any data point, and thus we can quickly compute the posterior ratio and determine
    which label is the most probable for a given point.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的椭圆代表每个标签的高斯生成模型，中心区域的概率更高。有了每个类别的生成模型，我们可以简单地计算任何数据点的似然<math alttext="upper
    P left-parenthesis normal f normal e normal a normal t normal u normal r normal
    e normal s vertical-bar upper L 1 right-parenthesis"><mrow><mi>P</mi> <mo>(</mo>
    <mi>features</mi> <mo>|</mo> <msub><mi>L</mi> <mn>1</mn></msub> <mo>)</mo></mrow></math>，因此我们可以快速计算后验比率，并确定给定点最有可能的标签。
- en: 'This procedure is implemented in Scikit-Learn’s `sklearn.naive_bayes.GaussianNB`
    estimator:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程在Scikit-Learn的`sklearn.naive_bayes.GaussianNB`估计器中实现：
- en: '[PRE2]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Let’s generate some new data and predict the label:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们生成一些新数据并预测标签：
- en: '[PRE3]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Now we can plot this new data to get an idea of where the decision boundary
    is (see [Figure 41-3](#fig_0505-naive-bayes_files_in_output_13_0)).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以绘制这些新数据，以了解决策边界的位置（见[图 41-3](#fig_0505-naive-bayes_files_in_output_13_0)）。
- en: '[PRE4]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![output 13 0](assets/output_13_0.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![output 13 0](assets/output_13_0.png)'
- en: Figure 41-3\. Visualization of the Gaussian naive Bayes classification
  id: totrans-28
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 41-3\. 高斯朴素贝叶斯分类可视化
- en: We see a slightly curved boundary in the classifications—in general, the boundary
    produced by a Gaussian naive Bayes model will be quadratic.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到分类中有一个略微弯曲的边界——一般来说，高斯朴素贝叶斯模型产生的边界将是二次的。
- en: 'A nice aspect of this Bayesian formalism is that it naturally allows for probabilistic
    classification, which we can compute using the `predict_proba` method:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这种贝叶斯形式主义的一个优点是它自然地允许概率分类，我们可以使用`predict_proba`方法来计算：
- en: '[PRE5]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The columns give the posterior probabilities of the first and second labels,
    respectively. If you are looking for estimates of uncertainty in your classification,
    Bayesian approaches like this can be a good place to start.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这些列分别给出了第一个和第二个标签的后验概率。如果您正在寻找分类中的不确定性估计，像这样的贝叶斯方法可能是一个很好的起点。
- en: Of course, the final classification will only be as good as the model assumptions
    that lead to it, which is why Gaussian naive Bayes often does not produce very
    good results. Still, in many cases—especially as the number of features becomes
    large—this assumption is not detrimental enough to prevent Gaussian naive Bayes
    from being a reliable method.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，最终的分类结果将仅仅与导致它的模型假设一样好，这就是为什么高斯朴素贝叶斯通常不会产生非常好的结果的原因。尽管如此，在许多情况下——特别是特征数量变得很大时——这种假设并不足以阻止高斯朴素贝叶斯成为一种可靠的方法。
- en: Multinomial Naive Bayes
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多项式朴素贝叶斯
- en: The Gaussian assumption just described is by no means the only simple assumption
    that could be used to specify the generative distribution for each label. Another
    useful example is multinomial naive Bayes, where the features are assumed to be
    generated from a simple multinomial distribution. The multinomial distribution
    describes the probability of observing counts among a number of categories, and
    thus multinomial naive Bayes is most appropriate for features that represent counts
    or count rates.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 刚刚描述的高斯假设远非是可以用于指定每个标签生成分布的唯一简单假设。另一个有用的例子是多项式朴素贝叶斯，其中假设特征是从简单的多项分布生成的。多项分布描述了在多个类别中观察计数的概率，因此多项式朴素贝叶斯最适合表示计数或计数率的特征。
- en: The idea is precisely the same as before, except that instead of modeling the
    data distribution with the best-fit Gaussian, we model it with a best-fit multinomial
    distribution.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这个想法与之前完全相同，只是不再用最佳拟合的高斯来建模数据分布，而是用最佳拟合的多项分布来建模。
- en: 'Example: Classifying Text'
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 示例：文本分类
- en: One place where multinomial naive Bayes is often used is in text classification,
    where the features are related to word counts or frequencies within the documents
    to be classified. We discussed the extraction of such features from text in [Chapter 40](ch40.xhtml#section-0504-feature-engineering);
    here we will use the sparse word count features from the 20 Newsgroups corpus
    made available through Scikit-Learn to show how we might classify these short
    documents into categories.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 多项式朴素贝叶斯经常用于文本分类的一个场合，其中特征与要分类的文档中的单词计数或频率相关。我们在[第40章](ch40.xhtml#section-0504-feature-engineering)中讨论了从文本中提取这些特征；在这里，我们将使用通过Scikit-Learn提供的20个新闻组语料库中的稀疏单词计数特征来展示如何将这些简短文档分类到不同的类别中。
- en: 'Let’s download the data and take a look at the target names:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们下载数据并查看目标名称：
- en: '[PRE6]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'For simplicity here, we will select just a few of these categories and download
    the training and testing sets:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简单起见，在这里我们将只选择其中几个类别并下载训练和测试集：
- en: '[PRE7]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Here is a representative entry from the data:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是数据的一个代表性条目：
- en: '[PRE8]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'In order to use this data for machine learning, we need to be able to convert
    the content of each string into a vector of numbers. For this we will use the
    TF-IDF vectorizer (introduced in [Chapter 40](ch40.xhtml#section-0504-feature-engineering)),
    and create a pipeline that attaches it to a multinomial naive Bayes classifier:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将这些数据用于机器学习，我们需要将每个字符串的内容转换为一个数字向量。为此，我们将使用TF-IDF向量化器（在[第40章](ch40.xhtml#section-0504-feature-engineering)介绍），并创建一个管道，将其附加到多项式朴素贝叶斯分类器：
- en: '[PRE9]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'With this pipeline, we can apply the model to the training data and predict
    labels for the test data:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个管道，我们可以将模型应用于训练数据，并预测测试数据的标签：
- en: '[PRE10]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Now that we have predicted the labels for the test data, we can evaluate them
    to learn about the performance of the estimator. For example, let’s take a look
    at the confusion matrix between the true and predicted labels for the test data
    (see [Figure 41-4](#fig_0505-naive-bayes_files_in_output_29_0)).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经预测了测试数据的标签，我们可以评估它们以了解估计器的性能。例如，让我们看一下测试数据的真实标签和预测标签之间的混淆矩阵（参见[图41-4](#fig_0505-naive-bayes_files_in_output_29_0)）。
- en: '[PRE11]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Evidently, even this very simple classifier can successfully separate space
    discussions from computer discussions, but it gets confused between discussions
    about religion and discussions about Christianity. This is perhaps to be expected!
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，即使这个非常简单的分类器可以成功地将关于空间讨论与计算机讨论分开，但它会在宗教讨论和基督教讨论之间感到困惑。这或许是可以预料的！
- en: 'The cool thing here is that we now have the tools to determine the category
    for *any* string, using the `predict` method of this pipeline. Here’s a utility
    function that will return the prediction for a single string:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这里很酷的一点是，我们现在有工具来确定*任何*字符串的类别，只需使用此管道的`predict`方法。下面是一个实用函数，用于返回单个字符串的预测结果：
- en: '[PRE12]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Let’s try it out:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们试试它：
- en: '[PRE13]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '![output 29 0](assets/output_29_0.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![output 29 0](assets/output_29_0.png)'
- en: Figure 41-4\. Confusion matrix for the multinomial naive Bayes text classifier
  id: totrans-59
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 41-4\. 多项式朴素贝叶斯文本分类器的混淆矩阵
- en: Remember that this is nothing more sophisticated than a simple probability model
    for the (weighted) frequency of each word in the string; nevertheless, the result
    is striking. Even a very naive algorithm, when used carefully and trained on a
    large set of high-dimensional data, can be surprisingly effective.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，这只是一个对字符串中每个单词（加权）频率的简单概率模型；尽管如此，结果令人印象深刻。即使是非常朴素的算法，在小心使用并在大量高维数据上训练时，也可以出奇地有效。
- en: When to Use Naive Bayes
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 何时使用朴素贝叶斯
- en: 'Because naive Bayes classifiers make such stringent assumptions about data,
    they will generally not perform as well as more complicated models. That said,
    they have several advantages:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 由于朴素贝叶斯分类器对数据做出如此严格的假设，它们通常不如更复杂的模型表现好。尽管如此，它们有几个优点：
- en: They are fast for both training and prediction.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们在训练和预测时都非常快速。
- en: They provide straightforward probabilistic prediction.
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们提供直观的概率预测。
- en: They are often easily interpretable.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们通常易于解释。
- en: They have few (if any) tunable parameters.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们具有少量（如果有的话）可调参数。
- en: 'These advantages mean a naive Bayes classifier is often a good choice as an
    initial baseline classification. If it performs suitably, then congratulations:
    you have a very fast, very interpretable classifier for your problem. If it does
    not perform well, then you can begin exploring more sophisticated models, with
    some baseline knowledge of how well they should perform.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这些优势意味着朴素贝叶斯分类器通常是作为初始基线分类的不错选择。如果它表现得合适，那么恭喜你：你已经拥有了一个非常快速、易于解释的分类器来解决你的问题。如果它表现不佳，那么你可以开始探索更复杂的模型，同时具备一些关于它们应该如何表现的基础知识。
- en: 'Naive Bayes classifiers tend to perform especially well in the following situations:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 朴素贝叶斯分类器在以下情况下表现特别好：
- en: When the naive assumptions actually match the data (very rare in practice)
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当朴素假设实际上与数据匹配时（在实践中非常罕见）
- en: For very well-separated categories, when model complexity is less important
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于非常分离的类别，当模型复杂度不那么重要时
- en: For very high-dimensional data, when model complexity is less important
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于非常高维数据，当模型复杂度不那么重要时
- en: 'The last two points seem distinct, but they actually are related: as the dimensionality
    of a dataset grows, it is much less likely for any two points to be found close
    together (after all, they must be close in *every single dimension* to be close
    overall). This means that clusters in high dimensions tend to be more separated,
    on average, than clusters in low dimensions, assuming the new dimensions actually
    add information. For this reason, simplistic classifiers like the ones discussed
    here tend to work as well or better than more complicated classifiers as the dimensionality
    grows: once you have enough data, even a simple model can be very powerful.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 最后两点看似不同，但实际上是相关的：随着数据集维度的增长，任何两个点在一起的可能性大大降低（毕竟，它们必须在*每个维度*上都很接近才能在总体上接近）。这意味着在高维空间中，簇通常比低维空间中更为分离，平均而言。基于这个原因，像这里讨论的简单分类器往往在维度增加时表现得同样或更好：一旦你有足够的数据，即使是简单模型也可以非常强大。
- en: ^([1](ch41.xhtml#idm45858738121616-marker)) A full-color version of this figure
    can be found on [GitHub](https://oreil.ly/PDSH_GitHub).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch41.xhtml#idm45858738121616-marker)) 此图的全彩版本可在[GitHub](https://oreil.ly/PDSH_GitHub)上找到。
- en: ^([2](ch41.xhtml#idm45858738117008-marker)) Code to produce this figure can
    be found in the [online appendix](https://oreil.ly/o0ENq).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch41.xhtml#idm45858738117008-marker)) 生成此图的代码可在[在线附录](https://oreil.ly/o0ENq)中找到。
