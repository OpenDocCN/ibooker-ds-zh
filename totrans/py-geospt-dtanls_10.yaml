- en: Chapter 10\. Using Python to Measure Climate Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Developing technical skills and pathways for learning Python and geospatial
    analysis is important, but unless you provide context or create a narrative to
    share, it’s all simply data on a shelf.
  prefs: []
  type: TYPE_NORMAL
- en: In this final chapter, you will explore three approaches to exploring time-series
    data by accessing satellite image layers from [Landsat](https://oreil.ly/9bHFj),
    [China–Brazil Earth Resources Satellite (CBERS)](https://oreil.ly/PM7Lt), and
    [Sentinel](https://oreil.ly/1Kym6). You will use your geospatial analysis skills
    to examine questions about climate change and deforestation.
  prefs: []
  type: TYPE_NORMAL
- en: Spatial modeling is a crucial tool for forecasting, predicting, and monitoring
    the real-time status of global temperature increases and deforestation, which
    in turn helps us anticipate the consequences of these phenomena and potentially
    intervene or prepare for them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Three examples are presented to highlight some powerful Python packages: Xarray,
    Web Time Series Service (WTSS), and Forest at Risk (FAR). Although these may appear
    to be new tools, you have been introduced to many of their dependencies in earlier
    chapters. The last example is a deeper dive into the statistical power of packages
    designed for predictive modeling, which you’ll use in analyzing deforestation.
    You can run code in [the accompanying notebook](https://oreil.ly/9ADWy), since
    complete explanations of everything in it is beyond the scope of this book.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Example 1: Examining Climate Prediction with Precipitation Data'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spatial analysis often relies on multidimensional data analysis. Think of a
    gridded dataset as resembling a cube. In Python (and in computer programming in
    general), arrays store lists of data. The objects in the list can be referenced
    individually or collectively. This is important when calling a Python array because
    you can access each item by its index number.
  prefs: []
  type: TYPE_NORMAL
- en: Multidimensional and N-dimensional arrays, or *tensors*, are displayed in NumPy
    ndarrays. Think of a tensor as a container of data or information. In Python,
    [NumPy](http://www.numpy.org) provides the fundamental data structure and API
    for working with raw ndarrays.
  prefs: []
  type: TYPE_NORMAL
- en: You will be working with real-world datasets that encode information about how
    the array’s values map to locations. The data you are working with is labeled
    with encoded information such as timestamps, coordinates, elevation, land cover,
    and precipitation.
  prefs: []
  type: TYPE_NORMAL
- en: Goals
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The mission of the US government’s [National Oceanic and Atmospheric Administration
    (NOAA)](https://oreil.ly/NKI29) is to “predict changes in climate, weather, oceans,
    and coasts” and to inform and address urgent societal and environmental impacts
    from extreme weather events. In this exercise, you will work with a publicly available
    dataset to analyze daily precipitation. Comparing data from the continental US
    from 2015 and 2021, you will observe patterns in the data to determine if there
    are distinct observable differences.
  prefs: []
  type: TYPE_NORMAL
- en: First, I will introduce you to Xarray, an open source project that interoperates
    with NumPy, SciPy, and matplotlib, extending beyond NumPy ndarrays and pandas
    dataframes.
  prefs: []
  type: TYPE_NORMAL
- en: As with the preceding chapters, after you complete an introduction to a package
    and its supporting documentation, I strongly encourage you to experiment with
    different datasets and applications of Python packages and libraries.
  prefs: []
  type: TYPE_NORMAL
- en: Downloading Your Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, navigate to [Gridded Climate Datasets: Precipitation](https://oreil.ly/NhTn2)
    and choose the Climate Prediction Center (CPC) [Global Precipitation dataset](https://oreil.ly/M8ynH).
    Weather data for a given latitude/longitude is returned for the grid cell aligned
    with the requested lat/long.'
  prefs: []
  type: TYPE_NORMAL
- en: Download the years of interest, 2015 and 2021, and upload the files to either
    your Google drive or directly to your computer. [Figure 10-1](#files_in_google_colab)
    shows the folder and file hierarchy.
  prefs: []
  type: TYPE_NORMAL
- en: '![Files in Google Colab](assets/pgda_1001.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-1\. Files in Google Colab
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Working in Xarray
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Xarray is a Python library that hosts many dependencies that should by now be
    familiar to you, such as NumPy and pandas, as well as a few optional dependencies
    that you’ll need to work with the CPC Global Precipitation dataset. Originally
    developed by the Climate Corporation, [Xarray](https://oreil.ly/NjJmL) has become
    a useful open source resource for analyzing climate change data files for plotting
    and analysis. It’s useful for exploring weather, water, and climate extremes and
    their impact.
  prefs: []
  type: TYPE_NORMAL
- en: The NOAA Physical Sciences Laboratory relies on the Network Common Data Form
    (netCDF) format of Xarray, which is an interface for array-oriented data containing
    dimensions, variables, and attributes on top of NumPy arrays. The dimensions are
    often time and latitude or longitude, so this format is directly applicable to
    spatial observation and analysis.
  prefs: []
  type: TYPE_NORMAL
- en: You will download daily precipitation data and apply functions to it, including
    `groupby` (for grouping), `concat` (to combine files, also called *concatenation*),
    and `sel & isel` (to select data for specific dates or particular locations).
    Additionally, you will learn how to handle leap years. You’ll save the desired
    outputs as netCDF files.
  prefs: []
  type: TYPE_NORMAL
- en: '*Gridded data* combines point data (such as data from an individual weather
    station) and other data sources and maintains a spatially and temporally consistent
    method to account for factors like temperature changes and precipitation caused
    by location or elevation. The weather data is provided from a data distribution
    containing more than 2,500 monthly gridded data points, representing the entire
    globe.'
  prefs: []
  type: TYPE_NORMAL
- en: Xarray’s data structures are *N-dimensional*, meaning that they have a variable
    number of dimensions. This makes Xarray suitable for dealing with multidimensional
    scientific data. Because it uses dimension names instead of axis labels (dim=‘time’
    instead of axis=0), these arrays are more manageable than arrays in NumPy ndarrays.
    With Xarray, you don’t need to keep track of the order of dimensions or insert
    placeholder dimensions of size 1 to align arrays.
  prefs: []
  type: TYPE_NORMAL
- en: 'You will work with a few of the available functions to analyze geospatial data.
    Running `open_mfdataset` opens multiple files at one time. You will use this to
    compare different timepoints. You are going to run these examples in [Google Colab](https://oreil.ly/J8wam).
    I always connect to my Google Drive, as I keep most of the datasets on the cloud
    and not on my local computer. It is simple to connect your drive to Colab and
    select the directory where you are hosting the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Not everyone has a Google Drive. You can also access your data by simply uploading
    it to Google Colab. I actually upgraded to Google Colab Pro to improve runtime
    performance. Connecting to your Google Drive is usually the best option, but your
    mileage may vary.
  prefs: []
  type: TYPE_NORMAL
- en: You will need to establish the connection by selecting and approving it. Mounting
    the drive will take a little time, but then the files will show up in your available
    files.
  prefs: []
  type: TYPE_NORMAL
- en: 'Google Colab has a preinstalled version of Xarray, but I suggest a `pip install`
    to make certain the dependencies are all included:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: I make it a habit to run installs separately but bundle the import functions;
    this allows me to isolate and address any errors.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take a look at the less familiar modules you will be importing. The `glob`
    module (short for “global”) returns the files or folders that you specify. This
    also applies to paths inside directories/files and subdirectories/subfiles. You
    should recognize matplotlib as our plotting library, built on NumPy, and `urllib.request`
    as a module for retrieving URLs. You can download data to upload to Colab or,
    if your data is available as a URL, add the import to your code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Combining Your 2015 and 2021 Datasets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Xarray dataset is a container of labeled arrays. It is similar to a dataframe,
    except it is multidimensional and aligns with the netCDF dataset representation,
    shown in [Figure 10-2](#netcdf_data_array).
  prefs: []
  type: TYPE_NORMAL
- en: '![NetCDF data array](assets/pgda_1002.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-2\. NetCDF data array
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Enter the code to create your two variables, `ds2015` for 2015 and `ds2021`
    for 2021:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[Figure 10-3](#dataset_properties_in_xarray) shows the key properties of the
    Xarray dataset, including dimensions, coordinates, data variables, and attributes.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Dataset properties in Xarray](assets/pgda_1003.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-3\. Dataset properties in Xarray
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Click the database icon (it looks like stacked discs) to see more details. You
    will see the cell expand, as shown in [Figure 10-4](#expanded_dataset_details).
    The expanded metadata reveals the array and additional information.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, you are going to [concatenate](https://oreil.ly/E60L2) (join) Xarray
    objects along the `time` dimension:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '![Expanded dataset details](assets/pgda_1004.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-4\. Expanded dataset details
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The output ([Figure 10-5](#time_coordinates_concatenated)) will confirm that
    the datasets have been combined. The time coordinate now lists both years that
    you selected.
  prefs: []
  type: TYPE_NORMAL
- en: '![Time coordinates concatenated](assets/pgda_1005.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-5\. Time coordinates concatenated
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The function `dataarray.groupby` returns an object for group operations. You
    will be grouping operations by month (`time.month` in the code). There is a `dataarray`
    object for a daily dataset that spans a few years. This object has one variable
    and three dimensions: latitude, longitude, and time (daily). You can generate
    a graphic for both 2015 and 2021 ([Figure 10-6](#monthly_precipitation_in_the_continenta)).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Monthly precipitation in the continental US in 2015 (left) and 2021 (right)](assets/pgda_1006.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-6\. Monthly precipitation in the continental US in 2015 (left) and
    2021 (right)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Generating the Images
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To generate the plot for your 2015 image, state the lower bound by the first
    item (`0`), the default for the upper bound, and the step size default. Monthly
    precipitation is an object to be sliced, as indicated by `mon.precip`. We are
    operating across the three dimensions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: You can use the same method to generate a map for 2021\. The output for both
    is in [Figure 10-6](#monthly_precipitation_in_the_continenta), and the different
    patterns of precipitation are visible.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s expand beyond a single month to compare different years. Python has a
    calendar module that will display an entire year for comparison. Follow this code
    to iterate over each month in a calendar year, using matplotlib and the `calendar`
    module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, you will apply the sum along the `time` dimension. The landmask you are
    creating will “mask out” certain observations instead of returning NaN values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Matplotlib allows you to format and plot a series of maps. There are defaults
    for these parameters, but when you want to customize them, you can provide a width
    and height in inches for the image (`figsize`) and a background color (`facecolor`).
    The parameter `subplots_adjust` allows you to adjust the position of the subplot
    or axes. Let’s do 2015 first:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Experiment with different values to see how the image changes. The edges of
    the subplots are nudged along with the amount of width space (`wspace`) or height
    space (`hspace`).
  prefs: []
  type: TYPE_NORMAL
- en: You can explore different plot types in the [matplotlib](https://oreil.ly/7Nrc9)
    usage guide. The output is shown in [Figure 10-7](#yearly_precipitation_in_the_continental).
  prefs: []
  type: TYPE_NORMAL
- en: '![Yearly precipitation in the continental US by month, 2015, in Xarray](assets/pgda_1007.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-7\. Yearly precipitation in the continental US by month, 2015, in
    Xarray
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Next, update the code cell to run the 2021 data ([Figure 10-8](#yearly_precipitationcomma_twozerotwoone))
    and visually compare the difference in precipitation between the two years.
  prefs: []
  type: TYPE_NORMAL
- en: '![Yearly precipitation, 2021](assets/pgda_1008.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-8\. Yearly precipitation, 2021
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: More Exploration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are additional queries you can try if you want to see the data underlying
    the imagery. For instance, `ds2021.precip.dims` will list the dimensions in the
    data, `ds2021.precip.attrs` will list attributes, and `ds2021.precip.data` will
    show the daily precipitation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Precipitation data might be more meaningful if grouped seasonally, so let’s
    try that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The seasons are named by the months they contain: `DJF` for winter, `JJA` for
    summer, `MAM` for spring, and `SON` for fall. Let’s order them in an intuitive
    manner:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The seasons are now displayed chronologically, as they occur in a single year.
    Visualizing precipitation levels allows you to observe how precipitation varies
    by season and location ([Figure 10-9](#mean_precipitation_arranged_by_season)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Mean precipitation arranged by season](assets/pgda_1009.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-9\. Mean precipitation arranged by season
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Observations of precipitation levels demonstrate patterns in the data. You may
    have noticed that temperature datasets are also available in gridded format. Now
    that you have been introduced to Xarray and how to work with multidimensional
    arrays, try exploring maximum temperature, minimum temperature, and average temperature
    to expand your insights. What can you learn by plotting this data as well?
  prefs: []
  type: TYPE_NORMAL
- en: 'Example 2: Deforestation and Carbon Emissions in the Amazon Rain Forest Using
    WTSS Series'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Amazon rain forest is shrinking. Roadway construction in the forest, contact
    between humans and animals, and contact between visitors and the Indigenous peoples
    who live in the rain forest all contribute to ongoing deforestation and forest
    degradation. *Degradation* refers to a decline in forest density that does not
    quite meet the level of deforestation—typically, when tree cover is diminished
    between 10% and 30% and the land is being converted to alternative uses, such
    as timber harvesting or agriculture. When the forest cover is disturbed, *biodiversity—*the
    Amazon’s mixture of living things and ecosystems that exists nowhere else on earth—is
    threatened. Deforestation also has an impact on carbon emissions.
  prefs: []
  type: TYPE_NORMAL
- en: Web Time Series Service (WTSS) is a web service that processes time-series data
    gathered by remote sensing imagery.^([1](ch10.xhtml#ch01fn14)) Again, you’ll be
    working with a 3D array with spatial (latitude and longitude) and temporal references.
    WTSS is a Python client library, so you can obtain a list of actual values recorded
    at a specific location over time.
  prefs: []
  type: TYPE_NORMAL
- en: Setup
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this exercise, you will explore data from a research project of Brazil’s
    National Institute for Space Research (INPE), [Brazil Data Cube (BDC)](https://oreil.ly/wZ1DD).
    This data cube is a temporal composite from CBERS-4 and surface reflectance data
    from Advanced Wide Field Imager (AWFI), an instrument that captures high-resolution
    land and vegetation imagery.
  prefs: []
  type: TYPE_NORMAL
- en: Obtaining the data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Begin by registering at the website. You will need to create a profile, then
    generate and save an access token using the menu pictured in [Figure 10-10](#bdc_access_token).
  prefs: []
  type: TYPE_NORMAL
- en: '![BDC access token](assets/pgda_1010.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-10\. BDC access token
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The BDC project collects continuous data on land cover (an example is shown
    in [Figure 10-11](#brazil_data_cube)).^([2](ch10.xhtml#ch01fn15)) To identify
    an area of interest, select the cube stack CBERS-4-AWFI within the CBB4_64_16D_STK-1
    collection. The information icon (“i”) will provide background information.
  prefs: []
  type: TYPE_NORMAL
- en: '![Brazil Data Cube](assets/pgda_1011.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-11\. Brazil Data Cube
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'You’ll investigate if the vegetation density of the Amazon has changed over
    time by computing the NDVI, which you learned about in [Chapter 6](ch06.xhtml#the_arcgis_python_api).
    Recall that the NDVI calculates the difference between near-infrared (which is
    reflected by vegetation) and red light (absorbed by vegetation). These bands highlight
    vegetated areas and allow you to observe the density of vegetative growth. You
    will select a location in the Amazon rain forest within a region of dense vegetation.
    Enter the coordinates: latitude = –16.350000 and longitude = –56.666668.'
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As you explore BDC on your own after finishing this exercise, try entering latitude
    and longitude coordinates of your own choosing. As an alternative, you can also
    use the bounding box (bbox) information to locate areas of interest.
  prefs: []
  type: TYPE_NORMAL
- en: Creating your environment
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Open a Jupyter Notebook and install the libraries using `pip install`. (I know,
    I know—I typically advocate for Conda installs, but the WTSS is only stable in
    pip. Often, we must adapt!)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'To access WTSS, use the link in the following code and the BDC access token
    you generated when creating your account:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'This outputs a list of services available at the link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Creating Your Map
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Select the CBERS coverage for the code cell:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The output ([Figure 10-12](#cbfour_sixfour_datacube_with_descriptio)) provides
    the attributes of the CB4_64 cube.
  prefs: []
  type: TYPE_NORMAL
- en: '![CB4_64 datacube with description](assets/pgda_1012.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-12\. CB4_64 datacube with description
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Notice how, when you assign variables, you use the name *exactly* as it appears
    in the table. We are interested in the near-infrared and red bands, so let’s assign
    variables for those:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The time-series data for the location and dates listed is retrieved by the
    `ts` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Plot the static visualization of the time series with the `plot` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The output is pictured in [Figure 10-13](#static_visualization_of_cbfour_sixfour).
  prefs: []
  type: TYPE_NORMAL
- en: '![Static visualization of CB4_64_16D_STK-1 data cube time series](assets/pgda_1013.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-13\. Static visualization of CB4_64_16D_STK-1 data cube time series
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As you can see in [Figure 10-13](#static_visualization_of_cbfour_sixfour), the
    NDVI in this area of the Amazonian rain forest trends upward between 2016 and
    2022\. This could suggest an increase in atmospheric carbon dioxide (CO[2]) levels,
    although precipitation and temperature are more important for explaining interannual
    NDVI variability. Indeed, ecosystem models suggest that most of the observed increase
    in the seasonal amplitude of atmospheric CO[2], indicated by increasing plant
    growth over recent decades, is mostly due to CO[2] fertilization, with climate
    and land-use changes playing secondary roles. We can’t claim anything definitively
    based on this data, but the ability to capture surface reflectance and observe
    NDVI can help in generating hypotheses.^([3](ch10.xhtml#ch01fn16))
  prefs: []
  type: TYPE_NORMAL
- en: Refinements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s look at a few more ways to improve this map.
  prefs: []
  type: TYPE_NORMAL
- en: Making your map interactive
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Next, try creating an interactive display of your findings, using the pandas
    dataframe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: You can hover in the notebook to watch the information display ([Figure 10-14](#interactive_graphic_cbers_four)).
    Select different bands in different years for additional information.
  prefs: []
  type: TYPE_NORMAL
- en: '![Interactive graphic CBERS-4](assets/pgda_1014.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-14\. Interactive graphic CBERS-4
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '`px.line()` is calling a plotly function. The `dataframe` parameter allows
    you to plot data. The `x` parameter specifies the variable on the *x*-axis, and
    the `y` parameter does the same for the *y*-axis. The title is described as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Reducing cloud coverage with masking
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Satellite images observed in time series often display noise or distortion from
    clouds, which can change the spectral behavior of the regions being analyzed.
    Clouds and their shadows can decrease reflectance.
  prefs: []
  type: TYPE_NORMAL
- en: '*Interpolation* is one way of replacing pixel values or points that have been
    distorted by cloud cover. The process of interpolation constructs new data points
    within the range of discrete values of known data points. Data cubes contain *masks*
    that capture the influence of cloud cover, pixel by pixel.'
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The Jupyter Notebook contains additional exercises working with masking and
    interpolation using BDC.
  prefs: []
  type: TYPE_NORMAL
- en: Remote sensing technology relies on cloud masking to process data imagery and
    improve its quality. Cloud masks let users identify cloudy and cloud-free pixels
    by analyzing the percentage of cloud in the mask.
  prefs: []
  type: TYPE_NORMAL
- en: The code in this section will walk you through first detecting clouds, then
    obtaining a mask to “hide” any low-quality data and prevent it from skewing the
    results.
  prefs: []
  type: TYPE_NORMAL
- en: 'When you run the code, it will output metadata with one of three values:'
  prefs: []
  type: TYPE_NORMAL
- en: A value of `0` means the pixel contains no data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A value of `127` means the pixel is clear, with no clouds.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A value of `255` means the pixel is obscured by clouds.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s look at a slightly different location:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'You can see that most of the pixels are clear, but a few cloud observations
    record a value of `255.0`. The `set` function displays the values as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: The output is `{127.0, 255.0}`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code will display cloud observations over the time selected in
    the range:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The timeline data is stored as a list of dates, but you will need to transform
    it to a `datetime` object. To do that, run the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, you’ll store the transformed data in the variables `ndvi_data` and `cmask_data`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is to remove the clouds.  The NumPy array (`np`) will convert
    all `127` values (no clouds) to `1`, and the `255` (cloudy) values will now show
    as NaN (not a number):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Now multiply the NaN values by the NDVI array to interpolate the time-series
    data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'This creates a new dataframe for the NaN values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: And sure enough, when you run the code, there they are in the output ([Table 10-1](#nan_values_in_interpolated_cloud_mask_d)).
  prefs: []
  type: TYPE_NORMAL
- en: Table 10-1\. NaN values in interpolated cloud mask data
  prefs: []
  type: TYPE_NORMAL
- en: '| 2018-01-01 | NaN |'
  prefs: []
  type: TYPE_TB
- en: '| 2018-03-06 | NaN |'
  prefs: []
  type: TYPE_TB
- en: '| 2020-02-18 | NaN |'
  prefs: []
  type: TYPE_TB
- en: '| 2021-03-06 | NaN |'
  prefs: []
  type: TYPE_TB
- en: 'Now we can pull out all of the NaN data and view the interpolated data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s visualize the interpolated data! Run this code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Your output should look like [Figure 10-15](#comparison_of_the_time_series_data_afte).
    Removing the cloud observations yields unobstructed values of the pixels and the
    surface reflectance to more accurately depict the vegetation density.
  prefs: []
  type: TYPE_NORMAL
- en: '![Comparison of the time-series data after interpolating](assets/pgda_1015.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-15\. Comparison of the time-series data after interpolating
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This exercise showed you how to use WTSS to access and analyze remote sensing
    data with Python and APIs and introduced you to the BDC resource. You also saw
    that data analysis isn’t just for backing up your arguments; in fact, exploring
    data with these tools can be an important part of the process of *generating*
    a hypothesis and can lead your inquiries to new and unexpected places.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example 3: Modeling and Forecasting Deforestation in Guadeloupe with Forest
    at Risk'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Beyond the benefit of modeling the spatial probability of deforestation, this
    final section is a great example of modeling large georeferenced raster files.
    We will be working with a Python package that analyzes deforestation: [Forest
    at Risk (FAR)](https://oreil.ly/2MLdk). The functions in this package process
    blocks of data modeled over large geographic scales at a high resolution.'
  prefs: []
  type: TYPE_NORMAL
- en: The area of interest is Guadeloupe, an archipelago and overseas department of
    France located in the eastern Caribbean Sea. According to Global Forest Watch,
    more than 240 acres of Guadeloupe’s humid primary forest have been lost in 2021
    alone. Tree-cover loss was responsible for about 1.07 metric tons (2,358.95 pounds)
    of CO[2] emissions.
  prefs: []
  type: TYPE_NORMAL
- en: The goal of working with FAR is to model Guadeloupe’s deforestation spatially,
    predict the risk of deforestation based on a set of variables, and forecast future
    forest density. The spatial variables you’ll be working with include topography
    (altitude, slope, and aspect), accessibility (distance to roads, towns, and the
    forest edge), and distance to previous areas of deforestation or protected areas.
  prefs: []
  type: TYPE_NORMAL
- en: Any statistical model class with a `.predict()` method can potentially be used
    with the function `forestatrisk.predict_raster()` to predict the spatial risk
    of deforestation. Performing computations on arrays using NumPy and GDAL increases
    efficiency without the need for high computational power. You’ll use this data
    to define the geographic area and to measure deforestation, forest degradation,
    and forest regrowth. Let’s see if we can visualize the change in surface reflectance
    and underlying tree cover by relying on geospatial tools.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll be using data from International Forest Resources and Carbon Emissions
    (IFORCE), part of the European Commission’s Joint Research Center, which is responsible
    for independent scientific and technical support. This data measures Landsat time-series
    forest-cover change in tropical moist forests (TMFs). It was gathered from seasonal
    rain forests at increasing distance from the equator over 30 years, from 1990
    to 2020.
  prefs: []
  type: TYPE_NORMAL
- en: Setup
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Although I am working with Google Colab in this example, there were challenges
    with reproducibility, and I found creating a Conda environment to be a more stable
    option. The dashboard in [Figure 10-16](#opening_the_far_dataset_in_google_colab)
    shows the output files that will not show up in your notebook automatically. Select
    them from the file hierarchy in Colab or from your Jupyter Notebook file structure.
  prefs: []
  type: TYPE_NORMAL
- en: '![Opening the FAR dataset in Google Colab Pro](assets/pgda_1016.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-16\. Opening the FAR dataset in Google Colab Pro
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Depending on your individual setup, Google Colab usually runs quite well. I
    default to Colab when instructing because it is straightforward in highlighting
    the folder structure within your Google Drive (or the import paths, if you choose
    to upload files). There are occasions when you might not be able to get this to
    work. In those instances, I have found creating an environment in the terminal
    to be the solution.
  prefs: []
  type: TYPE_NORMAL
- en: Creating your environment
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: I am using Python 3.10, which as of this writing is the current version, but
    you’ll enter your own version when you run the following code. Everything should
    work, but note that when you build environments and adjust versioning, the details
    might need a little fine-tuning. (You are far enough along in your Python journey
    to be familiar with a few of these options, but return to the preface if you need
    a reminder.)
  prefs: []
  type: TYPE_NORMAL
- en: 'Now you can create your environment using Conda:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'If you get an error message that forestatrisk is not recognized, you can create
    a kernel inside your FAR environment in the terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Enter `**jupyter notebook**` and select the FAR kernel. Your package should
    now run.
  prefs: []
  type: TYPE_NORMAL
- en: Downloading and importing packages
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Importing the necessary files and packages is the first step. The `os` module
    allows you to interact with folders on your operating system. The shutil and copy2
    packages preserve the metadata of the files you are importing, including permissions.
    (There are limits to this when working on MacOS, so consult the [Python documentation](https://oreil.ly/cJTAA).)
    The urllib package collects several modules for opening and reading URLs. You’ll
    also be working with ZIP files, and the `ZipFile` module provides the necessary
    tools.
  prefs: []
  type: TYPE_NORMAL
- en: 'Go ahead and import your packages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Downloading and importing the data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Next you’ll create an output directory, which is especially convenient when
    working in a notebook environment. When you run your scripts, the output files
    will be in this designated location, including the figures you are generating:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The ZIP file *data_GLP.zip* is located at the link in the following code block,
    and `z.extractall` will unzip the file into your Colab or Jupyter Notebook directory,
    as shown in [Figure 10-13](#static_visualization_of_cbfour_sixfour):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The *.zip* file contains a selection of environmental variables and maps from
    across a timespan: all of the data you will need for this brief walk through the
    FAR tutorial, in which you will model the spatial probability of deforestation
    in a specific geography.'
  prefs: []
  type: TYPE_NORMAL
- en: As you [download the data](https://oreil.ly/lkzTB), take a few moments to explore
    the details regarding symbology, values, and labels on the [IFORCE](https://oreil.ly/wtHAi)
    site.
  prefs: []
  type: TYPE_NORMAL
- en: Plotting the Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Run the following code to plot the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: When you run the code cell, this code will generate a map as output, but it
    will not populate in the notebook automatically. To find the output files in Google
    Colab, use the dashboard to navigate through the folder hierarchy to your output
    folder. In a Jupyter Notebook, review the files listed in the tab at the top left
    corner ([Figure 10-17](#locating_your_output_in_the_jupyter_not)). The data files
    you downloaded will also be visible.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/pgda_1017.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-17\. Locating your output in the Jupyter Notebook file structure
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: When you plot the data ([Figure 10-18](#plotting_far_data_on_forest_cover_chang)),
    the forest appears as green and the deforested areas as red. Can you see the perimeter
    where deforestation is more likely?
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/pgda_1018.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-18\. Plotting FAR data on forest cover change in Guadeloupe as a GeoTiff
    raster file
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Sampling the Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Next, it’s time to sample your data. You’ll use the function `far.sample`. The
    help documentation, which you can access by running `help(far.sample)`, tells
    us that this function “randomly draws spatial points in deforested and forested
    areas and…extract[s] environmental variable values for each spatial point.” In
    other words, it samples 10,000 randomly selected pixel centers (points) in the
    areas identified as either deforested or as remaining forest.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take a moment to look at the arguments this function takes. First, `seed`
    tells the function to reproduce the random sampling data from the previous step.
    Next, the spatial cells for each sample are grouped by the `csize` argument. Each
    spatial cell is attributed to a parameter, so they need to be of a sufficient
    size to ease the burden of memory and large datasets. These grouped observations
    are estimates of spatial autocorrelation in deforestation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Remember Tobler’s First Law, introduced in [Chapter 1](ch01.xhtml#introduction_to_geospatial_analytics):
    “Everything is related to everything else, but near things are more related than
    distant things.” *Spatial autocorrelation* is a measure of spatial variation within
    adjacent observations where it is possible for near things to not be similar!
    Adjacent observations can have similar values (positive spatial autocorrelation)
    or have contrasting values (negative autocorrelation). The presence or absence
    of a pixel value between measurements has important implications for spatial statistics.
    There are many reasons why this might be the case, but one could be that a small
    sample from a much larger dataset simply isn’t representative of the larger sample.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code cell, you’ll also see the argument `var_dir`. This points
    to the directory where all of the GeoTIFF files are located and to *fcc23.tif*
    as a forest-cover-change raster file. Once more, pay attention to the output file
    and notice it appearing in your directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'I’m showing you the output here to demonstrate how the calculations are conducted:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code removes “not available” (NA) values from the dataset, then
    computes the number of neighbors of each spatial cell. It outputs a pandas dataframe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: If you do not remove the NAs, you won’t be able to model the data in the next
    steps—recall that the logistic regression model is reporting on the presence or
    absence of a pixel value between measurements.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output shown here is a sample of the first five rows. The variables represent
    the spatial explanatory variables used to model the impact of deforestation and
    include topography represented by altitude and slope; accessibility, measured
    as distance to nearest road, town, river, and forest edge; deforestation history,
    calculated as distance to past deforestation; and land conservation status as
    a protected area (`pa`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Sample-size calculations for forest cover change the map—the pixel value is
    set to `1` when there is forest and to `0` when there is no forest:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Correlation Plots
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The next step is to correlate all this data. This means actually comparing the
    locations of forested areas with the other variables, such as roads, rivers, towns,
    and other deforested areas, to plot how likely it is that the point in question
    will be deforested.
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the code to correlate and then plot the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: This outputs the *correlation plots* shown in [Figure 10-19](#correlation_plots_of_selected_variables).
  prefs: []
  type: TYPE_NORMAL
- en: You can read the probability of deforestation by looking first at the distribution
    of the data. Notice the shape of the probability curve. Look at how far the distance
    a pixel is from a deforested state. In [Figure 10-16](#opening_the_far_dataset_in_google_colab),
    you can see that the bulk of pixels in the sample are relatively close to a deforested
    area already—and, as you know, as the distance increases, the probability of deforestation
    declines.
  prefs: []
  type: TYPE_NORMAL
- en: Looking at the parameter estimates in [Figure 10-19](#correlation_plots_of_selected_variables),
    you can see that the probability of deforestation decreases with altitude, slope,
    distance to past deforestation, and forest edge. The distances to road, town,
    and river all “cross zero,” meaning that the values are not significantly different
    from zero. The confidence interval range includes zero.
  prefs: []
  type: TYPE_NORMAL
- en: '![Correlation plots of selected variables](assets/pgda_1019.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-19\. Correlation plots of selected variables
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Modeling the Probability of Deforestation with the iCAR Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You won’t need to understand the underlying statistics to be able to render
    the output. The statistical model `model_binomial_iCAR` estimates the probability
    of deforestation by examining each pixel and evaluating a set of environmental
    variables like altitude, distance to edge of forest, distance to road, and distance
    to the nearest towns. These variables tell us how accessible the forest is to
    humans. In simple terms, we need this data because fixed environmental variables
    don’t completely explain the process of deforestation, at least not at large scales
    like a country or geographic region.
  prefs: []
  type: TYPE_NORMAL
- en: The probability of deforestation at any given point depends on the probability
    of deforestation in its neighboring cells. Nearest-neighbor methods look for training
    samples near a given point to indicate the number of adjacent entities. In short,
    it is observing patterns in the data and using them to predict new observations.
  prefs: []
  type: TYPE_NORMAL
- en: The statistics behind many of the methods used in this example, such as Markov
    Chain Monte Carlo (MCMC) methods, involve advanced concepts beyond the focus of
    this book, but if you’re interested in learning more, see the [Appendix](app01.xhtml#appendix).
  prefs: []
  type: TYPE_NORMAL
- en: 'For those who know some statistics, briefly, the Intrinsic Conditional Auto-Regressive
    (iCAR) model allows estimations of varying probability of deforestation regardless
    of the nature of immeasurability. For example, it is impossible to measure each
    individual tree and its impact on the tree cover. We use a logistic regression
    model of a binary outcome: 1 if a forest pixel is deforested (fewer pixels) and
    0 if it is not.'
  prefs: []
  type: TYPE_NORMAL
- en: Run the code in the notebook if you are interested in the model preparation
    or variable selection models. The deep statistical tangent is out of the scope
    of this book, but understanding the process is quite interesting.^([4](ch10.xhtml#ch01fn17))
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at the model summary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: What you are observing is that when the coefficient is negative, the variable
    has a negative effect on what you are measuring. For example, the likelihood of
    deforestation is higher when the distance from the road is smaller.
  prefs: []
  type: TYPE_NORMAL
- en: The MCMC Distance Matrix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First, we sample the data, then we prepare, run, and test the model. The model
    is summarized in [Figure 10-20](#mcmc_traces). The statistics behind many of the
    methods are advanced concepts beyond the focus of this book but are highlighted
    in the article “Spatial Scenario of Tropical Deforestation and Carbon Emissions
    for the 21st Century” by Ghislain Vieilledent et al. Briefly, MCMC methods are
    used in Bayesian inference and are based on techniques used to generate random
    sampling sequences to approximate a probability distribution.
  prefs: []
  type: TYPE_NORMAL
- en: MCMC tools, in brief, verify if an algorithm-generated sample is sufficient
    to serve as an approximation of the at-large distribution. Is your sample size
    large enough? To simplify a discussion on traces and posteriors, MCMC draws samples
    from posterior distributions, summaries of uncertainty that you can update based
    on new information. In a nutshell, once you see the data, you are likely to update
    your knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: The main purpose is to make sure that the effective size of your sample is not
    too small and that the samples are similar to the target population. Compare the
    trace to the shape of the sample histograms to the normal distributions in [Figure 10-20](#mcmc_traces).
    Do you notice any potential skew?
  prefs: []
  type: TYPE_NORMAL
- en: '![MCMC traces](assets/pgda_1020.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-20\. MCMC traces
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Modeling Deforestation Probability with predict_raster_binomial_iCAR
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Using `predict_raster_binomial_iCAR`, you can predict the spatial probability
    of deforestation from the `model_binomial_iCAR` model by computing block by block
    over large geographical areas. Selecting just 10 rows should help limit memory
    drain.
  prefs: []
  type: TYPE_NORMAL
- en: Run the code in the notebooks, and let’s look at the generated graphics here.
  prefs: []
  type: TYPE_NORMAL
- en: The output shown in [Figure 10-21](#probability_tiff) shows deforested areas
    as white and forested areas as black.
  prefs: []
  type: TYPE_NORMAL
- en: '![Probability TIFF](assets/pgda_1021.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-21\. Probability TIFF
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This model predicts that annual deforestation between 2010 and 2020 was 498.375
    hectares per year. For comparison, that is the equivalent of more than 11,000
    NBA basketball courts or 930 football fields (including the endzones). The notebook
    includes code to analyze predictions for future forest-cover change. [Figure 10-22](#forest_cover_changecomma_twozerozerozer)
    shows historical forest-cover change for the period between 2000 and 2020.
  prefs: []
  type: TYPE_NORMAL
- en: 'Modeling data and generating probabilities and projections are powerful tools
    for open source communities tasked with monitoring the threat to our rain forests
    and the deleterious downstream impact on our ecosystems and climate. Run the code
    to try modeling the data forward to 2050 and 2100:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: '![Forest-cover change, 2000, 2010, and 2020](assets/pgda_1022.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-22\. Forest-cover change, 2000, 2010, and 2020
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The output is shown in [Figure 10-23](#future_forest_covercomma_twozerofivezer):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: '![Future forest cover, 2050](assets/pgda_1023.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-23\. Future forest cover, 2050
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The persistent impact of ongoing deforestation can be monitored by tracking
    carbon emissions as well.
  prefs: []
  type: TYPE_NORMAL
- en: Carbon Emissions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The carbon emissions of current and future deforestation can be calculated
    from the aboveground biomass (*AGB.tif*) file and the function `.emissions()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'To view the dataframe of deforestation rate estimates:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Estimating the persistent rate of change in deforestation rates, annual carbon
    emissions will continue to increase, as depicted in the output of the code cell:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: The model predicts that, in the absence of robust preservation or conservation
    in the area, carbon emissions will continue to increase rapidly.
  prefs: []
  type: TYPE_NORMAL
- en: Analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section showed you an approach for using your Python and geospatial analytics
    skills to model and forecast tropical deforestation at the country level. This
    approach allows us to estimate the individual effect of each of several different
    environmental factors on the probability of deforestation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The findings are easy to interpret: deforestation becomes more likely as forested
    land becomes more accessible. Deforestation is less likely inside protected areas.
    We also showed that there is a great deal of spatial variability in the deforestation
    process, and we need to account for that to be able to forecast deforestation
    in a realistic way at the national scale.'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter provided a glimpse into the available packages you can use to explore
    and analyze climate change and its potential risks using location intelligence
    and geospatial tools.
  prefs: []
  type: TYPE_NORMAL
- en: In this book, you have been introduced to geospatial analysis and important
    Python libraries, packages, and tools. In the beginning, introductory packages
    like pandas and GeoPandas introduced you to working with dataframes and GeoDataFrames.
    Tools like NumPy, matplotlib, and Plotly began to appear as important dependencies
    for more complex packages.
  prefs: []
  type: TYPE_NORMAL
- en: I hope the long journey has been worth your time, and that you’ll continue to
    explore these functions, tools, packages, and more. Any work in data science is
    iterative. You gain confidence from practice—and, dare I say, from navigating
    the hiccups along the way.
  prefs: []
  type: TYPE_NORMAL
- en: Kudos on your journey.
  prefs: []
  type: TYPE_NORMAL
- en: '^([1](ch10.xhtml#ch01fn14-marker)) To learn more, I recommend this paper, which
    introduces WTSS to create a time series from remote sensing data in the Brazilian
    rainforest: Vinhas, L., Queiroz, G. R., Ferreira, K. R., and Camara, G. 2017\.
    “Web Services for Big Earth Observation Data.” *Revista Brasileira de Cartografia*
    69: 5\. [English translation](https://oreil.ly/rbckx).'
  prefs: []
  type: TYPE_NORMAL
- en: '^([2](ch10.xhtml#ch01fn15-marker)) Earth Observation Data Cubes for Brazil:
    Requirements, Methodology and Products, Earth Observation and Geoinformatics Division,
    National Institute for Space Research (INPE), Avenida dos Astronautas, 1758, Jardim
    da Granja, Sao Jose dos Campos, SP 12227-010, Brazil.'
  prefs: []
  type: TYPE_NORMAL
- en: '^([3](ch10.xhtml#ch01fn16-marker)) Ito, Akihiko. 2019\. “Disequilibrium of
    Terrestrial Ecosystem CO[2] Budget Caused by Disturbance-Induced Emissions and
    Non-CO[2] Carbon Export Flows: A Global Model Assessment.” *Earth System Dynamics*
    10: 685–709\. [*https://doi.org/10.5194/esd-10-685-2019*](https://doi.org/10.5194/esd-10-685-2019).'
  prefs: []
  type: TYPE_NORMAL
- en: '^([4](ch10.xhtml#ch01fn17-marker)) For a deeper dive into the statistical methods
    at work here, please see: Vieilledent, Ghislain, Vancutsem, Christelle, Bourgoin,
    Clément, Ploton, Pierre, Verley, Philippe, and Achard, Frédéric. 2022\. “Spatial
    Scenario of Tropical Deforestation and Carbon Emissions for the 21st Century.”
    BioRxiv preprint. [*https://www.biorxiv.org/content/biorxiv/early/2022/07/23/2022.03.22.485306.full.pdf*](https://www.biorxiv.org/content/biorxiv/early/2022/07/23/2022.03.22.485306.full.pdf)'
  prefs: []
  type: TYPE_NORMAL
