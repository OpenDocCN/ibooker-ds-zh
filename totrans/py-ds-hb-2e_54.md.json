["```py\nIn [1]: %matplotlib inline\n        import matplotlib.pyplot as plt\n        plt.style.use('seaborn-whitegrid')\n        import numpy as np\n```", "```py\nIn [2]: def make_data(N, f=0.3, rseed=1):\n            rand = np.random.RandomState(rseed)\n            x = rand.randn(N)\n            x[int(f * N):] += 5\n            return x\n\n        x = make_data(1000)\n```", "```py\nIn [3]: hist = plt.hist(x, bins=30, density=True)\n```", "```py\nIn [4]: density, bins, patches = hist\n        widths = bins[1:] - bins[:-1]\n        (density * widths).sum()\nOut[4]: 1.0\n```", "```py\nIn [5]: x = make_data(20)\n        bins = np.linspace(-5, 10, 10)\n```", "```py\nIn [6]: fig, ax = plt.subplots(1, 2, figsize=(12, 4),\n                               sharex=True, sharey=True,\n                               subplot_kw={'xlim':(-4, 9),\n                                           'ylim':(-0.02, 0.3)})\n        fig.subplots_adjust(wspace=0.05)\n        for i, offset in enumerate([0.0, 0.6]):\n            ax[i].hist(x, bins=bins + offset, density=True)\n            ax[i].plot(x, np.full_like(x, -0.01), '|k',\n                       markeredgewidth=1)\n```", "```py\nIn [7]: fig, ax = plt.subplots()\n        bins = np.arange(-3, 8)\n        ax.plot(x, np.full_like(x, -0.1), '|k',\n                markeredgewidth=1)\n        for count, edge in zip(*np.histogram(x, bins)):\n            for i in range(count):\n                ax.add_patch(plt.Rectangle(\n                    (edge, i), 1, 1, ec='black', alpha=0.5))\n        ax.set_xlim(-4, 8)\n        ax.set_ylim(-0.2, 8)\nOut[7]: (-0.2, 8.0)\n```", "```py\nIn [8]: x_d = np.linspace(-4, 8, 2000)\n        density = sum((abs(xi - x_d) < 0.5) for xi in x)\n\n        plt.fill_between(x_d, density, alpha=0.5)\n        plt.plot(x, np.full_like(x, -0.1), '|k', markeredgewidth=1)\n\n        plt.axis([-4, 8, -0.2, 8]);\n```", "```py\nIn [9]: from scipy.stats import norm\n        x_d = np.linspace(-4, 8, 1000)\n        density = sum(norm(xi).pdf(x_d) for xi in x)\n\n        plt.fill_between(x_d, density, alpha=0.5)\n        plt.plot(x, np.full_like(x, -0.1), '|k', markeredgewidth=1)\n\n        plt.axis([-4, 8, -0.2, 5]);\n```", "```py\nIn [10]: from sklearn.neighbors import KernelDensity\n\n         # instantiate and fit the KDE model\n         kde = KernelDensity(bandwidth=1.0, kernel='gaussian')\n         kde.fit(x[:, None])\n\n         # score_samples returns the log of the probability density\n         logprob = kde.score_samples(x_d[:, None])\n\n         plt.fill_between(x_d, np.exp(logprob), alpha=0.5)\n         plt.plot(x, np.full_like(x, -0.01), '|k', markeredgewidth=1)\n         plt.ylim(-0.02, 0.22);\n```", "```py\nIn [11]: from sklearn.model_selection import GridSearchCV\n         from sklearn.model_selection import LeaveOneOut\n\n         bandwidths = 10 ** np.linspace(-1, 1, 100)\n         grid = GridSearchCV(KernelDensity(kernel='gaussian'),\n                             {'bandwidth': bandwidths},\n                             cv=LeaveOneOut())\n         grid.fit(x[:, None]);\n```", "```py\nIn [12]: grid.best_params_\nOut[12]: {'bandwidth': 1.1233240329780276}\n```", "```py\nIn [13]: from sklearn.base import BaseEstimator, ClassifierMixin\n\n         class KDEClassifier(BaseEstimator, ClassifierMixin):\n             \"\"\"Bayesian generative classification based on KDE\n\n Parameters\n ----------\n bandwidth : float\n the kernel bandwidth within each class\n kernel : str\n the kernel name, passed to KernelDensity\n \"\"\"\n             def __init__(self, bandwidth=1.0, kernel='gaussian'):\n                 self.bandwidth = bandwidth\n                 self.kernel = kernel\n\n             def fit(self, X, y):\n                 self.classes_ = np.sort(np.unique(y))\n                 training_sets = [X[y == yi] for yi in self.classes_]\n                 self.models_ = [KernelDensity(bandwidth=self.bandwidth,\n                                               kernel=self.kernel).fit(Xi)\n                                 for Xi in training_sets]\n                 self.logpriors_ = [np.log(Xi.shape[0] / X.shape[0])\n                                    for Xi in training_sets]\n                 return self\n\n             def predict_proba(self, X):\n                 logprobs = np.array([model.score_samples(X)\n                                      for model in self.models_]).T\n                 result = np.exp(logprobs + self.logpriors_)\n                 return result / result.sum(axis=1, keepdims=True)\n\n             def predict(self, X):\n                 return self.classes_[np.argmax(self.predict_proba(X), 1)]\n```", "```py\nfrom sklearn.base import BaseEstimator, ClassifierMixin\n\nclass KDEClassifier(BaseEstimator, ClassifierMixin):\n    \"\"\"Bayesian generative classification based on KDE\n\n Parameters\n ----------\n bandwidth : float\n the kernel bandwidth within each class\n kernel : str\n the kernel name, passed to KernelDensity\n \"\"\"\n```", "```py\n    def __init__(self, bandwidth=1.0, kernel='gaussian'):\n        self.bandwidth = bandwidth\n        self.kernel = kernel\n```", "```py\n    def fit(self, X, y):\n        self.classes_ = np.sort(np.unique(y))\n        training_sets = [X[y == yi] for yi in self.classes_]\n        self.models_ = [KernelDensity(bandwidth=self.bandwidth,\n                                      kernel=self.kernel).fit(Xi)\n                        for Xi in training_sets]\n        self.logpriors_ = [np.log(Xi.shape[0] / X.shape[0])\n                           for Xi in training_sets]\n        return self\n```", "```py\nlabel = model.fit(X, y).predict(X)\n```", "```py\n    def predict_proba(self, X):\n        logprobs = np.vstack([model.score_samples(X)\n                              for model in self.models_]).T\n        result = np.exp(logprobs + self.logpriors_)\n        return result / result.sum(axis=1, keepdims=True)\n\n    def predict(self, X):\n        return self.classes_[np.argmax(self.predict_proba(X), 1)]\n```", "```py\nIn [14]: from sklearn.datasets import load_digits\n         from sklearn.model_selection import GridSearchCV\n\n         digits = load_digits()\n\n         grid = GridSearchCV(KDEClassifier(),\n                             {'bandwidth': np.logspace(0, 2, 100)})\n         grid.fit(digits.data, digits.target);\n```", "```py\nIn [15]: fig, ax = plt.subplots()\n         ax.semilogx(np.array(grid.cv_results_['param_bandwidth']),\n                     grid.cv_results_['mean_test_score'])\n         ax.set(title='KDE Model Performance', ylim=(0, 1),\n                xlabel='bandwidth', ylabel='accuracy')\n         print(f'best param: {grid.best_params_}')\n         print(f'accuracy = {grid.best_score_}')\nOut[15]: best param: {'bandwidth': 6.135907273413174}\n         accuracy = 0.9677298050139276\n```", "```py\nIn [16]: from sklearn.naive_bayes import GaussianNB\n         from sklearn.model_selection import cross_val_score\n         cross_val_score(GaussianNB(), digits.data, digits.target).mean()\nOut[16]: 0.8069281956050759\n```"]