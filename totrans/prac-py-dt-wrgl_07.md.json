["```py\n2020-09-01 00:00:01.0430\n```", "```py\n# objectives: filter all September 2020 Citi Bike rides, and output a new\n#             file containing only the rides from 2020-09-01\n\n# program outline:\n# 1\\. read in the data file: 202009-citibike-tripdata.csv\n# 2\\. create a new output file, and write the header row to it.\n# 3\\. for each row in the file, split the `starttime` value on space:\n#       a. if the first item in the resulting list is '2020-09-01', write\n#          the row to our output file\n# 4\\. close the output file\n\n# import the \"csv\" library\nimport csv\n\n# open our data file in \"read\" mode\nsource_file = open(\"202009-citibike-tripdata.csv\",\"r\")\n\n# open our output file in \"write\" mode\noutput_file = open(\"2020-09-01-citibike-tripdata.csv\",\"w\")\n\n# pass our source_file to the DictReader \"recipe\"\n# and store the result in a variable called `citibike_reader`\ncitibike_reader = csv.DictReader(source_file)\n\n# create a corresponding DictWriter and specify that the\n# header should be the same as the `citibike_reader` fieldnames\noutput_writer = csv.DictWriter(output_file, fieldnames=citibike_reader.fieldnames)\n\n# write the header row to the output file\noutput_writer.writeheader()\n\n# use a `for...in` loop to go through our `citibike_reader` list of rows\nfor a_row in citibike_reader:\n\n    # get the value in the 'starttime' column\n    start_timestamp = a_row[\"starttime\"]\n\n    # split the value in 'starttime' on the space character\n    timelist = start_timestamp.split(\" \")\n\n    # the \"date\" part of the string will be the first item, position 0\n    the_date = timelist[0]\n\n    # if `the_date` matches our desired date\n    if the_date == \"2020-09-01\":\n\n        # write that row of data to our output file\n        output_writer.writerow(a_row)\n\n# close the output file\noutput_file.close()\n```", "```py\n if the_date == \"2020-09-01\" or the_date == \"2020-09-02\":\n```", "```py\n 2020-09-01 00:00:01.0430\n```", "```py\n 2020-09-01 00:08:17.5150\n```", "```py\n['2020-09-01 00', '08', '17.5150']\n```", "```py\n# the goal of this script is to try out how a couple of regular expressions\n# fare with some sample test data. ![1](assets/1.png)\n\n# import the regular expression library\nimport re\n\n# using the `re.compile()` method is a helpful way of keeping a reference to\n# our various regular expressions\nbookend_regex = re.compile(\"\\s0[7-9]:\") ![2](assets/2.png)\n\n# always try to be descriptive with the variable names\none_sided_regex = re.compile(\"0[7-9]:\")\n\n# this example should *fail*\nsample1 = \"2020-09-01 00:00:01.0430\"\n\n# this example should *match*\nsample2 = \"2020-09-01 09:04:23.7930\"\n\n# this example should *fail*\nsample3 = \"2020-09-01 10:07:02.0510\"\n\n# let's see what happens!\nprint(\"bookend_regex:\")\nprint(bookend_regex.search(sample1))\nprint(bookend_regex.search(sample2))\nprint(bookend_regex.search(sample3))\n\nprint(\"one_sided_regex:\")\nprint(one_sided_regex.search(sample1))\nprint(one_sided_regex.search(sample2))\nprint(one_sided_regex.search(sample3))\n```", "```py\nbookend_regex:\nNone\n<re.Match object; span=(10, 14), match=' 09:'>\nNone\none_sided_regex:\nNone\n<re.Match object; span=(11, 14), match='09:'>\n<re.Match object; span=(14, 17), match='07:'>\n```", "```py\nplus_ten = re.compile(\"\\s[01][0789]:\")\n\nprint(\"plus_ten\")\nprint(plus_ten.search(\"2020-09-01 18:09:11.0980\"))\n```", "```py\nplus_ten\n<re.Match object; span=(10, 14), match=' 18:'>\n```", "```py\nseven_to_ten = re.compile(\"\\s0[7-9]:|\\s10:\")\n```", "```py\nseptember2020_weekday = re.compile(\"-0[123489]-|-1[0145678]-|-2[1234589]-|-30-\")\n```", "```py\n# objectives: filter all September 2020 Citi Bike rides, and output a new\n#             file containing only weekday rides\n\n# program outline:\n# 1\\. read in the data file: 202009-citibike-tripdata.csv\n# 2\\. create a new output file, and write the header row to it.\n# 3\\. for each row in the file, make a date from the `starttime`:\n#       a. if it's a weekday, write the row to our output file\n# 4\\. close the output file\n\n# import the \"csv\" library\nimport csv\n\n# import the \"datetime\" library\nfrom datetime import datetime\n\n# open our data file in \"read\" mode\nsource_file = open(\"202009-citibike-tripdata.csv\",\"r\")\n\n# open our output file in \"write\" mode\noutput_file = open(\"202009-citibike-weekday-tripdata.csv\",\"w\")\n\n# convert source data to a DictReader; store the result in `citibike_reader`\ncitibike_reader = csv.DictReader(source_file)\n\n# create a corresponding DictWriter and specify its fieldnames\noutput_writer = csv.DictWriter(output_file, fieldnames=citibike_reader.fieldnames)\n\n# actually write the header row to the output file\noutput_writer.writeheader()\n\n# use a `for...in` loop to go through our `citibike_reader` list of rows\nfor a_row in citibike_reader:\n\n    # convert the value in the 'starttime' column to a date object\n    the_date = datetime.strptime(a_row['starttime'], '%Y-%m-%d %H:%M:%S.%f') ![1](assets/1.png)\n\n    # if `the_date` is a weekday\n    if the_date.weekday() <= 4: ![2](assets/2.png)\n        # write that row of data to our output file\n        output_writer.writerow(a_row)\n\n# close the output file\noutput_file.close()\n```", "```py\n# converting data in an .xls file with Python to csv + metadata file\n# using the \"xrld\" library. First, pip install the xlrd library:\n# https://pypi.org/project/xlrd/2.0.1/\n\n# import the \"xlrd\" library\nimport xlrd\n\n# import the `csv` library, to create our output file\nimport csv\n\n# pass our filename as an ingredient to the `xlrd` library's\n# `open_workbook()` \"recipe\"\n# store the result in a variable called `source_workbook`\nsource_workbook = xlrd.open_workbook(\"fredgraph.xls\")\n\n# open and name a simple metadata text file\nsource_workbook_metadata = open(\"fredgraph_metadata.txt\",\"w\") ![1](assets/1.png)\n\n# an `.xls` workbook can have multiple sheets\nfor sheet_name in source_workbook.sheet_names():\n\n    # create a variable that points to the current worksheet by\n    # passing the current value of `sheet_name` to the `sheet_by_name` recipe\n    current_sheet = source_workbook.sheet_by_name(sheet_name)\n\n    # create \"xls_\"+sheet_name+\".csv\" as an output file for the current sheet\n    output_file = open(\"xls_\"+sheet_name+\".csv\",\"w\")\n\n    # use the `csv` library's \"writer\" recipe to easily write rows of data\n    # to `output_file`, instead of reading data *from* it\n    output_writer = csv.writer(output_file)\n\n    # create a Boolean variable to detect if we've hit our table-type data yet\n    is_table_data = False ![2](assets/2.png)\n\n    # now, we need to loop through every row in our sheet\n    for row_num, row in enumerate(current_sheet.get_rows()):\n\n        # pulling out the value in the first column of the current row\n        first_entry = current_sheet.row_values(row_num)[0]\n\n        # if we've hit the header row of our data table\n        if first_entry == 'observation_date':\n\n            # it's time to switch our \"flag\" value to \"True\"\n            is_table_data = True\n\n        # if `is_table_data` is True\n        if is_table_data:\n\n            # write this row to the data output file\n            output_writer.writerow(current_sheet.row_values(row_num))\n\n        # otherwise, this row must be metadata\n        else:\n\n            # since we'd like our metadata file to be nicely formatted, we\n            # need to loop through the individual cells of each metadata row\n            for item in current_sheet.row(row_num):\n\n                    # write the value of the cell\n                    source_workbook_metadata.write(item.value)\n\n                    # separate it from the next cell with a tab\n                    source_workbook_metadata.write('\\t')\n\n            # at the end of each line of metadata, add a newline\n            source_workbook_metadata.write('\\n')\n\n    # just for good measure, let's close our output files\n    output_file.close()\n    source_workbook_metadata.close()\n```", "```py\n# converting data in an .xls file with Python to csv + metadata file, with\n# functional date values using the \"xrld\" library.\n# first, pip install the xlrd library:\n# https://pypi.org/project/xlrd/2.0.1/\n\n# then, import the `xlrd` library\nimport xlrd\n\n# import the csv library\nimport csv\n\n# needed to test if a given value is *some* type of number\nfrom numbers import Number\n\n# for parsing/formatting our newly interpreted Excel dates\nfrom datetime import datetime\n\n# pass our filename as an ingredient to the `xlrd` library's\n# `open_workbook()` \"recipe\"\n# store the result in a variable called `source_workbook`\nsource_workbook = xlrd.open_workbook(\"fredgraph.xls\")\n\n# open and name a simple metadata text file\nsource_workbook_metadata = open(\"fredgraph_metadata.txt\",\"w\")\n\n# an `.xls` workbook can have multiple sheets\nfor sheet_name in source_workbook.sheet_names():\n\n    # create a variable that points to the current worksheet by\n    # passing the current value of `sheet_name` to the `sheet_by_name` recipe\n    current_sheet = source_workbook.sheet_by_name(sheet_name)\n\n    # create \"xls_\"+sheet_name+\".csv\" as an output file for the current sheet\n    output_file = open(\"xls_\"+sheet_name+\"_dates.csv\",\"w\")\n\n    # use the `csv` library's \"writer\" recipe to easily write rows of data\n    # to `output_file`, instead of reading data *from* it\n    output_writer = csv.writer(output_file)\n\n    # create a Boolean variable to detect if we've hit our table-type data yet\n    is_table_data = False\n\n    # now, we need to loop through every row in our sheet\n    for row_num, row in enumerate(current_sheet.get_rows()):\n\n        # pulling out the value in the first column of the current row\n        first_entry = current_sheet.row_values(row_num)[0]\n\n        # if we've hit the header row of our data table\n        if first_entry == 'observation_date':\n\n            # it's time to switch our \"flag\" value to \"True\"\n            is_table_data = True\n\n        # if `is_table_data` is True\n        if is_table_data:\n\n            # extract the table-type data values into separate variables\n            the_date_num = current_sheet.row_values(row_num)[0]\n            U6_value = current_sheet.row_values(row_num)[1]\n\n            # create a new row object with each of the values\n            new_row = [the_date_num, U6_value]\n\n            # if the `the_date_num` is a number, then the current row is *not*\n            # the header row. We need to transform the date.\n            if isinstance(the_date_num, Number):\n\n                # use the xlrd library's `xldate_as_datetime()` to generate\n                # a Python datetime object\n                the_date_num = xlrd.xldate.xldate_as_datetime(\n                    the_date_num, source_workbook.datemode) ![1](assets/1.png)\n\n                # overwrite the first value in the new row with\n                # the reformatted date\n                new_row[0] = the_date_num.strftime('%m/%d/%Y') ![2](assets/2.png)\n\n            # write this new row to the data output file\n            output_writer.writerow(new_row)\n\n        # otherwise, this row must be metadata\n        else:\n\n            # since we'd like our metadata file to be nicely formatted, we\n            # need to loop through the individual cells of each metadata row\n            for item in current_sheet.row(row_num):\n\n                    # write the value of the cell\n                    source_workbook_metadata.write(item.value)\n\n                    # separate it from the next cell with a tab\n                    source_workbook_metadata.write('\\t')\n\n            # at the end of each line of metadata, add a newline\n            source_workbook_metadata.write('\\n')\n\n    # just for good measure, let's close our output files\n    output_file.close()\n    source_workbook_metadata.close()\n```", "```py\n# an example of reading data from a fixed-width file with Python.\n# the source file for this example comes from the NOAA and can be accessed here:\n# https://www1.ncdc.noaa.gov/pub/data/ghcn/daily/ghcnd-stations.txt\n# the metadata for the file can be found here:\n# https://www1.ncdc.noaa.gov/pub/data/ghcn/daily/readme.txt\n\n# import the `csv` library, to create our output file\nimport csv\n\nfilename = \"ghcnd-stations\"\n\n# reading from a basic text file doesn't require any special libraries\n# so we'll just open the file in read format (\"r\") as usual\nsource_file = open(filename+\".txt\", \"r\")\n\n# the built-in \"readlines()\" method does just what you'd think:\n# it reads in a text file and converts it to a list of lines\nstations_list = source_file.readlines()\n\n# create an output file for our transformed data\noutput_file = open(filename+\".csv\",\"w\")\n\n# use the `csv` library's \"writer\" recipe to easily write rows of data\n# to `output_file`, instead of reading data *from* it\noutput_writer = csv.writer(output_file)\n\n# create the header list\nheaders = [\"ID\",\"LATITUDE\",\"LONGITUDE\",\"ELEVATION\",\"STATE\",\"NAME\",\"GSN_FLAG\",\n           \"HCNCRN_FLAG\",\"WMO_ID\"]\n\n# write our headers to the output file\noutput_writer.writerow(headers)\n\n# loop through each line of our file (multiple \"sheets\" are not possible)\nfor line in stations_list:\n\n    # create an empty list, to which we'll append each set of characters that\n    # makes up a given \"column\" of data\n    new_row = []\n\n    # ID: positions 1-11\n    new_row.append((line[0:11]).strip()) ![1](assets/1.png)\n\n    # LATITUDE: positions 13-20\n    new_row.append((line[12:20]).strip())\n\n    # LONGITUDE: positions 22-30\n    new_row.append((line[21:30]).strip())\n\n    # ELEVATION: positions 32-37\n    new_row.append((line[31:37]).strip())\n\n    # STATE: positions 39-40\n    new_row.append((line[38:40]).strip())\n\n    # NAME: positions 42-71\n    new_row.append((line[41:71]).strip())\n\n    # GSN_FLAG: positions 73-75\n    new_row.append((line[72:75]).strip())\n\n    # HCNCRN_FLAG: positions 77-79\n    new_row.append((line[76:79]).strip())\n\n    # WMO_ID: positions 81-85\n    new_row.append((line[80:85]).strip())\n\n    # now all that's left is to use the\n    # `writerow` function to write new_row to our output file\n    output_writer.writerow(new_row)\n\n# just for good measure, let's close the `.csv` file we just created\noutput_file.close()\n```", "```py\nLoanNumber                                          9547507704\nDateApproved                                        05/01/2020\nSBAOfficeCode                                              464\nProcessingMethod                                           PPP\nBorrowerName                             SUMTER COATINGS, INC.\nBorrowerAddress                          2410 Highway 15 South\nBorrowerCity                                            Sumter\nBorrowerState                                             <NA>\nBorrowerZip                                         29150-9662\nLoanStatusDate                                      12/18/2020\nLoanStatus                                        Paid in Full\nTerm                                                        24\nSBAGuarantyPercentage                                      100\nInitialApprovalAmount                                769358.78\nCurrentApprovalAmount                                769358.78\nUndisbursedAmount                                            0\nFranchiseName                                             <NA>\nServicingLenderLocationID                                19248\nServicingLenderName                               Synovus Bank\nServicingLenderAddress                           1148 Broadway\nServicingLenderCity                                   COLUMBUS\nServicingLenderState                                        GA\nServicingLenderZip                                  31901-2429\nRuralUrbanIndicator                                          U\nHubzoneIndicator                                             N\nLMIIndicator                                              <NA>\nBusinessAgeDescription       Existing or more than 2 years old\nProjectCity                                             Sumter\nProjectCountyName                                       SUMTER\nProjectState                                                SC\nProjectZip                                          29150-9662\nCD                                                       SC-05\nJobsReported                                                62\nNAICSCode                                               325510\nRaceEthnicity                                       Unanswered\nUTILITIES_PROCEED                                         <NA>\nPAYROLL_PROCEED                                      769358.78\nMORTGAGE_INTEREST_PROCEED                                 <NA>\nRENT_PROCEED                                              <NA>\nREFINANCE_EIDL_PROCEED                                    <NA>\nHEALTH_CARE_PROCEED                                       <NA>\nDEBT_INTEREST_PROCEED                                     <NA>\nBusinessType                                       Corporation\nOriginatingLenderLocationID                              19248\nOriginatingLender                                 Synovus Bank\nOriginatingLenderCity                                 COLUMBUS\nOriginatingLenderState                                      GA\nGender                                              Unanswered\nVeteran                                             Unanswered\nNonProfit                                                 <NA>\n```", "```py\n# quick script for adding a \"fingerprint\" column to our loan data, which will\n# help us confirm/correct for any typos or inconsistencies in, e.g., bank names\n\n# import the csv library\nimport csv\n\n# importing the `fingerprints` library\nimport fingerprints\n\n# read the recent data sample into a variable\nppp_data = open('public_150k_plus_recent.csv','r')\n\n# the DictReader function makes our source data more usable\nppp_data_reader = csv.DictReader(ppp_data)\n\n# create an output file to write our modified dataset to\naugmented_ppp_data = open('public_150k_plus_fingerprints.csv','w')\n\n# create a \"writer\" so that we can output whole rows at once\naugmented_data_writer = csv.writer(augmented_ppp_data)\n\n# because we're adding a column, we need to create a new header row as well\nheader_row = []\n\n# for every column header\nfor item in ppp_data_reader.fieldnames: ![1](assets/1.png)\n\n    # append the existing column header\n    header_row.append(item)\n\n    # if we're at 'OriginatingLender'\n    if item == 'OriginatingLender':\n\n        # it's time to add a new column\n        header_row.append('OriginatingLenderFingerprint')\n\n# now we can write our expanded header row to the output file\naugmented_data_writer.writerow(header_row)\n\n# iterate through every row in our data\nfor row in ppp_data_reader:\n\n    # create an empty list to hold our new data row\n    new_row = [] ![2](assets/2.png)\n\n    # for each column of data in the *original* dataset\n    for column_name in ppp_data_reader.fieldnames:\n\n        # first, append this row's value for that column\n        new_row.append(row[column_name])\n\n        # when we get to the 'OriginatingLender' column, it's time\n        # to add our new \"fingerprint\" value\n        if column_name == 'OriginatingLender':\n\n            # our fingerprint will consist of the generated fingerprint PLUS\n            # the OriginatingLenderLocationID\n            the_fingerprint = fingerprints.generate(row[column_name]) + \\\n                              \" \" + row['OriginatingLenderLocationID']\n\n            # append the compound fingerprint value to our row\n            new_row.append(the_fingerprint)\n\n    # once the whole row is complete, write it to our output file\n    augmented_data_writer.writerow(new_row)\n\n# close both files\naugmented_ppp_data.close()\nppp_data.close()\n```", "```py\nLoanNumber                                           9547507704\nDateApproved                                         05/01/2020\nSBAOfficeCode                                               464\nProcessingMethod                                            PPP\nBorrowerName                              SUMTER COATINGS, INC.\nBorrowerAddress                           2410 Highway 15 South\nBorrowerCity                                             Sumter\nBorrowerState                                               NaN\nBorrowerZip                                          29150-9662\nLoanStatusDate                                       12/18/2020\nLoanStatus                                         Paid in Full\nTerm                                                         24\nSBAGuarantyPercentage                                       100\nInitialApprovalAmount                                 769358.78\nCurrentApprovalAmount                                 769358.78\nUndisbursedAmount                                           0.0\nFranchiseName                                               NaN\nServicingLenderLocationID                                 19248\nServicingLenderName                                Synovus Bank\nServicingLenderAddress                            1148 Broadway\nServicingLenderCity                                    COLUMBUS\nServicingLenderState                                         GA\nServicingLenderZip                                   31901-2429\nRuralUrbanIndicator                                           U\nHubzoneIndicator                                              N\nLMIIndicator                                                NaN\nBusinessAgeDescription        Existing or more than 2 years old\nProjectCity                                              Sumter\nProjectCountyName                                        SUMTER\nProjectState                                                 SC\nProjectZip                                           29150-9662\nCD                                                        SC-05\nJobsReported                                               62.0\nNAICSCode                                              325510.0\nRaceEthnicity                                        Unanswered\nUTILITIES_PROCEED                                           NaN\nPAYROLL_PROCEED                                       769358.78\nMORTGAGE_INTEREST_PROCEED                                   NaN\nRENT_PROCEED                                                NaN\nREFINANCE_EIDL_PROCEED                                      NaN\nHEALTH_CARE_PROCEED                                         NaN\nDEBT_INTEREST_PROCEED                                       NaN\nBusinessType                                        Corporation\nOriginatingLenderLocationID                               19248\nOriginatingLender                                  Synovus Bank\nOriginatingLenderFingerprint                 bank synovus 19248\nOriginatingLenderCity                                  COLUMBUS\nOriginatingLenderState                                       GA\nGender                                               Unanswered\nVeteran                                              Unanswered\nNonProfit                                                   NaN\n```", "```py\n# script to merge our PPP loan data with information from the SBA's NAICS\n# size requirements, found here:\n# https://www.sba.gov/document/support--table-size-standards\n\n# import pandas to facilitate the merging and sorting\nimport pandas as pd\n\n# read our PPP loan data into a new DataFrame\nppp_data = pd.read_csv('public_150k_plus_fingerprints.csv', dtype='string') ![1](assets/1.png)\n\n# read the NAICS data into a separate DataFrame\nsba_naics_data = pd.read_csv('SBA-NAICS-data.csv', dtype='string')\n\n# if there's no value in the 'NAICSCode' column, replace it with \"None\"\nppp_data['NAICSCode'] = ppp_data['NAICSCode'].fillna(\"None\") ![2](assets/2.png)\n\n# merge the two datasets using a \"left\" merge\nmerged_data = pd.merge(ppp_data, sba_naics_data, how='left',\n                      left_on=['NAICSCode'], right_on=['NAICS Codes'],\n                      indicator=True)\n\n# open a file to save our merged data to\nmerged_data_file = open('ppp-fingerprints-and-naics.csv', 'w')\n\n# write the merged data to an output file as a CSV\nmerged_data_file.write(merged_data.to_csv())\n\n# print out the values in the '_merge' column to see how many\n# entries in our loan data don't get matched to an NAICS code\nprint(merged_data.value_counts('_merge'))\n\n# create a new DataFrame that is *just* the unmatched rows\nunmatched_values = merged_data[merged_data['_merge']=='left_only']\n\n# open a file to write the unmatched values to\nunmatched_values_file = open('ppp-unmatched-naics-codes.csv', 'w')\n\n# write a new CSV file that contains all the unmatched NAICS codes in our\n# PPP loan data, along with how many times it appears\nunmatched_values_file.write(unmatched_values.value_counts('NAICSCode').to_csv())\n```"]