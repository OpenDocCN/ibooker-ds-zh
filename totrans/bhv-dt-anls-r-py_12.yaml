- en: 'Chapter 8\. Experimental Design: The Basics'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let’s start our exploration of experimentation with a very simple experiment:
    influenced by a leading online store, AirCnC’s management has decided that a “1-click
    booking” button is just what’s needed to boost AirCnC’s booking rate. As I discussed
    earlier, we’ll assign customers to our experimental groups one by one as they
    connect to the website. This is the simplest possible type of experiment, and
    many companies offer interfaces that allow you to create and start running A/B
    tests like this in a matter of minutes.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This straightforward experiment will be the opportunity to go through the process
    without getting bogged down in technical considerations:'
  prefs: []
  type: TYPE_NORMAL
- en: The first step is to plan the experiment. This is where the causal-behavioral
    perspective comes in, to help ensure that you have clearly defined criteria for
    success, and that you understand what it is you’re testing and how you expect
    it to impact your target metric.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, after reviewing the data and packages that we’ll use in the rest of the
    chapter, I’ll show you how to do the random assignment and determine the sample
    size for your experiment.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, we’ll analyze the results of the experiment, which in such a simple
    case will be very quick.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The vocabulary of experimental design owes much to its statistical and scientific
    roots. I’ll talk of “control” and “treatment” groups as well as “interventions,”
    which may sound ominous or like overkill when we’re really discussing the position
    of a button on a website or the amount of a discount. When talking about experiments
    in general, there isn’t much I can do to use a simpler vocabulary; but when you’re
    talking to your business partners about a specific experiment, I would encourage
    you to stick with concrete terms relating to that experiment (e.g. “the old and
    the new creatives,” “the group with the lower discount and the group with the
    higher discount,” etc.).
  prefs: []
  type: TYPE_NORMAL
- en: 'Anecdote: once when I suggested an “intervention,” a business partner thought
    I meant that they weren’t doing their job well and I needed to intervene. Not
    the best start for a fruitful and trusting relationship. Meet people where they
    are and make an effort to speak their language instead of expecting them to know
    yours.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Planning the Experiment: Theory of Change'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Planning is a crucial step of experimental design. Experiments can fail for
    a variety of reasons, many of which you can’t control, such as the implementation
    going awry; but poor planning is both a frequent cause of failure and one you
    *can* control. Anyone who runs experiments for a living has horror stories of
    experiments that may or may not have been technically impeccable but were utterly
    pointless because people didn’t have clarity about what was tested.
  prefs: []
  type: TYPE_NORMAL
- en: 'At the end of the day, no process can save you if you’re just going through
    the motions without exercising your business acumen and common sense, but hopefully
    the formula I’m going to outline will help you make sure you have covered all
    your bases. We’ll borrow a concept from nonprofit and governmental planning, namely
    the *theory of change* (ToC). In one sentence, your ToC should connect what you’re
    doing to your ultimate business goal and target metric through a behavioral change:'
  prefs: []
  type: TYPE_NORMAL
- en: Implementing [INTERVENTION] will help us achieve [BUSINESS GOAL], as measured
    by [TARGET METRIC], through [BEHAVIORAL LOGIC].
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'I’ll elaborate on each of the four components in turn, but to give you a sense
    of where we’ll land, here’s what our final theory of change will look like:'
  prefs: []
  type: TYPE_NORMAL
- en: Implementing [a 1-click booking button] will help us achieve [higher revenue],
    as measured by [booking probability], through [a reduction in the duration of
    the booking process].
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This can be represented in CD format, as in [Figure 8-1](#theory_of_change_for_our_experiment).
  prefs: []
  type: TYPE_NORMAL
- en: '![Theory of change for our experiment](Images/BEDA_0801.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-1\. Theory of change for our experiment
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Let’s first review our business goal and target metric, then our intervention,
    and finally our behavioral logic.
  prefs: []
  type: TYPE_NORMAL
- en: Business Goal and Target Metric
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You might be surprised that I start with the business goal and target metric
    instead of the definition of the intervention. After all, shouldn’t we know what
    we’re testing first? Unfortunately, a common cause of failure is the decision
    to test something (often the latest management fad or something that your boss’s
    boss read about) without a clear sense of what we’re trying to achieve.
  prefs: []
  type: TYPE_NORMAL
- en: Business goal
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The first step is to determine the business goal for the experiment. Companies
    are usually trying to increase their profit, but just putting “higher profit”
    as your business goal would not be very helpful. Instead, I would recommend going
    one level deeper and using a variable such as revenue, costs, customer retention,
    etc., that is more concrete but of obvious benefit to the company. This may seem
    a trivial step, but it can actually surface disagreements about the goal of an
    experiment (e.g., is it to reduce costs or increase revenue?). Here, the business
    goal for the 1-click button experiment is higher revenue ([Figure 8-2](#our_business_goal_is_revenue)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Our business goal is revenue](Images/BEDA_0802.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-2\. Our business goal is revenue
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Target metric
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The second step is to decide how you’ll measure success at the end of the experiment,
    i.e., your target metric. There is a trade-off at play here: on the one hand,
    you want to use a metric as close to profit as possible, such as dollars of additional
    revenue or decreased cost; on the other hand, you want to pick a metric as close
    to your intervention as possible, to reduce extraneous noise.'
  prefs: []
  type: TYPE_NORMAL
- en: A compromise you’ll often have to make at this point is to use “leading indicators”—basically
    causes of the variables you’re ultimately interested in. For example, you may
    ultimately care about your customers’ LTV (lifetime value, the total amount they’ll
    spend with your company) but have to settle for a three-month booking amount.
    Similarly, sign-up can be used as a leading indicator for usage, usage for amount
    purchased, etc. This will allow you to report results much earlier than if you
    used long-term business metrics, while still having a clear connection to your
    business goal.
  prefs: []
  type: TYPE_NORMAL
- en: However, if your target metric is an operational metric instead of a financial
    metric, things can get hairy. If the button shortens how long it takes someone
    to book a trip but otherwise doesn’t increase bookings in any way, is that a success
    or not? What about satisfaction with the booking experience and net promoter score?
    These may not translate directly into dollar figures, but at the same time it
    is not unreasonable to assume that improving them has a positive impact for your
    business. This is sometimes done informally, by picking operational metrics as
    targets for experimentation and assuming with some hand-waving that these will
    end up benefiting the company’s bottom line. Of course, armed with this book,
    we can do better. We can validate and measure the causal connection between a
    short-term operational metric and long-term business outcomes through an observational
    study or a dedicated experiment, as we’ll see later.
  prefs: []
  type: TYPE_NORMAL
- en: Pitfalls of poor target metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The goal here again is to make good business decisions. I don’t want to be a
    fanatic of dollar figures, because it would be unduly restrictive and would exclude
    a large range of business improvements. At the same time, you want to make sure
    that you have a measurable target metric that you’ll be able to track. There are
    several potential pitfalls that you should avoid here.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first one is picking something you can’t measure reliably. What would it
    mean to say, “The 1-click button makes the booking experience easier”? How would
    you measure it? Asking the product manager or the product owner to decide after
    the fact whether there has been an improvement is not a measurement. “Our customers
    rate the website as easier to navigate, as measured by a two-question survey at
    the end of a visit” may be an imperfect proxy but at least it can be measured.
    This is why it makes sense to express the business goal and the target metric
    separately: the former expresses your true intent, even if it’s not measurable,
    while the latter shows clearly what you intend to measure. This avoids misunderstandings
    and moving goal posts.'
  prefs: []
  type: TYPE_NORMAL
- en: The second pitfall is to write down a laundry list of metrics, such as “success
    would be an improvement in booking rate, booking amount, customer satisfaction,
    or net promoter score,” or even worse, to wait until after seeing the results
    to determine the metrics for success—e.g., you were originally thinking that the
    experiment would improve customer experience, but when the results come, customer
    experience is flat and average website session duration has improved. The problem
    with that approach is that it increases the risk of false positives (calling the
    result a success when it was a random fluke).^([1](ch08.xhtml#ch01fn13)) It’s
    okay however to have up to two or three target metrics that are clearly defined
    before the experiment as long as you take that into account when analyzing the
    results (more on that later). Some people advocate using a composite of multiple
    metrics (e.g., a weighted average), which is called an *Overall Evaluation Criterion*
    (OEC), but I personally feel that it often obscures things more than it helps.
    I would rather encourage you to clearly articulate your theory of change and how
    the various metrics are related to each other—for example, do you expect the 1-click
    button to improve booking rate *and* customer experience, or booking rate *through*
    customer experience?
  prefs: []
  type: TYPE_NORMAL
- en: To conclude, in the case of the 1-click button experiment, we could directly
    use booking revenue as our target metric, but we don’t expect that our intervention
    will modify the average amount per booking, so it makes more sense to use the
    probability that a customer will complete a booking ([Figure 8-3](#adding_the_target_metriccomma_the_proba)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Adding the target metric, the probability of completing a booking](Images/BEDA_0803.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-3\. Adding the target metric, the probability of completing a booking
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Intervention
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Once we have our business goal and target metric, we can work on defining our
    intervention. Here the idea for the “1-click button” intervention comes from the
    company’s management ([Figure 8-4](#adding_the_interventioncomma_the_one_cl)),
    but it could also have come from UX or behavioral research: identifying issues
    and opportunities for improvement in a company’s processes, products, and services
    is indeed one of the main tasks of researchers in business but it’s outside the
    scope of this book.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Adding the intervention, the 1-click button](Images/BEDA_0804.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-4\. Adding the intervention, the 1-click button
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'On the face of it, what could be simpler than a “1-click booking” button? Most
    of us have seen it implemented on the website of an online retailer or another
    and the idea seems perfectly straightforward. But there can be a large gap between
    a business idea, so used and familiar that everyone immediately feels they know
    what it is, and a specific implementation. If you think about the nitty-gritty
    details of how it would be implemented, there are actually a lot of questions
    to answer, each with multiple possible answers:'
  prefs: []
  type: TYPE_NORMAL
- en: At which point in the process does the button become available?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Where is the button located?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What does the button look like? Is it in the same colors as the other buttons
    on the page or does it stand out in a bright and rich color?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is written on the button?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What information do we need about a customer to make 1-click booking available
    and how do we make sure we have it?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What happens after the customer clicks the button? What page are they taken
    to, and what, if any, actions do they still have to take?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Etc.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s say for example that the website’s color theme is pastel green and blue,
    to hint at nature and travel, and the new button is a shiny red. If that button
    increases booking, it might be because of the attractiveness of 1-click booking,
    but it could also be because customers otherwise struggle to see the normal booking
    button and give up. In that case, the cause of the increase in booking is really
    “a more visible booking button” rather than “a 1-click booking button,” but you
    can’t distinguish between the two because the 1-click button was also more visible.
    Unfortunately, you can’t ever just test one idea, as you’re always also testing
    the many aspects of how it is implemented.
  prefs: []
  type: TYPE_NORMAL
- en: The lesson here is that A/B testing is a powerful but narrow tool. You need
    to be careful not to make—or let others make—grand statements about what a specific
    experiment says. It is definitely easier said than done, because business partners
    often want answers that are broad, clear-cut, and without fine print. With that
    said, even simply stating in your presentation that you’re testing a specific
    implementation and not a general idea can be useful, for example, “this experiment
    will test the impact of a 1-click button under such and such conditions, and its
    results should not be interpreted as applying to booking buttons more broadly.”
  prefs: []
  type: TYPE_NORMAL
- en: More generally, I would recommend you test the smallest possible intervention
    you can. In this case, you could try changing the color or position of the booking
    button before implementing the bigger change that is a 1-click button. You might
    get pushback from your business partners, who often want to run an “omnibus” test
    with multiple changes at once; in that case, make clear that you’re really just
    testing that none of the changes break the experience, as opposed to actually
    measuring impacts. A better alternative would be to test different implementations
    of the same concept in the experiment. If four slightly different 1-click button
    treatments have the same impact, you can more confidently draw conclusions regarding
    the general impact of 1-click booking; on the other hand, if they have very different
    impacts, then this suggests that implementation matters a lot and that you need
    to be very careful with your conclusions regarding how a specific implementation
    will generalize.
  prefs: []
  type: TYPE_NORMAL
- en: Behavioral Logic
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Once we know our business goal and target metrics and we have defined our intervention,
    the last step is to connect the two through the behavioral logic of your theory
    of change: why and how would our intervention impact our target metric?'
  prefs: []
  type: TYPE_NORMAL
- en: 'This is another surprisingly common source of failure for experiments: a problem
    has been identified and someone decides to implement the latest management fad
    that they have been thinking about recently, even though it’s unclear why it would
    help with this specific problem. Or someone decides that a more attractive and
    simpler user interface (UI) will increase purchase amounts. To have confidence
    in our experiment, you need to be able to articulate a reasonable behavioral story.^([2](ch08.xhtml#ch01fn14))
    In the case of 1-click booking, you might hypothesize that customers identify
    an attractive booking but give up before completing their booking because the
    booking process is cumbersome; the 1-click button would impact the probability
    of booking by shortening and simplifying the booking process. This is typically
    where your ToC comes together in a CD, in this case the one I showed you at the
    beginning of the chapter ([Figure 8-5](#the_complete_cd_for_our_theory_of_chang)).'
  prefs: []
  type: TYPE_NORMAL
- en: '![The complete CD for our theory of change](Images/BEDA_0805.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-5\. The complete CD for our theory of change
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Overall, articulating your behavioral logic has two benefits. First, it is often
    testable in itself. Do a large number of customers actually drop off between starting
    a booking and completing it? If so, the hypothesized logic makes sense from a
    behavioral data perspective. But if, for example, most customers leave without
    having started the booking process with a specific product, e.g., because they
    couldn’t find something that appealed to them or they were overwhelmed with the
    number of options, then AirCnC is trying to solve the wrong problem; it’s unlikely
    that offering 1-click booking will improve their numbers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Ask yourself: what would confirm or refute our logic? What would the data look
    like if it were true or false? If you don’t have the necessary data at hand, it
    might be worth running a preliminary test, such as bringing in 10 people for user
    experience testing, e.g., observing them trying to use your website while they’re
    thinking out loud. This may not fully confirm or refute your logic, but it will
    probably give you some indication, at a fraction of the development cost for the
    solution considered. As the often-cited quote by Albert Einstein goes, “If I had
    an hour to solve a problem, I’d spend 55 minutes thinking about the problem and
    5 minutes thinking about solutions.”'
  prefs: []
  type: TYPE_NORMAL
- en: The second benefit of articulating the behavioral logic of your intervention
    is that it will generally give you a sense of the potential upside. What would
    the numbers look like, in the best-case scenario, if the problem at hand was solved?
    Assuming that all customers who drop off during the booking process would complete
    it with 1-click booking, how much would the booking rate increase? This is the
    best-case scenario from the perspective of the experiment because we’re assuming
    that it would fully solve the problem, which is unlikely in reality. If the increase
    in booking rate under this scenario would not pay for the implementation of the
    1-click booking, then don’t even bother testing it.
  prefs: []
  type: TYPE_NORMAL
- en: Once you’ve validated that your best-case scenario would be profitable, you
    can start thinking about your most likely scenario. How much would we expect 1-click
    booking to improve the booking rate? There is undoubtedly a lot of subjectivity
    and uncertainty involved, but having articulated the behavioral mechanism at play,
    you can generally make reasonable guesses. Do you really expect that 75% of people
    are dropping off the booking process because it’s taking too long? In addition,
    it can be a worthwhile exercise by making explicit people’s assumptions and gut
    feelings. If the product manager and the UX researcher vastly disagree on the
    percentage of customers who drop off because the process is taking too long, you
    need to close that gap first. What does one of them know that the other doesn’t?
    Use your business sense and understanding of the processes. If most customers
    are dropping off at the exact same step in the process, e.g., payment, then it’s
    likely that there is something wrong with that specific step—people don’t all
    run out of patience at the exact same time. You can then compare the expected
    benefits with the cost of implementing the solution. Is it still worthwhile?
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also approach this question from the other side: start by determining
    the breakeven point of the solution, i.e., the improvement in the target metric
    that would make the solution profitable to implement, and then consider whether
    that improvement is realistic from a behavioral standpoint. From a psychological
    perspective, it’s better to start with the expected result than with the breakeven
    point: if you start with the breakeven point, you’re more likely to anchor on
    it and find reasons to justify that it’s achievable. However, in many cases, you’ll
    know the breakeven point first, for example if it was calculated during a preliminary
    cost-benefit analysis; your company or business partners might also request it
    and refuse to think about the expected result first. Don’t worry too much about
    it. Regardless of whether you’re working with your expected or best-case result,
    we’ll need it to determine the minimum detectable effect for our experiment.'
  prefs: []
  type: TYPE_NORMAL
- en: It’s important that your behavioral logic connects your proposed solution with
    your target metric. Don’t leave it to “this will improve the customer experience.”
    How will you know it? If your logic is solid, you should be able to express it
    in terms of a causal diagram, with at least some of the effects being observable.
  prefs: []
  type: TYPE_NORMAL
- en: A useful rule of thumb to help you articulate your logic is to break down your
    business metric into components. For example, revenue (or most of its variations)
    can be broken down into number of customers, probability/frequency of purchase,
    quantity purchased, and price paid. Determining which components are likely to
    be affected can allow you to better articulate the business case. If your business
    partners are concerned by a decrease in the number of customers and the proposed
    intervention would most likely only increase the quantity purchased, you need
    to clarify with them that they would still call it a win. This approach can also
    reduce the noise in your experiment; if the proposed intervention would most likely
    only increase the quantity purchased, you can focus on that metric and disregard
    somewhat random fluctuations in price paid.
  prefs: []
  type: TYPE_NORMAL
- en: Data and Packages
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The [GitHub folder for this chapter](https://oreil.ly/BehavioralDataAnalysisCh8)
    contains two CSV files with the variables listed in [Table 8-1](#variables_in_our_data).
    In the table, the check mark (✓) indicates the variables present in that file,
    while the cross (☓) indicates the variables missing.
  prefs: []
  type: TYPE_NORMAL
- en: Table 8-1\. Variables in our data
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Variable description | chap8-historical_data.csv | chap8-experimental_data.csv
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| *Gender* | Categorical, “male”/ “female” | ✓ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| *Period* | Month index, 1-32 in historical data, 33 in experimental data
    | ✓ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| *Seasonality* | Annual seasonality, between 0 and 1 | ✓ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| *Month* | Month of year, 1-12 | ✓ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| *Booked* | Binary 0/1, target variable | ✓ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| *Oneclick* | Binary 0/1, experimental treatment | ☓ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: 'In this chapter, we’ll use the following packages in addition to the standard
    ones called out in the Preface:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Determining Random Assignment and Sample Size/Power
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once you’ve built and validated the theory of change of your experiment, the
    next step is to determine how you’ll do the random assignment and how big a sample
    size you’ll need.
  prefs: []
  type: TYPE_NORMAL
- en: In my experience, this is often a big step the first time you’re running an
    experiment in a certain environment. Taking a serious look at your historical
    data often yields surprising insights that can reshape an experiment. In addition,
    depending on the noise in your data and the expected impact size (small if you’ve
    done your job correctly of defining a narrow scope for your experiment), discovering
    how large a sample you’ll need can be humbling. I still remember the first time
    the numbers came back and I was told that we would need to run an experiment for
    almost a year.
  prefs: []
  type: TYPE_NORMAL
- en: Random Assignment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The theory of random assignment could not be simpler: whenever a customer reaches
    the relevant page, they should be shown the current version of the page (called
    “control” in experimental jargon) with a certain probability and the version with
    the 1-click booking button (the “treatment”) with the opposite probability.'
  prefs: []
  type: TYPE_NORMAL
- en: The most straightforward option is a 50%-50% allocation until you’ve reached
    your target sample size, but you may want to use a different split if you have
    a very high volume of transactions. Let’s imagine for example that you’re managing
    a website with 100 million visits a day, and you have determined that your necessary
    sample size is 2 million. You could simply go with 50%-50%, and be done with your
    experiment in about 30 minutes. However, if anything goes wrong with your treatment
    (e.g., a bug crashes the website, admittedly an extreme case), you’ll have 1 million
    unhappy customers on your hands before you know it. In addition, maybe customers
    during these 30 minutes are not representative of your full customer base (e.g.,
    China is asleep at that time and you get mostly American visitors or vice versa).
    In such a situation, it would be better to get the 1 million visits you want in
    your treatment group over the course of a more representative period, such as
    a week or a month (you don’t have to worry about the control group being bigger
    than 1 million). For a 100 million visits/day website, that would translate into
    splits of respectively 99.86%-0.14% (because 1 / (7 * 100) = 0.14%) and 99.97%-0.03%
    (because 1 / (30 * 100) = 0.03%). For the sake of simplicity, I’ll assume a 50%-50%
    split in the rest of the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Code implementation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'From a coding perspective, assuming that you are not using a software that
    takes care of it for you, this can easily be implemented in R or Python:'
  prefs: []
  type: TYPE_NORMAL
- en: Whenever a new customer reaches the relevant page, we generate a random number
    between 0 and 1.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Then we assign the customer a group based on that random number: if K is the
    number of groups we want (including a control group), then all individuals with
    a random number less than 1/K are assigned to the first group; all individuals
    with a random number between 1/K and 2/K are assigned to the second group, and
    so on.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Here, K is equal to 2, which translates into a very simple formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Pitfalls of random assignment
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There are, however, a certain number of subtleties that can trip novice experimenters.
    I’ll cover two in this chapter: the timing and the level of the assignment.'
  prefs: []
  type: TYPE_NORMAL
- en: Random assignment timing
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The first one is determining the right point in the process for random assignment.
    Let’s say that whenever a customer gets on the first page of the website, you
    assign them to either the control or the treatment group. Many of these customers
    will never reach the point of making a booking and will therefore not see your
    booking interface. This would dramatically reduce the effectiveness of your experiment
    because you would in effect experiment only on a fraction of your sample.
  prefs: []
  type: TYPE_NORMAL
- en: When determining which customers should be part of the experiment and when they
    should be assigned to an experimental group, you should reflect on how the treatment
    will be implemented if the experiment is a success. Your experimental design should
    include the same people who would see the treatment if it got implemented in business
    as usual and only them. For instance, visitors who leave the website before booking
    will still not see the button, whereas any future promotion or change to the booking
    page would be built in addition so to speak “on top of” the button, which would
    always be present. Therefore, nonbooking visitors should be excluded but customers
    with a promotion should be included.
  prefs: []
  type: TYPE_NORMAL
- en: Random assignment level
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The second challenge is making sure that the random assignment is happening
    at the “right” behavioral level. I’ll explain what this means with an example.
    Let’s say that a visitor comes on the AirCnC website and starts a booking but
    then for whatever reason leaves the website (they got disconnected, it’s time
    for dinner, etc.) and comes back to it later. Should they see the same booking
    page? If they were offered 1-click booking the first time, should they still be
    offered it the second time?
  prefs: []
  type: TYPE_NORMAL
- en: The problem here is that there are really multiple levels that could potentially
    make sense. You could assign control or treatment at the level of a single website
    visit, of a booking, however many visits it takes, or at the level of a customer
    account (which may or may not be the same person if several people in a household
    use the same account). Unfortunately, there are no hard rules here, the right
    approach must be determined on a case-by-case basis by thinking about the conclusions
    you want to draw and what a permanent implementation would look like.
  prefs: []
  type: TYPE_NORMAL
- en: 'In many cases, it makes sense to do an assignment at the closest level you
    can to a human being: customer account if you can’t distinguish people in a household,
    or individual customer if they each have a subaccount, as is the case for example
    with Netflix. Human beings have persistent memories and alternating options for
    the same person can get confusing. Here, this would mean that our AirCnC customer
    should see the 1-click button for the whole duration of the experiment, regardless
    of how many visits and bookings they make during that time. Unfortunately, this
    means that you can’t just roll the dice metaphorically each time someone starts
    a booking on the website to determine their assignment; you need to keep track
    of whether they have been assigned in the past and if so to which group. For a
    website experiment, this can be done through cookies (assuming the customer allows
    them!).'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The level at which you’re making your random assignment should also be the level
    at which you calculate your sample size. If you make assignments at the customer
    level and customers make an average of three visits per month, you’ll need a three
    times longer experiment than if you make assignments at the visit level. But the
    level you choose for your random assignment should determine your sample size,
    not the other way around!
  prefs: []
  type: TYPE_NORMAL
- en: Whatever level you choose, you’ll have to keep track of the assignment(s) to
    be able to link them to business outcomes later. This is why the best-in-class
    approach is to use centralized systems that record all assignments and connect
    them to customer IDs in a database, so that they can serve a consistent experience
    to customers over time.
  prefs: []
  type: TYPE_NORMAL
- en: More broadly, what these two challenges point at is that the implementation
    of a business experiment is almost always a complex technical affair. A variety
    of vendors now offer somewhat plug-and-play solutions that hide the complexity
    under the hood, especially for website experimentation. Whether you rely on them
    or on your internal tech people, you’ll need to understand how they’re doing the
    random assignment to make sure that you’re getting the experiment you want.
  prefs: []
  type: TYPE_NORMAL
- en: A good way to check that the system works correctly is to start with an A/A
    test, where there is a random assignment but the two groups see the same version
    of the page. This will allow you to check that there are indeed the same number
    of people in the two groups and that they don’t differ in any significant way.
  prefs: []
  type: TYPE_NORMAL
- en: Sample Size and Power Analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Once we know what we’re going to test and how, we need to determine our sample
    size. In some cases, as here with our 1-click booking experiment, we can choose
    our sample size: we can just decide how long we want to run the experiment. In
    other cases, our sample size might be defined for us, or at least its maximum.
    If we’re going to run a test across our whole customer or employee population,
    we can’t increase that population just for the sake of experimentation!'
  prefs: []
  type: TYPE_NORMAL
- en: Regardless of the situation we’re in, we will look at our sample size in relation
    to other experimental variables, such as statistical significance, and not in
    a vacuum. Understanding how these variables are related is crucial to ensure that
    our experimental results are usable and that we’re drawing the right conclusions
    from them. Unfortunately, these are highly complex and nuanced statistical concepts
    and they have not been developed for business decisions.
  prefs: []
  type: TYPE_NORMAL
- en: In line with the spirit of this book, I’ll do my best to explain these statistical
    concepts and conventions in the context of business decisions. I’ll then share
    my reservations about the traditional conventions and offer my two cents regarding
    how to tweak them while remaining in the traditional framework. And finally, I’ll
    describe an alternative approach that has been slowly gaining momentum and which
    I think is superior, namely using computer simulations.
  prefs: []
  type: TYPE_NORMAL
- en: A little bit of statistics theory without math
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'When running an experiment such as the “1-click booking” button, our goal is
    to make the right decision: should we implement it or not? Unfortunately, even
    after running an experiment (or a hundred), we can never be 100% sure that we’re
    making the right decision, because we have only partial information. Certainly,
    if we ran an experiment for years on end, we might reach a point where there is
    only one chance in a million that we’re wrong, but never exactly zero. Moreover,
    we generally don’t want to run an experiment for years on end, when we could be
    running other experiments instead! Therefore, there’s a trade-off between the
    sample size of an experiment and our degree of certainty.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Because we can never know after the fact whether a specific decision was right
    or not, our approach will be to try to select a good sample size and a good decision
    rule before the experiment. What does “good” mean here? Well, the very best possible
    sample size and rule would be those that maximize our expected profit over time.
    The corresponding calculations are doable but require advanced methods that are
    beyond the scope of this book.^([3](ch08.xhtml#ch01fn15)) Instead, we’ll rely
    on the following measures:'
  prefs: []
  type: TYPE_NORMAL
- en: Assuming that our 1-click button does increase our booking rate, what is the
    probability that we’ll rightly implement the button? This is called the “true
    positive” probability. On the other hand, if there is a positive effect and we
    wrongly conclude that there is none, it’s called a “false negative.”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Assuming that our 1-click button has no discernible effect (or, God forbid,
    a negative effect!) on our booking rate, what is the probability that we’ll wrongly
    implement the button? This probability is called the “false positive” probability.
    On the other hand, if there is no effect and we rightly conclude that there is
    no effect, it’s called a “true negative.”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These various configurations are summarized in [Table 8-2](#making_the_right_decision_in_the_right).
  prefs: []
  type: TYPE_NORMAL
- en: Table 8-2\. Making the right decision in the right situation
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | Do we implement the 1-click booking button? |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | **YES** | **NO** |'
  prefs: []
  type: TYPE_TB
- en: '| **Does the 1-click booking button increase the booking rate?** | **YES**
    | True positive | False negative |'
  prefs: []
  type: TYPE_TB
- en: '|  | **NO** | False positive | True negative |'
  prefs: []
  type: TYPE_TB
- en: 'We would want our true positive and our true negative rates to be as high as
    possible, and our false positive and false negative rates to be as low as possible.
    However, the simplicity of this table is deceptive, and it actually encompasses
    an infinite number of situations: when we say that the button increases the booking
    rate, it could mean that the increase is 1%, 2%, etc. On the other hand, when
    we say that the button does not increase the booking rate, it could mean that
    it has exactly zero effect, or that it is decreasing the booking rate by 1%, 2%,
    etc. All these effect sizes would have to be factored in to calculate the overall
    true positive and true negative rates, which would be too complicated. Instead,
    we will rely on two threshold values.'
  prefs: []
  type: TYPE_NORMAL
- en: The first one is an impact of exactly zero for all subjects, also called the
    “sharp null hypothesis” (the nonsharp null hypothesis would be an *average* zero
    effect across subjects). The false positive rate for this value is called the
    statistical significance of our experiment. Because a negative impact would be
    easier to catch than a null effect, the false positive rate for any negative value
    will be at least as large as the statistical significance, and larger negative
    effects will have higher false positive rates. The most common convention in academic
    research is to set the statistical significance at 5%, although in certain fields
    such as [particle physics](https://oreil.ly/U48vk), it can sometimes be as low
    as 0.00005%.
  prefs: []
  type: TYPE_NORMAL
- en: 'The second threshold value is set at some positive effect that we’re interested
    in measuring. For example, we might say that we want to choose a sample size so
    that we can be “reasonably sure” that we will capture a 1% increase in booking
    rate, but we’re OK with missing smaller effects than that. This value is often
    called the “alternative hypothesis,” and the true positive rate for this value
    is called the statistical power of our experiment. Because larger effects would
    be easier to catch, the true positive rate for any larger value will be at least
    as large as the statistical power, and larger positive effects will have higher
    true positive rates. “Reasonably sure” is traditionally taken to mean 80%. To
    reiterate, this doesn’t mean that your experiment “has a power of 80%” and that
    phrase is actually meaningless by itself: the experiment also has a power of 90%
    for a certain larger effect size, and a power of 70% for a certain smaller effect
    size, and so on.'
  prefs: []
  type: TYPE_NORMAL
- en: Our table updated according to the traditional convention would therefore look
    like [Table 8-3](#the_thresholds_used_in_the_traditional).
  prefs: []
  type: TYPE_NORMAL
- en: Table 8-3\. The thresholds used in the traditional statistical approach
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Do we implement the 1-click booking button? |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '|  | **YES** | **NO** |'
  prefs: []
  type: TYPE_TB
- en: '| The 1-click booking button increases the booking rate by more than 1%. |
    > 80% (larger for larger effect sizes) | < 20% (smaller for larger effect sizes)
    |'
  prefs: []
  type: TYPE_TB
- en: '| **The 1-click booking button increases the booking rate by exactly 1%.**
    | **80% (statistical power)** | **20% (1 minus statistical power)** |'
  prefs: []
  type: TYPE_TB
- en: '| The 1-click booking button increases the booking rate by less than 1%. |
    < 80% (larger for larger effect sizes) | > 20% (smaller for larger effect sizes)
    |'
  prefs: []
  type: TYPE_TB
- en: '| **The 1-click booking button has exactly no impact on the booking rate.**
    | **5% (statistical significance)** | **95% (1 minus statistical significance)**
    |'
  prefs: []
  type: TYPE_TB
- en: '| The 1-click booking button strictly decreases the booking rate. | < 5% (smaller
    for larger negative effect sizes) | > 95% (larger for larger negative effect sizes)
    |'
  prefs: []
  type: TYPE_TB
- en: I am no big fan of using an arbitrary number purely because it’s conventional,
    and you should feel free to adjust the “80% power” convention to fit your needs.
    Using a power of 80% for your relevant threshold effect size would mean that if
    the intervention had exactly that effect size, on average, you would have a 20%
    chance of not implementing the intervention because you wrongly got a negative
    result. For big and costly interventions that are hard to test, my opinion is
    that it’s too low and I would personally target 90% power. On the other hand,
    the higher the power you want, the larger your sample size will need to be. You
    may not want to spend half a year getting absolutely certain of the value of the
    1-click button if in that time your competitor has completely revamped their website
    twice and is eating your lunch.
  prefs: []
  type: TYPE_NORMAL
- en: 'In my personal experience, one key but often ignored consideration for power
    analysis and sample size determination in the real world is organizational testing
    velocity: how many experiments can you run in a year? In many companies, that
    number is constrained by someone’s time (either the analyst’s or the business
    partner’s), by the company’s planning cycle, by budget limits, etc., but not by
    the number of customers available. If you can realistically hope to plan, test,
    and implement only one intervention per year, do you really want to run a three-month
    experiment and then do nothing for the rest of the year? On the other hand, if
    you can run one experiment a week, do you really want to spend three months getting
    certain of a positive but mediocre impact instead of taking 12 chances at a big
    one? Therefore, after doing the math, you should always do a sanity check of your
    experiment duration based on your testing velocity and adjust it appropriately.'
  prefs: []
  type: TYPE_NORMAL
- en: Regarding statistical significance, the conventional approach introduces an
    asymmetry between the control and the treatment with a statistical significance
    threshold of 95%. The bar of evidence the treatment has to pass to get implemented
    is much higher than for the control, which is implemented by default. Let’s say
    that you’re setting up a new marketing email campaign and you have two options
    to test. Why should one version be given the benefit of the doubt over the other?
    On the other hand, if you have a campaign that has been running for years and
    for which you have run hundreds of tests, the current version is probably extremely
    good and a 5% chance of wrongly abandoning it might be too high; the right threshold
    here might be 99% instead of 95%. More broadly, relying on a conventional value
    that is the same for all experiments feels to me like a missed opportunity to
    reflect on the respective costs of false positives and false negatives. In the
    case of the 1-click button, which is easily reversible and has minimal costs of
    implementation, I would target a statistical significance threshold of 90% as
    well.
  prefs: []
  type: TYPE_NORMAL
- en: 'To recap, from a statistical perspective, our experiment can be summarized
    by four values:'
  prefs: []
  type: TYPE_NORMAL
- en: The statistical significance, often represented by the Greek letter beta (β)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The effect size chosen for the alternative hypothesis, a.k.a. the minimal detectable
    effect
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The statistical power, often represented as 1 − α where α is the false negative
    rate for the chosen alternative effect size
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The sample size of our experiment, represented by N
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These four variables are referred to as the B.E.A.N. (beta, effect size, alpha,
    sample size N), and determining them for an experiment is called a “power analysis.”^([4](ch08.xhtml#ch01fn16))
    For our 1-click button experiment, we have decided on the first three of them
    and we only have to determine the sample size. We’ll see next how to do it with
    traditional statistical formulas and then how to do it with computer simulations.
  prefs: []
  type: TYPE_NORMAL
- en: Traditional power analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Statisticians have developed formulas to determine the required sample size
    for certain statistical tests. Given that we’ll rely on regression instead of
    tests, you might wonder why we would want to use these formulas. In my experience,
    these will give you values that are of the same order of magnitude as the “true”
    required sample size. That’s a quick and easy way to get reasonable starting values
    for your simulations if you have no idea whether your sample size should be 100
    or 100,000 (in this particular example, we’ll end up with almost exactly the same
    sample size at the end of our simulations!).
  prefs: []
  type: TYPE_NORMAL
- en: The Test of Proportions is a standard test, and the formula to calculate the
    corresponding sample size is easily available in R and Python. Let’s first look
    at the R formula.
  prefs: []
  type: TYPE_NORMAL
- en: 'With an average booking rate of 18.25% in our historical data, the chosen effect
    size of 1% would translate into an expected booking rate for our treatment group
    of 19.25%. For the standard values of the parameters—statistical significance
    = 0.05 and power = 0.8—the corresponding formula in R would be:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The syntax of all the functions for power analysis in the `pwr` package is
    the same, with the exception of the notation for the effect size, which changes
    from one formula to another:'
  prefs: []
  type: TYPE_NORMAL
- en: '`h` is the effect size, based on the increase in probability we want to be
    able to observe over the baseline probability.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`n` is the sample size for each group.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sig.level` is the statistical significance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`power` is the statistical power, equal to 1 − α.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When entering the formula, you should enter the values for three of these variables
    and set the remaining one to NULL. In the preceding formula, we’re calculating
    the sample size, so we set `n` = NULL.
  prefs: []
  type: TYPE_NORMAL
- en: Note that for a test of two proportions, the effect size for statistical purposes
    depends on the baseline rate; an increase of 5% from a baseline of 10% or 90%
    is more “important” than from a baseline of 50%. Fortunately, the package `pwr`
    provides the `ES.h``()` function, which translates the expected probability and
    the baseline probability into the right effect size for the formula.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note also the parameter at the end of the formula: `alternative` indicates
    whether you want to run a one-sided (`greater` or `less`) or a two-sided (`two.sided`)
    test. As long as our treatment doesn’t increase our booking rate, we don’t really
    care whether it has the same booking rate or a lower booking rate compared to
    our control; either way, we won’t implement it. This implies that we can run a
    one-sided test instead of a two-sided test by setting `alternative = ''greater''`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The code for Python is similar, using the `proportion_effectsize()` function
    from the package `statsmodels.stats.proportion`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The sample size returned by the formula is 18,800 per group (plus or minus some
    minor variation between R and Python), i.e., 37,600 total, which means we can
    achieve the necessary sample size in a bit less than four months. That was easy!
    Using a statistical significance of 0.1 and a power of 0.9 would yield a sample
    size of 20,000 per group, just a bit longer.
  prefs: []
  type: TYPE_NORMAL
- en: 'What does a total sample size of 40,000 for a statistical significance of 0.1
    and a power of 0.9 mean in terms of the decision model I outlined in the previous
    section? Imagine the following:'
  prefs: []
  type: TYPE_NORMAL
- en: You run a very large number of experiments with a total sample size of 40,000,
    as described.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Your decision rule in each case is that you will implement the 1-click button
    if the statistics from the test of proportions has a p-value lower than 0.1.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In all of these experiments, the true effect size is 1%.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then you will find a significant positive result and implement the 1-click button
    in 90% (i.e., 0.9) of these experiments; in the remaining 10% of these experiments,
    you’ll get a null result and wrongly reject implementing the 1-click button.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are some equivalent formulas for regression, but only for the simplest
    cases, and I find that even in those situations, their complexity vastly outweighs
    their usefulness. Nonetheless, as a conceptual step toward our simulation approach,
    let’s review what the traditional statistical approach would look like in terms
    of the decision model with regression. Let’s run a logistic regression on some
    mock-up data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The traditional decision rule would be to consider the impact of the 1-click
    button to be significant and implement it if the corresponding coefficient (here
    approximately 0.0475) had a p-value less than 0.1\. Because it’s approximately
    0.28 with this mock-up data, we would consider the effect not significant and
    decline to implement the button (the actual numbers for you will vary randomly
    depending on your simulation).
  prefs: []
  type: TYPE_NORMAL
- en: Determining the sample size for our analysis based on this approach would entail
    determining the sample size such that in 90% of a large number of experiments
    where the true effect is 1%, we would get a p-value for the regression coefficient
    less than 0.1\. But as I described in [Chapter 7](ch07.xhtml#measuring_uncertainty_with_the_bootstra),
    this implicitly makes statistical assumptions about our data being normally distributed,
    which can be problematic, so we’ll use Bootstrap simulations instead as we’ll
    now see.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Using the sample size formula for the test of proportions can still be useful
    as a quick and fast first step, because its result should be on the same order
    of magnitude as the final sample size you’ll need. A total sample size of 40,000
    for the test of proportions means that unless your other predictors have a crazy
    high predictive power, the order of magnitude for your required sample size is
    going to be 10,000, and not 1,000 or 100,000 (i.e., your sample size will have
    five figures). We will start our simulations with a tentative sample size of 20,000,
    and based on how much effective power it gives us, we’ll adjust that number upward
    or downward.
  prefs: []
  type: TYPE_NORMAL
- en: 'Power analysis without statistics: Bootstrap simulations'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The traditional statistical analysis made perfect sense when data was limited
    and calculations were done painstakingly by hand. I strongly believe that it has
    now outlived its usefulness: Bootstrap simulations offer an alternative that better
    reflects the realities and needs of applied data analysis. How wrong an experiment
    can get (e.g., saying that the treatment is 1% better than the control when in
    reality it’s 10% worse) is often a bigger concern for business partners than how
    likely it is that the difference is zero.^([5](ch08.xhtml#ch01fn17))'
  prefs: []
  type: TYPE_NORMAL
- en: Connecting simulations and statistical theory
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'When using Bootstrap simulations, our decision rule doesn’t rely on p-values.
    Instead, we implement the treatment if the Bootstrap confidence interval for the
    coefficient of interest is above a certain threshold, usually zero. If the assumptions
    of statistical power analysis are verified, Bootstrap simulations yield results
    that are very similar and intuitively connected:'
  prefs: []
  type: TYPE_NORMAL
- en: Under the sharp null hypothesis of no effect, we expect that a 90%-CI will include
    zero 90% of the time, an 80%-CI will include zero 80% of the time, and so on.
    This property, called the *coverage* of the CIs, implies that the percentage we
    use to define our CI is equivalent to statistical significance, i.e., a 90%-CI
    will have an approximately 5% false positive rate in each direction. In 5% of
    cases we’ll observe a CI that is strictly negative and in 5% of cases a CI that
    is strictly positive.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Given an alternative hypothesis, i.e., a target effect size, we can define our
    power as the percentage of simulations that yield a true positive. For example,
    if we set the effect of the 1-click button at 1%, simulate a large number of experiments,
    and observe that 75% of our Bootstrap CIs are strictly positive, then our power
    is 75%.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As we’ll see in the next chapter, if the assumptions of traditional statistical
    power analysis are *not* verified, the coverage of the Bootstrap CI may vary.
    That is, a 90%-CI may include zero more or less than 90% of the time. This effective
    coverage represents the real risk of false positives that needs to be set to the
    desired level of significance. This is just a heads-up; we’ll get into more details
    in [Chapter 9](ch09.xhtml#stratified_randomizatio).
  prefs: []
  type: TYPE_NORMAL
- en: Simulations offer a very versatile but transparent way of determining the necessary
    sample size for any experiment, however weird the data or complex the business
    decision at hand. These advantages come from making you state explicitly how you’ll
    analyze your data and write the corresponding code before actually running the
    experiment, which provides an additional sanity check and opportunity to make
    adjustments. The counterpart to these benefits is that we’ll have to do more coding
    ourselves instead of relying on off-the-shelf formulas. I’ll attempt to limit
    the complexity of the code by breaking it into intuitive functions.
  prefs: []
  type: TYPE_NORMAL
- en: Writing our analysis code
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Let’s first create a function that will output our metric of interest, namely
    the coefficient for *OneClick* in our logistic regression:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: This is just a functional wrapper for our preceding analysis, and applying that
    function to our mock-up data set would return the same coefficient, approximately
    0.0475.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s then calculate Bootstrap CIs for this metric, reusing the function from
    [Chapter 7](ch07.xhtml#measuring_uncertainty_with_the_bootstra):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, we’ll take as our decision rule that we’ll implement the button
    if and only if the Bootstrap 90%-CI is strictly positive (i.e., it doesn’t include
    zero):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: This is equivalent to the decision rule of implementing the button if and only
    if the p-value is under the 0.10 threshold. You can check by yourself that applying
    this function to our mock-up data set returns 0 as it should.
  prefs: []
  type: TYPE_NORMAL
- en: 'The definition of the power of our experiment for a given effect size and a
    given sample size remains the same: it is the percentage of a large number of
    such experiments for which we would implement the button. Let’s now turn to simulating
    this large number of experiments!'
  prefs: []
  type: TYPE_NORMAL
- en: Power simulation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We’ll then write our function to run a single simulation. The code works as
    follows (the callout numbers apply to both R and Python):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#comarker81)'
  prefs: []
  type: TYPE_NORMAL
- en: Add the predicted probability of booking to the data.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#comarker82)'
  prefs: []
  type: TYPE_NORMAL
- en: Filter down to the desired sample size.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](Images/3.png)](#comarker83)'
  prefs: []
  type: TYPE_NORMAL
- en: Assign experimental groups.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](Images/4.png)](#comarker84)'
  prefs: []
  type: TYPE_NORMAL
- en: Add effect to the treatment group.
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](Images/5.png)](#comarker85)'
  prefs: []
  type: TYPE_NORMAL
- en: Apply the decision function and return its output.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can then write our power function for a certain effect size and sample size.
    This function repeatedly generates experimental data sets and then applies our
    decision function to them; it returns the fraction of them for which we would
    implement the button:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'How many data sets should you simulate? Twenty is a good starting point; it
    will give you a noisy estimate, but if you get a power of 0 or 1, you’ll know
    you have to adjust your sample size:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: This first estimate is 90% power; like I said, traditional formulas give you
    a reasonable ballpark to start your simulations. I then run the power simulation
    function with 30,000 and 50,000 rows for 100 simulations each, and finally 35,000
    and 45,000 rows for 200 simulations each. Basically, as you get a narrower and
    narrower interval of sample sizes, you want to improve accuracy by increasing
    the number of simulations. [Figure 8-6](#power_simulations_for_various_sample_si)
    shows the results of my successive iterations.
  prefs: []
  type: TYPE_NORMAL
- en: '![Power simulations for various sample sizes](Images/BEDA_0806.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-6\. Power simulations for various sample sizes
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As announced earlier, we get a power of 0.9 at about 40,000\. We could keep
    running simulations if we needed to get more precise (e.g., should we get a sample
    size of 38,000? 41,000?) but that’s good enough for this example.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve determined our sample size, the last thing I like to do is plot
    the power curve for a few effect sizes at that sample size. This gives us a better
    sense of how likely overall we are to get a positive result, assuming the actual
    effect size is positive. It also allows you to better convey to your business
    partners that the power of your experiment is not just defined for one effect
    size. Here, we can see how the power of the experiment increases going from a
    0.5% effect to a 2% effect ([Figure 8-7](#power_simulations_for_various_effect_si)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Power simulations for various effect sizes at N = 40,000, with 200 simulations
    per effect size, dashed line at power = 0.9](Images/BEDA_0807.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-7\. Power simulations for various effect sizes at N = 40,000, with
    200 simulations per effect size, dashed line at power = 0.9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: With 200 simulations per effect size, the estimated values for power should
    be pretty accurate, although still not perfect, as illustrated by the lack of
    smoothness of the curve. In other words, the fact that we see a power of 1 for
    an effect size of 2% doesn’t mean that we literally have 100% power for that effect
    size, but very close to it.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Reminder: the simulated statistical significance of a Bootstrap CI should be
    pretty close to a normal CI if your variables are reasonably smoothly and normally
    distributed. For weirder data (multiple peaks, fat tails, etc.), this may not
    hold anymore, and you should definitely check that your simulated statistical
    significance is not widely off.'
  prefs: []
  type: TYPE_NORMAL
- en: If you’d like, you can also run a simulation with an effect size of zero. This
    will give you the empirical statistical significance of your analysis. Because
    we’re using Bootstrap 90%-CIs, about 5% of these simulations should end up with
    the decision to (wrongly) implement the 1-click button, and this is what we observe
    here.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: These are not big data simulations by any means, but they take long enough (the
    longest one took about half an hour on my laptop) that you’ll want to improve
    the performance of your functions, get your code running while you do something
    else, or both. The [code on GitHub](https://oreil.ly/BehavioralDataAnalysis) contains
    functions that I have optimized using the `Rfast` and `doParallel` packages in
    R and the `joblib` and `psutil` packages in Python.
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing and Interpreting Experimental Results
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'After running the experiment and collecting the corresponding data, you can
    analyze them. After all the simulated analyses you ran for the power estimation,
    the final analyses themselves should be a walk in the park. We run a logistic
    regression and determine the corresponding Bootstrap 90%-CI. Because of the random
    assignment, we know that our coefficient for the 1-click button is unconfounded—we
    don’t need to control for any confounder. However, by adding other variables that
    are also causes of the booking probability, we can reduce noise and significantly
    improve the precision of our estimation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The coefficient for the 1-click button is equal to 0.15784, and the Bootstrap
    90%-CI for it is approximately [0.073; 0.250]. Based on our decision rule, we
    would go ahead and implement the 1-click button.
  prefs: []
  type: TYPE_NORMAL
- en: 'The coefficients from a logistic regression are not directly interpretable,
    and I find that the recommended solution of using the odds ratio only helps marginally
    (in particular when you have moderation effects). My preferred rule of thumb is
    to generate two copies of the experimental data, one where the variable for the
    1-click button is set to 1 for everyone, and the other where it is set to 0\.
    By comparing the probability of booking predicted by our logistic model for these
    two data sets, we can calculate an “average” effect that is very close to the
    effect you would observe if implementing the treatment for everyone. It’s unscientific
    but useful:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#comarker881)'
  prefs: []
  type: TYPE_NORMAL
- en: We create a data set called `no_button` for which we set the variable `oneclick`
    to zero for all rows (and convert it to factor so that the predict function later
    will work).
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#comarker882)'
  prefs: []
  type: TYPE_NORMAL
- en: We create a data set called `button` for which we set the variable `oneclick`
    to one for all rows.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](Images/3.png)](#comarker883)'
  prefs: []
  type: TYPE_NORMAL
- en: We calculate the predicted probability of booking in each case by using the
    `predict()` function with our model `log_mod_exp`.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](Images/4.png)](#comarker884)'
  prefs: []
  type: TYPE_NORMAL
- en: We calculate the difference between the predicted probabilities.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see that our average effect across our experimental population is about
    0.712pp, positive but lower than our target of 1pp. As usual, let’s build the
    Bootstrap 90%-CI, which is approximately [0.705pp; 0.721pp]. This interval is
    very narrow and does not cross zero. Therefore, we can treat our result as empirically
    statistically significant at the 5% level. In this case, we can even get much
    more confident than that: the 99.8%-CI is approximately [0.697pp; 0.728pp], still
    far from zero, so we can treat our result as significant at the (1 − 0.998) /
    2 = 0.1% level.'
  prefs: []
  type: TYPE_NORMAL
- en: To cover all cases, let’s recap our decision rule ([Table 8-4](#decision_rule_for_one_click_booking_but)).
    This way, you’ll see what to do depending on whether the observed estimated effect
    is statistically significant or not and economically significant (taken here to
    mean a 1pp increase) or not. In the present case, I would implement the button,
    given that it has a strictly positive effect, and the cost of implementation is
    low.
  prefs: []
  type: TYPE_NORMAL
- en: Table 8-4\. Decision rule for 1-click booking button
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | Observed estimated effect |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | **estimated effect <= 0** | **0 < estimated effect < 1pp** | **1pp
    <= estimated effect** |'
  prefs: []
  type: TYPE_TB
- en: '| **Empirical statistical significance of observed results** | **“High” (the
    Bootstrap CI for 90% or higher doesn’t cross 0)** | Don’t implement button | Implement
    button or not, depending on estimated effect size, costs, and risk appetite **(our
    case here)** | Implement button |'
  prefs: []
  type: TYPE_TB
- en: '|  | **“Low” (the Bootstrap CI for 90% crosses 0)** | Don’t implement button
    | Don’t implement button | Implement button or run new test, depending on confidence
    interval and risk appetite |'
  prefs: []
  type: TYPE_TB
- en: 'The last thing I will note is that the average effect across our experimental
    population, 0.712pp, is pretty far from the straightforward difference between
    our control group and treatment group, which is about 0.337pp. This is due to
    random differences between our two experimental groups. The average age in our
    control group is 40.63 years versus 40.78 in the treatment group. The proportion
    of men is also a bit higher in the treatment group. With very small effect sizes,
    these minute differences are enough to muddy a direct comparison of the two groups:
    our sample size is large enough that our two groups are identical within about
    0.3pp, which is pretty close in absolute terms, but that’s about half of our experimental
    effect.'
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, there’s nothing we can do about that in this experiment, where
    customers are allocated randomly to the two groups as they come. But if we know
    our entire experimental sample at the beginning of the experiment, we can do significantly
    better by ensuring that the control group and the experimental group are as identical
    as possible through stratified randomization, as we’ll see in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we saw how to design the simplest form of experiment, an online
    A/B test with straightforward randomization. I emphasized that a well-designed
    experiment is much more than just throwing out random different versions of a
    website or an email to customers. You need to determine your business goal and
    target metric, and then articulate how your intervention is connected to them
    through behavioral logic. Taken together, your business goal, target metric, intervention,
    and behavioral logic constitute the theory of change of your experiment.
  prefs: []
  type: TYPE_NORMAL
- en: We then turned to the quantitative aspects of experimental design. In this first
    chapter on experimentation, the random assignment was extremely simple, and I
    spent more time on the power analysis and sample size calculations. While there
    are statistical formulas available, I prefer to use regressions rather than statistical
    tests as analysis tools, and Bootstrap confidence intervals rather than p-values
    as a measure of the uncertainty around our estimated coefficient, which leads
    to using power simulations instead of formulas. In this case, the results of the
    two are almost identical, but in the next two chapters we’ll get into more complex
    designs where there are no formulas available.
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch08.xhtml#ch01fn13-marker)) This is why pharmaceutical trials, as well
    as an increasing number of social science experiments, are preregistered. You
    can’t set out to test a drug for a heart condition and then decide after the fact
    that it is an effective drug against hair loss because the patients in the treatment
    group have seen their hairline move forward; you’d need to run a second experiment,
    itself preregistered, for the purpose of testing the new hypothesized effect.
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch08.xhtml#ch01fn14-marker)) Wendel (2020) is a great resource to understand
    the obstacles and drivers of a behavior and build a strong behavioral logic.
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](ch08.xhtml#ch01fn15-marker)) In case you’re wondering, you would need
    to use Bayesian methods. Maybe I’ll get to that in the next edition of this book!
    In the meantime, [*Think Bayes*](https://www.oreilly.com/library/view/think-bayes/9781491945407/)
    (O’Reilly) by Allen Downey is one of the most approachable introductions I know
    to the topic.
  prefs: []
  type: TYPE_NORMAL
- en: ^([4](ch08.xhtml#ch01fn16-marker)) See Aberson (2019).
  prefs: []
  type: TYPE_NORMAL
- en: ^([5](ch08.xhtml#ch01fn17-marker)) Hubbard (2010) is a good resource if you
    want to think more about how to design useful measurements in business, even with
    very limited information.
  prefs: []
  type: TYPE_NORMAL
