<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <title>chapter-10</title>
  <link href="../../stylesheet.css" rel="stylesheet" type="text/css"/>
 </head>
 <body>
  <div class="readable-text " id="p1"> 
   <h1 class=" readable-text-h1"><span class="chapter-title-numbering"><span class="num-string">10</span></span> A customer support chatbot with LangGraph and Streamlit</h1>
  </div>
  <div class="introduction-summary"> 
   <h3>This chapter covers</h3>
   <ul> 
    <li class="readable-text" id="p2">Developing a chatbot frontend with Streamlit's chat elements</li>
    <li class="readable-text" id="p3">Using LangGraph and LangChain to streamline an advanced AI app</li>
    <li class="readable-text" id="p4">How embeddings and vector databases work</li>
    <li class="readable-text" id="p5">Augmenting an LLM's pre-trained knowledge using Retrieval Augmented Generation (RAG)</li>
    <li class="readable-text" id="p6">Enabling an LLM to access and execute real-world actions</li>
   </ul>
  </div>
  <div class="readable-text " id="p7"> 
   <p>Creating a fun and engaging experience—like the trivia game we built in chapter nine—is exciting, but the true power of AI lies in its ability to drive real business value. AI isn't just about answering questions or generating text; it's about transforming industries, streamlining operations, and enabling entirely new business models.</p>
  </div>
  <div class="readable-text  intended-text" id="p8"> 
   <p>However, building AI applications that deliver economic value requires more than just calling a pre-trained model. For AI to be useful in real-world scenarios, it needs to be aware of the context in which it operates, connect to external data sources, and take meaningful actions. Companies need AI to understand and respond to domain-specific queries, interact with business systems, and provide personalized assistance.</p>
  </div>
  <div class="readable-text  intended-text" id="p9"> 
   <p>In this chapter, we'll build such an application: a customer service chatbot that will retrieve real company data, help customers track and cancel orders, and intelligently decide when to escalate issues to a human agent. By the end of this chapter, you'll understand how to integrate LLMs with private knowledge bases, implement retrieval-augmented generation (RAG), and enable an AI agent to take action in the real world. Let's dive in.</p>
  </div>
  <div class="callout-container admonition-block"> 
   <div class="readable-text" id="p10"> 
    <h5 class=" callout-container-h5 readable-text-h5">Note</h5>
   </div>
   <div class="readable-text" id="p11"> 
    <p>The GitHub repo for this book is <a href="https://github.com/aneevdavis/streamlit-in-action">https://github.com/aneevdavis/streamlit-in-action</a>. The chapter_10 folder has this chapter's code and a requirements.txt file with exact versions of all the required Python libraries.</p>
   </div>
  </div>
  <div class="readable-text" id="p12"> 
   <h2 class=" readable-text-h2">10.1 Nibby: A customer service bot</h2>
  </div>
  <div class="readable-text " id="p13"> 
   <p>Under the leadership of Note n' Nib's new CEO—renowned for his legendary decision-making prowess, aided by a certain dashboard revered across the company—the brand has flourished into a stationery powerhouse with rocketing sales.</p>
  </div>
  <div class="readable-text  intended-text" id="p14"> 
   <p>But success brings its own challenges. The customer support department is swamped with calls from buyers who are impatient for their orders to arrive or seeking advice about fountain pen maintenance. After a month of complaints about long wait times, the CEO summons the one person known company-wide for reliable innovation.</p>
  </div>
  <div class="readable-text  intended-text" id="p15"> 
   <p>And so it is that <em>you</em> are tasked with solving the support crisis. When you're not delivering seminars on Streamlit, you're reading up on the latest advances in AI; it is not long before an intriguing possibility hits you: might it be possible to <em>automate</em> customer support?</p>
  </div>
  <div class="readable-text  intended-text" id="p16"> 
   <p>Over the course of a sleepless night, you sketch out plans for a Streamlit support bot named Nibby. Whispers of your project spread across the company. "We are saved!" some declare. "Nibby will not fail us!" Skeptics scoff: "'Tis folly! No <em>robot</em> can fix this."</p>
  </div>
  <div class="readable-text  intended-text" id="p17"> 
   <p>Who will prove right? Let's find out.</p>
  </div>
  <div class="readable-text" id="p18"> 
   <h3 class=" readable-text-h3">10.1.1 Stating the concept and requirements</h3>
  </div>
  <div class="readable-text " id="p19"> 
   <p>As always, we start with a distilled one-line description of what we intend to build.</p>
  </div>
  <div class="callout-container admonition-block"> 
   <div class="readable-text" id="p20"> 
    <h5 class=" callout-container-h5 readable-text-h5">Concept</h5>
   </div>
   <div class="readable-text" id="p21"> 
    <p>Nibby, a customer support chatbot that can help Note n' Nib's customers with information and basic service requests</p>
   </div>
  </div>
  <div class="readable-text " id="p22"> 
   <p>"Customer support" obviously spans a lot of territory, so let's define the exact requirements more clearly.</p>
  </div>
  <div class="readable-text sub-sub-section-heading" id="p23"> 
   <h4 class=" readable-text-h4">Requirements</h4>
  </div>
  <div class="readable-text " id="p24"> 
   <p>In our vision of automating customer support, Nibby will be able to:</p>
  </div>
  <ul> 
   <li class="readable-text" id="p25">hold a human-like conversation with a customer</li>
   <li class="readable-text" id="p26">answer relevant questions about Note n' Nib and its products based on a custom knowledge base</li>
   <li class="readable-text buletless-item" id="p27">handle the following requests from the customer:
    <ul> 
     <li class="readable-text" id="p28">tracking an order</li>
     <li class="readable-text" id="p29">canceling an order</li>
    </ul></li>
   <li class="readable-text" id="p30">redirect to a human customer support agent if it cannot fulfill the request on its own</li>
  </ul>
  <div class="readable-text " id="p31"> 
   <p>In essence, we want Nibby to take as much load off Note n' Nib's overworked human support agents as possible. Nibby should act as a "frontline" agent who can take care of most basic requests, such as providing product information or canceling orders, and redirect to a human only when necessary.</p>
  </div>
  <div class="readable-text sub-sub-section-heading" id="p32"> 
   <h4 class=" readable-text-h4">What's out of scope</h4>
  </div>
  <div class="readable-text " id="p33"> 
   <p>To keep this project manageable and small enough to fit into this chapter, we'll decide <em>not </em>to implement the following:</p>
  </div>
  <ul> 
   <li class="readable-text" id="p34">Storing or remembering prior conversations with a user</li>
   <li class="readable-text" id="p35">Any "actions" other than the two provided above, i.e., tracking and canceling orders</li>
   <li class="readable-text" id="p36">Actual working logic for the two actions discussed, e.g., building an order tracking or cancellation system</li>
  </ul>
  <div class="readable-text " id="p37"> 
   <p>From a learning perspective, we truly want to focus on building a relatively complex AI system that can converse with users, understand a custom knowledge base, and take real-world actions.</p>
  </div>
  <div class="readable-text  intended-text" id="p38"> 
   <p>The specific actions we enable the app to perform don't matter. For instance, the fact that our app can cancel an order—as opposed to replacing an item—is not of any particular significance. Indeed, as implied by the third point above, the order cancellation we will implement is dummy "toy" logic. What <em>is </em>significant is that our bot should be able to intelligently choose to run that logic based on the free-form conversation the user is having with it.</p>
  </div>
  <div class="readable-text" id="p39"> 
   <h3 class=" readable-text-h3">10.1.2 Visualizing the user experience</h3>
  </div>
  <div class="readable-text " id="p40"> 
   <p>The user interface for Nibby might be the most straightforward among all the apps in this book. Figure 10.1 shows a sketch of what we'll build.</p>
  </div>
  <div class="browsable-container figure-container" id="p41">  
   <img src="../Images/10__image001.png" alt="image"/> 
   <h5 class=" figure-container-h5">Figure 10.1 UI sketch for Nibby, our customer support chatbot.</h5>
  </div>
  <div class="readable-text " id="p42"> 
   <p>Nibby's UI isn't significantly different from any chat or instant messaging app you may have used—from WhatsApp on your phone to Slack on your corporate laptop. You'll notice a familiar-looking text box at the bottom for users to type messages. Each user message triggers an AI response, which is appended to the conversation view above.</p>
  </div>
  <div class="readable-text" id="p43"> 
   <h3 class=" readable-text-h3">10.1.3 Brainstorming the implementation</h3>
  </div>
  <div class="readable-text " id="p44"> 
   <p>The difficult part of building this app will be the backend—specifically, getting the bot to answer questions correctly and connect to outside tools. Figure 10.2 shows our overall design.</p>
  </div>
  <div class="browsable-container figure-container" id="p45">  
   <img src="../Images/10__image002.png" alt="image"/> 
   <h5 class=" figure-container-h5">Figure 10.2 Overall design for Nibby</h5>
  </div>
  <div class="readable-text " id="p46"> 
   <p>While our trivia app from chapter 9 had an interesting design when it came to state management, its "intelligent" aspect was fairly simple—feed a prompt to an LLM and have it respond.</p>
  </div>
  <div class="readable-text  intended-text" id="p47"> 
   <p>On the other hand, a customer support app with the capabilities we're envisioning has a more involved design in at least two respects:</p>
  </div>
  <ul> 
   <li class="readable-text" id="p48">It needs a way to augment a customer's query with private knowledge about the company that a human agent would possess.</li>
   <li class="readable-text" id="p49">It needs to be able to execute code in the real world.</li>
  </ul>
  <div class="readable-text " id="p50"> 
   <p>Figure 10.2 gives a basic overview of how we achieve these. When a user message comes in through our frontend, we retrieve context relevant to the message from a private knowledge store—a vector database, as we'll see later—and send that to the LLM.</p>
  </div>
  <div class="readable-text  intended-text" id="p51"> 
   <p>We also organize the actions we want our bot to be able to take into so-called <em>tools</em> and make the LLM aware of their existence, what each tool does, and how to call them. For any given input, the LLM can either issue a call to a tool or respond to the user directly.</p>
  </div>
  <div class="readable-text  intended-text" id="p52"> 
   <p>In the former case, we execute the tool as specified by the LLM and send the results for further processing, while in the latter case, we append the response to a conversation view on the frontend and await the user's next message.</p>
  </div>
  <div class="readable-text" id="p53"> 
   <h3 class=" readable-text-h3">10.1.4 Installing dependencies</h3>
  </div>
  <div class="readable-text " id="p54"> 
   <p>We'll be using several Python libraries in this chapter. To get everything ready in advance, install them all at once by running the following command:</p>
  </div>
  <div class="browsable-container listing-container" id="p55"> 
   <div class="code-area-container"> 
    <pre class="code-area">pip install langchain-community langchain-core langchain-openai langchain-pinecone langgraph pinecone toml</pre>
   </div>
  </div>
  <div class="readable-text" id="p56"> 
   <h2 class=" readable-text-h2">10.2 Creating a basic chatbot</h2>
  </div>
  <div class="readable-text " id="p57"> 
   <p>Chapter nine introduced LLMs and demonstrated how to use the OpenAI API for simple applications. While OpenAI's API is easy to integrate, developing more sophisticated AI-driven apps—such as those leveraging Retrieval-Augmented Generation (RAG) or agent-based workflows, which we'll encounter soon—adds complexity.</p>
  </div>
  <div class="readable-text  intended-text" id="p58"> 
   <p>A new ecosystem of libraries and tools has emerged to make creating complex AI apps as easy as possible. In this chapter, we'll explore LangGraph and LangChain, two libraries that work together to smooth the application creation process.</p>
  </div>
  <div class="readable-text" id="p59"> 
   <h3 class=" readable-text-h3">10.2.1 Intro to LangGraph and LangChain</h3>
  </div>
  <div class="readable-text " id="p60"> 
   <p>LLMs have undoubtedly been the most influential technological advance of the last decade. At their core, interacting with an LLM consists of providing a prompt that the LLM can "complete." That's what everything else is built around.</p>
  </div>
  <div class="readable-text  intended-text" id="p61"> 
   <p>Contrast this with the complexities that modern AI applications have to deal with:</p>
  </div>
  <ul> 
   <li class="readable-text" id="p62">Handling multi-step workflows (e.g., retrieving information before responding)</li>
   <li class="readable-text" id="p63">Integrating with external tools</li>
   <li class="readable-text" id="p64">Retaining conversation context across multiple turns</li>
  </ul>
  <div class="readable-text " id="p65"> 
   <p>Managing this complexity manually is difficult, which is where LangChain and LangGraph—both Python libraries—come in. LangChain provides building blocks for working with LLMs, including prompt management, memory, and tool integration. LangGraph—developed by the same company—extends LangChain by structuring AI workflows as <em>graphs</em>, allowing for decision-making, branching logic, and multi-step processing. By combining these, we can design structured, intelligent AI applications that go beyond simple chat responses—enabling Nibby to retrieve knowledge, call APIs, and make decisions dynamically.</p>
  </div>
  <div class="readable-text  intended-text" id="p66"> 
   <p>In the rest of this chapter, we'll use these libraries extensively to achieve the functionality we want.</p>
  </div>
  <div class="callout-container admonition-block"> 
   <div class="readable-text" id="p67"> 
    <h5 class=" callout-container-h5 readable-text-h5">Note</h5>
   </div>
   <div class="readable-text" id="p68"> 
    <p>Since we will model our chatbot as a <em>graph </em>in LangGraph, we'll primarily speak about and refer to LangGraph rather than LangChain. However, you'll notice that many of the underlying classes and functions we'll use are imported from LangChain.</p>
   </div>
  </div>
  <div class="readable-text" id="p69"> 
   <h3 class=" readable-text-h3">10.2.2 Graphs, nodes, edges, and state</h3>
  </div>
  <div class="readable-text " id="p70"> 
   <p>In LangGraph, you construct an AI application by building a <em>graph </em>of <em>nodes</em> that transform the application's <em>state</em>. If you don't have a background in computer science, that statement might trip you up, so let's break it down.</p>
  </div>
  <div class="readable-text sub-sub-section-heading" id="p71"> 
   <h4 class=" readable-text-h4">What is a graph, exactly?</h4>
  </div>
  <div class="readable-text " id="p72"> 
   <p>In graph theory, a graph is a network of interconnected vertices (also called nodes) and edges, which connect the vertices with each other. Software developers often use graphs to create conceptual models of real-world objects and their relationships. For instance, figure 10.3 shows a graph of people you might expect to find on a social media website.</p>
  </div>
  <div class="browsable-container figure-container" id="p73">  
   <img src="../Images/10__image003.png" alt="image"/> 
   <h5 class=" figure-container-h5">Figure 10.3 Using a graph to model friend relationships in a social network</h5>
  </div>
  <div class="readable-text " id="p74"> 
   <p>Here, each person is a vertex or node (shown by a circle), and the "friend"-relationship between any two people is an edge (the lines between the circles).</p>
  </div>
  <div class="readable-text  intended-text" id="p75"> 
   <p>By modeling relationships in this way, the social media website can apply various algorithms developed for graphs to do useful real-world things. For instance, there's an algorithm called breadth-first search (BFS) that finds the shortest path from one node to any other node. In this case, we could use it to find the fewest common friends required to connect two people.</p>
  </div>
  <div class="readable-text  intended-text" id="p76"> 
   <p>In LangGraph, we model an application as a graph of <em>actions</em>, where a <em>node</em> signifies a single action that the application performs. A graph has a <em>state</em>, simply a collection of named attributes with values (similar to Streamit's concept of "session state"). Each node takes the current state of the graph as input, does something to modify the state, and returns the new state as its output.</p>
  </div>
  <div class="browsable-container figure-container" id="p77">  
   <img src="../Images/10__image004.png" alt="image"/> 
   <h5 class=" figure-container-h5">Figure 10.4 In LangGraph, nodes take in the graph state and modify it.</h5>
  </div>
  <div class="readable-text " id="p78"> 
   <p>An edge in the graph represents a connection between two nodes, or in other words, the fact that the output of one node may be the input to another. Unlike in the case of a social media graph where the edges had no direction (i.e., if two people are friends, each is a friend of the other), edges in LangGraph are <em>directed</em> because one node in the edge is executed <em>before </em>the other. Visually, we represent the direction as an arrow on the edge (figure 10.4).</p>
  </div>
  <div class="readable-text  intended-text" id="p79"> 
   <p>The input to the graph is its initial state that is passed to the <em>first</em> node that's executed, while the output is the final state returned by the <em>last</em> node that's executed.</p>
  </div>
  <div class="readable-text  intended-text" id="p80"> 
   <p>That was a fair bit of theory; let's now consider a toy example (figure 10.5) to make this all real.</p>
  </div>
  <div class="browsable-container figure-container" id="p81">  
   <img src="../Images/10__image005.png" alt="image"/> 
   <h5 class=" figure-container-h5">Figure 10.5 A graph in LangGraph that computes the sum of squares of numbers</h5>
  </div>
  <div class="readable-text " id="p82"> 
   <p>The application shown in figure 10.5 is a very simple one. There's no AI involved; it's just a program that takes a list of numbers and returns the sum of their squares—e.g., for the input <kbd>[3, 4]</kbd>, the graph would calculate the output <kbd>25</kbd> (<kbd>3^2 + 4^2 = 25</kbd>)</p>
  </div>
  <div class="readable-text  intended-text" id="p83"> 
   <p>The graph's state contains three values: <kbd>numbers</kbd>, <kbd>squares</kbd>, and <kbd>answer</kbd>. <kbd>numbers</kbd> holds the list of input values (e.g. <kbd>[3, 4]</kbd>), while <kbd>squares</kbd> and <kbd>answer</kbd> don't have a value to start with.</p>
  </div>
  <div class="readable-text  intended-text" id="p84"> 
   <p>Each LangGraph graph has dummy nodes called <kbd>START</kbd> and <kbd>END</kbd>, which represent the start and end of execution. There are two other "real" nodes: <kbd>squaring_node</kbd> and <kbd>summing_node</kbd>.</p>
  </div>
  <div class="readable-text  intended-text" id="p85"> 
   <p>Here's how the graph is executed:</p>
  </div>
  <ul> 
   <li class="readable-text" id="p86">The <kbd>START</kbd> node receives the initial state.</li>
   <li class="readable-text" id="p87">Since there's a directed edge from <kbd>START</kbd> to <kbd>squaring_node</kbd>, <kbd>squaring_node</kbd> is executed first.</li>
   <li class="readable-text" id="p88"><kbd>squaring_node</kbd> takes in the starting state, squares the numbers in the <kbd>numbers</kbd> list and saves the new list (<kbd>[9, 16]</kbd>) under the variable <kbd>squares</kbd> in the state.</li>
   <li class="readable-text" id="p89">As there's an edge from <kbd>squaring_node</kbd> to <kbd>summing_node</kbd>, <kbd>summing_node</kbd> takes as input this modified state returned by <kbd>squaring_node</kbd>.</li>
   <li class="readable-text" id="p90"><kbd>summing_node</kbd> adds up the numbers in <kbd>squares</kbd>, and saves the result as <kbd>answer</kbd>.</li>
   <li class="readable-text" id="p91"><kbd>summing_node</kbd> has an edge to <kbd>END</kbd>, which means the end of execution. The final state returned will contain <kbd>25</kbd> under <kbd>answer</kbd>.</li>
  </ul>
  <div class="readable-text " id="p92"> 
   <p>Of course, this is a simple graph with only one path the execution can take. In a later part of this chapter, you'll encounter a graph with multiple paths—where a single node may branch into multiple nodes based on the state at that point.</p>
  </div>
  <div class="readable-text  intended-text" id="p93"> 
   <p>I hope this helped crystallize the concept of graphs and how LangGraph uses them to perform a task. It's now time to use what we've learned to start building our app.</p>
  </div>
  <div class="readable-text" id="p94"> 
   <h3 class=" readable-text-h3">10.2.3 A one-node LLM graph</h3>
  </div>
  <div class="readable-text " id="p95"> 
   <p>The basic graph we built in the previous section had nothing to do with AI or LLMs. Indeed, you can use LangGraph to build anything you like, whether or not AI is involved, but in practice, the point of LangGraph is to make building AI applications easier.</p>
  </div>
  <div class="readable-text sub-sub-section-heading" id="p96"> 
   <h4 class=" readable-text-h4">Creating and running your first graph</h4>
  </div>
  <div class="readable-text " id="p97"> 
   <p>In chapter nine, we encountered the OpenAI API's chat completions endpoint. In this endpoint, we pass a list of messages to the API, which predicts the next message in the conversation. In LangGraph, such an application could be represented by a simple one-node graph, as shown in figure 10.6.</p>
  </div>
  <div class="browsable-container figure-container" id="p98">  
   <img src="../Images/10__image006.png" alt="image"/> 
   <h5 class=" figure-container-h5">Figure 10.6 A basic single-node (apart from START and END) graph</h5>
  </div>
  <div class="readable-text " id="p99"> 
   <p>The state of the graph consists of a single variable, <kbd>messages</kbd>, which is—as you might expect—a list of messages.</p>
  </div>
  <div class="readable-text  intended-text" id="p100"> 
   <p>The only node in the graph, <kbd>assistant_node</kbd>, passes <kbd>messages</kbd> to an LLM, and returns the same list with an AI response message appended.</p>
  </div>
  <div class="readable-text  intended-text" id="p101"> 
   <p>Listing 10.1 shows this graph translated to real (non-Streamlit) Python code.</p>
  </div>
  <div class="browsable-container listing-container" id="p102"> 
   <h5 class=" listing-container-h5 browsable-container-h5">Listing 10.1 graph_example.py</h5>
   <div class="code-area-container"> 
    <pre class="code-area">from langgraph.graph import START, END, StateGraph
from langchain_core.messages import AnyMessage, HumanMessage
from langchain_openai import ChatOpenAI
from typing import TypedDict
 
llm = ChatOpenAI(model_name="gpt-4o-mini", openai_api_key="sk-proj-...")
 
class MyGraphState(TypedDict):
  messages: list[AnyMessage]
 
builder = StateGraph(MyGraphState)
 
def assistant_node(state):
  messages = state["messages"]
  ai_response_message = llm.invoke(messages)
  return {"messages": messages + [ai_response_message]}
 
builder.add_node("assistant", assistant_node)
builder.add_edge(START, "assistant")
builder.add_edge("assistant", END)
graph = builder.compile()
 
input_message = input("Talk to the bot: ")
initial_state = {"messages": [HumanMessage(content=input_message)]}
final_state = graph.invoke(initial_state)
 
print("Bot:\n" + final_state["messages"][-1].content)</pre>
   </div>
  </div>
  <div class="readable-text " id="p103"> 
   <p>(<kbd>chapter_10/graph_example.py</kbd> in the GitHub repo)</p>
  </div>
  <div class="readable-text  intended-text" id="p104"> 
   <p>First, we initialize the LLM in this line:</p>
  </div>
  <div class="browsable-container listing-container" id="p105"> 
   <div class="code-area-container"> 
    <pre class="code-area">llm = ChatOpenAI(model_name="gpt-4o-mini", openai_api_key="sk-proj-...")</pre>
   </div>
  </div>
  <div class="readable-text " id="p106"> 
   <p>This is similar to what we did in chapter 9 when we created an OpenAI API client. LangChain—a library closely related to LangGraph—provides a class called <kbd>ChatOpenAI</kbd> that does essentially the same thing but is slightly easier to use. As before, don't forget to replace <kbd>sk-proj...</kbd> with your actual OpenAI API key.</p>
  </div>
  <div class="readable-text  intended-text" id="p107"> 
   <p>Consider the next part:</p>
  </div>
  <div class="browsable-container listing-container" id="p108"> 
   <div class="code-area-container"> 
    <pre class="code-area">class MyGraphState(TypedDict):
  messages: list[AnyMessage]</pre>
   </div>
  </div>
  <div class="readable-text " id="p109"> 
   <p>As we've discussed, a graph has a state. For each graph you define, you specify what fields exist in the state by creating a class that contains those fields.</p>
  </div>
  <div class="readable-text  intended-text" id="p110"> 
   <p>In the above two lines, we're creating <kbd>MyGraphState</kbd> to represent the state of the graph we're about to define. In line with the example in figure 10.6, <kbd>MyGraphState</kbd> contains one field—<kbd>messages</kbd>—which is a list of objects of the type <kbd>AnyMessage</kbd>.</p>
  </div>
  <div class="readable-text  intended-text" id="p111"> 
   <p>In chapter nine, we saw that each message in an (OpenAI) LLM conversation has a <em>role</em>—one of <kbd>"user"</kbd>, <kbd>"assistant"</kbd>, or <kbd>"system"</kbd>. In LangChain, the same concept is represented by the <kbd>AnyMessage</kbd> superclass. <kbd>HumanMessage</kbd>, <kbd>AIMessage</kbd>, and <kbd>SystemMessage</kbd> are subclasses that inherit functionality from <kbd>AnyMessage</kbd>, and correspond to the <kbd>"user"</kbd>, <kbd>"assistant"</kbd>, and <kbd>"system"</kbd> roles, respectively.</p>
  </div>
  <div class="readable-text  intended-text" id="p112"> 
   <p><kbd>MyGraphState</kbd> itself is a subclass of <kbd>TypedDict</kbd>, a specialized dictionary type from Python's typing module that allows us to define a dictionary with a fixed set of keys and associated value types. Since it inherits all of TypedDict's behaviors, we can treat instances of TypedDict—and therefore, MyGraphState— as regular dictionaries, using the same syntax for accessing its keys (i.e. the fields in the class) and values.</p>
  </div>
  <div class="callout-container admonition-block"> 
   <div class="readable-text" id="p113"> 
    <h5 class=" callout-container-h5 readable-text-h5">Note</h5>
   </div>
   <div class="readable-text" id="p114"> 
    <p>We don't technically <em>have </em>to use a <kbd>TypedDict</kbd> to represent the state of the graph. We could also have used a regular class, a dataclass, or a Pydantic <kbd>BaseModel,</kbd> which we used in chapter nine. We've introduced and used TypedDict here because it plays well with <kbd>MessagesState</kbd>, a built-in LangGraph class that we'll discuss shortly.</p>
   </div>
  </div>
  <div class="readable-text " id="p115"> 
   <p>The next line, <kbd>builder = StateGraph(MyGraphState)</kbd>, initializes the construction of our graph. Here we're telling LangGraph that we're building a <kbd>StateGraph</kbd>—the type of graph we've been talking about where the nodes read from and write into a shared state—whose state is represented by a <kbd>MyGraphState</kbd> instance (which, as we've seen, will have a <kbd>messages</kbd> list).</p>
  </div>
  <div class="readable-text  intended-text" id="p116"> 
   <p>We now define the only node in our graph thus:</p>
  </div>
  <div class="browsable-container listing-container" id="p117"> 
   <div class="code-area-container"> 
    <pre class="code-area">def assistant_node(state):
  messages = state["messages"]
  ai_response_message = llm.invoke(messages)
  return {"messages": messages + [ai_response_message]}</pre>
   </div>
  </div>
  <div class="readable-text " id="p118"> 
   <p>Each node in a LangGraph graph is a regular Python function that accepts the current state of the graph—a <kbd>MyGraphState</kbd> instance—as input, and returns the parts of the state it wants to modify.</p>
  </div>
  <div class="readable-text  intended-text" id="p119"> 
   <p>The <kbd>assistant_node</kbd> we've defined above is quite minimal; it simply passes the <kbd>messages</kbd> list—accessed using square brackets as <kbd>state["messages"]</kbd> just like in a regular dictionary—to the <kbd>invoke</kbd> method of <kbd>llm</kbd>, obtaining the AI's response message. It then modifies the <kbd>"messages"</kbd> key of the state, adding <kbd>ai_response_message</kbd> to the end, and returns the result.</p>
  </div>
  <div class="callout-container admonition-block"> 
   <div class="readable-text" id="p120"> 
    <h5 class=" callout-container-h5 readable-text-h5">Note</h5>
   </div>
   <div class="readable-text" id="p121"> 
    <p>In the above code, since <kbd>MyGraphState</kbd> only has a single key, <kbd>messages</kbd>, it seems that <kbd>assistant_node</kbd> is simply returning the entirety of the modified state. That's not strictly true—it's actually only returning the keys it wants to modify, leaving any other keys untouched. This will become clear in later sections.</p>
   </div>
  </div>
  <div class="readable-text " id="p122"> 
   <p>Now that we've created our only node, it's time to put it in our graph:</p>
  </div>
  <div class="browsable-container listing-container" id="p123"> 
   <div class="code-area-container"> 
    <pre class="code-area">builder.add_node("assistant", assistant_node)
builder.add_edge(START, "assistant")
builder.add_edge("assistant", END)</pre>
   </div>
  </div>
  <div class="readable-text " id="p124"> 
   <p>The first line adds a node called <kbd>assistant</kbd> to the graph, pointing to the <kbd>assistant_node</kbd> function we just developed as the logic for the node.</p>
  </div>
  <div class="readable-text  intended-text" id="p125"> 
   <p>As mentioned earlier, each graph has dummy <kbd>START</kbd> and <kbd>END</kbd> nodes. The remaining two lines create directed edges from <kbd>START</kbd> to our <kbd>assistant</kbd> node, and from our <kbd>assistant</kbd> node to <kbd>END</kbd>, thus completing the graph.</p>
  </div>
  <div class="readable-text  intended-text" id="p126"> 
   <p>The immediately following line, <kbd>graph = builder.compile()</kbd>, <em>compiles </em>the graph, readying it for execution.</p>
  </div>
  <div class="readable-text  intended-text" id="p127"> 
   <p>The last few lines of code in the file show how a graph can be invoked:</p>
  </div>
  <div class="browsable-container listing-container" id="p128"> 
   <div class="code-area-container"> 
    <pre class="code-area">input_message = input("Talk to the bot: ")
initial_state = {"messages": [HumanMessage(content=input_message)]}
final_state = graph.invoke(initial_state)
 
print("The LLM responded with:\n" + final_state["messages"][-1].content)</pre>
   </div>
  </div>
  <div class="readable-text " id="p129"> 
   <p>We first use the <kbd>input()</kbd> function—which prompts the user to enter something in the terminal—to collect the user's input message.</p>
  </div>
  <div class="readable-text  intended-text" id="p130"> 
   <p>We then construct the starting state of the graph as a dictionary with the key <kbd>"messages."</kbd> The message itself is an instance of <kbd>HumanMessage</kbd> with its content attribute set to the <kbd>input_message</kbd> we just collected.</p>
  </div>
  <div class="readable-text  intended-text" id="p131"> 
   <p>Passing <kbd>initial_state</kbd> to the graph's <kbd>invoke</kbd> method finally causes the graph to execute, effectively passing our user input to the LLM through <kbd>assistant_node</kbd>, returning the final state.</p>
  </div>
  <div class="readable-text  intended-text" id="p132"> 
   <p><kbd>final_state</kbd> contains all of the messages in the conversation so far (our user message and the LLM's response message), so we access the response message using <kbd>final_state["messages"][-1]</kbd> and print its content to the screen.</p>
  </div>
  <div class="readable-text  intended-text" id="p133"> 
   <p>To see this in action, copy all the code to a new file called <kbd>graph_example.py</kbd>, and run your code in the terminal using the <kbd>python</kbd> command like this:</p>
  </div>
  <div class="browsable-container listing-container" id="p134"> 
   <div class="code-area-container"> 
    <pre class="code-area">python graph_example.py</pre>
   </div>
  </div>
  <div class="readable-text " id="p135"> 
   <p>Enter a message when you see the <kbd>"Talk to the bot"</kbd> prompt. As an example output, here's what I got:</p>
  </div>
  <div class="browsable-container listing-container" id="p136"> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area"><b>$ python graph_example.py</b>
Talk to the bot: Howdy! Could you write a haiku about Note n' Nib for me?
Bot:
Ink and paper dance,
Whispers of thoughts intertwine—
Note n' Nib's embrace.</pre>
   </div>
  </div>
  <div class="readable-text " id="p137"> 
   <p>It looks like AI stole my dream haiku gig. Maybe I'll pivot to the performing arts—everyone loves a good mime.</p>
  </div>
  <div class="readable-text sub-sub-section-heading" id="p138"> 
   <h4 class=" readable-text-h4">Turning our graph into a class</h4>
  </div>
  <div class="readable-text " id="p139"> 
   <p>We've run our first graph in the terminal, but what we really want is to use it to power our customer support bot. We'll organize our code using object-oriented principles as we did in the last two chapters.</p>
  </div>
  <div class="readable-text  intended-text" id="p140"> 
   <p>Let's start by converting the code we wrote in the prior section into a <kbd>SupportAgentGraph</kbd> class in <kbd>graph.py</kbd>, shown in listing 10.2.</p>
  </div>
  <div class="browsable-container listing-container" id="p141"> 
   <h5 class=" listing-container-h5 browsable-container-h5">Listing 10.2 graph.py</h5>
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">from langgraph.graph import START, END, StateGraph, <b>MessagesState</b>
from langchain_core.messages import HumanMessage
<span style="mso-fareast-font-family:&quot;Roboto Mono&quot;"></span>
class SupportAgentGraph:
  <b>def __init__(self, llm):</b>
<b>    self.llm = llm</b>
<b>    self.graph = self.build_graph()</b>
<span style="mso-fareast-font-family:&quot;Roboto Mono&quot;"></span>
  <b>def get_assistant_node(self):</b>
    def assistant_node(state):
      ai_response_message = self.llm.invoke(state["messages"])
      <b>return {"messages": [ai_response_message]}</b>
    <b>return assistant_node</b>
<span style="mso-fareast-font-family:&quot;Roboto Mono&quot;"></span>
  <b>def build_graph(self):</b>
    builder = StateGraph(MessagesState)
    builder.add_node("assistant", <b>self.get_assistant_node()</b>)
    builder.add_edge(START, "assistant")
    builder.add_edge("assistant", END)
    return builder.compile()
<span style="mso-fareast-font-family:&quot;Roboto Mono&quot;"></span>
  <b>def invoke(self, human_message_text):</b>
    human_msg = HumanMessage(content=human_message_text)
    state = {"messages": [human_msg]}
    return self.graph.invoke(state)</pre>
   </div>
  </div>
  <div class="readable-text " id="p142"> 
   <p>(<kbd>chapter_10/in_progress_01/graph.py</kbd> in the GitHub repo)</p>
  </div>
  <div class="readable-text  intended-text" id="p143"> 
   <p>The code here is very similar to that in <kbd>graph_example.py</kbd>, but I want to highlight a few differences.</p>
  </div>
  <div class="readable-text  intended-text" id="p144"> 
   <p>Most obviously, we're encapsulating our graph in a class—<kbd>SupportAgentGraph</kbd>—which has a method for building the actual graph (<kbd>build_graph</kbd>) and another (<kbd>invoke</kbd>) for invoking it by passing a human (user) message.</p>
  </div>
  <div class="readable-text  intended-text" id="p145"> 
   <p>Rather than creating the LLM object within the class, we accept it as a parameter in <kbd>SupportAgentGraph</kbd>'s <kbd>__init__</kbd>, which builds the graph by calling <kbd>self.build_graph()</kbd> and saves it under <kbd>self.graph</kbd> for future invocations.</p>
  </div>
  <div class="readable-text  intended-text" id="p146"> 
   <p>You'll notice that our <kbd>MyGraphState</kbd> class, which we defined earlier, is nowhere to be found. We've swapped it out for <kbd>MessagesState</kbd>, a built-in LangGraph class that does more or less the same thing. <kbd>MessagesState</kbd>, like <kbd>MyGraphState</kbd>, has a <kbd>messages</kbd> field, which is a list of <kbd>AnyMessage</kbd> objects. The big difference between <kbd>MyGraphState</kbd> and <kbd>MessagesState</kbd> is how the <kbd>messages</kbd> field can be modified in a node—more on that in a second.</p>
  </div>
  <div class="readable-text  intended-text" id="p147"> 
   <p>Next, consider the <kbd>get_assistant_node</kbd> method:</p>
  </div>
  <div class="browsable-container listing-container" id="p148"> 
   <div class="code-area-container"> 
    <pre class="code-area">def get_assistant_node(self):
  def assistant_node(state):
    ai_response_message = self.llm.invoke(state["messages"])
    return {"messages": [ai_response_message]}
  return assistant_node</pre>
   </div>
  </div>
  <div class="readable-text " id="p149"> 
   <p>This method has a function definition—for <kbd>assistant_node</kbd>, which we encountered in the previous section—nested under it. It seems to do nothing other than return the function. What's that about?</p>
  </div>
  <div class="readable-text  intended-text" id="p150"> 
   <p>Well, since <kbd>assistant_node</kbd> needs to access the LLM object (<kbd>self.llm</kbd>), its code must live inside a method of the <kbd>SupportAgentGraph</kbd> class. But <kbd>assistant_node</kbd> can't <em>itself</em> be a method of the class, because the first argument passed to a method is <kbd>self</kbd>—the current instance of the class—while the first (and only) argument passed to a valid LangGraph node must be the graph state.</p>
  </div>
  <div class="readable-text  intended-text" id="p151"> 
   <p>So instead, we define <kbd>assistant_node</kbd> as an inner function within an outer method called <kbd>get_assistant_node</kbd>—taking advantage of the outer method's scope to access <kbd>self.llm</kbd> within the inner function—and have the outer method <em>return</em> the inner function so we can plug it into the graph. This programming pattern is called a <em>closure</em> since the inner function retains access to variables from its <em>enclosing</em> scope, even after the outer function has returned.</p>
  </div>
  <div class="readable-text  intended-text" id="p152"> 
   <p>The aforementioned plugging-in of the node happens in the <kbd>build_graph</kbd> method in this line:</p>
  </div>
  <div class="browsable-container listing-container" id="p153"> 
   <div class="code-area-container"> 
    <pre class="code-area">builder.add_node("assistant", self.get_assistant_node())</pre>
   </div>
  </div>
  <div class="readable-text " id="p154"> 
   <p>Since <kbd>get_assistant_node()</kbd> <em>returns</em> the <kbd>assistant_node</kbd> function (as opposed to calling it), we can use the call to <kbd>get_assistant_node</kbd> to refer to the inner function.</p>
  </div>
  <div class="readable-text  intended-text" id="p155"> 
   <p>The <kbd>assistant_node</kbd> function differs from the one of the same name we defined in the prior section in one important way. Consider the return statement, which has changed from:</p>
  </div>
  <div class="browsable-container listing-container" id="p156"> 
   <div class="code-area-container"> 
    <pre class="code-area">return {"messages": messages + [ai_response_message]}</pre>
   </div>
  </div>
  <div class="readable-text " id="p157"> 
   <p>to:</p>
  </div>
  <div class="browsable-container listing-container" id="p158"> 
   <div class="code-area-container"> 
    <pre class="code-area">return {"messages": [ai_response_message]}</pre>
   </div>
  </div>
  <div class="readable-text " id="p159"> 
   <p>Why do we not return the other items in the <kbd>messages</kbd> list anymore? The answer has to do with our having replaced <kbd>MyGraphState</kbd> with <kbd>MessagesState</kbd>. You see, each node in LangGraph's <kbd>StateGraph</kbd> receives the complete state as input, but the value it returns is treated as a set of <em>updates </em>to each key in the state. How exactly these updates are merged with the existing values depends on how we've specified it in our state type.</p>
  </div>
  <div class="readable-text  intended-text" id="p160"> 
   <p>In <kbd>MyGraphState</kbd>, we didn't mention any particular way of handling this, so the value associated with the key <kbd>messages</kbd> is simply replaced by whatever the node returns for that key. This is why we needed to return the entire list—because we would have lost the earlier messages otherwise.</p>
  </div>
  <div class="readable-text  intended-text" id="p161"> 
   <p>On the other hand, <kbd>MessagesState</kbd> internally specifies that the value returned by a node should be appended to the existing list. So, <kbd>ai_response_message</kbd> is simply tacked on to the existing messages, and we don't have to return the older messages separately.</p>
  </div>
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p162"> 
    <h5 class=" callout-container-h5 readable-text-h5">Note</h5>
   </div>
   <div class="readable-text" id="p163"> 
    <p><kbd>MessagesState</kbd> implements this append feature through a function called <kbd>add_messages</kbd>. In fact, the only difference between <kbd>MyGraphState</kbd> and <kbd>MessagesState</kbd> is that the <kbd>messages</kbd> field in <kbd>MessagesState</kbd> is defined (internally) like this:</p>
   </div>
   <div class="browsable-container listing-container" id="p164"> 
    <div class="code-area-container"> 
     <pre class="code-area">messages: Annotated[list[AnyMessage], add_messages]</pre>
    </div>
   </div>
   <div class="readable-text" id="p165"> 
    <p>I won't go into this in detail, but this is essentially saying that when updates occur, they should be handled by the <kbd>add_messages</kbd> function rather than a simple replacement.</p>
   </div>
  </div>
  <div class="readable-text " id="p166"> 
   <p>Whew! That was a lot of explanation, but hopefully, you now understand how graphs are modeled in LangGraph.</p>
  </div>
  <div class="readable-text sub-sub-section-heading" id="p167"> 
   <h4 class=" readable-text-h4">The Bot class</h4>
  </div>
  <div class="readable-text " id="p168"> 
   <p>Let's set aside <kbd>SupportAgentGraph</kbd> now and pivot to our main backend class, which we'll call <kbd>Bot</kbd>. <kbd>Bot</kbd> will be the single point of entry to the backend for our Streamlit frontend, similar to the <kbd>Game</kbd> and <kbd>Hub</kbd> classes in earlier chapters.</p>
  </div>
  <div class="readable-text  intended-text" id="p169"> 
   <p>Importantly, <kbd>Bot</kbd> will supply the LLM object that <kbd>SupportAgentGraph</kbd> needs and provide a user-friendly method that our frontend can call to chat with the bot.</p>
  </div>
  <div class="readable-text  intended-text" id="p170"> 
   <p>To create it, copy the code in listing 10.3 to a new file, <kbd>bot.py</kbd>.</p>
  </div>
  <div class="browsable-container listing-container" id="p171"> 
   <h5 class=" listing-container-h5 browsable-container-h5">Listing 10.3 bot.py</h5>
   <div class="code-area-container"> 
    <pre class="code-area">from langchain_openai import ChatOpenAI
from graph import SupportAgentGraph
 
class Bot:
  def __init__(self, api_keys):
    self.api_keys = api_keys
    self.llm = self.get_llm()
    self.graph = SupportAgentGraph(llm=self.llm)
 
  def get_llm(self):
    return ChatOpenAI(
      model_name="gpt-4o-mini",
      openai_api_key=self.api_keys["OPENAI_API_KEY"],
      max_tokens=2000
    )
 
  def chat(self, human_message_text):
    final_state = self.graph.invoke(human_message_text)
    return final_state["messages"][-1].content</pre>
   </div>
  </div>
  <div class="readable-text " id="p172"> 
   <p>(<kbd>chapter_10/in_progress_01/bot.py</kbd> in the GitHub repo)</p>
  </div>
  <div class="readable-text  intended-text" id="p173"> 
   <p>Luckily, the <kbd>Bot</kbd> class is a lot more straightforward than <kbd>SupportAgentGraph</kbd>. <kbd>__init__</kbd> accepts a dictionary of API keys—which, spoiler alert, we'll supply through <kbd>st.secrets</kbd> again—before setting up the LLM object with a call to the <kbd>get_llm</kbd> method, and passing it to the <kbd>SupportAgentGraph</kbd> instance, saved to <kbd>self.graph</kbd>.</p>
  </div>
  <div class="readable-text  intended-text" id="p174"> 
   <p><kbd>get_llm</kbd> simply uses LangChain's <kbd>ChatOpenAI</kbd> class to create the LLM object as discussed earlier. Notice that we've added a new parameter called <kbd>max_tokens</kbd>. As you may remember from the previous chapter, tokens are the basic units of text that language models process. By setting <kbd>max_tokens=2000</kbd>, we're telling OpenAI's API to limit responses to a maximum of 2000 tokens (about 1,500 words), which helps in both cost reduction and keeping responses (relatively) concise.</p>
  </div>
  <div class="readable-text  intended-text" id="p175"> 
   <p>The <kbd>chat</kbd> method abstracts away the complexity of dealing with graphs and states. It has a simple contract—put a human message string in, and get an AI response string out. It fulfills this promise by calling the <kbd>invoke</kbd> method of our <kbd>SupportAgentGraph</kbd>'s instance, and returning the content of the last message—which happens to be the AI message, as we saw earlier.</p>
  </div>
  <div class="readable-text sub-sub-section-heading" id="p176"> 
   <h4 class=" readable-text-h4">A chatbot frontend in Streamlit</h4>
  </div>
  <div class="readable-text " id="p177"> 
   <p>Our app's backend is now ready, so let's focus on the frontend next. Streamlit really shines when it comes to chatbot interfaces because of its native support for them.</p>
  </div>
  <div class="readable-text  intended-text" id="p178"> 
   <p>This is evident in the fact that our first iteration of <kbd>frontend.py</kbd>—shown in listing 10.4—is only 12 lines long.</p>
  </div>
  <div class="browsable-container listing-container" id="p179"> 
   <h5 class=" listing-container-h5 browsable-container-h5">Listing 10.4 frontend.py</h5>
   <div class="code-area-container"> 
    <pre class="code-area">import streamlit as st
from bot import Bot
 
if "bot" not in st.session_state:
  api_keys = st.secrets["api_keys"]
  st.session_state.bot = Bot(api_keys)
bot = st.session_state.bot
 
if human_message_text := st.chat_input("Chat with me!"):
  st.chat_message("human").markdown(human_message_text)
  ai_message_text = bot.chat(human_message_text)
  st.chat_message("ai").markdown(ai_message_text)</pre>
   </div>
  </div>
  <div class="readable-text " id="p180"> 
   <p>(<kbd>chapter_10/in_progress_01/frontend.py</kbd> in the GitHub repo)</p>
  </div>
  <div class="readable-text  intended-text" id="p181"> 
   <p>We start by putting a reference to our <kbd>Bot</kbd> instance—<kbd>bot</kbd>—in <kbd>st.session_state</kbd>, which is essentially the same pattern we've used in the last two chapters for the <kbd>Hub</kbd> and <kbd>Game</kbd> classes. To do so, we pass in the <kbd>api_keys</kbd> object from <kbd>st.secrets</kbd> to do so. We'll create <kbd>secrets.toml</kbd> in a bit.</p>
  </div>
  <div class="readable-text  intended-text" id="p182"> 
   <p>The interesting part is the last four lines. The first of these introduces a new Streamlit widget called <kbd>st.chat_input</kbd>:</p>
  </div>
  <div class="browsable-container listing-container" id="p183"> 
   <div class="code-area-container"> 
    <pre class="code-area">if human_message_text := st.chat_input("Chat with me!"):
  ...</pre>
   </div>
  </div>
  <div class="readable-text " id="p184"> 
   <p><kbd>st.chat_input</kbd> creates a text input box with a "Send" icon, similar to what you're probably used to in various messaging apps. Besides the "Send" icon, it's different from <kbd>st.text_input</kbd> in a few noticeable ways:</p>
  </div>
  <ul> 
   <li class="readable-text" id="p185">It's pinned to the <em>bottom</em> of the screen or the containing widget you put it in</li>
   <li class="readable-text" id="p186">Unlike <kbd>st.text_input,</kbd> which returns a value once a user clicks out of the textbox, <kbd>st.chat_input</kbd> only returns a value once the user has clicked "Send" or pressed Enter.</li>
  </ul>
  <div class="readable-text " id="p187"> 
   <p>Apart from <kbd>st.chat_input</kbd>, the code above may look unfamiliar for another reason; we're using the character sequence <kbd>:=</kbd>, which is called a <em>walrus operator</em> in Python (because if you tilt your head to the side, it kind of looks like a walrus).</p>
  </div>
  <div class="readable-text  intended-text" id="p188"> 
   <p>The walrus operator is just a trick to make your code slightly more concise. It allows you to assign values to variables as part of a larger expression rather than requiring a separate line for assignment. In other words, instead of the line we're discussing, we could have written the following to obtain the same effect:</p>
  </div>
  <div class="browsable-container listing-container" id="p189"> 
   <div class="code-area-container"> 
    <pre class="code-area">human_message_text = st.chat_input("Chat with Nibby!")
if human_message_text:
  ...
 </pre>
   </div>
  </div>
  <div class="callout-container admonition-block"> 
   <div class="readable-text" id="p190"> 
    <h5 class=" callout-container-h5 readable-text-h5">Note</h5>
   </div>
   <div class="readable-text" id="p191"> 
    <p>Python developers are divided on whether the walrus operator increases or decreases the readability of your code. Regardless of whether you choose to use it, it's a good idea to know what it is.</p>
   </div>
  </div>
  <div class="readable-text " id="p192"> 
   <p>Once we have an input message from the user, we can display the conversation:</p>
  </div>
  <div class="browsable-container listing-container" id="p193"> 
   <div class="code-area-container"> 
    <pre class="code-area">st.chat_message("human").markdown(human_message_text)
ai_message_text = bot.chat(human_message_text)
st.chat_message("ai").markdown(ai_message_text)</pre>
   </div>
  </div>
  <div class="readable-text " id="p194"> 
   <p><kbd>st.chat_message</kbd> is a Streamlit display widget that accepts either of two strings—<kbd>"human"</kbd> or <kbd>"ai"</kbd>—and styles the container accordingly. This includes showing an avatar corresponding to a user or a robot.</p>
  </div>
  <div class="readable-text  intended-text" id="p195"> 
   <p>In this case, we display <kbd>human_message_text</kbd> using <kbd>st.chat_message("human")</kbd>, call the <kbd>chat</kbd> method of our Bot instance, and display the response AI text with <kbd>st.chat_message("ai")</kbd>.</p>
  </div>
  <div class="readable-text  intended-text" id="p196"> 
   <p>Just to be clear, <kbd>st.chat_message</kbd> is similar to other Streamlit elements like <kbd>st.column</kbd> in that we could also have written:</p>
  </div>
  <div class="browsable-container listing-container" id="p197"> 
   <div class="code-area-container"> 
    <pre class="code-area">with st.chat_message("human"):
  st.markdown(human_message_text)</pre>
   </div>
  </div>
  <div class="readable-text " id="p198"> 
   <p>To complete the first version of Nibby, we need to create a <kbd>secrets.toml</kbd> file for our OpenAI API key, in a new <kbd>.streamlit</kbd> folder. The contents of this file are in listing 10.5.</p>
  </div>
  <div class="browsable-container listing-container" id="p199"> 
   <h5 class=" listing-container-h5 browsable-container-h5">Listing 10.5 .streamlit/secrets.toml</h5>
   <div class="code-area-container"> 
    <pre class="code-area">[api_keys]
OPENAI_API_KEY = 'sk-proj-...'    #A</pre>
    <div class="code-annotations-overlay-container">
     #A Replace sk-proj-... with your actual OpenAI API key.
     <br/>
    </div>
   </div>
  </div>
  <div class="readable-text " id="p200"> 
   <p>Go ahead and run your app with <kbd>streamlit run frontend.py</kbd> to test it out. Figure 10.7 shows our chatbot in action.</p>
  </div>
  <div class="browsable-container figure-container" id="p201">  
   <img src="../Images/10__image007.png" alt="image"/> 
   <h5 class=" figure-container-h5">Figure 10.7 A single-prompt-single-response chatbot in Streamlit (see chapter_10/in_progress_01 in the GitHub repo for the full code).</h5>
  </div>
  <div class="readable-text " id="p202"> 
   <p>Sweet! Notice the human and bot avatars, as well as the subtle background shading to distinguish between the two kinds of displayed messages.</p>
  </div>
  <div class="readable-text  intended-text" id="p203"> 
   <p>If you play around with the app, you'll realize that Nibby can't hold a conversation at this point, only respond to single messages. Next up, let's fix that!</p>
  </div>
  <div class="readable-text" id="p204"> 
   <h2 class=" readable-text-h2">10.3 Multi-turn conversations</h2>
  </div>
  <div class="readable-text " id="p205"> 
   <p>It took some effort to get there, but we've built an initial version of Nibby. Unfortunately, at the moment, Nibby's idea of a conversation is a single response to a single message.</p>
  </div>
  <div class="readable-text  intended-text" id="p206"> 
   <p>For instance, consider the exchange in figure 10.8</p>
  </div>
  <div class="browsable-container figure-container" id="p207">  
   <img src="../Images/10__image008.png" alt="image"/> 
   <h5 class=" figure-container-h5">Figure 10.8 Our chatbot doesn't remember the information we gave it.</h5>
  </div>
  <div class="readable-text " id="p208"> 
   <p>There are two things wrong here:</p>
  </div>
  <ul> 
   <li class="readable-text" id="p209">The bot didn't remember the information I gave it in the prior message.</li>
   <li class="readable-text" id="p210">Our frontend treats the second message-response pair as a completely new conversation, removing all traces of the first.</li>
  </ul>
  <div class="readable-text " id="p211"> 
   <p>In this section, we'll iterate on Nibby, solving both issues.</p>
  </div>
  <div class="readable-text" id="p212"> 
   <h3 class=" readable-text-h3">10.3.1 Adding memory to our graph</h3>
  </div>
  <div class="readable-text " id="p213"> 
   <p>Recall the simple one-node graph we created: it starts with a state containing a human message, passes this message to an LLM, appends the AI's response to the state, and then returns the updated state.</p>
  </div>
  <div class="readable-text  intended-text" id="p214"> 
   <p>What happens if you invoke the graph again with a follow-up message? Well, the process repeats—a <em>new</em> state containing <em>only</em> the follow-up message is created and passed to the graph, which treats this as a brand-new independent execution.</p>
  </div>
  <div class="readable-text  intended-text" id="p215"> 
   <p>This is a clear problem since conversations very rarely consist exclusively of a single message and response. The user <em>will </em>want to follow up, and the chatbot needs to remember what came before.</p>
  </div>
  <div class="readable-text  intended-text" id="p216"> 
   <p>To enable our graph to remember prior executions, we need to <em>persist</em> the state, rather than starting from scratch every time. Luckily, LangGraph makes this a snap through the concept of <em>checkpointers</em>, which can save the state of the graph at each step.</p>
  </div>
  <div class="readable-text  intended-text" id="p217"> 
   <p>Specifically, we're going to use a checkpointer to allow our graph state to be stored in memory. We can then assign a <em>thread ID</em> to each invocation of the graph. Whenever we pass the same thread ID while invoking the graph, the graph will recall the state previously stored in memory for that thread ID and start from <em>there</em> rather than from a clean slate.</p>
  </div>
  <div class="readable-text  intended-text" id="p218"> 
   <p>To implement this, make the changes shown below to <kbd>graph.py</kbd>:</p>
  </div>
  <div class="browsable-container listing-container" id="p219"> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area"><b>from langgraph.checkpoint.memory import MemorySaver</b>
from langgraph.graph import START, END, StateGraph, MessagesState
...
<span style="mso-fareast-font-family:&quot;Roboto Mono&quot;"></span>
class SupportAgentGraph:
  def __init__(self, llm):
    self.llm = llm
<span style="mso-fareast-font-family:&quot;Roboto Mono&quot;"></span>
  <b>  self.config = {"configurable": {"thread_id": "1"}}</b>
    ...
...
  def build_graph(self):
    <b>memory = MemorySaver()</b>
    builder = StateGraph(MessagesState)
    ...
    return builder.compile(<b>checkpointer=memory</b>)
<span style="mso-fareast-font-family:&quot;Roboto Mono&quot;"></span>
  def invoke(self, human_message_text):
    ...
    return self.graph.invoke(state, <b>self.config</b>)</pre>
   </div>
  </div>
  <div class="readable-text " id="p220"> 
   <p>(<kbd>chapter_10/in_progress_02/graph.py</kbd> in the GitHub repo)</p>
  </div>
  <div class="readable-text  intended-text" id="p221"> 
   <p>Let's start our discussion of the code above with <kbd>build_graph</kbd>. We've added a line at the top of this method:</p>
  </div>
  <div class="browsable-container listing-container" id="p222"> 
   <div class="code-area-container"> 
    <pre class="code-area">memory = MemorySaver()</pre>
   </div>
  </div>
  <div class="readable-text " id="p223"> 
   <p><kbd>MemorySaver</kbd> is a checkpointer built into LangGraph that can store graph states in memory. Various other kinds of checkpointers are available, depending on where you want to save your graph state. For instance, you could use different checkpointers to store conversations in a database like PostgreSQL or SQLite.</p>
  </div>
  <div class="readable-text  intended-text" id="p224"> 
   <p>We pass this to our graph when we compile it at the end of the method:</p>
  </div>
  <div class="browsable-container listing-container" id="p225"> 
   <div class="code-area-container"> 
    <pre class="code-area">return builder.compile(checkpointer=memory)</pre>
   </div>
  </div>
  <div class="readable-text " id="p226"> 
   <p>This allows our graph to save its state, but that's not enough. If we don't make any more changes, each graph invocation would still be a new, independent one. We need a way to tell the graph that a particular invocation belongs to a <em>thread </em>it has seen before.</p>
  </div>
  <div class="readable-text  intended-text" id="p227"> 
   <p>Direct your attention to <kbd>__init__</kbd>, where we've assigned a strange-looking value to a field called <kbd>self.config</kbd>:</p>
  </div>
  <div class="browsable-container listing-container" id="p228"> 
   <div class="code-area-container"> 
    <pre class="code-area">self.config = {"configurable": {"thread_id": "1"}}</pre>
   </div>
  </div>
  <div class="readable-text " id="p229"> 
   <p>The important part to notice here is <kbd>{"thread_id": "1"}</kbd>. Further down, in the <kbd>invoke</kbd> method, we pass this to the graph while invoking it:</p>
  </div>
  <div class="browsable-container listing-container" id="p230"> 
   <div class="code-area-container"> 
    <pre class="code-area">return self.graph.invoke(state, self.config)</pre>
   </div>
  </div>
  <div class="readable-text " id="p231"> 
   <p>We're essentially passing the thread ID <kbd>1</kbd> to the graph here so it knows that every time we invoke it, we're always in the same conversation thread which has an ID of <kbd>1</kbd>.</p>
  </div>
  <div class="readable-text  intended-text" id="p232"> 
   <p>As a result of this change, the first invocation of the graph (<kbd>"Hi, my name's Bob"</kbd> in the example that prompted these changes) will be saved under the thread ID <kbd>1</kbd>. At this point, the state will have two messages: the original human message and the AI response.</p>
  </div>
  <div class="readable-text  intended-text" id="p233"> 
   <p>When the follow-up message (<kbd>"What's my name"</kbd>) arrives, since we already have an existing thread with ID <kbd>1</kbd>, it will be <em>appended </em>to the existing state. The state that's passed to <kbd>assistant_node</kbd> (and therefore to the LLM) will have <em>three</em> messages, enabling it to respond correctly.</p>
  </div>
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p234"> 
    <h5 class=" callout-container-h5 readable-text-h5">Additional questions you may have</h5>
   </div>
   <div class="readable-text" id="p235"> 
    <p>Two natural questions may arise at this juncture:</p>
   </div>
   <ul> 
    <li class="readable-text" id="p236">Why is the thread ID always 1?</li>
    <li class="readable-text buletless-item" style="list-style-type: none;" id="p237">Recall that our Streamlit app session doesn't persist beyond a single browser refresh. So each time the user accesses the app by opening it in a new tab or refreshing the browser, the <kbd>SupportAgentGraph</kbd> instance is rebuilt, and the graph is re-compiled with a new <kbd>MemorySaver</kbd> object. Since <kbd>MemorySaver</kbd> stores graph states in memory instead of persisting it to an external data store like PostgreSQL, any thread from a different browser session—whatever the thread ID—is inaccessible, so we can safely use the same thread ID <kbd>1</kbd> for the new session.</li>
    <li class="readable-text buletless-item" style="list-style-type: none;" id="p238">Long story short, how we've set things up guarantees that a single graph instance will see at most one conversation in its lifetime, so we only need to specify one thread ID.</li>
    <li class="readable-text" id="p239">Why is the value of self.config so convoluted?</li>
    <li class="readable-text buletless-item" style="list-style-type: none;" id="p240">Looking at our explanation of checkpointers and memory, it seems that all we need to pass the graph when we invoke it is the value <kbd>1</kbd>. So why do we have this monstrosity: <kbd>{"configurable": {"thread_id": "1"}}</kbd>?</li>
    <li class="readable-text buletless-item" style="list-style-type: none;" id="p241">Though they are beyond the scope of this book, LangGraph offers many options when you invoke a graph, such as the ability to specify metadata or the number of parallel calls it can make. The thread ID is the only configuration we're using here, but it's far from the only one available. The convoluted-seeming structure of self.config reflects this.</li>
   </ul>
  </div>
  <div class="readable-text " id="p242"> 
   <p>Try running Nibby again and entering the same messages as before. This time you should see something similar to figure 10.9.</p>
  </div>
  <div class="browsable-container figure-container" id="p243">  
   <img src="../Images/10__image009.png" alt="image"/> 
   <h5 class=" figure-container-h5">Figure 10.9 Nibby now remembers information we told it earlier in the conversation (see chapter_10/in_progress_02 in the GitHub repo for the full code).</h5>
  </div>
  <div class="readable-text " id="p244"> 
   <p>As you can see, the app does remember the previous information we gave it this time, but we still need to update the frontend to show the entire conversation.</p>
  </div>
  <div class="readable-text" id="p245"> 
   <h3 class=" readable-text-h3">10.3.2 Displaying the conversation history</h3>
  </div>
  <div class="readable-text " id="p246"> 
   <p>Our Streamlit frontend is currently only set up to show the latest user-entered message and the AI's response.</p>
  </div>
  <div class="readable-text  intended-text" id="p247"> 
   <p>To display the full history, we need to first expose it in the backend. Let's start by adding a method to <kbd>graph.py</kbd> to get the entire conversation so far at any point:</p>
  </div>
  <div class="browsable-container listing-container" id="p248"> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">...
class SupportAgentGraph:
  
  ...
  <b>def get_conversation(self):</b>
<b>    state = self.graph.get_state(self.config)</b>
<b>    if "messages" not in state.values:</b>
<b>      return []</b>
<b>    return state.values["messages"]</b></pre>
   </div>
  </div>
  <div class="readable-text " id="p249"> 
   <p>(<kbd>chapter_10/in_progress_03/graph.py</kbd> in the GitHub repo)</p>
  </div>
  <div class="readable-text  intended-text" id="p250"> 
   <p>The <kbd>get_conversation</kbd> method in <kbd>SupportAgentGraph</kbd> simply returns the <kbd>messages</kbd> list in the graph's current state.</p>
  </div>
  <div class="readable-text  intended-text" id="p251"> 
   <p>To do this, it first gets a reference to the state (<kbd>self.graph.get_state(self.config)</kbd>), and then accesses the <kbd>"messages"</kbd> key using <kbd>state.values["messages"]</kbd>. Passing <kbd>self.config</kbd> to <kbd>get_state</kbd> is required to get us the correct conversation thread, though—as the sidebar in the previous section discusses—there's only one.</p>
  </div>
  <div class="readable-text  intended-text" id="p252"> 
   <p>Next, let's expose the full <kbd>messages</kbd> list in <kbd>bot.py</kbd>:</p>
  </div>
  <div class="browsable-container listing-container" id="p253"> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">...
class Bot:
 
  ...
  <b>def get_history(self):</b>
<b>    return self.graph.get_conversation()</b></pre>
   </div>
  </div>
  <div class="readable-text " id="p254"> 
   <p>(<kbd>chapter_10/in_progress_03/bot.py</kbd> in the GitHub repo)</p>
  </div>
  <div class="readable-text  intended-text" id="p255"> 
   <p>All the <kbd>get_history</kbd> method does is to pass the result of the <kbd>get_conversation</kbd> method we just defined faithfully through to its caller.</p>
  </div>
  <div class="browsable-container listing-container" id="p256"> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">We can now make the changes required in frontend.py:
...
bot = st.session_state.bot
 
<b>for message in bot.get_history():</b>
<b>    st.chat_message(message.type).markdown(message.content)</b>
<span style="mso-fareast-font-family:&quot;Roboto Mono&quot;"></span>
if human_message_text := st.chat_input("Chat with Nibby!"):
    ...</pre>
   </div>
  </div>
  <div class="readable-text " id="p257"> 
   <p>(<kbd>chapter_10/in_progress_03/frontend.py</kbd> in the GitHub repo)</p>
  </div>
  <div class="readable-text  intended-text" id="p258"> 
   <p>We call <kbd>bot.get_history()</kbd> to get the list of messages and iterate through it, displaying each in its own <kbd>st.chat_message</kbd> container.</p>
  </div>
  <div class="readable-text  intended-text" id="p259"> 
   <p>Recall that each message in the <kbd>messages</kbd> list is an instance of either <kbd>HumanMessage</kbd> or <kbd>AIMessage</kbd>. Either way, it also has a type field with a value of <kbd>"human"</kbd> in the case of <kbd>HumanMessage</kbd> and <kbd>"ai"</kbd> for an <kbd>AIMessage</kbd>. This therefore works perfectly as the type indicator argument in <kbd>st.chat_message</kbd>.</p>
  </div>
  <div class="readable-text  intended-text" id="p260"> 
   <p><kbd>message.content</kbd> has the message's text, so we display that using <kbd>st.markdown</kbd>.</p>
  </div>
  <div class="readable-text  intended-text" id="p261"> 
   <p>Rerun the app to see figure 10.10.</p>
  </div>
  <div class="browsable-container figure-container" id="p262">  
   <img src="../Images/10__image010.png" alt="image"/> 
   <h5 class=" figure-container-h5">Figure 10.10 Our frontend now displays the full conversation history (see chapter_10/in_progress_03 in the GitHub repo for the full code)</h5>
  </div>
  <div class="readable-text " id="p263"> 
   <p>As expected, Nibby now shows the full conversation so we can keep track of what's going on.</p>
  </div>
  <div class="readable-text" id="p264"> 
   <h2 class=" readable-text-h2">10.4 Restricting our bot to customer support</h2>
  </div>
  <div class="readable-text " id="p265"> 
   <p>Thus far, we've focused on getting Nibby's basic functionality right—including calling an LLM and handling a full conversation. The result is a <em>general</em> chatbot you can ask for pretty much anything.</p>
  </div>
  <div class="readable-text  intended-text" id="p266"> 
   <p>For example, consider what happens if we ask Nibby to sing a song (figure 10.11):</p>
  </div>
  <div class="browsable-container figure-container" id="p267">  
   <img src="../Images/10__image011.png" alt="image"/> 
   <h5 class=" figure-container-h5">Figure 10.11 Nibby entertains frivolous requests, potentially costing us money.</h5>
  </div>
  <div class="readable-text " id="p268"> 
   <p>Nibby can sing a song all right. It can also help you solve math problems or write an essay about the fall of the Roman Empire. Unfortunately, it does all of that on the company's dime. Remember, interacting with a cloud-based LLM costs <em>money</em>.</p>
  </div>
  <div class="readable-text  intended-text" id="p269"> 
   <p>Each time someone makes a frivolous request to your customer support bot, and the bot indulges the request with a long-winded response, it spends precious LLM tokens and costs you something. Sure, each message is only a fraction of a cent but add up all the pleas for coding assistance or role-playing as a character from Battlestar Galactica, and suddenly your boss wants to know why there's a Nibby-shaped hole in the company's quarterly earnings report.</p>
  </div>
  <div class="readable-text  intended-text" id="p270"> 
   <p>Of course, I'm being hyperbolic, but the point stands: we want Nibby to be strictly business.</p>
  </div>
  <div class="readable-text  intended-text" id="p271"> 
   <p>The best thing about an LLM is that you can simply tell it what you want it to do or not do, so this turns out to be an easy fix; we'll simply add an appropriate <em>prompt</em>.</p>
  </div>
  <div class="readable-text" id="p272"> 
   <h3 class=" readable-text-h3">10.4.1 Creating a base prompt</h3>
  </div>
  <div class="readable-text " id="p273"> 
   <p>As we did in chapter nine, we want to give the LLM some context about the use case we want it to serve. In that chapter, we did this by creating a message with the role "<kbd>system</kbd>." That's essentially what we're going to do here, too, though the abstractions we'll use are slightly different.</p>
  </div>
  <div class="readable-text  intended-text" id="p274"> 
   <p>Create a file called <kbd>prompts.py</kbd> with the content shown in listing 10.6.</p>
  </div>
  <div class="browsable-container listing-container" id="p275"> 
   <h5 class=" listing-container-h5 browsable-container-h5">Listing 10.6 prompts.py</h5>
   <div class="code-area-container"> 
    <pre class="code-area">BASE_SYS_MSG = """
  You are a customer support agent for Note n' Nib, an online stationery
  retailer. You are tasked with providing customer support to customers who
  have questions or concerns about the products or services offered by the
  company.
  
  You must refuse to answer any questions or entertain any requests that
  are not related to Note n' Nib or its products and services.
"""</pre>
   </div>
  </div>
  <div class="readable-text " id="p276"> 
   <p>(<kbd>chapter_10/in_progress_04/prompts.py</kbd> in the GitHub repo)</p>
  </div>
  <div class="readable-text  intended-text" id="p277"> 
   <p>The prompt gives Nibby its first indication that Note n' Nib exists and that it's supposed to be providing customer support for the company.</p>
  </div>
  <div class="readable-text  intended-text" id="p278"> 
   <p>Importantly, <kbd>BASE_SYS_MSG</kbd> also has an instruction to refuse any requests that are unrelated to Note n' Nib. Next, let's incorporate this into our graph.</p>
  </div>
  <div class="readable-text" id="p279"> 
   <h3 class=" readable-text-h3">10.4.2 Inserting a base context node in our graph</h3>
  </div>
  <div class="readable-text " id="p280"> 
   <p>As we learned in chapter nine, using OpenAI's chat completions endpoint involves passing a sequence of messages to the LLM.</p>
  </div>
  <div class="readable-text  intended-text" id="p281"> 
   <p>In our current graph, the list starts with the user's first instruction and contains only user messages and AI responses. To prevent Nibby from responding to frivolous requests, we just need to insert the system prompt we just created as the first message in the list we send to the LLM.</p>
  </div>
  <div class="readable-text  intended-text" id="p282"> 
   <p>We'll do this by inserting a new node in the graph to add the system message to the graph state and modifying the existing <kbd>assistant_node</kbd> to pass this message to the LLM before anything else.</p>
  </div>
  <div class="readable-text  intended-text" id="p283"> 
   <p>Figure 10.12 shows a visual representation of the new graph.</p>
  </div>
  <div class="browsable-container figure-container" id="p284">  
   <img src="../Images/10__image012.png" alt="image"/> 
   <h5 class=" figure-container-h5">Figure 10.12 Adding a base context node in our graph</h5>
  </div>
  <div class="readable-text " id="p285"> 
   <p>The changes required to <kbd>graph.py</kbd> are shown in listing 10.7:</p>
  </div>
  <div class="browsable-container listing-container" id="p286"> 
   <h5 class=" listing-container-h5 browsable-container-h5">Listing 10.7 graph.py (modified)</h5>
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">...
from langchain_core.messages import HumanMessage, <b>SystemMessage</b>
<b>from prompts import *</b>
<span style="mso-fareast-font-family:&quot;Roboto Mono&quot;"></span>
<b>class AgentState(MessagesState):</b>
<b>  sys_msg_text: str</b>
<span style="mso-fareast-font-family:&quot;Roboto Mono&quot;"></span>
class SupportAgentGraph:
  def __init__(self, llm):
    ...
<span style="mso-fareast-font-family:&quot;Roboto Mono&quot;"></span>
  <b>@staticmethod</b>
<b>  def base_context_node(state):</b>
<b>    return {"sys_msg_text": BASE_SYS_MSG}</b>
<span style="mso-fareast-font-family:&quot;Roboto Mono&quot;"></span>
  def get_assistant_node(self):
    def assistant_node(state):
      <b>sys_msg = SystemMessage(content=state["sys_msg_text"])</b>
<b>      messages_to_send = [sys_msg] + state["messages"]</b>
      ai_response_message = self.llm.invoke(<b>messages_to_send</b>)
      return {"messages": [ai_response_message]}
    return assistant_node
<span style="mso-fareast-font-family:&quot;Roboto Mono&quot;"></span>
  def build_graph(self):
    memory = MemorySaver()
    builder = StateGraph(<b>AgentState</b>)
<span style="mso-fareast-font-family:&quot;Roboto Mono&quot;"></span>
    <b>builder.add_node("base_context", self.base_context_node)</b>
    builder.add_node("assistant", self.get_assistant_node())
<span style="mso-fareast-font-family:&quot;Roboto Mono&quot;"></span>
    builder.add_edge(START, <b>"base_context"</b>)
    <b>builder.add_edge("base_context", "assistant")</b>
    builder.add_edge("assistant", END)
<span style="mso-fareast-font-family:&quot;Roboto Mono&quot;"></span>
    return builder.compile(checkpointer=memory)
<span style="mso-fareast-font-family:&quot;Roboto Mono&quot;"></span>
  def invoke(self, human_message_text):
    ...
  ...</pre>
   </div>
  </div>
  <div class="readable-text " id="p287"> 
   <p>(<kbd>chapter_10/in_progress_04/graph.py</kbd> in the GitHub repo)</p>
  </div>
  <div class="readable-text  intended-text" id="p288"> 
   <p>Starting from the top, we've added a couple of imports; we need the <kbd>SystemMessage</kbd> class in addition to <kbd>HumanMessage</kbd>, so that's one.</p>
  </div>
  <div class="readable-text  intended-text" id="p289"> 
   <p>The statement <kbd>from prompts import *</kbd> allows us to access any prompt we may add to <kbd>prompts.py</kbd> using only its variable name—without a prefix like <kbd>prompt.</kbd>. Since we're using the <kbd>*</kbd> wildcard here rather than importing specific objects, every object in <kbd>prompt.py</kbd>'s global scope becomes part of <kbd>graph.py</kbd>'s scope. In this case, it means we can refer to <kbd>BASE_SYS_MSG</kbd> directly, as we do later in the code.</p>
  </div>
  <div class="readable-text  intended-text" id="p290"> 
   <p>We've defined a new <kbd>AgentState</kbd> class:</p>
  </div>
  <div class="browsable-container listing-container" id="p291"> 
   <div class="code-area-container"> 
    <pre class="code-area">class AgentState(MessagesState):
  sys_msg_text: str</pre>
   </div>
  </div>
  <div class="readable-text " id="p292"> 
   <p><kbd>AgentState</kbd> inherits from <kbd>MessagesState</kbd>, so it also contains the <kbd>messages</kbd> field we've been using thus far. What we're effectively doing here is adding a new field to the state—called <kbd>sys_msg_text</kbd>—meant to hold the text of the system message.</p>
  </div>
  <div class="readable-text  intended-text" id="p293"> 
   <p>Next, within the class itself, we've added a new static method:</p>
  </div>
  <div class="browsable-container listing-container" id="p294"> 
   <div class="code-area-container"> 
    <pre class="code-area">@staticmethod
def base_context_node(state):
  return {"sys_msg_text": BASE_SYS_MSG}</pre>
   </div>
  </div>
  <div class="readable-text " id="p295"> 
   <p>This function represents the new node we're adding to the graph, called <kbd>base_context</kbd>. All this node does is to populate the <kbd>sys_msg_text</kbd> field we've added to the state. By returning <kbd>{"sys_msg_text": BASE_SYS_MSG}</kbd>, this node sets <kbd>sys_msg_text</kbd> to <kbd>BASE_SYS_MSG</kbd>—the context prompt we created a few minutes ago—in the graph's current state.</p>
  </div>
  <div class="readable-text  intended-text" id="p296"> 
   <p>To understand how this works, it's helpful to remember that a graph node does not return the entirety of the state; rather it only returns the keys in the state that need to be modified. Therefore, even though there's no mention of the <kbd>messages</kbd> field here, once this node has been executed, the state will continue to have that field—unmodified—in addition to <kbd>sys_msg_text</kbd>.</p>
  </div>
  <div class="callout-container admonition-block"> 
   <div class="readable-text" id="p297"> 
    <h5 class=" callout-container-h5 readable-text-h5">Note</h5>
   </div>
   <div class="readable-text" id="p298"> 
    <p>Unlike in the case of <kbd>messages</kbd>, when we return a dictionary with a <kbd>sys_msg_text</kbd> key it <em>replaces</em> the value of <kbd>sys_msg_text</kbd> in the state. This is because <kbd>sys_msg_text</kbd> uses the default update behavior, as opposed to the <em>append</em> behavior (enabled internally by the <kbd>add_messages</kbd> function) that <kbd>messages</kbd> uses.</p>
   </div>
  </div>
  <div class="readable-text " id="p299"> 
   <p>Why have we made <kbd>base_context_node</kbd> a static method? Well, recall once again that each node in the graph needs to accept the graph state as its first argument. We would like to put <kbd>base_context_node</kbd> inside <kbd>SupportAgentGraph</kbd> for logical code organization purposes, but if we make it a regular method, it'll need to accept the class instance (<kbd>self</kbd>) as its first argument. Making it a static method removes that requirement, and frees us to add a state argument.</p>
  </div>
  <div class="readable-text  intended-text" id="p300"> 
   <p>Some of you might be asking, "Wait a minute, didn't we structure <kbd>assistant_node</kbd> as a nested function for the same reason? Why didn't we do <em>that</em> here?"</p>
  </div>
  <div class="readable-text  intended-text" id="p301"> 
   <p>We could indeed have used a closure-based solution for <kbd>base_context_node</kbd> too, but we don't need to; unlike <kbd>assistant_node</kbd> which references <kbd>self.llm</kbd>, <kbd>base_context_node</kbd> doesn't need to access <kbd>self</kbd> at all. We therefore employ the more straightforward technique of applying the <kbd>@staticmethod</kbd> decorator to <kbd>base_context_node</kbd>.</p>
  </div>
  <div class="readable-text  intended-text" id="p302"> 
   <p>Speaking of <kbd>assistant_node</kbd>, consider the changes we've made to its code above:</p>
  </div>
  <div class="browsable-container listing-container" id="p303"> 
   <div class="code-area-container"> 
    <pre class="code-area">sys_msg = SystemMessage(content=state["sys_msg_text"])
messages_to_send = [sys_msg] + state["messages"]
ai_response_message = self.llm.invoke(messages_to_send)</pre>
   </div>
  </div>
  <div class="readable-text " id="p304"> 
   <p>Rather than invoking the LLM directly with <kbd>state["messages"]</kbd>, we now create a <kbd>SystemMessage</kbd> object with the <kbd>sys_msg_text</kbd> field we populated in <kbd>base_context_node</kbd> as the content and prepend it to <kbd>state["messages"]</kbd> to form the list we pass the LLM.</p>
  </div>
  <div class="readable-text  intended-text" id="p305"> 
   <p>Finally, note our updates to <kbd>build_graph</kbd>. Since we've extended <kbd>MessagesState</kbd> to include a <kbd>sys_msg_text</kbd> field, we use that to initialize the <kbd>StateGraph</kbd>:</p>
  </div>
  <div class="browsable-container listing-container" id="p306"> 
   <div class="code-area-container"> 
    <pre class="code-area">builder = StateGraph(AgentState)</pre>
   </div>
  </div>
  <div class="readable-text " id="p307"> 
   <p>We add the base context node like this:</p>
  </div>
  <div class="browsable-container listing-container" id="p308"> 
   <div class="code-area-container"> 
    <pre class="code-area">builder.add_node("base_context", self.base_context_node)</pre>
   </div>
  </div>
  <div class="readable-text " id="p309"> 
   <p>Notice how we're passing a reference to the <kbd>self.base_context_node</kbd> method itself here, as opposed to calling it with the double parentheses.</p>
  </div>
  <div class="readable-text  intended-text" id="p310"> 
   <p>We also reorder the edges in the graph to insert the <kbd>base_context</kbd> node between <kbd>START</kbd> and <kbd>assistant</kbd>:</p>
  </div>
  <div class="browsable-container listing-container" id="p311"> 
   <div class="code-area-container"> 
    <pre class="code-area">builder.add_edge(START, "base_context")
builder.add_edge("base_context", "assistant")</pre>
   </div>
  </div>
  <div class="readable-text " id="p312"> 
   <p>That should be all we need. Go ahead and re-run your app. Try requesting the bot to sing a song again to get a response similar to figure 10.13.</p>
  </div>
  <div class="browsable-container figure-container" id="p313">  
   <img src="../Images/10__image013.png" alt="image"/> 
   <h5 class=" figure-container-h5">Figure 10.13 Nibby now refuses to entertain frivolous requests (see chapter_10/in_progress_04 in the GitHub repo for the full code).</h5>
  </div>
  <div class="readable-text " id="p314"> 
   <p>It looks like Nibby got the memo! It won't help the user with irrelevant requests anymore. In the next section, we'll solve the opposite problem: getting it to help with <em>relevant</em> questions.</p>
  </div>
  <div class="readable-text" id="p315"> 
   <h2 class=" readable-text-h2">10.5 Retrieval Augmented Generation</h2>
  </div>
  <div class="readable-text " id="p316"> 
   <p>Models like GPT-4o are so effective because they have been pre-trained on a huge corpus of publicly available information, such as books, magazines, and websites. It's why Fact Frenzy, our trivia app from chapter nine, was able to ask and answer questions on such a wide range of topics.</p>
  </div>
  <div class="readable-text  intended-text" id="p317"> 
   <p>However, many of the more economically valuable use cases of generative AI require more than information in the public domain. Truly molding AI into something that fits your specific use case usually requires providing it with private information that only you possess.</p>
  </div>
  <div class="readable-text  intended-text" id="p318"> 
   <p>Take Nibby, for instance, who is ultimately meant to assist customers of Note n' Nib with their queries. What happens if we pose a valid question about a stationery product to Nibby? Figure 10.14 shows such a conversation.</p>
  </div>
  <div class="browsable-container figure-container" id="p319">  
   <img src="../Images/10__image014.png" alt="image"/> 
   <h5 class=" figure-container-h5">Figure 10.14 Nibby makes up information when it doesn't know the answer.</h5>
  </div>
  <div class="readable-text " id="p320"> 
   <p>Seems like Nibby knocked that one out of the park, right? Not quite. We never told our bot what kinds of pens Note n' Nib carries, so where is it getting its information from? Additionally, where are the fictional brands—InkStream and RoyalQuill—we encountered in chapter six? As it turns out, Nibby had no information available about fountain pens and, therefore, simply hallucinated this response!</p>
  </div>
  <div class="readable-text  intended-text" id="p321"> 
   <p>In this section, we'll discover a way to augment Nibby's existing store of worldly knowledge with custom information that we provide.</p>
  </div>
  <div class="readable-text" id="p322"> 
   <h3 class=" readable-text-h3">10.5.1 What is Retrieval Augmented Generation?</h3>
  </div>
  <div class="readable-text " id="p323"> 
   <p>So, how do we supplement all of the information an LLM has been trained on with our own? For relatively small pieces of information, it's actually trivially easy—in fact, we already know how! All we need to do is provide the info to the LLM as part of our prompt!</p>
  </div>
  <div class="readable-text  intended-text" id="p324"> 
   <p>We could get Nibby to get the question we posed in figure 10.14 right simply by listing the products Note n' Nib sells directly in the system message we send to the LLM.</p>
  </div>
  <div class="readable-text  intended-text" id="p325"> 
   <p>What about other questions, though? Technically, we could give the model all of the contextual information it might realistically need to answer any question directly in the prompt. The maximum amount of such information we can provide is measured in tokens, called the model's <em>context window length. </em></p>
  </div>
  <div class="readable-text  intended-text" id="p326"> 
   <p>Relatively recent models have a huge context window. For instance, gpt-4o-mini can take up to 128,000 tokens (about 96,000 words, since—on average—a token is roughly three-quarters of a word), while o3-mini—a newer reasoning model from OpenAI—has a context window of 200,000 tokens. Models from other providers can take even more tokens in a single prompt. Google's Gemini 2.0 Pro has a context window that is a whopping <em>2 million</em> tokens long—enough to fit the entire Harry Potter series of books, with space left over for almost all of the Lord of the Rings trilogy.</p>
  </div>
  <div class="readable-text  intended-text" id="p327"> 
   <p>Surely our problem is solved then? We can simply assemble all the information we possess about Note n' Nib and feed it to the LLM in each prompt, correct?</p>
  </div>
  <div class="readable-text  intended-text" id="p328"> 
   <p>Certainly, we could, but we probably don't want to for a couple of reasons:</p>
  </div>
  <ul> 
   <li class="readable-text" id="p329">LLMs are prone to information overload; we generally see degraded performance with extremely large prompts.</li>
   <li class="readable-text" id="p330">Even if there were no such degradation, LLM providers usually charge by the token, so if we had to pass our entire custom knowledge base in every LLM call, the costs would go through the roof.</li>
  </ul>
  <div class="readable-text " id="p331"> 
   <p>No, we need a different solution. If only we could <strong>read in a user's question and feed the LLM just the </strong><em><strong>relevant </strong></em><strong>parts of our knowledge base required to answer it</strong>.</p>
  </div>
  <div class="readable-text  intended-text" id="p332"> 
   <p>And that—in case you've somehow failed to realize where this spiel is going—is exactly what Retrieval Augmented Generation (RAG) is.</p>
  </div>
  <div class="readable-text  intended-text" id="p333"> 
   <p>RAG has the following essential steps:</p>
  </div>
  <ul> 
   <li class="readable-text" id="p334">Read the user's question</li>
   <li class="readable-text" id="p335"><strong>Retrieve</strong> the context <em>relevant</em> to the question from the knowledge base</li>
   <li class="readable-text" id="p336"><strong>Augment </strong>the question with the context required to answer it</li>
   <li class="readable-text" id="p337"><strong>Generate </strong>the answer to the question by feeding the question and context to the LLM</li>
  </ul>
  <div class="readable-text " id="p338"> 
   <p>The hard part of RAG is the <em>retrieve</em> step. Specifically, given a user question and a large custom knowledge base, how do you identify the parts of the knowledge base that are relevant to the question and extract only those parts from the base?</p>
  </div>
  <div class="readable-text  intended-text" id="p339"> 
   <p>The answer lies in the concept of <em>embeddings</em> and a piece of software known as a <em>vector database</em>.</p>
  </div>
  <div class="readable-text sub-sub-section-heading" id="p340"> 
   <h4 class=" readable-text-h4">Embeddings and Vector databases</h4>
  </div>
  <div class="readable-text " id="p341"> 
   <p>While we don't—strictly speaking—need to learn how embeddings—or even vector databases—work under the hood to implement RAG, it would be a good idea to gain a basic understanding of these concepts.</p>
  </div>
  <div class="readable-text  intended-text" id="p342"> 
   <p>Let's start with a simplified example to achieve this. Say you're known as something of a movie buff in your friend circle. Your buddy approaches you and says, "Hey, I watched <em>The Dark Knight</em> yesterday and loved it! Could you recommend another movie like it?"</p>
  </div>
  <div class="readable-text  intended-text" id="p343"> 
   <p>You're in a fix because—though you have an encyclopedic knowledge of movies—you're not sure how exactly to measure the <em>similarity</em> between two movies, so you can recommend the one that's the <em>most</em> similar to <em>The Dark Knight</em>. Refusing to accept defeat, you flee to your underground lair and try to work it out in solitude.</p>
  </div>
  <div class="readable-text  intended-text" id="p344"> 
   <p>Eventually, you come up with a system. You reckon that when people express their preference for various movies, they're subconsciously talking about two attributes: <em>comedic value</em> and <em>explosions per hour</em>. Therefore, you rate your entire catalog of movies against those two scales and plot the results in a chart (partially reproduced in figure 10.15).</p>
  </div>
  <div class="browsable-container figure-container" id="p345">  
   <img src="../Images/10__image015.png" alt="image"/> 
   <h5 class=" figure-container-h5">Figure 10.15 Converting movies into vectors and plotting them on a chart</h5>
  </div>
  <div class="readable-text " id="p346"> 
   <p>As you can see, <em>The Dark Knight </em>has a comedic value of <kbd>1.2</kbd> but a relatively high explosions-per-hour of <kbd>6</kbd>. We can represent it as a list of numbers: <kbd>[1.2, 6]</kbd>, called a <em>vector</em>. We can call the vector <kbd>[1.2, 6]</kbd> the <em>embedding</em> of the movie <em>The Dark Knight</em> in the two-dimensional comedic-value/explosions-per-hour space.</p>
  </div>
  <div class="readable-text  intended-text" id="p347"> 
   <p>Converting movies into numbers in this way makes it possible to measure their similarity. For instance, <em>Office Space </em>is represented as <kbd>[7.2, 0.4]</kbd> in the same space. The similarity (or, rather, lack thereof) between <em>Office Space</em> and <em>The Dark Knight</em> can be calculated mathematically by considering their <em>geometric</em> <em>distance</em>. The closer the embeddings of two movies are geometrically—as measured by the length of a straight line drawn between them—the more similar the underlying movies are.</p>
  </div>
  <div class="readable-text  intended-text" id="p348"> 
   <p>After several such calculations, you find that <em>The Bourne Ultimatum</em>, which has the vector <kbd>[1.1, 5.9]</kbd> is the closest to <em>The Dark Knight</em>. Having concluded your research, you get back to your friend and let them know (to which your friend responds, "Thank goodness you're alive! It's been two years, where have you <em>been</em>?").</p>
  </div>
  <div class="readable-text  intended-text" id="p349"> 
   <p>The question we're facing with Nibby is analogous to the movie recommendation problem above. Given a user's message (the movie your friend liked), and a knowledge base (your catalog of movies), we have to find the paragraphs/chunks (movies) that are most relevant (most "similar") to the user's message.</p>
  </div>
  <div class="readable-text  intended-text" id="p350"> 
   <p>To answer this efficiently, we need two things:</p>
  </div>
  <ul> 
   <li class="readable-text" id="p351">A way to convert a given piece of text into embeddings that capture its meaning (or <em>semantics</em>)</li>
   <li class="readable-text" id="p352">A way to store these embeddings and quickly calculate distances between them</li>
  </ul>
  <div class="readable-text " id="p353"> 
   <p>Obviously, the movie example above is overly simplistic. Our "space" only had two dimensions: comedic value and explosions per hour. Encoding the meaning of a piece of text requires a lot more dimensions (like hundreds or thousands), and the dimensions themselves would not be human-understandable concepts like "comedic value." We'll use a text embedding model provided by OpenAI for our use case.</p>
  </div>
  <div class="readable-text  intended-text" id="p354"> 
   <p>To store the embeddings, we'll use a program called a <em>vector database</em>. Vector databases make it easy to calculate the distance between embeddings or find the entries closest to a specific one. Rather than the "straight line" (or <em>Euclidean</em>) distance between vectors, we'll use a score called <em>cosine similarity</em>, which measures the <em>angle </em>between two vectors to determine their similarity. The vector database we'll use is Pinecone, a cloud-hosted service.</p>
  </div>
  <div class="readable-text" id="p355"> 
   <h3 class=" readable-text-h3">10.5.2 Implementing RAG in our app</h3>
  </div>
  <div class="readable-text " id="p356"> 
   <p>Armed with a conceptual understanding of RAG, let's now focus on implementing it to assist Nibby in answering customer questions.</p>
  </div>
  <div class="readable-text sub-sub-section-heading" id="p357"> 
   <h4 class=" readable-text-h4">Preparing the knowledge base</h4>
  </div>
  <div class="readable-text " id="p358"> 
   <p>The GitHub folder for this chapter (<a href="https://github.com/aneevdavis/streamlit-in-action/tree/main/chapter_10">https://github.com/aneevdavis/streamlit-in-action/tree/main/chapter_10</a>) has a subdirectory called <kbd>articles</kbd> with a series of customer support articles for Note n' Nib. Each article is a text file containing information about Note n' Nib's products or how it conducts business. For instance, <kbd>our_products.txt</kbd> includes a list of product descriptions, while <kbd>fountain_pen_maintenance.txt</kbd> is about maintaining RoyalQuill and InkStream pens.</p>
  </div>
  <div class="readable-text  intended-text" id="p359"> 
   <p>Here's an excerpt from the article that we'll reference later:</p>
  </div>
  <div class="browsable-container listing-container" id="p360"> 
   <div class="code-area-container"> 
    <pre class="code-area">Proper care ensures your InkStream and RoyalQuill fountain pens write smoothly for years.
 
- Cleaning: Flush the nib with warm water every few weeks.
- Refilling: Use high-quality ink to prevent clogging.
- Storage: Store pens upright to avoid leaks and ensure ink flow.</pre>
   </div>
  </div>
  <div class="readable-text " id="p361"> 
   <p>Copy the <kbd>articles</kbd> folder into your working directory now. This is the knowledge base that Nibby will have access to.</p>
  </div>
  <div class="readable-text sub-sub-section-heading" id="p362"> 
   <h4 class=" readable-text-h4">Setting up a vector database</h4>
  </div>
  <div class="readable-text " id="p363"> 
   <p>As mentioned briefly in the previous section, we will be using Pinecone, a managed vector database optimized for fast and scalable similarity search that's popular for AI applications. Pinecone's free "Starter" plan is more than enough for this chapter.</p>
  </div>
  <div class="readable-text  intended-text" id="p364"> 
   <p>Go to <a href="https://www.pinecone.io/">https://www.pinecone.io/</a> and sign up for an account now. As soon as you're done with the setup, you'll be presented with an API key, which you should save immediately. Once that's done, create a new <em>index</em>. An index is analogous to a table in a regular non-vector database like PostgreSQL. The index will store our support articles broken into parts, along with their embeddings.</p>
  </div>
  <div class="readable-text  intended-text" id="p365"> 
   <p>You'll be asked to supply values for various options during the creation process:</p>
  </div>
  <ul> 
   <li class="readable-text" id="p366"><strong>Index name:</strong> You can use whatever you like, but remember to save it since we'll use it in our code.</li>
   <li class="readable-text" id="p367"><strong>Model configuration: </strong>Choose <kbd>text-embedding-ada-002</kbd>, the OpenAI embedding model we'll use; the value of dimensions should automatically be set to 1,536.</li>
   <li class="readable-text" id="p368"><strong>Metric: </strong>Pick <kbd>cosine</kbd> to use the cosine similarity score we briefly discussed earlier.</li>
   <li class="readable-text" id="p369"><strong>Capacity mode </strong>should be <kbd>Serverless</kbd></li>
   <li class="readable-text" id="p370"><strong>Cloud provider</strong>: <kbd>AWS</kbd> is fine for this.</li>
   <li class="readable-text" id="p371"><strong>Region: </strong>At the time of writing, only <kbd>us-east-1</kbd> is available in the free plan, so pick that.</li>
  </ul>
  <div class="readable-text sub-sub-section-heading" id="p372"> 
   <h4 class=" readable-text-h4">Ingesting the knowledge base</h4>
  </div>
  <div class="readable-text " id="p373"> 
   <p>To incorporate the vector store into our chatbot app, we'll create a <kbd>VectorStore</kbd> class. Before doing so, add your Pinecone API key and the index name you just created to <kbd>secrets.toml</kbd> so it now looks like this:</p>
  </div>
  <div class="browsable-container listing-container" id="p374"> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">[api_keys]
OPENAI_API_KEY = 'sk-proj-...'    #A
<b>VECTOR_STORE_API_KEY = 'pcsk_...'</b>    #B
<span style="mso-fareast-font-family:&quot;Roboto Mono&quot;"></span>
<b>[config]</b>
<b>VECTOR_STORE_INDEX_NAME = 'index_name_you_chose'</b>    #C</pre>
    <div class="code-annotations-overlay-container">
     #A Replace with your actual OpenAI API key.
     <br/>#B Replace with your Pinecone API key.
     <br/>#C Replace sk-proj-... with the actual index name.
     <br/>
    </div>
   </div>
  </div>
  <div class="readable-text " id="p375"> 
   <p>We've added a new key to <kbd>api_keys</kbd> and a new section called <kbd>config</kbd> to hold the index name.</p>
  </div>
  <div class="readable-text  intended-text" id="p376"> 
   <p>Now on to the <kbd>VectorStore</kbd> class. Create a file named <kbd>vector_store.py</kbd> with the contents shown in listing 10.8.</p>
  </div>
  <div class="browsable-container listing-container" id="p377"> 
   <h5 class=" listing-container-h5 browsable-container-h5">Listing 10.8 vector_store.py</h5>
   <div class="code-area-container"> 
    <pre class="code-area">from pinecone import Pinecone
from langchain_openai import OpenAIEmbeddings
from langchain_pinecone import PineconeVectorStore
from langchain_community.document_loaders import DirectoryLoader, TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
 
class VectorStore:
  def __init__(self, api_keys, index_name):
    pc = Pinecone(api_key=api_keys["VECTOR_STORE_API_KEY"])
    embeddings = OpenAIEmbeddings(api_key=api_keys["OPENAI_API_KEY"])
    index = pc.Index(index_name)
    self.store = PineconeVectorStore(index=index, embedding=embeddings)
 
  def ingest_folder(self, folder_path):
    loader = DirectoryLoader(
      folder_path,
      glob="**/*.txt",
      loader_cls=TextLoader
    )
    documents = loader.load()
    splitter = RecursiveCharacterTextSplitter(
      chunk_size=1000,
      chunk_overlap=200
    )
    texts = splitter.split_documents(documents)
    self.store.add_documents(texts)
 
  def retrieve(self, query):
    return self.store.similarity_search(query)</pre>
   </div>
  </div>
  <div class="readable-text " id="p378"> 
   <p>(<kbd>chapter_10/in_progress_05/vector_store.py</kbd> in the GitHub repo)</p>
  </div>
  <div class="readable-text  intended-text" id="p379"> 
   <p>There's a fair bit going on here, so let's go through it step-by-step.</p>
  </div>
  <div class="readable-text  intended-text" id="p380"> 
   <p>The <kbd>__init__</kbd> has the boilerplate code required to set up the vector store connection. It accepts two arguments: <kbd>api_keys</kbd> (the dictionary of API keys) and <kbd>index_name</kbd> (the name of the Pinecone index).</p>
  </div>
  <div class="readable-text  intended-text" id="p381"> 
   <p><kbd>__init__</kbd> first creates the <kbd>Pinecone</kbd> object—<kbd>pc</kbd>—using the Pinecone API key we noted a minute ago, and then an <kbd>OpenAIEmbeddings</kbd> object by passing it the OpenAI key. <kbd>pc.Index(index_name)</kbd> refers to the index we created earlier. Finally, we obtain a vector store object from the index and embeddings and assign it to <kbd>self.store</kbd> so we can use it in other methods.</p>
  </div>
  <div class="readable-text  intended-text" id="p382"> 
   <p>The <kbd>ingest_folder</kbd> method accepts the path to a folder and saves its contents to the Pinecone index. Consider the first part of this method:</p>
  </div>
  <div class="browsable-container listing-container" id="p383"> 
   <div class="code-area-container"> 
    <pre class="code-area">loader = DirectoryLoader(
  folder_path,
  glob="**/*.txt",
  loader_cls=TextLoader
)</pre>
   </div>
  </div>
  <div class="readable-text " id="p384"> 
   <p>LangChain provides various <em>document loaders</em> to help ingest and parse various types of data (text files, PDFs, web pages, databases, etc.) into a structured format suitable for processing. <kbd>DirectoryLoader</kbd> makes it easy to load files from a specified directory.</p>
  </div>
  <div class="readable-text  intended-text" id="p385"> 
   <p>The <kbd>glob="**/*.txt"</kbd> ensures that all text files (<kbd>.txt</kbd>) in the folder (including subfolders) are included.</p>
  </div>
  <div class="readable-text  intended-text" id="p386"> 
   <p><kbd>loader_cls=TextLoader</kbd> tells <kbd>DirectoryLoader</kbd> to use another loader class called <kbd>TextLoader</kbd>—also provided by LangChain—for loading individual text files.</p>
  </div>
  <div class="readable-text  intended-text" id="p387"> 
   <p>Once the <kbd>DirectoryLoader</kbd> is created, the next step is to load the documents:</p>
  </div>
  <div class="browsable-container listing-container" id="p388"> 
   <div class="code-area-container"> 
    <pre class="code-area">documents = loader.load()</pre>
   </div>
  </div>
  <div class="readable-text " id="p389"> 
   <p>This reads all <kbd>.txt</kbd> files in the directory and loads them into a list of <kbd>Document</kbd> objects, which LangChain uses to store the raw text and metadata.</p>
  </div>
  <div class="readable-text  intended-text" id="p390"> 
   <p>At the moment, each <kbd>Document</kbd> consists of an entire article. While the articles in our folder are relatively small, one can easily imagine a support article being thousands of words long. The point of fetching just the relevant text from our knowledge base is to reduce the overall size of the prompt; simply fetching entire articles defeats this purpose.</p>
  </div>
  <div class="readable-text  intended-text" id="p391"> 
   <p>Therefore, we want to divvy up the articles into manageable <em>chunks</em> of text that are roughly of equal size. That's what the next part of our code does:</p>
  </div>
  <div class="browsable-container listing-container" id="p392"> 
   <div class="code-area-container"> 
    <pre class="code-area">splitter = RecursiveCharacterTextSplitter(
  chunk_size=500,
  chunk_overlap=200
)
texts = splitter.split_documents(documents)</pre>
   </div>
  </div>
  <div class="readable-text " id="p393"> 
   <p><kbd>RecursiveCharacterTextSplitter</kbd> is a text-splitting utility from LangChain that can break documents into chunks while preserving meaningful context.</p>
  </div>
  <div class="readable-text  intended-text" id="p394"> 
   <p><kbd>chunk_size=500</kbd> sets the length of each chunk to 500 characters.</p>
  </div>
  <div class="readable-text  intended-text" id="p395"> 
   <p><kbd>chunk_overlap=200</kbd> means that chunks will have a 200-character overlap to maintain context.</p>
  </div>
  <div class="readable-text  intended-text" id="p396"> 
   <p>After splitting the documents, the chunks—available in <kbd>texts</kbd>—are ready to be stored:</p>
  </div>
  <div class="browsable-container listing-container" id="p397"> 
   <div class="code-area-container"> 
    <pre class="code-area">self.store.add_documents(texts)</pre>
   </div>
  </div>
  <div class="readable-text " id="p398"> 
   <p><kbd>add_documents</kbd> takes the split text chunks and adds them to the Pinecone index, storing both the text and embeddings generated using the <kbd>text-embedding-ada-002</kbd> model, making them searchable.</p>
  </div>
  <div class="readable-text  intended-text" id="p399"> 
   <p>The <kbd>retrieve</kbd> method allows callers to query the vector store:</p>
  </div>
  <div class="browsable-container listing-container" id="p400"> 
   <div class="code-area-container"> 
    <pre class="code-area">def retrieve(self, query):
  return self.store.similarity_search(query)</pre>
   </div>
  </div>
  <div class="readable-text " id="p401"> 
   <p>The <kbd>similarity_search</kbd> method of our <kbd>PineconeVectorStore</kbd> instance (<kbd>self.store</kbd>) searches the vector store for documents similar to <kbd>query</kbd>—the user's message—using the generated embeddings, returning a list of relevant <kbd>Document</kbd> objects based on the query.</p>
  </div>
  <div class="readable-text  intended-text" id="p402"> 
   <p>At this point, we've coded up the vector store functionality into a handy little class; next, let's use the class to ingest our <kbd>articles/</kbd> folder.</p>
  </div>
  <div class="readable-text  intended-text" id="p403"> 
   <p>This is an <em>offline</em> step that you only need to perform one time; once you've stored your articles in Pinecone, they'll remain there until you remove them.</p>
  </div>
  <div class="readable-text  intended-text" id="p404"> 
   <p>Go ahead and create a file called <kbd>ingest_to_vector_store.py</kbd>, copying the contents from listing 10.9.</p>
  </div>
  <div class="browsable-container listing-container" id="p405"> 
   <h5 class=" listing-container-h5 browsable-container-h5">Listing 10.9 ingest_to_vector_store.py</h5>
   <div class="code-area-container"> 
    <pre class="code-area">import toml
from vector_store import VectorStore
 
secrets = toml.load(".streamlit/secrets.toml")
api_keys = secrets["api_keys"]
index_name = secrets["config"]["VECTOR_STORE_INDEX_NAME"]
vector_store = VectorStore(api_keys, index_name)
vector_store.ingest_folder("articles/")</pre>
   </div>
  </div>
  <div class="readable-text " id="p406"> 
   <p>(<kbd>chapter_10/in_progress_05/ingest_to_vector_store.py</kbd> in the GitHub repo)</p>
  </div>
  <div class="readable-text  intended-text" id="p407"> 
   <p>Since this isn't meant to be run using Streamlit, we use the <kbd>toml</kbd> module directly to read our <kbd>secrets.toml</kbd>:</p>
  </div>
  <div class="browsable-container listing-container" id="p408"> 
   <div class="code-area-container"> 
    <pre class="code-area">secrets = toml.load(".streamlit/secrets.toml")</pre>
   </div>
  </div>
  <div class="readable-text " id="p409"> 
   <p>At the end of this, <kbd>secrets</kbd> should be a dictionary that contains the <kbd>api_keys</kbd> and <kbd>config</kbd> keys we organized <kbd>secrets.toml</kbd> into earlier.</p>
  </div>
  <div class="readable-text  intended-text" id="p410"> 
   <p>Next we grab the values we need:</p>
  </div>
  <div class="browsable-container listing-container" id="p411"> 
   <div class="code-area-container"> 
    <pre class="code-area">api_keys = secrets["api_keys"]
index_name = secrets["config"]["VECTOR_STORE_INDEX_NAME"]</pre>
   </div>
  </div>
  <div class="readable-text " id="p412"> 
   <p>We can now instantiate our <kbd>VectorStore</kbd> class:</p>
  </div>
  <div class="browsable-container listing-container" id="p413"> 
   <div class="code-area-container"> 
    <pre class="code-area">vector_store = VectorStore(api_keys, index_name)</pre>
   </div>
  </div>
  <div class="readable-text " id="p414"> 
   <p>Finally, we trigger the <kbd>ingest_folder</kbd> method:</p>
  </div>
  <div class="browsable-container listing-container" id="p415"> 
   <div class="code-area-container"> 
    <pre class="code-area">vector_store.ingest_folder("articles/")</pre>
   </div>
  </div>
  <div class="readable-text " id="p416"> 
   <p>To perform the actual ingestion, run this file in your terminal with the <kbd>python</kbd> command:</p>
  </div>
  <div class="browsable-container listing-container" id="p417"> 
   <div class="code-area-container"> 
    <pre class="code-area">python ingest_to_vector_store.py</pre>
   </div>
  </div>
  <div class="readable-text " id="p418"> 
   <p>Once it completes, you can go to the page corresponding to your index on the Pinecone website to see the newly ingested chunks, as seen in figure 10.16.</p>
  </div>
  <div class="browsable-container figure-container" id="p419">  
   <img src="../Images/10__image016.png" alt="image"/> 
   <h5 class=" figure-container-h5">Figure 10.16 You can see the chunks in your index on the Pinecone website.</h5>
  </div>
  <div class="readable-text " id="p420"> 
   <p>Notice the source field which holds the file name each chunk came from. You can also click the edit button on a record to add more metadata or to see its numeric vector values. If you were to count them, you'd find that there are 1536 of them, corresponding to the number of dimensions in the embedding model.</p>
  </div>
  <div class="readable-text sub-sub-section-heading" id="p421"> 
   <h4 class=" readable-text-h4">Adding RAG to the graph</h4>
  </div>
  <div class="readable-text " id="p422"> 
   <p>The Pinecone index we need for RAG is ready to go, but we still need to incorporate the functionality in our chatbot. For this, let's first lay out the additional instructions Nibby needs to use the knowledge base. Append the following to <kbd>prompts.py</kbd>:</p>
  </div>
  <div class="browsable-container listing-container" id="p423"> 
   <div class="code-area-container"> 
    <pre class="code-area">SYS_MSG_AUGMENTATION = """
  You have the following excerpts from Note n' Nib's
  customer service manual:
  ```
  {docs_content}
  ```
  If you're unable to answer the customer's question confidently with the
  given information, please redirect the user to call a human customer
  service representative at 1-800-NOTENIB.
"""</pre>
   </div>
  </div>
  <div class="readable-text " id="p424"> 
   <p>(<kbd>chapter_10/in_progress_05/prompts.py</kbd> in the GitHub repo)</p>
  </div>
  <div class="readable-text  intended-text" id="p425"> 
   <p>In a moment, we'll write the logic to replace <kbd>{docs_content}</kbd> with the document chunks we retrieve from Pinecone. The idea here is to give Nibby the context it needs and to get it to stop fabricating an answer out of thin air if it's not confident.</p>
  </div>
  <div class="readable-text  intended-text" id="p426"> 
   <p>Next, let's modify <kbd>graph.py</kbd> as shown in listing 10.10 so that it implements RAG.</p>
  </div>
  <div class="browsable-container listing-container" id="p427"> 
   <h5 class=" listing-container-h5 browsable-container-h5">Listing 10.10 graph.py (with RAG nodes)</h5>
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">...
from langchain_core.messages import HumanMessage, SystemMessage
<b>from langchain_core.documents import Document</b>
from prompts import *
<span style="mso-fareast-font-family:&quot;Roboto Mono&quot;"></span>
class AgentState(MessagesState):
  sys_msg_text: str
  <b>retrieved_docs: list[Document]</b>
<span style="mso-fareast-font-family:&quot;Roboto Mono&quot;"></span>
class SupportAgentGraph:
  def __init__(self, llm, <b>vector_store</b>):
    self.llm = llm
    <b>self.vector_store = vector_store</b>
<span style="mso-fareast-font-family:&quot;Roboto Mono&quot;"></span>
    self.config = {"configurable": {"thread_id": "1"}}
    self.graph = self.build_graph()
<span style="mso-fareast-font-family:&quot;Roboto Mono&quot;"></span>
  ...
  <b>def get_retrieve_node(self):</b>
<b>    def retrieve_node(state: AgentState):</b>
<b>      messages = state["messages"]</b>
<b>      message_contents = [message.content for message in messages]</b>
<b>      retrieval_query = "\n".join(message_contents)</b>
<b>      docs = self.vector_store.retrieve(retrieval_query)</b>
<b>      return {"retrieved_docs": docs}</b>
<b>    return retrieve_node</b>
<b></b>
<b>  @staticmethod</b>
<b>  def augment_node(state: AgentState):</b>
<b>    docs = state["retrieved_docs"]</b>
<b>    docs_content_list = [doc.page_content for doc in docs]</b>
<b>    content = "\n".join(docs_content_list)</b>
<b>    new_text = SYS_MSG_AUGMENTATION.replace("{docs_content}", content)</b>
<b>    return {"sys_msg_text": BASE_SYS_MSG + "\n\n" + new_text}</b>
<span style="mso-fareast-font-family:&quot;Roboto Mono&quot;"></span>
  ...
  def build_graph(self):
    ...
    builder.add_node("base_context", self.base_context_node)
   <b> builder.add_node("retrieve", self.get_retrieve_node())</b>
<b>    builder.add_node("augment", self.augment_node)</b>
    builder.add_node("assistant", self.get_assistant_node())
<span style="mso-fareast-font-family:&quot;Roboto Mono&quot;"></span>
    builder.add_edge(START, "base_context")
    <b>builder.add_edge("base_context", "retrieve")</b>
<b>    builder.add_edge("retrieve", "augment")</b>
<b>    builder.add_edge("augment", "assistant")</b>
    builder.add_edge("assistant", END)
<span style="mso-fareast-font-family:&quot;Roboto Mono&quot;"></span>
    return builder.compile(checkpointer=memory)
<span style="mso-fareast-font-family:&quot;Roboto Mono&quot;"></span>
  ...</pre>
   </div>
  </div>
  <div class="readable-text " id="p428"> 
   <p>(<kbd>chapter_10/in_progress_05/graph.py</kbd> in the GitHub repo)</p>
  </div>
  <div class="readable-text  intended-text" id="p429"> 
   <p>The first change is to <kbd>AgentState</kbd>, which now looks like this:</p>
  </div>
  <div class="browsable-container listing-container" id="p430"> 
   <div class="code-area-container"> 
    <pre class="code-area">class AgentState(MessagesState):
  sys_msg_text: str
  retrieved_docs: list[Document]</pre>
   </div>
  </div>
  <div class="readable-text " id="p431"> 
   <p>We'll now store the list of chunks retrieved from Pinecone in the graph state in a variable called <kbd>retrieved_docs</kbd>.</p>
  </div>
  <div class="readable-text  intended-text" id="p432"> 
   <p><kbd>__init__</kbd> now accepts <kbd>vector_store</kbd>—an instance of our <kbd>VectorStore</kbd> class—as an argument and saves it to <kbd>self.vector_store</kbd>.</p>
  </div>
  <div class="readable-text  intended-text" id="p433"> 
   <p>Note that rather than create the <kbd>VectorStore</kbd> instance <em>within</em> <kbd>graph.py</kbd>, we've chosen to have it be created elsewhere (<kbd>bot.py</kbd>, as we'll soon find out) and simply pass it to the <kbd>SupportAgentGraph</kbd> class. This is because we want <kbd>graph.py</kbd> to only contain the core logic of the graph. Objects that the graph <em>depends</em> on such as <kbd>llm</kbd> and <kbd>vector_store</kbd> should be passed to it. This coding pattern is called <em>dependency injection</em>, and is helpful while writing automated tests.</p>
  </div>
  <div class="readable-text  intended-text" id="p434"> 
   <p>Next, we need to introduce the process of retrieval-augmented generation to our graph. Figure 10.17 shows what the graph should look like by the end of this.</p>
  </div>
  <div class="browsable-container figure-container" id="p435">  
   <img src="../Images/10__image017.png" alt="image"/> 
   <h5 class=" figure-container-h5">Figure 10.17 Our graph now has retrieve and augment nodes for RAG.</h5>
  </div>
  <div class="readable-text " id="p436"> 
   <p>We've inserted two nodes in between the <kbd>base_context</kbd> and <kbd>assistant</kbd> nodes: a <kbd>retrieve</kbd> node that grabs context from the knowledge base that's relevant to the user's query and an <kbd>augment</kbd> node that adds this info to the prompt.</p>
  </div>
  <div class="readable-text  intended-text" id="p437"> 
   <p>Here's the code for the <kbd>retrieve</kbd> node:</p>
  </div>
  <div class="browsable-container listing-container" id="p438"> 
   <div class="code-area-container"> 
    <pre class="code-area">def get_retrieve_node(self):
  def retrieve_node(state: AgentState):
    messages = state["messages"]
    message_contents = [message.content for message in messages]
    retrieval_query = "\n".join(message_contents)
    docs = self.vector_store.retrieve(retrieval_query)
    return {"retrieved_docs": docs}
  return retrieve_node</pre>
   </div>
  </div>
  <div class="readable-text " id="p439"> 
   <p>As in the case of <kbd>assistant_node</kbd>, <kbd>retrieve_node</kbd> is structured as a nested function within a class method. It simply extracts the content of all the messages in the conversation and puts them in a single string to form the "query" that we'll pass to the vector store.</p>
  </div>
  <div class="readable-text  intended-text" id="p440"> 
   <p>The idea here is to find the chunks most relevant to the conversation. Since we're measuring relevance against the query, it makes sense that this is simply the text of the conversation.</p>
  </div>
  <div class="readable-text  intended-text" id="p441"> 
   <p>Once we have the query, we can call the <kbd>retrieve</kbd> method we defined earlier and return the list of retrieved documents in a dictionary, thereby updating the <kbd>retrieved_docs</kbd> key of the graph state.</p>
  </div>
  <div class="callout-container admonition-block"> 
   <div class="readable-text" id="p442"> 
    <h5 class=" callout-container-h5 readable-text-h5">Note</h5>
   </div>
   <div class="readable-text" id="p443"> 
    <p>While we've kept things simple here by including the text of the entire conversation in the retrieval query, you'll likely run into challenges as the conversation gets longer and longer. For any particular AI response, the most <em>recent</em> messages in the conversation are probably more contextually relevant—so it might be a good idea to form the retrieval query from just the last, say, five or six messages in the conversation.</p>
   </div>
  </div>
  <div class="readable-text " id="p444"> 
   <p>The augmentation node is shown below:</p>
  </div>
  <div class="browsable-container listing-container" id="p445"> 
   <div class="code-area-container"> 
    <pre class="code-area">@staticmethod
def augment_node(state: AgentState):
  docs = state["retrieved_docs"]
  docs_content_list = [doc.page_content for doc in docs]
  content = "\n".join(docs_content_list)
  new_text = SYS_MSG_AUGMENTATION.replace("{docs_content}", content)
  return {"sys_msg_text": BASE_SYS_MSG + "\n\n" + new_text}</pre>
   </div>
  </div>
  <div class="readable-text " id="p446"> 
   <p>This one doesn't need to access anything from <kbd>SupportAgentGraph</kbd>, so we structure it as a static method as we did for <kbd>base_context_node</kbd>.</p>
  </div>
  <div class="readable-text  intended-text" id="p447"> 
   <p><kbd>augment_node</kbd> mostly does the tedious work of wrangling and inserting the retrieved chunks into the system message. Once it has formed a string by concatenating the contents of the retrieved <kbd>Document</kbd> chunks, it simply plugs it into the text of the <kbd>SYS_MSG_AUGMENTATION</kbd> value we added to <kbd>prompts.py</kbd>, replacing <kbd>{docs_content}</kbd>.</p>
  </div>
  <div class="readable-text  intended-text" id="p448"> 
   <p>At the end of this node, <kbd>sys_msg_text</kbd> has the full system message—the earlier base message warning Nibby not to entertain frivolous questions, as well as the retrieved context.</p>
  </div>
  <div class="readable-text  intended-text" id="p449"> 
   <p>We already have a node for the "generate" step in RAG—<kbd>assistant_node</kbd>—so there's no need to add another.</p>
  </div>
  <div class="readable-text  intended-text" id="p450"> 
   <p>The edits to <kbd>build_graph</kbd> should be fairly obvious given figure 10.17; we officially add the <kbd>retrieve</kbd> and <kbd>augment</kbd> nodes, and attach them to the right edges.</p>
  </div>
  <div class="readable-text  intended-text" id="p451"> 
   <p>The next file to edit is <kbd>bot.py</kbd>. Make the changes below:</p>
  </div>
  <div class="browsable-container listing-container" id="p452"> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">...
<b>from vector_store import VectorStore</b>
<span style="mso-fareast-font-family:&quot;Roboto Mono&quot;"></span>
class Bot:
  def __init__(self, api_keys, <b>config</b>):
    self.api_keys = api_keys
    <b>self.config = config</b>
<span style="mso-fareast-font-family:&quot;Roboto Mono&quot;"></span>
    self.llm = self.get_llm()
    <b>self.vector_store = self.get_vector_store()</b>
<span style="mso-fareast-font-family:&quot;Roboto Mono&quot;"></span>
    self.graph = SupportAgentGraph(
      llm=self.llm, <b>vector_store=self.vector_store</b>)
<span style="mso-fareast-font-family:&quot;Roboto Mono&quot;"></span>
  <b>def get_vector_store(self):</b>
<b>    index_name = self.config["VECTOR_STORE_INDEX_NAME"]</b>
<b>    return VectorStore(api_keys=self.api_keys, index_name=index_name)</b>
  ...</pre>
   </div>
  </div>
  <div class="readable-text " id="p453"> 
   <p>(<kbd>chapter_10/in_progress_05/bot.py</kbd> in the GitHub repo)</p>
  </div>
  <div class="readable-text  intended-text" id="p454"> 
   <p><kbd>__init__</kbd> now accepts a <kbd>config</kbd> parameter, saved to <kbd>self.config</kbd>. It also creates the <kbd>vector_store</kbd> object by calling the <kbd>get_vector_store</kbd> method we'll discuss below, and passes it to the <kbd>SupportAgentGraph</kbd> constructor.</p>
  </div>
  <div class="readable-text  intended-text" id="p455"> 
   <p><kbd>get_vector_store</kbd> has the code required to complete the loop. It obtains the Pinecone index name from <kbd>self.config</kbd> and passes both <kbd>self.api_keys</kbd> and <kbd>index_name</kbd> to create an instance of <kbd>VectorStore</kbd> before returning it.</p>
  </div>
  <div class="readable-text  intended-text" id="p456"> 
   <p>The last change we need to make is a pretty small one in <kbd>frontend.py</kbd>:</p>
  </div>
  <div class="browsable-container listing-container" id="p457"> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">...
if "bot" not in st.session_state:
  api_keys = st.secrets["api_keys"]
  <b>config = st.secrets["config"]</b>
  st.session_state.bot = Bot(api_keys, <b>config</b>)
bot = st.session_state.bot
...</pre>
   </div>
  </div>
  <div class="readable-text " id="p458"> 
   <p>(<kbd>chapter_10/in_progress_05/frontend.py</kbd> in the GitHub repo)</p>
  </div>
  <div class="readable-text  intended-text" id="p459"> 
   <p>Since the <kbd>Bot</kbd> class now accepts a <kbd>config</kbd> parameter, we get its value from <kbd>st.secrets</kbd> and pass it in while instantiating the class.</p>
  </div>
  <div class="readable-text  intended-text" id="p460"> 
   <p>Let's try asking Nibby what products Note n' Nib sells again. Re-run the app and talk to it. Figure 10.18 shows an example of the new interaction.</p>
  </div>
  <div class="browsable-container figure-container" id="p461">  
   <img src="../Images/10__image018.png" alt="image"/> 
   <h5 class=" figure-container-h5">Figure 10.18 Nibby can now access and use information from our knowledge base (see chapter_10/in_progress_05 in the GitHub repo for the full code).</h5>
  </div>
  <div class="readable-text " id="p462"> 
   <p>Notice how Nibby answers our questions using information from our knowledge base and redirects to a 1-800 number when it encounters a question it doesn't know how to answer.</p>
  </div>
  <div class="readable-text" id="p463"> 
   <h2 class=" readable-text-h2">10.6 Turning our bot into an agent</h2>
  </div>
  <div class="readable-text " id="p464"> 
   <p>Giving Nibby access to Note n' Nib's customer support knowledge base has turned Nibby into a capable assistant, but it's still a purely informational bot. When customers call a support number or chat with a service representative, they're more often than not looking for help with their specific situation or order.</p>
  </div>
  <div class="readable-text  intended-text" id="p465"> 
   <p>For instance, a customer might wonder why it's taking so long for their placed order to arrive and want to check on the status, or they might want to simply cancel it. To resolve such issues, Nibby can't just rely on static textual articles; it needs to connect to Note n' Nib's systems and retrieve the right information or take the appropriate action.</p>
  </div>
  <div class="readable-text  intended-text" id="p466"> 
   <p>AI applications that can interact with the real world in this way have a special name: agents.</p>
  </div>
  <div class="readable-text" id="p467"> 
   <h3 class=" readable-text-h3">10.6.1 What are agents?</h3>
  </div>
  <div class="readable-text " id="p468"> 
   <p>Traditional AI chatbots follow a question-answer pattern, providing helpful but static responses. However, when users need personalized assistance—such as checking an order status, updating an address, or canceling an order—an informational bot falls short.</p>
  </div>
  <div class="readable-text  intended-text" id="p469"> 
   <p>This is where <em>agents</em> come in. Unlike passive chatbots, AI agents—also called <em>agentic apps</em>—can reason, plan, and interact with external systems to complete tasks. They don't just retrieve knowledge; they take actions based on it. These agents often rely on <em>tool use</em>, meaning they can call APIs, run database queries, and even trigger workflows in real-world applications.</p>
  </div>
  <div class="readable-text  intended-text" id="p470"> 
   <p>For example, instead of telling a customer to reach out to a customer care number for an order tracking number, an agent can fetch the tracking details and provide an update directly. Instead of directing a user to a cancellation policy page, it can process a cancellation request on their behalf.</p>
  </div>
  <div class="readable-text  intended-text" id="p471"> 
   <p>In short, agents make AI practical by allowing it to interface with the systems people already use. The key to making an agent work effectively is a framework that enables it to reason and decide the next steps dynamically. One such popular framework is called <em>ReAct</em>, short for <em>Reasoning + Acting</em>.</p>
  </div>
  <div class="readable-text" id="p472"> 
   <h3 class=" readable-text-h3">10.6.2 The ReAct framework</h3>
  </div>
  <div class="readable-text " id="p473"> 
   <p>To function as a true agent, an AI system must do more than just retrieve facts—it needs to reason about a situation, determine the right action, execute that action, and then incorporate the result into its next steps. The ReAct framework is designed to facilitate this process.</p>
  </div>
  <div class="callout-container admonition-block"> 
   <div class="readable-text" id="p474"> 
    <h5 class=" callout-container-h5 readable-text-h5">Note</h5>
   </div>
   <div class="readable-text" id="p475"> 
    <p>The ReAct AI framework is not to be confused with React, a Javascript toolkit for building web apps. It just so happens that we'll encounter the latter in chapter 13.</p>
   </div>
  </div>
  <div class="readable-text " id="p476"> 
   <p>Figure 10.19 is a visual representation of the ReAct framework.</p>
  </div>
  <div class="browsable-container figure-container" id="p477">  
   <img src="../Images/10__image019.png" alt="image"/> 
   <h5 class=" figure-container-h5">Figure 10.19 The ReAct framework</h5>
  </div>
  <div class="readable-text " id="p478"> 
   <p>ReAct structures an AI agent's behavior as an interleaving of reasoning steps and actions:</p>
  </div>
  <ol> 
   <li class="readable-text" id="p479"><strong>Reason:</strong> The agent analyzes the user's query, breaks it down into logical steps, and determines what needs to be done.</li>
   <li class="readable-text" id="p480"><strong>Act:</strong> The agent takes a concrete action, such as calling an API or querying a database, to retrieve relevant data or execute a task.</li>
   <li class="readable-text" id="p481"><strong>Reason:</strong> The agent incorporates the action's results into its reasoning and decides whether further steps are necessary.</li>
   <li class="readable-text" id="p482"><strong>Repeat:</strong> If further steps are needed, the cycle repeats.</li>
   <li class="readable-text" id="p483"><strong>Respond:</strong> If no further steps are needed, respond to the user.</li>
  </ol>
  <div class="readable-text " id="p484"> 
   <p>For example, consider a customer asking Nibby: "What's the status of my order?" Using the ReAct framework, the bot might follow these steps:</p>
  </div>
  <ol> 
   <li class="readable-text" id="p485"><strong>Reason:</strong> "The customer wants to check their order status. I need to fetch order details from the order database."</li>
   <li class="readable-text" id="p486"><strong>Act:</strong> Call Note &amp; Nib's order management system to retrieve the order status.</li>
   <li class="readable-text" id="p487"><strong>Reason:</strong> "The system shows the order has shipped and is in transit. I should provide the expected delivery date."</li>
   <li class="readable-text" id="p488"><strong>Respond:</strong> Respond to the customer with the latest tracking details and estimated delivery date.</li>
  </ol>
  <div class="readable-text " id="p489"> 
   <p>Creating tools our agent can use</p>
  </div>
  <div class="readable-text " id="p490"> 
   <p>The concept of <em>tools</em> is central to the process of developing a ReAct AI agent. In AI parlance, a tool is a function or API that an AI agent can call to perform real-world actions.</p>
  </div>
  <div class="readable-text  intended-text" id="p491"> 
   <p>Referring back to the requirements we drafted, we want our bot to be able to track and cancel orders on behalf of Note n' Nib's customers. Of course, Note n' Nib is a fictional company with no real orders or customers.</p>
  </div>
  <div class="readable-text  intended-text" id="p492"> 
   <p>This means we need some example data to work with. You can find this in the database.py file in the GitHub repository (<a href="https://github.com/aneevdavis/streamlit-in-action/tree/main/chapter_10/in_progress_06/">https://github.com/aneevdavis/streamlit-in-action/tree/main/chapter_10/in_progress_06/</a>). Copy the file to your working directory. Listing 10.11 shows a couple of excerpts from it.</p>
  </div>
  <div class="browsable-container listing-container" id="p493"> 
   <h5 class=" listing-container-h5 browsable-container-h5">Listing 10.11 database.py</h5>
   <div class="code-area-container"> 
    <pre class="code-area">users = {
    1: {
        "first_name": "Alice",
        "last_name": "Johnson",
        "date_of_birth": "1990-05-14",
        "email_address": "alice.johnson@example.com"
    },
    ...
}
 
orders = {
    101: {
        "user_id": 1,
        "order_placed_date": "2025-02-10",
        "order_status": "Shipped",
        "tracking_number": "TRK123456789",
        "items_purchased": ["RoyalQuill", "RedPinner"],
        "quantity": [1, 1],
        "shipping_address": "123 Main St, Springfield, IL, 62701",
        "expected_delivery_date": "2025-02-18"
    },
    ...
}</pre>
   </div>
  </div>
  <div class="readable-text " id="p494"> 
   <p>(<kbd>chapter_10/in_progress_06/database.py</kbd> in the GitHub repo)</p>
  </div>
  <div class="readable-text  intended-text" id="p495"> 
   <p>The file has two dictionaries—<kbd>users</kbd> and <kbd>orders</kbd>—keyed on user ID and order ID, respectively. Highlighted in the listing is user ID <kbd>1</kbd> (<kbd>Alice Johnson</kbd>) and a corresponding order the user placed on <kbd>2025-02-10</kbd>.</p>
  </div>
  <div class="readable-text  intended-text" id="p496"> 
   <p>In a real application, this information would be stored in a database like PostgreSQL. Still, since our focus is on the mechanics of building an AI agent, the static data in <kbd>database.py</kbd> will suffice for our purposes.</p>
  </div>
  <div class="readable-text  intended-text" id="p497"> 
   <p>There's another file in the repo called <kbd>tools.py</kbd>. Copy that one over too.</p>
  </div>
  <div class="readable-text  intended-text" id="p498"> 
   <p><kbd>tools.py</kbd> contains all the functions or tools that Nibby will be able to call. The file defines four such functions:</p>
  </div>
  <ul> 
   <li class="readable-text" id="p499"><kbd class="bold">retrieve_user_id</kbd> looks up a user's user ID, given their email address and date of birth.</li>
   <li class="readable-text" id="p500"><kbd class="bold">get_order_id</kbd> returns the ID of a particular order, given the user ID of the user that placed the order, and the date that they placed it. For simplicity, we assume that a user can only place one order on a particular day.</li>
   <li class="readable-text" id="p501"><kbd class="bold">get_order_status</kbd> accepts an order ID and returns its order status, tracking number, and expected delivery date.</li>
   <li class="readable-text" id="p502"><kbd class="bold">cancel_order</kbd> cancels an order, given an order ID.</li>
  </ul>
  <div class="readable-text " id="p503"> 
   <p>We're not interested in the actual implementations of these functions, though you can read through them in <kbd>tools.py</kbd>. What matters is that Nibby needs to be able to call and use them correctly.</p>
  </div>
  <div class="readable-text  intended-text" id="p504"> 
   <p>For this, we'll rely on type hints and docstrings. For instance, consider the definition of one of these functions in <kbd>tools.py</kbd>:</p>
  </div>
  <div class="browsable-container listing-container" id="p505"> 
   <div class="code-area-container"> 
    <pre class="code-area">def retrieve_user_id(email: str, dob: str) -&gt; str:
  """
  Look up a user's user ID, given their email address and date of birth.
 
  If the user is not found, return None.
 
  Args:
    email (str): The email address of the user.
    dob (str): The date of birth of the user in the format "YYYY-MM-DD".
 
  Returns:
    int: The user ID of the user, or None if the user is not found.
  """
  for user_id, user_info in users.items():
    if (user_info["email_address"] == email and
        user_info["date_of_birth"] == dob):
      return user_id</pre>
   </div>
  </div>
  <div class="readable-text " id="p506"> 
   <p>(<kbd>chapter_10/in_progress_06/tools.py</kbd> in the GitHub repo)</p>
  </div>
  <div class="readable-text  intended-text" id="p507"> 
   <p>Notice how we're using type hints in the function signature (<kbd>(email: str, dob: str) -&gt; str</kbd>), and explaining in detail what the function does in a docstring (the multi-line string after the signature), along with details about the arguments and return value.</p>
  </div>
  <div class="readable-text  intended-text" id="p508"> 
   <p>These auxiliary items are what the AI model will use to figure out what tool to call when.</p>
  </div>
  <div class="readable-text  intended-text" id="p509"> 
   <p>At the bottom of <kbd>tools.py</kbd>, there's this line that exports all the functions in the file as a list named tools:</p>
  </div>
  <div class="browsable-container listing-container" id="p510"> 
   <div class="code-area-container"> 
    <pre class="code-area">tools = [retrieve_user_id, get_order_id, get_order_status, cancel_order]</pre>
   </div>
  </div>
  <div class="readable-text " id="p511"> 
   <p>We'll use the <kbd>tools</kbd> variable in our graph in a bit.</p>
  </div>
  <div class="readable-text" id="p512"> 
   <h3 class=" readable-text-h3">10.6.3 Making our graph agentic</h3>
  </div>
  <div class="readable-text " id="p513"> 
   <p>Consider an example how Nibby might use the four tools we've made available to help a customer. Let's say the customer in the example data, Alice Johnson, wants to know the status of their order (order 101 shown in the excerpt from <kbd>database.py</kbd>). Here's how the interaction might proceed:</p>
  </div>
  <ul> 
   <li class="readable-text" id="p514"><strong>User: </strong>"Hi, my name's Alice! I placed an order on Feb 10, 2025. Could you check on its status, please?"</li>
   <li class="readable-text" id="p515"><strong>Nibby (reasoning to itself): </strong>"I have a tool called <kbd>get_order_status</kbd> that will give me the status of an order. To call it, I need an <kbd>order_id</kbd>. I don't have one, but I can get one by calling the <kbd>get_order_id</kbd> tool, which needs a <kbd>user_id</kbd> and an <kbd>order_placed_date</kbd>. The user says the <kbd>order_placed_date</kbd> is Feb 10, 2025, so I have that. I still need a <kbd>user_id</kbd>. To obtain <em>that</em>, I can call the <kbd>retrieve_user_id</kbd> tool, which requires an email and a date of birth. Therefore, I should ask the customer for this info."</li>
   <li class="readable-text" id="p516"><strong>Nibby: </strong>"Could you give me your email address and date of birth?"</li>
   <li class="readable-text" id="p517"><strong>User: </strong>"Sure, my email is <span class=" link-like">alice.johnson@example.com</span> and my date of birth is May 10, 1990."</li>
   <li class="readable-text" id="p518"><strong>Nibby (tool call): </strong>Call the tool <kbd>retrieve_user_id</kbd> with the parameters <kbd>email=</kbd><span class=" link-like">alice.johnson@example.com</span> and <kbd>dob=1990-05-10</kbd></li>
   <li class="readable-text" id="p519"><kbd class="bold">retrieve_user_id('</kbd><span class=" link-like">alice.johnson@example.com</span><kbd class="bold">', '1990-05-10')</kbd>: &lt;returns the value <kbd>1</kbd>, which is Alice's <kbd>user_id</kbd>&gt;</li>
   <li class="readable-text" id="p520"><strong>Nibby (tool call): </strong>Call the tool <kbd>get_order_id</kbd> with the parameters <kbd>user_id=1</kbd> and <kbd>order_placed_date=2025-02-10</kbd></li>
   <li class="readable-text" id="p521"><kbd class="bold">get_order_id(1, 1990-05-10)</kbd>: &lt;returns the value <kbd>101</kbd>, the correct order ID&gt;</li>
   <li class="readable-text" id="p522"><strong>Nibby (tool call): </strong>Call the tool <kbd>get_order_status</kbd> with the parameter <kbd>order_id=101</kbd></li>
   <li class="readable-text" id="p523"><strong>get_order_status(101)</strong>: &lt;returns a dictionary: {'order_status': 'Shipped', 'tracking_number': 'TRK123456789', 'expected_delivery_date': '2025-02-18'}&gt;</li>
   <li class="readable-text" id="p524"><strong>Nibby: </strong>"Your order has been shipped and should arrive on Feb 18, 2025. You can use the tracking number TRK123456789 to track it."</li>
  </ul>
  <div class="readable-text " id="p525"> 
   <p>Note how the bot needs to alternate between conversing with the user, reasoning about what needs to be done next, and issuing a call to a tool. How do we code all of this up? LangGraph actually makes this surprisingly easy.</p>
  </div>
  <div class="readable-text  intended-text" id="p526"> 
   <p>First, let's modify <kbd>bot.py</kbd> to make our LLM aware of the tools available to it:</p>
  </div>
  <div class="browsable-container listing-container" id="p527"> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">...
from vector_store import VectorStore
<b>from tools import tools</b>
<span style="mso-fareast-font-family:&quot;Roboto Mono&quot;"></span>
class Bot:
  def __init__(self, api_keys, config):
    ...
    self.llm = self.get_llm()<b>.bind_tools(tools)</b>
    ...
  ...</pre>
   </div>
  </div>
  <div class="readable-text " id="p528"> 
   <p>Here we're <em>binding</em> the tools we defined earlier—imported from <kbd>tools.py</kbd>—to our LLM object so that it knows they exist. As a result, if the LLM thinks it's appropriate, it can respond with a <em>tool call</em>.</p>
  </div>
  <div class="readable-text  intended-text" id="p529"> 
   <p>In the example above, the tool calls are marked with "Nibby (tool call)". In practical terms, these are <kbd>AIMessage</kbd>s produced by the LLM that have a property called <kbd>tool_calls</kbd> which contains information about any tools that the LLM wants us to call on its behalf and the parameters to call them with.</p>
  </div>
  <div class="readable-text  intended-text" id="p530"> 
   <p>The binding logic uses the docstrings and type hints we specified in <kbd>tools.py</kbd> to explain to the LLM what each tool does and how to use it.</p>
  </div>
  <div class="readable-text  intended-text" id="p531"> 
   <p>What about the graph itself? Listing 10.12 shows the changes required to <kbd>graph.py</kbd> to turn our bot into an agent.</p>
  </div>
  <div class="browsable-container listing-container" id="p532"> 
   <h5 class=" listing-container-h5 browsable-container-h5">Listing 10.12 graph.py (for an agentic app)</h5>
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import <b>START, StateGraph, MessagesState</b>
<b>from langgraph.prebuilt import tools_condition, ToolNode</b>
<b>from langchain_core.documents import Document</b>
from langchain_core.messages import HumanMessage, SystemMessage
from prompts import *
<b>from tools import tools</b>
<span style="mso-fareast-font-family:&quot;Roboto Mono&quot;"></span>
...
class SupportAgentGraph:
  ...
  def build_graph(self):
    ...
    <b>builder.add_node("tools", ToolNode(tools))</b>
    ...
    builder.add_edge("augment", "assistant")
    <b>builder.add_conditional_edges("assistant", tools_condition)</b>
<b>    builder.add_edge("tools", "assistant")</b>
<span style="mso-fareast-font-family:&quot;Roboto Mono&quot;"></span>
    return builder.compile(checkpointer=memory)
  ...</pre>
   </div>
  </div>
  <div class="readable-text " id="p533"> 
   <p>(<kbd>chapter_10/in_progress_06/graph.py</kbd> in the GitHub repo)</p>
  </div>
  <div class="readable-text  intended-text" id="p534"> 
   <p>Incredibly, all we needed to do was to add three lines to the <kbd>build_graph</kbd> method, and import a few extra items!</p>
  </div>
  <div class="readable-text  intended-text" id="p535"> 
   <p>Let's go through the line additions, starting with the first one:</p>
  </div>
  <div class="browsable-container listing-container" id="p536"> 
   <div class="code-area-container"> 
    <pre class="code-area">builder.add_node("tools", ToolNode(tools))</pre>
   </div>
  </div>
  <div class="readable-text " id="p537"> 
   <p>This adds a node named tools to our graph. <kbd>ToolNode</kbd> is a node already built into LangGraph, so we don't have to define it ourselves. It essentially does the following:</p>
  </div>
  <ul> 
   <li class="readable-text" id="p538">It takes the last message in the messages list (from the graph state) and executes any tool calls in it based on the list of tools we pass it.</li>
   <li class="readable-text" id="p539">Appends the value returned by the tool(s) as a <kbd>ToolMessage</kbd>—like <kbd>HumanMessage</kbd> and <kbd>AIMessage</kbd>—to the messages list.</li>
  </ul>
  <div class="readable-text " id="p540"> 
   <p>At the end of a <kbd>ToolNode</kbd>, the last message in the messages variable is a <kbd>ToolMessage</kbd> representing the output of calling a tool.</p>
  </div>
  <div class="readable-text  intended-text" id="p541"> 
   <p>We now have all the <em>pieces </em>we need, but how do we orchestrate the kind of thinking process outlined in the example interaction at the beginning of this section?</p>
  </div>
  <div class="readable-text  intended-text" id="p542"> 
   <p>Before we get into that, turn your attention to figure 10.20, which is what our graph will look like at the end of these changes:</p>
  </div>
  <div class="browsable-container figure-container" id="p543">  
   <img src="../Images/10__image020.png" alt="image"/> 
   <h5 class=" figure-container-h5">Figure 10.20 Our graph now has a tools node and a conditional edge</h5>
  </div>
  <div class="readable-text " id="p544"> 
   <p>Notice that there are now <em>two</em> lines flowing out from the assistant node—one goes to <kbd>END</kbd> as before, while another goes to our newly added <kbd>tools</kbd> node.</p>
  </div>
  <div class="readable-text  intended-text" id="p545"> 
   <p>In LangGraph, this is known as a <em>conditional edge</em>. We can choose the next node to execute from multiple options depending on a specified condition. A conditional edge is implemented as a function that takes the following form:</p>
  </div>
  <div class="browsable-container listing-container" id="p546"> 
   <div class="code-area-container"> 
    <pre class="code-area">def some_condition(state):
  # Branching
  if &lt;something is true&gt;:
    return "name_of_node_1"
  elif &lt;something else is true&gt;:
    return "name_of_node_2"
  ...</pre>
   </div>
  </div>
  <div class="readable-text " id="p547"> 
   <p>In our case, we don't actually need to build our own conditional edge because LangGraph already has exactly what we want:</p>
  </div>
  <div class="browsable-container listing-container" id="p548"> 
   <div class="code-area-container"> 
    <pre class="code-area">builder.add_conditional_edges("assistant", tools_condition)</pre>
   </div>
  </div>
  <div class="readable-text " id="p549"> 
   <p><kbd>tools_condition</kbd>—imported from <kbd>langgraph.prebuilt</kbd>—simply routes to our <kbd>ToolNode</kbd> (named <kbd>tools</kbd>) if the last message in the conversation—i.e. the LLM's response—contains any tool calls, or to END otherwise.</p>
  </div>
  <div class="readable-text  intended-text" id="p550"> 
   <p>Since <kbd>tools_condition</kbd> already has the logic to route to <kbd>END</kbd>, we can remove the earlier line that created a direct (non-conditional) edge between <kbd>assistant</kbd> and <kbd>END</kbd>.</p>
  </div>
  <div class="readable-text  intended-text" id="p551"> 
   <p>Once the <kbd>ToolNode</kbd> executes, we need the LLM to read the return value and decide what to do with it—whether it's calling <em>another</em> tool, or responding to the user.</p>
  </div>
  <div class="readable-text  intended-text" id="p552"> 
   <p>Therefore, we create a <em>loop</em> in the graph by connecting the tools node <em>back </em>to <kbd>assistant</kbd>:</p>
  </div>
  <div class="browsable-container listing-container" id="p553"> 
   <div class="code-area-container"> 
    <pre class="code-area">builder.add_edge("tools", "assistant")</pre>
   </div>
  </div>
  <div class="readable-text " id="p554"> 
   <p>And that's all it takes! Now when a request comes in, the LLM will reason about what to do. If it decides to call a tool, it'll put a tool call in its response, causing <kbd>tools_condition</kbd> to route to the <kbd>ToolNode</kbd> which executes the call. Since <kbd>ToolNode</kbd> has a direct edge to assistant, the LLM will get the updated list of messages with the appended <kbd>ToolMessage</kbd> and can again reason about what to do with the response.</p>
  </div>
  <div class="readable-text  intended-text" id="p555"> 
   <p>If the LLM decides it doesn't need to call any more tools—or that the next step in the process is to get some information from the user—it won't include any tool calls in its response, which means that <kbd>tools_condition</kbd> will route to <kbd>END</kbd>, and the final message will be displayed to the user.</p>
  </div>
  <div class="readable-text  intended-text" id="p556"> 
   <p>While that's all that's required to get the bot to work correctly, there's a last change we need to make in <kbd>graph.py</kbd> that's related to what gets shown to the customer in the frontend.</p>
  </div>
  <div class="readable-text  intended-text" id="p557"> 
   <p>As the above paragraphs hopefully make clear, the communication between the <kbd>assistant</kbd> and <kbd>tools</kbd> nodes happens through the <kbd>messages</kbd> variable in the graph state, and consists of internal messages of two types: <kbd>AIMessages</kbd> containing tool calls, and <kbd>ToolMessages</kbd> containing tool return values.</p>
  </div>
  <div class="readable-text  intended-text" id="p558"> 
   <p>Since we don't want to expose these internal messages to the user of our Streamlit app, we need to hide them when we pass the conversation history back. Recall that this history is relayed through the <kbd>get_history</kbd> method in <kbd>bot.py</kbd>, which calls the <kbd>get_conversation</kbd> method in <kbd>graph.py</kbd></p>
  </div>
  <div class="readable-text  intended-text" id="p559"> 
   <p>Let's make the appropriate changes in <kbd>graph.py</kbd> to remove these messages:</p>
  </div>
  <div class="browsable-container listing-container" id="p560"> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">...
<b>@staticmethod</b>
<b>def is_internal_message(msg):</b>
<b>  return msg.type == "tool" or "tool_calls" in msg.additional_kwargs</b>
<b></b>
def get_conversation(self):
  state = self.graph.get_state(self.config)
  if "messages" not in state.values:
    return []
<b>  messages = state.values["messages"]</b>
<b>  return [msg for msg in messages if not self.is_internal_message(msg)]</b></pre>
   </div>
  </div>
  <div class="readable-text " id="p561"> 
   <p>(<kbd>chapter_10/in_progress_06/graph.py</kbd> in the GitHub repo)</p>
  </div>
  <div class="readable-text  intended-text" id="p562"> 
   <p>First, we define <kbd>is_internal_message</kbd>, a static method that determines whether a message that we pass it is an "internal" one, i.e., one that's not fit to show the user. Above, we're defining an internal message as one that either has the type "<kbd>tool"</kbd>—which is true of <kbd>ToolMessages</kbd>—or has a <kbd>"tool_calls"</kbd> property (within <kbd>additional_kwargs</kbd>, a property that the LLM will use to set metadata within a message).</p>
  </div>
  <div class="readable-text  intended-text" id="p563"> 
   <p>Then, rather than returning all the messages from the state in <kbd>get_conversation</kbd>, we now filter for the non-internal messages and only return those.</p>
  </div>
  <div class="readable-text  intended-text" id="p564"> 
   <p>With that out of the way, re-run the app and test out its new capabilities! Figure 10.21 shows an example.</p>
  </div>
  <div class="browsable-container figure-container" id="p565">  
   <img src="../Images/10__image021.png" alt="image"/> 
   <h5 class=" figure-container-h5">Figure 10.21 Nibby can handle real-world actions like tracking and canceling orders (see chapter_10/in_progress_06 in the GitHub repo for the full code).</h5>
  </div>
  <div class="readable-text " id="p566"> 
   <p>By allowing Nibby to access external tools, we've given it superpowers and saved Note n' Nib's customer support division some serious time!</p>
  </div>
  <div class="readable-text  intended-text" id="p567"> 
   <p>This has been our most advanced app yet. If you've been following along and working on these projects yourself, you probably appreciate that the more complex an app becomes, the more things can go wrong in the real world when users actually start interacting with it. In the next chapter, we'll explore how to catch these proble</p>
  </div>
  <div class="readable-text  intended-text" id="p568"> 
   <p>ms beforehand and test your app to make it as robust as possible.</p>
  </div>
  <div class="readable-text" id="p569"> 
   <h2 class=" readable-text-h2">10.7 Summary</h2>
  </div>
  <ul> 
   <li class="readable-text" id="p570">Real-world AI applications require advanced capabilities, such as knowledge retrieval and action execution.</li>
   <li class="readable-text" id="p571">LangGraph structures AI workflows as graphs; nodes represent steps in the AI process, and edges define the flow between them.</li>
   <li class="readable-text" id="p572">Each node in LangGraph takes in the graph state and modifies it.</li>
   <li class="readable-text" id="p573"><kbd>st.chat_input</kbd> renders a textbox where users can type messages.</li>
   <li class="readable-text" id="p574"><kbd>st.chat_message</kbd> displays human and AI messages appropriately.</li>
   <li class="readable-text" id="p575">LangGraph uses checkpointers to persist information across multiple graph executions.</li>
   <li class="readable-text" id="p576">It's important to instruct the LLM explicitly to stay on-topic and ignore irrelevant requests; the system message is a good place for this.</li>
   <li class="readable-text" id="p577">Embeddings involve converting an object—such as a piece of text—into lists of numbers called vectors to find relevant content using similarity search.</li>
   <li class="readable-text" id="p578">Retrieval Augmented Generation (RAG) is a technique for providing custom knowledge to a pre-trained LLM by retrieving chunks of context relevant to the user query from a vector database like Pinecone.</li>
   <li class="readable-text" id="p579">A <em>tool</em> is simply a well-documented function that an LLM can choose to call in response to a user query.</li>
   <li class="readable-text" id="p580">An agentic app—or simply an <em>agent</em>—is one that can use tools to interact with the real world.</li>
   <li class="readable-text" id="p581">LangGraph makes it extremely easy to write agentic apps through its <kbd>ToolNode</kbd> and <kbd>tools_condition</kbd> abstractions.</li>
  </ul>
</body>
</html>