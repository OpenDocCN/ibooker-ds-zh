<html><head></head><body><section data-pdf-bookmark="Chapter 4. Correlation and Regression" data-type="chapter" epub:type="chapter"><div class="chapter" id="foundations-of-data-analytics">&#13;
<h1><span class="label">Chapter 4. </span>Correlation and Regression</h1>&#13;
&#13;
&#13;
<p>Have you heard that ice cream consumption is linked to shark attacks?&#13;
Apparently Jaws has a lethal appetite for mint chocolate chip. <a data-type="xref" href="#ice-cream-shark-attack">Figure 4-1</a> visualizes this proposed relationship.</p>&#13;
&#13;
<figure><div class="figure" id="ice-cream-shark-attack">&#13;
<img alt="Ice cream versus shark attacks" src="assets/aina_0401.png"/>&#13;
<h6><span class="label">Figure 4-1. </span>The proposed relationship between ice cream consumption and shark attacks</h6>&#13;
</div></figure>&#13;
&#13;
<p>“Not so,” you may retort. “This does not necessarily mean that shark&#13;
attacks are triggered by ice cream consumption.”</p>&#13;
&#13;
<p>“It could be,” you reason, “that as the outside temperature increases, more ice&#13;
cream is consumed. People also spend more time near the ocean when the weather is warm, and <em>that</em> coincidence leads to more shark attacks.”</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="“Correlation Does Not Imply Causation”" data-type="sect1"><div class="sect1" id="idm46274546845400">&#13;
<h1>“Correlation Does Not Imply Causation”</h1>&#13;
&#13;
<p>You’ve likely heard <a data-primary="correlation and regression" data-type="indexterm" id="ch4_term1"/><a data-primary="correlation and regression" data-secondary="causation, implication of" data-type="indexterm" id="idm46274546843080"/><a data-primary="causation, correlation and" data-type="indexterm" id="idm46274546842104"/>repeatedly that “correlation does not imply&#13;
causation.”</p>&#13;
&#13;
<p>In <a data-type="xref" href="ch03.html#foundations-of-inference">Chapter 3</a>, you learned that <em>causation</em> is a fraught expression in statistics. We really only <a data-primary="rejection of null" data-type="indexterm" id="idm46274546839288"/><a data-primary="null, rejection of" data-type="indexterm" id="idm46274546838584"/><a data-primary="null (H0) hypothesis" data-type="indexterm" id="idm46274546837912"/>reject the null hypothesis because we simply don’t have all the data to claim causality for sure. That semantic difference aside, does correlation have <em>anything</em> to do with causation? The standard expression somewhat oversimplifies their relationship; you’ll see why in this chapter using the tools&#13;
of inferential statistics you picked up earlier.</p>&#13;
&#13;
<p>This will be our final chapter conducted primarily in Excel. After that,&#13;
you will have sufficiently grasped the framework of analytics to be&#13;
ready to be move into R and Python.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Introducing Correlation" data-type="sect1"><div class="sect1" id="idm46274546835608">&#13;
<h1>Introducing Correlation</h1>&#13;
&#13;
<p>So far, we’ve mostly been analyzing statistics one variable at a&#13;
time: we’ve found the average reading score or the <a data-primary="descriptive statistics" data-secondary="for variance" data-secondary-sortas="variance" data-type="indexterm" id="idm46274546833416"/><a data-primary="variance, measuring" data-type="indexterm" id="idm46274546832168"/>variance in housing prices,&#13;
for example. This is <a data-primary="univariate analysis" data-type="indexterm" id="idm46274546831368"/>known as <em>univariate</em> analysis.</p>&#13;
&#13;
<p>We’ve also done a <a data-primary="bivariate analysis" data-type="indexterm" id="idm46274546829704"/>bit of <em>bivariate</em> analysis. For example, we&#13;
compared the <a data-primary="two-way frequency tables" data-type="indexterm" id="idm46274546828360"/><a data-primary="frequency tables" data-type="indexterm" id="idm46274546827688"/>frequencies of two categorical variables using a two-way&#13;
frequency table. We also analyzed a continuous variable when grouped by&#13;
multiple levels of a categorical variable, finding descriptive statistics for each group.</p>&#13;
&#13;
<p>We will now calculate a bivariate measure of <a data-primary="continuous variables" data-secondary="relationships between" data-type="indexterm" id="ch4_term2"/><a data-primary="linear relationships of variables" data-type="indexterm" id="ch4_term3"/><a data-primary="nonlinear relationships of variables" data-type="indexterm" id="ch4_term4"/><a data-primary="variables" data-secondary="linear relationships of" data-type="indexterm" id="ch4_term5"/><a data-primary="continuous variables" data-secondary="correlation analysis with" data-type="indexterm" id="ch4_term6"/><a data-primary="correlation and regression" data-secondary="correlation calculation and analysis" data-type="indexterm" id="ch4_term7"/>two continuous variables&#13;
using correlation. More specifically, we will <a data-primary="correlation and regression" data-secondary="Pearson" data-type="indexterm" id="idm46274546819128"/><a data-primary="Pearson correlation coefficient" data-type="indexterm" id="idm46274546818088"/>use the <em>Pearson correlation coefficient</em> to measure the strength of the <em>linear relationship</em>  between two variables. Without a linear relationship, the Pearson correlation is unsuitable.</p>&#13;
&#13;
<p>So, how do we know our data is linear? There are more rigorous ways to check, but, as usual, a&#13;
visualization is a great start. In particular, we will <a data-primary="scatterplots" data-secondary="linear/nonlinear relationships in" data-type="indexterm" id="idm46274546815864"/>use a <em>scatterplot</em> to depict all&#13;
observations <a data-primary="x-axis, mapping" data-type="indexterm" id="idm46274546814328"/><a data-primary="y-axis, mapping" data-type="indexterm" id="idm46274546813624"/>based on their x and y coordinates.</p>&#13;
&#13;
<p>If it appears a line could be drawn through the scatterplot&#13;
that summarizes the overall pattern, then it’s a linear relationship and the Pearson correlation can be used. If you would need a curve or some other shape to summarize the pattern, then the opposite holds. <a data-type="xref" href="#linear-vs-non-linear">Figure 4-2</a> depicts one linear and two nonlinear relationships.</p>&#13;
&#13;
<figure><div class="figure" id="linear-vs-non-linear">&#13;
<img alt="Linear versus non-linear scatterplot relationships" src="assets/aina_0402.png"/>&#13;
<h6><span class="label">Figure 4-2. </span>Linear versus nonlinear relationships</h6>&#13;
</div></figure>&#13;
&#13;
<p>In particular, <a data-type="xref" href="#linear-vs-non-linear">Figure 4-2</a> gives an <a data-primary="positive correlation of variables" data-type="indexterm" id="idm46274546807848"/>example of a <em>positive</em> linear relationship: as&#13;
values on the x-axis increase, so do the values on the y-axis (at a&#13;
linear rate).</p>&#13;
&#13;
<p>It’s also possible to <a data-primary="negative correlation of variables" data-type="indexterm" id="idm46274546805992"/>have a <em>negative</em> correlation, where a negative line summarizes the relationship, or no correlation&#13;
at all, in which a flat line summarizes it. These different types of linear relationships are <a data-primary="zero correlation of variables" data-type="indexterm" id="idm46274546804488"/>shown in <a data-type="xref" href="#negative-positive-no-correlation">Figure 4-3</a>. Remember, these all must be linear relationships for correlation to apply.</p>&#13;
&#13;
<figure><div class="figure" id="negative-positive-no-correlation">&#13;
<img alt="Correlation types" src="assets/aina_0403.png"/>&#13;
<h6><span class="label">Figure 4-3. </span>Negative, zero, and positive correlations</h6>&#13;
</div></figure>&#13;
&#13;
<p>Once we’ve established that the data is linear, we can find the <a data-primary="correlation coefficient" data-secondary="interpretation of" data-type="indexterm" id="idm46274546800424"/>correlation coefficient. It always takes a value between –1 and 1, with  –1&#13;
indicating a perfect negative linear relationship, 1 a perfect positive&#13;
linear relationship, and 0 no linear relationship at all. <a data-type="xref" href="#correlation-interpretation">Table 4-1</a> shows some <a data-primary="evaluation metrics" data-type="indexterm" id="idm46274546798200"/>rules of thumb for evaluating the&#13;
strength of a correlation coefficient. These are not official standards by any means, but a useful jumping-off point for interpretation.</p>&#13;
<table id="correlation-interpretation">&#13;
<caption><span class="label">Table 4-1. </span>Interpretation of correlation coefficients</caption>&#13;
<thead>&#13;
<tr>&#13;
<th>Correlation coefficient</th>&#13;
<th>Interpretation</th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr>&#13;
<td><p>–1.0</p></td>&#13;
<td><p>Perfect negative linear relationship</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>–0.7</p></td>&#13;
<td><p>Strong negative relationship</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>–0.5</p></td>&#13;
<td><p>Moderate negative relationship</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>–0.3</p></td>&#13;
<td><p>Weak negative relationship</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>0</p></td>&#13;
<td><p>No linear relationship</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>+0.3</p></td>&#13;
<td><p>Weak positive relationship</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>+0.5</p></td>&#13;
<td><p>Moderate positive relationship</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>+0.7</p></td>&#13;
<td><p>Strong positive relationship</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>+1.0</p></td>&#13;
<td><p>Perfect positive linear relationship</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
&#13;
<p>With the basic conceptual framework for correlations in mind, let’s do&#13;
some analysis in <a data-primary="Excel" data-secondary="correlation analysis with" data-type="indexterm" id="ch4_term8"/><a data-primary="correlation and regression" data-secondary="mpg dataset example for" data-type="indexterm" id="ch4_term9"/><a data-primary="datasets" data-secondary="mpg example" data-type="indexterm" id="ch4_term10"/><a data-primary="mpg dataset example" data-type="indexterm" id="ch4_term11"/><a data-primary="vehicle mileage (mpg) dataset example" data-type="indexterm" id="ch4_term12"/>Excel. We will be using a vehicle mileage dataset; you can find <em>mpg.xlsx</em> in the <em>mpg</em> subfolder of the book repository’s <a href="https://oreil.ly/ygWQn"> <em>datasets</em> folder</a>. This is a new dataset, so take some time to learn about it: what types&#13;
of variables are we working with? Summarize and visualize them using the tools covered in <a data-type="xref" href="ch01.html#foundations-of-eda">Chapter 1</a>. To help with subsequent analysis, don’t forget to add an index column and convert the dataset into a table, which I will call <em>mpg</em>.</p>&#13;
&#13;
<p>Excel <a data-primary="CORREL() function, Excel" data-type="indexterm" id="idm46274546768776"/>includes the <code>CORREL()</code> function to calculate the correlation&#13;
coefficient between two arrays:</p>&#13;
<pre>CORREL(array1, array2)</pre>&#13;
&#13;
<p>Let’s use this <a data-primary="correlation coefficient" data-secondary="with Excel" data-secondary-sortas="Excel" data-type="indexterm" id="ch4_term13"/>function to find the correlation between <code>weight</code> and&#13;
<code>mpg</code> in our dataset:</p>&#13;
<pre>=CORREL(mpg[weight], mpg[mpg])</pre>&#13;
&#13;
<p>This indeed returns a value between –1 and 1: –0.832. (Do you remember how to interpret this?)</p>&#13;
&#13;
<p>A <em>correlation matrix</em> presents the <a data-primary="correlation matrices" data-secondary="defined" data-type="indexterm" id="idm46274546762120"/><a data-primary="correlation matrices" data-secondary="with Excel" data-secondary-sortas="Excel" data-type="indexterm" id="idm46274546761112"/><a data-primary="Data Analysis ToolPak, Excel" data-secondary="correlation matrix with" data-type="indexterm" id="idm46274546759896"/>correlations across all pairs of variables. Let’s build one using the Data Analysis ToolPak. From the ribbon, head&#13;
to Data → Data Analysis → Correlation.</p>&#13;
&#13;
<p>Remember that this is a measure of linear relationship between two&#13;
<em>continuous</em> variables, so we should <a data-primary="categorical variables" data-secondary="about" data-type="indexterm" id="idm46274546757480"/>exclude categorical variables like <em>origin</em> and <a data-primary="discrete variables" data-type="indexterm" id="idm46274546755992"/><a data-primary="variables" data-secondary="discrete" data-type="indexterm" id="idm46274546755256"/>be judicious about including discrete variables like <em>cylinders</em> or <em>model.year</em>. The ToolPak insists on all variables being in a contiguous range, so I will cautiously include <em>cylinders</em>. <a data-type="xref" href="#insert-correlation-matrix-excel">Figure 4-4</a> shows what the ToolPak source menu should look like.</p>&#13;
&#13;
<figure><div class="figure" id="insert-correlation-matrix-excel">&#13;
<img alt="Insert correlation matrix" src="assets/aina_0404.png"/>&#13;
<h6><span class="label">Figure 4-4. </span>Inserting a correlation matrix in Excel</h6>&#13;
</div></figure>&#13;
&#13;
<p>This results in a correlation matrix as shown in <a data-type="xref" href="#correlation-matrix-excel">Figure 4-5</a>.</p>&#13;
&#13;
<figure><div class="figure" id="correlation-matrix-excel">&#13;
<img alt="Correlation matrix" src="assets/aina_0405.png"/>&#13;
<h6><span class="label">Figure 4-5. </span>Correlation matrix in Excel</h6>&#13;
</div></figure>&#13;
&#13;
<p>We can see the –0.83 in cell <code>B6</code>: it’s the intersection of <em>weight</em> and&#13;
<em>mpg</em>. We would also see the same value in cell <code>F2</code>, but Excel left this half of the matrix blank, as it’s redundant information. All values along the diagonal are 1, as any variable&#13;
is perfectly correlated with <a data-primary="correlation coefficient" data-secondary="Pearson" data-type="indexterm" id="idm46274546743992"/><a data-primary="Pearson correlation coefficient" data-type="indexterm" id="idm46274546743016"/>itself.</p>&#13;
<div data-type="warning" epub:type="warning"><h6>Warning</h6>&#13;
<p>The Pearson correlation coefficient is only a suitable measure when the relationship between the two variables is linear.</p>&#13;
</div>&#13;
&#13;
<p>We’ve made a big leap in our assumptions about our variables by&#13;
analyzing their correlations. Can you think of what that is? <em>We assumed their relationship is linear.</em> Let’s check that <a data-startref="ch4_term2" data-type="indexterm" id="idm46274546739736"/>assumption&#13;
with <a data-primary="scatterplots" data-secondary="with Excel" data-secondary-sortas="Excel" data-type="indexterm" id="ch4_term14"/>scatterplots. Unfortunately, there is no way in basic Excel to generate scatterplots&#13;
of each pair of variables at once. For practice, consider plotting them all,&#13;
but let’s try it with the <em>weight</em> and <em>mpg</em> variables. Highlight this data, then head to the ribbon and click Insert → Scatter.</p>&#13;
&#13;
<p>I will add a custom chart title and relabel the axes to aid in interpretation. To change the chart title, double-click on it. To relabel the axes, click the perimeter of the chart and then select the plus sign that appears to expand the <a data-primary="Chart Elements, Excel" data-type="indexterm" id="idm46274546735672"/><a data-primary="Excel" data-secondary="Chart Elements in" data-type="indexterm" id="idm46274546734968"/>Chart Elements menu. (On Mac, click inside the chart and then Chart Design → Add Chart Element.) Select Axis Titles from the menu. <a data-type="xref" href="#weight-mileage-scatter">Figure 4-6</a> shows the resulting scatterplot. It’s not a bad idea to include units of measurement on the axes to help outsiders make sense of the data.</p>&#13;
&#13;
<p><a data-type="xref" href="#weight-mileage-scatter">Figure 4-6</a> looks basically like a <a data-primary="negative correlation of variables" data-type="indexterm" id="idm46274546731480"/><a data-primary="scatterplots" data-secondary="linear/nonlinear relationships in" data-type="indexterm" id="idm46274546730840"/>negative linear relationship, with a greater spread given lower weights and higher mileages. By default, Excel <a data-primary="Excel" data-secondary="visualizations with" data-type="indexterm" id="idm46274546729624"/><a data-primary="visualization of data" data-secondary="with Excel" data-secondary-sortas="Excel" data-type="indexterm" id="idm46274546728680"/><a data-primary="x-axis, mapping" data-type="indexterm" id="idm46274546727464"/><a data-primary="y-axis, mapping" data-type="indexterm" id="idm46274546726792"/>plotted the first variable in our data selection along the x-axis and the&#13;
second along the y-axis. But why not the other way around? Try switching the order&#13;
of these columns in your worksheet so that <em>weight</em> is in column <code>E</code> and <em>mpg</em> in column <code>F</code>, then insert a new scatterplot.</p>&#13;
&#13;
<figure><div class="figure" id="weight-mileage-scatter">&#13;
<img alt="scatterplot of weight and mileage" src="assets/aina_0406.png"/>&#13;
<h6><span class="label">Figure 4-6. </span>Scatterplot of weight and mileage</h6>&#13;
</div></figure>&#13;
&#13;
<p><a data-type="xref" href="#mileage-weight-scatter">Figure 4-7</a> shows a mirror image of the relationship. Excel is a great tool, but as with any tool, you have to tell it&#13;
what to do. Excel will calculate correlations regardless of whether the relationship is linear. It will also make you a scatterplot without&#13;
concern for which variable should go on which axis.</p>&#13;
&#13;
<p>So, which scatterplot is “right?” Does it matter? By <a data-primary="dependent variables" data-secondary="on y-axis" data-secondary-sortas="y-axis" data-type="indexterm" id="idm46274546720360"/><a data-primary="independent variables" data-secondary="on x-axis" data-secondary-sortas="x-axis" data-type="indexterm" id="idm46274546719032"/>convention, the independent variable goes on the x-axis, and&#13;
dependent on the y-axis. Take a moment to consider which is which. If you’re not sure, remember&#13;
that the independent variable is generally the one measured first.</p>&#13;
&#13;
<p>Our independent variable is <em>weight</em> because it was determined by the&#13;
design and building of the car. <em>mpg</em> is the <em>dependent</em> variable&#13;
because we assume it’s affected by the <a data-startref="ch4_term8" data-type="indexterm" id="idm46274546715416"/><a data-startref="ch4_term9" data-type="indexterm" id="idm46274546714712"/><a data-startref="ch4_term10" data-type="indexterm" id="idm46274546714040"/><a data-startref="ch4_term11" data-type="indexterm" id="idm46274546713368"/><a data-startref="ch4_term12" data-type="indexterm" id="idm46274546712696"/><a data-startref="ch4_term13" data-type="indexterm" id="idm46274546712024"/><a data-startref="ch4_term14" data-type="indexterm" id="idm46274546711352"/>car’s weight. This puts <em>weight</em>&#13;
on the x-axis and <em>mpg</em> on the y-axis.</p>&#13;
&#13;
<p>In business <a data-primary="business analytics" data-type="indexterm" id="idm46274546709128"/><a data-primary="data analytics" data-secondary="cautions with" data-type="indexterm" id="idm46274546708392"/>analytics, it is uncommon to have collected data&#13;
just for the sake of statistical analysis; for example, the cars in our&#13;
<em>mpg</em> dataset were built to generate revenue, not for a&#13;
research study on the impact of weight on mileage. Because there are not always clear independent and dependent variables, we&#13;
need to be all the more aware of <em>what</em> these variables are measuring,&#13;
and <em>how</em> they are measured. This is why having some knowledge of the&#13;
domain you are studying, or at least descriptions of your variables and&#13;
how your observations were <a data-startref="ch4_term4" data-type="indexterm" id="idm46274546705560"/><a data-startref="ch4_term6" data-type="indexterm" id="idm46274546704856"/><a data-startref="ch4_term7" data-type="indexterm" id="idm46274546704184"/>collected, is so valuable.</p>&#13;
&#13;
<figure><div class="figure" id="mileage-weight-scatter">&#13;
<img alt="scatterplot of mileage and weight" src="assets/aina_0407.png"/>&#13;
<h6><span class="label">Figure 4-7. </span>Scatterplot of mileage and weight</h6>&#13;
</div></figure>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="From Correlation to Regression" data-type="sect1"><div class="sect1" id="idm46274546834664">&#13;
<h1>From Correlation to Regression</h1>&#13;
&#13;
<p>Though it’s conventional to place the <a data-primary="dependent variables" data-secondary="in linear regression" data-secondary-sortas="linear regression" data-type="indexterm" id="ch4_term15"/><a data-primary="independent variables" data-secondary="in linear regression" data-secondary-sortas="linear regression" data-type="indexterm" id="ch4_term16"/><a data-primary="linear regression" data-secondary="independent/dependent variables in" data-type="indexterm" id="ch4_term17"/>independent variable on the x-axis, it makes no difference to the related correlation coefficient. However, there is a big caveat here, and it relates to the earlier idea of using&#13;
a line to summarize the relationship found by the scatterplot. This practice begins to diverge from <a data-primary="correlation and regression" data-secondary="linear regression" data-type="indexterm" id="ch4_term19"/>correlation, and it’s one you may&#13;
have heard of: <em>linear regression</em>.</p>&#13;
&#13;
<p>Correlation is agnostic to which variable you call&#13;
independent and which you call dependent; that doesn’t factor into its&#13;
definition as&#13;
“the extent to which two variables move together linearly.”</p>&#13;
&#13;
<p>On the other hand, linear regression is inherently affected by this&#13;
relationship as&#13;
“the estimated impact of a unit change of the independent variable <em>X</em> on&#13;
the dependent variable <em>Y</em>.”</p>&#13;
&#13;
<p>You are going to see that the line we fit through our scatterplot&#13;
can be expressed as an equation; unlike the correlation coefficient, this equation&#13;
depends on how we define our independent and dependent&#13;
variables.</p>&#13;
<aside class="pagebreak-before" data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46274546690696">&#13;
<h5>Linear Regression and Linear Models</h5>&#13;
<p>You will often hear linear regression <a data-primary="linear models" data-type="indexterm" id="idm46274546689032"/><a data-primary="linear regression" data-secondary="linear models and" data-type="indexterm" id="idm46274546688328"/>referred to as a linear <em>model</em>, which is just one of many statistical models. Just like a model train you might build, a <a data-primary="statistical models" data-type="indexterm" id="idm46274546686712"/>statistical model serves as a workable approximation of a real-life subject. In particular, we use statistical models to understand the relationship between dependent and independent variables. Models won’t be able to explain <em>everything</em> about what they represent, but that doesn’t mean they can’t help. But as British mathematician George Box famously stated, “All models are wrong, but some are useful.”</p>&#13;
</div></aside>&#13;
&#13;
<p>Like correlation, linear regression assumes that a linear relationship&#13;
exists between the two variables. Other assumptions do exist and are important to consider when modeling&#13;
data. For example, we do not want to have extreme observations that may disproportionately affect the overall trend of the linear relationship.</p>&#13;
&#13;
<p>For the purposes of our demonstration, we will overlook this and other assumptions for now. These assumptions are often difficult to test using Excel; your&#13;
knowledge of statistical programming will serve you well when looking&#13;
into the deeper points of linear regression.</p>&#13;
&#13;
<p>Take a deep breath; it’s time for another <a data-primary="linear regression" data-secondary="equations for" data-type="indexterm" id="idm46274546683000"/><a data-primary="regression equations" data-type="indexterm" id="ch4_term24"/>equation:</p>&#13;
<div data-type="equation" id="linear-regression-equation">&#13;
<h5><span class="label">Equation 4-1. </span>The equation for linear regression</h5>&#13;
<math alttext="upper Y equals beta 0 plus beta 1 times upper X plus epsilon" display="block">&#13;
  <mrow>&#13;
    <mi>Y</mi>&#13;
    <mo>=</mo>&#13;
    <msub><mi>β</mi> <mn>0</mn> </msub>&#13;
    <mo>+</mo>&#13;
    <msub><mi>β</mi> <mn>1</mn> </msub>&#13;
    <mo>×</mo>&#13;
    <mi>X</mi>&#13;
    <mo>+</mo>&#13;
    <mi>ϵ</mi>&#13;
    <mspace width="2.em"/>&#13;
    <mspace width="4pt"/>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
&#13;
<p>The goal of <a data-type="xref" href="#linear-regression-equation">Equation 4-1</a> is to predict our dependent&#13;
variable <em>Y</em>. That’s the left side. You may remember from school that a <a data-primary="intercept of line" data-type="indexterm" id="idm46274546669272"/><a data-primary="slope in linear regression" data-type="indexterm" id="ch4_term20"/>line can be broken into its&#13;
<em>intercept</em> and <em>slope</em>. That’s where <math alttext="beta 0">&#13;
  <msub><mi>β</mi> <mn>0</mn> </msub>&#13;
</math> and&#13;
<math alttext="beta 1 times upper X Subscript i">&#13;
  <mrow>&#13;
    <msub><mi>β</mi> <mn>1</mn> </msub>&#13;
    <mo>×</mo>&#13;
    <msub><mi>X</mi> <mi>i</mi> </msub>&#13;
  </mrow>&#13;
</math>, respectively, come in. In the second term, we multiply&#13;
our independent <a data-primary="coefficient of slope" data-type="indexterm" id="idm46274546660424"/>variable by a slope <em>coefficient</em>.</p>&#13;
&#13;
<p>Finally, there will be a part of the relationship between our&#13;
independent and dependent variable that can be explained not by the model&#13;
per se but by some external influence. This is <a data-primary="linear regression" data-secondary="error in" data-type="indexterm" id="idm46274546658664"/>known as the&#13;
model’s <em>error</em> and is indicated by <math alttext="epsilon Subscript i">&#13;
  <msub><mi>ε</mi> <mi>i</mi> </msub>&#13;
</math>.</p>&#13;
&#13;
<p>Earlier we used the independent samples t-test to examine a significant difference in means between two groups. Here, we are measuring the linear influence of one <a data-primary="continuous variables" data-secondary="relationships between" data-type="indexterm" id="idm46274546654328"/><a data-primary="hypothesis testing" data-secondary="for linear regression" data-secondary-sortas="linear regression" data-type="indexterm" id="ch4_term21"/><a data-primary="linear regression" data-secondary="hypothesis testing for" data-type="indexterm" id="ch4_term22"/>continuous variable&#13;
on another. We will do this by examining whether the <em>slope</em> of the fit regression line is statistically different than zero. That means our hypothesis <a data-primary="alternative (Ha) hypothesis" data-type="indexterm" id="idm46274546650024"/><a data-primary="H0, null hypothesis" data-type="indexterm" id="idm46274546649352"/><a data-primary="Ha, alternative hypothesis" data-type="indexterm" id="idm46274546648680"/><a data-primary="null (H0) hypothesis" data-type="indexterm" id="idm46274546647992"/>test will work something like this:</p>&#13;
<blockquote class="pagebreak-before">&#13;
<p>H0: There is no linear influence of our independent variable on our dependent variable. (The slope of the regression line equals zero.)</p>&#13;
&#13;
<p>Ha: There is a linear influence of our independent variable on our dependent variable. (The slope of the regression line does not equal zero.)</p></blockquote>&#13;
&#13;
<p><a data-type="xref" href="#hypothesis-test">Figure 4-8</a> shows some <a data-startref="ch4_term15" data-type="indexterm" id="idm46274546644536"/><a data-startref="ch4_term16" data-type="indexterm" id="idm46274546643832"/><a data-startref="ch4_term17" data-type="indexterm" id="idm46274546643160"/>examples of what significant and insignificant slopes might look like.</p>&#13;
&#13;
<p>Remember, we don’t have <em>all</em> the data, so we don’t know what the “true” slope would be for the <a data-startref="ch4_term3" data-type="indexterm" id="idm46274546641256"/><a data-startref="ch4_term5" data-type="indexterm" id="idm46274546640552"/>population. Instead, we are inferring whether, given our sample, this slope would be statistically different from zero. We can use the same <a data-primary="p-values" data-secondary="methodology for" data-type="indexterm" id="idm46274546639592"/>p-value methodology to estimate the slope’s significance that we did to find the difference&#13;
in means of two <a data-startref="ch4_term20" data-type="indexterm" id="idm46274546638392"/>groups. We will continue to conduct <a data-primary="two-tailed tests" data-type="indexterm" id="idm46274546637592"/><a data-primary="hypothesis testing" data-secondary="two-tailed direction for" data-type="indexterm" id="idm46274546636920"/><a data-primary="confidence intervals" data-secondary="in Excel" data-secondary-sortas="Excel" data-type="indexterm" id="idm46274546635912"/><a data-primary="confidence intervals" data-secondary="for linear regression" data-secondary-sortas="linear regression" data-type="indexterm" id="idm46274546634696"/>two-tailed tests at the 95% confidence interval. Let’s jump into finding the results using Excel.</p>&#13;
&#13;
<figure><div class="figure" id="hypothesis-test">&#13;
<img alt="Regression slope hypothesis" src="assets/aina_0408.png"/>&#13;
<h6><span class="label">Figure 4-8. </span>Regression models with significant and insignificant slopes</h6>&#13;
</div></figure>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Linear Regression in Excel" data-type="sect1"><div class="sect1" id="idm46274546701304">&#13;
<h1>Linear Regression in Excel</h1>&#13;
&#13;
<p>In this <a data-primary="correlation and regression" data-secondary="mpg dataset example for" data-type="indexterm" id="ch4_term25"/><a data-primary="datasets" data-secondary="mpg example" data-type="indexterm" id="ch4_term26"/><a data-primary="linear regression" data-secondary="mpg dataset example for" data-type="indexterm" id="ch4_term27"/><a data-primary="mpg dataset example" data-type="indexterm" id="ch4_term28"/><a data-primary="vehicle mileage (mpg) dataset example" data-type="indexterm" id="ch4_term29"/><a data-primary="Excel" data-secondary="linear regression with" data-type="indexterm" id="ch4_term30"/><a data-primary="hypothesis testing" data-secondary="with Excel" data-secondary-sortas="Excel" data-type="indexterm" id="ch4_term31"/><a data-primary="Excel" data-secondary="hypothesis testing with" data-type="indexterm" id="ch4_term42"/><a data-primary="linear regression" data-secondary="with Excel" data-secondary-sortas="Excel" data-type="indexterm" id="ch4_term32"/>demo of linear regression on the <em>mpg</em> dataset in Excel, we test whether a car’s weight (<em>weight</em>) has a significant influence on its mileage (<em>mpg</em>). That means our <a data-primary="alternative (Ha) hypothesis" data-type="indexterm" id="idm46274546616616"/><a data-primary="H0, null hypothesis" data-type="indexterm" id="idm46274546615816"/><a data-primary="Ha, alternative hypothesis" data-type="indexterm" id="idm46274546615144"/><a data-primary="null (H0) hypothesis" data-type="indexterm" id="idm46274546614456"/>hypotheses will be:</p>&#13;
<blockquote>&#13;
<p>H0: There is no linear influence of weight on mileage.</p>&#13;
&#13;
<p>Ha: There is a linear influence of weight on mileage.</p></blockquote>&#13;
&#13;
<p>Before getting started, it’s a good idea to write out the <a data-primary="linear regression" data-secondary="equations for" data-type="indexterm" id="ch4_term23"/>regression equation using&#13;
the specific variables of interest, which I’ve done in <a data-type="xref" href="#mpg-regression-equation">Equation 4-2</a>:</p>&#13;
<div data-type="equation" id="mpg-regression-equation">&#13;
<h5><span class="label">Equation 4-2. </span>Our regression equation for estimating mileage</h5>&#13;
<math alttext="m p g equals beta 0 plus beta 1 times w e i g h t plus epsilon" display="block">&#13;
  <mrow>&#13;
    <mi>m</mi>&#13;
    <mi>p</mi>&#13;
    <mi>g</mi>&#13;
    <mo>=</mo>&#13;
    <msub><mi>β</mi> <mn>0</mn> </msub>&#13;
    <mo>+</mo>&#13;
    <msub><mi>β</mi> <mn>1</mn> </msub>&#13;
    <mo>×</mo>&#13;
    <mi>w</mi>&#13;
    <mi>e</mi>&#13;
    <mi>i</mi>&#13;
    <mi>g</mi>&#13;
    <mi>h</mi>&#13;
    <mi>t</mi>&#13;
    <mo>+</mo>&#13;
    <mi>ϵ</mi>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
&#13;
<p>Let’s start with visualizing the results of the regression: we already&#13;
have the scatterplot from <a data-type="xref" href="#weight-mileage-scatter">Figure 4-6</a>, now it’s just a matter of <a data-primary="regression line, fitting" data-secondary="with Excel" data-secondary-sortas="Excel" data-type="indexterm" id="idm46274546596616"/>overlaying or “fitting” the regression line onto it. Click on the perimeter of the plot to <a data-primary="Chart Elements, Excel" data-type="indexterm" id="idm46274546595096"/><a data-primary="Excel" data-secondary="Chart Elements in" data-type="indexterm" id="idm46274546594424"/><a data-primary="Format Trendline, Excel" data-type="indexterm" id="idm46274546593480"/>launch the “Chart Elements” menu. Click on “Trendline,” then “More Options” to the side. Click the radio button at the bottom of the “Format Trendline” screen reading “Display Equation on chart.”</p>&#13;
&#13;
<p>Now let’s click on the resulting equation on the graph to add bold formatting and increase its font size to 14. We’ll make the trendline solid&#13;
black and give it a 2.5-point width by clicking on it in the graph, then going to the paint bucket icon at the top of the Format Trendline menu. We now have the making of linear regression. Our scatterplot with trendline <a data-primary="scatterplots" data-secondary="linear/nonlinear relationships in" data-type="indexterm" id="idm46274546591464"/><a data-primary="scatterplots" data-secondary="with Excel" data-secondary-sortas="Excel" data-type="indexterm" id="idm46274546590520"/><a data-primary="Excel" data-secondary="visualizations with" data-type="indexterm" id="idm46274546589304"/><a data-primary="visualization of data" data-secondary="with Excel" data-secondary-sortas="Excel" data-type="indexterm" id="idm46274546588360"/><a data-primary="visualization of data" data-secondary="relationships with" data-type="indexterm" id="idm46274546587144"/>looks like <a data-type="xref" href="#scatter-plot-trendline">Figure 4-9</a>. Excel also includes the <em>regression</em> equation we are looking for from <a data-type="xref" href="#mpg-regression-equation">Equation 4-2</a> to estimate a&#13;
car’s mileage based on its weight.</p>&#13;
&#13;
<figure><div class="figure" id="scatter-plot-trendline">&#13;
<img alt="scatterplot regression equation in Excel" src="assets/aina_0409.png"/>&#13;
<h6><span class="label">Figure 4-9. </span>Scatterplot with trendline and regression equation for the effect of weight on mileage</h6>&#13;
</div></figure>&#13;
&#13;
<p>We can place the <a data-primary="intercept of line" data-type="indexterm" id="idm46274546581528"/><a data-primary="slope in linear regression" data-type="indexterm" id="ch4_term34"/>intercept before the slope in our equation to get <a data-type="xref" href="#excel-fit-regression-equation">Equation 4-3</a>.</p>&#13;
<div data-type="equation" id="excel-fit-regression-equation">&#13;
<h5><span class="label">Equation 4-3. </span><span class="label">Equation 4-3. </span>Our fit regression equation for estimating mileage</h5>&#13;
<math alttext="m p g equals 46.217 minus 0.0076 times w e i g h t" display="block">&#13;
  <mrow>&#13;
    <mi>m</mi>&#13;
    <mi>p</mi>&#13;
    <mi>g</mi>&#13;
    <mo>=</mo>&#13;
    <mn>46.217</mn>&#13;
    <mo>-</mo>&#13;
    <mn>0.0076</mn>&#13;
    <mo>×</mo>&#13;
    <mi>w</mi>&#13;
    <mi>e</mi>&#13;
    <mi>i</mi>&#13;
    <mi>g</mi>&#13;
    <mi>h</mi>&#13;
    <mi>t</mi>&#13;
    <mspace width="2.em"/>&#13;
    <mspace width="4pt"/>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
&#13;
<p>Notice that <a data-primary="linear regression" data-secondary="error in" data-type="indexterm" id="idm46274546567288"/>Excel does not include the error term as part of the regression equation. Now that we’ve fit the regression line, we’ve quantified the difference between what values we expect from the equation and what values are found in the data. This <a data-primary="residuals" data-type="indexterm" id="idm46274546565896"/>difference is known as the <em>residual</em>, and we’ll come back to it later in this chapter. First, we’ll get back to what we set out to do: establish <a data-primary="hypothesis testing" data-secondary="statistical significance in" data-type="indexterm" id="idm46274546564584"/><a data-primary="linear regression" data-secondary="statistical significance in" data-type="indexterm" id="idm46274546563592"/><a data-primary="statistical significance" data-type="indexterm" id="idm46274546562632"/>statistical significance.</p>&#13;
&#13;
<p class="pagebreak-before">It’s great that Excel fit the line for us and gave us the resulting <a data-startref="ch4_term23" data-type="indexterm" id="idm46274546561048"/><a data-startref="ch4_term24" data-type="indexterm" id="idm46274546560264"/>equation. But&#13;
this does <em>not</em> give us enough information to conduct&#13;
the hypothesis test: we <a data-primary="Regression, Excel" data-type="indexterm" id="ch4_term33"/>still don’t know whether the line’s slope is&#13;
statistically different than zero. To get this information, we will <a data-primary="Data Analysis ToolPak, Excel" data-secondary="regression analysis with" data-type="indexterm" id="ch4_term35"/>again use the Analysis ToolPak. From the ribbon, go to Data → Data&#13;
Analysis → Regression. You’ll be asked to select your Y and X ranges; these are your dependent&#13;
and independent variables, respectively. Make sure to indicate that&#13;
your inputs include labels, as shown in <a data-type="xref" href="#regression-settings">Figure 4-10</a>.</p>&#13;
&#13;
<figure><div class="figure" id="regression-settings">&#13;
<img alt="ToolPak regression setup" src="assets/aina_0410.png"/>&#13;
<h6><span class="label">Figure 4-10. </span>Menu settings for deriving a regression with the ToolPak</h6>&#13;
</div></figure>&#13;
&#13;
<p>This results in quite a lot of information, which is shown in <a data-type="xref" href="#regression-output-excel">Figure 4-11</a>. Let’s step &#13;
<span class="keep-together">through it.</span></p>&#13;
&#13;
<p>Ignore the first section in cells <code>A3:B8</code> for now; we will return to it later. Our second section in <code>A10:F14</code> is labeled <a data-primary="analysis of variance (ANOVA)" data-type="indexterm" id="idm46274546549976"/><a data-primary="variance, analysis of (ANOVA)" data-type="indexterm" id="idm46274546549224"/>ANOVA (short for <em>analysis of variance</em>).&#13;
This tells us whether our regression performs significantly better with the <a data-primary="coefficient of slope" data-type="indexterm" id="idm46274546547992"/>coefficient of the slope included versus one with just the <a data-primary="intercept of line" data-type="indexterm" id="idm46274546547160"/><a data-primary="regression model" data-secondary="with Excel" data-secondary-sortas="Excel" data-type="indexterm" id="idm46274546546408"/>intercept.</p>&#13;
&#13;
<figure><div class="figure" id="regression-output-excel">&#13;
<img alt="Regression results" src="assets/aina_0411.png"/>&#13;
<h6><span class="label">Figure 4-11. </span>Regression output</h6>&#13;
</div></figure>&#13;
&#13;
<p><a data-type="xref" href="#intercept-vs-full">Table 4-2</a> spells out what the competing equations are here.</p>&#13;
<table id="intercept-vs-full">&#13;
<caption><span class="label">Table 4-2. </span>Intercept-only versus full regression model</caption>&#13;
<thead>&#13;
<tr>&#13;
<th>Incercept-only model</th>&#13;
<th>Model with coefficients</th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr>&#13;
<td><p><em>mpg</em> = 46.217</p></td>&#13;
<td><p><em>mpg</em> = 46.217 − 0.0076 × <em>weight</em></p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
&#13;
<p>A statistically <a data-primary="hypothesis testing" data-secondary="statistical significance in" data-type="indexterm" id="idm46274546535208"/><a data-primary="linear regression" data-secondary="statistical significance in" data-type="indexterm" id="idm46274546534232"/><a data-primary="statistical significance" data-type="indexterm" id="idm46274546533272"/>significant result indicates that our coefficients do&#13;
improve the model. We can determine the results of the test from the p-value found in cell <code>F12</code>&#13;
of <a data-type="xref" href="#regression-output-excel">Figure 4-11</a>. Remember, this is scientific notation, so read the p-value as 6.01 times 10 to the power of –102: much smaller than 0.05. We can conclude that <em>weight</em> is&#13;
worth keeping as a coefficient in the regression model.</p>&#13;
&#13;
<p>That brings us to the third section in cells <code>A16:I18</code>; here is where we find what we were&#13;
originally looking for. This range contains a lot of information, so&#13;
let’s go column by column starting with the coefficients in cells <code>B17:B18</code>. These should look familiar as the intercept and slope of the line that were given in <a data-type="xref" href="#excel-fit-regression-equation">Equation 4-3</a>.</p>&#13;
&#13;
<p>Next, the <a data-primary="standard error" data-type="indexterm" id="idm46274546527816"/><a data-primary="linear regression" data-secondary="error in" data-type="indexterm" id="idm46274546527080"/>standard error in <code>C17:C18</code>. We talked about this in <a data-type="xref" href="ch03.html#foundations-of-inference">Chapter 3</a>: it’s a measure of variability across repeated samples and in this case can be thought of as a measure of our coefficients’ precision.</p>&#13;
&#13;
<p>We then have what <a data-primary="t Stat, Excel" data-type="indexterm" id="idm46274546524152"/>Excel calls the “t Stat,” otherwise known as the t-statistic or test statistic, in <code>D17:D18</code>; this can be derived by dividing the coefficient&#13;
by the standard error. We can compare it to our <a data-primary="critical values" data-type="indexterm" id="idm46274546522792"/>critical value of 1.96 to establish statistical significance at 95% confidence.</p>&#13;
&#13;
<p>It’s more common, however, to interpret and <a data-primary="p-values" data-secondary="as basis for decisions" data-secondary-sortas="basis for decisions" data-type="indexterm" id="idm46274546521288"/><a data-primary="p-values" data-secondary="in Excel" data-secondary-sortas="Excel" data-type="indexterm" id="idm46274546520040"/><a data-primary="p-values" data-secondary="methodology for" data-type="indexterm" id="idm46274546518824"/>report on the p-value, which gives the same information. We have two p-values to interpret. First, the intercept’s coefficient in <code>E17</code>. This tells us&#13;
whether the intercept is significantly different than zero. The &#13;
<span class="keep-together">significance</span> of the intercept is <em>not</em> part of our hypothesis test, so this information is irrelevant. (This is another good example of why we can’t always take Excel’s output at face value.)</p>&#13;
<div data-type="warning" epub:type="warning"><h6>Warning</h6>&#13;
<p>While most statistical packages (including Excel) report the p-value of the intercept, it’s usually not relevant information.</p>&#13;
</div>&#13;
&#13;
<p>Instead, we want the p-value of <em>weight</em> in cell <code>E18</code>: this is related to the line’s slope. The p-value is well under 0.05, so we <a data-primary="rejection of null, failure of" data-type="indexterm" id="idm46274546513256"/><a data-primary="null, failure to reject" data-type="indexterm" id="idm46274546512536"/>fail to reject the null and conclude that weight does likely&#13;
influence mileage. In other words, the line’s slope is significantly different than zero. Just like with our earlier hypothesis tests, we will shy away from concluding that we’ve “proven” a relationship, or that more weight <em>causes</em> lower mileage. Again, we are <a data-primary="inferential statistics" data-secondary="uncertainty and" data-type="indexterm" id="idm46274546511048"/>making inferences about a <a data-primary="descriptive statistics" data-secondary="for sample versus population" data-secondary-sortas="sample versus population" data-type="indexterm" id="idm46274546509896"/><a data-primary="population versus sample data" data-type="indexterm" id="idm46274546508600"/><a data-primary="sample versus population data" data-type="indexterm" id="idm46274546507912"/>population based on a sample, so uncertainty is inherent.</p>&#13;
&#13;
<p>The output also <a data-primary="confidence intervals" data-secondary="in Excel" data-secondary-sortas="Excel" data-type="indexterm" id="idm46274546506760"/><a data-primary="confidence intervals" data-secondary="for linear regression" data-secondary-sortas="linear regression" data-type="indexterm" id="idm46274546505480"/>gives us the 95% confidence interval for our intercept&#13;
and slope in cells <code>F17:I18</code>. By default, this is stated twice: had we asked for a&#13;
different confidence interval in the input menu, we’d have received both here.</p>&#13;
&#13;
<p>Now that you’re getting the hang of interpreting the regression output,&#13;
let’s try <a data-primary="point estimate" data-type="indexterm" id="idm46274546503096"/>making a <em>point estimate</em> based on the equation line: what&#13;
would we expect the mileage to be for a car weighing 3,021 pounds? Let’s plug it into our <a data-primary="linear regression" data-secondary="equations for" data-type="indexterm" id="idm46274546501608"/><a data-primary="regression equations" data-type="indexterm" id="idm46274546500632"/>regression equation in <a data-type="xref" href="#regression-point-estimate">Equation 4-4</a>:</p>&#13;
<div data-type="equation" id="regression-point-estimate">&#13;
<h5><span class="label">Equation 4-4. </span><span class="label">Equation 4-4. </span>Making a point estimate based on our equation</h5>&#13;
<math alttext="m p g equals 46.217 minus 0.0076 times 3021" display="block">&#13;
  <mrow>&#13;
    <mi>m</mi>&#13;
    <mi>p</mi>&#13;
    <mi>g</mi>&#13;
    <mo>=</mo>&#13;
    <mn>46.217</mn>&#13;
    <mo>-</mo>&#13;
    <mn>0.0076</mn>&#13;
    <mo>×</mo>&#13;
    <mn>3021</mn>&#13;
    <mspace width="2.em"/>&#13;
    <mspace width="4pt"/>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
&#13;
<p>Based on <a data-type="xref" href="#regression-point-estimate">Equation 4-4</a>, we expect a car weighing 3,021&#13;
pounds to get 23.26 miles per gallon. Take a look at the source dataset: there <em>is</em> an observation weighing&#13;
3,021 pounds (Ford Maverick, row <code>101</code> in the dataset) and it gets 18 miles per gallon, not 23.26. <em>What&#13;
gives?</em></p>&#13;
&#13;
<p>This <a data-primary="residuals" data-type="indexterm" id="idm46274546486776"/>discrepancy is the <em>residual</em> that was mentioned earlier: it’s the difference between the values we estimated in the regression equation and those that are found in the actual data. I’ve included this and some other observations in <a data-type="xref" href="#residuals-labeled">Figure 4-12</a>. The <a data-primary="scatterplots" data-secondary="with Excel" data-secondary-sortas="Excel" data-type="indexterm" id="idm46274546484440"/><a data-primary="scatterplots" data-secondary="regression residuals in" data-type="indexterm" id="idm46274546483192"/><a data-primary="Excel" data-secondary="visualizations with" data-type="indexterm" id="idm46274546482248"/><a data-primary="visualization of data" data-secondary="with Excel" data-secondary-sortas="Excel" data-type="indexterm" id="idm46274546481304"/><a data-primary="visualization of data" data-secondary="relationships with" data-type="indexterm" id="idm46274546480088"/>scatterpoints represent what values are actually found in the dataset, and the line represents what values we predicted with the regression.</p>&#13;
&#13;
<p>It stands to reason that we’d be motivated to minimize the difference between these values. Excel and most regression <a data-primary="OLS (ordinary least squares)" data-type="indexterm" id="idm46274546478456"/><a data-primary="ordinary least squares (OLS)" data-type="indexterm" id="idm46274546477736"/>applications use <em>ordinary least squares</em> (OLS) to do this. Our goal in OLS is to <a data-primary="squared residuals" data-type="indexterm" id="idm46274546476504"/>minimize residuals, specifically, the <em>sum of squared residuals</em>, so that both negative and positive residuals are measured equally. The lower the sum of squared residuals, the less of a difference there is between our actual and expected values, and the better our regression equation is at making estimates.</p>&#13;
&#13;
<figure><div class="figure" id="residuals-labeled">&#13;
<img alt="Residuals shown in Excel" src="assets/aina_0412.png"/>&#13;
<h6><span class="label">Figure 4-12. </span>Residuals as the differences between actual and predicted values</h6>&#13;
</div></figure>&#13;
&#13;
<p>We learned from the <a data-primary="p-values" data-secondary="misinterpretations and limitations of" data-type="indexterm" id="idm46274546472264"/><a data-primary="dependent variables" data-secondary="in linear regression" data-secondary-sortas="linear regression" data-type="indexterm" id="ch4_term36"/><a data-primary="independent variables" data-secondary="in linear regression" data-secondary-sortas="linear regression" data-type="indexterm" id="ch4_term37"/><a data-primary="linear regression" data-secondary="independent/dependent variables in" data-type="indexterm" id="ch4_term38"/><a data-primary="variables" data-secondary="linear relationships of" data-type="indexterm" id="ch4_term39"/><a data-primary="linear relationships of variables" data-type="indexterm" id="ch4_term43"/>p-value of our slope that there is a significant&#13;
relationship between independent and dependent <a data-startref="ch4_term34" data-type="indexterm" id="idm46274546464664"/>variables. But this&#13;
does not tell us how <em>much</em> of the variability in our dependent variable&#13;
is explained by our independent variable.</p>&#13;
&#13;
<p>Remember that variability is at the heart of what we study as analysts; variables&#13;
vary, and we want to study <em>why</em> they vary. Experiments let us do that, by understanding the relationship between an&#13;
independent and dependent variable. But we won’t be able to explain everything about our dependent variable with our independent variable. There will <a data-primary="linear regression" data-secondary="error in" data-type="indexterm" id="idm46274546462328"/>always be some unexplained error.</p>&#13;
&#13;
<p><em>R-squared</em>, or the <a data-primary="R-squared (coefficient of determination)" data-type="indexterm" id="idm46274546460328"/><a data-primary="coefficient of determination (R-squared)" data-type="indexterm" id="idm46274546459496"/>coefficient of determination (which <a data-primary="R-square, Excel" data-type="indexterm" id="idm46274546458664"/>Excel refers to as <em>R-square</em>), expresses as a percentage how much variability&#13;
in the dependent variable is explained by our regression model. For example, an R-squared of 0.4 indicates that 40% of variability in Y can be explained by the model. This means that 1 minus R-squared is what variability  <em>can’t</em> be explained by the model. If R-squared is 0.4, then 60% of Y’s variability is unaccounted for.</p>&#13;
&#13;
<p>Excel calculates R-squared for us in the first box of regression output; take a look back to cell <code>B5</code> in <a data-type="xref" href="#regression-output-excel">Figure 4-11</a>. The square root of R-squared is <a data-primary="multiple R, Excel" data-type="indexterm" id="idm46274546454712"/>multiple R, which is also seen in cell <code>B4</code> of the output. Adjusted <a data-primary="adjusted R-square, Excel" data-type="indexterm" id="idm46274546453432"/>R-square (cell <code>B6</code>) is used as a more conservative estimate of R-squared for a model with multiple independent variables. This measure is of interest when <a data-primary="multiple linear regression" data-type="indexterm" id="idm46274546451960"/>conducting <em>multiple</em> linear regression, which is beyond the scope of this book.</p>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46274546450696">&#13;
<h5>Multiple Linear Regression</h5>&#13;
<p>This chapter has <a data-primary="univariate linear regression" data-type="indexterm" id="idm46274546449096"/>focused on <em>univariate</em> linear regression, or the influence of one independent on one dependent variable. It’s also possible to build multiple, or <em>multivariate</em>, regression models to estimate the influence of several independent variables on a dependent variable. These independent variables can include <a data-primary="categorical variables" data-secondary="in multiple linear regression" data-secondary-sortas="multiple linear regression" data-type="indexterm" id="idm46274546447128"/>categorical, not just <a data-primary="continuous variables" data-secondary="relationships between" data-type="indexterm" id="idm46274546445720"/>continuous, variables, interactions between variables, and more. For a <a data-primary="Excel" data-secondary="further reading on" data-type="indexterm" id="idm46274546444616"/><a data-primary="linear regression" data-secondary="further reading on" data-type="indexterm" id="idm46274546443592"/>detailed look at performing more complex linear regression in Excel, check out Conrad Carlberg’s <em>Regression Analysis Microsoft Excel</em> (Que).</p>&#13;
</div></aside>&#13;
&#13;
<p>There are other ways than R-squared to measure the&#13;
performance of regression: <a data-primary="regression model" data-secondary="with Excel" data-secondary-sortas="Excel" data-type="indexterm" id="idm46274546441384"/>Excel includes one of them, the <a data-primary="standard error" data-type="indexterm" id="idm46274546440008"/>standard&#13;
error of the regression, in its output (cell <code>B7</code> in <a data-type="xref" href="#regression-output-excel">Figure 4-11</a>). This measure tells us the average distance that observed values deviate from the regression line. Some <a data-primary="evaluation metrics" data-type="indexterm" id="idm46274546437816"/>analysts prefer this or other measures to R-squared for evaluating regression models, although R-squared remains a dominant choice. Regardless of preferences, the best evaluation often comes from evaluating multiple figures in their proper context, so there’s no need to swear by or swear off any one measure.</p>&#13;
&#13;
<p>Congratulations: you conducted and interpreted a complete <a data-startref="ch4_term19" data-type="indexterm" id="idm46274546436216"/><a data-startref="ch4_term21" data-type="indexterm" id="idm46274546435432"/><a data-startref="ch4_term22" data-type="indexterm" id="idm46274546434760"/><a data-startref="ch4_term25" data-type="indexterm" id="idm46274546434088"/><a data-startref="ch4_term26" data-type="indexterm" id="idm46274546433416"/><a data-startref="ch4_term27" data-type="indexterm" id="idm46274546432744"/><a data-startref="ch4_term28" data-type="indexterm" id="idm46274546432072"/><a data-startref="ch4_term29" data-type="indexterm" id="idm46274546431400"/><a data-startref="ch4_term30" data-type="indexterm" id="idm46274546430728"/><a data-startref="ch4_term31" data-type="indexterm" id="idm46274546430056"/><a data-startref="ch4_term32" data-type="indexterm" id="idm46274546429384"/><a data-startref="ch4_term42" data-type="indexterm" id="idm46274546428712"/>regression analysis.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Rethinking Our Results: Spurious Relationships" data-type="sect1"><div class="sect1" id="idm46274546630552">&#13;
<h1>Rethinking Our Results: Spurious Relationships</h1>&#13;
&#13;
<p>Based on their <a data-primary="correlation and regression" data-secondary="spurious relationships" data-type="indexterm" id="idm46274546426664"/><a data-primary="data analytics" data-secondary="spurious relationships in" data-type="indexterm" id="idm46274546425688"/>temporal ordering and our own logic, it’s nearly absolute in our mileage example that <em>weight</em> should be the independent variable and <em>mpg</em> the dependent. But what happens if we <a data-primary="regression line, fitting" data-secondary="with Excel" data-secondary-sortas="Excel" data-type="indexterm" id="idm46274546423672"/>fit the regression line with these variables reversed? Go ahead and give it a try using the ToolPak. The <a data-primary="linear regression" data-secondary="equations for" data-type="indexterm" id="idm46274546422104"/><a data-primary="regression equations" data-type="indexterm" id="idm46274546421160"/>resulting <a data-startref="ch4_term35" data-type="indexterm" id="idm46274546420360"/>regression equation is shown in <a data-type="xref" href="#weight-mpg-regression-equation">Equation 4-5</a>.</p>&#13;
<div data-type="equation" id="weight-mpg-regression-equation">&#13;
<h5><span class="label">Equation 4-5. </span><span class="label">Equation 4-5. </span>A regression equation to estimate weight based on mileage</h5>&#13;
<math alttext="w e i g h t equals 5101.1 minus 90.571 times m p g" display="block">&#13;
  <mrow>&#13;
    <mi>w</mi>&#13;
    <mi>e</mi>&#13;
    <mi>i</mi>&#13;
    <mi>g</mi>&#13;
    <mi>h</mi>&#13;
    <mi>t</mi>&#13;
    <mo>=</mo>&#13;
    <mn>5101.1</mn>&#13;
    <mo>-</mo>&#13;
    <mn>90.571</mn>&#13;
    <mo>×</mo>&#13;
    <mi>m</mi>&#13;
    <mi>p</mi>&#13;
    <mi>g</mi>&#13;
    <mspace width="2.em"/>&#13;
    <mspace width="4pt"/>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
&#13;
<p>We can flip our independent and dependent variables and get the <a data-primary="correlation coefficient" data-secondary="with regression" data-secondary-sortas="regression" data-type="indexterm" id="idm46274546407240"/>same correlation coefficient. But when we change them for regression, <em>our coefficients change</em>.</p>&#13;
&#13;
<p>Were we to find out that <em>mpg</em> and <em>weight</em> were both influenced simultaneously by some outside variable, then neither of these models would be correct. And this is the same scenario that we’re faced with in ice cream consumption and shark attacks. It’s silly to say that ice cream consumption has any influence on shark attacks, because both of these are influenced by temperature, as <a data-type="xref" href="#ice-cream-shark-attack-confound">Figure 4-13</a> depicts.</p>&#13;
&#13;
<figure><div class="figure" id="ice-cream-shark-attack-confound">&#13;
<img alt="Ice cream versus shark attacks spurious relationship" src="assets/aina_0413.png"/>&#13;
<h6><span class="label">Figure 4-13. </span>Ice cream consumption and shark attacks: a spurious relationship</h6>&#13;
</div></figure>&#13;
&#13;
<p>This is called a <em>spurious</em> relationship. It’s frequently found in data, and it may not be as obvious as this example. Having some domain knowledge of the data you are studying can be invaluable for detecting spurious <a data-startref="ch4_term36" data-type="indexterm" id="idm46274546399656"/><a data-startref="ch4_term37" data-type="indexterm" id="idm46274546398952"/><a data-startref="ch4_term38" data-type="indexterm" id="idm46274546398280"/><a data-startref="ch4_term39" data-type="indexterm" id="idm46274546397608"/><a data-startref="ch4_term43" data-type="indexterm" id="idm46274546396936"/>relationships.</p>&#13;
<div data-type="warning" epub:type="warning"><h6>Warning</h6>&#13;
<p>Variables can be correlated; there could even be evidence of a causal relationship. But the relationship might be driven by some variable you’ve not even accounted for.</p>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Conclusion" data-type="sect1"><div class="sect1" id="idm46274546394680">&#13;
<h1>Conclusion</h1>&#13;
&#13;
<p>Remember this old phrase?</p>&#13;
<blockquote>&#13;
<p>Correlation doesn’t imply causation.</p></blockquote>&#13;
&#13;
<p>Analytics is highly incremental: we usually layer one concept on top of the&#13;
next to build increasingly complex analyses. For example, we’ll always start with <a data-primary="descriptive statistics" data-secondary="for sample versus population" data-secondary-sortas="sample versus population" data-type="indexterm" id="idm46274546391768"/><a data-primary="population versus sample data" data-type="indexterm" id="idm46274546390440"/><a data-primary="sample versus population data" data-type="indexterm" id="idm46274546389752"/>descriptive statistics of the sample before attemping to infer parameters of the population. While <a data-primary="causation, correlation and" data-type="indexterm" id="idm46274546388824"/><a data-primary="correlation and regression" data-secondary="causation, implication of" data-type="indexterm" id="idm46274546388136"/>correlation may not imply causation, causation is built on the foundations of correlation. That means a better way to summarize the relationship might be:</p>&#13;
<blockquote>&#13;
<p>Correlation is a necessary but not sufficient condition for causation.</p></blockquote>&#13;
&#13;
<p>We’ve just scratched the surface of inferential statistics in this and previous chapters. A whole world of tests exists, but all of them use the same framework of  <em>hypothesis testing</em> that we’ve used here. Get this process down, and you’ll be able to test for all sorts of different data relationships.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Advancing into Programming" data-type="sect1"><div class="sect1" id="idm46274546384824">&#13;
<h1>Advancing into Programming</h1>&#13;
&#13;
<p>I hope you’ve seen and agree that Excel is a fantastic tool for&#13;
learning statistics and analytics. You got a hands-on look at the statistical principles that power much of this work, and learned how to explore and test for relationships in real datasets.</p>&#13;
&#13;
<p>That said, <a data-primary="Excel" data-secondary="and programming languages" data-secondary-sortas="programming languages" data-type="indexterm" id="idm46274546382728"/>Excel can have diminishing returns when it comes to more advanced analytics. For example, we’ve been checking for properties like normality and linearity using visualizations; this is a good start, but there are more robust ways to test them (often, in fact, using statistical inference). These techniques often rely on matrix algebra and other computationally intensive operations that can be tedious to derive in Excel. While add-ins are available to make up for these shortcomings, they can be expensive and lack particular features. On the other hand, as open source tools R and Python are free, and include <a data-primary="packages" data-secondary="Python" data-type="indexterm" id="idm46274546380680"/><a data-primary="packages" data-secondary="R" data-type="indexterm" id="idm46274546379736"/><a data-primary="Python programming language" data-secondary="packages" data-type="indexterm" id="idm46274546378792"/><a data-primary="R programming language" data-secondary="packages" data-type="indexterm" id="idm46274546377832"/>many app-like features called <em>packages</em> that serve nearly any use case. This environment will allow you to focus on the conceptual analysis of your data rather than raw computation, but you will need to learn how to program. These tools, and the analytics toolkit in general, will be the focus of <a data-type="xref" href="ch05.html#data-analytics-stack">Chapter 5</a>.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Exercises" data-type="sect1"><div class="sect1" id="idm46274546375144">&#13;
<h1>Exercises</h1>&#13;
&#13;
<p>Practice your <a data-primary="correlation and regression" data-secondary="exercises" data-type="indexterm" id="idm46274546373816"/>correlation and regression chops by <a data-primary="ais dataset example" data-type="indexterm" id="idm46274546372664"/><a data-primary="datasets" data-secondary="ais example" data-type="indexterm" id="idm46274546371992"/>analyzing the <em>ais</em> dataset found in the book repository’s <a href="https://oreil.ly/hazKQ"> <em>datasets</em> folder</a>. This dataset includes height, weight, and blood readings from male and female Australian athletes of different sports.</p>&#13;
&#13;
<p>With the dataset, try the following:</p>&#13;
<ol>&#13;
<li>&#13;
<p>Produce a correlation matrix of the relevant variables in this dataset.</p>&#13;
</li>&#13;
<li>&#13;
<p>Visualize the relationship of <em>ht</em> and <em>wt</em>. Is this a linear relationship? If so, it is negative or positive?</p>&#13;
</li>&#13;
<li>&#13;
<p>Of <em>ht</em> and <em>wt</em>, which do you presume is the independent and dependent variable?</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Is there a significant influence of the independent variable on the dependent variable?</p>&#13;
</li>&#13;
<li>&#13;
<p>What is the slope of your fit regression line?</p>&#13;
</li>&#13;
<li>&#13;
<p>What percentage of the variance in the dependent variable is explained by the independent variable?</p>&#13;
</li>&#13;
</ul>&#13;
</li>&#13;
<li>&#13;
<p>This dataset contains a variable for body mass index, <em>bmi</em>. If you are not familiar with this metric, take a moment to research how it’s calculated. Knowing this, would you want to analyze the relationship between <em>ht</em> and <em>bmi</em>? Don’t hesitate to lean on <a data-startref="ch4_term1" data-type="indexterm" id="idm46274546358056"/>common sense here rather than just statistical reasoning.</p>&#13;
</li>&#13;
&#13;
</ol>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section></body></html>