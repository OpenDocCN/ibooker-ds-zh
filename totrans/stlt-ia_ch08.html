<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <title>chapter-9</title>
  <link href="../../stylesheet.css" rel="stylesheet" type="text/css"/>
 </head>
 <body>
  <div class="readable-text " id="p1"> 
   <h1 class=" readable-text-h1"><span class="chapter-title-numbering"><span class="num-string">9</span></span> An AI-powered trivia game</h1>
  </div>
  <div class="introduction-summary"> 
   <h3>This chapter covers</h3>
   <ul> 
    <li class="readable-text" id="p2">Connecting your app to a Large Language Model (LLM)</li>
    <li class="readable-text" id="p3">Engineering LLM prompts to achieve your desired results</li>
    <li class="readable-text" id="p4">Using Structured Outputs to obtain LLM responses in a custom parsable format</li>
    <li class="readable-text" id="p5">Managing state in a sequential Streamlit app</li>
    <li class="readable-text" id="p6">Using st.data_editor to create editable tables</li>
   </ul>
  </div>
  <div class="readable-text " id="p7"> 
   <p>Creating software is significantly different from what it used to be just a few short years ago. The difference stems from major developments in the field of AI (Artificial Intelligence), which—unless you've been living under a rock—you've probably heard orf.</p>
  </div>
  <div class="readable-text  intended-text" id="p8"> 
   <p>I'm talking, of course, about breakthroughs in LLMs or Large Language Models, and the tremendously exciting possibilities they open up. By processing and generating natural language, LLMs can understand context, answer complex questions, and even write software on their own—all with astonishing fluency. Tasks that once required domain-specific expertise or painstaking programming can now be achieved with just a few well-crafted "prompts".</p>
  </div>
  <div class="readable-text  intended-text" id="p9"> 
   <p>In this chapter, we'll dive into how you can harness the power of LLMs in your own applications, relying on AI prompts and responses to implement product features that would have required highly advanced techniques half a decade ago. Along the way, we'll also discuss how to tune your LLM interactions to get the results you want, and how to do so without burning a hole in your pocket.</p>
  </div>
  <div class="callout-container admonition-block"> 
   <div class="readable-text" id="p10"> 
    <h5 class=" callout-container-h5 readable-text-h5">Note</h5>
   </div>
   <div class="readable-text" id="p11"> 
    <p>The GitHub repo for this book is <a href="https://github.com/aneevdavis/streamlit-in-action">https://github.com/aneevdavis/streamlit-in-action</a>. The chapter_09 folder has this chapter's code and a requirements.txt file with exact versions of all the required Python libraries.</p>
   </div>
  </div>
  <div class="readable-text" id="p12"> 
   <h2 class=" readable-text-h2">9.1 Fact Frenzy: An AI trivia game</h2>
  </div>
  <div class="readable-text " id="p13"> 
   <p>If you watched the game show <em>Jeopardy!</em> as a kid—or as an adult for that matter—you're going to love this chapter. Having grown up outside the US, I was in my thirties before I watched my first episode of the show, but I suffered no dearth of trivia shows to obsess over when I was a boy—<em>Mastermind</em>, <em>Bournvita Quiz Contest</em>, and <em>Kaun Banega Crorepati? </em>(an Indian take on the original British show <em>Who Wants to Be a Millionaire?</em>) were all staples of my youth.</p>
  </div>
  <div class="readable-text  intended-text" id="p14"> 
   <p>Fifteen-year-old me would never forgive adult me if I didn't include at least one trivia app in this book. Fortunately, trivia is a great fit for our first AI-powered Streamlit app, which will generate questions, evaluate answers, and even mix it up with a variety of quizmaster styles, all using artificial intelligence.</p>
  </div>
  <div class="readable-text" id="p15"> 
   <h3 class=" readable-text-h3">9.1.1 Stating the concept and requirements</h3>
  </div>
  <div class="readable-text " id="p16"> 
   <p>Once again, the first step we'll take is to state the concept of the app we want to build.</p>
  </div>
  <div class="callout-container admonition-block"> 
   <div class="readable-text" id="p17"> 
    <h5 class=" callout-container-h5 readable-text-h5">Concept</h5>
   </div>
   <div class="readable-text" id="p18"> 
    <p>Fact Frenzy, a trivia game that asks users a set of trivia questions and evaluates their answers using AI</p>
   </div>
  </div>
  <div class="readable-text " id="p19"> 
   <p>As we've done several times before by this point in the book, the requirements will flesh out this simple idea further.</p>
  </div>
  <div class="readable-text sub-sub-section-heading" id="p20"> 
   <h4 class=" readable-text-h4">Requirements</h4>
  </div>
  <div class="readable-text " id="p21"> 
   <p>Fact Frenzy will:</p>
  </div>
  <ul> 
   <li class="readable-text" id="p22">use an AI model to generate trivia questions</li>
   <li class="readable-text" id="p23">ask these questions to a player</li>
   <li class="readable-text" id="p24">allow the user to enter a free-text response to each question</li>
   <li class="readable-text" id="p25">use AI to evaluate whether the answer is correct, and to provide the correct answer</li>
   <li class="readable-text" id="p26">keep track of a player's score</li>
   <li class="readable-text" id="p27">allow the player to set a difficulty level for questions</li>
   <li class="readable-text" id="p28">offer a variety of quizmaster speaking styles</li>
  </ul>
  <div class="readable-text " id="p29"> 
   <p>While we <em>generally </em>want our requirements to be free of "implementation" language (as discussed in Chapter 3), in this case the entire point of our app is to demonstrate the use of AI—so we definitely need it to use AI models to perform its functions.</p>
  </div>
  <div class="readable-text  intended-text" id="p30"> 
   <p>We've also added a fun element in the form of quizmaster speaking styles—or in other words, mimicking the style of various people while asking questions, something we'd only be able to achieve using AI.</p>
  </div>
  <div class="readable-text sub-sub-section-heading" id="p31"> 
   <h4 class=" readable-text-h4">What's out of scope</h4>
  </div>
  <div class="readable-text " id="p32"> 
   <p>What are we leaving out? While a professional trivia game could be arbitrarily complex, in building Fact Frenzy, we want to create a minimal app to get hands-on practice with topics we haven't encountered before. So we won't focus on any of the following:</p>
  </div>
  <ul> 
   <li class="readable-text" id="p33">persistent storage and retrieval of questions, answers, and scores</li>
   <li class="readable-text" id="p34">creating and managing users</li>
   <li class="readable-text" id="p35">letting users choose particular categories of questions</li>
  </ul>
  <div class="readable-text " id="p36"> 
   <p>Placing the above items out of scope will let us concentrate on things like interacting with large language models or LLMs, state management, and of course, new Streamlit elements.</p>
  </div>
  <div class="readable-text" id="p37"> 
   <h3 class=" readable-text-h3">9.1.2 Visualizing the user experience</h3>
  </div>
  <div class="readable-text " id="p38"> 
   <p>To give ourselves a concrete idea of what Fact Frenzy is about, take a look at a sketch of our proposed UI, displayed in figure 9.1.</p>
  </div>
  <div class="browsable-container figure-container" id="p39">  
   <img src="../Images/09__image001.png" alt="image"/> 
   <h5 class=" figure-container-h5">Figure 9.1 A UI sketch of Fact Frenzy</h5>
  </div>
  <div class="readable-text " id="p40"> 
   <p>The game window shown in the sketch has a two-column layout. The left column has a "New game" button, as well as a couple of settings: Quizmaster and Difficulty.</p>
  </div>
  <div class="readable-text  intended-text" id="p41"> 
   <p>Referring to the last requirement identified in the previous section, the Quizmaster setting is supposed to use AI to mimic speaking styles of various characters. In figure 9.1, the selected value is "Gollum", a character from <em>The Lord of the Rings</em> who speaks in a distinctive, hissing manner using phrases like "my precious."</p>
  </div>
  <div class="readable-text  intended-text" id="p42"> 
   <p>The column on the right shows an AI-generated question "spoken" in Gollum's voice along with the player's score and a box to enter the answer.</p>
  </div>
  <div class="readable-text  intended-text" id="p43"> 
   <p>Gimmicks aside, it's a pretty standard question-and-answer trivia game that lets the player enter free-form text to answer.</p>
  </div>
  <div class="readable-text" id="p44"> 
   <h3 class=" readable-text-h3">9.1.3 Brainstorming the implementation</h3>
  </div>
  <div class="readable-text " id="p45"> 
   <p>While we'll "outsource" large parts of our logic to an LLM, we'll still need to own the overall game flow. Figure 9.2 shows the design we'll work to implement in the rest of the chapter.</p>
  </div>
  <div class="readable-text  intended-text" id="p46"> 
   <p>Unlike some of the other apps we've written where users could take a variety of actions at any point, Fact Frenzy is quite linear. As the diagram shows, the basic logic runs in a loop—using an LLM to retrieve a trivia question, posing it to the player, getting the LLM to evaluate the answer, stating whether the provided answer was right or not, and doing it all over again for the next question, until the game is done.</p>
  </div>
  <div class="browsable-container figure-container" id="p47">  
   <img src="../Images/09__image002.png" alt="image"/> 
   <h5 class=" figure-container-h5">Figure 9.2 The flow of logic in our AI-based trivia game</h5>
  </div>
  <div class="readable-text " id="p48"> 
   <p>Later on, we'll add a few bells and whistles such as allowing the player to set the difficulty level and quizmaster, but figure 9.2 is a pretty good representation of the core flow.</p>
  </div>
  <div class="readable-text" id="p49"> 
   <h2 class=" readable-text-h2">9.2 Using AI to generate trivia questions</h2>
  </div>
  <div class="readable-text " id="p50"> 
   <p>At the heart of Fact Frenzy is an AI model that powers the trivia experience. This model generates questions, evaluates player responses, and even adds personality to the quizmaster. To achieve all this, we use a Large Language Model (LLM)—a powerful AI system designed to process and generate human-like text.</p>
  </div>
  <div class="readable-text  intended-text" id="p51"> 
   <p>LLMs are trained on vast amounts of textual data, making them incredibly versatile. They can perform a wide range of tasks from answering factual questions to generating poetry, coding, or—importantly for our purposes—role-playing as a quizmaster. Popular examples of LLMs include OpenAI's GPT series, Anthropic's Claude, and Google's Gemini—all of which leverage cutting-edge machine learning techniques to generate coherent, contextually appropriate text.</p>
  </div>
  <div class="readable-text" id="p52"> 
   <h3 class=" readable-text-h3">9.2.1 Why use an LLM in Fact Frenzy?</h3>
  </div>
  <div class="readable-text " id="p53"> 
   <p>What makes LLMs suitable for use in our trivia game? To answer this, let's take a minute to consider some possible components of such a game and how an LLM can support building each one:</p>
  </div>
  <div class="readable-text sub-sub-section-heading" id="p54"> 
   <h4 class=" readable-text-h4">A giant list of questions</h4>
  </div>
  <div class="readable-text " id="p55"> 
   <p>Without an LLM, we would need to maintain a set of trivia questions large and diverse enough to keep our app engaging. Since the major LLMs of today are all trained on text and data pertaining to history, culture, geography, astronomy, and every other category of trivia you can think of, they're instead able to generate questions on the fly.</p>
  </div>
  <div class="readable-text sub-sub-section-heading" id="p56"> 
   <h4 class=" readable-text-h4">The ability to evaluate answers</h4>
  </div>
  <div class="readable-text " id="p57"> 
   <p>If we were going the traditional non-LLM route, we would realistically only be able to ask multiple-choice questions, as we'd need to match the answer a player gives to the actual one accurately. LLMs, on the other hand, can interpret and respond to free-text user inputs. This allows us to ask open-ended trivia questions and still evaluate player responses correctly.</p>
  </div>
  <div class="readable-text sub-sub-section-heading" id="p58"> 
   <h4 class=" readable-text-h4">Entertainment value</h4>
  </div>
  <div class="readable-text " id="p59"> 
   <p>In addition to being able to handle factual information, LLMs can also be creative, and provide humor for engagement. Later in the chapter, we'll ask one to mimic the style of various characters as quizmasters, giving the game a personality, so to speak.</p>
  </div>
  <div class="readable-text  intended-text" id="p60"> 
   <p>Thus, in Fact Frenzy, the LLM will play the triple role of question generator, answer evaluator, and comic relief provider. In the next section, we'll set up an account with a top-tier LLM provider—OpenAI—enabling us to start interacting with LLMs for the first time.</p>
  </div>
  <div class="callout-container admonition-block"> 
   <div class="readable-text" id="p61"> 
    <h5 class=" callout-container-h5 readable-text-h5">Note</h5>
   </div>
   <div class="readable-text" id="p62"> 
    <p>While LLMs are undeniably powerful, they are not omniscient. They rely on patterns in their training data to generate responses and may occasionally make mistakes, especially when evaluating highly nuanced or ambiguous answers. However, for our trivia app, these models strike a perfect balance between intelligence, versatility, and entertainment.</p>
   </div>
  </div>
  <div class="readable-text" id="p63"> 
   <h3 class=" readable-text-h3">9.2.2 Setting up an OpenAI API key</h3>
  </div>
  <div class="readable-text " id="p64"> 
   <p>For this chapter, we'll use an LLM provided by OpenAI, perhaps the best-known among the set of new-ish AI firms that have been dominating tech news lately.</p>
  </div>
  <div class="readable-text  intended-text" id="p65"> 
   <p>As we've done many times in this book so far, we'll need to open an account with an external service, and wire our Python code to it.</p>
  </div>
  <div class="readable-text  intended-text" id="p66"> 
   <p>If you don't already have an OpenAI account (your ChatGPT account counts), go to <a href="https://platform.openai.com/">https://platform.openai.com/</a> and sign up for one.</p>
  </div>
  <div class="callout-container admonition-block"> 
   <div class="readable-text" id="p67"> 
    <h5 class=" callout-container-h5 readable-text-h5">Note</h5>
   </div>
   <div class="readable-text" id="p68"> 
    <p>If you created your OpenAI account recently or just now, you <em>may</em> have some free credits applied to your account. However, the free tier currently comes with relatively severe usage limits, such as only being able to call some models three times per minute. While you could <em>technically </em>get away with using the free tier for this chapter (assuming your account has the free credits in the first place), if you plan to do any amount of serious AI development, I recommend upgrading to a paid tier. As you'll see, you can get a lot of LLM usage for fairly low prices. Though you'll likely need to <em>buy</em> $5 in usage credits to get to the lowest paid tier, you won't need to <em>spend</em> much of that in this chapter. For reference, I spent less than 15 cents in credits for all the testing I did while developing this lesson. You can learn more about cost optimization in the sidebar named "Cost considerations".</p>
   </div>
  </div>
  <div class="readable-text " id="p69"> 
   <p>Once you're in, go to the Settings page—at the time of writing, you can do this by clicking the gear icon at the top right corner of the page—and then click on "API keys" in the panel to the left.</p>
  </div>
  <div class="readable-text  intended-text" id="p70"> 
   <p>Create a new secret key and note it down. You can leave all the defaults unchanged (see figure 9.3).</p>
  </div>
  <div class="browsable-container figure-container" id="p71">  
   <img src="../Images/09__image003.png" alt="image"/> 
   <h5 class=" figure-container-h5">Figure 9.3 Creating a secret key in your OpenAI account</h5>
  </div>
  <div class="readable-text " id="p72"> 
   <p>You'll only be able to see your key once, so copy-and-paste it somewhere. This goes without saying, but keep it safe!</p>
  </div>
  <div class="readable-text" id="p73"> 
   <h3 class=" readable-text-h3">9.2.3 Calling the OpenAI API in Python</h3>
  </div>
  <div class="readable-text " id="p74"> 
   <p>Before we start any development on the app, let's make sure we can call the OpenAI API in Python without any issues.</p>
  </div>
  <div class="readable-text  intended-text" id="p75"> 
   <p>To begin, install the OpenAI's Python library with <kbd>pip install openai</kbd>. You could technically call the API via HTTP calls using the <kbd>requests</kbd> module (as we did in Chapter 5), but this library is more convenient to use.</p>
  </div>
  <div class="readable-text  intended-text" id="p76"> 
   <p>Once that's done, open a Python shell and import the <kbd>OpenAI</kbd> class:</p>
  </div>
  <div class="browsable-container listing-container" id="p77"> 
   <div class="code-area-container"> 
    <pre class="code-area">&gt;&gt;&gt; from openai import OpenAI</pre>
   </div>
  </div>
  <div class="readable-text " id="p78"> 
   <p>The <kbd>OpenAI</kbd> class lets you instantiate a client using an API key to make calls to the OpenAI API:</p>
  </div>
  <div class="browsable-container listing-container" id="p79"> 
   <div class="code-area-container"> 
    <pre class="code-area">&gt;&gt;&gt; client = OpenAI(api_key='sk-proj-...')</pre>
   </div>
  </div>
  <div class="readable-text " id="p80"> 
   <p>Replace <kbd>sk-proj-...</kbd> with the actual API key you copied in the previous step. With a <kbd>client</kbd> object created, let's prepare an instruction to send the LLM:</p>
  </div>
  <div class="browsable-container listing-container" id="p81"> 
   <div class="code-area-container"> 
    <pre class="code-area">&gt;&gt;&gt; messages = [
...     {'role': 'system', 'content': 'You are a helpful programming assistant'},
...     {'role': 'user', 'content': 'Explain what Streamlit is in 10 words or fewer'}
... ]
 </pre>
   </div>
  </div>
  <div class="readable-text " id="p82"> 
   <p>Each request you send to the LLM is called a <em>prompt</em>. A prompt—or at least the type we'll use here—consists of <em>messages</em>. In the code above, we're assembling these into a list. Each message takes the form of a dictionary with two keys: <kbd>role</kbd> and <kbd>content</kbd>.</p>
  </div>
  <div class="readable-text  intended-text" id="p83"> 
   <p><kbd>role</kbd> can be one of <kbd>user</kbd>, <kbd>system</kbd>, and <kbd>assistant</kbd>. We'll explore more examples later, but the value of <kbd>role</kbd> signifies the perspective of the speaker in the conversation:</p>
  </div>
  <ul> 
   <li class="readable-text" id="p84"><kbd>system</kbd> represents instructions or context-setting for the model, such as rules for how it should behave.</li>
   <li class="readable-text" id="p85"><kbd>user</kbd> represents the person interacting with the model (you).</li>
   <li class="readable-text" id="p86"><kbd>assistant</kbd> represents the model's responses—we'll discuss this in the next chapter.</li>
  </ul>
  <div class="readable-text " id="p87"> 
   <p>The prompt we're creating here tells the LLM (in the system message) that it should behave like a helpful programming assistant. The actual instruction we want the LLM to respond to is in the user message: <kbd>Explain what Streamlit is in 10 words or fewer</kbd>.</p>
  </div>
  <div class="readable-text  intended-text" id="p88"> 
   <p>We can now make an actual request to the API:</p>
  </div>
  <div class="browsable-container listing-container" id="p89"> 
   <div class="code-area-container"> 
    <pre class="code-area">completion = client.chat.completions.create(model='gpt-4o-mini', messages=messages)</pre>
   </div>
  </div>
  <div class="readable-text " id="p90"> 
   <p>The OpenAI API has several different endpoints—one for turning audio to text, one for creating images, and so on. The one we'll be using is the <em>chat completions </em>endpoint, and it's used for text generation.</p>
  </div>
  <div class="readable-text  intended-text" id="p91"> 
   <p>Given a list of messages in a conversation, this endpoint is supposed to return what comes next—hence the term <em>completion</em>. OpenAI has a plethora of models we could use, but here we've picked <kbd>gpt-4o-mini</kbd>, which provides a good balance between intelligence, speed, and cost.</p>
  </div>
  <div class="callout-container admonition-block"> 
   <div class="readable-text" id="p92"> 
    <h5 class=" callout-container-h5 readable-text-h5">Note</h5>
   </div>
   <div class="readable-text" id="p93"> 
    <p>While gpt-4o-mini is currently the most suitable model OpenAI offers for our use case, given the speed of developments in the AI space, by the time this book goes to print, we may have newer models that are smarter <em>and</em> cheaper. Keep an eye on OpenAI's pricing page at <a href="https://openai.com/api/pricing/">https://openai.com/api/pricing/</a> to ensure you're using the model that fits best.</p>
   </div>
  </div>
  <div class="readable-text " id="p94"> 
   <p>The above statement should take a few seconds to execute, and will return a <kbd>ChatCompletion</kbd> object. If you like, you can inspect this object by typing just <kbd>completion</kbd> into the shell, but you can access the actual text response we want like so:</p>
  </div>
  <div class="browsable-container listing-container" id="p95"> 
   <div class="code-area-container"> 
    <pre class="code-area">&gt;&gt;&gt; completion.choices[0].message.content
'Streamlit is a framework for building interactive web applications easily.'</pre>
   </div>
  </div>
  <div class="readable-text " id="p96"> 
   <p>I couldn't have said it better myself! That concludes our first programmatic interaction with an LLM. Next, let's build this into our trivia game's code.</p>
  </div>
  <div class="readable-text" id="p97"> 
   <h3 class=" readable-text-h3">9.2.4 Writing an LLM class</h3>
  </div>
  <div class="readable-text " id="p98"> 
   <p>In Chapter 8, we created a <kbd>Database</kbd> class that encapsulated the interaction that our app could have with an external database. We'll follow the same model in this chapter with an <kbd>Llm</kbd> class that handles all communication with the external LLM. This allows us to separate the logistics of talking to the LLM from the rest of our app, making it easier to maintain, test, or even swap it out entirely, without touching the remaining code.</p>
  </div>
  <div class="readable-text  intended-text" id="p99"> 
   <p>We've covered the basics of calling the LLM in the prior section, so all that's left is to put the logic in a class.</p>
  </div>
  <div class="readable-text  intended-text" id="p100"> 
   <p>Create a new file, <kbd>llm.py</kbd> with the content shown in listing 9.1.</p>
  </div>
  <div class="browsable-container listing-container" id="p101"> 
   <h5 class=" listing-container-h5 browsable-container-h5">Listing 9.1 llm.py</h5>
   <div class="code-area-container"> 
    <pre class="code-area">from openai import OpenAI
 
class Llm:
  def __init__(self, api_key):
    self.client = OpenAI(api_key=api_key)
 
  @staticmethod
  def construct_messages(user_msg, sys_msg=None):
    messages = []
    if sys_msg:
      messages.append({"role": "system", "content": sys_msg})
    messages.append({"role": "user", "content": user_msg})
    return messages
 
  def ask(self, user_msg, sys_msg=None):
    messages = self.construct_messages(user_msg, sys_msg)
    completion = self.client.chat.completions.create(
      model="gpt-4o-mini",
      messages=messages
    )
    return completion.choices[0].message.content</pre>
   </div>
  </div>
  <div class="readable-text " id="p102"> 
   <p>(<kbd>chapter_09/in_progress_01/llm.py</kbd> in the GitHub repo)</p>
  </div>
  <div class="readable-text  intended-text" id="p103"> 
   <p>The <kbd>Llm</kbd> class' <kbd>__init__</kbd> simply creates a new OpenAI client object using an API key that's passed to it, assigning this to <kbd>self.client</kbd>.</p>
  </div>
  <div class="readable-text  intended-text" id="p104"> 
   <p>The <kbd>ask</kbd> method is what logic outside the <kbd>Llm</kbd> class will interact with, and returns the LLM's response to our prompt. Its code is essentially the same as what we ran in the Python shell earlier, except that we take in <kbd>user_msg</kbd> and <kbd>sys_msg</kbd> as arguments and put the creation of the <kbd>messages</kbd> list in its own method, called <kbd>construct_messages</kbd>.</p>
  </div>
  <div class="readable-text  intended-text" id="p105"> 
   <p>Since we don't <em>have </em>to pass a system role message—the LLM will try to be helpful anyway—we give <kbd>sys_msg</kbd> a default value of None. <kbd>construct_messages</kbd> takes this fact into account while generating the <kbd>messages</kbd> list. Since this is a utility function that doesn't depend on anything else in the object, we make it a static method by decorating it with <kbd>@staticmethod</kbd>.</p>
  </div>
  <div class="readable-text  intended-text" id="p106"> 
   <p>We'll refine the <kbd>Llm</kbd> class further along in the chapter, but for now, let's move on to writing the code that calls it.</p>
  </div>
  <div class="readable-text" id="p107"> 
   <h3 class=" readable-text-h3">9.2.5 The Game class</h3>
  </div>
  <div class="readable-text " id="p108"> 
   <p>As in Chapter 8, we'll have a single class—appropriately named <kbd>Game</kbd>—that contains all the backend logic that our app's frontend will call directly. This is somewhat analogous to the <kbd>Hub</kbd> class from Chapter 8, though we'll structure <kbd>Game</kbd> differently.</p>
  </div>
  <div class="readable-text  intended-text" id="p109"> 
   <p>For the moment we'll keep it quite simple, as all it needs to do is to pass a prompt to our <kbd>Llm</kbd> class. The initial version of the <kbd>Game</kbd> class that we'll place in <kbd>game.py</kbd> is shown in listing 9.2.</p>
  </div>
  <div class="browsable-container listing-container" id="p110"> 
   <h5 class=" listing-container-h5 browsable-container-h5">Listing 9.2 game.py</h5>
   <div class="code-area-container"> 
    <pre class="code-area">from llm import Llm
 
class Game:
  def __init__(self, llm_api_key):
    self.llm = Llm(llm_api_key)
 
  def ask_llm_for_question(self):
    return self.llm.ask(
      'Ask a trivia question. Do not provide choices or the answer.',
      'You are a quizmaster.'
    )</pre>
   </div>
  </div>
  <div class="readable-text " id="p111"> 
   <p>(<kbd>chapter_09/in_progress_01/game.py</kbd> in the GitHub repo)</p>
  </div>
  <div class="readable-text  intended-text" id="p112"> 
   <p>The initialization of a <kbd>Game</kbd> instance (through <kbd>__init__</kbd>) involves creating an <kbd>Llm</kbd> object by passing it the API key that we'll presumably get from the calling code.</p>
  </div>
  <div class="readable-text  intended-text" id="p113"> 
   <p>The <kbd>ask_llm_for_question</kbd> method passes a simple prompt asking the LLM to generate a trivia question. Notice that the system message now tells the LLM to behave like a quizmaster.</p>
  </div>
  <div class="readable-text  intended-text" id="p114"> 
   <p>The user message instructs the LLM to ask a question, warning it to not provide any choices or reveal the answer.</p>
  </div>
  <div class="readable-text" id="p115"> 
   <h3 class=" readable-text-h3">9.2.6 Calling the Game class in our app</h3>
  </div>
  <div class="readable-text " id="p116"> 
   <p>We can now write a minimal version of our frontend code to test out everything we've done. As usual, our API key needs to be kept secret and safe, so we'll put it in a <kbd>secrets.toml</kbd> file in a new <kbd>.streamlit</kbd> directory, as seen in listing 9.3.</p>
  </div>
  <div class="browsable-container listing-container" id="p117"> 
   <h5 class=" listing-container-h5 browsable-container-h5">Listing 9.3 .streamlit/secrets.toml</h5>
   <div class="code-area-container"> 
    <pre class="code-area">llm_api_key = "sk-proj-..."    #A</pre>
    <div class="code-annotations-overlay-container">
     #A Replace sk-proj-... with your actual API key.
     <br/>
    </div>
   </div>
  </div>
  <div class="readable-text " id="p118"> 
   <p>We've kept <kbd>secrets.toml</kbd> quite simple this time around with a non-nested structure—notice the absence of a section like <kbd>[config]</kbd>. The O in TOML does stand for "obvious" after all.</p>
  </div>
  <div class="readable-text  intended-text" id="p119"> 
   <p>Go ahead and create <kbd>main.py</kbd> (shown in listing 9.4) now.</p>
  </div>
  <div class="browsable-container listing-container" id="p120"> 
   <h5 class=" listing-container-h5 browsable-container-h5">Listing 9.4 main.py</h5>
   <div class="code-area-container"> 
    <pre class="code-area">import streamlit as st
from game import Game
 
game = Game(st.secrets['llm_api_key'])
 
question = game.ask_llm_for_question()
st.container(border=True).write(question)</pre>
   </div>
  </div>
  <div class="readable-text " id="p121"> 
   <p>(<kbd>chapter_09/in_progress_01/main.py</kbd> in the GitHub repo)</p>
  </div>
  <div class="readable-text  intended-text" id="p122"> 
   <p>There's nothing fancy here yet; we simply create a <kbd>Game</kbd> instance called <kbd>game</kbd>, call its <kbd>ask_llm_for_question</kbd> method to generate a trivia question, and write it to the screen.</p>
  </div>
  <div class="readable-text  intended-text" id="p123"> 
   <p>Notice how we've combined <kbd>st.container</kbd> with a border and <kbd>st.write</kbd> into a single statement:</p>
  </div>
  <div class="browsable-container listing-container" id="p124"> 
   <div class="code-area-container"> 
    <pre class="code-area">st.container(border=True).write(question)</pre>
   </div>
  </div>
  <div class="readable-text " id="p125"> 
   <p>Concise and pretty, much like Streamlit itself. Run your app using <kbd>streamlit run main.py</kbd> (as in prior chapters, make sure you first <kbd>cd</kbd> into the directory that contains <kbd>main.py</kbd> so Streamlit can find the <kbd>.streamlit</kbd> directory) to see something like figure 9.4.</p>
  </div>
  <div class="browsable-container figure-container" id="p126">  
   <img src="../Images/09__image004.png" alt="image"/> 
   <h5 class=" figure-container-h5">Figure 9.4 The app can now successfully call the OpenAI API to obtain a trivia question (see chapter_09/in_progress_01 in the GitHub repo for the full code)</h5>
  </div>
  <div class="readable-text " id="p127"> 
   <p>Excellent—Our AI quizmaster can now ask the player a question! Next we'll have it evaluate the player's answer.</p>
  </div>
  <div class="readable-text" id="p128"> 
   <h2 class=" readable-text-h2">9.3 Using AI to evaluate answers</h2>
  </div>
  <div class="readable-text " id="p129"> 
   <p>The prompt we fed GPT-4o mini is a simple one. Vitally, we didn't have to do anything particularly complicated with the output—just display it on the screen as-is. As such, we didn't really care about the <em>format</em> in which the LLM responded.</p>
  </div>
  <div class="readable-text  intended-text" id="p130"> 
   <p>However, evaluating answers presents a slightly different challenge. When a player enters an answer into the app, we need the LLM to tell us two things:</p>
  </div>
  <ul> 
   <li class="readable-text" id="p131">Whether the answer is correct</li>
   <li class="readable-text" id="p132">If not, what the correct answer actually is</li>
  </ul>
  <div class="readable-text " id="p133"> 
   <p>If you've interacted with AI assistants before, this sounds well within their capabilities. But let's examine a practical challenge: how do you reliably parse the LLM's response?</p>
  </div>
  <div class="readable-text  intended-text" id="p134"> 
   <p>For instance, let's say we tell the LLM "Hey, you asked this question: &lt;question&gt;, and the player said the answer was &lt;answer&gt;. Tell me if that's right, or if not, what the correct answer is".</p>
  </div>
  <div class="readable-text  intended-text" id="p135"> 
   <p>The LLM does its thing and responds with:</p>
  </div>
  <div class="readable-text  intended-text" id="p136"> 
   <p>"Incorrect, the answer is actually &lt;correct answer&gt;".</p>
  </div>
  <div class="readable-text  intended-text" id="p137"> 
   <p>What do we do with this reply? Sure, we could display it on the screen, but we probably also need to perform additional actions, like deciding whether or not to increment the player's score. Which means that we need to <em>parse </em>the response to understand whether the answer was right.</p>
  </div>
  <div class="readable-text  intended-text" id="p138"> 
   <p>How do we do that? A naive approach would be to look for the word "incorrect" in the response and mark the answer as being correct or not accordingly.</p>
  </div>
  <div class="readable-text  intended-text" id="p139"> 
   <p>But what if the LLM's answer is actually "Nope, that's not right, the answer is &lt;correct answer&gt;" or even "Hah! It <em>would</em> have been correct if they'd said &lt;correct answer&gt;. But they didn't, so tough luck."</p>
  </div>
  <div class="readable-text  intended-text" id="p140"> 
   <p>The point is that there are tons of creative ways the LLM could answer, and while we would be able to understand them as humans, we need a simple way to determine their meaning in a machine-friendly way.</p>
  </div>
  <div class="readable-text  intended-text" id="p141"> 
   <p>We could request the LLM in our prompt to include the words "correct" or "incorrect" somewhere, but it might still occasionally mess it up. Luckily, there's a better approach.</p>
  </div>
  <div class="readable-text" id="p142"> 
   <h3 class=" readable-text-h3">9.3.1 Structured Outputs</h3>
  </div>
  <div class="readable-text " id="p143"> 
   <p>Being able to parse LLM outputs reliably is a natural concern for developers, so OpenAI has a solution for this called <em>Structured Outputs</em>.</p>
  </div>
  <div class="readable-text  intended-text" id="p144"> 
   <p>Structured Outputs is a feature that ensures that the model will generate a response that adheres to a schema that <em>you</em> provide, making it simple to parse programmatically.</p>
  </div>
  <div class="readable-text  intended-text" id="p145"> 
   <p>For our use case, this means we can request the LLM to provide two structured fields in its response: a boolean field that says whether the provided answer is correct, and the actual correct answer.</p>
  </div>
  <div class="readable-text  intended-text" id="p146"> 
   <p>Let's create this schema as a class named <kbd>AnswerEvaluation</kbd> (listing 9.5). We'll need the third-party <kbd>pydantic</kbd> module to get this working, so install that first with <kbd>pip install pydantic</kbd>.</p>
  </div>
  <div class="browsable-container listing-container" id="p147"> 
   <h5 class=" listing-container-h5 browsable-container-h5">Listing 9.5 answer_evaluation.py</h5>
   <div class="code-area-container"> 
    <pre class="code-area">from pydantic import BaseModel
 
class AnswerEvaluation(BaseModel):
  is_correct: bool
  correct_answer: str</pre>
   </div>
  </div>
  <div class="readable-text " id="p148"> 
   <p>(<kbd>chapter_09/in_progress_02/answer_evaluation.py</kbd> in the GitHub repo)</p>
  </div>
  <div class="readable-text  intended-text" id="p149"> 
   <p><kbd>pydantic</kbd> is a data validation library that uses type hints to ensure that data conforms to a specified type.</p>
  </div>
  <div class="readable-text  intended-text" id="p150"> 
   <p><kbd>BaseModel</kbd>, which we import from <kbd>pydantic</kbd>, is a class that allows you to define a schema and perform data validation. It works well with OpenAI's Structured Outputs.</p>
  </div>
  <div class="readable-text  intended-text" id="p151"> 
   <p>The class we're defining, <kbd>AnswerEvaluation</kbd>, is a <em>subclass</em> of <kbd>BaseModel</kbd>. <em>Subclasses</em> and <em>superclasses</em> are related to the concept of <em>inheritance</em> in object-oriented programming. Explaining inheritance in detail is beyond the scope of this book, but just know that a subclass can <em>inherit</em> functionality and attributes from its superclass, allowing you to reuse code and build on existing functionality without starting from scratch.</p>
  </div>
  <div class="readable-text  intended-text" id="p152"> 
   <p>In this case, <kbd>AnswerEvaluation</kbd> (subclass) inherits the features of <kbd>BaseModel</kbd> (superclass), such as data validation, serialization, and type checking, making it easy to define and work with structured data.</p>
  </div>
  <div class="readable-text  intended-text" id="p153"> 
   <p>The body of <kbd>AnswerEvaluation</kbd> is identical to what you might expect if it were a dataclass instead. Indeed, dataclasses are similar to <kbd>pydantic</kbd>'s <kbd>BaseModel</kbd> except that dataclasses do not provide the complex validations and related functionality that <kbd>BaseModel</kbd> does.</p>
  </div>
  <div class="readable-text  intended-text" id="p154"> 
   <p>Fortunately, we don't really need to worry about the internals of how this works—just note that <kbd>AnswerEvaluation</kbd> has the two fields we spoke of: a boolean <kbd>is_correct</kbd> and a string, <kbd>correct_answer</kbd>.</p>
  </div>
  <div class="readable-text  intended-text" id="p155"> 
   <p>Next, let's modify our <kbd>ask</kbd> function in the <kbd>Llm</kbd> class (<kbd>llm.py</kbd>) to support Structured Outputs:</p>
  </div>
  <div class="browsable-container listing-container" id="p156"> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">from openai import OpenAI
 
class Llm:
  ...
  def ask(self, user_message, sys_message=None, <b>schema=None</b>):
    messages = self.construct_messages(user_message, sys_message)
<span style="mso-fareast-font-family:&quot;Roboto Mono&quot;"></span>
    <b>if schema:</b>
<b>      completion = self.client.beta.chat.completions.parse(</b>
<b>        model="gpt-4o-mini",</b>
<b>        messages=messages,</b>
<b>        response_format=schema</b>
<b>      )</b>
<b>      return completion.choices[0].message.parsed</b>
    <b>else:</b>
      completion = self.client.chat.completions.create(
        model="gpt-4o-mini",
        messages=messages
      )
      return completion.choices[0].message.content</pre>
   </div>
  </div>
  <div class="readable-text " id="p157"> 
   <p>(<kbd>chapter_09/in_progress_02/llm.py</kbd> in the GitHub repo)</p>
  </div>
  <div class="readable-text  intended-text" id="p158"> 
   <p>Here we've added an argument called <kbd>schema</kbd> that's <kbd>None</kbd> by default. If a value is provided (<kbd>if schema</kbd>), we call a different method in our OpenAI client (<kbd>beta.chat.completions.parse</kbd> as opposed to <kbd>chat.completions.create</kbd> from earlier).</p>
  </div>
  <div class="readable-text  intended-text" id="p159"> 
   <p>The first two parameters we pass to this new method are the same as before, but we've added a third one: <kbd>response_format</kbd>, to which we provide the value of <kbd>schema</kbd>.</p>
  </div>
  <div class="readable-text  intended-text" id="p160"> 
   <p>The final value we return is also different: <kbd>completion.choices[0].message.parsed</kbd> rather than <kbd>completion.choices[0].message.content</kbd>.</p>
  </div>
  <div class="readable-text  intended-text" id="p161"> 
   <p>If <kbd>schema</kbd> is not provided, we simply default to the earlier behavior, thereby ensuring that the <kbd>ask</kbd> method can handle both Structured Outputs and regular text.</p>
  </div>
  <div class="readable-text  intended-text" id="p162"> 
   <p>The value we need to pass to <kbd>schema</kbd> is a <em>class—</em>not an instance of the class, but the class <em>itself</em>. The value <em>returned</em> will then be an <em>instance</em> of that class, and will therefore follow the schema. As you may already have guessed, for our use case, we'll pass the <kbd>AnswerEvaluation</kbd> class to <kbd>schema</kbd>.</p>
  </div>
  <div class="readable-text  intended-text" id="p163"> 
   <p>We'll create this calling code in a bit, but first let's create a new LLM prompt asking the model to evaluate a player's answer.</p>
  </div>
  <div class="readable-text  intended-text" id="p164"> 
   <p>During development, you should expect to have to tweak your prompt many times to get better results—in fact, there's a whole field called <em>prompt engineering</em> that has sprung up in recent years.</p>
  </div>
  <div class="readable-text  intended-text" id="p165"> 
   <p>Since our prompts are meaningfully different from our code, let's put them in a different file where we can edit them without touching the rest of the code. We'll name this <kbd>prompts.py</kbd> and give it the contents in listing 9.6.</p>
  </div>
  <div class="browsable-container listing-container" id="p166"> 
   <h5 class=" listing-container-h5 browsable-container-h5">Listing 9.6 prompts.py</h5>
   <div class="code-area-container"> 
    <pre class="code-area">QUESTION_PROMPT = {
  'system': 'You are a quizmaster.',
  'user': 'Ask a trivia question. Do not provide choices or the answer.'
}
 
ANSWER_PROMPT = {
  'system': 'You are an expert quizmaster.',
  'user': '''
    You have asked the following question: {question}
    The player answered the following: {answer}
 
    Evaluate if the answer provided by the player is close enough
    to be correct.
 
    Also, provide the correct answer.
  '''
}</pre>
   </div>
  </div>
  <div class="readable-text " id="p167"> 
   <p>(<kbd>chapter_09/in_progress_02/prompts.py</kbd> in the GitHub repo)</p>
  </div>
  <div class="readable-text  intended-text" id="p168"> 
   <p>Each prompt is structured as a dictionary with keys <kbd>system</kbd> and <kbd>user</kbd>, corresponding to the system and user messages.</p>
  </div>
  <div class="readable-text  intended-text" id="p169"> 
   <p><kbd>QUESTION_PROMPT</kbd> is our prompt from earlier, while <kbd>ANSWER_PROMPT</kbd> is new. Notice that its user message (a Python <em>multi-line string</em> bounded by <kbd>'''</kbd>s if you're wondering about the syntax) contains these lines:</p>
  </div>
  <div class="browsable-container listing-container" id="p170"> 
   <div class="code-area-container"> 
    <pre class="code-area">You have asked the following question: {question}
The player answered the following: {answer}</pre>
   </div>
  </div>
  <div class="readable-text " id="p171"> 
   <p>We're treating <kbd>{question}</kbd> and <kbd>{answer}</kbd> here as variables that we can replace with real values when we send the prompt to the LLM later.</p>
  </div>
  <div class="readable-text  intended-text" id="p172"> 
   <p>Also note the last two lines in this message where we're telling the LLM to evaluate the answer for correctness and <em>also </em>to provide the correct answer. The model is smart enough to interpret this instruction and provide the results in our <kbd>AnswerEvaluation</kbd> schema.</p>
  </div>
  <div class="readable-text  intended-text" id="p173"> 
   <p>Speaking of which, let's actually write the code that passes this schema along with the prompt to fulfill our answer evaluation use case. We'll do this by modifying <kbd>game.py</kbd>:</p>
  </div>
  <div class="browsable-container listing-container" id="p174"> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">from llm import Llm
<b>from prompts import QUESTION_PROMPT, ANSWER_PROMPT</b>
<b>from answer_evaluation import AnswerEvaluation</b>
<span style="mso-fareast-font-family:&quot;Roboto Mono&quot;"></span>
class Game:
  def __init__(self, llm_api_key):
    self.llm = Llm(llm_api_key)
<span style="mso-fareast-font-family:&quot;Roboto Mono&quot;"></span>
<b>  def ask_llm_for_question(self):</b>
<b>    usr_msg, sys_msg = QUESTION_PROMPT['user'], QUESTION_PROMPT['system']</b>
<b>    return self.llm.ask(usr_msg, sys_msg)</b>
<span style="mso-fareast-font-family:&quot;Roboto Mono&quot;"></span>
  <b>def ask_llm_to_evaluate_answer(self, question, answer):</b>
<b>    sys_msg = ANSWER_PROMPT['system']</b>
<b>    user_msg = (</b>
<b>      ANSWER_PROMPT['user']</b>
<b>      .replace('{question}', question)</b>
<b>      .replace('{answer}', answer)</b>
<b>    )</b>
<b>    return self.llm.ask(user_msg, sys_msg, AnswerEvaluation)</b></pre>
   </div>
  </div>
  <div class="readable-text " id="p175"> 
   <p>(<kbd>chapter_09/in_progress_02/game.py</kbd> in the GitHub repo)</p>
  </div>
  <div class="readable-text  intended-text" id="p176"> 
   <p>We've refactored the <kbd>ask_llms_for_question</kbd> method to use our new <kbd>prompts.py</kbd> module.</p>
  </div>
  <div class="readable-text  intended-text" id="p177"> 
   <p>But the main change here is the new function, <kbd>ask_llm_to_evaluate_answer</kbd>, which takes in the originally asked <kbd>question</kbd>, and the <kbd>answer</kbd> provided by the user, plugging these values into the <kbd>{question}</kbd> and <kbd>{answer}</kbd> slots in the user message we discussed a minute ago.</p>
  </div>
  <div class="readable-text  intended-text" id="p178"> 
   <p>This time, we pass in <kbd>AnswerEvaluation</kbd> (imported from <kbd>answer_evaluation.py</kbd>) as a third argument to <kbd>self.llm</kbd>'s ask method—<kbd>schema</kbd>, as I hope you recall. One interesting aspect to this is that we're passing in <kbd>AnswerEvaluation</kbd> <em>itself</em> here—not an <em>instance</em> of <kbd>AnswerEvaluation</kbd>, but the class. In Python, most of the constructs in your code are <em>objects </em>that you can pass around like this, including classes—something that enables powerful and flexible programing patterns.</p>
  </div>
  <div class="readable-text  intended-text" id="p179"> 
   <p>But I digress. Let's get back to enabling our game to accept and evaluate answers from a player. The last step is to make the requisite changes to the frontend in <kbd>main.py</kbd>:</p>
  </div>
  <div class="browsable-container listing-container" id="p180"> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">import streamlit as st
from game import Game
 
game = Game(st.secrets['llm_api_key'])
 
question = game.ask_llm_for_question()
st.container(border=True).write(question)
 
<b>answer = st.text_input("Enter your answer")</b>
<b>if st.button("Submit"):</b>
<b>  evaluation = game.ask_llm_to_evaluate_answer(question, answer)</b>
<b>  if evaluation.is_correct:</b>
<b>    st.success("That's correct!")</b>
<b>  else:</b>
<b>    st.error("Sorry, that's incorrect.")</b>
<b>    st.info(f"The correct answer was: {evaluation.correct_answer}")</b></pre>
   </div>
  </div>
  <div class="readable-text " id="p181"> 
   <p>(<kbd>chapter_09/in_progress_02/main.py</kbd> in the GitHub repo)</p>
  </div>
  <div class="readable-text  intended-text" id="p182"> 
   <p>You should be able to understand what we're doing here pretty easily. Once we've posed the trivia question to the player, we display a text input for their answer, as well as a "Submit" button that when clicked, triggers the <kbd>ask_llm_to_evaluate_answer</kbd> method in <kbd>game.py</kbd>.</p>
  </div>
  <div class="readable-text  intended-text" id="p183"> 
   <p>The resulting value—stored in <kbd>evaluation</kbd>—is an instance of the <kbd>AnswerEvaluation</kbd> class. We use its <kbd>is_correct</kbd> attribute to display the appropriate correct/incorrect message, and <kbd>evaluation.correct_answer</kbd> for the real answer.</p>
  </div>
  <div class="readable-text  intended-text" id="p184"> 
   <p>Try re-running your app now and supplying an answer to the trivia question. Figure 9.5 shows what I got.</p>
  </div>
  <div class="browsable-container figure-container" id="p185">  
   <img src="../Images/09__image005.png" alt="image"/> 
   <h5 class=" figure-container-h5">Figure 9.5 An issue with session state causes our answer to be matched with the wrong question (see chapter_09/in_progress_02 in the GitHub repo for the full code)</h5>
  </div>
  <div class="readable-text " id="p186"> 
   <p>Oops! There are shenanigans afoot! The question I answered "Ottawa" to in the left part of figure 9.5 was "What is the capital of Canada?". But when I press the "Submit" button, the app displays a different question—"What is the largest planet in our solar system?"—and then has the nerve to tell me that "Ottawa" is, in fact, <em>not </em>the right answer to that.</p>
  </div>
  <div class="readable-text  intended-text" id="p187"> 
   <p>You'll notice a similar bait-and-switch in your testing. What's going on? Are our AI overlords toying with us?</p>
  </div>
  <div class="readable-text  intended-text" id="p188"> 
   <p>As it turns out, the LLM is entirely innocent of this alleged mischief. The issue—as we've seen several times before in this book—is related to session state and Streamlit app re-runs. In this particular instance, a click on the "Submit" button triggers a re-run from the <em>top</em>, meaning that before the code under<kbd> if st.button("Submit"):</kbd> can run, <kbd>game.ask_llm_for_question()</kbd> is called once again, resulting in a <em>new </em>question, which is what is passed as the first argument to <kbd>ask_llm_to_evaluate_answer</kbd>, along with "Ottawa" as the second argument pulled from the text input.</p>
  </div>
  <div class="readable-text  intended-text" id="p189"> 
   <p>Well, at the very least, the Structured Outputs part of our code seems to be working, since Jupiter is indeed the largest planet in the solar system, and Ottawa is incorrect.</p>
  </div>
  <div class="readable-text  intended-text" id="p190"> 
   <p>However, to get Fact Frenzy to exhibit the behavior we expect, we'll need to think through state management quite thoroughly in the next section.</p>
  </div>
  <div class="readable-text" id="p191"> 
   <h2 class=" readable-text-h2">9.4 Moving through game states</h2>
  </div>
  <div class="readable-text " id="p192"> 
   <p>In chapters prior, Streamlit's rerun-the-whole-app-each-time model meant that we had to make extensive use of <kbd>st.session_state</kbd> to get the app to remember values. That holds true here as well.</p>
  </div>
  <div class="readable-text  intended-text" id="p193"> 
   <p>Fact Frenzy, however, is <em>sequential</em> in a way that our previous apps weren't. You can think of the desired behavior of our game as moving between various <em>states</em>, taking a different set of actions and displaying different widgets in each. Figure 9.6 illustrates this.</p>
  </div>
  <div class="browsable-container figure-container" id="p194">  
   <img src="../Images/09__image006.png" alt="image"/> 
   <h5 class=" figure-container-h5">Figure 9.6 Our game consists of four sequential states</h5>
  </div>
  <div class="readable-text " id="p195"> 
   <p>The diagram identifies four states that the game can be in:</p>
  </div>
  <ul> 
   <li class="readable-text" id="p196">GET_QUESTION, in which we retrieve a question from the LLM</li>
   <li class="readable-text" id="p197">ASK_QUESTION, where we pose said question to the player</li>
   <li class="readable-text" id="p198">EVALUATE_ANSWER, which involves calling the LLM to evaluate the answer</li>
   <li class="readable-text" id="p199">STATE_RESULT, where we tell the player whether they were right or not</li>
  </ul>
  <div class="readable-text " id="p200"> 
   <p>Each of these actions should only happen while the game is in the state that it corresponds to. Additionally, we need to ensure that each LLM request we're making only happens once per question—because the alternative messes up our logic <em>and</em> costs money.</p>
  </div>
  <div class="readable-text  intended-text" id="p201"> 
   <p>A pattern that we'll find useful in sequential apps such as this one and others you may write in the future is to formally retain a state/status attribute in the app's central class (<kbd>Game</kbd> in our case) coupled with methods to modify it, and to use conditional logic based on this attribute in the frontend to display the right elements on the screen. Figure 9.7 should make what I'm talking about clearer.</p>
  </div>
  <div class="browsable-container figure-container" id="p202">  
   <img src="../Images/09__image007.png" alt="image"/> 
   <h5 class=" figure-container-h5">Figure 9.7 Conditional branching in the frontend based on the game's state</h5>
  </div>
  <div class="readable-text " id="p203"> 
   <p>As you can see from our proposed logic in figure 9.7, the <kbd>Game</kbd> class will have a <kbd>status</kbd> attribute that indicates the state the game is in. We'll read this state in the frontend to branch between the various display elements—whether it's a text input for the answer in the <kbd>ASK_QUESTION</kbd> state, or simply a wait-during-the-LLM-request status element during the <kbd>GET_QUESTION</kbd> and <kbd>EVALUATE_ANSWER</kbd> states.</p>
  </div>
  <div class="readable-text  intended-text" id="p204"> 
   <p>The <kbd>Game</kbd> class also has methods that change its <kbd>status</kbd> attribute, controlling all of this. <kbd>obtain_question</kbd> would presumably get the question from the LLM, but also change the status to <kbd>ASK_QUESTION</kbd> when it's done. <kbd>accept_answer</kbd> would take in the answer from the player and also switch the status to <kbd>EVALUATE_ANSWER</kbd>, and the <kbd>evaluate_answer</kbd> method, in addition to getting the LLM to give us the result, would shift the status to <kbd>STATE_RESULT</kbd>.</p>
  </div>
  <div class="readable-text  intended-text" id="p205"> 
   <p>Let's put all this in action now, starting with the <kbd>Game</kbd> class:</p>
  </div>
  <div class="browsable-container listing-container" id="p206"> 
   <h5 class=" listing-container-h5 browsable-container-h5">Listing 9.7 game.py (modified)</h5>
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">from llm import Llm
from prompts import QUESTION_PROMPT, ANSWER_PROMPT
from answer_evaluation import AnswerEvaluation
 
class Game:
  def __init__(self, llm_api_key):
    self.llm = Llm(llm_api_key)
    <b>self.status = 'GET_QUESTION'</b>
<b></b>
<b>    self.curr_question = None</b>
<b>    self.curr_answer = None</b>
<b>    self.curr_eval = None</b>
<span style="mso-fareast-font-family:&quot;Roboto Mono&quot;"></span>
  def ask_llm_for_question(self):
    usr_msg, sys_msg = QUESTION_PROMPT['user'], QUESTION_PROMPT['system']
    return self.llm.ask(usr_msg, sys_msg)
<span style="mso-fareast-font-family:&quot;Roboto Mono&quot;"></span>
  def ask_llm_to_evaluate_answer<b>(self)</b>:
    sys_msg = ANSWER_PROMPT['system']
    user_msg = (
      ANSWER_PROMPT['user']
      .replace('{question}', <b>self.curr_question</b>)
      .replace('{answer}', <b>self.curr_answer</b>)
    )
    reply = self.llm.ask(user_msg, sys_msg, AnswerEvaluation)
    return reply
<span style="mso-fareast-font-family:&quot;Roboto Mono&quot;"></span>
  <b>def obtain_question(self):</b>
<b>    self.curr_question = self.ask_llm_for_question()</b>
<b>    self.status = 'ASK_QUESTION'</b>
<b>    return self.curr_question</b>
<b></b>
<b>  def accept_answer(self, answer):</b>
<b>    self.curr_answer = answer</b>
<b>    self.status = 'EVALUATE_ANSWER'</b>
<b></b>
<b>  def evaluate_answer(self):</b>
<b>    self.curr_eval = self.ask_llm_to_evaluate_answer()</b>
<b>    self.status = 'STATE_RESULT'</b></pre>
   </div>
  </div>
  <div class="readable-text " id="p207"> 
   <p>(<kbd>chapter_09/in_progress_03/game.py</kbd> in the GitHub repo)</p>
  </div>
  <div class="readable-text  intended-text" id="p208"> 
   <p>As discussed, the first big change here is the inclusion of a <kbd>self.status</kbd> attribute that formally indicates the state of the game. We initialize this to <kbd>GET_QUESTION</kbd> as that's the first sequential state we want.</p>
  </div>
  <div class="readable-text  intended-text" id="p209"> 
   <p>You'll notice we also have three other attributes—<kbd>curr_question</kbd>, <kbd>curr_answer</kbd>, and <kbd>curr_eval</kbd>—to hold the current question, answer, and evaluation within the <kbd>Game</kbd> instance. This is a departure from the earlier version of <kbd>game.py</kbd> where we were handling <kbd>question</kbd> and <kbd>answer</kbd> as variables outside the class. Keeping track of these within the class is better-suited to our new <em>stateful </em>approach.</p>
  </div>
  <div class="readable-text  intended-text" id="p210"> 
   <p>You'll see this reflected in the <kbd>ask_llm_to_evaluate_answer</kbd> method, where we've dispensed with the <kbd>question</kbd> and <kbd>answer</kbd> parameters in favor of the <kbd>self.curr_question</kbd> and <kbd>self.curr_answer</kbd> attributes.</p>
  </div>
  <div class="readable-text  intended-text" id="p211"> 
   <p>Additionally, we've introduced three new methods (also discussed earlier)—<kbd>obtain_question</kbd>, <kbd>accept_answer</kbd>, and <kbd>evaluate_answer</kbd>. <kbd>obtain_question</kbd> and <kbd>evaluate_answer</kbd> are wrappers around <kbd>ask_llm_for_question</kbd> and<kbd> ask_llm_to_evaluate_answer</kbd> respectively, each merely assigning the result to its associated attribute—<kbd>self.curr_question</kbd> or <kbd>self.curr_answer</kbd>—before adding a line to move <kbd>self.status</kbd> to its next value.</p>
  </div>
  <div class="readable-text  intended-text" id="p212"> 
   <p><kbd>accept_answer</kbd> is even simpler; it just sets <kbd>self.answer</kbd> to an answer presumably provided by the player.</p>
  </div>
  <div class="readable-text  intended-text" id="p213"> 
   <p>The second half of the set of changes we need to implement the state management approach we've envisioned lies in <kbd>main.py</kbd>:</p>
  </div>
  <div class="browsable-container listing-container" id="p214"> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">import streamlit as st
from game import Game
 
<b>if 'game' not in st.session_state:</b>
<b>  st.session_state.game = Game(st.secrets['llm_api_key'])</b>
<b>game = st.session_state.game</b>
<span style="mso-fareast-font-family:&quot;Roboto Mono&quot;"></span>
<b>if game.status == 'GET_QUESTION':</b>
<b>  with st.spinner('Obtaining question...') as status:</b>
<b>    question = game.obtain_question()</b>
<b>    st.rerun()</b>
<span style="mso-fareast-font-family:&quot;Roboto Mono&quot;"></span>
<b>elif game.status == 'ASK_QUESTION':</b>
  st.container(border=True).write(game.curr_question)
  answer = st.text_input("Enter your answer")
  if st.button("Submit", type='primary'):
    <b>game.accept_answer(answer)</b>
<b>    st.rerun()</b>
<span style="mso-fareast-font-family:&quot;Roboto Mono&quot;"></span>
<b>elif game.status == 'EVALUATE_ANSWER':</b>
<b>  with st.spinner('Evaluating answer...') as status:</b>
<b>    game.evaluate_answer()</b>
<b>    st.rerun()</b>
<span style="mso-fareast-font-family:&quot;Roboto Mono&quot;"></span>
<b>elif game.status == 'STATE_RESULT':</b>
  if <b>game.curr_eval.is_correct</b>:
    st.success("That's correct!")
  else:
    st.error("Sorry, that's incorrect.")
    st.info(f"The correct answer was: {<b>game.curr_eval.correct_answer</b>}")</pre>
   </div>
  </div>
  <div class="readable-text " id="p215"> 
   <p>(<kbd>chapter_09/in_progress_03/main.py</kbd> in the GitHub repo)</p>
  </div>
  <div class="readable-text  intended-text" id="p216"> 
   <p>We start by placing our <kbd>Game</kbd> instance in <kbd>st.session_state</kbd>, ensuring that we'll be dealing with the same instance across re-runs:</p>
  </div>
  <div class="browsable-container listing-container" id="p217"> 
   <div class="code-area-container"> 
    <pre class="code-area">if 'game' not in st.session_state:
  st.session_state.game = Game(st.secrets['llm_api_key'])
game = st.session_state.game</pre>
   </div>
  </div>
  <div class="readable-text " id="p218"> 
   <p>The last line here is for convenience, enabling us to refer to our <kbd>Game</kbd> instance simply as <kbd>game</kbd>, instead of spelling out <kbd>st.session_state.game</kbd> each time.</p>
  </div>
  <div class="readable-text  intended-text" id="p219"> 
   <p>The remaining code builds out conditional branching based on <kbd>game</kbd>'s <kbd>status</kbd> attribute. Let's briefly consider each such condition:</p>
  </div>
  <div class="browsable-container listing-container" id="p220"> 
   <div class="code-area-container"> 
    <pre class="code-area">if game.status == 'GET_QUESTION':
  with st.spinner('Obtaining question...') as status:
    question = game.obtain_question()
    st.rerun()</pre>
   </div>
  </div>
  <div class="readable-text " id="p221"> 
   <p>The first branch deals with the <kbd>GET_QUESTION</kbd> state. The app handles this state without any interaction from the user, as it merely obtains the question from the LLM. This can take a perceptible amount of time, however, so we display what's known as a <em>status element</em>.</p>
  </div>
  <div class="readable-text  intended-text" id="p222"> 
   <p>A status element is simply a widget that gives some indication of what's going on to the user while a long-running operation happens in the background. Streamlit has several different status elements—<kbd>st.spinner</kbd>, <kbd>st.status</kbd>, <kbd>st.toast</kbd>, <kbd>st.progress</kbd>—each of which has slightly different characteristics.</p>
  </div>
  <div class="readable-text  intended-text" id="p223"> 
   <p><kbd>st.spinner</kbd>, which we've used here, simply displays a spinning circle animation (the same one we saw in Chapter 6 when we applied the <kbd>show_spinner</kbd> parameter to <kbd>@st.cache_data</kbd>) until the background operation has been completed.</p>
  </div>
  <div class="readable-text  intended-text" id="p224"> 
   <p>Notice the <kbd>st.rerun()</kbd> after we've called <kbd>game.obtain_question()</kbd>. This exists because once <kbd>obtain_question</kbd> (from <kbd>game.py</kbd>) has changed the status to <kbd>ASK_QUESTION</kbd>, we need the code to run again, so as to enter the <em>next </em>conditional branch given by <kbd>elif game.status == 'ASK_QUESTION':</kbd>.</p>
  </div>
  <div class="readable-text  intended-text" id="p225"> 
   <p>The remaining branches are quite similar. In each case, there is some kind of trigger causing the app to move to the next state, followed by a rerun. In the <kbd>ASK_QUESTION</kbd> state, a click on "Submit" calls <kbd>game.accept_answer(answer)</kbd>, which sets <kbd>game</kbd>'s <kbd>curr_answer</kbd> attribute and changes the state to <kbd>EVALUATE_ANSWER</kbd>.</p>
  </div>
  <div class="readable-text  intended-text" id="p226"> 
   <p>In <kbd>EVALUATE_ANSWER</kbd>, we call <kbd>game.evaluate_answer()</kbd> and display another <kbd>st.spinner</kbd> while we wait for it to return, eventually changing the status to <kbd>STATE_RESULT</kbd>.</p>
  </div>
  <div class="readable-text  intended-text" id="p227"> 
   <p>After the rerun, we simply display the appropriate messages based on <kbd>game.curr_eval</kbd>, our <kbd>AnswerEvaluation</kbd> object.</p>
  </div>
  <div class="readable-text  intended-text" id="p228"> 
   <p>Check out the results now by rerunning the app (figure 9.8)</p>
  </div>
  <div class="browsable-container figure-container" id="p229">  
   <img src="../Images/09__image008.png" alt="image"/> 
   <h5 class=" figure-container-h5">Figure 9.8 The question-answer mismatch issue has been resolved (see chapter_09/in_progress_03 in the GitHub repo for the full code)</h5>
  </div>
  <div class="readable-text " id="p230"> 
   <p>This time you'll see that the app matches the question and answer correctly, fixing the issue we saw earlier.</p>
  </div>
  <div class="readable-text" id="p231"> 
   <h2 class=" readable-text-h2">9.5 Game mechanics: Keeping score, a New Game button, and Game Over</h2>
  </div>
  <div class="readable-text " id="p232"> 
   <p>Fact Frenzy currently does the bare minimum we need it to do: it asks a player a question, and evaluates the answer, all using AI. It isn't much of a <em>game</em> though; there's no score, and no concept of when the game starts and ends. Let's tackle each of these problems in turn.</p>
  </div>
  <div class="readable-text sub-sub-section-heading" id="p233"> 
   <h4 class=" readable-text-h4">Keeping score</h4>
  </div>
  <div class="readable-text " id="p234"> 
   <p>We want Fact Frenzy to be easy to pick up, so we'll keep our scoring mechanism basic; each correct answer fetches one point.</p>
  </div>
  <div class="readable-text  intended-text" id="p235"> 
   <p>This should be fairly trivial to incorporate into the <kbd>Game</kbd> class:</p>
  </div>
  <div class="browsable-container listing-container" id="p236"> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">...
class Game:
  def __init__(self, llm_api_key):
    self.llm = Llm(llm_api_key)
    ...
 
    <b>self.score = 0</b>
<span style="mso-fareast-font-family:&quot;Roboto Mono&quot;"></span>
  ...
  def evaluate_answer(self):
    self.curr_eval = self.ask_llm_to_evaluate_answer()
    <b>if self.curr_eval.is_correct:</b>
<b>      self.score += 1</b>
    self.status = 'STATE_RESULT'</pre>
   </div>
  </div>
  <div class="readable-text " id="p237"> 
   <p>(<kbd>chapter_09/in_progress_04/game.py</kbd> in the GitHub repo)</p>
  </div>
  <div class="readable-text  intended-text" id="p238"> 
   <p>We simply add a new <kbd>score</kbd> attribute to the <kbd>Game</kbd> instance in <kbd>__init__</kbd>, setting it to 0 to start with.</p>
  </div>
  <div class="readable-text  intended-text" id="p239"> 
   <p>Later, in the <kbd>evaluate_answer</kbd> method, we increment this score by 1 if the LLM determines that the answer is correct.</p>
  </div>
  <div class="readable-text sub-sub-section-heading" id="p240"> 
   <h4 class=" readable-text-h4">Game Over</h4>
  </div>
  <div class="readable-text " id="p241"> 
   <p>Our app doesn't yet have any concept of when the game is over, so let's define this. Again, we'll keep the logic simple: we'll ask a predefined number of questions, and the game is done when all of them have been asked and answered.</p>
  </div>
  <div class="readable-text  intended-text" id="p242"> 
   <p>This involves modifying the <kbd>Game</kbd> class again:</p>
  </div>
  <div class="browsable-container listing-container" id="p243"> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">...
class Game:
  def __init__(self, llm_api_key):
    ...
    self.score = 0
    <b>self.num_questions_completed = 0</b>
<b>    self.max_questions = 1</b>
<b>  </b>
<b>  </b>...
  def evaluate_answer(self):
    self.curr_eval = self.ask_llm_to_evaluate_answer()
    <b>self.num_questions_completed += 1</b>
    if self.curr_eval.is_correct:
      self.score += 1
    self.status = 'STATE_RESULT'
<span style="mso-fareast-font-family:&quot;Roboto Mono&quot;"></span>
  <b>def is_over(self):</b>
<b>    return self.num_questions_completed &gt;= self.max_questions</b></pre>
   </div>
  </div>
  <div class="readable-text " id="p244"> 
   <p>(<kbd>chapter_09/in_progress_04/game.py</kbd> in the GitHub repo)</p>
  </div>
  <div class="readable-text  intended-text" id="p245"> 
   <p>Again, this should be pretty easy to follow. We add two more attributes to the instance in <kbd>__init__</kbd>: <kbd>num_question_completed</kbd> and <kbd>max_questions</kbd> (set to 1 for now since we don't actually support multiple questions yet—that's coming up in the next section).</p>
  </div>
  <div class="readable-text  intended-text" id="p246"> 
   <p>We increment <kbd>num_questions_completed</kbd> by 1 in <kbd>evaluate_answer</kbd>, and add a new method called <kbd>is_over</kbd> that returns <kbd>True</kbd> if the number of questions completed matches or exceeds <kbd>self.max_questions</kbd>.</p>
  </div>
  <div class="readable-text sub-sub-section-heading" id="p247"> 
   <h4 class=" readable-text-h4">A New Game button</h4>
  </div>
  <div class="readable-text " id="p248"> 
   <p>At the moment, Fact Frenzy jumps straight into asking the LLM for a question as soon as the page loads. A "New Game" button would let users trigger the start of a game or take any other action we may want to add later before we engage the LLM.</p>
  </div>
  <div class="readable-text  intended-text" id="p249"> 
   <p>This will primarily affect our frontend code, so let's update <kbd>main.py</kbd> thus:</p>
  </div>
  <div class="browsable-container listing-container" id="p250"> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">import streamlit as st
from game import Game
 
<b>def start_new_game():</b>
<b>  st.session_state.game = Game(st.secrets['llm_api_key'])</b>
<b>  st.rerun()</b>
<b></b>
<b>def new_game_button(game):</b>
<b>  if game and not game.is_over():</b>
<b>    button_text, button_type = "Restart game", "secondary"</b>
<b>  else:</b>
<b>    button_text, button_type = "Start new game", "primary"</b>
<b>  if st.button(button_text, use_container_width=True, type=button_type):</b>
<b>    start_new_game()</b>
<span style="mso-fareast-font-family:&quot;Roboto Mono&quot;"></span>
<b>game = st.session_state.game if 'game' in st.session_state else None</b>
<b>side_col, main_col = st.columns([2, 3])</b>
<b>with side_col:</b>
<b>  st.header("</b>⚡ Fact Frenzy", divider='gray')<span style="mso-fareast-font-family:&quot;Roboto Mono&quot;;mso-bidi-font-family:&quot;Times New Roman&quot;"></span>
<b>  new_game_button(game)</b>
<span style="mso-fareast-font-family:&quot;Roboto Mono&quot;"></span>
<b>with main_col:</b>
<b>  if game:</b>
<b>    st.header("Question", divider='gray')</b>
    if game.status == 'GET_QUESTION':
      ...
<span style="mso-fareast-font-family:&quot;Roboto Mono&quot;"></span>
    ...
    elif game.status == 'STATE_RESULT':
      if game.curr_eval.is_correct:
        st.success("That's correct!")
      else:
        st.error("Sorry, that's incorrect.")
        st.info(f"The correct answer was: {game.curr_eval.correct_answer}")
<span style="mso-fareast-font-family:&quot;Roboto Mono&quot;"></span>
      <b>if game.is_over():</b>
<b>        with st.container(border=True):</b>
<b>          st.markdown(f"Game over! Your final score is: **{game.score}**")</b></pre>
   </div>
  </div>
  <div class="readable-text " id="p251"> 
   <p>(<kbd>chapter_09/in_progress_04/main.py</kbd> in the GitHub repo)</p>
  </div>
  <div class="readable-text  intended-text" id="p252"> 
   <p>There are several changes to highlight here. Firstly, there are two new functions: <kbd>start_new_game</kbd> and <kbd>new_game_button</kbd>, which we'll look into in a second.</p>
  </div>
  <div class="readable-text  intended-text" id="p253"> 
   <p>Since it's now possible for a game to not have started yet—before the "New Game" button is clicked, we allow for game to be <kbd>None</kbd> if it hasn't been added to <kbd>st.session_state</kbd> yet:</p>
  </div>
  <div class="browsable-container listing-container" id="p254"> 
   <div class="code-area-container"> 
    <pre class="code-area">game = st.session_state.game if 'game' in st.session_state else None</pre>
   </div>
  </div>
  <div class="readable-text " id="p255"> 
   <p>We've also changed the layout of the app to have two columns: a side column (<kbd>side_col</kbd>) and a main one (<kbd>main_col</kbd>):</p>
  </div>
  <div class="browsable-container listing-container" id="p256"> 
   <div class="code-area-container"> 
    <pre class="code-area">side_col, main_col = st.columns([2, 3])</pre>
   </div>
  </div>
  <div class="readable-text " id="p257"> 
   <p>This side column could have simply been an <kbd>st.sidebar</kbd>, but in a later section, it'll turn out we need this column to have a higher width than <kbd>st.sidebar</kbd> offers by default.</p>
  </div>
  <div class="readable-text  intended-text" id="p258"> 
   <p>Anyway, <kbd>side_col</kbd> contains a header introducing Fact Frenzy, and a call to <kbd>new_game_button</kbd>:</p>
  </div>
  <div class="browsable-container listing-container" id="p259"> 
   <div class="code-area-container"> 
    <pre class="code-area">with side_col:
  st.header("⚡ Fact Frenzy", divider='gray')
  new_game_button(game)</pre>
   </div>
  </div>
  <div class="readable-text " id="p260"> 
   <p>In Chapter 8, we used the Material library to display icons. Here, we've given Fact Frenzy a lightning-bolt icon using a different approach: by pasting an emoji into our code. We're able to do this because emoji are part of the <em>Unicode</em> standard, which defines a consistent way to represent text and symbols across different systems and platforms. Whenever you want to add an emoji, you can search for it on a website like <kbd>emojipedia.org</kbd> and copy it.</p>
  </div>
  <div class="readable-text  intended-text" id="p261"> 
   <p>The <kbd>new_game_button</kbd> function is defined thus:</p>
  </div>
  <div class="browsable-container listing-container" id="p262"> 
   <div class="code-area-container"> 
    <pre class="code-area">def new_game_button(game):
  if game and not game.is_over():
    button_text, button_type = "Restart game", "secondary"
  else:
    button_text, button_type = "Start new game", "primary"
  if st.button(button_text, use_container_width=True, type=button_type):
    start_new_game()</pre>
   </div>
  </div>
  <div class="readable-text " id="p263"> 
   <p>In essence, we're checking if the game is already in progress—<kbd>if game and not game.is_over()</kbd> determines that <kbd>game</kbd> doesn't have the value <kbd>None</kbd> and that its <kbd>is_over</kbd> method returns <kbd>False</kbd>—and displaying a different button according to the result.</p>
  </div>
  <div class="readable-text  intended-text" id="p264"> 
   <p>We vary two characteristics of the button—its text and its type. The text says "Restart game" if the game is in progress, and "Start new game" if it's not.</p>
  </div>
  <div class="readable-text  intended-text" id="p265"> 
   <p>How about the button's <kbd>type</kbd> parameter? We may have given it a value in previous chapters, but let's examine it more thoroughly now. The three values <kbd>type</kbd> can take are <kbd>primary</kbd>, <kbd>secondary</kbd>, and <kbd>tertiary</kbd>—each indicating how prominent the button should be. A button with type <kbd>primary</kbd> has a solid color (usually orange) with white text, a <kbd>secondary</kbd> button—the default if you don't specify a type—is white with solid-colored text, while a <kbd>tertiary</kbd> button is more subtle and appears as regular text without a border.</p>
  </div>
  <div class="readable-text  intended-text" id="p266"> 
   <p>In UI design, it's a good practice to guide the user towards the "correct" or most likely action they might want to take at any given point—it makes for a more intuitive design. If the game hasn't started yet, the choice that makes the most sense is to click the "Start new game" button, so we give it a type of <kbd>primary</kbd>. If the game is in progress, the default action should be to answer the question, not to restart the game. Therefore, while we make that possibility available, we don't overly emphasize it.</p>
  </div>
  <div class="readable-text  intended-text" id="p267"> 
   <p>These differences in the button are mostly cosmetic, however. In either case, a click issues a call to <kbd>start_new_game</kbd>, which has the following code:</p>
  </div>
  <div class="browsable-container listing-container" id="p268"> 
   <div class="code-area-container"> 
    <pre class="code-area">def start_new_game():
  st.session_state.game = Game(st.secrets['llm_api_key'])
  st.rerun()</pre>
   </div>
  </div>
  <div class="readable-text " id="p269"> 
   <p>As before, we create a <kbd>Game</kbd> instance and assign it to <kbd>st.session_state.game</kbd>. Since the presence of <kbd>game</kbd> in <kbd>st.session_state</kbd> changes what should be displayed on the screen, we also issue an <kbd>st.rerun()</kbd>.</p>
  </div>
  <div class="readable-text  intended-text" id="p270"> 
   <p>By wrapping this logic in a function, we're preventing it from executing by default, instead requiring the New Game button to actually be clicked.</p>
  </div>
  <div class="readable-text  intended-text" id="p271"> 
   <p>The main column of the game—<kbd>main_col</kbd>—is, of course, where the content is meant to be displayed. In this iteration of <kbd>main.py</kbd>, we've simply moved the widgets we had before into <kbd>main_col</kbd>. There are a few additions worth highlighting though.</p>
  </div>
  <div class="readable-text  intended-text" id="p272"> 
   <p>If <kbd>game</kbd> is <kbd>None</kbd>—which means no game has started yet—we want the main column to be completely blank so the player's attention is focused on <kbd>side_col</kbd>. This explains why the code under with <kbd>main_col</kbd> starts with <kbd>if game</kbd>:</p>
  </div>
  <div class="browsable-container listing-container" id="p273"> 
   <div class="code-area-container"> 
    <pre class="code-area">with main_col:
  if game:
    st.header("Question", divider='gray')
    if game.status == 'GET_QUESTION':
      ...</pre>
   </div>
  </div>
  <div class="readable-text " id="p274"> 
   <p>We've also added a header that just says "Question." We'll update this later to show the question number.</p>
  </div>
  <div class="readable-text  intended-text" id="p275"> 
   <p>Finally, we've added some logic to handle the case of the game being over under the <kbd>STATE_RESULT</kbd> state:</p>
  </div>
  <div class="browsable-container listing-container" id="p276"> 
   <div class="code-area-container"> 
    <pre class="code-area">elif game.status == 'STATE_RESULT':
      ...
      if game.is_over():
        with st.container(border=True):
          st.markdown(f"Game over! Your final score is: **{game.score}**")</pre>
   </div>
  </div>
  <div class="readable-text " id="p277"> 
   <p>This should be quite clear. We use the <kbd>is_over</kbd> method we defined earlier to check if the game is done, and show an appropriate message and the final score (<kbd>game.score</kbd>) if so.</p>
  </div>
  <div class="readable-text  intended-text" id="p278"> 
   <p>That concludes another iteration of our code. Go ahead and re-run your app to get figure 9.9.</p>
  </div>
  <div class="browsable-container figure-container" id="p279">  
   <img src="../Images/09__image009.png" alt="image"/> 
   <h5 class=" figure-container-h5">Figure 9.9 Keeping score, a New Game button, and Game Over (see chapter_09/in_progress_04 in the GitHub repo for the full code)</h5>
  </div>
  <div class="readable-text " id="p280"> 
   <p>Sweet! Fact Frenzy is starting to look pretty slick. Adding support for multiple questions is next!</p>
  </div>
  <div class="readable-text" id="p281"> 
   <h2 class=" readable-text-h2">9.6 Including multiple questions</h2>
  </div>
  <div class="readable-text " id="p282"> 
   <p>In the previous section, we introduced key game mechanics to our app—adding a score system and defining the game's start and end—to make it feel more like a real game.</p>
  </div>
  <div class="readable-text  intended-text" id="p283"> 
   <p>Fact Frenzy still only asks one question though, so it's not much of one yet. It's time to change that. But before we do, let's explore an LLM-related challenge we'll face.</p>
  </div>
  <div class="readable-text" id="p284"> 
   <h3 class=" readable-text-h3">9.6.1 Response variability, or lack thereof</h3>
  </div>
  <div class="readable-text " id="p285"> 
   <p>In many ways, an LLM is like a black box. Unlike a piece of "regular" code that tends to be deterministic—meaning that the same input always produces the same output—LLMs are <em>probabilistic</em>, which means you may get different responses for the same input (or similar inputs), based on a set of probabilities.</p>
  </div>
  <div class="readable-text  intended-text" id="p286"> 
   <p>Depending on what you're trying to achieve, this variability can be a good or a bad thing. For example, if you're trying to get an LLM to generate poetry, you may want a fairly high amount of creativity or variability in the response, whereas if you're evaluating a mathematical equation, you want less.</p>
  </div>
  <div class="readable-text  intended-text" id="p287"> 
   <p>Vendors like OpenAI generally expose a few controls for this variability, making it easier to manage, but often we need to engineer the prompt to extract the behavior we want from the model.</p>
  </div>
  <div class="readable-text  intended-text" id="p288"> 
   <p>For our use case of generating questions, we want relatively high variability. If you've played around with our current app for a while, you may have noticed that the questions we get from the LLM are often repeated. In my testing, for instance, the model had a particular fondness for asking about the only planet in the solar system that rotates on its side.</p>
  </div>
  <div class="readable-text  intended-text" id="p289"> 
   <p>This won't work for us. For one thing, if a single game includes multiple questions, all of them <em>must</em> be unique. Secondly, even if a particular question isn't asked twice in the same game, we don't want it to repeat too often across <em>different</em> games either.</p>
  </div>
  <div class="readable-text  intended-text" id="p290"> 
   <p>Let's take a look at a few ways in which we can control variability in the LLM's answer.</p>
  </div>
  <div class="callout-container admonition-block"> 
   <div class="readable-text" id="p291"> 
    <h5 class=" callout-container-h5 readable-text-h5">Note</h5>
   </div>
   <div class="readable-text" id="p292"> 
    <p>One consequence of the fact that LLMs generate text based on probabilistic patterns (rather than an understanding of facts) is what we call "hallucinations"—instances where the model produces outputs that are plausible-sounding but factually incorrect or entirely fabricated. These hallucinations arise because LLMs rely on the statistical relationships in their training data, which can sometimes lead to confident but misleading responses. Strategies exist to reduce the likelihood of hallucinations, such as enabling LLMs to connect to external sources of information, but there's no way to guarantee that they won't occur at all. Dealing with hallucinations is outside the scope of this chapter-~-we'll tackle supplementing an LLM's knowledge base with our own sources in the next one. Just be aware that our app may occasionally produce a question or answer that isn't factual. Fortunately, based on my testing, these occurrences tend to be infrequent.</p>
   </div>
  </div>
  <div class="readable-text sub-sub-section-heading" id="p293"> 
   <h4 class=" readable-text-h4">Varying temperature and top_p</h4>
  </div>
  <div class="readable-text " id="p294"> 
   <p>The prompts we send to a large language model and the response we get back from it both consist of <em>tokens</em>. A token may be a single word or it may be part of a word.</p>
  </div>
  <div class="readable-text  intended-text" id="p295"> 
   <p>At its heart, an LLM constructs the response to a prompt step-by-step, or rather, token-by-token. In each step, it considers a wide range of possibilities for the next token to include in its response, assigning a <em>probability</em> of being picked (from high school math, a number between 0 and 1) to each token option.</p>
  </div>
  <div class="readable-text  intended-text" id="p296"> 
   <p>These tokens form what's known as a <em>probability distribution</em>—think of it as a curve that represents the likelihood of each token being the next one, with the more likely tokens plotted at a higher place on the curve than the less likely ones.</p>
  </div>
  <div class="readable-text  intended-text" id="p297"> 
   <p>OpenAI offers two parameters—<kbd>temperature</kbd> and <kbd>top_p</kbd>—that can adjust the composition of this curve. Figure 9.10 illustrates the effect of varying these parameters</p>
  </div>
  <div class="browsable-container figure-container" id="p298">  
   <img src="../Images/09__image010.png" alt="image"/> 
   <h5 class=" figure-container-h5">Figure 9.10 How temperature and top_p affect the creativity and predictability of an LLM's output</h5>
  </div>
  <div class="readable-text " id="p299"> 
   <p><kbd>temperature</kbd> can take values from 0 to 2, with a higher value serving to make the curve flatter, and a lower value making it more pronounced. A higher temperature thus tends to "even" out the curve, increasing the probability that some of the less likely token options may be picked, which makes the LLM take more "risks" and increases the overall creativity of its response.</p>
  </div>
  <div class="readable-text  intended-text" id="p300"> 
   <p><kbd>top_p</kbd> can go from 0 to 1. It represents a cutoff for the cumulative probability of the tokens that the model will choose from. To take an example, the LLM may determine that the five most likely next tokens in its response and their respective probabilities are: "but": 0.6, "then": 0.2, "a": 0.1, "this": 0.06, and "that": 0.02—with all other tokens having much lower probabilities.</p>
  </div>
  <div class="readable-text  intended-text" id="p301"> 
   <p>A <kbd>top_p</kbd> of 0.8 would mean that the model should only choose between the most likely tokens with a combined probability of at least 0.8. In this case, since "but" and "then" together cover a probability of 0.6 + 0.2 &gt;= 0.8, the model discards everything else.</p>
  </div>
  <div class="readable-text  intended-text" id="p302"> 
   <p>A <kbd>top_p</kbd> of 0.95, on the other hand, would require the model to also consider "a" and "this" to cover the required cumulative probability (0.6 + 0.2 + 0.1 + 0.06 &gt;= 0.95).</p>
  </div>
  <div class="readable-text  intended-text" id="p303"> 
   <p>A higher <kbd>top_p</kbd>, therefore, means that the LLM will consider more token options, potentially reducing the predictability and coherence of the response, but increasing its diversity.</p>
  </div>
  <div class="readable-text  intended-text" id="p304"> 
   <p>Getting back to our original goal of generating a variety of questions, we probably want a moderately high <kbd>temperature</kbd>—say 0.7 to 0.8—and a relatively high <kbd>top_p</kbd> of, say, 0.9.</p>
  </div>
  <div class="readable-text sub-sub-section-heading" id="p305"> 
   <h4 class=" readable-text-h4">Including previous questions in the prompt</h4>
  </div>
  <div class="readable-text " id="p306"> 
   <p>As discussed, while it would be ideal for questions to not repeat across <em>different</em> games, we must practically <em>guarantee </em>that the same question isn't asked twice in a <em>single</em> game.</p>
  </div>
  <div class="readable-text  intended-text" id="p307"> 
   <p>Fortunately, this is easily achieved—by explicitly telling the LLM which questions have been asked so far in the game so it knows to steer clear of those.</p>
  </div>
  <div class="readable-text  intended-text" id="p308"> 
   <p>For reinforcement, we could even tell the LLM to make sure to never ask the same question twice.</p>
  </div>
  <div class="readable-text sub-sub-section-heading" id="p309"> 
   <h4 class=" readable-text-h4">Injecting randomness</h4>
  </div>
  <div class="readable-text " id="p310"> 
   <p>Another way to get a wider variety of questions back is to inject some structured randomness into the prompt. You may have heard of the word game "Mad Libs" where players are provided a story with various parts of speech replaced with blanks. Each player then fills in a blank with a word of their choice, with the completed story often being hilarious.</p>
  </div>
  <div class="readable-text  intended-text" id="p311"> 
   <p>We could do something similar here. We could change our prompt to something like "Generate a unique trivia question in the category ______ and a topic ______ within that category. The question should reference a person or thing whose name starts with the letter ______".</p>
  </div>
  <div class="readable-text  intended-text" id="p312"> 
   <p>Within our code, we could then maintain lists of categories, topics, and letters, randomly picking one from each list to fill in the blanks before sending the prompt to the LLM. If we have say, 10 categories, 10 topics within each category, we would have 26 (letters in the alphabet) x 10 x 10 = 2600 unique combinations, in addition to the variability that the LLM itself provides.</p>
  </div>
  <div class="readable-text  intended-text" id="p313"> 
   <p>Or to save us the trouble of maintaining these lists, why not ask the LLM to pick a category and topic first? Interestingly, doing this appears to increase the diversity of the responses generated.</p>
  </div>
  <div class="readable-text  intended-text" id="p314"> 
   <p>Yet another way to inject randomness that seems to help is to explicitly provide a random <em>seed</em> in your prompt. In programming, a random seed is a value (generally an integer) that initializes a random number generator. While it's not very clear that adding one to the text of your prompt actually causes the LLM to generate a random number, in my testing, doing so did increase the variability of responses.</p>
  </div>
  <div class="callout-container admonition-block"> 
   <div class="readable-text" id="p315"> 
    <h5 class=" callout-container-h5 readable-text-h5">Note</h5>
   </div>
   <div class="readable-text" id="p316"> 
    <p>It's important to note here that modifying your prompt to get the results you want is not pure science; often you'll need to experiment with various techniques and prompts to identify the approach that works best. You may also see surprising results—for instance, AI researchers have found asking an LLM to think through its approach to solving a problem step-by-step often improves how well it performs the task.</p>
   </div>
  </div>
  <div class="readable-text" id="p317"> 
   <h3 class=" readable-text-h3">9.6.2 Implementing multiple questions</h3>
  </div>
  <div class="readable-text " id="p318"> 
   <p>Now that we've reviewed how variability works in LLMs and possible approaches to ensure we get different questions each time, let's modify Fact Frenzy so it asks the user multiple questions in a game.</p>
  </div>
  <div class="readable-text sub-sub-section-heading" id="p319"> 
   <h4 class=" readable-text-h4">Modifying the LLM prompt</h4>
  </div>
  <div class="readable-text " id="p320"> 
   <p>Let's first make the requisite changes to our prompts to put into practice what we've learned.</p>
  </div>
  <div class="readable-text  intended-text" id="p321"> 
   <p>Before we do that—our <kbd>Llm</kbd> class doesn't currently offer a way to change the <kbd>temperature</kbd> and <kbd>top_p</kbd>, so we should modify its code (in <kbd>llm.py</kbd>) thus:</p>
  </div>
  <div class="browsable-container listing-container" id="p322"> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">from openai import OpenAI
 
class Llm:
  ...
 
  def ask(self, user_message, sys_message=None, schema=None,
          <b>temperature=None, top_p=None</b>):
    messages = self.construct_messages(user_message, sys_message)
<span style="mso-fareast-font-family:&quot;Roboto Mono&quot;"></span>
    <b>llm_args = {'model': 'gpt-4o-mini', 'messages': messages}</b>
<b>    if temperature:</b>
<b>      llm_args['temperature'] = temperature</b>
<b>    if top_p:</b>
<b>      llm_args['top_p'] = top_p</b>
<b></b>
<b>    if schema:</b>
<b>      completion = self.client.beta.chat.completions.parse(</b>
<b>        response_format=schema,</b>
<b>        **llm_args</b>
<b>      )</b>
      return completion.choices[0].message.parsed
    else:
      completion = self.client.chat.completions.create(<b>**llm_args</b>)
      return completion.choices[0].message.content</pre>
   </div>
  </div>
  <div class="readable-text " id="p323"> 
   <p>(<kbd>chapter_09/in_progress_05/llm.py</kbd> in the GitHub repo)</p>
  </div>
  <div class="readable-text  intended-text" id="p324"> 
   <p>As you can see above, we've refactored the <kbd>ask</kbd> method in the <kbd>Llm</kbd> class a fair bit. First, it accepts <kbd>temperature</kbd> and <kbd>top_p</kbd> as new arguments, both defaulting to <kbd>None</kbd>.</p>
  </div>
  <div class="readable-text  intended-text" id="p325"> 
   <p>Instead of repeating the <kbd>model</kbd>, <kbd>messages</kbd>, <kbd>temperature</kbd>, and <kbd>top_p</kbd> arguments to the OpenAI client's <kbd>beta.chat.completions.parse</kbd> or <kbd>chat.completions.create</kbd>, we construct an <kbd>llm_args</kbd> dictionary that holds the right arguments and their values depending on whether each is provided.</p>
  </div>
  <div class="readable-text  intended-text" id="p326"> 
   <p>We then use the <kbd>**</kbd> dictionary unpacking operator (that we encountered in Chapter 7) to pass the arguments to the OpenAI methods. Note that we can combine this with the normal way of passing arguments:</p>
  </div>
  <div class="browsable-container listing-container" id="p327"> 
   <div class="code-area-container"> 
    <pre class="code-area">completion = self.client.beta.chat.completions.parse(
  response_format=schema,
  **llm_args
)</pre>
   </div>
  </div>
  <div class="readable-text " id="p328"> 
   <p>Here we pass <kbd>reponse_format</kbd> in the regular way, but unpack <kbd>llm_args</kbd> for the remaining arguments.</p>
  </div>
  <div class="readable-text  intended-text" id="p329"> 
   <p>Next, in <kbd>prompts.py</kbd>, edit our question-generation prompt so it now reads:</p>
  </div>
  <div class="browsable-container listing-container" id="p330"> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">QUESTION_PROMPT = {
  'system': 'You are a quizmaster <b>who never asks the same question twice.</b>',
  <b>'user': '''</b>
<b>    First think of a unique category for a trivia question.</b>
<b>    Then think of a topic within that category.</b>
<b>    </b>
<b>    Finally, ask a unique trivia question, generated using the random seed</b>
<b>    {seed}, without revealing the category or topic.</b>
<b>    </b>
<b>    Do not provide choices, or reveal the answer.</b>
<b>    </b>
<b>    The following questions have already been asked:</b>
<b>    {already_asked}</b>
<b>  '''</b>
}
<span style="mso-fareast-font-family:&quot;Roboto Mono&quot;"></span>
ANSWER_PROMPT = {
  ...</pre>
   </div>
  </div>
  <div class="readable-text " id="p331"> 
   <p>(<kbd>chapter_09/in_progress_05/prompts.py</kbd> in the GitHub repo)</p>
  </div>
  <div class="readable-text  intended-text" id="p332"> 
   <p>You'll see that we've incorporated several of the techniques discussed in the last section:</p>
  </div>
  <ul> 
   <li class="readable-text" id="p333">The system prompt requests the LLM to behave like a quizmaster who never asks the same question twice.</li>
   <li class="readable-text" id="p334">We ask the LLM to think of a unique category and a topic within it.</li>
   <li class="readable-text" id="p335">We add a "seed" variable and ask the LLM to generate the question using that seed.</li>
   <li class="readable-text" id="p336">At the end of the prompt, for references, we provide the questions that have already been asked, so the LLM can avoid those.</li>
  </ul>
  <div class="readable-text " id="p337"> 
   <p>We need to accompany these changes with additional ones in the <kbd>Game</kbd> class. Besides the LLM stuff, to enable asking multiple questions in a game, we need to be able to repeat the movement from the first game state to the last many times. In effect, our state diagram now becomes a <em>cycle </em>as opposed to a line, as shown in figure 9.11.</p>
  </div>
  <div class="browsable-container figure-container" id="p338">  
   <img src="../Images/09__image011.png" alt="image"/> 
   <h5 class=" figure-container-h5">Figure 9.11 To implement multiple questions, we now cycle through the game states in a loop</h5>
  </div>
  <div class="readable-text " id="p339"> 
   <p>This means we also need another method to move from the <kbd>STATE_RESULT</kbd> state back to <kbd>GET_QUESTION</kbd>. Let’s add this method along with the rest of the changes to <kbd>game.py</kbd> now:</p>
  </div>
  <div class="browsable-container listing-container" id="p340"> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area"><b>import time</b>
<span style="mso-fareast-font-family:&quot;Roboto Mono&quot;"></span>
from llm import Llm
...
<span style="mso-fareast-font-family:&quot;Roboto Mono&quot;"></span>
class Game:
  def __init__(self, llm_api_key):
    ...
    self.max_questions = <b>5</b>
    <b>self.questions = []</b>
<span style="mso-fareast-font-family:&quot;Roboto Mono&quot;"></span>
  def ask_llm_for_question(self):
    <b>seed = int(time.time())</b>
    <b>sys_msg = QUESTION_PROMPT['system']</b>
<b>    usr_msg = (</b>
<b>      QUESTION_PROMPT['user']</b>
<b>      .replace('{already_asked}', '\n'.join(self.questions))</b>
<b>      .replace('{seed}', str(seed))</b>
<b>    )</b>
    return self.llm.ask(usr_msg, sys_msg, <b>temperature=0.7, top_p=0.9</b>)
<span style="mso-fareast-font-family:&quot;Roboto Mono&quot;"></span>
  ...
  def obtain_question(self):
    self.curr_question = self.ask_llm_for_question()
    <b>self.questions.append(self.curr_question)</b>
    self.status = 'ASK_QUESTION'
    return self.curr_question
<span style="mso-fareast-font-family:&quot;Roboto Mono&quot;"></span>
  ...
  <b>def proceed_to_next_question(self):</b>
<b>    self.status = 'GET_QUESTION'</b>
<b></b>
<b>  </b>...</pre>
   </div>
  </div>
  <div class="readable-text " id="p341"> 
   <p>(<kbd>chapter_09/in_progress_05/game.py</kbd> in the GitHub repo)</p>
  </div>
  <div class="readable-text  intended-text" id="p342"> 
   <p>You'll see that we've added a new attribute, <kbd>self.questions</kbd>, within <kbd>__init__</kbd>, initializing it to an empty list. As you've likely guessed, this will hold all the questions we get from the LLM. We accomplish this through this addition in the <kbd>obtain_questions</kbd> method:</p>
  </div>
  <div class="browsable-container listing-container" id="p343"> 
   <div class="code-area-container"> 
    <pre class="code-area">self.questions.append(self.curr_question)</pre>
   </div>
  </div>
  <div class="readable-text " id="p344"> 
   <p>Additionally, since we'll finally have more than one question to ask, we've changed the value of <kbd>self.max_questions</kbd> to 5. Feel free to change this to whatever number you like.</p>
  </div>
  <div class="readable-text  intended-text" id="p345"> 
   <p>We've revamped the <kbd>ask_llm_for_question</kbd> method entirely, since our user message now has a couple of variables we need to provide values for. <kbd>{already_asked}</kbd> can simply be replaced by <kbd>self.questions</kbd> (with the individual list items separated by newlines).</p>
  </div>
  <div class="readable-text  intended-text" id="p346"> 
   <p>For the random seed, we simply use the current timestamp converted to an integer:</p>
  </div>
  <div class="browsable-container listing-container" id="p347"> 
   <div class="code-area-container"> 
    <pre class="code-area">seed = int(time.time())</pre>
   </div>
  </div>
  <div class="readable-text " id="p348"> 
   <p>Since timestamps always go up by definition, the current timestamp is guaranteed to be something the LLM has never gotten before from us.</p>
  </div>
  <div class="readable-text  intended-text" id="p349"> 
   <p>We also now pass <kbd>temperature</kbd> and <kbd>top_p</kbd> values to <kbd>self.llm.ask</kbd> in line with our exploration of these parameters.</p>
  </div>
  <div class="readable-text  intended-text" id="p350"> 
   <p>To enable multiple questions, a newly added <kbd>proceed_to_next_question</kbd> sets the game's status back to <kbd>GET_QUESTION</kbd>, completing the state cycle in figure 9.11.</p>
  </div>
  <div class="readable-text  intended-text" id="p351"> 
   <p>The changes required to the frontend are relatively simple. Edit <kbd>main.py</kbd> thus:</p>
  </div>
  <div class="browsable-container listing-container" id="p352"> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">import streamlit as st
from game import Game
 
...
with main_col:
  if game:
    <b>st.header(</b>
<b>      f"Question {len(game.questions)} / {game.max_questions}",</b>
<b>      divider='gray'</b>
<b>    )</b>
    <b>st.subheader(f"Score: {game.score}")</b>
<span style="mso-fareast-font-family:&quot;Roboto Mono&quot;"></span>
    if game.status == 'GET_QUESTION':
      ...
<span style="mso-fareast-font-family:&quot;Roboto Mono&quot;"></span>
    ...
    elif game.status == 'STATE_RESULT':
      ...
      if game.is_over():
        with st.container(border=True):
          st.markdown(f"Game over! Your final score is: **{game.score}**")
      <b>else:</b>
<b>        st.button(</b>
<b>          "Next question",</b>
<b>          type='primary',</b>
<b>          on_click=lambda: game.proceed_to_next_question()</b>
<b>        )</b></pre>
   </div>
  </div>
  <div class="readable-text " id="p353"> 
   <p>(<kbd>chapter_09/in_progress_05/main.py</kbd> in the GitHub repo)</p>
  </div>
  <div class="readable-text  intended-text" id="p354"> 
   <p>Firstly, we've modified the header of the main column to provide the question number (<kbd>len(game.questions)</kbd>) and the total number of questions (<kbd>game.max_questions</kbd>):</p>
  </div>
  <div class="browsable-container listing-container" id="p355"> 
   <div class="code-area-container"> 
    <pre class="code-area">st.header(
  f"Question {len(game.questions)} / {game.max_questions}",
  divider='gray'
)</pre>
   </div>
  </div>
  <div class="readable-text " id="p356"> 
   <p>We've also added a subheader to display the score:</p>
  </div>
  <div class="browsable-container listing-container" id="p357"> 
   <div class="code-area-container"> 
    <pre class="code-area">st.subheader(f"Score: {game.score}")</pre>
   </div>
  </div>
  <div class="readable-text " id="p358"> 
   <p>To facilitate the state change from <kbd>STATE_RESULT</kbd> to <kbd>GET_QUESTION</kbd>, we've added an <kbd>else</kbd> clause that will—if the game isn't over—display a "Next question" button that triggers the <kbd>game</kbd> object's <kbd>proceed_to_next_question()</kbd> when clicked.</p>
  </div>
  <div class="readable-text  intended-text" id="p359"> 
   <p>Notice the unfamiliar way in which we've written the <kbd>st.button</kbd> widget:</p>
  </div>
  <div class="browsable-container listing-container" id="p360"> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">st.button(
  "Next question",
  type='primary',
  on_click=lambda: game.proceed_to_next_question()
)<b></b></pre>
   </div>
  </div>
  <div class="readable-text " id="p361"> 
   <p><kbd>st.button</kbd>'s <kbd>on_click</kbd> parameter lets you specify a function to execute when the button is clicked. We <em>could </em>also have written this in the way we've done so far in this book, i.e. as:</p>
  </div>
  <div class="browsable-container listing-container" id="p362"> 
   <div class="code-area-container"> 
    <pre class="code-area">if st.button("Next question", type='primary'):
  game.proceed_to_next_question()
  st.rerun()</pre>
   </div>
  </div>
  <div class="readable-text " id="p363"> 
   <p>The difference lies in when the triggered function is actually executed. Specifically:</p>
  </div>
  <ul> 
   <li class="readable-text" id="p364">When we use the <kbd>if st.button</kbd> notation, the button click first triggers a re-run of the page, causing everything above the button to be re-rendered again before the code under the <kbd>if</kbd> executes—triggered by the fact that <kbd>st.button</kbd> evaluates to <kbd>True</kbd> in the re-run. After this code executes, we may need to manually trigger <em>another</em> re-run as shown above—and as we've been doing throughout this book, really—to see any changes in the page caused by it.</li>
   <li class="readable-text" id="p365">With the <kbd>on_click</kbd> notation, the button click causes the function listed under <kbd>on_click</kbd> (called a <em>callback</em> by the way) to execute <em>before </em>the page is re-run and everything above the button is re-rendered. We don't need a manual <kbd>st.rerun()</kbd> in this case, because the re-run triggered by the button-click already takes into account the changes made by the callback since it has already executed.</li>
  </ul>
  <div class="readable-text " id="p366"> 
   <p>So why haven't we been using this method all along? Well, the <kbd>if st.button</kbd> structure tends to be a little easier to grasp for simple apps. Besides, callbacks have some restrictions—you can't trigger an app rerun within a callback, for instance.</p>
  </div>
  <div class="readable-text  intended-text" id="p367"> 
   <p>In any case, you should be able to re-run your app at this point to try out a working multi-question game (figure 9.12).</p>
  </div>
  <div class="browsable-container figure-container" id="p368">  
   <img src="../Images/09__image012.png" alt="image"/> 
   <h5 class=" figure-container-h5">Figure 9.12 A working multi-question trivia game (see chapter_09/in_progress_05 in the GitHub repo for the full code)</h5>
  </div>
  <div class="readable-text " id="p369"> 
   <p>Our trivia game is technically complete now, but let's see if we can make it more engaging in the next section.</p>
  </div>
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p370"> 
    <h5 class=" callout-container-h5 readable-text-h5">Cost considerations</h5>
   </div>
   <div class="readable-text" id="p371"> 
    <p>LLMs are an incredible general-purpose tool, but it is important to realize that using them in your app—especially in production where it may be accessed by hundreds of users—is not free. The last thing you want is to accidentally run up a huge bill.</p>
   </div>
   <div class="readable-text" id="p372"> 
    <p><strong>How cost is calculated</strong></p>
   </div>
   <div class="readable-text" id="p373"> 
    <p>When using an LLM, costs are typically calculated based on the number of <em>tokens</em> processed. As discussed in section 9.5.1, a "token" represents a chunk of text—a word or a part of a word—that the model reads (input) or generates (output).</p>
   </div>
   <div class="readable-text" id="p374"> 
    <p>OpenAI charges people based on the sum of the input and output tokens processed. This means that the size of the input prompt and the size of the output text <em>both </em>contribute to the cost. The pricing also differs by model. At the time of writing, the model we've been using in this chapter—gpt-4o-mini—costs 15 cents for every 1 million tokens processed.</p>
   </div>
   <div class="readable-text" id="p375"> 
    <p>You can use tools like <a href="https://platform.openai.com/tokenizer">https://platform.openai.com/tokenizer</a> to count the tokens in a piece of text and determine costs.</p>
   </div>
   <div class="readable-text" id="p376"> 
    <p><strong>Cost optimization strategies</strong></p>
   </div>
   <div class="readable-text" id="p377"> 
    <p>There are several ways in which you could optimize cost while working with LLMs. Here are a few ideas:</p>
   </div>
   <ul> 
    <li class="readable-text" id="p378">Keep your input prompts short and to-the-point to reduce costs associated with input tokens.</li>
    <li class="readable-text" id="p379">Reduce the size of the output text, either by instructing the LLM to keep its response short, or by explicitly restricting the number of tokens processed to a maximum value (e.g. by passing a value to the <kbd>max_tokens</kbd> argument while calling the OpenAI endpoint).</li>
    <li class="readable-text" id="p380">Batch together multiple requests to reduce the total number of prompts sent to the LLM. For example, instead of providing a list of previously asked questions each time we need a new question—as we're doing here—we could simply ask the LLM to generate the total number of questions we want in one go.</li>
    <li class="readable-text" id="p381">Use less capable but cheaper models for some of your prompts. In our case, we're using gpt-4o-mini, which strikes a pretty good balance of cost and intelligence, but depending on your application, it may be possible to use even cheaper models for less complex tasks. Familiarize yourself with OpenAI's pricing page.</li>
    <li class="readable-text" id="p382">Avoid LLM costs altogether by asking the user to provide their <em>own </em>LLM API key. For our game, this involves a major hit to user experience as it requires players to create an OpenAI account before they can play, but you're guaranteed to not have to pay a dime in LLM-related costs.</li>
   </ul>
  </div>
  <div class="readable-text" id="p383"> 
   <h2 class=" readable-text-h2">9.7 Adding AI personalities</h2>
  </div>
  <div class="readable-text " id="p384"> 
   <p>As an informative trivia game, Fact Frenzy works perfectly fine now. The end-to-end flow—from starting a new game to cycling through the questions and keeping score until the game ends—has been established.</p>
  </div>
  <div class="readable-text  intended-text" id="p385"> 
   <p>However, it still lacks a certain <em>je ne sais quoi</em>—it's rather dry and mechanical. Wouldn't it be cool if we could give our game a personality? Fortunately, this is exactly the kind of thing that LLMs are great at. We could, for instance, ask GPT-4o to mimic the style of various characters while asking questions.</p>
  </div>
  <div class="readable-text  intended-text" id="p386"> 
   <p>In fact, we could let players choose what character they want their quizmaster to take. Sound fun? Let's get to it!</p>
  </div>
  <div class="readable-text" id="p387"> 
   <h3 class=" readable-text-h3">9.7.1 Adding game settings</h3>
  </div>
  <div class="readable-text " id="p388"> 
   <p>We don't currently have a page or place in the app where players can view or change any settings, so we'll tackle that first.</p>
  </div>
  <div class="readable-text  intended-text" id="p389"> 
   <p>What options do we want the user to be able to set? We've already talked about the quizmaster speaking style, so that can be the first. We could also let the player pick a difficulty level that suits them.</p>
  </div>
  <div class="readable-text  intended-text" id="p390"> 
   <p>There are several different designs we could use for a "settings editor", but I want to use this opportunity to introduce a handy Streamlit widget we haven't encountered before: <kbd>st.data_editor</kbd>.</p>
  </div>
  <div class="readable-text sub-sub-section-heading" id="p391"> 
   <h4 class=" readable-text-h4">st.data_editor</h4>
  </div>
  <div class="readable-text " id="p392"> 
   <p>In Chapters 6 and 7, we learned about Pandas dataframes, which make working with tabular data in Python easy. We discovered <kbd>st.dataframe</kbd>, used to render dataframes as a table in a Streamlit app for viewing.</p>
  </div>
  <div class="readable-text  intended-text" id="p393"> 
   <p><kbd>st.data_editor</kbd> displays dataframes too, but also makes them <em>editable</em>, providing users with an experience that you might expect in a spreadsheet.</p>
  </div>
  <div class="readable-text  intended-text" id="p394"> 
   <p>What does this have to do with adding settings to our app? Well, we could place the settings we want in a dataframe, and enable people to edit the dataframe to modify a setting.</p>
  </div>
  <div class="readable-text  intended-text" id="p395"> 
   <p>With the two settings we discussed a few paragraphs above, the settings dataframe might look like the following:</p>
  </div>
  <div class="browsable-container listing-container" id="p396"> 
   <div class="code-area-container"> 
    <pre class="code-area">+----------------+------------+
|   Quizmaster   | Difficulty |
+----------------+------------+
| Alex Trebek    | Medium     |
+----------------+------------+</pre>
   </div>
  </div>
  <div class="readable-text " id="p397"> 
   <p>If this dataframe is stored in <kbd>default_settings</kbd>, we might write our <kbd>st.data_editor</kbd> widget like so:</p>
  </div>
  <div class="browsable-container listing-container" id="p398"> 
   <div class="code-area-container"> 
    <pre class="code-area">settings = st.data_editor(default_settings, num_rows='fixed', key='settings_editor')</pre>
   </div>
  </div>
  <div class="readable-text " id="p399"> 
   <p>This would display the widget shown in figure 9.13.</p>
  </div>
  <div class="browsable-container figure-container" id="p400">  
   <img src="../Images/09__image013.png" alt="image"/> 
   <h5 class=" figure-container-h5">Figure 9.13 A simple output of st.data_editor</h5>
  </div>
  <div class="readable-text " id="p401"> 
   <p>The first argument here is the initial state of the data we want to edit—in this case, the default settings.</p>
  </div>
  <div class="readable-text  intended-text" id="p402"> 
   <p>The <kbd>num_rows='fixed'</kbd> means that the data editor widget shouldn't allow users to add any new rows. This makes sense because we don't want to have multiple rows in the dataframe shown above—a single setting can only have one value.</p>
  </div>
  <div class="readable-text  intended-text" id="p403"> 
   <p>In any run of the app that happens before the user interacts with the data editor, <kbd>settings</kbd> will hold the same value as <kbd>default_settings</kbd>. Once the user changes a setting—for example, they might change Difficulty to <kbd>Easy</kbd>—<kbd>settings</kbd> will hold the edited dataframe across future re-runs until the user edits it again.</p>
  </div>
  <div class="callout-container admonition-block"> 
   <div class="readable-text" id="p404"> 
    <h5 class=" callout-container-h5 readable-text-h5">Note</h5>
   </div>
   <div class="readable-text" id="p405"> 
    <p>The <kbd>key='settings_editor'</kbd> parameter adds a widget key to the session state for the data editor. While this isn't strictly required for the app to function correctly, it protects us from a certain quirk of Streamlit where it forgets the values of a widget without an explicit key between re-runs if that widget isn't rendered for some reason in a particular run. Adding a widget key doesn't cost us anything, so it's safer to provide one to avoid unforeseen bugs.</p>
   </div>
  </div>
  <div class="readable-text " id="p406"> 
   <p>Getting back to our example, we don't necessarily want users to have to type in the name of the quizmaster or a difficulty level; we'd rather have them select from a list of options. <kbd>st.data_editor</kbd> supports this in the form of <em>column configurations</em>:</p>
  </div>
  <div class="browsable-container listing-container" id="p407"> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">st.data_editor(
  default_settings,
  <b>column_config={</b>
<b>    'Quizmaster': st.column_config.SelectboxColumn(</b>
<b>       options=['Alex Trebek', 'Eminem', 'Gollum'],</b>
<b>       required=True</b>
<b>    ),</b>
<b>    'Difficulty': st.column_config.SelectboxColumn(</b>
<b>       options=['Easy', 'Medium', 'Difficult'],</b>
<b>       required=True</b>
<b>    )</b>
<b>  },</b>
  num_rows='fixed',
  key='settings_editor'
)</pre>
   </div>
  </div>
  <div class="readable-text " id="p408"> 
   <p>Here we exert more granular control over the editable data, configuring each column in the data through <kbd>st.column_config</kbd>.</p>
  </div>
  <div class="readable-text  intended-text" id="p409"> 
   <p>For each of our columns, we use a <kbd>SelectBoxColumn</kbd> which lets us specify a list of options to choose from, and whether a value must be specified (the <kbd>required</kbd> parameter, set to <kbd>True</kbd> above).</p>
  </div>
  <div class="readable-text  intended-text" id="p410"> 
   <p>The results are shown in figure 9.14.</p>
  </div>
  <div class="browsable-container figure-container" id="p411">  
   <img src="../Images/09__image014.png" alt="image"/> 
   <h5 class=" figure-container-h5">Figure 9.14 st.data_editor with one of the columns showing a SelectBoxColumn</h5>
  </div>
  <div class="readable-text " id="p412"> 
   <p><kbd>st.column_config</kbd> supports many different column types besides <kbd>SelectBoxColumn</kbd>, such as a <kbd>CheckboxColumn</kbd> for boolean values, a <kbd>DatetimeColumn</kbd> that displays a date/time picker, and a <kbd>LinkColumn</kbd> for clickable URLs.</p>
  </div>
  <div class="readable-text  intended-text" id="p413"> 
   <p>It also supports non-editable types that can be used with <kbd>st.dataframe</kbd>, including <kbd>AreaChartColumn</kbd>, <kbd>BarChartColumn</kbd>, a <kbd>ListColumn</kbd> for lists, and even a <kbd>ProgressColumn</kbd> for numbers (displayed in a progress bar versus a target).</p>
  </div>
  <div class="readable-text sub-sub-section-heading" id="p414"> 
   <h4 class=" readable-text-h4">Creating the settings editor</h4>
  </div>
  <div class="readable-text " id="p415"> 
   <p>Now that we know how <kbd>st.data_editor</kbd> works, let's go ahead and create a settings editor UI. We'll put this in a new file called <kbd>settings.py</kbd> (shown in listing 9.8).</p>
  </div>
  <div class="browsable-container listing-container" id="p416"> 
   <h5 class=" listing-container-h5 browsable-container-h5">Listing 9.8 settings.py</h5>
   <div class="code-area-container"> 
    <pre class="code-area">import streamlit as st
 
QM_OPTIONS = ["Alex Trebek", "Eminem", "Gollum", "Gruk the Caveman"]
DIFFICULTY_OPTIONS = ["Easy", "Medium", "Hard"]
 
default_settings = {
  "Quizmaster": [QM_OPTIONS[0]],
  "Difficulty": [DIFFICULTY_OPTIONS[1]]
}
 
def settings_editor():
  with st.popover("Settings", use_container_width=True):
    return st.data_editor(
      default_settings,
      key='settings_editor',
      column_config={
        'Quizmaster': st.column_config.SelectboxColumn(
          options=QM_OPTIONS, required=True),
        'Difficulty': st.column_config.SelectboxColumn(
          options=DIFFICULTY_OPTIONS, required=True)
      },
      num_rows='fixed',
      use_container_width=True,
    )</pre>
   </div>
  </div>
  <div class="readable-text " id="p417"> 
   <p>(<kbd>chapter_09/in_progress_06/settings.py</kbd> in the GitHub repo)</p>
  </div>
  <div class="readable-text  intended-text" id="p418"> 
   <p>We place the options for the Quizmaster and Difficulty settings right at the top for easy access, listing them under <kbd>QM_OPTIONS</kbd> and <kbd>DIFFICULTY_OPTIONS</kbd>.</p>
  </div>
  <div class="readable-text  intended-text" id="p419"> 
   <p>The Quizmaster options range from an actual quizmaster, the late Alex Trebek of <em>Jeopardy!</em> fame, to a range of fictional characters like Gollum from <em>The Lord Of The Rings</em>, and Gruk the Caveman, an entirely made-up figure to let the LLM go wild.</p>
  </div>
  <div class="readable-text  intended-text" id="p420"> 
   <p>We've initialized <kbd>default_settings</kbd> thus:</p>
  </div>
  <div class="browsable-container listing-container" id="p421"> 
   <div class="code-area-container"> 
    <pre class="code-area">default_settings = {
  "Quizmaster": [QM_OPTIONS[0]],
  "Difficulty": [DIFFICULTY_OPTIONS[1]]
}</pre>
   </div>
  </div>
  <div class="readable-text " id="p422"> 
   <p>Note how this <em>isn't </em>a dataframe as we suggested initially—it's a dictionary instead, with the name of each setting as a key, and a one-element list containing the default option for that setting as the corresponding value.</p>
  </div>
  <div class="readable-text  intended-text" id="p423"> 
   <p>Interestingly, <kbd>st.data_editor</kbd> can display things that are not Pandas dataframes. This includes native Python types such as dictionaries, lists, and sets. The quality of being able to display these types even applies to <kbd>st.dataframe</kbd>, despite the name. In this case, it means we don't actually have to maintain the settings as a dataframe; we can use the more readable dictionary form above.</p>
  </div>
  <div class="readable-text  intended-text" id="p424"> 
   <p>The <kbd>settings_editor</kbd> function renders the actual settings editor UI. We place everything within yet another new Streamlit widget called <kbd>st.popover</kbd>:</p>
  </div>
  <div class="browsable-container listing-container" id="p425"> 
   <div class="code-area-container"> 
    <pre class="code-area">with st.popover("Settings", use_container_width=True):
  ...</pre>
   </div>
  </div>
  <div class="readable-text " id="p426"> 
   <p><kbd>st.popover</kbd> displays a popover widget, which is a small pop-up screen that you can trigger by clicking an associated button—in a similar manner to <kbd>st.expander</kbd>. The first argument is the label for the button that triggers the <kbd>st.popover</kbd>.</p>
  </div>
  <div class="readable-text  intended-text" id="p427"> 
   <p>The contents of the popover are written within the <kbd>with st.popover(...)</kbd> context manager. In this case, we're displaying the <kbd>st.data_editor</kbd> widget and returning its value, i.e. the edited <kbd>settings</kbd> dictionary:</p>
  </div>
  <div class="browsable-container listing-container" id="p428"> 
   <div class="code-area-container"> 
    <pre class="code-area">return st.data_editor(
  default_settings,
  key='settings_editor',
  column_config={
    'Quizmaster': st.column_config.SelectboxColumn(
      options=QM_OPTIONS, required=True),
    'Difficulty': st.column_config.SelectboxColumn(
      options=DIFFICULTY_OPTIONS, required=True)
  },
  num_rows='fixed',
  use_container_width=True,
)</pre>
   </div>
  </div>
  <div class="readable-text " id="p429"> 
   <p>This is pretty much the same code that we wrote in the previous section while we were discussing<kbd> st.data_editor</kbd>, though with the addition of a <kbd>use_container_width=True</kbd> argument, which adjusts the width of the popover.</p>
  </div>
  <div class="readable-text sub-sub-section-heading" id="p430"> 
   <h4 class=" readable-text-h4">Applying the settings</h4>
  </div>
  <div class="readable-text " id="p431"> 
   <p>How do we use these settings in Fact Frenzy? Both the Quizmaster and Difficulty settings relate to the question text generated by the LLM, so let's incorporate them in the question prompt in <kbd>prompts.py</kbd>, which becomes:</p>
  </div>
  <div class="browsable-container listing-container" id="p432"> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">QUESTION_PROMPT = {
  'system': '''
    You are a quizmaster <b>who mimics the speaking style of {quizmaster}</b> and
    never asks the same question twice.
  ''',
  'user': '''
    First think of a unique category for a trivia question.
    Then think of a topic within that category.
    
    Finally, ask a unique trivia question <b>that has a difficulty rating of</b>
<b>    {difficulty}</b> and is generated using the random seed {seed}, without
    revealing the category or topic.
    
    Do not provide choices, or reveal the answer.
    
    The following questions have already been asked:
    {already_asked}
  '''
}
...</pre>
   </div>
  </div>
  <div class="readable-text " id="p433"> 
   <p>(<kbd>chapter_09/in_progress_06/prompts.py</kbd> in the GitHub repo)</p>
  </div>
  <div class="readable-text  intended-text" id="p434"> 
   <p>Obviously, there are plenty of ways to work our two new variables into the prompt—the above is just one.</p>
  </div>
  <div class="readable-text  intended-text" id="p435"> 
   <p>We'll modify <kbd>game.py</kbd> next:</p>
  </div>
  <div class="browsable-container listing-container" id="p436"> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">...
class Game:
  def __init__(self, llm_api_key, <b>settings</b>):
    self.llm = Llm(llm_api_key)
    <b>self.settings = settings</b>
    self.status = 'GET_QUESTION'
    ...
<span style="mso-fareast-font-family:&quot;Roboto Mono&quot;"></span>
  <b>def get_setting(self, setting_name):</b>
<b>    return self.settings[setting_name][0]</b>
<b></b>
<b>  def modify_settings(self, new_settings):</b>
<b>    self.settings = new_settings</b>
<span style="mso-fareast-font-family:&quot;Roboto Mono&quot;"></span>
  def ask_llm_for_question(self):
    seed = int(time.time())
    <b>sys_msg = (</b>
<b>      QUESTION_PROMPT['system']</b>
<b>      .replace('{quizmaster}', self.get_setting('Quizmaster'))</b>
<b>    )</b>
    usr_msg = (
      QUESTION_PROMPT['user']
      .replace('{already_asked}', '\n'.join(self.questions))
      .replace('{seed}', str(seed))
      <b>.replace('{difficulty}', self.get_setting('Difficulty'))</b>
    )
    return self.llm.ask(usr_msg, sys_msg)
<span style="mso-fareast-font-family:&quot;Roboto Mono&quot;"></span>
  def ask_llm_to_evaluate_answer(self):
    ...
  ...</pre>
   </div>
  </div>
  <div class="readable-text " id="p437"> 
   <p>(<kbd>chapter_09/in_progress_06/game.py</kbd> in the GitHub repo)</p>
  </div>
  <div class="readable-text  intended-text" id="p438"> 
   <p><kbd>Game</kbd>'s <kbd>__init__</kbd> now accepts a <kbd>settings</kbd> parameter, which—as you'd expect—is in the dictionary format we used in <kbd>settings.py</kbd>. This is assigned to <kbd>self.settings</kbd> so other methods can access it.</p>
  </div>
  <div class="readable-text  intended-text" id="p439"> 
   <p>We've added two associated methods: <kbd>get_setting</kbd> and <kbd>modify_settings</kbd>. <kbd>get_setting</kbd> deals with getting the value of a given setting, which is slightly tricky because each dictionary value is a single-element list (designed that way so it works with <kbd>st.data_editor</kbd>). <kbd>get_setting</kbd> abstracts away this somewhat unsightly logic so we restrict it to one place in the code.</p>
  </div>
  <div class="readable-text  intended-text" id="p440"> 
   <p><kbd>modify_settings</kbd> replaces <kbd>self.settings</kbd> with a given <kbd>new_settings</kbd> dictionary. This will come into play when the user changes a setting.</p>
  </div>
  <div class="readable-text  intended-text" id="p441"> 
   <p>Turning to the <kbd>ask_llm_for_question</kbd> method, we replace the <kbd>{quizmaster}</kbd> and <kbd>{difficulty}</kbd> variables we added to the prompt with their corresponding values from the settings, obtained through <kbd>get_setting</kbd>.</p>
  </div>
  <div class="readable-text  intended-text" id="p442"> 
   <p>The changes to <kbd>main.py</kbd> are all that remain now, so let's make those:</p>
  </div>
  <div class="browsable-container listing-container" id="p443"> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">import streamlit as st
from game import Game
<b>from settings import default_settings, settings_editor</b>
<span style="mso-fareast-font-family:&quot;Roboto Mono&quot;"></span>
def start_new_game():
  st.session_state.game = Game(st.secrets['llm_api_key'], <b>default_settings</b>)
  st.rerun()
<span style="mso-fareast-font-family:&quot;Roboto Mono&quot;"></span>
...
with side_col:
  st.header("⚡ Fact Frenzy", divider='gray')<span style="mso-fareast-font-family:&quot;Roboto Mono&quot;"></span>
  <b>settings = settings_editor()</b>
  new_game_button(game)
<span style="mso-fareast-font-family:&quot;Roboto Mono&quot;"></span>
with main_col:
  if game:
    <b>game.modify_settings(settings)</b>
    st.header(
      ...
    ...</pre>
   </div>
  </div>
  <div class="readable-text " id="p444"> 
   <p>(<kbd>chapter_09/in_progress_06/main.py</kbd> in the GitHub repo)</p>
  </div>
  <div class="readable-text  intended-text" id="p445"> 
   <p>In <kbd>start_new_game</kbd>, to obtain the initial <kbd>Game</kbd> instance, we now pass in <kbd>default_settings</kbd>, directly imported from <kbd>settings.py</kbd>.</p>
  </div>
  <div class="readable-text  intended-text" id="p446"> 
   <p>The actual settings editor is displayed within the side column (<kbd>side_col</kbd>), right above the New Game button. The return value—recall that this would be the <kbd>default_settings</kbd> dictionary before the user changes the value of any settings, and the modified dictionary afterward—is stored in the <kbd>settings</kbd> variable.</p>
  </div>
  <div class="readable-text  intended-text" id="p447"> 
   <p>And, finally, in every re-run—provided that we're in a game—we run <kbd>game.modify_settings(settings)</kbd> to pick up any changes the user has made to the settings.</p>
  </div>
  <div class="readable-text  intended-text" id="p448"> 
   <p>That should do it. Run your app again to see figure 9.15. Play around with the AI quizmaster options and difficulties; it's now more fun to read the questions!</p>
  </div>
  <div class="browsable-container figure-container" id="p449">  
   <img src="../Images/09__image015.png" alt="image"/> 
   <h5 class=" figure-container-h5">Figure 9.15 A final version of Fact Frenzy with editable settings (see chapter_09/in_progress_06 in the GitHub repo for the full code)</h5>
  </div>
  <div class="readable-text " id="p450"> 
   <p>That concludes our development of Fact Frenzy, the first—and only—game we'll build in this book. In Chapter 10, we'll continue our exploration of AI apps with a more practical application: a customer support chatbot.</p>
  </div>
  <div class="readable-text" id="p451"> 
   <h2 class=" readable-text-h2">9.8 Summary</h2>
  </div>
  <ul> 
   <li class="readable-text" id="p452">A Large Language Model (LLM) is an AI system designed to process and generate human-like text.</li>
   <li class="readable-text" id="p453">LLMs can perform both creative and analytical tasks.</li>
   <li class="readable-text" id="p454">OpenAI, one of the most popular LLM providers, allows developers to access its GPT series through an API.</li>
   <li class="readable-text" id="p455">The <kbd>openai</kbd> library provides a convenient way to call the OpenAI API in Python.</li>
   <li class="readable-text" id="p456">You can pass a conversation to OpenAI API's chat completion endpoint with messages tagged as <kbd>"system"</kbd>, <kbd>"assistant"</kbd> or <kbd>"user"</kbd>, causing the model to complete the conversation in a logical way.</li>
   <li class="readable-text" id="p457">Structured Outputs is a feature provided by OpenAI that ensures the model will generate a response that adheres to a given schema.</li>
   <li class="readable-text" id="p458">A common pattern in linear Streamlit apps is to implement conditional branching logic based on a variable that's held in <kbd>st.session_state</kbd>.</li>
   <li class="readable-text" id="p459">You can vary parameters such as <kbd>temperature</kbd> and <kbd>top_p</kbd> to affect the creativity and predictability of responses generated by an LLM.</li>
   <li class="readable-text" id="p460">Injecting randomness into the prompt is a good way to ensure we get different responses to similar prompts.</li>
   <li class="readable-text" id="p461">It's important to optimize cost in LLM-based applications. You can do this by having the LLM process fewer input and output tokens, reducing the number of prompts, using cheaper models, or even having users bear the cost by requiring them to supply their own API key.</li>
   <li class="readable-text" id="p462"><kbd>st.data_editor</kbd> provides a way to create editable tables in Streamlit apps.</li>
   <li class="readable-text" id="p463"><kbd>st.column_config</kbd> lets you configure columns to be of certain editable and non-editable types in <kbd>st.data_editor</kbd> and <kbd>st.dataframe</kbd>.</li>
   <li class="readable-text" id="p464"><kbd>st.popover</kbd> displays a small pop-up screen triggered by a click on an associated button.</li>
  </ul>
</body>
</html>