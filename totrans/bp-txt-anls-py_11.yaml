- en: Chapter 11\. Performing Sentiment Analysis on Text Data
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第11章。在文本数据上执行情感分析
- en: In every interaction that we have in the real world, our brain subconsciously
    registers feedback not just in the words said but also using facial expressions,
    body language, and other physical cues. However, as more of our communication
    becomes digital, it increasingly appears in the form of text, where we do not
    have the possibility of evaluating physical cues. Therefore, it’s important to
    understand the mood or sentiment felt by a person through the text they write
    in order to form a complete understanding of their message.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们在现实世界中的每一次互动中，我们的大脑在潜意识中不仅通过所说的话来注册反馈，还使用面部表情、身体语言和其他物理线索。然而，随着越来越多的沟通变成数字化形式，它越来越多地出现在文本形式中，我们无法评估物理线索。因此，通过他们写的文本理解一个人的情绪或感受是非常重要的，以便形成对他们信息完整理解。
- en: For example, a lot of customer support is now automated through the use of a
    software ticketing system or even an automated chatbot. As a result, the only
    way to understand how a customer is feeling is by understanding the sentiment
    from their responses. Therefore, if we are dealing with a particularly irate customer,
    it’s important to be extra careful with our responses to not annoy them further.
    Similarly, if we want to understand what customers think about a particular product
    or brand, we can analyze the sentiment from their posts, comments, or reviews
    about that brand in social media channels and understand how they feel about the
    brand.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，现在很多客户支持都通过软件服务系统或者自动聊天机器人来自动化。因此，了解客户感受的唯一方式就是通过理解他们回复中的情感。因此，如果我们处理一个特别愤怒的客户，就非常重要要在回复时特别小心，以免进一步激怒他们。同样，如果我们想要了解客户对特定产品或品牌的看法，我们可以分析他们在社交媒体渠道上关于该品牌的帖子、评论或者评价的情感，并理解他们对品牌的感受。
- en: 'Understanding sentiment from text is challenging because there are several
    aspects that need to be inferred that are not directly evident. A simple example
    is the following customer review for a laptop purchased on Amazon:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 从文本中理解情感是具有挑战性的，因为有几个方面需要推断，这些方面并不直接明显。一个简单的例子是来自亚马逊购买的笔记本电脑的以下客户评价：
- en: This laptop is full of series problem. Its speed is exactly as per specifications
    which is very slow! Boot time is more.”
  id: totrans-4
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这台笔记本电脑存在严重问题。它的速度完全符合规格，非常慢！启动时间更长。
- en: If a human were to read it, they could detect the irony expressed about the
    speed of the laptop and the fact that it takes a long time to boot up, which leads
    us to conclude that this is a negative review. However, if we analyze only the
    text, it’s clear that the speed is exactly as specified. The fact that the boot
    time is high might also be perceived as a good thing unless we know that this
    is a parameter that needs to be small. The task of sentiment analysis is also
    specific to the type of text data being used. For example, a newspaper article
    is written in a structured manner, whereas tweets and other social media text
    follow a loose structure with the presence of slang and incorrect punctuation.
    As a result, there isn’t one blueprint that might work for every scenario. Instead,
    we will present a set of blueprints that can be used to produce a successful sentiment
    analysis.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个人类读它，他们可以察觉到关于笔记本电脑速度的讽刺表达，以及它启动时间长的事实，这导致我们得出结论这是一个负面评价。然而，如果我们只分析文本，很明显速度完全符合规格。启动时间较长的事实也可能被认为是一件好事，除非我们知道这是需要小的参数。情感分析的任务也特定于所使用的文本数据类型。例如，报纸文章以结构化方式编写，而推文和其他社交媒体文本则遵循松散结构，并且存在俚语和不正确的标点符号。因此，并不存在一种可以适用于所有情景的蓝图。相反，我们将提供一套可以用来进行成功情感分析的蓝图。
- en: What You’ll Learn and What We’ll Build
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 您将学到什么，我们将构建什么
- en: In this chapter, we will explore multiple techniques to estimate the sentiment
    from a snippet of text data. We will start with simple rule-based techniques and
    work our way through more complex methods, finally using state-of-the-art language
    models such as BERT from Google. The purpose of walking through these techniques
    is to improve our understanding of customer sentiment and provide you with a set
    of blueprints that can be used for various use cases. For example, combined with
    the Twitter API blueprint from [Chapter 2](ch02.xhtml#ch-api), you could determine
    the public sentiment about a certain personality or political issue. You could
    also use these blueprints within your organization to analyze the sentiment in
    customer complaints or support emails and determine how happy your clients are.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨多种技术，用于从文本数据片段中估计情感。我们将从简单的基于规则的技术开始，并逐步深入到更复杂的方法，最终使用来自Google的BERT等最新语言模型。通过这些技术的介绍，我们的目的是提升对客户情感的理解，并为您提供一套可以应用于各种用例的蓝图。例如，结合第2章中的Twitter
    API蓝图（见[ch02.xhtml#ch-api](ch02.xhtml#ch-api)），您可以确定公众对某一特定人物或政治问题的情感。您还可以在组织内使用这些蓝图来分析客户投诉或支持电子邮件中的情感，从而了解客户的满意度。
- en: Sentiment Analysis
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 情感分析
- en: A lot of information is available in the form of text, and based on the context
    of the communication, the information can be categorized into objective texts
    and subjective texts. *Objective texts* contain a simple statement of facts, like
    we might find in a textbook or Wikipedia article. Such texts generally present
    the facts and do not express an opinion or sentiment. *Subjective texts*, on the
    other hand, convey someone’s reaction or contain information about emotion, mood,
    or feelings. This might be typically found in social media channels in tweets
    or where customers express their opinions, such as in product reviews. We undertake
    a study of sentiment in order to understand the state of mind of an individual
    expressed through the medium of text. Therefore, sentiment analysis works best
    on subjective texts that contain this kind of information rather than objective
    texts. Before starting our analysis, we must ensure that we have the right kind
    of dataset that captures the sentiment information we are looking for.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 大量信息以文本形式提供，根据通信的上下文，可以将信息分类为客观文本和主观文本。*客观文本*包含简单的事实陈述，如我们在教科书或维基百科文章中找到的内容。这类文本通常只呈现事实，不表达观点或情感。另一方面，*主观文本*传达了某人的反应，或包含了情感、情绪或感觉的信息。这在社交媒体渠道如推特中或顾客在产品评论中典型地表现出来。我们进行情感分析研究，以了解通过文本表达的个体心态状态。因此，情感分析最适用于包含此类信息的主观文本，而不是客观文本。在开始分析之前，我们必须确保拥有捕捉我们寻找的情感信息的正确类型数据集。
- en: The sentiment of a piece of text can be determined at the phrase, sentence,
    or document level. For example, if we take the case of a customer writing an email
    to a company, there will be several paragraphs, with each paragraph containing
    multiple sentences. Sentiment can be calculated for each sentence and also for
    each paragraph. While paragraph 1 may be positive, paragraphs 3 and 4 could be
    negative. So, if we want to determine the overall sentiment expressed by this
    customer, we would have to determine the best way to aggregate the sentiment for
    each paragraph up to the document level. In the blueprints that we present, we
    calculate sentiment at a sentence level.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 一段文本的情感可以在短语、句子或文档级别确定。例如，如果我们以客户写给公司的电子邮件为例，将会有几段，每段中包含多个句子。可以为每个句子和每个段落计算情感。虽然第1段可能是积极的，但第3和第4段可能是消极的。因此，如果我们想要确定该客户表达的整体情感，我们需要确定将每段的情感聚合到文档级别的最佳方法。在我们提供的蓝图中，我们在句子级别计算情感。
- en: 'The techniques for performing sentiment analysis can be broken down into simple
    rule-based techniques and supervised machine learning approaches. Rule-based techniques
    are easier to apply since they do not require annotated training data. Supervised
    learning approaches provide better results but include the additional effort of
    labeling the data. There might be simple ways to work around this requirement
    as we will show in our use case. In this chapter, we will provide the following
    set of blueprints:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 进行情感分析的技术可以分解为简单的基于规则的技术和监督式机器学习方法。基于规则的技术更容易应用，因为它们不需要标注的训练数据。监督学习方法提供更好的结果，但包括标记数据的额外努力。我们将在我们的用例中展示，可能有简单的方法来绕过这个要求。在本章中，我们将提供以下一套蓝图：
- en: Sentiment analysis using lexicon-based approaches
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用基于词典的方法进行情感分析。
- en: Sentiment analysis by building additional features from text data and applying
    a supervised machine learning algorithm
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过从文本数据构建附加特征并应用监督式机器学习算法进行情感分析。
- en: Sentiment analysis using transfer learning technique and pretrained language
    models like BERT
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用转移学习技术和预训练语言模型如BERT进行情感分析。
- en: Introducing the Amazon Customer Reviews Dataset
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍亚马逊客户评论数据集。
- en: Let’s assume you are an analyst working in the marketing department of a leading
    consumer electronics company and would like to know how your smartphone products
    compare with competitors. You can easily compare the technical specifications,
    but it is more interesting to understand the consumer perception of the product.
    You could determine this by analyzing the sentiment expressed by customers in
    product reviews on Amazon. Using the blueprints and aggregating the sentiment
    for each review for a brand, you would be able to identify how customers perceive
    each brand. Similarly, what if your company is looking to expand their business
    by introducing products in an adjacent category? You could analyze customer reviews
    for all products in a segment, such as media tablets, smartwatches, or action
    cameras, and based on the aggregated sentiment determine a segment with poor customer
    satisfaction and therefore higher potential success of your product.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您是一家领先消费电子公司市场部门的分析师，并希望了解您的智能手机产品与竞争对手的比较情况。您可以轻松比较技术规格，但更有趣的是了解产品的消费者感知。您可以通过分析顾客在亚马逊产品评论中表达的情感来确定这一点。利用蓝图并对每个品牌的每条评论的情感进行汇总，您将能够确定顾客如何看待每个品牌。同样，如果您的公司计划通过在相邻类别引入产品来扩展业务，该怎么办？您可以分析一个段落中所有产品的顾客评论，例如媒体平板电脑、智能手表或行动摄像机，并根据汇总的情感确定一个顾客满意度较低的段落，因此您的产品具有更高的潜在成功机会。
- en: For our blueprints, we will use a dataset containing a collection of Amazon
    customer reviews for different products across multiple product categories. This
    dataset of Amazon customer reviews has already been scraped and compiled by researchers
    at Stanford University.^([1](ch11.xhtml#idm45634178845144)) The [last updated
    version](https://oreil.ly/QcMIz) consists of product reviews from the Amazon website
    between 1996 and 2018 across several categories. It includes product reviews,
    product ratings, and other information such as helpfulness votes and product metadata.
    For our blueprints, we are going to focus on product reviews and use only those
    that are one sentence long. This is to keep the blueprint simple and remove the
    step of aggregation. A review containing multiple sentences can include both positive
    and negative sentiment. Therefore, if we tag all sentences in a review with the
    same sentiment, this would be incorrect. We only use data for some of the categories
    so that it can fit in memory and reduce processing time. This dataset has already
    been prepared, but you can refer to the `Data_Preparation` notebook present in
    the repository to understand the steps and possibly extend it. The blueprints
    work on any kind of dataset, and therefore if you have access to powerful hardware
    or cloud infrastructure, then you can choose more categories.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的蓝图，我们将使用一个包含亚马逊不同产品的客户评论的数据集，涵盖多个产品类别。这个亚马逊客户评论数据集已经由斯坦福大学的研究人员抓取和编译好了。^([1](ch11.xhtml#idm45634178845144))
    [最新版本](https://oreil.ly/QcMIz) 包括了1996年至2018年间从亚马逊网站抓取的产品评论，涵盖了多个类别。它包括产品评论、产品评级以及其他信息，如有用的投票和产品元数据。对于我们的蓝图，我们将专注于产品评论，并仅使用那些只有一句话的评论。这是为了保持蓝图的简单性，并且去掉聚合步骤。一个包含多个句子的评论可能包含积极和消极的情感。因此，如果我们标记一个评论中所有句子具有相同的情感，那将是不正确的。我们只使用部分类别的数据，以便其可以适应内存并减少处理时间。这个数据集已经准备好了，但你可以参考存储库中的
    `Data_Preparation` 笔记本了解步骤并可能扩展它。蓝图适用于任何类型的数据集，因此如果你可以访问强大的硬件或云基础设施，那么你可以选择更多的类别。
- en: 'Let’s now take a look at the dataset:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看一下数据集：
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '`Out:`'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '|   | overall | verified | reviewerID | asin | text | summary |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '|   | overall | verified | reviewerID | asin | text | summary |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| 163807 | 5 | False | A2A8GHFXUG1B28 | B0045Z4JAI | Good Decaf... it has a
    good flavour for a decaf :) | Nice! |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| 163807 | 5 | False | A2A8GHFXUG1B28 | B0045Z4JAI | 不错的无咖啡因... 对于一种无咖啡因咖啡来说味道不错
    :) | 好！ |'
- en: '| 195640 | 5 | True | A1VU337W6PKAR3 | B00K0TIC56 | I could not ask for a better
    system for my small greenhouse, easy to set up and nozzles do very well | I could
    not ask for a better system for my small greenhouse |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| 195640 | 5 | True | A1VU337W6PKAR3 | B00K0TIC56 | 对于我的小温室来说，我无法找到比这个系统更好的选择，设置容易，喷嘴也表现非常好。
    | 对于我的小温室来说，我无法找到比这个系统更好的选择。'
- en: '| 167820 | 4 | True | A1Z5TT1BBSDLRM | B0012ORBT6 | good product at a good
    price and saves a trip to the store | Four Stars |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| 167820 | 4 | True | A1Z5TT1BBSDLRM | B0012ORBT6 | 品质不错的产品，价格合理，省去了一趟商店的旅程。
    | 四星评价 |'
- en: '| 104268 | 1 | False | A4PRXX2G8900X | B005SPI45U | I like the principle of
    a raw chip - something I can eat with my homemade salsa and guac - but these taste
    absolutely revolting. | No better alternatives but still tastes bad. |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| 104268 | 1 | False | A4PRXX2G8900X | B005SPI45U | 我喜欢生的薯片的理念 - 可以和我自制的莎莎酱和鳄梨酱一起吃
    - 但这些味道真是太恶心了。 | 没有更好的选择，但味道仍然很差。'
- en: '| 51961 | 1 | True | AYETYLNYDIS2S | B00D1HLUP8 | Fake China knockoff, you
    get what you pay for. | Definitely not OEM |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| 51961 | 1 | True | AYETYLNYDIS2S | B00D1HLUP8 | 仿制品来自中国，一分钱一分货。 | 绝对不是原装产品
    |'
- en: 'Looking at a summary of the dataset, we can see that it contains the following
    columns:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 查看数据集摘要，我们可以看到它包含以下列：
- en: Overall
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: Overall
- en: This is the final rating provided by the reviewer to the product. Ranges from
    1 (lowest) to 5 (highest).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这是评论者对产品的最终评级。从1（最低）到5（最高）。
- en: Verified
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: Verified
- en: This indicates whether the product purchase has been verified by Amazon.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明产品购买是否经过了亚马逊的验证。
- en: ReviewerID
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ReviewerID
- en: This is the unique identifier allocated by Amazon to each reviewer.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这是亚马逊为每个评论者分配的唯一标识符。
- en: ASIN
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: ASIN
- en: This is a unique product code that Amazon uses to identify the product.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这是亚马逊用来识别产品的唯一产品代码。
- en: Text
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 文本
- en: The actual text in the review provided by the user.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 用户提供的评论中的实际文本。
- en: Summary
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: Summary
- en: This is the headline or summary of the review that the user provided.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这是用户提供的评论的标题或摘要。
- en: The column `text` contains the main content of the customer review and expresses
    the user’s opinion. While the rest of the information can be useful, we will focus
    on using this column in the blueprints.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 列 `text` 包含客户评价的主要内容，表达了用户的观点。尽管其他信息也有用，但我们将专注于在蓝图中使用此列。
- en: 'Blueprint: Performing Sentiment Analysis Using Lexicon-Based Approaches'
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 蓝图：使用基于词典的方法执行情感分析
- en: As an analyst working on the Amazon customer reviews data, the first challenge
    that might come up is the absence of target labels. We do not automatically know
    whether a particular review is positive or negative. Does the text express happiness
    because the product worked perfectly or anger because the product has broken at
    the first use? We cannot determine this until we actually read the review. This
    is challenging because we would have to read close to 300,000 reviews and manually
    assign a target sentiment to each of the reviews. We overcome this problem by
    using a lexicon-based approach.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 作为分析师在亚马逊客户评价数据上工作，可能遇到的第一个挑战是缺少目标标签。我们无法自动知道特定评价是积极还是消极的。文本是因为产品完美运作而表达快乐，还是因为产品在第一次使用时损坏而表达愤怒？直到我们实际阅读评价，我们都无法确定这一点。这是具有挑战性的，因为我们将不得不阅读接近30万条评价，并手动为每一条评价分配目标情感。我们通过使用基于词典的方法来解决这个问题。
- en: 'What is a lexicon? A *lexicon* is like a dictionary that contains a collection
    of words and has been compiled using expert knowledge. The key differentiating
    factor for a lexicon is that it incorporates specific knowledge and has been collected
    for a specific purpose. We will use sentiment lexicons that contain commonly used
    words and capture the sentiment associated with them. A simple example of this
    is the word *happy*, with a sentiment score of 1, and another is the word *frustrated*,
    which would have a score of -1\. Several standardized lexicons are available for
    the English language, and the popular ones are AFINN Lexicon, SentiWordNet, Bing
    Liu’s lexicon, and VADER lexicon, among others. They differ from each other in
    the size of their vocabulary and their representation. For example, the [AFINN
    Lexicon](https://oreil.ly/YZ9WB) comes in the form of a single dictionary with
    3,300 words, with each word assigned a signed sentiment score ranging from -3
    to +3\. Negative/positive indicate the polarity, and the magnitude indicates the
    strength. On the other hand, if we look at [Bing Liu lexicon](https://oreil.ly/jTj_u),
    it comes in the form of two lists: one for positive words and another for negative,
    with a combined vocabulary of 6,800 words. Most sentiment lexicons are available
    for English, but there are also lexicons available for German^([2](ch11.xhtml#idm45634178750888))
    and for 81 other languages as generated by this research paper.^([3](ch11.xhtml#idm45634178748968))'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 什么是词典？*词典* 就像一个包含一系列词汇并使用专家知识编制的字典。词典的关键区别因素在于它包含特定知识并且是为特定目的而收集的。我们将使用包含常用词汇和捕捉与之关联情感的情感词典。一个简单的例子是词汇
    *happy*，情感得分为1，另一个例子是词汇 *frustrated*，其得分为-1。有几种标准化的词典可供使用，流行的包括 AFINN 词典、SentiWordNet、Bing
    Liu 的词典以及 VADER 词典等。它们在词汇量和表达方式上各不相同。例如，[AFINN 词典](https://oreil.ly/YZ9WB) 是一个包含3,300个词汇的单一词典，每个词汇都分配了从-3到+3的有符号情感分数。负/正表示极性，大小表示强度。另一方面，如果我们看
    [Bing Liu 词典](https://oreil.ly/jTj_u)，它以两个列表的形式存在：一个为积极词汇，另一个为消极词汇，总共有6,800个词汇。大多数情感词典适用于英语，但也有适用于德语^([2](ch11.xhtml#idm45634178750888))及其他81种语言的词典，这是由该研究论文生成的^([3](ch11.xhtml#idm45634178748968))。
- en: The sentiment of a sentence or phrase is determined by first identifying the
    sentiment score for each word from the chosen lexicon and then adding them up
    to arrive at the overall sentiment. By using this technique, we avoid the need
    to manually look at each review and assign the sentiment label. Instead, we rely
    on the lexicon that provides expert sentiment scores for each word. For our first
    blueprint, we will use the Bing Liu lexicon, but you are free to extend the blueprint
    to use other lexicons as well. The lexicons normally contain several variants
    of the word and exclude stop words, and therefore the standard preprocessing steps
    are not essential in this approach. Only those words that are present in the lexicon
    will actually be scored. This also leads to one of the disadvantages of this method,
    which we will discuss at the end of the blueprint.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 句子或短语的情感是通过首先从选择的 lexicon 中识别每个单词的情感分数，然后将它们相加以得出整体情感来确定的。通过使用这种技术，我们避免了手动查看每个评论并分配情感标签的需要。相反，我们依赖于
    lexicon，它为每个单词提供专家情感分数。对于我们的第一个蓝图，我们将使用必应刘 lexicon，但您可以自由地扩展蓝图以使用其他 lexicon。 lexicon
    通常包含单词的多个变体并排除停用词，因此在这种方法中标准的预处理步骤并不重要。只有 lexicon 中存在的单词才会真正得分。这也导致了这种方法的一个缺点，我们将在蓝图的末尾讨论它。
- en: Bing Liu Lexicon
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 必应刘 lexicon
- en: The Bing Liu lexicon has been compiled by dividing the words into those that
    express positive opinion and those that express negative opinion. This lexicon
    also contains misspelled words and is more suitable to be used on text that has
    been extracted from online discussion forums, social media, and other such sources
    and should therefore produce better results on the Amazon customer reviews data.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 必应刘 lexicon 已经编制，将单词分成表达积极意见和表达消极意见的两类。这个 lexicon 还包含拼写错误的单词，更适合用于从在线讨论论坛、社交媒体和其他类似来源提取的文本，并因此应该在亚马逊客户评论数据上产生更好的结果。
- en: 'The Bing Liu lexicon is available from the authors’ website as a [zip file](https://oreil.ly/A_O4Q)
    that contains a set of positive and negative words. It is also available within
    the NLTK library as a corpus that we can use after downloading. Once we have extracted
    the lexicon, we will create a dictionary that can hold the lexicon words and their
    corresponding sentiment score. Our next step is to generate the score for each
    review in our dataset. We convert the contents of text to lowercase first; then
    using the `word_tokenize` function from the NLTK package, we split the sentence
    into words and check whether this word is part of our lexicon, and if so, we add
    the corresponding sentiment score of the word to the total sentiment score for
    the review. As the final step, we normalize this score based on the number of
    words in the sentence. This functionality is encapsulated in the function `bing_liu_score`
    and is applied to every review in our dataset:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 必应刘 lexicon 可从作者的网站作为[zip 文件](https://oreil.ly/A_O4Q)获得，其中包含一组积极和消极的单词。它也作为
    NLTK 库中的语料库提供，我们可以在下载后使用。一旦我们提取了 lexicon，我们将创建一个可以保存 lexicon 单词及其相应情感分数的字典。我们的下一步是为数据集中的每个评论生成评分。我们首先将文本内容转换为小写；然后使用
    NLTK 包中的 `word_tokenize` 函数，将句子分割成单词，并检查这个单词是否属于我们的 lexicon，如果是，我们将单词的相应情感分数添加到评论的总情感分数中。作为最后一步，我们基于句子中的单词数量对这个分数进行归一化。这个功能被封装在函数
    `bing_liu_score` 中，并应用于数据集中的每个评论：
- en: '[PRE1]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '`Out:`'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE2]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Then:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 然后：
- en: '[PRE3]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '`Out:`'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '|   | asin | text | Bing_Liu_Score |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '|   | asin | text | Bing_Liu_Score |'
- en: '| --- | --- | --- | --- |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| 188097 | B00099QWOU | As expected | 0.00 |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| 188097 | B00099QWOU | 一如预期 | 0.00 |'
- en: '| 184654 | B000RW1XO8 | Works as designed... | 0.25 |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| 184654 | B000RW1XO8 | 按设计工作... | 0.25 |'
- en: 'Now that we have calculated the sentiment score, we would like to check whether
    the calculated score matches the expectation based on the rating provided by the
    customer. Instead of checking this for each review, we could compare the sentiment
    score across reviews that have different ratings. We would expect that a review
    that has a five-star rating would have a higher sentiment score than a review
    with a one-star rating. In the next step, we scale the score for each review between
    1 and -1 and compute the average sentiment scores across all reviews for each
    type of star rating:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经计算出情感分数，我们想要检查计算出的分数是否符合基于客户提供的评分的预期。我们可以比较具有不同评分的评论的情感分数，而不是对每个评论都进行检查。我们预期，一个五星评价的评论的情感分数会高于一个一星评价的评论。在下一步中，我们将为每个类型的星级评分缩放每个评论的分数在1到-1之间，并计算所有评论的平均情感分数：
- en: '[PRE5]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '`Out:`'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '| overall | Bing_Liu_Score |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| overall | Bing_Liu_Score |'
- en: '| --- | --- |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| 1 | -0.587061 |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| 1 | -0.587061 |'
- en: '| 2 | -0.426529 |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| 2 | -0.426529 |'
- en: '| 4 | 0.344645 |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 0.344645 |'
- en: '| 5 | 0.529065 |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 0.529065 |'
- en: The previous blueprint allows us to use any kind of sentiment lexicon to quickly
    determine a sentiment score and can also serve as a baseline to compare other
    sophisticated techniques, which should improve the accuracy of sentiment prediction.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 前述蓝图使我们能够使用任何类型的情感词汇表快速确定情感分数，并且还可以作为比较其他复杂技术的基准，这应该能提高情感预测的准确性。
- en: Disadvantages of a Lexicon-Based Approach
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于词汇表的方法的缺点
- en: 'While the lexicon-based approach is simple, it has some obvious disadvantages
    that we observed:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管基于词汇表的方法很简单，但我们观察到它有一些明显的缺点：
- en: First, we are bound by the size of the lexicon; if a word does not exist in
    the chosen lexicon, then we are unable to use this information while determining
    the sentiment score for this review. In the ideal scenario, we would like to use
    a lexicon that captures all the words in the language, but this is not feasible.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，我们受限于词汇表的大小；如果一个词不在所选的词汇表中，那么我们无法在确定该评论的情感分数时使用这些信息。在理想情况下，我们希望使用一个涵盖语言中所有单词的词汇表，但这是不可行的。
- en: Second, we assume that the chosen lexicon is a gold standard and trust the sentiment
    score/polarity provided by the author(s). This is a problem because a particular
    lexicon may not be the right fit for a given use case. In the previous example,
    the Bing Liu lexicon is relevant because it captures the online usage of language
    and includes common typos and slang in its lexicon. But if we were working on
    a dataset of tweets, then the VADER lexicon would be better suited since it includes
    support for popular acronyms (e.g., LOL) and emojis.
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其次，我们假设所选的词汇表是一个金标准，并信任作者提供的情感分数/极性。这是一个问题，因为特定的词汇表可能不适合特定的用例。在前面的例子中，Bing Liu词汇表是相关的，因为它捕捉到了在线语言的使用，并在其词汇表中包含了常见的拼写错误和俚语。但如果我们正在处理推文数据集，那么VADER词汇表将更适合，因为它支持流行缩写（例如，LOL）和表情符号。
- en: Finally, one of the biggest disadvantages of lexicons is that they overlook
    negation. Since the lexicon only matches words and not phrases, this would result
    in a negative score for a sentence that contains *not bad* when it actually is
    more neutral.
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，词汇表的最大缺点之一是它忽略了否定词。由于词汇表只匹配单词而不是短语，这将导致包含“not bad”的句子获得负分，而实际上它更中性。
- en: To improve our sentiment detection, we must explore the use of supervised machine
    learning approaches.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 要改进我们的情感检测，我们必须探索使用监督式机器学习方法。
- en: Supervised Learning Approaches
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监督学习方法
- en: The use of a supervised learning approach is beneficial because it allows us
    to model the patterns in the data and create a prediction function that is close
    to reality. It also gives us the flexibility to choose from different techniques
    and identify the one that provides maximum accuracy. A more detailed overview
    of supervised machine learning is provided in [Chapter 6](ch06.xhtml#ch-classification).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 使用监督学习方法是有益的，因为它允许我们对数据中的模式进行建模，并创建一个接近现实的预测函数。它还为我们提供了选择不同技术并确定提供最大准确性的技术的灵活性。有关监督式机器学习的更详细概述，请参阅[第6章](ch06.xhtml#ch-classification)。
- en: To use such an approach, we would need labeled data that may not be easily available.
    Often, it involves two or more human annotators looking at each review and determining
    the sentiment. If the annotators do not agree, then a third annotator might be
    needed to break the deadlock. It is common to have five annotators, with three
    of them agreeing on the opinion to confirm the label. This can be tedious and
    expensive but is the preferred approach when working with real business problems.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用这种方法，我们需要标记数据，这可能不容易得到。通常，需要两个或更多的人类注释者查看每个评论，并确定情感。如果注释者意见不一致，那么可能需要第三个注释者来打破僵局。通常会有五个注释者，其中三个人对意见达成一致以确认标签。这可能会很乏味和昂贵，但在处理实际业务问题时是首选的方法。
- en: However, in many cases we will be able to test a supervised learning approach
    without going through the expensive labeling process. A simpler option is to check
    for any proxy indicators within the data that might help us annotate it automatically.
    Let’s illustrate this in the case of the Amazon reviews. If somebody has given
    a five-star product rating, then we can assume that they liked the product they
    used, and this should be reflected in their review. Similarly, if somebody has
    provided a one-star rating for a product, then they are dissatisfied with it and
    would have some negative things to say. Therefore, we could use the product rating
    as a proxy measure of whether a particular review would be positive or negative.
    The higher the rating, the more positive a particular review should be.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在许多情况下，我们可以在不经过昂贵的标注过程的情况下测试监督学习方法。一个更简单的选择是检查数据中可能帮助我们自动注释的任何代理指标。让我们在亚马逊评论的案例中说明这一点。如果有人给了一个五星级的产品评分，那么我们可以假设他们喜欢他们使用的产品，并且这应该在他们的评论中反映出来。同样，如果有人为一个产品提供了一星评级，那么他们对此不满意，并且可能有一些负面的话要说。因此，我们可以将产品评分作为衡量特定评论是积极还是消极的代理措施。评级越高，特定评论就越积极。
- en: Preparing Data for a Supervised Learning Approach
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备数据以进行监督学习方法
- en: Therefore, as the first step in converting our dataset into a supervised machine
    learning problem, we will automatically annotate our reviews using the rating.
    We have chosen to annotate all reviews with a rating of 4 and 5 as positive and
    with ratings 1 and 2 as negative based on the reasoning provided earlier. In the
    data preparation process, we also filtered out reviews with a rating of 3 to provide
    a clearer separation between positive and negative reviews. This step can be tailored
    based on your use case.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在将我们的数据集转换为监督学习问题的第一步中，我们将使用评级自动注释我们的评论。我们选择将所有评级为4和5的评论标注为积极，并根据之前提供的推理将评级为1和2的评论标注为消极。在数据准备过程中，我们还过滤掉了评级为3的评论，以提供积极和消极评论之间更清晰的分离。这一步骤可以根据您的用例进行定制。
- en: '[PRE6]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '`Out:`'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '|   | verified | asin | text | sentiment |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '|   | verified | asin | text | sentiment |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 176400 | True | B000C5BN72 | everything was as listed and is in use all appear
    to be in good working order | 1 |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| 176400 | True | B000C5BN72 | everything was as listed and is in use all appear
    to be in good working order | 1 |'
- en: '| 65073 | True | B00PK03IVI | this is not the product i received. | 0 |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| 65073 | True | B00PK03IVI | this is not the product i received. | 0 |'
- en: '| 254348 | True | B004AIKVPC | Just like the dealership part. | 1 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| 254348 | True | B004AIKVPC | Just like the dealership part. | 1 |'
- en: 'As you can see from the selection of reviews presented, we have created a new
    column named `sentiment` that contains a value of 1 or 0 depending on the rating
    provided by the user. We can now treat this as a supervised machine learning problem
    where we will use the content present in `text` to predict the sentiment: positive
    (1) or negative (0).'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您从呈现的评论选择中可以看出，我们创建了一个名为`sentiment`的新列，其中包含根据用户提供的评分值为1或0的值。现在我们可以将其视为一个监督学习问题，我们将使用`text`中的内容来预测情感：积极（1）或消极（0）。
- en: 'Blueprint: Vectorizing Text Data and Applying a Supervised Machine Learning
    Algorithm'
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 蓝图：文本数据向量化和应用监督学习算法
- en: In this blueprint, we will build a supervised machine learning algorithm by
    first cleaning the text data, then performing vectorization, and finally applying
    a support vector machine model for the classification.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个蓝图中，我们将通过首先清洗文本数据，然后进行向量化，最后应用支持向量机模型来构建一个监督学习的机器学习算法。
- en: 'Step 1: Data Preparation'
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤1：数据准备
- en: 'To preprocess the data, we will apply the regex blueprint from [Chapter 4](ch04.xhtml#ch-preparation)
    to remove any special characters, HTML tags, and URLs:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 为了预处理数据，我们将应用来自第四章的正则表达式蓝图，以删除任何特殊字符、HTML标签和URL：
- en: '[PRE7]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Then, we will apply the data preparation blueprint from the same chapter that
    uses the spaCy pipeline. This ensures that the text is standardized to lowercase,
    does not include numerals and punctuations, and is in a format that can be used
    by subsequent steps. Please note that it might take a couple of minutes to complete
    execution. In a few cases, it’s possible that all tokens in a review are removed
    during the cleaning step, and it doesn’t make sense to include such reviews anymore:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将应用来自同一章节的数据准备蓝图，该蓝图使用了spaCy流水线。这确保文本被标准化为小写形式，不包括数字和标点，并且格式化为后续步骤可以使用的格式。请注意，执行此步骤可能需要几分钟的时间。在某些情况下，可能在清理步骤中删除了评论中的所有标记，这种情况下不再有必要包括这样的评论：
- en: '[PRE8]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Step 2: Train-Test Split'
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤2：训练-测试分割
- en: 'We split the data so that the next step of vectorization is performed using
    only the training dataset. We create an 80-20 split of the data and confirm that
    the positive and negative classes show a similar distribution across the two splits
    by specifying the target variable, sentiment, as the `stratify` argument:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将数据分割，使得接下来的向量化步骤仅使用训练数据集。我们按照80-20的比例划分数据，并通过指定目标变量情感为`stratify`参数来确认正负类在两个划分中显示出类似的分布：
- en: '[PRE9]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '`Out:`'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE10]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Step 3: Text Vectorization'
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤3：文本向量化
- en: 'The next step is where we convert the cleaned text to usable features. Machine
    learning models do not understand text data and are capable of working only with
    numeric data. We reuse the TF-IDF vectorization blueprint from [Chapter 5](ch05.xhtml#ch-vectorization)
    to create the vectorized representation. We select the parameters of `min_df`
    as 10 and do not include bigrams. In addition, our previous step has already removed
    stop words, and therefore we do not need to take care of this during vectorization.
    We will use the same vectorizer to transform the test split, which will be used
    during evaluation:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是将清理后的文本转换为可用特征的步骤。机器学习模型无法理解文本数据，只能处理数值数据。我们重新使用了TF-IDF向量化的蓝图来创建向量化表示。我们选择了`min_df`参数为10，并且不包括二元组。此外，我们在前一步已经移除了停用词，因此在向量化过程中无需再处理此问题。我们将使用相同的向量化器来转换测试集，该测试集将在评估过程中使用：
- en: '[PRE11]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Step 4: Training the Machine Learning Model'
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤4：训练机器学习模型
- en: 'As described in [Chapter 6](ch06.xhtml#ch-classification), support vector machines
    are the preferred machine learning algorithms when working with text data. SVMs
    are known to work well with datasets with a large number of numeric features,
    and in particular the LinearSVC module we use is quite fast. We can also select
    tree-based methods like random forest or XGBoost, but in our experience the accuracy
    is comparable, and thanks to quick training times, experimentation can be faster:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 如第六章所述，当处理文本数据时，支持向量机是首选的机器学习算法。SVM在处理具有大量数值特征的数据集时表现良好，特别是我们使用的LinearSVC模块非常快速。我们还可以选择基于树的方法，如随机森林或XGBoost，但根据我们的经验，准确性相当，并且由于训练时间快，可以更快地进行实验：
- en: '[PRE12]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '`Out:`'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE13]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Then:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 然后：
- en: '[PRE14]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '`Out:`'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE15]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'As we can see, this model achieves an accuracy of around 86%. Let’s look at
    some of the model predictions and the review text to perform a sense check of
    the model:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，该模型的准确率约为86%。让我们来看一些模型的预测结果和评论文本，以对模型进行一次审查：
- en: '[PRE16]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '`Out:`'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE17]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '|   | text_orig | sentiment_prediction |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '|   | text_orig | sentiment_prediction |'
- en: '| --- | --- | --- |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 29500 | Its a nice night light, but not much else apparently! | 1 |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| 29500 | 这是一个不错的夜灯，但显然用途不多！ | 1 |'
- en: '| 98387 | Way to small, do not know what to do with them or how to use them
    | 0 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| 98387 | 太小了，不知道该怎么做或如何使用它们 | 0 |'
- en: '| 113648 | Didn’t make the room “blue” enough - returned with no questions
    asked | 0 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| 113648 | 没有使房间“足够蓝” - 无条件退回 | 0 |'
- en: '| 281527 | Excellent | 1 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| 281527 | 卓越 | 1 |'
- en: '| 233713 | fit like oem and looks good | 1 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| 233713 | 与OEM相匹配，看起来不错 | 1 |'
- en: We can see that this model is able to predict the reviews reasonably well. For
    example, review 98387 where the user found the product to be too small and unusable
    is marked as negative. Consider review 233713 where the user says that the product
    was fitting well and looks good is marked as positive. How does the model compare
    with a baseline that uses the Bing Liu lexicon?
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，该模型能够合理地预测评论。例如，用户在评论98387中认为产品太小不好用，被标记为负面。再看评论233713，用户表示产品穿着合适且外观不错，被标记为正面。该模型与使用Bing
    Liu词汇表的基准模型相比如何？
- en: '[PRE18]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '`Out:`'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出：`'
- en: '[PRE19]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: It does provide an uplift on the lexicon baseline of 75%, and while the accuracy
    can be improved further, this is a simple blueprint that provides quick results.
    For example, if you’re looking to determine the customer perception of your brand
    versus competitors, then using this blueprint and aggregating sentiments for each
    brand will give you a fair understanding. Or let’s say you want to create an app
    that helps people decide whether to watch a movie. Using this blueprint on data
    collected from Twitter or YouTube comments, you could determine whether people
    feel more positively or negatively and use that to provide a suggestion. In the
    next blueprint, we will describe a more sophisticated technique that can be used
    to improve the accuracy.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 它确实提升了75%的基准模型准确率，虽然准确率还可以进一步提高，但这是一个能够快速产生结果的简单蓝图。例如，如果你想要了解客户对你的品牌与竞争对手的感知，那么使用这个蓝图并聚合每个品牌的情感将会给你一个公平的理解。或者，假设你想要创建一个帮助人们决定是否观看电影的应用程序。使用这个蓝图分析从Twitter或YouTube评论中收集的数据，你可以确定人们的情感倾向，然后提供建议。在下一个蓝图中，我们将描述一种更复杂的技术，可以用来提高准确性。
- en: Pretrained Language Models Using Deep Learning
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用深度学习的预训练语言模型
- en: Languages have evolved over centuries and are still continuously changing. While
    there are rules of grammar and guidelines to forming sentences, these are often
    not strictly followed and depend heavily on context. The words that a person chooses
    while tweeting would be quite different when writing an email to express the same
    thought. And in many languages (including English) the exceptions to the rules
    are far too many! As a result, it is difficult for a computer program to understand
    text-based communication. This can be overcome by giving algorithms a deeper language
    understanding by making use of language models.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 语言在几个世纪以来不断演变，并且仍在不断变化中。虽然有语法规则和形成句子的指导方针，但这些规则通常不严格遵循，且严重依赖于上下文。一个人在发推文时选择的词语与写电子邮件表达相同思想时选择的词语会有很大不同。而且在许多语言（包括英语）中，例外情况实在太多！因此，计算机程序要理解基于文本的交流是很困难的。通过使算法深入理解语言，使用语言模型可以克服这一难题。
- en: Language models are a mathematical representation of natural language that allows
    us to understand the structure of a sentence and the words in it. There are several
    types of language models, but we will focus on the use of pretrained language
    models in this blueprint. The most important characteristic of these language
    models is that they make use of deep neural network architectures and are trained
    on a large corpus of data. The use of language models greatly improves the performance
    of NLP tasks such as language translation, automatic spelling correction, and
    text summarization.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型是自然语言的数学表示，允许我们理解句子的结构和其中的词语。有几种类型的语言模型，但在本蓝图中我们将专注于预训练语言模型的使用。这些语言模型的最重要特征是它们利用深度神经网络架构，并在大型数据语料库上进行训练。语言模型的使用极大地提高了自然语言处理任务的性能，如语言翻译、自动拼写校正和文本摘要。
- en: Deep Learning and Transfer Learning
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度学习和迁移学习
- en: Deep learning is commonly used to describe a set of machine learning methods
    that leverage artificial neural networks (ANNs). ANNs were inspired by the human
    brain and try to mimic the connections and information processing activity between
    neurons in biological systems. Simply explained, it tries to model a function
    using an interconnected network of nodes spanning several layers with the weights
    of the network edges learned with the help of data. For a more detailed explanation,
    please refer to Section II of [*Hands-On Machine Learning*](https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632)
    (O’Reilly, 2019) by Aurélien Géron.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习通常用来描述一组利用人工神经网络（ANNs）的机器学习方法。人工神经网络受人类大脑启发，试图模仿生物系统中神经元之间的连接和信息处理活动。简单来说，它试图使用一个由多层节点组成的互连网络来建模函数，网络边的权重通过数据学习。有关更详细的解释，请参考[*Hands-On
    Machine Learning*](https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632)（O’Reilly，2019）的第II部分，作者是Aurélien
    Géron。
- en: Transfer learning is a technique within deep learning that allows us to benefit
    from pretrained, widely available language models by *transferring* a model to
    our specific use case. It gives us the ability to use the knowledge and information
    obtained in one task and apply it to another problem. As humans, we are good at
    doing this. For example, we initially learn to play the guitar but can then relatively
    easily apply that knowledge to pick up the cello or harp more quickly (than a
    complete beginner). When the same concepts are applied with regard to a machine
    learning algorithm, then it’s referred to as *transfer learning*.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 转移学习是深度学习中的一项技术，允许我们通过将模型*转移*到特定用例来受益于预训练的广泛可用语言模型。它使我们能够利用在一个任务中获得的知识和信息，并将其应用到另一个问题上。作为人类，我们擅长这样做。例如，我们最初学习弹吉他，但随后可以相对容易地应用这些知识来更快地学会大提琴或竖琴（比完全初学者快）。当相同的概念应用于机器学习算法时，就被称为*转移学习*。
- en: This idea was first popularized in the computer vision industry, where a large-scale
    [image recognition challenge](https://oreil.ly/ISv5j) led to several research
    groups competing to build complex neural networks that are several layers deep
    to reduce the error on the challenge. Other researchers discovered that these
    complex models work well not just for that challenge but also on other image recognition
    tasks with small tweaks. These large models had already learned basic features
    about images (think of edges, shapes, etc.) and could be fine-tuned for the specific
    application without the need to train from scratch. In the last two years, the
    same techniques have been successfully applied to text analytics. First, a deep
    neural network is trained on a large text corpus (often derived from publicly
    available data sources like Wikipedia). The chosen model architecture is a variant
    of LSTM or Transformer.^([4](ch11.xhtml#idm45634177455096)) When training these
    models, one word is removed (masked) in the sentence, and the prediction task
    is to determine the masked word given all the other words in the sentence. To
    go back to our human analogy, there might be far more YouTube videos that teach
    you how to play the guitar than the harp or cello. Therefore, it would be beneficial
    to first learn to play the guitar because of the large number of resources available
    and then apply this knowledge to a different task, like playing the harp or cello.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这个想法首次在计算机视觉行业中流行起来，一个大规模的[图像识别挑战](https://oreil.ly/ISv5j)促使几个研究小组竞相建立复杂的深度神经网络，网络层数深达数层，以降低挑战中的错误。其他研究人员发现，这些复杂模型不仅对该挑战有效，还可以通过微小调整适用于其他图像识别任务。这些大型模型已经学习了关于图像的基本特征（如边缘、形状等），可以在不需要从头开始训练的情况下，针对特定应用进行微调。在过去两年中，同样的技术已成功应用于文本分析。首先，在大型文本语料库（通常来自公开可用的数据源，如维基百科）上训练一个深度神经网络。所选择的模型架构是LSTM或Transformer的变体。^([4](ch11.xhtml#idm45634177455096))
    在训练这些模型时，会在句子中去掉一个词（掩码），预测任务是确定给定句子中所有其他词的情况下的掩码词。回到我们的人类类比，也许有更多的YouTube视频教你如何弹吉他而不是竖琴或大提琴。因此，首先学习弹吉他将是有益的，因为有大量的资源可用，然后将这些知识应用到不同的任务，如学习竖琴或大提琴。
- en: Such large models take a lot of time to train and can be time-consuming. Fortunately,
    many research groups have made such pretrained models publicly available, including
    [ULMFiT](https://oreil.ly/ukMdf) from fastai, [BERT](https://oreil.ly/GtSpY) from
    Google, [GPT-2](https://oreil.ly/LVwyy) from OpenAI, and [Turing](https://msturing.org)
    from Microsoft. [Figure 11-1](#fig-transfer-learning) shows the final step of
    applying transfer learning, where the initial layers of the pretrained models
    are kept fixed, and the final layers of the model are retrained to better suit
    the task at hand. In this way, we can apply a pretrained model to specific tasks
    such as text classification and sentiment analysis.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 大型模型训练时间长，耗时较多。幸运的是，许多研究团队已经公开了这些预训练模型，包括来自fastai的[ULMFiT](https://oreil.ly/ukMdf)，来自Google的[BERT](https://oreil.ly/GtSpY)，来自OpenAI的[GPT-2](https://oreil.ly/LVwyy)，以及来自Microsoft的[Turing](https://msturing.org)。[图 11-1](#fig-transfer-learning)展示了应用迁移学习的最后一步，即保持预训练模型的初始层不变，重新训练模型的最终层以更好地适应手头的任务。通过这种方式，我们可以将预训练模型应用于文本分类和情感分析等特定任务。
- en: '![](Images/btap_1101.jpg)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/btap_1101.jpg)'
- en: Figure 11-1\. Transfer learning. The parameters of earlier layers in the network
    are learned by training the model on the large corpus, and the parameters of the
    final layers are unfrozen and allowed to be fine-tuned during the training on
    the specific dataset.
  id: totrans-139
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 11-1\. 迁移学习。网络中较早层的参数通过对大语料进行训练而学习，而最终层的参数则被解冻，并允许在特定数据集上进行微调训练。
- en: For our blueprint we will use the BERT pretrained model released by Google.
    BERT is an acronym for *Bidirectional Encoder Representations from Transformers*.
    It uses the Transformers architecture and trains a model using a large corpus
    of text data. The model that we use in this blueprint (`bert-base-uncased`) is
    trained on the combined English Wikipedia and Books corpus using a Masked Language
    Model (MLM). There are other versions of the BERT model that can be trained on
    different corpora. For example, there is a BERT model trained on German Wikipedia
    articles. The masked language model randomly masks (hides) some of the tokens
    from the input, and the objective is to predict the original vocabulary ID of
    the masked word based only on its context (surrounding words). Since it’s bidirectional,
    the model looks at each sentence in both directions and is able to understand
    context better. In addition, BERT also uses subwords as tokens, which provides
    more granularity when identifying the meaning of a word. Another advantage is
    that BERT generates context-aware embeddings. For example, depending on the surrounding
    words in a sentence where the word *cell* is used, it can have a biological reference
    or actually refer to a prison cell. For a much more detailed understanding of
    how BERT works, please see [“Further Reading”](#ch11_reading).
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的蓝图，我们将使用Google发布的预训练模型BERT。BERT是*双向编码器表示转换*的缩写。它使用Transformers架构，并使用大量文本数据训练模型。在本蓝图中使用的模型(`bert-base-uncased`)是在结合了英文维基百科和Books语料库的基础上，使用掩蔽语言模型（MLM）进行训练的。BERT模型的其他版本可以基于不同语料库进行训练。例如，有一个BERT模型是在德语维基百科文章上训练的。掩蔽语言模型随机掩盖输入中的一些标记（单词），其目标是仅基于上下文（周围单词）预测掩蔽词的原始词汇ID。由于是双向的，模型从两个方向查看每个句子，能够更好地理解上下文。此外，BERT还使用子词作为标记，这在识别单词含义时提供了更精细的控制。另一个优点是BERT生成上下文感知的嵌入。例如，在一个句子中使用单词*cell*时，根据周围单词，它可以具有生物参考或实际上指的是监狱单元的含义。要更详细地了解BERT的工作原理，请参阅[“进一步阅读”](#ch11_reading)。
- en: 'Blueprint: Using the Transfer Learning Technique and a Pretrained Language
    Model'
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 蓝图：使用迁移学习技术和预训练语言模型
- en: This blueprint will show you how we can leverage pretrained language models
    to perform sentiment classification. Consider the use case where you would like
    to take action based on the sentiment expressed. For example, if a customer is
    particularly unhappy, you would like to route them to your best customer service
    representative. It’s important that you are able to detect the sentiment accurately
    or else you risk losing them. Or let’s say you are a small business that relies
    heavily on reviews and ratings on public websites like [Yelp](https://yelp.com).
    To improve your ratings, you would like to follow up with unhappy customers by
    offering them coupons or special services. It’s important to be accurate so that
    you target the right customers. In such use cases, we may not have a lot of data
    to train the model, but having a high accuracy is important. We know that sentiment
    is influenced by the context in which a word is used, and the use of a pretrained
    language model can improve our sentiment predictions. This gives us the ability
    to go beyond the limited dataset that we have to incorporate knowledge from general
    usage.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 这份蓝图将向您展示如何利用预训练语言模型进行情感分类。考虑这样一个使用案例，您希望根据表达的情感采取行动。例如，如果一个客户特别不满意，您希望将他们转接到最优秀的客户服务代表那里。能够准确检测情感非常重要，否则您可能会失去他们。或者，假设您是一个依赖公共网站如[Yelp](https://yelp.com)上的评价和评级的小企业。为了提高评分，您希望通过向不满意的客户提供优惠券或特别服务来跟进。准确性对于定位正确的客户非常重要。在这些使用案例中，我们可能没有大量数据来训练模型，但高准确度是至关重要的。我们知道情感受到词语使用上下文的影响，而使用预训练语言模型可以改善我们的情感预测。这使我们能够超越我们拥有的有限数据集，融入来自一般使用的知识。
- en: In our blueprint we will use the Transformers library because of its easy-to-use
    functionality and wide support for multiple pretrained models. [“Choosing the
    Transformers Library”](#choosing) provides more details about this topic. The
    Transformers library is continuously updated, with multiple researchers contributing.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的蓝图中，我们将使用Transformers库，因为它具有易于使用的功能和对多个预训练模型的广泛支持。["选择Transformers库"](https://wiki.example.org/choosing_transformers_library)提供了关于这个主题的更多详细信息。Transformers库不断更新，多位研究人员在其中贡献。
- en: 'Step 1: Loading Models and Tokenization'
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第一步：加载模型和标记化
- en: The first step when using the Transformers library is to import the three classes
    needed for the chosen model. This includes the *config* class, used to store important
    model parameters; the *tokenizer*, to tokenize and prepare the text for model
    training; and the *model* class, which defines the model architecture and weights.
    These classes are specific to the model architecture, and if we want to use a
    different architecture, then the appropriate classes need to be imported instead.
    We instantiate these classes from a pretrained model and choose the smallest BERT
    model, `bert-base-uncased`, which is 12 layers deep and contains 110 million parameters!
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Transformers库的第一步是导入所选模型所需的三个类。这包括*config*类，用于存储重要的模型参数；*tokenizer*，用于标记化和准备文本进行模型训练；以及*model*类，定义模型架构和权重。这些类特定于模型架构，如果我们想要使用不同的架构，那么需要导入相应的类。我们从预训练模型中实例化这些类，并选择最小的BERT模型，`bert-base-uncased`，它有12层深，并包含1.1亿个参数！
- en: The advantage of using the Transformers library is that it already provides
    multiple pretrained models for many model architectures, which you can [check
    here](https://oreil.ly/QdC7E). When we instantiate a model class from a pretrained
    model, the model architecture and weights are downloaded from an AWS S3 bucket
    hosted by Hugging Face. This might take a while the first time, but it is then
    cached on your machine, which removes the need for subsequent downloads. Note
    that since we are using the pretrained model to predict the sentiment (positive
    versus negative), we specify `finetuning_task='binary'`. We have provided additional
    instructions in the accompanying notebook to ensure that additional Python packages
    are installed before running this blueprint.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Transformers库的优势在于，它已经为许多模型架构提供了多个预训练模型，您可以[在这里查看](https://oreil.ly/QdC7E)。当我们从预训练模型实例化一个模型类时，模型架构和权重将从由Hugging
    Face托管的AWS S3存储桶中下载。这可能会花费一些时间，但在您的机器上缓存后，就不需要再次下载。请注意，由于我们使用预训练模型来预测情感（积极与消极），我们指定`finetuning_task='binary'`。在运行此蓝图之前，我们在附带的笔记本中提供了额外的安装Python包的说明。
- en: '[PRE20]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We have to transform the input text data into a standard format required by
    the model architecture. We define a simple `get_tokens` method to convert the
    raw text of our reviews to numeric values. The pretrained model accepts each observation
    as a fixed length sequence. So, if an observation is shorter than the maximum
    sequence length, then it is padded with empty (zero) tokens, and if it’s longer,
    then it is truncated. Each model architecture has a maximum sequence length that
    it supports. The tokenizer class provides a tokenize function that splits the
    sentence to tokens, pads the sentence to create the fixed-length sequence, and
    finally represents it as a numerical value that can be used during model training.
    This function also adds an attention mask to differentiate those positions where
    we have actual words from those that contain padding characters. Here is an example
    of how this process works:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须将输入文本数据转换为模型架构所需的标准格式。我们定义一个简单的`get_tokens`方法，将我们评论的原始文本转换为数值。预训练模型将每个观察作为固定长度序列接受。因此，如果一个观察比最大序列长度短，则用空（零）标记进行填充，如果它更长，则进行截断。每个模型架构都有一个它支持的最大序列长度。分词器类提供了一个分词函数，它将句子分割成标记，填充句子以创建固定长度序列，并最终表示为可在模型训练期间使用的数值。此函数还添加了注意力掩码，以区分那些包含实际单词的位置和包含填充字符的位置。以下是这个过程如何工作的示例：
- en: '[PRE21]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '`Out:`'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE22]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The first token that we observe is the `[CLS]` token, which stands for classification,
    which is one of the pretraining tasks of the BERT model. This token is used to
    identify the start of a sentence and stores the aggregated representation of the
    entire sentence within the model. We also see the `[SEP]` token at the end of
    the sentence, which stands for *separator*. When BERT is used for nonclassification
    tasks like language translation, each observation would include a pair of texts
    (for example, text in English and text in French), and the `[SEP]` token is used
    to separate the first text from the second. However, since we are building a classification
    model, the separator token is followed by `[PAD]` tokens. We specified the sequence
    length to be 30, and since our test observation was not that long, multiple padding
    tokens have been added at the end. Another interesting observation is that a word
    like *embedding* is not one token but actually split into `em`, `##bed`, `##ding`,
    and `##s`. The `##` is used to identify tokens that are subwords, which is a special
    characteristic of the BERT model. This allows the model to have a better distinction
    between root words, prefixes, and suffixes and also try to infer the meaning of
    words that it may not have seen before.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我们观察到的第一个标记是`[CLS]`标记，它代表分类，这是BERT模型的预训练任务之一。此标记用于标识句子的开始，并在模型内存储整个句子的聚合表示。我们还在句子末尾看到了`[SEP]`标记，它代表*分隔符*。当BERT用于非分类任务（如语言翻译）时，每个观察将包括一对文本（例如，英文文本和法文文本），而`[SEP]`标记用于将第一个文本与第二个文本分隔开。然而，由于我们正在构建一个分类模型，分隔符标记后面跟随着`[PAD]`标记。我们指定了序列长度为30，由于我们的测试观察并不那么长，在末尾添加了多个填充标记。另一个有趣的观察是，像*embedding*这样的词不是一个标记，而实际上被分割成`em`、`##bed`、`##ding`和`##s`。`##`用于识别子词标记，这是BERT模型的一个特殊特性。这使得模型能够更好地区分词根、前缀和后缀，并尝试推断它以前可能没有见过的单词的含义。
- en: 'An important point to note is that since deep learning models use a context-based
    approach, it is advisable to use the text in the original form without any preprocessing,
    thus allowing the tokenizer to produce all possible tokens from its vocabulary.
    As a result, we must split the data again using the original `text_orig` column
    rather than the cleaned `text` column. After that, let’s apply the same function
    to our train and test data and this time use a `max_seq_length` of 50:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 一个重要的注意点是，由于深度学习模型使用基于上下文的方法，建议使用原始形式的文本而不进行任何预处理，这样允许分词器从其词汇表中生成所有可能的标记。因此，我们必须再次使用原始的`text_orig`列而不是清理过的`text`列来分割数据。然后，让我们将相同的函数应用于我们的训练和测试数据，这次使用`max_seq_length`为50：
- en: '[PRE23]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Deep learning models are trained on GPUs using frameworks like [TensorFlow](https://tensorflow.org)
    and [PyTorch](https://pytorch.org). A tensor is the basic data structure used
    by these frameworks to represent and work with data and can store data in N dimensions.
    A simple way to visualize a tensor is by drawing an analogy with a chessboard.
    Let’s suppose that we mark an unoccupied position with 0, a position occupied
    by a white piece with 1, and a position occupied by a black piece with 2\. We
    get an 8 × 8 matrix denoting the status of the chessboard at a given point in
    time. If we now want to track and store this over several moves, then we get multiple
    8 × 8 matrices, which can be stored in what we call a *tensor*. Tensors are n-dimensional
    representations of data, containing an array of components that are functions
    of the coordinates of a space. The tensor that tracks the historical chess moves
    would be a rank 3 tensor, whereas the initial 8 × 8 matrix could also be considered
    a tensor, but with rank 2.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习模型使用像[TensorFlow](https://tensorflow.org)和[PyTorch](https://pytorch.org)这样的框架在GPU上进行训练。张量是这些框架用来表示和处理数据的基本数据结构，可以在N维中存储数据。用象征性的方式来可视化张量，我们可以将其类比为棋盘。假设我们用0标记未占用的位置，用1标记白子占用的位置，用2标记黑子占用的位置。我们得到一个8×8矩阵，表示特定时间点上棋盘的状态。如果我们现在想要跟踪并存储多个动作，我们将得到多个8×8矩阵，这些可以存储在我们所谓的*tensor*中。张量是数据的n维表示，包含一组坐标空间函数的分量数组。跟踪历史棋局动作的张量将是一个3阶张量，而初始的8×8矩阵也可以被认为是张量，但是是一个2阶张量。
- en: 'This is a simplistic explanation, but to get a more in-depth understanding,
    we would recommend reading [“An Introduction to Tensors for Students of Physics
    and Engineering”](https://oreil.ly/VC_80) by Joseph C. Kolecki. In our case, we
    create three tensors that contain the tokens (tensors containing multiple arrays
    of size 50), input masks (tensors containing arrays of size 50), and target labels
    (tensors containing scalars of size 1):'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是一个简单的解释，但是为了更深入地理解，我们建议阅读Joseph C. Kolecki的[“An Introduction to Tensors for
    Students of Physics and Engineering”](https://oreil.ly/VC_80)。在我们的案例中，我们创建了三个张量，包含标记（包含大小为50的多个数组的张量）、输入掩码（包含大小为50的数组的张量）和目标标签（包含大小为1的标量的张量）：
- en: '[PRE24]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '`Out:`'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE25]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We can take a peek at what is in this tensor and see that it contains a mapping
    to the BERT vocabulary for each of the tokens in a sentence. The number 101 indicates
    the start, and 102 indicates the end of the review sentence. We combine these
    tensors together into a TensorDataset, which is the basic data structure used
    to load all observations during model training:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以窥探一下这个张量中的内容，并看到它包含了句子中每个标记对应的BERT词汇映射。数字101表示开始，102表示结束评论句子。我们将这些张量组合成一个TensorDataset，这是模型训练期间用来加载所有观察结果的基本数据结构。
- en: '[PRE26]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '`Out:`'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE27]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Then:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 然后：
- en: '[PRE28]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Step 2: Model Training'
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤 2：模型训练
- en: 'Now that we have preprocessed and tokenized the data, we are ready to train
    the model. Because of the large memory usage and computation demands of deep learning
    models, we follow a different approach compared to the SVM model used in the previous
    blueprint. All training observations are split into batches (defined by `train_batch_size`
    and randomly sampled from all observations using `RandomSampler`) and passed forward
    through the layers of the model. When the model has seen all the training observations
    by going through the batches, it is said to have been trained for one epoch. An
    epoch is therefore one pass through all the observations in the training data.
    The combination of `batch_size` and number of epochs determines how long the model
    takes to train. Choosing a larger `batch_size` reduces the number of forward passes
    in an epoch but might result in higher memory consumption. Choosing a larger number
    of epochs gives the model more time to learn the right value of the parameters
    but will also result in a longer training time. For this blueprint we have defined
    `batch_size` to be 64 and `num_train_epochs` to be 2:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经预处理和标记化了数据，我们准备训练模型。由于深度学习模型的大内存使用和计算需求，我们采用了与前一蓝图中使用的SVM模型不同的方法。所有训练观测数据被分成批次（由`train_batch_size`定义），并从所有观测数据中随机采样（使用`RandomSampler`），然后通过模型的各层向前传递。当模型通过所有批次看到了所有训练观测数据时，就说它已经训练了一个epoch。因此，一个epoch是通过训练数据中的所有观测值的一次传递。`batch_size`的组合和epoch数确定了模型训练的时间长度。选择较大的`batch_size`减少了epoch中的前向传递次数，但可能会导致更高的内存消耗。选择更多的epochs给模型更多时间来学习参数的正确值，但也会导致更长的训练时间。对于这个蓝图，我们定义了`batch_size`为64，`num_train_epochs`为2：
- en: '[PRE29]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '`Out:`'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出：`'
- en: '[PRE30]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'When all the observations in one batch have passed forward through the layers
    of the model, the backpropagation algorithm is applied in the backward direction.
    This technique allows us to automatically compute the gradients for each parameter
    in the neural network, giving us a way to tweak the parameters to reduce the error.
    This is similar to how stochastic gradient descent works, but we do not attempt
    a detailed explanation. Chapter 4 of *Hands-On Machine Learning* (O’Reilly, 2019)
    provides a good introduction and mathematical explanation. The key thing to note
    is that when training a deep learning algorithm, parameters that influence backpropagation
    like the learning rate and choice of optimizer determine how quickly the model
    is able to learn the parameters and reach higher accuracies. However, there isn’t
    a scientific reason for why a certain method or value is better, but a lot of
    researchers^([5](ch11.xhtml#idm45634176762520)) attempt to determine what the
    best options could be. We make informed choices for the blueprint based on the
    parameters in the BERT paper and recommendations in the Transformers library,
    as shown here:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个批次中的所有观测数据通过模型的各层向前传递后，反向传播算法将以反向方向应用。这种技术允许我们自动计算神经网络中每个参数的梯度，从而为我们提供了一种调整参数以减少误差的方法。这类似于随机梯度下降的工作原理，但我们不打算详细解释。《动手学习机器学习》（O’Reilly，2019）第4章提供了一个很好的介绍和数学解释。需要注意的关键点是，在训练深度学习算法时，影响反向传播的参数（如学习率和优化器的选择）决定了模型学习参数并达到更高准确度的速度。然而，并没有科学上的原因说明某种方法或值更好，但许多研究者^([5](ch11.xhtml#idm45634176762520))试图确定最佳选择。根据BERT论文中的参数和Transformers库中的推荐，我们为蓝图做出了明智的选择，如下所示：
- en: '[PRE31]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Before setting up the training loop, we check whether a GPU is available (see
    [“Using GPUs for Free with Google Colab”](#usinggpus)). If so, the model and input
    data are transferred to the GPU, and then we set up the forward pass by running
    the inputs through the model to produce outputs. Since we have specified the labels,
    we already know the deviation from actual (loss), and we adjust the parameters
    using backpropagation that calculates gradients. The optimizer and scheduler steps
    are used to determine the amount of parameter adjustment. Note the special condition
    to clip the gradients to a max value to prevent the problem of [exploding gradients](https://oreil.ly/Ry0Vi).
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在设置训练循环之前，我们检查是否有可用的GPU（见[“在Google Colab免费使用GPU”](#usinggpus)）。如果有，模型和输入数据将被传输到GPU，然后我们通过模型运行输入来设置前向传递以产生输出。由于我们已经指定了标签，我们已经知道与实际情况的偏差（损失），并且我们使用反向传播来调整参数以计算梯度。优化器和调度器步骤用于确定参数调整的量。请注意特殊条件，即将梯度剪裁到最大值，以防止[梯度爆炸](https://oreil.ly/Ry0Vi)问题的出现。
- en: 'We will now wrap all these steps in nested `for` loops—one for each epoch and
    another for each batch in the epoch—and use the TQDM library introduced earlier
    to keep track of the training progress while printing the loss value:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将所有这些步骤包装在嵌套的`for`循环中——一个用于每个时期，另一个用于每个时期中的每个批次——并使用之前介绍的TQDM库来跟踪训练进度，同时打印损失值：
- en: '[PRE32]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The steps we have performed up to now have fine-tuned the parameters of the
    BERT model that we downloaded to fit the sentiment analysis of the Amazon customer
    reviews. If the model is learning the parameter values correctly, you should observe
    that the loss value reduces over multiple iterations. At the end of the training
    step, we can save the model and tokenizer into a chosen output folder:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经对下载的BERT模型进行了参数微调，以适应对亚马逊客户评论的情感分析。如果模型正确学习参数值，您应该观察到损失值在多次迭代中减少。在训练步骤结束时，我们可以将模型和分词器保存到选择的输出文件夹中：
- en: '[PRE33]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Step 3: Model Evaluation'
  id: totrans-178
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第三步：模型评估
- en: 'Evaluating our model on the test data is similar to the training steps, with
    only minor differences. First, we have to evaluate the entire test dataset and
    therefore don’t need to make random samples; instead, we use the `SequentialSampler`
    class to load observations. However, we are still constrained by the number of
    observations we can load at a time and therefore must use `test_batch_size` to
    determine this. Second, we do not need a backward pass or adjustment of parameters
    and only perform the forward pass. The model provides us with output tensors that
    contain the value of loss and output probabilities. We use the `np.argmax` function
    to determine the output label with maximum probability and calculate the accuracy
    by comparing with actual labels:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在测试数据上评估我们的模型类似于训练步骤，只有细微差别。首先，我们必须评估整个测试数据集，因此不需要进行随机抽样；相反，我们使用`SequentialSampler`类加载观测值。然而，我们仍然受限于一次加载的观测数目，因此必须使用`test_batch_size`来确定这一点。其次，我们不需要进行反向传播或调整参数，只执行前向传播。模型为我们提供包含损失值和输出概率值的输出张量。我们使用`np.argmax`函数确定具有最大概率的输出标签，并通过与实际标签比较来计算准确率：
- en: '[PRE34]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '`Out:`'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出：`'
- en: '[PRE35]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: The results for our test data show an increase in model accuracy to 95%—a 10
    percentage point jump compared to our previous baseline with TF-IDF and SVM. These
    are the benefits of using a state-of-the-art language model and is most likely
    a result of BERT being trained using a large corpus of data. The reviews are quite
    short, and the earlier model has only that data to learn a relationship. BERT,
    on the other hand, is context aware and can *transfer* the prior information it
    has about the words in the review. The accuracy can be improved by fine-tuning
    the hyperparameters like `learning_rate` or by training for more epochs. Because
    the number of parameters for pretrained language models far exceeds the number
    of observations we use for fine-tuning, we must be careful to avoid overfitting
    during this process!
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的测试数据结果显示模型准确率提高到95%，比我们先前基于TF-IDF和SVM的基线提高了10个百分点。这些都是使用最先进的语言模型的好处，这很可能是BERT在大型语料库上训练的结果。评论内容相当简短，早期的模型只有这些数据来学习关系。另一方面，BERT是上下文感知的，并且可以将其对评论中单词的先前信息*传递*出来。通过微调`learning_rate`等超参数或增加训练轮次，可以提高准确性。由于预训练语言模型的参数数量远远超过我们用于微调的观测数目，因此在此过程中必须小心避免过拟合！
- en: Using Saved Models
  id: totrans-184
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 使用保存的模型
- en: If you are running the evaluation separately, you can load the fine-tuned model
    directly without the need to train again. Note that this is the same function
    that we initially used to load the pretrained model from transformers, but this
    time we are using the fine-tuned model that we trained ourselves.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您单独运行评估，则可以直接加载微调的模型，而无需再次进行训练。请注意，这与最初用于从transformers加载预训练模型的相同函数相同，但这次我们使用的是我们自己训练的微调模型。
- en: As you can see, using a pretrained language model improves the accuracy of our
    model but also involves many additional steps and can incur costs like the use
    of a GPU (training a useful model on CPU can take 50–100 times longer). The pretrained
    models are quite large and not memory efficient. Using these models in production
    is often more complicated because of the time taken to load millions of parameters
    in memory, and they are inefficient for real-time scenarios because of longer
    inference times. Some pretrained models like [DistilBERT](https://oreil.ly/o4xEU)
    and [ALBERT](https://oreil.ly/m715P) have been specifically developed for a more
    favorable trade-off between accuracy and model simplicity. You can easily try
    this by reusing the blueprint and changing the appropriate model classes to choose
    the `distil-bert-uncased` or `albert-base-v1` model, which is available in the
    Transformers library, to check the accuracy.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，使用预训练语言模型可以提高模型的准确性，但也涉及许多额外步骤，并可能会带来成本，如使用GPU（在CPU上训练一个有用的模型可能需要50到100倍的时间）。预训练模型非常庞大且不够内存高效。在生产中使用这些模型通常更加复杂，因为加载数百万参数到内存中需要时间，并且它们在实时场景中的推理时间较长。一些像[DistilBERT](https://oreil.ly/o4xEU)和[ALBERT](https://oreil.ly/m715P)这样的预训练模型已经专门开发，以在准确性和模型简单性之间取得更有利的权衡。您可以通过重复使用蓝图并更改适当的模型类来轻松尝试此功能，以选择Transformers库中提供的`distil-bert-uncased`或`albert-base-v1`模型，以检查准确性。
- en: Closing Remarks
  id: totrans-187
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结语
- en: In this chapter, we introduced several blueprints that can be used for sentiment
    analysis. They range from simple lexicon-based approaches to complex state-of-the-art
    language models. If your use case is a one-time analysis to determine the sentiment
    of a particular topic using Twitter data, then the first blueprint would be most
    suitable. If you are looking to create a ranking of products/brands using sentiment
    expressed in customer reviews or route customer complaints based on their sentiment,
    then a supervised machine learning approach as described in the second and third
    blueprints would be more suitable. If accuracy is of utmost importance, the best
    results are obtained by using a pretrained language model, but this is also a
    more complicated and expensive technique. Each blueprint is appropriate for a
    given use case, and the crucial thing is to determine which approach is suitable
    for your needs. In general, you must find a method that works well for your use
    case, and the suggestion would always be to keep it simple at the start and then
    increase the complexity to get better results.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了几种可用于情感分析的蓝图。它们从简单的基于词汇的方法到复杂的最新语言模型。如果您的用例是对特定主题使用Twitter数据进行一次性分析以确定情感，则第一个蓝图最合适。如果您希望根据客户评论中表达的情感创建产品/品牌排名或根据情感对客户投诉进行路由，则监督式机器学习方法（如第二和第三个蓝图中描述的方法）更加合适。如果准确性最重要，则使用预训练语言模型可以获得最佳结果，但这也是一种更复杂且昂贵的技术。每个蓝图都适合特定的用例，关键是确定哪种方法适合您的需求。总体而言，您必须找到一种适合您用例的方法，建议始终从简单开始，然后增加复杂性以获得更好的结果。
- en: Further Reading
  id: totrans-189
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: Kolecki, Joseph C. “An Introduction to Tensors for Students of Physics and Engineering.”
    [*https://www.grc.nasa.gov/WWW/k-12/Numbers/Math/documents/Tensors_TM2002211716.pdf*](https://www.grc.nasa.gov/WWW/k-12/Numbers/Math/documents/Tensors_TM2002211716.pdf).
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kolecki, Joseph C. “物理学和工程学学生的张量导论。” [*https://www.grc.nasa.gov/WWW/k-12/Numbers/Math/documents/Tensors_TM2002211716.pdf*](https://www.grc.nasa.gov/WWW/k-12/Numbers/Math/documents/Tensors_TM2002211716.pdf).
- en: McCormick, Chris, and Nick Ryan. “BERT Word Embedding Tutorial.” [*http://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial*](http://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial).
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: McCormick, Chris, and Nick Ryan. “BERT词嵌入教程。” [*http://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial*](http://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial).
- en: Olah, Christopher. “Understanding LSTMs.” [*https://colah.github.io/posts/2015-08-Understanding-LSTMs*](https://colah.github.io/posts/2015-08-Understanding-LSTMs).
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Olah, Christopher. “理解LSTMs。” [*https://colah.github.io/posts/2015-08-Understanding-LSTMs*](https://colah.github.io/posts/2015-08-Understanding-LSTMs).
- en: 'Uszkoreit, Jakob. “Transformer: A Novel Neural Network Architecture for Language
    Understanding.” [*https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html*](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html).'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Uszkoreit, Jakob. “Transformer：一种新颖的神经网络架构用于语言理解。” [*https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html*](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html).
- en: '^([1](ch11.xhtml#idm45634178845144-marker)) J. McAuley and J. Leskovec. “Hidden
    Factors and Hidden Topics: Understanding Rating Dimensions with Review Text.”
    RecSys, 2013\. [*https://snap.stanford.edu/data/web-Amazon.html*](https://snap.stanford.edu/data/web-Amazon.html).'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch11.xhtml#idm45634178845144-marker)) J. McAuley和J. Leskovec。“隐藏因素和隐藏主题：理解评论文本中的评分维度。”
    RecSys，2013\. [*https://snap.stanford.edu/data/web-Amazon.html*](https://snap.stanford.edu/data/web-Amazon.html).
- en: ^([2](ch11.xhtml#idm45634178750888-marker)) “Interest Group on German Sentiment
    Analysis, Multi-Domain Sentiment Lexicon for German,” [*https://oreil.ly/WpMhF*](https://oreil.ly/WpMhF).
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch11.xhtml#idm45634178750888-marker)) “德国情感分析兴趣小组，德语多领域情感词典”，[*https://oreil.ly/WpMhF*](https://oreil.ly/WpMhF).
- en: ^([3](ch11.xhtml#idm45634178748968-marker)) Yanqing Chen and Steven Skiena.
    [*Building Sentiment Lexicons for All Major Languages*](https://oreil.ly/Inbs8).
    Lexicons available on [Kaggle](https://oreil.ly/xTeH4).
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch11.xhtml#idm45634178748968-marker)) Yanqing Chen和Steven Skiena。[*为所有主要语言构建情感词典*](https://oreil.ly/Inbs8)。词典可在[Kaggle](https://oreil.ly/xTeH4)上获取。
- en: ^([4](ch11.xhtml#idm45634177455096-marker)) Ashish Vaswani et al. “Attention
    Is All You Need.” 2017\. [*https://arxiv.org/abs/1706.03762*](https://arxiv.org/abs/1706.03762).
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch11.xhtml#idm45634177455096-marker)) Ashish Vaswani等人。“关注就是一切：Attention
    Is All You Need。” 2017\. [*https://arxiv.org/abs/1706.03762*](https://arxiv.org/abs/1706.03762).
- en: '^([5](ch11.xhtml#idm45634176762520-marker)) Robin M. Schmidt, Frank Schneider,
    and Phillipp Hennig. “Descending through a Crowded Valley: Benchmarking Deep Learning
    Optimizers.” 2020\. [*https://arxiv.org/pdf/2007.01547.pdf*](https://arxiv.org/pdf/2007.01547.pdf).'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch11.xhtml#idm45634176762520-marker)) Robin M. Schmidt, Frank Schneider和Phillipp
    Hennig。“穿越拥挤山谷：深度学习优化器基准测试。” 2020\. [*https://arxiv.org/pdf/2007.01547.pdf*](https://arxiv.org/pdf/2007.01547.pdf).
