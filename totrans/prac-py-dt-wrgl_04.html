<html><head></head><body><section data-pdf-bookmark="Chapter 4. Working with File-Based and Feed-Based Data in Python" data-type="chapter" epub:type="chapter"><div class="chapter" id="chapter4">&#13;
<h1><span class="label">Chapter 4. </span>Working with File-Based and Feed-Based Data in Python</h1>&#13;
&#13;
&#13;
<p>In <a data-type="xref" href="ch03.html#chapter3">Chapter 3</a>, we <a data-primary="data sources" data-secondary="file-based" data-tertiary="feed-based versus" data-type="indexterm" id="data-source-file-feed"/><a data-primary="data sources" data-secondary="feed-based" data-tertiary="file-based versus" data-type="indexterm" id="data-source-feed-file"/><a data-primary="file-based data sources" data-secondary="feed-based versus" data-type="indexterm" id="file-based-feed"/><a data-primary="feed-based data sources" data-secondary="file-based versus" data-type="indexterm" id="feed-based-file"/>focused on the many characteristics that contribute to data quality—from the completeness, consistency, and clarity of data <em>integrity</em> to the reliability, validity, and representativeness of data <em>fit</em>. We discussed the need to both “clean” and standardize data, as well as the need to augment it by combining it with other datasets. But how do we actually accomplish these things in practice?</p>&#13;
&#13;
<p>Obviously, it’s impossible to begin assessing the <em>quality</em> of a dataset without first reviewing its contents—but this is sometimes easier said than done. For decades, data wrangling was a highly specialized pursuit, leading companies and organizations to create a whole range of distinct (and sometimes proprietary) digital data formats designed to meet their particular needs. Often, these formats came with their own file extensions—some of which you may have seen: <em>xls</em>, <em>csv</em>, <em>dbf</em>, and <em>spss</em> are all file formats typically associated with “data” files.<sup><a data-type="noteref" href="ch04.html#idm45143426969488" id="idm45143426969488-marker">1</a></sup> While their specific structures and details vary, all of these formats are what I would describe as <em>file-based</em>—that is, they contain (more or less) historical data in static files that can be downloaded from a database, emailed by a colleague, or accessed via file-sharing sites. Most significantly, a file-based dataset will, for the most part, contain the same information whether you open it today or a week, a month, or a year from now.</p>&#13;
&#13;
<p>Today, these file-based formats stand in contrast to the data formats and interfaces that have emerged alongside real-time web services over the past 20 years. Web-based data today is available for everything from news to weather monitoring to social media sites, and these feed-style data sources have their own unique formats and structures. Extensions like <em>xml</em>, <em>json</em>, and <em>rss</em> indicate this type of real-time data, which often needs to be accessed via specialized application programming interfaces, or APIs. Unlike file-based formats, accessing the same web-based data location or “endpoint” via an API will always show you the most <em>recent</em> data available—and that data may change in days, hours, or even seconds.</p>&#13;
&#13;
<p>These aren’t perfect distinctions, of course. There are many organizations (especially government departments) that provide file-based data for download—but then overwrite those files with new ones that have the same name whenever the source data is updated. At the same time, feed-style data formats <em>can</em> be downloaded and saved for future reference—but their source location online will not generally provide access to older versions. Despite these sometimes unconventional uses for each class of data format, however, in most cases you can use the high-level differences between file-based and feed-based data formats to help you choose the most appropriate sources for a given data wrangling project.</p>&#13;
&#13;
<p>How do you know if you want file-based or feed-based data? In many cases, you won’t have a choice. Social media companies, for example, provide ready access to their data feeds through their APIs but don’t generally provide retrospective data. Other types of data—especially data that is itself synthesized from other sources or heavily reviewed before release—are much more likely to be made available in file-based formats. If you <em>do</em> have a choice between file-based and feed-based formats, then which you choose will really depend on the nature of your data wrangling question: if it hinges on having the most <em>recent</em> data available, then a feed-style format will probably be preferable. But if you’re concerned about <em>trends</em>, file-based data, which is more likely to contain information collected over time, will probably be your best bet. That said, even when both formats are available, there’s no guarantee they contain the same fields, which once again might make your decision for<a data-primary="data sources" data-secondary="file-based" data-startref="data-source-file-feed" data-tertiary="feed-based versus" data-type="indexterm" id="idm45143426959696"/><a data-primary="data sources" data-secondary="feed-based" data-startref="data-source-feed-file" data-tertiary="file-based versus" data-type="indexterm" id="idm45143426958176"/><a data-primary="file-based data sources" data-secondary="feed-based versus" data-startref="file-based-feed" data-type="indexterm" id="idm45143426956688"/><a data-primary="feed-based data sources" data-secondary="file-based versus" data-startref="feed-based-file" data-type="indexterm" id="idm45143426955472"/> you.</p>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="one_data_two_ways">&#13;
<h5>One Data Source, Two Ways</h5>&#13;
<p>We’ve <a data-primary="Citi Bike dataset" data-secondary="file-based versus feed-based" data-type="indexterm" id="citibike-file-feed"/><a data-primary="data sources" data-secondary="file-based" data-tertiary="Citi Bike dataset example" data-type="indexterm" id="data-source-file-citibike"/><a data-primary="data sources" data-secondary="feed-based" data-tertiary="Citi Bike dataset example" data-type="indexterm" id="data-source-feed-citibike"/><a data-primary="file-based data sources" data-secondary="Citi Bike dataset example" data-type="indexterm" id="file-based-citibike"/><a data-primary="feed-based data sources" data-secondary="Citi Bike dataset example" data-type="indexterm" id="feed-based-citibike"/>actually already worked with a data source that is available in both a file-based and a feed-based format: our Citi Bike data from <a data-type="xref" href="ch02.html#chapter2">Chapter 2</a>. In  <a data-type="xref" href="ch02.html#hitting_the_road_intro">“Hitting the Road with Citi Bike Data”</a>, we used a subset of the file-based data to examine how many “Subscriber” versus “Customer” rides were taken by Citi Bike riders on September 1, 2020. For data wrangling questions about trends in ridership and station activity, these retrospective files are invaluable.</p>&#13;
&#13;
<p>At the same time, if we wanted information about how many bikes were available at a particular Citi Bike station <em>right now</em>, that file-based data just can’t help us. But we <em>can</em> access this information using Citi Bike’s <a href="https://gbfs.citibikenyc.com/gbfs/en/station_status.json">real-time <em>json</em> feed</a>. The wall of text you’ll see when you open that link in most web browsers isn’t especially user-friendly,<sup><a data-type="noteref" href="ch04.html#idm45143426940336" id="idm45143426940336-marker">2</a></sup> but it <em>does</em> contain information that the file-based data doesn’t—like a count of currently available bikes and parking spots, as well as how many bikes there are not usable, etc. In order to determine the physical location of a particular <code>station_id</code>, however, you’d need to <em>augment</em> this data with information from either a different Citi Bike data feed or with the file-based data we’ve already used.</p>&#13;
&#13;
<p>In other words, there are plenty of data sources where it makes sense to have both file-based and feed-based data formats—which is better for you will really depend (as always!) on your particular data wrangling <a data-primary="Citi Bike dataset" data-secondary="file-based versus feed-based" data-startref="citibike-file-feed" data-type="indexterm" id="idm45143426937424"/><a data-primary="data sources" data-secondary="file-based" data-startref="data-source-file-citibike" data-tertiary="Citi Bike dataset example" data-type="indexterm" id="idm45143426936160"/><a data-primary="data sources" data-secondary="feed-based" data-startref="data-source-feed-citibike" data-tertiary="Citi Bike dataset example" data-type="indexterm" id="idm45143426934640"/><a data-primary="file-based data sources" data-secondary="Citi Bike dataset example" data-startref="file-based-citibike" data-type="indexterm" id="idm45143426933120"/><a data-primary="feed-based data sources" data-secondary="Citi Bike dataset example" data-startref="feed-based-citibike" data-type="indexterm" id="idm45143426931888"/>question.</p>&#13;
</div></aside>&#13;
&#13;
<p>Over the course of this chapter, we’ll work through hands-on examples of wrangling data from several of the most common file-based and feed-based data formats, with the goal of making them easier to review, clean, augment, and analyze. We’ll also take a look at some of the tougher-to-wrangle data formats that you might need to work with strictly out of necessity. Throughout these processes, we’ll rely heavily on the excellent variety of libraries that the Python community has developed for these purposes, including specialty libraries and programs for processing everything from spreadsheets to images. By the time we finish, you’ll have the skills and sample scripts you need to tackle a huge variety of data wrangling projects, paving the way forward for your next data wrangling project!</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Structured Versus Unstructured Data" data-type="sect1"><div class="sect1" id="structured_vs_unstructured">&#13;
<h1>Structured Versus Unstructured Data</h1>&#13;
&#13;
<p>Before <a data-primary="data sources" data-secondary="structured versus unstructured" data-type="indexterm" id="data-sources-structured-unstructured"/><a data-primary="structured data" data-secondary="unstructured versus" data-type="indexterm" id="structured-data-unstructured"/><a data-primary="unstructured data sources" data-secondary="structured versus" data-type="indexterm" id="unstructured-data-structured"/>we dive into writing code and wrangling data, I want to briefly discuss one other key attribute of data sources that can impact the direction (and speed) of your data wrangling projects—working with <em>structured</em> versus <em>unstructured</em> data.</p>&#13;
&#13;
<p>The goal of most data wrangling projects is to generate insight and, often, to use data to make better decisions. But decisions are time sensitive, so our work with data also requires balancing trade-offs: instead of waiting for the “perfect” dataset, we may combine two or three not-so-perfect ones in order to build a valid approximation of the phenomenon we’re investigating, or we may look for datasets that share common identifiers (for example, zip codes), even if that means we need to later derive the particular dimensional structure (like neighborhood) that truly interests us. As long as we can gain these efficiencies without sacrificing too much in terms of data quality, improving the timeliness of our data work can also increase its impact.</p>&#13;
&#13;
<p>One of the simplest ways to make our data wrangling more efficient is to seek out data formats that are easy for Python and other computational tools to access and understand. Although advances in computer vision, natural language processing, and machine learning have made it easier for computers to analyze data regardless of its underlying structure or format, the fact is that <em>structured</em>, <em>machine-readable</em> data remains—unsurprisingly, perhaps—the most straightforward type of data to work with. In truth, while anything from interviews to images to the text of books can be used as a data source, when many of us think of “data,” we often think of structured, numerical data more than anything else.</p>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45143423530416">&#13;
<h5>What Do We Mean by “Machine-Readable”?</h5>&#13;
<p>Believe <a data-primary="data sources" data-secondary="machine-readable" data-type="indexterm" id="idm45143423529120"/><a data-primary="machine-readable data sources" data-type="indexterm" id="idm45143423528112"/>it or not, the United States actually has a legal definition of “machine-readable” data, thanks to the <a href="https://govinfo.gov/content/pkg/PLAW-115publ435/html/PLAW-115publ435.htm">Foundations for Evidence-Based Policymaking Act</a>, which went into effect in early 2019. According to the law, machine-readable data is:</p>&#13;
<blockquote>&#13;
<p>data in a format that can be easily processed by a computer without human intervention while ensuring no semantic meaning is lost.</p></blockquote>&#13;
&#13;
<p>Helpful, right? While it may seem a bit formal now, as you continue to work with data (especially if you are requesting it from government agencies), you’ll begin to appreciate why this definition is important. Particularly when it comes to dealing with “unstructured” data (like text documents), for example, there is a big difference between data that was generated by a word-processing program and data that was scanned in from a piece of paper. While both are “machine-readable” in the sense that you can view them on a computer, good luck trying to access the underlying text from the scanned document programmatically!<sup><a data-type="noteref" href="ch04.html#idm45143423524592" id="idm45143423524592-marker">3</a></sup> Since there will be times that the folks providing your data might be a little reluctant to do so, a clear (legal!) definition of what “machine-readable” means is actually very valuable.</p>&#13;
</div></aside>&#13;
&#13;
<p><em>Structured</em> data is any type of data that has been organized and classified in some way, into some version of records and fields. In file-based formats, these are usually rows and columns; in feed-based formats they are often (essentially) lists of objects or <em>dict</em>ionaries.</p>&#13;
&#13;
<p><em>Unstructured</em> data, by contrast, may consist of a mash-up of different data types, combining text, numbers, and even photographs or illustrations. The contents of a magazine or a novel, or the waveforms of a song, for example, would typically be considered unstructured data.</p>&#13;
&#13;
<p>If right now you’re thinking, “Hang on, novels have structure! What about chapters?” then congratulations: you’re already thinking like a data wrangler. We can create data about almost anything by collecting information about the world and applying structure <a data-primary="data versus information" data-type="indexterm" id="idm45143423519568"/><a data-primary="information versus data" data-type="indexterm" id="idm45143423518864"/>to it.<sup><a data-type="noteref" href="ch04.html#idm45143423518064" id="idm45143423518064-marker">4</a></sup> And in truth, this is how <em>all</em> data is created: the datasets that we access via files and feeds are all the product of someone’s decisions about how to collect and organize information. In other words, there is always more than one way to organize information, but the structure chosen influences how it can be analyzed. This is why it’s a <em>little</em> ridiculous to suggest that data can somehow be “objective”; after all, it’s the product of (inherently subjective) human choices.</p>&#13;
&#13;
<p>For example, try conducting this mini-experiment: think about how you organize some collection of yours (it could be a collection of music, books, games, or varieties of tea—you name it). Now ask a friend how they organize their own collection of that item. Do you do it the same way? Which is “better”? Now ask someone else, and maybe even a third person. While you may find similarities among the systems that you and your friends use for organizing your music collections, for example, I would be very surprised if you find that any two of you do it precisely the same way. In fact, you’ll probably find that everyone does it a little bit differently but <em>also</em> feels passionately that their way is “best.” And it is! For <em>them</em>.</p>&#13;
&#13;
<p>If this is reminding you of our discussion in <a data-type="xref" href="ch03.html#how_for_whom">“How? And for Whom?”</a>, that’s no coincidence, because the result of your data wrangling question and efforts will eventually be—you guessed it!—another dataset, which will reflect <em>your</em> interests and priorities. It, too, will be structured and organized, which makes working with it in certain ways easier than others. But the takeaway here is not that any given way is right or wrong, just that every choice involves trade-offs. Identifying and acknowledging those trade-offs is a key part of using data honestly and responsibly.</p>&#13;
&#13;
<p>So a key trade-off when using <em>structured</em> data is that it requires depending on someone else’s judgments and priorities in organizing the underlying information. Obviously, this can be a good—or even great!—thing if that data has been structured according to an open, transparent process that involves well-qualified experts. Thoughtfully applied data structures like this can give us early insight into a subject we may otherwise know little to nothing about. On the other hand, there is also the possibility that we will inherit someone else’s biased or poorly designed choices.</p>&#13;
&#13;
<p><em>Unstructured</em> data, of course, gives us complete freedom to organize information into data structures that suit our needs best. Unsurprisingly, this requires <em>us</em> to take responsibility for engaging a robust data quality process, which may be both complex and time-consuming.</p>&#13;
&#13;
<p>How do we know if a particular dataset is structured or unstructured up front? In this case, file extensions can definitely help us out. Feed-based data formats always have at least <em>some</em> structure to them, even if they contain chunks of “free text,” like social media posts. So if you see the file extensions <em>.json</em>, <em>.xml</em>, <em>.rss</em>, or <em>.atom</em>, the data has at least some type of record-and-field structure, as we’ll explore in <a data-type="xref" href="#feed_based_data">“Feed-Based Data—Web-Driven Live Updates”</a>. File-based data that ends in <em>.csv</em>, <em>.tsv</em>, <em>.txt</em>, <em>.xls(x)</em>, or <em>.ods</em> tends to follow a table-type, rows-and-columns structure, as we’ll see in&#13;
the next section.&#13;
Truly unstructured data, meanwhile, is most likely to come to us as <em>.doc(x)</em> or <em>.pdf</em>.</p>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="smart_searching">&#13;
<h5>Smart Searching for Specific Data Types</h5>&#13;
<p>Since<a data-primary="search operators" data-type="indexterm" id="idm45143423497776"/><a data-primary="data sources" data-secondary="searching" data-type="indexterm" id="idm45143423497040"/><a data-primary="searching for specific data types" data-type="indexterm" id="idm45143423496096"/><a data-primary="data types" data-secondary="searching for specific" data-type="indexterm" id="idm45143423495360"/> search engines are the first place most of us turn when we’re looking for information, wouldn’t it be great to be able to use the power of search engines to more efficiently find the data sources (and formats) we need?</p>&#13;
&#13;
<p>While using keywords, sentences, or phrases is a typical way to begin an online search, tweaking those habits just a little can make all the difference when it comes to turning up useful data formats. By mixing in one or more <em>search operators</em>, you can tell your search engine to return results from only specific websites or domain types (like <em>.gov</em>), or even results with specific file extensions. For example:</p>&#13;
<dl>&#13;
<dt><em><code>Search terms or keywords</code></em> <code>filetype: .csv</code></dt>&#13;
<dd>&#13;
<p>You can replace <code>.csv</code> with any file extension you like, including <code>.tsv</code>, <code>.txt</code>, <code>.pdf</code>, or even <code>.jpg</code>, <code>.mp3</code>, etc. This will return files with the specified extension that also match your search terms or keywords.</p>&#13;
</dd>&#13;
<dt><em><code>Search terms or keywords</code></em> <code>site: oreilly.com</code></dt>&#13;
<dd>&#13;
<p>This search will return only results matching your search terms or keywords from within the site <em>oreilly.com</em>. This is particularly handy if you are looking for data created or published by a particular organization. Note that you can also use this to search within an entire <em>top-level domain</em> like <em>.gov</em> or <em>.co.uk</em>.</p>&#13;
</dd>&#13;
<dt><em><code>Search terms or keywords</code></em> <code>inurl: https</code></dt>&#13;
<dd>&#13;
<p>A handy way to locate only secure websites.</p>&#13;
</dd>&#13;
<dt><em><code>Search terms or keywords</code></em> <code>-</code> <em><code>other search terms or keywords</code></em></dt>&#13;
<dd>&#13;
<p>While this won’t help you find specific files, using the hyphen (<code>-</code>) can be a great way to focus your search for information when you specifically want to exclude results that would be commonly associated with your main search term. To see this in action, compare the results of the search <code>steve jobs</code> with <code>steve jobs -apple</code> on your favorite search engine.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>Of course, a search engine is just one way to find data; for more ideas about locating the data you need, check out <a data-type="xref" href="app03.html#appendix_c">Appendix C</a>.</p>&#13;
</div></aside>&#13;
&#13;
<p>Now that we’ve got a good handle on the different types of data sources that we’re likely to encounter—and even some sense of how to locate them—let’s get <a data-primary="data sources" data-secondary="structured versus unstructured" data-startref="data-sources-structured-unstructured" data-type="indexterm" id="idm45143423474496"/><a data-primary="structured data" data-secondary="unstructured versus" data-startref="structured-data-unstructured" data-type="indexterm" id="idm45143423473264"/><a data-primary="unstructured data sources" data-secondary="structured versus" data-startref="unstructured-data-structured" data-type="indexterm" id="idm45143423472032"/>wrangling!</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Working with Structured Data" data-type="sect1"><div class="sect1" id="idm45143426928544">&#13;
<h1>Working with Structured Data</h1>&#13;
&#13;
<p>Since <a data-primary="table-type data" data-secondary="file-based" data-see="file-based data sources" data-type="indexterm" id="idm45143423469168"/><a data-primary="table-type data" data-secondary="feed-based" data-see="feed-based data sources" data-type="indexterm" id="idm45143423467888"/><a data-primary="structured data" data-secondary="file-based" data-see="file-based data sources" data-type="indexterm" id="idm45143423466672"/><a data-primary="structured data" data-secondary="feed-based" data-see="feed-based data sources" data-type="indexterm" id="idm45143423465456"/>the early days of digital computing, the <em>table</em> has been one of the most common ways to structure data. Even today, many of the most common and easy-to-wrangle data formats are little more than tables or collections of tables. In fact, we already worked with one very common table-type data format in <a data-type="xref" href="ch02.html#chapter2">Chapter 2</a>: the <em>.csv</em> or comma-separated value format.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="File-Based, Table-Type Data—Take It to Delimit" data-type="sect2"><div class="sect2" id="delimited_data">&#13;
<h2>File-Based, Table-Type Data—Take It to Delimit</h2>&#13;
&#13;
<p>In <a data-primary="data sources" data-secondary="file-based" data-tertiary="delimited files" data-type="indexterm" id="data-sources-file-delimited"/><a data-primary="file-based data sources" data-secondary="delimited" data-type="indexterm" id="file-based-delimited"/><a data-primary="delimited files" data-secondary="types of" data-type="indexterm" id="delimited-types"/>general, all of the table-type data formats you’ll typically encounter are examples of what are known as <em>delimited</em> files: each data record is on its own line or row, and the boundaries between fields or columns of data values are indicated—or <em>delimited</em>—by a specific text character. Often, an indication of <em>which</em> text character is being used as the <em>delimiter</em>  in a file  is incorporated into the dataset’s file extension. For example, the <em>.csv</em> file extension stands for <em>comma-separated value</em>, because these files use a comma character (<code>,</code>) as a delimiter; the <em>.tsv</em> file extension stands for <em>tab-separated value</em>, because the data columns are separated by a tab. A list of file extensions commonly associated with delimited data follows:</p>&#13;
<dl>&#13;
<dt><em>.csv</em></dt>&#13;
<dd>&#13;
<p><em>Comma-separated value</em> files <a data-primary=".csv files (comma-separated values)" data-primary-sortas="csv files" data-type="indexterm" id="idm45143423449408"/><a data-primary="comma-separated value (.csv) files" data-type="indexterm" id="idm45143423448432"/>are among the most common form of table-type structured data files you’ll encounter. Almost any software system that handles tabular data (such as government or corporate data systems, spreadsheet programs, and even specialized commercial data programs) can output data as a <em>.csv</em>, and, as we saw in <a data-type="xref" href="ch02.html#chapter2">Chapter 2</a>, there are handy libraries for easily working with this data type in Python.</p>&#13;
</dd>&#13;
<dt><em>.tsv</em></dt>&#13;
<dd>&#13;
<p><em>Tab-separated value</em> files <a data-primary=".tsv files (tab-separated values)" data-primary-sortas="tsv files" data-type="indexterm" id="idm45143423444288"/><a data-primary="tab-separated value (.tsv) files" data-type="indexterm" id="idm45143423443264"/>have been around for a long time, but the descriptive <em>.tsv</em> extension has only become common relatively recently. While data providers don’t often explain why they choose one delimiter over another, tab-delimited files may be more common for datasets whose values need to include commas, such as postal addresses.</p>&#13;
</dd>&#13;
<dt><em>.txt</em></dt>&#13;
<dd>&#13;
<p>Structured<a data-primary=".txt files (text)" data-primary-sortas="txt files" data-type="indexterm" id="idm45143423440288"/><a data-primary="text (.txt) files" data-type="indexterm" id="idm45143423439280"/> data files with this extension are often <em>.tsv</em> files in disguise; older data systems often labeled tab-separated data with the <em>.txt</em> extension. As you’ll see in the worked examples that follow, it’s a good idea to open and review <em>any</em> data file you want to wrangle with a basic text program (or a code editor like Atom) before you write any code, since looking at the contents of the file is the only surefire way to know what delimiters you’re working with.</p>&#13;
</dd>&#13;
<dt><em>.xls(x)</em></dt>&#13;
<dd>&#13;
<p>This is <a data-primary=".xls files (Microsoft Excel spreadsheets)" data-primary-sortas="xls files" data-type="indexterm" id="idm45143423435312"/><a data-primary="Microsoft Excel" data-see=".xls  files" data-type="indexterm" id="idm45143423434208"/><a data-primary="spreadsheets" data-secondary=".xls files (Microsoft Excel)" data-secondary-sortas="xls files" data-type="indexterm" id="idm45143423433264"/>the file extension of spreadsheets produced with Microsoft Excel. Because these files can contain multiple “sheets” in addition to formulas, formatting, and other features that simple delimited files cannot replicate, they need more memory to store the same amount of data. They also have other limitations (like only being able to handle a certain number of rows) that can have implications for your dataset’s integrity.</p>&#13;
</dd>&#13;
<dt><em>.ods</em></dt>&#13;
<dd>&#13;
<p><em>Open-document spreadsheet</em> files<a data-primary=".ods files (open-document spreadsheets)" data-primary-sortas="ods files" data-type="indexterm" id="idm45143423429568"/><a data-primary="open-document spreadsheet (.ods) files" data-type="indexterm" id="idm45143423428544"/><a data-primary="spreadsheets" data-secondary=".ods files (open-document spreadsheet)" data-secondary-sortas="ods files" data-type="indexterm" id="idm45143423427856"/> are the default extension for spreadsheets produced by a number of open source software suites like <a href="https://libreoffice.org">LibreOffice</a> and <a href="https://openoffice.org/download/index.html">OpenOffice</a> and have limitations and features similar to those of <em>.xls(x)</em> files.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>Before we dive into how to work with each of these file types in Python, it’s worth spending just a little time thinking about when we might <em>want</em> to work with table-type data and where to find it when<a data-primary="data sources" data-secondary="file-based" data-startref="data-sources-file-delimited" data-tertiary="delimited files" data-type="indexterm" id="idm45143423423632"/><a data-primary="file-based data sources" data-secondary="delimited" data-startref="file-based-delimited" data-type="indexterm" id="idm45143423422144"/><a data-primary="delimited files" data-secondary="types of" data-startref="delimited-types" data-type="indexterm" id="idm45143423420928"/> we do.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="When to work with table-type data" data-type="sect3"><div class="sect3" id="idm45143423419456">&#13;
<h3>When to work with table-type data</h3>&#13;
&#13;
<p>Most of<a data-primary="data sources" data-secondary="file-based" data-tertiary="when to use" data-type="indexterm" id="idm45143423418160"/><a data-primary="file-based data sources" data-secondary="when to use" data-type="indexterm" id="idm45143423416880"/> the time, we don’t get much of a choice about the format of our source data. In fact, much of the reason we need to do data wrangling in the first place is because the data we have doesn’t quite meet our needs. That said, it’s still valuable to know what data format you would <em>prefer</em> to be able to work with so that you can use that to inform your initial search for data.</p>&#13;
&#13;
<p>In <a data-type="xref" href="#structured_vs_unstructured">“Structured Versus Unstructured Data”</a>, we talked about the benefits and limitations of structured data, and we now know that table-type data is one of the oldest and most common forms of machine-readable data. This history means, in part, that over the years many forms of source data have been crammed into tables, even though they may <em>not</em> necessarily be well suited to table-like representations. Still, this format can be especially useful for answering questions about trends and patterns over time. In our Citi Bike exercise from <a data-type="xref" href="ch02.html#chapter2">Chapter 2</a>, for example, we examined how many “Customers” versus “Subscribers” had taken Citi Bike rides over the course of a single month. If we wanted to, we could perform the same calculation for <em>every</em> available month of Citi Bike rides in order to understand any patterns in this ratio over time.</p>&#13;
&#13;
<p>Of course, table-type data is generally not a great format for real-time data, or data where not every observation contains the same possible values. These kinds of data are often better suited to the feed-based data formats that we discuss in <a data-type="xref" href="#feed_based_data">“Feed-Based Data—Web-Driven Live Updates”</a>.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Where to find table-type data" data-type="sect3"><div class="sect3" id="idm45143423409504">&#13;
<h3>Where to find table-type data</h3>&#13;
&#13;
<p>Since<a data-primary="data sources" data-secondary="file-based" data-tertiary="finding" data-type="indexterm" id="idm45143423408208"/><a data-primary="file-based data sources" data-secondary="finding" data-type="indexterm" id="idm45143423406928"/><a data-primary="finding" data-secondary="file-based data sources" data-type="indexterm" id="idm45143423405984"/> the vast majority of machine-readable data is still in table-type data formats, it is among the easiest data formats to locate. Spreadsheets are common in every discipline, and a large number of government and commercial information systems rely on software that organizes data in this way. Almost any time you request data from an expert or organization, a table-type format is what you are likely to get. This is also true of almost every open-data portal and data-sharing site you’ll find online. As we covered in <a data-type="xref" href="#smart_searching">“Smart Searching for Specific Data Types”</a>, you can even find table-type data (and other specific file formats) via search engines, if you know how to look.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Wrangling Table-Type Data with Python" data-type="sect2"><div class="sect2" id="idm45143423403184">&#13;
<h2>Wrangling Table-Type Data with Python</h2>&#13;
&#13;
<p>To help<a data-primary="data sources" data-secondary="file-based" data-see="reading data" data-tertiary="reading data from" data-type="indexterm" id="idm45143423401920"/><a data-primary="file-based data sources" data-secondary="reading data from" data-see="reading data" data-type="indexterm" id="idm45143423400368"/> illustrate how simple it is to work with table-type data in Python, we’ll walk through examples of how to read in data from all of the file types mentioned in this section—plus a few others, just for good measure. While in later chapters we’ll look at how to do more with cleaning, transformation, and data quality assessments, our focus for the time being will simply be on accessing the data within each type of data file and interacting with it using Python.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Reading data from CSVs" data-type="sect3"><div class="sect3" id="idm45143423398416">&#13;
<h3>Reading data from CSVs</h3>&#13;
&#13;
<p>In <a data-primary="data wrangling" data-secondary="reading data" data-tertiary="from .csv files" data-tertiary-sortas="csv files" data-type="indexterm" id="data-wrangling-reading-csv"/><a data-primary="Python" data-secondary="reading data" data-tertiary="from .csv files" data-tertiary-sortas="csv files" data-type="indexterm" id="python-read-csv"/><a data-primary="reading data" data-secondary="from .csv files" data-secondary-sortas="csv files" data-type="indexterm" id="read-data-csv"/><a data-primary=".csv files (comma-separated values)" data-primary-sortas="csv files" data-secondary="reading data from" data-type="indexterm" id="csv-read"/><a data-primary="comma-separated value (.csv) files" data-secondary="reading data from" data-type="indexterm" id="comma-separate-read"/><a data-primary="delimited files" data-secondary="reading data" data-tertiary="from .csv files" data-tertiary-sortas="csv files" data-type="indexterm" id="delimited-read-csv"/>case you didn’t follow along in <a data-type="xref" href="ch02.html#chapter2">Chapter 2</a>, here’s a refresher on how to read data from a <em>.csv</em> file, using a sample from the Citi Bike dataset (<a data-type="xref" href="#csv_parsing">Example 4-1</a>). As always, I’ve included a description of what the program is doing—as well as links to any source files—in the comments at the top of my script. Since we’ve worked with this data format before, for now we’ll just worry about printing out the first few rows of data to see what they look like.</p>&#13;
<div data-type="example" id="csv_parsing">&#13;
<h5><span class="label">Example 4-1. </span>csv_parsing.py</h5>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="c1"># a simple example of reading data from a .csv file with Python</code><code>&#13;
</code><code class="c1"># using the "csv" library.</code><code>&#13;
</code><code class="c1"># the source data was sampled from the Citi Bike system data:</code><code>&#13;
</code><code class="c1"># https://drive.google.com/file/d/17b461NhSjf_akFWvjgNXQfqgh9iFxCu_/</code><code>&#13;
</code><code class="c1"># which can be found here:</code><code>&#13;
</code><code class="c1"># https://s3.amazonaws.com/tripdata/index.html</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># import the `csv` library </code><a class="co" href="#callout_working_with_file_based_and_feed_based_data_in_python_CO1-1" id="co_working_with_file_based_and_feed_based_data_in_python_CO1-1"><img alt="1" src="assets/1.png"/></a><code>&#13;
</code><code class="kn">import</code><code> </code><code class="nn">csv</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># open the `202009CitibikeTripdataExample.csv` file in read ("r") mode</code><code>&#13;
</code><code class="c1"># this file should be in the same folder as our Python script or notebook</code><code>&#13;
</code><code class="n">source_file</code><code> </code><code class="o">=</code><code> </code><code class="nb">open</code><code class="p">(</code><code class="s2">"</code><code class="s2">202009CitibikeTripdataExample.csv</code><code class="s2">"</code><code class="p">,</code><code class="s2">"</code><code class="s2">r</code><code class="s2">"</code><code class="p">)</code><code> </code><a class="co" href="#callout_working_with_file_based_and_feed_based_data_in_python_CO1-2" id="co_working_with_file_based_and_feed_based_data_in_python_CO1-2"><img alt="2" src="assets/2.png"/></a><code>&#13;
</code><code>&#13;
</code><code class="c1"># pass our `source_file` as an ingredient to the `csv` library's</code><code>&#13;
</code><code class="c1"># DictReader "recipe".</code><code>&#13;
</code><code class="c1"># store the result in a variable called `citibike_reader`</code><code>&#13;
</code><code class="n">citibike_reader</code><code> </code><code class="o">=</code><code> </code><code class="n">csv</code><code class="o">.</code><code class="n">DictReader</code><code class="p">(</code><code class="n">source_file</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># the DictReader method has added some useful information to our data,</code><code>&#13;
</code><code class="c1"># like a `fieldnames` property that lets us access all the values</code><code>&#13;
</code><code class="c1"># in the first or "header" row</code><code>&#13;
</code><code class="k">print</code><code class="p">(</code><code class="n">citibike_reader</code><code class="o">.</code><code class="n">fieldnames</code><code class="p">)</code><code> </code><a class="co" href="#callout_working_with_file_based_and_feed_based_data_in_python_CO1-3" id="co_working_with_file_based_and_feed_based_data_in_python_CO1-3"><img alt="3" src="assets/3.png"/></a><code>&#13;
</code><code>&#13;
</code><code class="c1"># let's just print out the first 5 rows</code><code>&#13;
</code><code class="k">for</code><code> </code><code class="n">i</code><code> </code><code class="ow">in</code><code> </code><code class="nb">range</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code><code class="mi">5</code><code class="p">)</code><code class="p">:</code><code> </code><a class="co" href="#callout_working_with_file_based_and_feed_based_data_in_python_CO1-4" id="co_working_with_file_based_and_feed_based_data_in_python_CO1-4"><img alt="4" src="assets/4.png"/></a><code>&#13;
</code><code>    </code><code class="k">print</code><code> </code><code class="p">(</code><code class="nb">next</code><code class="p">(</code><code class="n">citibike_reader</code><code class="p">)</code><code class="p">)</code></pre></div>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_working_with_file_based_and_feed_based_data_in_python_CO1-1" id="callout_working_with_file_based_and_feed_based_data_in_python_CO1-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>This is our workhorse library when it comes to dealing with table-type data.</p></dd>&#13;
<dt><a class="co" href="#co_working_with_file_based_and_feed_based_data_in_python_CO1-2" id="callout_working_with_file_based_and_feed_based_data_in_python_CO1-2"><img alt="2" src="assets/2.png"/></a></dt>&#13;
<dd><p><code>open()</code> is a built-in function that takes a filename and a “mode” as parameters. In this example, the target file (<code>202009CitibikeTripdataExample.csv</code>) should be in the same folder as our Python script or notebook. Values for the “mode” can be <code>r</code> for “read” or <code>w</code> for “write.”</p></dd>&#13;
<dt><a class="co" href="#co_working_with_file_based_and_feed_based_data_in_python_CO1-3" id="callout_working_with_file_based_and_feed_based_data_in_python_CO1-3"><img alt="3" src="assets/3.png"/></a></dt>&#13;
<dd><p>By printing out the <code>citibike_reader.fieldnames</code> values, we can see that the exact label for the “User Type” column is <code>usertype</code>.</p></dd>&#13;
<dt><a class="co" href="#co_working_with_file_based_and_feed_based_data_in_python_CO1-4" id="callout_working_with_file_based_and_feed_based_data_in_python_CO1-4"><img alt="4" src="assets/4.png"/></a></dt>&#13;
<dd><p>The <code>range()</code> function gives us a way to execute some piece of code a specific number of times, starting with the value of the first argument and ending just <em>before</em> the value of the second argument. For example, the code indented below this line will be executed five times, going through the <code>i</code> values of <code>0</code>, <code>1</code>, <code>2</code>, <code>3</code>, and <code>4</code>. For more on the <code>range()</code> function, see <a data-type="xref" href="#add_iterators">“Adding Iterators: The range Function”</a>.</p></dd>&#13;
</dl>&#13;
&#13;
<p>The output from running this should look something like:</p>&#13;
<pre>['tripduration', 'starttime', 'StartDate', 'stoptime', 'start station id',&#13;
'start station name', 'start station latitude', 'start station longitude', 'end&#13;
station id', 'end station name', 'end station latitude', 'end station&#13;
longitude', 'bikeid', 'usertype', 'birth year', 'gender']&#13;
{'tripduration': '4225', 'starttime': '2020-09-01 00:00:01.0430', 'StartDate':&#13;
'2020-09-01', 'stoptime': '2020-09-01 01:10:26.6350', 'start station id':&#13;
'3508', 'start station name': 'St Nicholas Ave &amp; Manhattan Ave', 'start station&#13;
latitude': '40.809725', 'start station longitude': '-73.953149', 'end station&#13;
id': '116', 'end station name': 'W 17 St &amp; 8 Ave', 'end station latitude': '40.&#13;
74177603', 'end station longitude': '-74.00149746', 'bikeid': '44317',&#13;
'usertype': 'Customer', 'birth year': '1979', 'gender': '1'}&#13;
 ...&#13;
{'tripduration': '1193', 'starttime': '2020-09-01 00:00:12.2020', 'StartDate':&#13;
'2020-09-01', 'stoptime': '2020-09-01 00:20:05.5470', 'start station id':&#13;
'3081', 'start station name': 'Graham Ave &amp; Grand St', 'start station&#13;
latitude': '40.711863', 'start station longitude': '-73.944024', 'end station&#13;
id': '3048', 'end station name': 'Putnam Ave &amp; Nostrand Ave', 'end station&#13;
latitude': '40.68402', 'end station longitude': '-73.94977', 'bikeid': '26396',&#13;
'usertype': 'Customer', 'birth year': '1969', 'gender': '0'}</pre>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="add_iterators">&#13;
<h5>Adding Iterators: The range Function</h5>&#13;
<p>Unlike <a data-primary="Python" data-secondary="iterators" data-type="indexterm" id="idm45143423239504"/><a data-primary="iterators" data-type="indexterm" id="idm45143423238496"/><a data-primary="range() function" data-type="indexterm" id="idm45143423237824"/><a data-primary="functions" data-secondary="range()" data-type="indexterm" id="idm45143423237152"/><a data-primary="for loops" data-secondary="range() function in" data-type="indexterm" id="idm45143423236208"/><a data-primary="loops" data-secondary="for loops, range() function in" data-type="indexterm" id="idm45143423235264"/><a data-primary="Python" data-secondary="loops" data-tertiary="for loops, range function() in" data-type="indexterm" id="idm45143423234352"/>many other programming languages, Python’s <code>for</code> loop is designed to run through <em>all</em> values in a list or a dataset by default. While this can be handy for processing entire datasets quickly, it’s not so helpful when we just want to quickly review a handful of rows.</p>&#13;
&#13;
<p>This is where a special type of variable called an <em>iterator</em> comes in. Like any variable, you can name an iterator anything you like, though <code>i</code> (for <em>iterator</em>!) is traditional. As you can see from the preceding example, one place where Python <em>iterators</em> typically appear is within the  <a href="https://docs.python.org/3/tutorial/controlflow.html#the-range-function"><code>range</code> function</a>—another example of a <em>control flow</em> function that, like <code>for</code> loops and <code>if</code> statements, has special, built-in properties.</p>&#13;
&#13;
<p>For example, the <code>range</code> function includes an <em>iterator</em> variable that lets us write a slightly different kind of <code>for</code> loop—one that goes through a certain number of rows, rather than all of them. So rather than taking the form of:</p>&#13;
<pre data-type="programlisting">&#13;
for <em>item</em> in <em>complete_list_of_items</em>:&#13;
</pre>&#13;
&#13;
<p>it lets us write a <code>for</code> loop that starts at a particular point in our list and continues for a certain number of items:</p>&#13;
<pre data-type="programlisting">&#13;
for <em>item_position</em> in range (<em>starting_position</em>, <em>&gt;number_of_places_to_move</em>):&#13;
</pre>&#13;
&#13;
<p>In <a data-type="xref" href="#csv_parsing">Example 4-1</a>, we chose to print the first several rows of the file:</p>&#13;
<pre>for i in range(0,5):&#13;
   print (next(citibike_reader))</pre>&#13;
&#13;
<p>One thing<a data-primary="data wrangling" data-secondary="reading data" data-startref="data-wrangling-reading-csv" data-tertiary="from .csv files" data-tertiary-sortas="csv files" data-type="indexterm" id="idm45143423218992"/><a data-primary="Python" data-secondary="reading data" data-startref="python-read-csv" data-tertiary="from .csv files" data-tertiary-sortas="csv files" data-type="indexterm" id="idm45143423216832"/><a data-primary="reading data" data-secondary="from .csv files" data-secondary-sortas="csv files" data-startref="read-data-csv" data-type="indexterm" id="idm45143423215072"/><a data-primary=".csv files (comma-separated values)" data-primary-sortas="csv files" data-secondary="reading data from" data-startref="csv-read" data-type="indexterm" id="idm45143423213584"/><a data-primary="comma-separated value (.csv) files" data-secondary="reading data from" data-startref="comma-separate-read" data-type="indexterm" id="idm45143423212080"/><a data-primary="delimited files" data-secondary="reading data" data-startref="delimited-read-csv" data-tertiary="from .csv files" data-tertiary-sortas="csv files" data-type="indexterm" id="idm45143423210848"/> to note is that when the <code>range</code> iterates over the values specified in the parentheses, it <em>includes</em> the first number but <em>excludes</em> the second one. This means that in order to print five rows of data, we need to provide an initial value of <code>0</code>, because lists <a data-primary="zero-indexed lists" data-type="indexterm" id="idm45143423207088"/><a data-primary="lists" data-secondary="zero-indexed" data-type="indexterm" id="idm45143423206352"/><a data-primary="Python" data-secondary="lists, zero-indexed" data-type="indexterm" id="idm45143423205408"/>in Python are what’s known as <em>zero-indexed</em>—they start “counting” positions at <code>0</code> rather than <code>1</code>.</p>&#13;
</div></aside>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Reading data from TSV and TXT files" data-type="sect3"><div class="sect3" id="idm45143423241200">&#13;
<h3>Reading data from TSV and TXT files</h3>&#13;
&#13;
<p>Despite its <a data-primary="data wrangling" data-secondary="reading data" data-tertiary="from .tsv files" data-tertiary-sortas="tsv files" data-type="indexterm" id="data-wrangling-reading-tsv"/><a data-primary="Python" data-secondary="reading data" data-tertiary="from .tsv files" data-tertiary-sortas="tsv files" data-type="indexterm" id="python-read-tsv"/><a data-primary="reading data" data-secondary="from .tsv files" data-secondary-sortas="tsv files" data-type="indexterm" id="read-data-tsv"/><a data-primary=".tsv files (tab-separated values)" data-primary-sortas="tsv files" data-secondary="reading data from" data-type="indexterm" id="tsv-read"/><a data-primary="tab-separated value (.tsv) files" data-secondary="reading data from" data-type="indexterm" id="tab-separated-read"/><a data-primary=".txt files (text)" data-primary-sortas="txt files" data-secondary="reading data from" data-type="indexterm" id="txt-read"/><a data-primary="text (.txt) files" data-secondary="reading data from" data-type="indexterm" id="text-file-read"/><a data-primary="delimited files" data-secondary="reading data" data-tertiary="from .tsv files" data-tertiary-sortas="tsv files" data-type="indexterm" id="delimited-read-tsv"/>name, the Python <em>csv</em> library is basically a one-stop shop for wrangling table-type data in Python, thanks to the <code>DictReader</code> function’s <code>delimiter</code> option. Unless you tell it differently, <code>DictReader</code> assumes that the comma character (<code>,</code>) is the separator it should look for. Overriding that assumption is easy, however: you can simply specify a different character when you call the function. In <a data-type="xref" href="#tsv_parsing">Example 4-2</a>, we specify the tab character (<code>\t</code>), but we could easily substitute any delimiter we prefer (or that appears in a particular source file).</p>&#13;
<div data-type="example" id="tsv_parsing">&#13;
<h5><span class="label">Example 4-2. </span>tsv_parsing.py</h5>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="c1"># a simple example of reading data from a .tsv file with Python, using</code><code>&#13;
</code><code class="c1"># the `csv` library. The source data was downloaded as a .tsv file</code><code>&#13;
</code><code class="c1"># from Jed Shugerman's Google Sheet on prosecutor politicians: </code><a class="co" href="#callout_working_with_file_based_and_feed_based_data_in_python_CO2-1" id="co_working_with_file_based_and_feed_based_data_in_python_CO2-1"><img alt="1" src="assets/1.png"/></a><code>&#13;
</code><code class="c1"># https://docs.google.com/spreadsheets/d/1E6Z-jZWbrKmit_4lG36oyQ658Ta6Mh25HCOBaz7YVrA</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># import the `csv` library</code><code>&#13;
</code><code class="kn">import</code><code> </code><code class="nn">csv</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># open the `ShugermanProsecutorPoliticians-SupremeCourtJustices.tsv` file</code><code>&#13;
</code><code class="c1"># in read ("r") mode.</code><code>&#13;
</code><code class="c1"># this file should be in the same folder as our Python script or notebook</code><code>&#13;
</code><code class="n">tsv_source_file</code><code> </code><code class="o">=</code><code> </code><code class="nb">open</code><code class="p">(</code><code class="s2">"</code><code class="s2">ShugermanProsecutorPoliticians-SupremeCourtJustices.tsv</code><code class="s2">"</code><code class="p">,</code><code class="s2">"</code><code class="s2">r</code><code class="s2">"</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># pass our `tsv_source_file` as an ingredient to the csv library's</code><code>&#13;
</code><code class="c1"># DictReader "recipe."</code><code>&#13;
</code><code class="c1"># store the result in a variable called `politicians_reader`</code><code>&#13;
</code><code class="n">politicians_reader</code><code> </code><code class="o">=</code><code> </code><code class="n">csv</code><code class="o">.</code><code class="n">DictReader</code><code class="p">(</code><code class="n">tsv_source_file</code><code class="p">,</code><code> </code><code class="n">delimiter</code><code class="o">=</code><code class="s1">'</code><code class="se">\t</code><code class="s1">'</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># the DictReader method has added some useful information to our data,</code><code>&#13;
</code><code class="c1"># like a `fieldnames` property that lets us access all the values</code><code>&#13;
</code><code class="c1"># in the first or "header" row</code><code>&#13;
</code><code class="k">print</code><code class="p">(</code><code class="n">politicians_reader</code><code class="o">.</code><code class="n">fieldnames</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># we'll use the `next()` function to print just the first row of data</code><code>&#13;
</code><code class="k">print</code><code> </code><code class="p">(</code><code class="nb">next</code><code class="p">(</code><code class="n">politicians_reader</code><code class="p">)</code><code class="p">)</code></pre></div>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_working_with_file_based_and_feed_based_data_in_python_CO2-1" id="callout_working_with_file_based_and_feed_based_data_in_python_CO2-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>This dataset was listed in Jeremy Singer-Vine’s (@jsvine) “Data Is Plural” newsletter (<a href="https://data-is-plural.com"><em class="hyperlink">https://data-is-plural.com</em></a>).</p></dd>&#13;
</dl>&#13;
&#13;
<p>This should result in output that looks something like:</p>&#13;
<pre>['', 'Justice', 'Term Start/End', 'Party', 'State', 'Pres Appt', 'Other Offices&#13;
Held', 'Relevant Prosecutorial Background']&#13;
{'': '40', 'Justice': 'William Strong', 'Term Start/End': '1870-1880', 'Party':&#13;
'D/R', 'State': 'PA', 'Pres Appt': 'Grant', 'Other Offices Held': 'US House,&#13;
Supr Court of PA, elect comm for elec of 1876', 'Relevant Prosecutorial&#13;
Background': 'lawyer'}</pre>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="whats_in_an_extension">&#13;
<h5>What’s in a File Extension?</h5>&#13;
<p>While <a data-primary="file extensions, changing" data-type="indexterm" id="idm45143423156720"/><a data-primary="changing file extensions" data-type="indexterm" id="idm45143423156016"/>having computers take care of certain things for us “automagically” can often be convenient, one thing that learning to wrangle data and program in Python will hopefully make clear is that we have much more influence over how our computers behave than it might first appear.</p>&#13;
&#13;
<p>A perfect example of this is file extensions. Throughout the course of this chapter, I’ve highlighted how file extensions can give us clues about the format of a dataset and even help us search for them more efficiently. Of course, computers <em>also</em> use file extensions to make inferences about the contents of a particular file, typically relying on them to select the appropriate program for opening it. This is why if you’ve ever <em>changed</em> a file extension—whether intentionally or by accident—you’ve probably seen a warning message to the effect of “Are you sure you want to change this? The file might not work properly if you do.” While no doubt well-intentioned, those types of warning messages make it seem like you can actually break or corrupt your files by accidentally changing the file extension.</p>&#13;
&#13;
<p>In fact, nothing could be further from the truth. Changing the extension of a file (for example, from <em>.tsv</em> to <em>.txt</em> or vice versa) does absolutely <em>nothing</em> to change its contents. All it does is change what your computer assumes should be done with it.</p>&#13;
&#13;
<p>Fortunately, Python tools like the ones we’re using don’t make those sorts of assumptions. When we’re working with table-type data, as long as the delimiter we specify matches what’s actually used in the file, the extension on the data file doesn’t matter either way. In fact, the <em>.txt</em> file we’ll use in <a data-type="xref" href="#txt_parsing">Example 4-3</a> was created simply by saving a copy of the <em>.tsv</em> file from <a data-type="xref" href="#tsv_parsing">Example 4-2</a> and changing the extension!</p>&#13;
</div></aside>&#13;
&#13;
<p>Though the <em>.tsv</em> file extension has become relatively common nowadays, many files generated by older databases that are <em>actually</em> tab-separated may reach you with a <em>.txt</em> file extension. Fortunately, as described in&#13;
the preceding sidebar,&#13;
this changes nothing about how we handle the file as long as we specify the correct delimiter—as you can see in <a data-type="xref" href="#txt_parsing">Example 4-3</a>.</p>&#13;
<div data-type="example" id="txt_parsing">&#13;
<h5><span class="label">Example 4-3. </span>txt_parsing.py</h5>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="c1"># a simple example of reading data from a .tsv file with Python, using</code><code>&#13;
</code><code class="c1"># the `csv` library. The source data was downloaded as a .tsv file</code><code>&#13;
</code><code class="c1"># from Jed Shugerman's Google Sheet on prosecutor politicians:</code><code>&#13;
</code><code class="c1"># https://docs.google.com/spreadsheets/d/1E6Z-jZWbrKmit_4lG36oyQ658Ta6Mh25HCOBaz7YVrA</code><code>&#13;
</code><code class="c1"># the original .tsv file was renamed with a file extension of .txt</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># import the `csv` library</code><code>&#13;
</code><code class="kn">import</code><code> </code><code class="nn">csv</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># open the `ShugermanProsecutorPoliticians-SupremeCourtJustices.txt` file</code><code>&#13;
</code><code class="c1"># in read ("r") mode.</code><code>&#13;
</code><code class="c1"># this file should be in the same folder as our Python script or notebook</code><code>&#13;
</code><code class="n">txt_source_file</code><code> </code><code class="o">=</code><code> </code><code class="nb">open</code><code class="p">(</code><code class="s2">"</code><code class="s2">ShugermanProsecutorPoliticians-SupremeCourtJustices.txt</code><code class="s2">"</code><code class="p">,</code><code class="s2">"</code><code class="s2">r</code><code class="s2">"</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># pass our txt_source_file as an ingredient to the csv library's DictReader</code><code>&#13;
</code><code class="c1"># "recipe" and store the result in a variable called `politicians_reader`</code><code>&#13;
</code><code class="c1"># add the "delimiter" parameter and specify the tab character, "\t"</code><code>&#13;
</code><code class="n">politicians_reader</code><code> </code><code class="o">=</code><code> </code><code class="n">csv</code><code class="o">.</code><code class="n">DictReader</code><code class="p">(</code><code class="n">txt_source_file</code><code class="p">,</code><code> </code><code class="n">delimiter</code><code class="o">=</code><code class="s1">'</code><code class="se">\t</code><code class="s1">'</code><code class="p">)</code><code> </code><a class="co" href="#callout_working_with_file_based_and_feed_based_data_in_python_CO3-1" id="co_working_with_file_based_and_feed_based_data_in_python_CO3-1"><img alt="1" src="assets/1.png"/></a><code>&#13;
</code><code>&#13;
</code><code class="c1"># the DictReader function has added useful information to our data,</code><code>&#13;
</code><code class="c1"># like a label that shows us all the values in the first or "header" row</code><code>&#13;
</code><code class="k">print</code><code class="p">(</code><code class="n">politicians_reader</code><code class="o">.</code><code class="n">fieldnames</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># we'll use the `next()` function to print just the first row of data</code><code>&#13;
</code><code class="k">print</code><code> </code><code class="p">(</code><code class="nb">next</code><code class="p">(</code><code class="n">politicians_reader</code><code class="p">)</code><code class="p">)</code></pre></div>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_working_with_file_based_and_feed_based_data_in_python_CO3-1" id="callout_working_with_file_based_and_feed_based_data_in_python_CO3-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>As discussed in <a data-type="xref" href="ch01.html#no_spaces">“Don’t Leave Space!”</a>, whitespace characters have to be <em>escaped</em> when we’re using them in code. Here, we’re using the escaped character for a <code>tab</code>, which is <code>\t</code>. Another common whitespace character code is <code>\n</code> for <code>newline</code> (or <code>\r</code> for <code>return</code>, depending on your device).</p></dd>&#13;
</dl>&#13;
&#13;
<p>If everything has gone well, the output from this script should look exactly the same as that from <a data-type="xref" href="#tsv_parsing">Example 4-2</a>.</p>&#13;
&#13;
<p>One question you may be asking yourself at this point is “How do I know what delimiter my file has?” While there are programmatic ways to help detect this, the simple answer is: Look! Anytime you begin working with (or thinking about working with) a new dataset, start by opening it up in the most basic text program your device has to offer (any code editor will also be a reliable choice). Especially if the file is large, using the simplest program possible will let your device devote maximum memory and processing power to actually reading the data—reducing the likelihood that the program will hang or your device will crash (closing other programs and excess browser tabs will help, too)!</p>&#13;
&#13;
<p>Though I’ll talk about some ways to inspect small parts of <em>really</em> large files later on in the book, now is the time to start practicing the skills that are essential to assessing data quality—all of which require reviewing your data and making judgments about it. So while there <em>are</em> ways to “automate away” tasks like identifying the correct delimiter for your data, eyeballing it in a text editor will often be not just faster and more intuitive, but it will help you get more familiar with other important aspects of the data at the <a data-primary="data wrangling" data-secondary="reading data" data-startref="data-wrangling-reading-tsv" data-tertiary="from .tsv files" data-tertiary-sortas="tsv files" data-type="indexterm" id="idm45143422974272"/><a data-primary="Python" data-secondary="reading data" data-startref="python-read-tsv" data-tertiary="from .tsv files" data-tertiary-sortas="tsv files" data-type="indexterm" id="idm45143422972512"/><a data-primary="reading data" data-secondary="from .tsv files" data-secondary-sortas="tsv files" data-startref="read-data-tsv" data-type="indexterm" id="idm45143422946064"/><a data-primary=".tsv files (tab-separated values)" data-primary-sortas="tsv files" data-secondary="reading data from" data-startref="tsv-read" data-type="indexterm" id="idm45143422944576"/><a data-primary="tab-separated value (.tsv) files" data-secondary="reading data from" data-startref="tab-separated-read" data-type="indexterm" id="idm45143422943120"/><a data-primary=".txt files (text)" data-primary-sortas="txt files" data-secondary="reading data from" data-startref="txt-read" data-type="indexterm" id="idm45143422941936"/><a data-primary="text (.txt) files" data-secondary="reading data from" data-startref="text-file-read" data-type="indexterm" id="idm45143422940448"/><a data-primary="delimited files" data-secondary="reading data" data-startref="delimited-read-tsv" data-tertiary="from .tsv files" data-tertiary-sortas="tsv files" data-type="indexterm" id="idm45143422939232"/>same time.</p>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45143422937344">&#13;
<h5>It’s All Just Text</h5>&#13;
<p>One reason <a data-primary="data formats, text versus nontext" data-type="indexterm" id="idm45143422935792"/><a data-primary="text editors, opening delimited files in" data-type="indexterm" id="idm45143422935088"/><a data-primary="delimited files" data-secondary="as text" data-secondary-sortas="text" data-type="indexterm" id="idm45143422934448"/><a data-primary="nontext data formats" data-type="indexterm" id="idm45143422933232"/><a data-primary="proprietary data formats" data-type="indexterm" id="idm45143422932560"/>why opening <em>.csv</em>, <em>.tsv</em>, and many other data formats in a text editor is so helpful is that <em>most</em> of the data formats we will (or want to) deal with are, at the most basic level, just text. In exactly the same way that written English is organized into sentences and paragraphs through the (somewhat) standardized use of periods, spaces, capital letters, and newlines, what distinguishes one data format from another at a practical level is also just the punctuation used to organize it. As we’ve seen, basic table-type data is separated into fields or columns through the use of <em>delimiters</em>, and into records or rows by using newlines. Feed-type data, which we’ll look at in the next section, is a bit more flexible (and involved) but ultimately follows relatively simple punctuation and structure rules of its own.</p>&#13;
&#13;
<p>Of course, there are plenty of nontext data formats out there, which are usually the output of specialized, <em>proprietary</em> programs that are designed to make data wrangling (and analysis and visualization) possible <em>without</em> writing much, if any, code. While these programs can make very specific data tasks faster or more approachable, the trade-off is that they are often expensive, challenging to learn, and sometimes inflexible. As we’ll see in <a data-type="xref" href="#xlsx_and_others">“XLSX, ODS, and All the Rest”</a> and beyond, getting data <em>out</em> of these proprietary formats can also be difficult, unreliable, or even impossible. So while Python can still help us wrangle some of the most common proprietary data formats, in some cases using alternate software (or enlisting someone else’s help) is our best option.</p>&#13;
</div></aside>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Real-World Data Wrangling: Understanding Unemployment" data-type="sect1"><div class="sect1" id="introducing_fred">&#13;
<h1>Real-World Data Wrangling: Understanding Unemployment</h1>&#13;
&#13;
<p>The <a data-primary="data wrangling" data-secondary="reading data" data-tertiary="from unemployment example dataset" data-tertiary-sortas="unemploy" data-type="indexterm" id="data-wrangling-read-unemploy"/><a data-primary="Python" data-secondary="reading data" data-tertiary="from unemployment example dataset" data-tertiary-sortas="unemploy" data-type="indexterm" id="python-read-unemploy"/><a data-primary="reading data" data-secondary="from unemployment example dataset" data-secondary-sortas="unemploy" data-type="indexterm" id="read-data-unemploy"/><a data-primary="unemployment example" data-type="indexterm" id="unemployment"/><a data-primary="data sources" data-secondary="file-based" data-tertiary="unemployment example dataset" data-type="indexterm" id="data-source-file-unemploy"/><a data-primary="file-based data sources" data-secondary="unemployment example dataset" data-type="indexterm" id="file-based-unemploy"/>underlying dataset that we’ll use to explore some of our trickier table-type data formats is unemployment data about the United States. Why? In one way or another, unemployment affects most of us, and in recent decades the US has experienced some particularly high unemployment rates. Unemployment numbers for the US are released monthly by the Bureau of Labor Statistics (BLS), and while they are often reported by general-interest news sources, they are usually treated as a sort of abstract indicator of how “the economy” is doing. What the numbers really represent is rarely discussed in-depth.</p>&#13;
&#13;
<p>When I first joined the <em>Wall Street Journal</em> in 2007, building an interactive dashboard for exploring monthly economic indicator data—including unemployment—was my first major project. One of the more interesting things I learned in the process is that there isn’t “an” unemployment rate calculated each month, there are <em>several</em> (six, to be exact). The one that usually gets reported by news sources is the so-called “U3” unemployment rate, which the BLS describes as:</p>&#13;
<blockquote>&#13;
<p>Total unemployed, as a percent of the civilian labor force (official unemployment rate).</p></blockquote>&#13;
&#13;
<p>On its surface, this seems like a straightforward definition of unemployment: of all the people who reasonably <em>could</em> be working, what percentage are not?</p>&#13;
&#13;
<p>Yet the real story is a bit more complex. What does it mean to be “employed” or be counted as part of the “labor force”? A look at different unemployment numbers makes more clear what the “U3” number does <em>not</em> take into account. The “U6” unemployment rate is defined as:</p>&#13;
<blockquote>&#13;
<p>Total unemployed, plus all persons marginally attached to the labor force, plus total employed part time for economic reasons, as a percent of the civilian labor force plus all persons marginally attached to the labor force.</p></blockquote>&#13;
&#13;
<p>When we read the accompanying note, this longer definition starts to take shape:<sup><a data-type="noteref" href="ch04.html#idm45143422908112" id="idm45143422908112-marker">5</a></sup></p>&#13;
<blockquote>&#13;
<p>NOTE: Persons marginally attached to the labor force are those who currently are neither working nor looking for work but indicate that they want and are available for a job and have looked for work sometime in the past 12 months. Discouraged workers, a subset of the marginally attached, have given a job-market related reason for not currently looking for work. Persons employed part time for economic reasons are those who want and are available for full-time work but have had to settle for a part-time schedule. Updated population controls are introduced annually with the release of January data.</p></blockquote>&#13;
&#13;
<p>In other words, if you <em>want</em> a job (and have looked for one in the past year) but haven’t looked for one very recently—or if you have a part-time job but <em>want</em> a full-time job—then you don’t officially count as “unemployed” in the U3 definition. This means that the economic reality of Americans working multiple jobs (who are more likely to be women and have more children)<sup><a data-type="noteref" href="ch04.html#idm45143422903792" id="idm45143422903792-marker">6</a></sup>, and potentially of “gig” workers (recently estimated as up to 30% of the American workforce),<sup><a data-type="noteref" href="ch04.html#idm45143422901552" id="idm45143422901552-marker">7</a></sup> are not necessarily reflected in the U3 number. Unsurprisingly, the U6 rate is typically several percentage points higher each month than the U3 rate.</p>&#13;
&#13;
<p>To see how these rates compare over time, we can download them from the website of the St. Louis Federal Reserve, which provides thousands of economic datasets for download in a range of formats, including table-type <em>.xls(x)</em> files and, as we’ll see later in <a data-type="xref" href="#xml_parsing">Example 4-12</a>, feed-type formats as well.</p>&#13;
&#13;
<p>You can download the data for these exercises from the <a href="https://fred.stlouisfed.org/series/U6RATE">Federal Reserve Economic Database (FRED) website</a>. It shows the current U6 unemployment rate since the measure was first created in the early 1990s.</p>&#13;
&#13;
<p>To add the U3 rate to this graph, at the top right choose Edit graph → ADD LINE. In the search field, type <strong><code>UNRATE</code></strong> and then select “Unemployment Rate” when it populates below the search bar. Finally, click Add series. Close this side window using the X at the top right, and then select Download, being sure to select the first option, Excel.<sup><a data-type="noteref" href="ch04.html#idm45143422892896" id="idm45143422892896-marker">8</a></sup> This will be an <em>.xls</em> file, which we’ll handle last because although still widely available, this is a relatively outdated file format (it was replaced by <em>.xlsx</em> as the default format for <a class="orm:hideurl" href="https://en.wikipedia.org/wiki/Microsoft_Excel#File_formats">Microsoft Excel spreadsheets in 2007</a>).</p>&#13;
&#13;
<p>To get the additional file formats we need, just open the file you downloaded with a spreadsheet program like Google Sheets and choose “Save As,” then select <em>.xlsx</em>, then repeat the process choosing <em>.ods</em>. You should now have the following three files, all containing the same information: <em>fredgraph.xlsx</em>, <em>fredgraph.ods</em>, and <em>fredgraph.xls</em>.<sup><a data-type="noteref" href="ch04.html#idm45143422886480" id="idm45143422886480-marker">9</a></sup></p>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>If you opened the original <em>fredgraph.xls</em> file, you probably noticed that it contains more than just the unemployment data; it also contains some header information about where the data came from and the definitions of U3 and U6 unemployment, for example. While doing <em>analysis</em> on the unemployment rates these files contain would require separating this metadata from the table-type data further down, remember that our goal for the moment is simply to convert all of our various files to a <em>.csv</em> format. We’ll tackle the data cleaning process that would involve removing this<a data-primary="data wrangling" data-secondary="reading data" data-startref="data-wrangling-read-unemploy" data-tertiary="from unemployment example dataset" data-tertiary-sortas="unemploy" data-type="indexterm" id="idm45143422882384"/><a data-primary="Python" data-secondary="reading data" data-startref="python-read-unemploy" data-tertiary="from unemployment example dataset" data-tertiary-sortas="unemploy" data-type="indexterm" id="idm45143422880560"/><a data-primary="reading data" data-secondary="from unemployment example dataset" data-secondary-sortas="unemploy" data-startref="read-data-unemploy" data-type="indexterm" id="idm45143422878784"/><a data-primary="unemployment example" data-startref="unemployment" data-type="indexterm" id="idm45143422877280"/><a data-primary="data sources" data-secondary="file-based" data-startref="data-source-file-unemploy" data-tertiary="unemployment example dataset" data-type="indexterm" id="idm45143422876336"/><a data-primary="file-based data sources" data-secondary="unemployment example dataset" data-startref="file-based-unemploy" data-type="indexterm" id="idm45143422874816"/> metadata in <a data-type="xref" href="ch07.html#chapter7">Chapter 7</a>.</p>&#13;
</div>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="XLSX, ODS, and All the Rest" data-type="sect2"><div class="sect2" id="xlsx_and_others">&#13;
<h2>XLSX, ODS, and All the Rest</h2>&#13;
&#13;
<p>For the<a data-primary="data wrangling" data-secondary="reading data" data-tertiary="from .xls files" data-tertiary-sortas="xls files" data-type="indexterm" id="data-wrangling-reading-xls"/><a data-primary="Python" data-secondary="reading data" data-tertiary="from .xls files" data-tertiary-sortas="xls files" data-type="indexterm" id="python-read-xls"/><a data-primary="reading data" data-secondary="from .xls files" data-secondary-sortas="xls files" data-type="indexterm" id="read-data-xls"/><a data-primary="data wrangling" data-secondary="reading data" data-tertiary="from .ods files" data-tertiary-sortas="ods files" data-type="indexterm" id="data-wrangling-reading-ods"/><a data-primary="Python" data-secondary="reading data" data-tertiary="from .ods files" data-tertiary-sortas="ods files" data-type="indexterm" id="python-read-ods"/><a data-primary="reading data" data-secondary="from .ods files" data-secondary-sortas="ods files" data-type="indexterm" id="read-data-ods"/><a data-primary=".xls files (Microsoft Excel spreadsheets)" data-primary-sortas="xls files" data-secondary="reading data from" data-type="indexterm" id="xls-read"/><a data-primary="spreadsheets" data-secondary=".xls files (Microsoft Excel)" data-secondary-sortas="xls files" data-tertiary="reading data from" data-type="indexterm" id="spreadsheet-MSExcel-read"/><a data-primary=".ods files (open-document spreadsheets)" data-primary-sortas="ods files" data-secondary="reading data from" data-type="indexterm" id="ods-read"/><a data-primary="open-document spreadsheet (.ods) files" data-secondary="reading data from" data-type="indexterm" id="open-doc-read"/><a data-primary="spreadsheets" data-secondary=".ods files (open-document spreadsheet)" data-secondary-sortas="ods files" data-tertiary="reading data from" data-type="indexterm" id="spreadsheet-ods-read"/><a data-primary="delimited files" data-secondary="reading data" data-tertiary="from .xls files" data-tertiary-sortas="xls files" data-type="indexterm" id="delimited-read-xls"/><a data-primary="delimited files" data-secondary="reading data" data-tertiary="from .ods files" data-tertiary-sortas="ods files" data-type="indexterm" id="delimited-read-ods"/> most part, it’s preferable to avoid processing data saved as <em>.xlsx</em>, <em>.ods</em>, and most other nontext table-type data formats directly, if possible. If you’re just at the stage of exploring datasets, I suggest you review these files simply by opening them with your preferred spreadsheet program and saving them as a <em>.csv</em> or <em>.tsv</em> file format before accessing them in Python. Not only will this make them easier to work with, it will give you a chance to actually look at the contents of your data file and get a sense of what it contains.</p>&#13;
&#13;
<p>Resaving and reviewing <em>.xls(x)</em> and similar data formats as a <em>.csv</em> or equivalent text-based file format will both reduce the file size <em>and</em> give you a better sense of what the “real” data looks like. Because of the formatting options in spreadsheet programs, sometimes what you see onscreen is substantially different from the raw values that are stored in the actual file. For example, values that appear as percentages in a spreadsheet program (e.g., 10%) might actually be decimals (.1). This can lead to problems if you try to base aspects of your Python processing or analysis on what you saw in the spreadsheet as opposed to a text-based data format like <em>.csv</em>.</p>&#13;
&#13;
<p>Still, there will definitely be situations where you need to access <em>.xls(x)</em> and similar file types with Python directly.<sup><a data-type="noteref" href="ch04.html#idm45143422842640" id="idm45143422842640-marker">10</a></sup> For example, if there’s an <em>.xls</em> dataset you need to wrangle on a regular basis (say, every month), resaving the file manually each time would become unnecessarily time-consuming.</p>&#13;
&#13;
<p>Fortunately, that active Python community we talked about in <a data-type="xref" href="ch01.html#community">“Community”</a> has created libraries that can handle an impressive range of data formats with ease. To get a thorough feel for how these libraries work with more complex source data (and data formats), the following code examples read in the specified file format and then create a <em>new</em> <em>.csv</em> file that contains the same data.</p>&#13;
&#13;
<p>To make use of these libraries, however, you’ll first need to install them on your device by running the following commands one by one in a terminal window:<sup><a data-type="noteref" href="ch04.html#idm45143422837216" id="idm45143422837216-marker">11</a></sup></p>&#13;
<pre>pip install openpyxl&#13;
pip install pyexcel-ods&#13;
pip install xlrd==2.0.1</pre>&#13;
&#13;
<p>In the following code examples, we’ll <a data-primary="openpyxl library" data-type="indexterm" id="openpyxl"/><a data-primary="pyexcel-ods library" data-type="indexterm" id="idm45143422834384"/><a data-primary="xlrd library" data-type="indexterm" id="idm45143422833712"/>be using the <em>openpyxl</em> library to access (or <em>parse</em>) <em>.xlsx</em> files, the <em>pyexcel-ods</em> library for dealing with <em>.ods</em> files, and the <em>xlrd</em> library for reading from <em>.xls</em> files (for more on finding and selecting Python libraries, see <a data-type="xref" href="app01.html#where_to_find_libraries">“Where to Look for Libraries”</a>).</p>&#13;
&#13;
<p>To better illustrate the idiosyncrasies of these different file formats, we’re going to  do something similar to what we did in <a data-type="xref" href="#txt_parsing">Example 4-3</a>: we’ll take sample data that is being provided as an <em>.xls</em> file and create <em>.xlsx</em> and <em>.ods</em> files containing <em>the exact same data</em> by resaving that source file in the other formats using a spreadsheet program. Along the way, I think you’ll start to get a sense of how these nontext formats make the process of data wrangling more (and, I would argue, unnecessarily) complicated.</p>&#13;
&#13;
<p>We’ll start by working through an <em>.xlsx</em> file in  <em>\ref</em> (<a data-type="xref" href="#xlsx_parsing">Example 4-4</a>), using a version of the unemployment data downloaded from FRED. This example illustrates one of the first major differences between dealing with text-based table-type data files and nontext formats: because the nontext formats support multiple “sheets,” we needed to include a <code>for</code> loop at the top of our script, <em>within</em> which we put the code for creating our individual output files (one for each sheet).</p>&#13;
<div data-type="example" id="xlsx_parsing">&#13;
<h5><span class="label">Example 4-4. </span>xlsx_parsing.py</h5>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="c1"># an example of reading data from an .xlsx file with Python, using the "openpyxl"</code><code>&#13;
</code><code class="c1"># library. First, you'll need to pip install the openpyxl library:</code><code>&#13;
</code><code class="c1"># https://pypi.org/project/openpyxl/</code><code>&#13;
</code><code class="c1"># the source data can be composed and downloaded from:</code><code>&#13;
</code><code class="c1"># https://fred.stlouisfed.org/series/U6RATE</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># specify the "chapter" you want to import from the "openpyxl" library</code><code>&#13;
</code><code class="c1"># in this case, "load_workbook"</code><code>&#13;
</code><code class="kn">from</code><code> </code><code class="nn">openpyxl</code><code> </code><code class="kn">import</code><code> </code><code class="n">load_workbook</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># import the `csv` library, to create our output file</code><code>&#13;
</code><code class="kn">import</code><code> </code><code class="nn">csv</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># pass our filename as an ingredient to the `openpyxl` library's</code><code>&#13;
</code><code class="c1"># `load_workbook()` "recipe"</code><code>&#13;
</code><code class="c1"># store the result in a variable called `source_workbook`</code><code>&#13;
</code><code class="n">source_workbook</code><code> </code><code class="o">=</code><code> </code><code class="n">load_workbook</code><code class="p">(</code><code class="n">filename</code><code> </code><code class="o">=</code><code> </code><code class="s1">'</code><code class="s1">fredgraph.xlsx</code><code class="s1">'</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># an .xlsx workbook can have multiple sheets</code><code>&#13;
</code><code class="c1"># print their names here for reference</code><code>&#13;
</code><code class="k">print</code><code class="p">(</code><code class="n">source_workbook</code><code class="o">.</code><code class="n">sheetnames</code><code class="p">)</code><code> </code><a class="co" href="#callout_working_with_file_based_and_feed_based_data_in_python_CO4-1" id="co_working_with_file_based_and_feed_based_data_in_python_CO4-1"><img alt="1" src="assets/1.png"/></a><code>&#13;
</code><code>&#13;
</code><code class="c1"># loop through the worksheets in `source_workbook`</code><code>&#13;
</code><code class="k">for</code><code> </code><code class="n">sheet_num</code><code class="p">,</code><code> </code><code class="n">sheet_name</code><code> </code><code class="ow">in</code><code> </code><code class="nb">enumerate</code><code class="p">(</code><code class="n">source_workbook</code><code class="o">.</code><code class="n">sheetnames</code><code class="p">)</code><code class="p">:</code><code> </code><a class="co" href="#callout_working_with_file_based_and_feed_based_data_in_python_CO4-2" id="co_working_with_file_based_and_feed_based_data_in_python_CO4-2"><img alt="2" src="assets/2.png"/></a><code>&#13;
</code><code>&#13;
</code><code>    </code><code class="c1"># create a variable that points to the current worksheet by</code><code>&#13;
</code><code>    </code><code class="c1"># passing the current value of `sheet_name` to `source_workbook`</code><code>&#13;
</code><code>    </code><code class="n">current_sheet</code><code> </code><code class="o">=</code><code> </code><code class="n">source_workbook</code><code class="p">[</code><code class="n">sheet_name</code><code class="p">]</code><code>&#13;
</code><code>&#13;
</code><code>    </code><code class="c1"># print `sheet_name`, just to see what it is</code><code>&#13;
</code><code>    </code><code class="k">print</code><code class="p">(</code><code class="n">sheet_name</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code>    </code><code class="c1"># create an output file called "xlsx_"+sheet_name</code><code>&#13;
</code><code>    </code><code class="n">output_file</code><code> </code><code class="o">=</code><code> </code><code class="nb">open</code><code class="p">(</code><code class="s2">"</code><code class="s2">xlsx_</code><code class="s2">"</code><code class="o">+</code><code class="n">sheet_name</code><code class="o">+</code><code class="s2">"</code><code class="s2">.csv</code><code class="s2">"</code><code class="p">,</code><code class="s2">"</code><code class="s2">w</code><code class="s2">"</code><code class="p">)</code><code> </code><a class="co" href="#callout_working_with_file_based_and_feed_based_data_in_python_CO4-3" id="co_working_with_file_based_and_feed_based_data_in_python_CO4-3"><img alt="3" src="assets/3.png"/></a><code>&#13;
</code><code>&#13;
</code><code>    </code><code class="c1"># use this csv library's "writer" recipe to easily write rows of data</code><code>&#13;
</code><code>    </code><code class="c1"># to `output_file`, instead of reading data *from* it</code><code>&#13;
</code><code>    </code><code class="n">output_writer</code><code> </code><code class="o">=</code><code> </code><code class="n">csv</code><code class="o">.</code><code class="n">writer</code><code class="p">(</code><code class="n">output_file</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code>    </code><code class="c1"># loop through every row in our sheet</code><code>&#13;
</code><code>    </code><code class="k">for</code><code> </code><code class="n">row</code><code> </code><code class="ow">in</code><code> </code><code class="n">current_sheet</code><code class="o">.</code><code class="n">iter_rows</code><code class="p">(</code><code class="p">)</code><code class="p">:</code><code> </code><a class="co" href="#callout_working_with_file_based_and_feed_based_data_in_python_CO4-4" id="co_working_with_file_based_and_feed_based_data_in_python_CO4-4"><img alt="4" src="assets/4.png"/></a><code>&#13;
</code><code>&#13;
</code><code>        </code><code class="c1"># we'll create an empty list where we'll put the actual</code><code>&#13;
</code><code>        </code><code class="c1"># values of the cells in each row</code><code>&#13;
</code><code>        </code><code class="n">row_cells</code><code> </code><code class="o">=</code><code> </code><code class="p">[</code><code class="p">]</code><code> </code><a class="co" href="#callout_working_with_file_based_and_feed_based_data_in_python_CO4-5" id="co_working_with_file_based_and_feed_based_data_in_python_CO4-5"><img alt="5" src="assets/5.png"/></a><code>&#13;
</code><code>&#13;
</code><code>        </code><code class="c1"># for every cell (or column) in each row....</code><code>&#13;
</code><code>        </code><code class="k">for</code><code> </code><code class="n">cell</code><code> </code><code class="ow">in</code><code> </code><code class="n">row</code><code class="p">:</code><code>&#13;
</code><code>&#13;
</code><code>            </code><code class="c1"># let's print what's in here, just to see how the code sees it</code><code>&#13;
</code><code>            </code><code class="k">print</code><code class="p">(</code><code class="n">cell</code><code class="p">,</code><code> </code><code class="n">cell</code><code class="o">.</code><code class="n">value</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code>            </code><code class="c1"># add the values to the end of our list with the `append()` method</code><code>&#13;
</code><code>            </code><code class="n">row_cells</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">cell</code><code class="o">.</code><code class="n">value</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code>        </code><code class="c1"># write our newly (re)constructed data row to the output file</code><code>&#13;
</code><code>        </code><code class="n">output_writer</code><code class="o">.</code><code class="n">writerow</code><code class="p">(</code><code class="n">row_cells</code><code class="p">)</code><code> </code><a class="co" href="#callout_working_with_file_based_and_feed_based_data_in_python_CO4-6" id="co_working_with_file_based_and_feed_based_data_in_python_CO4-6"><img alt="6" src="assets/6.png"/></a><code>&#13;
</code><code>&#13;
</code><code>    </code><code class="c1"># officially close the `.csv` file we just wrote all that data to</code><code>&#13;
</code><code>    </code><code class="n">output_file</code><code class="o">.</code><code class="n">close</code><code class="p">(</code><code class="p">)</code></pre></div>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_working_with_file_based_and_feed_based_data_in_python_CO4-1" id="callout_working_with_file_based_and_feed_based_data_in_python_CO4-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>Like the <em>csv</em> library’s <code>DictReader()</code> function, <code>openpyxl</code>’s <code>load_workbook()</code> function adds properties to our source data, in this case, one that shows us the names of all the data sheets in our workbook.</p></dd>&#13;
<dt><a class="co" href="#co_working_with_file_based_and_feed_based_data_in_python_CO4-2" id="callout_working_with_file_based_and_feed_based_data_in_python_CO4-2"><img alt="2" src="assets/2.png"/></a></dt>&#13;
<dd><p>Even though our example workbook only includes one worksheet, we might have more in the future. We’ll use the <code>enumerate()</code> function so we can access both an iterator <em>and</em> the sheet name. This will help us create one <em>.csv</em> file per worksheet.</p></dd>&#13;
<dt><a class="co" href="#co_working_with_file_based_and_feed_based_data_in_python_CO4-3" id="callout_working_with_file_based_and_feed_based_data_in_python_CO4-3"><img alt="3" src="assets/3.png"/></a></dt>&#13;
<dd><p>Each sheet in our <code>source_workbook</code> will need its own, uniquely named output <em>.csv</em> file. To generate these, we’ll “open” a new file with the name <code>"xlsx_"+sheet_name+".csv"</code> and make it <em>writable</em> by passing <code>w</code> as the “mode” argument (up until now, we’ve used the <code>r</code> mode to <em>read</em> data from <em>.csv</em>s).</p></dd>&#13;
<dt><a class="co" href="#co_working_with_file_based_and_feed_based_data_in_python_CO4-4" id="callout_working_with_file_based_and_feed_based_data_in_python_CO4-4"><img alt="4" src="assets/4.png"/></a></dt>&#13;
<dd><p>The function <code>iter_rows()</code> is specific to the <em>openpyxl</em> library. Here, it converts the rows of <code>source_workbook</code> into a list that can be <em>iterated</em>, or looped, over.</p></dd>&#13;
<dt><a class="co" href="#co_working_with_file_based_and_feed_based_data_in_python_CO4-5" id="callout_working_with_file_based_and_feed_based_data_in_python_CO4-5"><img alt="5" src="assets/5.png"/></a></dt>&#13;
<dd><p>The <em>openpyxl</em> library treats each data cell as a Python <a href="https://docs.python.org/3/library/stdtypes.html#tuple"><code>tuple</code> data type</a>. If we try to just print the rows of <code>current_sheet</code> directly, we’ll get sort of unhelpful cell locations, rather than the data values they contain. To address this, we’ll make <em>another</em> loop inside this one to go through every cell in every row one at a time and add the actual data values to <code>row_cells</code>.</p></dd>&#13;
<dt><a class="co" href="#co_working_with_file_based_and_feed_based_data_in_python_CO4-6" id="callout_working_with_file_based_and_feed_based_data_in_python_CO4-6"><img alt="6" src="assets/6.png"/></a></dt>&#13;
<dd><p>Notice that this code is left-aligned with the <code>for cell in row</code> code in the example. This means that it is <em>outside</em> that loop and so will only be run <em>after</em> all the cells in a given row have been appended to our list.</p></dd>&#13;
</dl>&#13;
&#13;
<p>This script also begins to demonstrate the way that, just as two chefs may have different ways of preparing the same dish, library creators may make different choices about how to (re)structure each source file type—with corresponding implications for our code. The creators of the <em>openpyxl</em> library, for example, chose to store each data cell’s location label (e.g., <code>A6</code>) and the value it contains in a Python <code>tuple</code>. That design decision is why we need a second <code>for</code> loop to go through each row of data—because we actually have to access the data cell by cell in order to build the Python list that will become a single row in our output <em>.csv</em> file. Likewise, if you use a spreadsheet program to open the <em>xlsx_FRED Graph.csv</em> created by the script in <a data-type="xref" href="#xlsx_parsing">Example 4-4</a>, you’ll see that the original <em>.xls</em> file shows the values in the <code>observation_date</code> column in a YYYY-MM-DD format, but our output file shows those values in a YYYY-MM-DD HH:MM:SS format. This is because the creator(s) of <em>openpyxl</em> decided that it would automatically convert any “date-like” data strings into the Python <code>datetime</code> datatype. Obviously, none of these choices are right or wrong; we simply need to account for them in writing our code so that we don’t distort or misinterpret the <a data-primary="openpyxl library" data-startref="openpyxl" data-type="indexterm" id="idm45143422523664"/>source data.</p>&#13;
&#13;
<p>Now that we’ve wrangled the <em>.xlsx</em> version of our data file, let’s see what happens when we<a data-primary="pyexcel-ods library" data-type="indexterm" id="pyexcel-ods"/> parse it as an <em>.ods</em>, as shown in <a data-type="xref" href="#ods_parsing">Example 4-5</a>.</p>&#13;
<div data-type="example" id="ods_parsing">&#13;
<h5><span class="label">Example 4-5. </span>ods_parsing.py</h5>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="c1"># an example of reading data from an .ods file with Python, using the</code><code>&#13;
</code><code class="c1"># "pyexcel_ods" library. First, you'll need to pip install the library:</code><code>&#13;
</code><code class="c1"># https://pypi.org/project/pyexcel-ods/</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># specify the "chapter" of the "pyexcel_ods" library you want to import,</code><code>&#13;
</code><code class="c1"># in this case, `get_data`</code><code>&#13;
</code><code class="kn">from</code><code> </code><code class="nn">pyexcel_ods</code><code> </code><code class="kn">import</code><code> </code><code class="n">get_data</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># import the `csv` library, to create our output file</code><code>&#13;
</code><code class="kn">import</code><code> </code><code class="nn">csv</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># pass our filename as an ingredient to the `pyexcel_ods` library's</code><code>&#13;
</code><code class="c1"># `get_data()` "recipe"</code><code>&#13;
</code><code class="c1"># store the result in a variable called `source_workbook`</code><code>&#13;
</code><code class="n">source_workbook</code><code> </code><code class="o">=</code><code> </code><code class="n">get_data</code><code class="p">(</code><code class="s2">"</code><code class="s2">fredgraph.ods</code><code class="s2">"</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># an `.ods` workbook can have multiple sheets</code><code>&#13;
</code><code class="k">for</code><code> </code><code class="n">sheet_name</code><code class="p">,</code><code> </code><code class="n">sheet_data</code><code> </code><code class="ow">in</code><code> </code><code class="n">source_workbook</code><code class="o">.</code><code class="n">items</code><code class="p">(</code><code class="p">)</code><code class="p">:</code><code> </code><a class="co" href="#callout_working_with_file_based_and_feed_based_data_in_python_CO5-1" id="co_working_with_file_based_and_feed_based_data_in_python_CO5-1"><img alt="1" src="assets/1.png"/></a><code>&#13;
</code><code>&#13;
</code><code>    </code><code class="c1"># print `sheet_name`, just to see what it is</code><code>&#13;
</code><code>    </code><code class="k">print</code><code class="p">(</code><code class="n">sheet_name</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code>    </code><code class="c1"># create "ods_"+sheet_name+".csv" as an output file for the current sheet</code><code>&#13;
</code><code>    </code><code class="n">output_file</code><code> </code><code class="o">=</code><code> </code><code class="nb">open</code><code class="p">(</code><code class="s2">"</code><code class="s2">ods_</code><code class="s2">"</code><code class="o">+</code><code class="n">sheet_name</code><code class="o">+</code><code class="s2">"</code><code class="s2">.csv</code><code class="s2">"</code><code class="p">,</code><code class="s2">"</code><code class="s2">w</code><code class="s2">"</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code>    </code><code class="c1"># use this csv library's "writer" recipe to easily write rows of data</code><code>&#13;
</code><code>    </code><code class="c1"># to `output_file`, instead of reading data *from* it</code><code>&#13;
</code><code>    </code><code class="n">output_writer</code><code> </code><code class="o">=</code><code> </code><code class="n">csv</code><code class="o">.</code><code class="n">writer</code><code class="p">(</code><code class="n">output_file</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code>    </code><code class="c1"># now, we need to loop through every row in our sheet</code><code>&#13;
</code><code>    </code><code class="k">for</code><code> </code><code class="n">row</code><code> </code><code class="ow">in</code><code> </code><code class="n">sheet_data</code><code class="p">:</code><code> </code><a class="co" href="#callout_working_with_file_based_and_feed_based_data_in_python_CO5-2" id="co_working_with_file_based_and_feed_based_data_in_python_CO5-2"><img alt="2" src="assets/2.png"/></a><code>&#13;
</code><code>&#13;
</code><code>        </code><code class="c1"># use the `writerow` recipe to write each `row`</code><code>&#13;
</code><code>        </code><code class="c1"># directly to our output file</code><code>&#13;
</code><code>        </code><code class="n">output_writer</code><code class="o">.</code><code class="n">writerow</code><code class="p">(</code><code class="n">row</code><code class="p">)</code><code> </code><a class="co" href="#callout_working_with_file_based_and_feed_based_data_in_python_CO5-3" id="co_working_with_file_based_and_feed_based_data_in_python_CO5-3"><img alt="3" src="assets/3.png"/></a><code>&#13;
</code><code>&#13;
</code><code>    </code><code class="c1"># officially close the `.csv` file we just wrote all that data to</code><code>&#13;
</code><code>    </code><code class="n">output_file</code><code class="o">.</code><code class="n">close</code><code class="p">(</code><code class="p">)</code></pre></div>&#13;
<dl class="calloutlist pagebreak-before less_space">&#13;
<dt><a class="co" href="#co_working_with_file_based_and_feed_based_data_in_python_CO5-1" id="callout_working_with_file_based_and_feed_based_data_in_python_CO5-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>The <em>pyexcel_ods</em> library converts our source data into Python’s <code>OrderedDict</code> data type. The associated <code>items()</code> method then lets us access each sheet’s name and data as a key/value pair that we can loop through. In this case, <code>sheet_name</code> is the “key” and the entire worksheet’s data is the “value.”</p></dd>&#13;
<dt><a class="co" href="#co_working_with_file_based_and_feed_based_data_in_python_CO5-2" id="callout_working_with_file_based_and_feed_based_data_in_python_CO5-2"><img alt="2" src="assets/2.png"/></a></dt>&#13;
<dd><p>Here, <code>sheet_data</code> is already a list, so we can just loop through that list with a basic <code>for</code> loop.</p></dd>&#13;
<dt><a class="co" href="#co_working_with_file_based_and_feed_based_data_in_python_CO5-3" id="callout_working_with_file_based_and_feed_based_data_in_python_CO5-3"><img alt="3" src="assets/3.png"/></a></dt>&#13;
<dd><p>This library converts each row in a worksheet to a list, which is why we can pass these directly to the <code>writerow()</code> method.</p></dd>&#13;
</dl>&#13;
&#13;
<p>In the case of the <em>pyexcel_ods</em> library, the contents of our output <em>.csv</em> file <em>much</em> more closely resembles what we see visually when we open the original <em>fredgraph.xls</em> via a spreadsheet program like Google Sheets—the <code>observation_date</code> field, for example, is in a simple YYYY-MM-DD format. Moreover, the library creator(s) decided to treat the values in each row as a list, allowing us to write each record directly to our output file without creating any additional loops or<a data-primary="pyexcel-ods library" data-startref="pyexcel-ods" data-type="indexterm" id="idm45143422338160"/> lists.</p>&#13;
&#13;
<p>Finally, let’s see what happens when we use<a data-primary="xlrd library" data-type="indexterm" id="xlrd"/> the <em>xlrd</em> library to parse the original <em>.xls</em> file directly in <a data-type="xref" href="#xls_parsing">Example 4-6</a>.</p>&#13;
<div data-type="example" id="xls_parsing">&#13;
<h5><span class="label">Example 4-6. </span>xls_parsing.py</h5>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="c1"># a simple example of reading data from a .xls file with Python</code><code>&#13;
</code><code class="c1"># using the "xrld" library. First, pip install the xlrd library:</code><code>&#13;
</code><code class="c1"># https://pypi.org/project/xlrd/2.0.1/</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># import the "xlrd" library</code><code>&#13;
</code><code class="kn">import</code><code> </code><code class="nn">xlrd</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># import the `csv` library, to create our output file</code><code>&#13;
</code><code class="kn">import</code><code> </code><code class="nn">csv</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># pass our filename as an ingredient to the `xlrd` library's</code><code>&#13;
</code><code class="c1"># `open_workbook()` "recipe"</code><code>&#13;
</code><code class="c1"># store the result in a variable called `source_workbook`</code><code>&#13;
</code><code class="n">source_workbook</code><code> </code><code class="o">=</code><code> </code><code class="n">xlrd</code><code class="o">.</code><code class="n">open_workbook</code><code class="p">(</code><code class="s2">"</code><code class="s2">fredgraph.xls</code><code class="s2">"</code><code class="p">)</code><code> </code><a class="co" href="#callout_working_with_file_based_and_feed_based_data_in_python_CO6-1" id="co_working_with_file_based_and_feed_based_data_in_python_CO6-1"><img alt="1" src="assets/1.png"/></a><code>&#13;
</code><code>&#13;
</code><code class="c1"># an `.xls` workbook can have multiple sheets</code><code>&#13;
</code><code class="k">for</code><code> </code><code class="n">sheet_name</code><code> </code><code class="ow">in</code><code> </code><code class="n">source_workbook</code><code class="o">.</code><code class="n">sheet_names</code><code class="p">(</code><code class="p">)</code><code class="p">:</code><code>&#13;
</code><code>&#13;
</code><code>    </code><code class="c1"># create a variable that points to the current worksheet by</code><code>&#13;
</code><code>    </code><code class="c1"># passing the current value of `sheet_name` to the `sheet_by_name` recipe</code><code>&#13;
</code><code>    </code><code class="n">current_sheet</code><code> </code><code class="o">=</code><code> </code><code class="n">source_workbook</code><code class="o">.</code><code class="n">sheet_by_name</code><code class="p">(</code><code class="n">sheet_name</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code>    </code><code class="c1"># print `sheet_name`, just to see what it is</code><code>&#13;
</code><code>    </code><code class="k">print</code><code class="p">(</code><code class="n">sheet_name</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code>    </code><code class="c1"># create "xls_"+sheet_name+".csv" as an output file for the current sheet</code><code>&#13;
</code><code>    </code><code class="n">output_file</code><code> </code><code class="o">=</code><code> </code><code class="nb">open</code><code class="p">(</code><code class="s2">"</code><code class="s2">xls_</code><code class="s2">"</code><code class="o">+</code><code class="n">sheet_name</code><code class="o">+</code><code class="s2">"</code><code class="s2">.csv</code><code class="s2">"</code><code class="p">,</code><code class="s2">"</code><code class="s2">w</code><code class="s2">"</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code>    </code><code class="c1"># use the `csv` library's "writer" recipe to easily write rows of data</code><code>&#13;
</code><code>    </code><code class="c1"># to `output_file`, instead of reading data *from* it</code><code>&#13;
</code><code>    </code><code class="n">output_writer</code><code> </code><code class="o">=</code><code> </code><code class="n">csv</code><code class="o">.</code><code class="n">writer</code><code class="p">(</code><code class="n">output_file</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code>    </code><code class="c1"># now, we need to loop through every row in our sheet</code><code>&#13;
</code><code>    </code><code class="k">for</code><code> </code><code class="n">row_num</code><code class="p">,</code><code> </code><code class="n">row</code><code> </code><code class="ow">in</code><code> </code><code class="nb">enumerate</code><code class="p">(</code><code class="n">current_sheet</code><code class="o">.</code><code class="n">get_rows</code><code class="p">(</code><code class="p">)</code><code class="p">)</code><code class="p">:</code><code> </code><a class="co" href="#callout_working_with_file_based_and_feed_based_data_in_python_CO6-2" id="co_working_with_file_based_and_feed_based_data_in_python_CO6-2"><img alt="2" src="assets/2.png"/></a><code>&#13;
</code><code>&#13;
</code><code>        </code><code class="c1"># each row is already a list, but we need to use the `row_value()`</code><code>&#13;
</code><code>        </code><code class="c1"># method to access them</code><code>&#13;
</code><code>        </code><code class="c1"># then we can use the `writerow` recipe to write them</code><code>&#13;
</code><code>        </code><code class="c1"># directly to our output file</code><code>&#13;
</code><code>        </code><code class="n">output_writer</code><code class="o">.</code><code class="n">writerow</code><code class="p">(</code><code class="n">current_sheet</code><code class="o">.</code><code class="n">row_values</code><code class="p">(</code><code class="n">row_num</code><code class="p">)</code><code class="p">)</code><code> </code><a class="co" href="#callout_working_with_file_based_and_feed_based_data_in_python_CO6-3" id="co_working_with_file_based_and_feed_based_data_in_python_CO6-3"><img alt="3" src="assets/3.png"/></a><code>&#13;
</code><code>&#13;
</code><code>    </code><code class="c1"># officially close the `.csv` file we just wrote all that data to</code><code>&#13;
</code><code>    </code><code class="n">output_file</code><code class="o">.</code><code class="n">close</code><code class="p">(</code><code class="p">)</code></pre></div>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_working_with_file_based_and_feed_based_data_in_python_CO6-1" id="callout_working_with_file_based_and_feed_based_data_in_python_CO6-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>Notice that this structure is similar to the one we use when working with the <em>csv</em> library.</p></dd>&#13;
<dt><a class="co" href="#co_working_with_file_based_and_feed_based_data_in_python_CO6-2" id="callout_working_with_file_based_and_feed_based_data_in_python_CO6-2"><img alt="2" src="assets/2.png"/></a></dt>&#13;
<dd><p>The function <code>get_rows()</code> is specific to the <em>xlrd</em> library; it converts the rows of our current worksheet into a list that can be looped over.<sup><a data-type="noteref" href="ch04.html#idm45143422318528" id="idm45143422318528-marker">12</a></sup></p></dd>&#13;
<dt><a class="co" href="#co_working_with_file_based_and_feed_based_data_in_python_CO6-3" id="callout_working_with_file_based_and_feed_based_data_in_python_CO6-3"><img alt="3" src="assets/3.png"/></a></dt>&#13;
<dd><p>There will be some funkiness within the “dates” written to our output file.<sup><a data-type="noteref" href="ch04.html#idm45143422098736" id="idm45143422098736-marker">13</a></sup> We’ll look at how to fix up the dates in <a data-type="xref" href="ch07.html#decrypting_excel_dates">“Decrypting Excel Dates”</a>.</p></dd>&#13;
</dl>&#13;
&#13;
<p>One thing we’ll see in this output file is some <em>serious</em> weirdness in the values recorded in the <code>observation_date</code> field, reflecting the fact that, as the <em>xlrd</em> library’s creators put it:<sup><a data-type="noteref" href="ch04.html#idm45143422094400" id="idm45143422094400-marker">14</a></sup></p>&#13;
<blockquote>&#13;
<p>Dates in Excel spreadsheets: In reality, there are no such things. What you have are floating point numbers and pious hope.</p></blockquote>&#13;
&#13;
<p>As a result, getting a useful, human-readable date <a data-primary="xlrd library" data-startref="xlrd" data-type="indexterm" id="idm45143422087696"/>out of an <em>.xls</em> file requires some significant cleanup, which we’ll address in <a data-type="xref" href="ch07.html#decrypting_excel_dates">“Decrypting Excel Dates”</a>.</p>&#13;
&#13;
<p>As these exercises have hopefully demonstrated, with some clever libraries and a few tweaks to our basic code configuration, it’s possible to wrangle data from a wide range of table-type data formats with Python quickly and easily. At the same time, I hope that these examples have also illustrated why working with text-based and/or open source formats is almost always preferable,<sup><a data-type="noteref" href="ch04.html#idm45143422084224" id="idm45143422084224-marker">15</a></sup> because they often require less “cleaning” and transformation to get them into a clear, usable <a data-primary="data wrangling" data-secondary="reading data" data-startref="data-wrangling-reading-xls" data-tertiary="from .xls files" data-tertiary-sortas="xls files" data-type="indexterm" id="idm45143422082864"/><a data-primary="Python" data-secondary="reading data" data-startref="python-read-xls" data-tertiary="from .xls files" data-tertiary-sortas="xls files" data-type="indexterm" id="idm45143422081040"/><a data-primary="reading data" data-secondary="from .xls files" data-secondary-sortas="xls files" data-startref="read-data-xls" data-type="indexterm" id="idm45143422079280"/><a data-primary="data wrangling" data-secondary="reading data" data-startref="data-wrangling-reading-ods" data-tertiary="from .ods files" data-tertiary-sortas="ods files" data-type="indexterm" id="idm45143422077792"/><a data-primary="Python" data-secondary="reading data" data-startref="python-read-ods" data-tertiary="from .ods files" data-tertiary-sortas="ods files" data-type="indexterm" id="idm45143422076016"/><a data-primary="reading data" data-secondary="from .ods files" data-secondary-sortas="ods files" data-startref="read-data-ods" data-type="indexterm" id="idm45143422236080"/><a data-primary=".xls files (Microsoft Excel spreadsheets)" data-primary-sortas="xls files" data-secondary="reading data from" data-startref="xls-read" data-type="indexterm" id="idm45143422234592"/><a data-primary="spreadsheets" data-secondary=".xls files (Microsoft Excel)" data-secondary-sortas="xls files" data-startref="spreadsheet-MSExcel-read" data-tertiary="reading data from" data-type="indexterm" id="idm45143422233072"/><a data-primary=".ods files (open-document spreadsheets)" data-primary-sortas="ods files" data-secondary="reading data from" data-startref="ods-read" data-type="indexterm" id="idm45143422231280"/><a data-primary="open-document spreadsheet (.ods) files" data-secondary="reading data from" data-startref="open-doc-read" data-type="indexterm" id="idm45143422229776"/><a data-primary="spreadsheets" data-secondary=".ods files (open-document spreadsheet)" data-secondary-sortas="ods files" data-startref="spreadsheet-ods-read" data-tertiary="reading data from" data-type="indexterm" id="idm45143422228544"/><a data-primary="delimited files" data-secondary="reading data" data-startref="delimited-read-xls" data-tertiary="from .xls files" data-tertiary-sortas="xls files" data-type="indexterm" id="idm45143422226768"/><a data-primary="delimited files" data-secondary="reading data" data-startref="delimited-read-ods" data-tertiary="from .ods files" data-tertiary-sortas="ods files" data-type="indexterm" id="idm45143422225008"/>state.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Finally, Fixed-Width" data-type="sect2"><div class="sect2" id="fixed_width">&#13;
<h2>Finally, Fixed-Width</h2>&#13;
&#13;
<p>Though <a data-primary="data sources" data-secondary="file-based" data-tertiary="fixed-width files" data-type="indexterm" id="data-source-file-fixed"/><a data-primary="file-based data sources" data-secondary="fixed-width files" data-type="indexterm" id="file-based-fixed"/><a data-primary="fixed-width files" data-type="indexterm" id="fixed-width"/><a data-primary="delimited files" data-secondary="reading data" data-tertiary="from fixed-width files" data-tertiary-sortas="fixed-width" data-type="indexterm" id="delimited-fixed"/><a data-primary="data wrangling" data-secondary="reading data" data-tertiary="from fixed-width files" data-type="indexterm" id="data-wrangling-read-fixed"/><a data-primary="Python" data-secondary="reading data" data-tertiary="from fixed-width files" data-type="indexterm" id="python-read-fixed"/><a data-primary="reading data" data-secondary="from fixed-width files" data-type="indexterm" id="reading-fixed"/>I didn’t mention it at the top of this section, one of the very oldest versions of table-type data is what’s known as “fixed-width.” As the name implies, each data column in a fixed-width table contains a specific, predefined number of characters—and <em>always</em> that number of characters. This means that the meaningful data in fixed-width files are often padded with extra characters, such as spaces or zeroes.</p>&#13;
&#13;
<p>Though very uncommon in contemporary data systems, you are still likely to encounter fixed-width formats if you’re working with government data sources whose infrastructure may be decades old.<sup><a data-type="noteref" href="ch04.html#idm45143422210096" id="idm45143422210096-marker">16</a></sup> For example, the US <a href="https://noaa.gov/our-history">National Oceanic and Atmospheric Administration</a> (NOAA), whose origins date back to the early 19th century, offers a wide range of detailed, up-to-date weather information online for free through its <a href="https://ncdc.noaa.gov/data-access/land-based-station-data/land-based-datasets/global-historical-climatology-network-ghcn">Global Historical Climatology Network</a>, much of which is published in a fixed-width format. For example, information about the stations’ unique identifier, locations, and what network(s) they are a part of is stored in the <a href="https://www1.ncdc.noaa.gov/pub/data/ghcn/daily"><em>ghcnd-stations.txt</em> file</a>. To interpret any actual weather data readings (many of which are <em>also</em> released as fixed-width files), you’ll need to cross-reference the station data with the weather data.</p>&#13;
&#13;
<p>Even more than other table-type data files, working with fixed-width data can be especially tricky if you don’t have access to the metadata that describes how the file and its fields are organized. With delimited files, it’s often possible to eyeball the file in a text editor and identify the delimiter used with a reasonable level of confidence. At worst, you can simply try parsing the file using different delimiters and see which yields the best results. With fixed-width files—especially large ones—if there’s no data for a particular field in the sample of the data you inspect, it’s easy to end up inadvertently lumping together multiple data fields.</p>&#13;
<aside class="pagebreak-before less_space" data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45143422203296">&#13;
<h5>Tab-Separated Versus Fixed-Width</h5>&#13;
<p>As <a data-primary=".tsv files (tab-separated values)" data-primary-sortas="tsv files" data-secondary="fixed-width files versus" data-type="indexterm" id="idm45143422201248"/><a data-primary="tab-separated value (.tsv) files" data-secondary="fixed-width files versus" data-type="indexterm" id="idm45143422199936"/><a data-primary=".txt files (text)" data-primary-sortas="txt files" data-secondary="tab-separated versus fixed-width" data-type="indexterm" id="idm45143422198960"/><a data-primary="text (.txt) files" data-secondary="tab-separated versus fixed-width" data-type="indexterm" id="idm45143422197728"/>mentioned in <a data-type="xref" href="#delimited_data">“File-Based, Table-Type Data—Take It to Delimit”</a>, it’s still not uncommon to come across tab-separated files that have a file extension of <em>.txt</em>—which is the same one used for fixed-width files. Since both file formats <em>also</em> rely on whitespace (spaces or tabs) to separate fields, how can you be sure which one you’re working with?</p>&#13;
&#13;
<p>This is yet another situation where opening up your data file and looking at it in a text or code editor will save you some headaches, because there are clear visual differences between these two file formats. Start by looking at the right side of the document: tab-separated files will be “ragged-right” (the data records will have different line lengths), while fixed-width files will be “justified” (all the records will end at the same point).</p>&#13;
&#13;
<p>For example, here’s how a few lines of our <em>.tsv</em> data from <a data-type="xref" href="#tsv_parsing">Example 4-2</a> look &#13;
<span class="keep-together">in Atom:</span></p>&#13;
&#13;
<pre class="small-and-no-indent" data-type="programlisting">42	Ward Hunt	1873-1882	R	NY	Grant&#13;
43	Morrison Waite	1874-1888	R	OH	Grant&#13;
44	John Marshall Harlan	1877-1911	R	KT	Hayes</pre>&#13;
&#13;
<p>And here’s how a few lines of the <code>ghcnd-stations.txt</code> file look:</p>&#13;
&#13;
<pre class="small-and-no-indent" data-type="programlisting">AEM00041217  24.4330   54.6510   26.8    ABU DHABI INTL                    41217&#13;
AEM00041218  24.2620   55.6090  264.9    AL AIN INTL                       41218&#13;
AF000040930  35.3170   69.0170 3366.0    NORTH-SALANG              GSN     40930</pre>&#13;
&#13;
<p>As you can see, not only is the fixed-width file right justified but <em>so are all the numbers</em>. This is another clue that what you’re looking at is actually a fixed-width file, rather than a tab-delimited one.</p>&#13;
</div></aside>&#13;
&#13;
<p>Fortunately, metadata about the <em>ghcnd-stations.txt</em> file that we’re using as our data source <em>is</em> included in the <em>readme.txt</em> file in the same folder on <a href="https://www1.ncdc.noaa.gov/pub/data/ghcn/daily/readme.txt">the NOAA site</a>.</p>&#13;
&#13;
<p>Looking through that <em>readme.txt</em> file, we find the heading <code>IV. FORMAT OF "ghcnd-stations.txt"</code>, which contains the following table:</p>&#13;
<pre>------------------------------&#13;
Variable   Columns   Type&#13;
------------------------------&#13;
ID            1-11   Character&#13;
LATITUDE     13-20   Real&#13;
LONGITUDE    22-30   Real&#13;
ELEVATION    32-37   Real&#13;
STATE        39-40   Character&#13;
NAME         42-71   Character&#13;
GSN FLAG     73-75   Character&#13;
HCN/CRN FLAG 77-79   Character&#13;
WMO ID       81-85   Character&#13;
------------------------------</pre>&#13;
&#13;
<p>This is followed by a detailed description of what each field contains or means, including information like units. Thanks to this robust <em>data dictionary</em>, we now know not just how the <em>ghcnd-stations.txt</em> file is organized but also how to interpret the information it contains. As we’ll see in <a data-type="xref" href="ch06.html#chapter6">Chapter 6</a>, finding (or building) a data dictionary is an essential part of assessing or improving the quality of our data. At the moment, however, we can just focus on transforming <a data-primary=".csv files (comma-separated values)" data-primary-sortas="csv files" data-secondary="converting fixed-width files to" data-type="indexterm" id="csv-fixed"/><a data-primary="comma-separated value (.csv) files" data-secondary="converting fixed-width files to" data-type="indexterm" id="comma-separate-fixed-width"/>this fixed-width file into a <em>.csv</em>, as detailed in <a data-type="xref" href="#fixed_width_parsing">Example 4-7</a>.</p>&#13;
<div data-type="example" id="fixed_width_parsing">&#13;
<h5><span class="label">Example 4-7. </span>fixed_width_parsing.py</h5>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="c1"># an example of reading data from a fixed-width file with Python.</code><code>&#13;
</code><code class="c1"># the source file for this example comes from NOAA and can be accessed here:</code><code>&#13;
</code><code class="c1"># https://www1.ncdc.noaa.gov/pub/data/ghcn/daily/ghcnd-stations.txt</code><code>&#13;
</code><code class="c1"># the metadata for the file can be found here:</code><code>&#13;
</code><code class="c1"># https://www1.ncdc.noaa.gov/pub/data/ghcn/daily/readme.txt</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># import the `csv` library, to create our output file</code><code>&#13;
</code><code class="kn">import</code><code> </code><code class="nn">csv</code><code>&#13;
</code><code>&#13;
</code><code class="n">filename</code><code> </code><code class="o">=</code><code> </code><code class="s2">"</code><code class="s2">ghcnd-stations</code><code class="s2">"</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># reading from a basic text file doesn't require any special libraries</code><code>&#13;
</code><code class="c1"># so we'll just open the file in read format ("r") as usual</code><code>&#13;
</code><code class="n">source_file</code><code> </code><code class="o">=</code><code> </code><code class="nb">open</code><code class="p">(</code><code class="n">filename</code><code class="o">+</code><code class="s2">"</code><code class="s2">.txt</code><code class="s2">"</code><code class="p">,</code><code> </code><code class="s2">"</code><code class="s2">r</code><code class="s2">"</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># the built-in "readlines()" method does just what you'd think:</code><code>&#13;
</code><code class="c1"># it reads in a text file and converts it to a list of lines</code><code>&#13;
</code><code class="n">stations_list</code><code> </code><code class="o">=</code><code> </code><code class="n">source_file</code><code class="o">.</code><code class="n">readlines</code><code class="p">(</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># create an output file for our transformed data</code><code>&#13;
</code><code class="n">output_file</code><code> </code><code class="o">=</code><code> </code><code class="nb">open</code><code class="p">(</code><code class="n">filename</code><code class="o">+</code><code class="s2">"</code><code class="s2">.csv</code><code class="s2">"</code><code class="p">,</code><code class="s2">"</code><code class="s2">w</code><code class="s2">"</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># use the `csv` library's "writer" recipe to easily write rows of data</code><code>&#13;
</code><code class="c1"># to `output_file`, instead of reading data *from* it</code><code>&#13;
</code><code class="n">output_writer</code><code> </code><code class="o">=</code><code> </code><code class="n">csv</code><code class="o">.</code><code class="n">writer</code><code class="p">(</code><code class="n">output_file</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># create the header list</code><code>&#13;
</code><code class="n">headers</code><code> </code><code class="o">=</code><code> </code><code class="p">[</code><code class="s2">"</code><code class="s2">ID</code><code class="s2">"</code><code class="p">,</code><code class="s2">"</code><code class="s2">LATITUDE</code><code class="s2">"</code><code class="p">,</code><code class="s2">"</code><code class="s2">LONGITUDE</code><code class="s2">"</code><code class="p">,</code><code class="s2">"</code><code class="s2">ELEVATION</code><code class="s2">"</code><code class="p">,</code><code class="s2">"</code><code class="s2">STATE</code><code class="s2">"</code><code class="p">,</code><code class="s2">"</code><code class="s2">NAME</code><code class="s2">"</code><code class="p">,</code><code class="s2">"</code><code class="s2">GSN_FLAG</code><code class="s2">"</code><code class="p">,</code><code>&#13;
</code><code>           </code><code class="s2">"</code><code class="s2">HCNCRN_FLAG</code><code class="s2">"</code><code class="p">,</code><code class="s2">"</code><code class="s2">WMO_ID</code><code class="s2">"</code><code class="p">]</code><code> </code><a class="co" href="#callout_working_with_file_based_and_feed_based_data_in_python_CO7-1" id="co_working_with_file_based_and_feed_based_data_in_python_CO7-1"><img alt="1" src="assets/1.png"/></a><code>&#13;
</code><code>&#13;
</code><code class="c1"># write our headers to the output file</code><code>&#13;
</code><code class="n">output_writer</code><code class="o">.</code><code class="n">writerow</code><code class="p">(</code><code class="n">headers</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># loop through each line of our file (multiple "sheets" are not possible)</code><code>&#13;
</code><code class="k">for</code><code> </code><code class="n">line</code><code> </code><code class="ow">in</code><code> </code><code class="n">stations_list</code><code class="p">:</code><code>&#13;
</code><code>&#13;
</code><code>    </code><code class="c1"># create an empty list, to which we'll append each set of characters that</code><code>&#13;
</code><code>    </code><code class="c1"># makes up a given "column" of data</code><code>&#13;
</code><code>    </code><code class="n">new_row</code><code> </code><code class="o">=</code><code> </code><code class="p">[</code><code class="p">]</code><code>&#13;
</code><code>&#13;
</code><code>    </code><code class="c1"># ID: positions 1-11</code><code>&#13;
</code><code>    </code><code class="n">new_row</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">line</code><code class="p">[</code><code class="mi">0</code><code class="p">:</code><code class="mi">11</code><code class="p">]</code><code class="p">)</code><code> </code><a class="co" href="#callout_working_with_file_based_and_feed_based_data_in_python_CO7-2" id="co_working_with_file_based_and_feed_based_data_in_python_CO7-2"><img alt="2" src="assets/2.png"/></a><code>&#13;
</code><code>&#13;
</code><code>    </code><code class="c1"># LATITUDE: positions 13-20</code><code>&#13;
</code><code>    </code><code class="n">new_row</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">line</code><code class="p">[</code><code class="mi">12</code><code class="p">:</code><code class="mi">20</code><code class="p">]</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code>    </code><code class="c1"># LONGITUDE: positions 22-30</code><code>&#13;
</code><code>    </code><code class="n">new_row</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">line</code><code class="p">[</code><code class="mi">21</code><code class="p">:</code><code class="mi">30</code><code class="p">]</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code>    </code><code class="c1"># ELEVATION: positions 32-37</code><code>&#13;
</code><code>    </code><code class="n">new_row</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">line</code><code class="p">[</code><code class="mi">31</code><code class="p">:</code><code class="mi">37</code><code class="p">]</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code>    </code><code class="c1"># STATE: positions 39-40</code><code>&#13;
</code><code>    </code><code class="n">new_row</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">line</code><code class="p">[</code><code class="mi">38</code><code class="p">:</code><code class="mi">40</code><code class="p">]</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code>    </code><code class="c1"># NAME: positions 42-71</code><code>&#13;
</code><code>    </code><code class="n">new_row</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">line</code><code class="p">[</code><code class="mi">41</code><code class="p">:</code><code class="mi">71</code><code class="p">]</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code>    </code><code class="c1"># GSN_FLAG: positions 73-75</code><code>&#13;
</code><code>    </code><code class="n">new_row</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">line</code><code class="p">[</code><code class="mi">72</code><code class="p">:</code><code class="mi">75</code><code class="p">]</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code>    </code><code class="c1"># HCNCRN_FLAG: positions 77-79</code><code>&#13;
</code><code>    </code><code class="n">new_row</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">line</code><code class="p">[</code><code class="mi">76</code><code class="p">:</code><code class="mi">79</code><code class="p">]</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code>    </code><code class="c1"># WMO_ID: positions 81-85</code><code>&#13;
</code><code>    </code><code class="n">new_row</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">line</code><code class="p">[</code><code class="mi">80</code><code class="p">:</code><code class="mi">85</code><code class="p">]</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code>    </code><code class="c1"># now all that's left is to use the</code><code>&#13;
</code><code>    </code><code class="c1"># `writerow` function to write new_row to our output file</code><code>&#13;
</code><code>    </code><code class="n">output_writer</code><code class="o">.</code><code class="n">writerow</code><code class="p">(</code><code class="n">new_row</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># officially close the `.csv` file we just wrote all that data to</code><code>&#13;
</code><code class="n">output_file</code><code class="o">.</code><code class="n">close</code><code class="p">(</code><code class="p">)</code></pre></div>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_working_with_file_based_and_feed_based_data_in_python_CO7-1" id="callout_working_with_file_based_and_feed_based_data_in_python_CO7-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>Since we don’t have anything <em>within</em> the file that we can draw on for column headers, we have to “hard code” them based on the information in the <a href="https://www1.ncdc.noaa.gov/pub/data/ghcn/daily/readme.txt"><em>readme.txt</em> file</a>. Note that I’ve eliminated special characters and used underscores in place of spaces to  minimize hassles when cleaning and analyzing this data later on.</p></dd>&#13;
<dt><a class="co" href="#co_working_with_file_based_and_feed_based_data_in_python_CO7-2" id="callout_working_with_file_based_and_feed_based_data_in_python_CO7-2"><img alt="2" src="assets/2.png"/></a></dt>&#13;
<dd><p>Python actually views lines of text as just lists of characters, so we can just tell it to give us the characters between two numbered index positions. Like the <code>range()</code> function, the character at the first position is included, but the second number is not. Also recall that Python starts counting lists of items at zero (often called <em>zero-indexing</em>). This means that for each entry, the first number will be one <em>less</em> than whatever the metadata says, but the righthand number will be the same.</p></dd>&#13;
</dl>&#13;
&#13;
<p>If you run the script in <a data-type="xref" href="#fixed_width_parsing">Example 4-7</a> and open your output <em>.csv</em> file in a spreadsheet program, you’ll notice that the values in some of the columns are not formatted consistently. For example, in the <code>ELEVATION</code> column, the numbers with decimals are left justified, but those without decimals are right justified. What’s going on?</p>&#13;
&#13;
<p>Once again, opening the file in a text editor is enlightening. Although the file we’ve created is <em>technically</em> comma separated, the values we put into each of our newly “delimited” columns still contain the extra spaces that existed in the original file. As a result, our new file still looks pretty “fixed-width.”</p>&#13;
&#13;
<p>In other words—just as we saw in the case of Excel “dates”—converting our file to a <em>.csv</em> does not “automagically” generate sensible data types in our output file. Determining what data type each field should have—and cleaning them up so that they behave appropriately—is part of the data cleaning process that we’ll address<a data-primary="data sources" data-secondary="file-based" data-startref="data-source-file-fixed" data-tertiary="fixed-width files" data-type="indexterm" id="idm45143425954928"/><a data-primary="file-based data sources" data-secondary="fixed-width files" data-startref="file-based-fixed" data-type="indexterm" id="idm45143425965472"/><a data-primary="fixed-width files" data-startref="fixed-width" data-type="indexterm" id="idm45143425964256"/><a data-primary="delimited files" data-secondary="reading data" data-startref="delimited-fixed" data-tertiary="from fixed-width files" data-tertiary-sortas="fixed-width" data-type="indexterm" id="idm45143425925840"/><a data-primary=".csv files (comma-separated values)" data-primary-sortas="csv files" data-secondary="converting fixed-width files to" data-startref="csv-fixed" data-type="indexterm" id="idm45143425924080"/><a data-primary="comma-separated value (.csv) files" data-secondary="converting fixed-width files to" data-startref="comma-separate-fixed-width" data-type="indexterm" id="idm45143426000896"/><a data-primary="data wrangling" data-secondary="reading data" data-startref="data-wrangling-read-fixed" data-tertiary="from fixed-width files" data-type="indexterm" id="idm45143425999632"/><a data-primary="Python" data-secondary="reading data" data-startref="python-read-fixed" data-tertiary="from fixed-width files" data-type="indexterm" id="idm45143421803904"/><a data-primary="reading data" data-secondary="from fixed-width files" data-startref="reading-fixed" data-type="indexterm" id="idm45143421802416"/> in <a data-type="xref" href="ch07.html#chapter7">Chapter 7</a>.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Feed-Based Data—Web-Driven Live Updates" data-type="sect2"><div class="sect2" id="feed_based_data">&#13;
<h2>Feed-Based Data—Web-Driven Live Updates</h2>&#13;
&#13;
<p>The <a data-primary="data sources" data-secondary="feed-based" data-tertiary="types of" data-type="indexterm" id="data-source-feed-types"/><a data-primary="feed-based data sources" data-secondary="types of" data-type="indexterm" id="feed-based-types"/><a data-primary="live data sources" data-see="feed-based data sources" data-type="indexterm" id="idm45143425992256"/>structure of table-type data formats is well suited to a world where most “data” has already been filtered, revised, and processed into a relatively well-organized collection of numbers, dates, and short strings. With the rise of the internet, however, came the need to transmit large quantities of the type of “free” text found in, for example, news stories and social media feeds. Because this type of data content typically includes characters like commas, periods, and quotation marks that affect its semantic meaning, fitting it into a traditional delimited format will be problematic at best. What’s more, the horizontal bias of delimited formats (which involves lots of left-right scrolling) runs counter to the vertical-scrolling conventions of the web. Feed-based data formats have been designed to address both of these limitations.</p>&#13;
&#13;
<p>At a high level, there are two main types of feed-based data formats: XML and JSON. Both are text-based formats that allow the data provider to define their own unique data structure, making them extremely flexible and, consequently, useful for the wide variety of content found on internet-connected websites and platforms. Whether they’re located online or you save a copy locally, you’ll recognize these formats, in part, by their coordinating <em>.xml</em> and <em>.json</em> file extensions:</p>&#13;
<dl>&#13;
<dt><em>.xml</em></dt>&#13;
<dd>&#13;
<p>Extensible Markup Language encompasses<a data-primary=".xml files (Extensible Markup Language)" data-primary-sortas="xml files" data-type="indexterm" id="idm45143421849808"/><a data-primary="Extensible Markup Language (XML)" data-see=".xml files" data-type="indexterm" id="idm45143425759056"/> a broad range of file formats, including <em>.rss</em>, <em>.atom</em>, and even <em>.html</em>. As the most generic type of markup language, XML is extremely flexible and was perhaps the original data format for web-based data feeds.</p>&#13;
</dd>&#13;
<dt><em>.json</em></dt>&#13;
<dd>&#13;
<p>JavaScript Object Notation files<a data-primary=".json files (JavaScript Object Notation)" data-primary-sortas="json files" data-type="indexterm" id="idm45143421815792"/><a data-primary="JavaScript Object Notation" data-see=".json files" data-type="indexterm" id="idm45143421814848"/> are somewhat more recent than XML files but serve a similar purpose. In general, JSON files are less descriptive (and therefore shorter and more concise) than XML files. This means that they can encode an almost identical amount of data as an XML file while taking up less space, which is especially important for speed on the mobile web. Equally important is the fact that JSON files are essentially large <code>object</code> data types within the JavaScript programming language—which is the language that underpins many, if not most, websites and mobile apps. This means that parsing JSON-formatted data is very easy for any site or program that uses JavaScript, especially when compared with XML. Fortunately, JavaScript <code>object</code> data types are very similar to Python <code>dict</code> data types, which also makes working with JSON in Python very straightforward.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>Before we dive into how to work with each of these file types in Python, let’s review when we might <em>want</em> feed-type data and where to find it when<a data-primary="data sources" data-secondary="feed-based" data-startref="data-source-feed-types" data-tertiary="types of" data-type="indexterm" id="idm45143425942656"/><a data-primary="feed-based data sources" data-secondary="types of" data-startref="feed-based-types" data-type="indexterm" id="idm45143425941136"/> we do.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="When to work with feed-type data" data-type="sect3"><div class="sect3" id="idm45143425939664">&#13;
<h3>When to work with feed-type data</h3>&#13;
&#13;
<p>In a <a data-primary="data sources" data-secondary="feed-based" data-tertiary="when to use" data-type="indexterm" id="idm45143425850352"/><a data-primary="feed-based data sources" data-secondary="when to use" data-type="indexterm" id="idm45143425849072"/>sense, feed-type data is to the 21st century what table-type data was to the 20th: the sheer volume of feed-type data generated, stored, and exchanged on the web every day is probably millions of times greater than that of all of the table-type data in the world put together—in large part because feed-type data is what powers social media sites, news apps, and everything in between.</p>&#13;
&#13;
<p>From a data wrangling perspective, you’ll generally want feed-type data when the phenomenon you’re exploring is time sensitive and updated on a frequent and/or unpredictable basis. Typically, this type of data is generated in response to a human or natural process, such as (once again) posting to social media, publishing a news story, or recording an earthquake.</p>&#13;
&#13;
<p>Both file-based, table-type data and web-based, feed-type data can contain historical information, but as we discussed at the start of this chapter, the former usually reflects the data as it stood at a fixed point in time. The latter, by contrast, is typically organized in a “reverse-chronological” (most recent first) order, with the first entry being whatever data record was most recently created at the time you accessed the data, rather than a predetermined publication date.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Where to find feed-type data" data-type="sect3"><div class="sect3" id="finding_feed_data">&#13;
<h3>Where to find feed-type data</h3>&#13;
&#13;
<p>Feed-type data <a data-primary="data sources" data-secondary="feed-based" data-tertiary="finding" data-type="indexterm" id="idm45143425794800"/><a data-primary="feed-based data sources" data-secondary="finding" data-type="indexterm" id="idm45143425793520"/><a data-primary="finding" data-secondary="feed-based data sources" data-type="indexterm" id="idm45143425792576"/>is found almost exclusively on the web, often at special URLs known as application programming interface (API) <em>endpoints</em>. We’ll <a data-primary="API (application programming interface)" data-secondary="endpoints" data-type="indexterm" id="idm45143425790992"/><a data-primary="endpoints (APIs)" data-type="indexterm" id="idm45143425789968"/>get into the details of working with APIs in <a data-type="xref" href="ch05.html#chapter5">Chapter 5</a>, but for now all you need to know is that API endpoints are really just data-only web pages: you can view many of them using a regular web browser, but all you’ll see is the data itself. Some API endpoints will even return different data depending on the information <em>you</em> send to them, and this is part of what makes working with feed-type data so flexible: by changing just a few words or values in your code, you can access a totally different dataset!</p>&#13;
&#13;
<p>Finding APIs that offer feed-type data doesn’t require too much in the way of special search strategies because usually the sites and services that have APIs <em>want</em> you to find them. Why? Simply put, when someone writes code that makes use of an API, it (usually) returns some benefit to the company that provides it—even if that benefit is just more public exposure. In the early days of Twitter, for example, many web developers wrote programs using the Twitter API—both making the platform more useful <em>and</em> saving the company the expense and  effort of figuring out what users wanted and then building it. By making so much of their platform data available for free (at first), the API gave rise to several companies that Twitter would eventually purchase—though many more would also be put out of business when either the API or its terms of service changed.<sup><a data-type="noteref" href="ch04.html#idm45143421937168" id="idm45143421937168-marker">17</a></sup> This highlights one of the particular issues that can arise when working with any type of data, but especially the feed-type data made available by for-profit companies: both the data and your right to access it can change at any time, without warning. So while feed-type data sources are indeed valuable, they are also ephemeral in more ways than one.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Wrangling Feed-Type Data with Python" data-type="sect2"><div class="sect2" id="wrangling_feed_data">&#13;
<h2>Wrangling Feed-Type Data with Python</h2>&#13;
&#13;
<p>As <a data-primary="data sources" data-secondary="feed-based" data-see="reading data" data-tertiary="reading data from" data-type="indexterm" id="idm45143422037984"/><a data-primary="feed-based data sources" data-secondary="reading data from" data-see="reading data" data-type="indexterm" id="idm45143422036432"/>with table-type data, wrangling feed-type data in Python is made possible by a combination of helpful libraries and the fact that formats like JSON already resemble existing data types in the Python programming language. Moreover, we’ll see in the following sections that XML and JSON are often functionally interchangeable for our purposes (though many APIs will only offer data in one format or the other).</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="XML: One markup to rule them all" data-type="sect3"><div class="sect3" id="idm45143422034528">&#13;
<h3>XML: One markup to rule them all</h3>&#13;
&#13;
<p>Markup languages<a data-primary="data wrangling" data-secondary="reading data" data-tertiary="from .xml files" data-tertiary-sortas="xml files" data-type="indexterm" id="data-wrangling-read-xml"/><a data-primary="Python" data-secondary="reading data" data-tertiary="from .xml files" data-tertiary-sortas="xml files" data-type="indexterm" id="python-read-xml"/><a data-primary="reading data" data-secondary="from .xml files" data-secondary-sortas="xml files" data-type="indexterm" id="read-data-xml"/><a data-primary=".xml files (Extensible Markup Language)" data-primary-sortas="xml files" data-secondary="reading data from" data-type="indexterm" id="xml-read"/> are among the oldest forms of standardized document formats in computing, designed with the goal of creating text-based documents that can be easily read by both humans and machines.&#13;
XML became an increasingly important part of internet infrastructure in the 1990s as the variety of devices accessing and displaying web-based information made the separation of content (e.g., text and images) from formatting (e.g., page layout) more of a necessity. Unlike an HTML document—in which content and formatting are fully commingled—an XML document says pretty much nothing about how its information should be displayed. Instead, its tags and attributes act as <em>metadata</em> about<a data-primary="metadata" data-type="indexterm" id="idm45143422023104"/> what kind of information it contains, along with the data itself.</p>&#13;
&#13;
<p>To get a feel for what XML looks like, take<a data-primary=".xml files (Extensible Markup Language)" data-primary-sortas="xml files" data-secondary="components of" data-type="indexterm" id="xml-components"/> a look at <a data-type="xref" href="#sample_xml_document">Example 4-8</a>.</p>&#13;
<div data-type="example" id="sample_xml_document">&#13;
<h5><span class="label">Example 4-8. </span>A sample XML document</h5>&#13;
&#13;
<pre data-code-language="xml" data-type="programlisting"> <code class="cp">&lt;?xml version="1.0" encoding="UTF-8"?&gt;</code>&#13;
 <code class="nt">&lt;mainDoc&gt;</code>&#13;
    <code class="c">&lt;!--This is a comment--&gt;</code>&#13;
    <code class="nt">&lt;elements&gt;</code>&#13;
        <code class="nt">&lt;element1&gt;</code>This is some text in the document.<code class="nt">&lt;/element1&gt;</code>&#13;
        <code class="nt">&lt;element2&gt;</code>This is some other data in the document.<code class="nt">&lt;/element2&gt;</code>&#13;
        <code class="nt">&lt;element3</code> <code class="na">someAttribute=</code><code class="s">"aValue"</code> <code class="nt">/&gt;</code>&#13;
    <code class="nt">&lt;/elements&gt;</code>&#13;
    <code class="nt">&lt;someElement</code> <code class="na">anAttribute=</code><code class="s">"anotherValue"</code><code class="nt">&gt;</code>More content<code class="nt">&lt;/someElement&gt;</code>&#13;
<code class="nt">&lt;/mainDoc&gt;</code></pre></div>&#13;
&#13;
<p>There are a couple of things going here. The very first line is <a data-primary="document type declarations (XML)" data-type="indexterm" id="idm45143424467584"/>called the <em>document type</em> (or <code>doc-type</code>) declaration; it’s letting us know that the rest of the document should be interpreted as XML (as opposed to any of the other web or markup languages, some of which we’ll review later in this chapter).</p>&#13;
&#13;
<p>Starting with the line:</p>&#13;
<pre>&lt;mainDoc&gt;</pre>&#13;
&#13;
<p>we are into the substance of the document itself. Part of what makes XML so flexible is that it only contains two real grammatical structures, both of which are included in <a data-type="xref" href="#sample_xml_document">Example 4-8</a>:</p>&#13;
<dl>&#13;
<dt>tags</dt>&#13;
<dd>&#13;
<p>Tags can be <a data-primary="tags (XML)" data-type="indexterm" id="idm45143424222464"/>either paired (like <code>element1</code>, <code>element2</code>, <code>someElement</code>, or even <code>mainDoc</code>) or self-closed (like <code>element3</code>). The name of a tag is always enclosed by <em>carets</em> (<code>&lt;&gt;</code>). In the case of a closing tag, the opening caret is immediately followed by a forward slash (<code>/</code>). A matched pair of tags, or a self-closed tag, are also described as <a data-primary="elements (XML)" data-type="indexterm" id="idm45143412165312"/>XML <em>elements</em>.</p>&#13;
</dd>&#13;
<dt>attributes</dt>&#13;
<dd>&#13;
<p>Attributes <a data-primary="attributes (XML)" data-type="indexterm" id="idm45143411085072"/><a data-primary="key/value pairs" data-secondary="XML attributes as" data-type="indexterm" id="idm45143424389600"/>can exist only inside of tags (like <code>anAttribute</code>). Attributes are a type of <em>key/value pair</em> in which the attribute name (or <em>key</em>) is immediately followed &#13;
<span class="keep-together">by an</span> equals sign (<code>=</code>), followed by the <em>value</em> surrounded by double quotation &#13;
<span class="keep-together">marks (<code>""</code>).</span></p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>An XML element is whatever is contained between an opening tag and its matching closing tag (e.g., <code>&lt;elements&gt;</code> and <code>&lt;/elements&gt;</code>). As such, a given XML element may contain many tags, each of which may also contain other tags. Any tags may also have any number of attributes (including none). A self-closed tag is also considered an element.</p>&#13;
&#13;
<p>The only<a data-primary="nesting" data-secondary="XML tags" data-type="indexterm" id="idm45143413112416"/> other meaningful rule for structuring XML documents is that when tags appear inside other tags, <em>the most recently opened tag must be closed first</em>. In other words, while this is a legitimate XML structure:</p>&#13;
&#13;
<pre data-code-language="xml" data-type="programlisting"> <code class="nt">&lt;outerElement&gt;</code>&#13;
    <code class="c">&lt;!-- Notice that that the `innerElement1` is closed</code>&#13;
<code class="c">    before the `innerElement2` tag is opened --&gt;</code>&#13;
    <code class="nt">&lt;innerElement1&gt;</code>Some content<code class="nt">&lt;/innerElement1&gt;</code>&#13;
    <code class="nt">&lt;innerElement2&gt;</code>More content<code class="nt">&lt;/innerElement2&gt;</code>&#13;
 <code class="nt">&lt;/outerElement&gt;</code></pre>&#13;
&#13;
<p>this is not:</p>&#13;
&#13;
<pre data-code-language="xml" data-type="programlisting"> <code class="nt">&lt;outerElement&gt;</code>&#13;
    <code class="c">&lt;!-- NOPE! The `innerElement2` tag was opened</code>&#13;
<code class="c">    before the `innerElement1` tag was closed --&gt;</code>&#13;
    <code class="nt">&lt;innerElement1&gt;</code>Some content<code class="nt">&lt;innerElement2&gt;</code>More content<code class="nt">&lt;/innerElement1&gt;</code>&#13;
    <code class="nt">&lt;/innerElement2&gt;</code>&#13;
 <code class="nt">&lt;/outerElement&gt;</code></pre>&#13;
&#13;
<p>This principle of <em>last opened, first closed</em> is also described as <em>nesting</em>, similar to the “nested” <code>for...in</code> loops from <a data-type="xref" href="ch02.html#loop_nesting_diagram">Figure 2-3</a>.<sup><a data-type="noteref" href="ch04.html#idm45143413262208" id="idm45143413262208-marker">18</a></sup> Nesting is especially important in XML documents because it governs one of the primary mechanisms that we use to read or <em>parse</em> XML (and other markup language) documents with code. In an XML document, the first element after the <code>doc-type</code> declaration is known<a data-primary="root elements (XML)" data-type="indexterm" id="idm45143411615264"/><a data-primary="child elements (XML)" data-type="indexterm" id="idm45143425175856"/><a data-primary="parsing data" data-see="reading data" data-type="indexterm" id="idm45143424141072"/><a data-primary="parent elements (XML)" data-type="indexterm" id="idm45143422016256"/> as the <em>root</em> element. If the XML document has been formatted, the root element will always be left justified, and any element that is nested directly <em>within</em> that element will be indented one level to the right and is referred to as a <em>child</em> element. In <a data-type="xref" href="#sample_xml_document">Example 4-8</a>, then, <code>&lt;mainDoc&gt;</code> would be considered the <em>root</em> element, and <code>&lt;elements&gt;</code> would be its child. Likewise, <code>&lt;mainDoc&gt;</code> is the <em>parent</em> element of <code>&lt;elements&gt;</code> (<a data-type="xref" href="#sample_xml_annotated">Example 4-9</a>).</p>&#13;
<div data-type="example" id="sample_xml_annotated">&#13;
<h5><span class="label">Example 4-9. </span>An annotated XML document</h5>&#13;
&#13;
<pre data-code-language="xml" data-type="programlisting"> <code class="cp">&lt;?xml version="1.0" encoding="UTF-8"?&gt;</code>&#13;
 <code class="nt">&lt;mainDoc&gt;</code>&#13;
    <code class="c">&lt;!--`mainDoc` is the *root* element, and `elements` is its *child*--&gt;</code>&#13;
    <code class="nt">&lt;elements&gt;</code>&#13;
        <code class="c">&lt;!-- `elements` is the *parent* of `element1`, `element2`, and</code>&#13;
<code class="c">        `element3`, which are *siblings* of one another --&gt;</code>&#13;
        <code class="nt">&lt;element1&gt;</code>This is text data in the document.<code class="nt">&lt;/element1&gt;</code>&#13;
        <code class="nt">&lt;element2&gt;</code>This is some other data in the document.<code class="nt">&lt;/element2&gt;</code>&#13;
        <code class="nt">&lt;element3</code> <code class="na">someAttribute=</code><code class="s">"aValue"</code> <code class="nt">/&gt;</code>&#13;
    <code class="nt">&lt;/elements&gt;</code>&#13;
    <code class="c">&lt;!-- `someElement` is also a *child* of `mainDoc`,</code>&#13;
<code class="c">    and a *sibling* of `elements` --&gt;</code>&#13;
    <code class="nt">&lt;someElement</code> <code class="na">anAttribute=</code><code class="s">"anotherValue"</code><code class="nt">&gt;</code>More content<code class="nt">&lt;/someElement&gt;</code>&#13;
<code class="nt">&lt;/mainDoc&gt;</code></pre></div>&#13;
&#13;
<p>Given this trend for genealogical jargon, you might be wondering: if <code>&lt;elements&gt;</code> is the parent of <code>&lt;element3&gt;</code>, and <code>&lt;mainDoc&gt;</code> is the parent of <code>&lt;elements&gt;</code>, does that make <code>&lt;mainDoc&gt;</code> the <em>grandparent</em> of <code>&lt;element3&gt;</code>? The answer is: yes, but no. While <code>&lt;mainDoc&gt;</code> <em>is</em> the “parent” of the “parent” of <code>&lt;element3&gt;</code>, the term “grandparent” is never used in describing an XML structure—that could get complicated fast! Instead, we simply describe the relationship as exactly that: <code>&lt;mainDoc&gt;</code> is the <em>parent</em> of the <em>parent</em> of <code>&lt;element3&gt;</code>.</p>&#13;
&#13;
<p>Fortunately, there is no such complexity associated with XML attributes: they are simply key/value pairs, and they can <em>only</em> exist within XML tags, like so:</p>&#13;
&#13;
<pre data-code-language="xml" data-type="programlisting"> <code class="nt">&lt;element3</code> <code class="na">someAttribute=</code><code class="s">"aValue"</code> <code class="nt">/&gt;</code></pre>&#13;
&#13;
<p>Note that there is no space on either side of the equals sign, just as there is no space between the carets and slashes of an element tag.</p>&#13;
&#13;
<p>Like writing in English (or in Python), the question of when to use tags versus attributes for a particular piece of information is largely a matter of preference and style. Both Examples <a data-type="xref" data-xrefstyle="select:labelnumber" href="#sample_book_XML_1">4-10</a> and <a data-type="xref" data-xrefstyle="select:labelnumber" href="#sample_book_XML_2">4-11</a>, for example, contain the same information about this book, but each is structured slightly differently.</p>&#13;
<div data-type="example" id="sample_book_XML_1">&#13;
<h5><span class="label">Example 4-10. </span>Sample XML book data—more attributes</h5>&#13;
&#13;
<pre data-code-language="xml" data-type="programlisting"><code class="nt">&lt;aBook&gt;</code>&#13;
    <code class="nt">&lt;bookURL</code> <code class="na">url=</code><code class="s">"https://www.oreilly.com/library/view/practical-python-data/</code>&#13;
<code class="s">    9781492091493"</code><code class="nt">/&gt;</code>&#13;
    <code class="nt">&lt;bookAbstract&gt;</code>&#13;
    There are awesome discoveries to be made and valuable stories to be&#13;
    told in datasets--and this book will help you uncover them.&#13;
    <code class="nt">&lt;/bookAbstract&gt;</code>&#13;
    <code class="nt">&lt;pubDate</code> <code class="na">date=</code><code class="s">"2022-02-01"</code> <code class="nt">/&gt;</code>&#13;
<code class="nt">&lt;/aBook&gt;</code></pre></div>&#13;
<div data-type="example" id="sample_book_XML_2">&#13;
<h5><span class="label">Example 4-11. </span>Sample XML book data—more elements</h5>&#13;
&#13;
<pre data-code-language="xml" data-type="programlisting"><code class="nt">&lt;aBook&gt;</code>&#13;
    <code class="nt">&lt;bookURL&gt;</code>&#13;
        https://www.oreilly.com/library/view/practical-python-data/9781492091493&#13;
    <code class="nt">&lt;/bookURL&gt;</code>&#13;
    <code class="nt">&lt;bookAbstract&gt;</code>&#13;
        There are awesome discoveries to be made and valuable stories to be&#13;
        told in datasets--and this book will help you uncover them.&#13;
    <code class="nt">&lt;/bookAbstract&gt;</code>&#13;
    <code class="nt">&lt;pubDate&gt;</code>2022-02-01<code class="nt">&lt;/pubDate&gt;</code>&#13;
<code class="nt">&lt;/aBook&gt;</code></pre></div>&#13;
&#13;
<p>This degree of flexibility means XML is highly adaptable to a wide variety of data sources and formatting preferences. At the same time, it can easily create a situation where <em>every</em> new data source requires writing custom code. Obviously, this would be a pretty inefficient system, especially if many people and organizations were publishing pretty similar types<a data-primary=".xml files (Extensible Markup Language)" data-primary-sortas="xml files" data-secondary="components of" data-startref="xml-components" data-type="indexterm" id="idm45143410323312"/> of data.</p>&#13;
&#13;
<p>It’s not surprising, then, that there are a <a data-primary=".xml files (Extensible Markup Language)" data-primary-sortas="xml files" data-secondary="specifications" data-type="indexterm" id="idm45143410292240"/><a data-primary="specifications (XML)" data-type="indexterm" id="idm45143410290976"/>large number of XML <em>specifications</em> that define additional rules for formatting XML documents that are intended to hold particular types of data. I’m highlighting a few notable examples here, as these are formats you may come across in the course of your data wrangling work. Despite their various format names and file extensions, however, we can parse them all using the same method that we’ll look at shortly in <a data-type="xref" href="#xml_parsing">Example 4-12</a>:</p>&#13;
<dl>&#13;
<dt>RSS</dt>&#13;
<dd>&#13;
<p>Really Simple Syndication <a data-primary="RSS (Really Simple Syndication)" data-type="indexterm" id="idm45143410286800"/><a data-primary="Really Simple Syndication (RSS)" data-type="indexterm" id="idm45143410286080"/><a data-primary=".atom XML format" data-primary-sortas="atom xml format" data-type="indexterm" id="idm45143410285392"/>is an XML specification first introduced in the late 1990s for news information. The <em>.atom</em> XML format is also widely used for these purposes.</p>&#13;
</dd>&#13;
<dt>KML</dt>&#13;
<dd>&#13;
<p>Keyhole Markup Language<a data-primary="KML (Keyhole Markup Language)" data-type="indexterm" id="idm45143410282560"/> is an internationally accepted standard for encoding both two-dimensional and three-dimensional geographic data and is compatible with tools like Google Earth.</p>&#13;
</dd>&#13;
<dt>SVG</dt>&#13;
<dd>&#13;
<p>Scalable Vector Graphics <a data-primary="SVG (Scalable Vector Graphics)" data-type="indexterm" id="idm45143410280224"/><a data-primary="Scalable Vector Graphics (SVG)" data-type="indexterm" id="idm45143410279456"/>is a commonly used format for graphics on the web, thanks to its ability to scale drawings without loss of quality. Many common graphics programs can output <em>.svg</em> files, which can then be included in web pages and other documents that will look good on a wide variety of screen sizes and devices.</p>&#13;
</dd>&#13;
<dt>EPUB</dt>&#13;
<dd>&#13;
<p>Electronic publication format<a data-primary="EPUB (electronic publication format)" data-type="indexterm" id="idm45143410257376"/><a data-primary="electronic publication format (EPUB)" data-type="indexterm" id="idm45143410256768"/> (<em>.epub</em>) is the widely accepted open standard for digital book publishing.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>As you can see from the preceding list, some common XML formats clearly indicate their relationship to XML; many others do not.<sup><a data-type="noteref" href="ch04.html#idm45143410254992" id="idm45143410254992-marker">19</a></sup></p>&#13;
&#13;
<p>Now that we have a high-level sense of how XML files work, let’s see what it takes to parse one with Python. Although Python has some built-in tools for parsing XML, we’ll be using a library <a data-primary="lxml library" data-type="indexterm" id="idm45143410253136"/>called <em>lxml</em>, which is particularly good at <a href="https://nickjanetakis.com/blog/how-i-used-the-lxml-library-to-parse-xml-20x-faster-in-python#xmltodict-vs-python-s-standard-library-vs-lxml">quickly parsing large XML files</a>. Even though our example files that follow are quite small, know that we could use basically the same code even if our data files got considerably larger.</p>&#13;
&#13;
<p class="pagebreak-before less_space">To begin with, we’ll be using an XML version of the same “U6” unemployment data that I’ve already downloaded from the FRED website using its API.<sup><a data-type="noteref" href="ch04.html#idm45143410249984" id="idm45143410249984-marker">20</a></sup> After downloading a copy of this file from <a href="https://drive.google.com/file/d/1gPGaDTT9Nn6BtlTtVp7gQLSuocMyIaLU">Google Drive</a>, you can use the script in <a data-type="xref" href="#xml_parsing">Example 4-12</a> to convert the source XML to a <em>.csv</em>.&#13;
Start with the <code>pip install</code>:</p>&#13;
<pre>pip install lxml</pre>&#13;
<div data-type="example" id="xml_parsing">&#13;
<h5><span class="label">Example 4-12. </span>xml_parsing.py</h5>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="c1"># an example of reading data from an .xml file with Python, using the "lxml"</code><code>&#13;
</code><code class="c1"># library.</code><code>&#13;
</code><code class="c1"># first, you'll need to pip install the lxml library:</code><code>&#13;
</code><code class="c1"># https://pypi.org/project/lxml/</code><code>&#13;
</code><code class="c1"># a helpful tutorial can be found here: https://lxml.de/tutorial.html</code><code>&#13;
</code><code class="c1"># the data used here is an instance of</code><code>&#13;
</code><code class="c1"># https://api.stlouisfed.org/fred/series/observations?series_id=U6RATE&amp; \</code><code>&#13;
</code><code class="c1"># api_key=YOUR_API_KEY_HERE</code><code>&#13;
</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># specify the "chapter" of the `lxml` library you want to import,</code><code>&#13;
</code><code class="c1"># in this case, `etree`, which stands for "ElementTree"</code><code>&#13;
</code><code class="kn">from</code><code> </code><code class="nn">lxml</code><code> </code><code class="kn">import</code><code> </code><code class="n">etree</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># import the `csv` library, to create our output file</code><code>&#13;
</code><code class="kn">import</code><code> </code><code class="nn">csv</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># choose a filename</code><code>&#13;
</code><code class="n">filename</code><code> </code><code class="o">=</code><code> </code><code class="s2">"</code><code class="s2">U6_FRED_data</code><code class="s2">"</code><code> </code><a class="co" href="#callout_working_with_file_based_and_feed_based_data_in_python_CO8-1" id="co_working_with_file_based_and_feed_based_data_in_python_CO8-1"><img alt="1" src="assets/1.png"/></a><code>&#13;
</code><code>&#13;
</code><code class="c1"># open our data file in read format, using "rb" as the "mode"</code><code>&#13;
</code><code class="n">xml_source_file</code><code> </code><code class="o">=</code><code> </code><code class="nb">open</code><code class="p">(</code><code class="n">filename</code><code class="o">+</code><code class="s2">"</code><code class="s2">.xml</code><code class="s2">"</code><code class="p">,</code><code class="s2">"</code><code class="s2">rb</code><code class="s2">"</code><code class="p">)</code><code> </code><a class="co" href="#callout_working_with_file_based_and_feed_based_data_in_python_CO8-2" id="co_working_with_file_based_and_feed_based_data_in_python_CO8-2"><img alt="2" src="assets/2.png"/></a><code>&#13;
</code><code>&#13;
</code><code class="c1"># pass our xml_source_file as an ingredient to the `lxml` library's</code><code>&#13;
</code><code class="c1"># `etree.parse()` method and store the result in a variable called `xml_doc`</code><code>&#13;
</code><code class="n">xml_doc</code><code> </code><code class="o">=</code><code> </code><code class="n">etree</code><code class="o">.</code><code class="n">parse</code><code class="p">(</code><code class="n">xml_source_file</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># start by getting the current xml document's "root" element</code><code>&#13;
</code><code class="n">document_root</code><code> </code><code class="o">=</code><code> </code><code class="n">xml_doc</code><code class="o">.</code><code class="n">getroot</code><code class="p">(</code><code class="p">)</code><code> </code><a class="co" href="#callout_working_with_file_based_and_feed_based_data_in_python_CO8-3" id="co_working_with_file_based_and_feed_based_data_in_python_CO8-3"><img alt="3" src="assets/3.png"/></a><code>&#13;
</code><code>&#13;
</code><code class="c1"># let's print it out to see what it looks like</code><code>&#13;
</code><code class="k">print</code><code class="p">(</code><code class="n">etree</code><code class="o">.</code><code class="n">tostring</code><code class="p">(</code><code class="n">document_root</code><code class="p">)</code><code class="p">)</code><code> </code><a class="co" href="#callout_working_with_file_based_and_feed_based_data_in_python_CO8-4" id="co_working_with_file_based_and_feed_based_data_in_python_CO8-4"><img alt="4" src="assets/4.png"/></a><code>&#13;
</code><code>&#13;
</code><code class="c1"># confirm that `document_root` is a well-formed XML element</code><code>&#13;
</code><code class="k">if</code><code> </code><code class="n">etree</code><code class="o">.</code><code class="n">iselement</code><code class="p">(</code><code class="n">document_root</code><code class="p">)</code><code class="p">:</code><code>&#13;
</code><code>&#13;
</code><code>    </code><code class="c1"># create our output file, naming it "xml_"+filename+".csv</code><code>&#13;
</code><code>    </code><code class="n">output_file</code><code> </code><code class="o">=</code><code> </code><code class="nb">open</code><code class="p">(</code><code class="s2">"</code><code class="s2">xml_</code><code class="s2">"</code><code class="o">+</code><code class="n">filename</code><code class="o">+</code><code class="s2">"</code><code class="s2">.csv</code><code class="s2">"</code><code class="p">,</code><code class="s2">"</code><code class="s2">w</code><code class="s2">"</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code>    </code><code class="c1"># use the `csv` library's "writer" recipe to easily write rows of data</code><code>&#13;
</code><code>    </code><code class="c1"># to `output_file`, instead of reading data *from* it</code><code>&#13;
</code><code>    </code><code class="n">output_writer</code><code> </code><code class="o">=</code><code> </code><code class="n">csv</code><code class="o">.</code><code class="n">writer</code><code class="p">(</code><code class="n">output_file</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code>    </code><code class="c1"># grab the first element of our xml document (using `document_root[0]`)</code><code>&#13;
</code><code>    </code><code class="c1"># and write its attribute keys as column headers to our output file</code><code>&#13;
</code><code>    </code><code class="n">output_writer</code><code class="o">.</code><code class="n">writerow</code><code class="p">(</code><code class="n">document_root</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="o">.</code><code class="n">attrib</code><code class="o">.</code><code class="n">keys</code><code class="p">(</code><code class="p">)</code><code class="p">)</code><code> </code><a class="co" href="#callout_working_with_file_based_and_feed_based_data_in_python_CO8-5" id="co_working_with_file_based_and_feed_based_data_in_python_CO8-5"><img alt="5" src="assets/5.png"/></a><code>&#13;
</code><code>&#13;
</code><code>    </code><code class="c1"># now, we need to loop through every element in our XML file</code><code>&#13;
</code><code>      </code><code class="k">for</code><code> </code><code class="n">child</code><code> </code><code class="ow">in</code><code> </code><code class="n">document_root</code><code class="p">:</code><code> </code><a class="co" href="#callout_working_with_file_based_and_feed_based_data_in_python_CO8-6" id="co_working_with_file_based_and_feed_based_data_in_python_CO8-6"><img alt="6" src="assets/6.png"/></a><code>&#13;
</code><code>&#13;
</code><code>        </code><code class="c1"># now we'll use the `.values()` method to get each element's values</code><code>&#13;
</code><code>        </code><code class="c1"># as a list and then use that directly with the `writerow` recipe</code><code>&#13;
</code><code>        </code><code class="n">output_writer</code><code class="o">.</code><code class="n">writerow</code><code class="p">(</code><code class="n">child</code><code class="o">.</code><code class="n">attrib</code><code class="o">.</code><code class="n">values</code><code class="p">(</code><code class="p">)</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code>    </code><code class="c1"># officially close the `.csv` file we just wrote all that data to</code><code>&#13;
</code><code>    </code><code class="n">output_file</code><code class="o">.</code><code class="n">close</code><code class="p">(</code><code class="p">)</code></pre></div>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_working_with_file_based_and_feed_based_data_in_python_CO8-1" id="callout_working_with_file_based_and_feed_based_data_in_python_CO8-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>In this instance, there’s nothing within the data file (like a sheet name) that we can pull as a filename, so we’ll just make our own and use it to both load our source data and label our output file.</p></dd>&#13;
<dt><a class="co" href="#co_working_with_file_based_and_feed_based_data_in_python_CO8-2" id="callout_working_with_file_based_and_feed_based_data_in_python_CO8-2"><img alt="2" src="assets/2.png"/></a></dt>&#13;
<dd><p>I lied! The values we’ve been using for the “mode” of the <code>open()</code> function assume we want to interpret the source file as <em>text</em>. But because the <em>lxml</em> library expects byte data rather than text, we’ll use <code>rb</code> (“read bytes”) as the “mode.”</p></dd>&#13;
<dt><a class="co" href="#co_working_with_file_based_and_feed_based_data_in_python_CO8-3" id="callout_working_with_file_based_and_feed_based_data_in_python_CO8-3"><img alt="3" src="assets/3.png"/></a></dt>&#13;
<dd><p>There is a lot of malformed XML out there! In order to make sure that what looks like good XML actually <em>is</em>, we’ll retrieve the current XML document’s “root” element and make sure that it works.</p></dd>&#13;
<dt><a class="co" href="#co_working_with_file_based_and_feed_based_data_in_python_CO8-4" id="callout_working_with_file_based_and_feed_based_data_in_python_CO8-4"><img alt="4" src="assets/4.png"/></a></dt>&#13;
<dd><p>Because our XML is currently stored as byte data, we need to use the <code>etree.tostring()</code> method in order to view it as one.</p></dd>&#13;
<dt><a class="co" href="#co_working_with_file_based_and_feed_based_data_in_python_CO8-5" id="callout_working_with_file_based_and_feed_based_data_in_python_CO8-5"><img alt="5" src="assets/5.png"/></a></dt>&#13;
<dd><p>Thanks to the <em>lxml</em>, each XML element (or “node”) in our document has a property called <code>attrib</code>, whose data type is a Python dictionary (<code>dict</code>). Using the <a href="https://docs.python.org/3/library/stdtypes.html#typesmapping"><code>.keys()</code> method</a> returns all of our XML element’s attribute keys as a list. Since all of the elements in our source file are identical, we can use the keys of the first one to create a “header” row for our output file.</p></dd>&#13;
<dt><a class="co" href="#co_working_with_file_based_and_feed_based_data_in_python_CO8-6" id="callout_working_with_file_based_and_feed_based_data_in_python_CO8-6"><img alt="6" src="assets/6.png"/></a></dt>&#13;
<dd><p>The <em>lxml</em> library <a href="https://lxml.de/tutorial.html#elements-are-lists">converts XML elements to lists</a>, so we can use a simple <code>for...in</code> loop to go through the elements in our document.</p></dd>&#13;
</dl>&#13;
&#13;
<p>As it happens, the XML version of our unemployment data is structured very simply: it’s just a list of elements, and <em>all</em> the values we want to access are stored as attributes. As a result, we were able to pull the attribute values out of each element as a list and write them directly to our <em>.csv</em> file with only one line of code.</p>&#13;
&#13;
<p>Of course, there<a data-primary="RSS (Really Simple Syndication)" data-type="indexterm" id="rss-read"/><a data-primary="Really Simple Syndication (RSS)" data-type="indexterm" id="really-simple-read"/> are many times when we’ll want to pull data from more complex XML formats, especially those like RSS or Atom. To see what it takes to handle something slightly more complex, in <a data-type="xref" href="#bbc_example">Example 4-13</a> we’ll parse the BBC’s RSS feed of science and environment stories, which you can download a copy of from my <a href="https://drive.google.com/file/d/1zOaksshLfmXxLTipoOjTTnuO6PsVQgg2">Google Drive</a>.</p>&#13;
<div data-type="example" id="bbc_example">&#13;
<h5><span class="label">Example 4-13. </span>rss_parsing.py</h5>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="c1"># an example of reading data from an .xml file with Python, using the "lxml"</code><code>&#13;
</code><code class="c1"># library.</code><code>&#13;
</code><code class="c1"># first, you'll need to pip install the lxml library:</code><code>&#13;
</code><code class="c1"># https://pypi.org/project/lxml/</code><code>&#13;
</code><code class="c1"># the data used here is an instance of</code><code>&#13;
</code><code class="c1"># http://feeds.bbci.co.uk/news/science_and_environment/rss.xml</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># specify the "chapter" of the `lxml` library you want to import,</code><code>&#13;
</code><code class="c1"># in this case, `etree`, which stands for "ElementTree"</code><code>&#13;
</code><code class="kn">from</code><code> </code><code class="nn">lxml</code><code> </code><code class="kn">import</code><code> </code><code class="n">etree</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># import the `csv` library, to create our output file</code><code>&#13;
</code><code class="kn">import</code><code> </code><code class="nn">csv</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># choose a filename, for simplicity</code><code>&#13;
</code><code class="n">filename</code><code> </code><code class="o">=</code><code> </code><code class="s2">"</code><code class="s2">BBC News - Science &amp; Environment XML Feed</code><code class="s2">"</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># open our data file in read format, using "rb" as the "mode"</code><code>&#13;
</code><code class="n">xml_source_file</code><code> </code><code class="o">=</code><code> </code><code class="nb">open</code><code class="p">(</code><code class="n">filename</code><code class="o">+</code><code class="s2">"</code><code class="s2">.xml</code><code class="s2">"</code><code class="p">,</code><code class="s2">"</code><code class="s2">rb</code><code class="s2">"</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># pass our xml_source_file as an ingredient to the `lxml` library's</code><code>&#13;
</code><code class="c1"># `etree.parse()` method and store the result in a variable called `xml_doc`</code><code>&#13;
</code><code class="n">xml_doc</code><code> </code><code class="o">=</code><code> </code><code class="n">etree</code><code class="o">.</code><code class="n">parse</code><code class="p">(</code><code class="n">xml_source_file</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># start by getting the current xml document's "root" element</code><code>&#13;
</code><code class="n">document_root</code><code> </code><code class="o">=</code><code> </code><code class="n">xml_doc</code><code class="o">.</code><code class="n">getroot</code><code class="p">(</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># if the document_root is a well-formed XML element</code><code>&#13;
</code><code class="k">if</code><code> </code><code class="n">etree</code><code class="o">.</code><code class="n">iselement</code><code class="p">(</code><code class="n">document_root</code><code class="p">)</code><code class="p">:</code><code>&#13;
</code><code>&#13;
</code><code>    </code><code class="c1"># create our output file, naming it "rss_"+filename+".csv"</code><code>&#13;
</code><code>    </code><code class="n">output_file</code><code> </code><code class="o">=</code><code> </code><code class="nb">open</code><code class="p">(</code><code class="s2">"</code><code class="s2">rss_</code><code class="s2">"</code><code class="o">+</code><code class="n">filename</code><code class="o">+</code><code class="s2">"</code><code class="s2">.csv</code><code class="s2">"</code><code class="p">,</code><code class="s2">"</code><code class="s2">w</code><code class="s2">"</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code>    </code><code class="c1"># use the `csv` library's "writer" recipe to easily write rows of data</code><code>&#13;
</code><code>    </code><code class="c1"># to `output_file`, instead of reading data *from* it</code><code>&#13;
</code><code>    </code><code class="n">output_writer</code><code> </code><code class="o">=</code><code> </code><code class="n">csv</code><code class="o">.</code><code class="n">writer</code><code class="p">(</code><code class="n">output_file</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code>    </code><code class="c1"># document_root[0] is the "channel" element</code><code>&#13;
</code><code>    </code><code class="n">main_channel</code><code> </code><code class="o">=</code><code> </code><code class="n">document_root</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code>&#13;
</code><code>&#13;
</code><code>    </code><code class="c1"># the `find()` method returns *only* the first instance of the element name</code><code>&#13;
</code><code>    </code><code class="n">article_example</code><code> </code><code class="o">=</code><code> </code><code class="n">main_channel</code><code class="o">.</code><code class="n">find</code><code class="p">(</code><code class="s1">'</code><code class="s1">item</code><code class="s1">'</code><code class="p">)</code><code> </code><a class="co" href="#callout_working_with_file_based_and_feed_based_data_in_python_CO9-1" id="co_working_with_file_based_and_feed_based_data_in_python_CO9-1"><img alt="1" src="assets/1.png"/></a><code>&#13;
</code><code>&#13;
</code><code>    </code><code class="c1"># create an empty list in which to store our future column headers</code><code>&#13;
</code><code>    </code><code class="n">tag_list</code><code> </code><code class="o">=</code><code> </code><code class="p">[</code><code class="p">]</code><code>&#13;
</code><code>&#13;
</code><code>    </code><code class="k">for</code><code> </code><code class="n">child</code><code> </code><code class="ow">in</code><code> </code><code class="n">article_example</code><code class="o">.</code><code class="n">iterdescendants</code><code class="p">(</code><code class="p">)</code><code class="p">:</code><code> </code><a class="co" href="#callout_working_with_file_based_and_feed_based_data_in_python_CO9-2" id="co_working_with_file_based_and_feed_based_data_in_python_CO9-2"><img alt="2" src="assets/2.png"/></a><code>&#13;
</code><code>&#13;
</code><code>        </code><code class="c1"># add each tag to our would-be header list</code><code>&#13;
</code><code>        </code><code class="n">tag_list</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">child</code><code class="o">.</code><code class="n">tag</code><code class="p">)</code><code> </code><a class="co" href="#callout_working_with_file_based_and_feed_based_data_in_python_CO9-3" id="co_working_with_file_based_and_feed_based_data_in_python_CO9-3"><img alt="3" src="assets/3.png"/></a><code>&#13;
</code><code>&#13;
</code><code>        </code><code class="c1"># if the current tag has any attributes</code><code>&#13;
</code><code>        </code><code class="k">if</code><code> </code><code class="n">child</code><code class="o">.</code><code class="n">attrib</code><code class="p">:</code><code> </code><a class="co" href="#callout_working_with_file_based_and_feed_based_data_in_python_CO9-4" id="co_working_with_file_based_and_feed_based_data_in_python_CO9-4"><img alt="4" src="assets/4.png"/></a><code>&#13;
</code><code>&#13;
</code><code>            </code><code class="c1"># loop through the attribute keys in the tag</code><code>&#13;
</code><code>            </code><code class="k">for</code><code> </code><code class="n">attribute_name</code><code> </code><code class="ow">in</code><code> </code><code class="n">child</code><code class="o">.</code><code class="n">attrib</code><code class="o">.</code><code class="n">keys</code><code class="p">(</code><code class="p">)</code><code class="p">:</code><code> </code><a class="co" href="#callout_working_with_file_based_and_feed_based_data_in_python_CO9-5" id="co_working_with_file_based_and_feed_based_data_in_python_CO9-5"><img alt="5" src="assets/5.png"/></a><code>&#13;
</code><code>&#13;
</code><code>                </code><code class="c1"># append the attribute name to our `tag_list` column headers</code><code>&#13;
</code><code>                </code><code class="n">tag_list</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">attribute_name</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code>    </code><code class="c1"># write the contents of `tag_list` to our output file as column headers</code><code>&#13;
</code><code>    </code><code class="n">output_writer</code><code class="o">.</code><code class="n">writerow</code><code class="p">(</code><code class="n">tag_list</code><code class="p">)</code><code> </code><a class="co" href="#callout_working_with_file_based_and_feed_based_data_in_python_CO9-6" id="co_working_with_file_based_and_feed_based_data_in_python_CO9-6"><img alt="6" src="assets/6.png"/></a><code>&#13;
</code><code>&#13;
</code><code>    </code><code class="c1"># now we want to grab *every* &lt;item&gt; element in our file</code><code>&#13;
</code><code>    </code><code class="c1"># so we use the `findall()` method instead of `find()`</code><code>&#13;
</code><code>    </code><code class="k">for</code><code> </code><code class="n">item</code><code> </code><code class="ow">in</code><code> </code><code class="n">main_channel</code><code class="o">.</code><code class="n">findall</code><code class="p">(</code><code class="s1">'</code><code class="s1">item</code><code class="s1">'</code><code class="p">)</code><code class="p">:</code><code>&#13;
</code><code>&#13;
</code><code>        </code><code class="c1"># empty list for holding our new row's content</code><code>&#13;
</code><code>        </code><code class="n">new_row</code><code> </code><code class="o">=</code><code> </code><code class="p">[</code><code class="p">]</code><code>&#13;
</code><code>&#13;
</code><code>        </code><code class="c1"># now we'll use our list of tags to get the contents of each element</code><code>&#13;
</code><code>        </code><code class="k">for</code><code> </code><code class="n">tag</code><code> </code><code class="ow">in</code><code> </code><code class="n">tag_list</code><code class="p">:</code><code>&#13;
</code><code>&#13;
</code><code>            </code><code class="c1"># if there is anything in the element with a given tag name</code><code>&#13;
</code><code>            </code><code class="k">if</code><code> </code><code class="n">item</code><code class="o">.</code><code class="n">findtext</code><code class="p">(</code><code class="n">tag</code><code class="p">)</code><code class="p">:</code><code>&#13;
</code><code>&#13;
</code><code>                </code><code class="c1"># append it to our new row</code><code>&#13;
</code><code>                </code><code class="n">new_row</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">item</code><code class="o">.</code><code class="n">findtext</code><code class="p">(</code><code class="n">tag</code><code class="p">)</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code>            </code><code class="c1"># otherwise, make sure it's the "isPermaLink" attribute</code><code>&#13;
</code><code>            </code><code class="k">elif</code><code> </code><code class="n">tag</code><code> </code><code class="o">==</code><code> </code><code class="s2">"</code><code class="s2">isPermaLink</code><code class="s2">"</code><code class="p">:</code><code>&#13;
</code><code>&#13;
</code><code>                </code><code class="c1"># grab its value from the &lt;guid&gt; element</code><code>&#13;
</code><code>                </code><code class="c1"># and append it to our row</code><code>&#13;
</code><code>                </code><code class="n">new_row</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">item</code><code class="o">.</code><code class="n">find</code><code class="p">(</code><code class="s1">'</code><code class="s1">guid</code><code class="s1">'</code><code class="p">)</code><code class="o">.</code><code class="n">get</code><code class="p">(</code><code class="s2">"</code><code class="s2">isPermaLink</code><code class="s2">"</code><code class="p">)</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code>        </code><code class="c1"># write the new row to our output file!</code><code>&#13;
</code><code>        </code><code class="n">output_writer</code><code class="o">.</code><code class="n">writerow</code><code class="p">(</code><code class="n">new_row</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code>    </code><code class="c1"># officially close the `.csv` file we just wrote all that data to</code><code>&#13;
</code><code>    </code><code class="n">output_file</code><code class="o">.</code><code class="n">close</code><code class="p">(</code><code class="p">)</code></pre></div>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_working_with_file_based_and_feed_based_data_in_python_CO9-1" id="callout_working_with_file_based_and_feed_based_data_in_python_CO9-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>As always, we’ll want to balance what we handle programmatically and what we review visually. In looking at our data, it’s clear that each article’s information is stored in a separate <code>item</code> element. Since copying over the individual tag and attribute names would be time-consuming and error prone, however, we’ll go through <em>one</em> <code>item</code> element and make a list of all the tags (and attributes) within it, which we’ll then use as the column headers for our output <em>.csv</em> file.</p></dd>&#13;
<dt><a class="co" href="#co_working_with_file_based_and_feed_based_data_in_python_CO9-2" id="callout_working_with_file_based_and_feed_based_data_in_python_CO9-2"><img alt="2" src="assets/2.png"/></a></dt>&#13;
<dd><p>The <code>iterdescendants()</code> method is particular to the <em>lxml</em> library. It returns <em>only</em> the <em>descendants</em> of an XML element, while <a href="https://lxml.de/api.html#iteration">the more common <code>iter()</code> method would return <em>both</em> the element itself <em>and</em> its children or “descendants.”</a></p></dd>&#13;
<dt><a class="co" href="#co_working_with_file_based_and_feed_based_data_in_python_CO9-3" id="callout_working_with_file_based_and_feed_based_data_in_python_CO9-3"><img alt="3" src="assets/3.png"/></a></dt>&#13;
<dd><p>Using <code>child.tag</code> will retrieve the text of the tagname of the child element. For example, for the <code>&lt;pubDate&gt;`</code> element it will return <code>pubDate</code>.</p></dd>&#13;
<dt><a class="co" href="#co_working_with_file_based_and_feed_based_data_in_python_CO9-4" id="callout_working_with_file_based_and_feed_based_data_in_python_CO9-4"><img alt="4" src="assets/4.png"/></a></dt>&#13;
<dd><p>Only one tag in our <code>&lt;item&gt;</code> element has an attribute, but we still want to include it in our output.</p></dd>&#13;
<dt><a class="co" href="#co_working_with_file_based_and_feed_based_data_in_python_CO9-5" id="callout_working_with_file_based_and_feed_based_data_in_python_CO9-5"><img alt="5" src="assets/5.png"/></a></dt>&#13;
<dd><p>The <code>keys()</code> method will give us a list of all the keys in the list of attributes that belong to the tag. Be sure to get its name as a string (instead of a one-item list).</p></dd>&#13;
<dt><a class="co" href="#co_working_with_file_based_and_feed_based_data_in_python_CO9-6" id="callout_working_with_file_based_and_feed_based_data_in_python_CO9-6"><img alt="6" src="assets/6.png"/></a></dt>&#13;
<dd><p>That whole <code>article_example</code> <code>for</code> loop was just to build <code>tag_list</code>—but it was worth it!</p></dd>&#13;
</dl>&#13;
&#13;
<p>As you can see from <a data-type="xref" href="#bbc_example">Example 4-13</a>, with the help of the <em>lxml</em> library, parsing<a data-primary="RSS (Really Simple Syndication)" data-startref="rss-read" data-type="indexterm" id="idm45143409296304"/><a data-primary="Really Simple Syndication (RSS)" data-startref="really-simple-read" data-type="indexterm" id="idm45143409295232"/> even slightly more complex XML in Python is still reasonably straightforward.</p>&#13;
&#13;
<p>While XML is still a popular data format for news feeds and a handful of other file types, there are a number of features that make it less than ideal for handling the high-volume data feeds of the modern web.</p>&#13;
&#13;
<p>First, there is the simple issue of size. While XML files can be wonderfully descriptive—reducing the need for separate data dictionaries—the fact that most elements contain both an opening tag and a corresponding closing tag (e.g., <code>&lt;item&gt;</code> and <code>&lt;/item&gt;</code>) also makes XML somewhat <em>verbose</em>: there is a lot of text in an XML document that <em>isn’t</em> content. This isn’t a big deal when your document has a few dozen or even a few thousand elements, but when you’re trying to handle millions or billions of posts on the social web, all that redundant text can really slow things down.</p>&#13;
&#13;
<p>Second, while XML isn’t exactly <em>difficult</em> to transform into other data formats, the process also isn’t exactly seamless. The <em>lxml</em> library (among others) makes parsing XML with Python pretty simple, but doing the same task with web-focused languages like JavaScript is convoluted and onerous. Given JavaScript’s prevalence on the web, it’s not surprising that a feed-type data format that works seamlessly with JavaScript would be developed at some point. As we’ll see in&#13;
the next section,&#13;
many of XML’s limitations as a data format are addressed by the <code>object</code>-like nature of the <em>.json</em> format, which is at this point the most popular format for feed-type data on the<a data-primary="data wrangling" data-secondary="reading data" data-startref="data-wrangling-read-xml" data-tertiary="from .xml files" data-tertiary-sortas="xml files" data-type="indexterm" id="idm45143409228112"/><a data-primary="Python" data-secondary="reading data" data-startref="python-read-xml" data-tertiary="from .xml files" data-tertiary-sortas="xml files" data-type="indexterm" id="idm45143409226432"/><a data-primary="reading data" data-secondary="from .xml files" data-secondary-sortas="xml files" data-startref="read-data-xml" data-type="indexterm" id="idm45143409224672"/><a data-primary=".xml files (Extensible Markup Language)" data-primary-sortas="xml files" data-secondary="reading data from" data-startref="xml-read" data-type="indexterm" id="idm45143409223184"/> internet.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="JSON: Web data, the next generation" data-type="sect3"><div class="sect3" id="json_data">&#13;
<h3>JSON: Web data, the next generation</h3>&#13;
&#13;
<p>In <a data-primary="data wrangling" data-secondary="reading data" data-tertiary="from .json files" data-tertiary-sortas="json files" data-type="indexterm" id="data-wrangling-read-json"/><a data-primary="Python" data-secondary="reading data" data-tertiary="from .json files" data-tertiary-sortas="json files" data-type="indexterm" id="python-read-json"/><a data-primary="reading data" data-secondary="from .json files" data-secondary-sortas="json files" data-type="indexterm" id="read-data-json"/><a data-primary=".json files (JavaScript Object Notation)" data-primary-sortas="json files" data-secondary="reading data from" data-type="indexterm" id="json-read"/>principle, JSON is similar to XML in that it uses nesting to cluster related pieces of information into records and fields. JSON is also fairly human readable, though the fact that it doesn’t support comments means that JSON feeds may require more robust data dictionaries than XML documents.</p>&#13;
&#13;
<p>To get started, let’s take a look at the<a data-primary=".json files (JavaScript Object Notation)" data-primary-sortas="json files" data-secondary="components of" data-type="indexterm" id="json-components"/> small JSON document in <a data-type="xref" href="#sample_JSON_document">Example 4-14</a>.</p>&#13;
<div data-type="example" id="sample_JSON_document">&#13;
<h5><span class="label">Example 4-14. </span>Sample JSON document</h5>&#13;
&#13;
<pre data-code-language="json" data-type="programlisting"><code class="p">{</code>&#13;
<code class="nt">"author"</code><code class="p">:</code> <code class="s2">"Susan E. McGregor"</code><code class="p">,</code>&#13;
<code class="nt">"book"</code><code class="p">:</code> <code class="p">{</code>&#13;
    <code class="nt">"bookURL"</code><code class="p">:</code> <code class="s2">"https://www.oreilly.com/library/view/practical-python-data/</code>&#13;
<code class="s2">        9781492091493/"</code><code class="p">,</code>&#13;
    <code class="nt">"bookAbstract"</code><code class="p">:</code> <code class="s2">"There are awesome discoveries to be made and valuable</code>&#13;
<code class="s2">        stories to be told in datasets--and this book will help you uncover</code>&#13;
<code class="s2">        them."</code><code class="p">,</code>&#13;
    <code class="nt">"pubDate"</code><code class="p">:</code> <code class="s2">"2022-02-01"</code>&#13;
<code class="p">},</code>&#13;
<code class="nt">"papers"</code><code class="p">:</code> <code class="p">[{</code>&#13;
    <code class="nt">"paperURL"</code><code class="p">:</code> <code class="s2">"https://www.usenix.org/conference/usenixsecurity15/</code>&#13;
<code class="s2">        technical-sessions/presentation/mcgregor"</code><code class="p">,</code>&#13;
    <code class="nt">"paperTitle"</code><code class="p">:</code> <code class="s2">"Investigating the computer security practices and needs</code>&#13;
<code class="s2">        of journalists"</code><code class="p">,</code>&#13;
    <code class="nt">"pubDate"</code><code class="p">:</code> <code class="s2">"2015-08-12"</code>&#13;
<code class="p">},</code>&#13;
    <code class="p">{</code>&#13;
    <code class="nt">"paperURL"</code><code class="p">:</code> <code class="s2">"https://www.aclweb.org/anthology/W18-5104.pdf"</code><code class="p">,</code>&#13;
    <code class="nt">"paperTitle"</code><code class="p">:</code> <code class="s2">"Predictive embeddings for hate speech detection on</code>&#13;
<code class="s2">        twitter"</code><code class="p">,</code>&#13;
    <code class="nt">"pubDate"</code><code class="p">:</code> <code class="s2">"2018-10-31"</code>&#13;
<code class="p">}</code>&#13;
    <code class="p">]</code>&#13;
<code class="p">}</code></pre></div>&#13;
&#13;
<p>Like XML, the grammatical “rules” of JSON are quite simple: there are only three distinct data structures in JSON documents, all of which appear in <a data-type="xref" href="#sample_JSON_document">Example 4-14</a>:</p>&#13;
<dl>&#13;
<dt>Key/value pairs</dt>&#13;
<dd>&#13;
<p>Technically,<a data-primary="key/value pairs" data-secondary="in JSON" data-secondary-sortas="JSON" data-type="indexterm" id="idm45143409833344"/> everything within a JSON document is a key/value pair, with the <em>key</em> enclosed in quotes to the left of a colon (<code>:</code>) and the <em>value</em> being whatever appears to the right of the colon. Note that while keys must <em>always</em> be strings, <em>values</em> can be strings (as in <code>author</code>), objects (as in <code>book</code>), or lists (as in <code>papers</code>).</p>&#13;
</dd>&#13;
<dt>Objects</dt>&#13;
<dd>&#13;
<p>These are<a data-primary="objects (JSON)" data-type="indexterm" id="idm45143409826976"/> opened and closed using pairs of curly braces (<code>{}</code>). In <a data-type="xref" href="#sample_JSON_document">Example 4-14</a>, there are four objects total: the document itself (indicated by the left-justified curly braces), the <code>book</code> object, and the two unnamed objects in the <code>papers</code> list.</p>&#13;
</dd>&#13;
<dt>Lists</dt>&#13;
<dd>&#13;
<p>These are<a data-primary="lists" data-secondary="JSON" data-type="indexterm" id="idm45143409822592"/> enclosed by square brackets (<code>[]</code>) and can only contain comma-separated <a data-primary=".json files (JavaScript Object Notation)" data-primary-sortas="json files" data-secondary="components of" data-startref="json-components" data-type="indexterm" id="idm45143409821024"/>objects.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>While<a data-primary=".json files (JavaScript Object Notation)" data-primary-sortas="json files" data-secondary="XML versus" data-type="indexterm" id="idm45143409818736"/> XML and JSON can be used to encode the same data, there are some notable differences in what each allows. For example, JSON files do not contain a <code>doc-type</code> specification, nor can they include comments. Also, while XML lists are somewhat implicit (any repeated element functions something like a list), in JSON, lists must be specified by square brackets (<code>[]</code>).</p>&#13;
&#13;
<p>Finally, although JSON was designed with JavaScript in mind, you may have noticed that its structures are very similar to Python <code>dict</code> and <code>list</code> types. This is part of what makes parsing JSON very straightforward with Python as well as JavaScript (and a range of other languages).</p>&#13;
&#13;
<p>To see just how straightforward this is, in <a data-type="xref" href="#json_parsing">Example 4-15</a> we’ll parse the same data as we did in <a data-type="xref" href="#xml_parsing">Example 4-12</a> but in the <em>.json</em> format also provided by the FRED API. You can download the file from this Google Drive link: <a href="https://drive.google.com/file/d/1Mpb2f5qYgHnKcU1sTxTmhOPHfzIdeBsq/view?usp=sharing"><em class="hyperlink">https://drive.google.com/file/d/1Mpb2f5qYgHnKcU1sTxTmhOPHfzIdeBsq/view?usp=sharing</em></a>.</p>&#13;
<div data-type="example" id="json_parsing">&#13;
<h5><span class="label">Example 4-15. </span>json_parsing.py</h5>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="c1"># a simple example of reading data from a .json file with Python,</code><code>&#13;
</code><code class="c1"># using the built-in "json" library. The data used here is an instance of</code><code>&#13;
</code><code class="c1"># https://api.stlouisfed.org/fred/series/observations?series_id=U6RATE&amp; \</code><code>&#13;
</code><code class="c1"># file_type=json&amp;api_key=YOUR_API_KEY_HERE</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># import the `json` library, since that's our source file format</code><code>&#13;
</code><code class="kn">import</code><code> </code><code class="nn">json</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># import the `csv` library, to create our output file</code><code>&#13;
</code><code class="kn">import</code><code> </code><code class="nn">csv</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># choose a filename</code><code>&#13;
</code><code class="n">filename</code><code> </code><code class="o">=</code><code> </code><code class="s2">"</code><code class="s2">U6_FRED_data</code><code class="s2">"</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># open the file in read format ("r") as usual</code><code>&#13;
</code><code class="n">json_source_file</code><code> </code><code class="o">=</code><code> </code><code class="nb">open</code><code class="p">(</code><code class="n">filename</code><code class="o">+</code><code class="s2">"</code><code class="s2">.json</code><code class="s2">"</code><code class="p">,</code><code class="s2">"</code><code class="s2">r</code><code class="s2">"</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># pass the `json_source_file` as an ingredient to the json library's `load()`</code><code>&#13;
</code><code class="c1"># method and store the result in a variable called `json_data`</code><code>&#13;
</code><code class="n">json_data</code><code> </code><code class="o">=</code><code> </code><code class="n">json</code><code class="o">.</code><code class="n">load</code><code class="p">(</code><code class="n">json_source_file</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># create our output file, naming it "json_"+filename</code><code>&#13;
</code><code class="n">output_file</code><code> </code><code class="o">=</code><code> </code><code class="nb">open</code><code class="p">(</code><code class="s2">"</code><code class="s2">json_</code><code class="s2">"</code><code class="o">+</code><code class="n">filename</code><code class="o">+</code><code class="s2">"</code><code class="s2">.csv</code><code class="s2">"</code><code class="p">,</code><code class="s2">"</code><code class="s2">w</code><code class="s2">"</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># use the `csv` library's "writer" recipe to easily write rows of data</code><code>&#13;
</code><code class="c1"># to `output_file`, instead of reading data *from* it</code><code>&#13;
</code><code class="n">output_writer</code><code> </code><code class="o">=</code><code> </code><code class="n">csv</code><code class="o">.</code><code class="n">writer</code><code class="p">(</code><code class="n">output_file</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># grab the first element (at position "0"), and use its keys as the column headers</code><code>&#13;
</code><code class="n">output_writer</code><code class="o">.</code><code class="n">writerow</code><code class="p">(</code><code class="nb">list</code><code class="p">(</code><code class="n">json_data</code><code class="p">[</code><code class="s2">"</code><code class="s2">observations</code><code class="s2">"</code><code class="p">]</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="o">.</code><code class="n">keys</code><code class="p">(</code><code class="p">)</code><code class="p">)</code><code class="p">)</code><code> </code><a class="co" href="#callout_working_with_file_based_and_feed_based_data_in_python_CO10-1" id="co_working_with_file_based_and_feed_based_data_in_python_CO10-1"><img alt="1" src="assets/1.png"/></a><code>&#13;
</code><code>&#13;
</code><code class="k">for</code><code> </code><code class="n">obj</code><code> </code><code class="ow">in</code><code> </code><code class="n">json_data</code><code class="p">[</code><code class="s2">"</code><code class="s2">observations</code><code class="s2">"</code><code class="p">]</code><code class="p">:</code><code> </code><a class="co" href="#callout_working_with_file_based_and_feed_based_data_in_python_CO10-2" id="co_working_with_file_based_and_feed_based_data_in_python_CO10-2"><img alt="2" src="assets/2.png"/></a><code>&#13;
</code><code>&#13;
</code><code>    </code><code class="c1"># we'll create an empty list where we'll put the actual values of each object</code><code>&#13;
</code><code>    </code><code class="n">obj_values</code><code> </code><code class="o">=</code><code> </code><code class="p">[</code><code class="p">]</code><code>&#13;
</code><code>&#13;
</code><code>    </code><code class="c1"># for every `key` (which will become a column), in each object</code><code>&#13;
</code><code>    </code><code class="k">for</code><code> </code><code class="n">key</code><code class="p">,</code><code> </code><code class="n">value</code><code> </code><code class="ow">in</code><code> </code><code class="n">obj</code><code class="o">.</code><code class="n">items</code><code class="p">(</code><code class="p">)</code><code class="p">:</code><code> </code><a class="co" href="#callout_working_with_file_based_and_feed_based_data_in_python_CO10-3" id="co_working_with_file_based_and_feed_based_data_in_python_CO10-3"><img alt="3" src="assets/3.png"/></a><code>&#13;
</code><code>&#13;
</code><code>        </code><code class="c1"># let's print what's in here, just to see how the code sees it</code><code>&#13;
</code><code>        </code><code class="k">print</code><code class="p">(</code><code class="n">key</code><code class="p">,</code><code class="n">value</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code>        </code><code class="c1"># add the values to our list</code><code>&#13;
</code><code>        </code><code class="n">obj_values</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">value</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code>    </code><code class="c1"># now we've got the whole row, write the data to our output file</code><code>&#13;
</code><code>    </code><code class="n">output_writer</code><code class="o">.</code><code class="n">writerow</code><code class="p">(</code><code class="n">obj_values</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># officially close the `.csv` file we just wrote all that data to</code><code>&#13;
</code><code class="n">output_file</code><code class="o">.</code><code class="n">close</code><code class="p">(</code><code class="p">)</code></pre></div>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_working_with_file_based_and_feed_based_data_in_python_CO10-1" id="callout_working_with_file_based_and_feed_based_data_in_python_CO10-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>Because the <em>json</em> library interprets every object as a <a href="https://docs.python.org/3/library/stdtypes.html#dict-views">dictionary view object</a>, we need to tell Python to convert it to a regular list using the <code>list()</code> function.</p></dd>&#13;
<dt><a class="co" href="#co_working_with_file_based_and_feed_based_data_in_python_CO10-2" id="callout_working_with_file_based_and_feed_based_data_in_python_CO10-2"><img alt="2" src="assets/2.png"/></a></dt>&#13;
<dd><p>In most cases, the simplest way to find the name (or “key”) of the main JSON object in our document is just to look at it. Because JSON data is often rendered on a single line, however, we can get a better sense of its structure by pasting it into <a href="https://jsonlint.com">JSONLint</a>. This lets us see that our target data is a list whose key is <code>observations</code>.</p></dd>&#13;
<dt><a class="co" href="#co_working_with_file_based_and_feed_based_data_in_python_CO10-3" id="callout_working_with_file_based_and_feed_based_data_in_python_CO10-3"><img alt="3" src="assets/3.png"/></a></dt>&#13;
<dd><p>Because of the way that the <em>json</em> library works, if we try to just write the rows directly, we’ll get the values labeled with <code>dict</code>, rather than the data values &#13;
<span class="keep-together">themselves</span>. So we need to make another loop that goes through every value in every <code>json</code> object one at a time and appends that value to our <code>obj_values</code> list.</p></dd>&#13;
</dl>&#13;
&#13;
<p>Although<a data-primary=".json files (JavaScript Object Notation)" data-primary-sortas="json files" data-secondary="XML versus" data-type="indexterm" id="idm45143409789136"/> JSON is not <em>quite</em> as human readable as XML, it has other advantages that we’ve touched on, like smaller file size and broader code compatibility. Likewise, while JSON is not as descriptive as XML, JSON data sources (often APIs) are usually reasonably well documented; this reduces the need to simply infer what a given &#13;
<span class="keep-together">key/value</span> pair is describing. Like all work with data, however, the first step in wrangling JSON-formatted data is to understand its context as much as<a data-primary="data wrangling" data-secondary="reading data" data-startref="data-wrangling-read-json" data-tertiary="from .json files" data-tertiary-sortas="json files" data-type="indexterm" id="idm45143409786288"/><a data-primary="Python" data-secondary="reading data" data-startref="python-read-json" data-tertiary="from .json files" data-tertiary-sortas="json files" data-type="indexterm" id="idm45143409784528"/><a data-primary="reading data" data-secondary="from .json files" data-secondary-sortas="json files" data-startref="read-data-json" data-type="indexterm" id="idm45143409782768"/><a data-primary=".json files (JavaScript Object Notation)" data-primary-sortas="json files" data-secondary="reading data from" data-startref="json-read" data-type="indexterm" id="idm45143409781280"/> possible.</p>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45143409779696">&#13;
<h5>Wherefore Art Thou, Whitespace?</h5>&#13;
<p>Unlike <em>.tsv</em> and <em>.txt</em> files—and the Python programming language itself—neither XML <a data-primary=".json files (JavaScript Object Notation)" data-primary-sortas="json files" data-secondary="whitespace" data-type="indexterm" id="idm45143409777040"/><a data-primary=".xml files (Extensible Markup Language)" data-primary-sortas="xml files" data-secondary="whitespace" data-type="indexterm" id="idm45143409775744"/><a data-primary="data sources" data-secondary="feed-based" data-tertiary="whitespace in" data-type="indexterm" id="idm45143409774560"/><a data-primary="feed-based data sources" data-secondary="whitespace in" data-type="indexterm" id="idm45143409773344"/><a data-primary="whitespace" data-secondary="in XML/JSON files" data-secondary-sortas="XML/JSON files" data-type="indexterm" id="idm45143409772400"/>nor JSON is “whitespace dependent.” As long as the carets, curly braces, and other punctuation marks are all in the right place, these data feed-type formats can have everything crushed up on a single line and still work just fine. For the sake of readability, the examples I’ve presented in this chapter have all been nicely formatted, but that’s not how you’ll usually encounter these data types, especially on the web. Though many web browsers <em>will</em> show XML in its properly indented format (for example, see <em>Los Angeles Times’</em> <a href="https://latimes.com/sitemap-202101.xml">daily “sitemap”</a>), most JSON data will be rendered as run-on lines of text (as with the Citi Bike <a href="https://feeds.citibikenyc.com/stations/stations.json">real-time data feed</a>).</p>&#13;
&#13;
<p>Since effectively parsing either data format first requires understanding its overall structure, looking at a properly formatted version of any feed-type data file you’re working with is a crucial first step. With well-structured XML, opening the file (or URL) in a web browser in usually enough.<sup><a data-type="noteref" href="ch04.html#idm45143409767664" id="idm45143409767664-marker">21</a></sup></p>&#13;
&#13;
<p>For smaller <em>.json</em> files, you can copy and paste (using keyboard shortcuts to “Select All” and “Copy” is easiest) the data straight from the source into an online formatting tool like <a href="https://jsonlint.com">JSONLint</a> or <a href="https://jsonformatter.org/json-pretty-print">JSON formatter</a>. If the JSON file is especially large, or you don’t have internet access, however, it’s also possible to use Python in the terminal to create a new, formatted <em>.json</em> file from an unformatted source JSON file, using the following &#13;
<span class="keep-together">command</span>:</p>&#13;
<pre>cat ugly.json | python -mjson.tool &gt; pretty.json</pre>&#13;
&#13;
<p>where <code>ugly.json</code> is your unformatted file. This will create the output file <code>pretty.json</code>, which you can then open in Atom or another text editor in order to see the structure of the document.</p>&#13;
</div></aside>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Working with Unstructured Data" data-type="sect1"><div class="sect1" id="unstructured_data">&#13;
<h1>Working with Unstructured Data</h1>&#13;
&#13;
<p>As we<a data-primary="data sources" data-secondary="unstructured data" data-type="indexterm" id="idm45143409759488"/><a data-primary="unstructured data sources" data-type="indexterm" id="idm45143409758480"/> discussed in <a data-type="xref" href="#structured_vs_unstructured">“Structured Versus Unstructured Data”</a>, the process of creating data depends on introducing some structure to information; otherwise we can’t methodically analyze or derive meaning from it. Even though the latter often includes large segments of human-written “free” text, both table-type and feed-type data are relatively structured and, most importantly, machine readable.</p>&#13;
&#13;
<p>When we deal with unstructured data, by contrast, our work always involves approximations: we cannot be certain that our programmatic wrangling efforts will return an accurate interpretation of the underlying information. This is because most unstructured data is a representation of content that is designed to be perceived and interpreted by humans. And, as we discussed in <a data-type="xref" href="ch02.html#chapter2">Chapter 2</a>, while they can process large quantities of data much faster and with fewer errors than humans can, computers can still be tricked by unstructured data that would never fool a human, such as mistaking a <a href="https://spectrum.ieee.org/cars-that-think/transportation/sensors/slight-street-sign-modifications-can-fool-machine-learning-algorithms">slightly modified stop sign for a speed limit sign</a>. Naturally, this means that when dealing with data that <em>isn’t</em> machine readable, we always need to do extra proofing and verification—but Python can still help us wrangle such data into a more usable format.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Image-Based Text: Accessing Data in PDFs" data-type="sect2"><div class="sect2" id="idm45143409753168">&#13;
<h2>Image-Based Text: Accessing Data in PDFs</h2>&#13;
&#13;
<p>The<a data-primary="data sources" data-secondary="PDFs" data-type="indexterm" id="data-sources-unstructured-pdf"/><a data-primary="unstructured data sources" data-secondary="PDFs" data-type="indexterm" id="unstructured-data-pdf"/><a data-primary="PDFs (Portable Document Format)" data-type="indexterm" id="pdf"/><a data-primary="Portable Document Format" data-see="PDFs" data-type="indexterm" id="idm45143409747968"/><a data-primary="image-based text" data-see="PDFs" data-type="indexterm" id="idm45143409747008"/><a data-primary="text, image-based" data-see="PDFs" data-type="indexterm" id="idm45143409746064"/> Portable Document Format (PDF) was created in the early 1990s as a mechanism for preserving the visual integrity of electronic documents—whether they were created in a text-editing program or captured from printed materials.<sup><a data-type="noteref" href="ch04.html#idm45143409744752" id="idm45143409744752-marker">22</a></sup> Preserving documents’ visual appearance also meant that, <em>unlike</em> machine-readable formats (such as word-processing documents), it was difficult to alter or extract their contents—an important feature for creating everything from digital versions of contracts to formal letters.</p>&#13;
&#13;
<p>In other words, wrangling the data in PDFs was, at first, somewhat difficult by design. Because accessing the data in printed documents is a shared problem, however, work in <a data-primary="optical character recognition (OCR)" data-type="indexterm" id="idm45143409742160"/><a data-primary="OCR (optical character recognition)" data-type="indexterm" id="idm45143409741488"/>optical character recognition (OCR) actually began as early as the late <em>19th</em> century.<sup><a data-type="noteref" href="ch04.html#idm45143409740288" id="idm45143409740288-marker">23</a></sup> Even digital OCR tools have been widely available in software packages and online for decades, so while they are far from perfect, the data contained in this type of file is also not entirely out of reach.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="When to work with text in PDFs" data-type="sect3"><div class="sect3" id="idm45143409736576">&#13;
<h3>When to work with text in PDFs</h3>&#13;
&#13;
<p>In general, <a data-primary="PDFs (Portable Document Format)" data-secondary="when to use" data-type="indexterm" id="idm45143409735072"/>working with PDFs is a last resort (much, as we’ll see in <a data-type="xref" href="ch05.html#chapter5">Chapter 5</a>, as web scraping should be). In general, if you can avoid relying on PDF information, you should. As noted previously, the process of extracting information from PDFs will generally yield an <em>approximation</em> of the document’s contents, so proofing for accuracy is a nonnegotiable part of any <em>.pdf</em>-based data wrangling workflow. That said, there is an enormous quantity of information that is <em>only</em> available as images or PDFs of scanned documents, and Python is an efficient way to extract a reasonably accurate first version of such document text.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Where to find PDFs" data-type="sect3"><div class="sect3" id="idm45143409731216">&#13;
<h3>Where to find PDFs</h3>&#13;
&#13;
<p>If you’re <a data-primary="PDFs (Portable Document Format)" data-secondary="finding" data-type="indexterm" id="idm45143409729888"/><a data-primary="finding" data-secondary="PDFs" data-type="indexterm" id="idm45143409728912"/>confident that the data you want can only be found in PDF format, then you can (and should) use the tips in <a data-type="xref" href="#smart_searching">“Smart Searching for Specific Data Types”</a> to locate this file type using an online search. More than likely, you will request information from a person or organization, and they will provide it as PDFs, leaving you to deal with the problem of how to extract the information you need. As a result, most of the time you will not need to go looking for PDFs—more often than not they will, unfortunately, find you.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Wrangling PDFs with Python" data-type="sect2"><div class="sect2" id="wrangling_pdfs">&#13;
<h2>Wrangling PDFs with Python</h2>&#13;
&#13;
<p>Because PDFs <a data-primary="data wrangling" data-secondary="reading data" data-tertiary="from .pdf files" data-tertiary-sortas="pdf files" data-type="indexterm" id="data-wrangling-read-pdf"/><a data-primary="Python" data-secondary="reading data" data-tertiary="from .pdf files" data-tertiary-sortas="pdf files" data-type="indexterm" id="python-read-pdf"/><a data-primary="reading data" data-secondary="from .pdf files" data-secondary-sortas="pdf files" data-tertiary="with Python" data-type="indexterm" id="read-data-pdf-python"/><a data-primary="PDFs (Portable Document Format)" data-secondary="reading data from" data-tertiary="with Python" data-type="indexterm" id="pdf-read-python"/>can be generated both from machine-readable text (such as word-processing documents) and from scanned images, it is sometimes possible to extract the document’s “live” text programmatically with relatively few errors. While it seems straightforward, however, this method can still be unreliable because <em>.pdf</em> files can be generated with a wide range of encodings that can be difficult to detect accurately. So while this <em>can</em> be a high-accuracy method of extracting text from a <em>.pdf</em>, the likelihood of it working for any <em>given</em> file is low.</p>&#13;
&#13;
<p>Because of this, I’m going to focus here on using OCR to recognize and extract the text in <em>.pdf</em> files. This will require two steps:</p>&#13;
<ol>&#13;
<li>&#13;
<p>Convert the document pages into individual images.</p>&#13;
</li>&#13;
<li>&#13;
<p>Run OCR on the page images, extract the text, and write it to individual text files.</p>&#13;
</li>&#13;
&#13;
</ol>&#13;
&#13;
<p>Unsurprisingly, we’ll need to install quite a few more Python libraries in order to make this all possible. First, we’ll install a couple of libraries for converting our <em>.pdf</em> pages to images. The first is a general-purpose library<a data-primary="poppler library" data-type="indexterm" id="idm45143409710544"/><a data-primary="pdf2image library" data-type="indexterm" id="idm45143409709760"/> called <code>poppler</code> that is needed to make our Python-specific library <code>pdf2image</code> work. We’ll be using <code>pdf2image</code> to (you guessed it!) convert our <em>.pdf</em> file to a series of images:</p>&#13;
<pre>sudo apt install poppler-utils</pre>&#13;
&#13;
<p class="pagebreak-before less_space">Then:</p>&#13;
<pre>pip install pdf2image</pre>&#13;
&#13;
<p>Next, we need to install the tools for performing the OCR process. The first one is a general library called&#13;
<a href="https://github.com/tesseract-ocr/tesseract"><em>tesseract-ocr</em></a>, which uses machine learning to recognize the text in images; the second is a Python library that relies <a data-primary="tesseract-ocr library" data-type="indexterm" id="idm45143409704192"/><a data-primary="pytesseract library" data-type="indexterm" id="idm45143409703520"/>on <em>tesseract-ocr</em> called <em>pytesseract</em>:</p>&#13;
<pre>sudo apt-get install tesseract-ocr</pre>&#13;
&#13;
<p>Then:</p>&#13;
<pre>pip install pytesseract</pre>&#13;
&#13;
<p>Finally, we need a helper library for Python that can do the computer vision needed to bridge the<a data-primary="opencv-python library" data-type="indexterm" id="idm45143409699984"/> gap between our page images and our OCR library:</p>&#13;
<pre>pip install opencv-python</pre>&#13;
&#13;
<p>Phew! If that seems like a lot of extra libraries, keep in mind that what we’re technically using here are is <em>machine learning</em>, one <a data-primary="machine learning" data-type="indexterm" id="idm45143409697760"/>of those buzzy data science technologies that drives so much of the “artificial intelligence” out there. Fortunately for us, Tesseract in particular is relatively robust and inclusive: though it was originally developed by Hewlett-Packard as a proprietary system in the early 1980s, it was open sourced in 2005 and currently supports more than 100 languages—so feel free to try the solution in <a data-type="xref" href="#pdf_parsing">Example 4-16</a> out on non-English text as well!</p>&#13;
<div data-type="example" id="pdf_parsing">&#13;
<h5><span class="label">Example 4-16. </span>pdf_parsing.py</h5>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="c1"># a basic example of reading data from a .pdf file with Python,</code><code>&#13;
</code><code class="c1"># using `pdf2image` to convert it to images, and then using the</code><code>&#13;
</code><code class="c1"># openCV and `tesseract` libraries to extract the text</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># the source data was downloaded from:</code><code>&#13;
</code><code class="c1"># https://files.stlouisfed.org/files/htdocs/publications/page1-econ/2020/12/01/ \</code><code>&#13;
</code><code class="c1"># unemployment-insurance-a-tried-and-true-safety-net_SE.pdf</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># the built-in `operating system` or `os` Python library will let us create</code><code>&#13;
</code><code class="c1"># a new folder in which to store our converted images and output text</code><code>&#13;
</code><code class="kn">import</code><code> </code><code class="nn">os</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># we'll import the `convert_from_path` "chapter" of the `pdf2image` library</code><code>&#13;
</code><code class="kn">from</code><code> </code><code class="nn">pdf2image</code><code> </code><code class="kn">import</code><code> </code><code class="n">convert_from_path</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># the built-in `glob`library offers a handy way to loop through all the files</code><code>&#13;
</code><code class="c1"># in a folder that have a certain file extension, for example</code><code>&#13;
</code><code class="kn">import</code><code> </code><code class="nn">glob</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># `cv2` is the actual library name for `openCV`</code><code>&#13;
</code><code class="kn">import</code><code> </code><code class="nn">cv2</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># and of course, we need our Python library for interfacing</code><code>&#13;
</code><code class="c1"># with the tesseract OCR process</code><code>&#13;
</code><code class="kn">import</code><code> </code><code class="nn">pytesseract</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># we'll use the pdf name to name both our generated images and text files</code><code>&#13;
</code><code class="n">pdf_name</code><code> </code><code class="o">=</code><code> </code><code class="s2">"</code><code class="s2">SafetyNet</code><code class="s2">"</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># our source pdf is in the same folder as our Python script</code><code>&#13;
</code><code class="n">pdf_source_file</code><code> </code><code class="o">=</code><code> </code><code class="n">pdf_name</code><code class="o">+</code><code class="s2">"</code><code class="s2">.pdf</code><code class="s2">"</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># as long as a folder with the same name as the pdf does not already exist</code><code>&#13;
</code><code class="k">if</code><code> </code><code class="n">os</code><code class="o">.</code><code class="n">path</code><code class="o">.</code><code class="n">isdir</code><code class="p">(</code><code class="n">pdf_name</code><code class="p">)</code><code> </code><code class="o">==</code><code> </code><code class="bp">False</code><code class="p">:</code><code>&#13;
</code><code>&#13;
</code><code>    </code><code class="c1"># create a new folder with that name</code><code>&#13;
</code><code>    </code><code class="n">target_folder</code><code> </code><code class="o">=</code><code> </code><code class="n">os</code><code class="o">.</code><code class="n">mkdir</code><code class="p">(</code><code class="n">pdf_name</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># store all the pages of the PDF in a variable</code><code>&#13;
</code><code class="n">pages</code><code> </code><code class="o">=</code><code> </code><code class="n">convert_from_path</code><code class="p">(</code><code class="n">pdf_source_file</code><code class="p">,</code><code> </code><code class="mi">300</code><code class="p">)</code><code> </code><a class="co" href="#callout_working_with_file_based_and_feed_based_data_in_python_CO11-1" id="co_working_with_file_based_and_feed_based_data_in_python_CO11-1"><img alt="1" src="assets/1.png"/></a><code>&#13;
</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># loop through all the converted pages, enumerating them so that the page</code><code>&#13;
</code><code class="c1"># number can be used to label the resulting images</code><code>&#13;
</code><code class="k">for</code><code> </code><code class="n">page_num</code><code class="p">,</code><code> </code><code class="n">page</code><code> </code><code class="ow">in</code><code> </code><code class="nb">enumerate</code><code class="p">(</code><code class="n">pages</code><code class="p">)</code><code class="p">:</code><code>&#13;
</code><code>&#13;
</code><code>    </code><code class="c1"># create unique filenames for each page image, combining the</code><code>&#13;
</code><code>    </code><code class="c1"># folder name and the page number</code><code>&#13;
</code><code>    </code><code class="n">filename</code><code> </code><code class="o">=</code><code> </code><code class="n">os</code><code class="o">.</code><code class="n">path</code><code class="o">.</code><code class="n">join</code><code class="p">(</code><code class="n">pdf_name</code><code class="p">,</code><code class="s2">"</code><code class="s2">p</code><code class="s2">"</code><code class="o">+</code><code class="nb">str</code><code class="p">(</code><code class="n">page_num</code><code class="p">)</code><code class="o">+</code><code class="s2">"</code><code class="s2">.png</code><code class="s2">"</code><code class="p">)</code><code> </code><a class="co" href="#callout_working_with_file_based_and_feed_based_data_in_python_CO11-2" id="co_working_with_file_based_and_feed_based_data_in_python_CO11-2"><img alt="2" src="assets/2.png"/></a><code>&#13;
</code><code>&#13;
</code><code>    </code><code class="c1"># save the image of the page in system</code><code>&#13;
</code><code>    </code><code class="n">page</code><code class="o">.</code><code class="n">save</code><code class="p">(</code><code class="n">filename</code><code class="p">,</code><code> </code><code class="s1">'</code><code class="s1">PNG</code><code class="s1">'</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># next, go through all the files in the folder that end in `.png`</code><code>&#13;
</code><code class="k">for</code><code> </code><code class="n">img_file</code><code> </code><code class="ow">in</code><code> </code><code class="n">glob</code><code class="o">.</code><code class="n">glob</code><code class="p">(</code><code class="n">os</code><code class="o">.</code><code class="n">path</code><code class="o">.</code><code class="n">join</code><code class="p">(</code><code class="n">pdf_name</code><code class="p">,</code><code> </code><code class="s1">'</code><code class="s1">*.png</code><code class="s1">'</code><code class="p">)</code><code class="p">)</code><code class="p">:</code><code> </code><a class="co" href="#callout_working_with_file_based_and_feed_based_data_in_python_CO11-3" id="co_working_with_file_based_and_feed_based_data_in_python_CO11-3"><img alt="3" src="assets/3.png"/></a><code>&#13;
</code><code>&#13;
</code><code>    </code><code class="c1"># replace the slash in the image's filename with a dot</code><code>&#13;
</code><code>    </code><code class="n">temp_name</code><code> </code><code class="o">=</code><code> </code><code class="n">img_file</code><code class="o">.</code><code class="n">replace</code><code class="p">(</code><code class="s2">"</code><code class="s2">/</code><code class="s2">"</code><code class="p">,</code><code class="s2">"</code><code class="s2">.</code><code class="s2">"</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code>    </code><code class="c1"># pull the unique page name (e.g. `p2`) from the `temp_name`</code><code>&#13;
</code><code>    </code><code class="n">text_filename</code><code> </code><code class="o">=</code><code> </code><code class="n">temp_name</code><code class="o">.</code><code class="n">split</code><code class="p">(</code><code class="s2">"</code><code class="s2">.</code><code class="s2">"</code><code class="p">)</code><code class="p">[</code><code class="mi">1</code><code class="p">]</code><code> </code><a class="co" href="#callout_working_with_file_based_and_feed_based_data_in_python_CO11-4" id="co_working_with_file_based_and_feed_based_data_in_python_CO11-4"><img alt="4" src="assets/4.png"/></a><code>&#13;
</code><code>&#13;
</code><code>    </code><code class="c1"># now! create a new, writable file, also in our target folder, that</code><code>&#13;
</code><code>    </code><code class="c1"># has the same name as the image, but is a `.txt` file</code><code>&#13;
</code><code>    </code><code class="n">output_file</code><code> </code><code class="o">=</code><code> </code><code class="nb">open</code><code class="p">(</code><code class="n">os</code><code class="o">.</code><code class="n">path</code><code class="o">.</code><code class="n">join</code><code class="p">(</code><code class="n">pdf_name</code><code class="p">,</code><code class="n">text_filename</code><code class="o">+</code><code class="s2">"</code><code class="s2">.txt</code><code class="s2">"</code><code class="p">)</code><code class="p">,</code><code> </code><code class="s2">"</code><code class="s2">w</code><code class="s2">"</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code>    </code><code class="c1"># use the `cv2` library to interpret our image</code><code>&#13;
</code><code>    </code><code class="n">img</code><code> </code><code class="o">=</code><code> </code><code class="n">cv2</code><code class="o">.</code><code class="n">imread</code><code class="p">(</code><code class="n">img_file</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code>    </code><code class="c1"># create a new variable to hold the results of using pytesseract's</code><code>&#13;
</code><code>    </code><code class="c1"># `image_to_string()` function, which will do just that</code><code>&#13;
</code><code>    </code><code class="n">converted_text</code><code> </code><code class="o">=</code><code> </code><code class="n">pytesseract</code><code class="o">.</code><code class="n">image_to_string</code><code class="p">(</code><code class="n">img</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code>    </code><code class="c1"># write our extracted text to our output file</code><code>&#13;
</code><code>    </code><code class="n">output_file</code><code class="o">.</code><code class="n">write</code><code class="p">(</code><code class="n">converted_text</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code>    </code><code class="c1"># close the output file</code><code>&#13;
</code><code>    </code><code class="n">output_file</code><code class="o">.</code><code class="n">close</code><code class="p">(</code><code class="p">)</code></pre></div>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_working_with_file_based_and_feed_based_data_in_python_CO11-1" id="callout_working_with_file_based_and_feed_based_data_in_python_CO11-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>Here we pass the path to our source file (in this case, that is just the filename, because it is in the same folder as our script) and the desired dots per inch (DPI) resolution of the output images to the <code>convert_from_path()</code> function. While setting a lower DPI will be much faster, the poorer-quality images may yield significantly less accurate OCR results. 300 DPI is a standard “print” quality resolution.</p></dd>&#13;
<dt><a class="co" href="#co_working_with_file_based_and_feed_based_data_in_python_CO11-2" id="callout_working_with_file_based_and_feed_based_data_in_python_CO11-2"><img alt="2" src="assets/2.png"/></a></dt>&#13;
<dd><p>Here we use the <em>os</em> library’s <code>.join</code> function to save the new files into our target folder. We also have to use the <code>str()</code> function to convert the page number into a string for use in the filename.</p></dd>&#13;
<dt><a class="co" href="#co_working_with_file_based_and_feed_based_data_in_python_CO11-3" id="callout_working_with_file_based_and_feed_based_data_in_python_CO11-3"><img alt="3" src="assets/3.png"/></a></dt>&#13;
<dd><p>Note that <code>*.png</code> can be translated to “any file ending in .png.” The <code>glob()</code> function creates a list of all the filenames in the folder where our images are stored (which in this case has the value <code>pdf_name</code>).</p></dd>&#13;
<dt><a class="co" href="#co_working_with_file_based_and_feed_based_data_in_python_CO11-4" id="callout_working_with_file_based_and_feed_based_data_in_python_CO11-4"><img alt="4" src="assets/4.png"/></a></dt>&#13;
<dd><p>String manipulation is fiddly! To generate unique (but matching!) filenames for our OCR’d text files, we need to pull a unique page name out of <code>img_file</code> whose value starts with <code>SafetyNet/</code> and ends in <code>.png</code>. So we’ll replace the forward slash with a period to get something like <code>SafetyNet.p1.png</code>, and then if we <code>split()</code> <em>that</em> on the period, we’ll get a list like: <code>["SafetyNet", "p1", "png"]</code>. Finally, we can access the “page name” at position 1. We need to do all this because we can’t be sure that <code>glob()</code> will, for example, pull <code>p1.png</code> from the image folder <em>first</em>, or that it will pull the images in order at all.</p></dd>&#13;
</dl>&#13;
&#13;
<p>For the most part, running this script serves our purposes: with a few dozen lines of code, it converts a multipage PDF file first into images and then writes (most of) the contents to a series of new text files.</p>&#13;
&#13;
<p>This all-in-one approach also has its limitations, however. Converting a PDF to images—or images to text—is the kind of task that we might need to do quite often but not always at the same time. In other words, it would probably be much more useful in the long run to have two <em>separate</em> scripts for solving this problem, and then to run them one after the other. In fact, with a little bit of tweaking, we could probably break up the preceding script in such a way that we could convert <em>any</em> PDF to images or <em>any</em> images to text without having to write <em>any</em> new code at all. Sounds pretty nifty, right?</p>&#13;
&#13;
<p>This process of rethinking and reorganizing working code is known<a data-primary="refactoring code" data-type="indexterm" id="idm45143408910768"/><a data-primary="code refactoring" data-see="refactoring code" data-type="indexterm" id="idm45143408910176"/> as <em>code refactoring</em>. In writing English, we would describe this as revising or editing, and the objective in both cases is the same: to make your work simpler, clearer, and more effective. And just like documentation, refactoring is actually another important way to scale your data wrangling work, because it makes <em>reusing</em> your code much more straightforward. We’ll look at various strategies for code refactoring and script reuse<a data-primary="Python" data-secondary="reading data" data-startref="python-read-pdf" data-tertiary="from .pdf files" data-tertiary-sortas="pdf files" data-type="indexterm" id="idm45143408907824"/><a data-primary="reading data" data-secondary="from .pdf files" data-secondary-sortas="pdf files" data-startref="read-data-pdf-python" data-tertiary="with Python" data-type="indexterm" id="idm45143408906032"/><a data-primary="PDFs (Portable Document Format)" data-secondary="reading data from" data-startref="pdf-read-python" data-tertiary="with Python" data-type="indexterm" id="idm45143408904272"/> in <a data-type="xref" href="ch08.html#chapter8">Chapter 8</a>.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Accessing PDF Tables with Tabula" data-type="sect2"><div class="sect2" id="idm45143409725360">&#13;
<h2>Accessing PDF Tables with Tabula</h2>&#13;
&#13;
<p>If <a data-primary="Tabula" data-type="indexterm" id="idm45143408900784"/><a data-primary="reading data" data-secondary="from .pdf files" data-secondary-sortas="pdf files" data-tertiary="with Tabula" data-type="indexterm" id="idm45143408900048"/><a data-primary="PDFs (Portable Document Format)" data-secondary="reading data from" data-tertiary="with Tabula" data-type="indexterm" id="idm45143408898560"/><a data-primary="table-type data" data-secondary="accessing in PDFs" data-type="indexterm" id="idm45143408897328"/>you looked at the text files produced in the preceding section, you’ll likely have noticed that there are a lot of “extras” in those files: page numbers and headers, line breaks, and other <a href="https://en.wikipedia.org/wiki/Cruft">“cruft.”</a> There are also some key elements missing, like images and tables.</p>&#13;
&#13;
<p>While our data work won’t extend to analyzing images (that is a much more specialized area), it’s not unusual to find tables inside PDFs that hold data we might want to work with. In fact, this problem is common enough in my home field of journalism that a group of investigative journalists designed and built a tool called <a href="https://tabula.technology">Tabula</a> specifically to deal with this problem.</p>&#13;
&#13;
<p>Tabula isn’t a Python library—it’s actually a standalone piece of software. To try it out, <a href="https://tabula.technology/#download-install">download the installer</a> for your system; if you’re on a Chromebook or Linux machine, you’ll need to download the <em>.zip</em> file and follow the directions in <em>README.txt</em>. Whatever system you’re using, you’ll probably need to install the Java programming library first, which you can do by running the following command in a terminal window:</p>&#13;
<pre>sudo apt install default-jre</pre>&#13;
&#13;
<p>Like some of the other open source tools we’ll discuss in later chapters (like OpenRefine, which I used to prepare some of the sample data in <a data-type="xref" href="ch02.html#chapter2">Chapter 2</a> and cover briefly in <a data-type="xref" href="ch11.html#chapter11">Chapter 11</a>), Tabula does its work behind the scenes (though some of it is visible in the terminal window), and you interact with it in a web browser. This is a way to get access to a more traditional graphical interface while still leaving most of your computer’s resources free to do the heavy-duty <a data-primary="data wrangling" data-secondary="reading data" data-startref="data-wrangling-read-pdf" data-tertiary="from .pdf files" data-tertiary-sortas="pdf files" data-type="indexterm" id="idm45143408888576"/><a data-primary="data sources" data-secondary="PDFs" data-startref="data-sources-unstructured-pdf" data-type="indexterm" id="idm45143408886816"/><a data-primary="unstructured data sources" data-secondary="PDFs" data-startref="unstructured-data-pdf" data-type="indexterm" id="idm45143408885536"/>data work.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section class="pagebreak-before less_space" data-pdf-bookmark="Conclusion" data-type="sect1"><div class="sect1" id="idm45143422925040">&#13;
<h1>Conclusion</h1>&#13;
&#13;
<p>Hopefully, the coding examples in this chapter have started to give you an idea of the wide variety of data wrangling problems you can solve with relatively little Python code, thanks to a combination of carefully selected libraries and those few essential Python scripting concepts that were introduced in <a data-type="xref" href="ch02.html#chapter2">Chapter 2</a>.</p>&#13;
&#13;
<p>You may also have noticed that, with the exception of our PDF text, the output of <em>all</em> the exercises in this chapter was essentially a <em>.csv</em> file. This is not by accident. Not only are <em>.csv</em> files efficient and versatile, but it turns out that to do almost any basic statistical analyses or visualizations, we need table-type data to work with. That’s not to say that it isn’t <em>possible</em> to analyze nontable data; that, in fact, is what much of contemporary computer science research (like machine learning) is all about. However, because those systems are often both complex and opaque, they’re not really suitable for the type of data wrangling work we’re focused on here.  As such, we’ll spend our energy on the types of analyses that can help us understand, explain, and communicate new insights about the world.</p>&#13;
&#13;
<p>Finally, while our work in this chapter focused on file-based data and pre-saved versions of feed-based data, in <a data-type="xref" href="ch05.html#chapter5">Chapter 5</a> we’ll explore how we can use Python with APIs and web scraping to wrangle data out of online systems and, where necessary, right off of web pages themselves!</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<div data-type="footnotes"><p data-type="footnote" id="idm45143426969488"><sup><a href="ch04.html#idm45143426969488-marker">1</a></sup> Contrast these with some others you might know, like <em>mp4</em> or <em>png</em>, which are usually associated with music and images, respectively.</p><p data-type="footnote" id="idm45143426940336"><sup><a href="ch04.html#idm45143426940336-marker">2</a></sup> Though you’ll know how to wrangle it shortly!</p><p data-type="footnote" id="idm45143423524592"><sup><a href="ch04.html#idm45143423524592-marker">3</a></sup> Actually, you won’t need that much luck—we’ll look at how to do this in <a data-type="xref" href="#wrangling_pdfs">“Wrangling PDFs with Python”</a>.</p><p data-type="footnote" id="idm45143423518064"><sup><a href="ch04.html#idm45143423518064-marker">4</a></sup> In computer science, the terms <em>data</em> and <em>information</em> are applied in exactly the opposite way: <em>data</em> is the raw facts collected about the world, and <em>information</em> is the meaningful end product of organizing and structuring it. In recent years, however, as talk about “big data” has dominated many fields, the interpretation I use here has become more common, so I’ll stick with it throughout this book for clarity.</p><p data-type="footnote" id="idm45143422908112"><sup><a href="ch04.html#idm45143422908112-marker">5</a></sup> From the <a href="https://bls.gov/news.release/empsit.t15.htm">US Bureau of Labor Statistics</a>.</p><p data-type="footnote" id="idm45143422903792"><sup><a href="ch04.html#idm45143422903792-marker">6</a></sup> “Multiple Jobholders” by Stéphane Auray, David L. Fuller, and Guillaume Vandenbroucke, posted December 21, 2018, <a href="https://research.stlouisfed.org/publications/economic-synopses/2018/12/21/multiple-jobholders"><em class="hyperlink">https://research.stlouisfed.org/publications/economic-synopses/2018/12/21/multiple-jobholders</em></a>.</p><p data-type="footnote" id="idm45143422901552"><sup><a href="ch04.html#idm45143422901552-marker">7</a></sup> See “New Recommendations on Improving Data on Contingent and Alternative Work Arrangements,” <a href="https://blogs.bls.gov/blog/tag/contingent-workers"><em class="hyperlink">https://blogs.bls.gov/blog/tag/contingent-workers</em></a>; “The Value of Flexible Work: Evidence from Uber Drivers” by M. Keith Chen et al., <em>Nber Working Paper Series</em> No. 23296, <a href="https://nber.org/system/files/working_papers/w23296/w23296.pdf"><em class="hyperlink">https://nber.org/system/files/working_papers/w23296/w23296.pdf</em></a>.</p><p data-type="footnote" id="idm45143422892896"><sup><a href="ch04.html#idm45143422892896-marker">8</a></sup> You can also find instructions for this on the <a href="https://fredhelp.stlouisfed.org/fred/graphs/customize-a-fred-graph/data-transformation-add-series-to-existing-line">FRED website</a>.</p><p data-type="footnote" id="idm45143422886480"><sup><a href="ch04.html#idm45143422886480-marker">9</a></sup> You can also download copies of these files <a href="https://drive.google.com/drive/u/0/folders/1cU5Tdg_fvrCcwvAAyhMOhpbEcI2fF7sb">directly from my Google Drive</a>.</p><p data-type="footnote" id="idm45143422842640"><sup><a href="ch04.html#idm45143422842640-marker">10</a></sup> As of this writing, LibreOffice can handle the same number of rows as Microsoft Excel (2<sup>20</sup>), but far fewer columns. While Google Sheets can handle <em>more</em> columns than Excel, it can only handle about 40,000 rows.</p><p data-type="footnote" id="idm45143422837216"><sup><a href="ch04.html#idm45143422837216-marker">11</a></sup> As of this writing, all of these libraries are already available and ready to use in Google Colab.</p><p data-type="footnote" id="idm45143422318528"><sup><a href="ch04.html#idm45143422318528-marker">12</a></sup> For more about <code>get_rows()</code>, see the <a href="https://xlrd.readthedocs.io/en/latest/api.html#xlrd-sheet"><em>xlrd</em> documentation</a>.</p><p data-type="footnote" id="idm45143422098736"><sup><a href="ch04.html#idm45143422098736-marker">13</a></sup> See the <a href="https://xlrd.readthedocs.io/en/latest/dates.html"><em>xlrd</em> documentation</a> for more on this issue.</p><p data-type="footnote" id="idm45143422094400"><sup><a href="ch04.html#idm45143422094400-marker">14</a></sup> From Stephen John Machin and Chris Withers, <a href="https://xlrd.readthedocs.io/en/latest/dates.html">“Dates in Excel Spreadsheets”</a>.</p><p data-type="footnote" id="idm45143422084224"><sup><a href="ch04.html#idm45143422084224-marker">15</a></sup> If you open the output files from the three preceding code examples in a text editor, you’ll notice that the open source <em>.ods</em> format is the simplest and cleanest.</p><p data-type="footnote" id="idm45143422210096"><sup><a href="ch04.html#idm45143422210096-marker">16</a></sup> As in, for example, <a href="https://spotlightpa.org/news/2021/05/pa-unemployment-claims-overhaul-ibm-gsi-benefits-labor-industry">Pennsylvania</a> or <a href="https://denverpost.com/2021/01/10/colorado-unemployment-benefits-new-claims-system">Colorado</a>.</p><p data-type="footnote" id="idm45143421937168"><sup><a href="ch04.html#idm45143421937168-marker">17</a></sup> See Vassili van der Mersch’s post, <a href="https://nordicapis.com/twitter-10-year-struggle-with-developer-relations">“Twitter’s 10 Year Struggle with Developer Relations” from Nordic APIs</a>.</p><p data-type="footnote" id="idm45143413262208"><sup><a href="ch04.html#idm45143413262208-marker">18</a></sup> Unlike Python code, XML documents do <em>not</em> have to be properly indented in order to work, though it certainly makes them more readable!</p><p data-type="footnote" id="idm45143410254992"><sup><a href="ch04.html#idm45143410254992-marker">19</a></sup> Fun fact: the second <em>x</em> in the <em>.xlsx</em> format actually refers to XML!</p><p data-type="footnote" id="idm45143410249984"><sup><a href="ch04.html#idm45143410249984-marker">20</a></sup> Again, we’ll walk through using APIs like this one step by step in <a data-type="xref" href="ch05.html#chapter5">Chapter 5</a>, but using this document lets us see how different data formats influence our interactions with the data.</p><p data-type="footnote" id="idm45143409767664"><sup><a href="ch04.html#idm45143409767664-marker">21</a></sup> If a stylesheet has been applied, as in the case of the BBC feed we used in <a data-type="xref" href="#bbc_example">Example 4-13</a>, you can context+click the page and select “View Source” to see the “raw” XML.</p><p data-type="footnote" id="idm45143409744752"><sup><a href="ch04.html#idm45143409744752-marker">22</a></sup> See <a href="https://acrobat.adobe.com/us/en/acrobat/about-adobe-pdf.html">Adobe’s About PDF page for more information</a>.</p><p data-type="footnote" id="idm45143409740288"><sup><a href="ch04.html#idm45143409740288-marker">23</a></sup> George Nagy, “Disruptive Developments in Document Recognition,” <em>Pattern Recognition Letters</em> 79 (2016): 106–112, <a href="https://doi.org/10.1016/j.patrec.2015.11.024"><em class="hyperlink">https://doi.org/10.1016/j.patrec.2015.11.024</em></a>. Available at <a href="https://ecse.rpi.edu/~nagy/PDF_chrono/2016_PRL_Disruptive_asPublished.pdf"><em class="hyperlink">https://ecse.rpi.edu/~nagy/PDF_chrono/2016_PRL_Disruptive_asPublished.pdf</em></a>.</p></div></div></section></body></html>