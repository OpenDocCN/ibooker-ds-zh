- en: 'Chapter 45\. In Depth: Principal Component Analysis'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Up until now, we have been looking in depth at supervised learning estimators:
    those estimators that predict labels based on labeled training data. Here we begin
    looking at several unsupervised estimators, which can highlight interesting aspects
    of the data without reference to any known labels.'
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter we will explore what is perhaps one of the most broadly used
    unsupervised algorithms, principal component analysis (PCA). PCA is fundamentally
    a dimensionality reduction algorithm, but it can also be useful as a tool for
    visualization, noise filtering, feature extraction and engineering, and much more.
    After a brief conceptual discussion of the PCA algorithm, we will explore a couple
    examples of these further applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'We begin with the standard imports:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Introducing Principal Component Analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Principal component analysis is a fast and flexible unsupervised method for
    dimensionality reduction in data, which we saw briefly in [Chapter 38](ch38.xhtml#section-0502-introducing-scikit-learn).
    Its behavior is easiest to visualize by looking at a two-dimensional dataset.
    Consider these 200 points (see [Figure 45-1](#fig_0509-principal-component-analysis_files_in_output_4_0)).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![output 4 0](assets/output_4_0.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 45-1\. Data for demonstration of PCA
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'By eye, it is clear that there is a nearly linear relationship between the
    *x* and *y* variables. This is reminiscent of the linear regression data we explored
    in [Chapter 42](ch42.xhtml#section-0506-linear-regression), but the problem setting
    here is slightly different: rather than attempting to *predict* the *y* values
    from the *x* values, the unsupervised learning problem attempts to learn about
    the *relationship* between the *x* and *y* values.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In principal component analysis, this relationship is quantified by finding
    a list of the *principal axes* in the data, and using those axes to describe the
    dataset. Using Scikit-Learn’s `PCA` estimator, we can compute this as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The fit learns some quantities from the data, most importantly the components
    and explained variance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: To see what these numbers mean, let’s visualize them as vectors over the input
    data, using the components to define the direction of the vector and the explained
    variance to define the squared length of the vector (see [Figure 45-2](#fig_0509-principal-component-analysis_files_in_output_11_0)).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![output 11 0](assets/output_11_0.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 45-2\. Visualization of the principal axes in the data
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: These vectors represent the principal axes of the data, and the length of each
    vector is an indication of how “important” that axis is in describing the distribution
    of the data—more precisely, it is a measure of the variance of the data when projected
    onto that axis. The projection of each data point onto the principal axes are
    the principal components of the data.
  prefs: []
  type: TYPE_NORMAL
- en: If we plot these principal components beside the original data, we see the plots
    shown in [Figure 45-3](#fig_images_in_0509-pca-rotation).
  prefs: []
  type: TYPE_NORMAL
- en: '![05.09 PCA rotation](assets/05.09-PCA-rotation.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 45-3\. Transformed principal axes in the data^([1](ch45.xhtml#idm45858728565136))
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This transformation from data axes to principal axes is an *affine transformation*,
    which means it is composed of a translation, rotation, and uniform scaling.
  prefs: []
  type: TYPE_NORMAL
- en: While this algorithm to find principal components may seem like just a mathematical
    curiosity, it turns out to have very far-reaching applications in the world of
    machine learning and data exploration.
  prefs: []
  type: TYPE_NORMAL
- en: PCA as Dimensionality Reduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Using PCA for dimensionality reduction involves zeroing out one or more of the
    smallest principal components, resulting in a lower-dimensional projection of
    the data that preserves the maximal data variance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example of using PCA as a dimensionality reduction transform:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The transformed data has been reduced to a single dimension. To understand the
    effect of this dimensionality reduction, we can perform the inverse transform
    of this reduced data and plot it along with the original data (see [Figure 45-4](#fig_0509-principal-component-analysis_files_in_output_18_0)).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![output 18 0](assets/output_18_0.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 45-4\. Visualization of PCA as dimensionality reduction
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The light points are the original data, while the dark points are the projected
    version. This makes clear what a PCA dimensionality reduction means: the information
    along the least important principal axis or axes is removed, leaving only the
    component(s) of the data with the highest variance. The fraction of variance that
    is cut out (proportional to the spread of points about the line formed in the
    preceding figure) is roughly a measure of how much “information” is discarded
    in this reduction of dimensionality.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This reduced-dimension dataset is in some senses “good enough” to encode the
    most important relationships between the points: despite reducing the number of
    data features by 50%, the overall relationships between the data points are mostly
    preserved.'
  prefs: []
  type: TYPE_NORMAL
- en: 'PCA for Visualization: Handwritten Digits'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The usefulness of dimensionality reduction may not be entirely apparent in only
    two dimensions, but it becomes clear when looking at high-dimensional data. To
    see this, let’s take a quick look at the application of PCA to the digits dataset
    we worked with in [Chapter 44](ch44.xhtml#section-0508-random-forests).
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll start by loading the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Recall that the digits dataset consists of 8 × 8–pixel images, meaning that
    they are 64-dimensional. To gain some intuition into the relationships between
    these points, we can use PCA to project them into a more manageable number of
    dimensions, say two:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: We can now plot the first two principal components of each point to learn about
    the data, as seen in [Figure 45-5](#fig_0509-principal-component-analysis_files_in_output_25_0).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '![output 25 0](assets/output_25_0.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 45-5\. PCA applied to the handwritten digits data
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Recall what these components mean: the full data is a 64-dimensional point
    cloud, and these points are the projection of each data point along the directions
    with the largest variance. Essentially, we have found the optimal stretch and
    rotation in 64-dimensional space that allows us to see the layout of the data
    in two dimensions, and we have done this in an unsupervised manner—that is, without
    reference to the labels.'
  prefs: []
  type: TYPE_NORMAL
- en: What Do the Components Mean?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can go a bit further here, and begin to ask what the reduced dimensions
    *mean*. This meaning can be understood in terms of combinations of basis vectors.
    For example, each image in the training set is defined by a collection of 64 pixel
    values, which we will call the vector <math alttext="x"><mi>x</mi></math> :'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="x equals left-bracket x 1 comma x 2 comma x 3 ellipsis x 64 right-bracket"
    display="block"><mrow><mi>x</mi> <mo>=</mo> <mo>[</mo> <msub><mi>x</mi> <mn>1</mn></msub>
    <mo>,</mo> <msub><mi>x</mi> <mn>2</mn></msub> <mo>,</mo> <msub><mi>x</mi> <mn>3</mn></msub>
    <mo>⋯</mo> <msub><mi>x</mi> <mn>64</mn></msub> <mo>]</mo></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'One way we can think about this is in terms of a pixel basis. That is, to construct
    the image, we multiply each element of the vector by the pixel it describes, and
    then add the results together to build the image:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="normal i normal m normal a normal g normal e left-parenthesis
    x right-parenthesis equals x 1 dot left-parenthesis normal p normal i normal x
    normal e normal l 1 right-parenthesis plus x 2 dot left-parenthesis normal p normal
    i normal x normal e normal l 2 right-parenthesis plus x 3 dot left-parenthesis
    normal p normal i normal x normal e normal l 3 right-parenthesis ellipsis x 64
    dot left-parenthesis normal p normal i normal x normal e normal l 64 right-parenthesis"
    display="block"><mrow><mi>image</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow>
    <mo>=</mo> <msub><mi>x</mi> <mn>1</mn></msub> <mo>·</mo> <mrow><mo>(</mo> <mi>pixel</mi>
    <mn>1</mn> <mo>)</mo></mrow> <mo>+</mo> <msub><mi>x</mi> <mn>2</mn></msub> <mo>·</mo>
    <mrow><mo>(</mo> <mi>pixel</mi> <mn>2</mn> <mo>)</mo></mrow> <mo>+</mo> <msub><mi>x</mi>
    <mn>3</mn></msub> <mo>·</mo> <mrow><mo>(</mo> <mi>pixel</mi> <mn>3</mn> <mo>)</mo></mrow>
    <mo>⋯</mo> <msub><mi>x</mi> <mn>64</mn></msub> <mo>·</mo> <mrow><mo>(</mo> <mi>pixel</mi>
    <mn>64</mn> <mo>)</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'One way we might imagine reducing the dimensionality of this data is to zero
    out all but a few of these basis vectors. For example, if we use only the first
    eight pixels, we get an eight-dimensional projection of the data ([Figure 45-6](#fig_images_in_0509-digits-pixel-components)).
    However, it is not very reflective of the whole image: we’ve thrown out nearly
    90% of the pixels!'
  prefs: []
  type: TYPE_NORMAL
- en: '![05.09 digits pixel components](assets/05.09-digits-pixel-components.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 45-6\. A naive dimensionality reduction achieved by discarding pixels^([2](ch45.xhtml#idm45858728053776))
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The upper row of panels shows the individual pixels, and the lower row shows
    the cumulative contribution of these pixels to the construction of the image.
    Using only eight of the pixel-basis components, we can only construct a small
    portion of the 64-pixel image. Were we to continue this sequence and use all 64
    pixels, we would recover the original image.
  prefs: []
  type: TYPE_NORMAL
- en: 'But the pixel-wise representation is not the only choice of basis. We can also
    use other basis functions, which each contain some predefined contribution from
    each pixel, and write something like:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="i m a g e left-parenthesis x right-parenthesis equals normal
    m normal e normal a normal n plus x 1 dot left-parenthesis normal b normal a normal
    s normal i normal s 1 right-parenthesis plus x 2 dot left-parenthesis normal b
    normal a normal s normal i normal s 2 right-parenthesis plus x 3 dot left-parenthesis
    normal b normal a normal s normal i normal s 3 right-parenthesis ellipsis" display="block"><mrow><mi>i</mi>
    <mi>m</mi> <mi>a</mi> <mi>g</mi> <mi>e</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow>
    <mo>=</mo> <mi>mean</mi> <mo>+</mo> <msub><mi>x</mi> <mn>1</mn></msub> <mo>·</mo>
    <mrow><mo>(</mo> <mi>basis</mi> <mn>1</mn> <mo>)</mo></mrow> <mo>+</mo> <msub><mi>x</mi>
    <mn>2</mn></msub> <mo>·</mo> <mrow><mo>(</mo> <mi>basis</mi> <mn>2</mn> <mo>)</mo></mrow>
    <mo>+</mo> <msub><mi>x</mi> <mn>3</mn></msub> <mo>·</mo> <mrow><mo>(</mo> <mi>basis</mi>
    <mn>3</mn> <mo>)</mo></mrow> <mo>⋯</mo></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: PCA can be thought of as a process of choosing optimal basis functions, such
    that adding together just the first few of them is enough to suitably reconstruct
    the bulk of the elements in the dataset. The principal components, which act as
    the low-dimensional representation of our data, are simply the coefficients that
    multiply each of the elements in this series. [Figure 45-7](#fig_images_in_0509-digits-pca-components)
    shows a similar depiction of reconstructing the same digit using the mean plus
    the first eight PCA basis functions.
  prefs: []
  type: TYPE_NORMAL
- en: '![05.09 digits pca components](assets/05.09-digits-pca-components.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 45-7\. A more sophisticated dimensionality reduction achieved by discarding
    the least important principal components (compare to [Figure 45-6](#fig_images_in_0509-digits-pixel-components))^([3](ch45.xhtml#idm45858728025120))
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Unlike the pixel basis, the PCA basis allows us to recover the salient features
    of the input image with just a mean, plus eight components! The amount of each
    pixel in each component is the corollary of the orientation of the vector in our
    two-dimensional example. This is the sense in which PCA provides a low-dimensional
    representation of the data: it discovers a set of basis functions that are more
    efficient than the native pixel basis of the input data.'
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the Number of Components
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A vital part of using PCA in practice is the ability to estimate how many components
    are needed to describe the data. This can be determined by looking at the cumulative
    *explained variance ratio* as a function of the number of components (see [Figure 45-8](#fig_0509-principal-component-analysis_files_in_output_34_0)).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: This curve quantifies how much of the total, 64-dimensional variance is contained
    within the first <math alttext="upper N"><mi>N</mi></math> components. For example,
    we see that with the digits data the first 10 components contain approximately
    75% of the variance, while you need around 50 components to describe close to
    100% of the variance.
  prefs: []
  type: TYPE_NORMAL
- en: '![output 34 0](assets/output_34_0.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 45-8\. The cumulative explained variance, which measures how well PCA
    pre‐ serves the content of the data
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This tells us that our two-dimensional projection loses a lot of information
    (as measured by the explained variance) and that we’d need about 20 components
    to retain 90% of the variance. Looking at this plot for a high-dimensional dataset
    can help you understand the level of redundancy present in its features.
  prefs: []
  type: TYPE_NORMAL
- en: PCA as Noise Filtering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'PCA can also be used as a filtering approach for noisy data. The idea is this:
    any components with variance much larger than the effect of the noise should be
    relatively unaffected by the noise. So, if you reconstruct the data using just
    the largest subset of principal components, you should be preferentially keeping
    the signal and throwing out the noise.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see how this looks with the digits data. First we will plot several of
    the input noise-free input samples ([Figure 45-9](#fig_0509-principal-component-analysis_files_in_output_37_0)).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '![output 37 0](assets/output_37_0.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 45-9\. Digits without noise
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Now let’s add some random noise to create a noisy dataset, and replot it ([Figure 45-10](#fig_0509-principal-component-analysis_files_in_output_40_0)).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '![output 40 0](assets/output_40_0.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 45-10\. Digits with Gaussian random noise added
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The visualization makes the presence of this random noise clear. Let’s train
    a PCA model on the noisy data, requesting that the projection preserve 50% of
    the variance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Here 50% of the variance amounts to 12 principal components, out of the 64 original
    features. Now we compute these components, and then use the inverse of the transform
    to reconstruct the filtered digits; [Figure 45-11](#fig_0509-principal-component-analysis_files_in_output_44_0)
    shows the result.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '![output 44 0](assets/output_44_0.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 45-11\. Digits “denoised” using PCA
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This signal preserving/noise filtering property makes PCA a very useful feature
    selection routine—for example, rather than training a classifier on very high-dimensional
    data, you might instead train the classifier on the lower-dimensional principal
    component representation, which will automatically serve to filter out random
    noise in the inputs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: Eigenfaces'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Earlier we explored an example of using a PCA projection as a feature selector
    for facial recognition with a support vector machine (see [Chapter 43](ch43.xhtml#section-0507-support-vector-machines)).
    Here we will take a look back and explore a bit more of what went into that. Recall
    that we were using the Labeled Faces in the Wild (LFW) dataset made available
    through Scikit-Learn:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s take a look at the principal axes that span this dataset. Because this
    is a large dataset, we will use the `"random"` eigensolver in the `PCA` estimator:
    it uses a randomized method to approximate the first <math alttext="upper N"><mi>N</mi></math>
    principal components more quickly than the standard approach, at the expense of
    some accuracy. This trade-off can be useful for high-dimensional data (here, a
    dimensionality of nearly 3,000). We will take a look at the first 150 components:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'In this case, it can be interesting to visualize the images associated with
    the first several principal components (these components are technically known
    as *eigenvectors*, so these types of images are often called *eigenfaces*; as
    you can see in [Figure 45-12](#fig_0509-principal-component-analysis_files_in_output_51_0),
    they are as creepy as they sound):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '![output 51 0](assets/output_51_0.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 45-12\. A visualization of eigenfaces learned from the LFW dataset
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The results are very interesting, and give us insight into how the images vary:
    for example, the first few eigenfaces (from the top left) seem to be associated
    with the angle of lighting on the face, and later principal vectors seem to be
    picking out certain features, such as eyes, noses, and lips. Let’s take a look
    at the cumulative variance of these components to see how much of the data information
    the projection is preserving (see [Figure 45-13](#fig_0509-principal-component-analysis_files_in_output_53_0)).'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '![output 53 0](assets/output_53_0.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 45-13\. Cumulative explained variance for the LFW data
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The 150 components we have chosen account for just over 90% of the variance.
    That would lead us to believe that using these 150 components, we would recover
    most of the essential characteristics of the data. To make this more concrete,
    we can compare the input images with the images reconstructed from these 150 components
    (see [Figure 45-14](#fig_0509-principal-component-analysis_files_in_output_56_0)).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '![output 56 0](assets/output_56_0.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 45-14\. 150-dimensional PCA reconstruction of the LFW data
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The top row here shows the input images, while the bottom row shows the reconstruction
    of the images from just 150 of the ~3,000 initial features. This visualization
    makes clear why the PCA feature selection used in [Chapter 43](ch43.xhtml#section-0507-support-vector-machines)
    was so successful: although it reduces the dimensionality of the data by nearly
    a factor of 20, the projected images contain enough information that we might,
    by eye, recognize the individuals in each image. This means our classification
    algorithm only needs to be trained on 150-dimensional data rather than 3,000-dimensional
    data, which, depending on the particular algorithm we choose, can lead to much
    more efficient classification.'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter we explored the use of principal component analysis for dimensionality
    reduction, visualization of high-dimensional data, noise filtering, and feature
    selection within high-dimensional data. Because of its versatility and interpretability,
    PCA has been shown to be effective in a wide variety of contexts and disciplines.
    Given any high-dimensional dataset, I tend to start with PCA in order to visualize
    the relationships between points (as we did with the digits data), to understand
    the main variance in the data (as we did with the eigenfaces), and to understand
    the intrinsic dimensionality (by plotting the explained variance ratio). Certainly
    PCA is not useful for every high-dimensional dataset, but it offers a straightforward
    and efficient path to gaining insight into high-dimensional data.
  prefs: []
  type: TYPE_NORMAL
- en: PCA’s main weakness is that it tends to be highly affected by outliers in the
    data. For this reason, several robust variants of PCA have been developed, many
    of which act to iteratively discard data points that are poorly described by the
    initial components. Scikit-Learn includes a number of interesting variants on
    PCA in the `sklearn​.decom⁠position` submodule; one example is `SparsePCA`, which
    introduces a regularization term (see [Chapter 42](ch42.xhtml#section-0506-linear-regression))
    that serves to enforce sparsity of the components.
  prefs: []
  type: TYPE_NORMAL
- en: In the following chapters, we will look at other unsupervised learning methods
    that build on some of the ideas of PCA.
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch45.xhtml#idm45858728565136-marker)) Code to produce this figure can
    be found in the [online appendix](https://oreil.ly/VmpjC).
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch45.xhtml#idm45858728053776-marker)) Code to produce this figure can
    be found in the [online appendix](https://oreil.ly/ixfc1).
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](ch45.xhtml#idm45858728025120-marker)) Code to produce this figure can
    be found in the [online appendix](https://oreil.ly/WSe0T).
  prefs: []
  type: TYPE_NORMAL
