- en: Chapter 3\. Scraping Websites and Extracting Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Often, it will happen that you visit a website and find the content interesting.
    If there are only a few pages, it’s possible to read everything on your own. But
    as soon as there is a considerable amount of content, reading everything on your
    own will not be possible.
  prefs: []
  type: TYPE_NORMAL
- en: To use the powerful text analytics blueprints described in this book, you have
    to acquire the content first. Most websites won’t have a “download all content”
    button, so we have to find a clever way to download (“scrape”) the pages.
  prefs: []
  type: TYPE_NORMAL
- en: Usually we are mainly interested in the content part of each individual web
    page, less so in navigation, etc. As soon as we have the data locally available,
    we can use powerful extraction techniques to dissect the pages into elements such
    as title, content, and also some meta-information (publication date, author, and
    so on).
  prefs: []
  type: TYPE_NORMAL
- en: What You’ll Learn and What We’ll Build
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will show you how to acquire HTML data from websites and
    use powerful tools to extract the content from these HTML files. We will show
    this with content from one specific data source, the Reuters news archive.
  prefs: []
  type: TYPE_NORMAL
- en: In the first step, we will download single HTML files and extract data from
    each one with different methods.
  prefs: []
  type: TYPE_NORMAL
- en: Normally, you will not be interested in single pages. Therefore, we will build
    a blueprint solution. We will download and analyze a news archive page (which
    contains links to all articles). After completing this, we know the URLs of the
    referred documents. Then you can download the documents at the URLs and extract
    their content to a Pandas `DataFrame`.
  prefs: []
  type: TYPE_NORMAL
- en: After studying this chapter, you will have a good overview of methods that download
    HTML and extract data. You will be familiar with the different extraction methods
    for content provided by Python. We will have seen a complete example for downloading
    and extracting data. For your own work, you will be able to select an appropriate
    framework. In this chapter, we will provide standard blueprints for extracting
    often-used elements that you can reuse.
  prefs: []
  type: TYPE_NORMAL
- en: Scraping and Data Extraction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Scraping websites is a complex process consisting of typically three different
    phases, as illustrated in [Figure 3-1](#outline_of_scraping_process).
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/btap_0301.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-1\. Outline of scraping process.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In the first step, we have to generate all interesting URLs of a website. Afterward,
    we can use different tools to download the pages from the corresponding URLs.
    Finally, we will extract the “net” data from the downloaded pages; we can also
    use different strategies in this phase. Of course, it is crucial to permanently
    save extracted data. In this chapter, we use a Pandas `DataFrame` that offers
    a variety of persistence mechanisms.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing the Reuters News Archive
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s assume we are interested in analyzing the current and past political situation
    and are looking for an appropriate dataset. We want to find some trends, uncover
    when a word or topic was introduced for the first time, and so on. For this, our
    aim is to convert the documents to a Pandas `DataFrame`.
  prefs: []
  type: TYPE_NORMAL
- en: Obviously, news headlines and articles are well suited as a database for these
    requirements. If possible, we should find an archive that goes back a few years,
    ideally even some decades.
  prefs: []
  type: TYPE_NORMAL
- en: Some newspapers have such archives, but most of them will also have a certain
    political bias that we want to avoid if possible. We are looking for content that
    is as neutral as possible.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is why we decided to use the Reuters news archive. Reuters is an international
    news organization and works as a news agency; in other words, it provides news
    to many different publications. It was founded more than a hundred years ago and
    has a lot of news articles in its archives. It’s a good source of content for
    many reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: It is politically neutral.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It has a big archive of news.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: News articles are categorized in sections.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The focus is not on a specific region.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Almost everybody will find some interesting headlines there.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It has a liberal policy for downloading data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is very well connected, and the website itself is fast.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL Generation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For downloading content from the Reuters’ archive, we need to know the URLs
    of the content pages. After we know the URLs, the download itself is easy as there
    are powerful Python tools available to accomplish that.
  prefs: []
  type: TYPE_NORMAL
- en: At first sight it might seem easy to find URLs, but in practice it is often
    not so simple. The process is called *URL generation*, and in many crawling projects
    it is one of the most difficult tasks. We have to make sure that we do not systematically
    miss URLs; therefore, thinking carefully about the process in the beginning is
    crucial. Performed correctly, URL generation can also be a tremendous time-saver.
  prefs: []
  type: TYPE_NORMAL
- en: Before You Download
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Be careful: sometimes downloading data is illegal. The rules and legal situation
    might depend on the country where the data is hosted and into which country it
    is downloaded. Often, websites have a page called “terms of use” or something
    similar that might be worth taking a look at.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If data is saved only temporarily, the same rules for search engines might
    apply. As search engines like Google cannot read and understand the terms of use
    of every single page they index, there is a really old protocol called the [robots
    exclusion standard](https://oreil.ly/IWysG). Websites using this have a file called *robots.txt* at
    the top level. This file can be downloaded and interpreted automatically. For
    single websites, it is also possible to read it manually and interpret the data.
    The rule of thumb is that if there is no `Disallow: *`, you should be allowed
    to download and (temporarily) save the content.'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many different possibilities:'
  prefs: []
  type: TYPE_NORMAL
- en: Crawling
  prefs: []
  type: TYPE_NORMAL
- en: Start on the home page (or a section) of the website and download all links
    on the same website. Crawling might take some time.
  prefs: []
  type: TYPE_NORMAL
- en: URL generators
  prefs: []
  type: TYPE_NORMAL
- en: Writing a URL generator is a slightly more sophisticated solution. This is most
    suitable for use on hierarchically organized content like forums, blogs, etc.
  prefs: []
  type: TYPE_NORMAL
- en: Search engines
  prefs: []
  type: TYPE_NORMAL
- en: Ask search engines for specific URLs and download only these specific URLs.
  prefs: []
  type: TYPE_NORMAL
- en: Sitemaps
  prefs: []
  type: TYPE_NORMAL
- en: A standard called [*sitemap.xml*](https://oreil.ly/XANO0), which was originally
    conceived for search engines, is an interesting alternative. A file called *sitemap.xml*
    contains a list of all pages on a website (or references to sub-sitemaps). Contrary
    to *robots.txt*, the filename is not fixed and can sometimes be found in *robots.txt*
    itself. The best guess is to look for *sitemap.xml* on the top level of a website.
  prefs: []
  type: TYPE_NORMAL
- en: RSS
  prefs: []
  type: TYPE_NORMAL
- en: The [RSS format](https://oreil.ly/_aOOM) was originally conceived for newsfeeds
    and is still in wide use for subscribing to frequently changing content sources.
    It works via XML files and does not only contain URLs but also document titles
    and sometimes summaries of articles.
  prefs: []
  type: TYPE_NORMAL
- en: Specialized programs
  prefs: []
  type: TYPE_NORMAL
- en: Downloading data from social networks and similar content is often simplified
    by using specialized programs that are available on GitHub (such as [Facebook
    Chat Downloader](https://oreil.ly/ThyNf) for Facebook Chats, [Instaloader](https://oreil.ly/utGsC)
    for Instagram, and so on).
  prefs: []
  type: TYPE_NORMAL
- en: In the following sections, we focus on *robots.txt*, *sitemaps.xml*, and RSS
    feeds. Later in the chapter, we show a multistage download that uses URL generators.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note: Use an API for Downloading Data If It’s Available'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Instead of generating the URLs, downloading the content, and extracting it,
    using an API is much easier and more stable. You will find more information about
    that in [Chapter 2](ch02.xhtml#ch-api).
  prefs: []
  type: TYPE_NORMAL
- en: 'Blueprint: Downloading and Interpreting robots.txt'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Finding the content on a website is often not so easy. To see the techniques
    mentioned earlier in action, we’ll take a look at the Reuters news archive. Of
    course, (almost) any other website will work in a similar fashion.
  prefs: []
  type: TYPE_NORMAL
- en: 'As discussed, [*robots.txt*](https://www.reuters.com/robots.txt) is a good
    starting point:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Some user agents are not allowed to download anything, but the rest may do
    that. We can check that programmatically in Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Blueprint: Finding URLs from sitemap.xml'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Reuters is even nice enough to mention the URLs of the [sitemap for the news](https://reuters.com/sitemap_news_index.xml),
    which actually contains only a reference to [other sitemap files](https://www.reuters.com/sitemap_news_index1.xml).
    Let’s download that. An excerpt at the time of writing looks like this:^([1](ch03.xhtml#idm45634207731272))
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The most interesting part is the line with `<loc>`, as it contains the URL of
    the article. Filtering out all these `<loc>` lines leads to a list of URLs for
    news articles that can be downloaded afterward.
  prefs: []
  type: TYPE_NORMAL
- en: As Python has an incredibly rich ecosystem of libraries, it’s not hard to find
    a sitemap parser. There are several available, such as [`ultimate-sitemap-parser`](https://oreil.ly/XgY9z).
    However, this parser downloads the whole sitemap hierarchy, which is a bit too
    sophisticated for us as we just want the URLs.
  prefs: []
  type: TYPE_NORMAL
- en: It’s easy to convert *sitemap.xml* to an associative array (hash) that is called
    a `dict` in Python:^([2](ch03.xhtml#idm45634208553592))
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s check what is in the `dict` before actually downloading the files^([3](ch03.xhtml#idm45634212104584)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: We will use this list of URLs in the following section and download their content.
  prefs: []
  type: TYPE_NORMAL
- en: 'Blueprint: Finding URLs from RSS'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As Reuters is a news website, it also offers access to its articles via an RSS
    feed. Several years ago, browsers would show an RSS icon next to the URL if you
    could subscribe to this source. While those days are gone, it is still not too
    difficult to find the URLs for RSS feeds. At the bottom of the website, we can
    see a line with navigation icons, as shown in [Figure 3-2](#part_of_the_reuters_website_which_links_to_the_rss_feed).
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/btap_0302.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-2\. Part of the Reuters website that links to the RSS feed.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The icon that looks like a WIFI indicator is the link to the RSS feeds page.
    Often (and sometimes more easily) this can be found by taking a look at the source
    code of the corresponding webpage and searching for *RSS*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The world news RSS feed has the URL [*http://feeds.reuters.com/Reuters/worldNews*](http://feeds.reuters.com/Reuters/worldNews)^([4](ch03.xhtml#idm45634206884968))
    and can easily be parsed in Python, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The individual format of the RSS file might differ from site to site. However,
    most of the time we will find title and link as fields^([5](ch03.xhtml#idm45634206875096)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'In our case, we are more interested in the “real” URLs, which are contained
    in the `id` field:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Great, we have found an alternative way to get a list of URLs that can be used
    when no *sitemap.xml* is available.
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes you will still encounter so-called [*Atom feeds*](https://oreil.ly/Jcdgi), which
    basically offer the same information as RSS in a different format.
  prefs: []
  type: TYPE_NORMAL
- en: If you wanted to implement a website monitoring tool, taking a periodic look
    at Reuters news (or other news sources) or RSS (or Atom) would be a good way to
    go ahead.
  prefs: []
  type: TYPE_NORMAL
- en: If you are interested in whole websites, looking for *sitemap.xml* is an excellent
    idea. Sometimes it might be difficult to find (hints might be in *robots.txt*),
    but it is almost always worth the extra effort to find it.
  prefs: []
  type: TYPE_NORMAL
- en: If you cannot find *sitemap.xml* and you plan to regularly download content,
    going for RSS is a good second choice.
  prefs: []
  type: TYPE_NORMAL
- en: Whenever possible, try to avoid crawling websites for URLs. The process is largely
    uncontrollable, can take a long time, and might yield incomplete results.
  prefs: []
  type: TYPE_NORMAL
- en: Downloading Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At first sight, downloading data might seem like the most difficult and time-consuming
    part of the scraping process. Often, that’s not true as you can accomplish it
    in a highly standardized way.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we show different methods for downloading data, both with Python
    libraries and external tools. Especially for big projects, using external programs
    has some advantages.
  prefs: []
  type: TYPE_NORMAL
- en: Compared to several years ago, the Internet is much faster today. Big websites
    have reacted to this development by using content-delivery networks, which can
    speed them up by orders of magnitude. This helps us a lot as the actual downloading
    process is not as slow as it used to be but is more or less limited by our own
    bandwidth.
  prefs: []
  type: TYPE_NORMAL
- en: 'Blueprint: Downloading HTML Pages with Python'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To download HTML pages, it’s necessary to know the URLs. As we have seen, the
    URLs are contained in the sitemap. Let’s use this list to download the content:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Depending on your Internet connection, it might take longer, but that was quite
    fast. Using the session abstraction, we make sure to have maximum speed by leveraging
    keep-alive, SSL session caching, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Use Proper Error Handling When Downloading URLs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When downloading URLs, you are using a network protocol to communicate with
    remote servers. There are many kinds of errors that can happen, such as changed
    URLs, servers not responding, etc. The example just shows an error message; in
    real life, your solution should probably be more sophisticated.
  prefs: []
  type: TYPE_NORMAL
- en: 'Blueprint: Downloading HTML Pages with wget'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A good tool for mass downloading pages is [wget](https://oreil.ly/wget), which
    is a command line tool available for almost all platforms. On Linux and macOS, `wget`
    should already be installed or can easily be installed using a package manager.
    On Windows, there is a port available at [*https://oreil.ly/2Nl0b*](https://oreil.ly/2Nl0b).
  prefs: []
  type: TYPE_NORMAL
- en: '`wget` supports lists of URLs for downloads and HTTP keep-alive. Normally,
    each HTTP request needs a separate TCP connection (or a Diffie-Hellman key exchange;
    see [“Tips for Efficient Downloads”](#ch3-tips-for-efficient-downloads)). The
    `-nc` option of `wget` will check whether files have already been downloaded.
    This way, we can avoid downloading content twice. We can now stop the process
    at any time and restart without losing data, which is important if a web server
    blocks us, our Internet connection goes down, etc. Let’s save the list of URLs
    from the last blueprint to a file and use that as a template for downloading:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Now go to your command line (or a terminal tab in Jupyter) and call `wget`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The `-i` option tells `wget` the list of URLs to download. It’s fun to see how
    `wget` skips the existing files (due to the `-nc` option) and how fast the downloading
    works.
  prefs: []
  type: TYPE_NORMAL
- en: '`wget` can also be used for recursively downloading websites with the option `-r`.'
  prefs: []
  type: TYPE_NORMAL
- en: Danger of Lockout!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Be careful, this might lead to long-running processes, and eventually you might
    get locked out of the website. It’s often a good idea to combine `-r` with `-l` (recursion
    level) when experimenting with recursive downloads.
  prefs: []
  type: TYPE_NORMAL
- en: There are several different ways to download data. For a moderate number of
    pages (like a few hundred to a thousand), a download directly in a Python program
    is the standard way to go. We recommend the `requests` library, as it is easy
    to use.
  prefs: []
  type: TYPE_NORMAL
- en: Downloading more than a few thousand pages normally works better in a multistage
    process by first generating a list of URLs and then downloading them externally
    via a dedicated program like `wget`.
  prefs: []
  type: TYPE_NORMAL
- en: Extracting Semistructured Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the following section, we will explore different methods to extract data
    from Reuters articles. We will start with using regular expressions and then turn
    to a full-fledged HTML parser.
  prefs: []
  type: TYPE_NORMAL
- en: 'Eventually we will be interested in the data of more than one article, but
    as a first step we will concentrate on a single one. Let’s take [“Banned in Boston:
    Without vaping, medical marijuana patients must adapt”](https://oreil.ly/jg0Jr)
    as our example.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Blueprint: Extracting Data with Regular Expressions'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The browser will be one of the most important tools for dissecting the article.
    Start by opening the URL and using the View Source functionality. In the first
    step, we can see that the title is interesting. Taking a look at the HTML, the
    title is surrounded by both `<title>` and `<h1>`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: HTML Code Changes Over Time
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The programs described in this section work with the HTML code that was current
    when the book was written. However, publishers are free to change their website
    structure anytime and even remove content. An alternative is to use the data from
    the [Wayback Machine](https://archive.org). The Reuters website is mirrored there,
    and snapshots are kept that preserve the layout and the HTML structure.
  prefs: []
  type: TYPE_NORMAL
- en: Also take a look at the GitHub archive of the book. If the layout has changed
    and the programs would not work anymore, alternative links (and sitemaps) will
    be provided there.
  prefs: []
  type: TYPE_NORMAL
- en: Programmatically, the extraction of the title can be achieved with regular expressions
    without using any other libraries. Let’s first download the article and save it
    to a local file called *us-health-vaping-marijuana-idUSKBN1WG4KT.html*.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'A Python blueprint for extracting the title might look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The `re` library is not fully integrated into Python string handling. In other
    words, it cannot be invoked as methods of string. As our HTML documents consist
    of many lines, we have to use `re.MULTILINE|re.DOTALL`. Sometimes cascaded calls
    to `re.search` are necessary, but they do make the code harder to read.
  prefs: []
  type: TYPE_NORMAL
- en: It is crucial to use `re.search` and not `re.match` in Python, which is different
    than in many other programming languages. The latter tries to match the whole
    string, and as there is data before `<title>` and after `</title>`, it fails.
  prefs: []
  type: TYPE_NORMAL
- en: 'Blueprint: Using an HTML Parser for Extraction'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The article has more interesting parts that are tedious to extract with regular
    expressions. There’s text in the article, a publication date is associated with
    it, and the authors are named. This is much easier to accomplish with an HTML
    parser.^([6](ch03.xhtml#idm45634206104264)) Fortunately, with the Python package
    called [Beautiful Soup](https://oreil.ly/I2VJh), we have an extremely powerful
    library for handling this. If you don’t have Beautiful Soup installed, install
    it now with `pip install bs4` or `conda install bs4`. Beautiful Soup is tolerant
    and can also parse “bad” HTML that is often found on sloppily managed websites.
  prefs: []
  type: TYPE_NORMAL
- en: The next sections make use of the fact that all articles have the same structure
    in the news archive. Fortunately, this is true for most big websites as the pages
    are not hand-crafted but rather generated by a content management system from
    a database.
  prefs: []
  type: TYPE_NORMAL
- en: Extracting the title/headline
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Selecting content in Beautiful Soup uses so-called selectors that need to be
    given in the Python program. Finding them is a bit tricky, but there are structural
    approaches for that. Almost all modern browsers support a Web Inspector, which
    is useful for finding the CSS selectors. Open the Web Inspector in the browser
    (most commonly achieved by pressing F12) when the article is loaded, and click
    the Web Inspector icon, as shown in [Figure 3-3](#web_inspector_icon_in_chrome_browser).
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/btap_0303.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-3\. Web Inspector icon in the Chrome browser.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Hover over the headline and you will see the corresponding element highlighted,
    as shown in [Figure 3-4](#screenshot_of_the_chrome_browser_with_the_web_inspector).
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/btap_0304.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-4\. Chrome browser using the Web Inspector.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Clicking the headline to show it in the Web Inspector. It should look like
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Using CSS notation,^([7](ch03.xhtml#idm45634206031320)) this element can be
    selected with `h1.ArticleHeader_headline`. Beautiful Soup understands that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Beautiful Soup makes it even easier and lets us use the tag names directly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Normally, the most interesting part of the previous HTML fragment is the real
    text without the HTML clutter around it. Beautiful Soup can extract that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Note that in contrast to the regular expression solution, unnecessary whitespaces
    have been stripped by Beautiful Soup.
  prefs: []
  type: TYPE_NORMAL
- en: 'Unfortunately, that does not work as well for the title:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Here, we would need to manually strip the data and eliminate the `- Reuters`
    suffix.
  prefs: []
  type: TYPE_NORMAL
- en: Extracting the article text
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In a similar way to the previously described procedure for finding the headline
    selector, you can easily find the text content at the selector `div.StandardArticleBody_body`.
    When using `select`, Beautiful Soup returns a list. Often it is clear from the
    underlying HTML structure that the list consists of only one item or we are interested
    only in the first element. We can use the convenience method `select_one` here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Extracting image captions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'But wait, apart from the text, this part also contains images with captions
    that might be relevant separately. So again, use the Web Inspector to hover over
    the images and find the corresponding CSS selectors. All images are contained
    in `<figure>` elements, so let’s select them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Inspecting the result closely, this code contains only one image, whereas the
    browser displays many images. This is a pattern that can often be found in web
    pages. Code for the images is not in the page itself but is added later by client-side
    JavaScript. Technically this is possible, although it is not the best style. From
    a content perspective, it would be better if the image source were contained in
    the original server-generated page and made visible by CSS later. This would also
    help our extraction process.  Anyway, we are more interested in the caption of
    the image, so the correct selector would be to replace `img` with `figcaption`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Extracting the URL
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'When downloading many HTML files, it is often difficult to find the original
    URLs of the files if they have not been saved separately. Moreover, URLs might
    change, and normally it is best to use the standard (called *canonical*) URL.
    Fortunately, there is an HTML tag called `<link rel="canonical">` that can be
    used for this purpose. The tag is not mandatory, but it is extremely common, as
    it is also taken into account by search engines and contributes to a good ranking:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Extracting list information (authors)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Taking a look at the source code, the author of the article is mentioned in
    a `<meta name="Author">` tag.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'However, this returns only one author. Reading the text, there is another author,
    which is unfortunately not contained in the meta-information of the page. Of course,
    it can be extracted again by selecting the elements in the browser and using the
    CSS selector:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Extracting the author names is then straightforward:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Semantic and nonsemantic content
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In contrast to the previous examples, the `sel` selector is not *semantic*.
    Selection is performed based on layout-like classes. This works well for the moment
    but is likely to break if the layout is changed. Therefore, it’s a good idea to
    avoid these kinds of selections if the code is likely to be executed not only
    once or in a batch but should also run in the future.
  prefs: []
  type: TYPE_NORMAL
- en: Extracting text of links (section)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The section is easy to extract. Using the Web Inspector again, we can find
    that the CSS selector is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Extracting reading time
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Reading time can be found easily via the Web Inspector:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: Extracting attributes (ID)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Having a primary key that uniquely identifies an article is helpful. The ID
    is also present in the URL, but there might be some heuristics and advanced splitting
    necessary to find it. Using the browser’s View Source functionality and searching
    for this ID, we see that it is the `id` attribute of the article container:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: Extracting attribution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Apart from the authors, the article carries more attributions. They can be
    found at the end of the text and reside in a special container:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: Extracting timestamp
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For many statistical purposes, it is crucial to know the time that the article
    was posted. This is mentioned next to the section, but unfortunately it is constructed
    to be human-readable (like “3 days ago”). This can be parsed but is tedious. Knowing
    the real publishing time, the correct element can be found in the HTML head element:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'A string is already helpful (especially in this notation, as we will see later),
    but Python offers facilities to convert that to a datetime object easily:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: The same can be done for `modified_time` instead of `published_time`, if that
    is more relevant.
  prefs: []
  type: TYPE_NORMAL
- en: Use regular expressions only for crude extraction. An HTML parser is slower
    but much easier to use and more stable.
  prefs: []
  type: TYPE_NORMAL
- en: Often, it makes sense to take a look at the semantic structure of the documents
    and use HTML tags that have semantic class names to find the value of structural
    elements. These tags have the advantage that they are the same over a large class
    of web pages. Extraction of their content therefore has to be implemented only
    once and can be reused.
  prefs: []
  type: TYPE_NORMAL
- en: Apart from extremely simple cases, try to use an HTML parser whenever possible.
    Some standard structures that can be found in almost any HTML document are discussed
    in the following sidebar.
  prefs: []
  type: TYPE_NORMAL
- en: 'Blueprint: Spidering'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far we have taken a look at how we can download web pages and extract the
    content using HTML parsing techniques. From a business perspective, looking at
    single pages is often not so interesting, butyou want to see the whole picture.
    For this, you need much more content.
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, our acquired knowledge can be combined to download content archives
    or whole websites. This is often a multistage process where you need to generate
    URLs first, download the content, find more URLs, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: This section explains one of these “spidering” examples in detail and creates
    a scalable blueprint that can be used for downloading thousands (or millions)
    of pages.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing the Use Case
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Parsing a single Reuters article is a nice exercise, but the Reuters archive
    is much larger and contains many articles.  It is also possible to use the techniques
    we have covered to parse a larger amount. Imagine that you want to download and
    extract, for example, a whole forum with user-generated content or a website with
    scientific articles. As mentioned previously, it is often most difficult to find
    the correct URLs of the articles.
  prefs: []
  type: TYPE_NORMAL
- en: Not in this case, though. It would be possible to use *sitemap.xml*, but Reuters
    is generous enough to offer a dedicated archive page at [*https://www.reuters.com/news/archive*](https://www.reuters.com/news/archive/).
    A paging functionality is also available, so it’s possible to go backward in time.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 3-5](#flowchart_for_spidering_process) shows the steps for downloading
    part of the archive (called *spidering*). The process works as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Define how many pages of the archive should be downloaded.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Download each page of the archive into a file called *page-000001.html*, *page-000002.html*,
    and so on for easier inspection. Skip this step if the file is already present.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each *page-*.html* file, extract the URLs of the referenced articles.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each article URL, download the article into a local HTML file. Skip this
    step if the article file is already present.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each article file, extract the content into a `dict` and combine these `dict`s
    into a Pandas `DataFrame`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](Images/btap_0305.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-5\. Flowchart for spidering process.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In a more generic approach, it might be necessary to create intermediate URLs
    in step 3 (if there is an overview page for years, months, etc.) before we finally
    arrive at the article URLs.
  prefs: []
  type: TYPE_NORMAL
- en: The procedure is constructed in a way that each step can be run individually
    and downloads have to be performed only once. This has proven to be useful, especially
    when we have to extract a large number of articles/URLs, as a single missing download
    or malformed HTML page does not mean that the whole procedure including downloading
    has to be started again. Moreover, the process can be restarted anytime and downloads
    only data that has not yet been downloaded. This is called *idempotence* and is
    often a useful concept when interacting with “expensive” APIs.
  prefs: []
  type: TYPE_NORMAL
- en: 'The finished program looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Having defined these functions, they can be invoked with parameters (which
    can easily be changed):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: Error Handling and Production-Quality Software
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For simplicity, all example programs discussed in this chapter do not use error
    handling. For production software, however, you should use exception handling.
    As HTML can change frequently and pages might be incomplete, errors can happen
    at any time, so it is a good idea to use try/except generously and log the errors.
    If systematic errors occur, you should look for the root cause and eliminate it.
    If errors occur only sporadically or due to malformed HTML, you can probably ignore
    them, as they might also be due to server software.
  prefs: []
  type: TYPE_NORMAL
- en: Using the download and save file mechanism described earlier, the extraction
    procedure can be restarted anytime or also be applied to certain problematic files
    separately. This is often a big advantage and helps to achieve a cleanly extracted
    dataset fast.
  prefs: []
  type: TYPE_NORMAL
- en: Generating URLs is often as difficult as extracting content and is frequently
    related to it. In many cases, this has to be repeated several times to download,
    for example, hierarchical content.
  prefs: []
  type: TYPE_NORMAL
- en: When you download data, always find a filename for each URL and save it to the
    filesystem. You will have to restart the process more often than you think. Not
    having to download everything over and over is immensely useful, especially during
    the development process.
  prefs: []
  type: TYPE_NORMAL
- en: If you have downloaded and extracted the data, you will probably want to persist
    it for later use. An easy way is to save it in individual JSON files. If you have
    many files, using a directory structure might be a good option. With an increasing
    number of pages, even this might not scale well, and it’s a better idea to use
    a database or another columnar data store.
  prefs: []
  type: TYPE_NORMAL
- en: Density-Based Text Extraction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Extracting structured data from HTML is not complicated, but it is tedious.
    If you want to extract data from a whole website, it is well worth the effort
    as you only have to implement the extraction for a limited number of page types.
  prefs: []
  type: TYPE_NORMAL
- en: However, you may need to extract text from many different websites. Implementing
    the extraction for each of them does not scale well. There is some metadata that
    can be found easily, such as title, description, etc. But the text itself is not
    so easy to find.
  prefs: []
  type: TYPE_NORMAL
- en: Taking a look at the information density, there are some heuristics that allow
    extraction of the text. The algorithm behind it measures the *density of information* and
    therefore automatically eliminates repeated information such as headers, navigation,
    footers, and so on. The implementation is not so simple but is fortunately available
    in a library called [`python-readability`](https://oreil.ly/AemZh). The name originates
    from a now-orphaned browser plugin called Readability, which was conceived to
    remove clutter from web pages and make them easily readable—exactly what is needed
    here. To get started, we must first install `python-readability` (**`pip install
    readability-lxml`**).
  prefs: []
  type: TYPE_NORMAL
- en: Extracting Reuters Content with Readability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s see how this works in the Reuters example. We keep the HTML we have downloaded,
    but of course you can also use a file or URL:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, that was easy. The title can be extracted via the corresponding
    element. However, the library can do some additional tricks, such as finding the
    title or the summary of the page:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'That is already quite good. Let’s check how well it works for the actual content:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'The data still has some remaining HTML structure, which can be useful to keep
    because paragraphs are included. Of course, the body part can be extracted again
    with Beautiful Soup:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: In this case, the results are excellent. In most of the cases, `python-readability`
    works reasonably well and removes the need to implement too many special cases.
    However, the cost of using this library is uncertainty. Will it always work in
    the expected way with the impossibility of extracting structured data such as
    timestamps, authors, and so on (although there might be other heuristics for that)?
  prefs: []
  type: TYPE_NORMAL
- en: Summary Density-Based Text Extraction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Density-based text extraction is powerful when using both heuristics and statistical
    information about information distribution on an HTML page. You should keep in
    mind that the results are almost always worse when compared to implementing a
    specific extractor. However, if you need to extract content from many different
    page types or from an archive where you don’t have a fixed layout at all, it might
    well be worth it to go that way.
  prefs: []
  type: TYPE_NORMAL
- en: Performing a detailed quality assurance afterward is even more essential compared
    to the structured approach as both the heuristics and the statistics might sometimes
    go in the wrong direction.
  prefs: []
  type: TYPE_NORMAL
- en: All-in-One Approach
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[*Scrapy*](https://scrapy.org) is another Python package that offers an all-in-one
    approach to spidering and content extraction. The methods are similar to the ones
    described in the earlier sections, although Scrapy is more suited for downloading *whole* websites
    and not only parts of them.'
  prefs: []
  type: TYPE_NORMAL
- en: The object-oriented, holistic approach of Scrapy is definitely nice, and the
    code is readable. However, it turns out to be quite difficult to restart spidering
    and extraction without having to download the whole website again.
  prefs: []
  type: TYPE_NORMAL
- en: Compared to the approach described earlier, downloading must also happen in
    Python. For websites with a huge number of pages, HTTP keep-alive cannot be used,
    and gzip encoding is also difficult. Both can be easily integrated in the modular
    method by externalizing the downloads via tools such as wget.
  prefs: []
  type: TYPE_NORMAL
- en: 'Blueprint: Scraping the Reuters Archive with Scrapy'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s see how the download of the archive and the articles would look in Scrapy.
    Go ahead and install Scrapy (either via **`conda install scrapy`** or **`pip install
    scrapy`**).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: Scrapy works in an object-oriented way. For each so-called spider, a class needs
    to be implemented that is derived from `scrapy.Spider`. Scrapy adds a lot of debug
    output, which is reduced in the previous example by `logging.WARNING`. The base
    class automatically calls the parse function with the `start_urls`. This function
    extracts the links to the article and invokes `yield` with the function `parse_article`
    as a parameter. This function in turn extracts some attributes from the articles
    and yields them in a `dict`. Finally, the next page link is crawled, but we stop
    here before getting the second page.
  prefs: []
  type: TYPE_NORMAL
- en: '`yield` has a double functionality in Scrapy. If a `dict` is yielded, it is
    added to the results. If a `Request` object is yielded, the object is fetched
    and gets parsed.'
  prefs: []
  type: TYPE_NORMAL
- en: Scrapy and Jupyter
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Scrapy is optimized for command-line usage, but it can also be invoked in a
    Jupyter notebook. Because of Scrapy’s usage of the (ancient) [Twisted environment](https://oreil.ly/j6HCm),
    the scraping cannot be restarted, so you have only one shot if you try it in the
    notebook (otherwise you have to restart the notebook):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'Here are a few things worth mentioning:'
  prefs: []
  type: TYPE_NORMAL
- en: The all-in-one approach looks elegant and concise.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As most of the coding is spent in extracting data in the articles, this code
    has to change frequently. For this, spidering has to be restarted (and if you
    are running the script in Jupyter, you also have to start the Jupyter notebook
    server), which tremendously increases turnaround times.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It’s nice that JSON can directly be produced. Be careful as the JSON file is
    appended, which can result in an invalid JSON if you don’t delete the file before
    starting the spidering process. This can be solved by using the so-called jl format
    (JSON lines), but it is a workaround.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scrapy has some nice ideas. In our day-to-day work, we do not use it, mainly
    because debugging is hard. If persistence of the HTML files is needed (which we
    strongly suggest), it loses lots of advantages. The object-oriented approach is
    useful and can be implemented outside of Scrapy without too much effort.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As Scrapy also uses CSS selectors for extracting HTML content, the basic technologies
    are the same as with the other approaches. There are considerable differences
    in the downloading method, though. Having Twisted as a backend creates some overhead
    and imposes a special programming model.
  prefs: []
  type: TYPE_NORMAL
- en: Decide carefully whether an all-in-one approach suits your project needs. For
    some websites, ready-made Scrapy spiders might already be available and can be
    reused.
  prefs: []
  type: TYPE_NORMAL
- en: Possible Problems with Scraping
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before scraping content, it is always worthwhile to consider possible copyright
    and data protection issues.
  prefs: []
  type: TYPE_NORMAL
- en: More and more web applications are constructed using frameworks like [React](https://reactjs.org).
    They have only a single page, and data is transferred via an API. This often leads
    to websites not working without JavaScript. Sometimes there are specialized URLs
    constructed for search engines that are also useful for spidering. Usually, those
    can be found in *sitemap.xml*. You can try it by switching off JavaScript in your
    browser and then see whether the website still works.
  prefs: []
  type: TYPE_NORMAL
- en: If JavaScript is needed, you can find requests on the Network tab by using the
    Web Inspector of the browser and clicking around the application. Sometimes, JSON
    is used to transfer the data, which makes extraction often much easier compared
    to HTML. However, the individual JSON URLs still have to be generated, and there
    might be additional parameters to avoid [cross-site request forgery (CSRF)](https://oreil.ly/_6O_Q).
  prefs: []
  type: TYPE_NORMAL
- en: Requests can become quite complicated, such as in the Facebook timeline, on
    Instagram, or on Twitter. Obviously, these websites try to keep their content
    for themselves and avoid spidering.
  prefs: []
  type: TYPE_NORMAL
- en: For complicated cases, it can be useful to “remote control” the browser by using [Selenium](https://oreil.ly/YssLD),
    a framework that was originally conceived for the automated testing of web applications,
    or a [headless browser](https://oreil.ly/CH2ZI).
  prefs: []
  type: TYPE_NORMAL
- en: Websites like Google try to detect automatic download attempts and start sending
    captchas. This can also happen with other websites. Most of the time this is bound
    to certain IP addresses. The website must then be “unlocked” with a normal browser,
    and the automatic requests should be sent with larger pauses between them.
  prefs: []
  type: TYPE_NORMAL
- en: Another method to avoid content extraction is obfuscated HTML code where CSS
    classes have totally random names. If the names do not change, this is more work
    initially to find the correct selectors but should work automatically afterward.
    If the names change every day (for example), content extraction becomes extremely
    difficult.
  prefs: []
  type: TYPE_NORMAL
- en: Closing Remarks and Recommendation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Web scraping is a powerful and scalable technique to acquire content. The necessary
    Python infrastructure supports scraping projects in an excellent way. The combination
    of the requests library and Beautiful Soup is comfortable and works well for moderately
    large scraping jobs.
  prefs: []
  type: TYPE_NORMAL
- en: As we have seen throughout the chapter, we can systematically split up large
    scraping projects into URL generation and downloading phases. If the number of
    documents becomes really big, external tools like `wget` might be more appropriate
    compared to requests. As soon as everything is downloaded, Beautiful Soup can
    be used to extract the content.
  prefs: []
  type: TYPE_NORMAL
- en: If you want to minimize waiting time, all stages can be run in parallel.
  prefs: []
  type: TYPE_NORMAL
- en: In any case, you should be aware of the legal aspects and behave as an “ethical
    scraper” by respecting the rules in *robots.txt*.
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch03.xhtml#idm45634207731272-marker)) Reuters is a news website and changes
    daily. Therefore, expect completely different results when running the code!
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch03.xhtml#idm45634208553592-marker)) You might have to install the package
    first with **`pip install xmltodict`**.
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](ch03.xhtml#idm45634212104584-marker)) Reuters is a news site, and the
    content is continually updated. Note that your results will definitely be different!
  prefs: []
  type: TYPE_NORMAL
- en: ^([4](ch03.xhtml#idm45634206884968-marker)) Just after the time of writing,
    Reuters stopped providing RSS feeds, which led to a public outcry. We hope that
    RSS feeds will be restored. The Jupyter notebook for this chapter [on GitHub](https://oreil.ly/Wamlu)
    uses an archived version of the RSS feed from the Internet archive.
  prefs: []
  type: TYPE_NORMAL
- en: ^([5](ch03.xhtml#idm45634206875096-marker)) As stated previously, Reuters is
    a dynamically generated website, and your results will be different!
  prefs: []
  type: TYPE_NORMAL
- en: ^([6](ch03.xhtml#idm45634206104264-marker)) HTML cannot be parsed with [regular
    expressions](https://oreil.ly/EeCjy).
  prefs: []
  type: TYPE_NORMAL
- en: '^([7](ch03.xhtml#idm45634206031320-marker)) See *CSS: The Definitive Guide,
    4th Edition* by Eric A. Meyer and Estelle Weyl (O’Reilly, 2017)'
  prefs: []
  type: TYPE_NORMAL
