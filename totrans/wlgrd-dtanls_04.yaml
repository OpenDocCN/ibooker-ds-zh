- en: 5 Unusual data sources
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Thinking of data beyond what is available in structured formats
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using all the data sources available to you creatively, regardless of their
    format
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Navigating the tradeoff between time spent and value added when working with
    additional data sources
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Most datasets you will encounter in your career are not as clean and structured
    as those provided in a learning environment. The reality is that it’s often the
    analyst who must search for the right data, which may be hidden in complicated
    spreadsheets or hidden even further in unstructured, nontraditional data sources.
    This chapter is about practicing the creativity of identifying and using novel
    and unstructured data sources to answer interesting analytical questions.
  prefs: []
  type: TYPE_NORMAL
- en: Structured vs. unstructured data
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: For the sake of clarity, when I use the words “structured” and “unstructured”
    to describe a dataset, I mean tabular, two-dimensional data versus everything
    else. Analysts typically work with structured data—something with rows and columns
    that can be opened in Excel or something that sits in a database and could conceivably
    be opened in Excel. Unstructured data is anything that isn’t in a rows and columns
    format, ranging from documents or raw audio to free text or a binary data format.
  prefs: []
  type: TYPE_NORMAL
- en: In this project, you will be working with unstructured PDF files containing
    structured data tables. The semantics of whether we call this data unstructured,
    structured, semi-structured, or something else does not change the fact that working
    with PDFs is not the same as working with tabular data. That is the structured
    versus unstructured difference with which we are dealing in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Identifying novel data sources
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I always advocate starting with a problem to solve. This is no different when
    thinking about additional data sources to use for your analysis. Once you have
    a clear problem statement, it is easier to understand what data sources you still
    need. This is why identifying and obtaining data are steps 3 and 4, and not steps
    1 and 2, of the results-driven approach.
  prefs: []
  type: TYPE_NORMAL
- en: What data is available to you will vary between workplaces, but generally, the
    kinds of data that may be helpful to consider are
  prefs: []
  type: TYPE_NORMAL
- en: Data generated by typical business processes, such as emails.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data in operational systems, if this is not already available.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Self-hosted data, meaning data people create for themselves, such as spreadsheets
    on people’s computer desktops. These are important only if people rely on them
    for decision making. Otherwise, they may just be less accurate versions of existing
    operational data. One example is salespeople tracking their own client pipeline
    outside of the company CRM.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Industry data, such as market statistics published by a central body.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Government statistics, published as open data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: White papers, public documents that summarize research on a given topic in an
    accessible way, created either internally or by a competitor.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Real business case: Extracting published industry data from PDFs'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In many sectors, leading industry bodies publish market statistics, which are
    often the best indicators of the state of the market over time. Many of these
    statistics are published as tabular data, usually Excel files. However, in the
    past, I have had to resort to finding less structured forms of important statistics
    and writing my own PDF data extraction code. This is an experience every aspiring
    analyst should go through, hence the inclusion of this project.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.1 Considerations for using new datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There are some general considerations when deciding to use an additional data
    source to augment your analysis:'
  prefs: []
  type: TYPE_NORMAL
- en: Does this data source integrate with existing data? Can we join or merge this
    dataset with the one we are already using or is that not required? There will
    be instances where salespeople record their sales in their own spreadsheets, either
    outside the “official” CRM or alongside it. Would it be possible to join the data
    in their own custom spreadsheet to the data in the CRM system? Is there a common
    client identifier, like an ID, present in both datasets? If not, can we still
    link clients across those datasets somehow, such as by name? Refer to chapter
    3 for a specific example of just such a problem.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'How much effort is it to extract structured data from this data source? This
    might involve manipulating an unstructured format and creating a tabular representation
    or taking a structured dataset that has a different format from our existing data
    and therefore requires work to change and rename columns to match the format we
    need. Either way, it is important to estimate the effort involved in this work
    before deciding to use a new data source:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A subset of this question is, do you currently have the expertise to manipulate
    this data? Not having worked with a particular data format is not a dealbreaker,
    but learning the necessary skills factors into estimating the effort involved.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A related consideration is, does your tool support this data format? If you
    are used to working solely in Excel, for example, it may be harder to manipulate
    unusual data formats, but a programming language such as Python or R may have
    a relevant library that is easy to install and use.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: What is the value of this additional data? What questions can you answer with
    it that you couldn’t answer before? Knowing this will help determine whether the
    effort will be worth it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Does this data create additional dependencies? Will this additional data be
    used as a one-off, or is it something that will require engineering resources
    to continuously ingest and store?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '5.2 Project 4: Analyzing film industry trends using PDF data'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s take a look at the project in which we will extract structured data from
    PDF files to understand the effects of the COVID-19 pandemic on the film industry.
    We will look at the problems our stakeholders want to solve and the data sources
    they have provided. Section 5.3 will dive into how to approach this problem using
    the results-driven approach as well as some technical considerations when handling
    PDF files. As with every project, there is a section dedicated to a step-by-step
    example solution, which can be found in section 5.4\. As usual, our solutions
    will likely diverge, especially if you are not using Python, since I explore some
    Python-specific ways to read data tables from PDFs.
  prefs: []
  type: TYPE_NORMAL
- en: The data is available at [https://davidasboth.com/book-code](https://davidasboth.com/book-code).
    You will find the files with which you can attempt the project, as well as the
    example solution in the form of a Jupyter notebook.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.1 Problem statement
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this scenario, you are working for EchoTale Analytics, a research firm in
    the entertainment industry. Their primary mission is to publish analytical pieces
    about the evolution of the entertainment industry, and you have been placed in
    charge of a project in their film division. Specifically, the firm wants to publish
    a white paper about how the COVID-19 pandemic has affected the film industry.
    They don’t currently have a more focused topic, so your task is to complete and
    present the preliminary research. Since the target is a white paper, the priority
    is to be able to tell a story that would be interesting to a film-loving audience.
  prefs: []
  type: TYPE_NORMAL
- en: The firm works exclusively with external data sources, and for this project,
    they have given you PDF reports from the British Film Industry’s (BFI) Research
    and Statistics Unit (RSU), called Statistical Yearbooks. These Yearbooks contain
    annual summaries of statistics about the film industry, including embedded data
    tables. Most of the data relates to the film industry in the United Kingdom, but
    some global statistics are included as well. The data goes back almost 20 years,
    and naturally, the format of the PDF report is not consistent.
  prefs: []
  type: TYPE_NORMAL
- en: NOTE  Thanks to the BFI RSU and specifically John Sandow, senior research and
    data analyst, for permission to use the PDF reports.
  prefs: []
  type: TYPE_NORMAL
- en: To complete this project, you will need to
  prefs: []
  type: TYPE_NORMAL
- en: Identify the dimensions along which the film industry is analyzed in the Yearbooks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decide how far back to extract data, as well as what constitutes pre-COVID and
    post-lockdown periods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extract the necessary underlying data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analyze the film statistics to arrive at a narrative that could be useful to
    your stakeholders in preparing their white paper
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Your stakeholders’ priority is that the findings are interesting and unexpected.
    They already assume cinema admissions dropped during the pandemic period and went
    down to zero during lockdowns, and they do not want to publish a white paper with
    such obvious statistics. They would prefer you explore things such as
  prefs: []
  type: TYPE_NORMAL
- en: Have admissions recovered post-lockdown differently in different countries?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What genres of films were popular before and after the pandemic period? Has
    this changed since lockdown restrictions were lifted?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Which distributors have experienced the biggest change post-pandemic?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Has there been a change in people’s attitudes toward independent films?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You will spend most of the time in the analysis portion of this project exploring
    trends pre- and post-lockdowns and comparing them to identify the most marked
    changes, which are the ones most likely to be interesting to your stakeholders.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.2 Data dictionary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This project does not have an explicit data dictionary, which is a common problem
    in the real world. Even if you do not create a data dictionary, you will need
    to note down the types of data present in each Yearbook before deciding on what
    data to focus on. Some aspects of the data may only be present in older or newer
    Yearbooks, but you will need your data to be consistently available in all the
    documents you end up using.
  prefs: []
  type: TYPE_NORMAL
- en: One aspect of the documents that will make your work a bit easier is that the
    data is in tables that can be extracted with the right tools. An example of such
    a table is shown in figure 5.1.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/5-1.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.1 An example data table from a Statistical Yearbook PDF file
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Activity: Creating a data dictionary'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Try writing a data dictionary for the data you end up using. Often, analysts
    write dictionaries for data they end up using because they’re the first to use
    it for analysis. It’s good practice to create this document for future users of
    the data, which includes a future version of you!
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.3 Desired outcomes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The output of your analysis should be recommendations about potential topics
    for the white paper. The recommendations should be specific, so if you think there
    is a story about how people prefer different film genres since lockdown restrictions
    were lifted, your analysis should contain specific conclusions about which genres
    were popular before and after. Your recommendations should also be supported by
    visualizations, which may be included in the final white paper. As an additional
    consideration, you could also think about how your data extraction method, whether
    that is code or a specific tool, could be reused for future versions of this project.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.4 Required tools
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For the example solution in this chapter, I used the Python libraries `pandas`
    and `matplotlib` to explore and visualize the data. To extract the data from the
    PDFs in the first place, I ended up using the Python library `pdfplumber`, but
    I will discuss other options. Your chosen tools may be different, and as with
    every project, the tool is less important than the process, but the tool you select
    must be able to
  prefs: []
  type: TYPE_NORMAL
- en: Read a PDF file and extract tabular data from it into a more typical format,
    such as a CSV file
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Load multiple datasets from CSV or Excel files
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Combine two or more datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perform basic data manipulation tasks, such as sorting, grouping, and reshaping
    data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create data visualizations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I have opted to stay within my Python toolkit for this chapter as well, but
    I will discuss some other options at your disposal. You may choose to extract
    the data from the PDFs with a tool outside of your regular toolkit, in which case
    your usual tools only need to satisfy the latter bullet points and not necessarily
    the first one.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Applying the results-driven method to extracting data from PDFs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There is a lot of uncertainty in this project, partly due to the unknown and
    inconsistent structure of the PDF files and partly due to the vagueness of our
    stakeholders’ requests. Using the results-driven approach, we can formulate an
    action plan.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/5-unnumb-1.png)'
  prefs: []
  type: TYPE_IMG
- en: Our stakeholders haven’t given us much direction, so our understanding of the
    problem is incomplete at this stage. Our first iteration needs to focus on identifying
    common data tables across multiple years of the Yearbooks and analyzing what we
    have at our disposal. Once we start looking at pre-COVID and post-lockdown trends,
    we will have more information about what the main topic of our analysis will be.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/5-unnumb-2.png)'
  prefs: []
  type: TYPE_IMG
- en: Even though we don’t know precisely what the white paper topic will be, we do
    know that we need to focus on comparing the same data in different time periods.
    Our minimum viable answer will focus on this comparison. Even this information
    gives us an idea of the final output of our work, which we can work toward.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/5-unnumb-3.png)'
  prefs: []
  type: TYPE_IMG
- en: In this project, the identification stage will be crucial. This is where we
    explore the PDFs and note down common data themes we could explore. Once we have
    done that, we can move on to extracting only the specific data we need. This will
    save us time in the long run because extracting all the data tables and only then
    exploring them would take longer.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/5-unnumb-4.png)'
  prefs: []
  type: TYPE_IMG
- en: You could argue the data was obtained when our stakeholder provided the PDF
    files, but as we have seen, there is plenty of work to do before we have a structured
    dataset to explore. Not all projects following the results-driven approach will
    result in the same time spent on each section. In this project, I envisage this
    step taking up a large portion of our time.
  prefs: []
  type: TYPE_NORMAL
- en: '*![figure](../Images/5-unnumb-5.png)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s sketch out the steps we will take, building on the steps discussed in
    section 5.2.1:'
  prefs: []
  type: TYPE_NORMAL
- en: First, we will open the PDF Yearbooks in descending year order and note the
    data tables available to us. There is no substitute for actually looking at our
    data before opening our analysis tools to get a sense of what we’re working with.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, we will decide how far back we are aiming to go in time. We do not have
    a lot of data to establish post-lockdown trends, and we don’t want to spend too
    long looking at data that’s too far in the past.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can then decide on which aspects of the data we will be able to extract for
    analysis. This will depend on what is available consistently over the period we’ve
    settled on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The next step will be to find and extract the specific data tables we need and
    save them in a structured format, such as CSVs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Activity: Reusable methods'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: When you get to this point, think about how reusable your chosen extraction
    method is. Whether you manually extract the data or write a script to do it, it
    is possible that you will need to do it again in a future iteration. Thinking
    about reusability up front is a good practice to save yourself time in the long
    run.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, we can analyze the data by examining pre- and post-lockdown trends
    along the dimensions we have identified, whether that is changes in genre, admissions
    for independent films, or something else.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, we will settle on a story to present to our stakeholders for inclusion
    in the white paper.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*![figure](../Images/5-unnumb-6.png)'
  prefs: []
  type: TYPE_NORMAL
- en: We cannot simulate the actual interaction you would have with a stakeholder
    upon presenting your findings. We can, however, practice preparing for such an
    interaction by considering the follow-up questions you might hear, given the work
    you present, and prepare your answers to them. Being ready with suggestions for
    future iterations leads to more productive stakeholder conversations.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/5-unnumb-7.png)'
  prefs: []
  type: TYPE_IMG
- en: In this instance, we would want to present to our stakeholders as soon as we
    have solid evidence for one or more of our findings. What constitutes “solid”
    is not necessarily a question of statistical significance but more an intuition
    about what kind of story our stakeholders would be interested in hearing and publishing.
    This analysis, like others, is not a one-off piece of work you deliver; it is
    a conversation. The idea behind what to present is part of an analyst skill set
    that can only be developed by immersing yourself in real-world scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: '5.4 An example solution: Effects of the COVID-19 lockdown periods on the film
    industry'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s tackle an example solution for this problem. Reading my solution is more
    valuable once you have attempted the project yourself. Remember, our solutions
    may differ. You may end up doing things in a different order as well.
  prefs: []
  type: TYPE_NORMAL
- en: We will begin by inspecting our PDFs to see what kind of data is consistently
    available across multiple years. Once we have made a decision on which datasets
    to focus on, we will find a PDF extraction method and make sure it can reliably
    extract tables of data from our PDFs. In the second part, we will analyze our
    newly created structured data to answer some of our stakeholders’ questions.
  prefs: []
  type: TYPE_NORMAL
- en: 5.4.1 Inspecting the available data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The very first step is to decide which years of data to look at. If we look
    at the available files, a snapshot of which is shown in figure 5.2, we notice
    that the Yearbook used to be a single file per year until 2018, after which the
    files are broken down by category. However, files from 2018 onward also seem to
    have a “master” document, for example, the one titled “2018—BFI Statistical Yearbook.”
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/5-2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.2 A snapshot of the available PDF files
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We also appear to be missing the file for 2015 entirely, which would contain
    data for 2014\. That’s a problem we need a solution for if we want to go back
    further in time.
  prefs: []
  type: TYPE_NORMAL
- en: NOTE  The years indicated in the files are actually for the previous year, meaning
    the file called “2018 Statistical Yearbook” contains data for 2017.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can choose to extend our time period later, but for now, we will aim for
    the minimum amount of data that will get us pre- and post-lockdown trends. The
    first COVID lockdowns were in early 2020, so we definitely want 2019 at a minimum.
    Since we’re interested in patterns, more data would be better, so we’ll include
    two years pre-COVID: 2018 and 2019\. This also means we only include years where
    the format of the documents is consistent.'
  prefs: []
  type: TYPE_NORMAL
- en: A longer-term review would give us more information, but we want to balance
    time and complexity, so two years will suffice for our first iteration. If we
    were to go further back in time, which would be pre-2017, we would want to make
    sure that the categories in the single documents match those that are broken out
    into separate files from 2018 onward. That is, is there data for audiences, distribution,
    public investment, and so forth available?
  prefs: []
  type: TYPE_NORMAL
- en: Most of the lockdown restrictions were eased in 2021, so everything beyond that
    will be the “post-lockdowns” period, and we will need to decide how to categorize
    data in 2021.
  prefs: []
  type: TYPE_NORMAL
- en: Since we want our data to start in 2018, we will start with the 2019 Yearbook.
    Let’s see what tables of data are available. Looking at the table of contents,
    partially shown in figure 5.3, we see that the subheadings match the individual
    files for 2019, which suggests all the necessary data might be contained in this
    single file.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/5-3.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.3 A partial view of the table of contents from the 2019 Yearbook
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The file contains a mixture of text, charts, infographics, and data tables.
  prefs: []
  type: TYPE_NORMAL
- en: Identifying what data to extract
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Looking at the data tables, just for admissions alone, we have statistics for
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Total admissions by country
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monthly admissions in the United Kingdom
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Admissions by UK region
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Annual admission figures all the way back to 1935
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Beyond this, we also have data for broad categories such as
  prefs: []
  type: TYPE_NORMAL
- en: Gross box office revenue
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Top films of the year
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Countries of origin
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Genre
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Directors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Independent films
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distributors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Even just looking at these data tables inspires many directions for our analysis.
    Since the data is for a white paper, we should choose categories that are likely
    to contain interesting stories. Of course, we don’t know up front, but intuition
    and domain expertise can help us choose a path that is more likely to yield results.
    Intuition like that is built only with lots of practice. We will focus on
  prefs: []
  type: TYPE_NORMAL
- en: Admission patterns
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distribution of genres
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Market share across distributors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choosing these categories means we can ask whether seasonal patterns have changed
    over time, whether people now prefer different genres, and which if any, distributor
    has come out of the pandemic as the winner in terms of market share.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s summarize our work so far since we have just reached the first key decision
    point. Figure 5.4 shows the current step and the alternative options.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/5-4.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.4 The first step and decision point in the analysis
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Now, we can identify exactly which tables to look for in all our files to make
    sure we have the right data each year. For simplicity, let’s limit ourselves to
    one data table per category. Figures 5.5–5.7 show the data tables in the 2019
    document that we will search for in post-2019 files.
  prefs: []
  type: TYPE_NORMAL
- en: Based on our decision to focus on admissions, genres, and distributors, we can
    see the necessary data exists in just three tables. We can verify that these data
    tables exist for years beyond 2018 in their respective Yearbooks. If we had encountered
    a difference in structure, we would have had to investigate whether the same information
    was present in each table and ensure their structures matched so that we could
    combine them across multiple years into single files.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/5-5.png)![figure](../Images/5-5.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.5 Monthly UK admissions from the 2019 Yearbook
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![figure](../Images/5-6.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.6 Releases and revenue by genre from the 2019 Yearbook
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![figure](../Images/5-7.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.7 Market share by distributor from the 2019 Yearbook
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 5.4.2 Extracting data from PDFs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that we know what data tables we need, we have multiple options to extract
    them. We could
  prefs: []
  type: TYPE_NORMAL
- en: Manually copy a limited amount of our data from the PDFs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use a dedicated PDF extraction tool
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Find PDF extraction capabilities for our preferred tool (e.g., Python)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Table 5.1 shows the tradeoffs of these different approaches.
  prefs: []
  type: TYPE_NORMAL
- en: Table 5.1 Comparing PDF extraction techniques
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Option | Pros | Cons |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Manually copying data from PDFs into Excel  | • Quick for a small amount
    of data  | • Cannot be automated • Does not scale to more data'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Dedicated PDF extraction tool, either web or desktop based  | • Likely to
    be accurate • Web-based tool requires no installation'
  prefs: []
  type: TYPE_NORMAL
- en: '| • May not be free • Privacy concerns if uploading files to the web'
  prefs: []
  type: TYPE_NORMAL
- en: • Hard to automate
  prefs: []
  type: TYPE_NORMAL
- en: • May not scale to multiple files
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Finding PDF capabilities in our current tool  | • Allows automation and scales
    to many files • No need to leave/change our preferred tools'
  prefs: []
  type: TYPE_NORMAL
- en: '| • Current toolkit may not have such capabilities • If there are few files
    and a one-off task, it might be quicker to extract data manually.'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Whatever option we settle on, we still need to go through the process of choosing
    a tool, implementing it/setting it up, and using it to extract the data from the
    PDFs. Let’s look at each step in detail, starting with choosing the right tool
    with the help of AI.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing a PDF extraction method
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'There is a lot of uncertainty in these options. Working with a specific task
    that we only have to perform rarely is a perfect use case for AI tools. Figure
    5.8 shows the OpenAI GPT-3.5 model’s partial answer to the following prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '*What are my options for easily extracting data tables from PDFs into a machine-readable
    format? The suggested options must be free, open source, and can include Python
    libraries.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![figure](../Images/5-8.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.8 A list of options for PDF data extraction suggested by ChatGPT
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: First, it is important to remember that because these tools are evolving rapidly,
    the same prompt will give us different results depending on which AI tool we use
    and when we use it. We can see that ChatGPT recommends a mix of Python libraries
    and non-Python options as requested, which gives us plenty to explore. However,
    when investigating one of the non-Python options, it turns out the GitHub link
    for TabbyPDF is incorrect. In this case, we can find it ourselves, but it is a
    reminder that AI tools sometimes hallucinate in their suggestions, and they could
    even recommend tools that don’t exist.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s also use the AI tool to help us to start with one of the libraries. When
    asked, “Of the Python options, which one is the easiest to set up with the fewest
    dependencies?” its suggestion is to start with `tabula-py`, as it has the fewest
    dependencies. However, its dependency is to install a Java Runtime Environment,
    which is something we may prefer not to do. Its next suggestion, the `pdfplumber`
    library, has no such external dependencies, but its documentation suggests that
    table extraction features are a “plus.” Another suggestion, `camelot`, specializes
    in data table extraction, but according to ChatGPT, it is harder to set up due
    to its own external dependencies. So, we shouldn’t take the AI’s answer as perfect;
    it should be the start and not the end of the process.
  prefs: []
  type: TYPE_NORMAL
- en: Weighing up our options, let’s settle on trying `pdfplumber` first because other
    Python libraries that are easier to install are its only dependencies. If its
    table-reading capabilities do not give us the results we require, we can always
    try another library, but we will favor simplicity in our first attempt.
  prefs: []
  type: TYPE_NORMAL
- en: Installing `pdfplumber` can be done using any package manager you use, whether
    that is `pip`, `conda`, or `poetry`. As these tools install dependent libraries
    automatically, there is little for us to do in this step. However, if you choose
    to use a different tool for PDF extraction, this step may be more involved.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve decided on a method, let’s summarize our process until this point
    before moving on to the actual extraction step. Figure 5.9 shows the process so
    far.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/5-9.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.9 Steps up until settling on a PDF extraction method
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Let’s now use our chosen tool to extract the structured data from our PDFs.
  prefs: []
  type: TYPE_NORMAL
- en: Using our chosen tool to extract data from PDFs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'After installing `pdfplumber`, let’s start by importing our libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This might be the first time we use the `pdfplumber` library, so to get started,
    we would read its documentation to understand how to open a PDF and extract tables
    from it. Because we will extract multiple tables from multiple pages across multiple
    documents, we should write a reusable function that performs the extraction of
    one or more tables from one or more pages of a single PDF. This function needs
    to open a PDF file, extract all the tables in the pages we specify, and return
    them as `pandas` DataFrames ready for analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start with the extraction code. Given a specified path and a page number,
    the following code will open the PDF, enumerate through the pages, and extract
    all the tables it identifies:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '#1 page_tables is a list of lists of lists(!) extracted from special Table
    objects.'
  prefs: []
  type: TYPE_NORMAL
- en: At this point, the `page_tables` variable is a list that contains lists of lists.
    The output is shown in figure 5.10.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/5-10.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.10 A table extracted by `pdfplumber` from a single PDF page
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Each row of the table is a list of strings, starting with the column headers.
    These rows are themselves in a list, representing a single table. Using the `page.find_tables()`
    function means we end up with a list of these tables. Hence, our data structure
    is a list of lists of lists. Luckily, `pandas` makes it easy for us to convert
    this to a list of DataFrames, as shown in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'These code snippets can then be extended to work across multiple pages, and
    with a few additional print statements and logical checks, the entire function
    is shown in the following code snippet, a portion of the output of which is shown
    in figure 5.11:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '#1 In each case, the variable table is now a list of lists and the first list
    contains the column headers.'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/5-11.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.11 A part of the output of our `extract_tables` function
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The variable `tables` that stores the output of our function is now a list of
    DataFrames, each representing one table extracted from a PDF. It’s time to apply
    this function to our PDFs and extract the data we need. We will walk through this
    for 2018 data, but for completeness, all the data extraction code is included
    in the supplementary code listings.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we identify the page numbers of interest and use our function to extract
    the admissions, genre, and distributor data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The full admissions data is shown in figure 5.12.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/5-12.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.12 2018 admissions data as extracted from our PDF
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'One important aspect of this data that we notice is that the columns themselves
    do not tell us the year this data belongs to. This will be important when we combine
    admissions data over multiple years, so we will add a Year column. We also do
    not need 2017 data or the percentage change, so we can drop those columns. A sample
    of the modified admissions data is shown in figure 5.13:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![figure](../Images/5-13.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.13 A snapshot of the modified admissions data
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Moving onto genres, the second table in our extracted list of DataFrames contains
    the data we need, as shown in figure 5.14.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/5-14.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.14 Genre breakdown for 2018 as extracted from the PDF
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'As opposed to the admissions data, this breakdown is by genre and relates to
    the entire year of 2018\. We still need to add a Year column to differentiate
    genres across years, but we need to remember that this dataset has a different
    level of granularity. Again, we do not need all the columns, and we will need
    to clean up the column names to remove the `\n` newline characters. The following
    code snippet produces the modified genre data, a snapshot of which is shown in
    figure 5.15:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '![figure](../Images/5-15.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.15 The modified genre data from 2018
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Finally, moving on to distributors, the data directly extracted from the PDF
    is shown in figure 5.16.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/5-16.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.16 The raw distributors data, as extracted from the PDF
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'In this case, all our columns will be useful, and we need to add the Year column
    again. We should also remove row 10, as it is a total of the rows above it, which
    would skew our calculations if we left it in. The following code modifies the
    data to suit our needs, and a snapshot of the modified data is shown in figure
    5.17:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Drops the “top 10 total” row'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/5-17.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.17 The modified distributor data for 2018
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Note  Repeating this process for subsequent years shows some differences. In
    2019, there are two admissions tables on the same page, so we need to explicitly
    choose the right one. Also, in the 2022 Yearbooks, the tables are spread across
    multiple PDFs. Extracting these tables is the same process for all the years,
    but these minor differences are what make PDF data extraction complicated.
  prefs: []
  type: TYPE_NORMAL
- en: 'Every dataset of the same type across the years is structured identically,
    so we do not need to do any additional cleaning before being able to combine them.
    The following code shows how we combine the annual admissions datasets into a
    single one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'To ensure we have the right amount of data, we perform a quick sanity check
    to see how many rows of monthly data we have per year. We are expecting exactly
    12 rows per year, which is verified in the output shown in figure 5.18:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '![figure](../Images/5-18.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.18 Verifying that we have 12 rows of admissions data for each year
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Finally, we write the combined admissions data into its own file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The process for combining and exporting genre and distributor data is identical,
    and we end up with three files that we are ready to analyze.
  prefs: []
  type: TYPE_NORMAL
- en: 5.4.3 Analyzing the data extracted from PDFs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that we have our data extracted from our PDFs and combined, we can start
    exploring it to find stories to use in our white paper. We will take each dataset
    at a time, starting with admissions. We could continue the code, meaning we would
    already have our admissions data as a variable, but I have chosen to explicitly
    separate the extraction and analysis processes. In the accompanying resources,
    you will find the process split into two Jupyter notebooks. For the analysis portion,
    we will start a new Jupyter notebook using the data created in the first one.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are multiple benefits to separating extraction from analysis:'
  prefs: []
  type: TYPE_NORMAL
- en: Extraction and analysis can be worked on separately as long as the extraction
    step produces the output the analysis step expects.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You save time by not having to rerun the extraction steps every time the analysis
    changes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Steps can be more easily maintained because they are logically decoupled from
    each other.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In general, when you see an opportunity to create a cleaner solution by separating
    logical steps from each other, you should take it. Anyone looking at your work
    in the future, including yourself, will be grateful for the extra effort you put
    in early on.
  prefs: []
  type: TYPE_NORMAL
- en: Enhancing extracted data with custom logic
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We start by reading in the admissions data and taking a look at it. The following
    code produces the output in figure 5.19:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '![figure](../Images/5-19.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.19 A snapshot of the admissions data
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The dataset is small because it is a single monthly value across only a few
    years. However, it is sufficient to divide into three periods: pre-COVID lockdowns,
    during COVID lockdowns, and post-COVID lockdowns. We do this by creating a date
    column with the right data type. The output of the following code is shown in
    figure 5.20:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '#1 We define variables to mark cutoff points for COVID periods.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Our data has no days, so we arbitrarily set dates to the first of the month.'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/5-20.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.20 Verifying that our newly added Date column is as expected
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Next, we define cutoff points for the three periods and apply them to the data.
    We also use the `Categorical` data type in `pandas` to ensure the correct order
    is observed when sorting; otherwise, these periods would be sorted alphabetically.
    Then, we verify that this new column is distributed as we’d expect and that we
    haven’t left any missing data. The output of the following code is shown in figure
    5.21:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '![figure](../Images/5-21.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.21 Number of rows per different COVID period
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'We can now plot monthly admissions and mark each COVID period with a different
    color and line style using the following code, which produces the plot in figure
    5.22:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '![figure](../Images/5-22.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.22 Admissions across multiple COVID periods
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: At first glance, there are missing months due to the lockdown periods, and after
    the last lockdown was lifted, it appears that admissions have started to return
    to pre-pandemic levels. If we had more post-lockdown data, we could also examine
    whether seasonal patterns are similar to what they were before the pandemic.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we were to look at the average monthly admissions in the three periods,
    we could examine this further. The following code does this and produces the output
    shown in figure 5.23:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Different hatch patterns for different metrics'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Different colors for different metrics'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/5-23.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.23 Average admissions by COVID period
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The reason for looking at both the mean and the median is to investigate whether
    the data is skewed in either direction. That is, do pre-COVID or post-lockdown
    months tend to have outliers in either direction? Looking at histograms of monthly
    admissions by period investigates this further. The following code produces the
    histograms shown in figure 5.24:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '![figure](../Images/5-24.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.24 Histograms of admissions pre-COVID and post-lockdowns
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: There is not a lot of post-lockdown data, but we can observe that there are
    more months with fewer admissions, which is what we’d expect given the slow recovery.
    Pre-COVID, we expected approximately 15–17 million admissions per month, with
    a few outliers in both positive and negative directions. As it stands, there is
    not much of a story to tell about post-lockdown admissions habits, except for
    noting that admissions look to be trending toward pre-COVID levels again by the
    end of 2021.
  prefs: []
  type: TYPE_NORMAL
- en: Investigating changes in trends over time using data from multiple sources
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Now, let’s see what genres were popular on either side of the pandemic period.
    First, let’s read in and examine our data. Figure 5.25 shows a snapshot of rows
    produced by the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '![figure](../Images/5-25.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.25 A snapshot of rows from the genre dataset
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'There are two things to note. First, this dataset is small since it is a handful
    of genres recorded over just a few years. Second, this dataset is at an annual
    level, not a monthly one, which changes our definitions of COVID periods. Specifically,
    we must accept that the first couple of months of 2020 data will be allocated
    as “during COVID” even though they occurred before the first lockdown, and we
    need to decide what to do with 2021 data. 2021 still had lockdowns, but the second
    half of the year is useful data about post-lockdown trends, which we wouldn’t
    want to throw away. We will err on the side of keeping 2021 as “post-lockdowns”
    and categorizing 2020 as the only “during COVID” year. The following code does
    this and produces the output in figure 5.26:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '![figure](../Images/5-26.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.26 Distribution of rows per COVID period in the genres dataset
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Glancing at the data, we might notice that the gross box office column has
    non-numeric values, namely <0.1, to indicate genres that totaled less than £100,000\.
    To calculate revenue by genre, for example, we need this column to be numeric.
    We could convert that value to zero, but that would be misleading, and it is not
    the same value as 0.1, which is a value also present in our data. To indicate
    low revenue, we can add a placeholder value of, say, 0.05:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '![figure](../Images/5-27.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.27 Total revenue by genre
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Now, we can look at total revenue by genre in our dataset. The output of the
    following code is shown in figure 5.27:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'It seems that people like action and animation films the most. What we really
    want to see is this same distribution by year. We could also look at it by COVID
    period, but let’s look at the more granular picture. The following code achieves
    this and produces the output in figure 5.28:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '![figure](../Images/5-28.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.28 Breakdown of revenue by genre across multiple years
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'That gives quite a clear picture. Action films are the most popular, regardless
    of year. Comedy films historically haven’t done as well pre-COVID, but there has
    been a rise in their popularity in 2021\. Here are some theories about this result:'
  prefs: []
  type: TYPE_NORMAL
- en: Animated films take years of effort, and if animators weren’t working at any
    point during COVID, that would have delayed the release of animated films in 2021\.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Comparatively, comedies are probably cheaper to make, though that’s a question
    for the domain experts.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: People possibly prefer light-hearted relief in post-lockdown times.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s test that first assumption. If our theory holds, we should see fewer
    releases in the animation genre in 2021\. Figure 5.29 shows the output of this
    investigation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '![figure](../Images/5-29.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.29 Number of animation releases over time
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'There were almost as many films released in the animation genre in 2021 as
    in 2019, so our theory doesn’t explain why the revenue of that genre dropped.
    Another interesting point from figure 5.28 is the popularity of war films in 2020:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Looking into it, we find that the top-performing title in this category was
    the film *1917*, which was released in January 2020\. Although the revenue from
    this film accounts for its high place in the rankings in 2020, it was released
    before the first lockdowns, so it doesn’t tell us anything beyond that the filmmakers
    were lucky to bring in the revenue before the lockdown happened. It certainly
    isn’t evidence that people liked films about war during the pandemic.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, it appears that although action films are by far the most popular
    genre, comedies experienced a growth in revenue post-lockdowns. This is certainly
    a finding worth bringing to a domain expert to find out more.
  prefs: []
  type: TYPE_NORMAL
- en: Resolving different entity names across data sources
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Our final question relates to distributors. Which companies came out of the
    lockdown period grossing the most revenue for their films? Let’s look at our available
    data in figure 5.30:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '![figure](../Images/5-30.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.30 A snapshot of the distributors dataset
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'For each distributor, we have their revenue market share, number of films released,
    and total gross box office revenue per year. Let’s assign our COVID periods again.
    Figure 5.31 shows the number of rows of data we have per COVID period:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '![figure](../Images/5-31.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.31 Number of rows per COVID period in the distributors data
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'We should also verify that the market share column adds up to 100% each year.
    Figure 5.32 shows whether this is the case:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '![figure](../Images/5-32.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.32 Total market share per year
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The Statistical Yearbooks explicitly mention rounding errors, which are probably
    responsible for the numbers in figure 5.32\. Let us now look at market share by
    distributor for each year of our data. The easiest way to do this with `pandas`
    is to create a pivot table where each column is a different distributor, and each
    row is a different year. This way, when we call the `plot` function, we see one
    line per distributor over time. Let’s see what this reshaping of our data does.
    The following code produces the pivot table shown in figure 5.33:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '![figure](../Images/5-33.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.33 A snapshot of the pivot table aggregating across distributors and
    years
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'There are a number of data problems that become obvious. We need to merge the
    two separate values for 20th Century Fox, merge the “Other” categories into a
    single name so they get a single line over time, and investigate the various distributors
    with “Entertainment” in the name. Upon investigation, it appears Entertainment
    One and eOne Films, which both appear in our data, are the same entity ([https://www.entertainmentone.com/about-eone/](https://www.entertainmentone.com/about-eone/)).
    Entertainment Film Distributors is a legitimately separate entity from Entertainment
    One, but we don’t have evidence of whether the distributor marked simply “Entertainment”
    should be merged into any other category. We will leave it on its own. After these
    corrections, we can look at how many years each distributor is represented in
    our data. The result of this is shown in figure 5.34:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '![figure](../Images/5-34.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.34 Number of years each distributor is present in our data
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'We can also use this result to decide whether to plot all distributors. We
    are particularly interested in change over time, so we should keep only distributors
    that appear in all years of our data. This will potentially omit distributors
    who went out of business during or because of the pandemic. These could be analyzed
    further separately:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'We can use these distributors to recreate our pivot table and see how market
    share compares across years:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Converts each year to the first of January to make it a date type'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 One row per year, one column per distributor'
  prefs: []
  type: TYPE_NORMAL
- en: 'Visually, it’s easier to read the pivot table if years go across columns, so
    let’s transpose the data and examine the output shown in figure 5.35:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '![figure](../Images/5-35.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.35 Distributor market share (%) over time
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'From this pivot table, we can conclude the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Walt Disney completed their purchase of 20th Century Fox in 2019, yet this did
    not translate to a large increase in market share even in 2021\.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sony achieved the biggest increase in market share post-lockdown, doubling their
    market share from 2018, but Universal also increased their market share from pre-COVID
    levels.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apart from Sony and Universal, the distributors with bigger market share roughly
    returned to pre-COVID levels of market share in 2021\. Some smaller ones, like
    StudioCanal, failed to reach pre-COVID levels.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Initial speculation about these results might be that the bigger distributors
    are the ones that have the resources to bounce back from even a global pandemic,
    whereas smaller distributors will likely struggle more with this.
  prefs: []
  type: TYPE_NORMAL
- en: Before summarizing our findings, let’s review the whole process, including places
    where our latest step could have diverged. Figure 5.36 shows the whole process.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/5-36.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.36 The final steps taken in the example solution
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Let’s now review our findings and summarize our recommendations to our stakeholders.
  prefs: []
  type: TYPE_NORMAL
- en: 5.4.4 Project conclusions and recommendations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: What do our results mean for the white paper?
  prefs: []
  type: TYPE_NORMAL
- en: UK-level admissions data does not tell a very interesting story beyond admissions
    looking like they are reaching pre-COVID levels. An additional year of data would
    help clarify post-lockdown trends, as would looking at admissions at a more granular
    level, such as by region or film type.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is a difference in the breakdown of genres post-lockdowns, certainly enough
    of a difference to investigate this angle further.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distributors follow general market trends where larger entities bounce back
    from COVID more easily than smaller ones.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The limitations of our analysis extend mostly to a lack of data. We do not have
    a lot of post-lockdown data to compare with past trends, so our conclusions can
    only be tentative. Our data is focused on the United Kingdom, so it does not give
    a global picture. However, we can assume the research firm we work for would have
    more granular data for us to investigate our findings further. There is enough
    in our conclusions to have a conversation with our domain experts, but the first
    round of analysis does not suggest we have enough to support a white paper.
  prefs: []
  type: TYPE_NORMAL
- en: One benefit of this analysis, whether or not it leads to a published paper,
    is that we have written a data pipeline to extract data from the BFI’s published
    statistics. Having this data in a format that is ready to be analyzed will yield
    value for future projects. Sometimes, cleaning data in this way can be a valuable
    contribution in itself.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity: Further project ideas with this data'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The PDF files included in this chapter are a treasure trove of information
    about the film industry. This project focused on the effects of the pandemic,
    but there are many other angles to explore and many research questions that could
    be answered. Here are some ideas to get you started:'
  prefs: []
  type: TYPE_NORMAL
- en: How have people’s preferences of genre evolved over time?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is there a pattern in what kind of independent films is successful?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How have attendance figures changed over time in different countries? Are movie-goers
    changing the same way everywhere?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 5.5 Closing thoughts on exploring novel data sources
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are two takeaways from this chapter. One is that you will encounter situations
    where you need to learn a specific, narrow skill to extract data from an unusual
    format. This is a good opportunity to learn about more esoteric parts of your
    existing toolkit, such as its PDF-extracting capabilities. AI tools can accelerate
    this learning process, and this chapter’s project is an example of where depth
    is not required. To successfully complete the project in this chapter, you did
    not need to become an expert in PDF extraction or optical character recognition,
    the process of converting an image to a machine-readable text form. You only needed
    to find the relevant library and code snippets to extract tabular data from a
    PDF. It is an example of learning enough to stay focused on the end result and
    no more.
  prefs: []
  type: TYPE_NORMAL
- en: Second, the broader takeaway is that data is available in many forms. Knowing
    that our tools make it possible to extract data from unusual, unstructured sources
    will expand the potential data available to us for our analyses. It allows us
    to generate more creative solutions to problems, which will help us add more value
    with our work.
  prefs: []
  type: TYPE_NORMAL
- en: 5.5.1 Skills for exploring unusual data sources for any project
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this chapter, we explored a new source of data, namely, PDF files. The specific
    skills required for unusual data sources, and more broadly for exploring new data
    sources, which can be used for any problem, include
  prefs: []
  type: TYPE_NORMAL
- en: Finding the relevant data in unstructured files, such as PDFs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identifying existing tools to extract data from a novel source
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using AI tools such as ChatGPT to learn about specific tools for specific data
    formats
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using new tools to extract data from unstructured sources into structured formats
    (e.g., data from PDFs into CSV files)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enhancing extracted data with custom logic (e.g., pre- and post-COVID lockdown
    periods)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Investigating trends by extracting similar data from multiple sources over time
    (e.g., the same annual report across multiple years)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Resolving differences across multiple similar data sources (e.g., names of production
    companies changing over time but relating to the same entity)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Identifying novel and unstructured data sources is a core skill of a good analyst.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Considering new data sources in your analysis may introduce nontabular or unstructured
    data requiring effort to clean up: time you should consider when including them.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Telling the story that’s in the data, rather than the one our stakeholders asked
    for in their problem statement, is critical to avoid misleading ourselves.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Focusing on the problem to solve rather than the data increases the chances
    that any additional data you consider will be relevant.**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
