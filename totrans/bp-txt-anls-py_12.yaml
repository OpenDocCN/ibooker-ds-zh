- en: Chapter 12\. Building a Knowledge Graph
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第12章 构建知识图谱
- en: In this book, we have been working through many blueprints for text analysis.
    Our goal was always to identify patterns in the data with the help of statistics
    and machine learning. In [Chapter 10](ch10.xhtml#ch-embeddings) we explained how
    embeddings can be used to answer questions like “What is to Germany like Paris
    is to France?” Embeddings represent some kind of implicit knowledge that was learned
    from the training documents based on a notion of similarity.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们一直在探索文本分析的多个蓝图。我们的目标始终是通过统计和机器学习帮助识别数据中的模式。在[第10章](ch10.xhtml#ch-embeddings)中，我们解释了如何使用嵌入来回答类似“德国对应巴黎的是什么？”的问题。嵌入表示从训练文档中学习的某种隐含知识，基于相似性的概念。
- en: A knowledge base, in contrast, consists of structured statements of the form
    “Berlin capital-of Germany.” In this case, “capital-of” is a precisely defined
    relation between the two specific entities *Berlin* and *Germany*. The network
    formed by many entities and their relations is a graph in the mathematical sense,
    a *knowledge graph*. [Figure 12-1](#fig-simple-kg) shows a simple knowledge graph
    illustrating the example. In this chapter, we will introduce blueprints to extract
    structured information from unstructured text and build a basic knowledge graph.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 知识库相反，由“柏林是德国的首都”形式的结构化陈述组成。在这种情况下，“首都”是两个特定实体 *柏林* 和 *德国* 之间明确定义的关系。由许多实体及其关系形成的网络在数学意义上是一个图，即*知识图谱*。[图 12-1](#fig-simple-kg)展示了一个简单的知识图谱，说明了这个例子。在本章中，我们将介绍从非结构化文本中提取结构化信息并构建基本知识图谱的蓝图。
- en: '![](Images/btap_1201.jpg)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/btap_1201.jpg)'
- en: Figure 12-1\. A simple knowledge graph.
  id: totrans-4
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 12-1. 一个简单的知识图谱。
- en: What You’ll Learn and What We’ll Build
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: What You’ll Learn and What We’ll Build
- en: Information extraction is one of the hardest tasks in natural language processing
    because of the complexity and inherent ambiguity of language. Thus, we need to
    apply a sequence of different steps to discover the entities and relationships.
    Our example use case in this section is the creation of a knowledge graph based
    on business news articles about companies.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 信息抽取是自然语言处理中最困难的任务之一，因为语言的复杂性和固有歧义性。因此，我们需要应用一系列不同步骤来发现实体和关系。本节中的示例用例是基于公司业务新闻文章创建知识图谱。
- en: In the course of the chapter, we will take a deep dive into the advanced language
    processing features of spaCy. We will use the pretrained neural models in combination
    with custom rules for named-entity recognition, coreference resolution, and relation
    extraction. We will also explain the necessary steps to perform entity linking,
    but we won’t go into the implementation details.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将深入探讨spaCy的高级语言处理功能。我们将使用预训练的神经模型结合自定义规则进行命名实体识别、指代消解和关系抽取。我们还将解释执行实体链接的必要步骤，但不会深入到实现细节。
- en: After reading this chapter, you will have the basic linguistic and technical
    knowledge to start building your own knowledge base. You will find the source
    code for this chapter and additional information in our [GitHub repository](https://oreil.ly/5dF4g).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读完本章后，你将具备开始构建自己知识库的基本语言和技术知识。你可以在我们的[GitHub仓库](https://oreil.ly/5dF4g)找到本章的源代码和额外信息。
- en: Knowledge Graphs
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 知识图谱
- en: A knowledge graph is a large semantic network. It consists of nodes that are
    entities such as persons, places, events, or companies, and edges that represent
    formalized relations between those nodes, as shown in [Figure 12-1](#fig-simple-kg).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 知识图谱是一个大型语义网络。它包括节点，如人物、地点、事件或公司，以及代表这些节点之间正式关系的边，如[图 12-1](#fig-simple-kg)所示。
- en: 'All the big players such as Google, Microsoft, Facebook, etc., use knowledge
    graphs to power their search engines and query services.^([1](ch12.xhtml#idm45634176075192))
    And nowadays more and more companies are building their own knowledge graphs to
    gain market insights or power chatbots. But the largest knowledge graph is distributed
    all over the world: *Linked Open Data* refers to all the available data on the
    web that can be identified by a uniform resource identifier (URI). It is the result
    of 20 years of academic development in the area of the Semantic Web (see [“Semantic
    Web and RDF”](#semanticwebRDF)).'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 谷歌、微软、Facebook等大公司都使用知识图谱来支持他们的搜索引擎和查询服务。[^1](ch12.xhtml#idm45634176075192)
    现在，越来越多的公司开始构建自己的知识图谱，以获取市场洞察或为聊天机器人提供支持。但是最大的知识图谱分布在全球各地：*Linked Open Data*指的是网络上所有可通过统一资源标识符（URI）识别的可用数据。这是在语义网领域经过20年学术发展的结果（参见[“语义网和RDF”](#semanticwebRDF)）。
- en: The types of nodes and edges are precisely defined by an *ontology*, which is
    itself a knowledge base for the terminology used in a domain. For example, the
    public ontology Wikidata provides definitions for all types used in [Figure 12-1](#fig-simple-kg).^([2](ch12.xhtml#idm45634176061992))
    Each of these definitions has a unique URI (e.g., “city” is [*http://www.wikidata.org/wiki/Q515*](http://www.wikidata.org/wiki/Q515).).
    In fact, Wikidata contains both, the type definitions and the actual objects in
    a queryable format.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 节点和边的类型由*本体*精确定义，本体本身是一个领域术语使用的知识库。例如，公共本体Wikidata为[Figure 12-1](#fig-simple-kg)中使用的所有类型提供了定义。[^2](ch12.xhtml#idm45634176061992)
    每个定义都有一个唯一的URI（例如，“city”是[*http://www.wikidata.org/wiki/Q515*](http://www.wikidata.org/wiki/Q515)）。事实上，Wikidata包含了类型定义和实际对象，以可查询的格式存储。
- en: Information Extraction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 信息提取
- en: There are several typical steps needed to extract structured information from
    text, as shown in [Figure 12-2](#fig-ie). As a first step, *named-entity recognition*,
    finds *mentions* of named entities in the text and labels them with the correct
    type, e.g., person, organization, or location. The same entity is usually referenced
    multiple times in a document by different variants of the name or by pronouns.
    The second step, *coreference resolution*, identifies and resolves those *coreferences*
    to prevent duplicates and information loss.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 从文本中提取结构化信息需要几个典型步骤，如[Figure 12-2](#fig-ie)所示。首先是*命名实体识别*，找到文本中的命名实体并标记其正确类型，例如，人物、组织或地点。同一实体通常会在文档中被不同变体的名称或代词多次引用。第二步是*共指解析*，识别和解决这些*共指*，以防止重复和信息丢失。
- en: 'Closely related to coreference resolution, and usually the next step, is the
    task of *entity linking*. Here, the goal is to link a mention in the text to a
    unique real-world entity in an ontology, for example, *Berlin* to the URI [*http://www.wikidata.org/entity/Q64*](http://www.wikidata.org/entity/Q64).
    Thus, any ambiguities are removed: Q64 is the Berlin in Germany and not the one
    in New Hampshire (which is, by the way, Q821244 in Wikidata). This is essential
    to connect information from different sources and really build a knowledge base.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 与共指解析密切相关，并且通常是下一步，是*实体链接*的任务。在这里，目标是将文本中的提及链接到本体中的唯一现实世界实体，例如，*Berlin*链接到URI[*http://www.wikidata.org/entity/Q64*](http://www.wikidata.org/entity/Q64)。因此，任何歧义都被消除：Q64是德国的柏林，而不是新罕布什尔州的柏林（顺便说一下，在Wikidata中是Q821244）。这对于连接不同来源的信息并真正构建知识库至关重要。
- en: '![](Images/btap_1202.jpg)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/btap_1202.jpg)'
- en: Figure 12-2\. The process of information extraction.
  id: totrans-17
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-2。信息提取过程。
- en: The last step, *relation extraction*, identifies the relations between those
    entities. In an application scenario, you will usually consider only a few relations
    of interest because it is hard to extract this kind of information correctly from
    arbitrary text.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是*关系抽取*，识别这些实体之间的关系。在应用场景中，你通常只会考虑几个感兴趣的关系，因为从任意文本中正确提取这种信息很困难。
- en: Finally, you could store the graph in a graph database as the backend of a knowledge-based
    application. Such graph databases store the data either as RDF triples (*triple
    stores*) or in the form of a property graph, where nodes and edges can have arbitrary
    attributes. Commonly used graph databases are, for example, GraphDB (triple store),
    Neo4j, and Grakn (property graphs).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你可以将图存储在图数据库中，作为知识型应用程序的后端。这些图数据库将数据存储为RDF三元组（*三元存储*）或属性图形式，其中节点和边可以具有任意属性。常用的图数据库包括GraphDB（三元存储）、Neo4j和Grakn（属性图形式）。
- en: For each of the steps, you have the choice between a rule-based approach and
    machine learning. We will use available models of spaCy and rules in addition.
    We will not train our own models, though. The usage of rules for the extraction
    of domain-specific knowledge has the advantage that you can get started quickly
    without training data. As we will see, the results allow some really interesting
    analyses. But if you plan to build a corporate knowledge base on a large scale,
    you may have to train your own models for named-entity and relationship detection
    as well as for entity linking.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个步骤，您可以选择基于规则的方法或机器学习。我们将使用spaCy的现有模型以及规则进行补充。不过，我们不会训练自己的模型。使用规则来提取领域特定知识的优势在于，您可以快速开始，无需训练数据。正如我们将看到的那样，结果允许进行一些非常有趣的分析。但是，如果您计划在大规模上建立企业知识库，您可能需要为命名实体和关系检测以及实体链接训练自己的模型。
- en: Introducing the Dataset
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引入数据集
- en: Assume you are working in the financial business and want to track news on mergers
    and acquisitions. It would be great if you could automatically identify company
    names and the kind of deals they are involved in and make the results available
    in a knowledge base. In this chapter, we will explain the building blocks to extract
    some information about companies. For example, we will extract the relation “Company1
    acquires Company2.”
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您在金融业务中工作，并希望跟踪并购新闻。如果您能够自动识别公司名称及其所涉及的交易类型，并将结果存入知识库，那将是很棒的。在本章中，我们将解释有关提取公司信息的构建块。例如，我们将提取关系“公司1收购公司2”。
- en: 'To simulate such a scenario, we use a publicly available dataset, the well-known
    [Reuters-21578](https://oreil.ly/lltWo) news corpus. It contains more than 20,000
    news articles of 90 categories published by Reuters in 1987\. This dataset was
    chosen because it is free and easy to get. In fact, it is available as one of
    the NLTK standard corpora, and you can simply download it with NLTK:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 为了模拟这样的情景，我们使用了一个公开可用的数据集，著名的[Reuters-21578](https://oreil.ly/lltWo)新闻语料库。它包含由路透社在1987年发布的90个类别的超过20,000篇新闻文章。选择此数据集是因为它是免费且易于获取的。实际上，它作为NLTK标准语料库之一可用，并且您可以简单地使用NLTK下载它：
- en: '[PRE0]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We will work only with articles from the acquisitions category (acq). To prepare
    it for our purposes, we loaded all articles into a single `DataFrame` and did
    some data cleaning following the blueprints in [“Cleaning Text Data”](ch04.xhtml#ch4-cleaning).
    Clean data is crucial to recognize named-entities and relationships as the neural
    models benefit from well-structured sentences. For this dataset, we substituted
    HTML escapes, removed stock ticker symbols, replaced abbreviations like *mln*
    for *million*, and corrected some spelling mistakes. We also dropped the headlines
    because they are written in capital letters only. The complete article bodies
    are retained, though. All cleaning steps can be found in the notebook on [GitHub](https://oreil.ly/21p8d).
    Let’s take a look at a sample of the cleaned articles in our `DataFrame`:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们仅处理并购类别（acq）的文章。为了满足我们的目的，我们将所有文章加载到一个`DataFrame`中，并按照[“清理文本数据”](ch04.xhtml#ch4-cleaning)中的蓝图进行了一些数据清洗。干净的数据对于识别命名实体和关系至关重要，因为神经模型受益于结构良好的句子。对于这个数据集，我们替换了HTML转义字符，删除了股票代码符号，替换了诸如*mln*代表*million*的缩写，并纠正了一些拼写错误。我们也放弃了标题，因为它们仅以大写字母编写。但完整的文章内容仍保留下来。所有清洗步骤都可以在[GitHub](https://oreil.ly/21p8d)的笔记本中找到。让我们来看一下我们`DataFrame`中经过清理的文章样本：
- en: '[PRE1]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: So, this is the data we have in mind when we develop the blueprints for information
    extraction. However, most of the sentences in the following sections are simplified
    examples to better explain the concepts.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在我们制定信息提取蓝图时，这是我们心目中的数据。但是，以下各节中的大多数句子都是简化的示例，以更好地解释这些概念。
- en: Named-Entity Recognition
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 命名实体识别
- en: 'After data cleaning, we can start with the first step of our information extraction
    process: named-entity recognition. Named-entity recognition was briefly introduced
    in [Chapter 4](ch04.xhtml#ch-preparation) as part of spaCy’s standard pipeline.
    spaCy is our library of choice for all the blueprints in this chapter because
    it is fast and has an extensible API that we will utilize. But you could also
    use Stanza or Flair (see [“Alternatives for NER: Stanza and Flair”](#alts4ner)).'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据清洗后，我们可以开始我们信息提取过程的第一步：命名实体识别。命名实体识别在[第四章](ch04.xhtml#ch-preparation)中作为spaCy标准流水线的一部分进行了简要介绍。spaCy是我们在本章中所有蓝图的首选库，因为它快速且具有我们将利用的可扩展API。但您也可以使用Stanza或Flair（参见[“NER的替代方案：Stanza和Flair”](#alts4ner)）。
- en: spaCy provides trained NER models for many languages. The English models have
    been trained on the large [OntoNotes5 corpus](https://oreil.ly/gyOiH) containing
    18 different entity types. [Table 12-1](#tab-ner-types) lists a subset of these.
    The remaining types are for numeric entities.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy为许多语言提供了经过训练的NER模型。英语模型是在包含18种不同实体类型的大型[OntoNotes5语料库](https://oreil.ly/gyOiH)上训练的。[表12-1](#tab-ner-types)列出了这些类型的一个子集。其余类型适用于数值实体。
- en: Table 12-1\. Subset of NER types of the OntoNotes 5 corpus
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 表12-1. OntoNotes 5 语料库的部分NER类型
- en: '| NER Type | Description | NER Type | Description |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| NER 类型 | 描述 | NER 类型 | 描述 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| PERSON | People, including fictional | PRODUCT | Vehicles, weapons, foods,
    etc. (Not services) |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| PERSON | 人物，包括虚构的 | PRODUCT | 车辆、武器、食品等（不包括服务） |'
- en: '| NORP | Nationalities or religious or political groups | EVENT | Named hurricanes,
    battles, wars, sports events, etc. |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| NORP | 国籍或宗教或政治团体 | EVENT | 具名飓风、战役、战争、体育赛事等 |'
- en: '| FAC | Facilities: buildings, airports, highways, bridges, etc. | WORK_OF_ART
    | Titles of books, songs, etc. |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| FAC | 设施：建筑物、机场、高速公路、桥梁等 | WORK_OF_ART | 书籍、歌曲等的标题 |'
- en: '| ORG | Organizations: companies, agencies, institutions, etc. | LAW | Named
    documents made into laws |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| ORG | 组织：公司、机构等 | LAW | 公布为法律的具名文件 |'
- en: '| GPE | Countries, cities, states | LANGUAGE | Any named language |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| GPE | 国家、城市、州 | LANGUAGE | 任何具名语言 |'
- en: '| LOCATION | Non-GPE locations, mountain ranges, bodies of water |  |  |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| LOCATION | 非GPE位置、山脉、水体 |  |  |'
- en: The NER tagger is enabled by default when you load a language model. Let’s start
    by initializing an `nlp` object with the standard (small) English model `en_core_web_sm`
    and print the components of the NLP pipeline:^([4](ch12.xhtml#idm45634175830936))
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，加载语言模型时会启用NER标记器。我们首先通过使用标准（小型）英语模型 `en_core_web_sm` 初始化一个 `nlp` 对象，并打印NLP流水线的组件：^([4](ch12.xhtml#idm45634175830936))
- en: '[PRE2]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '`Out:`'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出：`'
- en: '[PRE3]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Once the text is processed, we can access the named entities directly with
    `doc.ents`. Each entity has a text and a label describing the entity type. These
    attributes are used in the last line in the following code to print the list of
    entities recognized in this text:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 处理文本后，我们可以直接通过 `doc.ents` 访问命名实体。每个实体都有一个文本和描述实体类型的标签。这些属性在下面代码的最后一行用于打印在文本中识别的实体列表：
- en: '[PRE4]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '`Out:`'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出：`'
- en: '[PRE5]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'With spaCy’s neat visualization module `displacy`, we can generate a visual
    representation of the sentence and its named entities. This is helpful to inspect
    the result:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 利用spaCy的漂亮的可视化模块 `displacy`，我们可以生成句子及其命名实体的视觉表示。这对检查结果非常有帮助：
- en: '[PRE6]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '`Out:`'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出：`'
- en: '![](Images/btap_12in01.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/btap_12in01.jpg)'
- en: In general, spaCy’s named-entity recognizer does a good job. In our example,
    it was able to detect all named entities. The labels of *Kistler* and *Baker*
    in the second and third sentence, however, are not correct. In fact, distinguishing
    between persons and organizations is quite a challenge for NER models because
    those entity types are used very similarly. We will resolve such problems later
    in the blueprint for name-based coreference resolution.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 总体来说，spaCy的命名实体识别器表现很好。在我们的例子中，它能够检测到所有命名实体。然而，第二句和第三句中 *Kistler* 和 *Baker*
    的标签并不正确。事实上，对于NER模型来说，区分人物和组织是一个挑战，因为这些实体类型的使用方式非常相似。我们将在后面的蓝图中解决这类问题，以进行基于名称的共指消解。
- en: 'Blueprint: Using Rule-Based Named-Entity Recognition'
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 蓝图：使用基于规则的命名实体识别
- en: If you want to identify domain-specific entities on which the model has not
    been trained, you can of course train your own model with [spaCy](https://oreil.ly/6EMig).
    But training a model requires a lot of training data. Often it is sufficient to
    specify simple rules for custom entity types. In this section, we will show how
    to use rules to detect government organizations like the “Department of Justice”
    (or alternatively the “Justice Department”) in the Reuters dataset.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您希望识别模型未经训练的领域特定实体，您当然可以使用 [spaCy](https://oreil.ly/6EMig) 自行训练您的模型。但训练模型需要大量的训练数据。通常，为自定义实体类型指定简单规则就足够了。在本节中，我们将展示如何使用规则来检测像“司法部”（或者“Justice
    Department”）这样的政府组织在Reuters数据集中的方法。
- en: spaCy provides an [`EntityRuler`](https://oreil.ly/A6MZ8) for this purpose,
    a pipeline component that can be used in combination with or instead of the statistical
    named-entity recognizer. Compared to regular expression search, spaCy’s matching
    engine is more powerful because patterns are defined on sequences of spaCy’s tokens
    instead of just strings. Thus, you can use any token property like the lemma or
    the part-of-speech tag to build your patterns.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy为此提供了一个[`EntityRuler`](https://oreil.ly/A6MZ8)，这是一个流水线组件，可以与或者代替统计命名实体识别器一起使用。与正则表达式搜索相比，spaCy的匹配引擎更强大，因为模式是在spaCy的标记序列上定义的，而不仅仅是字符串。因此，您可以使用任何标记属性，如词形或词性标签来构建您的模式。
- en: 'So, let’s define some pattern rules to match departments of the US government
    and the “Securities and Exchange Commission,” which is frequently mentioned in
    our corpus:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，让我们定义一些模式规则，以匹配美国政府的部门和经常在我们的语料库中提到的`证券交易委员会`：
- en: '[PRE7]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Each rule consists of a dictionary with a label, in our case the custom entity
    type `GOV`, and a pattern that the token sequence must match. You can specify
    multiple rules for the same label, as we did here.^([5](ch12.xhtml#idm45634175637736))
    The first rule, for example, matches sequences of tokens with the texts `"U.S."`
    (optional, denoted by `"OP": "?"`), `"Department"`, `"of"`, and either `"Justice"`
    or `"Transportation"`. Note that this and the second rule refine already recognized
    entities of type `ORG`. Thus, these patterns must be applied on top and not instead
    of spaCy’s named-entity model.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '每条规则由一个带有标签的字典组成，在我们的案例中是自定义实体类型`GOV`，以及令牌序列必须匹配的模式。您可以为同一标签指定多个规则，就像我们在这里所做的一样。^([5](ch12.xhtml#idm45634175637736))
    例如，第一条规则匹配带有文本`"U.S."`（可选，用`"OP": "?"`表示）、`"Department"`、`"of"`和`"Justice"`或`"Transportation"`的令牌序列。请注意，这些规则会对已识别出的类型`ORG`的实体进行进一步的细化。因此，这些模式必须在spaCy的命名实体模型之上而不是代替它应用。'
- en: 'Based on these patterns, we create an `EntityRuler` and add it to our pipeline:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这些模式，我们创建了一个`EntityRuler`并将其添加到我们的流水线中：
- en: '[PRE8]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now, when we call `nlp`, those organizations will automatically be labeled
    with the new type `GOV`:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，当我们调用`nlp`时，这些组织将自动用新类型`GOV`标记：
- en: '[PRE9]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '`Out:`'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出：`'
- en: '![](Images/btap_12in02.jpg)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/btap_12in02.jpg)'
- en: 'Blueprint: Normalizing Named Entities'
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 蓝图：规范化命名实体
- en: 'One approach to simplify the resolution of different entity mentions to a single
    name is the normalization or standardization of mentions. Here, we will do a first
    normalization, which is generally helpful: the removal of unspecific suffixes
    and prefixes. Take a look at this example:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 简化不同实体提及到单一名称的解析的一种方法是规范化或标准化提及。在这里，我们将进行第一次规范化，这通常是有帮助的：移除不具体的后缀和前缀。看看这个例子：
- en: '[PRE10]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '`Out:`'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出：`'
- en: '[PRE11]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: In the first sentence, the token sequence `Baker International's` was detected
    as an entity even though the genitive-s is not part of the company name. A similar
    case is the article in `the New York Stock Exchange`. Regardless of whether the
    article is actually part of the name or not, entities will likely be referenced
    sometimes with and sometimes without the article. Thus, the general removal of
    the article and an apostrophe-s simplifies the linking of mentions.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一句中，尽管所有格-s不是公司名称的一部分，令牌序列`Baker International's`被检测为一个实体。类似的情况是`《纽约证券交易所》`中的文章。无论文章实际上是否是名称的一部分，实体有时会在提及时带有文章，有时则没有。因此，通常移除文章和所有格-apostrophe-s简化了提及的链接。
- en: Warning
  id: totrans-71
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: 'As with any rules, there is a potential of errors: think of `The Wall Street
    Journal` or `McDonald''s`. If you need to preserve the article or the apostrophe-s
    in such cases, you must define exceptions for the rules.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 如同任何规则一样，存在着错误的可能性：想象一下`《华尔街日报》`或`麦当劳`。如果你需要保留这些情况下的冠词或者所有格-apostrophe，你必须为规则定义异常。
- en: 'Our blueprint function shows how to implement normalizations such as removing
    a leading article and a trailing apostrophe-s in spaCy. As we are not allowed
    to update entities in place, we create a copy of the entities and apply our modifications
    to this copy:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的蓝图函数展示了如何在spaCy中实现诸如移除前导冠词和尾随所有格-apostrophe-s等规范化。由于我们不允许直接更新实体，我们创建了实体的副本，并将修改应用于该副本：
- en: '[PRE12]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: An entity in spaCy is a `Span` object with a defined start and end plus an additional
    label denoting the type of the entity. We loop through the entities and adjust
    the position of the first and last token of the entity if necessary. Finally,
    we replace `doc.ents` with our modified copy.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在 spaCy 中，实体是具有定义的开始和结束以及额外标签的`Span`对象。我们循环遍历实体，并根据需要调整实体的第一个和最后一个标记的位置。最后，我们用修改后的副本替换`doc.ents`。
- en: 'The function takes a spaCy `Doc` object (named `doc`) as a parameter and returns
    a `Doc`. Therefore, we can use it as a another pipeline component and simply add
    it to the existing pipeline:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数以一个 spaCy 的`Doc`对象（命名为`doc`）作为参数，并返回一个`Doc`。因此，我们可以将其用作另一个管道组件，简单地将其添加到现有管道中：
- en: '[PRE13]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Now we can repeat the process on the example sentences and check the result:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以对示例句子重复这个过程，并检查结果：
- en: '[PRE14]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '`Out:`'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE15]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Merging Entity Tokens
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 合并实体标记
- en: 'In many cases, it makes sense to treat compound names like the ones from the
    previous example as single tokens because it simplifies the sentence structure.
    spaCy provides a built-in pipeline function `merge_entities` for that purpose.
    We add it to our NLP pipeline and get exactly one token per named-entity:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，将像前面示例中的复合名称视为单个标记是有意义的，因为它简化了句子结构。spaCy 提供了一个内置的管道函数`merge_entities`来实现这一目的。我们将其添加到我们的
    NLP 管道中，确保每个命名实体正好只有一个标记：
- en: '[PRE16]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '`Out:`'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE17]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Even though merging entities simplifies our blueprints later in this chapter,
    it may not always be a good idea. Think, for example, about compound entity names
    like `London Stock Exchange`. After merging into a single token, the implicit
    relation of this entity to the city of London will be lost.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 即使合并实体在本章后期简化了我们的蓝图，这并不总是一个好主意。例如，考虑像`伦敦证券交易所`这样的复合实体名称。将其合并为单个标记后，这个实体与伦敦市的隐含关系将会丢失。
- en: Coreference Resolution
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 共指消解
- en: 'One of the greatest obstacles in information extraction is the fact that entity
    mentions appear in many different spellings (also called *surface forms*). Look
    at the following sentences:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在信息提取中最大的障碍之一是实体提及出现在许多不同的拼写形式中（也称为*表面形式*）。看看以下句子：
- en: Hughes Tool Co Chairman W.A. Kistler said its merger with Baker International
    Corp. was still under consideration. We hope to come to a mutual agreement, Kistler
    said. Baker will force Hughes to complete the merger. A review by the U.S. Department
    of Justice was completed today. The Justice Department will block the merger after
    consultation with the SEC.
  id: totrans-90
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 休斯工具公司主席W.A.基斯勒表示其与贝克国际公司的合并仍在考虑中。基斯勒表示希望达成一致意见。贝克将迫使休斯完成合并。美国司法部的审查今天已经完成。司法部将在与证券交易委员会磋商后阻止这一合并。
- en: As we can see, entities are frequently introduced by their full name, while
    later mentions use abbreviated versions. This is one type of coreference that
    must resolved to understand what’s going on. [Figure 12-3](#fig-cooc-baker-hughes)
    shows a co-occurrence graph without (left) and with (right) unified names. Such
    a co-occurrence graph, as we will build in the next section, is a visualization
    of entity pairs appearing in the same article.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，实体通常以其全名引入，而后续提及则使用缩写版本。这是必须解决的一种共指类型，以理解正在发生的情况。[图 12-3](#fig-cooc-baker-hughes)
    显示了一个无（左）和有（右）统一名称的共现图。这样的共现图将在下一节中构建，是显示出现在同一文章中的实体对的可视化。
- en: '![](Images/btap_1203.jpg)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/btap_1203.jpg)'
- en: Figure 12-3\. A co-occurrence graph of the same articles before (left) and after
    coreference resolution (right).
  id: totrans-93
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 12-3\. 同一文章的共现图在核心引用解析前（左）和后（右）的对比。
- en: '*Coreference resolution* is the task of determining the different mentions
    of an entity within a single text, for example, abbreviated names, aliases, or
    pronouns. The result of this step is a group of coreferencing mentions called
    a *mention cluster*, for example, `{Hughes Tool Co, Hughes, its}`. Our target
    in this section is to identify related mentions and link them within a document.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '*共指消解* 是确定单个文本中实体不同提及的任务，例如缩写名称、别名或代词。这一步骤的结果是一组共指提及，称为*提及簇*，例如 `{休斯工具公司, 休斯,
    其}`。本节的目标是识别相关的提及并在文档中进行链接。'
- en: For this purpose, we develop a couple of blueprints for coreference resolution
    and name unification (see [Figure 12-4](#fig-coref-pipeline)). We will restrict
    ourselves to organizations and persons, as these are the entity types we are interested
    in. First, we will resolve aliases like *SEC* by a dictionary lookup. Then we
    will match names within a document to the first mention. For example, we will
    create a link from “Kistler” to “W.A. Kistler.” After that, indirect references
    (*anaphora*) like the pronoun *its* in the first sentence will be resolved. Finally,
    we will normalize again the names of the resolved entities. All of these steps
    will be implemented as additional pipeline functions.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 为了这个目的，我们为指代消解和名称统一开发了几个蓝图（见[图 12-4](#fig-coref-pipeline)）。我们将限制自己只处理组织和个人，因为这些是我们感兴趣的实体类型。首先，我们将通过字典查找解析像*SEC*这样的别名。然后我们将在文档中匹配名称到第一次提及。例如，我们将从“Kistler”创建到“W.A.
    Kistler”的链接。之后，间接指代（*回指*）如第一句中的代词*its*将被解决。最后，我们将再次规范化已解析实体的名称。所有这些步骤将作为附加的管道函数实现。
- en: '![](Images/btap_1204.jpg)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/btap_1204.jpg)'
- en: Figure 12-4\. Pipeline for named-entity recognition and coreference resolution.
  id: totrans-97
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-4\. 命名实体识别和指代消解的流程图。
- en: '*Entity linking* goes one step further. Here the mentions of an entity are
    disambiguated on a semantic level and linked to a unique entry in an existing
    knowledge base. Because entity linking is itself a challenging task, we will not
    provide a blueprint for that but just discuss it at the end of this section.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '*实体链接*更进一步。这里的实体提及在语义级别上被消歧，并链接到现有知识库中的唯一条目。因为实体链接本身是一项具有挑战性的任务，我们不会提供其蓝图，而只是在本节末讨论它。'
- en: 'Blueprint: Using spaCy’s Token Extensions'
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 蓝图：使用spaCy的标记扩展
- en: 'We need a way to technically create the link from the different mentions of
    an entity to the main reference, the *referent*. After coreference resolution,
    the token for “Kistler” of the example article should point to “(W.A. Kistler,
    PERSON).” spaCy’s extension mechanism allows us to define custom attributes, and
    this is the perfect way to store this kind of information with tokens. Thus, we
    create two token extensions `ref_n` (referent’s name) and `ref_t` (referent’s
    type). The attributes will be initialized for each token with the specified default
    values by spaCy for each token:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要一种技术上的方法，从不同实体的各个提及创建到主参照（*referent*）的链接。在核心ference解决后，例如文章示例中的“Kistler”的标记应指向“(W.A.
    Kistler, PERSON)”。spaCy的扩展机制允许我们定义自定义属性，这是将此类信息与标记一起存储的完美方法。因此，我们创建了两个标记扩展`ref_n`（参照名称）和`ref_t`（参照类型）。这些属性将为每个标记初始化为spaCy指定的默认值：
- en: '[PRE18]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The function `init_coref` shown next ensures that each entity mention of type
    `ORG`, `GOV`, and `PERSON` gets an initial reference to itself. This initialization
    is required for the succeeding functions:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个展示的`init_coref`函数确保每个类型为`ORG`、`GOV`和`PERSON`的实体提供一个初始参照。这种初始化对于接下来的功能是必需的：
- en: '[PRE19]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The custom attributes are accessed via the underscore property of the token.
    Note that after `merge_entities`, each entity mention `e` consists of a single
    token `e[0]` where we set the attributes. We could also define the attributes
    on the entity spans instead of tokens, but we want to use the same mechanism for
    pronoun resolution later.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 自定义属性通过标记的下划线属性访问。请注意，在`merge_entities`之后，每个实体提及`e`由一个单一标记`e[0]`组成，我们在其中设置了这些属性。我们也可以在实体跨度而不是标记上定义这些属性，但我们希望稍后对代词解析使用相同的机制。
- en: 'Blueprint: Performing Alias Resolution'
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 蓝图：执行别名解析
- en: 'Our first targets are well-known domain aliases like *Transportation Department*
    for “U.S. Department of Transportation” and acronyms like SEC or TWA. A simple
    solution to resolve such aliases is to use a lookup dictionary. We prepared such
    a dictionary for all the acronyms and some common aliases of the Reuters corpus
    and provided it as part of the blueprints module for this chapter.^([6](ch12.xhtml#idm45634174792232))
    Here are some example lookups:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先的目标是解决众所周知的领域别名，比如*Transportation Department*代表“美国交通部”，以及像SEC或TWA这样的缩写词。解决这类别名的简单方法是使用查找字典。我们为Reuters语料库中的所有缩写词和一些常见别名准备了这样一个字典，并将其作为本章蓝图模块的一部分提供。^([6](ch12.xhtml#idm45634174792232))
    这里是一些示例查找：
- en: '[PRE20]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '`Out:`'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE21]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Each token alias is mapped to a tuple consisting of an entity name and a type.
    The function `alias_resolver` shown next checks whether an entity’s text is found
    in the dictionary. If so, its `ref` attributes are updated to the looked-up value:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 每个令牌别名都映射到一个元组，包括实体名称和类型。下面显示的函数`alias_resolver`检查实体文本是否在字典中找到。如果是，将更新其`ref`属性为查找到的值：
- en: '[PRE22]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Once we have resolved the aliases, we can also correct the type of the named-entity
    in case it was misidentified. This is done by the function `propagate_ent_type`.
    It updates all resolved aliases and will also be used in the next blueprint for
    name-based coreference resolution:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 解决了别名后，我们还可以纠正命名实体类型，以防其被错误标识。这是通过函数`propagate_ent_type`完成的。它更新所有已解析的别名，并将在下一个基于名称的指代消解蓝图中使用：
- en: '[PRE23]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Again, we add the `alias_resolver` to our pipeline:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将`alias_resolver`添加到我们的流水线中：
- en: '[PRE24]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Now we can inspect the results. For this purpose, our provided blueprints package
    includes a utility function `display_ner` that creates a `DataFrame` for the tokens
    in a `doc` object with the relevant attributes for this chapter:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以检查结果。为此，我们提供的蓝图包含一个实用函数`display_ner`，用于为`doc`对象中的标记创建一个`DataFrame`，并包括本章相关属性：
- en: '[PRE25]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '`Out:`'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出：`'
- en: '|  | text | ent_type | ref_n | ref_t |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '|  | 文本 | 实体类型 | 参考编号 | 参考类型 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 3 | Trans World Airlines | ORG | Trans World Airlines Inc | ORG |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 美国国际航空公司 | ORG | 美国国际航空公司 | ORG |'
- en: '| 9 | U.S. Department of Transportation | GOV | U.S. Department of Transportation
    | GOV |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| 9 | 美国交通部 | GOV | 美国交通部 | GOV |'
- en: '| 12 | Transportation Department | GOV | U.S. Department of Transportation
    | GOV |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| 12 | 交通部 | GOV | 美国交通部 | GOV |'
- en: '| 18 | TWA | ORG | Trans World Airlines Inc | ORG |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| 18 | TWA | ORG | 美国国际航空公司 | ORG |'
- en: 'Blueprint: Resolving Name Variations'
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 蓝图：解决名称变体
- en: 'Alias resolution works only if the aliases are known up front. But because
    articles contain variations of almost any names, it is not feasible to build a
    dictionary for all of them. Take a look again at the recognized named entities
    in the first sentences of our introductory example:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 别名解析仅在别名在前期已知的情况下有效。但是由于文章中几乎任何名称都可能存在变体，因此构建所有这些名称的词典是不可行的。再次看一下我们介绍示例中第一句中识别的命名实体：
- en: '![](Images/btap_12in03.jpg)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/btap_12in03.jpg)'
- en: Here you find the coreference “Kistler” for W.A. Kistler (`PERSON`), “Baker”
    for Baker International Corp (`ORG`), and “Hughes” for Hughes Tool Co (`ORG`).
    And as you can see, abbreviated company names are often mistaken for people, especially
    when they are used in impersonated form, as in the examples. In this blueprint,
    we will resolve those coreferences and assign the correct entity types to each
    mention.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，您会找到“Kistler”的指代“W.A. Kistler（`PERSON`）”，“Baker”的指代“Baker International
    Corp（`ORG`）”，以及“休斯”的指代“休斯工具公司（`ORG`）”。正如您所看到的，缩写的公司名称经常被误认为是人物，特别是在以模拟形式使用时。在这个蓝图中，我们将解决这些指代，并为每个提及分配正确的实体类型。
- en: For that, we will exploit a common pattern in news articles. An entity is usually
    introduced first by its full name, while later mentions use abbreviated versions.
    Thus, we will resolve the secondary references by matching the names to the first
    mention of an entity. Of course, this is a heuristic rule that could produce false
    matches. For example, *Hughes* could also refer in the same article to the company
    and to the legendary entrepreneur Howard Hughes (who indeed founded Hughes Tool
    Co.). But such cases are rare in our dataset, and we decide to accept that uncertainty
    in favor of the many cases where our heuristics works correctly.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们将利用新闻文章中的一个常见模式。实体通常首先以其全名介绍，后续提及使用缩写版本。因此，我们将通过将名称与实体的第一次提及匹配来解决次要引用。当然，这是一个启发式规则，可能会产生错误的匹配。例如，*休斯*也可能指同一篇文章中的公司以及传奇企业家霍华德·休斯（确实是休斯工具公司的创始人）。但这类情况在我们的数据集中很少见，我们决定在正确的启发式案例中接受这种不确定性。
- en: 'We define a simple rule for name matching: a secondary mention matches a primary
    mention if all of its words appear in the primary mention in the same order. To
    check this, the function `name_match` shown next transforms a secondary mention
    `m2` into a regular expression and searches for a match in the primary mention
    `m1`:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为名称匹配定义了一个简单的规则：如果所有单词按相同顺序出现在主要提及中，次要提及就匹配主要提及。为了检查这一点，下一个显示的函数`name_match`将次要提及`m2`转换为正则表达式，并在主要提及`m1`中搜索匹配项：
- en: '[PRE26]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The secondary mention of Hughes Co., for example, would be converted into `'\bHughes\b.*\bCo\b'`,
    which matches Hughes Tool Co. The `\b` ensures that only whole words match and
    not subwords like *Hugh*.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，Hughes Co.的次要提及会被转换为`'\bHughes\b.*\bCo\b'`，这与Hughes Tool Co匹配。`\b`确保只匹配整个单词，而不是子词如*Hugh*。
- en: 'Based on this matching logic, the function `name_resolver` shown next implements
    the name-based coreference resolution for organizations and persons:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 基于此匹配逻辑，下面展示的`name_resolver`函数实现了基于名称的组织和个人共指解析：
- en: '[PRE27]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: First, we create a list of all organization and person entities. Then all pairs
    of entities `e1` and `e2` are compared against each other. The logic ensures that
    entity `e1` always comes before `e2` in the document. If `e2` matches `e1`, its
    referent is set to the same as in `e1`. Thus, the first matching entity is automatically
    propagated to its subsequent coreferences.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们创建一个所有组织和个人实体的列表。然后，将实体`e1`和`e2`的所有对比较。该逻辑确保实体`e1`在文档中始终出现在`e2`之前。如果`e2`匹配`e1`，其指示物将自动传播到其后续共指。
- en: 'We add this function to the `nlp` pipeline and check the result:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将此函数添加到`nlp`流程中，并检查结果：
- en: '[PRE28]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '`Out:`'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '![](Images/btap_12in04.jpg)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/btap_12in04.jpg)'
- en: 'Now each named-entity in our example has the correct type. We can also check
    that the entities are mapped to their first mention:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的示例中每个命名实体都具有正确的类型。我们还可以检查实体是否映射到其第一次提及：
- en: '[PRE29]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '`Out:`'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '|   | text | ent_type | ref_n | ref_t |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '|   | text | ent_type | ref_n | ref_t |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 0 | Hughes Tool Co | ORG | Hughes Tool Co | ORG |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| 0 | Hughes Tool Co | ORG | Hughes Tool Co | ORG |'
- en: '| 2 | W.A. Kistler | PERSON | W.A. Kistler | PERSON |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| 2 | W.A. Kistler | PERSON | W.A. Kistler | PERSON |'
- en: '| 7 | Baker International Corp. | ORG | Baker International Corp. | ORG |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| 7 | Baker International Corp. | ORG | Baker International Corp. | ORG |'
- en: '| 22 | Kistler | PERSON | W.A. Kistler | PERSON |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| 22 | Kistler | PERSON | W.A. Kistler | PERSON |'
- en: '| 25 | Baker | ORG | Baker International Corp. | ORG |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| 25 | Baker | ORG | Baker International Corp. | ORG |'
- en: '| 28 | Hughes | ORG | Hughes Tool Co | ORG |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| 28 | Hughes | ORG | Hughes Tool Co | ORG |'
- en: 'Blueprint: Performing Anaphora Resolution with NeuralCoref'
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 蓝图：使用NeuralCoref进行指代消解
- en: 'In linguistics, *anaphora* are words whose interpretation depends on the preceding
    text. Consider this variation of our example sentences:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在语言学中，*指代*是依赖于前文的词语。考虑我们示例句子的这种变化：
- en: '[PRE30]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Here *its*, *the company*, and *he* are anaphora. [NeuralCoref](https://oreil.ly/kQRhE)
    from Hugging Face is a library to resolve these kind of coreferences. The algorithm
    uses feature vectors based on word embeddings (see [Chapter 10](ch10.xhtml#ch-embeddings))
    in combination with two neural networks to identify coreference clusters and their
    main mentions.^([7](ch12.xhtml#idm45634173992248))
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的*其*、*公司*和*他*是指代词。来自Hugging Face的[NeuralCoref](https://oreil.ly/kQRhE)是一个解决这类共指的库。该算法结合基于词嵌入的特征向量（参见[第10章](ch10.xhtml#ch-embeddings)），使用两个神经网络识别共指簇及其主要提及物。
- en: 'NeuralCoref is implemented as a pipeline extension for spaCy, so it fits perfectly
    into our process. We create the neural coreference resolver with a `greedyness`
    value of 0.45 and add it to our pipeline. The `greedyness` controls the sensitivity
    of the model, and after some experiments, we decided to choose a little more restrictive
    (better accuracy, lower recall) value than the default 0.5:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: NeuralCoref作为spaCy的流水线扩展实现，因此完美地适合我们的流程。我们使用`greedyness`值为0.45创建神经共指解析器，并将其添加到我们的流水线中。`greedyness`控制模型的敏感性，在一些实验后，我们决定选择比默认值0.5稍微严格一些的值（更高的准确性，较低的召回率）：
- en: '[PRE31]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'NeuralCoref uses also spaCy’s extension mechanism to add custom attributes
    to `Doc`, `Span`, and `Token` objects. When a text is processed, we can access
    the detected coreference clusters with the `doc._.coref_clusters` attribute. In
    our example, three such clusters have been identified:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: NeuralCoref还利用spaCy的扩展机制向`Doc`、`Span`和`Token`对象添加自定义属性。处理文本时，我们可以通过`doc._.coref_clusters`属性访问检测到的共指簇。在我们的示例中，已经识别出三个这样的簇：
- en: '[PRE32]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '`Out:`'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE33]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'NeuralCoref works on `Span` objects (sequences of token) because coreferences
    in general are not limited to named entities. Thus, the blueprint function `anaphor_coref`
    retrieves for each token the first coreference cluster and searches for the first
    named-entity with a value in its `ref_n` attribute. In our case, this will be
    organizations and people only. Once found, it sets the values in `ref_n` and `ref_t`
    of the anaphor token to the same values as in the primary reference:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: NeuralCoref 在 `Span` 对象（令牌序列）上工作，因为一般的共指不仅限于命名实体。因此，蓝图函数 `anaphor_coref` 为每个令牌检索第一个共指集群，并搜索具有其
    `ref_n` 属性值的第一个命名实体。在我们的案例中，这只会是组织和人物。一旦找到，它将把代词令牌的 `ref_n` 和 `ref_t` 值设置为主参考中的相同值：
- en: '[PRE34]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Again, we add this resolver to our pipeline and check the result:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们将这个解析器加入到我们的流水线中并检查结果：
- en: '[PRE35]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '`Out:`'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '|  | text | ent_type | main_coref | ref_n | ref_t |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '|  | 文本 | 实体类型 | 主共指 | ref_n | ref_t |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| 0 | Hughes Tool Co | ORG | Hughes Tool Co | Hughes Tool Co | ORG |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| 0 | Hughes Tool Co | 组织 | Hughes Tool Co | Hughes Tool Co | 组织 |'
- en: '| 2 | its |  | Hughes Tool Co | Hughes Tool Co | ORG |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 其 |  | Hughes Tool Co | Hughes Tool Co | 组织 |'
- en: '| 5 | Baker | PERSON | None | Baker | PERSON |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| 5 | Baker | 人物 | None | Baker | 人物 |'
- en: '| 11 | Hughes | ORG | Hughes | Hughes Tool Co | ORG |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| 11 | Hughes | 组织 | Hughes | Hughes Tool Co | 组织 |'
- en: '| 18 | W.A. Kistler | PERSON | W.A. Kistler | W.A. Kistler | PERSON |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| 18 | W.A. Kistler | 人物 | W.A. Kistler | W.A. Kistler | 人物 |'
- en: '| 21 | the |  | Hughes | Hughes Tool Co | ORG |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| 21 | the |  | Hughes | Hughes Tool Co | 组织 |'
- en: '| 22 | company |  | Hughes | Hughes Tool Co | ORG |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| 22 | 公司 |  | Hughes | Hughes Tool Co | 组织 |'
- en: '| 29 | He |  | W.A. Kistler | W.A. Kistler | PERSON |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| 29 | He |  | W.A. Kistler | W.A. Kistler | 人物 |'
- en: Now our pipeline consists of all the steps shown in [Figure 12-4](#fig-coref-pipeline).
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的流水线包括图示 [12-4](#fig-coref-pipeline) 中显示的所有步骤。
- en: Warning
  id: totrans-177
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Beware of long runtimes! NeuralCoref increases the total processing time by
    a factor of 5–10\. So, you should use anaphora resolution only if necessary.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 警惕长时间运行时间！NeuralCoref 将总体处理时间增加了 5–10 倍。因此，您应该仅在必要时使用指代消解。
- en: Name Normalization
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 名称规范化
- en: Even though our name resolution unifies company mentions within an article,
    the company names are still inconsistent across articles. We find *Hughes Tool
    Co.* in one article and *Hughes Tool* in another one. An entity linker can be
    used to link different entity mentions to a unique canonical representation, but
    in absence of an entity linker we will use the (resolved) name entity as its unique
    identifier. Because of the previous steps for coreference resolution, the resolved
    names are always the first, and thus usually most complete, mentions in an article.
    So, the potential for errors is not that large.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们的名称解析在文章中统一了公司提及，但是公司名称在文章之间仍然不一致。在一篇文章中我们会看到 *Hughes Tool Co.*，而在另一篇文章中我们会看到
    *Hughes Tool*。实体链接器可以用来将不同的实体提及链接到唯一的规范表示，但在没有实体链接器的情况下，我们将使用（解析后的）名称实体作为其唯一标识符。由于前面的共指解析步骤，解析后的名称总是文章中第一个，因此通常也是最完整的提及。因此，错误的可能性并不大。
- en: 'Still, we have to harmonize company mentions by removing the legal suffixes
    like *Co.* or *Inc.* from company names. The following function uses a regular
    expression to achieve this:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 不过，我们必须通过去除诸如 *Co.* 或 *Inc.* 这样的法律后缀来协调公司提及。以下函数使用正则表达式来实现此目标：
- en: '[PRE36]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '`Out:`'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE37]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: The last pipeline function `norm_names` applies this final normalization to
    each of the coreference-resolved organization names stored in the `ref_n` attributes.
    Note that `Hughes (PERSON)` and `Hughes (ORG)` will still remain separate entities
    with this approach.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 最后的流水线函数 `norm_names` 将最终的规范化应用于存储在 `ref_n` 属性中的每个共指解析后的组织名称。请注意，使用这种方法，`Hughes
    (人物)` 和 `Hughes (组织)` 仍然会保持分开的实体。
- en: '[PRE38]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Sometimes the named-entity recognizer misclassifies a legal suffix like *Co.*
    or *Inc.* by itself as named-entity. If such an entity name gets stripped to the
    empty string, we just ignore it for later processing.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，命名实体识别器会错误地将法律后缀（例如 *Co.* 或 *Inc.*）单独分类为命名实体。如果这样的实体名称被剥离成空字符串，我们只需忽略它以便稍后处理。
- en: Entity Linking
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实体链接
- en: In the previous sections we developed a pipeline of operations with the purpose
    of unifying the different mentions of named entities. But all this is string based,
    and except for the syntactical representation, we have no connection between the
    string *U.S. Department of Justice* and the represented real-world entity. The
    task of an entity linker, in contrast, is to resolve named entities globally and
    link them to a uniquely identified real-world entity. Entity linking makes the
    step from “strings to things.”^([8](ch12.xhtml#idm45634173497096))
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们开发了一个操作流程，其目的是统一命名实体的不同提及。但所有这些都是基于字符串的，除了语法表示之外，我们没有将“美国司法部”这样的字符串与所代表的现实世界实体联系起来。相比之下，实体链接器的任务是全局解析命名实体，并将它们链接到唯一标识的现实世界实体。实体链接从“字符串到实体”的转换。^([8](ch12.xhtml#idm45634173497096))
- en: Technically, this means that each mention is mapped to a URI. URIs, in turn,
    address entities in an existing knowledge base. This can be a public ontology,
    like Wikidata or DBpedia, or a private knowledge base in your company. URIs can
    be URLs (e.g., web pages) but do not have to be. The U.S. Department of Justice,
    for example, has the Wikidata URI [*http://www.wikidata.org/entity/Q1553390*](http://www.wikidata.org/entity/Q1553390),
    which is also a web page where you find information about this entity. If you
    build your own knowledge base, it is not necessary to have a web page for each
    URI; they just must be unique. DBpedia and Wikidata, by the way, use different
    URIs, but you will find the Wikidata URI on DBpedia as a cross-reference. Both,
    of course, contain links to the Wikipedia web page.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 从技术上讲，这意味着每个提及都映射到一个URI。URI又可以用来指代现有知识库中的实体。这可以是公共本体，例如Wikidata或DBpedia，也可以是公司内部的私有知识库。URI可以是URL（例如网页），但不一定要是。例如，美国司法部在Wikidata有一个URI
    [*http://www.wikidata.org/entity/Q1553390*](http://www.wikidata.org/entity/Q1553390)，这也是一个包含有关该实体信息的网页。如果您构建自己的知识库，则不需要为每个URI创建网页；它们只需要是唯一的。顺便说一下，DBpedia和Wikidata使用不同的URI，但您将在DBpedia上找到对Wikidata
    URI的交叉引用。两者当然都包含指向维基百科网页的链接。
- en: Entity linking is simple if an entity is mentioned by a fully qualified name,
    like the *U.S. Department of Justice*. But the term *Department of Justice* without
    *U.S.* is already quite ambiguous because many states have a “Department of Justice.”
    The actual meaning depends on the context, and the task of an entity linker is
    to map such an ambiguous mention context-sensitively to the correct URI. This
    is quite a challenge and still an area of ongoing research. A common solution
    for entity linking in business projects is the usage of a public service (see
    [“Services for Entity Linking”](#services-entity-linking)).
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个实体通过完全限定名称（如“美国司法部”）提及，实体链接就很简单。但是，“司法部”而没有“美国”就已经相当模糊，因为许多州都有“司法部”。实际意义取决于上下文，实体链接器的任务是根据上下文敏感地将这种模糊提及映射到正确的URI。这是一个相当大的挑战，仍然是持续研究的领域。在商业项目中进行实体链接的常见解决方案是使用公共服务（参见[“实体链接服务”](#services-entity-linking)）。
- en: Alternatively, you could create your own entity linker. A simple solution would
    be a name-based lookup dictionary. But that does not take the context into account
    and would not resolve ambiguous names for different entities. For that, you need
    a more sophisticated approach. State-of-the-art solutions use embeddings and neural
    models for entity linking. spaCy also provides such an [entity linking functionality](https://oreil.ly/bqs8E).
    To use spaCy’s entity linker, you first have to create embeddings (see [Chapter 10](ch10.xhtml#ch-embeddings))
    for the real-world entities, which capture their semantics based on descriptions
    you specify. Then you can train a model to learn the context-sensitive mapping
    of mentions to the correct URI. The setup and training of an entity linker are,
    however, beyond the scope of this chapter.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，您可以创建自己的实体链接器。一个简单的解决方案是基于名称的查找字典。但这种方法不考虑上下文，并且无法解决不同实体的名称歧义。为此，您需要更复杂的方法。最先进的解决方案使用嵌入和神经模型进行实体链接。例如，spaCy还提供了这样的[实体链接功能](https://oreil.ly/bqs8E)。要使用spaCy的实体链接器，首先必须为您指定的描述创建嵌入（参见[第10章](ch10.xhtml#ch-embeddings)），从而捕获其语义。然后，您可以训练模型，学习将提及映射到正确URI的上下文敏感映射。然而，实体链接器的设置和训练超出了本章的范围。
- en: 'Blueprint: Creating a Co-Occurrence Graph'
  id: totrans-193
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 蓝图：创建共现图
- en: 'In the previous sections, we spent much effort to normalize named entities
    and to resolve at least the in-document coreferences. Now we are finally ready
    to analyze a first relationship among pairs of entities: their joint mention in
    an article. For this, we will create a co-occurrence graph, the simplest form
    of a knowledge graph. The nodes in the co-occurrence graph are the entities, e.g.,
    organizations. Two entities share an (undirected) edge if they are mentioned in
    the same context, for example, within an article, a paragraph, or a sentence.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的部分，我们花了很多精力来规范命名实体并至少解析文档内的核心参考。现在我们终于准备好分析实体对之间的第一个关系了：它们在文章中的共同提及。为此，我们将创建一个共现图，这是知识图的最简形式。共现图中的节点是实体，例如组织。如果两个实体在相同的上下文中提及，例如在一篇文章、一个段落或一个句子中，它们之间就会共享一个（无向）边。
- en: '[Figure 12-5](#fig-cooc) shows part of the co-occurrence graph for companies
    mentioned together in articles of the Reuters corpus. The width of the edges visualizes
    the co-occurrence frequency. The [*modularity*](https://oreil.ly/pGZ-s), a structural
    measure to identify closely related groups or communities in a network, was used
    to colorize the nodes and edges.^([9](ch12.xhtml#idm45634173458648))'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 12-5](#fig-cooc)显示了路透社语料库中一起提及的公司的共现图的部分。边的宽度可视化了共现频率。[*模块性*](https://oreil.ly/pGZ-s)，这是一种用于识别网络中紧密相关的群体或社群的结构性指标，被用来着色节点和边。^([9](ch12.xhtml#idm45634173458648))'
- en: '![](Images/btap_1205.jpg)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/btap_1205.jpg)'
- en: Figure 12-5\. Largest connected component of the co-occurrence graph generated
    from the Reuters corpus.
  id: totrans-197
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 12-5\. 从路透社语料库生成的共现图的最大连通分量。
- en: Of course, we don’t know anything about the type of relationship here. In fact,
    the joint mentioning of two entities merely indicates that there *might be some*
    relationship. We won’t know for sure unless we really analyze the sentences, and
    we will do that in the next section. But even the simple exploration of co-occurrences
    can already be revealing. For example, the central node in [Figure 12-5](#fig-cooc)
    is the “Securities and Exchange Commission” because it is mentioned in many articles
    together with a great variety of other entities. Obviously, this entity plays
    a major role in mergers and acquisitions. The different clusters give us an impression
    about groups of companies (or communities) involved in certain deals.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我们不知道这里的关系类型。实际上，两个实体的共同提及只是表明可能存在一些关系。除非我们真的分析句子，否则我们无法确定。但是，即使简单探索共现也可能有所启示。例如，[图 12-5](#fig-cooc)中的中心节点是“证券交易委员会”，因为它在许多文章中与许多其他实体一起提及。显然，该实体在并购中扮演重要角色。不同的集群给我们留下了一些关于涉及特定交易的公司（或社群）的印象。
- en: To plot a co-occurrence graph, we have to extract entity pairs from a document.
    For longer articles covering multiple topic areas, it may be better to search
    for co-occurrences within paragraphs or even sentences. But the Reuters articles
    on mergers and acquisitions are very focused, so we stick to the document level
    here. Let’s briefly walk through the process to extract and visualize co-occurrences.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 要绘制共现图，我们必须从文档中提取实体对。对于涵盖多个主题领域的较长文章，最好在段落甚至句子内搜索共现。但是涉及并购的路透社文章非常专注，所以我们在这里坚持使用文档级别。让我们简要地走一遍提取和可视化共现的过程。
- en: Extracting Co-Occurrences from a Document
  id: totrans-200
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从文档中提取共现
- en: 'The function `extract_coocs` returns the list of entities pairs of the specified
    types from a given `Doc` object:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 函数 `extract_coocs` 返回给定 `Doc` 对象中指定类型的实体对列表：
- en: '[PRE39]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: We first create a set of the coreference-resolved entity names and types. Having
    this, we use the function `combinations` from the Python standard library `itertools`
    to create all the entity pairs. Each pair is sorted lexicographically (`sorted(ents)`)
    to prevent duplicate entries like “(Baker, Hughes)” and “(Hughes, Baker).”
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先创建一个核心解析实体名称和类型的集合。有了这个，我们使用 Python 标准库 `itertools` 中的 `combinations` 函数来创建所有实体对。每对都按字典顺序排序（`sorted(ents)`），以防止重复条目，比如
    “(Baker, Hughes)” 和 “(Hughes, Baker)” 的出现。
- en: 'To process the whole dataset efficiently, we use again spaCy’s streaming by
    calling `nlp.pipe` (introduced in [Chapter 4](ch04.xhtml#ch-preparation)). As
    we do not need anaphora resolution to find in-document co-occurrences, we disable
    the respective components here:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 为了高效处理整个数据集，我们再次使用 spaCy 的流式处理，通过调用 `nlp.pipe`（在[第四章](ch04.xhtml#ch-preparation)介绍过）。由于我们不需要在文档中解析指代关系以找出文档内的共现，因此在这里禁用了相应的组件：
- en: '[PRE40]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Let’s take a look at the identified co-occurrences of the first article:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下第一篇文章识别出的共现：
- en: '[PRE41]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '`Out:`'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出：`'
- en: '[PRE42]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: In information extraction, it is always recommended to have some kind of traceability
    that allows you to identify the source of the information in the case of problems.
    Therefore, we retain the index of the article, which in our case is the file ID
    of the Reuters corpus, with each co-occurrence tuple (here the ID 10). Based on
    this list, we generate a `DataFrame` with exactly one entry per entity combination,
    its frequency, and the article IDs (limited to five) where this co-occurrence
    was found.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在信息提取中，始终建议具有某种可追溯性，以便在出现问题时识别信息的来源。因此，我们保留了文章的索引，这在我们的情况下是Reuters语料库的文件ID，与每个共现元组（这里是ID
    10）一起。根据这个列表，我们生成了一个`DataFrame`，每个实体组合有一个条目，其频率和找到这个共现的文章ID（限制为五个）。
- en: '[PRE43]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Here are the three most frequent entity pairs we found in the corpus:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是我们在语料库中发现的三对最频繁的实体：
- en: '[PRE44]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '`Out:`'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出：`'
- en: '|  | ent1 | type1 | ent2 | type2 | freq | articles |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '|  | ent1 | type1 | ent2 | type2 | freq | articles |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| 12667 | Trans World Airlines | ORG | USAir Group | ORG | 22 | 1735,1771,1836,1862,1996
    |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| 12667 | 美国世界航空公司 | ORG | USAir集团 | ORG | 22 | 1735,1771,1836,1862,1996 |'
- en: '| 5321 | Cyclops | ORG | Dixons Group | ORG | 21 | 4303,4933,6093,6402,7110
    |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| 5321 | 单眼巨人 | ORG | 迪克森斯集团 | ORG | 21 | 4303,4933,6093,6402,7110 |'
- en: '| 12731 | U.S. Department of Transportation | GOV | USAir Group | ORG | 20
    | 1735,1996,2128,2546,2799 |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| 12731 | 美国交通部 | GOV | USAir集团 | ORG | 20 | 1735,1996,2128,2546,2799 |'
- en: Visualizing the Graph with Gephi
  id: totrans-220
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Gephi可视化图形
- en: 'Actually, this `DataFrame` already represents the list of edges for our graph.
    For the visualization we prefer [Gephi](https://gephi.org), an open source tool
    for graph analysis. Because it is interactive, it is much better to use than Python’s
    graph library NetworkX.^([10](ch12.xhtml#idm45634172970888)) To work with Gephi,
    we need to save the list of nodes and edges of the graph in Graph Exchange XML
    format. Fortunately, NetworkX provides a function to export graphs in this format.
    So, we can simply convert our `DataFrame` into a NetworkX graph and save it as
    a `.gexf` file. We discard rare entity pairs to keep the graph compact and rename
    the frequency column because Gephi automatically uses a `weight` attribute to
    adjust the width of edges:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，这个`DataFrame`已经代表了我们图形的边缘列表。对于可视化，我们更喜欢[图表](https://gephi.org)，这是一个用于图形分析的开源工具。因为它是交互式的，所以比Python的图形库NetworkX要好得多。^([10](ch12.xhtml#idm45634172970888))
    为了使用Gephi，我们需要将图的节点和边的列表保存为Graph Exchange XML格式。幸运的是，NetworkX提供了一个将图导出为这种格式的函数。因此，我们可以简单地将我们的`DataFrame`转换为NetworkX图，并将其保存为`.gexf`文件。我们舍弃了罕见的实体对，以保持图的紧凑性，并重新命名了频率列，因为Gephi会自动使用`weight`属性来调整边的宽度。
- en: '[PRE45]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: After importing the file into Gephi, we selected only the largest component
    (connected subgraph) and removed some nodes with only a few connections manually
    for the sake of clarity.^([11](ch12.xhtml#idm45634172964856)) The result is presented
    in [Figure 12-5](#fig-cooc).
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 将文件导入Gephi后，我们仅选择了最大的组件（连接的子图），并手动删除了一些只有少数连接的节点，以清晰起见。^([11](ch12.xhtml#idm45634172964856))
    结果呈现在[图12-5](#fig-cooc)中。
- en: Note
  id: totrans-224
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Sometimes the most interesting relations are the ones that are not frequent.
    Take, for example, the first announcement on an upcoming merger or surprising
    relations that were mentioned a few times in the past but then forgotten. A sudden
    co-occurrence of entities that were previously unrelated can be a signal to start
    a deeper analysis of the relation.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，最有趣的关系是不频繁的关系。例如，考虑一下即将发生的合并的第一次公告，或者过去提到过但后来被遗忘的令人惊讶的关系。先前无关的实体的突然共现可能是开始对关系进行更深入分析的信号。
- en: Relation Extraction
  id: totrans-226
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关系提取
- en: 'Even though the co-occurrence graph already gave us some interesting insights
    about company networks, it does not tell us anything about the types of the relations.
    Take, for example, the subgraph formed by the companies Schlumberger, Fairchild
    Semiconductor, and Fujitsu in the lower-left corner of [Figure 12-5](#fig-cooc).
    So far, we know nothing about the relations between those companies; the information
    is still hidden in sentences like these:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 即使共现图已经为我们提供了关于公司网络的一些有趣见解，但它并没有告诉我们关系的类型。例如，考虑图中左下角由Schlumberger、Fairchild
    Semiconductor和Fujitsu公司组成的子图。到目前为止，我们对这些公司之间的关系一无所知；这些信息仍然隐藏在这样的句子中：
- en: '*Fujitsu wants to expand. It plans to acquire 80% of Fairchild Corp, an industrial
    unit of Schlumberger.*'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '*富士通希望扩展。它计划收购施伦贝尔格的工业单元傲胜公司80%的股份。*'
- en: 'In this section, we will introduce two blueprints for pattern-based relation
    extraction. The first and simpler blueprint searches for token phrases of the
    form “subject-predicate-object.” The second one uses the syntactical structure
    of a sentence, the dependency tree, to get more precise results at the price of
    more complex rules. In the end, we will generate a knowledge graph based on the
    four relations: *acquires*, *sells*, *subsidiary-of*, and *chairperson-of*. To
    be honest, we will use relaxed definitions of *acquires* and *sells*, which are
    easier to identify. They will also match sentences like “Fujitsu *plans to acquire
    80%* of Fairchild Corp” or even “Fujitsu *withdraws the option to acquire* Fairchild
    Corp.”'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍基于模式的关系抽取的两个蓝图。第一个更简单的蓝图搜索形式为“主语-谓语-宾语”的标记短语。第二个使用句子的语法结构——依赖树来以更复杂的规则获取更精确的结果。最终，我们将生成一个基于四种关系（*acquires*、*sells*、*subsidiary-of*和*chairperson-of*）的知识图谱。说实话，我们将使用较为宽松的*acquires*和*sells*定义，这样更容易识别。它们也会匹配句子如“富士通*计划收购*傲胜公司80%股权”甚至“富士通*撤回了收购*傲胜公司的选项”。
- en: Relation extraction is a complicated problem because of the ambiguity of natural
    language and the many different kinds and variations of relations. Model-based
    approaches to relation extraction are a current topic in research.^([12](ch12.xhtml#idm45634172825528))
    There are also some training datasets like [FewRel](http://zhuhao.me/fewrel) publicly
    available. However, training a model to identify relations is still pretty much
    in the research stage and out of the scope of this book.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 关系抽取是一个复杂的问题，因为自然语言的歧义性和不同种类及变化的关系。基于模型的关系抽取方法是当前研究的一个热门话题。^([12](ch12.xhtml#idm45634172825528))
    还有一些公开可用的训练数据集，如[FewRel](http://zhuhao.me/fewrel)。然而，训练一个模型来识别关系仍然主要停留在研究阶段，超出了本书的范围。
- en: 'Blueprint: Extracting Relations Using Phrase Matching'
  id: totrans-231
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 蓝图：使用短语匹配提取关系
- en: 'The first blueprint works like rule-based entity recognition: it tries to identify
    relations based on patterns for token sequences. Let’s start with a simplified
    version of the introductory example to explain the approach.'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个蓝图类似于基于规则的实体识别：它试图根据标记序列的模式识别关系。让我们从一个简化版本的介绍性例子开始解释这种方法。
- en: '[PRE46]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'We could find the relations in this sentence by searching for patterns like
    these:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过搜索以下模式来找到这个句子中的关系：
- en: '[PRE47]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '[spaCy’s rule-based matcher](https://oreil.ly/Mxd3m) allows us to find patterns
    that not only can involve the textual tokens but also their properties, like the
    lemma or part of speech. To use it, we must first define a matcher object. Then
    we can add rules with token patternsto the matcher:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '[spaCy的基于规则的匹配器](https://oreil.ly/Mxd3m)允许我们找到不仅涉及文本标记而且包括它们属性（如词形或词性）的模式。要使用它，我们必须首先定义一个匹配器对象。然后，我们可以向匹配器添加带有标记模式的规则：'
- en: '[PRE48]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '[PRE49]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: The first pattern is for the `acquires` relation. It returns all spans consisting
    of an organization, followed by arbitrary tokens that are not organizations, a
    verb matching several synonyms of *acquire*, again arbitrary tokens, and finally
    the second organization. The second pattern for `subsidiary-of` works similarly.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个模式是针对`acquires`关系的。它返回所有由组织名称组成的跨度，后跟任意不是组织的标记，匹配几个*acquire*的同义词的动词，再次是任意标记，最后是第二个组织名称。`subsidiary-of`的第二个模式工作方式类似。
- en: Granted, the expressions are hard to read. One reason is that we used the custom
    attribute `ref_t` instead of the standard `ENT_TYPE`. This is necessary to match
    coreferences that are not marked as entities, e.g., pronouns. Another one is that
    we have included some `NOT_IN` clauses. This is because rules with the asterisk
    operator (`*`) are always dangerous as they search patterns of unbounded length.
    Additional conditions on the tokens can reduce the risk for false matches. For
    example, we want to match “Fairchild, an industrial unit of Schlumberger” for
    the `subsidiary-of` relation, but not “Fujitsu mentioned a unit of Schlumberger.”
    When developing rules, you always have to pay for precision with complexity. We
    will discuss the problems of the `acquires` relation on that aspect in a minute.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这些表达式很难阅读。一个原因是我们使用了自定义属性`ref_t`而不是标准的`ENT_TYPE`。这是为了匹配没有标记为实体的指代词，例如代词。另一个原因是我们包含了一些`NOT_IN`子句。这是因为带有星号操作符(`*`)的规则总是危险的，因为它们搜索长度不受限制的模式。对标记的附加条件可以减少假匹配的风险。例如，我们希望匹配“施卢姆伯格的工业部门费尔德，但不是“富士通提到了施卢姆伯格的一个部门。”在开发规则时，您总是需要通过复杂性来换取精确性。我们将在一分钟内讨论*acquires*关系的问题。
- en: 'The blueprint function `extract_rel_match` now takes a processed `Doc` object
    and a matcher and transforms all matches to subject-predicate-object triples:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 现在蓝图功能`extract_rel_match`接收处理过的`Doc`对象和匹配器，并将所有匹配转换为主谓宾三元组：
- en: '[PRE50]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: The predicate is determined by the name of the rule; the involved entities are
    simply the first and last tokens of the matched span. We restrict the search to
    the sentence level because in a whole document we would have a high risk of finding
    false positives spanning multiple sentences.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 谓词由规则名称确定；所涉及的实体仅是匹配跨度的第一个和最后一个标记。我们限制搜索到句子级别，因为在整个文档中，我们可能会发现跨多个句子的假阳性的高风险。
- en: 'Usually, the rules match in the order “subject-predicate-object,” but often
    the entities appear in the text in reversed order, like in “the Schlumberger unit
    Fairchild Corp.” Here, the order of entities with regard to the `subsidiary-of`
    relation is “object-predicate-subject.” `extract_rel_match` is prepared to handle
    this and switches the subject and object if a rule has the prefix `rev-` like
    this one:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，规则按“主谓宾”的顺序匹配，但实体在文本中经常以相反的顺序出现，就像“施卢姆伯格的部门费尔德公司”。在这种情况下，关于`subsidiary-of`关系的实体顺序是“宾-谓-主”。`extract_rel_match`已经准备好处理这种情况，并在规则具有`rev-`前缀时切换主体和客体：
- en: '[PRE51]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Now we are able to detect `acquires` and both variants of `subsidiary-of` in
    sentences like these:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们能够检测到句子中的`acquires`和`subsidiary-of`的两种变体：
- en: '[PRE52]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '`Out:`'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE53]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Although the rules work nicely for our examples, the rule for *acquires* is
    not very reliable. The verb *acquire* can appear in many different constellations
    of entities. Thus, there is a high probability for false matches like this one:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这些规则在我们的例子中运行良好，但*acquires*的规则并不是很可靠。动词*acquire*可以出现在许多不同的实体组合中。因此，存在诸如以下这种的假阳性的高概率匹配：
- en: '[PRE54]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: '`Out:`'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE55]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Or this one:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 或者这样一个：
- en: '[PRE56]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: '`Out:`'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE57]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: Obviously, our rule wasn’t made for passive clauses (“was acquired by”) where
    the subject and object switch positions. We also cannot handle insertions containing
    named entities or negations because they produce false matches. To treat those
    cases correctly, we need knowledge about the syntactical structure of the sentence.
    And we get that from the dependency tree.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，我们的规则并不适用于被动从句（“被...收购”），在这种情况下，主语和宾语交换位置。我们也不能处理包含命名实体或否定的插入，因为它们会产生假匹配。要正确处理这些情况，我们需要了解句子的句法结构。而这些知识可以从依赖树中获取。
- en: 'But let’s first remove the unreliable rule for *acquires* from the matcher:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 但是让我们先移除匹配器中不可靠的*acquires*规则：
- en: '[PRE58]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Blueprint: Extracting Relations Using Dependency Trees'
  id: totrans-261
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 蓝图：使用依赖树提取关系
- en: 'The grammatical rules of a language impose a syntactical structure on each
    sentence. Each word serves a certain role in relation to the other words. A noun,
    for example, can be the subject or the object in a sentence; it depends on its
    relation to the verb. In linguistic theory, the words of a sentence are hierarchically
    interdependent, and the task of the parser in an NLP pipeline is to reconstruct
    these dependencies.^([13](ch12.xhtml#idm45634172046632)) The result is a *dependency
    tree*, which can also be visualized by `displacy`:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 语言的语法规则对每个句子强加了一种句法结构。每个词在与其他词的关系中起特定作用。例如，名词在句子中可以是主语或宾语；这取决于它与动词的关系。在语言学理论中，句子的词汇是层级相互依存的，而在自然语言处理管道中，解析器的任务是重建这些依赖关系。^([13](ch12.xhtml#idm45634172046632))
    其结果是*依赖树*，也可以通过`displacy`进行可视化：
- en: '[PRE59]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: '![](Images/btap_12in05.jpg)'
  id: totrans-264
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/btap_12in05.jpg)'
- en: Each node in the dependency tree represents a word. The edges are labeled with
    the dependency information. The root is usually the predicate of the sentence,
    in this case *acquired*, having a subject (`nsubj`) and an object (`obj`) as direct
    children. This first level, root plus children, already represents the essence
    of the sentence “Fujitsu acquired Fairchild Corp.”
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 依赖树中的每个节点代表一个词。边缘用依赖信息标记。根节点通常是句子的谓语，本例中是*acquired*，有一个主语(`nsubj`)和一个宾语(`obj`)作为直接子节点。这个第一层，根加子节点，已经代表了句子“Fujitsu
    acquired Fairchild Corp.”的本质。
- en: 'Let’s also take a look at the example with the passive clause. In this case,
    the auxiliary verb (`auxpass`) signals that *acquired* was used in passive form
    and *Fairchild* is the passive subject (`nsubjpass`):'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也来看看带有被动从句的例子。在这种情况下，助动词(`auxpass`)表示*acquired*以被动形式使用，*Fairchild*是被动主语(`nsubjpass`)：
- en: '![](Images/btap_12in06.jpg)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/btap_12in06.jpg)'
- en: Warning
  id: totrans-268
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: The values of the dependency labels depend on the corpus the parser model was
    trained on. They are also language dependent because different languages have
    different grammar rules. So, you definitely need to check which tag set is used
    by the dependency parser.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 依赖标签的值取决于解析器模型训练的语料库。它们还因语言而异，因为不同语言有不同的语法规则。因此，您绝对需要检查依赖解析器使用的标签集。
- en: 'The function `extract_rel_dep` implements a rule to identify verb-based relations
    like *acquires* based on the dependencies:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 函数`extract_rel_dep`实现了一个规则，用于基于依赖关系识别基于动词的关系，例如*acquires*：
- en: '[PRE60]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: The main loop iterates through all tokens in a doc and searches for a verb signaling
    our relationship. This condition is the same as in the flat pattern rule we used
    before. But when we detect a possible predicate, we now traverse the dependency
    tree to find the correct subject and the object. `find_subj` searches the left
    subtree, and `find_obj` searches the right subtree of the predicate. Those functions
    are not printed in the book, but you can find them in the GitHub notebook for
    this chapter. They use breadth-first search to find the closest subject and object,
    as nested sentences may have multiple subjects and objects. Finally, if the predicate
    indicates a passive clause, the subject and object will be swapped.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 主循环迭代文档中的所有标记，并搜索表明我们关系的动词。此条件与我们之前使用的平面模式规则相同。但是当我们检测到可能的谓语时，我们现在遍历依赖树以找到正确的主语和宾语。`find_subj`搜索谓语的左子树，而`find_obj`搜索谓语的右子树。这些功能未在书中打印，但您可以在本章的GitHub笔记本中找到它们。它们使用广度优先搜索来查找最接近的主语和宾语，因为嵌套句子可能有多个主语和宾语。最后，如果谓语表示被动从句，主语和宾语将被交换。
- en: 'Note, that this function also works for the *sells* relation:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这个函数也适用于*sells*关系：
- en: '[PRE61]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: '`Out:`'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE62]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'In this case, *Fairchild Inc.* is the closest object in the dependency tree
    to *sell* and identified correctly as the object of the investigated relation.
    But to be the “closest” is not always sufficient. Consider this example:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，*Fairchild Inc.*是与*sell*依赖树中最接近的对象，并正确地被识别为所调查关系的对象。但仅仅是“最接近”并不总是足够。考虑这个例子：
- en: '![](Images/btap_12in07.jpg)'
  id: totrans-278
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/btap_12in07.jpg)'
- en: 'Actually, we have a three-way relation here: Schlumberger sells Fairchild to
    Fujitsu. Our *sells* relation is intended to have the meaning “one company sells
    [whole or parts of] another company.” The other part is covered by the *acquires*
    relation. But how can we detect the right object here? Both Fujitsu and Fairchild
    are prepositional objects in this sentence (dependency `pobj`), and Fujitsu is
    the closest. The preposition is the key: Schlumberger sells something “to” Fujitsu,
    so that’s not the relation we are looking for. The purpose of the parameter `excl_prepos`
    in the extraction function is to skip objects with the specified prepositions.
    Here is the output without (A) and with (B) preposition filter:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，这里我们有一个三方关系：Schlumberger将Fairchild卖给Fujitsu。我们的*sells*关系意图表达“一家公司卖出（整体或部分）另一家公司”。另一部分由*acquires*关系覆盖。但是，我们如何在这里检测到正确的对象呢？在这个句子中，Fujitsu和Fairchild都是介词对象（依赖`pobj`），而Fujitsu是最接近的。介词是关键：Schlumberger将某物卖给“Fujitsu”，所以这不是我们要找的关系。在提取函数中，参数`excl_prepos`的目的是跳过具有指定介词的对象。以下是不带（A）和带有（B）介词过滤器的输出：
- en: '[PRE63]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: '`Out:`'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE64]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Let’s check how our new relation extraction function works on a few variations
    of the examples:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看我们的新关系提取函数在几个示例变体上的工作情况：
- en: '[PRE65]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: '`Out:`'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE66]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: As we can see, the relations in the first four sentences have been correctly
    extracted. Sentence 5, however, contains a negation and still returns `acquires`.
    This is a typical case of a false positive. We could extend our rules to handle
    this case correctly, but negations are rare in our corpus, and we accept the uncertainty
    in favor of the simpler algorithm. Sentence 6, in contrast, is an example for
    a possible false negative. Even though the relation was mentioned, it was not
    detected because the subject in this sentence is *competition* and not one of
    the companies.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，前四句中的关系已经被正确提取出来。然而，第5句包含否定，仍然返回`acquires`。这是一个典型的假阳性案例。我们可以扩展我们的规则以正确处理这种情况，但在我们的语料库中否定很少，我们接受了简单算法的不确定性。相比之下，第6句则是一个可能的假阴性的例子。尽管提到了关系，但由于这句子的主语是*competition*而不是公司之一，因此没有被检测到。
- en: Actually, dependency-based rules are inherently complex, and every approach
    to make them more precise results in even more complexity. It is a challenge to
    find a good balance between precision (fewer false positives) and recall (fewer
    false negatives) without making the code too complex.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，基于依赖的规则本质上是复杂的，每种使其更精确的方法都会导致更多的复杂性。在不使代码过于复杂的情况下，找到精度（更少的假阳性）和召回率（更少的假阴性）之间的良好平衡是一种挑战。
- en: Despite those deficiencies, the dependency-based rule still yields good results.
    This last step in the process, however, depends on the correctness of named-entity
    recognition, coreference resolution, and dependency parsing, all of which are
    not working with 100% accuracy. So, there will always be some false positives
    and false negatives. But the approach is good enough to produce highly interesting
    knowledge graphs, as we will do in the next section.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管存在这些缺陷，基于依赖的规则仍然能够产生良好的结果。然而，这一过程的最后一步取决于命名实体识别、共指消解和依赖解析的正确性，而所有这些都不能以100%的准确率工作。因此，总会有一些假阳性和假阴性。但是，这种方法已经足够好，可以产生非常有趣的知识图谱，正如我们将在下一节中做的那样。
- en: Creating the Knowledge Graph
  id: totrans-290
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建知识图谱
- en: Now that we know how to extract certain relationships, we can put everything
    together and create a knowledge graph from the entire Reuters corpus. We will
    extract organizations, persons and the four relations “acquires,” “sells,” “subsidiary-of,”
    and “executive-of.” [Figure 12-6](#fig-knowledge-graph) shows the resulting graph
    with some selected subgraphs.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道如何提取特定的关系，我们可以将所有内容整合在一起，并从整个Reuters语料库创建知识图谱。我们将提取组织、人员以及四个关系“acquires”、“sells”、“subsidiary-of”和“executive-of”。[图 12-6](#fig-knowledge-graph)显示了带有一些选择的子图的结果图。
- en: 'To get the best results in dependency parsing and named-entity recognition,
    we use spaCy’s large model with our complete pipeline. If possible, we will use
    the GPU to speed up NLP processing:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在依赖解析和命名实体识别中获得最佳结果，我们使用spaCy的大型模型和完整管道。如果可能，我们将使用GPU加速NLP处理：
- en: '[PRE67]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: '[PRE68]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Before we start the information extraction process, we create two additional
    rules for the “executive-of” relation similar to the “subsidiary-of” relation
    and add them to our rule-based `matcher`:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始信息提取过程之前，我们为“执行者”关系创建了两个类似于“子公司”关系的额外规则，并将它们添加到我们基于规则的`matcher`中：
- en: '[PRE69]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: '![](Images/btap_1206.jpg)'
  id: totrans-297
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/btap_1206.jpg)'
- en: Figure 12-6\. The knowledge graph extracted from the Reuters corpus with three
    selected subgraphs (visualized with the help of Gephi).
  id: totrans-298
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 12-6\. 从路透社语料库中提取的知识图，包括三个选定的子图（使用 Gephi 可视化）。
- en: 'We then define one function to extract all relationships. Two of our four relations
    are covered by the `matcher`, and the other two by the dependency-based matching
    algorithm:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们定义一个函数来提取所有关系。我们的四种关系中，两种被匹配器覆盖，另外两种被基于依赖的匹配算法覆盖：
- en: '[PRE70]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'The remaining steps to extract the relations, convert them into a NetworkX
    graph, and store the graph in a `gexf` file for Gephi are basically following
    [“Blueprint: Creating a Co-Occurrence Graph”](#ch12-cooc). We skip them here,
    but you will find the full code again in the GitHub repository.'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 提取关系的剩余步骤，将其转换为 NetworkX 图，并将图存储在 `gexf` 文件中供 Gephi 使用，基本上遵循[“蓝图：创建共现图”](#ch12-cooc)。我们在这里跳过它们，但您将再次在
    GitHub 存储库中找到完整的代码。
- en: 'Here are a few records of the final data frame containing the nodes and edges
    of the graph as they are written to the `gexf` file:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是最终数据框架的几条记录，包含了图的节点和边，正如它们被写入 `gexf` 文件的方式：
- en: '|  | subj | subj_type | pred | obj | obj_type | freq | articles |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '|  | 主体 | 主体类型 | 谓语 | 宾语 | 宾语类型 | 频率 | 文章 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| 883 | Trans World Airlines | ORG | acquires | USAir Group | ORG | 7 | 2950,2948,3013,3095,1862,1836,7650
    |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| 883 | 泛美航空公司 | ORG | 收购 | 美国航空集团 | ORG | 7 | 2950,2948,3013,3095,1862,1836,7650
    |'
- en: '| 152 | Carl Icahn | PERSON | executive-of | Trans World Airlines | ORG | 3
    | 1836,2799,3095 |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| 152 | 卡尔·伊坎 | PERSON | 担任执行者 | 泛美航空公司 | ORG | 3 | 1836,2799,3095 |'
- en: '| 884 | Trans World Airlines | ORG | sells | USAir Group | ORG | 1 | 9487 |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| 884 | 泛美航空公司 | ORG | 销售 | 美国航空集团 | ORG | 1 | 9487 |'
- en: The visualization of the Reuters graph in [Figure 12-6](#fig-knowledge-graph)
    was again created with the help of Gephi. The graph consists of many rather small
    components (disconnected subgraphs); because most companies got mentioned in only
    one or two news articles and we extracted only the four relations, simple co-occurrences
    are not included here. We manually magnified three of those subgraphs in the figure.
    They represent company networks that already appeared in the co-occurrence graph
    ([Figure 12-5](#fig-cooc)), but now we know the relation types and get a much
    clearer picture.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 用 Gephi 的帮助再次创建的[图 12-6](#fig-knowledge-graph)中的路透社图的可视化效果。该图由许多相当小的组件（不连通的子图）组成；因为大多数公司只在一两篇新闻文章中被提及，我们仅提取了四种关系，因此简单的共现不包括在这里。我们在图中手动放大了其中三个子图。它们代表的是公司网络，这些网络已经在共现图中出现过（[图 12-5](#fig-cooc)），但现在我们知道了关系类型，并且得到了一个更清晰的图像。
- en: Don’t Blindly Trust the Results
  id: totrans-309
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 不要盲目地信任结果
- en: Each processing step we went through has a potential of errors. Thus, the information
    stored in the graph is not completely reliable. In fact, this starts with data
    quality in the articles themselves. If you look carefully at the upper-left example
    in [Figure 12-6](#fig-knowledge-graph), you will notice that the two entities
    “Fujitsu” and “Futjitsu” appear in the graph. This is indeed a spelling error
    in the original text.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 我们经历的每个处理步骤都有潜在的错误可能性。因此，存储在图中的信息并不完全可靠。事实上，这始于文章本身的数据质量。如果您仔细观察[图 12-6](#fig-knowledge-graph)中左上角的示例，您会注意到图中出现了“富士通”和“Futjitsu”这两个实体。这实际上是原始文本中的拼写错误。
- en: In the magnified subnetwork to the right in [Figure 12-6](#fig-knowledge-graph)
    you can spot the seemingly contradictory information that “Piedmont acquires USAir”
    and “USAir acquires Piedmont.” In fact, both are true because both enterprises
    acquired parts of the shares of the other one. But it could also be a mistake
    by one of the involved rules or models. To track this kind of problem, it is indispensable
    to store some information about the source of the extracted relations. That’s
    why we included the list of articles in every record.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图 12-6](#fig-knowledge-graph)右侧放大的子网络中，您可以发现表面上相互矛盾的信息：“皮德蒙特收购美国航空”和“美国航空收购皮德蒙特”。事实上，两者都是正确的，因为这两家企业都收购了对方的部分股份。但这也可能是其中一家相关规则或模型的错误。要追踪这种问题，有必要存储一些关于提取关系来源的信息。这就是为什么我们在每条记录中包含文章列表的原因。
- en: 'Finally, be aware that our analysis did not consider one aspect at all: the
    timeliness of information. The world is constantly changing and so are the relationships.
    Each edge in our graph should therefore get time stamped. So, there is still much
    to be done to create a knowledge base with trustable information, but our blueprint
    provides a solid foundation for getting started.'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，请注意我们的分析完全没有考虑到一个方面：信息的及时性。世界在不断变化，关系也在变化。因此，我们图中的每一条边都应该有时间戳。因此，要创建一个具有可信信息的知识库，仍然有很多工作要做，但我们的蓝图为开始提供了坚实的基础。
- en: Closing Remarks
  id: totrans-313
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结语
- en: In this chapter, we explored how to build a knowledge graph by extracting structured
    information from unstructured text. We went through the whole process of information
    extraction, from named-entity recognition via coreference resolution to relation
    extraction.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了如何通过从非结构化文本中提取结构化信息来构建知识图谱。我们经历了信息提取的整个过程，从命名实体识别到通过指代消解到关系提取。
- en: As you have seen, each step is a challenge in itself, and we always have the
    choice between a rule-based and a model-based approach. Rule-based approaches
    have the advantage that you don’t need training data. So, you can start right
    away; you just need to define the rules. But if the entity type or relationship
    you try to capture is complex to describe, you end up either with rules that are
    too simple and return a lot of false matches or with rules that are extremely
    complex and hard to maintain. When using rules, it is always difficult to find
    a good balance between recall (find most of the matches) and precision (find only
    correct matches). And you need quite a bit of technical, linguistic, and domain
    expertise to write good rules. In practice, you will also have to test and experiment
    a lot until your rules are robust enough for your application.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所见，每一步都是一个挑战，我们总是在规则化方法和模型化方法之间做出选择。规则化方法的优势在于您无需训练数据。因此，您可以立即开始；您只需定义规则即可。但是，如果您尝试捕捉的实体类型或关系复杂难以描述，最终要么会得到过于简单并返回许多错误匹配的规则，要么是非常复杂且难以维护的规则。使用规则时，始终很难在召回率（找到大多数匹配项）和精确度（仅找到正确匹配项）之间找到良好的平衡。而且，您需要相当多的技术、语言和领域专业知识才能编写出好的规则。在实践中，您还必须测试和进行大量实验，直到您的规则足够强大以满足您的应用需求。
- en: Model-based approaches, in contrast, have the great advantage that they learn
    those rules from the training data. Of course, the downside is that you need lots
    of high-quality training data. And if those training data are specific to your
    application domain, you have to create them yourself. The manual labeling of training
    data is especially cumbersome and time-consuming in the area of text because somebody
    has to read and understand the text before the labels can be set. In fact, getting
    good training data is the biggest bottleneck today in machine learning.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，基于模型的方法具有一个巨大的优势，即它们可以从训练数据中学习这些规则。当然，其缺点是您需要大量高质量的训练数据。如果这些训练数据特定于您的应用领域，那么您必须自己创建它们。在文本领域，手动标记训练数据尤其麻烦且耗时，因为有人必须先阅读和理解文本，然后才能设置标签。事实上，今天在机器学习领域，获得好的训练数据是最大的瓶颈。
- en: A possible solution to the problem of missing training data is weak supervision.
    The idea is to create a large dataset by rules like the ones we defined in this
    chapter or even to generate them programmatically. Of course, this dataset will
    be noisy, as the rules are not perfect. But, surprisingly, it is possible to train
    a high-quality model on low-quality data. Weak supervision learning for named-entity
    recognition and relationship extraction is, like many other topics covered in
    this section, a current topic of research. If you want to learn more about the
    state of the art in information extraction and knowledge graph creation, you can
    check out the following references. They provide good starting points for further
    reading.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 缓解缺乏训练数据问题的一个可能解决方案是弱监督学习。其思想是通过像本章定义的规则或甚至通过程序生成它们的方式来创建一个大数据集。当然，由于规则并非完美，这个数据集会有噪音。但令人惊讶的是，我们可以在低质量数据上训练出高质量的模型。弱监督学习用于命名实体识别和关系提取，与本节中涵盖的许多其他主题一样，是当前的研究热点。如果您想了解更多关于信息提取和知识图谱创建的最新技术，可以查阅以下参考资料。它们为进一步阅读提供了良好的起点。
- en: Further Reading
  id: totrans-318
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'Barrière, Caroline. *Natural Language Understanding in a Semantic Web Context.*
    Switzerland: Springer Publishing. 2016\. [*https://www.springer.com/de/book/9783319413358*](https://www.springer.com/de/book/9783319413358).'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Barrière, Caroline的《语义网背景下的自然语言理解》。瑞士：斯普林格出版社。2016年。[*https://www.springer.com/de/book/9783319413358*](https://www.springer.com/de/book/9783319413358)。
- en: Gao, Yuqing, Jisheng Liang, Benjamin Han, Mohamed Yakout, and Ahmed Mohamed.
    *Building a Large-scale, Accurate and Fresh Knowledge Graph*. Tutorial at KDD.
    2018\. [*https://kdd2018tutorialt39.azurewebsites.net*](https://kdd2018tutorialt39.azurewebsites.net).
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao, Yuqing，Jisheng Liang，Benjamin Han，Mohamed Yakout和Ahmed Mohamed的《构建大规模、准确且更新的知识图谱》。KDD教程，2018年。[*https://kdd2018tutorialt39.azurewebsites.net*](https://kdd2018tutorialt39.azurewebsites.net)。
- en: 'Han, Xu, Hao Zhu, Pengfei Yu, Ziyun Wang, Yuan Yao, Zhiyuan Liu, and Maosong
    Sun. *FewRel: A Large-Scale Supervised Few-Shot Relation Classification Dataset
    with State-of-the-Art Evaluation*. Proceedings of EMNLP, 2018\. [*https://arxiv.org/abs/1810.10147*](https://arxiv.org/abs/1810.10147).'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Han, Xu，Hao Zhu，Pengfei Yu，Ziyun Wang，Yuan Yao，Zhiyuan Liu和Maosong Sun的《FewRel：一种大规模监督少样本关系分类数据集及其最新评估》。EMNLP会议论文，2018年。[*https://arxiv.org/abs/1810.10147*](https://arxiv.org/abs/1810.10147)。
- en: Jurafsky, Dan, and James H. Martin. *Speech and Language Processing*. 3rd Edition
    (draft), Chapters 18 and 22\. 2019\. [*https://web.stanford.edu/~jurafsky/slp3*](https://web.stanford.edu/~jurafsky/slp3).
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jurafsky, Dan和James H. Martin的《语音与语言处理》。第3版（草案），第18章和第22章。2019年。[*https://web.stanford.edu/~jurafsky/slp3*](https://web.stanford.edu/~jurafsky/slp3)。
- en: 'Lison, Pierre, Aliaksandr Hubin, Jeremy Barnes, and Samia Touileb. *Named-Entity
    Recognition without Labelled Data: A Weak Supervision Approach*. Proceedings of
    ACL, 2020 [*https://arxiv.org/abs/2004.14723*](https://arxiv.org/abs/2004.14723).'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lison, Pierre，Aliaksandr Hubin，Jeremy Barnes和Samia Touileb的《无标注数据命名实体识别：弱监督方法》。ACL会议论文，2020年[*https://arxiv.org/abs/2004.14723*](https://arxiv.org/abs/2004.14723)。
- en: '^([1](ch12.xhtml#idm45634176075192-marker)) See Natasha Noy, Yuqing Gao, Anshu
    Jain, Anant Narayanan, Alan Patterson, and Jamie Taylor. *Industry-scale Knowledge
    Graphs: Lessons and Challenges*. 2019\. [*https://queue.acm.org/detail.cfm?id=3332266*](https://queue.acm.org/detail.cfm?id=3332266).'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch12.xhtml#idm45634176075192-marker)) 参见Natasha Noy，Yuqing Gao，Anshu Jain，Anant
    Narayanan，Alan Patterson和Jamie Taylor的《产业规模知识图谱：经验与挑战》。2019年。[*https://queue.acm.org/detail.cfm?id=3332266*](https://queue.acm.org/detail.cfm?id=3332266)。
- en: ^([2](ch12.xhtml#idm45634176061992-marker)) See [*https://oreil.ly/nzhUR*](https://oreil.ly/nzhUR)
    for details.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch12.xhtml#idm45634176061992-marker)) 详情请见[*https://oreil.ly/nzhUR*](https://oreil.ly/nzhUR)。
- en: '^([3](ch12.xhtml#idm45634176054984-marker)) Tim Berners-Lee et al., “The Semantic
    Web: a New Form of Web Content that is Meaningful to Computers Will Unleash a
    Revolution of New Possibilities.” *Scientific American* 284 No. 5: May 2001.'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch12.xhtml#idm45634176054984-marker)) Tim Berners-Lee等人，《语义网：对计算机有意义的新形式的Web内容将引发新的可能性革命》。《科学美国人》284号5月2001年。
- en: ^([4](ch12.xhtml#idm45634175830936-marker)) The asterisk operator (*) unpacks
    the list into separate arguments for `print`.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch12.xhtml#idm45634175830936-marker)) 星号操作符（*）将列表展开为`print`的单独参数。
- en: ^([5](ch12.xhtml#idm45634175637736-marker)) See [spaCy’s rule-based matching
    usage docs](https://oreil.ly/Hvtgs) for an explanation of the syntax, and check
    out the interactive pattern explorer on [*https://explosion.ai/demos/matcher*](https://explosion.ai/demos/matcher).
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch12.xhtml#idm45634175637736-marker)) 参见[spaCy的基于规则匹配的使用文档](https://oreil.ly/Hvtgs)以了解语法的解释，并查看[*https://explosion.ai/demos/matcher*](https://explosion.ai/demos/matcher)上的交互式模式探索器。
- en: ^([6](ch12.xhtml#idm45634174792232-marker)) You will find an additional blueprint
    for acronym detection in the notebook for this chapter on [GitHub](https://oreil.ly/LlPHm).
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: ^([6](ch12.xhtml#idm45634174792232-marker)) 在本章的笔记本上，你将找到一个用于缩略语检测的额外蓝图，位于[GitHub](https://oreil.ly/LlPHm)。
- en: ^([7](ch12.xhtml#idm45634173992248-marker)) See Wolf (2017),[“State-Of-The-Art
    Neural Coreference Resolution For Chatbots”](https://oreil.ly/VV4Uy) for more.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: ^([7](ch12.xhtml#idm45634173992248-marker)) 更多详情请参见Wolf（2017）的[“Chatbots的最新神经指代消解技术”](https://oreil.ly/VV4Uy)。
- en: ^([8](ch12.xhtml#idm45634173497096-marker)) This slogan was coined by Google
    when it introduced its knowledge graph in 2012.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: ^([8](ch12.xhtml#idm45634173497096-marker)) 谷歌在2012年推出其知识图谱时提出了这一口号。
- en: ^([9](ch12.xhtml#idm45634173458648-marker)) You’ll find the colorized figures
    in the electronic versions of this book and in our [GitHub repository](https://oreil.ly/2ju0k).
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: ^([9](ch12.xhtml#idm45634173458648-marker)) 你可以在本书的电子版和我们的[GitHub存储库](https://oreil.ly/2ju0k)中找到彩色插图。
- en: ^([10](ch12.xhtml#idm45634172970888-marker)) You can find a NetworkX version
    of the graph in the notebook for this chapter on [GitHub](https://oreil.ly/OWTcO).
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: ^([10](ch12.xhtml#idm45634172970888-marker)) 你可以在本章笔记本的 [GitHub](https://oreil.ly/OWTcO)
    上找到该图的 NetworkX 版本。
- en: ^([11](ch12.xhtml#idm45634172964856-marker)) We provide more details on that
    in our [GitHub repository](https://oreil.ly/nri01) for this chapter.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: ^([11](ch12.xhtml#idm45634172964856-marker)) 更多详细信息请参阅我们本章的 [GitHub 仓库](https://oreil.ly/nri01)。
- en: ^([12](ch12.xhtml#idm45634172825528-marker)) See an overview of the [state of
    the art](https://oreil.ly/l6DIH).
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: ^([12](ch12.xhtml#idm45634172825528-marker)) 参见 [最新技术概述](https://oreil.ly/l6DIH)。
- en: ^([13](ch12.xhtml#idm45634172046632-marker)) Constituency parsers, in contrast
    to dependency parsers, create a hierarchical sentence structure based on nested
    phrases.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: ^([13](ch12.xhtml#idm45634172046632-marker)) 与依存分析器相反，成分分析器根据嵌套短语创建层次化的句子结构。
