- en: Chapter 12\. Building a Knowledge Graph
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this book, we have been working through many blueprints for text analysis.
    Our goal was always to identify patterns in the data with the help of statistics
    and machine learning. In [Chapter 10](ch10.xhtml#ch-embeddings) we explained how
    embeddings can be used to answer questions like “What is to Germany like Paris
    is to France?” Embeddings represent some kind of implicit knowledge that was learned
    from the training documents based on a notion of similarity.
  prefs: []
  type: TYPE_NORMAL
- en: A knowledge base, in contrast, consists of structured statements of the form
    “Berlin capital-of Germany.” In this case, “capital-of” is a precisely defined
    relation between the two specific entities *Berlin* and *Germany*. The network
    formed by many entities and their relations is a graph in the mathematical sense,
    a *knowledge graph*. [Figure 12-1](#fig-simple-kg) shows a simple knowledge graph
    illustrating the example. In this chapter, we will introduce blueprints to extract
    structured information from unstructured text and build a basic knowledge graph.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/btap_1201.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12-1\. A simple knowledge graph.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: What You’ll Learn and What We’ll Build
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Information extraction is one of the hardest tasks in natural language processing
    because of the complexity and inherent ambiguity of language. Thus, we need to
    apply a sequence of different steps to discover the entities and relationships.
    Our example use case in this section is the creation of a knowledge graph based
    on business news articles about companies.
  prefs: []
  type: TYPE_NORMAL
- en: In the course of the chapter, we will take a deep dive into the advanced language
    processing features of spaCy. We will use the pretrained neural models in combination
    with custom rules for named-entity recognition, coreference resolution, and relation
    extraction. We will also explain the necessary steps to perform entity linking,
    but we won’t go into the implementation details.
  prefs: []
  type: TYPE_NORMAL
- en: After reading this chapter, you will have the basic linguistic and technical
    knowledge to start building your own knowledge base. You will find the source
    code for this chapter and additional information in our [GitHub repository](https://oreil.ly/5dF4g).
  prefs: []
  type: TYPE_NORMAL
- en: Knowledge Graphs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A knowledge graph is a large semantic network. It consists of nodes that are
    entities such as persons, places, events, or companies, and edges that represent
    formalized relations between those nodes, as shown in [Figure 12-1](#fig-simple-kg).
  prefs: []
  type: TYPE_NORMAL
- en: 'All the big players such as Google, Microsoft, Facebook, etc., use knowledge
    graphs to power their search engines and query services.^([1](ch12.xhtml#idm45634176075192))
    And nowadays more and more companies are building their own knowledge graphs to
    gain market insights or power chatbots. But the largest knowledge graph is distributed
    all over the world: *Linked Open Data* refers to all the available data on the
    web that can be identified by a uniform resource identifier (URI). It is the result
    of 20 years of academic development in the area of the Semantic Web (see [“Semantic
    Web and RDF”](#semanticwebRDF)).'
  prefs: []
  type: TYPE_NORMAL
- en: The types of nodes and edges are precisely defined by an *ontology*, which is
    itself a knowledge base for the terminology used in a domain. For example, the
    public ontology Wikidata provides definitions for all types used in [Figure 12-1](#fig-simple-kg).^([2](ch12.xhtml#idm45634176061992))
    Each of these definitions has a unique URI (e.g., “city” is [*http://www.wikidata.org/wiki/Q515*](http://www.wikidata.org/wiki/Q515).).
    In fact, Wikidata contains both, the type definitions and the actual objects in
    a queryable format.
  prefs: []
  type: TYPE_NORMAL
- en: Information Extraction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are several typical steps needed to extract structured information from
    text, as shown in [Figure 12-2](#fig-ie). As a first step, *named-entity recognition*,
    finds *mentions* of named entities in the text and labels them with the correct
    type, e.g., person, organization, or location. The same entity is usually referenced
    multiple times in a document by different variants of the name or by pronouns.
    The second step, *coreference resolution*, identifies and resolves those *coreferences*
    to prevent duplicates and information loss.
  prefs: []
  type: TYPE_NORMAL
- en: 'Closely related to coreference resolution, and usually the next step, is the
    task of *entity linking*. Here, the goal is to link a mention in the text to a
    unique real-world entity in an ontology, for example, *Berlin* to the URI [*http://www.wikidata.org/entity/Q64*](http://www.wikidata.org/entity/Q64).
    Thus, any ambiguities are removed: Q64 is the Berlin in Germany and not the one
    in New Hampshire (which is, by the way, Q821244 in Wikidata). This is essential
    to connect information from different sources and really build a knowledge base.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/btap_1202.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12-2\. The process of information extraction.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The last step, *relation extraction*, identifies the relations between those
    entities. In an application scenario, you will usually consider only a few relations
    of interest because it is hard to extract this kind of information correctly from
    arbitrary text.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, you could store the graph in a graph database as the backend of a knowledge-based
    application. Such graph databases store the data either as RDF triples (*triple
    stores*) or in the form of a property graph, where nodes and edges can have arbitrary
    attributes. Commonly used graph databases are, for example, GraphDB (triple store),
    Neo4j, and Grakn (property graphs).
  prefs: []
  type: TYPE_NORMAL
- en: For each of the steps, you have the choice between a rule-based approach and
    machine learning. We will use available models of spaCy and rules in addition.
    We will not train our own models, though. The usage of rules for the extraction
    of domain-specific knowledge has the advantage that you can get started quickly
    without training data. As we will see, the results allow some really interesting
    analyses. But if you plan to build a corporate knowledge base on a large scale,
    you may have to train your own models for named-entity and relationship detection
    as well as for entity linking.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing the Dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Assume you are working in the financial business and want to track news on mergers
    and acquisitions. It would be great if you could automatically identify company
    names and the kind of deals they are involved in and make the results available
    in a knowledge base. In this chapter, we will explain the building blocks to extract
    some information about companies. For example, we will extract the relation “Company1
    acquires Company2.”
  prefs: []
  type: TYPE_NORMAL
- en: 'To simulate such a scenario, we use a publicly available dataset, the well-known
    [Reuters-21578](https://oreil.ly/lltWo) news corpus. It contains more than 20,000
    news articles of 90 categories published by Reuters in 1987\. This dataset was
    chosen because it is free and easy to get. In fact, it is available as one of
    the NLTK standard corpora, and you can simply download it with NLTK:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We will work only with articles from the acquisitions category (acq). To prepare
    it for our purposes, we loaded all articles into a single `DataFrame` and did
    some data cleaning following the blueprints in [“Cleaning Text Data”](ch04.xhtml#ch4-cleaning).
    Clean data is crucial to recognize named-entities and relationships as the neural
    models benefit from well-structured sentences. For this dataset, we substituted
    HTML escapes, removed stock ticker symbols, replaced abbreviations like *mln*
    for *million*, and corrected some spelling mistakes. We also dropped the headlines
    because they are written in capital letters only. The complete article bodies
    are retained, though. All cleaning steps can be found in the notebook on [GitHub](https://oreil.ly/21p8d).
    Let’s take a look at a sample of the cleaned articles in our `DataFrame`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: So, this is the data we have in mind when we develop the blueprints for information
    extraction. However, most of the sentences in the following sections are simplified
    examples to better explain the concepts.
  prefs: []
  type: TYPE_NORMAL
- en: Named-Entity Recognition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'After data cleaning, we can start with the first step of our information extraction
    process: named-entity recognition. Named-entity recognition was briefly introduced
    in [Chapter 4](ch04.xhtml#ch-preparation) as part of spaCy’s standard pipeline.
    spaCy is our library of choice for all the blueprints in this chapter because
    it is fast and has an extensible API that we will utilize. But you could also
    use Stanza or Flair (see [“Alternatives for NER: Stanza and Flair”](#alts4ner)).'
  prefs: []
  type: TYPE_NORMAL
- en: spaCy provides trained NER models for many languages. The English models have
    been trained on the large [OntoNotes5 corpus](https://oreil.ly/gyOiH) containing
    18 different entity types. [Table 12-1](#tab-ner-types) lists a subset of these.
    The remaining types are for numeric entities.
  prefs: []
  type: TYPE_NORMAL
- en: Table 12-1\. Subset of NER types of the OntoNotes 5 corpus
  prefs: []
  type: TYPE_NORMAL
- en: '| NER Type | Description | NER Type | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| PERSON | People, including fictional | PRODUCT | Vehicles, weapons, foods,
    etc. (Not services) |'
  prefs: []
  type: TYPE_TB
- en: '| NORP | Nationalities or religious or political groups | EVENT | Named hurricanes,
    battles, wars, sports events, etc. |'
  prefs: []
  type: TYPE_TB
- en: '| FAC | Facilities: buildings, airports, highways, bridges, etc. | WORK_OF_ART
    | Titles of books, songs, etc. |'
  prefs: []
  type: TYPE_TB
- en: '| ORG | Organizations: companies, agencies, institutions, etc. | LAW | Named
    documents made into laws |'
  prefs: []
  type: TYPE_TB
- en: '| GPE | Countries, cities, states | LANGUAGE | Any named language |'
  prefs: []
  type: TYPE_TB
- en: '| LOCATION | Non-GPE locations, mountain ranges, bodies of water |  |  |'
  prefs: []
  type: TYPE_TB
- en: The NER tagger is enabled by default when you load a language model. Let’s start
    by initializing an `nlp` object with the standard (small) English model `en_core_web_sm`
    and print the components of the NLP pipeline:^([4](ch12.xhtml#idm45634175830936))
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the text is processed, we can access the named entities directly with
    `doc.ents`. Each entity has a text and a label describing the entity type. These
    attributes are used in the last line in the following code to print the list of
    entities recognized in this text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'With spaCy’s neat visualization module `displacy`, we can generate a visual
    representation of the sentence and its named entities. This is helpful to inspect
    the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/btap_12in01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In general, spaCy’s named-entity recognizer does a good job. In our example,
    it was able to detect all named entities. The labels of *Kistler* and *Baker*
    in the second and third sentence, however, are not correct. In fact, distinguishing
    between persons and organizations is quite a challenge for NER models because
    those entity types are used very similarly. We will resolve such problems later
    in the blueprint for name-based coreference resolution.
  prefs: []
  type: TYPE_NORMAL
- en: 'Blueprint: Using Rule-Based Named-Entity Recognition'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you want to identify domain-specific entities on which the model has not
    been trained, you can of course train your own model with [spaCy](https://oreil.ly/6EMig).
    But training a model requires a lot of training data. Often it is sufficient to
    specify simple rules for custom entity types. In this section, we will show how
    to use rules to detect government organizations like the “Department of Justice”
    (or alternatively the “Justice Department”) in the Reuters dataset.
  prefs: []
  type: TYPE_NORMAL
- en: spaCy provides an [`EntityRuler`](https://oreil.ly/A6MZ8) for this purpose,
    a pipeline component that can be used in combination with or instead of the statistical
    named-entity recognizer. Compared to regular expression search, spaCy’s matching
    engine is more powerful because patterns are defined on sequences of spaCy’s tokens
    instead of just strings. Thus, you can use any token property like the lemma or
    the part-of-speech tag to build your patterns.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, let’s define some pattern rules to match departments of the US government
    and the “Securities and Exchange Commission,” which is frequently mentioned in
    our corpus:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Each rule consists of a dictionary with a label, in our case the custom entity
    type `GOV`, and a pattern that the token sequence must match. You can specify
    multiple rules for the same label, as we did here.^([5](ch12.xhtml#idm45634175637736))
    The first rule, for example, matches sequences of tokens with the texts `"U.S."`
    (optional, denoted by `"OP": "?"`), `"Department"`, `"of"`, and either `"Justice"`
    or `"Transportation"`. Note that this and the second rule refine already recognized
    entities of type `ORG`. Thus, these patterns must be applied on top and not instead
    of spaCy’s named-entity model.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on these patterns, we create an `EntityRuler` and add it to our pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, when we call `nlp`, those organizations will automatically be labeled
    with the new type `GOV`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/btap_12in02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Blueprint: Normalizing Named Entities'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'One approach to simplify the resolution of different entity mentions to a single
    name is the normalization or standardization of mentions. Here, we will do a first
    normalization, which is generally helpful: the removal of unspecific suffixes
    and prefixes. Take a look at this example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: In the first sentence, the token sequence `Baker International's` was detected
    as an entity even though the genitive-s is not part of the company name. A similar
    case is the article in `the New York Stock Exchange`. Regardless of whether the
    article is actually part of the name or not, entities will likely be referenced
    sometimes with and sometimes without the article. Thus, the general removal of
    the article and an apostrophe-s simplifies the linking of mentions.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'As with any rules, there is a potential of errors: think of `The Wall Street
    Journal` or `McDonald''s`. If you need to preserve the article or the apostrophe-s
    in such cases, you must define exceptions for the rules.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Our blueprint function shows how to implement normalizations such as removing
    a leading article and a trailing apostrophe-s in spaCy. As we are not allowed
    to update entities in place, we create a copy of the entities and apply our modifications
    to this copy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: An entity in spaCy is a `Span` object with a defined start and end plus an additional
    label denoting the type of the entity. We loop through the entities and adjust
    the position of the first and last token of the entity if necessary. Finally,
    we replace `doc.ents` with our modified copy.
  prefs: []
  type: TYPE_NORMAL
- en: 'The function takes a spaCy `Doc` object (named `doc`) as a parameter and returns
    a `Doc`. Therefore, we can use it as a another pipeline component and simply add
    it to the existing pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can repeat the process on the example sentences and check the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Merging Entity Tokens
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In many cases, it makes sense to treat compound names like the ones from the
    previous example as single tokens because it simplifies the sentence structure.
    spaCy provides a built-in pipeline function `merge_entities` for that purpose.
    We add it to our NLP pipeline and get exactly one token per named-entity:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Even though merging entities simplifies our blueprints later in this chapter,
    it may not always be a good idea. Think, for example, about compound entity names
    like `London Stock Exchange`. After merging into a single token, the implicit
    relation of this entity to the city of London will be lost.
  prefs: []
  type: TYPE_NORMAL
- en: Coreference Resolution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One of the greatest obstacles in information extraction is the fact that entity
    mentions appear in many different spellings (also called *surface forms*). Look
    at the following sentences:'
  prefs: []
  type: TYPE_NORMAL
- en: Hughes Tool Co Chairman W.A. Kistler said its merger with Baker International
    Corp. was still under consideration. We hope to come to a mutual agreement, Kistler
    said. Baker will force Hughes to complete the merger. A review by the U.S. Department
    of Justice was completed today. The Justice Department will block the merger after
    consultation with the SEC.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: As we can see, entities are frequently introduced by their full name, while
    later mentions use abbreviated versions. This is one type of coreference that
    must resolved to understand what’s going on. [Figure 12-3](#fig-cooc-baker-hughes)
    shows a co-occurrence graph without (left) and with (right) unified names. Such
    a co-occurrence graph, as we will build in the next section, is a visualization
    of entity pairs appearing in the same article.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/btap_1203.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12-3\. A co-occurrence graph of the same articles before (left) and after
    coreference resolution (right).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '*Coreference resolution* is the task of determining the different mentions
    of an entity within a single text, for example, abbreviated names, aliases, or
    pronouns. The result of this step is a group of coreferencing mentions called
    a *mention cluster*, for example, `{Hughes Tool Co, Hughes, its}`. Our target
    in this section is to identify related mentions and link them within a document.'
  prefs: []
  type: TYPE_NORMAL
- en: For this purpose, we develop a couple of blueprints for coreference resolution
    and name unification (see [Figure 12-4](#fig-coref-pipeline)). We will restrict
    ourselves to organizations and persons, as these are the entity types we are interested
    in. First, we will resolve aliases like *SEC* by a dictionary lookup. Then we
    will match names within a document to the first mention. For example, we will
    create a link from “Kistler” to “W.A. Kistler.” After that, indirect references
    (*anaphora*) like the pronoun *its* in the first sentence will be resolved. Finally,
    we will normalize again the names of the resolved entities. All of these steps
    will be implemented as additional pipeline functions.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/btap_1204.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12-4\. Pipeline for named-entity recognition and coreference resolution.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '*Entity linking* goes one step further. Here the mentions of an entity are
    disambiguated on a semantic level and linked to a unique entry in an existing
    knowledge base. Because entity linking is itself a challenging task, we will not
    provide a blueprint for that but just discuss it at the end of this section.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Blueprint: Using spaCy’s Token Extensions'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We need a way to technically create the link from the different mentions of
    an entity to the main reference, the *referent*. After coreference resolution,
    the token for “Kistler” of the example article should point to “(W.A. Kistler,
    PERSON).” spaCy’s extension mechanism allows us to define custom attributes, and
    this is the perfect way to store this kind of information with tokens. Thus, we
    create two token extensions `ref_n` (referent’s name) and `ref_t` (referent’s
    type). The attributes will be initialized for each token with the specified default
    values by spaCy for each token:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The function `init_coref` shown next ensures that each entity mention of type
    `ORG`, `GOV`, and `PERSON` gets an initial reference to itself. This initialization
    is required for the succeeding functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The custom attributes are accessed via the underscore property of the token.
    Note that after `merge_entities`, each entity mention `e` consists of a single
    token `e[0]` where we set the attributes. We could also define the attributes
    on the entity spans instead of tokens, but we want to use the same mechanism for
    pronoun resolution later.
  prefs: []
  type: TYPE_NORMAL
- en: 'Blueprint: Performing Alias Resolution'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Our first targets are well-known domain aliases like *Transportation Department*
    for “U.S. Department of Transportation” and acronyms like SEC or TWA. A simple
    solution to resolve such aliases is to use a lookup dictionary. We prepared such
    a dictionary for all the acronyms and some common aliases of the Reuters corpus
    and provided it as part of the blueprints module for this chapter.^([6](ch12.xhtml#idm45634174792232))
    Here are some example lookups:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Each token alias is mapped to a tuple consisting of an entity name and a type.
    The function `alias_resolver` shown next checks whether an entity’s text is found
    in the dictionary. If so, its `ref` attributes are updated to the looked-up value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we have resolved the aliases, we can also correct the type of the named-entity
    in case it was misidentified. This is done by the function `propagate_ent_type`.
    It updates all resolved aliases and will also be used in the next blueprint for
    name-based coreference resolution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Again, we add the `alias_resolver` to our pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can inspect the results. For this purpose, our provided blueprints package
    includes a utility function `display_ner` that creates a `DataFrame` for the tokens
    in a `doc` object with the relevant attributes for this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | text | ent_type | ref_n | ref_t |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | Trans World Airlines | ORG | Trans World Airlines Inc | ORG |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | U.S. Department of Transportation | GOV | U.S. Department of Transportation
    | GOV |'
  prefs: []
  type: TYPE_TB
- en: '| 12 | Transportation Department | GOV | U.S. Department of Transportation
    | GOV |'
  prefs: []
  type: TYPE_TB
- en: '| 18 | TWA | ORG | Trans World Airlines Inc | ORG |'
  prefs: []
  type: TYPE_TB
- en: 'Blueprint: Resolving Name Variations'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Alias resolution works only if the aliases are known up front. But because
    articles contain variations of almost any names, it is not feasible to build a
    dictionary for all of them. Take a look again at the recognized named entities
    in the first sentences of our introductory example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/btap_12in03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here you find the coreference “Kistler” for W.A. Kistler (`PERSON`), “Baker”
    for Baker International Corp (`ORG`), and “Hughes” for Hughes Tool Co (`ORG`).
    And as you can see, abbreviated company names are often mistaken for people, especially
    when they are used in impersonated form, as in the examples. In this blueprint,
    we will resolve those coreferences and assign the correct entity types to each
    mention.
  prefs: []
  type: TYPE_NORMAL
- en: For that, we will exploit a common pattern in news articles. An entity is usually
    introduced first by its full name, while later mentions use abbreviated versions.
    Thus, we will resolve the secondary references by matching the names to the first
    mention of an entity. Of course, this is a heuristic rule that could produce false
    matches. For example, *Hughes* could also refer in the same article to the company
    and to the legendary entrepreneur Howard Hughes (who indeed founded Hughes Tool
    Co.). But such cases are rare in our dataset, and we decide to accept that uncertainty
    in favor of the many cases where our heuristics works correctly.
  prefs: []
  type: TYPE_NORMAL
- en: 'We define a simple rule for name matching: a secondary mention matches a primary
    mention if all of its words appear in the primary mention in the same order. To
    check this, the function `name_match` shown next transforms a secondary mention
    `m2` into a regular expression and searches for a match in the primary mention
    `m1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: The secondary mention of Hughes Co., for example, would be converted into `'\bHughes\b.*\bCo\b'`,
    which matches Hughes Tool Co. The `\b` ensures that only whole words match and
    not subwords like *Hugh*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on this matching logic, the function `name_resolver` shown next implements
    the name-based coreference resolution for organizations and persons:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: First, we create a list of all organization and person entities. Then all pairs
    of entities `e1` and `e2` are compared against each other. The logic ensures that
    entity `e1` always comes before `e2` in the document. If `e2` matches `e1`, its
    referent is set to the same as in `e1`. Thus, the first matching entity is automatically
    propagated to its subsequent coreferences.
  prefs: []
  type: TYPE_NORMAL
- en: 'We add this function to the `nlp` pipeline and check the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/btap_12in04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now each named-entity in our example has the correct type. We can also check
    that the entities are mapped to their first mention:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '|   | text | ent_type | ref_n | ref_t |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | Hughes Tool Co | ORG | Hughes Tool Co | ORG |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | W.A. Kistler | PERSON | W.A. Kistler | PERSON |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | Baker International Corp. | ORG | Baker International Corp. | ORG |'
  prefs: []
  type: TYPE_TB
- en: '| 22 | Kistler | PERSON | W.A. Kistler | PERSON |'
  prefs: []
  type: TYPE_TB
- en: '| 25 | Baker | ORG | Baker International Corp. | ORG |'
  prefs: []
  type: TYPE_TB
- en: '| 28 | Hughes | ORG | Hughes Tool Co | ORG |'
  prefs: []
  type: TYPE_TB
- en: 'Blueprint: Performing Anaphora Resolution with NeuralCoref'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In linguistics, *anaphora* are words whose interpretation depends on the preceding
    text. Consider this variation of our example sentences:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Here *its*, *the company*, and *he* are anaphora. [NeuralCoref](https://oreil.ly/kQRhE)
    from Hugging Face is a library to resolve these kind of coreferences. The algorithm
    uses feature vectors based on word embeddings (see [Chapter 10](ch10.xhtml#ch-embeddings))
    in combination with two neural networks to identify coreference clusters and their
    main mentions.^([7](ch12.xhtml#idm45634173992248))
  prefs: []
  type: TYPE_NORMAL
- en: 'NeuralCoref is implemented as a pipeline extension for spaCy, so it fits perfectly
    into our process. We create the neural coreference resolver with a `greedyness`
    value of 0.45 and add it to our pipeline. The `greedyness` controls the sensitivity
    of the model, and after some experiments, we decided to choose a little more restrictive
    (better accuracy, lower recall) value than the default 0.5:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'NeuralCoref uses also spaCy’s extension mechanism to add custom attributes
    to `Doc`, `Span`, and `Token` objects. When a text is processed, we can access
    the detected coreference clusters with the `doc._.coref_clusters` attribute. In
    our example, three such clusters have been identified:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'NeuralCoref works on `Span` objects (sequences of token) because coreferences
    in general are not limited to named entities. Thus, the blueprint function `anaphor_coref`
    retrieves for each token the first coreference cluster and searches for the first
    named-entity with a value in its `ref_n` attribute. In our case, this will be
    organizations and people only. Once found, it sets the values in `ref_n` and `ref_t`
    of the anaphor token to the same values as in the primary reference:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Again, we add this resolver to our pipeline and check the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | text | ent_type | main_coref | ref_n | ref_t |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | Hughes Tool Co | ORG | Hughes Tool Co | Hughes Tool Co | ORG |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | its |  | Hughes Tool Co | Hughes Tool Co | ORG |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | Baker | PERSON | None | Baker | PERSON |'
  prefs: []
  type: TYPE_TB
- en: '| 11 | Hughes | ORG | Hughes | Hughes Tool Co | ORG |'
  prefs: []
  type: TYPE_TB
- en: '| 18 | W.A. Kistler | PERSON | W.A. Kistler | W.A. Kistler | PERSON |'
  prefs: []
  type: TYPE_TB
- en: '| 21 | the |  | Hughes | Hughes Tool Co | ORG |'
  prefs: []
  type: TYPE_TB
- en: '| 22 | company |  | Hughes | Hughes Tool Co | ORG |'
  prefs: []
  type: TYPE_TB
- en: '| 29 | He |  | W.A. Kistler | W.A. Kistler | PERSON |'
  prefs: []
  type: TYPE_TB
- en: Now our pipeline consists of all the steps shown in [Figure 12-4](#fig-coref-pipeline).
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Beware of long runtimes! NeuralCoref increases the total processing time by
    a factor of 5–10\. So, you should use anaphora resolution only if necessary.
  prefs: []
  type: TYPE_NORMAL
- en: Name Normalization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Even though our name resolution unifies company mentions within an article,
    the company names are still inconsistent across articles. We find *Hughes Tool
    Co.* in one article and *Hughes Tool* in another one. An entity linker can be
    used to link different entity mentions to a unique canonical representation, but
    in absence of an entity linker we will use the (resolved) name entity as its unique
    identifier. Because of the previous steps for coreference resolution, the resolved
    names are always the first, and thus usually most complete, mentions in an article.
    So, the potential for errors is not that large.
  prefs: []
  type: TYPE_NORMAL
- en: 'Still, we have to harmonize company mentions by removing the legal suffixes
    like *Co.* or *Inc.* from company names. The following function uses a regular
    expression to achieve this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: The last pipeline function `norm_names` applies this final normalization to
    each of the coreference-resolved organization names stored in the `ref_n` attributes.
    Note that `Hughes (PERSON)` and `Hughes (ORG)` will still remain separate entities
    with this approach.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Sometimes the named-entity recognizer misclassifies a legal suffix like *Co.*
    or *Inc.* by itself as named-entity. If such an entity name gets stripped to the
    empty string, we just ignore it for later processing.
  prefs: []
  type: TYPE_NORMAL
- en: Entity Linking
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous sections we developed a pipeline of operations with the purpose
    of unifying the different mentions of named entities. But all this is string based,
    and except for the syntactical representation, we have no connection between the
    string *U.S. Department of Justice* and the represented real-world entity. The
    task of an entity linker, in contrast, is to resolve named entities globally and
    link them to a uniquely identified real-world entity. Entity linking makes the
    step from “strings to things.”^([8](ch12.xhtml#idm45634173497096))
  prefs: []
  type: TYPE_NORMAL
- en: Technically, this means that each mention is mapped to a URI. URIs, in turn,
    address entities in an existing knowledge base. This can be a public ontology,
    like Wikidata or DBpedia, or a private knowledge base in your company. URIs can
    be URLs (e.g., web pages) but do not have to be. The U.S. Department of Justice,
    for example, has the Wikidata URI [*http://www.wikidata.org/entity/Q1553390*](http://www.wikidata.org/entity/Q1553390),
    which is also a web page where you find information about this entity. If you
    build your own knowledge base, it is not necessary to have a web page for each
    URI; they just must be unique. DBpedia and Wikidata, by the way, use different
    URIs, but you will find the Wikidata URI on DBpedia as a cross-reference. Both,
    of course, contain links to the Wikipedia web page.
  prefs: []
  type: TYPE_NORMAL
- en: Entity linking is simple if an entity is mentioned by a fully qualified name,
    like the *U.S. Department of Justice*. But the term *Department of Justice* without
    *U.S.* is already quite ambiguous because many states have a “Department of Justice.”
    The actual meaning depends on the context, and the task of an entity linker is
    to map such an ambiguous mention context-sensitively to the correct URI. This
    is quite a challenge and still an area of ongoing research. A common solution
    for entity linking in business projects is the usage of a public service (see
    [“Services for Entity Linking”](#services-entity-linking)).
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, you could create your own entity linker. A simple solution would
    be a name-based lookup dictionary. But that does not take the context into account
    and would not resolve ambiguous names for different entities. For that, you need
    a more sophisticated approach. State-of-the-art solutions use embeddings and neural
    models for entity linking. spaCy also provides such an [entity linking functionality](https://oreil.ly/bqs8E).
    To use spaCy’s entity linker, you first have to create embeddings (see [Chapter 10](ch10.xhtml#ch-embeddings))
    for the real-world entities, which capture their semantics based on descriptions
    you specify. Then you can train a model to learn the context-sensitive mapping
    of mentions to the correct URI. The setup and training of an entity linker are,
    however, beyond the scope of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Blueprint: Creating a Co-Occurrence Graph'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous sections, we spent much effort to normalize named entities
    and to resolve at least the in-document coreferences. Now we are finally ready
    to analyze a first relationship among pairs of entities: their joint mention in
    an article. For this, we will create a co-occurrence graph, the simplest form
    of a knowledge graph. The nodes in the co-occurrence graph are the entities, e.g.,
    organizations. Two entities share an (undirected) edge if they are mentioned in
    the same context, for example, within an article, a paragraph, or a sentence.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 12-5](#fig-cooc) shows part of the co-occurrence graph for companies
    mentioned together in articles of the Reuters corpus. The width of the edges visualizes
    the co-occurrence frequency. The [*modularity*](https://oreil.ly/pGZ-s), a structural
    measure to identify closely related groups or communities in a network, was used
    to colorize the nodes and edges.^([9](ch12.xhtml#idm45634173458648))'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/btap_1205.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12-5\. Largest connected component of the co-occurrence graph generated
    from the Reuters corpus.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Of course, we don’t know anything about the type of relationship here. In fact,
    the joint mentioning of two entities merely indicates that there *might be some*
    relationship. We won’t know for sure unless we really analyze the sentences, and
    we will do that in the next section. But even the simple exploration of co-occurrences
    can already be revealing. For example, the central node in [Figure 12-5](#fig-cooc)
    is the “Securities and Exchange Commission” because it is mentioned in many articles
    together with a great variety of other entities. Obviously, this entity plays
    a major role in mergers and acquisitions. The different clusters give us an impression
    about groups of companies (or communities) involved in certain deals.
  prefs: []
  type: TYPE_NORMAL
- en: To plot a co-occurrence graph, we have to extract entity pairs from a document.
    For longer articles covering multiple topic areas, it may be better to search
    for co-occurrences within paragraphs or even sentences. But the Reuters articles
    on mergers and acquisitions are very focused, so we stick to the document level
    here. Let’s briefly walk through the process to extract and visualize co-occurrences.
  prefs: []
  type: TYPE_NORMAL
- en: Extracting Co-Occurrences from a Document
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The function `extract_coocs` returns the list of entities pairs of the specified
    types from a given `Doc` object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: We first create a set of the coreference-resolved entity names and types. Having
    this, we use the function `combinations` from the Python standard library `itertools`
    to create all the entity pairs. Each pair is sorted lexicographically (`sorted(ents)`)
    to prevent duplicate entries like “(Baker, Hughes)” and “(Hughes, Baker).”
  prefs: []
  type: TYPE_NORMAL
- en: 'To process the whole dataset efficiently, we use again spaCy’s streaming by
    calling `nlp.pipe` (introduced in [Chapter 4](ch04.xhtml#ch-preparation)). As
    we do not need anaphora resolution to find in-document co-occurrences, we disable
    the respective components here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s take a look at the identified co-occurrences of the first article:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: In information extraction, it is always recommended to have some kind of traceability
    that allows you to identify the source of the information in the case of problems.
    Therefore, we retain the index of the article, which in our case is the file ID
    of the Reuters corpus, with each co-occurrence tuple (here the ID 10). Based on
    this list, we generate a `DataFrame` with exactly one entry per entity combination,
    its frequency, and the article IDs (limited to five) where this co-occurrence
    was found.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Here are the three most frequent entity pairs we found in the corpus:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | ent1 | type1 | ent2 | type2 | freq | articles |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 12667 | Trans World Airlines | ORG | USAir Group | ORG | 22 | 1735,1771,1836,1862,1996
    |'
  prefs: []
  type: TYPE_TB
- en: '| 5321 | Cyclops | ORG | Dixons Group | ORG | 21 | 4303,4933,6093,6402,7110
    |'
  prefs: []
  type: TYPE_TB
- en: '| 12731 | U.S. Department of Transportation | GOV | USAir Group | ORG | 20
    | 1735,1996,2128,2546,2799 |'
  prefs: []
  type: TYPE_TB
- en: Visualizing the Graph with Gephi
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Actually, this `DataFrame` already represents the list of edges for our graph.
    For the visualization we prefer [Gephi](https://gephi.org), an open source tool
    for graph analysis. Because it is interactive, it is much better to use than Python’s
    graph library NetworkX.^([10](ch12.xhtml#idm45634172970888)) To work with Gephi,
    we need to save the list of nodes and edges of the graph in Graph Exchange XML
    format. Fortunately, NetworkX provides a function to export graphs in this format.
    So, we can simply convert our `DataFrame` into a NetworkX graph and save it as
    a `.gexf` file. We discard rare entity pairs to keep the graph compact and rename
    the frequency column because Gephi automatically uses a `weight` attribute to
    adjust the width of edges:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: After importing the file into Gephi, we selected only the largest component
    (connected subgraph) and removed some nodes with only a few connections manually
    for the sake of clarity.^([11](ch12.xhtml#idm45634172964856)) The result is presented
    in [Figure 12-5](#fig-cooc).
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Sometimes the most interesting relations are the ones that are not frequent.
    Take, for example, the first announcement on an upcoming merger or surprising
    relations that were mentioned a few times in the past but then forgotten. A sudden
    co-occurrence of entities that were previously unrelated can be a signal to start
    a deeper analysis of the relation.
  prefs: []
  type: TYPE_NORMAL
- en: Relation Extraction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Even though the co-occurrence graph already gave us some interesting insights
    about company networks, it does not tell us anything about the types of the relations.
    Take, for example, the subgraph formed by the companies Schlumberger, Fairchild
    Semiconductor, and Fujitsu in the lower-left corner of [Figure 12-5](#fig-cooc).
    So far, we know nothing about the relations between those companies; the information
    is still hidden in sentences like these:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Fujitsu wants to expand. It plans to acquire 80% of Fairchild Corp, an industrial
    unit of Schlumberger.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we will introduce two blueprints for pattern-based relation
    extraction. The first and simpler blueprint searches for token phrases of the
    form “subject-predicate-object.” The second one uses the syntactical structure
    of a sentence, the dependency tree, to get more precise results at the price of
    more complex rules. In the end, we will generate a knowledge graph based on the
    four relations: *acquires*, *sells*, *subsidiary-of*, and *chairperson-of*. To
    be honest, we will use relaxed definitions of *acquires* and *sells*, which are
    easier to identify. They will also match sentences like “Fujitsu *plans to acquire
    80%* of Fairchild Corp” or even “Fujitsu *withdraws the option to acquire* Fairchild
    Corp.”'
  prefs: []
  type: TYPE_NORMAL
- en: Relation extraction is a complicated problem because of the ambiguity of natural
    language and the many different kinds and variations of relations. Model-based
    approaches to relation extraction are a current topic in research.^([12](ch12.xhtml#idm45634172825528))
    There are also some training datasets like [FewRel](http://zhuhao.me/fewrel) publicly
    available. However, training a model to identify relations is still pretty much
    in the research stage and out of the scope of this book.
  prefs: []
  type: TYPE_NORMAL
- en: 'Blueprint: Extracting Relations Using Phrase Matching'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The first blueprint works like rule-based entity recognition: it tries to identify
    relations based on patterns for token sequences. Let’s start with a simplified
    version of the introductory example to explain the approach.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'We could find the relations in this sentence by searching for patterns like
    these:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: '[spaCy’s rule-based matcher](https://oreil.ly/Mxd3m) allows us to find patterns
    that not only can involve the textual tokens but also their properties, like the
    lemma or part of speech. To use it, we must first define a matcher object. Then
    we can add rules with token patternsto the matcher:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: The first pattern is for the `acquires` relation. It returns all spans consisting
    of an organization, followed by arbitrary tokens that are not organizations, a
    verb matching several synonyms of *acquire*, again arbitrary tokens, and finally
    the second organization. The second pattern for `subsidiary-of` works similarly.
  prefs: []
  type: TYPE_NORMAL
- en: Granted, the expressions are hard to read. One reason is that we used the custom
    attribute `ref_t` instead of the standard `ENT_TYPE`. This is necessary to match
    coreferences that are not marked as entities, e.g., pronouns. Another one is that
    we have included some `NOT_IN` clauses. This is because rules with the asterisk
    operator (`*`) are always dangerous as they search patterns of unbounded length.
    Additional conditions on the tokens can reduce the risk for false matches. For
    example, we want to match “Fairchild, an industrial unit of Schlumberger” for
    the `subsidiary-of` relation, but not “Fujitsu mentioned a unit of Schlumberger.”
    When developing rules, you always have to pay for precision with complexity. We
    will discuss the problems of the `acquires` relation on that aspect in a minute.
  prefs: []
  type: TYPE_NORMAL
- en: 'The blueprint function `extract_rel_match` now takes a processed `Doc` object
    and a matcher and transforms all matches to subject-predicate-object triples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: The predicate is determined by the name of the rule; the involved entities are
    simply the first and last tokens of the matched span. We restrict the search to
    the sentence level because in a whole document we would have a high risk of finding
    false positives spanning multiple sentences.
  prefs: []
  type: TYPE_NORMAL
- en: 'Usually, the rules match in the order “subject-predicate-object,” but often
    the entities appear in the text in reversed order, like in “the Schlumberger unit
    Fairchild Corp.” Here, the order of entities with regard to the `subsidiary-of`
    relation is “object-predicate-subject.” `extract_rel_match` is prepared to handle
    this and switches the subject and object if a rule has the prefix `rev-` like
    this one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we are able to detect `acquires` and both variants of `subsidiary-of` in
    sentences like these:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Although the rules work nicely for our examples, the rule for *acquires* is
    not very reliable. The verb *acquire* can appear in many different constellations
    of entities. Thus, there is a high probability for false matches like this one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Or this one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: Obviously, our rule wasn’t made for passive clauses (“was acquired by”) where
    the subject and object switch positions. We also cannot handle insertions containing
    named entities or negations because they produce false matches. To treat those
    cases correctly, we need knowledge about the syntactical structure of the sentence.
    And we get that from the dependency tree.
  prefs: []
  type: TYPE_NORMAL
- en: 'But let’s first remove the unreliable rule for *acquires* from the matcher:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'Blueprint: Extracting Relations Using Dependency Trees'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The grammatical rules of a language impose a syntactical structure on each
    sentence. Each word serves a certain role in relation to the other words. A noun,
    for example, can be the subject or the object in a sentence; it depends on its
    relation to the verb. In linguistic theory, the words of a sentence are hierarchically
    interdependent, and the task of the parser in an NLP pipeline is to reconstruct
    these dependencies.^([13](ch12.xhtml#idm45634172046632)) The result is a *dependency
    tree*, which can also be visualized by `displacy`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: '![](Images/btap_12in05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Each node in the dependency tree represents a word. The edges are labeled with
    the dependency information. The root is usually the predicate of the sentence,
    in this case *acquired*, having a subject (`nsubj`) and an object (`obj`) as direct
    children. This first level, root plus children, already represents the essence
    of the sentence “Fujitsu acquired Fairchild Corp.”
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s also take a look at the example with the passive clause. In this case,
    the auxiliary verb (`auxpass`) signals that *acquired* was used in passive form
    and *Fairchild* is the passive subject (`nsubjpass`):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/btap_12in06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The values of the dependency labels depend on the corpus the parser model was
    trained on. They are also language dependent because different languages have
    different grammar rules. So, you definitely need to check which tag set is used
    by the dependency parser.
  prefs: []
  type: TYPE_NORMAL
- en: 'The function `extract_rel_dep` implements a rule to identify verb-based relations
    like *acquires* based on the dependencies:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: The main loop iterates through all tokens in a doc and searches for a verb signaling
    our relationship. This condition is the same as in the flat pattern rule we used
    before. But when we detect a possible predicate, we now traverse the dependency
    tree to find the correct subject and the object. `find_subj` searches the left
    subtree, and `find_obj` searches the right subtree of the predicate. Those functions
    are not printed in the book, but you can find them in the GitHub notebook for
    this chapter. They use breadth-first search to find the closest subject and object,
    as nested sentences may have multiple subjects and objects. Finally, if the predicate
    indicates a passive clause, the subject and object will be swapped.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note, that this function also works for the *sells* relation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'In this case, *Fairchild Inc.* is the closest object in the dependency tree
    to *sell* and identified correctly as the object of the investigated relation.
    But to be the “closest” is not always sufficient. Consider this example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/btap_12in07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Actually, we have a three-way relation here: Schlumberger sells Fairchild to
    Fujitsu. Our *sells* relation is intended to have the meaning “one company sells
    [whole or parts of] another company.” The other part is covered by the *acquires*
    relation. But how can we detect the right object here? Both Fujitsu and Fairchild
    are prepositional objects in this sentence (dependency `pobj`), and Fujitsu is
    the closest. The preposition is the key: Schlumberger sells something “to” Fujitsu,
    so that’s not the relation we are looking for. The purpose of the parameter `excl_prepos`
    in the extraction function is to skip objects with the specified prepositions.
    Here is the output without (A) and with (B) preposition filter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s check how our new relation extraction function works on a few variations
    of the examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, the relations in the first four sentences have been correctly
    extracted. Sentence 5, however, contains a negation and still returns `acquires`.
    This is a typical case of a false positive. We could extend our rules to handle
    this case correctly, but negations are rare in our corpus, and we accept the uncertainty
    in favor of the simpler algorithm. Sentence 6, in contrast, is an example for
    a possible false negative. Even though the relation was mentioned, it was not
    detected because the subject in this sentence is *competition* and not one of
    the companies.
  prefs: []
  type: TYPE_NORMAL
- en: Actually, dependency-based rules are inherently complex, and every approach
    to make them more precise results in even more complexity. It is a challenge to
    find a good balance between precision (fewer false positives) and recall (fewer
    false negatives) without making the code too complex.
  prefs: []
  type: TYPE_NORMAL
- en: Despite those deficiencies, the dependency-based rule still yields good results.
    This last step in the process, however, depends on the correctness of named-entity
    recognition, coreference resolution, and dependency parsing, all of which are
    not working with 100% accuracy. So, there will always be some false positives
    and false negatives. But the approach is good enough to produce highly interesting
    knowledge graphs, as we will do in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the Knowledge Graph
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we know how to extract certain relationships, we can put everything
    together and create a knowledge graph from the entire Reuters corpus. We will
    extract organizations, persons and the four relations “acquires,” “sells,” “subsidiary-of,”
    and “executive-of.” [Figure 12-6](#fig-knowledge-graph) shows the resulting graph
    with some selected subgraphs.
  prefs: []
  type: TYPE_NORMAL
- en: 'To get the best results in dependency parsing and named-entity recognition,
    we use spaCy’s large model with our complete pipeline. If possible, we will use
    the GPU to speed up NLP processing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'Before we start the information extraction process, we create two additional
    rules for the “executive-of” relation similar to the “subsidiary-of” relation
    and add them to our rule-based `matcher`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: '![](Images/btap_1206.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12-6\. The knowledge graph extracted from the Reuters corpus with three
    selected subgraphs (visualized with the help of Gephi).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'We then define one function to extract all relationships. Two of our four relations
    are covered by the `matcher`, and the other two by the dependency-based matching
    algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'The remaining steps to extract the relations, convert them into a NetworkX
    graph, and store the graph in a `gexf` file for Gephi are basically following
    [“Blueprint: Creating a Co-Occurrence Graph”](#ch12-cooc). We skip them here,
    but you will find the full code again in the GitHub repository.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are a few records of the final data frame containing the nodes and edges
    of the graph as they are written to the `gexf` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | subj | subj_type | pred | obj | obj_type | freq | articles |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 883 | Trans World Airlines | ORG | acquires | USAir Group | ORG | 7 | 2950,2948,3013,3095,1862,1836,7650
    |'
  prefs: []
  type: TYPE_TB
- en: '| 152 | Carl Icahn | PERSON | executive-of | Trans World Airlines | ORG | 3
    | 1836,2799,3095 |'
  prefs: []
  type: TYPE_TB
- en: '| 884 | Trans World Airlines | ORG | sells | USAir Group | ORG | 1 | 9487 |'
  prefs: []
  type: TYPE_TB
- en: The visualization of the Reuters graph in [Figure 12-6](#fig-knowledge-graph)
    was again created with the help of Gephi. The graph consists of many rather small
    components (disconnected subgraphs); because most companies got mentioned in only
    one or two news articles and we extracted only the four relations, simple co-occurrences
    are not included here. We manually magnified three of those subgraphs in the figure.
    They represent company networks that already appeared in the co-occurrence graph
    ([Figure 12-5](#fig-cooc)), but now we know the relation types and get a much
    clearer picture.
  prefs: []
  type: TYPE_NORMAL
- en: Don’t Blindly Trust the Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Each processing step we went through has a potential of errors. Thus, the information
    stored in the graph is not completely reliable. In fact, this starts with data
    quality in the articles themselves. If you look carefully at the upper-left example
    in [Figure 12-6](#fig-knowledge-graph), you will notice that the two entities
    “Fujitsu” and “Futjitsu” appear in the graph. This is indeed a spelling error
    in the original text.
  prefs: []
  type: TYPE_NORMAL
- en: In the magnified subnetwork to the right in [Figure 12-6](#fig-knowledge-graph)
    you can spot the seemingly contradictory information that “Piedmont acquires USAir”
    and “USAir acquires Piedmont.” In fact, both are true because both enterprises
    acquired parts of the shares of the other one. But it could also be a mistake
    by one of the involved rules or models. To track this kind of problem, it is indispensable
    to store some information about the source of the extracted relations. That’s
    why we included the list of articles in every record.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, be aware that our analysis did not consider one aspect at all: the
    timeliness of information. The world is constantly changing and so are the relationships.
    Each edge in our graph should therefore get time stamped. So, there is still much
    to be done to create a knowledge base with trustable information, but our blueprint
    provides a solid foundation for getting started.'
  prefs: []
  type: TYPE_NORMAL
- en: Closing Remarks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored how to build a knowledge graph by extracting structured
    information from unstructured text. We went through the whole process of information
    extraction, from named-entity recognition via coreference resolution to relation
    extraction.
  prefs: []
  type: TYPE_NORMAL
- en: As you have seen, each step is a challenge in itself, and we always have the
    choice between a rule-based and a model-based approach. Rule-based approaches
    have the advantage that you don’t need training data. So, you can start right
    away; you just need to define the rules. But if the entity type or relationship
    you try to capture is complex to describe, you end up either with rules that are
    too simple and return a lot of false matches or with rules that are extremely
    complex and hard to maintain. When using rules, it is always difficult to find
    a good balance between recall (find most of the matches) and precision (find only
    correct matches). And you need quite a bit of technical, linguistic, and domain
    expertise to write good rules. In practice, you will also have to test and experiment
    a lot until your rules are robust enough for your application.
  prefs: []
  type: TYPE_NORMAL
- en: Model-based approaches, in contrast, have the great advantage that they learn
    those rules from the training data. Of course, the downside is that you need lots
    of high-quality training data. And if those training data are specific to your
    application domain, you have to create them yourself. The manual labeling of training
    data is especially cumbersome and time-consuming in the area of text because somebody
    has to read and understand the text before the labels can be set. In fact, getting
    good training data is the biggest bottleneck today in machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: A possible solution to the problem of missing training data is weak supervision.
    The idea is to create a large dataset by rules like the ones we defined in this
    chapter or even to generate them programmatically. Of course, this dataset will
    be noisy, as the rules are not perfect. But, surprisingly, it is possible to train
    a high-quality model on low-quality data. Weak supervision learning for named-entity
    recognition and relationship extraction is, like many other topics covered in
    this section, a current topic of research. If you want to learn more about the
    state of the art in information extraction and knowledge graph creation, you can
    check out the following references. They provide good starting points for further
    reading.
  prefs: []
  type: TYPE_NORMAL
- en: Further Reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Barrière, Caroline. *Natural Language Understanding in a Semantic Web Context.*
    Switzerland: Springer Publishing. 2016\. [*https://www.springer.com/de/book/9783319413358*](https://www.springer.com/de/book/9783319413358).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gao, Yuqing, Jisheng Liang, Benjamin Han, Mohamed Yakout, and Ahmed Mohamed.
    *Building a Large-scale, Accurate and Fresh Knowledge Graph*. Tutorial at KDD.
    2018\. [*https://kdd2018tutorialt39.azurewebsites.net*](https://kdd2018tutorialt39.azurewebsites.net).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Han, Xu, Hao Zhu, Pengfei Yu, Ziyun Wang, Yuan Yao, Zhiyuan Liu, and Maosong
    Sun. *FewRel: A Large-Scale Supervised Few-Shot Relation Classification Dataset
    with State-of-the-Art Evaluation*. Proceedings of EMNLP, 2018\. [*https://arxiv.org/abs/1810.10147*](https://arxiv.org/abs/1810.10147).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jurafsky, Dan, and James H. Martin. *Speech and Language Processing*. 3rd Edition
    (draft), Chapters 18 and 22\. 2019\. [*https://web.stanford.edu/~jurafsky/slp3*](https://web.stanford.edu/~jurafsky/slp3).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lison, Pierre, Aliaksandr Hubin, Jeremy Barnes, and Samia Touileb. *Named-Entity
    Recognition without Labelled Data: A Weak Supervision Approach*. Proceedings of
    ACL, 2020 [*https://arxiv.org/abs/2004.14723*](https://arxiv.org/abs/2004.14723).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '^([1](ch12.xhtml#idm45634176075192-marker)) See Natasha Noy, Yuqing Gao, Anshu
    Jain, Anant Narayanan, Alan Patterson, and Jamie Taylor. *Industry-scale Knowledge
    Graphs: Lessons and Challenges*. 2019\. [*https://queue.acm.org/detail.cfm?id=3332266*](https://queue.acm.org/detail.cfm?id=3332266).'
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch12.xhtml#idm45634176061992-marker)) See [*https://oreil.ly/nzhUR*](https://oreil.ly/nzhUR)
    for details.
  prefs: []
  type: TYPE_NORMAL
- en: '^([3](ch12.xhtml#idm45634176054984-marker)) Tim Berners-Lee et al., “The Semantic
    Web: a New Form of Web Content that is Meaningful to Computers Will Unleash a
    Revolution of New Possibilities.” *Scientific American* 284 No. 5: May 2001.'
  prefs: []
  type: TYPE_NORMAL
- en: ^([4](ch12.xhtml#idm45634175830936-marker)) The asterisk operator (*) unpacks
    the list into separate arguments for `print`.
  prefs: []
  type: TYPE_NORMAL
- en: ^([5](ch12.xhtml#idm45634175637736-marker)) See [spaCy’s rule-based matching
    usage docs](https://oreil.ly/Hvtgs) for an explanation of the syntax, and check
    out the interactive pattern explorer on [*https://explosion.ai/demos/matcher*](https://explosion.ai/demos/matcher).
  prefs: []
  type: TYPE_NORMAL
- en: ^([6](ch12.xhtml#idm45634174792232-marker)) You will find an additional blueprint
    for acronym detection in the notebook for this chapter on [GitHub](https://oreil.ly/LlPHm).
  prefs: []
  type: TYPE_NORMAL
- en: ^([7](ch12.xhtml#idm45634173992248-marker)) See Wolf (2017),[“State-Of-The-Art
    Neural Coreference Resolution For Chatbots”](https://oreil.ly/VV4Uy) for more.
  prefs: []
  type: TYPE_NORMAL
- en: ^([8](ch12.xhtml#idm45634173497096-marker)) This slogan was coined by Google
    when it introduced its knowledge graph in 2012.
  prefs: []
  type: TYPE_NORMAL
- en: ^([9](ch12.xhtml#idm45634173458648-marker)) You’ll find the colorized figures
    in the electronic versions of this book and in our [GitHub repository](https://oreil.ly/2ju0k).
  prefs: []
  type: TYPE_NORMAL
- en: ^([10](ch12.xhtml#idm45634172970888-marker)) You can find a NetworkX version
    of the graph in the notebook for this chapter on [GitHub](https://oreil.ly/OWTcO).
  prefs: []
  type: TYPE_NORMAL
- en: ^([11](ch12.xhtml#idm45634172964856-marker)) We provide more details on that
    in our [GitHub repository](https://oreil.ly/nri01) for this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: ^([12](ch12.xhtml#idm45634172825528-marker)) See an overview of the [state of
    the art](https://oreil.ly/l6DIH).
  prefs: []
  type: TYPE_NORMAL
- en: ^([13](ch12.xhtml#idm45634172046632-marker)) Constituency parsers, in contrast
    to dependency parsers, create a hierarchical sentence structure based on nested
    phrases.
  prefs: []
  type: TYPE_NORMAL
