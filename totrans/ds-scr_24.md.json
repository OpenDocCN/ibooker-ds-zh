["```py\nusers_interests = [\n    [\"Hadoop\", \"Big Data\", \"HBase\", \"Java\", \"Spark\", \"Storm\", \"Cassandra\"],\n    [\"NoSQL\", \"MongoDB\", \"Cassandra\", \"HBase\", \"Postgres\"],\n    [\"Python\", \"scikit-learn\", \"scipy\", \"numpy\", \"statsmodels\", \"pandas\"],\n    [\"R\", \"Python\", \"statistics\", \"regression\", \"probability\"],\n    [\"machine learning\", \"regression\", \"decision trees\", \"libsvm\"],\n    [\"Python\", \"R\", \"Java\", \"C++\", \"Haskell\", \"programming languages\"],\n    [\"statistics\", \"probability\", \"mathematics\", \"theory\"],\n    [\"machine learning\", \"scikit-learn\", \"Mahout\", \"neural networks\"],\n    [\"neural networks\", \"deep learning\", \"Big Data\", \"artificial intelligence\"],\n    [\"Hadoop\", \"Java\", \"MapReduce\", \"Big Data\"],\n    [\"statistics\", \"R\", \"statsmodels\"],\n    [\"C++\", \"deep learning\", \"artificial intelligence\", \"probability\"],\n    [\"pandas\", \"R\", \"Python\"],\n    [\"databases\", \"HBase\", \"Postgres\", \"MySQL\", \"MongoDB\"],\n    [\"libsvm\", \"regression\", \"support vector machines\"]\n]\n```", "```py\nfrom collections import Counter\n\npopular_interests = Counter(interest\n                            for user_interests in users_interests\n                            for interest in user_interests)\n```", "```py\n[('Python', 4),\n ('R', 4),\n ('Java', 3),\n ('regression', 3),\n ('statistics', 3),\n ('probability', 3),\n # ...\n]\n```", "```py\nfrom typing import List, Tuple\n\ndef most_popular_new_interests(\n        user_interests: List[str],\n        max_results: int = 5) -> List[Tuple[str, int]]:\n    suggestions = [(interest, frequency)\n                   for interest, frequency in popular_interests.most_common()\n                   if interest not in user_interests]\n    return suggestions[:max_results]\n```", "```py\n[\"NoSQL\", \"MongoDB\", \"Cassandra\", \"HBase\", \"Postgres\"]\n```", "```py\n[('Python', 4), ('R', 4), ('Java', 3), ('regression', 3), ('statistics', 3)]\n```", "```py\n[('Java', 3), ('HBase', 3), ('Big Data', 3),\n ('neural networks', 2), ('Hadoop', 2)]\n```", "```py\nunique_interests = sorted({interest\n                           for user_interests in users_interests\n                           for interest in user_interests})\n```", "```py\nassert unique_interests[:6] == [\n    'Big Data',\n    'C++',\n    'Cassandra',\n    'HBase',\n    'Hadoop',\n    'Haskell',\n    # ...\n]\n```", "```py\ndef make_user_interest_vector(user_interests: List[str]) -> List[int]:\n    \"\"\"\n Given a list of interests, produce a vector whose ith element is 1\n if unique_interests[i] is in the list, 0 otherwise\n \"\"\"\n    return [1 if interest in user_interests else 0\n            for interest in unique_interests]\n```", "```py\nuser_interest_vectors = [make_user_interest_vector(user_interests)\n                         for user_interests in users_interests]\n```", "```py\nfrom scratch.nlp import cosine_similarity\n\nuser_similarities = [[cosine_similarity(interest_vector_i, interest_vector_j)\n                      for interest_vector_j in user_interest_vectors]\n                     for interest_vector_i in user_interest_vectors]\n```", "```py\n# Users 0 and 9 share interests in Hadoop, Java, and Big Data\nassert 0.56 < user_similarities[0][9] < 0.58, \"several shared interests\"\n\n# Users 0 and 8 share only one interest: Big Data\nassert 0.18 < user_similarities[0][8] < 0.20, \"only one shared interest\"\n```", "```py\ndef most_similar_users_to(user_id: int) -> List[Tuple[int, float]]:\n    pairs = [(other_user_id, similarity)                      # Find other\n             for other_user_id, similarity in                 # users with\n                enumerate(user_similarities[user_id])         # nonzero\n             if user_id != other_user_id and similarity > 0]  # similarity.\n\n    return sorted(pairs,                                      # Sort them\n                  key=lambda pair: pair[-1],                  # most similar\n                  reverse=True)                               # first.\n```", "```py\n[(9, 0.5669467095138409),\n (1, 0.3380617018914066),\n (8, 0.1889822365046136),\n (13, 0.1690308509457033),\n (5, 0.1543033499620919)]\n```", "```py\nfrom collections import defaultdict\n\ndef user_based_suggestions(user_id: int,\n                           include_current_interests: bool = False):\n    # Sum up the similarities\n    suggestions: Dict[str, float] = defaultdict(float)\n    for other_user_id, similarity in most_similar_users_to(user_id):\n        for interest in users_interests[other_user_id]:\n            suggestions[interest] += similarity\n\n    # Convert them to a sorted list\n    suggestions = sorted(suggestions.items(),\n                         key=lambda pair: pair[-1],  # weight\n                         reverse=True)\n\n    # And (maybe) exclude already interests\n    if include_current_interests:\n        return suggestions\n    else:\n        return [(suggestion, weight)\n                for suggestion, weight in suggestions\n                if suggestion not in users_interests[user_id]]\n```", "```py\n[('MapReduce', 0.5669467095138409),\n ('MongoDB', 0.50709255283711),\n ('Postgres', 0.50709255283711),\n ('NoSQL', 0.3380617018914066),\n ('neural networks', 0.1889822365046136),\n ('deep learning', 0.1889822365046136),\n ('artificial intelligence', 0.1889822365046136),\n #...\n]\n```", "```py\ninterest_user_matrix = [[user_interest_vector[j]\n                         for user_interest_vector in user_interest_vectors]\n                        for j, _ in enumerate(unique_interests)]\n```", "```py\n[1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0]\n```", "```py\ninterest_similarities = [[cosine_similarity(user_vector_i, user_vector_j)\n                          for user_vector_j in interest_user_matrix]\n                         for user_vector_i in interest_user_matrix]\n```", "```py\ndef most_similar_interests_to(interest_id: int):\n    similarities = interest_similarities[interest_id]\n    pairs = [(unique_interests[other_interest_id], similarity)\n             for other_interest_id, similarity in enumerate(similarities)\n             if interest_id != other_interest_id and similarity > 0]\n    return sorted(pairs,\n                  key=lambda pair: pair[-1],\n                  reverse=True)\n```", "```py\n[('Hadoop', 0.8164965809277261),\n ('Java', 0.6666666666666666),\n ('MapReduce', 0.5773502691896258),\n ('Spark', 0.5773502691896258),\n ('Storm', 0.5773502691896258),\n ('Cassandra', 0.4082482904638631),\n ('artificial intelligence', 0.4082482904638631),\n ('deep learning', 0.4082482904638631),\n ('neural networks', 0.4082482904638631),\n ('HBase', 0.3333333333333333)]\n```", "```py\ndef item_based_suggestions(user_id: int,\n                           include_current_interests: bool = False):\n    # Add up the similar interests\n    suggestions = defaultdict(float)\n    user_interest_vector = user_interest_vectors[user_id]\n    for interest_id, is_interested in enumerate(user_interest_vector):\n        if is_interested == 1:\n            similar_interests = most_similar_interests_to(interest_id)\n            for interest, similarity in similar_interests:\n                suggestions[interest] += similarity\n\n    # Sort them by weight\n    suggestions = sorted(suggestions.items(),\n                         key=lambda pair: pair[-1],\n                         reverse=True)\n\n    if include_current_interests:\n        return suggestions\n    else:\n        return [(suggestion, weight)\n                for suggestion, weight in suggestions\n                if suggestion not in users_interests[user_id]]\n```", "```py\n[('MapReduce', 1.861807319565799),\n ('Postgres', 1.3164965809277263),\n ('MongoDB', 1.3164965809277263),\n ('NoSQL', 1.2844570503761732),\n ('programming languages', 0.5773502691896258),\n ('MySQL', 0.5773502691896258),\n ('Haskell', 0.5773502691896258),\n ('databases', 0.5773502691896258),\n ('neural networks', 0.4082482904638631),\n ('deep learning', 0.4082482904638631),\n ('C++', 0.4082482904638631),\n ('artificial intelligence', 0.4082482904638631),\n ('Python', 0.2886751345948129),\n ('R', 0.2886751345948129)]\n```", "```py\n# This points to the current directory, modify if your files are elsewhere.\nMOVIES = \"u.item\"   # pipe-delimited: movie_id|title|...\nRATINGS = \"u.data\"  # tab-delimited: user_id, movie_id, rating, timestamp\n```", "```py\nfrom typing import NamedTuple\n\nclass Rating(NamedTuple):\n    user_id: str\n    movie_id: str\n    rating: float\n```", "```py\nimport csv\n# We specify this encoding to avoid a UnicodeDecodeError.\n# See: https://stackoverflow.com/a/53136168/1076346.\nwith open(MOVIES, encoding=\"iso-8859-1\") as f:\n    reader = csv.reader(f, delimiter=\"|\")\n    movies = {movie_id: title for movie_id, title, *_ in reader}\n```", "```py\n# Create a list of [Rating]\nwith open(RATINGS, encoding=\"iso-8859-1\") as f:\n    reader = csv.reader(f, delimiter=\"\\t\")\n    ratings = [Rating(user_id, movie_id, float(rating))\n               for user_id, movie_id, rating, _ in reader]\n\n# 1682 movies rated by 943 users\nassert len(movies) == 1682\nassert len(list({rating.user_id for rating in ratings})) == 943\n```", "```py\nimport re\n\n# Data structure for accumulating ratings by movie_id\nstar_wars_ratings = {movie_id: []\n                     for movie_id, title in movies.items()\n                     if re.search(\"Star Wars|Empire Strikes|Jedi\", title)}\n\n# Iterate over ratings, accumulating the Star Wars ones\nfor rating in ratings:\n    if rating.movie_id in star_wars_ratings:\n        star_wars_ratings[rating.movie_id].append(rating.rating)\n\n# Compute the average rating for each movie\navg_ratings = [(sum(title_ratings) / len(title_ratings), movie_id)\n               for movie_id, title_ratings in star_wars_ratings.items()]\n\n# And then print them in order\nfor avg_rating, movie_id in sorted(avg_ratings, reverse=True):\n    print(f\"{avg_rating:.2f} {movies[movie_id]}\")\n```", "```py\n4.36 Star Wars (1977)\n4.20 Empire Strikes Back, The (1980)\n4.01 Return of the Jedi (1983)\n```", "```py\nimport random\nrandom.seed(0)\nrandom.shuffle(ratings)\n\nsplit1 = int(len(ratings) * 0.7)\nsplit2 = int(len(ratings) * 0.85)\n\ntrain = ratings[:split1]              # 70% of the data\nvalidation = ratings[split1:split2]   # 15% of the data\ntest = ratings[split2:]               # 15% of the data\n```", "```py\navg_rating = sum(rating.rating for rating in train) / len(train)\nbaseline_error = sum((rating.rating - avg_rating) ** 2\n                     for rating in test) / len(test)\n\n# This is what we hope to do better than\nassert 1.26 < baseline_error < 1.27\n```", "```py\nfrom scratch.deep_learning import random_tensor\n\nEMBEDDING_DIM = 2\n\n# Find unique ids\nuser_ids = {rating.user_id for rating in ratings}\nmovie_ids = {rating.movie_id for rating in ratings}\n\n# Then create a random vector per id\nuser_vectors = {user_id: random_tensor(EMBEDDING_DIM)\n                for user_id in user_ids}\nmovie_vectors = {movie_id: random_tensor(EMBEDDING_DIM)\n                 for movie_id in movie_ids}\n```", "```py\nfrom typing import List\nimport tqdm\nfrom scratch.linear_algebra import dot\n\ndef loop(dataset: List[Rating],\n         learning_rate: float = None) -> None:\n    with tqdm.tqdm(dataset) as t:\n        loss = 0.0\n        for i, rating in enumerate(t):\n            movie_vector = movie_vectors[rating.movie_id]\n            user_vector = user_vectors[rating.user_id]\n            predicted = dot(user_vector, movie_vector)\n            error = predicted - rating.rating\n            loss += error ** 2\n\n            if learning_rate is not None:\n                #     predicted = m_0 * u_0 + ... + m_k * u_k\n                # So each u_j enters output with coefficent m_j\n                # and each m_j enters output with coefficient u_j\n                user_gradient = [error * m_j for m_j in movie_vector]\n                movie_gradient = [error * u_j for u_j in user_vector]\n\n                # Take gradient steps\n                for j in range(EMBEDDING_DIM):\n                    user_vector[j] -= learning_rate * user_gradient[j]\n                    movie_vector[j] -= learning_rate * movie_gradient[j]\n\n            t.set_description(f\"avg loss: {loss / (i + 1)}\")\n```", "```py\nlearning_rate = 0.05\nfor epoch in range(20):\n    learning_rate *= 0.9\n    print(epoch, learning_rate)\n    loop(train, learning_rate=learning_rate)\n    loop(validation)\nloop(test)\n```", "```py\nfrom scratch.working_with_data import pca, transform\n\noriginal_vectors = [vector for vector in movie_vectors.values()]\ncomponents = pca(original_vectors, 2)\n```", "```py\nratings_by_movie = defaultdict(list)\nfor rating in ratings:\n    ratings_by_movie[rating.movie_id].append(rating.rating)\n\nvectors = [\n    (movie_id,\n     sum(ratings_by_movie[movie_id]) / len(ratings_by_movie[movie_id]),\n     movies[movie_id],\n     vector)\n    for movie_id, vector in zip(movie_vectors.keys(),\n                                transform(original_vectors, components))\n]\n\n# Print top 25 and bottom 25 by first principal component\nprint(sorted(vectors, key=lambda v: v[-1][0])[:25])\nprint(sorted(vectors, key=lambda v: v[-1][0])[-25:])\n```"]