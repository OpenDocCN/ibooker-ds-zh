- en: 'Chapter 49\. In Depth: Kernel Density Estimation'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第49章\. 深入探讨：核密度估计
- en: 'In [Chapter 48](ch48.xhtml#section-0512-gaussian-mixtures) we covered Gaussian
    mixture models, which are a kind of hybrid between a clustering estimator and
    a density estimator. Recall that a density estimator is an algorithm that takes
    a <math alttext="upper D"><mi>D</mi></math> -dimensional dataset and produces
    an estimate of the <math alttext="upper D"><mi>D</mi></math> -dimensional probability
    distribution that data is drawn from. The GMM algorithm accomplishes this by representing
    the density as a weighted sum of Gaussian distributions. *Kernel density estimation*
    (KDE) is in some senses an algorithm that takes the mixture-of-Gaussians idea
    to its logical extreme: it uses a mixture consisting of one Gaussian component
    *per point*, resulting in an essentially nonparametric estimator of density. In
    this chapter, we will explore the motivation and uses of KDE.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第48章](ch48.xhtml#section-0512-gaussian-mixtures)中，我们讨论了高斯混合模型，这是一种聚类估计器和密度估计器之间的混合类型。回想一下，密度估计器是一种算法，它接受一个<math
    alttext="upper D"><mi>D</mi></math>维数据集，并生成数据抽取自其中的<math alttext="upper D"><mi>D</mi></math>维概率分布的估计。GMM算法通过将密度表示为高斯分布的加权和来实现这一点。*核密度估计*（KDE）在某种意义上是将高斯混合思想推向其逻辑极限的算法：它使用每个点一个高斯分量的混合，从而得到一个基本上是非参数的密度估计器。在本章中，我们将探讨KDE的动机和用途。
- en: 'We begin with the standard imports:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从标准导入开始：
- en: '[PRE0]'
  id: totrans-3
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Motivating Kernel Density Estimation: Histograms'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 激发核密度估计：直方图
- en: 'As mentioned previously, a density estimator is an algorithm that seeks to
    model the probability distribution that generated a dataset. For one-dimensional
    data, you are probably already familiar with one simple density estimator: the
    histogram. A histogram divides the data into discrete bins, counts the number
    of points that fall in each bin, and then visualizes the results in an intuitive
    manner.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，密度估计器是一种算法，旨在模拟生成数据集的概率分布。对于一维数据，你可能已经熟悉一个简单的密度估计器：直方图。直方图将数据分成离散的箱子，计算落入每个箱子的点的数量，然后直观地可视化结果。
- en: 'For example, let’s create some data that is drawn from two normal distributions:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，让我们创建一些从两个正态分布中绘制的数据：
- en: '[PRE1]'
  id: totrans-7
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: We have previously seen that the standard count-based histogram can be created
    with the `plt.hist` function. By specifying the `density` parameter of the histogram,
    we end up with a normalized histogram where the height of the bins does not reflect
    counts, but instead reflects probability density (see [Figure 49-1](#fig_0513-kernel-density-estimation_files_in_output_6_0)).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前看到，可以通过指定直方图的`density`参数来创建标准的基于计数的直方图。通过这种方式得到的是归一化的直方图，其中箱子的高度不反映计数，而是反映概率密度（参见[图 49-1](#fig_0513-kernel-density-estimation_files_in_output_6_0)）。
- en: '[PRE2]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![output 6 0](assets/output_6_0.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![output 6 0](assets/output_6_0.png)'
- en: Figure 49-1\. Data drawn from a combination of normal distributions
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 49-1\. 从正态分布组合中绘制的数据
- en: 'Notice that for equal binning, this normalization simply changes the scale
    on the y-axis, leaving the relative heights essentially the same as in a histogram
    built from counts. This normalization is chosen so that the total area under the
    histogram is equal to 1, as we can confirm by looking at the output of the histogram
    function:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 对于等距分箱，这种归一化仅仅改变了y轴的比例，使得相对高度基本上与计数直方图中的相同。选择这种归一化是为了使直方图下面积等于1，我们可以通过直方图函数的输出来确认：
- en: '[PRE3]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: One of the issues with using a histogram as a density estimator is that the
    choice of bin size and location can lead to representations that have qualitatively
    different features. For example, if we look at a version of this data with only
    20 points, the choice of how to draw the bins can lead to an entirely different
    interpretation of the data! Consider this example, visualized in [Figure 49-2](#fig_0513-kernel-density-estimation_files_in_output_11_0).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 使用直方图作为密度估计器的一个问题是，箱子的大小和位置的选择可能导致具有不同特征的表现。例如，如果我们查看只有20个点的此数据的一个版本，如何绘制箱子的选择可以导致对数据的完全不同解释！考虑到这个例子，在[图 49-2](#fig_0513-kernel-density-estimation_files_in_output_11_0)中可视化。
- en: '[PRE4]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![output 11 0](assets/output_11_0.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![output 11 0](assets/output_11_0.png)'
- en: 'Figure 49-2\. The problem with histograms: the bin locations can affect interpretation'
  id: totrans-18
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 49-2\. 直方图的问题：箱子的位置会影响解释
- en: On the left, the histogram makes clear that this is a bimodal distribution.
    On the right, we see a unimodal distribution with a long tail. Without seeing
    the preceding code, you would probably not guess that these two histograms were
    built from the same data. With that in mind, how can you trust the intuition that
    histograms confer? And how might we improve on this?
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 左侧，直方图清楚地显示这是一个双峰分布。右侧，我们看到一个长尾单峰分布。如果不看前面的代码，你可能不会猜到这两个直方图是由同一组数据构建的。有了这个认识，我们如何相信直方图所传达的直觉呢？我们如何改进这一点呢？
- en: Stepping back, we can think of a histogram as a stack of blocks, where we stack
    one block within each bin on top of each point in the dataset. Let’s view this
    directly (see [Figure 49-3](#fig_0513-kernel-density-estimation_files_in_output_13_1)).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，我们可以把直方图看作是一堆块，我们在数据集中的每个点上都堆叠一个块。让我们直接查看这一点（见[图 49-3](#fig_0513-kernel-density-estimation_files_in_output_13_1)）。
- en: '[PRE6]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![output 13 1](assets/output_13_1.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![output 13 1](assets/output_13_1.png)'
- en: Figure 49-3\. Histogram as stack of blocks
  id: totrans-23
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 49-3\. 堆叠的块直方图
- en: The problem with our two binnings stems from the fact that the height of the
    block stack often reflects not the actual density of points nearby, but coincidences
    of how the bins align with the data points. This misalignment between points and
    their blocks is a potential cause of the poor histogram results seen here. But
    what if, instead of stacking the blocks aligned with the *bins*, we were to stack
    the blocks aligned with the *points they represent*? If we do this, the blocks
    won’t be aligned, but we can add their contributions at each location along the
    x-axis to find the result. Let’s try this (see [Figure 49-4](#fig_0513-kernel-density-estimation_files_in_output_15_0)).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们两次分箱的问题源于这样一个事实：块堆叠的高度经常反映不出附近点的实际密度，而是由于分箱与数据点对齐的巧合。这种点与它们块之间的不对齐可能是导致这里糟糕直方图结果的一个潜在原因。但是，如果我们不是将块与*分箱*对齐，而是将块与*它们所代表的点*对齐会怎样呢？如果我们这样做，块就不会对齐，但我们可以在每个
    x 轴位置上添加它们的贡献来找到结果。让我们试试这个方法（见[图 49-4](#fig_0513-kernel-density-estimation_files_in_output_15_0)）。
- en: '[PRE7]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![output 15 0](assets/output_15_0.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![output 15 0](assets/output_15_0.png)'
- en: Figure 49-4\. A “histogram” where blocks center on each individual point; this
    is an example of a kernel density estimate
  id: totrans-27
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 49-4\. 一个“直方图”，其中每个块都以各个个体点为中心；这是一个核密度估计的示例
- en: The result looks a bit messy, but it’s a much more robust reflection of the
    actual data characteristics than is the standard histogram. Still, the rough edges
    are not aesthetically pleasing, nor are they reflective of any true properties
    of the data. In order to smooth them out, we might decide to replace the blocks
    at each location with a smooth function, like a Gaussian. Let’s use a standard
    normal curve at each point instead of a block (see [Figure 49-5](#fig_0513-kernel-density-estimation_files_in_output_17_0)).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 结果看起来有些杂乱，但它比标准直方图更能鲜明地反映实际数据特征。尽管如此，其粗糙的边缘既不美观，也不能反映数据的任何真实特性。为了平滑它们，我们可以决定在每个位置用一个平滑函数来取代这些块，比如一个高斯函数。让我们在每个点上使用一个标准正态曲线代替一个块（见[图 49-5](#fig_0513-kernel-density-estimation_files_in_output_17_0)）。
- en: '[PRE8]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![output 17 0](assets/output_17_0.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![output 17 0](assets/output_17_0.png)'
- en: Figure 49-5\. A kernel density estimate with a Gaussian kernel
  id: totrans-31
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 49-5\. 一个使用高斯核的核密度估计
- en: This smoothed-out plot, with a Gaussian distribution contributed at the location
    of each input point, gives a much more accurate idea of the shape of the data
    distribution, and one that has much less variance (i.e., changes much less in
    response to differences in sampling).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这个平滑处理后的图，每个输入点处贡献一个高斯分布，更准确地反映了数据分布的形状，并且具有更低的方差（即对不同采样差异的响应更小）。
- en: 'What we’ve landed on in the last two plots is what’s called kernel density
    estimation in one dimension: we have placed a “kernel”—a square or top hat–shaped
    kernel in the former, a Gaussian kernel in the latter—at the location of each
    point, and used their sum as an estimate of density. With this intuition in mind,
    we’ll now explore kernel density estimation in more detail.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的两个图中，我们所得到的就是一维核密度估计：我们在每个点的位置放置一个“核”—在前者中是一个方形或者顶帽形的核，在后者中是一个高斯核，并用它们的总和作为密度的估计。有了这个直觉，我们现在将更详细地探讨核密度估计。
- en: Kernel Density Estimation in Practice
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实践中的核密度估计
- en: 'The free parameters of kernel density estimation are the *kernel*, which specifies
    the shape of the distribution placed at each point, and the *kernel bandwidth*,
    which controls the size of the kernel at each point. In practice, there are many
    kernels you might use for kernel density estimation: in particular, the Scikit-Learn
    KDE implementation supports six kernels, which you can read about in the [“Density
    Estimation” section](https://oreil.ly/2Ae4a) of the documentation.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 核密度估计的自由参数包括 *核函数*，它指定放置在每个点处的分布的形状，以及 *核带宽*，它控制每个点处核的大小。实际上，可以使用许多核函数进行核密度估计：特别是，Scikit-Learn
    的 KDE 实现支持六种核函数，你可以在[“密度估计”部分](https://oreil.ly/2Ae4a)的文档中了解更多信息。
- en: While there are several versions of KDE implemented in Python (notably in the
    SciPy and `statsmodels` packages), I prefer to use Scikit-Learn’s version because
    of its efficiency and flexibility. It is implemented in the `sklearn.neighbors.KernelDensity`
    estimator, which handles KDE in multiple dimensions with one of six kernels and
    one of a couple dozen distance metrics. Because KDE can be fairly computationally
    intensive, the Scikit-Learn estimator uses a tree-based algorithm under the hood
    and can trade off computation time for accuracy using the `atol` (absolute tolerance)
    and `rtol` (relative tolerance) parameters. The kernel bandwidth can be determined
    using Scikit-Learn’s standard cross-validation tools, as we will soon see.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 Python 中有几个实现 KDE 的版本（特别是在 SciPy 和 `statsmodels` 包中），但我更倾向于使用 Scikit-Learn
    的版本，因为它高效且灵活。它是在 `sklearn.neighbors.KernelDensity` 估计器中实现的，可以使用六种核函数和几十种距离度量来处理多维
    KDE。由于 KDE 可能计算量较大，Scikit-Learn 的估计器在底层使用基于树的算法，并可以通过 `atol`（绝对容差）和 `rtol`（相对容差）参数在计算时间和准确性之间进行权衡。核带宽可以使用
    Scikit-Learn 的标准交叉验证工具来确定，这很快我们会看到。
- en: Let’s first show a simple example of replicating the previous plot using the
    Scikit-Learn `KernelDensity` estimator (see [Figure 49-6](#fig_0513-kernel-density-estimation_files_in_output_20_0)).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先展示一个简单的示例，使用 Scikit-Learn 的 `KernelDensity` 估计器复制先前的图（参见 [Figure 49-6](#fig_0513-kernel-density-estimation_files_in_output_20_0)）。
- en: '[PRE9]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![output 20 0](assets/output_20_0.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![output 20 0](assets/output_20_0.png)'
- en: Figure 49-6\. A kernel density estimate computed with Scikit-Learn
  id: totrans-40
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 49-6\. 使用 Scikit-Learn 计算的核密度估计
- en: The result here is normalized such that the area under the curve is equal to
    1.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 此处的结果已归一化，使得曲线下面积等于1。
- en: Selecting the Bandwidth via Cross-Validation
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过交叉验证选择带宽
- en: The final estimate produced by a KDE procedure can be quite sensitive to the
    choice of bandwidth, which is the knob that controls the bias–variance trade-off
    in the estimate of density. Too narrow a bandwidth leads to a high-variance estimate
    (i.e., overfitting), where the presence or absence of a single point makes a large
    difference. Too wide a bandwidth leads to a high-bias estimate (i.e., underfitting),
    where the structure in the data is washed out by the wide kernel.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: KDE 过程产生的最终估计对带宽的选择非常敏感，带宽是控制密度估计中偏差-方差权衡的旋钮。带宽太窄会导致高方差估计（即过拟合），其中单个点的存在或缺失会产生较大差异。带宽太宽会导致高偏差估计（即欠拟合），数据结构被广核模糊化。
- en: 'There is a long history in statistics of methods to quickly estimate the best
    bandwidth based on rather stringent assumptions about the data: if you look up
    the KDE implementations in the SciPy and `statsmodels` packages, for example,
    you will see implementations based on some of these rules.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在统计学中，有很长的历史可以快速估计基于数据的最佳带宽，基于对数据的严格假设：例如，如果你查看 SciPy 和 `statsmodels` 包中的 KDE
    实现，你会看到基于这些规则的实现。
- en: 'In machine learning contexts, we’ve seen that such hyperparameter tuning often
    is done empirically via a cross-validation approach. With this in mind, Scikit-Learn’s
    `KernelDensity` estimator is designed such that it can be used directly within
    the package’s standard grid search tools. Here we will use `GridSearchCV` to optimize
    the bandwidth for the preceding dataset. Because we are looking at such a small
    dataset, we will use leave-one-out cross-validation, which minimizes the reduction
    in training set size for each cross-validation trial:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习环境中，我们看到这种超参数调整通常通过经验交叉验证方法完成。考虑到这一点，Scikit-Learn 的 `KernelDensity` 估计器设计成可以直接在包的标准网格搜索工具中使用。在这里，我们将使用
    `GridSearchCV` 来优化前述数据集的带宽。由于我们正在查看一个如此小的数据集，我们将使用留一法交叉验证，以最小化每个交叉验证试验的训练集大小减少：
- en: '[PRE10]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now we can find the choice of bandwidth that maximizes the score (which in
    this case defaults to the log-likelihood):'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以找到使得分数最大化的带宽选择（在这种情况下，默认为对数似然）：
- en: '[PRE11]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The optimal bandwidth happens to be very close to what we used in the example
    plot earlier, where the bandwidth was 1.0 (i.e., the default width of `scipy.stats.norm`).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 最优带宽与我们在之前示例图中使用的非常接近，那里的带宽是1.0（即`scipy.stats.norm`的默认宽度）。
- en: 'Example: Not-so-Naive Bayes'
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 示例：不那么朴素的贝叶斯
- en: This example looks at Bayesian generative classification with KDE, and demonstrates
    how to use the Scikit-Learn architecture to create a custom estimator.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 此示例探讨了带KDE的贝叶斯生成分类，并演示了如何使用Scikit-Learn架构创建自定义估计器。
- en: In [Chapter 41](ch41.xhtml#section-0505-naive-bayes) we explored naive Bayesian
    classification, in which we create a simple generative model for each class, and
    use these models to build a fast classifier. For Gaussian naive Bayes, the generative
    model is a simple axis-aligned Gaussian. With a density estimation algorithm like
    KDE, we can remove the “naive” element and perform the same classification with
    a more sophisticated generative model for each class. It’s still Bayesian classification,
    but it’s no longer naive.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第41章](ch41.xhtml#section-0505-naive-bayes)中，我们探讨了朴素贝叶斯分类，其中我们为每个类别创建了一个简单的生成模型，并使用这些模型构建了一个快速分类器。对于高斯朴素贝叶斯，生成模型是一个简单的轴对齐高斯分布。使用KDE等密度估计算法，我们可以去除“朴素”元素，并使用更复杂的生成模型为每个类别执行相同的分类。它仍然是贝叶斯分类，但不再是朴素的。
- en: 'The general approach for generative classification is this:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 生成分类的一般方法如下：
- en: Split the training data by label.
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据标签将训练数据进行拆分。
- en: For each set, fit a KDE to obtain a generative model of the data. This allows
    you, for any observation <math alttext="x"><mi>x</mi></math> and label <math alttext="y"><mi>y</mi></math>
    , to compute a likelihood <math alttext="upper P left-parenthesis x vertical-bar
    y right-parenthesis"><mrow><mi>P</mi> <mo>(</mo> <mi>x</mi> <mo>|</mo> <mi>y</mi>
    <mo>)</mo></mrow></math> .
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对每个集合，拟合一个KDE以获得数据的生成模型。这允许你对于任意观测值<math alttext="x"><mi>x</mi></math>和标签<math
    alttext="y"><mi>y</mi></math>，计算出一个似然概率<math alttext="upper P left-parenthesis
    x vertical-bar y right-parenthesis"><mrow><mi>P</mi> <mo>(</mo> <mi>x</mi> <mo>|</mo>
    <mi>y</mi> <mo>)</mo></mrow></math>。
- en: From the number of examples of each class in the training set, compute the *class
    prior*, <math alttext="upper P left-parenthesis y right-parenthesis"><mrow><mi>P</mi>
    <mo>(</mo> <mi>y</mi> <mo>)</mo></mrow></math> .
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据训练集中每个类别的示例数量，计算*类先验*<math alttext="upper P left-parenthesis y right-parenthesis"><mrow><mi>P</mi>
    <mo>(</mo> <mi>y</mi> <mo>)</mo></mrow></math>。
- en: For an unknown point <math alttext="x"><mi>x</mi></math> , the posterior probability
    for each class is <math alttext="upper P left-parenthesis y vertical-bar x right-parenthesis
    proportional-to upper P left-parenthesis x vertical-bar y right-parenthesis upper
    P left-parenthesis y right-parenthesis"><mrow><mi>P</mi> <mo>(</mo> <mi>y</mi>
    <mo>|</mo> <mi>x</mi> <mo>)</mo> <mo>∝</mo> <mi>P</mi> <mo>(</mo> <mi>x</mi> <mo>|</mo>
    <mi>y</mi> <mo>)</mo> <mi>P</mi> <mo>(</mo> <mi>y</mi> <mo>)</mo></mrow></math>
    . The class that maximizes this posterior is the label assigned to the point.
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于未知点<math alttext="x"><mi>x</mi></math>，每个类别的后验概率为<math alttext="upper P left-parenthesis
    y vertical-bar x right-parenthesis proportional-to upper P left-parenthesis x
    vertical-bar y right-parenthesis upper P left-parenthesis y right-parenthesis"><mrow><mi>P</mi>
    <mo>(</mo> <mi>y</mi> <mo>|</mo> <mi>x</mi> <mo>)</mo> <mo>∝</mo> <mi>P</mi> <mo>(</mo>
    <mi>x</mi> <mo>|</mo> <mi>y</mi> <mo>)</mo> <mi>P</mi> <mo>(</mo> <mi>y</mi> <mo>)</mo></mrow></math>。最大化这个后验概率的类别是分配给该点的标签。
- en: The algorithm is straightforward and intuitive to understand; the more difficult
    piece is couching it within the Scikit-Learn framework in order to make use of
    the grid search and cross-validation architecture.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 算法很简单直观易懂；更难的部分是将其嵌入Scikit-Learn框架中，以便利用网格搜索和交叉验证架构。
- en: 'This is the code that implements the algorithm within the Scikit-Learn framework;
    we will step through it following the code block:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这是在Scikit-Learn框架中实现算法的代码；我们将在代码块后面逐步分析它：
- en: '[PRE12]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Anatomy of a Custom Estimator
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自定义估计器的解剖学。
- en: 'Let’s step through this code and discuss the essential features:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐步分析这段代码，并讨论其关键特性：
- en: '[PRE13]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Each estimator in Scikit-Learn is a class, and it is most convenient for this
    class to inherit from the `BaseEstimator` class as well as the appropriate mixin,
    which provides standard functionality. For example, here the `BaseEstimator` contains
    (among other things) the logic necessary to clone/copy an estimator for use in
    a cross-validation procedure, and `ClassifierMixin` defines a default `score`
    method used by such routines. We also provide a docstring, which will be captured
    by IPython’s help functionality (see [Chapter 1](ch01.xhtml#section-0101-help-and-documentation)).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-Learn 中的每个评估器都是一个类，最方便的是这个类也应该从 `BaseEstimator` 类以及适当的 mixin 继承，提供标准功能。例如，在这里，`BaseEstimator`
    包含了克隆/复制评估器以供交叉验证过程使用的必要逻辑，而 `ClassifierMixin` 定义了这些例程使用的默认 `score` 方法。我们还提供了一个文档字符串，这将被
    IPython 的帮助功能捕获（参见 [第一章](ch01.xhtml#section-0101-help-and-documentation)）。
- en: 'Next comes the class initialization method:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是类的初始化方法：
- en: '[PRE14]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'This is the actual code that is executed when the object is instantiated with
    `KDEClassifier`. In Scikit-Learn, it is important that *initialization contains
    no operations* other than assigning the passed values by name to `self`. This
    is due to the logic contained in `BaseEstimator` required for cloning and modifying
    estimators for cross-validation, grid search, and other functions. Similarly,
    all arguments to `__init__` should be explicit: i.e., `*args` or `**kwargs` should
    be avoided, as they will not be correctly handled within cross-validation routines.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用 `KDEClassifier` 实例化对象时，执行的实际代码是这样的。在 Scikit-Learn 中，*初始化不包含任何操作*，只是将传递的值按名称分配给
    `self`。这是由于 `BaseEstimator` 中包含的逻辑，用于克隆和修改评估器，以供交叉验证、网格搜索和其他功能使用。同样，所有传递给 `__init__`
    的参数都应该是明确的：即应避免使用 `*args` 或 `**kwargs`，因为它们在交叉验证过程中无法正确处理。
- en: 'Next comes the `fit` method, where we handle training data:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是 `fit` 方法，我们在这里处理训练数据：
- en: '[PRE15]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Here we find the unique classes in the training data, train a `KernelDensity`
    model for each class, and compute the class priors based on the number of input
    samples. Finally, `fit` should always return `self` so that we can chain commands.
    For example:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们找到训练数据中的唯一类别，为每个类别训练一个 `KernelDensity` 模型，并基于输入样本的数量计算类别先验概率。最后，`fit`
    应该始终返回 `self`，以便我们可以链接命令。例如：
- en: '[PRE16]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Notice that each persistent result of the fit is stored with a trailing underscore
    (e.g., `self.logpriors_`). This is a convention used in Scikit-Learn so that you
    can quickly scan the members of an estimator (using IPython’s tab completion)
    and see exactly which members are fit to training data.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，每次 `fit` 持久结果都带有下划线结尾（例如 `self.logpriors_`）。这是 Scikit-Learn 中的一种约定，因此您可以快速扫描评估器的成员（使用
    IPython 的 tab 自动完成），并查看哪些成员是适合训练数据的。
- en: 'Finally, we have the logic for predicting labels on new data:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们有预测新数据标签的逻辑：
- en: '[PRE17]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Because this is a probabilistic classifier, we first implement `predict_proba`,
    which returns an array of class probabilities of shape `[n_samples, n_classes]`.
    Entry `[i, j]` of this array is the posterior probability that sample `i` is a
    member of class `j`, computed by multiplying the likelihood by the class prior
    and normalizing.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 因为这是一个概率分类器，我们首先实现 `predict_proba`，它返回一个形状为 `[n_samples, n_classes]` 的类别概率数组。数组中的条目
    `[i, j]` 是计算得到的样本 `i` 是类别 `j` 的后验概率，通过将似然性乘以类先验并进行归一化计算得到。
- en: The `predict` method uses these probabilities and simply returns the class with
    the largest probability.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '`predict` 方法使用这些概率，简单地返回具有最大概率的类别。'
- en: Using Our Custom Estimator
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用我们的自定义评估器
- en: 'Let’s try this custom estimator on a problem we have seen before: the classification
    of handwritten digits. Here we will load the digits and compute the cross-validation
    score for a range of candidate bandwidths using the `GridSearchCV` meta-estimator
    (refer back to [Chapter 39](ch39.xhtml#section-0503-hyperparameters-and-model-validation)):'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试将这个自定义评估器应用于我们之前见过的问题：手写数字的分类。在这里，我们将加载数字并使用 `GridSearchCV` 元评估器计算一系列候选带宽的交叉验证分数（参考
    [第 39 章](ch39.xhtml#section-0503-hyperparameters-and-model-validation)）：
- en: '[PRE18]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Next we can plot the cross-validation score as a function of bandwidth (see
    [Figure 49-7](#fig_0513-kernel-density-estimation_files_in_output_37_1)).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以绘制交叉验证分数作为带宽的函数（参见 [图 49-7](#fig_0513-kernel-density-estimation_files_in_output_37_1)）。
- en: '[PRE19]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '![output 37 1](assets/output_37_1.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![output 37 1](assets/output_37_1.png)'
- en: Figure 49-7\. Validation curve for the KDE-based Bayesian classifier
  id: totrans-83
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 49-7\. 基于 KDE 的贝叶斯分类器的验证曲线
- en: 'This indicates that our KDE classifier reaches a cross-validation accuracy
    of over 96%, compared to around 80% for the naive Bayes classifier:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明我们的KDE分类器达到了超过96%的交叉验证准确率，而朴素贝叶斯分类器的准确率约为80%。
- en: '[PRE20]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'One benefit of such a generative classifier is interpretability of results:
    for each unknown sample, we not only get a probabilistic classification, but a
    *full model* of the distribution of points we are comparing it to! If desired,
    this offers an intuitive window into the reasons for a particular classification
    that algorithms like SVMs and random forests tend to obscure.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这样一个生成分类器的一个好处是结果的可解释性：对于每个未知样本，我们不仅获得一个概率分类，而且获得了与其比较的点分布的*完整模型*！如果需要，这为特定分类的原因提供了一个直观的窗口，而像SVM和随机森林这样的算法往往会掩盖这些原因。
- en: 'If you would like to take this further, here are some ideas for improvements
    that could be made to our KDE classifier model:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您希望进一步进行，这里有一些可以改进我们的KDE分类器模型的想法：
- en: You could allow the bandwidth in each class to vary independently.
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以允许每个类别中的带宽独立变化。
- en: You could optimize these bandwidths not based on their prediction score, but
    on the likelihood of the training data under the generative model within each
    class (i.e. use the scores from `KernelDensity` itself rather than the global
    prediction accuracy).
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不应该基于它们的预测分数来优化这些带宽，而是应该基于每个类别中生成模型下训练数据的可能性（即使用`KernelDensity`本身的分数而不是全局预测准确度）。
- en: Finally, if you want some practice building your own estimator, you might tackle
    building a similar Bayesian classifier using Gaussian mixture models instead of
    KDE.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，如果你想要一些练习来构建自己的估计器，可以尝试使用高斯混合模型而不是KDE来构建类似的贝叶斯分类器。
