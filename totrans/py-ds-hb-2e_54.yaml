- en: 'Chapter 49\. In Depth: Kernel Density Estimation'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In [Chapter 48](ch48.xhtml#section-0512-gaussian-mixtures) we covered Gaussian
    mixture models, which are a kind of hybrid between a clustering estimator and
    a density estimator. Recall that a density estimator is an algorithm that takes
    a <math alttext="upper D"><mi>D</mi></math> -dimensional dataset and produces
    an estimate of the <math alttext="upper D"><mi>D</mi></math> -dimensional probability
    distribution that data is drawn from. The GMM algorithm accomplishes this by representing
    the density as a weighted sum of Gaussian distributions. *Kernel density estimation*
    (KDE) is in some senses an algorithm that takes the mixture-of-Gaussians idea
    to its logical extreme: it uses a mixture consisting of one Gaussian component
    *per point*, resulting in an essentially nonparametric estimator of density. In
    this chapter, we will explore the motivation and uses of KDE.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We begin with the standard imports:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Motivating Kernel Density Estimation: Histograms'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As mentioned previously, a density estimator is an algorithm that seeks to
    model the probability distribution that generated a dataset. For one-dimensional
    data, you are probably already familiar with one simple density estimator: the
    histogram. A histogram divides the data into discrete bins, counts the number
    of points that fall in each bin, and then visualizes the results in an intuitive
    manner.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, let’s create some data that is drawn from two normal distributions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: We have previously seen that the standard count-based histogram can be created
    with the `plt.hist` function. By specifying the `density` parameter of the histogram,
    we end up with a normalized histogram where the height of the bins does not reflect
    counts, but instead reflects probability density (see [Figure 49-1](#fig_0513-kernel-density-estimation_files_in_output_6_0)).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![output 6 0](assets/output_6_0.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 49-1\. Data drawn from a combination of normal distributions
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Notice that for equal binning, this normalization simply changes the scale
    on the y-axis, leaving the relative heights essentially the same as in a histogram
    built from counts. This normalization is chosen so that the total area under the
    histogram is equal to 1, as we can confirm by looking at the output of the histogram
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: One of the issues with using a histogram as a density estimator is that the
    choice of bin size and location can lead to representations that have qualitatively
    different features. For example, if we look at a version of this data with only
    20 points, the choice of how to draw the bins can lead to an entirely different
    interpretation of the data! Consider this example, visualized in [Figure 49-2](#fig_0513-kernel-density-estimation_files_in_output_11_0).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![output 11 0](assets/output_11_0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 49-2\. The problem with histograms: the bin locations can affect interpretation'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: On the left, the histogram makes clear that this is a bimodal distribution.
    On the right, we see a unimodal distribution with a long tail. Without seeing
    the preceding code, you would probably not guess that these two histograms were
    built from the same data. With that in mind, how can you trust the intuition that
    histograms confer? And how might we improve on this?
  prefs: []
  type: TYPE_NORMAL
- en: Stepping back, we can think of a histogram as a stack of blocks, where we stack
    one block within each bin on top of each point in the dataset. Let’s view this
    directly (see [Figure 49-3](#fig_0513-kernel-density-estimation_files_in_output_13_1)).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '![output 13 1](assets/output_13_1.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 49-3\. Histogram as stack of blocks
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The problem with our two binnings stems from the fact that the height of the
    block stack often reflects not the actual density of points nearby, but coincidences
    of how the bins align with the data points. This misalignment between points and
    their blocks is a potential cause of the poor histogram results seen here. But
    what if, instead of stacking the blocks aligned with the *bins*, we were to stack
    the blocks aligned with the *points they represent*? If we do this, the blocks
    won’t be aligned, but we can add their contributions at each location along the
    x-axis to find the result. Let’s try this (see [Figure 49-4](#fig_0513-kernel-density-estimation_files_in_output_15_0)).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![output 15 0](assets/output_15_0.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 49-4\. A “histogram” where blocks center on each individual point; this
    is an example of a kernel density estimate
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The result looks a bit messy, but it’s a much more robust reflection of the
    actual data characteristics than is the standard histogram. Still, the rough edges
    are not aesthetically pleasing, nor are they reflective of any true properties
    of the data. In order to smooth them out, we might decide to replace the blocks
    at each location with a smooth function, like a Gaussian. Let’s use a standard
    normal curve at each point instead of a block (see [Figure 49-5](#fig_0513-kernel-density-estimation_files_in_output_17_0)).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '![output 17 0](assets/output_17_0.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 49-5\. A kernel density estimate with a Gaussian kernel
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This smoothed-out plot, with a Gaussian distribution contributed at the location
    of each input point, gives a much more accurate idea of the shape of the data
    distribution, and one that has much less variance (i.e., changes much less in
    response to differences in sampling).
  prefs: []
  type: TYPE_NORMAL
- en: 'What we’ve landed on in the last two plots is what’s called kernel density
    estimation in one dimension: we have placed a “kernel”—a square or top hat–shaped
    kernel in the former, a Gaussian kernel in the latter—at the location of each
    point, and used their sum as an estimate of density. With this intuition in mind,
    we’ll now explore kernel density estimation in more detail.'
  prefs: []
  type: TYPE_NORMAL
- en: Kernel Density Estimation in Practice
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The free parameters of kernel density estimation are the *kernel*, which specifies
    the shape of the distribution placed at each point, and the *kernel bandwidth*,
    which controls the size of the kernel at each point. In practice, there are many
    kernels you might use for kernel density estimation: in particular, the Scikit-Learn
    KDE implementation supports six kernels, which you can read about in the [“Density
    Estimation” section](https://oreil.ly/2Ae4a) of the documentation.'
  prefs: []
  type: TYPE_NORMAL
- en: While there are several versions of KDE implemented in Python (notably in the
    SciPy and `statsmodels` packages), I prefer to use Scikit-Learn’s version because
    of its efficiency and flexibility. It is implemented in the `sklearn.neighbors.KernelDensity`
    estimator, which handles KDE in multiple dimensions with one of six kernels and
    one of a couple dozen distance metrics. Because KDE can be fairly computationally
    intensive, the Scikit-Learn estimator uses a tree-based algorithm under the hood
    and can trade off computation time for accuracy using the `atol` (absolute tolerance)
    and `rtol` (relative tolerance) parameters. The kernel bandwidth can be determined
    using Scikit-Learn’s standard cross-validation tools, as we will soon see.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s first show a simple example of replicating the previous plot using the
    Scikit-Learn `KernelDensity` estimator (see [Figure 49-6](#fig_0513-kernel-density-estimation_files_in_output_20_0)).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '![output 20 0](assets/output_20_0.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 49-6\. A kernel density estimate computed with Scikit-Learn
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The result here is normalized such that the area under the curve is equal to
    1.
  prefs: []
  type: TYPE_NORMAL
- en: Selecting the Bandwidth via Cross-Validation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The final estimate produced by a KDE procedure can be quite sensitive to the
    choice of bandwidth, which is the knob that controls the bias–variance trade-off
    in the estimate of density. Too narrow a bandwidth leads to a high-variance estimate
    (i.e., overfitting), where the presence or absence of a single point makes a large
    difference. Too wide a bandwidth leads to a high-bias estimate (i.e., underfitting),
    where the structure in the data is washed out by the wide kernel.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is a long history in statistics of methods to quickly estimate the best
    bandwidth based on rather stringent assumptions about the data: if you look up
    the KDE implementations in the SciPy and `statsmodels` packages, for example,
    you will see implementations based on some of these rules.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In machine learning contexts, we’ve seen that such hyperparameter tuning often
    is done empirically via a cross-validation approach. With this in mind, Scikit-Learn’s
    `KernelDensity` estimator is designed such that it can be used directly within
    the package’s standard grid search tools. Here we will use `GridSearchCV` to optimize
    the bandwidth for the preceding dataset. Because we are looking at such a small
    dataset, we will use leave-one-out cross-validation, which minimizes the reduction
    in training set size for each cross-validation trial:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can find the choice of bandwidth that maximizes the score (which in
    this case defaults to the log-likelihood):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The optimal bandwidth happens to be very close to what we used in the example
    plot earlier, where the bandwidth was 1.0 (i.e., the default width of `scipy.stats.norm`).
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: Not-so-Naive Bayes'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This example looks at Bayesian generative classification with KDE, and demonstrates
    how to use the Scikit-Learn architecture to create a custom estimator.
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 41](ch41.xhtml#section-0505-naive-bayes) we explored naive Bayesian
    classification, in which we create a simple generative model for each class, and
    use these models to build a fast classifier. For Gaussian naive Bayes, the generative
    model is a simple axis-aligned Gaussian. With a density estimation algorithm like
    KDE, we can remove the “naive” element and perform the same classification with
    a more sophisticated generative model for each class. It’s still Bayesian classification,
    but it’s no longer naive.
  prefs: []
  type: TYPE_NORMAL
- en: 'The general approach for generative classification is this:'
  prefs: []
  type: TYPE_NORMAL
- en: Split the training data by label.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each set, fit a KDE to obtain a generative model of the data. This allows
    you, for any observation <math alttext="x"><mi>x</mi></math> and label <math alttext="y"><mi>y</mi></math>
    , to compute a likelihood <math alttext="upper P left-parenthesis x vertical-bar
    y right-parenthesis"><mrow><mi>P</mi> <mo>(</mo> <mi>x</mi> <mo>|</mo> <mi>y</mi>
    <mo>)</mo></mrow></math> .
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: From the number of examples of each class in the training set, compute the *class
    prior*, <math alttext="upper P left-parenthesis y right-parenthesis"><mrow><mi>P</mi>
    <mo>(</mo> <mi>y</mi> <mo>)</mo></mrow></math> .
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For an unknown point <math alttext="x"><mi>x</mi></math> , the posterior probability
    for each class is <math alttext="upper P left-parenthesis y vertical-bar x right-parenthesis
    proportional-to upper P left-parenthesis x vertical-bar y right-parenthesis upper
    P left-parenthesis y right-parenthesis"><mrow><mi>P</mi> <mo>(</mo> <mi>y</mi>
    <mo>|</mo> <mi>x</mi> <mo>)</mo> <mo>∝</mo> <mi>P</mi> <mo>(</mo> <mi>x</mi> <mo>|</mo>
    <mi>y</mi> <mo>)</mo> <mi>P</mi> <mo>(</mo> <mi>y</mi> <mo>)</mo></mrow></math>
    . The class that maximizes this posterior is the label assigned to the point.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The algorithm is straightforward and intuitive to understand; the more difficult
    piece is couching it within the Scikit-Learn framework in order to make use of
    the grid search and cross-validation architecture.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the code that implements the algorithm within the Scikit-Learn framework;
    we will step through it following the code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Anatomy of a Custom Estimator
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s step through this code and discuss the essential features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Each estimator in Scikit-Learn is a class, and it is most convenient for this
    class to inherit from the `BaseEstimator` class as well as the appropriate mixin,
    which provides standard functionality. For example, here the `BaseEstimator` contains
    (among other things) the logic necessary to clone/copy an estimator for use in
    a cross-validation procedure, and `ClassifierMixin` defines a default `score`
    method used by such routines. We also provide a docstring, which will be captured
    by IPython’s help functionality (see [Chapter 1](ch01.xhtml#section-0101-help-and-documentation)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Next comes the class initialization method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the actual code that is executed when the object is instantiated with
    `KDEClassifier`. In Scikit-Learn, it is important that *initialization contains
    no operations* other than assigning the passed values by name to `self`. This
    is due to the logic contained in `BaseEstimator` required for cloning and modifying
    estimators for cross-validation, grid search, and other functions. Similarly,
    all arguments to `__init__` should be explicit: i.e., `*args` or `**kwargs` should
    be avoided, as they will not be correctly handled within cross-validation routines.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next comes the `fit` method, where we handle training data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Here we find the unique classes in the training data, train a `KernelDensity`
    model for each class, and compute the class priors based on the number of input
    samples. Finally, `fit` should always return `self` so that we can chain commands.
    For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Notice that each persistent result of the fit is stored with a trailing underscore
    (e.g., `self.logpriors_`). This is a convention used in Scikit-Learn so that you
    can quickly scan the members of an estimator (using IPython’s tab completion)
    and see exactly which members are fit to training data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we have the logic for predicting labels on new data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Because this is a probabilistic classifier, we first implement `predict_proba`,
    which returns an array of class probabilities of shape `[n_samples, n_classes]`.
    Entry `[i, j]` of this array is the posterior probability that sample `i` is a
    member of class `j`, computed by multiplying the likelihood by the class prior
    and normalizing.
  prefs: []
  type: TYPE_NORMAL
- en: The `predict` method uses these probabilities and simply returns the class with
    the largest probability.
  prefs: []
  type: TYPE_NORMAL
- en: Using Our Custom Estimator
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s try this custom estimator on a problem we have seen before: the classification
    of handwritten digits. Here we will load the digits and compute the cross-validation
    score for a range of candidate bandwidths using the `GridSearchCV` meta-estimator
    (refer back to [Chapter 39](ch39.xhtml#section-0503-hyperparameters-and-model-validation)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Next we can plot the cross-validation score as a function of bandwidth (see
    [Figure 49-7](#fig_0513-kernel-density-estimation_files_in_output_37_1)).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '![output 37 1](assets/output_37_1.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 49-7\. Validation curve for the KDE-based Bayesian classifier
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'This indicates that our KDE classifier reaches a cross-validation accuracy
    of over 96%, compared to around 80% for the naive Bayes classifier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'One benefit of such a generative classifier is interpretability of results:
    for each unknown sample, we not only get a probabilistic classification, but a
    *full model* of the distribution of points we are comparing it to! If desired,
    this offers an intuitive window into the reasons for a particular classification
    that algorithms like SVMs and random forests tend to obscure.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you would like to take this further, here are some ideas for improvements
    that could be made to our KDE classifier model:'
  prefs: []
  type: TYPE_NORMAL
- en: You could allow the bandwidth in each class to vary independently.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You could optimize these bandwidths not based on their prediction score, but
    on the likelihood of the training data under the generative model within each
    class (i.e. use the scores from `KernelDensity` itself rather than the global
    prediction accuracy).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, if you want some practice building your own estimator, you might tackle
    building a similar Bayesian classifier using Gaussian mixture models instead of
    KDE.
  prefs: []
  type: TYPE_NORMAL
