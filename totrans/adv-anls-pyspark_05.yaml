- en: Chapter 5\. Anomaly Detection with K-means Clustering
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第五章。使用K均值聚类进行异常检测
- en: 'Classification and regression are powerful, well-studied techniques in machine
    learning. [Chapter 4](ch04.xhtml#making_predictions_with_decision_trees_and_decision_forests)
    demonstrated using a classifier as a predictor of unknown values. But there was
    a catch: to predict unknown values for new data, we had to know the target values
    for many previously seen examples. Classifiers can help only if we, the data scientists,
    know what we are looking for and can provide plenty of examples where input produced
    a known output. These were collectively known as *supervised learning* techniques,
    because their learning process receives the correct output value for each example
    in the input.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类和回归是机器学习中强大且深入研究的技术。[第四章](ch04.xhtml#making_predictions_with_decision_trees_and_decision_forests)展示了使用分类器作为未知值的预测器。但有一个问题：为了预测新数据的未知值，我们必须知道许多先前看到的示例的目标值。只有我们，数据科学家，知道我们在寻找什么，并能提供许多示例，其中输入产生已知输出时，分类器才能帮助。这些被统称为*监督学习*技术，因为它们的学习过程接收输入中每个示例的正确输出值。
- en: 'However, sometimes the correct output is unknown for some or all examples.
    Consider the problem of dividing up an ecommerce site’s customers by their shopping
    habits and tastes. The input features are their purchases, clicks, demographic
    information, and more. The output should be groupings of customers: perhaps one
    group will represent fashion-conscious buyers, another will turn out to correspond
    to price-sensitive bargain hunters, and so on.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，有时某些或所有示例的正确输出是未知的。考虑将电子商务网站的客户按其购物习惯和喜好分组的问题。输入特征包括他们的购买、点击、人口统计信息等。输出应该是客户的分组：也许一个组将代表时尚意识强的购买者，另一个组则会对应价格敏感的猎奇者，依此类推。
- en: 'If you were asked to determine this target label for each new customer, you
    would quickly run into a problem in applying a supervised learning technique like
    a classifier: you don’t know a priori who should be considered fashion-conscious,
    for example. In fact, you’re not even sure if “fashion-conscious” is a meaningful
    grouping of the site’s customers to begin with!'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 如果要求您为每个新客户确定目标标签，您在应用像分类器这样的监督学习技术时会迅速遇到问题：例如，您不知道谁应该被视为时尚意识，事实上，您甚至不确定“时尚意识”是否是该网站客户的一个有意义的分组的开始！
- en: Fortunately, *unsupervised learning* techniques can help. These techniques do
    not learn to predict a target value, because none is available. They can, however,
    learn structure in data and find groupings of similar inputs, or learn what types
    of input are likely to occur and what types are not. This chapter will introduce
    unsupervised learning using clustering implementations in MLlib. Specifically,
    we will use the K-means clustering algorithm for identifying anomalies in network
    traffic data. Anomaly detection is often used to find fraud, detect network attacks,
    or discover problems in servers or other sensor-equipped machinery. In these cases,
    it’s important to be able to find new types of anomalies that have never been
    seen before—new forms of fraud, intrusions, and failure modes for servers. Unsupervised
    learning techniques are useful in these cases because they can learn what input
    data normally looks like and therefore detect when new data is unlike past data.
    Such new data is not necessarily attacks or fraud; it is simply unusual and therefore
    worth further investigation.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，*无监督学习*技术可以帮助。这些技术不会学习预测目标值，因为目标值是不可用的。但它们可以学习数据中的结构，并找到类似输入的分组，或学习哪些类型的输入可能发生，哪些不可能发生。本章将介绍使用MLlib中的聚类实现进行无监督学习。具体来说，我们将使用K均值聚类算法来识别网络流量数据中的异常。异常检测通常用于发现欺诈、检测网络攻击或发现服务器或其他传感器设备中的问题。在这些情况下，能够发现以前从未见过的新类型异常是非常重要的——新形式的欺诈、入侵和服务器故障模式。无监督学习技术在这些情况下非常有用，因为它们可以学习输入数据通常的外观，因此可以检测到新数据与过去数据不同。这样的新数据不一定是攻击或欺诈；它只是不寻常，因此值得进一步调查。
- en: We will start with the basics of the K-means clustering algorithm. This will
    be followed by an introduction to the KDD Cup 1999 dataset. We’ll then create
    our first K-means model using PySpark. Then we’ll go over methods for determining
    a good value of *k*—number of clusters—when implementing the K-means algorithm.
    Next, we’ll improve our model by normalizing the input features and using previously
    discarded categorical features by implementing the one-hot encoding method. We
    will wrap up by going over the entropy metric and exploring some results from
    our model.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从K-means聚类算法的基础开始。接下来是对KDD Cup 1999数据集的介绍。然后我们将使用PySpark创建我们的第一个K-means模型。然后我们将讨论在实施K-means算法时确定良好*k*值（聚类数）的方法。接下来，我们通过实施独热编码方法来使用以前丢弃的分类特征来改进我们的模型。我们将通过熵度量来总结，并探索一些来自我们模型的结果。
- en: K-means Clustering
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: K-means聚类
- en: The inherent problem of anomaly detection is, as its name implies, that of finding
    unusual things. If we already knew what “anomalous” meant for a dataset, we could
    easily detect anomalies in the data with supervised learning. An algorithm would
    receive inputs labeled “normal” and “anomaly” and learn to distinguish the two.
    However, the nature of anomalies is that they are unknown unknowns. Put another
    way, an anomaly that has been observed and understood is no longer an anomaly.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 异常检测的固有问题正如其名称所示，即寻找异常的问题。如果我们已经知道数据集中什么是“异常”，我们可以很容易地通过监督学习检测数据中的异常。算法将接收标记为“正常”和“异常”的输入，并学会区分这两者。然而，异常的本质是未知的未知。换句话说，已经观察和理解的异常不再是异常。
- en: Clustering is the best-known type of unsupervised learning. Clustering algorithms
    try to find natural groupings in data. Data points that are like one another but
    unlike others are likely to represent a meaningful grouping, so clustering algorithms
    try to put such data into the same cluster.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类是最知名的无监督学习类型。聚类算法试图在数据中找到自然的分组。彼此相似但与其他数据不同的数据点可能代表一个有意义的分组，因此聚类算法试图将这样的数据放入同一个聚类中。
- en: K-means clustering may be the most widely used clustering algorithm. It attempts
    to detect *k* clusters in a dataset, where *k* is given by the data scientist.
    *k* is a hyperparameter of the model, and the right value will depend on the dataset.
    In fact, choosing a good value for *k* will be a central plot point in this chapter.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: K-means聚类可能是最广泛使用的聚类算法。它试图在数据集中检测*k*个聚类，其中*k*由数据科学家给定。*k*是模型的超参数，正确的值取决于数据集。事实上，在本章中选择一个好的*k*值将是一个核心情节。
- en: What does “like” mean when the dataset contains information such as customer
    activity? Or transactions? K-means requires a notion of distance between data
    points. It is common to use simple Euclidean distance to measure distance between
    data points with K-means, and as it happens, this is one of two distance functions
    supported by Spark MLlib as of this writing, the other one being Cosine. The Euclidean
    distance is defined for data points whose features are all numeric. “Like” points
    are those whose intervening distance is small.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据集包含客户活动或交易等信息时，“相似”意味着什么？K-means需要数据点之间的距离概念。通常使用简单的欧氏距离来测量K-means中数据点之间的距离是常见的，正如在本文写作时Spark
    MLlib支持的两种距离函数之一，另一种是余弦距离。欧氏距离适用于其特征全部为数值的数据点。“相似”的点是其间距离较小的点。
- en: 'To K-means, a cluster is simply a point: the center of all the points that
    make up the cluster. These are, in fact, just feature vectors containing all numeric
    features and can be called vectors. However, it may be more intuitive to think
    of them as points here, because they are treated as points in a Euclidean space.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 对于K-means来说，聚类仅仅是一个点：所有构成该聚类的点的中心。事实上，它们只是包含所有数值特征的特征向量，可以称为向量。然而，在这里将它们视为点可能更直观，因为它们在欧氏空间中被视为点。
- en: This center is called the cluster *centroid* and is the arithmetic mean of the
    points—hence the name K-*means*. To start, the algorithm picks some data points
    as the initial cluster centroids. Then each data point is assigned to the nearest
    centroid. Then for each cluster, a new cluster centroid is computed as the mean
    of the data points just assigned to that cluster. This process is repeated.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这个中心被称为聚类*中心*，是点的算术平均值，因此得名K-*means*。算法首先选择一些数据点作为初始聚类中心。然后将每个数据点分配给最近的中心。然后为每个聚类计算新的聚类中心，作为刚刚分配给该聚类的数据点的平均值。这个过程重复进行。
- en: We will now look at a use case that depicts how K-means clustering can help
    us identify potentially anomalous activity in a network.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看一个使用案例，描述了K均值聚类如何帮助我们识别网络中潜在的异常活动。
- en: Identifying Anomalous Network Traffic
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 识别异常网络流量
- en: Cyberattacks are increasingly visible in the news. Some attacks attempt to flood
    a computer with network traffic to crowd out legitimate traffic. But in other
    cases, attacks attempt to exploit flaws in networking software to gain unauthorized
    access to a computer. While it’s quite obvious when a computer is being bombarded
    with traffic, detecting an exploit can be like searching for a needle in an incredibly
    large haystack of network requests.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 网络攻击越来越多地出现在新闻中。一些攻击试图通过大量网络流量淹没计算机以排除合法流量。但在其他情况下，攻击试图利用网络软件中的缺陷未经授权访问计算机。当计算机遭受大量流量时显而易见，但检测到攻击行为则如同在网络请求的非常大的大海中搜索针一样困难。
- en: Some exploit behaviors follow known patterns. For example, accessing every port
    on a machine in rapid succession is not something any normal software program
    should ever need to do. However, it is a typical first step for an attacker looking
    for services running on the computer that may be exploitable.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 有些攻击行为遵循已知的模式。例如，迅速连续访问一台机器上的所有端口，这绝不是任何正常软件程序应该做的事情。然而，这是攻击者寻找可能易受攻击的计算机服务的典型第一步。
- en: If you were to count the number of distinct ports accessed by a remote host
    in a short time, you would have a feature that probably predicts a port-scanning
    attack quite well. A handful is probably normal; hundreds indicate an attack.
    The same goes for detecting other types of attacks from other features of network
    connections—number of bytes sent and received, TCP errors, and so forth.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你计算一个远程主机在短时间内访问的不同端口数量，你可能会得到一个相当好的预测端口扫描攻击的特征。少数端口可能是正常的；数百个则可能是攻击。通过网络连接的其他特征来检测其他类型的攻击也是如此——发送和接收的字节数，TCP错误等等。
- en: But what about those unknown unknowns? The biggest threat may be the one that
    has never yet been detected and classified. Part of detecting potential network
    intrusions is detecting anomalies. These are connections that aren’t known to
    be attacks but do not resemble connections that have been observed in the past.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 但未知的未知呢？最大的威胁可能是那些从未被检测和分类过的。检测潜在网络入侵的一部分是检测异常。这些是不被认为是攻击的连接，但与过去观察到的连接不相似。
- en: Here, unsupervised learning techniques like K-means can be used to detect anomalous
    network connections. K-means can cluster connections based on statistics about
    each of them. The resulting clusters themselves aren’t interesting per se, but
    they collectively define types of connections that are like past connections.
    Anything not close to a cluster could be anomalous. Clusters are interesting insofar
    as they define regions of normal connections; everything else is unusual and potentially
    anomalous.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，像K均值这样的无监督学习技术可以用来检测异常的网络连接。K均值可以根据每个连接的统计数据进行聚类。结果的聚类本身并不是特别有趣，但它们共同定义了类似过去连接的类型。不接近任何聚类的内容可能是异常的。聚类之所以有趣，是因为它们定义了正常连接的区域；其他一切都是异常的，有潜在风险。
- en: KDD Cup 1999 Dataset
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: KDD Cup 1999 数据集
- en: The [KDD Cup](https://oreil.ly/UtYd9) was an annual data mining competition
    organized by a special interest group of the Association for Computing Machinery
    (ACM). Each year, a machine learning problem was posed, along with a dataset,
    and researchers were invited to submit a paper detailing their best solution to
    the problem. In 1999, the topic was network intrusion, and the dataset is [still
    available](https://oreil.ly/ezBDa) at the KDD website. We will need to download
    the *kddcupdata.data.gz* and *kddcup.info* files from the website. The remainder
    of this chapter will walk through building a system to detect anomalous network
    traffic using Spark, by learning from this data.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '[KDD Cup](https://oreil.ly/UtYd9)是由计算机协会（ACM）的一个特别兴趣小组每年组织的数据挖掘竞赛。每年，都会提出一个机器学习问题，并附带一个数据集，邀请研究人员提交详细介绍他们解决问题的最佳方案的论文。1999年的主题是网络入侵，数据集仍然可以在KDD网站上找到[（链接）](https://oreil.ly/ezBDa)。我们需要从网站上下载*kddcupdata.data.gz*和*kddcup.info*文件。本章的其余部分将通过学习这些数据来构建一个使用Spark来检测异常网络流量的系统。'
- en: Don’t use this dataset to build a real network intrusion system! The data did
    not necessarily reflect real network traffic at the time—even if it did, it reflects
    traffic patterns from more than 20 years ago.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 不要使用这个数据集来构建真正的网络入侵系统！数据并不一定反映了当时的实际网络流量 —— 即使反映了，它也反映了20多年前的流量模式。
- en: Fortunately, the organizers had already processed raw network packet data into
    summary information about individual network connections. The dataset is about
    708 MB in size and contains about 4.9 million connections. This is large, if not
    massive, and is certainly sufficient for our purposes here. For each connection,
    the dataset contains information such as the number of bytes sent, login attempts,
    TCP errors, and so on. Each connection is one line of CSV-formatted data, containing
    38 features. Feature information and ordering can be found in the *kddcup.info*
    file.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，组织者已经将原始网络数据处理为关于单个网络连接的摘要信息。数据集大小约为708 MB，包含约490万个连接。这是一个大数据集，如果不是巨大的话，在这里绝对足够了。对于每个连接，数据集包含诸如发送的字节数、登录尝试、TCP错误等信息。每个连接都是一个CSV格式的数据行，包含38个特征。特征信息和顺序可以在
    *kddcup.info* 文件中找到。
- en: 'Unzip the *kddcup.data.gz* data file and copy it into your storage. This example,
    like others, will assume the file is available at *data/kddcup.data*. Let’s see
    the data in its raw form:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 解压 *kddcup.data.gz* 数据文件，并将其复制到您的存储中。像其他示例一样，假设文件可在 *data/kddcup.data* 处获得。让我们看看数据的原始形式：
- en: '[PRE0]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This connection, for example, was a TCP connection to an HTTP service—215 bytes
    were sent, and 45,706 bytes were received. The user was logged in, and so on.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，这个连接是一个TCP连接到一个HTTP服务 —— 发送了215字节，接收了45,706字节。用户已登录，等等。
- en: Many features are counts, like `num_file_creations` in the 17th column, as listed
    in the *kddcup.info* file. Many features take on the value 0 or 1, indicating
    the presence or absence of a behavior, like `su_attempted` in the 15th column.
    They look like the one-hot encoded categorical features from [Chapter 4](ch04.xhtml#making_predictions_with_decision_trees_and_decision_forests),
    but are not grouped and related in the same way. Each is like a yes/no feature,
    and is therefore arguably a categorical feature. It is not always valid to translate
    categorical features as numbers and treat them as if they had an ordering. However,
    in the special case of a binary categorical feature, in most machine learning
    algorithms, mapping these to a numeric feature taking on values 0 and 1 will work
    well.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 许多特征是计数，例如在第17列中列出的 `num_file_creations`，如 *kddcup.info* 文件所示。许多特征取值为0或1，表示某种行为的存在或不存在，例如在第15列中的
    `su_attempted`。它们看起来像是来自第四章的独热编码分类特征，但并非以同样的方式分组和相关。每一个都像是一个是/否特征，因此可以被视为分类特征。将分类特征翻译为数字并将其视为具有顺序的数字特征并不总是有效的。然而，在二元分类特征的特殊情况下，在大多数机器学习算法中，将其映射为取值为0和1的数字特征将非常有效。
- en: The rest are ratios like `dst_host_srv_rerror_rate` in the next-to-last column
    and take on values from 0.0 to 1.0, inclusive.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 其余的是像 `dst_host_srv_rerror_rate` 这样的比率，位于倒数第二列，并且取值从0.0到1.0，包括0.0和1.0。
- en: Interestingly, a label is given in the last field. Most connections are labeled
    `normal.`, but some have been identified as examples of various types of network
    attacks. These would be useful in learning to distinguish a known attack from
    a normal connection, but the problem here is anomaly detection and finding potentially
    new and unknown attacks. This label will be mostly set aside for our purposes.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，标签给出在最后一个字段中。大多数连接被标记为 `normal.`，但有些已被识别为各种类型的网络攻击示例。这些将有助于学习区分已知攻击与正常连接，但这里的问题是异常检测和发现潜在的新的未知攻击。对于我们的目的，这个标签将大部分被搁置。
- en: A First Take on Clustering
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关于聚类的初步尝试
- en: Open the `pyspark-shell`, and load the CSV data as a dataframe. It’s a CSV file
    again, but without header information. It’s necessary to supply column names as
    given in the accompanying *kddcup.info* file.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 打开 `pyspark-shell`，并将CSV数据加载为数据框架。这又是一个CSV文件，但没有头信息。需要根据附带的 *kddcup.info* 文件提供列名。
- en: '[PRE1]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Begin by exploring the dataset. What labels are present in the data, and how
    many are there of each? The following code simply counts by label and prints the
    results in descending order by count:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 从探索数据集开始。数据中存在哪些标签，每个标签有多少个？以下代码简单地按标签计数，并按计数降序打印结果：
- en: '[PRE2]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: There are 23 distinct labels, and the most frequent are `smurf.` and `neptune.`
    attacks.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 共有23个不同的标签，最常见的是 `smurf.` 和 `neptune.` 攻击。
- en: Note that the data contains nonnumeric features. For example, the second column
    may be `tcp`, `udp`, or `icmp`, but K-means clustering requires numeric features.
    The final label column is also nonnumeric. To begin, these will simply be ignored.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，数据包含非数字特征。例如，第二列可能是`tcp`、`udp`或`icmp`，但是 K-means 聚类需要数字特征。最终的标签列也是非数字的。首先，这些将被简单地忽略。
- en: Aside from this, creating a K-means clustering of the data follows the same
    pattern as was seen in [Chapter 4](ch04.xhtml#making_predictions_with_decision_trees_and_decision_forests).
    A `VectorAssembler` creates a feature vector, a `KMeans` implementation creates
    a model from the feature vectors, and a `Pipeline` stitches it all together. From
    the resulting model, it’s possible to extract and examine the cluster centers.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 除此之外，创建数据的 K-means 聚类遵循与[第四章](ch04.xhtml#making_predictions_with_decision_trees_and_decision_forests)中看到的相同模式。`VectorAssembler`
    创建特征向量，`KMeans` 实现从特征向量创建模型，`Pipeline` 将其全部连接起来。从生成的模型中，可以提取并检查聚类中心。
- en: '[PRE3]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: It’s not easy to interpret the numbers intuitively, but each of these represents
    the center (also known as centroid) of one of the clusters that the model produced.
    The values are the coordinates of the centroid in terms of each of the numeric
    input features.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 从直觉上解释这些数字并不容易，但每个数字代表了模型生成的一个聚类的中心（也称为质心）。这些值是质心在每个数字输入特征上的坐标。
- en: Two vectors are printed, meaning K-means was fitting *k*=2 clusters to the data.
    For a complex dataset that is known to exhibit at least 23 distinct types of connections,
    this is almost certainly not enough to accurately model the distinct groupings
    within the data.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 打印了两个向量，意味着 K-means 在数据上拟合了*k*=2个聚类。对于一个已知至少有23种不同连接类型的复杂数据集来说，这几乎肯定不足以准确建模数据中的不同分组。
- en: This is a good opportunity to use the given labels to get an intuitive sense
    of what went into these two clusters by counting the labels within each cluster.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个很好的机会，利用给定的标签来直观地了解这两个聚类中包含了什么，通过计算每个聚类内的标签数量。
- en: '[PRE4]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The result shows that the clustering was not at all helpful. Only one data point
    ended up in cluster 1!
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 结果显示，聚类毫无帮助。只有一个数据点最终进入了聚类1！
- en: Choosing k
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择*k*
- en: Two clusters are plainly insufficient. How many clusters are appropriate for
    this dataset? It’s clear that there are 23 distinct patterns in the data, so it
    seems that *k* could be at least 23, or likely even more. Typically, many values
    of *k* are tried to find the best one. But what is “best”?
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，两个聚类是不够的。对于这个数据集来说，适合多少个聚类？很明显，数据中存在23种不同的模式，因此*k*至少可以是23，甚至可能更多。通常会尝试许多*k*值来找到最佳值。但是什么是“最佳”呢？
- en: A clustering could be considered good if each data point were near its closest
    centroid, where “near” is defined by the Euclidean distance. This is a simple,
    common way to evaluate the quality of a clustering, by the mean of these distances
    over all points, or sometimes, the mean of the distances squared. In fact, `KMeansModel`
    offers a `ClusteringEvaluator` method that computes the sum of squared distances
    and can easily be used to compute the mean squared distance.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 如果每个数据点接近其最近的质心，则可以认为聚类是好的，“接近”由欧氏距离定义。这是评估聚类质量的一种简单常见方式，通过所有点的这些距离的均值，有时是距离平方的均值。事实上，`KMeansModel`
    提供了一个 `ClusteringEvaluator` 方法，可以计算平方距离的和，可以轻松用来计算均方距离。
- en: 'It’s simple enough to manually evaluate the clustering cost for several values
    of *k*. Note that this code could take 10 minutes or more to run:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 手动评估几个*k*值的聚类成本很简单。请注意，此代码可能需要运行10分钟或更长时间：
- en: '[PRE5]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[![1](assets/1.png)](#co_anomaly_detection__span_class__keep_together__with_k_means_clustering__span__CO1-1)'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_anomaly_detection__span_class__keep_together__with_k_means_clustering__span__CO1-1)'
- en: Scores will be shown here using scientific notation.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 分数将使用科学计数法显示。
- en: The printed result shows that the score decreases as *k* increases. Note that
    scores are shown in scientific notation; the first value is over 10⁷, not just
    a bit over 6.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 打印的结果显示，随着*k*的增加，分数下降。请注意，分数以科学计数法显示；第一个值超过10⁷，不仅仅是略高于6。
- en: Again, your values will be somewhat different. The clustering depends on a randomly
    chosen initial set of centroids.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 再次说明，您的值可能会有所不同。聚类取决于随机选择的初始质心集。
- en: However, this much is obvious. As more clusters are added, it should always
    be possible to put data points closer to the nearest centroid. In fact, if *k*
    is chosen to equal the number of data points, the average distance will be 0 because
    every point will be its own cluster of one!
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这是显而易见的。随着增加更多的聚类，总是可以将数据点放置在最近的质心附近。事实上，如果选择*k*等于数据点的数量，平均距离将为0，因为每个点将成为其自身的一个包含一个点的聚类！
- en: Worse, in the preceding results, the distance for *k*=80 is higher than for
    *k*=60\. This shouldn’t happen because a higher *k* always permits at least as
    good a clustering as a lower *k*. The problem is that K-means is not necessarily
    able to find the optimal clustering for a given *k*. Its iterative process can
    converge from a random starting point to a local minimum, which may be good but
    is not optimal.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 更糟糕的是，在前述结果中，*k*=80的距离比*k*=60的距离更高。这不应该发生，因为更高的*k*总是至少能够实现与较低*k*一样好的聚类。问题在于，K均值算法不一定能够为给定的*k*找到最优的聚类。其迭代过程可能会从一个随机起点收敛到局部最小值，这可能是良好但并非最优的。
- en: This is still true even when more intelligent methods are used to choose initial
    centroids. [K-means++ and K-means||](https://oreil.ly/zes8d) are variants of selection
    algorithms that are more likely to choose diverse, separated centroids and lead
    more reliably to good clustering. Spark MLlib, in fact, implements K-means||.
    However, all still have an element of randomness in selection and can’t guarantee
    an optimal clustering.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 即使使用更智能的方法选择初始质心，这仍然是真实的。[K-means++和K-means||](https://oreil.ly/zes8d)是选择算法的变体，更有可能选择多样化、分离的质心，并更可靠地导致良好的聚类。事实上，Spark
    MLlib实现了K-means||。然而，所有这些方法仍然在选择过程中具有随机性，并不能保证最优的聚类。
- en: The random starting set of clusters chosen for *k*=80 perhaps led to a particularly
    suboptimal clustering, or it may have stopped early before it reached its local
    optimum.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 对于*k*=80随机选择的起始聚类集可能导致特别次优的聚类，或者在达到局部最优之前可能提前停止。
- en: We can improve it by running the iteration longer. The algorithm has a threshold
    via `setTol` that controls the minimum amount of cluster centroid movement considered
    significant; lower values mean the K-means algorithm will let the centroids continue
    to move longer. Increasing the maximum number of iterations with `setMaxIter`
    also prevents it from potentially stopping too early at the cost of possibly more
    computation.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过增加迭代次数来改善它。该算法通过`setTol`设置一个阈值，控制被认为是显著的聚类质心移动的最小量；较低的值意味着K均值算法将允许质心继续移动更长时间。通过`setMaxIter`增加最大迭代次数也可以防止它在可能的计算成本更高的情况下过早停止。
- en: '[PRE6]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[![1](assets/1.png)](#co_anomaly_detection__span_class__keep_together__with_k_means_clustering__span__CO2-1)'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_anomaly_detection__span_class__keep_together__with_k_means_clustering__span__CO2-1)'
- en: Increase from default 20.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 从默认值20增加。
- en: '[![2](assets/2.png)](#co_anomaly_detection__span_class__keep_together__with_k_means_clustering__span__CO2-2)'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_anomaly_detection__span_class__keep_together__with_k_means_clustering__span__CO2-2)'
- en: Decrease from default 1.0e-4.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 从默认值1.0e-4减少。
- en: 'This time, at least the scores decrease consistently:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这一次，至少分数是持续下降的：
- en: '[PRE7]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We want to find a point past which increasing *k* stops reducing the score much—or
    an “elbow” in a graph of *k* versus score, which is generally decreasing but eventually
    flattens out. Here, it seems to be decreasing notably past 100\. The right value
    of *k* may be past 100.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望找到一个点，在增加*k*后停止显著降低分数——或者在*k*与分数图中找到一个“肘部”，通常情况下分数是递减的，但最终趋于平缓。在这里，看起来在100之后明显减少。正确的*k*值可能在100之后。
- en: Visualization with SparkR
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用SparkR进行可视化
- en: At this point, it could be useful to step back and understand more about the
    data before clustering again. In particular, looking at a plot of the data points
    could be helpful.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，重新聚类之前，深入了解数据可能会很有帮助。特别是查看数据点的图表可能会有所帮助。
- en: Spark itself has no tools for visualization, but the popular open source statistical
    environment [R](https://www.r-project.org) has libraries for both data exploration
    and data visualization. Furthermore, Spark also provides some basic integration
    with R via [SparkR](https://oreil.ly/XX0Q9). This brief section will demonstrate
    using R and SparkR to cluster the data and explore the clustering.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: Spark本身没有用于可视化的工具，但流行的开源统计环境[R](https://www.r-project.org)提供了数据探索和数据可视化的库。此外，Spark还通过[SparkR](https://oreil.ly/XX0Q9)提供了与R的基本集成。本简短部分将演示如何使用R和SparkR对数据进行聚类和探索聚类。
- en: SparkR is a variant of the `spark-shell` used throughout this book and is invoked
    with the command `sparkR`. It runs a local R interpreter, like `spark-shell` runs
    a variant of the Scala shell as a local process. The machine that runs `sparkR`
    needs a local installation of R, which is not included with Spark. This can be
    installed, for example, with `sudo apt-get install r-base` on Linux distributions
    like Ubuntu, or `brew install R` with [Homebrew](http://brew.sh) on macOS.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: SparkR 是本书中使用的 `spark-shell` 的变体，使用 `sparkR` 命令调用。它运行一个本地的 R 解释器，就像 `spark-shell`
    运行 Scala shell 的变体作为本地进程。运行 `sparkR` 的机器需要安装本地的 R，这不包含在 Spark 中。例如，在 Ubuntu 这样的
    Linux 发行版上可以通过 `sudo apt-get install r-base` 安装，或者在 macOS 上通过 [Homebrew](http://brew.sh)
    安装 R。
- en: SparkR is a command-line shell environment, like R. To view visualizations,
    it’s necessary to run these commands within an IDE-like environment that can display
    images. [RStudio](https://www.rstudio.com) is an IDE for R (and works with SparkR);
    it runs on a desktop operating system so it will be usable here only if you are
    experimenting with Spark locally rather than on a cluster.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: SparkR 是一个类似于 R 的命令行 shell 环境。要查看可视化效果，必须在能够显示图像的 IDE 类似环境中运行这些命令。[RStudio](https://www.rstudio.com)
    是一个适用于 R 的 IDE（与 SparkR 兼容）；它在桌面操作系统上运行，因此只有在本地实验 Spark 而不是在集群上时才能在此处使用。
- en: If you are running Spark locally, [download](https://oreil.ly/JZGQm) the free
    version of RStudio and install it. If not, then most of the rest of this example
    can still be run with `sparkR` on a command line—for example, on a cluster—though
    it won’t be possible to display visualizations this way.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在本地运行 Spark，请[下载](https://oreil.ly/JZGQm) RStudio 的免费版本并安装。如果没有，则本例的大部分其余部分仍可在命令行上（例如在集群上）通过
    `sparkR` 运行，尽管这种方式无法显示可视化。
- en: 'If you’re running via RStudio, launch the IDE and configure `SPARK_HOME` and
    `JAVA_HOME`, if your local environment does not already set them, to point to
    the Spark and JDK installation directories, respectively:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 如果通过 RStudio 运行，请启动 IDE 并配置 `SPARK_HOME` 和 `JAVA_HOME`，如果你的本地环境尚未设置这些路径，则指向
    Spark 和 JDK 的安装目录，分别如下：
- en: '[PRE8]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[![1](assets/1.png)](#comarker1a)'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#comarker1a)'
- en: Replace with actual paths, of course.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 当然要替换为实际路径。
- en: Note that these steps aren’t needed if you are running `sparkR` on the command
    line. Instead, it accepts command-line configuration parameters such as `--driver-memory`,
    just like `spark-shell`.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，如果在命令行上运行 `sparkR`，则无需执行这些步骤。相反，它接受类似于 `--driver-memory` 的命令行配置参数，就像 `spark-shell`
    一样。
- en: 'SparkR is an R-language wrapper around the same DataFrame and MLlib APIs that
    have been demonstrated in this chapter. It’s therefore possible to re-create a
    K-means simple clustering of the data:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: SparkR 是围绕相同的 DataFrame 和 MLlib API 的 R 语言封装，这些在本章中已经展示过。因此，可以重新创建数据的 K-means
    简单聚类：
- en: '[PRE9]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[![1](assets/1.png)](#comarker1b)'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#comarker1b)'
- en: Replace with path to *kddcup.data*.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 替换为 *kddcup.data* 的路径。
- en: '[![2](assets/2.png)](#comarker2)'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#comarker2)'
- en: Name columns.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 命名列。
- en: '[![3](assets/3.png)](#comarker3)'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#comarker3)'
- en: Drop nonnumeric columns again.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 再次丢弃非数字列。
- en: '[![4](assets/4.png)](#comarker4)'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](assets/4.png)](#comarker4)'
- en: '`~ .` means all columns.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '`~ .` 表示所有列。'
- en: From here, it’s straightforward to assign a cluster to each data point. The
    operations above show usage of the SparkR APIs which naturally correspond to core
    Spark APIs, but are expressed as R libraries in R-like syntax. The actual clustering
    is executed using the same JVM-based, Scala language implementation in MLlib.
    These operations are effectively a *handle*, or remote control, to distributed
    operations that are not executing in R.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 从这里开始，为每个数据点分配一个聚类很简单。上述操作展示了使用 SparkR API，这些 API 自然对应于核心 Spark API，但表达为 R 库的
    R-like 语法。实际的聚类是使用相同基于 JVM 的 Scala 语言实现的 MLlib 进行的。这些操作实际上是远程控制分布式操作，而不是在 R 中执行。
- en: R has its own rich set of libraries for analysis and its own similar concept
    of a dataframe. It is sometimes useful, therefore, to pull some data down into
    the R interpreter to be able to use these native R libraries, which are unrelated
    to Spark.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: R 拥有自己丰富的分析库以及类似的数据框架概念。因此，有时将一些数据下载到 R 解释器中以使用这些本地 R 库是很有用的，这些库与 Spark 无关。
- en: 'Of course, R and its libraries are not distributed, and so it’s not feasible
    to pull the whole dataset of 4,898,431 data points into R. However, it’s easy
    to pull only a sample:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，R及其库并未分发，因此将包含 4,898,431 个数据点的整个数据集导入 R 中是不可行的。不过，只导入一个样本却很容易：
- en: '[PRE10]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[![1](assets/1.png)](#co_anomaly_detection__span_class__keep_together__with_k_means_clustering__span__CO3-1)'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_anomaly_detection__span_class__keep_together__with_k_means_clustering__span__CO3-1)'
- en: 1% sample without replacement
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 无替换的1%样本
- en: '`clustering_sample` is actually a local R dataframe, not a Spark DataFrame,
    so it can be manipulated like any other data in R. Above, `str` shows the structure
    of the dataframe.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '`clustering_sample`实际上是一个本地的R数据框架，而不是Spark DataFrame，因此可以像R中的任何其他数据一样进行操作。上面的`str`显示了数据框架的结构。'
- en: 'For example, it’s possible to extract the cluster assignment and then show
    statistics about the distribution of assignments:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，可以提取集群分配，然后显示有关分配分布的统计信息：
- en: '[PRE11]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[![1](assets/1.png)](#co_anomaly_detection__span_class__keep_together__with_k_means_clustering__span__CO4-1)'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_anomaly_detection__span_class__keep_together__with_k_means_clustering__span__CO4-1)'
- en: Only the clustering assignment column
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 只有聚类分配列
- en: '[![2](assets/2.png)](#co_anomaly_detection__span_class__keep_together__with_k_means_clustering__span__CO4-2)'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_anomaly_detection__span_class__keep_together__with_k_means_clustering__span__CO4-2)'
- en: Everything but the clustering assignment
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 除了聚类分配之外的所有内容
- en: For example, this shows that most points fell into cluster 0\. Although much
    more could be done with this data in R, further coverage of this is beyond the
    scope of this book.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，这表明大多数点落入集群0。虽然在R中可以对此数据做更多处理，但进一步的覆盖范围超出了本书的范围。
- en: 'To visualize the data, a library called `rgl` is required. It will be functional
    only if running this example in RStudio. First, install (once) and load the library:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 要可视化数据，需要一个名为`rgl`的库。仅在RStudio中运行此示例时才会生效。首先安装（一次）并加载该库：
- en: '[PRE12]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Note that R may prompt you to download other packages or compiler tools to complete
    installation, because installing the package means compiling its source code.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，R可能会提示您下载其他包或编译器工具以完成安装，因为安装包意味着编译其源代码。
- en: 'This dataset is 38-dimensional. It will have to be projected down into at most
    three dimensions to visualize it with a *random projection*:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 此数据集有38个维度。必须将其投影到最多三个维度中以使用*随机投影*进行可视化：
- en: '[PRE13]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[![1](assets/1.png)](#co_anomaly_detection__span_class__keep_together__with_k_means_clustering__span__CO5-1)'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_anomaly_detection__span_class__keep_together__with_k_means_clustering__span__CO5-1)'
- en: Make a random 3-D projection and normalize.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 进行随机3-D投影并归一化。
- en: '[![2](assets/2.png)](#co_anomaly_detection__span_class__keep_together__with_k_means_clustering__span__CO5-2)'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_anomaly_detection__span_class__keep_together__with_k_means_clustering__span__CO5-2)'
- en: Project and make a new dataframe.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 投影并创建新的数据框架。
- en: This creates a 3-D dataset out of a 38-D dataset by choosing three random unit
    vectors and projecting the data onto them. This is a simplistic, rough-and-ready
    form of dimension reduction. Of course, there are more sophisticated dimension
    reduction algorithms, like principal component analysis or the singular value
    decomposition. These are available in R but take much longer to run. For purposes
    of visualization in this example, a random projection achieves much the same result,
    faster.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 通过选择三个随机单位向量并将数据投影到它们上，将38维数据集创建为3-D数据集。这是一种简单粗糙的降维方法。当然，还有更复杂的降维算法，比如主成分分析或奇异值分解。这些算法在R中都可以找到，但运行时间更长。在本示例中，随机投影可以更快地实现几乎相同的可视化效果。
- en: 'Finally, the clustered points can be plotted in an interactive 3-D visualization:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，可以在交互式3-D可视化中绘制集群点：
- en: '[PRE14]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Note that this will require running RStudio in an environment that supports
    the `rgl` library and graphics.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这将需要在支持`rgl`库和图形的环境中运行RStudio。
- en: The resulting visualization in [Figure 5-1](#AnomalyDetection_Projection1) shows
    data points in 3-D space. Many points fall on top of one another, and the result
    is sparse and hard to interpret. However, the dominant feature of the visualization
    is its L shape. The points seem to vary along two distinct dimensions, and little
    in other dimensions.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 5-1](#AnomalyDetection_Projection1)中的可视化展示了3-D空间中的数据点。许多点重叠在一起，结果稀疏且难以解释。然而，可视化的主要特征是其L形状。点似乎沿着两个不同的维度变化，其他维度变化较小。'
- en: This makes sense because the dataset has two features that are on a much larger
    scale than the others. Whereas most features have values between 0 and 1, the
    bytes-sent and bytes-received features vary from 0 to tens of thousands. The Euclidean
    distance between points is therefore almost completely determined by these two
    features. It’s almost as if the other features don’t exist! So it’s important
    to normalize away these differences in scale to put features on near-equal footing.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 这是有道理的，因为数据集中有两个特征的尺度远大于其他特征。大多数特征的值在 0 到 1 之间，而 bytes-sent 和 bytes-received
    特征的值则在 0 到数万之间变化。因此，点之间的欧氏距离几乎完全由这两个特征决定。其他特征几乎像不存在一样！因此，将这些尺度差异标准化是很重要的，以便使特征在接近相等的水平上。
- en: '![aaps 0501](assets/aaps_0501.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![aaps 0501](assets/aaps_0501.png)'
- en: Figure 5-1\. Random 3-D projection
  id: totrans-117
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-1\. 随机 3-D 投影
- en: Feature Normalization
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征标准化
- en: 'We can normalize each feature by converting it to a standard score. This means
    subtracting the mean of the feature’s values from each value and dividing by the
    standard deviation, as shown in the standard score equation:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过将每个特征转换为标准分数来标准化每个特征。这意味着从每个值的特征均值中减去该值，并除以标准差，如标准分数方程所示：
- en: <math alttext="n o r m a l i z e d Subscript i Baseline equals StartFraction
    f e a t u r e Subscript i Baseline minus mu Subscript i Baseline Over sigma Subscript
    i Baseline EndFraction" display="block"><mrow><mi>n</mi> <mi>o</mi> <mi>r</mi>
    <mi>m</mi> <mi>a</mi> <mi>l</mi> <mi>i</mi> <mi>z</mi> <mi>e</mi> <msub><mi>d</mi>
    <mi>i</mi></msub> <mo>=</mo> <mfrac><mrow><mi>f</mi><mi>e</mi><mi>a</mi><mi>t</mi><mi>u</mi><mi>r</mi><msub><mi>e</mi>
    <mi>i</mi></msub> <mo>-</mo><msub><mi>μ</mi> <mi>i</mi></msub></mrow> <msub><mi>σ</mi>
    <mi>i</mi></msub></mfrac></mrow></math>
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="n o r m a l i z e d Subscript i Baseline equals StartFraction
    f e a t u r e Subscript i Baseline minus mu Subscript i Baseline Over sigma Subscript
    i Baseline EndFraction" display="block"><mrow><mi>n</mi> <mi>o</mi> <mi>r</mi>
    <mi>m</mi> <mi>a</mi> <mi>l</mi> <mi>i</mi> <mi>z</mi> <mi>e</mi> <msub><mi>d</mi>
    <mi>i</mi></msub> <mo>=</mo> <mfrac><mrow><mi>f</mi><mi>e</mi><mi>a</mi><mi>t</mi><mi>u</mi><mi>r</mi><msub><mi>e</mi>
    <mi>i</mi></msub> <mo>-</mo><msub><mi>μ</mi> <mi>i</mi></msub></mrow> <msub><mi>σ</mi>
    <mi>i</mi></msub></mfrac></mrow></math>
- en: In fact, subtracting means has no effect on the clustering because the subtraction
    effectively shifts all the data points by the same amount in the same direction.
    This does not affect interpoint Euclidean distances.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，减去均值对聚类没有影响，因为减法实际上将所有数据点以相同的方向和相同的量移动。这不会影响点与点之间的欧氏距离。
- en: MLlib provides `StandardScaler`, a component that can perform this kind of standardization
    and be easily added to the clustering pipeline.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: MLlib 提供了 `StandardScaler`，这是一个可以执行这种标准化并且可以轻松添加到聚类管道中的组件。
- en: 'We can run the same test with normalized data on a higher range of *k*:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在更高范围的 *k* 上使用标准化数据运行相同的测试：
- en: '[PRE15]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: This has helped put dimensions on more equal footing, and the absolute distances
    between points (and thus the cost) is much smaller in absolute terms. However,
    the above output doesn’t yet provide an obvious value of *k* beyond which increasing
    it does little to improve the cost.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 这有助于使维度更平等，并且点之间的绝对距离（因此成本）在绝对值上要小得多。然而，上述输出尚未提供一个明显的 *k* 值，超过该值增加对成本的改进很少。
- en: Another 3-D visualization of the normalized data points reveals a richer structure,
    as expected. Some points are spaced in regular, discrete intervals in one direction;
    these are likely projections of discrete dimensions in the data, like counts.
    With 100 clusters, it’s hard to make out which points come from which clusters.
    One large cluster seems to dominate, and many clusters correspond to small, compact
    subregions (some of which are omitted from this zoomed detail of the entire 3-D
    visualization). The result, shown in [Figure 5-2](#AnomalyDetection_Projection2),
    does not necessarily advance the analysis but is an interesting sanity check.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 标准化后的数据点的另一个 3-D 可视化显示了预期的更丰富的结构。某些点在一个方向上以正规的离散间隔排列；这些可能是数据中离散维度（如计数）的投影。在
    100 个簇的情况下，很难确定哪些点来自哪些簇。一个大簇似乎占主导地位，许多簇对应于小型、紧凑的子区域（其中一些在整个 3-D 可视化的缩放细节中被省略）。在
    [图 5-2](#AnomalyDetection_Projection2) 中显示的结果并不一定推进分析，但是这是一个有趣的健全性检查。
- en: '![aaps 0502](assets/aaps_0502.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![aaps 0502](assets/aaps_0502.png)'
- en: Figure 5-2\. Random 3-D projection, normalized
  id: totrans-128
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-2\. 随机 3-D 投影，已标准化
- en: Categorical Variables
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类变量
- en: Normalization was a valuable step forward, but more can be done to improve the
    clustering. In particular, several features have been left out entirely because
    they aren’t numeric. This is throwing away valuable information. Adding them back,
    in some form, should produce a better-informed clustering.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 标准化是迈出的一大步，但可以做更多来改进聚类。特别是，由于它们不是数值的原因，一些特征已经完全被排除在外。这是在抛弃宝贵信息。以某种形式将它们重新添加回来应该会产生更为明智的聚类。
- en: Earlier, three categorical features were excluded because nonnumeric features
    can’t be used with the Euclidean distance function that K-means uses in MLlib.
    This is the reverse of the issue noted in [“Random Forests”](ch04.xhtml#RandomDecisionForests),
    where numeric features were used to represent categorical values but a categorical
    feature was desired.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 早些时候，由于非数值特征无法与 MLlib 中 K-means 使用的欧氏距离函数一起使用，三个分类特征被排除在外。这与 [“随机森林”](ch04.xhtml#RandomDecisionForests)
    中所指出的问题相反，那里使用数值特征来表示分类值，但却希望有一个分类特征。
- en: 'The categorical features can be translated into several binary indicator features
    using one-hot encoding, which can be viewed as numeric dimensions. For example,
    the second column contains the protocol type: `tcp`, `udp`, or `icmp`. This feature
    could be thought of as *three* features, as if features “is TCP,” “is UDP,” and
    “is ICMP” were in the dataset. The single feature value `tcp` might become `1,0,0`;
    `udp` might be `0,1,0`; and so on.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 使用独热编码将分类特征转换为多个二进制指示特征，可以看作是数值维度。例如，第二列包含协议类型：`tcp`、`udp`或`icmp`。这个特征可以被视为*三*个特征，就像数据集中有“是TCP”、“是UDP”和“是ICMP”一样。单个特征值`tcp`可能会变成`1,0,0`；`udp`可能是`0,1,0`；依此类推。
- en: Here again, MLlib provides components that implement this transformation. In
    fact, one-hot encoding string-valued features like `protocol_type` are actually
    a two-step process. First, the string values are converted to integer indices
    like 0, 1, 2, and so on using `StringIndexer`. Then, these integer indices are
    encoded into a vector with `OneHotEncoder`. These two steps can be thought of
    as a small `Pipeline` in themselves.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，MLlib提供了实现此转换的组件。事实上，对像`protocol_type`这样的字符串值特征进行独热编码实际上是一个两步过程。首先，将字符串值转换为像0、1、2等整数索引，使用`StringIndexer`。然后，这些整数索引被编码成一个向量，使用`OneHotEncoder`。这两个步骤可以被看作是一个小型的`Pipeline`。
- en: '[PRE16]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[![1](assets/1.png)](#co_anomaly_detection__span_class__keep_together__with_k_means_clustering__span__CO6-1)'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_anomaly_detection__span_class__keep_together__with_k_means_clustering__span__CO6-1)'
- en: Return pipeline and name of output vector column
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 返回管道和输出向量列的名称。
- en: This method produces a `Pipeline` that can be added as a component in the overall
    clustering pipeline; pipelines can be composed. All that is left is to make sure
    to add the new vector output columns into `VectorAssembler`’s output and proceed
    as before with scaling, clustering, and evaluation. The source code is omitted
    for brevity here, but can be found in the repository accompanying this chapter.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法生成一个`Pipeline`，可以作为整体聚类管道的组件添加；管道可以被组合。现在只需确保将新的向量输出列添加到`VectorAssembler`的输出中，并按照之前的方式进行缩放、聚类和评估。此处为了简洁起见省略了源代码，但可以在本章节附带的存储库中找到。
- en: '[PRE17]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: These sample results suggest, possibly, *k*=180 as a value where the score flattens
    out a bit. At least the clustering is now using all input features.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 这些样本结果表明，可能 *k*=180 是一个得分趋于平缓的值。至少聚类现在使用了所有的输入特征。
- en: Using Labels with Entropy
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用熵和标签
- en: Earlier, we used the given label for each data point to create a quick sanity
    check of the quality of the clustering. This notion can be formalized further
    and used as an alternative means of evaluating clustering quality and, therefore,
    of choosing *k*.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，我们使用每个数据点的给定标签来创建一个快速的聚类质量检查。这个概念可以进一步形式化，并用作评估聚类质量和因此选择*k*的替代手段。
- en: The labels tell us something about the true nature of each data point. A good
    clustering, it seems, should agree with these human-applied labels. It should
    put together points that share a label frequently and not lump together points
    of many different labels. It should produce clusters with relatively homogeneous
    labels.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 标签告诉我们关于每个数据点真实本质的信息。一个好的聚类应该与这些人工应用的标签一致。它应该将经常共享标签的点放在一起，而不是将许多不同标签的点放在一起。它应该产生具有相对均匀标签的簇。
- en: 'You may recall from [“Random Forests”](ch04.xhtml#RandomDecisionForests) that
    we have metrics for homogeneity: Gini impurity and entropy. These are functions
    of the proportions of labels in each cluster and produce a number that is low
    when the proportions are skewed toward few, or one, label. Entropy will be used
    here for illustration:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能还记得[“随机森林”](ch04.xhtml#RandomDecisionForests)中我们有关于同质性的度量：基尼不纯度和熵。这些是每个簇中标签比例的函数，并产生一个在标签比例偏向少数或一个标签时低的数字。这里将使用熵来进行说明：
- en: '[PRE18]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'A good clustering would have clusters whose collections of labels are homogeneous
    and so have low entropy. A weighted average of entropy can therefore be used as
    a cluster score:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 一个好的聚类应该有具有同质标签的簇，因此熵应该较低。因此，可以使用熵的加权平均作为簇得分：
- en: '[PRE19]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[![1](assets/1.png)](#co_anomaly_detection__span_class__keep_together__with_k_means_clustering__span__CO7-1)'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_anomaly_detection__span_class__keep_together__with_k_means_clustering__span__CO7-1)'
- en: Predict cluster for each datum.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 针对每个数据点预测簇。
- en: '[![2](assets/2.png)](#co_anomaly_detection__span_class__keep_together__with_k_means_clustering__span__CO7-2)'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_anomaly_detection__span_class__keep_together__with_k_means_clustering__span__CO7-2)'
- en: Count labels, per cluster
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 计算每个簇的标签数。
- en: '[![3](assets/3.png)](#co_anomaly_detection__span_class__keep_together__with_k_means_clustering__span__CO7-3)'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_anomaly_detection__span_class__keep_together__with_k_means_clustering__span__CO7-3)'
- en: Average entropy weighted by cluster size.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 由聚类大小加权的平均熵。
- en: 'As before, this analysis can be used to obtain some idea of a suitable value
    of *k*. Entropy will not necessarily decrease as *k* increases, so it is possible
    to look for a local minimum value. Here again, results suggest *k*=180 is a reasonable
    choice because its score is actually lower than 150 and 210:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前一样，此分析可用于获得关于 *k* 的适当值的一些想法。熵不一定会随着 *k* 的增加而减少，因此可以寻找局部最小值。在这里，结果再次表明 *k*=180
    是一个合理的选择，因为其分数实际上比 150 和 210 都要低：
- en: '[PRE20]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Clustering in Action
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚类实战
- en: 'Finally, with confidence, we can cluster the full, normalized dataset with
    *k*=180\. Again, we can print the labels for each cluster to get some sense of
    the resulting clustering. Clusters do seem to be dominated by one type of attack
    each and contain only a few types:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，有信心地使用 *k*=180 对完整的标准化数据集进行聚类。同样，我们可以打印每个聚类的标签，以对得到的聚类结果有所了解。聚类似乎主要由一种类型的攻击主导，并且只包含少数类型：
- en: '[PRE21]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[![1](assets/1.png)](#co_anomaly_detection__span_class__keep_together__with_k_means_clustering__span__CO8-1)'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_anomaly_detection__span_class__keep_together__with_k_means_clustering__span__CO8-1)'
- en: See accompanying source code for `fit_pipeline_4` definition.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 查看 `fit_pipeline_4` 定义的相关源代码。
- en: 'Now we can make an actual anomaly detector. Anomaly detection amounts to measuring
    a new data point’s distance to its nearest centroid. If this distance exceeds
    some threshold, it is anomalous. This threshold might be chosen to be the distance
    of, say, the 100th-farthest data point from among known data:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以制作一个实际的异常检测器。异常检测就是测量新数据点到其最近质心的距离。如果这个距离超过了某个阈值，那么它是异常的。此阈值可以选择为已知数据中第100个最远数据点的距离：
- en: '[PRE22]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The final step can be to apply this threshold to all new data points as they
    arrive. For example, Spark Streaming can be used to apply this function to small
    batches of input data arriving from sources like Kafka or files in cloud storage.
    Data points exceeding the threshold might trigger an alert that sends an email
    or updates a database.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步可以是将这个阈值应用于所有新数据点。例如，可以使用 Spark Streaming 将此函数应用于从诸如 Kafka 或云存储中的文件等来源到达的小批量输入数据。超过阈值的数据点可能会触发一个警报，发送电子邮件或更新数据库。
- en: Where to Go from Here
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 接下来做什么
- en: The `KMeansModel` is, by itself, the essence of an anomaly detection system.
    The preceding code demonstrated how to apply it to data to detect anomalies. This
    same code could be used within [Spark Streaming](https://oreil.ly/UHHBR) to score
    new data as it arrives in near real time, and perhaps trigger an alert or review.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '`KMeansModel` 本身就是异常检测系统的精髓。前面的代码演示了如何将其应用于数据以检测异常。这段代码也可以在[Spark Streaming](https://oreil.ly/UHHBR)中使用，以几乎实时地对新数据进行评分，并可能触发警报或审核。'
- en: MLlib also includes a variation called `StreamingKMeans`, which can update a
    clustering incrementally as new data arrives in a `StreamingKMeansModel`. We could
    use this to continue to learn, approximately, how new data affects the clustering,
    and not just to assess new data against existing clusters. It can be integrated
    with Spark Streaming as well. However, it has not been updated for the new DataFrame-based
    APIs.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: MLlib 还包括一种称为 `StreamingKMeans` 的变体，它可以将聚类作为新数据以增量方式到达时更新到 `StreamingKMeansModel`
    中。我们可以使用这个来继续学习，大致了解新数据如何影响聚类，而不仅仅是对现有聚类评估新数据。它也可以与 Spark Streaming 集成。但是，它尚未针对新的基于
    DataFrame 的 API 进行更新。
- en: This model is only a simplistic one. For example, Euclidean distance is used
    in this example because it is the only distance function supported by Spark MLlib
    at this time. In the future, it may be possible to use distance functions that
    can better account for the distributions of and correlations between features,
    such as the [Mahalanobis distance](https://oreil.ly/PKG7A).
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型只是一个简单的模型。例如，这个例子中使用欧几里得距离是因为这是 Spark MLlib 目前支持的唯一距离函数。在未来，可能会使用能更好地考虑特征分布和相关性的距离函数，比如[马哈拉诺比斯距离](https://oreil.ly/PKG7A)。
- en: There are also more sophisticated [cluster-quality evaluation metrics](https://oreil.ly/9yE9P)
    that could be applied (even without labels) to pick *k*, such as the [Silhouette
    coefficient](https://oreil.ly/LMN1h). These tend to evaluate not just closeness
    of points within one cluster, but closeness of points to other clusters. Finally,
    different models could be applied instead of simple K-means clustering; for example,
    a [Gaussian mixture model](https://oreil.ly/KTgD6) or [DBSCAN](https://oreil.ly/xlshs)
    could capture more subtle relationships between data points and the cluster centers.
    Spark MLlib already implements [Gaussian mixture models](https://oreil.ly/LG84u);
    implementations of others may become available in Spark MLlib or other Spark-based
    libraries in the future.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 还有更复杂的[聚类质量评估指标](https://oreil.ly/9yE9P)，即使在没有标签的情况下也可以应用于选择*k*，例如[轮廓系数](https://oreil.ly/LMN1h)。这些指标倾向于评估不仅仅是一个簇内点的紧密程度，还包括点与其他簇的紧密程度。最后，可以应用不同的模型来取代简单的K-means聚类；例如，[高斯混合模型](https://oreil.ly/KTgD6)或者[DBSCAN](https://oreil.ly/xlshs)可以捕捉数据点与簇中心之间更微妙的关系。Spark
    MLlib已经实现了[高斯混合模型](https://oreil.ly/LG84u)；其他模型的实现可能会在Spark MLlib或其他基于Spark的库中出现。
- en: Of course, clustering isn’t just for anomaly detection. In fact, it’s more often
    associated with use cases where the actual clusters matter! For example, clustering
    can also be used to group customers according to their behaviors, preferences,
    and attributes. Each cluster, by itself, might represent a usefully distinguishable
    type of customer. This is a more data-driven way to segment customers rather than
    leaning on arbitrary, generic divisions like “age 20–34” and “female.”
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，聚类不仅仅用于异常检测。事实上，它更常用于实际集群很重要的用例！例如，聚类还可以根据客户的行为、偏好和属性对客户进行分组。每个簇本身可能代表一种有用可区分的客户类型。这是一种更加数据驱动的客户分段方式，而不是依赖于像“年龄20-34岁”和“女性”这样任意的通用划分。
