- en: Chapter 5\. Anomaly Detection with K-means Clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Classification and regression are powerful, well-studied techniques in machine
    learning. [Chapter 4](ch04.xhtml#making_predictions_with_decision_trees_and_decision_forests)
    demonstrated using a classifier as a predictor of unknown values. But there was
    a catch: to predict unknown values for new data, we had to know the target values
    for many previously seen examples. Classifiers can help only if we, the data scientists,
    know what we are looking for and can provide plenty of examples where input produced
    a known output. These were collectively known as *supervised learning* techniques,
    because their learning process receives the correct output value for each example
    in the input.'
  prefs: []
  type: TYPE_NORMAL
- en: 'However, sometimes the correct output is unknown for some or all examples.
    Consider the problem of dividing up an ecommerce site’s customers by their shopping
    habits and tastes. The input features are their purchases, clicks, demographic
    information, and more. The output should be groupings of customers: perhaps one
    group will represent fashion-conscious buyers, another will turn out to correspond
    to price-sensitive bargain hunters, and so on.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you were asked to determine this target label for each new customer, you
    would quickly run into a problem in applying a supervised learning technique like
    a classifier: you don’t know a priori who should be considered fashion-conscious,
    for example. In fact, you’re not even sure if “fashion-conscious” is a meaningful
    grouping of the site’s customers to begin with!'
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, *unsupervised learning* techniques can help. These techniques do
    not learn to predict a target value, because none is available. They can, however,
    learn structure in data and find groupings of similar inputs, or learn what types
    of input are likely to occur and what types are not. This chapter will introduce
    unsupervised learning using clustering implementations in MLlib. Specifically,
    we will use the K-means clustering algorithm for identifying anomalies in network
    traffic data. Anomaly detection is often used to find fraud, detect network attacks,
    or discover problems in servers or other sensor-equipped machinery. In these cases,
    it’s important to be able to find new types of anomalies that have never been
    seen before—new forms of fraud, intrusions, and failure modes for servers. Unsupervised
    learning techniques are useful in these cases because they can learn what input
    data normally looks like and therefore detect when new data is unlike past data.
    Such new data is not necessarily attacks or fraud; it is simply unusual and therefore
    worth further investigation.
  prefs: []
  type: TYPE_NORMAL
- en: We will start with the basics of the K-means clustering algorithm. This will
    be followed by an introduction to the KDD Cup 1999 dataset. We’ll then create
    our first K-means model using PySpark. Then we’ll go over methods for determining
    a good value of *k*—number of clusters—when implementing the K-means algorithm.
    Next, we’ll improve our model by normalizing the input features and using previously
    discarded categorical features by implementing the one-hot encoding method. We
    will wrap up by going over the entropy metric and exploring some results from
    our model.
  prefs: []
  type: TYPE_NORMAL
- en: K-means Clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The inherent problem of anomaly detection is, as its name implies, that of finding
    unusual things. If we already knew what “anomalous” meant for a dataset, we could
    easily detect anomalies in the data with supervised learning. An algorithm would
    receive inputs labeled “normal” and “anomaly” and learn to distinguish the two.
    However, the nature of anomalies is that they are unknown unknowns. Put another
    way, an anomaly that has been observed and understood is no longer an anomaly.
  prefs: []
  type: TYPE_NORMAL
- en: Clustering is the best-known type of unsupervised learning. Clustering algorithms
    try to find natural groupings in data. Data points that are like one another but
    unlike others are likely to represent a meaningful grouping, so clustering algorithms
    try to put such data into the same cluster.
  prefs: []
  type: TYPE_NORMAL
- en: K-means clustering may be the most widely used clustering algorithm. It attempts
    to detect *k* clusters in a dataset, where *k* is given by the data scientist.
    *k* is a hyperparameter of the model, and the right value will depend on the dataset.
    In fact, choosing a good value for *k* will be a central plot point in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: What does “like” mean when the dataset contains information such as customer
    activity? Or transactions? K-means requires a notion of distance between data
    points. It is common to use simple Euclidean distance to measure distance between
    data points with K-means, and as it happens, this is one of two distance functions
    supported by Spark MLlib as of this writing, the other one being Cosine. The Euclidean
    distance is defined for data points whose features are all numeric. “Like” points
    are those whose intervening distance is small.
  prefs: []
  type: TYPE_NORMAL
- en: 'To K-means, a cluster is simply a point: the center of all the points that
    make up the cluster. These are, in fact, just feature vectors containing all numeric
    features and can be called vectors. However, it may be more intuitive to think
    of them as points here, because they are treated as points in a Euclidean space.'
  prefs: []
  type: TYPE_NORMAL
- en: This center is called the cluster *centroid* and is the arithmetic mean of the
    points—hence the name K-*means*. To start, the algorithm picks some data points
    as the initial cluster centroids. Then each data point is assigned to the nearest
    centroid. Then for each cluster, a new cluster centroid is computed as the mean
    of the data points just assigned to that cluster. This process is repeated.
  prefs: []
  type: TYPE_NORMAL
- en: We will now look at a use case that depicts how K-means clustering can help
    us identify potentially anomalous activity in a network.
  prefs: []
  type: TYPE_NORMAL
- en: Identifying Anomalous Network Traffic
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Cyberattacks are increasingly visible in the news. Some attacks attempt to flood
    a computer with network traffic to crowd out legitimate traffic. But in other
    cases, attacks attempt to exploit flaws in networking software to gain unauthorized
    access to a computer. While it’s quite obvious when a computer is being bombarded
    with traffic, detecting an exploit can be like searching for a needle in an incredibly
    large haystack of network requests.
  prefs: []
  type: TYPE_NORMAL
- en: Some exploit behaviors follow known patterns. For example, accessing every port
    on a machine in rapid succession is not something any normal software program
    should ever need to do. However, it is a typical first step for an attacker looking
    for services running on the computer that may be exploitable.
  prefs: []
  type: TYPE_NORMAL
- en: If you were to count the number of distinct ports accessed by a remote host
    in a short time, you would have a feature that probably predicts a port-scanning
    attack quite well. A handful is probably normal; hundreds indicate an attack.
    The same goes for detecting other types of attacks from other features of network
    connections—number of bytes sent and received, TCP errors, and so forth.
  prefs: []
  type: TYPE_NORMAL
- en: But what about those unknown unknowns? The biggest threat may be the one that
    has never yet been detected and classified. Part of detecting potential network
    intrusions is detecting anomalies. These are connections that aren’t known to
    be attacks but do not resemble connections that have been observed in the past.
  prefs: []
  type: TYPE_NORMAL
- en: Here, unsupervised learning techniques like K-means can be used to detect anomalous
    network connections. K-means can cluster connections based on statistics about
    each of them. The resulting clusters themselves aren’t interesting per se, but
    they collectively define types of connections that are like past connections.
    Anything not close to a cluster could be anomalous. Clusters are interesting insofar
    as they define regions of normal connections; everything else is unusual and potentially
    anomalous.
  prefs: []
  type: TYPE_NORMAL
- en: KDD Cup 1999 Dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The [KDD Cup](https://oreil.ly/UtYd9) was an annual data mining competition
    organized by a special interest group of the Association for Computing Machinery
    (ACM). Each year, a machine learning problem was posed, along with a dataset,
    and researchers were invited to submit a paper detailing their best solution to
    the problem. In 1999, the topic was network intrusion, and the dataset is [still
    available](https://oreil.ly/ezBDa) at the KDD website. We will need to download
    the *kddcupdata.data.gz* and *kddcup.info* files from the website. The remainder
    of this chapter will walk through building a system to detect anomalous network
    traffic using Spark, by learning from this data.
  prefs: []
  type: TYPE_NORMAL
- en: Don’t use this dataset to build a real network intrusion system! The data did
    not necessarily reflect real network traffic at the time—even if it did, it reflects
    traffic patterns from more than 20 years ago.
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, the organizers had already processed raw network packet data into
    summary information about individual network connections. The dataset is about
    708 MB in size and contains about 4.9 million connections. This is large, if not
    massive, and is certainly sufficient for our purposes here. For each connection,
    the dataset contains information such as the number of bytes sent, login attempts,
    TCP errors, and so on. Each connection is one line of CSV-formatted data, containing
    38 features. Feature information and ordering can be found in the *kddcup.info*
    file.
  prefs: []
  type: TYPE_NORMAL
- en: 'Unzip the *kddcup.data.gz* data file and copy it into your storage. This example,
    like others, will assume the file is available at *data/kddcup.data*. Let’s see
    the data in its raw form:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This connection, for example, was a TCP connection to an HTTP service—215 bytes
    were sent, and 45,706 bytes were received. The user was logged in, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Many features are counts, like `num_file_creations` in the 17th column, as listed
    in the *kddcup.info* file. Many features take on the value 0 or 1, indicating
    the presence or absence of a behavior, like `su_attempted` in the 15th column.
    They look like the one-hot encoded categorical features from [Chapter 4](ch04.xhtml#making_predictions_with_decision_trees_and_decision_forests),
    but are not grouped and related in the same way. Each is like a yes/no feature,
    and is therefore arguably a categorical feature. It is not always valid to translate
    categorical features as numbers and treat them as if they had an ordering. However,
    in the special case of a binary categorical feature, in most machine learning
    algorithms, mapping these to a numeric feature taking on values 0 and 1 will work
    well.
  prefs: []
  type: TYPE_NORMAL
- en: The rest are ratios like `dst_host_srv_rerror_rate` in the next-to-last column
    and take on values from 0.0 to 1.0, inclusive.
  prefs: []
  type: TYPE_NORMAL
- en: Interestingly, a label is given in the last field. Most connections are labeled
    `normal.`, but some have been identified as examples of various types of network
    attacks. These would be useful in learning to distinguish a known attack from
    a normal connection, but the problem here is anomaly detection and finding potentially
    new and unknown attacks. This label will be mostly set aside for our purposes.
  prefs: []
  type: TYPE_NORMAL
- en: A First Take on Clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Open the `pyspark-shell`, and load the CSV data as a dataframe. It’s a CSV file
    again, but without header information. It’s necessary to supply column names as
    given in the accompanying *kddcup.info* file.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Begin by exploring the dataset. What labels are present in the data, and how
    many are there of each? The following code simply counts by label and prints the
    results in descending order by count:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: There are 23 distinct labels, and the most frequent are `smurf.` and `neptune.`
    attacks.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the data contains nonnumeric features. For example, the second column
    may be `tcp`, `udp`, or `icmp`, but K-means clustering requires numeric features.
    The final label column is also nonnumeric. To begin, these will simply be ignored.
  prefs: []
  type: TYPE_NORMAL
- en: Aside from this, creating a K-means clustering of the data follows the same
    pattern as was seen in [Chapter 4](ch04.xhtml#making_predictions_with_decision_trees_and_decision_forests).
    A `VectorAssembler` creates a feature vector, a `KMeans` implementation creates
    a model from the feature vectors, and a `Pipeline` stitches it all together. From
    the resulting model, it’s possible to extract and examine the cluster centers.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: It’s not easy to interpret the numbers intuitively, but each of these represents
    the center (also known as centroid) of one of the clusters that the model produced.
    The values are the coordinates of the centroid in terms of each of the numeric
    input features.
  prefs: []
  type: TYPE_NORMAL
- en: Two vectors are printed, meaning K-means was fitting *k*=2 clusters to the data.
    For a complex dataset that is known to exhibit at least 23 distinct types of connections,
    this is almost certainly not enough to accurately model the distinct groupings
    within the data.
  prefs: []
  type: TYPE_NORMAL
- en: This is a good opportunity to use the given labels to get an intuitive sense
    of what went into these two clusters by counting the labels within each cluster.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The result shows that the clustering was not at all helpful. Only one data point
    ended up in cluster 1!
  prefs: []
  type: TYPE_NORMAL
- en: Choosing k
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Two clusters are plainly insufficient. How many clusters are appropriate for
    this dataset? It’s clear that there are 23 distinct patterns in the data, so it
    seems that *k* could be at least 23, or likely even more. Typically, many values
    of *k* are tried to find the best one. But what is “best”?
  prefs: []
  type: TYPE_NORMAL
- en: A clustering could be considered good if each data point were near its closest
    centroid, where “near” is defined by the Euclidean distance. This is a simple,
    common way to evaluate the quality of a clustering, by the mean of these distances
    over all points, or sometimes, the mean of the distances squared. In fact, `KMeansModel`
    offers a `ClusteringEvaluator` method that computes the sum of squared distances
    and can easily be used to compute the mean squared distance.
  prefs: []
  type: TYPE_NORMAL
- en: 'It’s simple enough to manually evaluate the clustering cost for several values
    of *k*. Note that this code could take 10 minutes or more to run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_anomaly_detection__span_class__keep_together__with_k_means_clustering__span__CO1-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Scores will be shown here using scientific notation.
  prefs: []
  type: TYPE_NORMAL
- en: The printed result shows that the score decreases as *k* increases. Note that
    scores are shown in scientific notation; the first value is over 10⁷, not just
    a bit over 6.
  prefs: []
  type: TYPE_NORMAL
- en: Again, your values will be somewhat different. The clustering depends on a randomly
    chosen initial set of centroids.
  prefs: []
  type: TYPE_NORMAL
- en: However, this much is obvious. As more clusters are added, it should always
    be possible to put data points closer to the nearest centroid. In fact, if *k*
    is chosen to equal the number of data points, the average distance will be 0 because
    every point will be its own cluster of one!
  prefs: []
  type: TYPE_NORMAL
- en: Worse, in the preceding results, the distance for *k*=80 is higher than for
    *k*=60\. This shouldn’t happen because a higher *k* always permits at least as
    good a clustering as a lower *k*. The problem is that K-means is not necessarily
    able to find the optimal clustering for a given *k*. Its iterative process can
    converge from a random starting point to a local minimum, which may be good but
    is not optimal.
  prefs: []
  type: TYPE_NORMAL
- en: This is still true even when more intelligent methods are used to choose initial
    centroids. [K-means++ and K-means||](https://oreil.ly/zes8d) are variants of selection
    algorithms that are more likely to choose diverse, separated centroids and lead
    more reliably to good clustering. Spark MLlib, in fact, implements K-means||.
    However, all still have an element of randomness in selection and can’t guarantee
    an optimal clustering.
  prefs: []
  type: TYPE_NORMAL
- en: The random starting set of clusters chosen for *k*=80 perhaps led to a particularly
    suboptimal clustering, or it may have stopped early before it reached its local
    optimum.
  prefs: []
  type: TYPE_NORMAL
- en: We can improve it by running the iteration longer. The algorithm has a threshold
    via `setTol` that controls the minimum amount of cluster centroid movement considered
    significant; lower values mean the K-means algorithm will let the centroids continue
    to move longer. Increasing the maximum number of iterations with `setMaxIter`
    also prevents it from potentially stopping too early at the cost of possibly more
    computation.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_anomaly_detection__span_class__keep_together__with_k_means_clustering__span__CO2-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Increase from default 20.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_anomaly_detection__span_class__keep_together__with_k_means_clustering__span__CO2-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Decrease from default 1.0e-4.
  prefs: []
  type: TYPE_NORMAL
- en: 'This time, at least the scores decrease consistently:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: We want to find a point past which increasing *k* stops reducing the score much—or
    an “elbow” in a graph of *k* versus score, which is generally decreasing but eventually
    flattens out. Here, it seems to be decreasing notably past 100\. The right value
    of *k* may be past 100.
  prefs: []
  type: TYPE_NORMAL
- en: Visualization with SparkR
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At this point, it could be useful to step back and understand more about the
    data before clustering again. In particular, looking at a plot of the data points
    could be helpful.
  prefs: []
  type: TYPE_NORMAL
- en: Spark itself has no tools for visualization, but the popular open source statistical
    environment [R](https://www.r-project.org) has libraries for both data exploration
    and data visualization. Furthermore, Spark also provides some basic integration
    with R via [SparkR](https://oreil.ly/XX0Q9). This brief section will demonstrate
    using R and SparkR to cluster the data and explore the clustering.
  prefs: []
  type: TYPE_NORMAL
- en: SparkR is a variant of the `spark-shell` used throughout this book and is invoked
    with the command `sparkR`. It runs a local R interpreter, like `spark-shell` runs
    a variant of the Scala shell as a local process. The machine that runs `sparkR`
    needs a local installation of R, which is not included with Spark. This can be
    installed, for example, with `sudo apt-get install r-base` on Linux distributions
    like Ubuntu, or `brew install R` with [Homebrew](http://brew.sh) on macOS.
  prefs: []
  type: TYPE_NORMAL
- en: SparkR is a command-line shell environment, like R. To view visualizations,
    it’s necessary to run these commands within an IDE-like environment that can display
    images. [RStudio](https://www.rstudio.com) is an IDE for R (and works with SparkR);
    it runs on a desktop operating system so it will be usable here only if you are
    experimenting with Spark locally rather than on a cluster.
  prefs: []
  type: TYPE_NORMAL
- en: If you are running Spark locally, [download](https://oreil.ly/JZGQm) the free
    version of RStudio and install it. If not, then most of the rest of this example
    can still be run with `sparkR` on a command line—for example, on a cluster—though
    it won’t be possible to display visualizations this way.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you’re running via RStudio, launch the IDE and configure `SPARK_HOME` and
    `JAVA_HOME`, if your local environment does not already set them, to point to
    the Spark and JDK installation directories, respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#comarker1a)'
  prefs: []
  type: TYPE_NORMAL
- en: Replace with actual paths, of course.
  prefs: []
  type: TYPE_NORMAL
- en: Note that these steps aren’t needed if you are running `sparkR` on the command
    line. Instead, it accepts command-line configuration parameters such as `--driver-memory`,
    just like `spark-shell`.
  prefs: []
  type: TYPE_NORMAL
- en: 'SparkR is an R-language wrapper around the same DataFrame and MLlib APIs that
    have been demonstrated in this chapter. It’s therefore possible to re-create a
    K-means simple clustering of the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#comarker1b)'
  prefs: []
  type: TYPE_NORMAL
- en: Replace with path to *kddcup.data*.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#comarker2)'
  prefs: []
  type: TYPE_NORMAL
- en: Name columns.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#comarker3)'
  prefs: []
  type: TYPE_NORMAL
- en: Drop nonnumeric columns again.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#comarker4)'
  prefs: []
  type: TYPE_NORMAL
- en: '`~ .` means all columns.'
  prefs: []
  type: TYPE_NORMAL
- en: From here, it’s straightforward to assign a cluster to each data point. The
    operations above show usage of the SparkR APIs which naturally correspond to core
    Spark APIs, but are expressed as R libraries in R-like syntax. The actual clustering
    is executed using the same JVM-based, Scala language implementation in MLlib.
    These operations are effectively a *handle*, or remote control, to distributed
    operations that are not executing in R.
  prefs: []
  type: TYPE_NORMAL
- en: R has its own rich set of libraries for analysis and its own similar concept
    of a dataframe. It is sometimes useful, therefore, to pull some data down into
    the R interpreter to be able to use these native R libraries, which are unrelated
    to Spark.
  prefs: []
  type: TYPE_NORMAL
- en: 'Of course, R and its libraries are not distributed, and so it’s not feasible
    to pull the whole dataset of 4,898,431 data points into R. However, it’s easy
    to pull only a sample:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_anomaly_detection__span_class__keep_together__with_k_means_clustering__span__CO3-1)'
  prefs: []
  type: TYPE_NORMAL
- en: 1% sample without replacement
  prefs: []
  type: TYPE_NORMAL
- en: '`clustering_sample` is actually a local R dataframe, not a Spark DataFrame,
    so it can be manipulated like any other data in R. Above, `str` shows the structure
    of the dataframe.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, it’s possible to extract the cluster assignment and then show
    statistics about the distribution of assignments:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_anomaly_detection__span_class__keep_together__with_k_means_clustering__span__CO4-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Only the clustering assignment column
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_anomaly_detection__span_class__keep_together__with_k_means_clustering__span__CO4-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Everything but the clustering assignment
  prefs: []
  type: TYPE_NORMAL
- en: For example, this shows that most points fell into cluster 0\. Although much
    more could be done with this data in R, further coverage of this is beyond the
    scope of this book.
  prefs: []
  type: TYPE_NORMAL
- en: 'To visualize the data, a library called `rgl` is required. It will be functional
    only if running this example in RStudio. First, install (once) and load the library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Note that R may prompt you to download other packages or compiler tools to complete
    installation, because installing the package means compiling its source code.
  prefs: []
  type: TYPE_NORMAL
- en: 'This dataset is 38-dimensional. It will have to be projected down into at most
    three dimensions to visualize it with a *random projection*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_anomaly_detection__span_class__keep_together__with_k_means_clustering__span__CO5-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Make a random 3-D projection and normalize.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_anomaly_detection__span_class__keep_together__with_k_means_clustering__span__CO5-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Project and make a new dataframe.
  prefs: []
  type: TYPE_NORMAL
- en: This creates a 3-D dataset out of a 38-D dataset by choosing three random unit
    vectors and projecting the data onto them. This is a simplistic, rough-and-ready
    form of dimension reduction. Of course, there are more sophisticated dimension
    reduction algorithms, like principal component analysis or the singular value
    decomposition. These are available in R but take much longer to run. For purposes
    of visualization in this example, a random projection achieves much the same result,
    faster.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, the clustered points can be plotted in an interactive 3-D visualization:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Note that this will require running RStudio in an environment that supports
    the `rgl` library and graphics.
  prefs: []
  type: TYPE_NORMAL
- en: The resulting visualization in [Figure 5-1](#AnomalyDetection_Projection1) shows
    data points in 3-D space. Many points fall on top of one another, and the result
    is sparse and hard to interpret. However, the dominant feature of the visualization
    is its L shape. The points seem to vary along two distinct dimensions, and little
    in other dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: This makes sense because the dataset has two features that are on a much larger
    scale than the others. Whereas most features have values between 0 and 1, the
    bytes-sent and bytes-received features vary from 0 to tens of thousands. The Euclidean
    distance between points is therefore almost completely determined by these two
    features. It’s almost as if the other features don’t exist! So it’s important
    to normalize away these differences in scale to put features on near-equal footing.
  prefs: []
  type: TYPE_NORMAL
- en: '![aaps 0501](assets/aaps_0501.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-1\. Random 3-D projection
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Feature Normalization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can normalize each feature by converting it to a standard score. This means
    subtracting the mean of the feature’s values from each value and dividing by the
    standard deviation, as shown in the standard score equation:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="n o r m a l i z e d Subscript i Baseline equals StartFraction
    f e a t u r e Subscript i Baseline minus mu Subscript i Baseline Over sigma Subscript
    i Baseline EndFraction" display="block"><mrow><mi>n</mi> <mi>o</mi> <mi>r</mi>
    <mi>m</mi> <mi>a</mi> <mi>l</mi> <mi>i</mi> <mi>z</mi> <mi>e</mi> <msub><mi>d</mi>
    <mi>i</mi></msub> <mo>=</mo> <mfrac><mrow><mi>f</mi><mi>e</mi><mi>a</mi><mi>t</mi><mi>u</mi><mi>r</mi><msub><mi>e</mi>
    <mi>i</mi></msub> <mo>-</mo><msub><mi>μ</mi> <mi>i</mi></msub></mrow> <msub><mi>σ</mi>
    <mi>i</mi></msub></mfrac></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: In fact, subtracting means has no effect on the clustering because the subtraction
    effectively shifts all the data points by the same amount in the same direction.
    This does not affect interpoint Euclidean distances.
  prefs: []
  type: TYPE_NORMAL
- en: MLlib provides `StandardScaler`, a component that can perform this kind of standardization
    and be easily added to the clustering pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can run the same test with normalized data on a higher range of *k*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: This has helped put dimensions on more equal footing, and the absolute distances
    between points (and thus the cost) is much smaller in absolute terms. However,
    the above output doesn’t yet provide an obvious value of *k* beyond which increasing
    it does little to improve the cost.
  prefs: []
  type: TYPE_NORMAL
- en: Another 3-D visualization of the normalized data points reveals a richer structure,
    as expected. Some points are spaced in regular, discrete intervals in one direction;
    these are likely projections of discrete dimensions in the data, like counts.
    With 100 clusters, it’s hard to make out which points come from which clusters.
    One large cluster seems to dominate, and many clusters correspond to small, compact
    subregions (some of which are omitted from this zoomed detail of the entire 3-D
    visualization). The result, shown in [Figure 5-2](#AnomalyDetection_Projection2),
    does not necessarily advance the analysis but is an interesting sanity check.
  prefs: []
  type: TYPE_NORMAL
- en: '![aaps 0502](assets/aaps_0502.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-2\. Random 3-D projection, normalized
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Categorical Variables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Normalization was a valuable step forward, but more can be done to improve the
    clustering. In particular, several features have been left out entirely because
    they aren’t numeric. This is throwing away valuable information. Adding them back,
    in some form, should produce a better-informed clustering.
  prefs: []
  type: TYPE_NORMAL
- en: Earlier, three categorical features were excluded because nonnumeric features
    can’t be used with the Euclidean distance function that K-means uses in MLlib.
    This is the reverse of the issue noted in [“Random Forests”](ch04.xhtml#RandomDecisionForests),
    where numeric features were used to represent categorical values but a categorical
    feature was desired.
  prefs: []
  type: TYPE_NORMAL
- en: 'The categorical features can be translated into several binary indicator features
    using one-hot encoding, which can be viewed as numeric dimensions. For example,
    the second column contains the protocol type: `tcp`, `udp`, or `icmp`. This feature
    could be thought of as *three* features, as if features “is TCP,” “is UDP,” and
    “is ICMP” were in the dataset. The single feature value `tcp` might become `1,0,0`;
    `udp` might be `0,1,0`; and so on.'
  prefs: []
  type: TYPE_NORMAL
- en: Here again, MLlib provides components that implement this transformation. In
    fact, one-hot encoding string-valued features like `protocol_type` are actually
    a two-step process. First, the string values are converted to integer indices
    like 0, 1, 2, and so on using `StringIndexer`. Then, these integer indices are
    encoded into a vector with `OneHotEncoder`. These two steps can be thought of
    as a small `Pipeline` in themselves.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_anomaly_detection__span_class__keep_together__with_k_means_clustering__span__CO6-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Return pipeline and name of output vector column
  prefs: []
  type: TYPE_NORMAL
- en: This method produces a `Pipeline` that can be added as a component in the overall
    clustering pipeline; pipelines can be composed. All that is left is to make sure
    to add the new vector output columns into `VectorAssembler`’s output and proceed
    as before with scaling, clustering, and evaluation. The source code is omitted
    for brevity here, but can be found in the repository accompanying this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: These sample results suggest, possibly, *k*=180 as a value where the score flattens
    out a bit. At least the clustering is now using all input features.
  prefs: []
  type: TYPE_NORMAL
- en: Using Labels with Entropy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Earlier, we used the given label for each data point to create a quick sanity
    check of the quality of the clustering. This notion can be formalized further
    and used as an alternative means of evaluating clustering quality and, therefore,
    of choosing *k*.
  prefs: []
  type: TYPE_NORMAL
- en: The labels tell us something about the true nature of each data point. A good
    clustering, it seems, should agree with these human-applied labels. It should
    put together points that share a label frequently and not lump together points
    of many different labels. It should produce clusters with relatively homogeneous
    labels.
  prefs: []
  type: TYPE_NORMAL
- en: 'You may recall from [“Random Forests”](ch04.xhtml#RandomDecisionForests) that
    we have metrics for homogeneity: Gini impurity and entropy. These are functions
    of the proportions of labels in each cluster and produce a number that is low
    when the proportions are skewed toward few, or one, label. Entropy will be used
    here for illustration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'A good clustering would have clusters whose collections of labels are homogeneous
    and so have low entropy. A weighted average of entropy can therefore be used as
    a cluster score:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_anomaly_detection__span_class__keep_together__with_k_means_clustering__span__CO7-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Predict cluster for each datum.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_anomaly_detection__span_class__keep_together__with_k_means_clustering__span__CO7-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Count labels, per cluster
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_anomaly_detection__span_class__keep_together__with_k_means_clustering__span__CO7-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Average entropy weighted by cluster size.
  prefs: []
  type: TYPE_NORMAL
- en: 'As before, this analysis can be used to obtain some idea of a suitable value
    of *k*. Entropy will not necessarily decrease as *k* increases, so it is possible
    to look for a local minimum value. Here again, results suggest *k*=180 is a reasonable
    choice because its score is actually lower than 150 and 210:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Clustering in Action
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Finally, with confidence, we can cluster the full, normalized dataset with
    *k*=180\. Again, we can print the labels for each cluster to get some sense of
    the resulting clustering. Clusters do seem to be dominated by one type of attack
    each and contain only a few types:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_anomaly_detection__span_class__keep_together__with_k_means_clustering__span__CO8-1)'
  prefs: []
  type: TYPE_NORMAL
- en: See accompanying source code for `fit_pipeline_4` definition.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we can make an actual anomaly detector. Anomaly detection amounts to measuring
    a new data point’s distance to its nearest centroid. If this distance exceeds
    some threshold, it is anomalous. This threshold might be chosen to be the distance
    of, say, the 100th-farthest data point from among known data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The final step can be to apply this threshold to all new data points as they
    arrive. For example, Spark Streaming can be used to apply this function to small
    batches of input data arriving from sources like Kafka or files in cloud storage.
    Data points exceeding the threshold might trigger an alert that sends an email
    or updates a database.
  prefs: []
  type: TYPE_NORMAL
- en: Where to Go from Here
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `KMeansModel` is, by itself, the essence of an anomaly detection system.
    The preceding code demonstrated how to apply it to data to detect anomalies. This
    same code could be used within [Spark Streaming](https://oreil.ly/UHHBR) to score
    new data as it arrives in near real time, and perhaps trigger an alert or review.
  prefs: []
  type: TYPE_NORMAL
- en: MLlib also includes a variation called `StreamingKMeans`, which can update a
    clustering incrementally as new data arrives in a `StreamingKMeansModel`. We could
    use this to continue to learn, approximately, how new data affects the clustering,
    and not just to assess new data against existing clusters. It can be integrated
    with Spark Streaming as well. However, it has not been updated for the new DataFrame-based
    APIs.
  prefs: []
  type: TYPE_NORMAL
- en: This model is only a simplistic one. For example, Euclidean distance is used
    in this example because it is the only distance function supported by Spark MLlib
    at this time. In the future, it may be possible to use distance functions that
    can better account for the distributions of and correlations between features,
    such as the [Mahalanobis distance](https://oreil.ly/PKG7A).
  prefs: []
  type: TYPE_NORMAL
- en: There are also more sophisticated [cluster-quality evaluation metrics](https://oreil.ly/9yE9P)
    that could be applied (even without labels) to pick *k*, such as the [Silhouette
    coefficient](https://oreil.ly/LMN1h). These tend to evaluate not just closeness
    of points within one cluster, but closeness of points to other clusters. Finally,
    different models could be applied instead of simple K-means clustering; for example,
    a [Gaussian mixture model](https://oreil.ly/KTgD6) or [DBSCAN](https://oreil.ly/xlshs)
    could capture more subtle relationships between data points and the cluster centers.
    Spark MLlib already implements [Gaussian mixture models](https://oreil.ly/LG84u);
    implementations of others may become available in Spark MLlib or other Spark-based
    libraries in the future.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, clustering isn’t just for anomaly detection. In fact, it’s more often
    associated with use cases where the actual clusters matter! For example, clustering
    can also be used to group customers according to their behaviors, preferences,
    and attributes. Each cluster, by itself, might represent a usefully distinguishable
    type of customer. This is a more data-driven way to segment customers rather than
    leaning on arbitrary, generic divisions like “age 20–34” and “female.”
  prefs: []
  type: TYPE_NORMAL
