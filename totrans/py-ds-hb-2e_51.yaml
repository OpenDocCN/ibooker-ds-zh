- en: 'Chapter 46\. In Depth: Manifold Learning'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter we saw how PCA can be used for dimensionality reduction,
    reducing the number of features of a dataset while maintaining the essential relationships
    between the points. While PCA is flexible, fast, and easily interpretable, it
    does not perform so well when there are *nonlinear* relationships within the data,
    some examples of which we will see shortly.
  prefs: []
  type: TYPE_NORMAL
- en: 'To address this deficiency, we can turn to *manifold learning algorithms*—a
    class of unsupervised estimators that seek to describe datasets as low-dimensional
    manifolds embedded in high-dimensional spaces. When you think of a manifold, I’d
    suggest imagining a sheet of paper: this is a two-dimensional object that lives
    in our familiar three-dimensional world.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the parlance of manifold learning, you can think of this sheet as a two-dimensional
    manifold embedded in three-dimensional space. Rotating, reorienting, or stretching
    the piece of paper in three-dimensional space doesn’t change its flat geometry:
    such operations are akin to linear embeddings. If you bend, curl, or crumple the
    paper, it is still a two-dimensional manifold, but the embedding into the three-dimensional
    space is no longer linear. Manifold learning algorithms seek to learn about the
    fundamental two-dimensional nature of the paper, even as it is contorted to fill
    the three-dimensional space.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here we will examine a number of manifold methods, going most deeply into a
    subset of these techniques: multidimensional scaling (MDS), locally linear embedding
    (LLE), and isometric mapping (Isomap).'
  prefs: []
  type: TYPE_NORMAL
- en: 'We begin with the standard imports:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Manifold Learning: “HELLO”'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To make these concepts more clear, let’s start by generating some two-dimensional
    data that we can use to define a manifold. Here is a function that will create
    data in the shape of the word “HELLO”:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Let’s call the function and visualize the resulting data ([Figure 46-1](#fig_0510-manifold-learning_files_in_output_6_0)).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The output is two dimensional, and consists of points drawn in the shape of
    the word “HELLO”. This data form will help us to see visually what these algorithms
    are doing.
  prefs: []
  type: TYPE_NORMAL
- en: '![output 6 0](assets/output_6_0.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 46-1\. Data for use with manifold learning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Multidimensional Scaling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Looking at data like this, we can see that the particular choices of *x* and
    *y* values of the dataset are not the most fundamental description of the data:
    we can scale, shrink, or rotate the data, and the “HELLO” will still be apparent.
    For example, if we use a rotation matrix to rotate the data, the *x* and *y* values
    change, but the data is still fundamentally the same (see [Figure 46-2](#fig_0510-manifold-learning_files_in_output_9_0)).'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![output 9 0](assets/output_9_0.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 46-2\. Rotated dataset
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'This confirms that the *x* and *y* values are not necessarily fundamental to
    the relationships in the data. What *is* fundamental, in this case, is the *distance*
    between each point within the dataset. A common way to represent this is to use
    a distance matrix: for <math alttext="upper N"><mi>N</mi></math> points, we construct
    an <math alttext="upper N times upper N"><mrow><mi>N</mi> <mo>×</mo> <mi>N</mi></mrow></math>
    array such that entry <math alttext="left-parenthesis i comma j right-parenthesis"><mrow><mo>(</mo>
    <mi>i</mi> <mo>,</mo> <mi>j</mi> <mo>)</mo></mrow></math> contains the distance
    between point <math alttext="i"><mi>i</mi></math> and point <math alttext="j"><mi>j</mi></math>
    . Let’s use Scikit-Learn’s efficient `pairwise_distances` function to do this
    for our original data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: As promised, for our *N*=1,000 points, we obtain a 1,000 × 1,000 matrix, which
    can be visualized as shown here (see [Figure 46-3](#fig_0510-manifold-learning_files_in_output_13_0)).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![output 13 0](assets/output_13_0.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 46-3\. Visualization of the pairwise distances between points
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'If we similarly construct a distance matrix for our rotated and translated
    data, we see that it is the same:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'This distance matrix gives us a representation of our data that is invariant
    to rotations and translations, but the visualization of the matrix in [Figure 46-3](#fig_0510-manifold-learning_files_in_output_13_0)
    is not entirely intuitive. In the representation shown there, we have lost any
    visible sign of the interesting structure in the data: the “HELLO” that we saw
    before.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Further, while computing this distance matrix from the (*x*, *y*) coordinates
    is straightforward, transforming the distances back into *x* and *y* coordinates
    is rather difficult. This is exactly what the multidimensional scaling algorithm
    aims to do: given a distance matrix between points, it recovers a <math alttext="upper
    D"><mi>D</mi></math> -dimensional coordinate representation of the data. Let’s
    see how it works for our distance matrix, using the `precomputed` dissimilarity
    to specify that we are passing a distance matrix ([Figure 46-4](#fig_0510-manifold-learning_files_in_output_17_0)).'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![output 17 0](assets/output_17_0.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 46-4\. An MDS embedding computed from the pairwise distances
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The MDS algorithm recovers one of the possible two-dimensional coordinate representations
    of our data, using *only* the <math alttext="upper N times upper N"><mrow><mi>N</mi>
    <mo>×</mo> <mi>N</mi></mrow></math> distance matrix describing the relationship
    between the data points.
  prefs: []
  type: TYPE_NORMAL
- en: MDS as Manifold Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The usefulness of this becomes more apparent when we consider the fact that
    distance matrices can be computed from data in *any* dimension. So, for example,
    instead of simply rotating the data in the two-dimensional plane, we can project
    it into three dimensions using the following function (essentially a three-dimensional
    generalization of the rotation matrix used earlier):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Let’s visualize these points to see what we’re working with ([Figure 46-5](#fig_0510-manifold-learning_files_in_output_22_0)).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '![output 22 0](assets/output_22_0.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 46-5\. Data embedded linearly into three dimensions
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We can now ask the `MDS` estimator to input this three-dimensional data, compute
    the distance matrix, and then determine the optimal two-dimensional embedding
    for this distance matrix. The result recovers a representation of the original
    data, as shown in [Figure 46-6](#fig_0510-manifold-learning_files_in_output_24_0).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'This is essentially the goal of a manifold learning estimator: given high-dimensional
    embedded data, it seeks a low-dimensional representation of the data that preserves
    certain relationships within the data. In the case of MDS, the quantity preserved
    is the distance between every pair of points.'
  prefs: []
  type: TYPE_NORMAL
- en: '![output 24 0](assets/output_24_0.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 46-6\. The MDS embedding of the three-dimensional data recovers the input
    up to a rotation and reflection
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Nonlinear Embeddings: Where MDS Fails'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Our discussion thus far has considered *linear* embeddings, which essentially
    consist of rotations, translations, and scalings of data into higher-dimensional
    spaces. Where MDS breaks down is when the embedding is nonlinear—that is, when
    it goes beyond this simple set of operations. Consider the following embedding,
    which takes the input and contorts it into an “S” shape in three dimensions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: This is again three-dimensional data, but as we can see in [Figure 46-7](#fig_0510-manifold-learning_files_in_output_29_0)
    the embedding is much more complicated.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '![output 29 0](assets/output_29_0.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 46-7\. Data embedded nonlinearly into three dimensions
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The fundamental relationships between the data points are still there, but
    this time the data has been transformed in a nonlinear way: it has been wrapped
    up into the shape of an “S.”'
  prefs: []
  type: TYPE_NORMAL
- en: If we try a simple MDS algorithm on this data, it is not able to “unwrap” this
    nonlinear embedding, and we lose track of the fundamental relationships in the
    embedded manifold (see [Figure 46-8](#fig_0510-manifold-learning_files_in_output_31_0)).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '![output 31 0](assets/output_31_0.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 46-8\. The MDS algorithm applied to the nonlinear data; it fails to recover
    the underlying structure
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The best two-dimensional *linear* embedding does not unwrap the S-curve, but
    instead discards the original y-axis.
  prefs: []
  type: TYPE_NORMAL
- en: 'Nonlinear Manifolds: Locally Linear Embedding'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: How can we move forward here? Stepping back, we can see that the source of the
    problem is that MDS tries to preserve distances between faraway points when constructing
    the embedding. But what if we instead modified the algorithm such that it only
    preserves distances between nearby points? The resulting embedding would be closer
    to what we want.
  prefs: []
  type: TYPE_NORMAL
- en: Visually, we can think of it as illustrated [Figure 46-9](#fig_images_in_0510-lle-vs-mds).
  prefs: []
  type: TYPE_NORMAL
- en: 'Here each faint line represents a distance that should be preserved in the
    embedding. On the left is a representation of the model used by MDS: it tries
    to preserve the distances between each pair of points in the dataset. On the right
    is a representation of the model used by a manifold learning algorithm called
    *locally linear embedding*: rather than preserving *all* distances, it instead
    tries to preserve only the distances between *neighboring points* (in this case,
    the nearest 100 neighbors of each point).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Thinking about the left panel, we can see why MDS fails: there is no way to
    unroll this data while adequately preserving the length of every line drawn between
    the two points. For the right panel, on the other hand, things look a bit more
    optimistic. We could imagine unrolling the data in a way that keeps the lengths
    of the lines approximately the same. This is precisely what LLE does, through
    a global optimization of a cost function reflecting this logic.'
  prefs: []
  type: TYPE_NORMAL
- en: '![05.10 LLE vs MDS](assets/05.10-LLE-vs-MDS.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 46-9\. Representation of linkages between points within MDS and LLE^([1](ch46.xhtml#idm45858725565312))
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: LLE comes in a number of flavors; here we will use the *modified LLE* algorithm
    to recover the embedded two-dimensional manifold. In general, modified LLE does
    better than other flavors of the algorithm at recovering well-defined manifolds
    with very little distortion (see [Figure 46-10](#fig_0510-manifold-learning_files_in_output_36_0)).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The result remains somewhat distorted compared to our original manifold, but
    captures the essential relationships in the data!
  prefs: []
  type: TYPE_NORMAL
- en: '![output 36 0](assets/output_36_0.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 46-10\. Locally linear embedding can recover the underlying data from
    a nonli‐ nearly embedded input
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Some Thoughts on Manifold Methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Compelling as these examples may be, in practice manifold learning techniques
    tend to be finicky enough that they are rarely used for anything more than simple
    qualitative visualization of high-dimensional data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are some of the particular challenges of manifold learning, which
    all contrast poorly with PCA:'
  prefs: []
  type: TYPE_NORMAL
- en: In manifold learning, there is no good framework for handling missing data.
    In contrast, there are straightforward iterative approaches for dealing with missing
    data in PCA.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In manifold learning, the presence of noise in the data can “short-circuit”
    the manifold and drastically change the embedding. In contrast, PCA naturally
    filters noise from the most important components.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The manifold embedding result is generally highly dependent on the number of
    neighbors chosen, and there is generally no solid quantitative way to choose an
    optimal number of neighbors. In contrast, PCA does not involve such a choice.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In manifold learning, the globally optimal number of output dimensions is difficult
    to determine. In contrast, PCA lets you find the number of output dimensions based
    on the explained variance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In manifold learning, the meaning of the embedded dimensions is not always clear.
    In PCA, the principal components have a very clear meaning.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In manifold learning, the computational expense of manifold methods scales as
    <math alttext="upper O left-bracket upper N squared right-bracket"><mrow><mi>O</mi>
    <mo>[</mo> <msup><mi>N</mi> <mn>2</mn></msup> <mo>]</mo></mrow></math> or <math
    alttext="upper O left-bracket upper N cubed right-bracket"><mrow><mi>O</mi> <mo>[</mo>
    <msup><mi>N</mi> <mn>3</mn></msup> <mo>]</mo></mrow></math> . For PCA, there exist
    randomized approaches that are generally much faster (though see the [*megaman*
    package](https://oreil.ly/VLBly) for some more scalable implementations of manifold
    learning).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With all that on the table, the only clear advantage of manifold learning methods
    over PCA is their ability to preserve nonlinear relationships in the data; for
    that reason I tend to explore data with manifold methods only after first exploring
    it with PCA.
  prefs: []
  type: TYPE_NORMAL
- en: 'Scikit-Learn implements several common variants of manifold learning beyond
    LLE and Isomap (which we’ve used in a few of the previous chapters and will look
    at in the next section): the Scikit-Learn documentation has a [nice discussion
    and comparison of them](https://oreil.ly/tFzS5). Based on my own experience, I
    would give the following recommendations:'
  prefs: []
  type: TYPE_NORMAL
- en: For toy problems such as the S-curve we saw before, LLE and its variants (especially
    modified LLE) perform very well. This is implemented in `sklearn.manifold.LocallyLinearEmbedding`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For high-dimensional data from real-world sources, LLE often produces poor results,
    and Isomap seems to generally lead to more meaningful embeddings. This is implemented
    in `sklearn.manifold.Isomap`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For data that is highly clustered, *t-distributed stochastic neighbor embedding*
    (t-SNE) seems to work very well, though it can be very slow compared to other
    methods. This is implemented in `sklearn.manifold.TSNE`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you’re interested in getting a feel for how these work, I’d suggest running
    each of the methods on the data in this section.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: Isomap on Faces'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One place manifold learning is often used is in understanding the relationship
    between high-dimensional data points. A common case of high-dimensional data is
    images: for example, a set of images with 1,000 pixels each can be thought of
    as a collection of points in 1,000 dimensions, with the brightness of each pixel
    in each image defining the coordinate in that dimension.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To illustrate, let’s apply Isomap on some data from the Labeled Faces in the
    Wild dataset, which we previously saw in Chapters [43](ch43.xhtml#section-0507-support-vector-machines)
    and [45](ch45.xhtml#section-0509-principal-component-analysis). Running this command
    will download the dataset and cache it in your home directory for later use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: We have 2,370 images, each with 2,914 pixels. In other words, the images can
    be thought of as data points in a 2,914-dimensional space!
  prefs: []
  type: TYPE_NORMAL
- en: Let’s display several of these images to remind us what we’re working with (see
    [Figure 46-11](#fig_0510-manifold-learning_files_in_output_43_0)).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '![output 43 0](assets/output_43_0.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 46-11\. Examples of the input faces
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'When we encountered this data in [Chapter 45](ch45.xhtml#section-0509-principal-component-analysis),
    our goal was essentially compression: to use the components to reconstruct the
    inputs from the lower-dimensional representation.'
  prefs: []
  type: TYPE_NORMAL
- en: PCA is versatile enough that we can also use it in this context, where we would
    like to plot a low-dimensional embedding of the 2,914-dimensional data to learn
    the fundamental relationships between the images. Let’s again look at the explained
    variance ratio, which will give us an idea of how many linear features are required
    to describe the data (see [Figure 46-12](#fig_0510-manifold-learning_files_in_output_45_0)).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '![output 45 0](assets/output_45_0.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 46-12\. Cumulative variance from the PCA projection
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We see that for this data, nearly 100 components are required to preserve 90%
    of the variance. This tells us that the data is intrinsically very high-dimensional—it
    can’t be described linearly with just a few components.
  prefs: []
  type: TYPE_NORMAL
- en: 'When this is the case, nonlinear manifold embeddings like LLE and Isomap may
    be helpful. We can compute an Isomap embedding on these faces using the same pattern
    shown before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is a two-dimensional projection of all the input images. To get
    a better idea of what the projection tells us, let’s define a function that will
    output image thumbnails at the locations of the projections:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Calling this function now, we see the result in [Figure 46-13](#fig_0510-manifold-learning_files_in_output_51_0).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '![output 51 0](assets/output_51_0.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 46-13\. Isomap embedding of the LFW data
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The result is interesting. The first two Isomap dimensions seem to describe
    global image features: the overall brightness of the image from left to right,
    and the general orientation of the face from bottom to top. This gives us a nice
    visual indication of some of the fundamental features in our data.'
  prefs: []
  type: TYPE_NORMAL
- en: From here, we could then go on to classify this data (perhaps using manifold
    features as inputs to the classification algorithm) as we did in [Chapter 43](ch43.xhtml#section-0507-support-vector-machines).
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: Visualizing Structure in Digits'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As another example of using manifold learning for visualization, let’s take
    a look at the MNIST handwritten digits dataset. This is similar to the digits
    dataset we saw in [Chapter 44](ch44.xhtml#section-0508-random-forests), but with
    many more pixels per image. It can be downloaded from [*http://openml.org*](http://openml.org)
    with the Scikit-Learn utility:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The dataset consists of 70,000 images, each with 784 pixels (i.e., the images
    are 28 × 28). As before, we can take a look at the first few images (see [Figure 46-14](#fig_0510-manifold-learning_files_in_output_56_0)).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '![output 56 0](assets/output_56_0.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 46-14\. Examples of MNIST digits
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This gives us an idea of the variety of handwriting styles in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s compute a manifold learning projection across the data. For speed here,
    we’ll only use 1/30 of the data, which is about ~2,000 points (because of the
    relatively poor scaling of manifold learning, I find that a few thousand samples
    is a good number to start with for relatively quick exploration before moving
    to a full calculation). [Figure 46-15](#fig_0510-manifold-learning_files_in_output_58_0)
    shows the result.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '![output 58 0](assets/output_58_0.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 46-15\. Isomap embedding of the MNIST digit data
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The resulting scatter plot shows some of the relationships between the data
    points, but is a bit crowded. We can gain more insight by looking at just a single
    number at a time (see [Figure 46-16](#fig_0510-manifold-learning_files_in_output_60_0)).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '![output 60 0](assets/output_60_0.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 46-16\. Isomap embedding of only the 1s within the MNIST dataset
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The result gives you an idea of the variety of forms that the number 1 can
    take within the dataset. The data lies along a broad curve in the projected space,
    which appears to trace the orientation of the digit. As you move up the plot,
    you find 1s that have hats and/or bases, though these are very sparse within the
    dataset. The projection lets us identify outliers that have data issues: for example,
    pieces of the neighboring digits that snuck into the extracted images.'
  prefs: []
  type: TYPE_NORMAL
- en: Now, this in itself may not be useful for the task of classifying digits, but
    it does help us get an understanding of the data, and may give us ideas about
    how to move forward—such as how we might want to preprocess the data before building
    a classification pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch46.xhtml#idm45858725565312-marker)) Code to produce this figure can
    be found in the [online appendix](https://oreil.ly/gu4iE).
  prefs: []
  type: TYPE_NORMAL
