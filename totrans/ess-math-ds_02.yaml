- en: Chapter 2\. Probability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When you think of probability, what images come to mind? Perhaps you think of
    gambling-related examples, like the probability of winning the lottery or getting
    a pair with two dice. Maybe it is predicting stock performance, the outcome of
    a political election, or whether your flight will arrive on time. Our world is
    full of uncertainties we want to measure.
  prefs: []
  type: TYPE_NORMAL
- en: 'Maybe that is the word we should focus on: uncertainty. How do we measure something
    that we are uncertain about?'
  prefs: []
  type: TYPE_NORMAL
- en: In the end, probability is the theoretical study of measuring certainty that
    an event will happen. It is a foundational discipline for statistics, hypothesis
    testing, machine learning, and other topics in this book. A lot of folks take
    probability for granted and assume they understand it. However, it is more nuanced
    and complicated than most people think. While the theorems and ideas of probability
    are mathematically sound, it gets more complex when we introduce data and venture
    into statistics. We will cover that in [Chapter 4](ch04.xhtml#ch04) on statistics
    and hypothesis testing.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will discuss what probability is. Then we will cover probability
    math concepts, Bayes’ Theorem, the binomial distribution, and the beta distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Probability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Probability* is how strongly we believe an event will happen, often expressed
    as a percentage. Here are some questions that might warrant a probability for
    an answer:'
  prefs: []
  type: TYPE_NORMAL
- en: How likely will I get 7 heads in 10 fair coin flips?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What are my chances of winning an election?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Will my flight be late?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How certain am I that a product is defective?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The most popular way to express probability is as a percentage, as in “There
    is a 70% chance my flight will be late.” We will call this probability <math alttext="upper
    P left-parenthesis upper X right-parenthesis"><mrow><mi>P</mi> <mo>(</mo> <mi>X</mi>
    <mo>)</mo></mrow></math> , where *X* is the event of interest. As you work with
    probabilities, though, you will more likely see it expressed as a decimal (in
    this case .70), which must be between 0.0 and 1.0:'
  prefs: []
  type: TYPE_NORMAL
- en: <math><mrow><mi>P</mi> <mo>(</mo> <mi>X</mi> <mo>)</mo> <mo>=</mo> <mn>.70</mn></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: '*Likelihood* is similar to probability, and it is easy to confuse the two (many
    dictionaries do as well). You can get away with using “probability” and “likelihood”
    interchangeably in everyday conversation. However, we should pin down these differences.
    Probability is about quantifying predictions of events yet to happen, whereas
    likelihood is measuring the frequency of events that already occurred. In statistics
    and machine learning, we often use likelihood (the past) in the form of data to
    predict probability (the future).'
  prefs: []
  type: TYPE_NORMAL
- en: 'It is important to note that a probability of an event happening must be strictly
    between 0% and 100%, or 0.0 and 1.0\. Logically, this means the probability of
    an event *not* happening is calculated by subtracting the probability of the event
    from 1.0:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mi>P</mi>
    <mo>(</mo> <mi>X</mi> <mo>)</mo> <mo>=</mo> <mn>.70</mn></mrow></mtd></mtr></mtable></math>
    <math display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mi>P</mi>
    <mo>(</mo> <mtext>not</mtext> <mi>X</mi> <mo>)</mo> <mo>=</mo> <mn>1</mn> <mo>-</mo>
    <mn>.70</mn> <mo>=</mo> <mn>.30</mn></mrow></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: This is another distinction between probability and likelihood. Probabilities
    of all possible mutually exclusive outcomes for an event (meaning only one outcome
    can occur, not multiple) must sum to 1.0 or 100%. Likelihoods, however, are not
    subject to this rule.
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, probability can be expressed as an *odds* <math alttext="upper
    O left-parenthesis upper X right-parenthesis"><mrow><mi>O</mi> <mo>(</mo> <mi>X</mi>
    <mo>)</mo></mrow></math> such as 7:3, 7/3, or <math display="inline"><mrow><mn>2.</mn>
    <mover><mn>333</mn> <mo>¯</mo></mover></mrow></math> .
  prefs: []
  type: TYPE_NORMAL
- en: 'To turn an odds <math alttext="upper O left-parenthesis upper X right-parenthesis"><mrow><mi>O</mi>
    <mo>(</mo> <mi>X</mi> <mo>)</mo></mrow></math> into a proportional probability
    <math alttext="upper P left-parenthesis upper X right-parenthesis"><mrow><mi>P</mi>
    <mo>(</mo> <mi>X</mi> <mo>)</mo></mrow></math> , use this formula:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="upper P left-parenthesis upper X right-parenthesis equals StartFraction
    upper O left-parenthesis upper X right-parenthesis Over 1 plus upper O left-parenthesis
    upper X right-parenthesis EndFraction" display="block"><mrow><mi>P</mi> <mrow><mo>(</mo>
    <mi>X</mi> <mo>)</mo></mrow> <mo>=</mo> <mfrac><mrow><mi>O</mi><mo>(</mo><mi>X</mi><mo>)</mo></mrow>
    <mrow><mn>1</mn><mo>+</mo><mi>O</mi><mo>(</mo><mi>X</mi><mo>)</mo></mrow></mfrac></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'So if I have an odds 7/3, I can convert it into a proportional probability
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mi>P</mi>
    <mrow><mo>(</mo> <mi>X</mi> <mo>)</mo></mrow> <mo>=</mo> <mfrac><mrow><mi>O</mi><mo>(</mo><mi>X</mi><mo>)</mo></mrow>
    <mrow><mn>1</mn><mo>+</mo><mi>O</mi><mo>(</mo><mi>X</mi><mo>)</mo></mrow></mfrac></mrow></mtd></mtr></mtable></math>
    <math display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mi>P</mi>
    <mrow><mo>(</mo> <mi>X</mi> <mo>)</mo></mrow> <mo>=</mo> <mfrac><mfrac><mn>7</mn>
    <mn>3</mn></mfrac> <mrow><mn>1</mn><mo>+</mo><mfrac><mn>7</mn> <mn>3</mn></mfrac></mrow></mfrac></mrow></mtd></mtr></mtable></math>
    <math display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mi>P</mi>
    <mo>(</mo> <mi>X</mi> <mo>)</mo> <mo>=</mo> <mn>.7</mn></mrow></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Conversely, you can turn an odds into a probability simply by dividing the
    probability of the event occurring by the probability it will not occur:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mi>O</mi>
    <mrow><mo>(</mo> <mi>X</mi> <mo>)</mo></mrow> <mo>=</mo> <mfrac><mrow><mi>P</mi><mo>(</mo><mi>X</mi><mo>)</mo></mrow>
    <mrow><mn>1</mn><mo>-</mo><mi>P</mi><mo>(</mo><mi>X</mi><mo>)</mo></mrow></mfrac></mrow></mtd></mtr></mtable></math>
    <math display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mi>O</mi>
    <mrow><mo>(</mo> <mi>X</mi> <mo>)</mo></mrow> <mo>=</mo> <mfrac><mrow><mn>.70</mn></mrow>
    <mrow><mn>1</mn><mo>-</mo><mn>.70</mn></mrow></mfrac></mrow></mtd></mtr></mtable></math>
    <math display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mi>O</mi>
    <mrow><mo>(</mo> <mi>X</mi> <mo>)</mo></mrow> <mo>=</mo> <mfrac><mn>7</mn> <mn>3</mn></mfrac></mrow></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: Probability Versus Statistics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Sometimes people use the terms *probability* and *statistics* interchangeably,
    and while it is understandable to conflate the two disciplines, they do have distinctions.
    *Probability* is purely theoretical of how likely an event is to happen and does
    not require data. *Statistics*, on the other hand, cannot exist without data and
    uses it to discover probability and provides tools to describe data.
  prefs: []
  type: TYPE_NORMAL
- en: Think of predicting the outcome of rolling a 4 on a die (that’s the singular
    of dice). Approaching the problem with a pure probability mindset, one simply
    says there are six sides on a die. We assume each side is equally likely, so the
    probability of getting a 4 is 1/6, or 16.666%.
  prefs: []
  type: TYPE_NORMAL
- en: However, a zealous statistician might say, “No! We need to roll the die to get
    data. If we can get 30 rolls or more, and the more rolls we do the better, only
    then will we have data to determine the probability of getting a 4.” This approach
    may seem silly if we assume the die is fair, but what if it’s not? If that’s the
    case, collecting data is the only way to discover the probability of rolling a
    4\. We will talk about hypothesis testing in [Chapter 3](ch03.xhtml#ch03).
  prefs: []
  type: TYPE_NORMAL
- en: Probability Math
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When we work with a single probability of an event <math alttext="upper P left-parenthesis
    upper X right-parenthesis"><mrow><mi>P</mi> <mo>(</mo> <mi>X</mi> <mo>)</mo></mrow></math>
    , known as a *marginal probability*, the idea is fairly straightforward, as discussed
    previously. But when we start combining probabilities of different events, it
    gets a little less intuitive.
  prefs: []
  type: TYPE_NORMAL
- en: Joint Probabilities
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s say you have a fair coin and a fair six-sided die. You want to find the
    probability of flipping a heads and rolling a 6 on the coin and die, respectively.
    These are two separate probabilities of two separate events, but we want to find
    the probability that both events will occur together. This is known as a *joint
    probability*.
  prefs: []
  type: TYPE_NORMAL
- en: Think of a joint probability as an AND operator. I want to find the probability
    of flipping a heads AND rolling a 6\. We want both events to happen together,
    so how do we calculate this probability?
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two sides on a coin and six sides on the die, so the probability
    of heads is 1/2 and the probability of six is 1/6\. The probability of both events
    occurring (assuming they are independent, more on this later!) is simply multiplying
    the two together:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mi>P</mi>
    <mo>(</mo> <mi>A</mi> <mtext>AND</mtext> <mi>B</mi> <mo>)</mo> <mo>=</mo> <mi>P</mi>
    <mo>(</mo> <mi>A</mi> <mo>)</mo> <mo>×</mo> <mi>P</mi> <mo>(</mo> <mi>B</mi> <mo>)</mo></mrow></mtd></mtr></mtable></math>
    <math display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mi>P</mi>
    <mrow><mo>(</mo> <mtext>heads</mtext> <mo>)</mo></mrow> <mo>=</mo> <mfrac><mn>1</mn>
    <mn>2</mn></mfrac></mrow></mtd></mtr></mtable></math> <math display="block"><mtable
    displaystyle="true"><mtr><mtd columnalign="right"><mrow><mi>P</mi> <mrow><mo>(</mo>
    <mn>6</mn> <mo>)</mo></mrow> <mo>=</mo> <mfrac><mn>1</mn> <mn>6</mn></mfrac></mrow></mtd></mtr></mtable></math>
    <math display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mi>P</mi>
    <mrow><mo>(</mo> <mtext>heads</mtext> <mtext>AND</mtext> <mtext>6</mtext> <mo>)</mo></mrow>
    <mo>=</mo> <mfrac><mn>1</mn> <mn>2</mn></mfrac> <mo>×</mo> <mfrac><mn>1</mn> <mn>6</mn></mfrac>
    <mo>=</mo> <mfrac><mn>1</mn> <mn>12</mn></mfrac> <mo>=</mo> <mn>.08</mn> <mover><mn>333</mn>
    <mo>¯</mo></mover></mrow></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Easy enough, but why is this the case? A lot of probability rules can be discovered
    by generating all possible combinations of events, which comes from an area of
    discrete math known as permutations and combinations. For this case, generate
    every possible outcome between the coin and die, pairing heads (H) and tails (T)
    with the numbers 1 through 6\. Note I put asterisks “*” around the outcome of
    interest where we get heads and a 6:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Notice there are 12 possible outcomes when flipping our coin and rolling our
    die. The only one that is of interest to us is “H6,” getting a heads and a 6\.
    So because there is only one outcome that satisfies our condition, and there are
    12 possible outcomes, the probability of getting a heads and a 6 is 1/12.
  prefs: []
  type: TYPE_NORMAL
- en: 'Rather than generate all possible combinations and counting the ones of interest
    to us, we can again use the multiplication as a shortcut to find the joint probability.
    This is known as the *product rule*:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mi>P</mi>
    <mo>(</mo> <mi>A</mi> <mtext>AND</mtext> <mi>B</mi> <mo>)</mo> <mo>=</mo> <mi>P</mi>
    <mo>(</mo> <mi>A</mi> <mo>)</mo> <mo>×</mo> <mi>P</mi> <mo>(</mo> <mi>B</mi> <mo>)</mo></mrow></mtd></mtr></mtable></math>
    <math display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mi>P</mi>
    <mrow><mo>(</mo> <mtext>heads</mtext> <mtext>AND</mtext> <mtext>6</mtext> <mo>)</mo></mrow>
    <mo>=</mo> <mfrac><mn>1</mn> <mn>2</mn></mfrac> <mo>×</mo> <mfrac><mn>1</mn> <mn>6</mn></mfrac>
    <mo>=</mo> <mfrac><mn>1</mn> <mn>12</mn></mfrac> <mo>=</mo> <mn>.08</mn> <mover><mn>333</mn>
    <mo>¯</mo></mover></mrow></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: Union Probabilities
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We discussed joint probabilities, which is the probability of two or more events
    occurring simultaneously. But what about the probability of getting event A or
    B? When we deal with OR operations with probabilities, this is known as a *union
    probability*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start with *mutually exclusive* events, which are events that cannot
    occur simultaneously. For example, if I roll one die I cannot simultaneously get
    a 4 and a 6\. I can only get one outcome. Getting the union probability for these
    cases is easy. I simply add them together. If I want to find the probability of
    getting a 4 or 6 on a die roll, it is going to be 2/6 = 1/3:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mi>P</mi>
    <mrow><mo>(</mo> <mn>4</mn> <mo>)</mo></mrow> <mo>=</mo> <mfrac><mn>1</mn> <mn>6</mn></mfrac></mrow></mtd></mtr></mtable></math>
    <math display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mi>P</mi>
    <mrow><mo>(</mo> <mn>6</mn> <mo>)</mo></mrow> <mo>=</mo> <mfrac><mn>1</mn> <mn>6</mn></mfrac></mrow></mtd></mtr></mtable></math>
    <math display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mi>P</mi>
    <mrow><mo>(</mo> <mtext>4</mtext> <mtext>OR</mtext> <mtext>6</mtext> <mo>)</mo></mrow>
    <mo>=</mo> <mfrac><mn>1</mn> <mn>6</mn></mfrac> <mo>+</mo> <mfrac><mn>1</mn> <mn>6</mn></mfrac>
    <mo>=</mo> <mfrac><mn>1</mn> <mn>3</mn></mfrac></mrow></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'But what about *nonmutually exclusive* events, which are events that can occur
    simultaneously? Let’s go back to the coin flip and die roll example. What is the
    probability of getting a heads OR a 6? Before you are tempted to add those probabilities,
    let’s generate all possible outcomes again and highlight the ones we are interested
    in:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Here we are interested in all the heads outcomes as well as the 6 outcomes.
    If we proportion the 7 out of 12 outcomes we are interested in, 7/12, we get a
    correct probability of <math alttext=".58 ModifyingAbove 333 With bar"><mrow><mo>.</mo>
    <mn>58</mn> <mover><mn>333</mn> <mo>¯</mo></mover></mrow></math> .
  prefs: []
  type: TYPE_NORMAL
- en: 'But what happens if we add the probabilities of heads and 6 together? We get
    a different (and wrong!) answer of <math alttext="period ModifyingAbove 666 With
    bar"><mrow><mo>.</mo> <mover><mn>666</mn> <mo>¯</mo></mover></mrow></math> :'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mi>P</mi> <mrow><mo>(</mo> <mi>h</mi> <mi>e</mi>
    <mi>a</mi> <mi>d</mi> <mi>s</mi> <mo>)</mo></mrow> <mo>=</mo> <mfrac><mn>1</mn>
    <mn>2</mn></mfrac></mrow></math> <math display="block"><mrow><mi>P</mi> <mrow><mo>(</mo>
    <mn>6</mn> <mo>)</mo></mrow> <mo>=</mo> <mfrac><mn>1</mn> <mn>6</mn></mfrac></mrow></math>
    <math display="block"><mrow><mi>P</mi> <mrow><mo>(</mo> <mtext>heads</mtext> <mtext>OR</mtext>
    <mtext>6</mtext> <mo>)</mo></mrow> <mo>=</mo> <mfrac><mn>1</mn> <mn>2</mn></mfrac>
    <mo>+</mo> <mfrac><mn>1</mn> <mn>6</mn></mfrac> <mo>=</mo> <mfrac><mn>4</mn> <mn>6</mn></mfrac>
    <mo>=</mo> <mover><mn>.666</mn> <mo>¯</mo></mover></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Why is that? Study the combinations of coin flip and die outcomes again and
    see if you can find something fishy. Notice when we add the probabilities, we
    double-count the probability of getting a 6 in both “H6” and “T6”! If this is
    not clear, try finding the probability of getting heads or a die roll of 1 through
    5:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mi>P</mi> <mrow><mo>(</mo> <mi>h</mi> <mi>e</mi>
    <mi>a</mi> <mi>d</mi> <mi>s</mi> <mo>)</mo></mrow> <mo>=</mo> <mfrac><mn>1</mn>
    <mn>2</mn></mfrac></mrow></math> <math display="block"><mrow><mi>P</mi> <mrow><mo>(</mo>
    <mtext>1</mtext> <mtext>through</mtext> <mtext>5</mtext> <mo>)</mo></mrow> <mo>=</mo>
    <mfrac><mn>5</mn> <mn>6</mn></mfrac></mrow></math> <math display="block"><mrow><mi>P</mi>
    <mrow><mo>(</mo> <mtext>heads</mtext> <mtext>OR</mtext> <mtext>1</mtext> <mtext>through</mtext>
    <mtext>5</mtext> <mo>)</mo></mrow> <mo>=</mo> <mfrac><mn>1</mn> <mn>2</mn></mfrac>
    <mo>+</mo> <mfrac><mn>5</mn> <mn>6</mn></mfrac> <mo>=</mo> <mfrac><mn>8</mn> <mn>6</mn></mfrac>
    <mo>=</mo> <mn>1.</mn> <mover><mn>333</mn> <mo>¯</mo></mover></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: We get a probability of 133.333%, which is definitely not correct because a
    probability must be no more than 100% or 1.0\. The problem again is we are double-counting
    outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you ponder long enough, you may realize the logical way to remove double-counting
    in a union probability is to subtract the joint probability. This is known as
    the *sum rule of probability* and ensures every joint event is counted only once:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mi>P</mi>
    <mo>(</mo> <mi>A</mi> <mtext>OR</mtext> <mi>B</mi> <mo>)</mo> <mo>=</mo> <mi>P</mi>
    <mo>(</mo> <mi>A</mi> <mo>)</mo> <mo>+</mo> <mi>P</mi> <mo>(</mo> <mi>B</mi> <mo>)</mo>
    <mo>-</mo> <mi>P</mi> <mo>(</mo> <mi>A</mi> <mtext>AND</mtext> <mi>B</mi> <mo>)</mo></mrow></mtd></mtr></mtable></math>
    <math display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mi>P</mi>
    <mo>(</mo> <mi>A</mi> <mtext>OR</mtext> <mi>B</mi> <mo>)</mo> <mo>=</mo> <mi>P</mi>
    <mo>(</mo> <mi>A</mi> <mo>)</mo> <mo>+</mo> <mi>P</mi> <mo>(</mo> <mi>B</mi> <mo>)</mo>
    <mo>-</mo> <mi>P</mi> <mo>(</mo> <mi>A</mi> <mo>)</mo> <mo>×</mo> <mi>P</mi> <mo>(</mo>
    <mi>B</mi> <mo>)</mo></mrow></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'So going back to our example of calculating the probability of a heads or a
    6, we need to subtract the joint probability of getting a heads or a 6 from the
    union probability:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mi>P</mi>
    <mrow><mo>(</mo> <mi>h</mi> <mi>e</mi> <mi>a</mi> <mi>d</mi> <mi>s</mi> <mo>)</mo></mrow>
    <mo>=</mo> <mfrac><mn>1</mn> <mn>2</mn></mfrac></mrow></mtd></mtr></mtable></math>
    <math display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mi>P</mi>
    <mrow><mo>(</mo> <mn>6</mn> <mo>)</mo></mrow> <mo>=</mo> <mfrac><mn>1</mn> <mn>6</mn></mfrac></mrow></mtd></mtr></mtable></math>
    <math display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mi>P</mi>
    <mo>(</mo> <mi>A</mi> <mtext>OR</mtext> <mi>B</mi> <mo>)</mo> <mo>=</mo> <mi>P</mi>
    <mo>(</mo> <mi>A</mi> <mo>)</mo> <mo>+</mo> <mi>P</mi> <mo>(</mo> <mi>B</mi> <mo>)</mo>
    <mo>-</mo> <mi>P</mi> <mo>(</mo> <mi>A</mi> <mo>)</mo> <mo>×</mo> <mi>P</mi> <mo>(</mo>
    <mi>B</mi> <mo>)</mo></mrow></mtd></mtr></mtable></math> <math display="block"><mtable
    displaystyle="true"><mtr><mtd columnalign="right"><mrow><mi>P</mi> <mrow><mo>(</mo>
    <mtext>heads</mtext> <mtext>OR</mtext> <mtext>6</mtext> <mo>)</mo></mrow> <mo>=</mo>
    <mfrac><mn>1</mn> <mn>2</mn></mfrac> <mo>+</mo> <mfrac><mn>1</mn> <mn>6</mn></mfrac>
    <mo>-</mo> <mrow><mo>(</mo> <mfrac><mn>1</mn> <mn>2</mn></mfrac> <mo>×</mo> <mfrac><mn>1</mn>
    <mn>6</mn></mfrac> <mo>)</mo></mrow> <mo>=</mo> <mn>.58</mn> <mover><mn>333</mn>
    <mo>¯</mo></mover></mrow></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: Note that this formula also applies to mutually exclusive events. If the events
    are mutually exclusive where only one outcome *A* or *B* is allowed but not both,
    then the joint probability *P*(*A* AND *B*) is going to be 0, and therefore remove
    itself from the formula. You are then left with simply summing the events as we
    did earlier.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, when you have a union probability between two or more events that
    are not mutually exclusive, be sure to subtract the joint probability so no probabilities
    are double-counted.
  prefs: []
  type: TYPE_NORMAL
- en: Conditional Probability and Bayes’ Theorem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A probability topic that easily confuses people is the concept of *conditional
    probability*, which is the probability of an event A occurring given event B has
    occurred. It is typically expressed as <math alttext="upper P left-parenthesis
    upper A GIVEN upper B right-parenthesis"><mrow><mi>P</mi> <mo>(</mo> <mtext>A</mtext>
    <mtext>GIVEN</mtext> <mtext>B</mtext> <mo>)</mo></mrow></math> or <math alttext="upper
    P left-parenthesis upper A vertical-bar upper B right-parenthesis"><mrow><mi>P</mi>
    <mo>(</mo> <mtext>A|B</mtext> <mo>)</mo></mrow></math> .
  prefs: []
  type: TYPE_NORMAL
- en: Let’s say a study makes a claim that 85% of cancer patients drank coffee. How
    do you react to this claim? Does this alarm you and make you want to abandon your
    favorite morning drink? Let’s first define this as a conditional probability <math
    alttext="upper P left-parenthesis Coffee given Cancer right-parenthesis"><mrow><mi>P</mi>
    <mo>(</mo> <mtext>Coffee</mtext> <mtext>given</mtext> <mtext>Cancer</mtext> <mo>)</mo></mrow></math>
    or <math alttext="upper P left-parenthesis Coffee vertical-bar Cancer right-parenthesis"><mrow><mi>P</mi>
    <mo>(</mo> <mtext>Coffee|Cancer</mtext> <mo>)</mo></mrow></math> . This represents
    a probability of people who drink coffee given they have cancer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Within the United States, let’s compare this to the percentage of people diagnosed
    with cancer (0.5% according to *cancer.gov*) and the percentage of people who
    drink coffee (65% according to *statista.com*):'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mi>P</mi>
    <mo>(</mo> <mtext>Coffee</mtext> <mo>)</mo> <mo>=</mo> <mn>.65</mn></mrow></mtd></mtr></mtable></math>
    <math display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mi>P</mi>
    <mo>(</mo> <mtext>Cancer</mtext> <mo>)</mo> <mo>=</mo> <mn>.005</mn></mrow></mtd></mtr></mtable></math>
    <math display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mi>P</mi>
    <mo>(</mo> <mtext>Coffee|Cancer</mtext> <mo>)</mo> <mo>=</mo> <mn>.85</mn></mrow></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: Hmmmm…study these numbers for a moment and ask whether coffee is really the
    problem here. Notice again that only 0.5% of the population has cancer at any
    given time. However 65% of the population drinks coffee regularly. If coffee contributes
    to cancer, should we not have much higher cancer numbers than 0.5%? Would it not
    be closer to 65%?
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the sneaky thing about proportional numbers. They may seem significant
    without any given context, and media headlines can certainly exploit this for
    clicks: “New Study Reveals 85% of Cancer Patients Drink Coffee” it might read.
    Of course, this is silly because we have taken a common attribute (drinking coffee)
    and associated it with an uncommon one (having cancer).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The reason people can be so easily confused by conditional probabilities is
    because the direction of the condition matters, and the two conditions are conflated
    as somehow being equal. The “probability of having cancer given you are a coffee
    drinker” is different from the “probability of being a coffee drinker given you
    have cancer.” To put it simply: few coffee drinkers have cancer, but many cancer
    patients drink coffee.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If we are interested in studying whether coffee contributes to cancer, we really
    are interested in the first conditional probability: the probability of someone
    having cancer given they are a coffee drinker.'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mi>P</mi>
    <mo>(</mo> <mtext>Coffee|Cancer</mtext> <mo>)</mo> <mo>=</mo> <mn>.85</mn></mrow></mtd></mtr></mtable></math>
    <math display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mi>P</mi>
    <mo>(</mo> <mtext>Cancer|Coffee</mtext> <mo>)</mo> <mo>=</mo> <mo>?</mo></mrow></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'How do we flip the condition? There’s a powerful little formula called *Bayes’
    Theorem*, and we can use it to flip conditional probabilities:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mi>P</mi> <mrow><mo>(</mo> <mi>A</mi><mtext>|</mtext><mi>B</mi>
    <mo>)</mo></mrow> <mo>=</mo> <mfrac><mrow><mi>P</mi><mo>(</mo><mi>B</mi><mtext>|</mtext><mi>A</mi><mo>)</mo><mi>P</mi><mo>(</mo><mi>A</mi><mo>)</mo></mrow>
    <mrow><mi>P</mi><mo>(</mo><mi>B</mi><mo>)</mo></mrow></mfrac></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'If we plug the information we already have into this formula, we can solve
    for the probability someone has cancer given they drink coffee:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mi>P</mi>
    <mrow><mo>(</mo> <mi>A</mi><mtext>|</mtext><mi>B</mi> <mo>)</mo></mrow> <mo>=</mo>
    <mfrac><mrow><mi>P</mi><mo>(</mo><mi>B</mi><mtext>|</mtext><mi>A</mi><mo>)</mo><mo>*</mo><mi>P</mi><mo>(</mo><mi>A</mi><mo>)</mo></mrow>
    <mrow><mi>P</mi><mo>(</mo><mi>B</mi><mo>)</mo></mrow></mfrac></mrow></mtd></mtr></mtable></math>
    <math display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mi>P</mi>
    <mrow><mo>(</mo> <mtext>Cancer|Coffee</mtext> <mo>)</mo></mrow> <mo>=</mo> <mfrac><mrow><mi>P</mi><mo>(</mo><mtext>Coffee|Cancer</mtext><mo>)</mo><mo>*</mo><mi>P</mi><mo>(</mo><mtext>Coffee</mtext><mo>)</mo></mrow>
    <mrow><mi>P</mi><mo>(</mo><mtext>Cancer</mtext><mo>)</mo></mrow></mfrac></mrow></mtd></mtr></mtable></math>
    <math display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mi>P</mi>
    <mrow><mo>(</mo> <mtext>Cancer|Coffee</mtext> <mo>)</mo></mrow> <mo>=</mo> <mfrac><mrow><mn>.85</mn><mo>*</mo><mo>.</mo><mn>005</mn></mrow>
    <mrow><mn>.65</mn></mrow></mfrac> <mo>=</mo> <mn>.0065</mn></mrow></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: If you want to calculate this in Python, check out [Example 2-1](#mRGVJslEkg).
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-1\. Using Bayes’ Theorem in Python
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: So the probability someone has cancer given they are a coffee drinker is only
    0.65%! This number is very different from the probability someone is a coffee
    drinker given they have cancer, which is 85%. Now do you see why the direction
    of the condition matters? Bayes’ Theorem is helpful for this reason. It can also
    be used to chain several conditional probabilities together to keep updating our
    beliefs based on new information.
  prefs: []
  type: TYPE_NORMAL
- en: If you want to explore the intuition behind Bayes’ Theorem more deeply, turn
    to [Appendix A](app01.xhtml#appendix). For now just know it helps us flip a conditional
    probability. Let’s talk about how conditional probabilities interact with joint
    and union operations next.
  prefs: []
  type: TYPE_NORMAL
- en: Naive Bayes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Bayes’ Theorem plays a central role in a common machine learning algorithm called
    Naive Bayes. Joel Grus covers it in his book *Data Science from Scratch* (O’Reilly).
  prefs: []
  type: TYPE_NORMAL
- en: Joint and Union Conditional Probabilities
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s revisit joint probabilities and how they interact with conditional probabilities.
    I want to find the probability somebody is a coffee drinker AND they have cancer.
    Should I multiply <math alttext="upper P left-parenthesis Coffee right-parenthesis"><mrow><mi>P</mi>
    <mo>(</mo> <mtext>Coffee</mtext> <mo>)</mo></mrow></math> and <math alttext="upper
    P left-parenthesis Cancer right-parenthesis"><mrow><mi>P</mi> <mo>(</mo> <mtext>Cancer</mtext>
    <mo>)</mo></mrow></math> ? Or should I use <math alttext="upper P left-parenthesis
    Coffee vertical-bar Cancer right-parenthesis"><mrow><mi>P</mi> <mo>(</mo> <mtext>Coffee|Cancer</mtext>
    <mo>)</mo></mrow></math> in place of <math alttext="upper P left-parenthesis Coffee
    right-parenthesis"><mrow><mi>P</mi> <mo>(</mo> <mtext>Coffee</mtext> <mo>)</mo></mrow></math>
    if it is available? Which one do I use?
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mtext>Option</mtext>
    <mtext>1:</mtext></mrow></mtd></mtr></mtable></math> <math display="block"><mtable
    displaystyle="true"><mtr><mtd columnalign="right"><mrow><mi>P</mi> <mo>(</mo>
    <mtext>Coffee</mtext> <mo>)</mo> <mo>×</mo> <mi>P</mi> <mo>(</mo> <mtext>Cancer</mtext>
    <mo>)</mo> <mo>=</mo> <mn>.65</mn> <mo>×</mo> <mn>.005</mn> <mo>=</mo> <mn>.00325</mn></mrow></mtd></mtr></mtable></math>
    <math display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mtext>Option</mtext>
    <mtext>2:</mtext></mrow></mtd></mtr></mtable></math> <math display="block"><mtable
    displaystyle="true"><mtr><mtd columnalign="right"><mrow><mi>P</mi> <mo>(</mo>
    <mtext>Coffee|Cancer</mtext> <mo>)</mo> <mo>×</mo> <mi>P</mi> <mo>(</mo> <mtext>Cancer</mtext>
    <mo>)</mo> <mo>=</mo> <mn>.85</mn> <mo>×</mo> <mn>.005</mn> <mo>=</mo> <mn>.00425</mn></mrow></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'If we already have established our probability applies only to people with
    cancer, does it not make sense to use <math alttext="upper P left-parenthesis
    Coffee vertical-bar Cancer right-parenthesis"><mrow><mi>P</mi> <mo>(</mo> <mtext>Coffee|Cancer</mtext>
    <mo>)</mo></mrow></math> instead of <math alttext="upper P left-parenthesis Coffee
    right-parenthesis"><mrow><mi>P</mi> <mo>(</mo> <mtext>Coffee</mtext> <mo>)</mo></mrow></math>
    ? One is more specific and applies to a condition that’s already been established.
    So we should use <math alttext="upper P left-parenthesis Coffee vertical-bar Cancer
    right-parenthesis"><mrow><mi>P</mi> <mo>(</mo> <mtext>Coffee|Cancer</mtext> <mo>)</mo></mrow></math>
    as <math alttext="upper P left-parenthesis Cancer right-parenthesis"><mrow><mi>P</mi>
    <mo>(</mo> <mtext>Cancer</mtext> <mo>)</mo></mrow></math> is already part of our
    joint probability. This means the probability of someone having cancer and being
    a coffee drinker is 0.425%:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mi>P</mi> <mo>(</mo> <mtext>Coffee</mtext> <mtext>and</mtext>
    <mtext>Cancer</mtext> <mo>)</mo> <mo>=</mo> <mi>P</mi> <mo>(</mo> <mtext>Coffee|Cancer</mtext>
    <mo>)</mo> <mo>×</mo> <mi>P</mi> <mo>(</mo> <mtext>Cancer</mtext> <mo>)</mo> <mo>=</mo>
    <mn>.85</mn> <mo>×</mo> <mn>.005</mn> <mo>=</mo> <mn>.00425</mn></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'This joint probability also applies in the other direction. I can find the
    probability of someone being a coffee drinker and having cancer by multiplying
    <math alttext="upper P left-parenthesis Cancer vertical-bar Coffee right-parenthesis"><mrow><mi>P</mi>
    <mo>(</mo> <mtext>Cancer|Coffee</mtext> <mo>)</mo></mrow></math> and <math alttext="upper
    P left-parenthesis Coffee right-parenthesis"><mrow><mi>P</mi> <mo>(</mo> <mtext>Coffee</mtext>
    <mo>)</mo></mrow></math> . As you can observe, I arrive at the same answer:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mi>P</mi> <mo>(</mo> <mtext>Cancer|Coffee</mtext>
    <mo>)</mo> <mo>×</mo> <mi>P</mi> <mo>(</mo> <mtext>Coffee</mtext> <mo>)</mo> <mo>=</mo>
    <mn>.0065</mn> <mo>×</mo> <mn>.65</mn> <mo>=</mo> <mn>.00425</mn></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'If we did not have any conditional probabilities available, then the best we
    can do is multiply <math alttext="upper P left-parenthesis Coffee Drinker right-parenthesis"><mrow><mi>P</mi>
    <mo>(</mo> <mtext>Coffee</mtext> <mtext>Drinker</mtext> <mo>)</mo></mrow></math>
    and <math alttext="upper P left-parenthesis Cancer right-parenthesis"><mrow><mi>P</mi>
    <mo>(</mo> <mtext>Cancer</mtext> <mo>)</mo></mrow></math> as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mi>P</mi> <mo>(</mo> <mtext>Coffee</mtext> <mtext>Drinker</mtext>
    <mo>)</mo> <mo>×</mo> <mi>P</mi> <mo>(</mo> <mtext>Cancer</mtext> <mo>)</mo> <mo>=</mo>
    <mn>.65</mn> <mo>×</mo> <mn>.005</mn> <mo>=</mo> <mn>.00325</mn></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Now think about this: if event A has no impact on event B, then what does that
    mean for conditional probability *P*(*B*|*A*)? That means *P*(*B*|*A*) = *P*(*B*),
    meaning event A occurring makes no difference to how likely event B is to occur.
    Therefore we can update our joint probability formula, regardless if the two events
    are dependent, to be:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mi>P</mi> <mo>(</mo> <mi>A</mi> <mtext>AND</mtext>
    <mi>B</mi> <mo>)</mo> <mo>=</mo> <mi>P</mi> <mo>(</mo> <mi>B</mi> <mo>)</mo> <mo>×</mo>
    <mi>P</mi> <mo>(</mo> <mi>A</mi><mtext>|</mtext><mi>B</mi> <mo>)</mo></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'And finally let’s talk about unions and conditional probability. If I wanted
    to calculate the probability of A or B occurring, but A may affect the probability
    of B, we update our sum rule like this:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mi>P</mi> <mo>(</mo> <mi>A</mi> <mtext>OR</mtext>
    <mi>B</mi> <mo>)</mo> <mo>=</mo> <mi>P</mi> <mo>(</mo> <mi>A</mi> <mo>)</mo> <mo>+</mo>
    <mi>P</mi> <mo>(</mo> <mi>B</mi> <mo>)</mo> <mo>-</mo> <mi>P</mi> <mo>(</mo> <mi>A</mi><mtext>|</mtext><mi>B</mi>
    <mo>)</mo> <mo>×</mo> <mi>P</mi> <mo>(</mo> <mi>B</mi> <mo>)</mo></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: As a reminder, this applies to mutually exclusive events as well. The sum rule
    *P*(*A*|*B*) × *P*(*B*) would yield 0 if the events A and B cannot happen simultaneously.
  prefs: []
  type: TYPE_NORMAL
- en: Binomial Distribution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For the remainder of this chapter, we are going to learn two probability distributions:
    the binomial and beta distributions. While we will not be using these for the
    rest of the book, they are useful tools in themselves and fundamental to understanding
    how events occur given a number of trials. They will also be a good segue to understanding
    probability distributions that we will use heavily in [Chapter 3](ch03.xhtml#ch03).
    Let’s explore a use case that could occur in a real-world scenario.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s say you are working on a new turbine jet engine and you ran 10 tests.
    The outcomes yielded eight successes and two failures:'
  prefs: []
  type: TYPE_NORMAL
- en: ✓ ✓ ✓ ✓ ✓ ✘ ✓ ✘ ✓ ✓
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You were hoping to get a 90% success rate, but based on this data you conclude
    that your tests have failed with only 80% success. Each test is time-consuming
    and expensive, so you decide it is time to go back to the drawing board to reengineer
    the design.
  prefs: []
  type: TYPE_NORMAL
- en: However, one of your engineers insists there should be more tests. “The only
    way we will know for sure is to run more tests,” she argues. “What if more tests
    yield 90% or greater success? After all, if you flip a coin 10 times and get 8
    heads, it does not mean the coin is fixed at 80%.”
  prefs: []
  type: TYPE_NORMAL
- en: You briefly consider the engineer’s argument and realize she has a point. Even
    a fair coin flip will not always have an equally split outcome, especially with
    only 10 flips. You are most likely to get five heads but you can also get three,
    four, six, or seven heads. You could even get 10 heads, although this is extremely
    unlikely. So how do you determine the likelihood of 80% success assuming the underlying
    probability is 90%?
  prefs: []
  type: TYPE_NORMAL
- en: One tool that might be relevant here is the *binomial distribution*, which measures
    how likely *k* successes can happen out of *n* trials given *p* probability.
  prefs: []
  type: TYPE_NORMAL
- en: Visually, a binomial distribution looks like [Figure 2-1](#lmITdmHGTo).
  prefs: []
  type: TYPE_NORMAL
- en: Here, we see the probability of *k* successes for each bar out of 10 trials.
    This binomial distribution assumes a probability *p* of 90%, meaning there is
    a .90 (or 90%) chance for a success to occur. If this is true, that means there
    is a .1937 probability we would get 8 successes out of 10 trials. The probability
    of getting 1 success out of 10 trials is extremely unlikely, .000000008999, hence
    why the bar is not even visible.
  prefs: []
  type: TYPE_NORMAL
- en: We can also calculate the probability of eight or fewer successes by adding
    up bars for eight or fewer successes. This would give us .2639 probability of
    eight or fewer successes.
  prefs: []
  type: TYPE_NORMAL
- en: '![emds 0201](Images/emds_0201.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-1\. A binomial distribution
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: So how do we implement the binomial distribution? We can do it from scratch
    relatively easily (as shared in [Appendix A](app01.xhtml#appendix)), or we can
    use libraries like SciPy. [Example 2-2](#HortDsUVsW) shows how we use SciPy’s
    `binom.pmf()` function (*PMF* stands for “probability mass function”) to print
    all 11 probabilities for our binomial distribution from 0 to 10 successes.
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-2\. Using SciPy for the binomial distribution
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, we provide *n* as the number of trials, *p* as the probability
    of success for each trial, and *k* as the number of successes we want to look
    up the probability for. We iterate each number of successes *x* with the corresponding
    probability we would see that many successes. As we can see in the output, the
    most likely number of successes is nine.
  prefs: []
  type: TYPE_NORMAL
- en: 'But if we add up the probability of eight or fewer successes, we would get
    .2639\. This means there is a 26.39% chance we would see eight or fewer successes
    even if the underlying success rate is 90%. So maybe the engineer is right: 26.39%
    chance is not nothing and certainly possible.'
  prefs: []
  type: TYPE_NORMAL
- en: However, we did make an assumption here in our model, which we will discuss
    next with the beta distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Binomial Distribution from Scratch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Turn to [Appendix A](app01.xhtml#appendix) to learn how to build the binomial
    distribution from scratch without scikit-learn.
  prefs: []
  type: TYPE_NORMAL
- en: Beta Distribution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What did I assume with my engine-test model using the binomial distribution?
    Is there a parameter I assumed to be true and then built my entire model around
    it? Think carefully and read on.
  prefs: []
  type: TYPE_NORMAL
- en: What might be problematic about my binomial distribution is I *assumed* the
    underlying success rate is 90%. That’s not to say my model is worthless. I just
    showed if the underlying success rate is 90%, there is a 26.39% chance I would
    see 8 or fewer successes with 10 trials. So the engineer is certainly not wrong
    that there could be an underlying success rate of 90%.
  prefs: []
  type: TYPE_NORMAL
- en: 'But let’s flip the question and consider this: what if there are other underlying
    rates of success that would yield 8/10 successes besides 90%? Could we see 8/10
    successes with an underlying 80% success rate? 70%? 30%? When we fix the 8/10
    successes, can we explore the probabilities of probabilities?'
  prefs: []
  type: TYPE_NORMAL
- en: Rather than create countless binomial distributions to answer this question,
    there is one tool that we can use. The *beta distribution* allows us to see the
    likelihood of different underlying probabilities for an event to occur given *alpha*
    successes and *beta* failures.
  prefs: []
  type: TYPE_NORMAL
- en: A chart of the beta distribution given eight successes and two failures is shown
    in [Figure 2-2](#rQmTNgSiTL).
  prefs: []
  type: TYPE_NORMAL
- en: '![emds 0202](Images/emds_0202.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-2\. Beta distribution
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Beta Distribution on Desmos
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you want to interact with the beta distribution, a Desmos graph is provided
    [here](https://oreil.ly/pN4Ep).
  prefs: []
  type: TYPE_NORMAL
- en: Notice that the x-axis represents all underlying rates of success from 0.0 to
    1.0 (0% to 100%), and the y-axis represents the likelihood of that probability
    given eight successes and two failures. In other words, the beta distribution
    allows us to see the probabilities of probabilities given 8/10 successes. Think
    of it as a meta-probability so take your time grasping this idea!
  prefs: []
  type: TYPE_NORMAL
- en: Notice also that the beta distribution is a continuous function, meaning it
    forms a continuous curve of decimal values (as opposed to the tidy and discrete
    integers in the binomial distribution). This is going to make the math with the
    beta distribution a bit harder, as a given density value on the y-axis is not
    a probability. We instead find probabilities using areas under the curve.
  prefs: []
  type: TYPE_NORMAL
- en: The beta distribution is a type of *probability distribution*, which means the
    area under the entire curve is 1.0, or 100%. To find a probability, we need to
    find the area within a range. For example, if we want to evaluate the probability
    8/10 successes would yield 90% or higher success rate, we need to find the area
    between 0.9 and 1.0, which is .225, as shaded in [Figure 2-3](#fLkskWCOBb).
  prefs: []
  type: TYPE_NORMAL
- en: '![emds 0203](Images/emds_0203.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-3\. The area between 90% and 100%, which is 22.5%
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As we did with the binomial distribution, we can use SciPy to implement the
    beta distribution. Every continuous probability distribution has a *cumulative
    density function (CDF)*, which calculates the area up to a given x-value. Let’s
    say I wanted to calculate the area up to 90% (0.0 to 0.90) as shaded in [Figure 2-4](#evRiSnkSmH).
  prefs: []
  type: TYPE_NORMAL
- en: '![emds 0204](Images/emds_0204.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-4\. Calculating the area up to 90% (0.0 to 0.90)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: It is easy enough to use SciPy with its `beta.cdf()` function, and the only
    parameters I need to provide are the x-value, the number of successes *a*, and
    the number of failures *b* as shown in [Example 2-3](#KJsBwOkgKr).
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-3\. Beta distribution using SciPy
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: So according to our calculation, there is a 77.48% chance the underlying probability
    of success is 90% or less.
  prefs: []
  type: TYPE_NORMAL
- en: How do we calculate the probability of success being 90% or more as shaded in
    [Figure 2-5](#ehSQgJemis)?
  prefs: []
  type: TYPE_NORMAL
- en: '![emds 0205](Images/emds_0205.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-5\. The probability of success being 90% or more
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Our CDF calculates area only to the left of our boundary, not the right. Think
    about our rules of probability, and with a probability distribution the total
    area under the curve is 1.0\. If we want to find the opposite probability of an
    event (greater than 0.90 as opposed to less than 0.90), just subtract the probability
    of being less than 0.90 from 1.0, and the remaining probability will capture being
    greater than 0.90\. [Figure 2-6](#mfPVFqWNTt) illustrates how we do this subtraction.
  prefs: []
  type: TYPE_NORMAL
- en: '![emds 0206](Images/emds_0206.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-6\. Finding the probability of success being greater than 90%
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '[Example 2-4](#TOceqNvaGE) shows how we calculate this subtraction operation
    in Python.'
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-4\. Subtracting to get a right area in a beta distribution
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: This means that out of 8/10 successful engine tests, there is only a 22.5% chance
    the underlying success rate is 90% or greater. But there is about a 77.5% chance
    it is less than 90%. The odds are not in our favor here that our tests were successful,
    but we could gamble on that 22.5% chance with more tests if we are feeling lucky.
    If our CFO granted funding for 26 more tests resulting in 30 successes and 6 failures,
    our beta distribution would look like [Figure 2-7](#khrqVeueje).
  prefs: []
  type: TYPE_NORMAL
- en: '![emds 0207](Images/emds_0207.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-7\. Beta distribution after 30 successes and 6 failures
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Notice our distribution became narrower, thus becoming more confident that the
    underlying rate of success is in a smaller range. Unfortunately, our probability
    of meeting our 90% success rate minimum has decreased, going from 22.5% to 13.16%
    as shown in [Example 2-5](#SOAVUgaWHU).
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-5\. A beta distribution with more trials
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: At this point, it might be a good idea to walk away and stop doing tests, unless
    you want to keep gambling against that 13.16% chance and hope the peak moves to
    the right.
  prefs: []
  type: TYPE_NORMAL
- en: Last but not least, how would we calculate an area in the middle? What if I
    want to find the probability my underlying rate of success is between 80% and
    90% as shown in [Figure 2-8](#rCjWtSlSLH)?
  prefs: []
  type: TYPE_NORMAL
- en: '![emds 0208](Images/emds_0208.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-8\. Probability the underlying rate of success is between 80% and 90%
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Think carefully how you might approach this. What if we were to subtract the
    area behind .80 from the area behind .90 like in [Figure 2-9](#lRIhUdRQQO)?
  prefs: []
  type: TYPE_NORMAL
- en: '![emds 0209](Images/emds_0209.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-9\. Obtaining the area between .80 and .90
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Would that give us the area between .80 and .90? Yes it would, and it would
    yield an area of .3386 or 33.86% probability. Here is how we would calculate it
    in Python ([Example 2-6](#OGFOWPsFrM)).
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-6\. Beta distribution middle area using SciPy
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The beta distribution is a fascinating tool to measure the probability of an
    event occurring versus not occurring, based on a limited set of observations.
    It allows us to reason about probabilities of probabilities, and we can update
    it as we get new data. We can also use it for hypothesis testing, but we will
    put more emphasis on using the normal distribution and T-distribution for that
    purpose in [Chapter 3](ch03.xhtml#ch03).
  prefs: []
  type: TYPE_NORMAL
- en: Beta Distribution from Scratch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To learn how to implement the beta distribution from scratch, refer to [Appendix A](app01.xhtml#appendix).
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We covered a lot in this chapter! We not only discussed the fundamentals of
    probability, its logical operators, and Bayes’ Theorem, but also introduced probability
    distributions, including the binomial and beta distributions. In the next chapter
    we will cover one of the more famous distributions, the normal distribution, and
    how it relates to hypothesis testing.
  prefs: []
  type: TYPE_NORMAL
- en: If you want to learn more about Bayesian probability and statistics, a great
    book is *Bayesian Statistics the Fun Way* by Will Kurt (No Starch Press). There
    are also interactive [Katacoda scenarios available on the O’Reilly platform](https://oreil.ly/OFbai).
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There is a 30% chance of rain today, and a 40% chance your umbrella order will
    arrive on time. You are eager to walk in the rain today and cannot do so without
    either!
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the probability it will rain AND your umbrella will arrive?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: There is a 30% chance of rain today, and a 40% chance your umbrella order will
    arrive on time.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You will be able to run errands only if it does not rain or your umbrella arrives.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: What is the probability it will not rain OR your umbrella arrives?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: There is a 30% chance of rain today, and a 40% chance your umbrella order will
    arrive on time.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: However, you found out if it rains there is only a 20% chance your umbrella
    will arrive on time.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: What is the probability it will rain AND your umbrella will arrive on time?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You have 137 passengers booked on a flight from Las Vegas to Dallas. However,
    it is Las Vegas on a Sunday morning and you estimate each passenger is 40% likely
    to not show up.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You are trying to figure out how many seats to overbook so the plane does not
    fly empty.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: How likely is it at least 50 passengers will not show up?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You flipped a coin 19 times and got heads 15 times and tails 4 times.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Do you think this coin has any good probability of being fair? Why or why not?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Answers are in [Appendix B](app02.xhtml#exercise_answers).
  prefs: []
  type: TYPE_NORMAL
