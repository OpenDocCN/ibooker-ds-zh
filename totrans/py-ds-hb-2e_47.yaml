- en: 'Chapter 42\. In Depth: Linear Regression'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Just as naive Bayes (discussed in [Chapter 41](ch41.xhtml#section-0505-naive-bayes))
    is a good starting point for classification tasks, linear regression models are
    a good starting point for regression tasks. Such models are popular because they
    can be fit quickly and are straightforward to interpret. You are already familiar
    with the simplest form of linear regression model (i.e., fitting a straight line
    to two-dimensional data), but such models can be extended to model more complicated
    data behavior.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter we will start with a quick walkthrough of the mathematics behind
    this well-known problem, before moving on to see how linear models can be generalized
    to account for more complicated patterns in data.
  prefs: []
  type: TYPE_NORMAL
- en: 'We begin with the standard imports:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Simple Linear Regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will start with the most familiar linear regression, a straight-line fit
    to data. A straight-line fit is a model of the form:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="y equals a x plus b" display="block"><mrow><mi>y</mi> <mo>=</mo>
    <mi>a</mi> <mi>x</mi> <mo>+</mo> <mi>b</mi></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: where <math alttext="a"><mi>a</mi></math> is commonly known as the *slope*,
    and <math alttext="b"><mi>b</mi></math> is commonly known as the *intercept*.
  prefs: []
  type: TYPE_NORMAL
- en: Consider the following data, which is scattered about a line with a slope of
    2 and an intercept of –5 (see [Figure 42-1](#fig_0506-linear-regression_files_in_output_4_0)).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![output 4 0](assets/output_4_0.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 42-1\. Data for linear regression
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We can use Scikit-Learn’s `LinearRegression` estimator to fit this data and
    construct the best-fit line, as shown in [Figure 42-2](#fig_0506-linear-regression_files_in_output_6_0).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![output 6 0](assets/output_6_0.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 42-2\. A simple linear regression model
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The slope and intercept of the data are contained in the model’s fit parameters,
    which in Scikit-Learn are always marked by a trailing underscore. Here the relevant
    parameters are `coef_` and `intercept_`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: We see that the results are very close to the values used to generate the data,
    as we might hope.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `LinearRegression` estimator is much more capable than this, however—in
    addition to simple straight-line fits, it can also handle multidimensional linear
    models of the form:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="y equals a 0 plus a 1 x 1 plus a 2 x 2 plus ellipsis" display="block"><mrow><mi>y</mi>
    <mo>=</mo> <msub><mi>a</mi> <mn>0</mn></msub> <mo>+</mo> <msub><mi>a</mi> <mn>1</mn></msub>
    <msub><mi>x</mi> <mn>1</mn></msub> <mo>+</mo> <msub><mi>a</mi> <mn>2</mn></msub>
    <msub><mi>x</mi> <mn>2</mn></msub> <mo>+</mo> <mo>⋯</mo></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: where there are multiple <math alttext="x"><mi>x</mi></math> values. Geometrically,
    this is akin to fitting a plane to points in three dimensions, or fitting a hyperplane
    to points in higher dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: 'The multidimensional nature of such regressions makes them more difficult to
    visualize, but we can see one of these fits in action by building some example
    data, using NumPy’s matrix multiplication operator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Here the <math alttext="y"><mi>y</mi></math> data is constructed from a linear
    combination of three random <math alttext="x"><mi>x</mi></math> values, and the
    linear regression recovers the coefficients used to construct the data.
  prefs: []
  type: TYPE_NORMAL
- en: In this way, we can use the single `LinearRegression` estimator to fit lines,
    planes, or hyperplanes to our data. It still appears that this approach would
    be limited to strictly linear relationships between variables, but it turns out
    we can relax this as well.
  prefs: []
  type: TYPE_NORMAL
- en: Basis Function Regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One trick you can use to adapt linear regression to nonlinear relationships
    between variables is to transform the data according to *basis functions*. We
    have seen one version of this before, in the `PolynomialRegression` pipeline used
    in Chapters [39](ch39.xhtml#section-0503-hyperparameters-and-model-validation)
    and [40](ch40.xhtml#section-0504-feature-engineering). The idea is to take our
    multidimensional linear model:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="y equals a 0 plus a 1 x 1 plus a 2 x 2 plus a 3 x 3 plus ellipsis"
    display="block"><mrow><mi>y</mi> <mo>=</mo> <msub><mi>a</mi> <mn>0</mn></msub>
    <mo>+</mo> <msub><mi>a</mi> <mn>1</mn></msub> <msub><mi>x</mi> <mn>1</mn></msub>
    <mo>+</mo> <msub><mi>a</mi> <mn>2</mn></msub> <msub><mi>x</mi> <mn>2</mn></msub>
    <mo>+</mo> <msub><mi>a</mi> <mn>3</mn></msub> <msub><mi>x</mi> <mn>3</mn></msub>
    <mo>+</mo> <mo>⋯</mo></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: and build the <math alttext="x 1 comma x 2 comma x 3 comma"><mrow><msub><mi>x</mi>
    <mn>1</mn></msub> <mo>,</mo> <msub><mi>x</mi> <mn>2</mn></msub> <mo>,</mo> <msub><mi>x</mi>
    <mn>3</mn></msub> <mo>,</mo></mrow></math> and so on from our single-dimensional
    input <math alttext="x"><mi>x</mi></math> . That is, we let <math alttext="x Subscript
    n Baseline equals f Subscript n Baseline left-parenthesis x right-parenthesis"><mrow><msub><mi>x</mi>
    <mi>n</mi></msub> <mo>=</mo> <msub><mi>f</mi> <mi>n</mi></msub> <mrow><mo>(</mo>
    <mi>x</mi> <mo>)</mo></mrow></mrow></math> , where <math alttext="f Subscript
    n Baseline left-parenthesis right-parenthesis"><mrow><msub><mi>f</mi> <mi>n</mi></msub>
    <mrow><mo>(</mo> <mo>)</mo></mrow></mrow></math> is some function that transforms
    our data.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, if <math alttext="f Subscript n Baseline left-parenthesis x right-parenthesis
    equals x Superscript n"><mrow><msub><mi>f</mi> <mi>n</mi></msub> <mrow><mo>(</mo>
    <mi>x</mi> <mo>)</mo></mrow> <mo>=</mo> <msup><mi>x</mi> <mi>n</mi></msup></mrow></math>
    , our model becomes a polynomial regression:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="y equals a 0 plus a 1 x plus a 2 x squared plus a 3 x cubed plus
    ellipsis" display="block"><mrow><mi>y</mi> <mo>=</mo> <msub><mi>a</mi> <mn>0</mn></msub>
    <mo>+</mo> <msub><mi>a</mi> <mn>1</mn></msub> <mi>x</mi> <mo>+</mo> <msub><mi>a</mi>
    <mn>2</mn></msub> <msup><mi>x</mi> <mn>2</mn></msup> <mo>+</mo> <msub><mi>a</mi>
    <mn>3</mn></msub> <msup><mi>x</mi> <mn>3</mn></msup> <mo>+</mo> <mo>⋯</mo></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: Notice that this is *still a linear model*—the linearity refers to the fact
    that the coefficients <math alttext="a Subscript n"><msub><mi>a</mi> <mi>n</mi></msub></math>
    never multiply or divide each other. What we have effectively done is taken our
    one-dimensional <math alttext="x"><mi>x</mi></math> values and projected them
    into a higher dimension, so that a linear fit can fit more complicated relationships
    between <math alttext="x"><mi>x</mi></math> and <math alttext="y"><mi>y</mi></math>
    .
  prefs: []
  type: TYPE_NORMAL
- en: Polynomial Basis Functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This polynomial projection is useful enough that it is built into Scikit-Learn,
    using the `PolynomialFeatures` transformer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: We see here that the transformer has converted our one-dimensional array into
    a three-dimensional array, where each column contains the exponentiated value.
    This new, higher-dimensional data representation can then be plugged into a linear
    regression.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we saw in [Chapter 40](ch40.xhtml#section-0504-feature-engineering), the
    cleanest way to accomplish this is to use a pipeline. Let’s make a 7th-degree
    polynomial model in this way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: With this transform in place, we can use the linear model to fit much more complicated
    relationships between <math alttext="x"><mi>x</mi></math> and <math alttext="y"><mi>y</mi></math>
    . For example, here is a sine wave with noise (see [Figure 42-3](#fig_0506-linear-regression_files_in_output_19_0)).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![output 19 0](assets/output_19_0.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 42-3\. A linear polynomial fit to nonlinear training data
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Our linear model, through the use of seventh-order polynomial basis functions,
    can provide an excellent fit to this nonlinear data!
  prefs: []
  type: TYPE_NORMAL
- en: Gaussian Basis Functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Of course, other basis functions are possible. For example, one useful pattern
    is to fit a model that is not a sum of polynomial bases, but a sum of Gaussian
    bases. The result might look something like [Figure 42-4](#fig_images_in_0506-gaussian-basis).
  prefs: []
  type: TYPE_NORMAL
- en: '![05.06 gaussian basis](assets/05.06-gaussian-basis.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 42-4\. A Gaussian basis function fit to nonlinear data^([1](ch42.xhtml#idm45858736130864))
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The shaded regions in the plot are the scaled basis functions, and when added
    together they reproduce the smooth curve through the data. These Gaussian basis
    functions are not built into Scikit-Learn, but we can write a custom transformer
    that will create them, as shown here and illustrated in [Figure 42-5](#fig_0506-linear-regression_files_in_output_24_0)
    (Scikit-Learn transformers are implemented as Python classes; reading Scikit-Learn’s
    source is a good way to see how they can be created):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '![output 24 0](assets/output_24_0.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 42-5\. A Gaussian basis function fit computed with a custom transformer
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'I’ve included this example just to make clear that there is nothing magic about
    polynomial basis functions: if you have some sort of intuition into the generating
    process of your data that makes you think one basis or another might be appropriate,
    you can use that instead.'
  prefs: []
  type: TYPE_NORMAL
- en: Regularization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The introduction of basis functions into our linear regression makes the model
    much more flexible, but it also can very quickly lead to overfitting (refer back
    to [Chapter 39](ch39.xhtml#section-0503-hyperparameters-and-model-validation)
    for a discussion of this). For example, [Figure 42-6](#fig_0506-linear-regression_files_in_output_27_0)
    shows what happens if we use a large number of Gaussian basis functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '![output 27 0](assets/output_27_0.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 42-6\. An overly complex basis function model that overfits the data
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: With the data projected to the 30-dimensional basis, the model has far too much
    flexibility and goes to extreme values between locations where it is constrained
    by data. We can see the reason for this if we plot the coefficients of the Gaussian
    bases with respect to their locations, as shown in [Figure 42-7](#fig_0506-linear-regression_files_in_output_29_0).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '![output 29 0](assets/output_29_0.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 42-7\. The coefficients of the Gaussian bases in the overly complex model
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The lower panel of this figure shows the amplitude of the basis function at
    each location. This is typical overfitting behavior when basis functions overlap:
    the coefficients of adjacent basis functions blow up and cancel each other out.
    We know that such behavior is problematic, and it would be nice if we could limit
    such spikes explicitly in the model by penalizing large values of the model parameters.
    Such a penalty is known as *regularization*, and comes in several forms.'
  prefs: []
  type: TYPE_NORMAL
- en: Ridge Regression (L[2] Regularization)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Perhaps the most common form of regularization is known as *ridge regression*
    or <math alttext="upper L 2"><msub><mi>L</mi> <mn>2</mn></msub></math> *regularization*
    (sometimes also called *Tikhonov regularization*). This proceeds by penalizing
    the sum of squares (2-norms) of the model coefficients <math alttext="theta Subscript
    n"><msub><mi>θ</mi> <mi>n</mi></msub></math> . In this case, the penalty on the
    model fit would be:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="upper P equals alpha sigma-summation Underscript n equals 1 Overscript
    upper N Endscripts theta Subscript n Superscript 2" display="block"><mrow><mi>P</mi>
    <mo>=</mo> <mi>α</mi> <munderover><mo>∑</mo> <mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>N</mi></munderover> <msubsup><mi>θ</mi> <mi>n</mi> <mn>2</mn></msubsup></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: where <math alttext="alpha"><mi>α</mi></math> is a free parameter that controls
    the strength of the penalty. This type of penalized model is built into Scikit-Learn
    with the `Ridge` estimator (see [Figure 42-8](#fig_0506-linear-regression_files_in_output_32_0)).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '![output 32 0](assets/output_32_0.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 42-8\. Ridge (L[2]) regularization applied to the overly complex model
    (compare to [Figure 42-7](#fig_0506-linear-regression_files_in_output_29_0))
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The <math alttext="alpha"><mi>α</mi></math> parameter is essentially a knob
    controlling the complexity of the resulting model. In the limit <math alttext="alpha
    right-arrow 0"><mrow><mi>α</mi> <mo>→</mo> <mn>0</mn></mrow></math> , we recover
    the standard linear regression result; in the limit <math alttext="alpha right-arrow
    normal infinity"><mrow><mi>α</mi> <mo>→</mo> <mi>∞</mi></mrow></math> , all model
    responses will be suppressed. One advantage of ridge regression in particular
    is that it can be computed very efficiently—at hardly more computational cost
    than the original linear regression model.
  prefs: []
  type: TYPE_NORMAL
- en: Lasso Regression (L[1] Regularization)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Another common type of regularization is known as *lasso regression* or *L[1]
    regularization* and involves penalizing the sum of absolute values (1-norms) of
    regression coefficients:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="upper P equals alpha sigma-summation Underscript n equals 1 Overscript
    upper N Endscripts StartAbsoluteValue theta Subscript n Baseline EndAbsoluteValue"
    display="block"><mrow><mi>P</mi> <mo>=</mo> <mi>α</mi> <munderover><mo>∑</mo>
    <mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow> <mi>N</mi></munderover> <mrow><mo>|</mo>
    <msub><mi>θ</mi> <mi>n</mi></msub> <mo>|</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Though this is conceptually very similar to ridge regression, the results can
    differ surprisingly. For example, due to its construction, lasso regression tends
    to favor *sparse models* where possible: that is, it preferentially sets many
    model coefficients to exactly zero.'
  prefs: []
  type: TYPE_NORMAL
- en: We can see this behavior if we duplicate the previous example using <math alttext="upper
    L 1"><msub><mi>L</mi> <mn>1</mn></msub></math> -normalized coefficients (see [Figure 42-9](#fig_0506-linear-regression_files_in_output_35_0)).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '![output 35 0](assets/output_35_0.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 42-9\. Lasso (L[1]) regularization applied to the overly complex model
    (compare to [Figure 42-8](#fig_0506-linear-regression_files_in_output_32_0))
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: With the lasso regression penalty, the majority of the coefficients are exactly
    zero, with the functional behavior being modeled by a small subset of the available
    basis functions. As with ridge regularization, the <math alttext="alpha"><mi>α</mi></math>
    parameter tunes the strength of the penalty and should be determined via, for
    example, cross-validation (refer back to [Chapter 39](ch39.xhtml#section-0503-hyperparameters-and-model-validation)
    for a discussion of this).
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: Predicting Bicycle Traffic'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As an example, let’s take a look at whether we can predict the number of bicycle
    trips across Seattle’s Fremont Bridge based on weather, season, and other factors.
    We already saw this data in [Chapter 23](ch23.xhtml#section-0311-working-with-time-series),
    but here we will join the bike data with another dataset and try to determine
    the extent to which weather and seasonal factors—temperature, precipitation, and
    daylight hours—affect the volume of bicycle traffic through this corridor. Fortunately,
    the National Oceanic and Atmospheric Administration (NOAA) makes its daily [weather
    station data](https://oreil.ly/sE5zO) available—I used station ID USW00024233—and
    we can easily use Pandas to join the two data sources. We will perform a simple
    linear regression to relate weather and other information to bicycle counts, in
    order to estimate how a change in any one of these parameters affects the number
    of riders on a given day.
  prefs: []
  type: TYPE_NORMAL
- en: In particular, this is an example of how the tools of Scikit-Learn can be used
    in a statistical modeling framework, in which the parameters of the model are
    assumed to have interpretable meaning. As discussed previously, this is not a
    standard approach within machine learning, but such interpretation is possible
    for some models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start by loading the two datasets, indexing by date:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'For simplicity, let’s look at data prior to 2020 in order to avoid the effects
    of the COVID-19 pandemic, which significantly affected commuting patterns in Seattle:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Next we will compute the total daily bicycle traffic, and put this in its own
    `DataFrame`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'We saw previously that the patterns of use generally vary from day to day.
    Let’s account for this in our data by adding binary columns that indicate the
    day of the week:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, we might expect riders to behave differently on holidays; let’s
    add an indicator of this as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: We also might suspect that the hours of daylight would affect how many people
    ride. Let’s use the standard astronomical calculation to add this information
    (see [Figure 42-10](#fig_0506-linear-regression_files_in_output_50_1)).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '![output 50 1](assets/output_50_1.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 42-10\. Visualization of hours of daylight in Seattle
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'We can also add the average temperature and total precipitation to the data.
    In addition to the inches of precipitation, let’s add a flag that indicates whether
    a day is dry (has zero precipitation):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, let’s add a counter that increases from day 1, and measures how many
    years have passed. This will let us measure any observed annual increase or decrease
    in daily crossings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that our data is in order, and we can take a look at it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'With this in place, we can choose the columns to use, and fit a linear regression
    model to our data. We will set `fit_intercept=False`, because the daily flags
    essentially operate as their own day-specific intercepts:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Finally, we can compare the total and predicted bicycle traffic visually (see
    [Figure 42-11](#fig_0506-linear-regression_files_in_output_60_0)).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '![output 60 0](assets/output_60_0.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 42-11\. Our model’s prediction of bicycle traffic
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'From the fact that the data and model predictions don’t line up exactly, it
    is evident that we have missed some key features. Either our features are not
    complete (i.e., people decide whether to ride to work based on more than just
    these features), or there are some nonlinear relationships that we have failed
    to take into account (e.g., perhaps people ride less at both high and low temperatures).
    Nevertheless, our rough approximation is enough to give us some insights, and
    we can take a look at the coefficients of the linear model to estimate how much
    each feature contributes to the daily bicycle count:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'These numbers are difficult to interpret without some measure of their uncertainty.
    We can compute these uncertainties quickly using bootstrap resamplings of the
    data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'With these errors estimated, let’s again look at the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The `effect` column here, roughly speaking, shows how the number of riders
    is affected by a change of the feature in question. For example, there is a clear
    divide when it comes to the day of the week: there are thousands fewer riders
    on weekends than on weekdays. We also see that for each additional hour of daylight,
    409 ± 26 more people choose to ride; a temperature increase of one degree Fahrenheit
    encourages 179 ± 7 people to grab their bicycle; a dry day means an average of
    2,111 ± 101 more riders, and every inch of rainfall leads 2,790 ± 186 riders to
    choose another mode of transport. Once all these effects are accounted for, we
    see a modest increase of 324 ± 22 new daily riders each year.'
  prefs: []
  type: TYPE_NORMAL
- en: Our simple model is almost certainly missing some relevant information. For
    example, as mentioned earlier, nonlinear effects (such as effects of precipitation
    *and* cold temperature) and nonlinear trends within each variable (such as disinclination
    to ride at very cold and very hot temperatures) cannot be accounted for in a simple
    linear model. Additionally, we have thrown away some of the finer-grained information
    (such as the difference between a rainy morning and a rainy afternoon), and we
    have ignored correlations between days (such as the possible effect of a rainy
    Tuesday on Wednesday’s numbers, or the effect of an unexpected sunny day after
    a streak of rainy days). These are all potentially interesting effects, and you
    now have the tools to begin exploring them if you wish!
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch42.xhtml#idm45858736130864-marker)) Code to produce this figure can
    be found in the [online appendix](https://oreil.ly/o1Zya).
  prefs: []
  type: TYPE_NORMAL
