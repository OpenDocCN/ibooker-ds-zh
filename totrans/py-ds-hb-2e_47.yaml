- en: 'Chapter 42\. In Depth: Linear Regression'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第42章\. 深入解析：线性回归
- en: Just as naive Bayes (discussed in [Chapter 41](ch41.xhtml#section-0505-naive-bayes))
    is a good starting point for classification tasks, linear regression models are
    a good starting point for regression tasks. Such models are popular because they
    can be fit quickly and are straightforward to interpret. You are already familiar
    with the simplest form of linear regression model (i.e., fitting a straight line
    to two-dimensional data), but such models can be extended to model more complicated
    data behavior.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 就像朴素贝叶斯（讨论见[第41章](ch41.xhtml#section-0505-naive-bayes)）对于分类任务是一个很好的起点一样，线性回归模型对于回归任务也是一个很好的起点。这样的模型很受欢迎，因为它们可以快速拟合并且易于解释。你已经熟悉了最简单形式的线性回归模型（即将直线拟合到二维数据），但是这样的模型可以扩展到对更复杂的数据行为进行建模。
- en: In this chapter we will start with a quick walkthrough of the mathematics behind
    this well-known problem, before moving on to see how linear models can be generalized
    to account for more complicated patterns in data.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将首先快速了解这个众所周知问题背后的数学知识，然后再看看线性模型如何被泛化以解决数据中更复杂的模式。
- en: 'We begin with the standard imports:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从标准导入开始：
- en: '[PRE0]'
  id: totrans-4
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Simple Linear Regression
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简单线性回归
- en: 'We will start with the most familiar linear regression, a straight-line fit
    to data. A straight-line fit is a model of the form:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从最熟悉的线性回归开始，即对数据进行直线拟合。直线拟合是一个形式为的模型：
- en: <math alttext="y equals a x plus b" display="block"><mrow><mi>y</mi> <mo>=</mo>
    <mi>a</mi> <mi>x</mi> <mo>+</mo> <mi>b</mi></mrow></math>
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="y equals a x plus b" display="block"><mrow><mi>y</mi> <mo>=</mo>
    <mi>a</mi> <mi>x</mi> <mo>+</mo> <mi>b</mi></mrow></math>
- en: where <math alttext="a"><mi>a</mi></math> is commonly known as the *slope*,
    and <math alttext="b"><mi>b</mi></math> is commonly known as the *intercept*.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 其中<math alttext="a"><mi>a</mi></math>通常被称为*斜率*，而<math alttext="b"><mi>b</mi></math>通常被称为*截距*。
- en: Consider the following data, which is scattered about a line with a slope of
    2 and an intercept of –5 (see [Figure 42-1](#fig_0506-linear-regression_files_in_output_4_0)).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下数据，这些数据分布在一条斜率为2，截距为-5的直线周围（见[图42-1](#fig_0506-linear-regression_files_in_output_4_0)）。
- en: '[PRE1]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![output 4 0](assets/output_4_0.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![output 4 0](assets/output_4_0.png)'
- en: Figure 42-1\. Data for linear regression
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图42-1\. 线性回归的数据
- en: We can use Scikit-Learn’s `LinearRegression` estimator to fit this data and
    construct the best-fit line, as shown in [Figure 42-2](#fig_0506-linear-regression_files_in_output_6_0).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用Scikit-Learn的`LinearRegression`估计器来拟合这些数据并构建最佳拟合线，如[图42-2](#fig_0506-linear-regression_files_in_output_6_0)所示。
- en: '[PRE2]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![output 6 0](assets/output_6_0.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![output 6 0](assets/output_6_0.png)'
- en: Figure 42-2\. A simple linear regression model
  id: totrans-16
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图42-2\. 一个简单的线性回归模型
- en: 'The slope and intercept of the data are contained in the model’s fit parameters,
    which in Scikit-Learn are always marked by a trailing underscore. Here the relevant
    parameters are `coef_` and `intercept_`:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 数据的斜率和截距包含在模型的拟合参数中，在Scikit-Learn中始终以下划线结尾标记。这里的相关参数是`coef_`和`intercept_`：
- en: '[PRE3]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We see that the results are very close to the values used to generate the data,
    as we might hope.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到结果与用于生成数据的值非常接近，这是我们所希望的。
- en: 'The `LinearRegression` estimator is much more capable than this, however—in
    addition to simple straight-line fits, it can also handle multidimensional linear
    models of the form:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，`LinearRegression`估计器要比这更强大——除了简单的直线拟合外，它还可以处理形式为的多维线性模型：
- en: <math alttext="y equals a 0 plus a 1 x 1 plus a 2 x 2 plus ellipsis" display="block"><mrow><mi>y</mi>
    <mo>=</mo> <msub><mi>a</mi> <mn>0</mn></msub> <mo>+</mo> <msub><mi>a</mi> <mn>1</mn></msub>
    <msub><mi>x</mi> <mn>1</mn></msub> <mo>+</mo> <msub><mi>a</mi> <mn>2</mn></msub>
    <msub><mi>x</mi> <mn>2</mn></msub> <mo>+</mo> <mo>⋯</mo></mrow></math>
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="y equals a 0 plus a 1 x 1 plus a 2 x 2 plus ellipsis" display="block"><mrow><mi>y</mi>
    <mo>=</mo> <msub><mi>a</mi> <mn>0</mn></msub> <mo>+</mo> <msub><mi>a</mi> <mn>1</mn></msub>
    <msub><mi>x</mi> <mn>1</mn></msub> <mo>+</mo> <msub><mi>a</mi> <mn>2</mn></msub>
    <msub><mi>x</mi> <mn>2</mn></msub> <mo>+</mo> <mo>⋯</mo></mrow></math>
- en: where there are multiple <math alttext="x"><mi>x</mi></math> values. Geometrically,
    this is akin to fitting a plane to points in three dimensions, or fitting a hyperplane
    to points in higher dimensions.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 其中有多个<math alttext="x"><mi>x</mi></math>值。从几何上讲，这类似于在三维空间中对点拟合平面，或者在更高维度中对点拟合超平面。
- en: 'The multidimensional nature of such regressions makes them more difficult to
    visualize, but we can see one of these fits in action by building some example
    data, using NumPy’s matrix multiplication operator:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这种回归的多维性使其更难以可视化，但我们可以通过构建一些示例数据，使用NumPy的矩阵乘法运算符来看到其中一个拟合的过程：
- en: '[PRE4]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Here the <math alttext="y"><mi>y</mi></math> data is constructed from a linear
    combination of three random <math alttext="x"><mi>x</mi></math> values, and the
    linear regression recovers the coefficients used to construct the data.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的<math alttext="y"><mi>y</mi></math>数据是从三个随机<math alttext="x"><mi>x</mi></math>值的线性组合构成的，线性回归恢复了用于构建数据的系数。
- en: In this way, we can use the single `LinearRegression` estimator to fit lines,
    planes, or hyperplanes to our data. It still appears that this approach would
    be limited to strictly linear relationships between variables, but it turns out
    we can relax this as well.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，我们可以使用单个 `LinearRegression` 评估器来拟合线条、平面或超平面到我们的数据。看起来这种方法仍然限制在变量之间严格的线性关系，但事实证明我们也可以放宽这一点。
- en: Basis Function Regression
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基函数回归
- en: 'One trick you can use to adapt linear regression to nonlinear relationships
    between variables is to transform the data according to *basis functions*. We
    have seen one version of this before, in the `PolynomialRegression` pipeline used
    in Chapters [39](ch39.xhtml#section-0503-hyperparameters-and-model-validation)
    and [40](ch40.xhtml#section-0504-feature-engineering). The idea is to take our
    multidimensional linear model:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以用的一个技巧是将线性回归适应变量之间的非线性关系，方法是根据 *基函数* 转换数据。我们之前已经见过这样的一个版本，在第[39](ch39.xhtml#section-0503-hyperparameters-and-model-validation)章和第[40](ch40.xhtml#section-0504-feature-engineering)章中使用的
    `PolynomialRegression` 流水线中。这个想法是将我们的多维线性模型：
- en: <math alttext="y equals a 0 plus a 1 x 1 plus a 2 x 2 plus a 3 x 3 plus ellipsis"
    display="block"><mrow><mi>y</mi> <mo>=</mo> <msub><mi>a</mi> <mn>0</mn></msub>
    <mo>+</mo> <msub><mi>a</mi> <mn>1</mn></msub> <msub><mi>x</mi> <mn>1</mn></msub>
    <mo>+</mo> <msub><mi>a</mi> <mn>2</mn></msub> <msub><mi>x</mi> <mn>2</mn></msub>
    <mo>+</mo> <msub><mi>a</mi> <mn>3</mn></msub> <msub><mi>x</mi> <mn>3</mn></msub>
    <mo>+</mo> <mo>⋯</mo></mrow></math>
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="y equals a 0 plus a 1 x 1 plus a 2 x 2 plus a 3 x 3 plus ellipsis"
    display="block"><mrow><mi>y</mi> <mo>=</mo> <msub><mi>a</mi> <mn>0</mn></msub>
    <mo>+</mo> <msub><mi>a</mi> <mn>1</mn></msub> <msub><mi>x</mi> <mn>1</mn></msub>
    <mo>+</mo> <msub><mi>a</mi> <mn>2</mn></msub> <msub><mi>x</mi> <mn>2</mn></msub>
    <mo>+</mo> <msub><mi>a</mi> <mn>3</mn></msub> <msub><mi>x</mi> <mn>3</mn></msub>
    <mo>+</mo> <mo>⋯</mo></mrow></math>
- en: and build the <math alttext="x 1 comma x 2 comma x 3 comma"><mrow><msub><mi>x</mi>
    <mn>1</mn></msub> <mo>,</mo> <msub><mi>x</mi> <mn>2</mn></msub> <mo>,</mo> <msub><mi>x</mi>
    <mn>3</mn></msub> <mo>,</mo></mrow></math> and so on from our single-dimensional
    input <math alttext="x"><mi>x</mi></math> . That is, we let <math alttext="x Subscript
    n Baseline equals f Subscript n Baseline left-parenthesis x right-parenthesis"><mrow><msub><mi>x</mi>
    <mi>n</mi></msub> <mo>=</mo> <msub><mi>f</mi> <mi>n</mi></msub> <mrow><mo>(</mo>
    <mi>x</mi> <mo>)</mo></mrow></mrow></math> , where <math alttext="f Subscript
    n Baseline left-parenthesis right-parenthesis"><mrow><msub><mi>f</mi> <mi>n</mi></msub>
    <mrow><mo>(</mo> <mo>)</mo></mrow></mrow></math> is some function that transforms
    our data.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 并从我们的单维输入 <math alttext="x"><mi>x</mi></math> 中构建 <math alttext="x 1 comma x
    2 comma x 3 comma"><mrow><msub><mi>x</mi> <mn>1</mn></msub> <mo>,</mo> <msub><mi>x</mi>
    <mn>2</mn></msub> <mo>,</mo> <msub><mi>x</mi> <mn>3</mn></msub> <mo>,</mo></mrow></math>
    等等。也就是说，我们让 <math alttext="x Subscript n Baseline equals f Subscript n Baseline
    left-parenthesis x right-parenthesis"><mrow><msub><mi>x</mi> <mi>n</mi></msub>
    <mo>=</mo> <msub><mi>f</mi> <mi>n</mi></msub> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></math>
    ，其中 <math alttext="f Subscript n Baseline left-parenthesis right-parenthesis"><mrow><msub><mi>f</mi>
    <mi>n</mi></msub> <mrow><mo>(</mo> <mo>)</mo></mrow></mrow></math> 是将我们的数据转换的某个函数。
- en: 'For example, if <math alttext="f Subscript n Baseline left-parenthesis x right-parenthesis
    equals x Superscript n"><mrow><msub><mi>f</mi> <mi>n</mi></msub> <mrow><mo>(</mo>
    <mi>x</mi> <mo>)</mo></mrow> <mo>=</mo> <msup><mi>x</mi> <mi>n</mi></msup></mrow></math>
    , our model becomes a polynomial regression:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果 <math alttext="f Subscript n Baseline left-parenthesis x right-parenthesis
    equals x Superscript n"><mrow><msub><mi>f</mi> <mi>n</mi></msub> <mrow><mo>(</mo>
    <mi>x</mi> <mo>)</mo></mrow> <mo>=</mo> <msup><mi>x</mi> <mi>n</mi></msup></mrow></math>
    ，我们的模型就会变成多项式回归：
- en: <math alttext="y equals a 0 plus a 1 x plus a 2 x squared plus a 3 x cubed plus
    ellipsis" display="block"><mrow><mi>y</mi> <mo>=</mo> <msub><mi>a</mi> <mn>0</mn></msub>
    <mo>+</mo> <msub><mi>a</mi> <mn>1</mn></msub> <mi>x</mi> <mo>+</mo> <msub><mi>a</mi>
    <mn>2</mn></msub> <msup><mi>x</mi> <mn>2</mn></msup> <mo>+</mo> <msub><mi>a</mi>
    <mn>3</mn></msub> <msup><mi>x</mi> <mn>3</mn></msup> <mo>+</mo> <mo>⋯</mo></mrow></math>
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="y equals a 0 plus a 1 x plus a 2 x squared plus a 3 x cubed plus
    ellipsis" display="block"><mrow><mi>y</mi> <mo>=</mo> <msub><mi>a</mi> <mn>0</mn></msub>
    <mo>+</mo> <msub><mi>a</mi> <mn>1</mn></msub> <mi>x</mi> <mo>+</mo> <msub><mi>a</mi>
    <mn>2</mn></msub> <msup><mi>x</mi> <mn>2</mn></msup> <mo>+</mo> <msub><mi>a</mi>
    <mn>3</mn></msub> <msup><mi>x</mi> <mn>3</mn></msup> <mo>+</mo> <mo>⋯</mo></mrow></math>
- en: Notice that this is *still a linear model*—the linearity refers to the fact
    that the coefficients <math alttext="a Subscript n"><msub><mi>a</mi> <mi>n</mi></msub></math>
    never multiply or divide each other. What we have effectively done is taken our
    one-dimensional <math alttext="x"><mi>x</mi></math> values and projected them
    into a higher dimension, so that a linear fit can fit more complicated relationships
    between <math alttext="x"><mi>x</mi></math> and <math alttext="y"><mi>y</mi></math>
    .
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这仍然是 *线性模型* —— 线性指的是系数 <math alttext="a Subscript n"><msub><mi>a</mi> <mi>n</mi></msub></math>
    从不相乘或相除。我们所做的实质上是将我们的一维 <math alttext="x"><mi>x</mi></math> 值投影到更高的维度，这样线性拟合可以拟合
    <math alttext="x"><mi>x</mi></math> 和 <math alttext="y"><mi>y</mi></math> 之间更复杂的关系。
- en: Polynomial Basis Functions
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多项式基函数
- en: 'This polynomial projection is useful enough that it is built into Scikit-Learn,
    using the `PolynomialFeatures` transformer:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这种多项式投影非常有用，以至于它被内置到 Scikit-Learn 中，使用 `PolynomialFeatures` 变换器：
- en: '[PRE5]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We see here that the transformer has converted our one-dimensional array into
    a three-dimensional array, where each column contains the exponentiated value.
    This new, higher-dimensional data representation can then be plugged into a linear
    regression.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里看到，变换器已经将我们的一维数组转换为一个三维数组，其中每列包含了指数化的值。这种新的、更高维的数据表示可以被插入到线性回归中。
- en: 'As we saw in [Chapter 40](ch40.xhtml#section-0504-feature-engineering), the
    cleanest way to accomplish this is to use a pipeline. Let’s make a 7th-degree
    polynomial model in this way:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在第[40](ch40.xhtml#section-0504-feature-engineering)章中看到的，实现这一点的最干净的方法是使用一个流水线。让我们用这种方式制作一个7次多项式模型：
- en: '[PRE6]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: With this transform in place, we can use the linear model to fit much more complicated
    relationships between <math alttext="x"><mi>x</mi></math> and <math alttext="y"><mi>y</mi></math>
    . For example, here is a sine wave with noise (see [Figure 42-3](#fig_0506-linear-regression_files_in_output_19_0)).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![output 19 0](assets/output_19_0.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
- en: Figure 42-3\. A linear polynomial fit to nonlinear training data
  id: totrans-43
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Our linear model, through the use of seventh-order polynomial basis functions,
    can provide an excellent fit to this nonlinear data!
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: Gaussian Basis Functions
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Of course, other basis functions are possible. For example, one useful pattern
    is to fit a model that is not a sum of polynomial bases, but a sum of Gaussian
    bases. The result might look something like [Figure 42-4](#fig_images_in_0506-gaussian-basis).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: '![05.06 gaussian basis](assets/05.06-gaussian-basis.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
- en: Figure 42-4\. A Gaussian basis function fit to nonlinear data^([1](ch42.xhtml#idm45858736130864))
  id: totrans-48
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The shaded regions in the plot are the scaled basis functions, and when added
    together they reproduce the smooth curve through the data. These Gaussian basis
    functions are not built into Scikit-Learn, but we can write a custom transformer
    that will create them, as shown here and illustrated in [Figure 42-5](#fig_0506-linear-regression_files_in_output_24_0)
    (Scikit-Learn transformers are implemented as Python classes; reading Scikit-Learn’s
    source is a good way to see how they can be created):'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![output 24 0](assets/output_24_0.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
- en: Figure 42-5\. A Gaussian basis function fit computed with a custom transformer
  id: totrans-52
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'I’ve included this example just to make clear that there is nothing magic about
    polynomial basis functions: if you have some sort of intuition into the generating
    process of your data that makes you think one basis or another might be appropriate,
    you can use that instead.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: Regularization
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The introduction of basis functions into our linear regression makes the model
    much more flexible, but it also can very quickly lead to overfitting (refer back
    to [Chapter 39](ch39.xhtml#section-0503-hyperparameters-and-model-validation)
    for a discussion of this). For example, [Figure 42-6](#fig_0506-linear-regression_files_in_output_27_0)
    shows what happens if we use a large number of Gaussian basis functions:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![output 27 0](assets/output_27_0.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
- en: Figure 42-6\. An overly complex basis function model that overfits the data
  id: totrans-58
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: With the data projected to the 30-dimensional basis, the model has far too much
    flexibility and goes to extreme values between locations where it is constrained
    by data. We can see the reason for this if we plot the coefficients of the Gaussian
    bases with respect to their locations, as shown in [Figure 42-7](#fig_0506-linear-regression_files_in_output_29_0).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![output 29 0](assets/output_29_0.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
- en: Figure 42-7\. The coefficients of the Gaussian bases in the overly complex model
  id: totrans-62
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The lower panel of this figure shows the amplitude of the basis function at
    each location. This is typical overfitting behavior when basis functions overlap:
    the coefficients of adjacent basis functions blow up and cancel each other out.
    We know that such behavior is problematic, and it would be nice if we could limit
    such spikes explicitly in the model by penalizing large values of the model parameters.
    Such a penalty is known as *regularization*, and comes in several forms.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 此图的下部面板显示了每个位置的基函数的幅度。当基函数重叠时，这是典型的过拟合行为：相邻基函数的系数会急剧增加并相互抵消。我们知道这种行为是有问题的，如果我们可以通过惩罚模型参数的大值来明确限制这样的峰值，那就太好了。这样的惩罚被称为*正则化*，有几种形式。
- en: Ridge Regression (L[2] Regularization)
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 岭回归（L\[2\]正则化）
- en: 'Perhaps the most common form of regularization is known as *ridge regression*
    or <math alttext="upper L 2"><msub><mi>L</mi> <mn>2</mn></msub></math> *regularization*
    (sometimes also called *Tikhonov regularization*). This proceeds by penalizing
    the sum of squares (2-norms) of the model coefficients <math alttext="theta Subscript
    n"><msub><mi>θ</mi> <mi>n</mi></msub></math> . In this case, the penalty on the
    model fit would be:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 或许最常见的正则化形式被称为*岭回归*或<math alttext="upper L 2"><msub><mi>L</mi> <mn>2</mn></msub></math>
    *正则化*（有时也称为*Tikhonov正则化*）。这通过对模型系数<math alttext="theta Subscript n"><msub><mi>θ</mi>
    <mi>n</mi></msub></math>的平方和（2-范数）进行惩罚来实现。在这种情况下，模型拟合的惩罚将是：
- en: <math alttext="upper P equals alpha sigma-summation Underscript n equals 1 Overscript
    upper N Endscripts theta Subscript n Superscript 2" display="block"><mrow><mi>P</mi>
    <mo>=</mo> <mi>α</mi> <munderover><mo>∑</mo> <mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>N</mi></munderover> <msubsup><mi>θ</mi> <mi>n</mi> <mn>2</mn></msubsup></mrow></math>
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper P equals alpha sigma-summation Underscript n equals 1 Overscript
    upper N Endscripts theta Subscript n Superscript 2" display="block"><mrow><mi>P</mi>
    <mo>=</mo> <mi>α</mi> <munderover><mo>∑</mo> <mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>N</mi></munderover> <msubsup><mi>θ</mi> <mi>n</mi> <mn>2</mn></msubsup></mrow></math>
- en: where <math alttext="alpha"><mi>α</mi></math> is a free parameter that controls
    the strength of the penalty. This type of penalized model is built into Scikit-Learn
    with the `Ridge` estimator (see [Figure 42-8](#fig_0506-linear-regression_files_in_output_32_0)).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 其中<math alttext="alpha"><mi>α</mi></math>是一个自由参数，用于控制惩罚的强度。这种类型的惩罚模型已经内置到Scikit-Learn中的`Ridge`估计器中（参见[Figure 42-8](#fig_0506-linear-regression_files_in_output_32_0)）。
- en: '[PRE11]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '![output 32 0](assets/output_32_0.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![output 32 0](assets/output_32_0.png)'
- en: Figure 42-8\. Ridge (L[2]) regularization applied to the overly complex model
    (compare to [Figure 42-7](#fig_0506-linear-regression_files_in_output_29_0))
  id: totrans-70
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 42-8\. 岭（L\[2\]）正则化应用于过度复杂的模型（与[Figure 42-7](#fig_0506-linear-regression_files_in_output_29_0)进行比较）
- en: The <math alttext="alpha"><mi>α</mi></math> parameter is essentially a knob
    controlling the complexity of the resulting model. In the limit <math alttext="alpha
    right-arrow 0"><mrow><mi>α</mi> <mo>→</mo> <mn>0</mn></mrow></math> , we recover
    the standard linear regression result; in the limit <math alttext="alpha right-arrow
    normal infinity"><mrow><mi>α</mi> <mo>→</mo> <mi>∞</mi></mrow></math> , all model
    responses will be suppressed. One advantage of ridge regression in particular
    is that it can be computed very efficiently—at hardly more computational cost
    than the original linear regression model.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 参数<math alttext="alpha"><mi>α</mi></math>本质上是一个控制生成模型复杂性的旋钮。在极限<math alttext="alpha
    right-arrow 0"><mrow><mi>α</mi> <mo>→</mo> <mn>0</mn></mrow></math>中，我们恢复了标准线性回归结果；在极限<math
    alttext="alpha right-arrow normal infinity"><mrow><mi>α</mi> <mo>→</mo> <mi>∞</mi></mrow></math>中，所有模型响应都将被抑制。岭回归的一个优点是它特别高效地计算—几乎没有比原始线性回归模型更多的计算成本。
- en: Lasso Regression (L[1] Regularization)
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 套索回归（L\[1\]正则化）
- en: 'Another common type of regularization is known as *lasso regression* or *L[1]
    regularization* and involves penalizing the sum of absolute values (1-norms) of
    regression coefficients:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种常见的正则化方法被称为*套索回归*或*L\[1\]正则化*，它涉及对回归系数的绝对值（1-范数）的惩罚：
- en: <math alttext="upper P equals alpha sigma-summation Underscript n equals 1 Overscript
    upper N Endscripts StartAbsoluteValue theta Subscript n Baseline EndAbsoluteValue"
    display="block"><mrow><mi>P</mi> <mo>=</mo> <mi>α</mi> <munderover><mo>∑</mo>
    <mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow> <mi>N</mi></munderover> <mrow><mo>|</mo>
    <msub><mi>θ</mi> <mi>n</mi></msub> <mo>|</mo></mrow></mrow></math>
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper P equals alpha sigma-summation Underscript n equals 1 Overscript
    upper N Endscripts StartAbsoluteValue theta Subscript n Baseline EndAbsoluteValue"
    display="block"><mrow><mi>P</mi> <mo>=</mo> <mi>α</mi> <munderover><mo>∑</mo>
    <mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow> <mi>N</mi></munderover> <mrow><mo>|</mo>
    <msub><mi>θ</mi> <mi>n</mi></msub> <mo>|</mo></mrow></mrow></math>
- en: 'Though this is conceptually very similar to ridge regression, the results can
    differ surprisingly. For example, due to its construction, lasso regression tends
    to favor *sparse models* where possible: that is, it preferentially sets many
    model coefficients to exactly zero.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这在概念上与岭回归非常相似，但结果可能出奇地不同。例如，由于其构造，套索回归倾向于偏爱可能的*稀疏模型*：也就是说，它更倾向于将许多模型系数设为零。
- en: We can see this behavior if we duplicate the previous example using <math alttext="upper
    L 1"><msub><mi>L</mi> <mn>1</mn></msub></math> -normalized coefficients (see [Figure 42-9](#fig_0506-linear-regression_files_in_output_35_0)).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用<math alttext="upper L 1"><msub><mi>L</mi> <mn>1</mn></msub></math> -归一化系数复制前面的示例，我们可以看到这种行为（参见[Figure 42-9](#fig_0506-linear-regression_files_in_output_35_0)）。
- en: '[PRE12]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '![output 35 0](assets/output_35_0.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![output 35 0](assets/output_35_0.png)'
- en: Figure 42-9\. Lasso (L[1]) regularization applied to the overly complex model
    (compare to [Figure 42-8](#fig_0506-linear-regression_files_in_output_32_0))
  id: totrans-79
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 42-9\. 套索（L\[1\]）正则化应用于过度复杂的模型（与[Figure 42-8](#fig_0506-linear-regression_files_in_output_32_0)进行比较）
- en: With the lasso regression penalty, the majority of the coefficients are exactly
    zero, with the functional behavior being modeled by a small subset of the available
    basis functions. As with ridge regularization, the <math alttext="alpha"><mi>α</mi></math>
    parameter tunes the strength of the penalty and should be determined via, for
    example, cross-validation (refer back to [Chapter 39](ch39.xhtml#section-0503-hyperparameters-and-model-validation)
    for a discussion of this).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 使用套索回归惩罚，大多数系数确实为零，功能行为由可用基函数的一小部分建模。与岭回归一样，<math alttext="alpha"><mi>α</mi></math>参数调节惩罚的强度，应通过例如交叉验证确定（请参阅[第39章](ch39.xhtml#section-0503-hyperparameters-and-model-validation)讨论此问题）。
- en: 'Example: Predicting Bicycle Traffic'
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 示例：预测自行车流量
- en: As an example, let’s take a look at whether we can predict the number of bicycle
    trips across Seattle’s Fremont Bridge based on weather, season, and other factors.
    We already saw this data in [Chapter 23](ch23.xhtml#section-0311-working-with-time-series),
    but here we will join the bike data with another dataset and try to determine
    the extent to which weather and seasonal factors—temperature, precipitation, and
    daylight hours—affect the volume of bicycle traffic through this corridor. Fortunately,
    the National Oceanic and Atmospheric Administration (NOAA) makes its daily [weather
    station data](https://oreil.ly/sE5zO) available—I used station ID USW00024233—and
    we can easily use Pandas to join the two data sources. We will perform a simple
    linear regression to relate weather and other information to bicycle counts, in
    order to estimate how a change in any one of these parameters affects the number
    of riders on a given day.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 举个例子，让我们看看是否能够预测西雅图弗里蒙特桥上的自行车出行次数，基于天气、季节和其他因素。我们在[第23章](ch23.xhtml#section-0311-working-with-time-series)已经看过这些数据，但在这里我们将自行车数据与另一个数据集结合，并尝试确定天气和季节因素——温度、降水和日照小时——对该走廊自行车流量的影响程度。幸运的是，美国国家海洋和大气管理局（NOAA）提供其日常[气象站数据](https://oreil.ly/sE5zO)——我使用的是站点ID
    USW00024233——我们可以轻松使用Pandas将这两个数据源连接起来。我们将执行简单的线性回归，将天气和其他信息与自行车计数关联起来，以估算这些参数中的任何变化如何影响给定日的骑行者数量。
- en: In particular, this is an example of how the tools of Scikit-Learn can be used
    in a statistical modeling framework, in which the parameters of the model are
    assumed to have interpretable meaning. As discussed previously, this is not a
    standard approach within machine learning, but such interpretation is possible
    for some models.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是，这是Scikit-Learn工具如何在统计建模框架中使用的示例，其中假定模型的参数具有可解释的含义。正如前面讨论的那样，这不是机器学习中的标准方法，但对于某些模型是可能的。
- en: 'Let’s start by loading the two datasets, indexing by date:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从加载两个数据集开始，以日期为索引：
- en: '[PRE13]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'For simplicity, let’s look at data prior to 2020 in order to avoid the effects
    of the COVID-19 pandemic, which significantly affected commuting patterns in Seattle:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 为简单起见，让我们查看2020年之前的数据，以避免新冠肺炎大流行的影响，这显著影响了西雅图的通勤模式：
- en: '[PRE15]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Next we will compute the total daily bicycle traffic, and put this in its own
    `DataFrame`:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们将计算每日自行车总流量，并将其放入独立的`DataFrame`中：
- en: '[PRE16]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We saw previously that the patterns of use generally vary from day to day.
    Let’s account for this in our data by adding binary columns that indicate the
    day of the week:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前看到使用模式通常从一天到另一天有所不同。让我们在数据中考虑这一点，通过添加指示星期几的二进制列：
- en: '[PRE17]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Similarly, we might expect riders to behave differently on holidays; let’s
    add an indicator of this as well:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们可能期待骑行者在假日有不同的行为表现；让我们也加上一个指示器：
- en: '[PRE18]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: We also might suspect that the hours of daylight would affect how many people
    ride. Let’s use the standard astronomical calculation to add this information
    (see [Figure 42-10](#fig_0506-linear-regression_files_in_output_50_1)).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可能怀疑白天的小时数会影响骑行的人数。让我们使用标准的天文计算来添加这些信息（参见[图 42-10](#fig_0506-linear-regression_files_in_output_50_1)）。
- en: '[PRE19]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '![output 50 1](assets/output_50_1.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![output 50 1](assets/output_50_1.png)'
- en: Figure 42-10\. Visualization of hours of daylight in Seattle
  id: totrans-98
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 42-10\. 西雅图的日照小时可视化
- en: 'We can also add the average temperature and total precipitation to the data.
    In addition to the inches of precipitation, let’s add a flag that indicates whether
    a day is dry (has zero precipitation):'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以将平均温度和总降水量添加到数据中。除了降水英寸外，让我们添加一个指示某一天是否干燥（降水量为零）的标志：
- en: '[PRE20]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Finally, let’s add a counter that increases from day 1, and measures how many
    years have passed. This will let us measure any observed annual increase or decrease
    in daily crossings:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Now that our data is in order, and we can take a look at it:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'With this in place, we can choose the columns to use, and fit a linear regression
    model to our data. We will set `fit_intercept=False`, because the daily flags
    essentially operate as their own day-specific intercepts:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Finally, we can compare the total and predicted bicycle traffic visually (see
    [Figure 42-11](#fig_0506-linear-regression_files_in_output_60_0)).
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '![output 60 0](assets/output_60_0.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
- en: Figure 42-11\. Our model’s prediction of bicycle traffic
  id: totrans-110
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'From the fact that the data and model predictions don’t line up exactly, it
    is evident that we have missed some key features. Either our features are not
    complete (i.e., people decide whether to ride to work based on more than just
    these features), or there are some nonlinear relationships that we have failed
    to take into account (e.g., perhaps people ride less at both high and low temperatures).
    Nevertheless, our rough approximation is enough to give us some insights, and
    we can take a look at the coefficients of the linear model to estimate how much
    each feature contributes to the daily bicycle count:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'These numbers are difficult to interpret without some measure of their uncertainty.
    We can compute these uncertainties quickly using bootstrap resamplings of the
    data:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'With these errors estimated, let’s again look at the results:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The `effect` column here, roughly speaking, shows how the number of riders
    is affected by a change of the feature in question. For example, there is a clear
    divide when it comes to the day of the week: there are thousands fewer riders
    on weekends than on weekdays. We also see that for each additional hour of daylight,
    409 ± 26 more people choose to ride; a temperature increase of one degree Fahrenheit
    encourages 179 ± 7 people to grab their bicycle; a dry day means an average of
    2,111 ± 101 more riders, and every inch of rainfall leads 2,790 ± 186 riders to
    choose another mode of transport. Once all these effects are accounted for, we
    see a modest increase of 324 ± 22 new daily riders each year.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: Our simple model is almost certainly missing some relevant information. For
    example, as mentioned earlier, nonlinear effects (such as effects of precipitation
    *and* cold temperature) and nonlinear trends within each variable (such as disinclination
    to ride at very cold and very hot temperatures) cannot be accounted for in a simple
    linear model. Additionally, we have thrown away some of the finer-grained information
    (such as the difference between a rainy morning and a rainy afternoon), and we
    have ignored correlations between days (such as the possible effect of a rainy
    Tuesday on Wednesday’s numbers, or the effect of an unexpected sunny day after
    a streak of rainy days). These are all potentially interesting effects, and you
    now have the tools to begin exploring them if you wish!
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的简单模型几乎肯定会缺少一些相关信息。例如，正如前面提到的，非线性效应（如降水和寒冷温度的影响）以及每个变量内的非线性趋势（例如对极冷和极热温度不愿意骑车的影响）无法在简单的线性模型中考虑。此外，我们还丢弃了一些更精细的信息（例如雨天早晨和雨天下午之间的差异），并且忽略了天之间的相关性（例如一个雨天星期二对星期三的可能影响，或者在连续多日雨天后出现意外晴天的影响）。这些都是潜在的有趣效应，如果你愿意，现在你已经有了开始探索它们的工具！
- en: ^([1](ch42.xhtml#idm45858736130864-marker)) Code to produce this figure can
    be found in the [online appendix](https://oreil.ly/o1Zya).
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch42.xhtml#idm45858736130864-marker)) 生成这个图表的代码可以在[在线附录](https://oreil.ly/o1Zya)中找到。
