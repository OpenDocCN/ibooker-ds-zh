<html><head></head><body><section data-pdf-bookmark="Chapter 16. Logistic Regression" data-type="chapter" epub:type="chapter"><div class="chapter" id="logistic_regression">&#13;
<h1><span class="label">Chapter 16. </span>Logistic Regression</h1>&#13;
&#13;
<blockquote data-type="epigraph" epub:type="epigraph">&#13;
    <p>A lot of people say there’s a fine line between genius and insanity. I don’t think there’s a fine line, I actually think there’s a yawning gulf.</p>&#13;
    <p data-type="attribution">Bill Bailey</p>&#13;
</blockquote>&#13;
&#13;
<p>In <a data-type="xref" href="ch01.html#introduction">Chapter 1</a>, we briefly<a data-primary="predictive models" data-secondary="logistic regression" data-type="indexterm" id="PMlog16"/> looked at the problem of trying to predict which DataSciencester users paid for premium accounts.  Here we’ll revisit that problem.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="The Problem" data-type="sect1"><div class="sect1" id="idm45635736036904">&#13;
<h1>The Problem</h1>&#13;
&#13;
<p>We<a data-primary="logistic regression" data-secondary="problem example" data-type="indexterm" id="idm45635736035208"/> have an anonymized dataset of about 200 users, containing each user’s salary, her years of experience as a data scientist, and whether she paid for a premium account (<a data-type="xref" href="#logit_image">Figure 16-1</a>). As is typical with categorical variables, we represent the dependent variable as either 0 (no premium account) or 1 (premium account).</p>&#13;
&#13;
<p>As usual, our data is a list of rows&#13;
<code>[experience, salary, paid_account]</code>. Let’s turn it into the format we need:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="n">xs</code> <code class="o">=</code> <code class="p">[[</code><code class="mf">1.0</code><code class="p">]</code> <code class="o">+</code> <code class="n">row</code><code class="p">[:</code><code class="mi">2</code><code class="p">]</code> <code class="k">for</code> <code class="n">row</code> <code class="ow">in</code> <code class="n">data</code><code class="p">]</code>  <code class="c1"># [1, experience, salary]</code>&#13;
<code class="n">ys</code> <code class="o">=</code> <code class="p">[</code><code class="n">row</code><code class="p">[</code><code class="mi">2</code><code class="p">]</code> <code class="k">for</code> <code class="n">row</code> <code class="ow">in</code> <code class="n">data</code><code class="p">]</code>           <code class="c1"># paid_account</code></pre>&#13;
&#13;
<p>An obvious first attempt is to use linear regression and find the best model:</p>&#13;
<div data-type="equation">&#13;
<math alttext="paid account equals beta 0 plus beta 1 experience plus beta 2 salary plus epsilon" display="block">&#13;
  <mrow>&#13;
    <mtext>paid</mtext>&#13;
    <mspace width="4.pt"/>&#13;
    <mtext>account</mtext>&#13;
    <mo>=</mo>&#13;
    <msub><mi>β</mi> <mn>0</mn> </msub>&#13;
    <mo>+</mo>&#13;
    <msub><mi>β</mi> <mn>1</mn> </msub>&#13;
    <mtext>experience</mtext>&#13;
    <mo>+</mo>&#13;
    <msub><mi>β</mi> <mn>2</mn> </msub>&#13;
    <mtext>salary</mtext>&#13;
    <mo>+</mo>&#13;
    <mi>ε</mi>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
&#13;
<figure><div class="figure" id="logit_image">&#13;
<img alt="Paid and Unpaid Users." src="assets/dsf2_1601.png"/>&#13;
<h6><span class="label">Figure 16-1. </span>Paid and unpaid users</h6>&#13;
</div></figure>&#13;
&#13;
<p>And certainly, there’s nothing preventing us from modeling the problem this way. The results are shown in <a data-type="xref" href="#linear_regression_for_probabilities">Figure 16-2</a>:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="kn">from</code> <code class="nn">matplotlib</code> <code class="kn">import</code> <code class="n">pyplot</code> <code class="k">as</code> <code class="n">plt</code>&#13;
<code class="kn">from</code> <code class="nn">scratch.working_with_data</code> <code class="kn">import</code> <code class="n">rescale</code>&#13;
<code class="kn">from</code> <code class="nn">scratch.multiple_regression</code> <code class="kn">import</code> <code class="n">least_squares_fit</code><code class="p">,</code> <code class="n">predict</code>&#13;
<code class="kn">from</code> <code class="nn">scratch.gradient_descent</code> <code class="kn">import</code> <code class="n">gradient_step</code>&#13;
&#13;
<code class="n">learning_rate</code> <code class="o">=</code> <code class="mf">0.001</code>&#13;
<code class="n">rescaled_xs</code> <code class="o">=</code> <code class="n">rescale</code><code class="p">(</code><code class="n">xs</code><code class="p">)</code>&#13;
<code class="n">beta</code> <code class="o">=</code> <code class="n">least_squares_fit</code><code class="p">(</code><code class="n">rescaled_xs</code><code class="p">,</code> <code class="n">ys</code><code class="p">,</code> <code class="n">learning_rate</code><code class="p">,</code> <code class="mi">1000</code><code class="p">,</code> <code class="mi">1</code><code class="p">)</code>&#13;
<code class="c1"># [0.26, 0.43, -0.43]</code>&#13;
<code class="n">predictions</code> <code class="o">=</code> <code class="p">[</code><code class="n">predict</code><code class="p">(</code><code class="n">x_i</code><code class="p">,</code> <code class="n">beta</code><code class="p">)</code> <code class="k">for</code> <code class="n">x_i</code> <code class="ow">in</code> <code class="n">rescaled_xs</code><code class="p">]</code>&#13;
&#13;
<code class="n">plt</code><code class="o">.</code><code class="n">scatter</code><code class="p">(</code><code class="n">predictions</code><code class="p">,</code> <code class="n">ys</code><code class="p">)</code>&#13;
<code class="n">plt</code><code class="o">.</code><code class="n">xlabel</code><code class="p">(</code><code class="s2">"predicted"</code><code class="p">)</code>&#13;
<code class="n">plt</code><code class="o">.</code><code class="n">ylabel</code><code class="p">(</code><code class="s2">"actual"</code><code class="p">)</code>&#13;
<code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">()</code></pre>&#13;
&#13;
<figure><div class="figure" id="linear_regression_for_probabilities">&#13;
<img alt="Using Linear Regression to Predict Premium Accounts." src="assets/dsf2_1602.png"/>&#13;
<h6><span class="label">Figure 16-2. </span>Using linear regression to predict premium accounts</h6>&#13;
</div></figure>&#13;
&#13;
<p>But this approach leads to a couple of immediate problems:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>We’d like for our predicted outputs to be 0 or 1, to indicate class membership.&#13;
It’s fine if they’re between 0 and 1, since we can interpret these as probabilities—an output of 0.25 could mean 25% chance of being a paid member.&#13;
But the outputs of the linear model can be huge positive numbers or even negative numbers, which it’s not clear how to interpret.  Indeed, here a lot of our predictions were negative.</p>&#13;
</li>&#13;
<li>&#13;
<p>The linear regression model assumed that the errors were uncorrelated with the columns of <em>x</em>.&#13;
But here, the regression coefficient for <code>experience</code> is 0.43, indicating that&#13;
more experience leads to a greater likelihood of a premium account.&#13;
This means that our model outputs very large values for people with lots of experience.  But we know that the actual values must be at most 1, which means that necessarily very large outputs (and therefore very large values of <code>experience</code>) correspond to very large negative values of the error term.  Because this is the case, our estimate of <code>beta</code> is biased.</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>What we’d like instead is for large positive values of <code>dot(x_i, beta)</code> to correspond to probabilities close to 1,&#13;
and for large negative values to correspond to probabilities close to 0.  We can accomplish this by applying another function to the result.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="The Logistic Function" data-type="sect1"><div class="sect1" id="idm45635736036312">&#13;
<h1>The Logistic Function</h1>&#13;
&#13;
<p>In<a data-primary="logistic regression" data-secondary="logistic function" data-type="indexterm" id="idm45635735827016"/> the case of logistic regression, we use the <em>logistic function</em>, pictured in <a data-type="xref" href="#graph_of_logistic_function">Figure 16-3</a>:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="k">def</code> <code class="nf">logistic</code><code class="p">(</code><code class="n">x</code><code class="p">:</code> <code class="nb">float</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">float</code><code class="p">:</code>&#13;
    <code class="k">return</code> <code class="mf">1.0</code> <code class="o">/</code> <code class="p">(</code><code class="mi">1</code> <code class="o">+</code> <code class="n">math</code><code class="o">.</code><code class="n">exp</code><code class="p">(</code><code class="o">-</code><code class="n">x</code><code class="p">))</code></pre>&#13;
&#13;
<figure><div class="figure" id="graph_of_logistic_function">&#13;
<img alt="Logistic function." src="assets/dsf2_1603.png"/>&#13;
<h6><span class="label">Figure 16-3. </span>The logistic function</h6>&#13;
</div></figure>&#13;
&#13;
<p>As its input gets large and positive, it gets closer and closer to 1.  As its input gets large and negative, it gets closer and closer to 0.  Additionally, it has the convenient property that its derivative is given by:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="k">def</code> <code class="nf">logistic_prime</code><code class="p">(</code><code class="n">x</code><code class="p">:</code> <code class="nb">float</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">float</code><code class="p">:</code>&#13;
    <code class="n">y</code> <code class="o">=</code> <code class="n">logistic</code><code class="p">(</code><code class="n">x</code><code class="p">)</code>&#13;
    <code class="k">return</code> <code class="n">y</code> <code class="o">*</code> <code class="p">(</code><code class="mi">1</code> <code class="o">-</code> <code class="n">y</code><code class="p">)</code></pre>&#13;
&#13;
<p>which we’ll make use of in a bit.  We’ll use this to fit a model:</p>&#13;
<div data-type="equation">&#13;
<math alttext="y Subscript i Baseline equals f left-parenthesis x Subscript i Baseline beta right-parenthesis plus epsilon Subscript i" display="block">&#13;
  <mrow>&#13;
    <msub><mi>y</mi> <mi>i</mi> </msub>&#13;
    <mo>=</mo>&#13;
    <mi>f</mi>&#13;
    <mrow>&#13;
      <mo>(</mo>&#13;
      <msub><mi>x</mi> <mi>i</mi> </msub>&#13;
      <mi>β</mi>&#13;
      <mo>)</mo>&#13;
    </mrow>&#13;
    <mo>+</mo>&#13;
    <msub><mi>ε</mi> <mi>i</mi> </msub>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
&#13;
<p>where <em>f</em> is the <code>logistic</code> function.</p>&#13;
&#13;
<p>Recall that for linear regression we fit the model by minimizing the sum of squared errors,&#13;
which ended up choosing the <em>β</em> that maximized the likelihood of the data.</p>&#13;
&#13;
<p>Here the two aren’t equivalent, so we’ll use gradient descent to maximize the likelihood directly. This means we need to calculate the likelihood function and its gradient.</p>&#13;
&#13;
<p>Given some <em>β</em>, our model says that each <math>&#13;
  <msub><mi>y</mi> <mi>i</mi> </msub>&#13;
</math> should equal 1 with probability <math>&#13;
  <mrow>&#13;
    <mi>f</mi>&#13;
    <mo>(</mo>&#13;
    <msub><mi>x</mi> <mi>i</mi> </msub>&#13;
    <mi>β</mi>&#13;
    <mo>)</mo>&#13;
  </mrow>&#13;
</math> and 0 with probability <math>&#13;
  <mrow>&#13;
    <mn>1</mn>&#13;
    <mo>-</mo>&#13;
    <mi>f</mi>&#13;
    <mo>(</mo>&#13;
    <msub><mi>x</mi> <mi>i</mi> </msub>&#13;
    <mi>β</mi>&#13;
    <mo>)</mo>&#13;
  </mrow>&#13;
</math>.</p>&#13;
&#13;
<p>In particular, the PDF for <math>&#13;
  <msub><mi>y</mi> <mi>i</mi> </msub>&#13;
</math> can be written as:</p>&#13;
<div data-type="equation">&#13;
<math alttext="p left-parenthesis y Subscript i Baseline vertical-bar x Subscript i Baseline comma beta right-parenthesis equals f left-parenthesis x Subscript i Baseline beta right-parenthesis Superscript y Super Subscript i Baseline left-parenthesis 1 minus f left-parenthesis x Subscript i Baseline beta right-parenthesis right-parenthesis Superscript 1 minus y Super Subscript i" display="block">&#13;
  <mrow>&#13;
    <mi>p</mi>&#13;
    <mrow>&#13;
      <mo>(</mo>&#13;
      <msub><mi>y</mi> <mi>i</mi> </msub>&#13;
      <mo>|</mo>&#13;
      <msub><mi>x</mi> <mi>i</mi> </msub>&#13;
      <mo>,</mo>&#13;
      <mi>β</mi>&#13;
      <mo>)</mo>&#13;
    </mrow>&#13;
    <mo>=</mo>&#13;
    <mi>f</mi>&#13;
    <msup><mrow><mo>(</mo><msub><mi>x</mi> <mi>i</mi> </msub><mi>β</mi><mo>)</mo></mrow> <msub><mi>y</mi> <mi>i</mi> </msub> </msup>&#13;
    <msup><mrow><mo>(</mo><mn>1</mn><mo>-</mo><mi>f</mi><mrow><mo>(</mo><msub><mi>x</mi> <mi>i</mi> </msub><mi>β</mi><mo>)</mo></mrow><mo>)</mo></mrow> <mrow><mn>1</mn><mo>-</mo><msub><mi>y</mi> <mi>i</mi> </msub></mrow> </msup>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
&#13;
<p>since if <math>&#13;
  <msub><mi>y</mi> <mi>i</mi> </msub>&#13;
</math> is 0, this equals:</p>&#13;
<div data-type="equation">&#13;
<math alttext="1 minus f left-parenthesis x Subscript i Baseline beta right-parenthesis" display="block">&#13;
  <mrow>&#13;
    <mn>1</mn>&#13;
    <mo>-</mo>&#13;
    <mi>f</mi>&#13;
    <mo>(</mo>&#13;
    <msub><mi>x</mi> <mi>i</mi> </msub>&#13;
    <mi>β</mi>&#13;
    <mo>)</mo>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
&#13;
<p>and if <math>&#13;
  <msub><mi>y</mi> <mi>i</mi> </msub>&#13;
</math> is 1, it equals:</p>&#13;
<div data-type="equation">&#13;
<math alttext="f left-parenthesis x Subscript i Baseline beta right-parenthesis" display="block">&#13;
  <mrow>&#13;
    <mi>f</mi>&#13;
    <mo>(</mo>&#13;
    <msub><mi>x</mi> <mi>i</mi> </msub>&#13;
    <mi>β</mi>&#13;
    <mo>)</mo>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
&#13;
<p>It turns out that it’s actually simpler to maximize the <em>log likelihood</em>:</p>&#13;
<div data-type="equation">&#13;
<math alttext="log upper L left-parenthesis beta vertical-bar x Subscript i Baseline comma y Subscript i Baseline right-parenthesis equals y Subscript i Baseline log f left-parenthesis x Subscript i Baseline beta right-parenthesis plus left-parenthesis 1 minus y Subscript i Baseline right-parenthesis log left-parenthesis 1 minus f left-parenthesis x Subscript i Baseline beta right-parenthesis right-parenthesis" display="block">&#13;
  <mrow>&#13;
    <mo form="prefix">log</mo>&#13;
    <mi>L</mi>&#13;
    <mrow>&#13;
      <mo>(</mo>&#13;
      <mi>β</mi>&#13;
      <mo>|</mo>&#13;
      <msub><mi>x</mi> <mi>i</mi> </msub>&#13;
      <mo>,</mo>&#13;
      <msub><mi>y</mi> <mi>i</mi> </msub>&#13;
      <mo>)</mo>&#13;
    </mrow>&#13;
    <mo>=</mo>&#13;
    <msub><mi>y</mi> <mi>i</mi> </msub>&#13;
    <mo form="prefix">log</mo>&#13;
    <mi>f</mi>&#13;
    <mrow>&#13;
      <mo>(</mo>&#13;
      <msub><mi>x</mi> <mi>i</mi> </msub>&#13;
      <mi>β</mi>&#13;
      <mo>)</mo>&#13;
    </mrow>&#13;
    <mo>+</mo>&#13;
    <mrow>&#13;
      <mo>(</mo>&#13;
      <mn>1</mn>&#13;
      <mo>-</mo>&#13;
      <msub><mi>y</mi> <mi>i</mi> </msub>&#13;
      <mo>)</mo>&#13;
    </mrow>&#13;
    <mo form="prefix">log</mo>&#13;
    <mrow>&#13;
      <mo>(</mo>&#13;
      <mn>1</mn>&#13;
      <mo>-</mo>&#13;
      <mi>f</mi>&#13;
      <mrow>&#13;
        <mo>(</mo>&#13;
        <msub><mi>x</mi> <mi>i</mi> </msub>&#13;
        <mi>β</mi>&#13;
        <mo>)</mo>&#13;
      </mrow>&#13;
      <mo>)</mo>&#13;
    </mrow>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
&#13;
<p>Because log is a strictly increasing function, any <code>beta</code> that maximizes the log likelihood also maximizes the likelihood, and vice versa. Because gradient descent minimizes things, we’ll actually work with the <em>negative</em> log likelihood, since maximizing the likelihood is the same as minimizing its negative:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="kn">import</code> <code class="nn">math</code>&#13;
<code class="kn">from</code> <code class="nn">scratch.linear_algebra</code> <code class="kn">import</code> <code class="n">Vector</code><code class="p">,</code> <code class="n">dot</code>&#13;
&#13;
<code class="k">def</code> <code class="nf">_negative_log_likelihood</code><code class="p">(</code><code class="n">x</code><code class="p">:</code> <code class="n">Vector</code><code class="p">,</code> <code class="n">y</code><code class="p">:</code> <code class="nb">float</code><code class="p">,</code> <code class="n">beta</code><code class="p">:</code> <code class="n">Vector</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">float</code><code class="p">:</code>&#13;
    <code class="sd">"""The negative log likelihood for one data point"""</code>&#13;
    <code class="k">if</code> <code class="n">y</code> <code class="o">==</code> <code class="mi">1</code><code class="p">:</code>&#13;
        <code class="k">return</code> <code class="o">-</code><code class="n">math</code><code class="o">.</code><code class="n">log</code><code class="p">(</code><code class="n">logistic</code><code class="p">(</code><code class="n">dot</code><code class="p">(</code><code class="n">x</code><code class="p">,</code> <code class="n">beta</code><code class="p">)))</code>&#13;
    <code class="k">else</code><code class="p">:</code>&#13;
        <code class="k">return</code> <code class="o">-</code><code class="n">math</code><code class="o">.</code><code class="n">log</code><code class="p">(</code><code class="mi">1</code> <code class="o">-</code> <code class="n">logistic</code><code class="p">(</code><code class="n">dot</code><code class="p">(</code><code class="n">x</code><code class="p">,</code> <code class="n">beta</code><code class="p">)))</code></pre>&#13;
&#13;
<p>If we assume different data points are independent from one another,&#13;
the overall likelihood is just the product of the individual likelihoods.&#13;
That means the overall log likelihood is the sum of the individual log likelihoods:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="kn">from</code> <code class="nn">typing</code> <code class="kn">import</code> <code class="n">List</code>&#13;
&#13;
<code class="k">def</code> <code class="nf">negative_log_likelihood</code><code class="p">(</code><code class="n">xs</code><code class="p">:</code> <code class="n">List</code><code class="p">[</code><code class="n">Vector</code><code class="p">],</code>&#13;
                            <code class="n">ys</code><code class="p">:</code> <code class="n">List</code><code class="p">[</code><code class="nb">float</code><code class="p">],</code>&#13;
                            <code class="n">beta</code><code class="p">:</code> <code class="n">Vector</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">float</code><code class="p">:</code>&#13;
    <code class="k">return</code> <code class="nb">sum</code><code class="p">(</code><code class="n">_negative_log_likelihood</code><code class="p">(</code><code class="n">x</code><code class="p">,</code> <code class="n">y</code><code class="p">,</code> <code class="n">beta</code><code class="p">)</code>&#13;
               <code class="k">for</code> <code class="n">x</code><code class="p">,</code> <code class="n">y</code> <code class="ow">in</code> <code class="nb">zip</code><code class="p">(</code><code class="n">xs</code><code class="p">,</code> <code class="n">ys</code><code class="p">))</code></pre>&#13;
&#13;
<p>A little bit of calculus gives us the gradient:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="kn">from</code> <code class="nn">scratch.linear_algebra</code> <code class="kn">import</code> <code class="n">vector_sum</code>&#13;
&#13;
<code class="k">def</code> <code class="nf">_negative_log_partial_j</code><code class="p">(</code><code class="n">x</code><code class="p">:</code> <code class="n">Vector</code><code class="p">,</code> <code class="n">y</code><code class="p">:</code> <code class="nb">float</code><code class="p">,</code> <code class="n">beta</code><code class="p">:</code> <code class="n">Vector</code><code class="p">,</code> <code class="n">j</code><code class="p">:</code> <code class="nb">int</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">float</code><code class="p">:</code>&#13;
    <code class="sd">"""</code>&#13;
<code class="sd">    The jth partial derivative for one data point.</code>&#13;
<code class="sd">    Here i is the index of the data point.</code>&#13;
<code class="sd">    """</code>&#13;
    <code class="k">return</code> <code class="o">-</code><code class="p">(</code><code class="n">y</code> <code class="o">-</code> <code class="n">logistic</code><code class="p">(</code><code class="n">dot</code><code class="p">(</code><code class="n">x</code><code class="p">,</code> <code class="n">beta</code><code class="p">)))</code> <code class="o">*</code> <code class="n">x</code><code class="p">[</code><code class="n">j</code><code class="p">]</code>&#13;
&#13;
<code class="k">def</code> <code class="nf">_negative_log_gradient</code><code class="p">(</code><code class="n">x</code><code class="p">:</code> <code class="n">Vector</code><code class="p">,</code> <code class="n">y</code><code class="p">:</code> <code class="nb">float</code><code class="p">,</code> <code class="n">beta</code><code class="p">:</code> <code class="n">Vector</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="n">Vector</code><code class="p">:</code>&#13;
    <code class="sd">"""</code>&#13;
<code class="sd">    The gradient for one data point.</code>&#13;
<code class="sd">    """</code>&#13;
    <code class="k">return</code> <code class="p">[</code><code class="n">_negative_log_partial_j</code><code class="p">(</code><code class="n">x</code><code class="p">,</code> <code class="n">y</code><code class="p">,</code> <code class="n">beta</code><code class="p">,</code> <code class="n">j</code><code class="p">)</code>&#13;
            <code class="k">for</code> <code class="n">j</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="nb">len</code><code class="p">(</code><code class="n">beta</code><code class="p">))]</code>&#13;
&#13;
<code class="k">def</code> <code class="nf">negative_log_gradient</code><code class="p">(</code><code class="n">xs</code><code class="p">:</code> <code class="n">List</code><code class="p">[</code><code class="n">Vector</code><code class="p">],</code>&#13;
                          <code class="n">ys</code><code class="p">:</code> <code class="n">List</code><code class="p">[</code><code class="nb">float</code><code class="p">],</code>&#13;
                          <code class="n">beta</code><code class="p">:</code> <code class="n">Vector</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="n">Vector</code><code class="p">:</code>&#13;
    <code class="k">return</code> <code class="n">vector_sum</code><code class="p">([</code><code class="n">_negative_log_gradient</code><code class="p">(</code><code class="n">x</code><code class="p">,</code> <code class="n">y</code><code class="p">,</code> <code class="n">beta</code><code class="p">)</code>&#13;
                       <code class="k">for</code> <code class="n">x</code><code class="p">,</code> <code class="n">y</code> <code class="ow">in</code> <code class="nb">zip</code><code class="p">(</code><code class="n">xs</code><code class="p">,</code> <code class="n">ys</code><code class="p">)])</code></pre>&#13;
&#13;
<p>at which point we have all the pieces we need.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Applying the Model" data-type="sect1"><div class="sect1" id="idm45635735828120">&#13;
<h1>Applying the Model</h1>&#13;
&#13;
<p>We’ll<a data-primary="logistic regression" data-secondary="model application" data-type="indexterm" id="idm45635735355496"/> want to split our data into a training set and a test set:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="kn">from</code> <code class="nn">scratch.machine_learning</code> <code class="kn">import</code> <code class="n">train_test_split</code>&#13;
<code class="kn">import</code> <code class="nn">random</code>&#13;
<code class="kn">import</code> <code class="nn">tqdm</code>&#13;
&#13;
<code class="n">random</code><code class="o">.</code><code class="n">seed</code><code class="p">(</code><code class="mi">0</code><code class="p">)</code>&#13;
<code class="n">x_train</code><code class="p">,</code> <code class="n">x_test</code><code class="p">,</code> <code class="n">y_train</code><code class="p">,</code> <code class="n">y_test</code> <code class="o">=</code> <code class="n">train_test_split</code><code class="p">(</code><code class="n">rescaled_xs</code><code class="p">,</code> <code class="n">ys</code><code class="p">,</code> <code class="mf">0.33</code><code class="p">)</code>&#13;
&#13;
<code class="n">learning_rate</code> <code class="o">=</code> <code class="mf">0.01</code>&#13;
&#13;
<code class="c1"># pick a random starting point</code>&#13;
<code class="n">beta</code> <code class="o">=</code> <code class="p">[</code><code class="n">random</code><code class="o">.</code><code class="n">random</code><code class="p">()</code> <code class="k">for</code> <code class="n">_</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">3</code><code class="p">)]</code>&#13;
&#13;
<code class="k">with</code> <code class="n">tqdm</code><code class="o">.</code><code class="n">trange</code><code class="p">(</code><code class="mi">5000</code><code class="p">)</code> <code class="k">as</code> <code class="n">t</code><code class="p">:</code>&#13;
    <code class="k">for</code> <code class="n">epoch</code> <code class="ow">in</code> <code class="n">t</code><code class="p">:</code>&#13;
        <code class="n">gradient</code> <code class="o">=</code> <code class="n">negative_log_gradient</code><code class="p">(</code><code class="n">x_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">,</code> <code class="n">beta</code><code class="p">)</code>&#13;
        <code class="n">beta</code> <code class="o">=</code> <code class="n">gradient_step</code><code class="p">(</code><code class="n">beta</code><code class="p">,</code> <code class="n">gradient</code><code class="p">,</code> <code class="o">-</code><code class="n">learning_rate</code><code class="p">)</code>&#13;
        <code class="n">loss</code> <code class="o">=</code> <code class="n">negative_log_likelihood</code><code class="p">(</code><code class="n">x_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">,</code> <code class="n">beta</code><code class="p">)</code>&#13;
        <code class="n">t</code><code class="o">.</code><code class="n">set_description</code><code class="p">(</code><code class="n">f</code><code class="s2">"loss: {loss:.3f} beta: {beta}"</code><code class="p">)</code></pre>&#13;
&#13;
<p>after which we find that <code>beta</code> is approximately:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="p">[</code><code class="o">-</code><code class="mf">2.0</code><code class="p">,</code> <code class="mf">4.7</code><code class="p">,</code> <code class="o">-</code><code class="mf">4.5</code><code class="p">]</code></pre>&#13;
&#13;
<p>These are coefficients for the <code>rescale</code>d data, but we can transform them back to the original data as well:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="kn">from</code> <code class="nn">scratch.working_with_data</code> <code class="kn">import</code> <code class="n">scale</code>&#13;
&#13;
<code class="n">means</code><code class="p">,</code> <code class="n">stdevs</code> <code class="o">=</code> <code class="n">scale</code><code class="p">(</code><code class="n">xs</code><code class="p">)</code>&#13;
<code class="n">beta_unscaled</code> <code class="o">=</code> <code class="p">[(</code><code class="n">beta</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code>&#13;
                  <code class="o">-</code> <code class="n">beta</code><code class="p">[</code><code class="mi">1</code><code class="p">]</code> <code class="o">*</code> <code class="n">means</code><code class="p">[</code><code class="mi">1</code><code class="p">]</code> <code class="o">/</code> <code class="n">stdevs</code><code class="p">[</code><code class="mi">1</code><code class="p">]</code>&#13;
                  <code class="o">-</code> <code class="n">beta</code><code class="p">[</code><code class="mi">2</code><code class="p">]</code> <code class="o">*</code> <code class="n">means</code><code class="p">[</code><code class="mi">2</code><code class="p">]</code> <code class="o">/</code> <code class="n">stdevs</code><code class="p">[</code><code class="mi">2</code><code class="p">]),</code>&#13;
                 <code class="n">beta</code><code class="p">[</code><code class="mi">1</code><code class="p">]</code> <code class="o">/</code> <code class="n">stdevs</code><code class="p">[</code><code class="mi">1</code><code class="p">],</code>&#13;
                 <code class="n">beta</code><code class="p">[</code><code class="mi">2</code><code class="p">]</code> <code class="o">/</code> <code class="n">stdevs</code><code class="p">[</code><code class="mi">2</code><code class="p">]]</code>&#13;
<code class="c1"># [8.9, 1.6, -0.000288]</code></pre>&#13;
&#13;
<p>Unfortunately, these are not as easy to interpret as linear regression coefficients.  All else being equal, an extra year of experience adds 1.6 to the input of <code>logistic</code>.  All else being equal, an extra $10,000 of salary subtracts 2.88 from the input of <code>logistic</code>.</p>&#13;
&#13;
<p>The impact on the output, however, depends on the other inputs as well.  If <code>dot(beta, x_i)</code> is already large (corresponding to a probability close to 1), increasing it even by a lot cannot affect the probability very much.  If it’s close to 0, increasing it just a little might increase the probability quite a bit.</p>&#13;
&#13;
<p>What we can say is that—all else being equal—people with more experience are more likely to pay for accounts.  And that—all else being equal—people with higher salaries are less likely to pay for accounts.  (This was also somewhat apparent when we plotted the data.)</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Goodness of Fit" data-type="sect1"><div class="sect1" id="idm45635735356376">&#13;
<h1>Goodness of Fit</h1>&#13;
&#13;
<p>We<a data-primary="logistic regression" data-secondary="goodness of fit" data-type="indexterm" id="idm45635734937176"/> haven’t yet used the test data that we held out.  Let’s see what happens if we predict <em>paid account</em> whenever the probability exceeds 0.5:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="n">true_positives</code> <code class="o">=</code> <code class="n">false_positives</code> <code class="o">=</code> <code class="n">true_negatives</code> <code class="o">=</code> <code class="n">false_negatives</code> <code class="o">=</code> <code class="mi">0</code>&#13;
&#13;
<code class="k">for</code> <code class="n">x_i</code><code class="p">,</code> <code class="n">y_i</code> <code class="ow">in</code> <code class="nb">zip</code><code class="p">(</code><code class="n">x_test</code><code class="p">,</code> <code class="n">y_test</code><code class="p">):</code>&#13;
    <code class="n">prediction</code> <code class="o">=</code> <code class="n">logistic</code><code class="p">(</code><code class="n">dot</code><code class="p">(</code><code class="n">beta</code><code class="p">,</code> <code class="n">x_i</code><code class="p">))</code>&#13;
&#13;
    <code class="k">if</code> <code class="n">y_i</code> <code class="o">==</code> <code class="mi">1</code> <code class="ow">and</code> <code class="n">prediction</code> <code class="o">&gt;=</code> <code class="mf">0.5</code><code class="p">:</code>  <code class="c1"># TP: paid and we predict paid</code>&#13;
        <code class="n">true_positives</code> <code class="o">+=</code> <code class="mi">1</code>&#13;
    <code class="k">elif</code> <code class="n">y_i</code> <code class="o">==</code> <code class="mi">1</code><code class="p">:</code>                      <code class="c1"># FN: paid and we predict unpaid</code>&#13;
        <code class="n">false_negatives</code> <code class="o">+=</code> <code class="mi">1</code>&#13;
    <code class="k">elif</code> <code class="n">prediction</code> <code class="o">&gt;=</code> <code class="mf">0.5</code><code class="p">:</code>             <code class="c1"># FP: unpaid and we predict paid</code>&#13;
        <code class="n">false_positives</code> <code class="o">+=</code> <code class="mi">1</code>&#13;
    <code class="k">else</code><code class="p">:</code>                               <code class="c1"># TN: unpaid and we predict unpaid</code>&#13;
        <code class="n">true_negatives</code> <code class="o">+=</code> <code class="mi">1</code>&#13;
&#13;
<code class="n">precision</code> <code class="o">=</code> <code class="n">true_positives</code> <code class="o">/</code> <code class="p">(</code><code class="n">true_positives</code> <code class="o">+</code> <code class="n">false_positives</code><code class="p">)</code>&#13;
<code class="n">recall</code> <code class="o">=</code> <code class="n">true_positives</code> <code class="o">/</code> <code class="p">(</code><code class="n">true_positives</code> <code class="o">+</code> <code class="n">false_negatives</code><code class="p">)</code></pre>&#13;
&#13;
<p>This gives a precision of 75% (“when we predict <em>paid account</em> we’re right 75% of the time”) and a recall of 80% (“when a user has a paid account we predict <em>paid account</em> 80% of the time”), which is not terrible considering how little data we have.</p>&#13;
&#13;
<p>We can also plot the predictions versus the actuals (<a data-type="xref" href="#logistic_prediction_vs_actual">Figure 16-4</a>), which also shows that the model performs well:</p>&#13;
&#13;
<pre data-code-language="py" data-type="programlisting"><code class="n">predictions</code> <code class="o">=</code> <code class="p">[</code><code class="n">logistic</code><code class="p">(</code><code class="n">dot</code><code class="p">(</code><code class="n">beta</code><code class="p">,</code> <code class="n">x_i</code><code class="p">))</code> <code class="k">for</code> <code class="n">x_i</code> <code class="ow">in</code> <code class="n">x_test</code><code class="p">]</code>&#13;
<code class="n">plt</code><code class="o">.</code><code class="n">scatter</code><code class="p">(</code><code class="n">predictions</code><code class="p">,</code> <code class="n">y_test</code><code class="p">,</code> <code class="n">marker</code><code class="o">=</code><code class="s1">'+'</code><code class="p">)</code>&#13;
<code class="n">plt</code><code class="o">.</code><code class="n">xlabel</code><code class="p">(</code><code class="s2">"predicted probability"</code><code class="p">)</code>&#13;
<code class="n">plt</code><code class="o">.</code><code class="n">ylabel</code><code class="p">(</code><code class="s2">"actual outcome"</code><code class="p">)</code>&#13;
<code class="n">plt</code><code class="o">.</code><code class="n">title</code><code class="p">(</code><code class="s2">"Logistic Regression Predicted vs. Actual"</code><code class="p">)</code>&#13;
<code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">()</code></pre>&#13;
&#13;
<figure><div class="figure" id="logistic_prediction_vs_actual">&#13;
<img alt="Logistic Regression Predicted vs Actual." src="assets/dsf2_1604.png"/>&#13;
<h6><span class="label">Figure 16-4. </span>Logistic regression predicted versus actual</h6>&#13;
</div></figure>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Support Vector Machines" data-type="sect1"><div class="sect1" id="idm45635734681320">&#13;
<h1>Support Vector Machines</h1>&#13;
&#13;
<p>The<a data-primary="logistic regression" data-secondary="support vector machines" data-type="indexterm" id="idm45635734679624"/><a data-primary="support vector machines" data-type="indexterm" id="idm45635734678616"/> set of points where <code>dot(beta, x_i)</code> equals 0 is the boundary between our classes.  We can plot this to see exactly what our model is doing (<a data-type="xref" href="#logit_image_part_two">Figure 16-5</a>).</p>&#13;
&#13;
<figure><div class="figure" id="logit_image_part_two">&#13;
<img alt="Paid and Unpaid Users With Decision Boundary." src="assets/dsf2_1605.png"/>&#13;
<h6><span class="label">Figure 16-5. </span>Paid and unpaid users with decision boundary</h6>&#13;
</div></figure>&#13;
&#13;
<p>This<a data-primary="hyperplanes" data-type="indexterm" id="idm45635734794248"/><a data-primary="decision boundary" data-type="indexterm" id="idm45635734793512"/> boundary is a <em>hyperplane</em> that splits the parameter space into two half-spaces corresponding to <em>predict paid</em> and <em>predict unpaid</em>.  We found it as a side effect of finding the most likely logistic model.</p>&#13;
&#13;
<p>An alternative approach to classification is to just look for the hyperplane that “best” separates the classes in the training data.  This is the idea behind the <em>support vector machine</em>, which finds the hyperplane that maximizes the distance to the nearest point in each class (<a data-type="xref" href="#separating_hyperplane">Figure 16-6</a>).</p>&#13;
&#13;
<figure><div class="figure" id="separating_hyperplane">&#13;
<img alt="A Separating Hyperplane" src="assets/dsf2_1606.png"/>&#13;
<h6><span class="label">Figure 16-6. </span>A separating hyperplane</h6>&#13;
</div></figure>&#13;
&#13;
<p>Finding such a hyperplane is an optimization problem that involves techniques that are too advanced for us.  A different problem is that a separating hyperplane might not exist at all.  In our “who pays?” dataset there simply is no line that perfectly separates the paid users from the unpaid users.</p>&#13;
&#13;
<p>We can sometimes get around this by transforming the data into a higher-dimensional space.  For example, consider the simple one-dimensional dataset shown in <a data-type="xref" href="#svm_non_separable">Figure 16-7</a>.</p>&#13;
&#13;
<figure><div class="figure" id="svm_non_separable">&#13;
<img alt="A Non-Separable One-Dimensional Data set" src="assets/dsf2_1607.png"/>&#13;
<h6><span class="label">Figure 16-7. </span>A nonseparable one-dimensional dataset</h6>&#13;
</div></figure>&#13;
&#13;
<p>It’s clear that there’s no hyperplane that separates the positive examples from the negative ones.  However, look at what happens when we map this dataset to two dimensions by sending the point <code>x</code> to <code>(x, x**2)</code>.  Suddenly it’s possible to find a hyperplane that splits the data (<a data-type="xref" href="#svm_separable">Figure 16-8</a>).</p>&#13;
&#13;
<figure><div class="figure" id="svm_separable">&#13;
<img alt="Becomes Separable In Higher Dimensions" src="assets/dsf2_1608.png"/>&#13;
<h6><span class="label">Figure 16-8. </span>Dataset becomes separable in higher dimensions</h6>&#13;
</div></figure>&#13;
&#13;
<p>This<a data-primary="kernel trick" data-type="indexterm" id="idm45635734778584"/> is usually called the <em>kernel trick</em> because&#13;
rather than actually mapping the points into the higher-dimensional space&#13;
(which could be expensive if there are a lot of points and the mapping is complicated),&#13;
we can use a “kernel” function to compute dot products in the higher-dimensional space&#13;
and use those to find a hyperplane.</p>&#13;
&#13;
<p>It’s hard (and probably not a good idea) to <em>use</em> support vector machines&#13;
without relying on specialized optimization software written by people&#13;
with the appropriate expertise, so we’ll have to leave our treatment here.<a data-primary="" data-startref="PMlog16" data-type="indexterm" id="idm45635734775896"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="For Further Investigation" data-type="sect1"><div class="sect1" id="further-invest">&#13;
<h1>For Further Investigation</h1>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>scikit-learn has modules<a data-primary="logistic regression" data-secondary="tools for" data-type="indexterm" id="idm45635734772808"/><a data-primary="scikit-learn" data-type="indexterm" id="idm45635734771832"/> for both <a href="https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression">logistic regression</a> and <a href="https://scikit-learn.org/stable/modules/svm.html">support vector machines</a>.</p>&#13;
</li>&#13;
<li>&#13;
<p><a href="https://www.csie.ntu.edu.tw/~cjlin/libsvm/">LIBSVM</a> is the<a data-primary="LIBSVM" data-type="indexterm" id="idm45635734768040"/> support vector machine implementation that scikit-learn is using behind the scenes. Its website has a variety of useful documentation about support vector machines.</p>&#13;
</li>&#13;
</ul>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section></body></html>