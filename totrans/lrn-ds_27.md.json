["```py\n`!`ls -l data/politifact/real `|` head -n `5`\n\n```", "```py\ntotal 0\ndrwxr-xr-x  2 sam  staff  64 Jul 14  2022 politifact100\ndrwxr-xr-x  3 sam  staff  96 Jul 14  2022 politifact1013\ndrwxr-xr-x  3 sam  staff  96 Jul 14  2022 politifact1014\ndrwxr-xr-x  2 sam  staff  64 Jul 14  2022 politifact10185\nls: stdout: Undefined error: 0\n\n```", "```py\n`!`ls -lh data/politifact/real/politifact1013/\n\n```", "```py\ntotal 16\n-rw-r--r--  1 sam  staff   5.7K Jul 14  2022 news content.json\n\n```", "```py\n`import` `json`\n`from` `pathlib` `import` `Path`\n\n`article_path` `=` `Path``(``'``data/politifact/real/politifact1013/news content.json``'``)`\n`article_json` `=` `json``.``loads``(``article_path``.``read_text``(``)``)`\n\n```", "```py\n`from` `pathlib` `import` `Path`\n\n`def` `df_row``(``content_json``)``:`\n    `return` `{`\n        `'``url``'``:` `content_json``[``'``url``'``]``,`\n        `'``text``'``:` `content_json``[``'``text``'``]``,`\n        `'``title``'``:` `content_json``[``'``title``'``]``,`\n        `'``publish_date``'``:` `content_json``[``'``publish_date``'``]``,`\n    `}`\n\n`def` `load_json``(``folder``,` `label``)``:`\n    `filepath` `=` `folder` `/` `'``news content.json``'`\n    `data` `=` `df_row``(``json``.``loads``(``filepath``.``read_text``(``)``)``)` `if` `filepath``.``exists``(``)` `else` `{``}`\n    `return` `{`\n        `*``*``data``,`\n        `'``label``'``:` `label``,`\n    `}`\n\n`fakes` `=` `Path``(``'``data/politifact/fake``'``)`\n`reals` `=` `Path``(``'``data/politifact/real``'``)`\n\n`df_raw` `=` `pd``.``DataFrame``(``[``load_json``(``path``,` `'``fake``'``)` `for` `path` `in` `fakes``.``iterdir``(``)``]` `+`\n                      `[``load_json``(``path``,` `'``real``'``)` `for` `path` `in` `reals``.``iterdir``(``)``]``)`\n\n```", "```py\n`df_raw``.``head``(``2``)`\n\n```", "```py\n`import` `re`\n\n`# [1], [2]`\n`def` `drop_nans``(``df``)``:`\n    `return` `df``[``~``(``df``[``'``url``'``]``.``isna``(``)` `|`\n                `(``df``[``'``text``'``]``.``str``.``strip``(``)` `==` `'``'``)` `|` \n                `(``df``[``'``title``'``]``.``str``.``strip``(``)` `==` `'``'``)``)``]`\n\n`# [3]`\n`def` `parse_timestamps``(``df``)``:`\n    `timestamp` `=` `pd``.``to_datetime``(``df``[``'``publish_date``'``]``,` `unit``=``'``s``'``,` `errors``=``'``coerce``'``)`\n    `return` `df``.``assign``(``timestamp``=``timestamp``)`\n\n`# [4], [5]`\n`archive_prefix_re` `=` `re``.``compile``(``r``'``https://web.archive.org/web/``\\``d+/``'``)`\n`site_prefix_re` `=` `re``.``compile``(``r``'``(https?://)?(www``\\``.)?``'``)`\n`port_re` `=` `re``.``compile``(``r``'``:``\\``d+``'``)`\n\n`def` `url_basename``(``url``)``:`\n    `if` `archive_prefix_re``.``match``(``url``)``:`\n        `url` `=` `archive_prefix_re``.``sub``(``'``'``,` `url``)`\n    `site` `=` `site_prefix_re``.``sub``(``'``'``,` `url``)``.``split``(``'``/``'``)``[``0``]`\n    `return` `port_re``.``sub``(``'``'``,` `site``)`\n\n`# [6]`\n`def` `combine_content``(``df``)``:`\n    `return` `df``.``assign``(``content``=``df``[``'``title``'``]` `+` `'` `'` `+` `df``[``'``text``'``]``)`\n\n`def` `subset_df``(``df``)``:`\n    `return` `df``[``[``'``timestamp``'``,` `'``baseurl``'``,` `'``content``'``,` `'``label``'``]``]`\n\n`df` `=` `(``df_raw`\n `.``pipe``(``drop_nans``)`\n `.``reset_index``(``drop``=``True``)`\n `.``assign``(``baseurl``=``lambda` `df``:` `df``[``'``url``'``]``.``apply``(``url_basename``)``)`\n `.``pipe``(``parse_timestamps``)`\n `.``pipe``(``combine_content``)`\n `.``pipe``(``subset_df``)`\n`)`\n\n```", "```py\n`df``.``head``(``2``)`\n\n```", "```py\n`from` `sklearn``.``model_selection` `import` `train_test_split`\n\n`df``[``'``label``'``]` `=` `(``df``[``'``label``'``]` `==` `'``fake``'``)``.``astype``(``int``)`\n\n`X_train``,` `X_test``,` `y_train``,` `y_test` `=` `train_test_split``(`\n    `df``[``[``'``timestamp``'``,` `'``baseurl``'``,` `'``content``'``]``]``,` `df``[``'``label``'``]``,`\n    `test_size``=``0.25``,` `random_state``=``42``,`\n`)`\n\n```", "```py\n`X_train``.``head``(``2``)`\n\n```", "```py\n`y_train``.``value_counts``(``)`\n\n```", "```py\nlabel\n0    320\n1    264\nName: count, dtype: int64\n\n```", "```py\n`X_train``.``info``(``)`\n\n```", "```py\n<class 'pandas.core.frame.DataFrame'>\nIndex: 584 entries, 164 to 102\nData columns (total 3 columns):\n #   Column     Non-Null Count  Dtype         \n---  ------     --------------  -----         \n 0   timestamp  306 non-null    datetime64[ns]\n 1   baseurl    584 non-null    object        \n 2   content    584 non-null    object        \ndtypes: datetime64[ns](1), object(2)\nmemory usage: 18.2+ KB\n\n```", "```py\n`X_train``[``'``baseurl``'``]``.``value_counts``(``)`\n\n```", "```py\nbaseurl\nwhitehouse.gov               21\nabcnews.go.com               20\nnytimes.com                  17\n                             ..\noccupydemocrats.com           1\nlegis.state.ak.us             1\ndailynewsforamericans.com     1\nName: count, Length: 337, dtype: int64\n\n```", "```py\nfig = px.histogram(X_train['baseurl'].value_counts(), width=450, height=250,\n                   labels={\"value\": \"Number of articles published at a URL\"})\n\n`fig``.``update_layout``(``showlegend``=``False``)`\n\n```", "```py\n`fig` `=` `px``.``histogram``(`\n    `X_train``[``\"``timestamp``\"``]``,`\n    `labels``=``{``\"``value``\"``:` `\"``Publication year``\"``}``,` `width``=``550``,` `height``=``250``,`\n`)`\n`fig``.``update_layout``(``showlegend``=``False``)`\n\n```", "```py\n`fig` `=` `px``.``histogram``(`\n    `X_train``.``loc``[``X_train``[``\"``timestamp``\"``]` `>` `\"``2000``\"``,` `\"``timestamp``\"``]``,`\n    `labels``=``{``\"``value``\"``:` `\"``Publication year``\"``}``,` `width``=``550``,` `height``=``250``,` \n`)`\n`fig``.``update_layout``(``showlegend``=``False``)`\n\n```", "```py\nword_features = [\n    # names of presidential candidates\n    'trump', 'clinton',\n    # congress words\n    'state', 'vote', 'congress', 'shutdown',\n\n    # other possibly useful words\n    'military', 'princ', 'investig', 'antifa', \n    'joke', 'homeless', 'swamp', 'cnn', 'the'\n]\n\n```", "```py\n`def` `make_word_features``(``df``,` `words``)``:`\n    `features` `=` `{` `word``:` `df``[``'``content``'``]``.``str``.``contains``(``word``)` `for` `word` `in` `words` `}`\n    `return` `pd``.``DataFrame``(``features``)`\n\n```", "```py\n`df_words` `=` `make_word_features``(``X_train``,` `word_features``)`\n`df_words``[``\"``label``\"``]` `=` `df``[``\"``label``\"``]`\n\n```", "```py\n`df_words``.``shape`\n\n```", "```py\n(584, 16)\n\n```", "```py\n`df_words``.``head``(``4``)`\n\n```", "```py\n4 rows Ã— 16 columns\n```", "```py\n`def` `lowercase``(``df``)``:`\n    `return` `df``.``assign``(``content``=``df``[``'``content``'``]``.``str``.``lower``(``)``)`\n\n```", "```py\n`one_word` `=` `[``'``vote``'``]`\n\n```", "```py\n`from` `sklearn``.``pipeline` `import` `make_pipeline`\n`from` `sklearn``.``linear_model` `import` `LogisticRegressionCV`\n`from` `sklearn``.``preprocessing` `import` `FunctionTransformer`\n\n```", "```py\n`model1` `=` `make_pipeline``(`\n    `FunctionTransformer``(``lowercase``)``,`\n    `FunctionTransformer``(``make_word_features``,` `kw_args``=``{``'``words``'``:` `one_word``}``)``,`\n    `LogisticRegressionCV``(``Cs``=``10``,` `solver``=``'``saga``'``,` `n_jobs``=``4``,` `max_iter``=``10000``)``,`\n`)`\n\n```", "```py\n`%``%``time`\n\n`model1``.``fit``(``X_train``,` `y_train``)`\n`print``(``f``'``{``model1``.``score``(``X_train``,` `y_train``)``:``.1%``}` `accuracy on training set.``'``)`\n\n```", "```py\n64.9% accuracy on training set.\nCPU times: user 110 ms, sys: 42.7 ms, total: 152 ms\nWall time: 144 ms\n\n```", "```py\n\"vote\" present: [[0.72 0.28]]\n\"vote\" absent: [[0.48 0.52]]\n\n```", "```py\n`print``(``f``'``Intercept:` `{``log_reg``.``intercept_``[``0``]``:``.2f``}``'``)`\n`[``[``coef``]``]` `=` `log_reg``.``coef_`\n`print``(``f``'``\"``vote``\"` `Coefficient:` `{``coef``:``.2f``}``'``)`\n\n```", "```py\nIntercept: 0.08\n\"vote\" Coefficient: -1.00\n\n```", "```py\n`np``.``exp``(``coef``)` \n\n```", "```py\n0.36836305405149367\n\n```", "```py\n`model2` `=` `make_pipeline``(`\n    `FunctionTransformer``(``lowercase``)``,`\n    `FunctionTransformer``(``make_word_features``,` `kw_args``=``{``'``words``'``:` `word_features``}``)``,`\n    `LogisticRegressionCV``(``Cs``=``10``,` `solver``=``'``saga``'``,` `n_jobs``=``4``,` `max_iter``=``10000``)``,`\n`)`\n\n```", "```py\n`%``%``time`\n\n`model2``.``fit``(``X_train``,` `y_train``)`\n`print``(``f``'``{``model2``.``score``(``X_train``,` `y_train``)``:``.1%``}` `accuracy on training set.``'``)`\n\n```", "```py\n74.8% accuracy on training set.\nCPU times: user 1.54 s, sys: 59.1 ms, total: 1.6 s\nWall time: 637 ms\n\n```", "```py\n`model1_precision` `=` `238` `/` `(``238` `+` `179``)`\n`model2_precision` `=` `205` `/` `(``205` `+` `88``)`\n\n`[``round``(``num``,` `2``)` `for` `num` `in` `[``model1_precision``,` `model2_precision``]``]`\n\n```", "```py\n[0.57, 0.7]\n\n```", "```py\n`tfidf` `=` `TfidfVectorizer``(``tokenizer``=``stemming_tokenizer``,` `token_pattern``=``None``)`\n\n```", "```py\n`from` `sklearn``.``compose` `import` `make_column_transformer`\n\n`model3` `=` `make_pipeline``(`\n    `FunctionTransformer``(``lowercase``)``,`\n    `make_column_transformer``(``(``tfidf``,` `'``content``'``)``)``,`\n    `LogisticRegressionCV``(``Cs``=``10``,`\n                         `solver``=``'``saga``'``,`\n                         `n_jobs``=``8``,`\n                         `max_iter``=``1000``)``,`\n    `verbose``=``True``,`\n`)`\n\n```", "```py\n`%``%``time`\n\n`model3``.``fit``(``X_train``,` `y_train``)`\n`print``(``f``'``{``model3``.``score``(``X_train``,` `y_train``)``:``.1%``}` `accuracy on training set.``'``)`\n\n```", "```py\n[Pipeline]  (step 1 of 3) Processing functiontransformer, total=   0.0s\n[Pipeline] . (step 2 of 3) Processing columntransformer, total=  14.5s\n[Pipeline]  (step 3 of 3) Processing logisticregressioncv, total=   6.3s\n100.0% accuracy on training set.\nCPU times: user 50.2 s, sys: 508 ms, total: 50.7 s\nWall time: 34.2 s\n\n```", "```py\n`tfidf` `=` `model3``.``named_steps``.``columntransformer``.``named_transformers_``.``tfidfvectorizer`\n`n_unique_tokens` `=` `len``(``tfidf``.``vocabulary_``.``keys``(``)``)`\n`print``(``f``'``{``n_unique_tokens``}` `tokens appeared across` `{``len``(``X_train``)``}` `examples.``'``)`\n\n```", "```py\n23800 tokens appeared across 584 examples.\n\n```"]