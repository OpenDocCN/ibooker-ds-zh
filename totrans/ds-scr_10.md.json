["```py\n# egrep.py\nimport sys, re\n\n# sys.argv is the list of command-line arguments\n# sys.argv[0] is the name of the program itself\n# sys.argv[1] will be the regex specified at the command line\nregex = sys.argv[1]\n\n# for every line passed into the script\nfor line in sys.stdin:\n    # if it matches the regex, write it to stdout\n    if re.search(regex, line):\n        sys.stdout.write(line)\n```", "```py\n# line_count.py\nimport sys\n\ncount = 0\nfor line in sys.stdin:\n    count += 1\n\n# print goes to sys.stdout\nprint(count)\n```", "```py\ntype SomeFile.txt | python egrep.py \"[0-9]\" | python line_count.py\n```", "```py\ncat SomeFile.txt | python egrep.py \"[0-9]\" | python line_count.py\n```", "```py\ntype SomeFile.txt | egrep.py \"[0-9]\" | line_count.py\n```", "```py\n# most_common_words.py\nimport sys\nfrom collections import Counter\n\n# pass in number of words as first argument\ntry:\n    num_words = int(sys.argv[1])\nexcept:\n    print(\"usage: most_common_words.py num_words\")\n    sys.exit(1)   # nonzero exit code indicates error\n\ncounter = Counter(word.lower()                      # lowercase words\n                  for line in sys.stdin\n                  for word in line.strip().split()  # split on spaces\n                  if word)                          # skip empty 'words'\n\nfor word, count in counter.most_common(num_words):\n    sys.stdout.write(str(count))\n    sys.stdout.write(\"\\t\")\n    sys.stdout.write(word)\n    sys.stdout.write(\"\\n\")\n```", "```py\n$ cat the_bible.txt | python most_common_words.py 10\n36397\tthe\n30031\tand\n20163\tof\n7154\tto\n6484\tin\n5856\tthat\n5421\the\n5226\this\n5060\tunto\n4297\tshall\n```", "```py\n# 'r' means read-only, it's assumed if you leave it out\nfile_for_reading = open('reading_file.txt', 'r')\nfile_for_reading2 = open('reading_file.txt')\n\n# 'w' is write -- will destroy the file if it already exists!\nfile_for_writing = open('writing_file.txt', 'w')\n\n# 'a' is append -- for adding to the end of the file\nfile_for_appending = open('appending_file.txt', 'a')\n\n# don't forget to close your files when you're done\nfile_for_writing.close()\n```", "```py\nwith open(filename) as f:\n    data = function_that_gets_data_from(f)\n\n# at this point f has already been closed, so don't try to use it\nprocess(data)\n```", "```py\nstarts_with_hash = 0\n\nwith open('input.txt') as f:\n    for line in f:                  # look at each line in the file\n        if re.match(\"^#\",line):     # use a regex to see if it starts with '#'\n            starts_with_hash += 1   # if it does, add 1 to the count\n```", "```py\ndef get_domain(email_address: str) -> str:\n    \"\"\"Split on '@' and return the last piece\"\"\"\n    return email_address.lower().split(\"@\")[-1]\n\n# a couple of tests\nassert get_domain('joelgrus@gmail.com') == 'gmail.com'\nassert get_domain('joel@m.datasciencester.com') == 'm.datasciencester.com'\n\nfrom collections import Counter\n\nwith open('email_addresses.txt', 'r') as f:\n    domain_counts = Counter(get_domain(line.strip())\n                            for line in f\n                            if \"@\" in line)\n```", "```py\n6/20/2014   AAPL    90.91\n6/20/2014   MSFT    41.68\n6/20/2014   FB  64.5\n6/19/2014   AAPL    91.86\n6/19/2014   MSFT    41.51\n6/19/2014   FB  64.34\n```", "```py\nimport csv\n\nwith open('tab_delimited_stock_prices.txt') as f:\n    tab_reader = csv.reader(f, delimiter='\\t')\n    for row in tab_reader:\n        date = row[0]\n        symbol = row[1]\n        closing_price = float(row[2])\n        process(date, symbol, closing_price)\n```", "```py\ndate:symbol:closing_price\n6/20/2014:AAPL:90.91\n6/20/2014:MSFT:41.68\n6/20/2014:FB:64.5\n```", "```py\nwith open('colon_delimited_stock_prices.txt') as f:\n    colon_reader = csv.DictReader(f, delimiter=':')\n    for dict_row in colon_reader:\n        date = dict_row[\"date\"]\n        symbol = dict_row[\"symbol\"]\n        closing_price = float(dict_row[\"closing_price\"])\n        process(date, symbol, closing_price)\n```", "```py\ntodays_prices = {'AAPL': 90.91, 'MSFT': 41.68, 'FB': 64.5 }\n\nwith open('comma_delimited_stock_prices.txt', 'w') as f:\n    csv_writer = csv.writer(f, delimiter=',')\n    for stock, price in todays_prices.items():\n        csv_writer.writerow([stock, price])\n```", "```py\nresults = [[\"test1\", \"success\", \"Monday\"],\n           [\"test2\", \"success, kind of\", \"Tuesday\"],\n           [\"test3\", \"failure, kind of\", \"Wednesday\"],\n           [\"test4\", \"failure, utter\", \"Thursday\"]]\n\n# don't do this!\nwith open('bad_csv.txt', 'w') as f:\n    for row in results:\n        f.write(\",\".join(map(str, row))) # might have too many commas in it!\n        f.write(\"\\n\")                    # row might have newlines as well!\n```", "```py\ntest1,success,Monday\ntest2,success, kind of,Tuesday\ntest3,failure, kind of,Wednesday\ntest4,failure, utter,Thursday\n```", "```py\n<html>\n  <head>\n    <title>A web page</title>\n  </head>\n  <body>\n    <p id=\"author\">Joel Grus</p>\n    <p id=\"subject\">Data Science</p>\n  </body>\n</html>\n```", "```py\npython -m pip install beautifulsoup4 requests html5lib\n```", "```py\nfrom bs4 import BeautifulSoup\nimport requests\n\n# I put the relevant HTML file on GitHub. In order to fit\n# the URL in the book I had to split it across two lines.\n# Recall that whitespace-separated strings get concatenated.\nurl = (\"https://raw.githubusercontent.com/\"\n       \"joelgrus/data/master/getting-data.html\")\nhtml = requests.get(url).text\nsoup = BeautifulSoup(html, 'html5lib')\n```", "```py\nfirst_paragraph = soup.find('p')        # or just soup.p\n```", "```py\nfirst_paragraph_text = soup.p.text\nfirst_paragraph_words = soup.p.text.split()\n```", "```py\nfirst_paragraph_id = soup.p['id']       # raises KeyError if no 'id'\nfirst_paragraph_id2 = soup.p.get('id')  # returns None if no 'id'\n```", "```py\nall_paragraphs = soup.find_all('p')  # or just soup('p')\nparagraphs_with_ids = [p for p in soup('p') if p.get('id')]\n```", "```py\nimportant_paragraphs = soup('p', {'class' : 'important'})\nimportant_paragraphs2 = soup('p', 'important')\nimportant_paragraphs3 = [p for p in soup('p')\n                         if 'important' in p.get('class', [])]\n```", "```py\n# Warning: will return the same <span> multiple times\n# if it sits inside multiple <div>s.\n# Be more clever if that's the case.\nspans_inside_divs = [span\n                     for div in soup('div')     # for each <div> on the page\n                     for span in div('span')]   # find each <span> inside it\n```", "```py\n<td>\n  <a href=\"https://jayapal.house.gov\">Jayapal, Pramila</a>\n</td>\n```", "```py\nfrom bs4 import BeautifulSoup\nimport requests\n\nurl = \"https://www.house.gov/representatives\"\ntext = requests.get(url).text\nsoup = BeautifulSoup(text, \"html5lib\")\n\nall_urls = [a['href']\n            for a in soup('a')\n            if a.has_attr('href')]\n\nprint(len(all_urls))  # 965 for me, way too many\n```", "```py\nimport re\n\n# Must start with http:// or https://\n# Must end with .house.gov or .house.gov/\nregex = r\"^https?://.*\\.house\\.gov/?$\"\n\n# Let's write some tests!\nassert re.match(regex, \"http://joel.house.gov\")\nassert re.match(regex, \"https://joel.house.gov\")\nassert re.match(regex, \"http://joel.house.gov/\")\nassert re.match(regex, \"https://joel.house.gov/\")\nassert not re.match(regex, \"joel.house.gov\")\nassert not re.match(regex, \"http://joel.house.com\")\nassert not re.match(regex, \"https://joel.house.gov/biography\")\n\n# And now apply\ngood_urls = [url for url in all_urls if re.match(regex, url)]\n\nprint(len(good_urls))  # still 862 for me\n```", "```py\ngood_urls = list(set(good_urls))\n\nprint(len(good_urls))  # only 431 for me\n```", "```py\nhtml = requests.get('https://jayapal.house.gov').text\nsoup = BeautifulSoup(html, 'html5lib')\n\n# Use a set because the links might appear multiple times.\nlinks = {a['href'] for a in soup('a') if 'press releases' in a.text.lower()}\n\nprint(links) # {'/media/press-releases'}\n```", "```py\nfrom typing import Dict, Set\n\npress_releases: Dict[str, Set[str]] = {}\n\nfor house_url in good_urls:\n    html = requests.get(house_url).text\n    soup = BeautifulSoup(html, 'html5lib')\n    pr_links = {a['href'] for a in soup('a') if 'press releases'\n                                             in a.text.lower()}\n    print(f\"{house_url}: {pr_links}\")\n    press_releases[house_url] = pr_links\n```", "```py\ndef paragraph_mentions(text: str, keyword: str) -> bool:\n    \"\"\"\n Returns True if a <p> inside the text mentions {keyword}\n \"\"\"\n    soup = BeautifulSoup(text, 'html5lib')\n    paragraphs = [p.get_text() for p in soup('p')]\n\n    return any(keyword.lower() in paragraph.lower()\n               for paragraph in paragraphs)\n```", "```py\ntext = \"\"\"<body><h1>Facebook</h1><p>Twitter</p>\"\"\"\nassert paragraph_mentions(text, \"twitter\")       # is inside a <p>\nassert not paragraph_mentions(text, \"facebook\")  # not inside a <p>\n```", "```py\nfor house_url, pr_links in press_releases.items():\n    for pr_link in pr_links:\n        url = f\"{house_url}/{pr_link}\"\n        text = requests.get(url).text\n\n        if paragraph_mentions(text, 'data'):\n            print(f\"{house_url}\")\n            break  # done with this house_url\n```", "```py\n{ \"title\" : \"Data Science Book\",\n  \"author\" : \"Joel Grus\",\n  \"publicationYear\" : 2019,\n  \"topics\" : [ \"data\", \"science\", \"data science\"] }\n```", "```py\nimport json\nserialized = \"\"\"{ \"title\" : \"Data Science Book\",\n \"author\" : \"Joel Grus\",\n \"publicationYear\" : 2019,\n \"topics\" : [ \"data\", \"science\", \"data science\"] }\"\"\"\n\n# parse the JSON to create a Python dict\ndeserialized = json.loads(serialized)\nassert deserialized[\"publicationYear\"] == 2019\nassert \"data science\" in deserialized[\"topics\"]\n```", "```py\n<Book>\n  <Title>Data Science Book</Title>\n  <Author>Joel Grus</Author>\n  <PublicationYear>2014</PublicationYear>\n  <Topics>\n    <Topic>data</Topic>\n    <Topic>science</Topic>\n    <Topic>data science</Topic>\n  </Topics>\n</Book>\n```", "```py\nimport requests, json\n\ngithub_user = \"joelgrus\"\nendpoint = f\"https://api.github.com/users/{github_user}/repos\"\n\nrepos = json.loads(requests.get(endpoint).text)\n```", "```py\n\"created_at\": \"2013-07-05T02:02:28Z\"\n```", "```py\npython -m pip install python-dateutil\n```", "```py\nfrom collections import Counter\nfrom dateutil.parser import parse\n\ndates = [parse(repo[\"created_at\"]) for repo in repos]\nmonth_counts = Counter(date.month for date in dates)\nweekday_counts = Counter(date.weekday() for date in dates)\n```", "```py\nlast_5_repositories = sorted(repos,\n                             key=lambda r: r[\"pushed_at\"],\n                             reverse=True)[:5]\n\nlast_5_languages = [repo[\"language\"]\n                    for repo in last_5_repositories]\n```", "```py\nimport os\n\n# Feel free to plug your key and secret in directly\nCONSUMER_KEY = os.environ.get(\"TWITTER_CONSUMER_KEY\")\nCONSUMER_SECRET = os.environ.get(\"TWITTER_CONSUMER_SECRET\")\n```", "```py\nimport webbrowser\nfrom twython import Twython\n\n# Get a temporary client to retrieve an authentication URL\ntemp_client = Twython(CONSUMER_KEY, CONSUMER_SECRET)\ntemp_creds = temp_client.get_authentication_tokens()\nurl = temp_creds['auth_url']\n\n# Now visit that URL to authorize the application and get a PIN\nprint(f\"go visit {url} and get the PIN code and paste it below\")\nwebbrowser.open(url)\nPIN_CODE = input(\"please enter the PIN code: \")\n\n# Now we use that PIN_CODE to get the actual tokens\nauth_client = Twython(CONSUMER_KEY,\n                      CONSUMER_SECRET,\n                      temp_creds['oauth_token'],\n                      temp_creds['oauth_token_secret'])\nfinal_step = auth_client.get_authorized_tokens(PIN_CODE)\nACCESS_TOKEN = final_step['oauth_token']\nACCESS_TOKEN_SECRET = final_step['oauth_token_secret']\n\n# And get a new Twython instance using them.\ntwitter = Twython(CONSUMER_KEY,\n                  CONSUMER_SECRET,\n                  ACCESS_TOKEN,\n                  ACCESS_TOKEN_SECRET)\n```", "```py\n# Search for tweets containing the phrase \"data science\"\nfor status in twitter.search(q='\"data science\"')[\"statuses\"]:\n    user = status[\"user\"][\"screen_name\"]\n    text = status[\"text\"]\n    print(f\"{user}: {text}\\n\")\n```", "```py\nhaithemnyc: Data scientists with the technical savvy &amp; analytical chops to\nderive meaning from big data are in demand. http://t.co/HsF9Q0dShP\n\nRPubsRecent: Data Science http://t.co/6hcHUz2PHM\n\nspleonard1: Using #dplyr in #R to work through a procrastinated assignment for\n@rdpeng in @coursera data science specialization. So easy and Awesome.\n```", "```py\nfrom twython import TwythonStreamer\n\n# Appending data to a global variable is pretty poor form\n# but it makes the example much simpler\ntweets = []\n\nclass MyStreamer(TwythonStreamer):\n    def on_success(self, data):\n        \"\"\"\n What do we do when Twitter sends us data?\n Here data will be a Python dict representing a tweet.\n \"\"\"\n        # We only want to collect English-language tweets\n        if data.get('lang') == 'en':\n            tweets.append(data)\n            print(f\"received tweet #{len(tweets)}\")\n\n        # Stop when we've collected enough\n        if len(tweets) >= 100:\n            self.disconnect()\n\n    def on_error(self, status_code, data):\n        print(status_code, data)\n        self.disconnect()\n```", "```py\nstream = MyStreamer(CONSUMER_KEY, CONSUMER_SECRET,\n                    ACCESS_TOKEN, ACCESS_TOKEN_SECRET)\n\n# starts consuming public statuses that contain the keyword 'data'\nstream.statuses.filter(track='data')\n\n# if instead we wanted to start consuming a sample of *all* public statuses\n# stream.statuses.sample()\n```", "```py\ntop_hashtags = Counter(hashtag['text'].lower()\n                       for tweet in tweets\n                       for hashtag in tweet[\"entities\"][\"hashtags\"])\n\nprint(top_hashtags.most_common(5))\n```"]