["```py\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ntraffic = pd.read_csv(\"./data/dft_traffic_counts_raw_counts.csv.gz\")\nprint(traffic.shape)\ntraffic.head()\n```", "```py\ntraffic.isnull().sum()\n```", "```py\nmeasurement_cols = [\n    'Pedal_cycles', 'Two_wheeled_motor_vehicles',\n    'Cars_and_taxis', 'Buses_and_coaches',\n    'LGVs', 'HGVs_2_rigid_axle', 'HGVs_3_rigid_axle',\n    'HGVs_4_or_more_rigid_axle', 'HGVs_3_or_4_articulated_axle',\n    'HGVs_5_articulated_axle', 'HGVs_6_articulated_axle',\n    'All_HGVs', 'All_motor_vehicles'\n]\n\nfor col in measurement_cols:\n    traffic[col] = traffic[col].fillna(0)\n```", "```py\ntraffic[\"Region_name\"].value_counts()\n```", "```py\nlen(traffic[[\"Count_point_id\", \"Year\", \"Count_date\",\n↪ \"hour\"]].drop_duplicates())\n```", "```py\nlen(traffic[[\"Count_point_id\", \"Year\", \"Count_date\",\n↪ \"hour\", \"Direction_of_travel\"]].drop_duplicates())\n```", "```py\nduplicate_groups = (\n    traffic\n    .groupby([\"Count_point_id\", \"Year\", \"Count_date\",\n↪ \"hour\", \"Direction_of_travel\"])\n    .size()\n    .loc[lambda x: x > 1]\n)\n\nduplicate_groups\n```", "```py\nexample_dupes = (\n  traffic[\n    (traffic[\"Count_point_id\"] == 7845)      #1\n      & (traffic[\"Count_date\"] == \"2014-09-03 00:00:00\")\n      & (traffic[\"hour\"] == 7)\n      & (traffic[\"Direction_of_travel\"] == \"W\")\n  ]\n)\n\n(\n  example_dupes\n  .eq(example_dupes.shift(-1))   #2\n  .iloc[0]\n  .loc[lambda x: x == False]\n)\n```", "```py\n(\n  example_dupes[[\n    'Two_wheeled_motor_vehicles', 'Cars_and_taxis', 'Buses_and_coaches',\n    'LGVs', 'HGVs_2_rigid_axle', 'HGVs_3_rigid_axle',\n    'HGVs_4_or_more_rigid_axle', 'HGVs_3_or_4_articulated_axle',\n    'HGVs_5_articulated_axle', 'HGVs_6_articulated_axle', 'All_HGVs',\n    'All_motor_vehicles']]\n  .transpose()\n)\n```", "```py\nprint(traffic[\"Link_length_km\"].min(),\n      traffic[\"Link_length_miles\"].min())\n```", "```py\nTEXT_PLACEHOLDER = \"PLACEHOLDER\"\nNUMBER_PLACEHOLDER = -9999\n\ngroup_cols = [\n  'Count_point_id', 'Direction_of_travel', 'Year', 'Count_date', 'hour',\n  'Region_id', 'Region_name', 'Region_ons_code', 'Local_authority_id',\n  'Local_authority_name', 'Local_authority_code', 'Road_name',\n  'Road_category', 'Road_type', 'Start_junction_road_name',\n  'End_junction_road_name', 'Easting', 'Northing', 'Latitude',\n  'Longitude', 'Link_length_km', 'Link_length_miles'\n]\n\ntraffic_deduped = (\n  traffic\n  .assign(\n    Start_junction_road_name = lambda df_:\n↪ df_[\"Start_junction_road_name\"].fillna(TEXT_PLACEHOLDER),\n    End_junction_road_name = lambda df_:\n↪ df_[\"End_junction_road_name\"].fillna(TEXT_PLACEHOLDER),\n    Link_length_km = lambda df_:\n↪ df_[\"Link_length_km\"].fillna(NUMBER_PLACEHOLDER),\n    Link_length_miles = lambda df_:\n↪ df_[\"Link_length_miles\"].fillna(NUMBER_PLACEHOLDER)\n  )\n  .groupby(group_cols)\n  .mean(numeric_only=True)\n  .reset_index()\n  .assign(\n    Start_junction_road_name = lambda df_:\n↪ df_[\"Start_junction_road_name\"].replace(TEXT_PLACEHOLDER, np.nan),\n    End_junction_road_name = lambda df_:\n↪ df_[\"End_junction_road_name\"].replace(TEXT_PLACEHOLDER, np.nan),\n    Link_length_km = lambda df_:\n↪ df_[\"Link_length_km\"].replace(NUMBER_PLACEHOLDER, np.nan),\n    Link_length_miles = lambda df_:\n↪ df_[\"Link_length_miles\"].replace(NUMBER_PLACEHOLDER, np.nan)\n  )\n)\n\nprint(traffic.shape, traffic_deduped.shape)\n```", "```py\ntraffic[\"Count_date\"] =\n↪ pd.to_datetime(traffic[\"Count_date\"], format=\"%Y-%m-%d %H:%M:%S\")\ntraffic[\"Count_date\"].agg([\"min\", \"max\"])\n```", "```py\ncoverage_by_point = (\n    traffic\n    .groupby(\"Count_point_id\")\n    [\"Count_date\"]\n    .agg([\"min\", \"max\"])\n    .assign(coverage_years = lambda x: (x[\"max\"] - x[\"min\"]).dt.days / 365)\n    .sort_values(\"coverage_years\", ascending=False)\n)\n\ncoverage_by_point\n```", "```py\nfig, axis = plt.subplots()\n\ncoverage_by_point[\"coverage_years\"].hist(bins=50, ax=axis)\n\naxis.set(\n    title=\"Distribution of coverage (years) by location\",\n    xlabel=\"Date range (number of years)\",\n    ylabel=\"Frequency\"\n)\n\nplt.show()\n```", "```py\nzero_location_ids = coverage_by_point                 #1\n↪ [coverage_by_point[\"coverage_years\"] == 0].index  \n\nzero_locations = (\n    traffic[traffic[\"Count_point_id\"].isin(zero_location_ids)]\n    .drop_duplicates(\"Count_point_id\")\n)\nprint(len(zero_locations))\nzero_locations[\"Region_name\"].value_counts()\n```", "```py\nlocation_sizes = (\n    traffic\n    .groupby(\"Region_name\")\n    [\"Count_point_id\"]\n    .nunique()\n)\n```", "```py\n(\n    location_sizes\n    .reset_index()\n    .merge(\n        zero_locations[\"Region_name\"]\n            .value_counts()\n            .reset_index(name=\"count\")\n            .rename(columns={\"index\": \"Region_name\"}),\n        on=\"Region_name\"\n    )\n    .rename(columns={\n        \"Count_point_id\": \"total_points\",\n        \"count\": \"number_of_zeros\"\n    })\n    .assign(pct_zeros = lambda x: x[\"number_of_zeros\"] / x[\"total_points\"])\n)\n```", "```py\npoints_and_dates = (\n    traffic\n    .groupby([\"Count_point_id\", \"Count_date\"])\n    .size()\n    .reset_index()\n    .sort_values([\"Count_point_id\", \"Count_date\"])\n)\n\npoints_and_dates.head()\n```", "```py\nnum_years_by_point = (\n    traffic\n    .groupby(\"Count_point_id\")\n    [\"Year\"]\n    .nunique()\n    .loc[lambda x: x > 10]\n    .sort_values(ascending=False)\n)\n\nnum_years_by_point\n```", "```py\nfig, axis = plt.subplots()\n\nLOCATION_ID = \"26010\"\n\n(\n    traffic\n    .query(f\"Count_point_id == {LOCATION_ID}\")\n    .groupby(\"Count_date\")\n    [\"All_motor_vehicles\"]\n    .sum()\n    .plot(ax=axis)\n)\n\naxis.set(\n    title=f\"Number of vehicles over time (location {LOCATION_ID})\",\n    xlabel=\"Date\",\n    ylabel=\"Number of vehicles (total)\"\n)\n\nplt.show()\n```", "```py\nlong_count_points = num_years_by_point.index\n\ngaps = (\n    traffic\n    .query(\"Count_point_id in @long_count_points\")\n    [[\"Count_point_id\", \"Year\"]]\n    .drop_duplicates()\n    .sort_values([\"Count_point_id\", \"Year\"])\n    .assign(\n        prev_year= lambda x: x[\"Year\"].shift(),\n        diff= lambda x: x[\"Year\"] - x[\"prev_year\"]\n    )\n)\n\ngaps.head()\n```", "```py\ngaps = (\n    gaps\n    .assign(\n        prev_id= lambda x: x[\"Count_point_id\"].shift()\n    )\n    .query(\"diff > 1 and Count_point_id == prev_id\")\n)\n\ngaps.head()\n```", "```py\ngap_ids = gaps[\"Count_point_id\"].unique()\n\nall_time_series_raw = (\n    traffic\n    .query(\"Count_point_id in @long_count_points \\\n    and Count_point_id not in @gap_ids\")\n)\n\nall_time_series_raw.head()\n```", "```py\nall_time_series = (\n    all_time_series_raw\n    .groupby([\"Count_point_id\", \"Count_date\"])\n    [\"All_motor_vehicles\"]\n    .sum()\n    .reset_index()\n)\n\nprint(all_time_series[\"Count_point_id\"].nunique())\nall_time_series.head()\n```", "```py\n(\n    all_time_series_raw[[\"Count_date\"]]\n    .drop_duplicates()\n    [\"Count_date\"]\n    .dt.weekday\n    .value_counts(normalize=True)\n    .sort_index()\n)\n```", "```py\nsame_month_time_series = (\n    all_time_series\n    .assign(month=lambda x: x[\"Count_date\"].dt.month)\n    .groupby(\"Count_point_id\")\n    [\"month\"]\n    .nunique()\n    .loc[lambda x: x == 1]\n)\n\nprint(len(same_month_time_series))\n\nsame_month_time_series.head()\n```", "```py\nall_time_series[all_time_series[\"Count_point_id\"] == 900056]\n```", "```py\nids_to_export = same_month_time_series.index\n\n(\n    traffic\n    .query(\"Count_point_id in @ids_to_export\")\n    .to_parquet(\"time_series.parquet.gz\", compression=\"gzip\")\n)\n```"]