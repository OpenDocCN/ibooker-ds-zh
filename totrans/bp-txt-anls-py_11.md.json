["```py\ndf = pd.read_json('reviews.json', lines=True)\ndf.sample(5)\n\n```", "```py\nfrom nltk.corpus import opinion_lexicon\nfrom nltk.tokenize import word_tokenize\nnltk.download('opinion_lexicon')\n\nprint('Total number of words in opinion lexicon', len(opinion_lexicon.words()))\nprint('Examples of positive words in opinion lexicon',\n      opinion_lexicon.positive()[:5])\nprint('Examples of negative words in opinion lexicon',\n      opinion_lexicon.negative()[:5])\n\n```", "```py\nTotal number of words in opinion lexicon 6789\nExamples of positive words in opinion lexicon ['a+', 'abound', 'abounds',\n'abundance', 'abundant']\nExamples of negative words in opinion lexicon ['2-faced', '2-faces',\n'abnormal', 'abolish', 'abominable']\n\n```", "```py\n# Let's create a dictionary which we can use for scoring our review text\ndf.rename(columns={\"reviewText\": \"text\"}, inplace=True)\npos_score = 1\nneg_score = -1\nword_dict = {}\n\n# Adding the positive words to the dictionary\nfor word in opinion_lexicon.positive():\n        word_dict[word] = pos_score\n\n# Adding the negative words to the dictionary\nfor word in opinion_lexicon.negative():\n        word_dict[word] = neg_score\n\ndef bing_liu_score(text):\n    sentiment_score = 0\n    bag_of_words = word_tokenize(text.lower())\n    for word in bag_of_words:\n        if word in word_dict:\n            sentiment_score += word_dict[word]\n    return sentiment_score / len(bag_of_words)\n\n```", "```py\ndf['Bing_Liu_Score'] = df['text'].apply(bing_liu_score)\ndf[['asin','text','Bing_Liu_Score']].sample(2)\n\n```", "```py\ndf['Bing_Liu_Score'] = preprocessing.scale(df['Bing_Liu_Score'])\ndf.groupby('overall').agg({'Bing_Liu_Score':'mean'})\n\n```", "```py\ndf = pd.read_json('reviews.json', lines=True)\n\n# Assigning a new [1,0] target class label based on the product rating\ndf['sentiment'] = 0\ndf.loc[df['overall'] > 3, 'sentiment'] = 1\ndf.loc[df['overall'] < 3, 'sentiment'] = 0\n\n# Removing unnecessary columns to keep a simple DataFrame\ndf.drop(columns=[\n    'reviewTime', 'unixReviewTime', 'overall', 'reviewerID', 'summary'],\n        inplace=True)\ndf.sample(3)\n\n```", "```py\ndf['text_orig'] = df['text'].copy()\ndf['text'] = df['text'].apply(clean)\n\n```", "```py\ndf[\"text\"] = df[\"text\"].apply(clean_text)\n\n# Remove observations that are empty after the cleaning step\ndf = df[df['text'].str.len() != 0]\n\n```", "```py\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(df['text'],\n                                                    df['sentiment'],\n                                                    test_size=0.2,\n                                                    random_state=42,\n                                                    stratify=df['sentiment'])\n\nprint ('Size of Training Data ', X_train.shape[0])\nprint ('Size of Test Data ', X_test.shape[0])\n\nprint ('Distribution of classes in Training Data :')\nprint ('Positive Sentiment ', str(sum(Y_train == 1)/ len(Y_train) * 100.0))\nprint ('Negative Sentiment ', str(sum(Y_train == 0)/ len(Y_train) * 100.0))\n\nprint ('Distribution of classes in Testing Data :')\nprint ('Positive Sentiment ', str(sum(Y_test == 1)/ len(Y_test) * 100.0))\nprint ('Negative Sentiment ', str(sum(Y_test == 0)/ len(Y_test) * 100.0))\n\n```", "```py\nSize of Training Data  234108\nSize of Test Data  58527\nDistribution of classes in Training Data :\nPositive Sentiment  50.90770071932612\nNegative Sentiment  49.09229928067388\nDistribution of classes in Testing Data :\nPositive Sentiment  50.9081278726058\nNegative Sentiment  49.09187212739419\n\n```", "```py\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\ntfidf = TfidfVectorizer(min_df = 10, ngram_range=(1,1))\nX_train_tf = tfidf.fit_transform(X_train)\nX_test_tf = tfidf.transform(X_test)\n\n```", "```py\nfrom sklearn.svm import LinearSVC\n\nmodel1 = LinearSVC(random_state=42, tol=1e-5)\nmodel1.fit(X_train_tf, Y_train)\n\n```", "```py\nLinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n          multi_class='ovr', penalty='l2', random_state=42, tol=1e-05,\n          verbose=0)\n\n```", "```py\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_auc_score\n\nY_pred = model1.predict(X_test_tf)\nprint ('Accuracy Score - ', accuracy_score(Y_test, Y_pred))\nprint ('ROC-AUC Score - ', roc_auc_score(Y_test, Y_pred))\n\n```", "```py\nAccuracy Score -  0.8658396979172006\nROC-AUC Score -  0.8660667427476778\n\n```", "```py\nsample_reviews = df.sample(5)\nsample_reviews_tf = tfidf.transform(sample_reviews['text'])\nsentiment_predictions = model1.predict(sample_reviews_tf)\nsentiment_predictions = pd.DataFrame(data = sentiment_predictions,\n                                     index=sample_reviews.index,\n                                     columns=['sentiment_prediction'])\nsample_reviews = pd.concat([sample_reviews, sentiment_predictions], axis=1)\nprint ('Some sample reviews with their sentiment - ')\nsample_reviews[['text_orig','sentiment_prediction']]\n\n```", "```py\nSome sample reviews with their sentiment -\n\n```", "```py\ndef baseline_scorer(text):\n    score = bing_liu_score(text)\n    if score > 0:\n        return 1\n    else:\n        return 0\n\nY_pred_baseline = X_test.apply(baseline_scorer)\nacc_score = accuracy_score(Y_pred_baseline, Y_test)\nprint (acc_score)\n\n```", "```py\n0.7521998393903668\n\n```", "```py\nfrom transformers import BertConfig, BertTokenizer, BertForSequenceClassification\n\nconfig = BertConfig.from_pretrained('bert-base-uncased',finetuning_task='binary')\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n\n```", "```py\ndef get_tokens(text, tokenizer, max_seq_length, add_special_tokens=True):\n  input_ids = tokenizer.encode(text,\n                               add_special_tokens=add_special_tokens,\n                               max_length=max_seq_length,\n                               pad_to_max_length=True)\n  attention_mask = [int(id > 0) for id in input_ids]\n  assert len(input_ids) == max_seq_length\n  assert len(attention_mask) == max_seq_length\n  return (input_ids, attention_mask)\n\ntext = \"Here is the sentence I want embeddings for.\"\ninput_ids, attention_mask = get_tokens(text,\n                                       tokenizer,\n                                       max_seq_length=30,\n                                       add_special_tokens = True)\ninput_tokens = tokenizer.convert_ids_to_tokens(input_ids)\nprint (text)\nprint (input_tokens)\nprint (input_ids)\nprint (attention_mask)\n\n```", "```py\nHere is the sentence I want embeddings for.\n['[CLS]', 'here', 'is', 'the', 'sentence', 'i', 'want', 'em', '##bed',\n'##ding', '##s', 'for', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]',\n'[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]',\n'[PAD]', '[PAD]', '[PAD]', '[PAD]']\n[101, 2182, 2003, 1996, 6251, 1045, 2215, 7861, 8270, 4667, 2015, 2005, 1012,\n102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n0, 0, 0, 0]\n\n```", "```py\nX_train, X_test, Y_train, Y_test = train_test_split(df['text_orig'],\n                                                    df['sentiment'],\n                                                    test_size=0.2,\n                                                    random_state=42,\n                                                    stratify=df['sentiment'])\nX_train_tokens = X_train.apply(get_tokens, args=(tokenizer, 50))\nX_test_tokens = X_test.apply(get_tokens, args=(tokenizer, 50))\n\n```", "```py\nimport torch\nfrom torch.utils.data import TensorDataset\n\ninput_ids_train = torch.tensor(\n    [features[0] for features in X_train_tokens.values], dtype=torch.long)\ninput_mask_train = torch.tensor(\n    [features[1] for features in X_train_tokens.values], dtype=torch.long)\nlabel_ids_train = torch.tensor(Y_train.values, dtype=torch.long)\n\nprint (input_ids_train.shape)\nprint (input_mask_train.shape)\nprint (label_ids_train.shape)\n\n```", "```py\ntorch.Size([234104, 50])\ntorch.Size([234104, 50])\ntorch.Size([234104])\n\n```", "```py\ninput_ids_train[1]\n\n```", "```py\ntensor([ 101, 2009, 2134, 1005, 1056, 2147, 6314, 2055, 2009, 1037, 5808, 1997,\n        2026, 2769,  102,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0])\n\n```", "```py\ntrain_dataset = TensorDataset(input_ids_train,input_mask_train,label_ids_train)\n\n```", "```py\nfrom torch.utils.data import DataLoader, RandomSampler\n\ntrain_batch_size = 64\nnum_train_epochs = 2\n\ntrain_sampler = RandomSampler(train_dataset)\ntrain_dataloader = DataLoader(train_dataset,\n                              sampler=train_sampler,\n                              batch_size=train_batch_size)\nt_total = len(train_dataloader) // num_train_epochs\n\nprint (\"Num examples = \", len(train_dataset))\nprint (\"Num Epochs = \", num_train_epochs)\nprint (\"Total train batch size  = \", train_batch_size)\nprint (\"Total optimization steps = \", t_total)\n\n```", "```py\nNum examples =  234104\nNum Epochs =  2\nTotal train batch size  =  64\nTotal optimization steps =  1829\n\n```", "```py\nfrom transformers import AdamW, get_linear_schedule_with_warmup\n\nlearning_rate = 1e-4\nadam_epsilon = 1e-8\nwarmup_steps = 0\n\noptimizer = AdamW(model.parameters(), lr=learning_rate, eps=adam_epsilon)\nscheduler = get_linear_schedule_with_warmup(optimizer,\n                                            num_warmup_steps=warmup_steps,\n                                            num_training_steps=t_total)\n\n```", "```py\nfrom tqdm import trange, notebook\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntrain_iterator = trange(num_train_epochs, desc=\"Epoch\")\n\n# Put model in 'train' mode\nmodel.train()\n\nfor epoch in train_iterator:\n    epoch_iterator = notebook.tqdm(train_dataloader, desc=\"Iteration\")\n    for step, batch in enumerate(epoch_iterator):\n\n        # Reset all gradients at start of every iteration\n        model.zero_grad()\n\n        # Put the model and the input observations to GPU\n        model.to(device)\n        batch = tuple(t.to(device) for t in batch)\n\n        # Identify the inputs to the model\n        inputs = {'input_ids':      batch[0],\n                  'attention_mask': batch[1],\n                  'labels':         batch[2]}\n\n        # Forward Pass through the model. Input -> Model -> Output\n        outputs = model(**inputs)\n\n        # Determine the deviation (loss)\n        loss = outputs[0]\n        print(\"\\r%f\" % loss, end='')\n\n        # Back-propogate the loss (automatically calculates gradients)\n        loss.backward()\n\n        # Prevent exploding gradients by limiting gradients to 1.0\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n        # Update the parameters and learning rate\n        optimizer.step()\n        scheduler.step()\n\n```", "```py\nmodel.save_pretrained('outputs')\n\n```", "```py\nimport numpy as np\nfrom torch.utils.data import SequentialSampler\n\ntest_batch_size = 64\ntest_sampler = SequentialSampler(test_dataset)\ntest_dataloader = DataLoader(test_dataset,\n                             sampler=test_sampler,\n                             batch_size=test_batch_size)\n\n# Load the pretrained model that was saved earlier\n# model = model.from_pretrained('/outputs')\n\n# Initialize the prediction and actual labels\npreds = None\nout_label_ids = None\n\n# Put model in \"eval\" mode\nmodel.eval()\n\nfor batch in notebook.tqdm(test_dataloader, desc=\"Evaluating\"):\n\n    # Put the model and the input observations to GPU\n    model.to(device)\n    batch = tuple(t.to(device) for t in batch)\n\n    # Do not track any gradients since in 'eval' mode\n    with torch.no_grad():\n        inputs = {'input_ids':      batch[0],\n                  'attention_mask': batch[1],\n                  'labels':         batch[2]}\n\n        # Forward pass through the model\n        outputs = model(**inputs)\n\n        # We get loss since we provided the labels\n        tmp_eval_loss, logits = outputs[:2]\n\n        # There maybe more than one batch of items in the test dataset\n        if preds is None:\n            preds = logits.detach().cpu().numpy()\n            out_label_ids = inputs['labels'].detach().cpu().numpy()\n        else:\n            preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n            out_label_ids = np.append(out_label_ids,\n                                      inputs['labels'].detach().cpu().numpy(),\n                                      axis=0)\n\n# Get final loss, predictions and accuracy\npreds = np.argmax(preds, axis=1)\nacc_score = accuracy_score(preds, out_label_ids)\nprint ('Accuracy Score on Test data ', acc_score)\n\n```", "```py\nAccuracy Score on Test data  0.9535086370393152\n\n```"]