- en: Chapter 17\. Decision Trees
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A tree is an incomprehensible mystery.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Jim Woodring
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: DataSciencester’s VP of Talent has interviewed a number of job candidates from
    the site, with varying degrees of success. He’s collected a dataset consisting
    of several (qualitative) attributes of each candidate, as well as whether that
    candidate interviewed well or poorly. Could you, he asks, use this data to build
    a model identifying which candidates will interview well, so that he doesn’t have
    to waste time conducting interviews?
  prefs: []
  type: TYPE_NORMAL
- en: This seems like a good fit for a *decision tree*, another predictive modeling
    tool in the data scientist’s kit.
  prefs: []
  type: TYPE_NORMAL
- en: What Is a Decision Tree?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A decision tree uses a tree structure to represent a number of possible *decision
    paths* and an outcome for each path.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you have ever played the game [Twenty Questions](http://en.wikipedia.org/wiki/Twenty_Questions),
    then you are familiar with decision trees. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: “I am thinking of an animal.”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “Does it have more than five legs?”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “No.”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “Is it delicious?”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “No.”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “Does it appear on the back of the Australian five-cent coin?”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “Yes.”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “Is it an echidna?”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “Yes, it is!”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This corresponds to the path:'
  prefs: []
  type: TYPE_NORMAL
- en: “Not more than 5 legs” → “Not delicious” → “On the 5-cent coin” → “Echidna!”
  prefs: []
  type: TYPE_NORMAL
- en: in an idiosyncratic (and not very comprehensive) “guess the animal” decision
    tree ([Figure 17-1](#guess_the_animal)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Guess the animal.](assets/dsf2_1701.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 17-1\. A “guess the animal” decision tree
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Decision trees have a lot to recommend them. They’re very easy to understand
    and interpret, and the process by which they reach a prediction is completely
    transparent. Unlike the other models we’ve looked at so far, decision trees can
    easily handle a mix of numeric (e.g., number of legs) and categorical (e.g., delicious/not
    delicious) attributes and can even classify data for which attributes are missing.
  prefs: []
  type: TYPE_NORMAL
- en: At the same time, finding an “optimal” decision tree for a set of training data
    is computationally a very hard problem. (We will get around this by trying to
    build a good-enough tree rather than an optimal one, although for large datasets
    this can still be a lot of work.) More important, it is very easy (and very bad)
    to build decision trees that are *overfitted* to the training data, and that don’t
    generalize well to unseen data. We’ll look at ways to address this.
  prefs: []
  type: TYPE_NORMAL
- en: Most people divide decision trees into *classification trees* (which produce
    categorical outputs) and *regression trees* (which produce numeric outputs). In
    this chapter, we’ll focus on classification trees, and we’ll work through the
    ID3 algorithm for learning a decision tree from a set of labeled data, which should
    help us understand how decision trees actually work. To make things simple, we’ll
    restrict ourselves to problems with binary outputs like “Should I hire this candidate?”
    or “Should I show this website visitor advertisement A or advertisement B?” or
    “Will eating this food I found in the office fridge make me sick?”
  prefs: []
  type: TYPE_NORMAL
- en: Entropy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order to build a decision tree, we will need to decide what questions to
    ask and in what order. At each stage of the tree there are some possibilities
    we’ve eliminated and some that we haven’t. After learning that an animal doesn’t
    have more than five legs, we’ve eliminated the possibility that it’s a grasshopper.
    We haven’t eliminated the possibility that it’s a duck. Each possible question
    partitions the remaining possibilities according to its answer.
  prefs: []
  type: TYPE_NORMAL
- en: Ideally, we’d like to choose questions whose answers give a lot of information
    about what our tree should predict. If there’s a single yes/no question for which
    “yes” answers always correspond to `True` outputs and “no” answers to `False`
    outputs (or vice versa), this would be an awesome question to pick. Conversely,
    a yes/no question for which neither answer gives you much new information about
    what the prediction should be is probably not a good choice.
  prefs: []
  type: TYPE_NORMAL
- en: We capture this notion of “how much information” with *entropy*. You have probably
    heard this term used to mean disorder. We use it to represent the uncertainty
    associated with data.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine that we have a set *S* of data, each member of which is labeled as belonging
    to one of a finite number of classes <math><mrow><msub><mi>C</mi> <mn>1</mn></msub>
    <mo>,</mo> <mo>...</mo> <mo>,</mo> <msub><mi>C</mi> <mi>n</mi></msub></mrow></math>
    . If all the data points belong to a single class, then there is no real uncertainty,
    which means we’d like there to be low entropy. If the data points are evenly spread
    across the classes, there is a lot of uncertainty and we’d like there to be high
    entropy.
  prefs: []
  type: TYPE_NORMAL
- en: 'In math terms, if <math><msub><mi>p</mi> <mi>i</mi></msub></math> is the proportion
    of data labeled as class <math><msub><mi>c</mi> <mi>i</mi></msub></math> , we
    define the entropy as:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="upper H left-parenthesis upper S right-parenthesis equals minus
    p 1 log Subscript 2 Baseline p 1 minus ellipsis minus p Subscript n Baseline log
    Subscript 2 Baseline p Subscript n" display="block"><mrow><mi>H</mi> <mrow><mo>(</mo>
    <mi>S</mi> <mo>)</mo></mrow> <mo>=</mo> <mo>-</mo> <msub><mi>p</mi> <mn>1</mn></msub>
    <msub><mo form="prefix">log</mo> <mn>2</mn></msub> <msub><mi>p</mi> <mn>1</mn></msub>
    <mo>-</mo> <mo>...</mo> <mo>-</mo> <msub><mi>p</mi> <mi>n</mi></msub> <msub><mo
    form="prefix">log</mo> <mn>2</mn></msub> <msub><mi>p</mi> <mi>n</mi></msub></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: with the (standard) convention that <math><mrow><mn>0</mn> <mo form="prefix">log</mo>
    <mn>0</mn> <mo>=</mo> <mn>0</mn></mrow></math> .
  prefs: []
  type: TYPE_NORMAL
- en: Without worrying too much about the grisly details, each term <math><mrow><mo>-</mo>
    <msub><mi>p</mi> <mi>i</mi></msub> <msub><mo form="prefix">log</mo> <mn>2</mn></msub>
    <msub><mi>p</mi> <mi>i</mi></msub></mrow></math> is non-negative and is close
    to 0 precisely when <math><msub><mi>p</mi> <mi>i</mi></msub></math> is either
    close to 0 or close to 1 ([Figure 17-2](#p_log_p)).
  prefs: []
  type: TYPE_NORMAL
- en: '![A graph of –p log p.](assets/dsf2_1702.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 17-2\. A graph of -p log p
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This means the entropy will be small when every <math><msub><mi>p</mi> <mi>i</mi></msub></math>
    is close to 0 or 1 (i.e., when most of the data is in a single class), and it
    will be larger when many of the <math><msub><mi>p</mi> <mi>i</mi></msub></math>
    ’s are not close to 0 (i.e., when the data is spread across multiple classes).
    This is exactly the behavior we desire.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is easy enough to roll all of this into a function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Our data will consist of pairs `(input, label)`, which means that we’ll need
    to compute the class probabilities ourselves. Notice that we don’t actually care
    which label is associated with each probability, only what the probabilities are:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The Entropy of a Partition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What we’ve done so far is compute the entropy (think “uncertainty”) of a single
    set of labeled data. Now, each stage of a decision tree involves asking a question
    whose answer partitions data into one or (hopefully) more subsets. For instance,
    our “does it have more than five legs?” question partitions animals into those
    that have more than five legs (e.g., spiders) and those that don’t (e.g., echidnas).
  prefs: []
  type: TYPE_NORMAL
- en: Correspondingly, we’d like some notion of the entropy that results from partitioning
    a set of data in a certain way. We want a partition to have low entropy if it
    splits the data into subsets that themselves have low entropy (i.e., are highly
    certain), and high entropy if it contains subsets that (are large and) have high
    entropy (i.e., are highly uncertain).
  prefs: []
  type: TYPE_NORMAL
- en: For example, my “Australian five-cent coin” question was pretty dumb (albeit
    pretty lucky!), as it partitioned the remaining animals at that point into <math><msub><mi>S</mi>
    <mn>1</mn></msub></math> = {echidna} and <math><msub><mi>S</mi> <mn>2</mn></msub></math>
    = {everything else}, where <math><msub><mi>S</mi> <mn>2</mn></msub></math> is
    both large and high-entropy. ( <math><msub><mi>S</mi> <mn>1</mn></msub></math>
    has no entropy, but it represents a small fraction of the remaining “classes.”)
  prefs: []
  type: TYPE_NORMAL
- en: 'Mathematically, if we partition our data *S* into subsets <math><mrow><msub><mi>S</mi>
    <mn>1</mn></msub> <mo>,</mo> <mo>...</mo> <mo>,</mo> <msub><mi>S</mi> <mi>m</mi></msub></mrow></math>
    containing proportions <math><mrow><msub><mi>q</mi> <mn>1</mn></msub> <mo>,</mo>
    <mo>...</mo> <mo>,</mo> <msub><mi>q</mi> <mi>m</mi></msub></mrow></math> of the
    data, then we compute the entropy of the partition as a weighted sum:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="upper H equals q 1 upper H left-parenthesis upper S 1 right-parenthesis
    plus period period period plus q Subscript m Baseline upper H left-parenthesis
    upper S Subscript m Baseline right-parenthesis" display="block"><mrow><mi>H</mi>
    <mo>=</mo> <msub><mi>q</mi> <mn>1</mn></msub> <mi>H</mi> <mrow><mo>(</mo> <msub><mi>S</mi>
    <mn>1</mn></msub> <mo>)</mo></mrow> <mo>+</mo> <mo>...</mo> <mo>+</mo> <msub><mi>q</mi>
    <mi>m</mi></msub> <mi>H</mi> <mrow><mo>(</mo> <msub><mi>S</mi> <mi>m</mi></msub>
    <mo>)</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'which we can implement as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: One problem with this approach is that partitioning by an attribute with many
    different values will result in a very low entropy due to overfitting. For example,
    imagine you work for a bank and are trying to build a decision tree to predict
    which of your customers are likely to default on their mortgages, using some historical
    data as your training set. Imagine further that the dataset contains each customer’s
    Social Security number. Partitioning on SSN will produce one-person subsets, each
    of which necessarily has zero entropy. But a model that relies on SSN is *certain*
    not to generalize beyond the training set. For this reason, you should probably
    try to avoid (or bucket, if appropriate) attributes with large numbers of possible
    values when creating decision trees.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a Decision Tree
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The VP provides you with the interviewee data, consisting of (per your specification)
    a `NamedTuple` of the relevant attributes for each candidate—her level, her preferred
    language, whether she is active on Twitter, whether she has a PhD, and whether
    she interviewed well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Our tree will consist of *decision nodes* (which ask a question and direct
    us differently depending on the answer) and *leaf nodes* (which give us a prediction).
    We will build it using the relatively simple *ID3* algorithm, which operates in
    the following manner. Let’s say we’re given some labeled data, and a list of attributes
    to consider branching on:'
  prefs: []
  type: TYPE_NORMAL
- en: If the data all have the same label, create a leaf node that predicts that label
    and then stop.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the list of attributes is empty (i.e., there are no more possible questions
    to ask), create a leaf node that predicts the most common label and then stop.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Otherwise, try partitioning the data by each of the attributes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choose the partition with the lowest partition entropy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add a decision node based on the chosen attribute.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recur on each partitioned subset using the remaining attributes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is what’s known as a “greedy” algorithm because, at each step, it chooses
    the most immediately best option. Given a dataset, there may be a better tree
    with a worse-looking first move. If so, this algorithm won’t find it. Nonetheless,
    it is relatively easy to understand and implement, which makes it a good place
    to begin exploring decision trees.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s manually go through these steps on the interviewee dataset. The dataset
    has both `True` and `False` labels, and we have four attributes we can split on.
    So our first step will be to find the partition with the least entropy. We’ll
    start by writing a function that does the partitioning:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'and one that uses it to compute entropy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we just need to find the minimum-entropy partition for the whole dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The lowest entropy comes from splitting on `level`, so we’ll need to make a
    subtree for each possible `level` value. Every `Mid` candidate is labeled `True`,
    which means that the `Mid` subtree is simply a leaf node predicting `True`. For
    `Senior` candidates, we have a mix of `True`s and `False`s, so we need to split
    again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: This shows us that our next split should be on `tweets`, which results in a
    zero-entropy partition. For these `Senior`-level candidates, “yes” tweets always
    result in `True` while “no” tweets always result in `False`.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, if we do the same thing for the `Junior` candidates, we end up splitting
    on `phd`, after which we find that no PhD always results in `True` and PhD always
    results in `False`.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 17-3](#hiring_decision_tree) shows the complete decision tree.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Hiring Decision Tree.](assets/dsf2_1703.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 17-3\. The decision tree for hiring
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Putting It All Together
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we’ve seen how the algorithm works, we would like to implement it
    more generally. This means we need to decide how we want to represent trees. We’ll
    use pretty much the most lightweight representation possible. We define a *tree*
    to be either:'
  prefs: []
  type: TYPE_NORMAL
- en: a `Leaf` (that predicts a single value), or
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: a `Split` (containing an attribute to split on, subtrees for specific values
    of that attribute, and possibly a default value to use if we see an unknown value).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'With this representation, our hiring tree would look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: There’s still the question of what to do if we encounter an unexpected (or missing)
    attribute value. What should our hiring tree do if it encounters a candidate whose
    `level` is `Intern`? We’ll handle this case by populating the `default_value`
    attribute with the most common label.
  prefs: []
  type: TYPE_NORMAL
- en: 'Given such a representation, we can classify an input with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'All that’s left is to build the tree representation from our training data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'In the tree we built, every leaf consisted entirely of `True` inputs or entirely
    of `False` inputs. This means that the tree predicts perfectly on the training
    dataset. But we can also apply it to new data that wasn’t in the training set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'And also to data with unexpected values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Since our goal was mainly to demonstrate *how* to build a tree, we built the
    tree using the entire dataset. As always, if we were really trying to create a
    good model for something, we would have collected more data and split it into
    train/validation/test subsets.
  prefs: []
  type: TYPE_NORMAL
- en: Random Forests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Given how closely decision trees can fit themselves to their training data,
    it’s not surprising that they have a tendency to overfit. One way of avoiding
    this is a technique called *random forests*, in which we build multiple decision
    trees and combine their outputs. If they’re classification trees, we might let
    them vote; if they’re regression trees, we might average their predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Our tree-building process was deterministic, so how do we get random trees?
  prefs: []
  type: TYPE_NORMAL
- en: 'One piece involves bootstrapping data (recall [“Digression: The Bootstrap”](ch15.html#the_bootstrap)).
    Rather than training each tree on all the `inputs` in the training set, we train
    each tree on the result of `bootstrap_sample(inputs)`. Since each tree is built
    using different data, each tree will be different from every other tree. (A side
    benefit is that it’s totally fair to use the nonsampled data to test each tree,
    which means you can get away with using all of your data as the training set if
    you are clever in how you measure performance.) This technique is known as *bootstrap
    aggregating* or *bagging*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A second source of randomness involves changing the way we choose the `best_attribute`
    to split on. Rather than looking at all the remaining attributes, we first choose
    a random subset of them and then split on whichever of those is best:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: This is an example of a broader technique called *ensemble learning* in which
    we combine several *weak learners* (typically high-bias, low-variance models)
    in order to produce an overall strong model.
  prefs: []
  type: TYPE_NORMAL
- en: For Further Exploration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: scikit-learn has many [decision tree](https://scikit-learn.org/stable/modules/tree.html)
    models. It also has an [`ensemble`](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.ensemble)
    module that includes a `RandomForestClassifier` as well as other ensemble methods.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[XGBoost](https://xgboost.ai/) is a library for training *gradient boosted*
    decision trees that tends to win a lot of Kaggle-style machine learning competitions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We’ve barely scratched the surface of decision trees and their algorithms. [Wikipedia](https://en.wikipedia.org/wiki/Decision_tree_learning)
    is a good starting point for broader exploration.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
