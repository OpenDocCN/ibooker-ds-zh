- en: Chapter 17\. Decision Trees
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第17章\. 决策树
- en: A tree is an incomprehensible mystery.
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 树是一个难以理解的神秘。
- en: ''
  id: totrans-2
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Jim Woodring
  id: totrans-3
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Jim Woodring
- en: DataSciencester’s VP of Talent has interviewed a number of job candidates from
    the site, with varying degrees of success. He’s collected a dataset consisting
    of several (qualitative) attributes of each candidate, as well as whether that
    candidate interviewed well or poorly. Could you, he asks, use this data to build
    a model identifying which candidates will interview well, so that he doesn’t have
    to waste time conducting interviews?
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: DataSciencester的人才副总裁已经面试了一些来自该网站的求职者，结果各不相同。他收集了一个数据集，其中包含每个候选人的几个（定性）属性，以及该候选人是否面试表现良好或不佳。他问道，你能否利用这些数据建立一个模型，识别哪些候选人会面试表现良好，这样他就不必浪费时间进行面试了？
- en: This seems like a good fit for a *decision tree*, another predictive modeling
    tool in the data scientist’s kit.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 这似乎非常适合*决策树*，这是数据科学家工具包中的另一种预测建模工具。
- en: What Is a Decision Tree?
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是决策树？
- en: A decision tree uses a tree structure to represent a number of possible *decision
    paths* and an outcome for each path.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树使用树结构来表示多条可能的*决策路径*和每条路径的结果。
- en: 'If you have ever played the game [Twenty Questions](http://en.wikipedia.org/wiki/Twenty_Questions),
    then you are familiar with decision trees. For example:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你玩过[Twenty Questions](http://en.wikipedia.org/wiki/Twenty_Questions)游戏，那么你就熟悉决策树了。例如：
- en: “I am thinking of an animal.”
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “我在想一个动物。”
- en: “Does it have more than five legs?”
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “它有五条腿以上吗？”
- en: “No.”
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “不。”
- en: “Is it delicious?”
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “它好吃吗？”
- en: “No.”
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “不。”
- en: “Does it appear on the back of the Australian five-cent coin?”
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “它出现在澳大利亚五分硬币的背面吗？”
- en: “Yes.”
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “是的。”
- en: “Is it an echidna?”
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “它是针鼹吗？”
- en: “Yes, it is!”
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “是的，就是它！”
- en: 'This corresponds to the path:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这对应于路径：
- en: “Not more than 5 legs” → “Not delicious” → “On the 5-cent coin” → “Echidna!”
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: “不超过5条腿” → “不好吃” → “在5分硬币上” → “针鼹！”
- en: in an idiosyncratic (and not very comprehensive) “guess the animal” decision
    tree ([Figure 17-1](#guess_the_animal)).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个古怪（并不是很全面的）“猜动物”决策树中（[Figure 17-1](#guess_the_animal)）。
- en: '![Guess the animal.](assets/dsf2_1701.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![猜动物。](assets/dsf2_1701.png)'
- en: Figure 17-1\. A “guess the animal” decision tree
  id: totrans-22
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图17-1\. “猜动物”决策树
- en: Decision trees have a lot to recommend them. They’re very easy to understand
    and interpret, and the process by which they reach a prediction is completely
    transparent. Unlike the other models we’ve looked at so far, decision trees can
    easily handle a mix of numeric (e.g., number of legs) and categorical (e.g., delicious/not
    delicious) attributes and can even classify data for which attributes are missing.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树有很多优点。它们非常容易理解和解释，它们达到预测的过程完全透明。与我们迄今所看到的其他模型不同，决策树可以轻松处理数值型（例如，腿的数量）和分类型（例如，好吃/不好吃）属性的混合数据，甚至可以对缺少属性的数据进行分类。
- en: At the same time, finding an “optimal” decision tree for a set of training data
    is computationally a very hard problem. (We will get around this by trying to
    build a good-enough tree rather than an optimal one, although for large datasets
    this can still be a lot of work.) More important, it is very easy (and very bad)
    to build decision trees that are *overfitted* to the training data, and that don’t
    generalize well to unseen data. We’ll look at ways to address this.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 与此同时，为一组训练数据找到一个“最优”决策树在计算上是一个非常困难的问题。（我们将通过尝试构建一个足够好的树来避开这个问题，尽管对于大型数据集来说，这仍然可能是一项艰巨的工作。）更重要的是，很容易（也很糟糕）构建过度拟合训练数据的决策树，这些树在未见数据上的泛化能力很差。我们将探讨解决这个问题的方法。
- en: Most people divide decision trees into *classification trees* (which produce
    categorical outputs) and *regression trees* (which produce numeric outputs). In
    this chapter, we’ll focus on classification trees, and we’ll work through the
    ID3 algorithm for learning a decision tree from a set of labeled data, which should
    help us understand how decision trees actually work. To make things simple, we’ll
    restrict ourselves to problems with binary outputs like “Should I hire this candidate?”
    or “Should I show this website visitor advertisement A or advertisement B?” or
    “Will eating this food I found in the office fridge make me sick?”
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数人将决策树分为*分类树*（生成分类输出）和*回归树*（生成数值输出）。在本章中，我们将专注于分类树，并通过ID3算法从一组标记数据中学习决策树，这将帮助我们理解决策树的实际工作原理。为了简化问题，我们将局限于具有二元输出的问题，例如“我应该雇佣这位候选人吗？”或“我应该向这位网站访客展示广告A还是广告B？”或“我在办公室冰箱里找到的这种食物会让我生病吗？”
- en: Entropy
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 熵
- en: In order to build a decision tree, we will need to decide what questions to
    ask and in what order. At each stage of the tree there are some possibilities
    we’ve eliminated and some that we haven’t. After learning that an animal doesn’t
    have more than five legs, we’ve eliminated the possibility that it’s a grasshopper.
    We haven’t eliminated the possibility that it’s a duck. Each possible question
    partitions the remaining possibilities according to its answer.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 要构建决策树，我们需要决定提出什么问题以及顺序。在树的每个阶段，我们消除了一些可能性，还有一些没有。学到动物的腿不超过五条后，我们排除了它是蚱蜢的可能性。我们还没排除它是一只鸭子的可能性。每个可能的问题根据其答案将剩余的可能性进行分区。
- en: Ideally, we’d like to choose questions whose answers give a lot of information
    about what our tree should predict. If there’s a single yes/no question for which
    “yes” answers always correspond to `True` outputs and “no” answers to `False`
    outputs (or vice versa), this would be an awesome question to pick. Conversely,
    a yes/no question for which neither answer gives you much new information about
    what the prediction should be is probably not a good choice.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，我们希望选择的问题答案能够提供关于我们的树应该预测什么的大量信息。如果有一个单一的是/否问题，“是”答案总是对应于 `True` 输出，而“否”答案对应于
    `False` 输出（反之亦然），那这将是一个很棒的问题选择。相反，如果一个是/否问题的任何答案都不能给出关于预测应该是什么的新信息，那可能不是一个好选择。
- en: We capture this notion of “how much information” with *entropy*. You have probably
    heard this term used to mean disorder. We use it to represent the uncertainty
    associated with data.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们用*熵*来捕捉这种“信息量”概念。你可能听说过这个术语用来表示无序。我们用它来表示与数据相关的不确定性。
- en: Imagine that we have a set *S* of data, each member of which is labeled as belonging
    to one of a finite number of classes <math><mrow><msub><mi>C</mi> <mn>1</mn></msub>
    <mo>,</mo> <mo>...</mo> <mo>,</mo> <msub><mi>C</mi> <mi>n</mi></msub></mrow></math>
    . If all the data points belong to a single class, then there is no real uncertainty,
    which means we’d like there to be low entropy. If the data points are evenly spread
    across the classes, there is a lot of uncertainty and we’d like there to be high
    entropy.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 想象我们有一个数据集 *S*，其中每个成员都被标记为属于有限数量的类别 <math><mrow><msub><mi>C</mi> <mn>1</mn></msub>
    <mo>,</mo> <mo>...</mo> <mo>,</mo> <msub><mi>C</mi> <mi>n</mi></msub></mrow></math>
    中的一种。如果所有数据点属于同一类，则没有真正的不确定性，这意味着我们希望熵很低。如果数据点均匀分布在各个类别中，就会有很多不确定性，我们希望熵很高。
- en: 'In math terms, if <math><msub><mi>p</mi> <mi>i</mi></msub></math> is the proportion
    of data labeled as class <math><msub><mi>c</mi> <mi>i</mi></msub></math> , we
    define the entropy as:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学角度来看，如果 <math><msub><mi>p</mi> <mi>i</mi></msub></math> 是标记为类别 <math><msub><mi>c</mi>
    <mi>i</mi></msub></math> 的数据的比例，我们定义熵如下：
- en: <math alttext="upper H left-parenthesis upper S right-parenthesis equals minus
    p 1 log Subscript 2 Baseline p 1 minus ellipsis minus p Subscript n Baseline log
    Subscript 2 Baseline p Subscript n" display="block"><mrow><mi>H</mi> <mrow><mo>(</mo>
    <mi>S</mi> <mo>)</mo></mrow> <mo>=</mo> <mo>-</mo> <msub><mi>p</mi> <mn>1</mn></msub>
    <msub><mo form="prefix">log</mo> <mn>2</mn></msub> <msub><mi>p</mi> <mn>1</mn></msub>
    <mo>-</mo> <mo>...</mo> <mo>-</mo> <msub><mi>p</mi> <mi>n</mi></msub> <msub><mo
    form="prefix">log</mo> <mn>2</mn></msub> <msub><mi>p</mi> <mi>n</mi></msub></mrow></math>
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper H left-parenthesis upper S right-parenthesis equals minus
    p 1 log Subscript 2 Baseline p 1 minus ellipsis minus p Subscript n Baseline log
    Subscript 2 Baseline p Subscript n" display="block"><mrow><mi>H</mi> <mrow><mo>(</mo>
    <mi>S</mi> <mo>)</mo></mrow> <mo>=</mo> <mo>-</mo> <msub><mi>p</mi> <mn>1</mn></msub>
    <msub><mo form="prefix">log</mo> <mn>2</mn></msub> <msub><mi>p</mi> <mn>1</mn></msub>
    <mo>-</mo> <mo>...</mo> <mo>-</mo> <msub><mi>p</mi> <mi>n</mi></msub> <msub><mo
    form="prefix">log</mo> <mn>2</mn></msub> <msub><mi>p</mi> <mi>n</mi></msub></mrow></math>
- en: with the (standard) convention that <math><mrow><mn>0</mn> <mo form="prefix">log</mo>
    <mn>0</mn> <mo>=</mo> <mn>0</mn></mrow></math> .
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 按照（标准）惯例，<math><mrow><mn>0</mn> <mo form="prefix">log</mo> <mn>0</mn> <mo>=</mo>
    <mn>0</mn></mrow></math> 。
- en: Without worrying too much about the grisly details, each term <math><mrow><mo>-</mo>
    <msub><mi>p</mi> <mi>i</mi></msub> <msub><mo form="prefix">log</mo> <mn>2</mn></msub>
    <msub><mi>p</mi> <mi>i</mi></msub></mrow></math> is non-negative and is close
    to 0 precisely when <math><msub><mi>p</mi> <mi>i</mi></msub></math> is either
    close to 0 or close to 1 ([Figure 17-2](#p_log_p)).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 不必过分担心可怕的细节，每个术语 <math><mrow><mo>-</mo> <msub><mi>p</mi> <mi>i</mi></msub> <msub><mo
    form="prefix">log</mo> <mn>2</mn></msub> <msub><mi>p</mi> <mi>i</mi></msub></mrow></math>
    都是非负的，当 <math><msub><mi>p</mi> <mi>i</mi></msub></math> 接近于 0 或接近于 1 时，它接近于 0（[图 17-2](#p_log_p)）。
- en: '![A graph of –p log p.](assets/dsf2_1702.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![–p log p 的图示。](assets/dsf2_1702.png)'
- en: Figure 17-2\. A graph of -p log p
  id: totrans-36
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 17-2\. -p log p 的图示
- en: This means the entropy will be small when every <math><msub><mi>p</mi> <mi>i</mi></msub></math>
    is close to 0 or 1 (i.e., when most of the data is in a single class), and it
    will be larger when many of the <math><msub><mi>p</mi> <mi>i</mi></msub></math>
    ’s are not close to 0 (i.e., when the data is spread across multiple classes).
    This is exactly the behavior we desire.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着当每个 <math><msub><mi>p</mi> <mi>i</msub></math> 接近于 0 或 1 时（即大多数数据属于单一类别时），熵将很小，当许多
    <math><msub><mi>p</mi> <mi>i</msub></math> 不接近于 0 时（即数据分布在多个类别中时），熵将较大。这正是我们期望的行为。
- en: 'It is easy enough to roll all of this into a function:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有这些内容整合到一个函数中是相当简单的：
- en: '[PRE0]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Our data will consist of pairs `(input, label)`, which means that we’ll need
    to compute the class probabilities ourselves. Notice that we don’t actually care
    which label is associated with each probability, only what the probabilities are:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的数据将由成对的`(输入，标签)`组成，这意味着我们需要自己计算类别概率。注意，我们实际上并不关心每个概率与哪个标签相关联，只关心这些概率是多少：
- en: '[PRE1]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The Entropy of a Partition
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分区的熵
- en: What we’ve done so far is compute the entropy (think “uncertainty”) of a single
    set of labeled data. Now, each stage of a decision tree involves asking a question
    whose answer partitions data into one or (hopefully) more subsets. For instance,
    our “does it have more than five legs?” question partitions animals into those
    that have more than five legs (e.g., spiders) and those that don’t (e.g., echidnas).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们所做的是计算单一标记数据集的熵（想想“不确定性”）。现在，决策树的每个阶段都涉及提出一个问题，其答案将数据分成一个或多个子集。例如，我们的“它有超过五条腿吗？”问题将动物分成有超过五条腿的动物（例如，蜘蛛）和没有超过五条腿的动物（例如，针鼹）。
- en: Correspondingly, we’d like some notion of the entropy that results from partitioning
    a set of data in a certain way. We want a partition to have low entropy if it
    splits the data into subsets that themselves have low entropy (i.e., are highly
    certain), and high entropy if it contains subsets that (are large and) have high
    entropy (i.e., are highly uncertain).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 相应地，我们希望从某种程度上了解通过某种方式对数据集进行分区所产生的熵。如果分区将数据分成具有低熵（即高度确定）的子集，我们希望分区具有低熵；如果包含具有高熵（即高度不确定）的子集（即大且）分区具有高熵。
- en: For example, my “Australian five-cent coin” question was pretty dumb (albeit
    pretty lucky!), as it partitioned the remaining animals at that point into <math><msub><mi>S</mi>
    <mn>1</mn></msub></math> = {echidna} and <math><msub><mi>S</mi> <mn>2</mn></msub></math>
    = {everything else}, where <math><msub><mi>S</mi> <mn>2</mn></msub></math> is
    both large and high-entropy. ( <math><msub><mi>S</mi> <mn>1</mn></msub></math>
    has no entropy, but it represents a small fraction of the remaining “classes.”)
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我的“澳大利亚五分硬币”问题非常愚蠢（尽管非常幸运！），因为它将那时剩下的动物分成<math><msub><mi>S</mi> <mn>1</mn></msub></math>
    = {针鼹}和<math><msub><mi>S</mi> <mn>2</mn></msub></math> = {其他所有动物}，其中<math><msub><mi>S</mi>
    <mn>2</mn></msub></math>既大又高熵。 (<math><msub><mi>S</mi> <mn>1</mn></msub></math>没有熵，但它表示剩余“类别”的小部分。)
- en: 'Mathematically, if we partition our data *S* into subsets <math><mrow><msub><mi>S</mi>
    <mn>1</mn></msub> <mo>,</mo> <mo>...</mo> <mo>,</mo> <msub><mi>S</mi> <mi>m</mi></msub></mrow></math>
    containing proportions <math><mrow><msub><mi>q</mi> <mn>1</mn></msub> <mo>,</mo>
    <mo>...</mo> <mo>,</mo> <msub><mi>q</mi> <mi>m</mi></msub></mrow></math> of the
    data, then we compute the entropy of the partition as a weighted sum:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲，如果我们将我们的数据*S*分成包含数据比例的子集<math><mrow><msub><mi>S</mi> <mn>1</mn></msub>
    <mo>,</mo> <mo>...</mo> <mo>,</mo> <msub><mi>S</mi> <mi>m</mi></msub></mrow></math>，那么我们将分区的熵计算为加权和：
- en: <math alttext="upper H equals q 1 upper H left-parenthesis upper S 1 right-parenthesis
    plus period period period plus q Subscript m Baseline upper H left-parenthesis
    upper S Subscript m Baseline right-parenthesis" display="block"><mrow><mi>H</mi>
    <mo>=</mo> <msub><mi>q</mi> <mn>1</mn></msub> <mi>H</mi> <mrow><mo>(</mo> <msub><mi>S</mi>
    <mn>1</mn></msub> <mo>)</mo></mrow> <mo>+</mo> <mo>...</mo> <mo>+</mo> <msub><mi>q</mi>
    <mi>m</mi></msub> <mi>H</mi> <mrow><mo>(</mo> <msub><mi>S</mi> <mi>m</mi></msub>
    <mo>)</mo></mrow></mrow></math>
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper H equals q 1 upper H left-parenthesis upper S 1 right-parenthesis
    plus period period period plus q Subscript m Baseline upper H left-parenthesis
    upper S Subscript m Baseline right-parenthesis" display="block"><mrow><mi>H</mi>
    <mo>=</mo> <msub><mi>q</mi> <mn>1</mn></msub> <mi>H</mi> <mrow><mo>(</mo> <msub><mi>S</mi>
    <mn>1</mn></msub> <mo>)</mo></mrow> <mo>+</mo> <mo>...</mo> <mo>+</mo> <msub><mi>q</mi>
    <mi>m</mi></msub> <mi>H</mi> <mrow><mo>(</mo> <msub><mi>S</mi> <mi>m</mi></msub>
    <mo>)</mo></mrow></mrow></math>
- en: 'which we can implement as:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以实现为：
- en: '[PRE2]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Note
  id: totrans-50
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: One problem with this approach is that partitioning by an attribute with many
    different values will result in a very low entropy due to overfitting. For example,
    imagine you work for a bank and are trying to build a decision tree to predict
    which of your customers are likely to default on their mortgages, using some historical
    data as your training set. Imagine further that the dataset contains each customer’s
    Social Security number. Partitioning on SSN will produce one-person subsets, each
    of which necessarily has zero entropy. But a model that relies on SSN is *certain*
    not to generalize beyond the training set. For this reason, you should probably
    try to avoid (or bucket, if appropriate) attributes with large numbers of possible
    values when creating decision trees.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的一个问题是，使用具有许多不同值的属性进行分区会由于过拟合而导致熵非常低。例如，假设你在银行工作，试图建立一个决策树来预测哪些客户可能会违约他们的抵押贷款，使用一些历史数据作为你的训练集。进一步假设数据集包含每个客户的社会安全号码。在社会安全号码上进行分区将产生单个人的子集，每个子集的熵必然为零。但是依赖社会安全号码的模型*肯定*无法超出训练集的范围。因此，在创建决策树时，你应该尽量避免（或适当地分桶）具有大量可能值的属性。
- en: Creating a Decision Tree
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建决策树
- en: 'The VP provides you with the interviewee data, consisting of (per your specification)
    a `NamedTuple` of the relevant attributes for each candidate—her level, her preferred
    language, whether she is active on Twitter, whether she has a PhD, and whether
    she interviewed well:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 副总裁为您提供了面试者数据，根据您的规定，每位候选人的相关属性是一个`NamedTuple`——她的级别、她偏爱的语言、她是否在 Twitter 上活跃、她是否有博士学位以及她是否面试表现良好：
- en: '[PRE3]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Our tree will consist of *decision nodes* (which ask a question and direct
    us differently depending on the answer) and *leaf nodes* (which give us a prediction).
    We will build it using the relatively simple *ID3* algorithm, which operates in
    the following manner. Let’s say we’re given some labeled data, and a list of attributes
    to consider branching on:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的树将包含*决策节点*（提出问题并根据答案引导我们不同路径）和*叶节点*（给出预测）。我们将使用相对简单的*ID3*算法构建它，该算法操作如下。假设我们有一些带标签的数据，并且有一个要考虑分支的属性列表：
- en: If the data all have the same label, create a leaf node that predicts that label
    and then stop.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果所有数据都具有相同的标签，请创建一个预测该标签的叶节点，然后停止。
- en: If the list of attributes is empty (i.e., there are no more possible questions
    to ask), create a leaf node that predicts the most common label and then stop.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果属性列表为空（即没有更多可能的问题可问），创建一个预测最常见标签的叶节点，然后停止。
- en: Otherwise, try partitioning the data by each of the attributes.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 否则，尝试按照每个属性对数据进行分割。
- en: Choose the partition with the lowest partition entropy.
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择具有最低分区熵的分区。
- en: Add a decision node based on the chosen attribute.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据选择的属性添加一个决策节点。
- en: Recur on each partitioned subset using the remaining attributes.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对剩余属性使用递归对每个分区的子集进行分割。
- en: This is what’s known as a “greedy” algorithm because, at each step, it chooses
    the most immediately best option. Given a dataset, there may be a better tree
    with a worse-looking first move. If so, this algorithm won’t find it. Nonetheless,
    it is relatively easy to understand and implement, which makes it a good place
    to begin exploring decision trees.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这被称为“贪婪”算法，因为在每一步中，它选择最即时的最佳选项。给定一个数据集，可能有一个看起来更差的第一步，但却会有一个更好的树。如果确实存在这样的情况，此算法将无法找到它。尽管如此，它相对容易理解和实现，这使得它成为探索决策树的一个很好的起点。
- en: 'Let’s manually go through these steps on the interviewee dataset. The dataset
    has both `True` and `False` labels, and we have four attributes we can split on.
    So our first step will be to find the partition with the least entropy. We’ll
    start by writing a function that does the partitioning:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们手动按照面试者数据集中的这些步骤进行。数据集具有`True`和`False`标签，并且我们有四个可以分割的属性。因此，我们的第一步将是找到熵最小的分区。我们将首先编写一个执行分割的函数：
- en: '[PRE4]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'and one that uses it to compute entropy:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一个使用它计算熵的函数：
- en: '[PRE5]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Then we just need to find the minimum-entropy partition for the whole dataset:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们只需为整个数据集找到最小熵分区：
- en: '[PRE6]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The lowest entropy comes from splitting on `level`, so we’ll need to make a
    subtree for each possible `level` value. Every `Mid` candidate is labeled `True`,
    which means that the `Mid` subtree is simply a leaf node predicting `True`. For
    `Senior` candidates, we have a mix of `True`s and `False`s, so we need to split
    again:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 最低熵来自于按`level`分割，因此我们需要为每个可能的`level`值创建一个子树。每个`Mid`候选人都标记为`True`，这意味着`Mid`子树只是一个预测`True`的叶节点。对于`Senior`候选人，我们有`True`和`False`的混合，因此我们需要再次分割：
- en: '[PRE7]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This shows us that our next split should be on `tweets`, which results in a
    zero-entropy partition. For these `Senior`-level candidates, “yes” tweets always
    result in `True` while “no” tweets always result in `False`.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这告诉我们我们接下来应该在`tweets`上进行分割，这会导致零熵分区。对于这些`Senior`级别的候选人，“是”推文总是导致`True`，而“否”推文总是导致`False`。
- en: Finally, if we do the same thing for the `Junior` candidates, we end up splitting
    on `phd`, after which we find that no PhD always results in `True` and PhD always
    results in `False`.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，如果我们对`Junior`候选人执行相同的操作，我们最终会在`phd`上进行分割，之后发现没有博士学位总是导致`True`，而有博士学位总是导致`False`。
- en: '[Figure 17-3](#hiring_decision_tree) shows the complete decision tree.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 17-3](#hiring_decision_tree) 显示了完整的决策树。'
- en: '![Hiring Decision Tree.](assets/dsf2_1703.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![招聘决策树。](assets/dsf2_1703.png)'
- en: Figure 17-3\. The decision tree for hiring
  id: totrans-75
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 17-3\. 招聘的决策树
- en: Putting It All Together
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 把所有这些整合起来
- en: 'Now that we’ve seen how the algorithm works, we would like to implement it
    more generally. This means we need to decide how we want to represent trees. We’ll
    use pretty much the most lightweight representation possible. We define a *tree*
    to be either:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了算法的工作原理，我们希望更普遍地实现它。这意味着我们需要决定如何表示树。我们将使用可能最轻量级的表示。我们将一个*树*定义为以下内容之一：
- en: a `Leaf` (that predicts a single value), or
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个`Leaf`（预测单个值），或者
- en: a `Split` (containing an attribute to split on, subtrees for specific values
    of that attribute, and possibly a default value to use if we see an unknown value).
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个`Split`（包含要拆分的属性、特定属性值的子树，以及在遇到未知值时可能使用的默认值）。
- en: '[PRE8]'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'With this representation, our hiring tree would look like:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个表示，我们的招聘树将如下所示：
- en: '[PRE9]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: There’s still the question of what to do if we encounter an unexpected (or missing)
    attribute value. What should our hiring tree do if it encounters a candidate whose
    `level` is `Intern`? We’ll handle this case by populating the `default_value`
    attribute with the most common label.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一个问题，即如果我们遇到意外的（或缺失的）属性值该怎么办。如果我们的招聘树遇到`level`是`Intern`的候选人会怎么样？我们将通过用最常见的标签填充`default_value`属性来处理这种情况。
- en: 'Given such a representation, we can classify an input with:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 给定这样的表示，我们可以对输入进行分类：
- en: '[PRE10]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'All that’s left is to build the tree representation from our training data:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 剩下的就是从我们的训练数据中构建树表示：
- en: '[PRE11]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'In the tree we built, every leaf consisted entirely of `True` inputs or entirely
    of `False` inputs. This means that the tree predicts perfectly on the training
    dataset. But we can also apply it to new data that wasn’t in the training set:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们构建的树中，每个叶子节点完全由`True`输入或完全由`False`输入组成。这意味着该树在训练数据集上的预测完全正确。但我们也可以将其应用于训练集中不存在的新数据：
- en: '[PRE12]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'And also to data with unexpected values:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 也适用于具有意外值的数据：
- en: '[PRE13]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Note
  id: totrans-92
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Since our goal was mainly to demonstrate *how* to build a tree, we built the
    tree using the entire dataset. As always, if we were really trying to create a
    good model for something, we would have collected more data and split it into
    train/validation/test subsets.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的目标主要是演示如何构建树，所以我们使用整个数据集构建了树。一如既往，如果我们真的试图为某事创建一个良好的模型，我们会收集更多数据并将其分割成训练/验证/测试子集。
- en: Random Forests
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机森林
- en: Given how closely decision trees can fit themselves to their training data,
    it’s not surprising that they have a tendency to overfit. One way of avoiding
    this is a technique called *random forests*, in which we build multiple decision
    trees and combine their outputs. If they’re classification trees, we might let
    them vote; if they’re regression trees, we might average their predictions.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于决策树可以如此紧密地适应其训练数据，他们很容易出现过拟合的倾向并不奇怪。避免这种情况的一种方法是一种称为*随机森林*的技术，其中我们构建多个决策树并组合它们的输出。如果它们是分类树，我们可以让它们投票；如果它们是回归树，我们可以平均它们的预测。
- en: Our tree-building process was deterministic, so how do we get random trees?
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的树构建过程是确定性的，那么我们如何获得随机树呢？
- en: 'One piece involves bootstrapping data (recall [“Digression: The Bootstrap”](ch15.html#the_bootstrap)).
    Rather than training each tree on all the `inputs` in the training set, we train
    each tree on the result of `bootstrap_sample(inputs)`. Since each tree is built
    using different data, each tree will be different from every other tree. (A side
    benefit is that it’s totally fair to use the nonsampled data to test each tree,
    which means you can get away with using all of your data as the training set if
    you are clever in how you measure performance.) This technique is known as *bootstrap
    aggregating* or *bagging*.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一部分涉及引导数据（参见[“插曲：自助法”](ch15.html#the_bootstrap)）。我们不是在整个训练集上训练每棵树，而是在`bootstrap_sample(inputs)`的结果上训练每棵树。由于每棵树都是使用不同的数据构建的，因此每棵树与其他每棵树都不同。（一个副作用是，使用未抽样数据来测试每棵树是完全公平的，这意味着如果在衡量性能时聪明地使用所有数据作为训练集，你就可以获得成功。）这种技术被称为*自助聚合*或*装袋*。
- en: 'A second source of randomness involves changing the way we choose the `best_attribute`
    to split on. Rather than looking at all the remaining attributes, we first choose
    a random subset of them and then split on whichever of those is best:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个随机性源涉及更改选择要拆分的`best_attribute`的方法。我们不是查看所有剩余属性，而是首先选择它们的随机子集，然后在其中最好的属性上进行拆分：
- en: '[PRE14]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: This is an example of a broader technique called *ensemble learning* in which
    we combine several *weak learners* (typically high-bias, low-variance models)
    in order to produce an overall strong model.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这是*集成学习*的一个例子，其中我们结合了多个*弱学习器*（通常是高偏差、低方差模型），以生成一个总体上强大的模型。
- en: For Further Exploration
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步探索
- en: scikit-learn has many [decision tree](https://scikit-learn.org/stable/modules/tree.html)
    models. It also has an [`ensemble`](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.ensemble)
    module that includes a `RandomForestClassifier` as well as other ensemble methods.
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: scikit-learn 包含许多[决策树](https://scikit-learn.org/stable/modules/tree.html)模型。它还有一个[`ensemble`](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.ensemble)模块，其中包括`RandomForestClassifier`以及其他集成方法。
- en: '[XGBoost](https://xgboost.ai/) is a library for training *gradient boosted*
    decision trees that tends to win a lot of Kaggle-style machine learning competitions.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[XGBoost](https://xgboost.ai/) 是一个用于训练*梯度提升*决策树的库，经常在许多 Kaggle 风格的机器学习竞赛中获胜。'
- en: We’ve barely scratched the surface of decision trees and their algorithms. [Wikipedia](https://en.wikipedia.org/wiki/Decision_tree_learning)
    is a good starting point for broader exploration.
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们只是浅尝了解决策树及其算法的表面。[维基百科](https://en.wikipedia.org/wiki/Decision_tree_learning)是一个更广泛探索的良好起点。
