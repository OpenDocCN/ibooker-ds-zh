["```py\ndata_without_header = spark.read.option(\"inferSchema\", True)\\\n                      .option(\"header\", False).csv(\"data/covtype.data\")\ndata_without_header.printSchema()\n...\nroot\n |-- _c0: integer (nullable = true)\n |-- _c1: integer (nullable = true)\n |-- _c2: integer (nullable = true)\n |-- _c3: integer (nullable = true)\n |-- _c4: integer (nullable = true)\n |-- _c5: integer (nullable = true)\n ...\n```", "```py\n$ cat data/covtype.info\n\n...\n[...]\n7.\tAttribute information:\n\nGiven is the attribute name, attribute type, the measurement unit and\na brief description.  The forest cover type is the classification\nproblem.  The order of this listing corresponds to the order of\nnumerals along the rows of the database.\n\nName                                    Data Type\nElevation                               quantitative\nAspect                                  quantitative\nSlope                                   quantitative\nHorizontal_Distance_To_Hydrology        quantitative\nVertical_Distance_To_Hydrology          quantitative\nHorizontal_Distance_To_Roadways         quantitative\nHillshade_9am                           quantitative\nHillshade_Noon                          quantitative\nHillshade_3pm                           quantitative\nHorizontal_Distance_To_Fire_Points      quantitative\nWilderness_Area (4 binary columns)      qualitative\nSoil_Type (40 binary columns)           qualitative\nCover_Type (7 types)                    integer\n\nMeasurement                  Description\n\nmeters                       Elevation in meters\nazimuth                      Aspect in degrees azimuth\ndegrees                      Slope in degrees\nmeters                       Horz Dist to nearest surface water features\nmeters                       Vert Dist to nearest surface water features\nmeters                       Horz Dist to nearest roadway\n0 to 255 index               Hillshade index at 9am, summer solstice\n0 to 255 index               Hillshade index at noon, summer soltice\n0 to 255 index               Hillshade index at 3pm, summer solstice\nmeters                       Horz Dist to nearest wildfire ignition point\n0 (absence) or 1 (presence)  Wilderness area designation\n0 (absence) or 1 (presence)  Soil Type designation\n1 to 7                       Forest Cover Type designation\n...\n```", "```py\nfrom pyspark.sql.types import DoubleType\nfrom pyspark.sql.functions import col\n\ncolnames = [\"Elevation\", \"Aspect\", \"Slope\", \\\n            \"Horizontal_Distance_To_Hydrology\", \\\n            \"Vertical_Distance_To_Hydrology\", \"Horizontal_Distance_To_Roadways\", \\\n            \"Hillshade_9am\", \"Hillshade_Noon\", \"Hillshade_3pm\", \\\n            \"Horizontal_Distance_To_Fire_Points\"] + \\ ![1](assets/1.png)\n[f\"Wilderness_Area_{i}\" for i in range(4)] + \\ [f\"Soil_Type_{i}\" for i in range(40)] + \\ [\"Cover_Type\"]\n\ndata = data_without_header.toDF(*colnames).\\\n                          withColumn(\"Cover_Type\",\n                                    col(\"Cover_Type\").cast(DoubleType()))\n\ndata.head()\n...\nRow(Elevation=2596,Aspect=51,Slope=3,Horizontal_Distance_To_Hydrology=258,...)\n```", "```py\n(train_data, test_data) = data.randomSplit([0.9, 0.1])\ntrain_data.cache()\ntest_data.cache()\n```", "```py\nfrom pyspark.ml.feature import VectorAssembler\n\ninput_cols = colnames[:-1] ![1](assets/1.png)\nvector_assembler = VectorAssembler(inputCols=input_cols,\n                                    outputCol=\"featureVector\")\n\nassembled_train_data = vector_assembler.transform(train_data)\n\nassembled_train_data.select(\"featureVector\").show(truncate = False)\n...\n+------------------------------------------------------------------- ...\n|featureVector                                                       ...\n+------------------------------------------------------------------- ...\n|(54,[0,1,2,5,6,7,8,9,13,18],[1874.0,18.0,14.0,90.0,208.0,209.0, ...\n|(54,[0,1,2,3,4,5,6,7,8,9,13,18],[1879.0,28.0,19.0,30.0,12.0,95.0, ...\n...\n```", "```py\nfrom pyspark.ml.classification import DecisionTreeClassifier\n\nclassifier = DecisionTreeClassifier(seed = 1234, labelCol=\"Cover_Type\",\n                                    featuresCol=\"featureVector\",\n                                    predictionCol=\"prediction\")\n\nmodel = classifier.fit(assembled_train_data)\nprint(model.toDebugString)\n...\nDecisionTreeClassificationModel: uid=DecisionTreeClassifier_da03f8ab5e28, ...\n  If (feature 0 <= 3036.5)\n   If (feature 0 <= 2546.5)\n    If (feature 10 <= 0.5)\n     If (feature 0 <= 2412.5)\n      If (feature 3 <= 15.0)\n       Predict: 4.0\n      Else (feature 3 > 15.0)\n       Predict: 3.0\n     Else (feature 0 > 2412.5)\n       ...\n```", "```py\nimport pandas as pd\n\npd.DataFrame(model.featureImportances.toArray(),\n            index=input_cols, columns=['importance']).\\\n            sort_values(by=\"importance\", ascending=False)\n...\n                                  importance\nElevation                         0.826854\nHillshade_Noon                    0.029087\nSoil_Type_1                       0.028647\nSoil_Type_3                       0.026447\nWilderness_Area_0                 0.024917\nHorizontal_Distance_To_Hydrology  0.024862\nSoil_Type_31                      0.018573\nWilderness_Area_2                 0.012458\nHorizontal_Distance_To_Roadways   0.003608\nHillshade_9am                     0.002840\n...\n```", "```py\npredictions = model.transform(assembled_train_data)\npredictions.select(\"Cover_Type\", \"prediction\", \"probability\").\\\n            show(10, truncate = False)\n\n...\n+----------+----------+------------------------------------------------ ...\n|Cover_Type|prediction|probability                                      ...\n+----------+----------+------------------------------------------------ ...\n|6.0       |4.0       |[0.0,0.0,0.028372324539571926,0.2936784469885515, ...\n|6.0       |3.0       |[0.0,0.0,0.024558587479935796,0.6454654895666132, ...\n|6.0       |3.0       |[0.0,0.0,0.024558587479935796,0.6454654895666132, ...\n|6.0       |3.0       |[0.0,0.0,0.024558587479935796,0.6454654895666132, ...\n...\n```", "```py\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\n\nevaluator = MulticlassClassificationEvaluator(labelCol=\"Cover_Type\",\n                                        predictionCol=\"prediction\")\n\nevaluator.setMetricName(\"accuracy\").evaluate(predictions)\nevaluator.setMetricName(\"f1\").evaluate(predictions)\n\n...\n0.6989423087953562\n0.6821216079701136\n```", "```py\nconfusion_matrix = predictions.groupBy(\"Cover_Type\").\\\n  pivot(\"prediction\", range(1,8)).count().\\\n  na.fill(0.0).\\ ![1](assets/1.png)\n  orderBy(\"Cover_Type\")\n\nconfusion_matrix.show()\n\n...\n\n+----------+------+------+-----+---+---+---+-----+\n|Cover_Type|     1|     2|    3|  4|  5|  6|    7|\n+----------+------+------+-----+---+---+---+-----+\n|       1.0|133792| 51547|  109|  0|  0|  0| 5223|\n|       2.0| 57026|192260| 4888| 57|  0|  0|  750|\n|       3.0|     0|  3368|28238|590|  0|  0|    0|\n|       4.0|     0|     0| 1493|956|  0|  0|    0|\n|       5.0|     0|  8282|  283|  0|  0|  0|    0|\n|       6.0|     0|  3371|11872|406|  0|  0|    0|\n|       7.0|  8122|    74|    0|  0|  0|  0|10319|\n+----------+------+------+-----+---+---+---+-----+\n```", "```py\nfrom pyspark.sql import DataFrame\n\ndef class_probabilities(data):\n    total = data.count()\n    return data.groupBy(\"Cover_Type\").count().\\ ![1](assets/1.png)\n    orderBy(\"Cover_Type\").\\ ![2](assets/2.png)\n    select(col(\"count\").cast(DoubleType())).\\\n    withColumn(\"count_proportion\", col(\"count\")/total).\\\n    select(\"count_proportion\").collect()\n\ntrain_prior_probabilities = class_probabilities(train_data)\ntest_prior_probabilities = class_probabilities(test_data)\n\ntrain_prior_probabilities\n...\n\n[Row(count_proportion=0.36455357859838705),\n Row(count_proportion=0.4875111371136425),\n Row(count_proportion=0.06155716924206445),\n Row(count_proportion=0.00468236760696409),\n Row(count_proportion=0.016375858943914835),\n Row(count_proportion=0.029920118693908142),\n Row(count_proportion=0.03539976980111887)]\n\n...\n\ntrain_prior_probabilities = [p[0] for p in train_prior_probabilities]\ntest_prior_probabilities = [p[0] for p in test_prior_probabilities]\n\nsum([train_p * cv_p for train_p, cv_p in zip(train_prior_probabilities,\n                                              test_prior_probabilities)]) ![3](assets/3.png)\n...\n\n0.37735294664034547\n```", "```py\nfrom pyspark.ml import Pipeline\n\nassembler = VectorAssembler(inputCols=input_cols, outputCol=\"featureVector\")\nclassifier = DecisionTreeClassifier(seed=1234, labelCol=\"Cover_Type\",\n                                    featuresCol=\"featureVector\",\n                                    predictionCol=\"prediction\")\n\npipeline = Pipeline(stages=[assembler, classifier])\n```", "```py\nfrom pyspark.ml.tuning import ParamGridBuilder\n\nparamGrid = ParamGridBuilder(). \\\n  addGrid(classifier.impurity, [\"gini\", \"entropy\"]). \\\n  addGrid(classifier.maxDepth, [1, 20]). \\\n  addGrid(classifier.maxBins, [40, 300]). \\\n  addGrid(classifier.minInfoGain, [0.0, 0.05]). \\\n  build()\n\nmulticlassEval = MulticlassClassificationEvaluator(). \\\n  setLabelCol(\"Cover_Type\"). \\\n  setPredictionCol(\"prediction\"). \\\n  setMetricName(\"accuracy\")\n```", "```py\nfrom pyspark.ml.tuning import TrainValidationSplit\n\nvalidator = TrainValidationSplit(seed=1234,\n  estimator=pipeline,\n  evaluator=multiclassEval,\n  estimatorParamMaps=paramGrid,\n  trainRatio=0.9)\n\nvalidator_model = validator.fit(train_data)\n```", "```py\nfrom pprint import pprint\n\nbest_model = validator_model.bestModel\npprint(best_model.stages[1].extractParamMap())\n\n...\n{Param(...name='predictionCol', doc='prediction column name.'): 'prediction',\n Param(...name='probabilityCol', doc='...'): 'probability',\n [...]\n Param(...name='impurity', doc='...'): 'entropy',\n Param(...name='maxDepth', doc='...'): 20,\n Param(...name='minInfoGain', doc='...'): 0.0,\n [...]\n Param(...name='featuresCol', doc='features column name.'): 'featureVector',\n Param(...name='maxBins', doc='...'): 40,\n [...]\n Param(...name='labelCol', doc='label column name.'): 'Cover_Type'}\n ...\n}\n```", "```py\nvalidator_model = validator.fit(train_data)\n\nmetrics = validator_model.validationMetrics\nparams = validator_model.getEstimatorParamMaps()\nmetrics_and_params = list(zip(metrics, params))\n\nmetrics_and_params.sort(key=lambda x: x[0], reverse=True)\nmetrics_and_params\n\n...\n[(0.9130409881445563,\n  {Param(...name='minInfoGain' ...): 0.0,\n   Param(...name='maxDepth'...): 20,\n   Param(...name='maxBins' ...): 40,\n   Param(...name='impurity'...): 'entropy'}),\n (0.9112655352131498,\n  {Param(...name='minInfoGain',...): 0.0,\n   Param(...name='maxDepth' ...): 20,\n   Param(...name='maxBins'...): 300,\n   Param(...name='impurity'...: 'entropy'}),\n...\n```", "```py\nmetrics.sort(reverse=True)\nprint(metrics[0])\n...\n\n0.9130409881445563\n...\n\nmulticlassEval.evaluate(best_model.transform(test_data)) ![1](assets/1.png)\n\n...\n0.9138921373048084\n```", "```py\ndef unencode_one_hot(data):\n    wilderness_cols = ['Wilderness_Area_' + str(i) for i in range(4)]\n    wilderness_assembler = VectorAssembler().\\\n                            setInputCols(wilderness_cols).\\\n                            setOutputCol(\"wilderness\")\n\n    unhot_udf = udf(lambda v: v.toArray().tolist().index(1)) ![1](assets/1.png)\n\n    with_wilderness = wilderness_assembler.transform(data).\\\n      drop(*wilderness_cols).\\ ![2](assets/2.png)\n      withColumn(\"wilderness\", unhot_udf(col(\"wilderness\")))\n\n    soil_cols = ['Soil_Type_' + str(i) for i in range(40)]\n    soil_assembler = VectorAssembler().\\\n                      setInputCols(soil_cols).\\\n                      setOutputCol(\"soil\")\n    with_soil = soil_assembler.\\\n                transform(with_wilderness).\\\n                drop(*soil_cols).\\\n                withColumn(\"soil\", unhot_udf(col(\"soil\")))\n\n    return with_soil\n```", "```py\nunenc_train_data = unencode_one_hot(train_data)\nunenc_train_data.printSchema()\n...\nroot\n |-- Elevation: integer (nullable = true)\n |-- Aspect: integer (nullable = true)\n |-- Slope: integer (nullable = true)\n |-- Horizontal_Distance_To_Hydrology: integer (nullable = true)\n |-- Vertical_Distance_To_Hydrology: integer (nullable = true)\n |-- Horizontal_Distance_To_Roadways: integer (nullable = true)\n |-- Hillshade_9am: integer (nullable = true)\n |-- Hillshade_Noon: integer (nullable = true)\n |-- Hillshade_3pm: integer (nullable = true)\n |-- Horizontal_Distance_To_Fire_Points: integer (nullable = true)\n |-- Cover_Type: double (nullable = true)\n |-- wilderness: string (nullable = true)\n |-- soil: string (nullable = true)\n...\n\nunenc_train_data.groupBy('wilderness').count().show()\n...\n\n+----------+------+\n|wilderness| count|\n+----------+------+\n|         3| 33271|\n|         0|234532|\n|         1| 26917|\n|         2|228144|\n+----------+------+\n```", "```py\nfrom pyspark.ml.feature import VectorIndexer\n\ncols = unenc_train_data.columns\ninputCols = [c for c in cols if c!='Cover_Type']\n\nassembler = VectorAssembler().setInputCols(inputCols).setOutputCol(\"featureVector\")\n\nindexer = VectorIndexer().\\\n  setMaxCategories(40).\\ ![1](assets/1.png)\n  setInputCol(\"featureVector\").setOutputCol(\"indexedVector\")\n\nclassifier = DecisionTreeClassifier().setLabelCol(\"Cover_Type\").\\\n                                      setFeaturesCol(\"indexedVector\").\\\n                                      setPredictionCol(\"prediction\")\n\npipeline = Pipeline().setStages([assembler, indexer, classifier])\n```", "```py\nfrom pyspark.ml.classification import RandomForestClassifier\n\nclassifier = RandomForestClassifier(seed=1234, labelCol=\"Cover_Type\",\n                                    featuresCol=\"indexedVector\",\n                                    predictionCol=\"prediction\")\n```", "```py\nforest_model = best_model.stages[1]\n\nfeature_importance_list = list(zip(input_cols,\n                                  forest_model.featureImportances.toArray()))\nfeature_importance_list.sort(key=lambda x: x[1], reverse=True)\n\npprint(feature_importance_list)\n...\n(0.28877055118903183,Elevation)\n(0.17288279582959612,soil)\n(0.12105056811661499,Horizontal_Distance_To_Roadways)\n(0.1121550648692802,Horizontal_Distance_To_Fire_Points)\n(0.08805270405239551,wilderness)\n(0.04467393191338021,Vertical_Distance_To_Hydrology)\n(0.04293099150373547,Horizontal_Distance_To_Hydrology)\n(0.03149644050848614,Hillshade_Noon)\n(0.028408483578137605,Hillshade_9am)\n(0.027185325937200706,Aspect)\n(0.027075578474331806,Hillshade_3pm)\n(0.015317564027809389,Slope)\n```", "```py\nunenc_test_data = unencode_one_hot(test_data)\nbestModel.transform(unenc_test_data.drop(\"Cover_Type\")).\\\n                    select(\"prediction\").show()\n\n...\n+----------+\n|prediction|\n+----------+\n|       6.0|\n+----------+\n```"]