- en: Chapter 9\. Getting Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To write it, it took three months; to conceive it, three minutes; to collect
    the data in it, all my life.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: F. Scott Fitzgerald
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In order to be a data scientist you need data. In fact, as a data scientist
    you will spend an embarrassingly large fraction of your time acquiring, cleaning,
    and transforming data. In a pinch, you can always type the data in yourself (or
    if you have minions, make them do it), but usually this is not a good use of your
    time. In this chapter, we’ll look at different ways of getting data into Python
    and into the right formats.
  prefs: []
  type: TYPE_NORMAL
- en: stdin and stdout
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you run your Python scripts at the command line, you can *pipe* data through
    them using `sys.stdin` and `sys.stdout`. For example, here is a script that reads
    in lines of text and spits back out the ones that match a regular expression:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'And here’s one that counts the lines it receives and then writes out the count:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'You could then use these to count how many lines of a file contain numbers.
    In Windows, you’d use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'whereas in a Unix system you’d use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The | is the pipe character, which means “use the output of the left command
    as the input of the right command.” You can build pretty elaborate data-processing
    pipelines this way.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'If you are using Windows, you can probably leave out the `python` part of this
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: If you are on a Unix system, doing so requires [a couple more steps](https://stackoverflow.com/questions/15587877/run-a-python-script-in-terminal-without-the-python-command).
    First add a “shebang” as the first line of your script `#!/usr/bin/env python`.
    Then, at the command line, use `chmod` x egrep.py++ to make the file executable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, here’s a script that counts the words in its input and writes out
    the most common ones:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'after which you could do something like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: (If you are using Windows, then use `type` instead of `cat`.)
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If you are a seasoned Unix programmer, you are probably familiar with a wide
    variety of command-line tools (for example, `egrep`) that are built into your
    operating system and are preferable to building your own from scratch. Still,
    it’s good to know you can if you need to.
  prefs: []
  type: TYPE_NORMAL
- en: Reading Files
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can also explicitly read from and write to files directly in your code.
    Python makes working with files pretty simple.
  prefs: []
  type: TYPE_NORMAL
- en: The Basics of Text Files
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The first step to working with a text file is to obtain a *file object* using
    `open`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Because it is easy to forget to close your files, you should always use them
    in a `with` block, at the end of which they will be closed automatically:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'If you need to read a whole text file, you can just iterate over the lines
    of the file using `for`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Every line you get this way ends in a newline character, so you’ll often want
    to `strip` it before doing anything with it.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, imagine you have a file full of email addresses, one per line,
    and you need to generate a histogram of the domains. The rules for correctly extracting
    domains are somewhat subtle—see, e.g., the [Public Suffix List](https://publicsuffix.org)—but
    a good first approximation is to just take the parts of the email addresses that
    come after the *@* (this gives the wrong answer for email addresses like *joel@mail.datasciencester.com*,
    but for the purposes of this example we’re willing to live with that):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Delimited Files
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The hypothetical email addresses file we just processed had one address per
    line. More frequently you’ll work with files with lots of data on each line. These
    files are very often either *comma-separated* or *tab-separated*: each line has
    several fields, with a comma or a tab indicating where one field ends and the
    next field starts.'
  prefs: []
  type: TYPE_NORMAL
- en: This starts to get complicated when you have fields with commas and tabs and
    newlines in them (which you inevitably will). For this reason, you should never
    try to parse them yourself. Instead, you should use Python’s `csv` module (or
    the pandas library, or some other library that’s designed to read comma-separated
    or tab-delimited files).
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Never parse a comma-separated file yourself. You will screw up the edge cases!
  prefs: []
  type: TYPE_NORMAL
- en: If your file has no headers (which means you probably want each row as a `list`,
    and which places the burden on you to know what’s in each column), you can use
    `csv.reader` to iterate over the rows, each of which will be an appropriately
    split list.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, if we had a tab-delimited file of stock prices:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'we could process them with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'If your file has headers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'you can either skip the header row with an initial call to `reader.next`, or
    get each row as a `dict` (with the headers as keys) by using `csv.DictReader`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Even if your file doesn’t have headers, you can still use `DictReader` by passing
    it the keys as a `fieldnames` parameter.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can similarly write out delimited data using `csv.writer`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '`csv.writer` will do the right thing if your fields themselves have commas
    in them. Your own hand-rolled writer probably won’t. For example, if you attempt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'You will end up with a *.csv* file that looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: and that no one will ever be able to make sense of.
  prefs: []
  type: TYPE_NORMAL
- en: Scraping the Web
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another way to get data is by scraping it from web pages. Fetching web pages,
    it turns out, is pretty easy; getting meaningful structured information out of
    them less so.
  prefs: []
  type: TYPE_NORMAL
- en: HTML and the Parsing Thereof
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Pages on the web are written in HTML, in which text is (ideally) marked up
    into elements and their attributes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: In a perfect world, where all web pages were marked up semantically for our
    benefit, we would be able to extract data using rules like “find the `<p>` element
    whose `id` is `subject` and return the text it contains.” In the actual world,
    HTML is not generally well formed, let alone annotated. This means we’ll need
    help making sense of it.
  prefs: []
  type: TYPE_NORMAL
- en: To get data out of HTML, we will use the [Beautiful Soup library](http://www.crummy.com/software/BeautifulSoup/),
    which builds a tree out of the various elements on a web page and provides a simple
    interface for accessing them. As I write this, the latest version is Beautiful
    Soup 4.6.0, which is what we’ll be using. We’ll also be using the [Requests library](http://docs.python-requests.org/en/latest/),
    which is a much nicer way of making HTTP requests than anything that’s built into
    Python.
  prefs: []
  type: TYPE_NORMAL
- en: Python’s built-in HTML parser is not that lenient, which means that it doesn’t
    always cope well with HTML that’s not perfectly formed. For that reason, we’ll
    also install the `html5lib` parser.
  prefs: []
  type: TYPE_NORMAL
- en: 'Making sure you’re in the correct virtual environment, install the libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'To use Beautiful Soup, we pass a string containing HTML into the `BeautifulSoup`
    function. In our examples, this will be the result of a call to `requests.get`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: after which we can get pretty far using a few simple methods.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll typically work with `Tag` objects, which correspond to the tags representing
    the structure of an HTML page.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, to find the first `<p>` tag (and its contents), you can use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'You can get the text contents of a `Tag` using its `text` property:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'And you can extract a tag’s attributes by treating it like a `dict`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'You can get multiple tags at once as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Frequently, you’ll want to find tags with a specific `class`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'And you can combine these methods to implement more elaborate logic. For example,
    if you want to find every `<span>` element that is contained inside a `<div>`
    element, you could do this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Just this handful of features will allow us to do quite a lot. If you end up
    needing to do more complicated things (or if you’re just curious), check the [documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/).
  prefs: []
  type: TYPE_NORMAL
- en: Of course, the important data won’t typically be labeled as `class="important"`.
    You’ll need to carefully inspect the source HTML, reason through your selection
    logic, and worry about edge cases to make sure your data is correct. Let’s look
    at an example.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: Keeping Tabs on Congress'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The VP of Policy at DataSciencester is worried about potential regulation of
    the data science industry and asks you to quantify what Congress is saying on
    the topic. In particular, he wants you to find all the representatives who have
    press releases about “data.”
  prefs: []
  type: TYPE_NORMAL
- en: At the time of publication, there is a page with links to all of the representatives’
    websites at *[*https://www.house.gov/representatives*](https://www.house.gov/representatives)*.
  prefs: []
  type: TYPE_NORMAL
- en: 'And if you “view source,” all of the links to the websites look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s start by collecting all of the URLs linked to from that page:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: This returns way too many URLs. If you look at them, the ones we want start
    with either *http://* or *https://*, have some kind of name, and end with either
    *.house.gov* or *.house.gov/*.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is a good place to use a regular expression:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'That’s still way too many, as there are only 435 representatives. If you look
    at the list, there are a lot of duplicates. Let’s use `set` to get rid of them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'There are always a couple of House seats empty, or maybe there’s a representative
    without a website. In any case, this is good enough. When we look at the sites,
    most of them have a link to press releases. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice that this is a relative link, which means we need to remember the originating
    site. Let’s do some scraping:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Normally it is impolite to scrape a site freely like this. Most sites will have
    a *robots.txt* file that indicates how frequently you may scrape the site (and
    which paths you’re not supposed to scrape), but since it’s Congress we don’t need
    to be particularly polite.
  prefs: []
  type: TYPE_NORMAL
- en: If you watch these as they scroll by, you’ll see a lot of */media/press-releases*
    and *media-center/press-releases*, as well as various other addresses. One of
    these URLs is *[*https://jayapal.house.gov/media/press-releases*](https://jayapal.house.gov/media/press-releases)*.
  prefs: []
  type: TYPE_NORMAL
- en: Remember that our goal is to find out which congresspeople have press releases
    mentioning “data.” We’ll write a slightly more general function that checks whether
    a page of press releases mentions any given term.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you visit the site and view the source, it seems like there’s a snippet
    from each press release inside a `<p>` tag, so we’ll use that as our first attempt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s write a quick test for it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'At last we’re ready to find the relevant congresspeople and give their names
    to the VP:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: When I run this I get a list of about 20 representatives. Your results will
    probably be different.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If you look at the various “press releases” pages, most of them are paginated
    with only 5 or 10 press releases per page. This means that we only retrieved the
    few most recent press releases for each congressperson. A more thorough solution
    would have iterated over the pages and retrieved the full text of each press release.
  prefs: []
  type: TYPE_NORMAL
- en: Using APIs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Many websites and web services provide *application programming interfaces*
    (APIs), which allow you to explicitly request data in a structured format. This
    saves you the trouble of having to scrape them!
  prefs: []
  type: TYPE_NORMAL
- en: JSON and XML
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Because HTTP is a protocol for transferring *text*, the data you request through
    a web API needs to be *serialized* into a string format. Often this serialization
    uses *JavaScript Object Notation* (JSON). JavaScript objects look quite similar
    to Python `dict`s, which makes their string representations easy to interpret:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'We can parse JSON using Python’s `json` module. In particular, we will use
    its `loads` function, which deserializes a string representing a JSON object into
    a Python object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Sometimes an API provider hates you and provides only responses in XML:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: You can use Beautiful Soup to get data from XML similarly to how we used it
    to get data from HTML; check its documentation for details.
  prefs: []
  type: TYPE_NORMAL
- en: Using an Unauthenticated API
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Most APIs these days require that you first authenticate yourself before you
    can use them. While we don’t begrudge them this policy, it creates a lot of extra
    boilerplate that muddies up our exposition. Accordingly, we’ll start by taking
    a look at [GitHub’s API](http://developer.github.com/v3/), with which you can
    do some simple things unauthenticated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: At this point `repos` is a `list` of Python `dict`s, each representing a public
    repository in my GitHub account. (Feel free to substitute your username and get
    your GitHub repository data instead. You do have a GitHub account, right?)
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use this to figure out which months and days of the week I’m most likely
    to create a repository. The only issue is that the dates in the response are strings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Python doesn’t come with a great date parser, so we’ll need to install one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'from which you’ll probably only ever need the `dateutil.parser.parse` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, you can get the languages of my last five repositories:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Typically we won’t be working with APIs at this low “make the requests and parse
    the responses ourselves” level. One of the benefits of using Python is that someone
    has already built a library for pretty much any API you’re interested in accessing.
    When they’re done well, these libraries can save you a lot of the trouble of figuring
    out the hairier details of API access. (When they’re not done well, or when it
    turns out they’re based on defunct versions of the corresponding APIs, they can
    cause you enormous headaches.)
  prefs: []
  type: TYPE_NORMAL
- en: Nonetheless, you’ll occasionally have to roll your own API access library (or,
    more likely, debug why someone else’s isn’t working), so it’s good to know some
    of the details.
  prefs: []
  type: TYPE_NORMAL
- en: Finding APIs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you need data from a specific site, look for a “developers” or “API” section
    of the site for details, and try searching the web for “python <sitename> api”
    to find a library.
  prefs: []
  type: TYPE_NORMAL
- en: There are libraries for the Yelp API, for the Instagram API, for the Spotify
    API, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: If you’re looking for a list of APIs that have Python wrappers, there’s a nice
    one from [Real Python on GitHub](https://github.com/realpython/list-of-python-api-wrappers).
  prefs: []
  type: TYPE_NORMAL
- en: And if you can’t find what you need, there’s always scraping, the last refuge
    of the data scientist.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: Using the Twitter APIs'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Twitter is a fantastic source of data to work with. You can use it to get real-time
    news. You can use it to measure reactions to current events. You can use it to
    find links related to specific topics. You can use it for pretty much anything
    you can imagine, just as long as you can get access to its data. And you can get
    access to its data through its APIs.
  prefs: []
  type: TYPE_NORMAL
- en: To interact with the Twitter APIs, we’ll be using the [Twython library](https://github.com/ryanmcgrath/twython)
    (`python -m pip install twython`). There are quite a few Python Twitter libraries
    out there, but this is the one that I’ve had the most success working with. You
    are encouraged to explore the others as well!
  prefs: []
  type: TYPE_NORMAL
- en: Getting Credentials
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In order to use Twitter’s APIs, you need to get some credentials (for which
    you need a Twitter account, which you should have anyway so that you can be part
    of the lively and friendly Twitter #datascience community).'
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Like all instructions that relate to websites that I don’t control, these may
    become obsolete at some point but will hopefully work for a while. (Although they
    have already changed multiple times since I originally started writing this book,
    so good luck!)
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Go to [*https://developer.twitter.com/*](https://developer.twitter.com/).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you are not signed in, click “Sign in” and enter your Twitter username and
    password.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click Apply to apply for a developer account.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Request access for your own personal use.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fill out the application. It requires 300 words (really) on why you need access,
    so to get over the limit you could tell them about this book and how much you’re
    enjoying it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Wait some indefinite amount of time.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you know someone who works at Twitter, email them and ask them if they can
    expedite your application. Otherwise, keep waiting.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once you get approved, go back to [developer.twitter.com](https://developer.twitter.com/),
    find the “Apps” section, and click “Create an app.”
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fill out all the required fields (again, if you need extra characters for the
    description, you could talk about this book and how edifying you’re finding it).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click CREATE.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now your app should have a “Keys and tokens” tab with a “Consumer API keys”
    section that lists an “API key” and an “API secret key.” Take note of those keys;
    you’ll need them. (Also, keep them secret! They’re like passwords.)
  prefs: []
  type: TYPE_NORMAL
- en: Caution
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Don’t share the keys, don’t publish them in your book, and don’t check them
    into your public GitHub repository. One simple solution is to store them in a
    *credentials.json* file that doesn’t get checked in, and to have your code use
    `json.loads` to retrieve them. Another solution is to store them in environment
    variables and use `os.environ` to retrieve them.
  prefs: []
  type: TYPE_NORMAL
- en: Using Twython
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The trickiest part of using the Twitter API is authenticating yourself. (Indeed,
    this is the trickiest part of using a lot of APIs.) API providers want to make
    sure that you’re authorized to access their data and that you don’t exceed their
    usage limits. They also want to know who’s accessing their data.
  prefs: []
  type: TYPE_NORMAL
- en: Authentication is kind of a pain. There is a simple way, OAuth 2, that suffices
    when you just want to do simple searches. And there is a complex way, OAuth 1,
    that’s required when you want to perform actions (e.g., tweeting) or (in particular
    for us) connect to the Twitter stream.
  prefs: []
  type: TYPE_NORMAL
- en: So we’re stuck with the more complicated way, which we’ll try to automate as
    much as we can.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, you need your API key and API secret key (sometimes known as the consumer
    key and consumer secret, respectively). I’ll be getting mine from environment
    variables, but feel free to substitute in yours however you wish:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can instantiate the client:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: At this point you may want to consider saving the `ACCESS_TOKEN` and `ACCESS_TOKEN_SECRET`
    somewhere safe, so that next time you don’t have to go through this rigmarole.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have an authenticated `Twython` instance, we can start performing searches:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'If you run this, you should get some tweets back like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: This isn’t that interesting, largely because the Twitter Search API just shows
    you whatever handful of recent results it feels like. When you’re doing data science,
    more often you want a lot of tweets. This is where the [Streaming API](https://developer.twitter.com/en/docs/tutorials/consuming-streaming-data)
    is useful. It allows you to connect to (a sample of) the great Twitter firehose.
    To use it, you’ll need to authenticate using your access tokens.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to access the Streaming API with Twython, we need to define a class
    that inherits from `TwythonStreamer` and that overrides its `on_success` method,
    and possibly its `on_error` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: '`MyStreamer` will connect to the Twitter stream and wait for Twitter to feed
    it data. Each time it receives some data (here, a tweet represented as a Python
    object), it passes it to the `on_success` method, which appends it to our `tweets`
    list if its language is English, and then disconnects the streamer after it’s
    collected 1,000 tweets.'
  prefs: []
  type: TYPE_NORMAL
- en: 'All that’s left is to initialize it and start it running:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'This will run until it collects 100 tweets (or until it encounters an error)
    and stop, at which point you can start analyzing those tweets. For instance, you
    could find the most common hashtags with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: Each tweet contains a lot of data. You can either poke around yourself or dig
    through the [Twitter API documentation](https://developer.twitter.com/en/docs/tweets/data-dictionary/overview/tweet-object).
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In a non-toy project, you probably wouldn’t want to rely on an in-memory `list`
    for storing the tweets. Instead you’d want to save them to a file or a database,
    so that you’d have them permanently.
  prefs: []
  type: TYPE_NORMAL
- en: For Further Exploration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[pandas](http://pandas.pydata.org/) is the primary library that data science
    types use for working with—and, in particular, importing—data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Scrapy](http://scrapy.org/) is a full-featured library for building complicated
    web scrapers that do things like follow unknown links.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Kaggle](https://www.kaggle.com/datasets) hosts a large collection of datasets.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
