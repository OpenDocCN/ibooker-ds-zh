- en: 'Chapter 43\. In Depth: Support Vector Machines'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç¬¬43ç« ï¼šæ·±å…¥æ¢è®¨æ”¯æŒå‘é‡æœº
- en: Support vector machines (SVMs) are a particularly powerful and flexible class
    of supervised algorithms for both classification and regression. In this chapter,
    we will explore the intuition behind SVMs and their use in classification problems.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: æ”¯æŒå‘é‡æœºï¼ˆSVMsï¼‰æ˜¯ä¸€ç§ç‰¹åˆ«å¼ºå¤§å’Œçµæ´»çš„ç›‘ç£ç®—æ³•ç±»ï¼Œé€‚ç”¨äºåˆ†ç±»å’Œå›å½’ã€‚åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬å°†æ¢è®¨SVMèƒŒåçš„ç›´è§‰åŠå…¶åœ¨åˆ†ç±»é—®é¢˜ä¸­çš„åº”ç”¨ã€‚
- en: 'We begin with the standard imports:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä»æ ‡å‡†å¯¼å…¥å¼€å§‹ï¼š
- en: '[PRE0]'
  id: totrans-3
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Note
  id: totrans-4
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: æ³¨
- en: Full-size, full-color figures are available in the [supplemental materials on
    GitHub](https://oreil.ly/PDSH_GitHub).
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: å…¨å°ºå¯¸ã€å…¨å½©è‰²å›¾åƒå¯åœ¨[GitHubçš„è¡¥å……ææ–™](https://oreil.ly/PDSH_GitHub)ä¸­æ‰¾åˆ°ã€‚
- en: Motivating Support Vector Machines
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ¿€åŠ±æ”¯æŒå‘é‡æœº
- en: As part of our discussion of Bayesian classification (see [ChapterÂ 41](ch41.xhtml#section-0505-naive-bayes)),
    we learned about a simple kind of model that describes the distribution of each
    underlying class, and experimented with using it to probabilistically determine
    labels for new points. That was an example of *generative classification*; here
    we will consider instead *discriminative classification*. That is, rather than
    modeling each class, we will simply find a line or curve (in two dimensions) or
    manifold (in multiple dimensions) that divides the classes from each other.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œä¸ºæˆ‘ä»¬è®¨è®ºè´å¶æ–¯åˆ†ç±»çš„ä¸€éƒ¨åˆ†ï¼ˆå‚è§[ç¬¬41ç« ](ch41.xhtml#section-0505-naive-bayes)ï¼‰ï¼Œæˆ‘ä»¬äº†è§£åˆ°äº†æè¿°æ¯ä¸ªæ½œåœ¨ç±»åˆ†å¸ƒçš„ç®€å•æ¨¡å‹ï¼Œå¹¶å°è¯•ä½¿ç”¨å®ƒæ¥æ¦‚ç‡åœ°ç¡®å®šæ–°ç‚¹çš„æ ‡ç­¾ã€‚é‚£æ˜¯ä¸€ä¸ªç”Ÿæˆåˆ†ç±»çš„ä¾‹å­ï¼›åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†è€ƒè™‘åˆ¤åˆ«åˆ†ç±»ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œæˆ‘ä»¬ä¸å†æ¨¡æ‹Ÿæ¯ä¸ªç±»ï¼Œè€Œæ˜¯ç®€å•åœ°æ‰¾åˆ°ä¸€ä¸ªï¼ˆåœ¨äºŒç»´ä¸­ä¸ºçº¿æˆ–æ›²çº¿ï¼Œåœ¨å¤šç»´ä¸­ä¸ºæµå½¢ï¼‰ï¼Œå°†ç±»å½¼æ­¤åˆ†å¼€ã€‚
- en: As an example of this, consider the simple case of a classification task in
    which the two classes of points are well separated (see [FigureÂ 43-1](#fig_0507-support-vector-machines_files_in_output_5_0)).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œä¸ºä¸€ä¸ªä¾‹å­ï¼Œè€ƒè™‘ä¸€ä¸ªåˆ†ç±»ä»»åŠ¡çš„ç®€å•æƒ…å†µï¼Œå…¶ä¸­ä¸¤ç±»ç‚¹æ˜¯å®Œå…¨åˆ†å¼€çš„ï¼ˆè§[å›¾43-1](#fig_0507-support-vector-machines_files_in_output_5_0)ï¼‰ã€‚
- en: '[PRE1]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![output 5 0](assets/output_5_0.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![output 5 0](assets/output_5_0.png)'
- en: Figure 43-1\. Simple data for classification
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾43-1\. åˆ†ç±»ç®€å•æ•°æ®
- en: 'A linear discriminative classifier would attempt to draw a straight line separating
    the two sets of data, and thereby create a model for classification. For two-dimensional
    data like that shown here, this is a task we could do by hand. But immediately
    we see a problem: there is more than one possible dividing line that can perfectly
    discriminate between the two classes!'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: çº¿æ€§åˆ¤åˆ«åˆ†ç±»å™¨å°†å°è¯•ç”»ä¸€æ¡ç›´çº¿åˆ†éš”ä¸¤ç»„æ•°æ®ï¼Œå¹¶å› æ­¤åˆ›å»ºä¸€ä¸ªåˆ†ç±»æ¨¡å‹ã€‚å¯¹äºåƒè¿™æ ·çš„äºŒç»´æ•°æ®ï¼Œæˆ‘ä»¬å¯ä»¥æ‰‹åŠ¨å®Œæˆè¿™é¡¹ä»»åŠ¡ã€‚ä½†æˆ‘ä»¬ç«‹å³çœ‹åˆ°äº†ä¸€ä¸ªé—®é¢˜ï¼šå­˜åœ¨ä¸æ­¢ä¸€æ¡å¯èƒ½å®Œå…¨åŒºåˆ†è¿™ä¸¤ç±»çš„åˆ†ç•Œçº¿ï¼
- en: 'We can draw some of them as follows; [FigureÂ 43-2](#fig_0507-support-vector-machines_files_in_output_7_0)
    shows the result:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥å¦‚ä¸‹ç»˜åˆ¶å…¶ä¸­ä¸€äº›ï¼›[å›¾43-2](#fig_0507-support-vector-machines_files_in_output_7_0)å±•ç¤ºäº†ç»“æœï¼š
- en: '[PRE2]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![output 7 0](assets/output_7_0.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![output 7 0](assets/output_7_0.png)'
- en: Figure 43-2\. Three perfect linear discriminative classifiers for our data
  id: totrans-16
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾43-2\. æˆ‘ä»¬æ•°æ®çš„ä¸‰ä¸ªå®Œç¾çº¿æ€§åˆ¤åˆ«åˆ†ç±»å™¨
- en: These are three *very* different separators which, nevertheless, perfectly discriminate
    between these samples. Depending on which you choose, a new data point (e.g.,
    the one marked by the â€œXâ€ in this plot) will be assigned a different label! Evidently
    our simple intuition of â€œdrawing a line between classesâ€ is not good enough, and
    we need to think a bit more deeply.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸‰ä¸ª*éå¸¸*ä¸åŒçš„åˆ†éš”ç¬¦ï¼Œå°½ç®¡å¦‚æ­¤ï¼Œå®ƒä»¬å®Œå…¨å¯ä»¥åŒºåˆ†è¿™äº›æ ·æœ¬ã€‚æ ¹æ®ä½ é€‰æ‹©çš„åˆ†ç•Œçº¿ï¼Œæ–°çš„æ•°æ®ç‚¹ï¼ˆä¾‹å¦‚ï¼Œåœ¨è¿™ä¸ªå›¾ä¸­ç”¨â€œXâ€æ ‡è®°çš„ç‚¹ï¼‰å°†è¢«åˆ†é…ä¸åŒçš„æ ‡ç­¾ï¼æ˜¾ç„¶ï¼Œæˆ‘ä»¬ç®€å•çš„â€œåœ¨ç±»ä¹‹é—´ç”»ä¸€æ¡çº¿â€çš„ç›´è§‰ä¸å¤Ÿå¥½ï¼Œæˆ‘ä»¬éœ€è¦æ›´æ·±å…¥åœ°æ€è€ƒã€‚
- en: 'Support Vector Machines: Maximizing the Margin'
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ”¯æŒå‘é‡æœºï¼šæœ€å¤§åŒ–è¾¹ç¼˜
- en: 'Support vector machines offer one way to improve on this. The intuition is
    this: rather than simply drawing a zero-width line between the classes, we can
    draw around each line a *margin* of some width, up to the nearest point. Here
    is an example of how this might look ([FigureÂ 43-3](#fig_0507-support-vector-machines_files_in_output_10_0)).'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: æ”¯æŒå‘é‡æœºæä¾›äº†ä¸€ç§æ”¹è¿›æ–¹æ³•ã€‚å…¶ç›´è§‰æ˜¯ï¼šä¸å…¶ç®€å•åœ°åœ¨ç±»ä¹‹é—´ç”»ä¸€æ¡é›¶å®½åº¦çº¿ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨æ¯æ¡çº¿å‘¨å›´ç»˜åˆ¶ä¸€å®šå®½åº¦çš„*è¾¹ç¼˜*ï¼Œç›´åˆ°æœ€è¿‘çš„ç‚¹ã€‚è¿™æ˜¯ä¸€ä¸ªå±•ç¤ºçš„ä¾‹å­ï¼ˆè§[å›¾43-3](#fig_0507-support-vector-machines_files_in_output_10_0)ï¼‰ã€‚
- en: '[PRE3]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The line that maximizes this margin is the one we will choose as the optimal
    model.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€å¤§åŒ–è¿™ä¸ªè¾¹ç¼˜çš„çº¿å°±æ˜¯æˆ‘ä»¬å°†é€‰æ‹©ä½œä¸ºæœ€ä¼˜æ¨¡å‹çš„çº¿ã€‚
- en: '![output 10 0](assets/output_10_0.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![output 10 0](assets/output_10_0.png)'
- en: Figure 43-3\. Visualization of â€œmarginsâ€ within discriminative classifiers
  id: totrans-23
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾43-3\. åˆ¤åˆ«åˆ†ç±»å™¨å†…â€œè¾¹ç¼˜â€çš„å¯è§†åŒ–
- en: Fitting a Support Vector Machine
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é€‚é…æ”¯æŒå‘é‡æœº
- en: 'Letâ€™s see the result of an actual fit to this data: we will use Scikit-Learnâ€™s
    support vector classifier (`SVC`) to train an SVM model on this data. For the
    time being, we will use a linear kernel and set the `C` parameter to a very large
    number (weâ€™ll discuss the meaning of these in more depth momentarily):'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨è®©æˆ‘ä»¬çœ‹çœ‹å¯¹è¿™äº›æ•°æ®è¿›è¡Œå®é™…æ‹Ÿåˆçš„ç»“æœï¼šæˆ‘ä»¬å°†ä½¿ç”¨Scikit-Learnçš„æ”¯æŒå‘é‡åˆ†ç±»å™¨ï¼ˆ`SVC`ï¼‰æ¥è®­ç»ƒä¸€ä¸ªSVMæ¨¡å‹ã€‚æš‚æ—¶åœ°ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨çº¿æ€§æ ¸å¹¶å°†å‚æ•°`C`è®¾ç½®ä¸ºä¸€ä¸ªéå¸¸å¤§çš„æ•°ï¼ˆç¨åæˆ‘ä»¬å°†æ·±å…¥è®¨è®ºå®ƒä»¬çš„å«ä¹‰ï¼‰ï¼š
- en: '[PRE4]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: To better visualize whatâ€™s happening here, letâ€™s create a quick convenience
    function that will plot SVM decision boundaries for us ([FigureÂ 43-4](#fig_0507-support-vector-machines_files_in_output_16_0)).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æ›´å¥½åœ°å¯è§†åŒ–è¿™é‡Œå‘ç”Ÿçš„æƒ…å†µï¼Œè®©æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªå¿«é€Ÿä¾¿åˆ©å‡½æ•°ï¼Œå®ƒå°†ä¸ºæˆ‘ä»¬ç»˜åˆ¶SVMå†³ç­–è¾¹ç•Œï¼ˆ[å›¾Â 43-4](#fig_0507-support-vector-machines_files_in_output_16_0)ï¼‰ã€‚
- en: '[PRE5]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![output 16 0](assets/output_16_0.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![output 16 0](assets/output_16_0.png)'
- en: Figure 43-4\. A support vector machine classifier fit to the data, with margins
    (dashed lines) and support vectors (circles) shown
  id: totrans-31
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾ 43-4\. ä¸€ä¸ªæ‹Ÿåˆåˆ°æ•°æ®çš„æ”¯æŒå‘é‡æœºåˆ†ç±»å™¨ï¼Œæ˜¾ç¤ºäº†è¾¹ç•Œï¼ˆè™šçº¿ï¼‰å’Œæ”¯æŒå‘é‡ï¼ˆåœ†ç‚¹ï¼‰
- en: 'This is the dividing line that maximizes the margin between the two sets of
    points. Notice that a few of the training points just touch the margin: they are
    circled in [FigureÂ 43-5](#fig_0507-support-vector-machines_files_in_output_20_0).
    These points are the pivotal elements of this fit; they are known as the *support
    vectors*, and give the algorithm its name. In Scikit-Learn, the identities of
    these points are stored in the `support_vectors_` attribute of the classifier:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯æœ€å¤§åŒ–ä¸¤ç»„ç‚¹ä¹‹é—´é—´éš”çš„åˆ†éš”çº¿ã€‚è¯·æ³¨æ„ï¼Œä¸€äº›è®­ç»ƒç‚¹æ°å¥½æ¥è§¦è¾¹ç•Œï¼šå®ƒä»¬åœ¨[å›¾Â 43-5](#fig_0507-support-vector-machines_files_in_output_20_0)ä¸­è¢«åœˆå‡ºæ¥ã€‚è¿™äº›ç‚¹æ˜¯æ­¤æ‹Ÿåˆçš„å…³é”®å…ƒç´ ï¼›å®ƒä»¬è¢«ç§°ä¸º*æ”¯æŒå‘é‡*ï¼Œå¹¶èµ‹äºˆäº†ç®—æ³•å…¶åç§°ã€‚åœ¨Scikit-Learnä¸­ï¼Œè¿™äº›ç‚¹çš„æ ‡è¯†å­˜å‚¨åœ¨åˆ†ç±»å™¨çš„`support_vectors_`å±æ€§ä¸­ï¼š
- en: '[PRE7]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: A key to this classifierâ€™s success is that for the fit, only the positions of
    the support vectors matter; any points further from the margin that are on the
    correct side do not modify the fit. Technically, this is because these points
    do not contribute to the loss function used to fit the model, so their position
    and number do not matter so long as they do not cross the margin.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤åˆ†ç±»å™¨æˆåŠŸçš„å…³é”®åœ¨äºå¯¹æ‹Ÿåˆæ¥è¯´ï¼Œåªæœ‰æ”¯æŒå‘é‡çš„ä½ç½®æ˜¯é‡è¦çš„ï¼›è¿œç¦»è¾¹ç•Œä½†åœ¨æ­£ç¡®ä¸€ä¾§çš„ç‚¹ä¸ä¼šä¿®æ”¹æ‹Ÿåˆã€‚ä»æŠ€æœ¯ä¸Šè®²ï¼Œè¿™æ˜¯å› ä¸ºè¿™äº›ç‚¹ä¸ä¼šå¯¹ç”¨äºæ‹Ÿåˆæ¨¡å‹çš„æŸå¤±å‡½æ•°äº§ç”Ÿè´¡çŒ®ï¼Œå› æ­¤å®ƒä»¬çš„ä½ç½®å’Œæ•°é‡å¹¶ä¸é‡è¦ï¼Œåªè¦å®ƒä»¬ä¸è·¨è¶Šè¾¹ç•Œã€‚
- en: We can see this, for example, if we plot the model learned from the first 60
    points and first 120 points of this dataset ([FigureÂ 43-5](#fig_0507-support-vector-machines_files_in_output_20_0)).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘ä»¬ç»˜åˆ¶ä»è¿™ä¸ªæ•°æ®é›†çš„å‰60ä¸ªç‚¹å’Œå‰120ä¸ªç‚¹å­¦ä¹ åˆ°çš„æ¨¡å‹ï¼ˆ[å›¾Â 43-5](#fig_0507-support-vector-machines_files_in_output_20_0)ï¼‰ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°è¿™ä¸€ç‚¹ã€‚
- en: '[PRE8]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![output 20 0](assets/output_20_0.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![output 20 0](assets/output_20_0.png)'
- en: Figure 43-5\. The influence of new training points on the SVM model
  id: totrans-38
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾ 43-5\. æ–°è®­ç»ƒç‚¹å¯¹SVMæ¨¡å‹çš„å½±å“
- en: 'In the left panel, we see the model and the support vectors for 60 training
    points. In the right panel, we have doubled the number of training points, but
    the model has not changed: the three support vectors in the left panel are the
    same as the support vectors in the right panel. This insensitivity to the exact
    behavior of distant points is one of the strengths of the SVM model.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å·¦ä¾§é¢æ¿ä¸­ï¼Œæˆ‘ä»¬çœ‹åˆ°äº†60ä¸ªè®­ç»ƒç‚¹çš„æ¨¡å‹å’Œæ”¯æŒå‘é‡ã€‚åœ¨å³ä¾§é¢æ¿ä¸­ï¼Œæˆ‘ä»¬å¢åŠ äº†è®­ç»ƒç‚¹çš„æ•°é‡ï¼Œä½†æ¨¡å‹æ²¡æœ‰æ”¹å˜ï¼šå·¦ä¾§é¢æ¿ä¸­çš„ä¸‰ä¸ªæ”¯æŒå‘é‡ä¸å³ä¾§é¢æ¿ä¸­çš„æ”¯æŒå‘é‡ç›¸åŒã€‚è¿™ç§å¯¹è¿œç‚¹è¡Œä¸ºçš„ç¡®åˆ‡ä¸æ•æ„Ÿæ€§æ˜¯SVMæ¨¡å‹çš„ä¸€ç§ä¼˜åŠ¿ä¹‹ä¸€ã€‚
- en: 'If you are running this notebook live, you can use IPythonâ€™s interactive widgets
    to view this feature of the SVM model interactively:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨æ­£åœ¨å®æ—¶è¿è¡Œæ­¤ç¬”è®°æœ¬ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨IPythonçš„äº¤äº’å¼å°éƒ¨ä»¶æ¥äº¤äº’åœ°æŸ¥çœ‹SVMæ¨¡å‹çš„æ­¤åŠŸèƒ½ï¼š
- en: '[PRE9]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Beyond Linear Boundaries: Kernel SVM'
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è¶…è¶Šçº¿æ€§è¾¹ç•Œï¼šæ ¸æ”¯æŒå‘é‡æœº
- en: Where SVM can become quite powerful is when it is combined with *kernels*. We
    have seen a version of kernels before, in the basis function regressions of [ChapterÂ 42](ch42.xhtml#section-0506-linear-regression).
    There we projected our data into a higher-dimensional space defined by polynomials
    and Gaussian basis functions, and thereby were able to fit for nonlinear relationships
    with a linear classifier.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: å½“SVMä¸*æ ¸*ç»“åˆæ—¶ï¼Œå®ƒå¯ä»¥å˜å¾—éå¸¸å¼ºå¤§ã€‚æˆ‘ä»¬ä¹‹å‰åœ¨[ç¬¬42ç« ](ch42.xhtml#section-0506-linear-regression)ä¸­å·²ç»çœ‹åˆ°äº†æ ¸çš„ä¸€ä¸ªç‰ˆæœ¬ï¼Œå³åŸºå‡½æ•°å›å½’ã€‚åœ¨é‚£é‡Œï¼Œæˆ‘ä»¬å°†æ•°æ®æŠ•å½±åˆ°ç”±å¤šé¡¹å¼å’Œé«˜æ–¯åŸºå‡½æ•°å®šä¹‰çš„æ›´é«˜ç»´ç©ºé—´ä¸­ï¼Œä»è€Œèƒ½å¤Ÿä½¿ç”¨çº¿æ€§åˆ†ç±»å™¨æ‹Ÿåˆéçº¿æ€§å…³ç³»ã€‚
- en: In SVM models, we can use a version of the same idea. To motivate the need for
    kernels, letâ€™s look at some data that is not linearly separable ([FigureÂ 43-6](#fig_0507-support-vector-machines_files_in_output_25_0)).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![output 25 0](assets/output_25_0.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
- en: Figure 43-6\. A linear classifier performs poorly for nonlinear boundaries
  id: totrans-47
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'It is clear that no linear discrimination will *ever* be able to separate this
    data. But we can draw a lesson from the basis function regressions in [ChapterÂ 42](ch42.xhtml#section-0506-linear-regression),
    and think about how we might project the data into a higher dimension such that
    a linear separator *would* be sufficient. For example, one simple projection we
    could use would be to compute a *radial basis function* (RBF) centered on the
    middle clump:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: We can visualize this extra data dimension using a three-dimensional plot, as
    seen in [FigureÂ 43-7](#fig_0507-support-vector-machines_files_in_output_29_0).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '![output 29 0](assets/output_29_0.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
- en: Figure 43-7\. A third dimension added to the data allows for linear separation
  id: totrans-53
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We can see that with this additional dimension, the data becomes trivially linearly
    separable, by drawing a separating plane at, say, *r*=0.7.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case we had to choose and carefully tune our projection: if we had
    not centered our radial basis function in the right location, we would not have
    seen such clean, linearly separable results. In general, the need to make such
    a choice is a problem: we would like to somehow automatically find the best basis
    functions to use.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: One strategy to this end is to compute a basis function centered at *every*
    point in the dataset, and let the SVM algorithm sift through the results. This
    type of basis function transformation is known as a *kernel transformation*, as
    it is based on a similarity relationship (or kernel) between each pair of points.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: A potential problem with this strategyâ€”projecting <math alttext="upper N"><mi>N</mi></math>
    points into <math alttext="upper N"><mi>N</mi></math> dimensionsâ€”is that it might
    become very computationally intensive as <math alttext="upper N"><mi>N</mi></math>
    grows large. However, because of a neat little procedure known as the [*kernel
    trick*](https://oreil.ly/h7PBj), a fit on kernel-transformed data can be done
    implicitlyâ€”that is, without ever building the full <math alttext="upper N"><mi>N</mi></math>
    -dimensional representation of the kernel projection. This kernel trick is built
    into the SVM, and is one of the reasons the method is so powerful.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: 'In Scikit-Learn, we can apply kernelized SVM simply by changing our linear
    kernel to an RBF kernel, using the `kernel` model hyperparameter:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Letâ€™s use our previously defined function to visualize the fit and identify
    the support vectors ([FigureÂ 43-8](#fig_0507-support-vector-machines_files_in_output_33_0)).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '![output 33 0](assets/output_33_0.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
- en: Figure 43-8\. Kernel SVM fit to the data
  id: totrans-63
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Using this kernelized support vector machine, we learn a suitable nonlinear
    decision boundary. This kernel transformation strategy is used often in machine
    learning to turn fast linear methods into fast nonlinear methods, especially for
    models in which the kernel trick can be used.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨è¿™ç§æ ¸åŒ–æ”¯æŒå‘é‡æœºï¼Œæˆ‘ä»¬å­¦ä¹ åˆ°äº†ä¸€ä¸ªé€‚åˆçš„éçº¿æ€§å†³ç­–è¾¹ç•Œã€‚è¿™ç§æ ¸å˜æ¢ç­–ç•¥åœ¨æœºå™¨å­¦ä¹ ä¸­ç»å¸¸è¢«ä½¿ç”¨ï¼Œå°†å¿«é€Ÿçš„çº¿æ€§æ–¹æ³•è½¬æ¢ä¸ºå¿«é€Ÿçš„éçº¿æ€§æ–¹æ³•ï¼Œç‰¹åˆ«é€‚ç”¨äºå¯ä»¥ä½¿ç”¨æ ¸æŠ€å·§çš„æ¨¡å‹ã€‚
- en: 'Tuning the SVM: Softening Margins'
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è°ƒæ•´ SVMï¼šè½¯åŒ–é—´éš”
- en: Our discussion thus far has centered around very clean datasets, in which a
    perfect decision boundary exists. But what if your data has some amount of overlap?
    For example, you may have data like this (see [FigureÂ 43-9](#fig_0507-support-vector-machines_files_in_output_36_0)).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬çš„è®¨è®ºé›†ä¸­åœ¨éå¸¸å¹²å‡€çš„æ•°æ®é›†ä¸Šï¼Œå…¶ä¸­å­˜åœ¨å®Œç¾çš„å†³ç­–è¾¹ç•Œã€‚ä½†æ˜¯å¦‚æœæ‚¨çš„æ•°æ®æœ‰ä¸€å®šçš„é‡å å‘¢ï¼Ÿä¾‹å¦‚ï¼Œæ‚¨å¯èƒ½æœ‰è¿™æ ·çš„æ•°æ®ï¼ˆè§[å›¾ 43-9](#fig_0507-support-vector-machines_files_in_output_36_0)ï¼‰ã€‚
- en: '[PRE15]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '![output 36 0](assets/output_36_0.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![output 36 0](assets/output_36_0.png)'
- en: Figure 43-9\. Data with some level of overlap
  id: totrans-69
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾ 43-9\. å…·æœ‰ä¸€å®šé‡å çº§åˆ«çš„æ•°æ®
- en: 'To handle this case, the SVM implementation has a bit of a fudge factor that
    â€œsoftensâ€ the margin: that is, it allows some of the points to creep into the
    margin if that allows a better fit. The hardness of the margin is controlled by
    a tuning parameter, most often known as `C`. For a very large `C`, the margin
    is hard, and points cannot lie in it. For a smaller `C`, the margin is softer
    and can grow to encompass some points.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†å¤„ç†è¿™ç§æƒ…å†µï¼ŒSVM å®ç°ä¸­æœ‰ä¸€ä¸ªâ€œè½¯åŒ–â€é—´éš”çš„ä¿®æ­£å› å­ï¼šå³ï¼Œå¦‚æœå…è®¸æ›´å¥½çš„æ‹Ÿåˆï¼ŒæŸäº›ç‚¹å¯ä»¥è¿›å…¥é—´éš”ã€‚é—´éš”çš„ç¡¬åº¦ç”±è°ƒæ•´å‚æ•°æ§åˆ¶ï¼Œé€šå¸¸ç§°ä¸º`C`ã€‚å¯¹äºå¾ˆå¤§çš„`C`ï¼Œé—´éš”æ˜¯ç¡¬çš„ï¼Œç‚¹ä¸èƒ½ä½äºå…¶ä¸­ã€‚å¯¹äºè¾ƒå°çš„`C`ï¼Œé—´éš”è¾ƒè½¯ï¼Œå¹¶ä¸”å¯ä»¥åŒ…å«ä¸€äº›ç‚¹ã€‚
- en: 'The plot shown in [FigureÂ 43-10](#fig_0507-support-vector-machines_files_in_output_38_0)
    gives a visual picture of how a changing `C` affects the final fit via the softening
    of the margin:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '[å›¾ 43-10](#fig_0507-support-vector-machines_files_in_output_38_0)ä¸­æ˜¾ç¤ºçš„å›¾è¡¨å±•ç¤ºäº†é€šè¿‡è½¯åŒ–é—´éš”æ¥æ”¹å˜`C`å¦‚ä½•å½±å“æœ€ç»ˆæ‹Ÿåˆçš„è§†è§‰æ•ˆæœï¼š'
- en: '[PRE16]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '![output 38 0](assets/output_38_0.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![output 38 0](assets/output_38_0.png)'
- en: Figure 43-10\. The effect of the `C` parameter on the support vector fit
  id: totrans-74
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾ 43-10\. `C` å‚æ•°å¯¹æ”¯æŒå‘é‡æ‹Ÿåˆçš„å½±å“
- en: The optimal value of `C` will depend on your dataset, and you should tune this
    parameter using cross-validation or a similar procedure (refer back to [ChapterÂ 39](ch39.xhtml#section-0503-hyperparameters-and-model-validation)).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '`C` çš„æœ€ä½³å€¼å°†å–å†³äºæ‚¨çš„æ•°æ®é›†ï¼Œæ‚¨åº”è¯¥ä½¿ç”¨äº¤å‰éªŒè¯æˆ–ç±»ä¼¼çš„ç¨‹åºæ¥è°ƒæ•´æ­¤å‚æ•°ï¼ˆå‚è€ƒ[ç¬¬ 39 ç« ](ch39.xhtml#section-0503-hyperparameters-and-model-validation)ï¼‰ã€‚'
- en: 'Example: Face Recognition'
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼šäººè„¸è¯†åˆ«
- en: 'As an example of support vector machines in action, letâ€™s take a look at the
    facial recognition problem. We will use the Labeled Faces in the Wild dataset,
    which consists of several thousand collated photos of various public figures.
    A fetcher for the dataset is built into Scikit-Learn:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œä¸ºæ”¯æŒå‘é‡æœºåœ¨å®é™…ä¸­çš„åº”ç”¨ç¤ºä¾‹ï¼Œè®©æˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹äººè„¸è¯†åˆ«é—®é¢˜ã€‚æˆ‘ä»¬å°†ä½¿ç”¨â€œé‡å¤–æ ‡è®°äººè„¸â€æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŒ…å«æ•°åƒå¼ å„ç§å…¬ä¼—äººç‰©çš„åˆå¹¶ç…§ç‰‡ã€‚Scikit-Learn
    å†…ç½®äº†è¯¥æ•°æ®é›†çš„è·å–å™¨ï¼š
- en: '[PRE17]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Letâ€™s plot a few of these faces to see what weâ€™re working with (see [FigureÂ 43-11](#fig_0507-support-vector-machines_files_in_output_43_0)).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ç»˜åˆ¶å‡ å¼ è¿™äº›äººè„¸ï¼Œçœ‹çœ‹æˆ‘ä»¬æ­£åœ¨å¤„ç†çš„å†…å®¹ï¼ˆè§[å›¾ 43-11](#fig_0507-support-vector-machines_files_in_output_43_0)ï¼‰ã€‚
- en: '[PRE18]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '![output 43 0](assets/output_43_0.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![output 43 0](assets/output_43_0.png)'
- en: Figure 43-11\. Examples from the Labeled Faces in the Wild dataset
  id: totrans-82
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾ 43-11\. æ¥è‡ªé‡å¤–æ ‡è®°äººè„¸æ•°æ®é›†çš„ç¤ºä¾‹
- en: 'Each image contains 62 Ã— 47, or around 3,000, pixels. We could proceed by simply
    using each pixel value as a feature, but often it is more effective to use some
    sort of preprocessor to extract more meaningful features; here we will use principal
    component analysis (see [ChapterÂ 45](ch45.xhtml#section-0509-principal-component-analysis))
    to extract 150 fundamental components to feed into our support vector machine
    classifier. We can do this most straightforwardly by packaging the preprocessor
    and the classifier into a single pipeline:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯ä¸ªå›¾åƒåŒ…å« 62 Ã— 47ï¼Œçº¦ 3,000 ä¸ªåƒç´ ã€‚æˆ‘ä»¬å¯ä»¥ç®€å•åœ°ä½¿ç”¨æ¯ä¸ªåƒç´ å€¼ä½œä¸ºç‰¹å¾ï¼Œä½†é€šå¸¸ä½¿ç”¨æŸç§é¢„å¤„ç†å™¨æ¥æå–æ›´æœ‰æ„ä¹‰çš„ç‰¹å¾æ›´ä¸ºæœ‰æ•ˆï¼›åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ä¸»æˆåˆ†åˆ†æï¼ˆè§[ç¬¬
    45 ç« ](ch45.xhtml#section-0509-principal-component-analysis)ï¼‰æå– 150 ä¸ªåŸºæœ¬ç»„åˆ†ï¼Œä»¥ä¾›æ”¯æŒå‘é‡æœºåˆ†ç±»å™¨ä½¿ç”¨ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡å°†é¢„å¤„ç†å™¨å’Œåˆ†ç±»å™¨æ‰“åŒ…åˆ°å•ä¸ªç®¡é“ä¸­æ¥å®ç°è¿™ä¸€ç‚¹ï¼š
- en: '[PRE19]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'For the sake of testing our classifier output, we will split the data into
    a training set and a testing set:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æµ‹è¯•æˆ‘ä»¬åˆ†ç±»å™¨çš„è¾“å‡ºï¼Œæˆ‘ä»¬å°†æ•°æ®åˆ†å‰²ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼š
- en: '[PRE20]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Finally, we can use grid search cross-validation to explore combinations of
    parameters. Here we will adjust `C` (which controls the margin hardness) and `gamma`
    (which controls the size of the radial basis function kernel), and determine the
    best model:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ç½‘æ ¼æœç´¢äº¤å‰éªŒè¯æ¥æ¢ç´¢å‚æ•°çš„ç»„åˆã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†è°ƒæ•´`C`ï¼ˆæ§åˆ¶è¾¹ç•Œç¡¬åº¦ï¼‰å’Œ`gamma`ï¼ˆæ§åˆ¶å¾„å‘åŸºå‡½æ•°æ ¸çš„å¤§å°ï¼‰ï¼Œå¹¶ç¡®å®šæœ€ä½³æ¨¡å‹ï¼š
- en: '[PRE21]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The optimal values fall toward the middle of our grid; if they fell at the edges,
    we would want to expand the grid to make sure we have found the true optimum.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€ä¼˜å€¼é›†ä¸­åœ¨æˆ‘ä»¬ç½‘æ ¼çš„ä¸­é—´ï¼›å¦‚æœå®ƒä»¬åœ¨è¾¹ç¼˜ï¼Œæˆ‘ä»¬å°†æ‰©å±•ç½‘æ ¼ä»¥ç¡®ä¿æ‰¾åˆ°çœŸæ­£çš„æœ€ä¼˜å€¼ã€‚
- en: 'Now with this cross-validated model we can predict the labels for the test
    data, which the model has not yet seen:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æœ‰äº†è¿™ä¸ªç»è¿‡äº¤å‰éªŒè¯çš„æ¨¡å‹ï¼Œæˆ‘ä»¬å¯ä»¥é¢„æµ‹æµ‹è¯•æ•°æ®çš„æ ‡ç­¾ï¼Œè¿™äº›æ•°æ®æ¨¡å‹å°šæœªè§è¿‡ï¼š
- en: '[PRE22]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Letâ€™s take a look at a few of the test images along with their predicted values
    (see [FigureÂ 43-12](#fig_0507-support-vector-machines_files_in_output_53_0)).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬çœ‹ä¸€äº›æµ‹è¯•å›¾åƒåŠå…¶é¢„æµ‹å€¼ï¼ˆè§[å›¾Â 43-12](#fig_0507-support-vector-machines_files_in_output_53_0)ï¼‰ã€‚
- en: '[PRE23]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '![output 53 0](assets/output_53_0.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![output 53 0](assets/output_53_0.png)'
- en: Figure 43-12\. Labels predicted by our model
  id: totrans-95
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾ 43-12\. æˆ‘ä»¬æ¨¡å‹é¢„æµ‹çš„æ ‡ç­¾
- en: 'Out of this small sample, our optimal estimator mislabeled only a single face
    (Bushâ€™s face in the bottom row was mislabeled as Blair). We can get a better sense
    of our estimatorâ€™s performance using the classification report, which lists recovery
    statistics label by label:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªå°æ ·æœ¬ä¸­ï¼Œæˆ‘ä»¬çš„æœ€ä¼˜ä¼°è®¡å™¨åªè¯¯æ ‡äº†ä¸€ä¸ªé¢å­”ï¼ˆåº•éƒ¨è¡Œçš„å¸ƒä»€é¢å­”è¢«è¯¯æ ‡ä¸ºå¸ƒè±å°”ï¼‰ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡åˆ†ç±»æŠ¥å‘Šæ›´å¥½åœ°äº†è§£æˆ‘ä»¬ä¼°è®¡å™¨çš„æ€§èƒ½ï¼ŒæŠ¥å‘Šä¼šé€æ ‡ç­¾åˆ—å‡ºæ¢å¤ç»Ÿè®¡ä¿¡æ¯ï¼š
- en: '[PRE24]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: We might also display the confusion matrix between these classes (see [FigureÂ 43-13](#fig_0507-support-vector-machines_files_in_output_57_0)).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è¿˜å¯ä»¥æ˜¾ç¤ºè¿™äº›ç±»åˆ«ä¹‹é—´çš„æ··æ·†çŸ©é˜µï¼ˆè§[å›¾Â 43-13](#fig_0507-support-vector-machines_files_in_output_57_0)ï¼‰ã€‚
- en: '[PRE25]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '![output 57 0](assets/output_57_0.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![output 57 0](assets/output_57_0.png)'
- en: Figure 43-13\. Confusion matrix for the faces data
  id: totrans-101
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾ 43-13\. é¢éƒ¨æ•°æ®çš„æ··æ·†çŸ©é˜µ
- en: This helps us get a sense of which labels are likely to be confused by the estimator.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å¸®åŠ©æˆ‘ä»¬äº†è§£å“ªäº›æ ‡ç­¾å¯èƒ½ä¼šè¢«ä¼°è®¡å™¨æ··æ·†ã€‚
- en: 'For a real-world facial recognition task, in which the photos do not come pre-cropped
    into nice grids, the only difference in the facial classification scheme is the
    feature selection: you would need to use a more sophisticated algorithm to find
    the faces, and extract features that are independent of the pixellation. For this
    kind of application, one good option is to make use of [OpenCV](http://opencv.org),
    which, among other things, includes pretrained implementations of state-of-the-art
    feature extraction tools for images in general and faces in particular.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºä¸€ä¸ªç°å®ä¸–ç•Œçš„äººè„¸è¯†åˆ«ä»»åŠ¡ï¼Œåœ¨è¿™ç§ä»»åŠ¡ä¸­ç…§ç‰‡å¹¶æœªé¢„å…ˆè£å‰ªæˆæ¼‚äº®çš„ç½‘æ ¼ï¼Œé¢éƒ¨åˆ†ç±»æ–¹æ¡ˆå”¯ä¸€çš„åŒºåˆ«åœ¨äºç‰¹å¾é€‰æ‹©ï¼šæ‚¨éœ€è¦ä½¿ç”¨æ›´å¤æ‚çš„ç®—æ³•æ¥æ‰¾åˆ°é¢éƒ¨ï¼Œå¹¶æå–ä¸åƒç´ åŒ–æ— å…³çš„ç‰¹å¾ã€‚å¯¹äºè¿™ç§åº”ç”¨ï¼Œä¸€ä¸ªå¥½çš„é€‰æ‹©æ˜¯åˆ©ç”¨[OpenCV](http://opencv.org)ï¼Œå®ƒåŒ…æ‹¬å¯¹ä¸€èˆ¬å›¾åƒå’Œç‰¹åˆ«æ˜¯äººè„¸çš„å…ˆå‰å®ç°çš„æœ€æ–°ç‰¹å¾æå–å·¥å…·ã€‚
- en: Summary
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ‘˜è¦
- en: 'This has been a brief intuitive introduction to the principles behind support
    vector machines. These models are a powerful classification method for a number
    of reasons:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯æ”¯æŒå‘é‡æœºèƒŒååŸç†çš„ç®€æ˜ç›´è§‚ä»‹ç»ã€‚è¿™äº›æ¨¡å‹ç”±äºä»¥ä¸‹å‡ ä¸ªåŸå› è€Œæ˜¯ä¸€ç§å¼ºå¤§çš„åˆ†ç±»æ–¹æ³•ï¼š
- en: Their dependence on relatively few support vectors means that they are compact
    and take up very little memory.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å®ƒä»¬ä¾èµ–äºç›¸å¯¹è¾ƒå°‘çš„æ”¯æŒå‘é‡ï¼Œå› æ­¤ç´§å‡‘ä¸”å ç”¨æå°‘çš„å†…å­˜ç©ºé—´ã€‚
- en: Once the model is trained, the prediction phase is very fast.
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€æ—¦æ¨¡å‹è®­ç»ƒå®Œæˆï¼Œé¢„æµ‹é˜¶æ®µéå¸¸å¿«é€Ÿã€‚
- en: Because they are affected only by points near the margin, they work well with
    high-dimensional dataâ€”even data with more dimensions than samples, which is challenging
    for other algorithms.
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å› ä¸ºå®ƒä»¬åªå—åˆ°é è¿‘è¾¹ç•Œçš„ç‚¹çš„å½±å“ï¼Œæ‰€ä»¥å®ƒä»¬åœ¨å¤„ç†é«˜ç»´æ•°æ®æ—¶è¡¨ç°è‰¯å¥½â€”â€”å³ä½¿æ˜¯æ¯”æ ·æœ¬æ›´å¤šç»´åº¦çš„æ•°æ®ï¼Œè¿™å¯¹å…¶ä»–ç®—æ³•æ¥è¯´æ˜¯ä¸ªæŒ‘æˆ˜ã€‚
- en: Their integration with kernel methods makes them very versatile, able to adapt
    to many types of data.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å®ƒä»¬ä¸æ ¸æ–¹æ³•çš„é›†æˆä½¿å®ƒä»¬éå¸¸çµæ´»ï¼Œèƒ½å¤Ÿé€‚åº”è®¸å¤šç±»å‹çš„æ•°æ®ã€‚
- en: 'However, SVMs have several disadvantages as well:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œæ”¯æŒå‘é‡æœºï¼ˆSVMsï¼‰ä¹Ÿæœ‰å‡ ä¸ªç¼ºç‚¹ï¼š
- en: The scaling with the number of samples <math alttext="upper N"><mi>N</mi></math>
    is <math alttext="script upper O left-bracket upper N cubed right-bracket"><mrow><mi>ğ’ª</mi>
    <mo>[</mo> <msup><mi>N</mi> <mn>3</mn></msup> <mo>]</mo></mrow></math> at worst,
    or <math alttext="script upper O left-bracket upper N squared right-bracket"><mrow><mi>ğ’ª</mi>
    <mo>[</mo> <msup><mi>N</mi> <mn>2</mn></msup> <mo>]</mo></mrow></math> for efficient
    implementations. For large numbers of training samples, this computational cost
    can be prohibitive.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ ·æœ¬æ•°é‡<math alttext="upper N"><mi>N</mi></math>çš„ç¼©æ”¾ä¸ºæœ€åæƒ…å†µä¸‹æ˜¯<math alttext="script
    upper O left-bracket upper N cubed right-bracket"><mrow><mi>ğ’ª</mi> <mo>[</mo>
    <msup><mi>N</mi> <mn>3</mn></msup> <mo>]</mo></mrow></math>ï¼Œæˆ–è€…å¯¹äºé«˜æ•ˆå®ç°æ˜¯<math alttext="script
    upper O left-bracket upper N squared right-bracket"><mrow><mi>ğ’ª</mi> <mo>[</mo>
    <msup><mi>N</mi> <mn>2</mn></msup> <mo>]</mo></mrow></math>ã€‚å¯¹äºå¤§é‡çš„è®­ç»ƒæ ·æœ¬ï¼Œè¿™ç§è®¡ç®—æˆæœ¬å¯èƒ½æ˜¯é™åˆ¶æ€§çš„ã€‚
- en: The results are strongly dependent on a suitable choice for the softening parameter
    `C`. This must be carefully chosen via cross-validation, which can be expensive
    as datasets grow in size.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç»“æœå¼ºçƒˆä¾èµ–äºåˆé€‚çš„è½¯åŒ–å‚æ•°`C`çš„é€‰æ‹©ã€‚å¿…é¡»é€šè¿‡äº¤å‰éªŒè¯ä»”ç»†é€‰æ‹©ï¼Œéšç€æ•°æ®é›†å¢å¤§ï¼Œè¿™å¯èƒ½æ˜¯æ˜‚è´µçš„ã€‚
- en: The results do not have a direct probabilistic interpretation. This can be estimated
    via an internal cross-validation (see the `probability` parameter of `SVC`), but
    this extra estimation is costly.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç»“æœæ²¡æœ‰ç›´æ¥çš„æ¦‚ç‡è§£é‡Šã€‚å¯ä»¥é€šè¿‡å†…éƒ¨äº¤å‰éªŒè¯æ¥ä¼°è®¡ï¼ˆå‚è§`SVC`çš„`probability`å‚æ•°ï¼‰ï¼Œä½†è¿™é¢å¤–çš„ä¼°è®¡æ˜¯æ˜‚è´µçš„ã€‚
- en: With those traits in mind, I generally only turn to SVMs once other simpler,
    faster, and less tuning-intensive methods have been shown to be insufficient for
    my needs. Nevertheless, if you have the CPU cycles to commit to training and cross-validating
    an SVM on your data, the method can lead to excellent results.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: è€ƒè™‘åˆ°è¿™äº›ç‰¹æ€§ï¼Œæˆ‘é€šå¸¸åªæœ‰åœ¨å…¶ä»–æ›´ç®€å•ã€æ›´å¿«é€Ÿã€ä¸éœ€è¦è¿‡å¤šè°ƒæ•´çš„æ–¹æ³•è¢«è¯æ˜ä¸è¶³ä»¥æ»¡è¶³æˆ‘çš„éœ€æ±‚æ—¶ï¼Œæ‰ä¼šè½¬å‘æ”¯æŒå‘é‡æœºï¼ˆSVMï¼‰ã€‚å°½ç®¡å¦‚æ­¤ï¼Œå¦‚æœä½ æœ‰è¶³å¤Ÿçš„CPUå‘¨æœŸæ¥è¿›è¡Œæ•°æ®è®­ç»ƒå’Œäº¤å‰éªŒè¯SVMï¼Œè¿™ç§æ–¹æ³•å¯ä»¥å¸¦æ¥å‡ºè‰²çš„ç»“æœã€‚
