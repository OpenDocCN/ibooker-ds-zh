- en: 'Chapter 43\. In Depth: Support Vector Machines'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第43章：深入探讨支持向量机
- en: Support vector machines (SVMs) are a particularly powerful and flexible class
    of supervised algorithms for both classification and regression. In this chapter,
    we will explore the intuition behind SVMs and their use in classification problems.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 支持向量机（SVMs）是一种特别强大和灵活的监督算法类，适用于分类和回归。在本章中，我们将探讨SVM背后的直觉及其在分类问题中的应用。
- en: 'We begin with the standard imports:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从标准导入开始：
- en: '[PRE0]'
  id: totrans-3
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Note
  id: totrans-4
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注
- en: Full-size, full-color figures are available in the [supplemental materials on
    GitHub](https://oreil.ly/PDSH_GitHub).
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 全尺寸、全彩色图像可在[GitHub的补充材料](https://oreil.ly/PDSH_GitHub)中找到。
- en: Motivating Support Vector Machines
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 激励支持向量机
- en: As part of our discussion of Bayesian classification (see [Chapter 41](ch41.xhtml#section-0505-naive-bayes)),
    we learned about a simple kind of model that describes the distribution of each
    underlying class, and experimented with using it to probabilistically determine
    labels for new points. That was an example of *generative classification*; here
    we will consider instead *discriminative classification*. That is, rather than
    modeling each class, we will simply find a line or curve (in two dimensions) or
    manifold (in multiple dimensions) that divides the classes from each other.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 作为我们讨论贝叶斯分类的一部分（参见[第41章](ch41.xhtml#section-0505-naive-bayes)），我们了解到了描述每个潜在类分布的简单模型，并尝试使用它来概率地确定新点的标签。那是一个生成分类的例子；在这里，我们将考虑判别分类。也就是说，我们不再模拟每个类，而是简单地找到一个（在二维中为线或曲线，在多维中为流形），将类彼此分开。
- en: As an example of this, consider the simple case of a classification task in
    which the two classes of points are well separated (see [Figure 43-1](#fig_0507-support-vector-machines_files_in_output_5_0)).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个例子，考虑一个分类任务的简单情况，其中两类点是完全分开的（见[图43-1](#fig_0507-support-vector-machines_files_in_output_5_0)）。
- en: '[PRE1]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![output 5 0](assets/output_5_0.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![output 5 0](assets/output_5_0.png)'
- en: Figure 43-1\. Simple data for classification
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图43-1\. 分类简单数据
- en: 'A linear discriminative classifier would attempt to draw a straight line separating
    the two sets of data, and thereby create a model for classification. For two-dimensional
    data like that shown here, this is a task we could do by hand. But immediately
    we see a problem: there is more than one possible dividing line that can perfectly
    discriminate between the two classes!'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 线性判别分类器将尝试画一条直线分隔两组数据，并因此创建一个分类模型。对于像这样的二维数据，我们可以手动完成这项任务。但我们立即看到了一个问题：存在不止一条可能完全区分这两类的分界线！
- en: 'We can draw some of them as follows; [Figure 43-2](#fig_0507-support-vector-machines_files_in_output_7_0)
    shows the result:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以如下绘制其中一些；[图43-2](#fig_0507-support-vector-machines_files_in_output_7_0)展示了结果：
- en: '[PRE2]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![output 7 0](assets/output_7_0.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![output 7 0](assets/output_7_0.png)'
- en: Figure 43-2\. Three perfect linear discriminative classifiers for our data
  id: totrans-16
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图43-2\. 我们数据的三个完美线性判别分类器
- en: These are three *very* different separators which, nevertheless, perfectly discriminate
    between these samples. Depending on which you choose, a new data point (e.g.,
    the one marked by the “X” in this plot) will be assigned a different label! Evidently
    our simple intuition of “drawing a line between classes” is not good enough, and
    we need to think a bit more deeply.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这是三个*非常*不同的分隔符，尽管如此，它们完全可以区分这些样本。根据你选择的分界线，新的数据点（例如，在这个图中用“X”标记的点）将被分配不同的标签！显然，我们简单的“在类之间画一条线”的直觉不够好，我们需要更深入地思考。
- en: 'Support Vector Machines: Maximizing the Margin'
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 支持向量机：最大化边缘
- en: 'Support vector machines offer one way to improve on this. The intuition is
    this: rather than simply drawing a zero-width line between the classes, we can
    draw around each line a *margin* of some width, up to the nearest point. Here
    is an example of how this might look ([Figure 43-3](#fig_0507-support-vector-machines_files_in_output_10_0)).'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 支持向量机提供了一种改进方法。其直觉是：与其简单地在类之间画一条零宽度线，我们可以在每条线周围绘制一定宽度的*边缘*，直到最近的点。这是一个展示的例子（见[图43-3](#fig_0507-support-vector-machines_files_in_output_10_0)）。
- en: '[PRE3]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The line that maximizes this margin is the one we will choose as the optimal
    model.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 最大化这个边缘的线就是我们将选择作为最优模型的线。
- en: '![output 10 0](assets/output_10_0.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![output 10 0](assets/output_10_0.png)'
- en: Figure 43-3\. Visualization of “margins” within discriminative classifiers
  id: totrans-23
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图43-3\. 判别分类器内“边缘”的可视化
- en: Fitting a Support Vector Machine
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 适配支持向量机
- en: 'Let’s see the result of an actual fit to this data: we will use Scikit-Learn’s
    support vector classifier (`SVC`) to train an SVM model on this data. For the
    time being, we will use a linear kernel and set the `C` parameter to a very large
    number (we’ll discuss the meaning of these in more depth momentarily):'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看对这些数据进行实际拟合的结果：我们将使用Scikit-Learn的支持向量分类器（`SVC`）来训练一个SVM模型。暂时地，我们将使用线性核并将参数`C`设置为一个非常大的数（稍后我们将深入讨论它们的含义）：
- en: '[PRE4]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: To better visualize what’s happening here, let’s create a quick convenience
    function that will plot SVM decision boundaries for us ([Figure 43-4](#fig_0507-support-vector-machines_files_in_output_16_0)).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地可视化这里发生的情况，让我们创建一个快速便利函数，它将为我们绘制SVM决策边界（[图 43-4](#fig_0507-support-vector-machines_files_in_output_16_0)）。
- en: '[PRE5]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![output 16 0](assets/output_16_0.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![output 16 0](assets/output_16_0.png)'
- en: Figure 43-4\. A support vector machine classifier fit to the data, with margins
    (dashed lines) and support vectors (circles) shown
  id: totrans-31
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 43-4\. 一个拟合到数据的支持向量机分类器，显示了边界（虚线）和支持向量（圆点）
- en: 'This is the dividing line that maximizes the margin between the two sets of
    points. Notice that a few of the training points just touch the margin: they are
    circled in [Figure 43-5](#fig_0507-support-vector-machines_files_in_output_20_0).
    These points are the pivotal elements of this fit; they are known as the *support
    vectors*, and give the algorithm its name. In Scikit-Learn, the identities of
    these points are stored in the `support_vectors_` attribute of the classifier:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这是最大化两组点之间间隔的分隔线。请注意，一些训练点恰好接触边界：它们在[图 43-5](#fig_0507-support-vector-machines_files_in_output_20_0)中被圈出来。这些点是此拟合的关键元素；它们被称为*支持向量*，并赋予了算法其名称。在Scikit-Learn中，这些点的标识存储在分类器的`support_vectors_`属性中：
- en: '[PRE7]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: A key to this classifier’s success is that for the fit, only the positions of
    the support vectors matter; any points further from the margin that are on the
    correct side do not modify the fit. Technically, this is because these points
    do not contribute to the loss function used to fit the model, so their position
    and number do not matter so long as they do not cross the margin.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 此分类器成功的关键在于对拟合来说，只有支持向量的位置是重要的；远离边界但在正确一侧的点不会修改拟合。从技术上讲，这是因为这些点不会对用于拟合模型的损失函数产生贡献，因此它们的位置和数量并不重要，只要它们不跨越边界。
- en: We can see this, for example, if we plot the model learned from the first 60
    points and first 120 points of this dataset ([Figure 43-5](#fig_0507-support-vector-machines_files_in_output_20_0)).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们绘制从这个数据集的前60个点和前120个点学习到的模型（[图 43-5](#fig_0507-support-vector-machines_files_in_output_20_0)），我们可以看到这一点。
- en: '[PRE8]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![output 20 0](assets/output_20_0.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![output 20 0](assets/output_20_0.png)'
- en: Figure 43-5\. The influence of new training points on the SVM model
  id: totrans-38
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 43-5\. 新训练点对SVM模型的影响
- en: 'In the left panel, we see the model and the support vectors for 60 training
    points. In the right panel, we have doubled the number of training points, but
    the model has not changed: the three support vectors in the left panel are the
    same as the support vectors in the right panel. This insensitivity to the exact
    behavior of distant points is one of the strengths of the SVM model.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在左侧面板中，我们看到了60个训练点的模型和支持向量。在右侧面板中，我们增加了训练点的数量，但模型没有改变：左侧面板中的三个支持向量与右侧面板中的支持向量相同。这种对远点行为的确切不敏感性是SVM模型的一种优势之一。
- en: 'If you are running this notebook live, you can use IPython’s interactive widgets
    to view this feature of the SVM model interactively:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您正在实时运行此笔记本，您可以使用IPython的交互式小部件来交互地查看SVM模型的此功能：
- en: '[PRE9]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Beyond Linear Boundaries: Kernel SVM'
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 超越线性边界：核支持向量机
- en: Where SVM can become quite powerful is when it is combined with *kernels*. We
    have seen a version of kernels before, in the basis function regressions of [Chapter 42](ch42.xhtml#section-0506-linear-regression).
    There we projected our data into a higher-dimensional space defined by polynomials
    and Gaussian basis functions, and thereby were able to fit for nonlinear relationships
    with a linear classifier.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 当SVM与*核*结合时，它可以变得非常强大。我们之前在[第42章](ch42.xhtml#section-0506-linear-regression)中已经看到了核的一个版本，即基函数回归。在那里，我们将数据投影到由多项式和高斯基函数定义的更高维空间中，从而能够使用线性分类器拟合非线性关系。
- en: In SVM models, we can use a version of the same idea. To motivate the need for
    kernels, let’s look at some data that is not linearly separable ([Figure 43-6](#fig_0507-support-vector-machines_files_in_output_25_0)).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![output 25 0](assets/output_25_0.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
- en: Figure 43-6\. A linear classifier performs poorly for nonlinear boundaries
  id: totrans-47
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'It is clear that no linear discrimination will *ever* be able to separate this
    data. But we can draw a lesson from the basis function regressions in [Chapter 42](ch42.xhtml#section-0506-linear-regression),
    and think about how we might project the data into a higher dimension such that
    a linear separator *would* be sufficient. For example, one simple projection we
    could use would be to compute a *radial basis function* (RBF) centered on the
    middle clump:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: We can visualize this extra data dimension using a three-dimensional plot, as
    seen in [Figure 43-7](#fig_0507-support-vector-machines_files_in_output_29_0).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '![output 29 0](assets/output_29_0.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
- en: Figure 43-7\. A third dimension added to the data allows for linear separation
  id: totrans-53
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We can see that with this additional dimension, the data becomes trivially linearly
    separable, by drawing a separating plane at, say, *r*=0.7.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case we had to choose and carefully tune our projection: if we had
    not centered our radial basis function in the right location, we would not have
    seen such clean, linearly separable results. In general, the need to make such
    a choice is a problem: we would like to somehow automatically find the best basis
    functions to use.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: One strategy to this end is to compute a basis function centered at *every*
    point in the dataset, and let the SVM algorithm sift through the results. This
    type of basis function transformation is known as a *kernel transformation*, as
    it is based on a similarity relationship (or kernel) between each pair of points.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: A potential problem with this strategy—projecting <math alttext="upper N"><mi>N</mi></math>
    points into <math alttext="upper N"><mi>N</mi></math> dimensions—is that it might
    become very computationally intensive as <math alttext="upper N"><mi>N</mi></math>
    grows large. However, because of a neat little procedure known as the [*kernel
    trick*](https://oreil.ly/h7PBj), a fit on kernel-transformed data can be done
    implicitly—that is, without ever building the full <math alttext="upper N"><mi>N</mi></math>
    -dimensional representation of the kernel projection. This kernel trick is built
    into the SVM, and is one of the reasons the method is so powerful.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: 'In Scikit-Learn, we can apply kernelized SVM simply by changing our linear
    kernel to an RBF kernel, using the `kernel` model hyperparameter:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Let’s use our previously defined function to visualize the fit and identify
    the support vectors ([Figure 43-8](#fig_0507-support-vector-machines_files_in_output_33_0)).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '![output 33 0](assets/output_33_0.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
- en: Figure 43-8\. Kernel SVM fit to the data
  id: totrans-63
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Using this kernelized support vector machine, we learn a suitable nonlinear
    decision boundary. This kernel transformation strategy is used often in machine
    learning to turn fast linear methods into fast nonlinear methods, especially for
    models in which the kernel trick can be used.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种核化支持向量机，我们学习到了一个适合的非线性决策边界。这种核变换策略在机器学习中经常被使用，将快速的线性方法转换为快速的非线性方法，特别适用于可以使用核技巧的模型。
- en: 'Tuning the SVM: Softening Margins'
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 调整 SVM：软化间隔
- en: Our discussion thus far has centered around very clean datasets, in which a
    perfect decision boundary exists. But what if your data has some amount of overlap?
    For example, you may have data like this (see [Figure 43-9](#fig_0507-support-vector-machines_files_in_output_36_0)).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们的讨论集中在非常干净的数据集上，其中存在完美的决策边界。但是如果您的数据有一定的重叠呢？例如，您可能有这样的数据（见[图 43-9](#fig_0507-support-vector-machines_files_in_output_36_0)）。
- en: '[PRE15]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '![output 36 0](assets/output_36_0.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![output 36 0](assets/output_36_0.png)'
- en: Figure 43-9\. Data with some level of overlap
  id: totrans-69
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 43-9\. 具有一定重叠级别的数据
- en: 'To handle this case, the SVM implementation has a bit of a fudge factor that
    “softens” the margin: that is, it allows some of the points to creep into the
    margin if that allows a better fit. The hardness of the margin is controlled by
    a tuning parameter, most often known as `C`. For a very large `C`, the margin
    is hard, and points cannot lie in it. For a smaller `C`, the margin is softer
    and can grow to encompass some points.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 为了处理这种情况，SVM 实现中有一个“软化”间隔的修正因子：即，如果允许更好的拟合，某些点可以进入间隔。间隔的硬度由调整参数控制，通常称为`C`。对于很大的`C`，间隔是硬的，点不能位于其中。对于较小的`C`，间隔较软，并且可以包含一些点。
- en: 'The plot shown in [Figure 43-10](#fig_0507-support-vector-machines_files_in_output_38_0)
    gives a visual picture of how a changing `C` affects the final fit via the softening
    of the margin:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 43-10](#fig_0507-support-vector-machines_files_in_output_38_0)中显示的图表展示了通过软化间隔来改变`C`如何影响最终拟合的视觉效果：'
- en: '[PRE16]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '![output 38 0](assets/output_38_0.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![output 38 0](assets/output_38_0.png)'
- en: Figure 43-10\. The effect of the `C` parameter on the support vector fit
  id: totrans-74
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 43-10\. `C` 参数对支持向量拟合的影响
- en: The optimal value of `C` will depend on your dataset, and you should tune this
    parameter using cross-validation or a similar procedure (refer back to [Chapter 39](ch39.xhtml#section-0503-hyperparameters-and-model-validation)).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '`C` 的最佳值将取决于您的数据集，您应该使用交叉验证或类似的程序来调整此参数（参考[第 39 章](ch39.xhtml#section-0503-hyperparameters-and-model-validation)）。'
- en: 'Example: Face Recognition'
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 示例：人脸识别
- en: 'As an example of support vector machines in action, let’s take a look at the
    facial recognition problem. We will use the Labeled Faces in the Wild dataset,
    which consists of several thousand collated photos of various public figures.
    A fetcher for the dataset is built into Scikit-Learn:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 作为支持向量机在实际中的应用示例，让我们来看一下人脸识别问题。我们将使用“野外标记人脸”数据集，该数据集包含数千张各种公众人物的合并照片。Scikit-Learn
    内置了该数据集的获取器：
- en: '[PRE17]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Let’s plot a few of these faces to see what we’re working with (see [Figure 43-11](#fig_0507-support-vector-machines_files_in_output_43_0)).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们绘制几张这些人脸，看看我们正在处理的内容（见[图 43-11](#fig_0507-support-vector-machines_files_in_output_43_0)）。
- en: '[PRE18]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '![output 43 0](assets/output_43_0.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![output 43 0](assets/output_43_0.png)'
- en: Figure 43-11\. Examples from the Labeled Faces in the Wild dataset
  id: totrans-82
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 43-11\. 来自野外标记人脸数据集的示例
- en: 'Each image contains 62 × 47, or around 3,000, pixels. We could proceed by simply
    using each pixel value as a feature, but often it is more effective to use some
    sort of preprocessor to extract more meaningful features; here we will use principal
    component analysis (see [Chapter 45](ch45.xhtml#section-0509-principal-component-analysis))
    to extract 150 fundamental components to feed into our support vector machine
    classifier. We can do this most straightforwardly by packaging the preprocessor
    and the classifier into a single pipeline:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 每个图像包含 62 × 47，约 3,000 个像素。我们可以简单地使用每个像素值作为特征，但通常使用某种预处理器来提取更有意义的特征更为有效；在这里，我们将使用主成分分析（见[第
    45 章](ch45.xhtml#section-0509-principal-component-analysis)）提取 150 个基本组分，以供支持向量机分类器使用。我们可以通过将预处理器和分类器打包到单个管道中来实现这一点：
- en: '[PRE19]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'For the sake of testing our classifier output, we will split the data into
    a training set and a testing set:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试我们分类器的输出，我们将数据分割为训练集和测试集：
- en: '[PRE20]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Finally, we can use grid search cross-validation to explore combinations of
    parameters. Here we will adjust `C` (which controls the margin hardness) and `gamma`
    (which controls the size of the radial basis function kernel), and determine the
    best model:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以使用网格搜索交叉验证来探索参数的组合。在这里，我们将调整`C`（控制边界硬度）和`gamma`（控制径向基函数核的大小），并确定最佳模型：
- en: '[PRE21]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The optimal values fall toward the middle of our grid; if they fell at the edges,
    we would want to expand the grid to make sure we have found the true optimum.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 最优值集中在我们网格的中间；如果它们在边缘，我们将扩展网格以确保找到真正的最优值。
- en: 'Now with this cross-validated model we can predict the labels for the test
    data, which the model has not yet seen:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 现在有了这个经过交叉验证的模型，我们可以预测测试数据的标签，这些数据模型尚未见过：
- en: '[PRE22]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Let’s take a look at a few of the test images along with their predicted values
    (see [Figure 43-12](#fig_0507-support-vector-machines_files_in_output_53_0)).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一些测试图像及其预测值（见[图 43-12](#fig_0507-support-vector-machines_files_in_output_53_0)）。
- en: '[PRE23]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '![output 53 0](assets/output_53_0.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![output 53 0](assets/output_53_0.png)'
- en: Figure 43-12\. Labels predicted by our model
  id: totrans-95
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 43-12\. 我们模型预测的标签
- en: 'Out of this small sample, our optimal estimator mislabeled only a single face
    (Bush’s face in the bottom row was mislabeled as Blair). We can get a better sense
    of our estimator’s performance using the classification report, which lists recovery
    statistics label by label:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个小样本中，我们的最优估计器只误标了一个面孔（底部行的布什面孔被误标为布莱尔）。我们可以通过分类报告更好地了解我们估计器的性能，报告会逐标签列出恢复统计信息：
- en: '[PRE24]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: We might also display the confusion matrix between these classes (see [Figure 43-13](#fig_0507-support-vector-machines_files_in_output_57_0)).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以显示这些类别之间的混淆矩阵（见[图 43-13](#fig_0507-support-vector-machines_files_in_output_57_0)）。
- en: '[PRE25]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '![output 57 0](assets/output_57_0.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![output 57 0](assets/output_57_0.png)'
- en: Figure 43-13\. Confusion matrix for the faces data
  id: totrans-101
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 43-13\. 面部数据的混淆矩阵
- en: This helps us get a sense of which labels are likely to be confused by the estimator.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这帮助我们了解哪些标签可能会被估计器混淆。
- en: 'For a real-world facial recognition task, in which the photos do not come pre-cropped
    into nice grids, the only difference in the facial classification scheme is the
    feature selection: you would need to use a more sophisticated algorithm to find
    the faces, and extract features that are independent of the pixellation. For this
    kind of application, one good option is to make use of [OpenCV](http://opencv.org),
    which, among other things, includes pretrained implementations of state-of-the-art
    feature extraction tools for images in general and faces in particular.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个现实世界的人脸识别任务，在这种任务中照片并未预先裁剪成漂亮的网格，面部分类方案唯一的区别在于特征选择：您需要使用更复杂的算法来找到面部，并提取与像素化无关的特征。对于这种应用，一个好的选择是利用[OpenCV](http://opencv.org)，它包括对一般图像和特别是人脸的先前实现的最新特征提取工具。
- en: Summary
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'This has been a brief intuitive introduction to the principles behind support
    vector machines. These models are a powerful classification method for a number
    of reasons:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这是支持向量机背后原理的简明直观介绍。这些模型由于以下几个原因而是一种强大的分类方法：
- en: Their dependence on relatively few support vectors means that they are compact
    and take up very little memory.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们依赖于相对较少的支持向量，因此紧凑且占用极少的内存空间。
- en: Once the model is trained, the prediction phase is very fast.
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一旦模型训练完成，预测阶段非常快速。
- en: Because they are affected only by points near the margin, they work well with
    high-dimensional data—even data with more dimensions than samples, which is challenging
    for other algorithms.
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因为它们只受到靠近边界的点的影响，所以它们在处理高维数据时表现良好——即使是比样本更多维度的数据，这对其他算法来说是个挑战。
- en: Their integration with kernel methods makes them very versatile, able to adapt
    to many types of data.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们与核方法的集成使它们非常灵活，能够适应许多类型的数据。
- en: 'However, SVMs have several disadvantages as well:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，支持向量机（SVMs）也有几个缺点：
- en: The scaling with the number of samples <math alttext="upper N"><mi>N</mi></math>
    is <math alttext="script upper O left-bracket upper N cubed right-bracket"><mrow><mi>𝒪</mi>
    <mo>[</mo> <msup><mi>N</mi> <mn>3</mn></msup> <mo>]</mo></mrow></math> at worst,
    or <math alttext="script upper O left-bracket upper N squared right-bracket"><mrow><mi>𝒪</mi>
    <mo>[</mo> <msup><mi>N</mi> <mn>2</mn></msup> <mo>]</mo></mrow></math> for efficient
    implementations. For large numbers of training samples, this computational cost
    can be prohibitive.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 样本数量<math alttext="upper N"><mi>N</mi></math>的缩放为最坏情况下是<math alttext="script
    upper O left-bracket upper N cubed right-bracket"><mrow><mi>𝒪</mi> <mo>[</mo>
    <msup><mi>N</mi> <mn>3</mn></msup> <mo>]</mo></mrow></math>，或者对于高效实现是<math alttext="script
    upper O left-bracket upper N squared right-bracket"><mrow><mi>𝒪</mi> <mo>[</mo>
    <msup><mi>N</mi> <mn>2</mn></msup> <mo>]</mo></mrow></math>。对于大量的训练样本，这种计算成本可能是限制性的。
- en: The results are strongly dependent on a suitable choice for the softening parameter
    `C`. This must be carefully chosen via cross-validation, which can be expensive
    as datasets grow in size.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结果强烈依赖于合适的软化参数`C`的选择。必须通过交叉验证仔细选择，随着数据集增大，这可能是昂贵的。
- en: The results do not have a direct probabilistic interpretation. This can be estimated
    via an internal cross-validation (see the `probability` parameter of `SVC`), but
    this extra estimation is costly.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结果没有直接的概率解释。可以通过内部交叉验证来估计（参见`SVC`的`probability`参数），但这额外的估计是昂贵的。
- en: With those traits in mind, I generally only turn to SVMs once other simpler,
    faster, and less tuning-intensive methods have been shown to be insufficient for
    my needs. Nevertheless, if you have the CPU cycles to commit to training and cross-validating
    an SVM on your data, the method can lead to excellent results.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这些特性，我通常只有在其他更简单、更快速、不需要过多调整的方法被证明不足以满足我的需求时，才会转向支持向量机（SVM）。尽管如此，如果你有足够的CPU周期来进行数据训练和交叉验证SVM，这种方法可以带来出色的结果。
