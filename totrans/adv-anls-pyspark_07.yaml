- en: Chapter 7\. Geospatial and Temporal Data Analysis on Taxi Trip Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Geospatial data refers to data that has location information embedded in it
    in some form. Such data is being generated currently at a massive scale by billions
    of sources, such as mobile phones and sensors, every day. Data about movement
    of humans and machines, and from remote sensing, is significant for our economy
    and general well-being. Geospatial analytics can provide us with the tools and
    methods we need to make sense of all that data and put it to use in solving problems
    we face.
  prefs: []
  type: TYPE_NORMAL
- en: The PySpark and PyData ecosystems have evolved considerably over the last few
    years when it comes to geospatial analysis. They are being used across industries
    for handling location-rich data and, in turn, impacting our daily lives. One daily
    activity where geospatial data manifests itself in a visible way is local transport.
    The phenomenon of digital cab hailing services becoming popular over the last
    few years has led to us being more aware of geospatial technology. In this chapter,
    we’ll use our PySpark and data analysis skills in this domain as we work with
    a dataset containing information about trips taken by cabs in New York City.
  prefs: []
  type: TYPE_NORMAL
- en: 'One statistic that is important to understanding the economics of taxis is
    *utilization*: the fraction of time that a cab is on the road and is occupied
    by one or more passengers. One factor that impacts utilization is the passenger’s
    destination: a cab that drops off passengers near Union Square at midday is much
    more likely to find its next fare in just a minute or two, whereas a cab that
    drops someone off at 2 A.M. on Staten Island may have to drive all the way back
    to Manhattan before it finds its next fare. We’d like to quantify these effects
    and find out the average time it takes for a cab to find its next fare as a function
    of the borough in which it dropped its passengers off—Manhattan, Brooklyn, Queens,
    the Bronx, Staten Island, or none of the above (e.g., if it dropped the passenger
    off somewhere outside of the city, like Newark Liberty International Airport).'
  prefs: []
  type: TYPE_NORMAL
- en: We’ll start by setting up our dataset, and then we’ll dive into geospatial analysis.
    We will learn about the GeoJSON format and use tools from the PyData ecosystem
    in combination with PySpark. We’ll use GeoPandas for working with *geospatial
    information*, like points of longitude and latitude and spatial boundaries. To
    wrap things up, we will work with temporal features of our data, such as date
    and time, by performing a type of analysis called sessionization. This will help
    us understand utilization of New York City cabs. PySpark’s DataFrame API provides
    out-of-the-box data types and methods to handle temporal data.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s get going by downloading our dataset and exploring it using PySpark.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For this analysis, we’re only going to consider the fare data from January
    2013, which will be about 2.5 GB of data after we uncompress it. You can access
    the [data for each month of 2013](https://oreil.ly/7m7Ki), and if you have a sufficiently
    large PySpark cluster at your disposal, you can re-create the following analysis
    against all of the data for the year. For now, let’s create a working directory
    on our client machine and take a look at the structure of the fare data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Each row of the file after the header represents a single taxi ride in CSV format.
    For each ride, we have some attributes of the cab (a hashed version of the medallion
    number) as well as the driver (a hashed version of the *hack license*, which is
    what a license to drive a taxi is called), some temporal information about when
    the trip started and ended, and the longitude/latitude coordinates for where the
    passengers were picked up and dropped off.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s create a *taxidata* directory and copy the trip data into the storage:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: We have used a local filesystem here, but this may not be the case for you.
    It’s more likely nowadays to use a cloud native filesystem such as AWS S3 or GCS.
    In such a scenario, you will upload the data to S3 or GCS, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now start the PySpark shell:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the PySpark shell has loaded, we can create a dataset from the taxi data
    and examine the first few lines, just as we have in other chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'This looks like a well-formatted dataset at first glance. Let’s have a look
    at the DataFrame’s schema:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: We are representing the `pickup_datetime` and `dropoff_datetime` fields as `Strings`
    and storing the individual `(x,y)` coordinates of the pickup and drop-off locations
    in their own fields as `Doubles`. We want the datetime fields as timestamps since
    that will allow us to manipulate and analyze them conveniently.
  prefs: []
  type: TYPE_NORMAL
- en: Converting Datetime Strings to Timestamps
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As mentioned previously, PySpark provides out-of-the-box methods for handling
    temporal data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, we will use the `to_timestamp` function to parse the datetime
    strings and convert them into timestamps:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s have a look at the schema again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The `pickup_datetime` and `dropoff_datetime` fields are timestamps now. Well
    done!
  prefs: []
  type: TYPE_NORMAL
- en: 'We’d mentioned that this dataset contains trips from January 2013\. Don’t just
    take our word for this, though. We can confirm this by sorting the `pickup_datetime`
    field to get the latest datetime in the data. For this, we use DataFrame’s `sort`
    method combined with PySpark column’s `desc` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: With our data types in place, let’s check if there are any inconsistencies in
    our data.
  prefs: []
  type: TYPE_NORMAL
- en: Handling Invalid Records
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Anyone who has been working with large-scale, real-world datasets knows that
    they invariably contain at least a few records that do not conform to the expectations
    of the person who wrote the code to handle them. Many PySpark pipelines have failed
    because of invalid records that caused the parsing logic to throw an exception.
    When performing interactive analysis, we can get a sense of potential anomalies
    in the data by focusing on key variables.
  prefs: []
  type: TYPE_NORMAL
- en: In our case, variables containing geospatial and temporal information are worth
    looking at for inconsistencies. Presence of null values in these columns will
    definitely throw off our analysis.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s remove the null values from our data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Another commonsense check that we can do is for latitude and longitude records
    where the values are zero. We know that for the region we’re concerned with, those
    would be invalid values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_geospatial_and_temporal_data_analysis___span_class__keep_together__on_taxi_trip_data__span__CO1-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Multiple `OR` conditions will be true if either of them evaluates to `True`
    for any record.
  prefs: []
  type: TYPE_NORMAL
- en: We have quite a few of these. If it looks as if a taxi took a passenger to the
    South Pole, we can be reasonably confident that the record is invalid and should
    be excluded from our analysis. We will not remove them but get back to them toward
    the end of the next section to see how they can affect our analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 'In production settings, we handle these exceptions one at a time by checking
    the logs for the individual tasks, figuring out which line of code threw the exception,
    and then figuring out how to tweak the code to ignore or correct the invalid records.
    This is a tedious process, and it often feels like we’re playing whack-a-mole:
    just as we get one exception fixed, we discover another one on a record that came
    later within the partition.'
  prefs: []
  type: TYPE_NORMAL
- en: One strategy that experienced data scientists deploy when working with a new
    dataset is to add a `try-except` block to their parsing code so that any invalid
    records can be written out to the logs without causing the entire job to fail.
    If there are only a handful of invalid records in the entire dataset, we might
    be okay with ignoring them and continuing with our analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have prepared our dataset, let’s get started with geospatial analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Geospatial Analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are two major kinds of geospatial data—vector and raster—and there are
    different tools for working with each type. In our case, we have latitude and
    longitude for our taxi trip records, and vector data stored in the GeoJSON format
    that represents the boundaries of the different boroughs of New York. We’ve looked
    at the latitude and longitude points. Let’s start by having a look at the GeoJSON
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Intro to GeoJSON
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The data we’ll use for the boundaries of boroughs in New York City comes written
    in a format called *GeoJSON*. The core object in GeoJSON is called a *feature*,
    which is made up of a *geometry* instance and a set of key-value pairs called
    *properties*. A geometry is a shape like a point, line, or polygon. A set of features
    is called a `FeatureCollection`. Let’s pull down the GeoJSON data for the NYC
    borough maps and take a look at its structure.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the *taxidata* directory on your client machine, download the data and rename
    the file to something a bit shorter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Open the file and look at a feature record. Note the properties and the geometry
    objects—in this case, a polygon representing the boundaries of the borough and
    the properties containing the name of the borough and other related information.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: GeoPandas
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first thing you should consider when choosing a library to perform geospatial
    analysis is determine what kind of data you will need to work with. We need a
    library that can parse GeoJSON data and can handle spatial relationships, like
    detecting whether a given longitude/latitude pair is contained inside a polygon
    that represents the boundaries of a particular borough. We will use the [GeoPandas
    library](https://geopandas.org) for this task. GeoPandas is an open source project
    to make working with geospatial data in Python easier. It extends the data types
    used by the pandas library, which we used in previous chapters, to allow spatial
    operations on geometric data types.
  prefs: []
  type: TYPE_NORMAL
- en: 'Install the GeoPandas package using pip:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Let us now start examining the geospatial aspects of the taxi data. For each
    trip, we have longitude/latitude pairs representing where the passenger was picked
    up and dropped off. We would like to be able to determine which borough each of
    these longitude/latitude pairs belongs to, and identify any trips that did not
    start or end in any of the five boroughs. For example, if a taxi took passengers
    from Manhattan to Newark Liberty International Airport, that would be a valid
    ride that would be interesting to analyze, even though it would not end within
    one of the five boroughs.
  prefs: []
  type: TYPE_NORMAL
- en: 'To perform our borough analysis, we need to load the GeoJSON data we downloaded
    earlier and stored in the *nyc-boroughs.geojson* file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Before we use the GeoJSON features on the taxi trip data, we should take a moment
    to think about how to organize this geospatial data for maximum efficiency. One
    option would be to research data structures that are optimized for geospatial
    lookups, such as quad trees, and then find or write our own implementation. Instead,
    we will try to come up with a quick heuristic that will allow us to bypass that
    bit of work.
  prefs: []
  type: TYPE_NORMAL
- en: We will iterate through the `gdf` until we find a feature whose geometry contains
    a given `point` of longitude/latitude. Most taxi rides in NYC begin and end in
    Manhattan, so if the geospatial features that represent Manhattan are earlier
    in the sequence, our searches will end relatively quickly. We can use the fact
    that the `boroughCode` property of each feature can be used as a sorting key,
    with the code for Manhattan equal to 1 and the code for Staten Island equal to
    5\. Within the features for each borough, we want the features associated with
    the largest polygons to come before those associated with the smaller polygons,
    because most trips will be to and from the “major” region of each borough.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will calculate area associated with each feature’s geometry and store it
    as a new column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Sorting the features by the combination of the borough code and the area of
    each feature’s geometry should do the trick:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Note that we’re sorting based on area value in descending order because we want
    the largest polygons to come first, and `sort_values` sorts in ascending order
    by default.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we can broadcast the sorted features in the `gdf` GeoPandas DataFrame to
    the cluster and write a function that uses these features to find out in which
    of the five boroughs (if any) a particular trip ended:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'We can apply `find_borough` to the trips in the `taxi_raw` DataFrame to create
    a histogram of trips by borough:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: As we expected, the vast majority of trips end in the borough of Manhattan,
    while relatively few trips end in Staten Island. One surprising observation is
    the number of trips that end outside of any borough; the number of `null` records
    is substantially larger than the number of taxi rides that end in the Bronx.
  prefs: []
  type: TYPE_NORMAL
- en: We had talked about handling such invalid records earlier but did not remove
    them. It is left as an exercise for you to remove such records and create a histogram
    from the cleaned-up data. Once done, you will notice a reduction in the number
    of `null` entries, leaving a much more reasonable number of observations that
    had drop-offs outside the city.
  prefs: []
  type: TYPE_NORMAL
- en: Having worked with the geospatial aspects of our data, let us now dig deeper
    into the temporal nature of our data by performing sessionization using PySpark.
  prefs: []
  type: TYPE_NORMAL
- en: Sessionization in PySpark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The kind of analysis, in which we want to analyze a single entity as it executes
    a series of events over time, is called *sessionization*, and is commonly performed
    over web logs to analyze the behavior of the users of a website. PySpark provides
    `Window` and aggregation functions out of the box that can be used to perform
    such analysis. These allow us to focus on business logic instead of trying to
    implement complex data manipulation and calculation. We will use these in the
    next section to better understand utilization of taxi cabs in our dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Sessionization can be a very powerful technique for uncovering insights in data
    and building new data products that can be used to help people make better decisions.
    For example, Google’s spell-correction engine is built on top of the sessions
    of user activity that Google builds each day from the logged records of every
    event (searches, clicks, maps visits, etc.) occurring on its web properties. To
    identify likely spell-correction candidates, Google processes those sessions looking
    for situations where a user typed a query, didn’t click anything, typed a slightly
    different query a few seconds later, and then clicked a result and didn’t come
    back to Google. Then it counts how often this pattern occurs for any pair of queries.
    If it occurs frequently enough (e.g., if every time we see the query “untied stats,”
    it’s followed a few seconds later by the query “united states”), then we assume
    that the second query is a spell correction of the first.
  prefs: []
  type: TYPE_NORMAL
- en: This analysis takes advantage of the patterns of human behavior that are represented
    in the event logs to build a spell-correction engine from data that is more powerful
    than any engine that could be created from a dictionary. The engine can be used
    to perform spell correction in any language and can correct words that might not
    be included in any dictionary (e.g., the name of a new startup) or queries like
    “untied stats” where none of the words are misspelled! Google uses similar techniques
    to show recommended and related searches, as well as to decide which queries should
    return a OneBox result that gives the answer to a query on the search page itself,
    without requiring that the user click through to a different page. There are OneBoxes
    for weather, scores from sporting events, addresses, and lots of other kinds of
    queries.
  prefs: []
  type: TYPE_NORMAL
- en: So far, information about the set of events that occurs to each entity is spread
    out across the DataFrame’s partitions, so, for analysis, we need to place these
    relevant events next to each other and in chronological order. In the next section,
    we will show how to efficiently construct and analyze sessions using advanced
    PySpark functionality.
  prefs: []
  type: TYPE_NORMAL
- en: 'Building Sessions: Secondary Sorts in PySpark'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The naive way to create sessions in PySpark is to perform a `groupBy` on the
    identifier we want to create sessions for and then sort the events post-shuffle
    by a timestamp identifier. If we only have a small number of events for each entity,
    this approach will work reasonably well. However, because this approach requires
    all the events for any particular entity to be in memory at the same time, it
    will not scale as the number of events for each entity gets larger and larger.
    We need a way of building sessions that does not require all of the events for
    a particular entity to be held in memory at the same time for sorting.
  prefs: []
  type: TYPE_NORMAL
- en: 'In MapReduce, we can build sessions by performing a *secondary sort*, where
    we create a composite key made up of an identifier and a timestamp value, sort
    all of the records on the composite key, and then use a custom partitioner and
    grouping function to ensure that all of the records for the same identifier appear
    in the same output partition. Fortunately, PySpark can also support a similar
    pattern by using `Window` functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'First, we use the `partitionBy` method to ensure that all of the records that
    have the same value for the `license` column end up in the same partition. Then,
    within each of these partitions, we sort the records by their `license` value
    (so all trips by the same driver appear together) and then by their `pickupTime`
    so that the sequence of trips appears in sorted order within the partition. Now
    when we aggregate the trip records, we can be sure that the trips are ordered
    in a way that is optimal for sessions analysis. Because this operation triggers
    a shuffle and a fair bit of computation and we’ll need to use the results more
    than once, we cache them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Executing a sessionization pipeline is an expensive operation, and the sessionized
    data is often useful for many different analysis tasks that we might want to perform.
    In settings where one might want to pick up on the analysis later or collaborate
    with other data scientists, it is a good idea to amortize the cost of sessionizing
    a large dataset by only performing the sessionization once and then writing the
    sessionized data to a filesystem such as S3 or HDFS so that it can be used to
    answer lots of different questions. Performing sessionization once is also a good
    way to enforce standard rules for session definitions across the entire data science
    team, which has the same benefits for ensuring apples-to-apples comparisons of
    results.
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, we are ready to analyze our sessions data to see how long it
    takes for a driver to find his next fare after a drop-off in a particular borough.
    We will use the `lag` function along with the `window_spec` object created earlier
    to take two trips and compute the duration in seconds between the drop-off time
    of the first trip and the pickup time of the second:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we should do a validation check to ensure that most of the durations are
    nonnegative:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Only a few of the records have a negative duration, and when we examine them
    more closely, there don’t seem to be any common patterns to them that we could
    use to understand the source of the erroneous data. If we exclude these negative
    duration records from our input dataset and look at the average and standard deviation
    of the pickup times by borough, we see this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: As we would expect, the data shows that drop-offs in Manhattan have the shortest
    amount of downtime for drivers, at around 10 minutes. Taxi rides that end in Brooklyn
    have a downtime of more than twice that, and the relatively few rides that end
    in Staten Island take drivers an average of almost 45 minutes to get to their
    next fare.
  prefs: []
  type: TYPE_NORMAL
- en: As the data demonstrates, taxi drivers have a major financial incentive to discriminate
    among passengers based on their final destination; drop-offs in Staten Island,
    in particular, involve an extensive amount of downtime for a driver. The NYC Taxi
    and Limousine Commission has made a major effort over the years to identify this
    discrimination and has fined drivers who have been caught rejecting passengers
    because of where they wanted to go. It would be interesting to attempt to examine
    the data for unusually short taxi rides that could be indicative of a dispute
    between the driver and the passenger about where the passenger wanted to be dropped
    off.
  prefs: []
  type: TYPE_NORMAL
- en: Where to Go from Here
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we worked with both temporal and spatial features of a real-world
    dataset. The familiarity with geospatial analysis that you have gained so far
    can be used to dive into frameworks such as Apache Sedona or GeoMesa. They will
    have a steeper learning curve compared to working with GeoPandas and UDFs but
    will be more efficient. There’s also a lot of scope for using data visualization
    with geospatial and temporal data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Further, imagine using this same technique on the taxi data to build an application
    that could recommend the best place for a cab to go after a drop-off based on
    current traffic patterns and the historical record of next-best locations contained
    within this data. You could also look at the information from the perspective
    of someone trying to catch a cab: given the current time, place, and weather data,
    what is the probability that I will be able to hail a cab from the street within
    the next five minutes? This sort of information could be incorporated into applications
    like Google Maps to help travelers decide when to leave and which travel option
    they should take.'
  prefs: []
  type: TYPE_NORMAL
