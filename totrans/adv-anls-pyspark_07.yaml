- en: Chapter 7\. Geospatial and Temporal Data Analysis on Taxi Trip Data
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第七章。纽约市出租车行程数据的地理空间和时间数据分析
- en: Geospatial data refers to data that has location information embedded in it
    in some form. Such data is being generated currently at a massive scale by billions
    of sources, such as mobile phones and sensors, every day. Data about movement
    of humans and machines, and from remote sensing, is significant for our economy
    and general well-being. Geospatial analytics can provide us with the tools and
    methods we need to make sense of all that data and put it to use in solving problems
    we face.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 地理空间数据指的是数据中嵌入了某种形式的位置信息。这类数据目前以每天数十亿源的规模产生。这些源包括移动电话和传感器等，涉及人类和机器的移动数据，以及来自遥感的数据，对我们的经济和总体福祉至关重要。地理空间分析能够为我们提供处理这些数据并解决相关问题所需的工具和方法。
- en: The PySpark and PyData ecosystems have evolved considerably over the last few
    years when it comes to geospatial analysis. They are being used across industries
    for handling location-rich data and, in turn, impacting our daily lives. One daily
    activity where geospatial data manifests itself in a visible way is local transport.
    The phenomenon of digital cab hailing services becoming popular over the last
    few years has led to us being more aware of geospatial technology. In this chapter,
    we’ll use our PySpark and data analysis skills in this domain as we work with
    a dataset containing information about trips taken by cabs in New York City.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 过去几年来，PySpark 和 PyData 生态系统在地理空间分析方面有了显著发展。它们被各行各业用于处理富含位置信息的数据，从而影响我们的日常生活。一个日常活动中，地理空间数据以显著方式展现出来的例子是本地交通。过去几年间数字打车服务的流行使得我们更加关注地理空间技术。在本章中，我们将利用我们的
    PySpark 和数据分析技能，处理一个包含纽约市出租车行程信息的数据集。
- en: 'One statistic that is important to understanding the economics of taxis is
    *utilization*: the fraction of time that a cab is on the road and is occupied
    by one or more passengers. One factor that impacts utilization is the passenger’s
    destination: a cab that drops off passengers near Union Square at midday is much
    more likely to find its next fare in just a minute or two, whereas a cab that
    drops someone off at 2 A.M. on Staten Island may have to drive all the way back
    to Manhattan before it finds its next fare. We’d like to quantify these effects
    and find out the average time it takes for a cab to find its next fare as a function
    of the borough in which it dropped its passengers off—Manhattan, Brooklyn, Queens,
    the Bronx, Staten Island, or none of the above (e.g., if it dropped the passenger
    off somewhere outside of the city, like Newark Liberty International Airport).'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 了解出租车经济学的一个重要统计量是*利用率*：出租车在路上并有一名或多名乘客的时间比例。影响利用率的一个因素是乘客的目的地：白天在联合广场附近下车的出租车更有可能在一两分钟内找到下一单，而凌晨2点在史泰登岛下车的出租车可能需要驱车返回曼哈顿才能找到下一单。我们希望量化这些影响，并找出出租车在各区域下车后找到下一单的平均时间，例如曼哈顿、布鲁克林、皇后区、布朗克斯、史泰登岛，或者在城市之外（如纽瓦克自由国际机场）下车的情况。
- en: We’ll start by setting up our dataset, and then we’ll dive into geospatial analysis.
    We will learn about the GeoJSON format and use tools from the PyData ecosystem
    in combination with PySpark. We’ll use GeoPandas for working with *geospatial
    information*, like points of longitude and latitude and spatial boundaries. To
    wrap things up, we will work with temporal features of our data, such as date
    and time, by performing a type of analysis called sessionization. This will help
    us understand utilization of New York City cabs. PySpark’s DataFrame API provides
    out-of-the-box data types and methods to handle temporal data.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从设置数据集开始，然后深入地理空间分析。我们将学习 GeoJSON 格式，并结合 PyData 生态系统中的工具和 PySpark 使用。我们将使用
    GeoPandas 处理*地理空间信息*，如经度和纬度点和空间边界。最后，我们将通过进行会话化分析处理数据的时间特征，如日期和时间。这将帮助我们了解纽约市出租车的使用情况。PySpark
    的 DataFrame API 提供了处理时间数据所需的数据类型和方法。
- en: Let’s get going by downloading our dataset and exploring it using PySpark.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始通过下载数据集并使用 PySpark 进行探索。
- en: Preparing the Data
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备数据
- en: 'For this analysis, we’re only going to consider the fare data from January
    2013, which will be about 2.5 GB of data after we uncompress it. You can access
    the [data for each month of 2013](https://oreil.ly/7m7Ki), and if you have a sufficiently
    large PySpark cluster at your disposal, you can re-create the following analysis
    against all of the data for the year. For now, let’s create a working directory
    on our client machine and take a look at the structure of the fare data:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个分析，我们只考虑 2013 年 1 月的车费数据，解压后大约为 2.5 GB 数据。你可以访问 [2013 年每个月的数据](https://oreil.ly/7m7Ki)，如果你有足够大的
    PySpark 集群，可以对整年的数据进行类似分析。现在，让我们在客户端机器上创建一个工作目录，并查看车费数据的结构：
- en: '[PRE0]'
  id: totrans-8
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Each row of the file after the header represents a single taxi ride in CSV format.
    For each ride, we have some attributes of the cab (a hashed version of the medallion
    number) as well as the driver (a hashed version of the *hack license*, which is
    what a license to drive a taxi is called), some temporal information about when
    the trip started and ended, and the longitude/latitude coordinates for where the
    passengers were picked up and dropped off.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 每个文件头之后的行代表 CSV 格式中的单个出租车行程。对于每次行程，我们有一些关于出租车的属性（车牌号的哈希版本）以及司机的信息（*hack license*
    的哈希版本，这是驾驶出租车的许可证称呼），以及有关行程开始和结束的时间信息，以及乘客上车和下车的经度/纬度坐标。
- en: 'Let’s create a *taxidata* directory and copy the trip data into the storage:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个 *taxidata* 目录，并将行程数据复制到存储中：
- en: '[PRE1]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: We have used a local filesystem here, but this may not be the case for you.
    It’s more likely nowadays to use a cloud native filesystem such as AWS S3 or GCS.
    In such a scenario, you will upload the data to S3 or GCS, respectively.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们这里使用的是本地文件系统，但你可能不是这种情况。现在更常见的是使用像 AWS S3 或 GCS 这样的云原生文件系统。在这种情况下，你需要分别将数据上传到
    S3 或 GCS。
- en: 'Now start the PySpark shell:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 现在启动 PySpark shell：
- en: '[PRE2]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Once the PySpark shell has loaded, we can create a dataset from the taxi data
    and examine the first few lines, just as we have in other chapters:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦 PySpark shell 加载完毕，我们可以从出租车数据创建一个数据集，并查看前几行，就像我们在其他章节中做的一样：
- en: '[PRE3]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This looks like a well-formatted dataset at first glance. Let’s have a look
    at the DataFrame’s schema:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 乍看之下，这看起来是一个格式良好的数据集。让我们再次查看 DataFrame 的模式：
- en: '[PRE4]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We are representing the `pickup_datetime` and `dropoff_datetime` fields as `Strings`
    and storing the individual `(x,y)` coordinates of the pickup and drop-off locations
    in their own fields as `Doubles`. We want the datetime fields as timestamps since
    that will allow us to manipulate and analyze them conveniently.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将 `pickup_datetime` 和 `dropoff_datetime` 字段表示为 `Strings`，并将乘客上车和下车位置的个别 `(x,y)`
    坐标存储在自己的 `Doubles` 字段中。我们希望将日期时间字段作为时间戳，因为这样可以方便地进行操作和分析。
- en: Converting Datetime Strings to Timestamps
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将日期时间字符串转换为时间戳
- en: As mentioned previously, PySpark provides out-of-the-box methods for handling
    temporal data.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，PySpark 提供了处理时间数据的开箱即用方法。
- en: 'Specifically, we will use the `to_timestamp` function to parse the datetime
    strings and convert them into timestamps:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，我们将使用 `to_timestamp` 函数来解析日期时间字符串并将其转换为时间戳：
- en: '[PRE5]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Let’s have a look at the schema again:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 再次查看一下模式（schema）：
- en: '[PRE6]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The `pickup_datetime` and `dropoff_datetime` fields are timestamps now. Well
    done!
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，`pickup_datetime` 和 `dropoff_datetime` 字段已经是时间戳了。做得好！
- en: 'We’d mentioned that this dataset contains trips from January 2013\. Don’t just
    take our word for this, though. We can confirm this by sorting the `pickup_datetime`
    field to get the latest datetime in the data. For this, we use DataFrame’s `sort`
    method combined with PySpark column’s `desc` method:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提到过，这个数据集包含了 2013 年 1 月的行程。不过，不要仅仅听我们的话。我们可以通过对 `pickup_datetime` 字段进行排序来确认数据中的最新日期时间。为此，我们使用
    DataFrame 的 `sort` 方法结合 PySpark 列的 `desc` 方法：
- en: '[PRE7]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: With our data types in place, let’s check if there are any inconsistencies in
    our data.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 数据类型就位后，让我们检查一下数据中是否存在任何不一致之处。
- en: Handling Invalid Records
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 处理无效记录
- en: Anyone who has been working with large-scale, real-world datasets knows that
    they invariably contain at least a few records that do not conform to the expectations
    of the person who wrote the code to handle them. Many PySpark pipelines have failed
    because of invalid records that caused the parsing logic to throw an exception.
    When performing interactive analysis, we can get a sense of potential anomalies
    in the data by focusing on key variables.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 所有在大规模、实际数据集上工作的人都知道，这些数据不可避免地会包含一些不符合编写处理代码期望的记录。许多 PySpark 管道因为无效记录导致解析逻辑抛出异常而失败。在进行交互式分析时，我们可以通过关注关键变量来感知数据中潜在的异常情况。
- en: In our case, variables containing geospatial and temporal information are worth
    looking at for inconsistencies. Presence of null values in these columns will
    definitely throw off our analysis.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的情况下，包含地理空间和时间信息的变量值得关注是否存在不一致。这些列中的空值肯定会影响我们的分析。
- en: '[PRE8]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Let’s remove the null values from our data:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从数据中删除空值：
- en: '[PRE9]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Another commonsense check that we can do is for latitude and longitude records
    where the values are zero. We know that for the region we’re concerned with, those
    would be invalid values:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以进行一个常识性检查，即检查纬度和经度记录中值为零的情况。我们知道对于我们关心的区域，这些将是无效值：
- en: '[PRE10]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[![1](assets/1.png)](#co_geospatial_and_temporal_data_analysis___span_class__keep_together__on_taxi_trip_data__span__CO1-1)'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_geospatial_and_temporal_data_analysis___span_class__keep_together__on_taxi_trip_data__span__CO1-1)'
- en: Multiple `OR` conditions will be true if either of them evaluates to `True`
    for any record.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任何记录，如果多个 `OR` 条件为真，则任何一个条件为真时都为真。
- en: We have quite a few of these. If it looks as if a taxi took a passenger to the
    South Pole, we can be reasonably confident that the record is invalid and should
    be excluded from our analysis. We will not remove them but get back to them toward
    the end of the next section to see how they can affect our analysis.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有相当多这样的记录。如果看起来一辆出租车把乘客带到了南极点，我们可以合理地认为该记录是无效的，并应该从我们的分析中排除。我们不会删除它们，而是在下一节结束时回来看看它们如何影响我们的分析。
- en: 'In production settings, we handle these exceptions one at a time by checking
    the logs for the individual tasks, figuring out which line of code threw the exception,
    and then figuring out how to tweak the code to ignore or correct the invalid records.
    This is a tedious process, and it often feels like we’re playing whack-a-mole:
    just as we get one exception fixed, we discover another one on a record that came
    later within the partition.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在生产环境中，我们逐个检查这些异常，查看各个任务的日志，找出引发异常的代码行，然后调整代码以忽略或纠正无效记录。这是一个繁琐的过程，通常感觉就像我们在打地鼠：刚解决一个异常，我们又发现了下一个异常记录。
- en: One strategy that experienced data scientists deploy when working with a new
    dataset is to add a `try-except` block to their parsing code so that any invalid
    records can be written out to the logs without causing the entire job to fail.
    If there are only a handful of invalid records in the entire dataset, we might
    be okay with ignoring them and continuing with our analysis.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据科学家在处理新数据集时，常用的一种策略是在其解析代码中添加 `try-except` 块，以便任何无效记录可以写入日志而不会导致整个作业失败。如果整个数据集中只有少数无效记录，我们可能可以忽略它们并继续分析。
- en: Now that we have prepared our dataset, let’s get started with geospatial analysis.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好我们的数据集，让我们开始地理空间分析。
- en: Geospatial Analysis
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 地理空间分析
- en: There are two major kinds of geospatial data—vector and raster—and there are
    different tools for working with each type. In our case, we have latitude and
    longitude for our taxi trip records, and vector data stored in the GeoJSON format
    that represents the boundaries of the different boroughs of New York. We’ve looked
    at the latitude and longitude points. Let’s start by having a look at the GeoJSON
    data.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种主要的地理空间数据类型——矢量和栅格——每种类型都有不同的工具用于处理。在我们的情况下，我们有出租车行程记录的纬度和经度，以及以 GeoJSON
    格式存储的矢量数据，该数据表示了纽约不同区域的边界。我们已经查看了纬度和经度点。让我们首先看看 GeoJSON 数据。
- en: Intro to GeoJSON
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GeoJSON 简介
- en: The data we’ll use for the boundaries of boroughs in New York City comes written
    in a format called *GeoJSON*. The core object in GeoJSON is called a *feature*,
    which is made up of a *geometry* instance and a set of key-value pairs called
    *properties*. A geometry is a shape like a point, line, or polygon. A set of features
    is called a `FeatureCollection`. Let’s pull down the GeoJSON data for the NYC
    borough maps and take a look at its structure.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用的纽约市各区边界的数据以*GeoJSON*格式编写。 GeoJSON中的核心对象称为*feature*，由*geometry*实例和一组称为*properties*的键值对组成。
    几何体是指点、线或多边形等形状。 一组要素称为`FeatureCollection`。 让我们下载纽约市各区地图的GeoJSON数据并查看其结构。
- en: 'In the *taxidata* directory on your client machine, download the data and rename
    the file to something a bit shorter:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在客户端机器上的*taxidata*目录中，下载数据并将文件重命名为更短的名称：
- en: '[PRE11]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Open the file and look at a feature record. Note the properties and the geometry
    objects—in this case, a polygon representing the boundaries of the borough and
    the properties containing the name of the borough and other related information.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 打开文件并查看要素记录。 注意属性和几何对象 - 在本例中，多边形表示区域的边界和包含区域名称和其他相关信息的属性。
- en: '[PRE12]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: GeoPandas
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GeoPandas
- en: The first thing you should consider when choosing a library to perform geospatial
    analysis is determine what kind of data you will need to work with. We need a
    library that can parse GeoJSON data and can handle spatial relationships, like
    detecting whether a given longitude/latitude pair is contained inside a polygon
    that represents the boundaries of a particular borough. We will use the [GeoPandas
    library](https://geopandas.org) for this task. GeoPandas is an open source project
    to make working with geospatial data in Python easier. It extends the data types
    used by the pandas library, which we used in previous chapters, to allow spatial
    operations on geometric data types.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择执行地理空间分析的库时，首先要考虑的是确定您需要处理哪种类型的数据。 我们需要一个可以解析GeoJSON数据并处理空间关系的库，例如检测给定的经度/纬度对是否包含在表示特定区域边界的多边形内。
    我们将使用[GeoPandas库](https://geopandas.org)来执行此任务。 GeoPandas是一个开源项目，旨在使Python中的地理空间数据处理更加简单。
    它扩展了pandas库使用的数据类型，我们在之前的章节中使用过，以允许在几何数据类型上进行空间操作。
- en: 'Install the GeoPandas package using pip:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 使用pip安装GeoPandas包：
- en: '[PRE13]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Let us now start examining the geospatial aspects of the taxi data. For each
    trip, we have longitude/latitude pairs representing where the passenger was picked
    up and dropped off. We would like to be able to determine which borough each of
    these longitude/latitude pairs belongs to, and identify any trips that did not
    start or end in any of the five boroughs. For example, if a taxi took passengers
    from Manhattan to Newark Liberty International Airport, that would be a valid
    ride that would be interesting to analyze, even though it would not end within
    one of the five boroughs.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们开始检查出租车数据的地理空间方面。 对于每次行程，我们都有表示乘客上车和下车位置的经度/纬度对。 我们希望能够确定这些经度/纬度对中的每一个属于哪个区，并确定任何未在五个区域中的经纬度对。
    例如，如果一辆出租车将乘客从曼哈顿送到纽瓦克自由国际机场，那将是一次有效的行程，我们很有兴趣分析，即使它并没有在五个区域之一结束。
- en: 'To perform our borough analysis, we need to load the GeoJSON data we downloaded
    earlier and stored in the *nyc-boroughs.geojson* file:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行我们的区域分析，我们需要加载我们之前下载并存储在*nyc-boroughs.geojson*文件中的GeoJSON数据：
- en: '[PRE14]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Before we use the GeoJSON features on the taxi trip data, we should take a moment
    to think about how to organize this geospatial data for maximum efficiency. One
    option would be to research data structures that are optimized for geospatial
    lookups, such as quad trees, and then find or write our own implementation. Instead,
    we will try to come up with a quick heuristic that will allow us to bypass that
    bit of work.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用GeoJSON功能在出租车行程数据上之前，我们应该花点时间考虑如何为最大效率组织这些地理空间数据。 一个选择是研究针对地理空间查找进行优化的数据结构，例如四叉树，然后找到或编写我们自己的实现。
    相反，我们将尝试提出一个快速的启发式方法，使我们能够跳过那一部分工作。
- en: We will iterate through the `gdf` until we find a feature whose geometry contains
    a given `point` of longitude/latitude. Most taxi rides in NYC begin and end in
    Manhattan, so if the geospatial features that represent Manhattan are earlier
    in the sequence, our searches will end relatively quickly. We can use the fact
    that the `boroughCode` property of each feature can be used as a sorting key,
    with the code for Manhattan equal to 1 and the code for Staten Island equal to
    5\. Within the features for each borough, we want the features associated with
    the largest polygons to come before those associated with the smaller polygons,
    because most trips will be to and from the “major” region of each borough.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过 `gdf` 迭代，直到找到一个几何图形包含给定经度/纬度点的要素为止。大多数纽约市的出租车行程始发和结束在曼哈顿，因此如果表示曼哈顿的地理空间要素在序列中较早出现，我们的搜索将相对较快结束。我们可以利用每个要素的
    `boroughCode` 属性作为排序键，曼哈顿的代码为1，斯塔滕岛的代码为5。在每个区的要素中，我们希望与较大多边形相关联的要素优先于与较小多边形相关联的要素，因为大多数行程将在每个区的“主要”区域内进行。
- en: 'We will calculate area associated with each feature’s geometry and store it
    as a new column:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将计算与每个要素几何图形相关联的面积，并将其存储为新列：
- en: '[PRE15]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Sorting the features by the combination of the borough code and the area of
    each feature’s geometry should do the trick:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 按照区号和每个要素几何图形的面积的组合进行排序应该可以解决问题：
- en: '[PRE16]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Note that we’re sorting based on area value in descending order because we want
    the largest polygons to come first, and `sort_values` sorts in ascending order
    by default.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们基于面积值的降序排序，因为我们希望最大的多边形首先出现，而`sort_values`默认按升序排序。
- en: 'Now we can broadcast the sorted features in the `gdf` GeoPandas DataFrame to
    the cluster and write a function that uses these features to find out in which
    of the five boroughs (if any) a particular trip ended:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以将 `gdf` GeoPandas DataFrame 中排序后的要素广播到集群，并编写一个函数，该函数使用这些要素来找出特定行程结束在哪个（如果有的话）五个区中的哪一个：
- en: '[PRE17]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We can apply `find_borough` to the trips in the `taxi_raw` DataFrame to create
    a histogram of trips by borough:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将 `find_borough` 应用于 `taxi_raw` DataFrame 中的行程，以创建一个按区划分的直方图：
- en: '[PRE18]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: As we expected, the vast majority of trips end in the borough of Manhattan,
    while relatively few trips end in Staten Island. One surprising observation is
    the number of trips that end outside of any borough; the number of `null` records
    is substantially larger than the number of taxi rides that end in the Bronx.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们预期的那样，绝大多数行程都在曼哈顿区结束，而在斯塔滕岛结束的行程相对较少。一个令人惊讶的观察是，结束在任何区域之外的行程的数量；`null`记录的数量远远超过结束在布朗克斯的出租车行程的数量。
- en: We had talked about handling such invalid records earlier but did not remove
    them. It is left as an exercise for you to remove such records and create a histogram
    from the cleaned-up data. Once done, you will notice a reduction in the number
    of `null` entries, leaving a much more reasonable number of observations that
    had drop-offs outside the city.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前谈论过如何处理这些无效记录，但没有将它们删除。现在留给你的练习是删除这些记录并从清理后的数据中创建直方图。一旦完成，您将注意到`null`条目的数量减少，剩下的观察数量更加合理，这些观察是在城市外部结束的行程。
- en: Having worked with the geospatial aspects of our data, let us now dig deeper
    into the temporal nature of our data by performing sessionization using PySpark.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理了数据的地理空间方面后，现在让我们通过使用 PySpark 执行会话化来深入了解我们数据的时间特性。
- en: Sessionization in PySpark
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PySpark 中的会话化
- en: The kind of analysis, in which we want to analyze a single entity as it executes
    a series of events over time, is called *sessionization*, and is commonly performed
    over web logs to analyze the behavior of the users of a website. PySpark provides
    `Window` and aggregation functions out of the box that can be used to perform
    such analysis. These allow us to focus on business logic instead of trying to
    implement complex data manipulation and calculation. We will use these in the
    next section to better understand utilization of taxi cabs in our dataset.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种分析中，我们希望分析单个实体随着时间执行一系列事件的情况，称为*会话化*，通常在网站的日志中执行，以分析用户的行为。 PySpark 提供了窗口和聚合函数，可以直接用于执行此类分析。这些函数允许我们专注于业务逻辑，而不是试图实现复杂的数据操作和计算。我们将在下一节中使用它们来更好地理解数据集中出租车的使用情况。
- en: Sessionization can be a very powerful technique for uncovering insights in data
    and building new data products that can be used to help people make better decisions.
    For example, Google’s spell-correction engine is built on top of the sessions
    of user activity that Google builds each day from the logged records of every
    event (searches, clicks, maps visits, etc.) occurring on its web properties. To
    identify likely spell-correction candidates, Google processes those sessions looking
    for situations where a user typed a query, didn’t click anything, typed a slightly
    different query a few seconds later, and then clicked a result and didn’t come
    back to Google. Then it counts how often this pattern occurs for any pair of queries.
    If it occurs frequently enough (e.g., if every time we see the query “untied stats,”
    it’s followed a few seconds later by the query “united states”), then we assume
    that the second query is a spell correction of the first.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 会话化可以是发现数据见解和构建新数据产品的强大技术。例如，Google 的拼写校正引擎是建立在每天从其网站属性上发生的每个事件（搜索、点击、地图访问等）的记录构成的会话之上的。为了识别可能的拼写校正候选项，Google
    处理这些会话，寻找用户输入查询但没有点击任何内容，然后几秒钟后输入稍微不同的查询，然后点击结果并且不再返回 Google 的情况。然后，它统计这种模式对于任何一对查询发生的频率。如果发生频率足够高（例如，如果我们每次看到查询“解开的统计数据”，它后面都跟着几秒钟后的查询“美国”），那么我们就假定第二个查询是第一个查询的拼写校正。
- en: This analysis takes advantage of the patterns of human behavior that are represented
    in the event logs to build a spell-correction engine from data that is more powerful
    than any engine that could be created from a dictionary. The engine can be used
    to perform spell correction in any language and can correct words that might not
    be included in any dictionary (e.g., the name of a new startup) or queries like
    “untied stats” where none of the words are misspelled! Google uses similar techniques
    to show recommended and related searches, as well as to decide which queries should
    return a OneBox result that gives the answer to a query on the search page itself,
    without requiring that the user click through to a different page. There are OneBoxes
    for weather, scores from sporting events, addresses, and lots of other kinds of
    queries.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 此分析利用了事件日志中所表示的人类行为模式，从而构建了一个比可以从字典中创建的任何引擎更强大的数据拼写校正引擎。该引擎可用于执行任何语言的拼写校正，并且可以纠正可能不包含在任何字典中的单词（例如，新创业公司的名称）或查询，如“解开的统计数据”，其中没有任何单词拼写错误！Google
    使用类似的技术显示推荐和相关搜索，以及决定哪些查询应返回 OneBox 结果，该结果在搜索页面本身上给出查询的答案，而不需要用户点击转到不同页面。有关天气、体育比分、地址和许多其他类型查询的
    OneBoxes。
- en: So far, information about the set of events that occurs to each entity is spread
    out across the DataFrame’s partitions, so, for analysis, we need to place these
    relevant events next to each other and in chronological order. In the next section,
    we will show how to efficiently construct and analyze sessions using advanced
    PySpark functionality.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，每个实体发生的事件集合的信息分布在 DataFrame 的分区中，因此，为了分析，我们需要将这些相关事件放在一起，并按时间顺序排列。在下一节中，我们将展示如何使用高级
    PySpark 功能有效地构建和分析会话。
- en: 'Building Sessions: Secondary Sorts in PySpark'
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建会话：PySpark 中的次要排序
- en: The naive way to create sessions in PySpark is to perform a `groupBy` on the
    identifier we want to create sessions for and then sort the events post-shuffle
    by a timestamp identifier. If we only have a small number of events for each entity,
    this approach will work reasonably well. However, because this approach requires
    all the events for any particular entity to be in memory at the same time, it
    will not scale as the number of events for each entity gets larger and larger.
    We need a way of building sessions that does not require all of the events for
    a particular entity to be held in memory at the same time for sorting.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在 PySpark 中创建会话的天真方式是对我们想要为其创建会话的标识符执行 `groupBy`，然后在洗牌后按时间戳标识符对事件进行排序。如果每个实体只有少量事件，这种方法会工作得相当不错。然而，由于这种方法需要将任何特定实体的所有事件同时保存在内存中进行排序，随着每个实体的事件数量越来越多，它将不会扩展。我们需要一种构建会话的方法，不需要将某个特定实体的所有事件同时保存在内存中进行排序。
- en: 'In MapReduce, we can build sessions by performing a *secondary sort*, where
    we create a composite key made up of an identifier and a timestamp value, sort
    all of the records on the composite key, and then use a custom partitioner and
    grouping function to ensure that all of the records for the same identifier appear
    in the same output partition. Fortunately, PySpark can also support a similar
    pattern by using `Window` functions:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在 MapReduce 中，我们可以通过执行*二次排序*来构建会话，其中我们创建一个由标识符和时间戳值组成的复合键，对所有记录按照复合键排序，然后使用自定义分区器和分组函数确保相同标识符的所有记录出现在同一个输出分区中。幸运的是，PySpark
    也可以通过使用`Window`函数来支持类似的模式：
- en: '[PRE19]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'First, we use the `partitionBy` method to ensure that all of the records that
    have the same value for the `license` column end up in the same partition. Then,
    within each of these partitions, we sort the records by their `license` value
    (so all trips by the same driver appear together) and then by their `pickupTime`
    so that the sequence of trips appears in sorted order within the partition. Now
    when we aggregate the trip records, we can be sure that the trips are ordered
    in a way that is optimal for sessions analysis. Because this operation triggers
    a shuffle and a fair bit of computation and we’ll need to use the results more
    than once, we cache them:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们使用`partitionBy`方法确保所有具有相同`license`列值的记录最终进入同一个分区。然后，在每个分区内部，我们按照它们的`license`值（以便同一司机的所有行程出现在一起）和`pickupTime`排序这些记录，以便在分区内按排序顺序显示行程序列。现在，当我们聚合行程记录时，我们可以确保行程按照会话分析最佳顺序排序。因为这个操作会触发重分区和相当多的计算，并且我们需要多次使用结果，所以我们将其缓存：
- en: '[PRE20]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Executing a sessionization pipeline is an expensive operation, and the sessionized
    data is often useful for many different analysis tasks that we might want to perform.
    In settings where one might want to pick up on the analysis later or collaborate
    with other data scientists, it is a good idea to amortize the cost of sessionizing
    a large dataset by only performing the sessionization once and then writing the
    sessionized data to a filesystem such as S3 or HDFS so that it can be used to
    answer lots of different questions. Performing sessionization once is also a good
    way to enforce standard rules for session definitions across the entire data science
    team, which has the same benefits for ensuring apples-to-apples comparisons of
    results.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 执行会话化管道是一个昂贵的操作，会话化数据通常对我们可能要执行的许多不同分析任务都很有用。在可能需要稍后进行分析或与其他数据科学家合作的设置中，通过仅执行一次会话化大型数据集并将会话化数据写入诸如
    S3 或 HDFS 的文件系统中，可以分摊会话化成本，以便用于回答许多不同的问题。只执行一次会话化还是确保整个数据科学团队会话定义标准规则的好方法，这对于确保结果的苹果与苹果之间的比较具有相同的好处。
- en: 'At this point, we are ready to analyze our sessions data to see how long it
    takes for a driver to find his next fare after a drop-off in a particular borough.
    We will use the `lag` function along with the `window_spec` object created earlier
    to take two trips and compute the duration in seconds between the drop-off time
    of the first trip and the pickup time of the second:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，我们已经准备好分析我们的会话数据，以查看司机在特定区域下车后找到下一个行程所需的时间。我们将使用`lag`函数以及之前创建的`window_spec`对象来获取两次行程之间的持续时间（以秒为单位）：
- en: '[PRE21]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Now, we should do a validation check to ensure that most of the durations are
    nonnegative:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们应该进行验证检查，确保大部分持续时间是非负的：
- en: '[PRE22]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Only a few of the records have a negative duration, and when we examine them
    more closely, there don’t seem to be any common patterns to them that we could
    use to understand the source of the erroneous data. If we exclude these negative
    duration records from our input dataset and look at the average and standard deviation
    of the pickup times by borough, we see this:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 只有少数记录持续时间为负，当我们更仔细地检查它们时，似乎没有任何共同的模式可以帮助我们理解错误数据的来源。如果我们从输入数据集中排除这些负持续时间记录，并查看按区域分组的接驳时间的平均值和标准偏差，我们会看到这样的结果：
- en: '[PRE23]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: As we would expect, the data shows that drop-offs in Manhattan have the shortest
    amount of downtime for drivers, at around 10 minutes. Taxi rides that end in Brooklyn
    have a downtime of more than twice that, and the relatively few rides that end
    in Staten Island take drivers an average of almost 45 minutes to get to their
    next fare.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所预期的那样，数据显示，在曼哈顿的乘车等待时间最短，大约为10分钟。在布鲁克林结束的出租车行程的等待时间是这个的两倍多，而在史泰登岛结束的相对较少的行程则需要司机平均将近45分钟才能找到下一个乘客。
- en: As the data demonstrates, taxi drivers have a major financial incentive to discriminate
    among passengers based on their final destination; drop-offs in Staten Island,
    in particular, involve an extensive amount of downtime for a driver. The NYC Taxi
    and Limousine Commission has made a major effort over the years to identify this
    discrimination and has fined drivers who have been caught rejecting passengers
    because of where they wanted to go. It would be interesting to attempt to examine
    the data for unusually short taxi rides that could be indicative of a dispute
    between the driver and the passenger about where the passenger wanted to be dropped
    off.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 正如数据所表明的，出租车司机有很大的经济激励来根据乘客的最终目的地进行区别对待；尤其是在史泰登岛的乘客下车后，司机需要花费大量时间等待下一个订单。多年来，纽约市出租车和豪华轿车委员会一直在努力识别这种歧视，并对因乘客目的地而拒载的司机进行罚款。有趣的是，试图检验数据中异常短的出租车行程，可能是司机和乘客关于乘客想下车的位置存在争执的迹象。
- en: Where to Go from Here
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 下一步该何去何从
- en: In this chapter, we worked with both temporal and spatial features of a real-world
    dataset. The familiarity with geospatial analysis that you have gained so far
    can be used to dive into frameworks such as Apache Sedona or GeoMesa. They will
    have a steeper learning curve compared to working with GeoPandas and UDFs but
    will be more efficient. There’s also a lot of scope for using data visualization
    with geospatial and temporal data.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们处理了一个真实世界数据集的时间和空间特征。到目前为止，你已经获得了地理空间分析的熟练度，可以用于深入研究如Apache Sedona或GeoMesa等框架。与使用GeoPandas和UDFs相比，它们的学习曲线会更陡峭，但效率更高。在使用地理空间和时间数据进行数据可视化方面，还有很大的发展空间。
- en: 'Further, imagine using this same technique on the taxi data to build an application
    that could recommend the best place for a cab to go after a drop-off based on
    current traffic patterns and the historical record of next-best locations contained
    within this data. You could also look at the information from the perspective
    of someone trying to catch a cab: given the current time, place, and weather data,
    what is the probability that I will be able to hail a cab from the street within
    the next five minutes? This sort of information could be incorporated into applications
    like Google Maps to help travelers decide when to leave and which travel option
    they should take.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，想象一下，使用相同的技术来分析出租车数据，构建一个能够根据当前交通模式和包含在这些数据中的历史最佳位置记录，推荐出租车在落客后去的最佳地点的应用程序。你还可以从需要打车的人的角度来看待这些信息：在当前时间、地点和天气数据下，我能在接下来的五分钟内从街上拦到一辆出租车的概率是多少？这种信息可以整合到诸如Google地图之类的应用程序中，帮助旅行者决定何时出发以及选择哪种出行方式。
