<html><head></head><body><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 6. Handling Missing Data"><div class="chapter" id="handling_missing_data-id00044">
<h1><span class="label">Chapter 6. </span>Handling Missing Data</h1>
<p>Missing data is a common occurrence in data analysis. In the age of big data,<a contenteditable="false" data-type="indexterm" data-primary="data" data-secondary="missing data" data-see="missing data" id="idm45968170536344"/><a contenteditable="false" data-type="indexterm" data-primary="missing data" data-secondary="about" id="idm45968170534696"/><a contenteditable="false" data-type="indexterm" data-primary="AirCnC (Air Coach and Couch)" data-secondary="missing data" data-seealso="missing data" id="idm45968170533320"/><a contenteditable="false" data-type="indexterm" data-primary="variables" data-secondary="missing data" data-see="missing data" id="idm45968170531704"/> many authors and even more practitioners treat it as a minor annoyance that is given scant thought: just filter out the rows with missing data—if you go from 12 million rows to 11 million, what’s the big deal? That still leaves you with plenty of data to run your analyses.</p>
<p>Unfortunately, filtering out the rows with missing data <a contenteditable="false" data-type="indexterm" data-primary="biases in data and analyses" data-secondary="missing data handling introducing" id="idm45968170529224"/><a contenteditable="false" data-type="indexterm" data-primary="missing data" data-secondary="biases in data" id="idm45968170527784"/>can introduce significant biases in your analysis. Let’s say that older customers are more likely to have missing data, for example because they are less likely to set up automated payments; by filtering these customers out you would bias your analysis toward younger customers, who would be overrepresented in your filtered data. Other common methods to handle missing data, such as replacing them by the average value for that variable, also introduce biases of their own.</p>
<p>Statisticians and methodologists have developed methods that have much smaller or even no bias. These methods have not been adopted broadly by practitioners yet, but hopefully this chapter will help you get ahead of the curve!</p>
<p>The theory of missing values is rooted in statistics and can easily get very mathematical. <a contenteditable="false" data-type="indexterm" data-primary="missing data" data-secondary="about example business problem" id="idm45968170524600"/>To make our journey in this chapter more concrete, we’ll work through a simulated data set for AirCnC. The business context is that the marketing department, in an effort to better understand customer characteristics and motivations, has sent out by email a survey to a sample of 2,000 customers in three states and collected the following information:</p>
<ul class="pagebreak-before less_space">
<li><p>Demographic characteristics</p>
<ul>
<li><p>Age</p></li>
<li><p>Gender</p></li>
<li><p>State (selecting customers from only three states, which for convenience we’ll call A, B, and C)</p></li>
</ul></li>
<li><p>Personality traits</p>
<ul>
<li><p>Openness</p></li>
<li><p>Extraversion</p></li>
<li><p>Neuroticism</p></li>
</ul></li>
<li><p>Booking amount</p></li>
</ul>
<p>To keep things simpler, we’ll assume that the <a contenteditable="false" data-type="indexterm" data-primary="demographic variables" data-secondary="primary causes" id="idm45968170515432"/><a contenteditable="false" data-type="indexterm" data-primary="personal characteristics in human behavior" data-secondary="primary causes" id="idm45968170514056"/><a contenteditable="false" data-type="indexterm" data-primary="causes" data-secondary="primary causes" data-tertiary="demographics as" id="idm45968170512584"/><a contenteditable="false" data-type="indexterm" data-primary="primary causes" data-secondary="demographics as" id="idm45968170510936"/>demographic variables are all causes of booking amount and unrelated to each other (<a data-type="xref" href="#the_demographic_variables_cause_booking">Figure 6-1</a>).</p>
<figure><div id="the_demographic_variables_cause_booking" class="figure">
<img src="Images/BEDA_0601.png" alt="The demographic variables cause booking amount" width="1203" height="365"/>
<h6><span class="label">Figure 6-1. </span>The demographic variables cause booking amount</h6>
</div></figure>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>As we discussed in <a data-type="xref" href="ch02.xhtml#understanding_behavioral_data">Chapter 2</a>, when I say that demographic variables such as <em>Gender</em> and <em>Extraversion</em> are causes of <em>BookingAmount</em>, I mean two things: first that they are <a contenteditable="false" data-type="indexterm" data-primary="primary causes" data-secondary="exogenous variables as" id="idm45968170502712"/><a contenteditable="false" data-type="indexterm" data-primary="exogenous variables" id="idm45968170501336"/><a contenteditable="false" data-type="indexterm" data-primary="variables" data-secondary="exogenous variables" id="idm45968170500232"/><a contenteditable="false" data-type="indexterm" data-primary="causes" data-secondary="primary causes" data-tertiary="exogenous variables as" id="idm45968170498856"/>exogenous variables (i.e., variables that are primary causes for our purposes), and second that they are predictors of <em>BookingAmount</em> because of causal effects that are heavily mediated as well as moderated by social <span class="keep-together">phenomena.</span></p>
<p>For instance, the effect of <em>Gender</em> is probably mediated by the person’s income, occupation, and family status, among many others. In that sense, it would be more accurate to say that <em>Gender</em> is a cause of causes of <em>BookingAmount</em>. However, it is important to note that this effect is <em>not</em> confounded, and as such it is truly causal.</p>
</div>
<p>The flow of this chapter will follow the steps you would take when facing a new data set: first, we’ll visualize our missing data, to get a rough sense of what’s going on. Then, we’ll learn how to diagnose missing data and see the classification developed by the statistician Donald Rubin, which is the reference in the matter. The last three sections will show how to handle each one of the categories in that classification.</p>
<p>Unfortunately for Python users, the excellent R packages that we’ll be using don’t have direct Python counterparts. I’ll do my best to show you alternatives and workarounds in Python, but the code will be significantly longer and less elegant. Sorry!</p>
<section data-type="sect1" data-pdf-bookmark="Data and Packages"><div class="sect1" id="data_and_libraries">
<h1>Data and Packages</h1>
<p>One of the luxuries of using simulated data is that we know the true values for the missing data. <a contenteditable="false" data-type="indexterm" data-primary="GitHub" data-secondary="missing data simulated data" id="idm45968170489928"/><a contenteditable="false" data-type="indexterm" data-primary="missing data" data-secondary="data and packages for example" id="idm45968170488488"/>The <a href="https://oreil.ly/BehavioralDataAnalysisCh6">GitHub folder for this chapter</a> contains three data sets (<a data-type="xref" href="#variables_in_our_dat">Table 6-1</a>):</p>
<ul>
<li><p>The complete data for our four variables</p></li>
<li><p>The “available” data where some values are missing for some of the variables</p></li>
<li><p>A secondary data set of auxiliary variables that we’ll use to complement our <span class="keep-together">analysis</span></p></li>
</ul>
<table class="border" id="variables_in_our_dat">
<caption><span class="label">Table 6-1. </span>Variables in our data</caption>
<thead>
<tr>
<th/>
<th>Variable description</th>
<th>chap6-complete_data.csv</th>
<th>chap6-available_data.csv</th>
<th>chap6-available_data_supp.csv</th>
</tr>
</thead>
<tbody>
<tr>
<td><em>Age</em></td>
<td>Age of customer</td>
<td>Complete</td>
<td>Complete</td>
<td/>
</tr>
<tr>
<td><em>Open</em></td>
<td>Openness psychological trait, 0-10</td>
<td>Complete</td>
<td>Complete</td>
<td/>
</tr>
<tr>
<td><em>Extra</em></td>
<td>Extraversion psychological trait, 0-10</td>
<td>Complete</td>
<td>Partial</td>
<td/>
</tr>
<tr>
<td><em>Neuro</em></td>
<td>Neuroticism psychological trait, 0-10</td>
<td>Complete</td>
<td>Partial</td>
<td/>
</tr>
<tr>
<td><em>Gender</em></td>
<td>Categorical variable for customer gender, F/M</td>
<td>Complete</td>
<td>Complete</td>
<td/>
</tr>
<tr>
<td><em>State</em></td>
<td>Categorical variable for customer state of residence, A/B/C</td>
<td>Complete</td>
<td>Partial</td>
<td/>
</tr>
<tr>
<td><em>Bkg_amt</em></td>
<td>Amount booked by customer</td>
<td>Complete</td>
<td>Partial</td>
<td/>
</tr>
<tr>
<td><em>Insurance</em></td>
<td>Amount of travel insurance purchased by customer</td>
<td/>
<td/>
<td>Complete</td>
</tr>
<tr>
<td><em>Active</em></td>
<td>Numeric measure of how active the customer bookings are</td>
<td/>
<td/>
<td>Complete</td>
</tr>
</tbody>
</table>
<p>In this chapter, we’ll use the following packages in addition to the common ones:<a contenteditable="false" data-type="indexterm" data-primary="packages" data-secondary="multiple imputation" id="idm45968170452360"/><a contenteditable="false" data-type="indexterm" data-primary="packages" data-secondary="md.pattern() visualization function" id="idm45968170450984"/><a contenteditable="false" data-type="indexterm" data-primary="packages" data-secondary="logistic() function" id="idm45968170449592"/><a contenteditable="false" data-type="indexterm" data-primary="packages" data-secondary="melt() function" id="idm45968170448216"/><a contenteditable="false" data-type="indexterm" data-primary="md.pattern() visualization function" id="idm45968170446840"/><a contenteditable="false" data-type="indexterm" data-primary="R" data-secondary="missing data visualization function" id="idm45968170445720"/></p>
<pre data-type="programlisting" data-code-language="r"><code class="c1">## R</code>
<code class="nf">library</code><code class="p">(</code><code class="n">mice</code><code class="p">)</code> <code class="c1"># For multiple imputation</code>
<code class="nf">library</code><code class="p">(</code><code class="n">reshape</code><code class="p">)</code> <code class="c1">#For function melt()</code>
<code class="nf">library</code><code class="p">(</code><code class="n">psych</code><code class="p">)</code> <code class="c1">#For function logistic()</code>
</pre>
<pre data-type="programlisting" data-code-language="python"><code class="c1">## Python</code>
<code class="kn">from</code> <code class="nn">statsmodels.imputation</code> <code class="kn">import</code> <code class="n">mice</code> <code class="c1"># For multiple imputation</code>
<code class="kn">import</code> <code class="nn">statsmodels.api</code> <code class="kn">as</code> <code class="nn">sm</code> <code class="c1"># For OLS call in Mice</code></pre>
</div></section>
<section data-type="sect1" data-pdf-bookmark="Visualizing Missing Data"><div class="sect1" id="visualizing_missing_data">
<h1>Visualizing Missing Data</h1>
<p>By definition, missing data is hard to visualize. Univariate methods<a contenteditable="false" data-type="indexterm" data-primary="visualizing missing data" id="ch06-msdat2"/><a contenteditable="false" data-type="indexterm" data-primary="missing data" data-secondary="visualization of missing data" id="ch06-msdat3"/><a contenteditable="false" data-type="indexterm" data-primary="bivariate graphs to visualize missing data" id="idm45968170410456"/> (i.e., one variable at a time) will only get us so far, so we’ll mostly rely on bivariate methods, plotting two variables against each other to tease out some insights. Used in conjunction with causal diagrams, bivariate graphs will allow us to visualize relationships that would otherwise be very complex to grasp.</p>
<p>Our first step is to get a sense of “how” our data is missing. <a contenteditable="false" data-type="indexterm" data-primary="R" data-secondary="missing data visualization function" id="idm45968170408440"/><a contenteditable="false" data-type="indexterm" data-primary="md.pattern() visualization function" id="idm45968170407048"/><a contenteditable="false" data-type="indexterm" data-primary="packages" data-secondary="md.pattern() visualization function" id="idm45968170405928"/><a contenteditable="false" data-type="indexterm" data-primary="R" data-secondary="mice package" id="idm45968170382792"/><a contenteditable="false" data-type="indexterm" data-primary="mice package in R" id="idm45968170381416"/><a contenteditable="false" data-type="indexterm" data-primary="missing data" data-secondary="visualization of missing data" data-tertiary="R" id="idm45968170380312"/>The mice package in R has a very convenient function <code>md.pattern()</code> to visualize missing data:</p>
<pre data-type="programlisting" data-code-language="r"><code class="c1">## R</code>
<code class="o">&gt;</code> <code class="nf">md.pattern</code><code class="p">(</code><code class="n">available_data</code><code class="p">)</code>
    <code class="n">age</code> <code class="n">open</code> <code class="n">gender</code> <code class="n">bkg_amt</code> <code class="n">state</code> <code class="n">extra</code> <code class="n">neuro</code>     
<code class="m">368</code>   <code class="m">1</code>    <code class="m">1</code>      <code class="m">1</code>       <code class="m">1</code>     <code class="m">1</code>     <code class="m">1</code>     <code class="m">1</code>    <code class="m">0</code>
<code class="m">358</code>   <code class="m">1</code>    <code class="m">1</code>      <code class="m">1</code>       <code class="m">1</code>     <code class="m">1</code>     <code class="m">1</code>     <code class="m">0</code>    <code class="m">1</code>
<code class="m">249</code>   <code class="m">1</code>    <code class="m">1</code>      <code class="m">1</code>       <code class="m">1</code>     <code class="m">1</code>     <code class="m">0</code>     <code class="m">1</code>    <code class="m">1</code>
<code class="m">228</code>   <code class="m">1</code>    <code class="m">1</code>      <code class="m">1</code>       <code class="m">1</code>     <code class="m">1</code>     <code class="m">0</code>     <code class="m">0</code>    <code class="m">2</code>
<code class="m">163</code>   <code class="m">1</code>    <code class="m">1</code>      <code class="m">1</code>       <code class="m">1</code>     <code class="m">0</code>     <code class="m">1</code>     <code class="m">1</code>    <code class="m">1</code>
<code class="m">214</code>   <code class="m">1</code>    <code class="m">1</code>      <code class="m">1</code>       <code class="m">1</code>     <code class="m">0</code>     <code class="m">1</code>     <code class="m">0</code>    <code class="m">2</code>
<code class="m">125</code>   <code class="m">1</code>    <code class="m">1</code>      <code class="m">1</code>       <code class="m">1</code>     <code class="m">0</code>     <code class="m">0</code>     <code class="m">1</code>    <code class="m">2</code>
<code class="m">120</code>   <code class="m">1</code>    <code class="m">1</code>      <code class="m">1</code>       <code class="m">1</code>     <code class="m">0</code>     <code class="m">0</code>     <code class="m">0</code>    <code class="m">3</code>
<code class="m">33</code>    <code class="m">1</code>    <code class="m">1</code>      <code class="m">1</code>       <code class="m">0</code>     <code class="m">1</code>     <code class="m">1</code>     <code class="m">1</code>    <code class="m">1</code>
<code class="m">23</code>    <code class="m">1</code>    <code class="m">1</code>      <code class="m">1</code>       <code class="m">0</code>     <code class="m">1</code>     <code class="m">1</code>     <code class="m">0</code>    <code class="m">2</code>
<code class="m">15</code>    <code class="m">1</code>    <code class="m">1</code>      <code class="m">1</code>       <code class="m">0</code>     <code class="m">1</code>     <code class="m">0</code>     <code class="m">1</code>    <code class="m">2</code>
<code class="m">15</code>    <code class="m">1</code>    <code class="m">1</code>      <code class="m">1</code>       <code class="m">0</code>     <code class="m">1</code>     <code class="m">0</code>     <code class="m">0</code>    <code class="m">3</code>
<code class="m">24</code>    <code class="m">1</code>    <code class="m">1</code>      <code class="m">1</code>       <code class="m">0</code>     <code class="m">0</code>     <code class="m">1</code>     <code class="m">1</code>    <code class="m">2</code>
<code class="m">24</code>    <code class="m">1</code>    <code class="m">1</code>      <code class="m">1</code>       <code class="m">0</code>     <code class="m">0</code>     <code class="m">1</code>     <code class="m">0</code>    <code class="m">3</code>
<code class="m">23</code>    <code class="m">1</code>    <code class="m">1</code>      <code class="m">1</code>       <code class="m">0</code>     <code class="m">0</code>     <code class="m">0</code>     <code class="m">1</code>    <code class="m">3</code>
<code class="m">18</code>    <code class="m">1</code>    <code class="m">1</code>      <code class="m">1</code>       <code class="m">0</code>     <code class="m">0</code>     <code class="m">0</code>     <code class="m">0</code>    <code class="m">4</code>
      <code class="m">0</code>    <code class="m">0</code>      <code class="m">0</code>     <code class="m">175</code>   <code class="m">711</code>   <code class="m">793</code>  <code class="m">1000</code> <code class="m">2679</code></pre>
<p>The <code>md.pattern()</code> function returns a table where each row represents a pattern of data availability. The first row has a “1” for each variable, so it represents complete records. The number on the left of the table indicates the number of rows with that pattern, and the number on the right indicates the number of fields that are missing in that pattern. We have 368 complete rows in our data. The second row has a “0” for <em>Neuroticism</em> only, so it represents records where only <em>Neuroticism</em> is missing; we have 358 such rows. The numbers at the bottom of the table indicate the number of missing values for the corresponding variables, and the variables are ordered by increasing number of missing values. <em>Neuroticism</em> is the last variable to the right, which means it has the highest number of missing values, 1,000. This function also conveniently returns a visual representation of the table (<a data-type="xref" href="#patterns_of_missing_data">Figure 6-2</a>).</p>
<figure><div id="patterns_of_missing_data" class="figure">
<img src="Images/BEDA_0602.png" alt="Patterns of missing data" width="1024" height="2047"/>
<h6><span class="label">Figure 6-2. </span>Patterns of missing data</h6>
</div></figure>
<p class="pagebreak-before less_space">As we can see in <a data-type="xref" href="#patterns_of_missing_data">Figure 6-2</a>, the variables <em>Age</em>, <em>Openness,</em> and <em>Gender</em> don’t have any missing data, but all the other variables do. We can obtain the same results in Python with an ad hoc function I wrote, although in a less readable format:<a contenteditable="false" data-type="indexterm" data-primary="missing data" data-secondary="visualization of missing data" data-tertiary="Python" id="idm45968170173384"/><a contenteditable="false" data-type="indexterm" data-primary="Python" data-secondary="missing data visualization" id="idm45968170171672"/></p>
<pre data-type="programlisting" data-code-language="python"><code class="c1">## Python </code>
<code class="k">def</code> <code class="nf">md_pattern_fun</code><code class="p">(</code><code class="n">dat_df</code><code class="p">):</code>
    <code class="c1"># Getting all column names</code>
    <code class="n">all_cols</code> <code class="o">=</code> <code class="n">dat_df</code><code class="o">.</code><code class="n">columns</code><code class="o">.</code><code class="n">tolist</code><code class="p">()</code>
    <code class="c1"># Getting the names of columns with some missing values</code>
    <code class="n">miss_cols</code> <code class="o">=</code> <code class="p">[</code><code class="n">col</code> <code class="k">for</code> <code class="n">col</code> <code class="ow">in</code> <code class="n">all_cols</code> <code class="k">if</code> <code class="n">dat_df</code><code class="p">[</code><code class="n">col</code><code class="p">]</code><code class="o">.</code><code class="n">isnull</code><code class="p">()</code><code class="o">.</code><code class="n">sum</code><code class="p">()]</code>
    <code class="k">if</code> <code class="n">miss_cols</code> <code class="o">==</code> <code class="n">all_cols</code><code class="p">:</code> <code class="n">dat_df</code><code class="p">[</code><code class="s1">'index'</code><code class="p">]</code> <code class="o">=</code> <code class="n">dat_df</code><code class="o">.</code><code class="n">index</code>
    <code class="c1"># Removing columns with no missing values</code>
    <code class="n">dat_df</code> <code class="o">=</code> <code class="n">dat_df</code><code class="o">.</code><code class="n">loc</code><code class="p">[:,</code><code class="n">miss_cols</code><code class="p">]</code>
    <code class="c1">#Showing total number of missing values per variable</code>
    <code class="k">print</code><code class="p">(</code><code class="n">dat_df</code><code class="o">.</code><code class="n">isnull</code><code class="p">()</code><code class="o">.</code><code class="n">sum</code><code class="p">())</code> 
    <code class="c1"># Adding count value</code>
    <code class="n">dat_df</code><code class="p">[</code><code class="s1">'count'</code><code class="p">]</code> <code class="o">=</code> <code class="mi">1</code>
    <code class="c1"># Showing count for missingness combinations</code>
    <code class="k">print</code><code class="p">(</code><code class="n">dat_df</code><code class="o">.</code><code class="n">isnull</code><code class="p">()</code><code class="o">.</code><code class="n">groupby</code><code class="p">(</code><code class="n">miss_cols</code><code class="p">)</code><code class="o">.</code><code class="n">count</code><code class="p">())</code>
<code class="n">md_pattern_fun</code><code class="p">(</code><code class="n">available_data_df</code><code class="p">)</code>

<code class="n">extra</code>       <code class="mi">793</code>
<code class="n">neuro</code>      <code class="mi">1000</code>
<code class="n">state</code>       <code class="mi">711</code>
<code class="n">bkg_amt</code>     <code class="mi">175</code>
<code class="n">dtype</code><code class="p">:</code> <code class="n">int64</code>
                           <code class="n">count</code>
<code class="n">extra</code> <code class="n">neuro</code> <code class="n">state</code> <code class="n">bkg_amt</code>       
<code class="bp">False</code> <code class="bp">False</code> <code class="bp">False</code> <code class="bp">False</code>      <code class="mi">368</code>
                  <code class="bp">True</code>        <code class="mi">33</code>
            <code class="bp">True</code>  <code class="bp">False</code>      <code class="mi">163</code>
                  <code class="bp">True</code>        <code class="mi">24</code>
      <code class="bp">True</code>  <code class="bp">False</code> <code class="bp">False</code>      <code class="mi">358</code>
                  <code class="bp">True</code>        <code class="mi">23</code>
            <code class="bp">True</code>  <code class="bp">False</code>      <code class="mi">214</code>
                  <code class="bp">True</code>        <code class="mi">24</code>
<code class="bp">True</code>  <code class="bp">False</code> <code class="bp">False</code> <code class="bp">False</code>      <code class="mi">249</code>
                  <code class="bp">True</code>        <code class="mi">15</code>
            <code class="bp">True</code>  <code class="bp">False</code>      <code class="mi">125</code>
                  <code class="bp">True</code>        <code class="mi">23</code>
      <code class="bp">True</code>  <code class="bp">False</code> <code class="bp">False</code>      <code class="mi">228</code>
                  <code class="bp">True</code>        <code class="mi">15</code>
            <code class="bp">True</code>  <code class="bp">False</code>      <code class="mi">120</code>
                  <code class="bp">True</code>        <code class="mi">18</code></pre>
<p class="pagebreak-before less_space">The output is composed of two tables:</p>
<ul>
<li><p>The first table indicates the total number of missing values for each variable in our data, as seen at the bottom of <a data-type="xref" href="#patterns_of_missing_data">Figure 6-2</a>. <em>Extraversion</em> has 793 missing values, and so on.</p></li>
<li><p>The second table represents the details of each missing data pattern. The variables above the logical values on the left (i.e., <em>Extraversion, Neuroticism, State, BookingAmount)</em> are the ones with some missing values in the data. Each line of the table indicates the number of rows with a certain pattern of missing data. The first line is made of four <code>False</code>, i.e., the pattern where none of the variables has missing data, and there are 368 such rows in our data, as you’ve seen previously in the first line of <a data-type="xref" href="#patterns_of_missing_data">Figure 6-2</a>. The second line only changes the last <code>False</code> to <code>True</code>, with the first three <code>False</code> omitted for readability (i.e., any blank logical value should be read up). This pattern <code>False</code><code>/</code><code>False</code><code>/</code><code>False</code><code>/</code><code>True</code> happens when only <em>BookingAmount</em> has a missing value, which happens in 33 rows of our data, and so on.</p></li>
</ul>
<p>Even with such a small data set, this visualization is very rich and it can be hard to know what to look for. We will explore<a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch06-msdat2" id="idm45968169906728"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch06-msdat3" id="idm45968169905352"/> two aspects:</p>
<dl>
<dt>Amount of missing data</dt>
<dd><p>How much of our data is missing? For which variables do we have the highest percentage of missing data? Can we just discard rows with missing data?</p></dd>
<dt>Correlation of missingness</dt>
<dd><p> Is data missing at the individual or variable level?</p></dd>
</dl>
<section data-type="sect2" data-pdf-bookmark="Amount of Missing Data"><div class="sect2" id="amount_of_missing_data">
<h2>Amount of Missing Data</h2>
<p>The first order of business is to determine how much of our data<a contenteditable="false" data-type="indexterm" data-primary="missing data" data-secondary="amount of missing data" id="ch06-amtms2"/> is missing and which variables have the highest percentage of missing data. We can find the necessary values at the bottom of <a data-type="xref" href="#patterns_of_missing_data">Figure 6-2</a>, with the number of missing values per variable, in increasing order of missingness, or at the bottom of the Python output. <a contenteditable="false" data-type="indexterm" data-primary="missing data" data-secondary="amount of missing data" data-tertiary="amount safe to drop" id="idm45968169895928"/><a contenteditable="false" data-type="indexterm" data-primary="numeric variables" data-secondary="missing data safe to drop" id="idm45968169894312"/>If the amount of missing data is very limited, e.g., you have a 10-million-row data set where no variable has more than 10 missing values, then handling them properly through multiple imputation would be overkill as we’ll see later. Just drop all the rows with missing data and be done with it. The rationale here is that even if the missing values are extremely biased, there are too few of them to materially influence the outcomes of your analysis in any way.</p>
<p>In our example, the variable with the highest number of missing values is <em>Neuroticism</em>, with 1,000 missing values. Is that a lot? Where is the limit? Is it 10, 100, 1,000 rows or more? It depends on the context. A quick-and-dirty strategy that you can use is as follows:</p>
<ol>
<li><p>Take the variable with the highest number of missing values and create two new data sets: one where all the missing values are replaced by the minimum of that variable and one where they are replaced by the maximum of that variable.</p></li>
<li><p>Run a regression for that variable’s most important relationship with each of the three data sets you now have. For example, if that variable is a predictor of your effect of interest, then run that regression.</p></li>
<li><p>If the regression coefficient is not materially different across the three regressions, i.e., you would draw the same business implications or take the same actions based on the different values, then you’re below the limit and you can drop the missing data. In simpler words: would these numbers mean the same thing to your business partners? If so, you can drop the missing data.</p></li>
</ol>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>This rule of thumb is easily applied to numeric variables, but what about binary or categorical variables?</p>
<p>For binary variables, the minimum will be 0 and the maximum<a contenteditable="false" data-type="indexterm" data-primary="binary variables" data-secondary="missing data safe to drop" id="idm45968169886712"/> will be 1, and that’s OK. The two data sets you create translate into a best-case scenario and a worst-case scenario.</p>
<p>For categorical variables, the minimum and maximum rule must<a contenteditable="false" data-type="indexterm" data-primary="categorical variables" data-secondary="missing data safe to drop" id="idm45968169884632"/> be slightly adjusted: replace all the missing values with either the least frequent or the most frequent category.</p>
</div>
<p>Let’s do that here, for example for <em>Neuroticism</em>. <em>Neuroticism</em> is a predictor of our effect of interest, <em>BookingAmount</em>, so we’ll use that relationship as indicated earlier:</p>
<pre data-type="programlisting" data-code-language="r"><code class="c1">## R (output not shown)</code>
<code class="n">min_data</code> <code class="o">&lt;-</code> <code class="n">available_data</code> <code class="o">%&gt;%</code>
  <code class="nf">mutate</code><code class="p">(</code><code class="n">neuro</code> <code class="o">=</code> <code class="nf">ifelse</code><code class="p">(</code><code class="o">!</code><code class="nf">is.na</code><code class="p">(</code><code class="n">neuro</code><code class="p">),</code> <code class="n">neuro</code><code class="p">,</code> <code class="nf">min</code><code class="p">(</code><code class="n">neuro</code><code class="p">,</code> <code class="n">na.rm</code> <code class="o">=</code> <code class="kc">TRUE</code><code class="p">)))</code>
<code class="n">max_data</code> <code class="o">&lt;-</code> <code class="n">available_data</code> <code class="o">%&gt;%</code>
  <code class="nf">mutate</code><code class="p">(</code><code class="n">neuro</code> <code class="o">=</code> <code class="nf">ifelse</code><code class="p">(</code><code class="o">!</code><code class="nf">is.na</code><code class="p">(</code><code class="n">neuro</code><code class="p">),</code> <code class="n">neuro</code><code class="p">,</code> <code class="nf">max</code><code class="p">(</code><code class="n">neuro</code><code class="p">,</code> <code class="n">na.rm</code> <code class="o">=</code> <code class="kc">TRUE</code><code class="p">)))</code>
<code class="nf">summary</code><code class="p">(</code><code class="nf">lm</code><code class="p">(</code><code class="n">bkg_amt</code><code class="o">~</code><code class="n">neuro</code><code class="p">,</code> <code class="n">data</code><code class="o">=</code><code class="n">available_data</code><code class="p">))</code>
<code class="nf">summary</code><code class="p">(</code><code class="nf">lm</code><code class="p">(</code><code class="n">bkg_amt</code><code class="o">~</code><code class="n">neuro</code><code class="p">,</code> <code class="n">data</code><code class="o">=</code><code class="n">min_data</code><code class="p">))</code>
<code class="nf">summary</code><code class="p">(</code><code class="nf">lm</code><code class="p">(</code><code class="n">bkg_amt</code><code class="o">~</code><code class="n">neuro</code><code class="p">,</code> <code class="n">data</code><code class="o">=</code><code class="n">max_data</code><code class="p">))</code>
</pre>
<pre data-type="programlisting" data-code-language="python">
<code class="c1">## Python (output not shown) </code>
<code class="n">min_data_df</code> <code class="o">=</code> <code class="n">available_data_df</code><code class="o">.</code><code class="n">copy</code><code class="p">()</code>
<code class="n">min_data_df</code><code class="o">.</code><code class="n">neuro</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">where</code><code class="p">(</code><code class="n">min_data_df</code><code class="o">.</code><code class="n">neuro</code><code class="o">.</code><code class="n">isna</code><code class="p">(),</code> <code class="n">min_data_df</code><code class="o">.</code><code class="n">neuro</code><code class="o">.</code><code class="n">min</code><code class="p">(),</code> 
                             <code class="n">min_data_df</code><code class="o">.</code><code class="n">neuro</code><code class="p">)</code>

<code class="n">max_data_df</code> <code class="o">=</code> <code class="n">available_data_df</code><code class="o">.</code><code class="n">copy</code><code class="p">()</code>
<code class="n">max_data_df</code><code class="o">.</code><code class="n">neuro</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">where</code><code class="p">(</code><code class="n">max_data_df</code><code class="o">.</code><code class="n">neuro</code><code class="o">.</code><code class="n">isna</code><code class="p">(),</code> <code class="n">max_data_df</code><code class="o">.</code><code class="n">neuro</code><code class="o">.</code><code class="n">max</code><code class="p">(),</code> 
                             <code class="n">max_data_df</code><code class="o">.</code><code class="n">neuro</code><code class="p">)</code>


<code class="k">print</code><code class="p">(</code><code class="n">ols</code><code class="p">(</code><code class="s2">"bkg_amt~neuro"</code><code class="p">,</code> <code class="n">data</code><code class="o">=</code><code class="n">available_data_df</code><code class="p">)</code><code class="o">.</code><code class="n">fit</code><code class="p">()</code><code class="o">.</code><code class="n">summary</code><code class="p">())</code>
<code class="k">print</code><code class="p">(</code><code class="n">ols</code><code class="p">(</code><code class="s2">"bkg_amt~neuro"</code><code class="p">,</code> <code class="n">data</code><code class="o">=</code><code class="n">min_data_df</code><code class="p">)</code><code class="o">.</code><code class="n">fit</code><code class="p">()</code><code class="o">.</code><code class="n">summary</code><code class="p">())</code>
<code class="k">print</code><code class="p">(</code><code class="n">ols</code><code class="p">(</code><code class="s2">"bkg_amt~neuro"</code><code class="p">,</code> <code class="n">data</code><code class="o">=</code><code class="n">max_data_df</code><code class="p">)</code><code class="o">.</code><code class="n">fit</code><code class="p">()</code><code class="o">.</code><code class="n">summary</code><code class="p">())</code></pre>
<p>The results are as follows:</p>
<ul>
<li><p>The coefficient based on the available data is −5.9.</p></li>
<li><p>The coefficient based on replacing the missing values with the minimum of <em>Neuroticism</em> is −8.0.</p></li>
<li><p>The coefficient based on replacing the missing values with the maximum of <em>Neuroticism</em> is 2.7.</p></li>
</ul>
<p>These values are very different from each other, to the point of having different signs, therefore we’re definitely above the threshold for material significance. We can’t simply drop the rows that have missing data for <em>Neuroticism</em>. Applying the same approach to the other variables would also show that we can’t disregard their missing values and need to handle them adequately.<a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch06-amtms2" id="idm45968169603528"/></p>
</div></section>
<section data-type="sect2" data-pdf-bookmark="Correlation of Missingness"><div class="sect2" id="correlation_of_missingness">
<h2>Correlation of Missingness</h2>
<p>Once we have determined which variables we need to deal with,<a contenteditable="false" data-type="indexterm" data-primary="missing data" data-secondary="correlation of missingness" id="ch06-corms2"/><a contenteditable="false" data-type="indexterm" data-primary="correlation" data-secondary="missingness of missing data" id="ch06-corms3"/> we’ll want to know how much their missingness is correlated. If you have variables whose missingness is highly correlated, this suggests that the missingness of one causes the missingness of others (e.g., if someone stops answering a survey halfway through, then all answers after a certain point will be missing). Alternatively, their missingness may have a common cause (e.g., if some subjects are more reluctant to reveal information about themselves). In both of these cases, identifying correlation in missingness will help you build a more accurate CD, which will save you time and make your analyses more effective.</p>
<p>Let’s look at it through a simple illustration: imagine that we have interview data for two offices: Tampa and Tacoma. In both offices, candidates must go through the same mandatory three interview sections, but in Tampa the first interviewer is responsible for recording all the scores of a candidate whereas in Tacoma each interviewer records the score for their section. Interviewers are human beings and they sometimes forget to turn in the data to HR. In Tampa, when an interviewer forgets to turn in the data, we have no data whatsoever for the candidate, apart from their ID in the system (<a data-type="xref" href="#highly_correlated_missingness_in_tampa">Figure 6-3</a> shows the data for Tampa only).</p>
<figure><div id="highly_correlated_missingness_in_tampa" class="figure">
<img src="Images/BEDA_0603.png" alt="Highly correlated missingness in Tampa data" width="898" height="481"/>
<h6><span class="label">Figure 6-3. </span>Highly correlated missingness in Tampa data</h6>
</div></figure>
<p>The telltale sign for high missingness correlation is a line with a large number of light-shaded squares (here 3) that represent a high number of cases (here 400 out of 2,000 total rows). In addition, the figure has no line with only one or two light squares.</p>
<p>In such a situation, it wouldn’t make sense to analyze our missing data one variable at a time. If we find that data for the first section is highly likely to be missing when Murphy is the first interviewer, then it will also be true for the other sections. (You had one job, Murphy!)</p>
<p>In Tacoma, on the other hand, the missingness of the different sections is entirely uncorrelated (<a data-type="xref" href="#uncorrelated_missingness_in_tacomaapost">Figure 6-4</a>).</p>
<p>The pattern is the opposite of Tampa’s:</p>
<ul>
<li><p>We have a high number of lines with few missing variables (see all the 1s and 2s on the right of the figure).</p></li>
<li><p>These lines represent the bulk of our data (we can see on the left that only 17 rows have 3 missing variables).</p></li>
<li><p>Lines with a high number of light squares at the bottom of the figure represent very few cases (the same 17 individuals) because they are the result of independent randomness.</p></li>
</ul>
<figure><div id="uncorrelated_missingness_in_tacomaapost" class="figure">
<img src="Images/BEDA_0604.png" alt="Uncorrelated missingness in Tacoma’s data" width="949" height="1594"/>
<h6><span class="label">Figure 6-4. </span>Uncorrelated missingness in Tacoma’s data</h6>
</div></figure>
<p>The argument for the last bullet point can be extended by looking more broadly at what we could call “Russian dolls” sequences of increasing missingness where each pattern adds a missing variable on the previous pattern, for example (I3) → (I3, I2) → (I3, I2, I1). The corresponding numbers of cases are 262 → 55 → 17. These numbers form a decreasing sequence, which is logical because if the missingness of the variables is completely uncorrelated, we have:</p>
<div data-type="equation">
<p><em>Prob</em>(<em>I</em>3 <em>missing &amp; I</em>2 <em>missing</em>) = <em>Prob</em>(<em>I</em>3 <em>missing</em>) * <em>Prob</em>(<em>I</em>2 <em>missing</em>)</p>
</div>
<div data-type="equation">
<p><em>Prob</em>(<em>I</em>3 <em>missing &amp; I</em>2 <em>missing &amp; I</em>1 <em>missing</em>) = <em>Prob</em>(<em>I</em>3 <em>missing</em>) * <em>Prob</em>(<em>I</em>2 <em>missing</em>) <em>* Prob</em>(<em>I</em>1 <em>missing</em>)</p>
</div>
<p>With a small sample and/or very high levels of missingness, these equations may not hold exactly true in our data, but if less than 50% of any variable is missing we should generally have:</p>
<div data-type="equation">
<p><em>Prob</em>(<em>I</em>3 <em>missing</em> &amp; <em>I</em>2 <em>missing</em> &amp; <em>I</em>1 <em>missing</em>) &lt; <em>Prob</em>(<em>I</em>3 <em>missing</em> &amp; <em>I</em>2 <em>missing</em>) &lt; <em>Prob</em>(<em>I</em>3 <em>missing</em>)</p>
</div>
<p>In real-life situations, it would be rather cumbersome to test all these inequalities by yourself, although you could write a function to do so at scale. Instead, I would recommend looking at the visualization for any significant outlier (i.e., a value for several variables much larger than the values for some of the same variables).</p>
<p>More broadly, this visualization is easy to use with only a few variables. As soon as you have a large number of variables, you’ll have to build and visualize the correlation matrix for missingness:</p>
<pre data-type="programlisting" data-code-language="r"><code class="c1">## R (output not shown)</code>
<code class="c1"># Building the correlation matrices</code>
<code class="n">tampa_miss</code> <code class="o">&lt;-</code> <code class="n">tampa</code> <code class="o">%&gt;%</code>
  <code class="nf">select</code><code class="p">(</code><code class="o">-</code><code class="n">ID</code><code class="p">)</code> <code class="o">%&gt;%</code>
  <code class="nf">mutate</code><code class="p">(</code><code class="nf">across</code><code class="p">(</code><code class="nf">everything</code><code class="p">(),</code><code class="n">is.na</code><code class="p">))</code>
<code class="n">tampa_cor</code> <code class="o">&lt;-</code> <code class="nf">cor</code><code class="p">(</code><code class="n">tampa_miss</code><code class="p">)</code> <code class="o">%&gt;%</code>
  <code class="nf">melt</code><code class="p">()</code>

<code class="n">tacoma_miss</code> <code class="o">&lt;-</code> <code class="n">tacoma</code> <code class="o">%&gt;%</code>
  <code class="nf">select</code><code class="p">(</code><code class="o">-</code><code class="n">ID</code><code class="p">)</code> <code class="o">%&gt;%</code>
  <code class="nf">mutate</code><code class="p">(</code><code class="nf">across</code><code class="p">(</code><code class="nf">everything</code><code class="p">(),</code><code class="n">is.na</code><code class="p">))</code>
<code class="n">tacoma_cor</code> <code class="o">&lt;-</code> <code class="nf">cor</code><code class="p">(</code><code class="n">tacoma_miss</code><code class="p">)</code> <code class="o">%&gt;%</code>
  <code class="nf">melt</code><code class="p">()</code>
 </pre>
<pre data-type="programlisting" data-code-language="python">
<code class="c1">## Python (output not shown)</code>
<code class="c1"># Building the correlation matrices</code>
<code class="n">tampa_miss_df</code> <code class="o">=</code> <code class="n">tampa_df</code><code class="o">.</code><code class="n">copy</code><code class="p">()</code><code class="o">.</code><code class="n">drop</code><code class="p">([</code><code class="s1">'ID'</code><code class="p">],</code> <code class="n">axis</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code><code class="o">.</code><code class="n">isna</code><code class="p">()</code>
<code class="n">tacoma_miss_df</code> <code class="o">=</code> <code class="n">tacoma_df</code><code class="o">.</code><code class="n">copy</code><code class="p">()</code><code class="o">.</code><code class="n">drop</code><code class="p">([</code><code class="s1">'ID'</code><code class="p">],</code> <code class="n">axis</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code><code class="o">.</code><code class="n">isna</code><code class="p">()</code>

<code class="n">tampa_cor</code> <code class="o">=</code> <code class="n">tampa_miss_df</code><code class="o">.</code><code class="n">corr</code><code class="p">()</code>
<code class="n">tacoma_cor</code> <code class="o">=</code> <code class="n">tacoma_miss_df</code><code class="o">.</code><code class="n">corr</code><code class="p">()</code></pre>
<p class="pagebreak-before less_space"><a data-type="xref" href="#correlation_matrices_for_completely_cor">Figure 6-5</a> shows the resulting correlation matrices. In the one on the left, for Tampa, all the values are equal to 1: if one variable is missing, then the other two are as well. In the correlation matrix on the right, for Tacoma, the values are equal to 1 on the main diagonal but 0 everywhere else: knowing that one variable is missing tells you nothing about the missingness of the others.</p>
<figure><div id="correlation_matrices_for_completely_cor" class="figure">
<img src="Images/BEDA_0605.png" alt="Correlation matrices for completely correlated missingness (left) and completely uncorrelated missingness (right)" width="1903" height="1105"/>
<h6><span class="label">Figure 6-5. </span>Correlation matrices for completely correlated missingness (left) and completely uncorrelated missingness (right)</h6>
</div></figure>
<p>Let’s get back to our AirCnC data set and see where it falls between the two extremes outlined in our theoretical interview example. <a data-type="xref" href="#patterns_of_missing_data_left_parenthes">Figure 6-6</a> repeats <a data-type="xref" href="#patterns_of_missing_data">Figure 6-2</a> for ease of access.</p>
<p class="pagebreak-before less_space"><a data-type="xref" href="#patterns_of_missing_data_left_parenthes">Figure 6-6</a> falls somewhere in the middle: all possible patterns of missingness are fairly represented, which suggests that we don’t have strongly clustered sources of missingness.</p>
<figure><div id="patterns_of_missing_data_left_parenthes" class="figure">
<img src="Images/BEDA_0606.png" alt="Patterns of missing data (repeats)" width="1024" height="2047"/>
<h6><span class="label">Figure 6-6. </span>Patterns of missing data (repeats <a data-type="xref" href="#patterns_of_missing_data">Figure 6-2</a>)</h6>
</div></figure>
<p class="pagebreak-before less_space"><a data-type="xref" href="#correlation_matrix_of_missingness_in_ou">Figure 6-7</a> shows the correlation matrix of missingness for our AirCnC data. As you can see, the missingness of our variables is almost entirely uncorrelated, well within the range of random fluctuations. If you want to familiarize yourself more with correlation patterns in missingness, one of the <a href="https://oreil.ly/BehavioralDataAnalysisCh6">exercises for this chapter on GitHub</a> asks you to identify a few of them. As a reminder, looking at correlation patterns is never necessary in itself, but it can often be illuminating and save you time.<a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch06-corms2" id="idm45968169328680"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch06-corms3" id="idm45968169327336"/></p>
<figure><div id="correlation_matrix_of_missingness_in_ou" class="figure">
<img src="Images/BEDA_0607.png" alt="Correlation matrix of missingness in our AirCnC data" width="1902" height="1110"/>
<h6><span class="label">Figure 6-7. </span>Correlation matrix of missingness in our AirCnC data</h6>
</div></figure>
</div></section>
</div></section>
<section data-type="sect1" data-pdf-bookmark="Diagnosing Missing Data"><div class="sect1" id="diagnosing_missing_data">
<h1>Diagnosing Missing Data</h1>
<p>Now that we have visualized our missing data, it’s time to<a contenteditable="false" data-type="indexterm" data-primary="causal diagrams (CDs)" data-secondary="missing data cause diagnosed" id="ch06-ccdia3"/><a contenteditable="false" data-type="indexterm" data-primary="missing data" data-secondary="cause diagnosed with CDs" id="ch06-cddia"/> understand what causes it. This is where causal diagrams come in, as we’ll use them to represent the causal mechanisms of missing data.</p>
<p>Let’s start with a very simple example from <a data-type="xref" href="ch01.xhtml#the_causal_behavioral_framework_for_da">Chapter 1</a>. <a contenteditable="false" data-type="indexterm" data-primary="causal diagrams (CDs)" data-secondary="fundamental structures of" data-tertiary="shaded rectangles or ovals as unobserved variables" id="idm45968169317064"/><a contenteditable="false" data-type="indexterm" data-primary="variables" data-secondary="unobserved variables" data-tertiary="ovals in causal diagrams" id="idm45968169315256"/><a contenteditable="false" data-type="indexterm" data-primary="unobserved variables" data-secondary="ovals in causal diagrams" id="idm45968169313592"/>When introducing causal diagrams, I mentioned that unobserved variables, such as a customer’s taste for vanilla ice cream, are represented in a darker shaded rectangle (<a data-type="xref" href="#unobserved_variables_are_represented_in">Figure 6-8</a>).</p>
<figure><div id="unobserved_variables_are_represented_in" class="figure">
<img src="Images/BEDA_0608.png" alt="Unobserved variables are represented in an oval" width="1047" height="139"/>
<h6><span class="label">Figure 6-8. </span>Unobserved variables are represented in a darker-shaded rectangle</h6>
</div></figure>
<p class="pagebreak-before less_space">Unobserved variables, which are sometimes called “latent” variables<a contenteditable="false" data-type="indexterm" data-primary="variables" data-secondary="unobserved variables" id="idm45968169308104"/><a contenteditable="false" data-type="indexterm" data-primary="unobserved variables" id="idm45968169306728"/><a contenteditable="false" data-type="indexterm" data-primary="latent variables" data-see="unobserved variables" id="idm45968169305624"/> in certain disciplines, refer to information that we don’t have in practice, even though it may or may not in theory be accessible. In the present case, let’s say that we force our customers to disclose their taste for vanilla before making a purchase. This would create the corresponding data in our systems, which we would then use for our data analyses (<a data-type="xref" href="#collecting_previously_unobserved_inform">Figure 6-9</a>).</p>
<figure><div id="collecting_previously_unobserved_inform" class="figure">
<img src="Images/BEDA_0609.png" alt="Collecting previously unobserved information" width="1310" height="139"/>
<h6><span class="label">Figure 6-9. </span>Collecting previously unobserved information</h6>
</div></figure>
<p>However, it is generally poor business practice to try and force customers to disclose information they don’t want to, and it’s often left optional. More generally, a large amount of data is collected on some customers but not others. We’ll represent that situation by drawing the corresponding box in the CD with dashes (<a data-type="xref" href="#representing_partially_observed_variabl">Figure 6-10</a>).</p>
<figure><div id="representing_partially_observed_variabl" class="figure">
<img src="Images/BEDA_0610.png" alt="Representing partially observed variables with a dashed box" width="1310" height="139"/>
<h6><span class="label">Figure 6-10. </span>Representing partially observed variables with a dashed box</h6>
</div></figure>
<p>For example, with three customers, we could have the following data, with one customer refusing to disclose their taste for vanilla ice cream (<a data-type="xref" href="#the_data_underlying_our_cd">Table 6-2</a>).</p>
<table class="border" id="the_data_underlying_our_cd">
<caption><span class="label">Table 6-2. </span>The data underlying our CD</caption>
<thead>
<tr>
<th>Customer name</th>
<th>Taste for vanilla</th>
<th>Stated taste</th>
<th>Purchased IC at stand (Y/N)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Ann</td>
<td>Low</td>
<td>Low</td>
<td>N</td>
</tr>
<tr>
<td>Bob</td>
<td>High</td>
<td>High</td>
<td>Y</td>
</tr>
<tr>
<td>Carolyn</td>
<td>High</td>
<td>N/A</td>
<td>Y</td>
</tr>
</tbody>
</table>
<aside data-type="sidebar" epub:type="sidebar" class="pagebreak-before less_space"><div class="sidebar" id="from_missing_values_to_wrong_or_false_v">
<h5>From Missing Values to Wrong or False Values</h5>
<p>In this book, we’ll make the simplifying assumption that values<a contenteditable="false" data-type="indexterm" data-primary="missing data" data-secondary="wrong or false values" id="idm45968169255224"/> are either correct or missing. People never lie, misremember, or make entry errors. Of course, they do so in real life, and that’s something you’ll have to address by using the insights from this chapter and <a data-type="xref" href="ch02.xhtml#understanding_behavioral_data">Chapter 2</a>: assume that there’s a hidden variable with the true values, and an observable variable that reflects the hidden variable with some “noise.” Then determine if that noise is purely random, dependent on the value of another variable, or dependent on the hidden variable, similar to Rubin’s classification (discussed later in this chapter). For example, people who have bought a product they may be embarrassed to confess using (a toupee?) are somewhat likely to pretend they don’t, but the converse is highly unlikely. That would make <em>ToupeePurchases</em> “false not at random” in Rubin’s terminology.</p>
</div></aside>
<p>In this chapter, we’re interested in understanding what causes missingness of a variable, and not just what causes the values of a variable. <a contenteditable="false" data-type="indexterm" data-primary="missing data" data-secondary="adding tracking variable for when missing" id="idm45968169250808"/>Therefore, we’ll create a variable to track when the stated taste variable is missing (<a data-type="xref" href="#adding_a_missingness_variable">Table 6-3</a>).</p>
<table class="border" id="adding_a_missingness_variable">
<caption><span class="label">Table 6-3. </span>Adding a missingness variable</caption>
<thead>
<tr>
<th>Customer name</th>
<th>Taste for vanilla</th>
<th>Stated taste for vanilla</th>
<th>Stated taste missing (Y/N)</th>
<th>Purchased IC at stand</th>
</tr>
</thead>
<tbody>
<tr>
<td>Ann</td>
<td>Low</td>
<td>Low</td>
<td>N</td>
<td>N</td>
</tr>
<tr>
<td>Bob</td>
<td>High</td>
<td>High</td>
<td>N</td>
<td>Y</td>
</tr>
<tr>
<td>Carolyn</td>
<td>High</td>
<td>N/A</td>
<td>Y</td>
<td>Y</td>
</tr>
</tbody>
</table>
<p>Let’s add that variable to our CD<a contenteditable="false" data-type="indexterm" data-primary="causal diagrams (CDs)" data-secondary="missing data cause diagnosed" data-tertiary="adding missingness variable" id="idm45968169234168"/><a contenteditable="false" data-type="indexterm" data-primary="missing data" data-secondary="cause diagnosed with CDs" data-tertiary="adding missingness variable" id="idm45968169232440"/> (<a data-type="xref" href="#adding_missingness_to_our_causal_diagra">Figure 6-11</a>).</p>
<figure><div id="adding_missingness_to_our_causal_diagra" class="figure">
<img src="Images/BEDA_0611.png" alt="Adding missingness to our causal diagram" width="1310" height="505"/>
<h6><span class="label">Figure 6-11. </span>Adding missingness to our causal diagram</h6>
</div></figure>
<p class="pagebreak-before less_space">We conventionally make missingness a cause of the corresponding partially observed variable. The intuition is that the information exists fully in the unobserved variable, and the partially observed variable is equal to the unobserved variable, unless information is “hidden” by the missingness variable. This convention will make our lives much easier, because it will allow us to express and discuss causes of missingness in the CD that represent our relationships of interest, instead of having to consider missingness separately.<a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch06-cddia" id="idm45968169226264"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch06-ccdia3" id="idm45968169224888"/></p>
<p>Now that missingness is part of our CD, the natural next step is to ask ourselves, “What causes it?”</p>
<section data-type="sect2" data-pdf-bookmark="Causes of Missingness: Rubin’s Classification"><div class="sect2" id="causes_of_missingness_rubinapostrophes">
<h2>Causes of Missingness: Rubin’s Classification</h2>
<p>There are three basic and mutually exclusive possibilities for<a contenteditable="false" data-type="indexterm" data-primary="missing data" data-secondary="Rubin’s classification of causes" id="ch06-rub"/><a contenteditable="false" data-type="indexterm" data-primary="Rubin, Donald" id="idm45968169219192"/><a contenteditable="false" data-type="indexterm" data-primary="Rubin’s classification of missingness causes" id="ch06-rub4"/> what causes missingness of a variable, which have been categorized by statistician Donald Rubin.</p>
<p>First, if the missingness of a variable depends only <a contenteditable="false" data-type="indexterm" data-primary="missing data" data-secondary="Rubin’s classification of causes" data-tertiary="missing completely at random" id="idm45968169215960"/><a contenteditable="false" data-type="indexterm" data-primary="MCAR (missing completely at random)" id="idm45968169214280"/>on variables outside of our data, such as purely random factors, that variable is said to be <em>m</em><em>issing completely at random</em> (MCAR) (<a data-type="xref" href="#stated_taste_is_missing_completely_at_r">Figure 6-12</a>).</p>
<figure><div id="stated_taste_is_missing_completely_at_r" class="figure">
<img src="Images/BEDA_0612.png" alt="Stated taste is missing completely at random" width="1310" height="505"/>
<h6><span class="label">Figure 6-12. </span>Stated taste is missing completely at random</h6>
</div></figure>
<p>Then, a variable goes from MCAR to<a contenteditable="false" data-type="indexterm" data-primary="missing data" data-secondary="Rubin’s classification of causes" data-tertiary="missing at random" id="idm45968169208392"/><a contenteditable="false" data-type="indexterm" data-primary="MAR (missing at random)" id="idm45968169206728"/> <em>missing at random</em> (MAR) if even one variable in our data affects its missingness. Variables outside of our data and random factors may also play a role, but the value of the variable in question may not affect its own missingness. This would be the case for instance if <em>Purchased</em> caused the missingness of <em>StatedVanillaTaste</em>, e.g., because we only interview customers who purchased instead of passersby (<a data-type="xref" href="#stated_taste_is_missing_at_random">Figure 6-13</a>).</p>
<figure><div id="stated_taste_is_missing_at_random" class="figure">
<img src="Images/BEDA_0613.png" alt="Stated taste is missing at random" width="1320" height="513"/>
<h6><span class="label">Figure 6-13. </span>Stated taste is missing at random</h6>
</div></figure>
<p>Finally, any variable whose value influences its own missingness is<a contenteditable="false" data-type="indexterm" data-primary="missing data" data-secondary="Rubin’s classification of causes" data-tertiary="missing not at random" id="idm45968169200408"/><a contenteditable="false" data-type="indexterm" data-primary="MNAR (missing not at random)" id="idm45968169198744"/> <em>m</em><em>issing not at random</em> (MNAR), even if other variables inside or outside of the data also affect the missingness. Other variables inside or outside of our data may also play a role, but a variable goes from MCAR or MAR to MNAR as soon as the variable influences its own missingness. In our example, this would mean that <em>VanillaTaste</em> causes the missingness of <em>StatedVanillaTaste</em> (<a data-type="xref" href="#stated_taste_is_missing_not_at_randomdo">Figure 6-14</a>).</p>
<figure><div id="stated_taste_is_missing_not_at_randomdo" class="figure">
<img src="Images/BEDA_0614.png" alt="Stated taste is missing not at random" width="1319" height="516"/>
<h6><span class="label">Figure 6-14. </span>Stated taste is missing not at random</h6>
</div></figure>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>We represent the idea that the values of a variable influence its missingness by drawing an arrow from the unobserved variable, and not the partially observed variable. This way, we can make meaningful statements such as “all values that are in reality below a certain threshold are missing in our data.” If the arrow came from the partially observable variable, we would be stuck with uninformative statements such as “values that are missing cause themselves to be missing.”</p>
</div>
<p>In an ideal world, the rest of the section would consist of recipes to identify each category of missingness. Unfortunately, missing data analysis is still an open area of research that has not yet been fully explored. <a contenteditable="false" data-type="indexterm" data-primary="missing data" data-secondary="causality interaction unknown" id="idm45968169190360"/><a contenteditable="false" data-type="indexterm" data-primary="causes" data-secondary="missingness and causality interaction unknown" id="idm45968169188920"/>In particular, how missingness and causality interact is not well understood. Therefore, dealing with missing data remains more art than science. Trying to create systematic recipes would require dealing with an intractable number of exceptions, as well as introducing circular arguments such as “pattern X indicates that variable 1 is MAR unless variable 2 is MNAR; pattern Y indicates that variable 2 is MNAR unless variable 1 is MAR.” I have done my best to cover as many cases as possible within a limited data set, but in the real world you might encounter situations that are “a little bit of this and a little bit of that” and you’ll have to make judgment calls as to how to proceed.</p>
<p>Some good news, however, is that with a few exceptions that I’ll call out, <a contenteditable="false" data-type="indexterm" data-primary="missing data" data-secondary="biases in data" data-tertiary="assuming worst reducing" id="idm45968169186184"/><a contenteditable="false" data-type="indexterm" data-primary="biases in data and analyses" data-secondary="missing data handling introducing" id="idm45968169184536"/>being cautious takes more time but doesn’t introduce bias. When you’re uncertain whether a variable is MCAR, MAR, or MNAR, just assume the worst of the possible scenarios and your analyses will be as unbiased as they can be.</p>
<p>With that caveat in mind, let’s get back to our AirCnC data and see how we can diagnose missingness in a realistic data set. As a quick refresher, our data set contains the following variables:</p>
<ul>
<li><p>Demographic characteristics</p>
<ul>
<li><p>Age</p></li>
<li><p>Gender</p></li>
<li><p>State (A, B, and C)</p></li>
</ul></li>
<li><p>Personality traits</p>
<ul>
<li><p>Openness</p></li>
<li><p>Extraversion</p></li>
<li><p>Neuroticism</p></li>
</ul></li>
<li><p>Booking amount</p></li>
</ul>
</div></section>
<section data-type="sect2" data-pdf-bookmark="Diagnosing MCAR Variables"><div class="sect2" id="diagnosing_mcar_variables">
<h2>Diagnosing MCAR Variables</h2>
<p>MCAR variables are the simplest case. A sensor went faulty, a<a contenteditable="false" data-type="indexterm" data-primary="missing data" data-secondary="Rubin’s classification of causes" data-tertiary="missing completely at random diagnosed" id="idm45968169173848"/><a contenteditable="false" data-type="indexterm" data-primary="MCAR (missing completely at random)" data-secondary="diagnosing" id="idm45968169172040"/> bug prevented data transmission from a customer’s mobile app, or a customer just missed the field to enter their taste for vanilla ice cream. Regardless, missingness happens in a way that is intuitively “random.” We diagnose MCAR variables by default: if a variable doesn’t appear to be MAR, we’ll treat it as MCAR. In other words, you can think of MCAR as our null hypothesis in the absence of evidence to the contrary.</p>
<p>The main tool that we’ll use to diagnose missingness is a logistic regression of whether a variable is missing on all the other variables in our data set. Let’s look, for example, at the <em>Extraversion</em> variable:<a contenteditable="false" data-type="indexterm" data-primary="Python" data-secondary="missingness logistic regression" id="idm45968169169032"/><a contenteditable="false" data-type="indexterm" data-primary="missing data" data-secondary="missingness logistic regression" data-tertiary="Python" id="idm45968169167608"/><a contenteditable="false" data-type="indexterm" data-primary="missing data" data-secondary="missingness logistic regression" data-tertiary="R" id="idm45968169165944"/><a contenteditable="false" data-type="indexterm" data-primary="R" data-secondary="missingness logistic regression" id="idm45968169164280"/></p>
<pre data-type="programlisting" data-code-language="python"><code class="c1">## Python (output not shown)</code>
<code class="n">available_data_df</code><code class="p">[</code><code class="s1">'md_extra'</code><code class="p">]</code> <code class="o">=</code> <code class="n">available_data_df</code><code class="p">[</code><code class="s1">'extra'</code><code class="p">]</code><code class="o">.</code><code class="n">isnull</code><code class="p">()</code><code class="o">.</code><code class="n">astype</code><code class="p">(</code><code class="nb">float</code><code class="p">)</code>
<code class="n">md_extra_mod</code> <code class="o">=</code><code class="n">smf</code><code class="o">.</code><code class="n">logit</code><code class="p">(</code><code class="s1">'md_extra~age+open+neuro+gender+state+bkg_amt'</code><code class="p">,</code>
                      <code class="n">data</code><code class="o">=</code><code class="n">available_data_df</code><code class="p">)</code>
<code class="n">md_extra_mod</code><code class="o">.</code><code class="n">fit</code><code class="p">()</code><code class="o">.</code><code class="n">summary</code><code class="p">()</code>
</pre>
<pre data-type="programlisting" data-code-language="r">
<code class="c1">## R</code>
<code class="o">&gt;</code> <code class="n">md_extra_mod</code> <code class="o">&lt;-</code> <code class="nf">glm</code><code class="p">(</code><code class="nf">is.na</code><code class="p">(</code><code class="n">extra</code><code class="p">)</code><code class="o">~</code><code class="n">.,</code>
                        <code class="n">family</code> <code class="o">=</code> <code class="nf">binomial</code><code class="p">(</code><code class="n">link</code> <code class="o">=</code> <code class="s">"logit"</code><code class="p">),</code> 
                        <code class="n">data</code><code class="o">=</code><code class="n">available_data</code><code class="p">)</code>
<code class="o">&gt;</code> <code class="nf">summary</code><code class="p">(</code><code class="n">md_extra_mod</code><code class="p">)</code>

<code class="kc">...</code>
<code class="n">Coefficients</code><code class="o">:</code>
              <code class="n">Estimate</code> <code class="n">Std.</code> <code class="n">Error</code> <code class="n">z</code> <code class="n">value</code> <code class="nf">Pr</code><code class="p">(</code><code class="o">&gt;|</code><code class="n">z</code><code class="o">|</code><code class="p">)</code>
<code class="p">(</code><code class="n">Intercept</code><code class="p">)</code> <code class="m">-0.7234738</code>  <code class="m">0.7048598</code>  <code class="m">-1.026</code>    <code class="m">0.305</code>
<code class="n">age</code>         <code class="m">-0.0016082</code>  <code class="m">0.0090084</code>  <code class="m">-0.179</code>    <code class="m">0.858</code>
<code class="n">open</code>         <code class="m">0.0557508</code>  <code class="m">0.0425013</code>   <code class="m">1.312</code>    <code class="m">0.190</code>
<code class="n">neuro</code>        <code class="m">0.0501370</code>  <code class="m">0.0705626</code>   <code class="m">0.711</code>    <code class="m">0.477</code>
<code class="n">genderF</code>     <code class="m">-0.0236904</code>  <code class="m">0.1659661</code>  <code class="m">-0.143</code>    <code class="m">0.886</code>
<code class="n">stateB</code>      <code class="m">-0.0780339</code>  <code class="m">0.2000428</code>  <code class="m">-0.390</code>    <code class="m">0.696</code>
<code class="n">stateC</code>      <code class="m">-0.0556228</code>  <code class="m">0.2048822</code>  <code class="m">-0.271</code>    <code class="m">0.786</code>
<code class="n">bkg_amt</code>     <code class="m">-0.0007701</code>  <code class="m">0.0011301</code>  <code class="m">-0.681</code>    <code class="m">0.496</code>
<code class="kc">...</code></pre>
<p>None of the variables has a large and strongly statistically significant coefficient. In the absence of any other evidence, this suggests that the source of missingness for <em>Extraversion</em> is purely random and we’ll treat our <em>Extraversion</em> variable as MCAR.</p>
<p>You can think of MCAR data as rolling dice or flipping a coin. Both of these actions are “random” from our perspective, but they still obey the laws of physics. Theoretically, if we had enough information and computing power, the outcome would be entirely predictable. The same could happen here. By saying that <em>Extraversion</em> is MCAR, we’re not saying “the missingness of <em>Extraversion</em> is fundamentally random and unpredictable,” we’re just saying “none of the variables <em>currently included</em> in our analysis is correlated with the missingness of <em>Extraversion.</em>” But maybe—and even probably—other variables (conscientiousness? trust? familiarity with technology?) would be correlated. Our goal is not to make a philosophical statement about <em>Extraversion</em>, but to determine if its missingness may bias our analyses, given the data currently available.</p>
<aside data-type="sidebar" epub:type="sidebar" class="pagebreak-before less_space"><div class="sidebar" id="statistical_significance">
<h5>Statistical Significance</h5>
<p>In <a data-type="xref" href="part04.xhtml#designing_and_analyzing_experiments">Part IV</a>, <a contenteditable="false" data-type="indexterm" data-primary="statistical significance (p-value)" id="idm45968168976296"/><a contenteditable="false" data-type="indexterm" data-primary="p-value" id="idm45968168975128"/><a contenteditable="false" data-type="indexterm" data-primary="missing data" data-secondary="statistical significance" id="idm45968168974024"/><a contenteditable="false" data-type="indexterm" data-primary="logistic regressions" data-secondary="statistical significance" id="idm45968168972632"/><a contenteditable="false" data-type="indexterm" data-primary="regression" data-secondary="statistical significance" id="idm45968168971240"/>I’ll explain at length what statistical significance is and why you shouldn’t put too much stock in it. In particular, you should not use the 0.05 threshold as any sort of stringent source of truth. Here, you should just exert your best judgment as to whether a coefficient matters or not. If in doubt, you can always treat a coefficient as if it was indeed strong and statistically significant—as we’ll see, it will add some extra work but won’t bias your analyses.</p>
</div></aside>
</div></section>
<section data-type="sect2" data-pdf-bookmark="Diagnosing MAR Variables"><div class="sect2" id="diagnosing_mar_variables">
<h2>Diagnosing MAR Variables</h2>
<p>MAR variables are variables whose missingness depends on the<a contenteditable="false" data-type="indexterm" data-primary="missing data" data-secondary="Rubin’s classification of causes" data-tertiary="missing at random diagnosed" id="idm45968168967032"/><a contenteditable="false" data-type="indexterm" data-primary="MAR (missing at random)" data-secondary="diagnosing" id="idm45968168965304"/> values of other variables in our data. If other variables in our data set are predictive of a variable’s missingness, then MAR becomes our default hypothesis for that variable, unless we have strong enough evidence that it’s MNAR. Let’s see what this looks like with the <em>State</em> variable:</p>
<pre data-type="programlisting" data-code-language="r"><code class="c1">## R (output not shown)</code>
<code class="n">md_state_mod</code> <code class="o">&lt;-</code> <code class="nf">glm</code><code class="p">(</code><code class="nf">is.na</code><code class="p">(</code><code class="n">state</code><code class="p">)</code><code class="o">~</code><code class="n">.,</code>
                    <code class="n">family</code> <code class="o">=</code> <code class="nf">binomial</code><code class="p">(</code><code class="n">link</code> <code class="o">=</code> <code class="s">"logit"</code><code class="p">),</code> 
                    <code class="n">data</code><code class="o">=</code><code class="n">available_data</code><code class="p">)</code>
<code class="nf">summary</code><code class="p">(</code><code class="n">md_state_mod</code><code class="p">)</code>
</pre>
<pre data-type="programlisting" data-code-language="python">
<code class="c1">## Python</code>
<code class="n">available_data_df</code><code class="p">[</code><code class="s1">'md_state'</code><code class="p">]</code> <code class="o">=</code> <code class="n">available_data_df</code><code class="p">[</code><code class="s1">'state'</code><code class="p">]</code><code class="o">.</code><code class="n">isnull</code><code class="p">()</code>\
    <code class="o">.</code><code class="n">astype</code><code class="p">(</code><code class="nb">float</code><code class="p">)</code>
<code class="n">md_state_mod</code> <code class="o">=</code><code class="n">smf</code><code class="o">.</code><code class="n">logit</code><code class="p">(</code><code class="s1">'md_state~age+open+extra+neuro+gender+bkg_amt'</code><code class="p">,</code>
                      <code class="n">data</code><code class="o">=</code><code class="n">available_data_df</code><code class="p">)</code>
<code class="n">md_state_mod</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">disp</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code><code class="o">.</code><code class="n">summary</code><code class="p">()</code>
<code class="o">...</code>
              <code class="n">coef</code>   <code class="n">std</code> <code class="n">err</code>       <code class="n">z</code>         <code class="n">P</code><code class="o">&gt;|</code><code class="n">z</code><code class="o">|</code>   <code class="p">[</code><code class="mf">0.025</code>  <code class="mf">0.975</code><code class="p">]</code>
<code class="n">Intercept</code>   <code class="o">-</code><code class="mf">0.2410</code>   <code class="mf">0.809</code>     <code class="o">-</code><code class="mf">0.298</code>       <code class="mf">0.766</code>   <code class="o">-</code><code class="mf">1.826</code>  <code class="mf">1.344</code>
<code class="n">gender</code><code class="p">[</code><code class="n">T</code><code class="o">.</code><code class="n">F</code><code class="p">]</code> <code class="o">-</code><code class="mf">0.1742</code>   <code class="mf">0.192</code>     <code class="o">-</code><code class="mf">0.907</code>       <code class="mf">0.364</code>   <code class="o">-</code><code class="mf">0.551</code>  <code class="mf">0.202</code>
<code class="n">age</code>          <code class="mf">0.0206</code>   <code class="mf">0.010</code>      <code class="mf">2.035</code>       <code class="mf">0.042</code>    <code class="mf">0.001</code>  <code class="mf">0.040</code>
<code class="nb">open</code>         <code class="mf">0.0362</code>   <code class="mf">0.050</code>      <code class="mf">0.727</code>       <code class="mf">0.467</code>   <code class="o">-</code><code class="mf">0.061</code>  <code class="mf">0.134</code>
<code class="n">extra</code>        <code class="mf">0.0078</code>   <code class="mf">0.048</code>      <code class="mf">0.162</code>       <code class="mf">0.871</code>   <code class="o">-</code><code class="mf">0.087</code>  <code class="mf">0.102</code>
<code class="n">neuro</code>       <code class="o">-</code><code class="mf">0.1462</code>   <code class="mf">0.087</code>     <code class="o">-</code><code class="mf">1.687</code>       <code class="mf">0.092</code>   <code class="o">-</code><code class="mf">0.316</code>  <code class="mf">0.024</code>
<code class="n">bkg_amt</code>     <code class="o">-</code><code class="mf">0.0019</code>   <code class="mf">0.001</code>     <code class="o">-</code><code class="mf">1.445</code>       <code class="mf">0.149</code>   <code class="o">-</code><code class="mf">0.005</code>  <code class="mf">0.001</code>
<code class="o">...</code></pre>
<p><em>Age</em> is mildly significant, with a positive coefficient. In other words, older customers appear less likely to provide their state. The corresponding causal diagram is represented in <a data-type="xref" href="#gender_missing_at_random">Figure 6-15</a>.</p>
<figure><div id="gender_missing_at_random" class="figure">
<img src="Images/BEDA_0615.png" alt="Gender missing at random" width="943" height="508"/>
<h6><span class="label">Figure 6-15. </span>Gender missing at random</h6>
</div></figure>
<p>We can confirm that correlation by plotting the density of<a contenteditable="false" data-type="indexterm" data-primary="correlation" data-secondary="missingness of missing data" id="idm45968158804952"/><a contenteditable="false" data-type="indexterm" data-primary="missing data" data-secondary="correlation of missingness" id="idm45968158803560"/> <em>State</em> missingness by recorded <em>Age</em> (<a data-type="xref" href="#density_of_missing_and_observed_state">Figure 6-16</a>). <em>State</em> has more observed values for younger customers than for older customers, or conversely, more missing values for older customers than for younger customers.</p>
<figure><div id="density_of_missing_and_observed_state" class="figure">
<img src="Images/BEDA_0616.png" alt="Density of missing and observed State data, by observed Age" width="1902" height="910"/>
<h6><span class="label">Figure 6-16. </span>Density of missing and observed State data, by observed Age</h6>
</div></figure>
<p>One limitation of this density plot is that it doesn’t show the rows where the X variable (here <em>Age</em>) is also missing. This could be problematic or misleading when that variable also has missing values. A possible trick is to replace the missing values for the X variable by a nonsensical value, such as −10. <em>Age</em> doesn’t have any missing value, so instead we’ll use as our X variable <em>Extraversion</em>, which has missing values. Let’s plot the density of observed and missing <em>State</em> data, by values of <em>Extraversion</em> (<a data-type="xref" href="#density_of_missing_and_observed_state_d">Figure 6-17</a>).</p>
<figure><div id="density_of_missing_and_observed_state_d" class="figure">
<img src="Images/BEDA_0617.png" alt="Density of missing and observed State data by level of Extraversion, including missing Extraversion" width="1902" height="904"/>
<h6><span class="label">Figure 6-17. </span>Density of missing and observed State data by level of Extraversion, including missing Extraversion</h6>
</div></figure>
<p><a data-type="xref" href="#density_of_missing_and_observed_state_d">Figure 6-17</a> shows that among individuals with no data for <em>Extraversion</em>, there are disproportionately more individuals for which we observe <em>State</em> than individuals for which <em>State</em> is missing. Overall, we’re seeing strong evidence that <em>State</em> is not MCAR but indeed MAR, because its missingness appears correlated with other variables available in our data set.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>You may have noticed that I used the word “correlation” earlier<a contenteditable="false" data-type="indexterm" data-primary="correlation" data-secondary="missingness of missing data" id="idm45968158787144"/><a contenteditable="false" data-type="indexterm" data-primary="missing data" data-secondary="correlation of missingness" id="idm45968158785672"/> when talking about the relationship between <em>Age</em> (or <em>Extraversion</em>) and the missingness of <em>State</em>. Indeed, we’ve only shown correlation so far, and it is entirely possible that <em>Age</em> doesn’t cause missingness of <em>State</em> but that they are both caused by a third unobserved variable. Fortunately, when talking about missingness, the causal nature of a correlation (or lack thereof) doesn’t affect our analyses. Loosely equating the two will not introduce bias because we’ll never actually deal with the coefficient for that relationship.</p>
</div>
</div></section>
<section data-type="sect2" data-pdf-bookmark="Diagnosing MNAR Variables"><div class="sect2" id="diagnosing_mnar_variables">
<h2>Diagnosing MNAR Variables</h2>
<p>MNAR variables are variables whose missingness depends on<a contenteditable="false" data-type="indexterm" data-primary="missing data" data-secondary="Rubin’s classification of causes" data-tertiary="missing not at random diagnosed" id="idm45968158779688"/><a contenteditable="false" data-type="indexterm" data-primary="MNAR (missing not at random)" data-secondary="diagnosing" id="idm45968168764280"/> their own values: higher values are more likely to be missing than lower values, or vice versa. This situation is both the most problematic for data analysis and the trickiest to diagnose. It is trickiest to diagnose because, by definition, we don’t know the values that are missing. Therefore, we’ll need to do a bit more sleuthing.</p>
<p class="pagebreak-before less_space">Let’s look at the <em>Neuroticism</em> variable, and as before, start by running a regression of its missingness on the other variables in the data:</p>
<pre data-type="programlisting" data-code-language="python"><code class="c1">## Python (output not shown)</code>
<code class="n">available_data_df</code><code class="p">[</code><code class="s1">'md_neuro'</code><code class="p">]</code> <code class="o">=</code> <code class="n">available_data_df</code><code class="p">[</code><code class="s1">'neuro'</code><code class="p">]</code><code class="o">.</code><code class="n">isnull</code><code class="p">()</code>\
    <code class="o">.</code><code class="n">astype</code><code class="p">(</code><code class="nb">float</code><code class="p">)</code>
<code class="n">md_neuro_mod</code> <code class="o">=</code><code class="n">smf</code><code class="o">.</code><code class="n">logit</code><code class="p">(</code><code class="s1">'md_neuro~age+open+extra+state+gender+bkg_amt'</code><code class="p">,</code>
                      <code class="n">data</code><code class="o">=</code><code class="n">available_data_df</code><code class="p">)</code>
<code class="n">md_neuro_mod</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">disp</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code><code class="o">.</code><code class="n">summary</code><code class="p">()</code>
</pre>
<pre data-type="programlisting" data-code-language="r">
<code class="c1">## R</code>
<code class="n">md_neuro_mod</code> <code class="o">&lt;-</code> <code class="nf">glm</code><code class="p">(</code><code class="nf">is.na</code><code class="p">(</code><code class="n">neuro</code><code class="p">)</code><code class="o">~</code><code class="n">.,</code>
                    <code class="n">family</code> <code class="o">=</code> <code class="nf">binomial</code><code class="p">(</code><code class="n">link</code> <code class="o">=</code> <code class="s">"logit"</code><code class="p">),</code> 
                    <code class="n">data</code><code class="o">=</code><code class="n">available_data</code><code class="p">)</code>
<code class="nf">summary</code><code class="p">(</code><code class="n">md_neuro_mod</code><code class="p">)</code>

<code class="kc">...</code>
<code class="n">Coefficients</code><code class="o">:</code>
             <code class="n">Estimate</code> <code class="n">Std.</code> <code class="n">Error</code> <code class="n">z</code> <code class="n">value</code> <code class="nf">Pr</code><code class="p">(</code><code class="o">&gt;|</code><code class="n">z</code><code class="o">|</code><code class="p">)</code>   
<code class="p">(</code><code class="n">Intercept</code><code class="p">)</code> <code class="m">-0.162896</code>   <code class="m">0.457919</code>  <code class="m">-0.356</code>  <code class="m">0.72204</code>   
<code class="n">age</code>         <code class="m">-0.012610</code>   <code class="m">0.008126</code>  <code class="m">-1.552</code>  <code class="m">0.12071</code>   
<code class="n">open</code>         <code class="m">0.052419</code>   <code class="m">0.038502</code>   <code class="m">1.361</code>  <code class="m">0.17337</code>   
<code class="n">extra</code>       <code class="m">-0.084991</code>   <code class="m">0.040617</code>  <code class="m">-2.092</code>  <code class="m">0.03639</code> <code class="o">*</code> 
<code class="n">genderF</code>     <code class="m">-0.093537</code>   <code class="m">0.151376</code>  <code class="m">-0.618</code>  <code class="m">0.53663</code>   
<code class="n">stateB</code>       <code class="m">0.047106</code>   <code class="m">0.181932</code>   <code class="m">0.259</code>  <code class="m">0.79570</code>   
<code class="n">stateC</code>      <code class="m">-0.128346</code>   <code class="m">0.187978</code>  <code class="m">-0.683</code>  <code class="m">0.49475</code>   
<code class="n">bkg_amt</code>      <code class="m">0.003216</code>   <code class="m">0.001065</code>   <code class="m">3.020</code>  <code class="m">0.00253</code> <code class="o">**</code>
<code class="kc">...</code></pre>
<p>We can see that <em>BookingAmount</em> has a strongly significant coefficient. On the face of it, this would suggest that <em>Neuroticism</em> is MAR on <em>BookingAmount</em>. However, and this is a key clue, <em>BookingAmount</em> is a child of <em>Neuroticism</em> in our CD. From a behavioral perspective, it also seems more likely that <em>Neuroticism</em> is MNAR rather than MAR on <em>BookingAmount</em> (i.e., the missingness is driven by a personality trait rather than by the amount a customer spent).</p>
<p>A way to confirm our suspicion is to identify another child of the variable with missing data, ideally as correlated with it as possible and as little correlated as possible with the first child. In our secondary data set, we have data about the total amount of travel insurance that customers purchased over their lifetime with the company. The fee per trip depends on trip characteristics that are only very loosely correlated with the booking amount, so we’re good on that front. Adding <em>Insurance</em> to our data set, we find that it’s strongly predictive of the missingness of <em>Neuroticism</em>, and that the distributions of <em>Insurance</em> amount with observed and missing <em>Neuroticism</em> are vastly different from each other (<a data-type="xref" href="#density_of_missing_and_observed_neuroti">Figure 6-18</a>).</p>
<figure><div id="density_of_missing_and_observed_neuroti" class="figure">
<img src="Images/BEDA_0618.png" alt="Density of missing and observed Neuroticism data, by observed Insurance amount" width="1902" height="904"/>
<h6><span class="label">Figure 6-18. </span>Density of missing and observed Neuroticism data, by observed Insurance amount</h6>
</div></figure>
<p>The more children variables we find correlated with the missingness of <em>Neuroticism</em>, the stronger our case becomes that the variable is MNAR. As we’ll see later, the way to handle a MNAR variable is to add auxiliary variables to our imputation model, and our children variables are perfect candidates for that, so finding several of them is not a waste of time but a head start for the next step.</p>
<p>Technically, we can never fully prove that a variable is MNAR and not just MAR on several of its children, but that is not an issue: auxiliary variables do not bias the imputation if the original variable is indeed MAR and not MNAR.<a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch06-rub" id="idm45968158663464"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch06-rub4" id="idm45968158662088"/></p>
</div></section>
<section data-type="sect2" data-pdf-bookmark="Missingness as a Spectrum"><div class="sect2" id="missingness_as_a_spectrum">
<h2>Missingness as a Spectrum</h2>
<p>Rubin’s classification relies on binary tests. <a contenteditable="false" data-type="indexterm" data-primary="missing data" data-secondary="Rubin’s classification of causes" data-tertiary="binary nature of" id="idm45968158658808"/><a contenteditable="false" data-type="indexterm" data-primary="missing data" data-secondary="Rubin’s classification of causes" data-tertiary="missingness probabilistic–deterministic spectrum" id="ch06-prdet"/>For example, as soon as a variable is more likely to be missing for higher values than for lower values (or vice versa), it is MNAR, regardless of any other consideration. However, the shape of that relationship between values and missingness matters for practical purposes: if <em>all</em> values of a variable are missing above or below a certain threshold, we’ll need to handle this variable differently from our default approach. This situation can also occur with MAR variables so it’s worth taking a step back and thinking more broadly about shapes of <span class="keep-together">missingness.</span></p>
<p>We can think of the missingness of a variable as falling on a spectrum from fully probabilistic to fully deterministic. At the “probabilistic” end of the spectrum, the variable is MCAR and all values are as likely to be missing. At the “deterministic” end of the spectrum, there is a threshold: the values are missing for all individuals on one side of the threshold and available for all individuals on the other side of the threshold. This often results from the application of a business rule. For example, in a <span class="keep-together">hiring</span> context, if only candidates with a GPA above 3.0 get interviewed, you wouldn’t have any interview score for candidates below that threshold. This would make <span class="keep-together"><em>InterviewScore</em></span> MAR on <em>GPA</em> (<a data-type="xref" href="#interview_score_being_mar_on_gpa">Figure 6-19</a>).</p>
<figure><div id="interview_score_being_mar_on_gpa" class="figure">
<img src="Images/BEDA_0619.png" alt="Interview score being MAR on GPA" width="943" height="508"/>
<h6><span class="label">Figure 6-19. </span>Interview score being MAR on GPA</h6>
</div></figure>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Rubin’s classification of MCAR/MAR/MNAR is based solely on what the cause of missingness is. It doesn’t take into account whether that causal relationship shows randomness or not. Here, counterintuitively, the fact that the missingness of <em>InterviewScore</em> is based deterministically on <em>GPA</em> makes <em>InterviewScore MAR</em> on <em>GPA</em> even though there’s no randomness involved.</p>
</div>
<p>This can also happen for variables that are MNAR, where only values above or below a certain threshold get recorded. For instance, only values outside of a normal range may be saved in a file, or only people under or above a certain threshold will register themselves (e.g., for tax purposes).</p>
<p>In between these two extremes of complete randomness and complete determinism (either of the MAR or MNAR type), there are situations where the probability of missingness increases or decreases continuously based on the values of the cause of missingness.</p>
<p><a data-type="xref" href="#missingness_spectrumcomma_from_mcar_lef">Figure 6-20</a> shows what this looks like in the simplest case of two variables, X and Y, where X has missing values. For the sake of readability, available values are shown as full squares, whereas “missing” values are shown as crosses. The first row of <a data-type="xref" href="#missingness_spectrumcomma_from_mcar_lef">Figure 6-20</a> shows scatterplots of Y against X and the second row shows for each one of these a line plot of the relationship between X and the probability of missingness:</p>
<ul class="pagebreak-before less_space">
<li><p>The leftmost column shows X being MCAR. The probability of missingness is constant at 0.5 and independent of X. Squares and crosses are spread similarly across the plot.</p></li>
<li><p>The central columns show X being probabilistically MNAR with increasing strength. Squares are more common on the left of the plot and crosses more common on the right, but there are still crosses on the left and squares on the right.</p></li>
<li><p>The rightmost column shows X being deterministically MNAR. All values of X below 5 are available (squares) and all the values above 5 are “missing” (crosses).</p></li>
</ul>
<figure><div id="missingness_spectrumcomma_from_mcar_lef" class="figure">
<img src="Images/BEDA_0620.png" alt="Missingness spectrum, from MCAR (leftmost) to deterministic MNAR (rightmost) through probabilistic MNAR (center)" width="1920" height="904"/>
<h6><span class="label">Figure 6-20. </span>Missingness spectrum, from MCAR (leftmost) to deterministic MNAR (rightmost) through probabilistic MNAR (center)</h6>
</div></figure>
<p>This spectrum of missingness is rarely discussed in statistical treatments of missing data, because it is difficult to confirm through purely mathematical methods. But this is a book about behavioral analytics, so we can and should use common sense and business knowledge. In the GPA example, the threshold in data results from the application of a business rule that you should be aware of. In most situations, you expect a variable to be in a certain range of values, and you should have a sense of how likely it is for a possible value to not be represented in your data.</p>
<p class="pagebreak-before less_space">In our AirCnC survey data, we have three personality traits: <em>Openness</em>, <em>Extraversion</em>, and <em>Neuroticism</em>. In real life, these variables would result from the aggregation of the answers to several questions and would have a bell-shaped distribution over a known interval (see Funder (2016) for a good introduction to personality psychology). Let’s assume that the relevant interval is 0 to 10 in our data and let’s look at the distribution of our variables<a contenteditable="false" data-type="indexterm" data-primary="histograms" data-secondary="personality traits" id="idm45968158631016"/> (<a data-type="xref" href="#histograms_of_personality_traits_in_our">Figure 6-21</a>).</p>
<figure><div id="histograms_of_personality_traits_in_our" class="figure">
<img src="Images/BEDA_0621.png" alt="Histograms of personality traits in our data" width="1916" height="906"/>
<h6><span class="label">Figure 6-21. </span>Histograms of personality traits in our data</h6>
</div></figure>
<p>Clearly, something is going on with <em>Neuroticism</em>. Based on how the personality traits are constructed, we would expect the same type of curve for all three variables, and we would certainly not expect to have a large number of customers with a value of 5, and none with a value of 4. This overwhelmingly suggests a deterministically MNAR variable, which we’ll have to handle accordingly.<a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch06-prdet" id="idm45968158625400"/></p>
<p>You should now be able to form a reasonable opinion of the pattern of missingness in a data set. How many missing values are there? Does their missingness appear related to the values of the variable itself (MNAR), another variable (MAR), or neither (MCAR)? Are these missingness relationships probabilistic or deterministic?</p>
<p><a data-type="xref" href="#decision_tree_to_diagnose_missing_data">Figure 6-22</a> <a contenteditable="false" data-type="indexterm" data-primary="missing data" data-secondary="decision tree for diagnosing" id="idm45968158621960"/>presents a decision tree recapping our logic to diagnose missing data.</p>
<figure><div id="decision_tree_to_diagnose_missing_data" class="figure">
<img src="Images/BEDA_0622.png" alt="Decision tree to diagnose missing data" width="1451" height="1273"/>
<h6><span class="label">Figure 6-22. </span>Decision tree to diagnose missing data</h6>
</div></figure>
<p>In the next section, we’ll see how to deal with missing data in each of these cases.</p>
</div></section>
</div></section>
<section data-type="sect1" data-pdf-bookmark="Handling Missing Data"><div class="sect1" id="handling_missing_data-id00045">
<h1>Handling Missing Data</h1>
<p>The first thing to keep in mind as we get into the how-to section<a contenteditable="false" data-type="indexterm" data-primary="missing data" data-secondary="fixing missing data" data-tertiary="about" id="idm45968158616440"/> of the chapter is that we’re not trying to deal with missing data for the sake of it: our goal is to obtain unbiased and accurate estimates of causal relationships in our data. Missing data is problematic only to the extent that it interferes with that goal.</p>
<p>This bears emphasizing, because your first instinct might be that the outcome of successfully addressing missing data is a data set with no missing data, and that’s just not the case. The method we’ll be using, multiple imputation (MI), creates multiple copies of your data, each one with its own imputed values. In layman’s terms, we will never be saying “the correct replacement for Bob’s missing age is 42” but rather “Bob might be 42, 38, 44, 42, or 38 years old.” There is no single best guess, but instead a distribution of possibilities. Another best-practice approach, maximum likelihood estimation, doesn’t even make any guess for individual values and only deals with higher-order coefficients such as means and covariances.</p>
<p>In the next subsection, I will give you a high-level overview of the MI approach. After that, we’ll dive into more detailed algorithm specifications for the model:</p>
<ol>
<li><p>First, the predictive mean matching algorithm</p></li>
<li><p>Then, the normal algorithm</p></li>
<li><p>Finally, how to add auxiliary variables to an algorithm</p></li>
</ol>
<p>Unfortunately, there isn’t a one-to-one relationship <a contenteditable="false" data-type="indexterm" data-primary="missing data" data-secondary="Rubin’s classification of causes" data-tertiary="fixes aren’t recipes per classification" id="idm45968158609992"/>between the type of missingness in Rubin’s classifications and the appropriate algorithm specification, as the amount of information available also matters (<a data-type="xref" href="#optimal_mi_parameters_based_on_type_of">Table 6-4</a>).</p>
<table class="border" id="optimal_mi_parameters_based_on_type_of">
<caption><span class="label">Table 6-4. </span>Optimal MI parameters based on type of missingness and information available</caption>
<thead>
<tr>
<th>Missingness type</th>
<th>No info</th>
<th>Distribution of variable is normal</th>
<th>Distribution of missingness is deterministic</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>MCAR</strong></td>
<td>Mean matching</td>
<td>Normal</td>
<td>(not possible)</td>
</tr>
<tr>
<td><strong>MAR</strong></td>
<td>Mean matching</td>
<td>Normal</td>
<td>Normal + auxiliary variables</td>
</tr>
<tr>
<td><strong>MNAR</strong></td>
<td>Mean matching + auxiliary variables</td>
<td>Normal + auxiliary variables</td>
<td>Normal + auxiliary variables</td>
</tr>
</tbody>
</table>
<section data-type="sect2" data-pdf-bookmark="Introduction to Multiple Imputation (MI)"><div class="sect2" id="introduction_to_multiple_imputation_lef">
<h2>Introduction to Multiple Imputation (MI)</h2>
<p>To understand how multiple imputation works, it helps to <a contenteditable="false" data-type="indexterm" data-primary="missing data" data-secondary="fixing missing data" data-tertiary="multiple imputation" id="ch06-miin"/><a contenteditable="false" data-type="indexterm" data-primary="multiple imputation (MI)" data-secondary="introduction" id="ch06-miin2"/>contrast it with the traditional approaches to missing data. Apart from simply dropping all rows with missing values, traditional approaches all rely on replacing missing values with a specific value. The replacement value may be the overall mean of the variable, or the predicted values based on the other variables available for that customer. <a contenteditable="false" data-type="indexterm" data-primary="uncertainty" data-secondary="missing data introducing" id="idm45968158566312"/><a contenteditable="false" data-type="indexterm" data-primary="biases in data and analyses" data-secondary="missing data handling introducing" id="idm45968158564920"/><a contenteditable="false" data-type="indexterm" data-primary="missing data" data-secondary="biases in data" id="idm45968158563512"/>Regardless of the rule used for the replacement value, these approaches are fundamentally flawed because they ignore the additional uncertainty introduced by the presence of missing data, and they may introduce bias in our analyses.</p>
<p>The MI solution to this problem is, as its name indicates, to build multiple data sets where missing values are replaced by different values, then run our analysis of interest with each one of them, and finally aggregate the resulting coefficients.</p>
<p>In both R and Python, this whole process is managed behind the scenes, and if you want to keep it simple, you can just specify the data and analyses you want to run.</p>
<p>Let’s look first at the R code:<a contenteditable="false" data-type="indexterm" data-primary="multiple imputation (MI)" data-secondary="R code" id="idm45968158560056"/><a contenteditable="false" data-type="indexterm" data-primary="R" data-secondary="multiple imputation" id="idm45968158558616"/></p>
<pre data-type="programlisting" data-code-language="r"><code class="c1">## R</code>
<code class="o">&gt;</code> <code class="n">MI_data</code> <code class="o">&lt;-</code> <code class="nf">mice</code><code class="p">(</code><code class="n">available_data</code><code class="p">,</code> <code class="n">print</code> <code class="o">=</code> <code class="kc">FALSE</code><code class="p">)</code>
<code class="o">&gt;</code> <code class="n">MI_summ</code> <code class="o">&lt;-</code> <code class="n">MI_data</code>  <code class="o">%&gt;%</code>
    <code class="nf">with</code><code class="p">(</code><code class="nf">lm</code><code class="p">(</code><code class="n">bkg_amt</code><code class="o">~</code><code class="n">age</code><code class="o">+</code><code class="n">open</code><code class="o">+</code><code class="n">extra</code><code class="o">+</code><code class="n">neuro</code><code class="o">+</code><code class="n">gender</code><code class="o">+</code><code class="n">state</code><code class="p">))</code> <code class="o">%&gt;%</code>
    <code class="nf">pool</code><code class="p">()</code> <code class="o">%&gt;%</code>
    <code class="nf">summary</code><code class="p">()</code>
<code class="o">&gt;</code> <code class="nf">print</code><code class="p">(</code><code class="n">MI_summ</code><code class="p">)</code>
         <code class="n">term</code>   <code class="n">estimate</code>  <code class="n">std.error</code> <code class="n">statistic</code>        <code class="n">df</code>      <code class="n">p.value</code>
<code class="m">1</code> <code class="p">(</code><code class="n">Intercept</code><code class="p">)</code> <code class="m">240.990671</code> <code class="m">15.9971117</code> <code class="m">15.064636</code>  <code class="m">22.51173</code> <code class="m">3.033129e-13</code>
<code class="m">2</code>         <code class="n">age</code>  <code class="m">-1.051678</code>  <code class="m">0.2267569</code> <code class="m">-4.637912</code>  <code class="m">11.61047</code> <code class="m">6.238993e-04</code>
<code class="m">3</code>        <code class="n">open</code>   <code class="m">3.131074</code>  <code class="m">0.8811587</code>  <code class="m">3.553360</code> <code class="m">140.26375</code> <code class="m">5.186727e-04</code>
<code class="m">4</code>       <code class="n">extra</code>  <code class="m">11.621288</code>  <code class="m">1.2787856</code>  <code class="m">9.087753</code>  <code class="m">10.58035</code> <code class="m">2.531137e-06</code>
<code class="m">5</code>       <code class="n">neuro</code>  <code class="m">-6.799830</code>  <code class="m">1.9339658</code> <code class="m">-3.516003</code>  <code class="m">15.73106</code> <code class="m">2.929145e-03</code>
<code class="m">6</code>     <code class="n">genderF</code> <code class="m">-11.409747</code>  <code class="m">4.2044368</code> <code class="m">-2.713740</code>  <code class="m">20.73345</code> <code class="m">1.310002e-02</code>
<code class="m">7</code>      <code class="n">stateB</code>  <code class="m">-9.063281</code>  <code class="m">4.0018260</code> <code class="m">-2.264786</code> <code class="m">432.54286</code> <code class="m">2.401986e-02</code>
<code class="m">8</code>      <code class="n">stateC</code>  <code class="m">-5.334055</code>  <code class="m">4.7478347</code> <code class="m">-1.123471</code>  <code class="m">42.72826</code> <code class="m">2.675102e-01</code></pre>
<p>The <code>mice</code> package (Multiple Imputation by Chained Equations)<a contenteditable="false" data-type="indexterm" data-primary="packages" data-secondary="multiple imputation" id="idm45968158553848"/> has the <code>mice()</code> function, which generates the multiple data sets. We then apply our regression of interest to each one of them by using the keyword <code>with()</code>. Finally, the <code>pool()</code> function from <code>mice</code> aggregates the results in a format that we can read with the traditional <span class="keep-together"><code>summary()</code></span> function.</p>
<p>The Python code is almost identical, because it implements the same approach:<a contenteditable="false" data-type="indexterm" data-primary="multiple imputation (MI)" data-secondary="Python code" id="idm45968158436424"/><a contenteditable="false" data-type="indexterm" data-primary="Python" data-secondary="multiple imputation" id="idm45968158435080"/></p>
<pre data-type="programlisting" data-code-language="python"><code class="c1">## Python            </code>
<code class="n">MI_data_df</code> <code class="o">=</code> <code class="n">mice</code><code class="o">.</code><code class="n">MICEData</code><code class="p">(</code><code class="n">available_data_df</code><code class="p">)</code>                                 
<code class="n">fit</code> <code class="o">=</code> <code class="n">mice</code><code class="o">.</code><code class="n">MICE</code><code class="p">(</code><code class="n">model_formula</code><code class="o">=</code><code class="s1">'bkg_amt~age+open+extra+neuro+gender+state'</code><code class="p">,</code> 
                <code class="n">model_class</code><code class="o">=</code><code class="n">sm</code><code class="o">.</code><code class="n">OLS</code><code class="p">,</code> <code class="n">data</code><code class="o">=</code><code class="n">MI_data_df</code><code class="p">)</code>                
<code class="n">MI_summ</code> <code class="o">=</code> <code class="n">fit</code><code class="o">.</code><code class="n">fit</code><code class="p">()</code><code class="o">.</code><code class="n">summary</code><code class="p">()</code>                                                       
<code class="k">print</code><code class="p">(</code><code class="n">MI_summ</code><code class="p">)</code>

                          <code class="n">Results</code><code class="p">:</code> <code class="n">MICE</code>
<code class="o">===================================================================</code>
<code class="n">Method</code><code class="p">:</code>                  <code class="n">MICE</code>         <code class="n">Sample</code> <code class="n">size</code><code class="p">:</code>          <code class="mi">2000</code>   
<code class="n">Model</code><code class="p">:</code>                   <code class="n">OLS</code>          <code class="n">Scale</code>                 <code class="mf">5017.30</code>
<code class="n">Dependent</code> <code class="n">variable</code><code class="p">:</code>      <code class="n">bkg_amt</code>      <code class="n">Num</code><code class="o">.</code> <code class="n">imputations</code>      <code class="mi">20</code>     
<code class="o">-------------------------------------------------------------------</code>
           <code class="n">Coef</code><code class="o">.</code>   <code class="n">Std</code><code class="o">.</code><code class="n">Err</code><code class="o">.</code>    <code class="n">t</code>    <code class="n">P</code><code class="o">&gt;|</code><code class="n">t</code><code class="o">|</code>   <code class="p">[</code><code class="mf">0.025</code>   <code class="mf">0.975</code><code class="p">]</code>   <code class="n">FMI</code>  
<code class="o">-------------------------------------------------------------------</code>
<code class="n">Intercept</code> <code class="mf">120.3570</code>   <code class="mf">8.8662</code> <code class="mf">13.5748</code> <code class="mf">0.0000</code> <code class="mf">102.9795</code> <code class="mf">137.7344</code> <code class="mf">0.4712</code>
<code class="n">age</code>        <code class="o">-</code><code class="mf">1.1318</code>   <code class="mf">0.1726</code> <code class="o">-</code><code class="mf">6.5555</code> <code class="mf">0.0000</code>  <code class="o">-</code><code class="mf">1.4702</code>  <code class="o">-</code><code class="mf">0.7934</code> <code class="mf">0.2689</code>
<code class="nb">open</code>        <code class="mf">3.1316</code>   <code class="mf">0.8923</code>  <code class="mf">3.5098</code> <code class="mf">0.0004</code>   <code class="mf">1.3828</code>   <code class="mf">4.8804</code> <code class="mf">0.1723</code>
<code class="n">extra</code>      <code class="mf">11.1265</code>   <code class="mf">1.0238</code> <code class="mf">10.8680</code> <code class="mf">0.0000</code>   <code class="mf">9.1200</code>  <code class="mf">13.1331</code> <code class="mf">0.3855</code>
<code class="n">neuro</code>      <code class="o">-</code><code class="mf">4.5894</code>   <code class="mf">1.7968</code> <code class="o">-</code><code class="mf">2.5542</code> <code class="mf">0.0106</code>  <code class="o">-</code><code class="mf">8.1111</code>  <code class="o">-</code><code class="mf">1.0677</code> <code class="mf">0.4219</code>
<code class="n">gender_M</code>   <code class="mf">65.9603</code>   <code class="mf">4.8191</code> <code class="mf">13.6873</code> <code class="mf">0.0000</code>  <code class="mf">56.5151</code>  <code class="mf">75.4055</code> <code class="mf">0.4397</code>
<code class="n">gender_F</code>   <code class="mf">54.3966</code>   <code class="mf">4.6824</code> <code class="mf">11.6171</code> <code class="mf">0.0000</code>  <code class="mf">45.2192</code>  <code class="mf">63.5741</code> <code class="mf">0.4154</code>
<code class="n">state_A</code>    <code class="mf">40.9352</code>   <code class="mf">3.9080</code> <code class="mf">10.4748</code> <code class="mf">0.0000</code>  <code class="mf">33.2757</code>  <code class="mf">48.5946</code> <code class="mf">0.3921</code>
<code class="n">state_B</code>    <code class="mf">37.3490</code>   <code class="mf">4.0727</code>  <code class="mf">9.1706</code> <code class="mf">0.0000</code>  <code class="mf">29.3666</code>  <code class="mf">45.3313</code> <code class="mf">0.2904</code>
<code class="n">state_C</code>    <code class="mf">42.0728</code>   <code class="mf">3.8643</code> <code class="mf">10.8875</code> <code class="mf">0.0000</code>  <code class="mf">34.4989</code>  <code class="mf">49.6468</code> <code class="mf">0.2298</code>
<code class="o">===================================================================</code></pre>
<p>Here, the <code>mice</code> algorithm is imported from the <code>statsmodels.imputation</code> package. The <code>MICEData()</code> function generates the multiple data sets. We then indicate through the <code>MICE()</code> function the model formula, regression type (here, <code>statsmodels.OLS</code>), and data we want to use. We fit our model with the <code>.fit()</code> and <code>.summary()</code> methods before printing the outcome.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>One complication with the Python implementation of <code>mice</code> is that it doesn’t accommodate <a contenteditable="false" data-type="indexterm" data-primary="multiple imputation (MI)" data-secondary="Python code" data-tertiary="categorical variables" id="idm45968158188024"/><a contenteditable="false" data-type="indexterm" data-primary="categorical variables" data-secondary="multiple imputation in Python" id="idm45968158186360"/>categorical variables as predictors. If you really want to use Python nonetheless, you’ll have to one-hot encode categorical variables first. The following code shows how to do it for the <em>Gender</em> variable:</p>
<pre data-type="programlisting" data-code-language="python"><code class="c1">## Python</code>
<code class="n">gender_dummies</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">get_dummies</code><code class="p">(</code><code class="n">available_data_df</code><code class="o">.</code>\
                                <code class="n">gender</code><code class="p">,</code> 
                                <code class="n">prefix</code><code class="o">=</code><code class="s1">'gender'</code><code class="p">)</code>
<code class="n">available_data_df</code> <code class="o">=</code>  <code class="n">pd</code><code class="o">.</code><code class="n">concat</code><code class="p">([</code><code class="n">available_data_df</code><code class="p">,</code> 
                                <code class="n">gender_dummies</code><code class="p">],</code> 
                               <code class="n">axis</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>
<code class="n">available_data_df</code><code class="o">.</code><code class="n">gender_F</code> <code class="o">=</code> \
<code class="n">np</code><code class="o">.</code><code class="n">where</code><code class="p">(</code><code class="n">available_data_df</code><code class="o">.</code><code class="n">gender</code><code class="o">.</code><code class="n">isna</code><code class="p">(),</code> 
         <code class="nb">float</code><code class="p">(</code><code class="s1">'NaN'</code><code class="p">),</code> <code class="n">available_data_df</code><code class="o">.</code><code class="n">gender_F</code><code class="p">)</code>
<code class="n">available_data_df</code><code class="o">.</code><code class="n">gender_M</code> <code class="o">=</code> \
<code class="n">np</code><code class="o">.</code><code class="n">where</code><code class="p">(</code><code class="n">available_data_df</code><code class="o">.</code><code class="n">gender</code><code class="o">.</code><code class="n">isna</code><code class="p">(),</code> 
         <code class="nb">float</code><code class="p">(</code><code class="s1">'NaN'</code><code class="p">),</code> <code class="n">available_data_df</code><code class="o">.</code><code class="n">gender_M</code><code class="p">)</code>
<code class="n">available_data_df</code> <code class="o">=</code>  <code class="n">available_data_df</code><code class="o">.</code>\
<code class="n">drop</code><code class="p">([</code><code class="s1">'gender'</code><code class="p">],</code> <code class="n">axis</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code></pre>
<p>First, we use the <code>get_dummies()</code> function from pandas to create variables <code>gender_F</code> and <code>gender_M</code>. After adding these columns to our dataframe, we indicate where the missing values are (by default, the one-hot encoding function sets all binary variables to 0 when the value for the categorical variable is missing). Finally, we drop our original categorical variable from our data and fit our model with the new variables included.</p>
<p>However, one-hot encoding breaks some of the internal structure of the data by removing the logical connections between variables, so your mileage may vary (e.g., you can see that the coefficients for the categorical variables are different between R and Python because of the different structures) and I would encourage you to use R instead if categorical variables play an important role in your data.</p>
</div>
<p>Voilà! If you were to stop reading this chapter right now, you would have a solution to handle missing data that
 would be significantly better than the traditional approaches. However, we can do even better by taking the time to lift the hood and better understand the imputation algorithms.<a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch06-miin" id="idm45968157922504"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch06-miin2" id="idm45968158139560"/></p>
</div></section>
<section data-type="sect2" class="pagebreak-before" data-pdf-bookmark="Default Imputation Method: Predictive Mean Matching"><div class="sect2" id="default_imputation_method_predictive_me">
<h2 class="less_space">Default Imputation Method: Predictive Mean Matching</h2>
<p>In the previous subsection, we left the imputation method unspecified and relied on <code>mice</code>’s defaults. <a contenteditable="false" data-type="indexterm" data-primary="missing data" data-secondary="fixing missing data" data-tertiary="MI predictive mean matching" id="idm45968158135672"/><a contenteditable="false" data-type="indexterm" data-primary="multiple imputation (MI)" data-secondary="Python code" data-tertiary="predictive mean matching only" id="idm45968158134024"/><a contenteditable="false" data-type="indexterm" data-primary="Python" data-secondary="multiple imputation" data-tertiary="predictive mean matching only" id="idm45968158132344"/><a contenteditable="false" data-type="indexterm" data-primary="predictive mean matching (PMM)" data-secondary="Python PMM only imputation method" id="idm45968158130680"/>In Python, the only imputation method available is predictive mean matching, so there’s nothing to do there. Let’s check what the default imputation methods are in R by asking for a summary of the imputation process:<a contenteditable="false" data-type="indexterm" data-primary="multiple imputation (MI)" data-secondary="R code" data-tertiary="default imputation methods" id="idm45968158128904"/><a contenteditable="false" data-type="indexterm" data-primary="R" data-secondary="multiple imputation" data-tertiary="default imputation methods" id="idm45968158127224"/></p>
<pre data-type="programlisting" data-code-language="r"><code class="c1">## R</code>
<code class="o">&gt;</code> <code class="nf">summary</code><code class="p">(</code><code class="n">MI_data</code><code class="p">)</code>
<code class="n">Class</code><code class="o">:</code> <code class="n">mids</code>
<code class="n">Number</code> <code class="n">of</code> <code class="n">multiple</code> <code class="n">imputations</code><code class="o">:</code>  <code class="m">5</code> 
<code class="n">Imputation</code> <code class="n">methods</code><code class="o">:</code>
     <code class="n">age</code>     <code class="n">open</code>    <code class="n">extra</code>    <code class="n">neuro</code>   <code class="n">gender</code>    <code class="n">state</code>  <code class="n">bkg_amt</code> 
      <code class="s">""</code>       <code class="s">""</code>    <code class="s">"pmm"</code>    <code class="s">"pmm"</code>       <code class="s">""</code> <code class="s">"logreg"</code>    <code class="s">"pmm"</code> 
<code class="n">PredictorMatrix</code><code class="o">:</code>
       <code class="n">age</code> <code class="n">open</code> <code class="n">extra</code> <code class="n">neuro</code> <code class="n">gender</code> <code class="n">state</code> <code class="n">bkg_amt</code>
<code class="n">age</code>      <code class="m">0</code>    <code class="m">1</code>     <code class="m">1</code>     <code class="m">1</code>      <code class="m">1</code>     <code class="m">1</code>       <code class="m">1</code>
<code class="n">open</code>     <code class="m">1</code>    <code class="m">0</code>     <code class="m">1</code>     <code class="m">1</code>      <code class="m">1</code>     <code class="m">1</code>       <code class="m">1</code>
<code class="n">extra</code>    <code class="m">1</code>    <code class="m">1</code>     <code class="m">0</code>     <code class="m">1</code>      <code class="m">1</code>     <code class="m">1</code>       <code class="m">1</code>
<code class="n">neuro</code>    <code class="m">1</code>    <code class="m">1</code>     <code class="m">1</code>     <code class="m">0</code>      <code class="m">1</code>     <code class="m">1</code>       <code class="m">1</code>
<code class="n">gender</code>   <code class="m">1</code>    <code class="m">1</code>     <code class="m">1</code>     <code class="m">1</code>      <code class="m">0</code>     <code class="m">1</code>       <code class="m">1</code>
<code class="n">state</code>    <code class="m">1</code>    <code class="m">1</code>     <code class="m">1</code>     <code class="m">1</code>      <code class="m">1</code>     <code class="m">0</code>       <code class="m">1</code></pre>
<p>That’s a lot of information. For now, let’s look only at the <code>Imputation methods</code> line. Variables that don’t have missing data have an empty field <code>""</code>, which makes sense because they don’t get imputed. Categorical variables have the method <code>logreg</code>, i.e., logistic regression. <a contenteditable="false" data-type="indexterm" data-primary="predictive mean matching (PMM)" data-secondary="R pmm method" id="idm45968158121752"/>Finally, numeric variables have the method <code>pmm</code>, which stands for predictive mean matching (PMM). The <code>pmm</code> method works by selecting the closest neighbors of the individual with the missing value and replacing the missing value with the value of one of the neighbors. Imagine, for example, a data set with only two variables: <em>Age</em> and <em>ZipCode</em>. If you have a customer from zip code 60612 with a missing age, the algorithm will pick at random the age of another customer in the same zip code, or as close as possible.</p>
<p>Because of some randomness baked in the process, each of the imputed data sets will end up with slightly different values, as we can visualize through the convenient <code>densityplot</code><code>()</code> function from the <code>mice</code> package in R:</p>
<pre data-type="programlisting" data-code-language="r"><code class="c1">## R </code>
<code class="o">&gt;</code> <code class="nf">densityplot</code><code class="p">(</code><code class="n">MI_data</code><code class="p">,</code> <code class="n">thicker</code> <code class="o">=</code> <code class="m">3</code><code class="p">,</code> <code class="n">lty</code> <code class="o">=</code> <code class="nf">c</code><code class="p">(</code><code class="m">1</code><code class="p">,</code><code class="nf">rep</code><code class="p">(</code><code class="m">2</code><code class="p">,</code><code class="m">5</code><code class="p">)))</code></pre>
<p><a data-type="xref" href="#distributions_of_imputed_values_for_num">Figure 6-23</a> shows the distributions of the numeric variables in the original available data (thick line) and in the imputed data sets (thin dashed lines). As you can see, the distributions stick pretty close to the original data; the exception is <em>BookingAmount</em>, which is overall more concentrated around the mean (i.e., “higher peaks”) in the imputed data set than in the original data.</p>
<figure><div id="distributions_of_imputed_values_for_num" class="figure">
<img src="Images/BEDA_0623.png" alt="Distributions of imputed values for numeric variables in our data" width="1907" height="805"/>
<h6><span class="label">Figure 6-23. </span>Distributions of imputed values for numeric variables in our data</h6>
</div></figure>
<p>PMM has some important properties that may or may not be<a contenteditable="false" data-type="indexterm" data-primary="multiple imputation (MI)" data-secondary="predictive mean matchings" id="idm45968157844600"/><a contenteditable="false" data-type="indexterm" data-primary="predictive mean matching (PMM)" id="idm45968157843288"/> desirable, depending on the context. The most important property is that it’s basically an interpolation method. Therefore, you can picture PMM as creating values that are between existing values. By doing so, it minimizes the risk of creating nonsensical situations such as pregnant fathers or negative amounts. This approach will also work well when a variable has a weird distribution, such as <em>Age</em> in our data, which has two peaks, because it makes no assumptions about the shape of the overall distribution; it just grabs a neighbor.</p>
<p>There are several downsides to PMM, however: it’s slow and doesn’t scale well to large data sets, because it must constantly recalculate distances between individuals. In addition, when you have many variables or a lot of missing values, the closest neighbors may be “far away,” and the quality of the imputation will deteriorate. This is why PMM will not be our preferred option when we have distributional information, as we’ll see in the next subsection.</p>
</div></section>
<section data-type="sect2" data-pdf-bookmark="From PMM to Normal Imputation (R Only)"><div class="sect2" id="from_pmm_to_normal_imputation_left_pare">
<h2>From PMM to Normal Imputation (R Only)</h2>
<p>While PMM is a decent starting point, we often have <a contenteditable="false" data-type="indexterm" data-primary="multiple imputation (MI)" data-secondary="R code" data-tertiary="normal imputation" id="idm45968157838072"/><a contenteditable="false" data-type="indexterm" data-primary="missing data" data-secondary="fixing missing data" data-tertiary="normal imputation" id="idm45968157836408"/><a contenteditable="false" data-type="indexterm" data-primary="multiple imputation (MI)" data-secondary="normal imputation" id="idm45968157834760"/>information about the distribution of numeric variables that we can leverage to speed up and improve our imputation models in R. In particular, behavioral and natural sciences often assume that numeric variables follow a normal distribution, because it is very common. When that’s the case, we can fit a normal distribution to a variable and then draw imputation values from that distribution instead of using PMM. This is done by creating a vector of imputation methods, with the value <code>"norm.nob"</code> for the variables for which we’ll assume normality, and then passing that vector to the <code>parameter</code> method of the <code>mice()</code> function:</p>
<pre data-type="programlisting" data-code-language="r"><code class="c1">## R</code>
<code class="o">&gt;</code> <code class="n">imp_meth_dist</code> <code class="o">&lt;-</code> <code class="nf">c</code><code class="p">(</code><code class="s">"pmm"</code><code class="p">,</code> <code class="nf">rep</code><code class="p">(</code><code class="s">"norm.nob"</code><code class="p">,</code><code class="m">3</code><code class="p">),</code> <code class="s">""</code><code class="p">,</code> <code class="s">"logreg"</code><code class="p">,</code> <code class="s">"norm.nob"</code><code class="p">)</code>
<code class="o">&gt;</code> <code class="n">MI_data_dist</code> <code class="o">&lt;-</code> <code class="nf">mice</code><code class="p">(</code><code class="n">available_data</code><code class="p">,</code> <code class="n">print</code> <code class="o">=</code> <code class="kc">FALSE</code><code class="p">,</code> <code class="n">method</code> <code class="o">=</code> <code class="n">imp_meth_dist</code><code class="p">)</code></pre>
<p>As you can see, the syntax is very simple. The only question is to determine for which of the numeric variables we want to use a normal imputation. Let’s look at the numeric variables in our available data (<a data-type="xref" href="#distribution_of_numeric_variables_in_ou">Figure 6-24</a>).</p>
<figure><div id="distribution_of_numeric_variables_in_ou" class="figure">
<img src="Images/BEDA_0624.png" alt="Distribution of numeric variables in our data" width="1934" height="910"/>
<h6><span class="label">Figure 6-24. </span>Distribution of numeric variables in our data</h6>
</div></figure>
<p><em>Age</em> is obviously not normal with its two peaks, but all the other variables have only one peak. <em>Openness</em>, <em>Extraversion,</em> and <em>BookingAmount</em> also appear reasonably symmetrical (in technical terms, they’re not skewed). Statistical simulations show that as long as a variable is one-peaked and doesn’t have a “fat tail” in only one direction, assuming normality does not introduce bias. Therefore, we can assume normality for <em>Openness</em>, <em>Extraversion,</em> and <em>BookingAmount</em>.</p>
<p>As we saw in the previous section, <em>Neuroticism</em> presents an unusual asymmetrical pattern: values are restricted to [5,10] even though the psychological scale we’re using goes from 0 to 10, which suggests that <em>Neuroticism</em> might be “deterministically” MNAR, i.e., all values of <em>Neuroticism</em> below a certain threshold are missing. Using PMM for imputation is problematic in such a situation: there are only a few or no neighbors for imputation on a significant range of values. At the extreme, all the missing values of X would be imputed as 5, the value of the threshold. This is a situation where the normal method will be better able to recover the true missing values. We can see that by comparing the values imputed by the two methods. <a data-type="xref" href="#pmm_imputation_left_parenthesistopright">Figure 6-25</a> shows the available values of <em>Neuroticism</em> (squares) and the values imputed with the PMM method (crosses, top panel) and with the normal method (crosses, bottom panel).</p>
<figure><div id="pmm_imputation_left_parenthesistopright" class="figure">
<img src="Images/BEDA_0625.png" alt="PMM imputation (top) and normal imputation (bottom) with a deterministically MNAR variable" width="1912" height="904"/>
<h6><span class="label">Figure 6-25. </span>PMM imputation (top) and normal imputation (bottom) with a deterministically MNAR variable</h6>
</div></figure>
<p>As you can see, the PMM method doesn’t impute any value for <em>Neuroticism</em> below 5, whereas the normal method does. In addition, the PMM method imputes too many values close to 10, whereas the normal method more adequately captures the overall shape of the distribution. Still, the normal method is far from recovering the true distribution (which goes all the way to zero). This is a common problem for variables that are deterministically MAR or MNAR. We can further improve on the normal imputation by using auxiliary variables, as we’ll see in the next subsection.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="robust_imputation">
<h5>Robust Imputation</h5>
<p>If you have variables that are one-peaked but not <a contenteditable="false" data-type="indexterm" data-primary="multiple imputation (MI)" data-secondary="robust imputation" id="idm45968157697464"/><a contenteditable="false" data-type="indexterm" data-primary="multiple imputation (MI)" data-secondary="R code" data-tertiary="ImputeRobust" id="idm45968157696152"/><a contenteditable="false" data-type="indexterm" data-primary="missing data" data-secondary="fixing missing data" data-tertiary="robust imputation" id="idm45968157694488"/>normal (e.g., at least one tail of the distribution is unusually fat or thin compared to the normal distribution) and you care about the extra accuracy, the R package <code>ImputeRobust</code> builds on the <code>mice</code> package to refine your imputation model. See the <a href="https://oreil.ly/5quMo">CRAN vignette</a> for more details.</p>
</div></aside>
</div></section>
<section data-type="sect2" data-pdf-bookmark="Adding Auxiliary Variables"><div class="sect2" id="adding_auxiliary_variables">
<h2>Adding Auxiliary Variables</h2>
<p>Quite often, we’ll have variables that are correlated with one of<a contenteditable="false" data-type="indexterm" data-primary="missing data" data-secondary="fixing missing data" data-tertiary="adding auxiliary variables" id="idm45968157688984"/><a contenteditable="false" data-type="indexterm" data-primary="multiple imputation (MI)" data-secondary="adding auxiliary variables" id="idm45968157687320"/> our variables with missing data (e.g., causes or effects of that variable) but don’t belong in our regression model. This is a situation where the <code>mice</code> algorithm especially shines, because we can add these variables to our imputation model to increase its accuracy. They are then referred to as the “auxiliary variables” of our imputation model.</p>
<p>For our AirCnC example, the supplementary available data set contains two variables, <em>Insurance</em> and <em>Active</em>. The former indicates the amount of travel insurance bought by the customer and is strongly correlated with <em>Neuroticism</em>, while the latter measures the degree to which the customer has picked active vacations (e.g., rock climbing) and is strongly correlated with <em>Extraversion</em>. We’ll use them to help impute the two personality variables.</p>
<p>Adding auxiliary variables to the imputation model is extremely simple: we simply need to add them to our data set before the imputation phase:</p>
<pre data-type="programlisting" data-code-language="r"><code class="c1">## R</code>
<code class="n">augmented_data</code> <code class="o">&lt;-</code> <code class="nf">cbind</code><code class="p">(</code><code class="n">available_data</code><code class="p">,</code> <code class="n">available_data_supp</code><code class="p">)</code>
<code class="n">MI_data_aux</code> <code class="o">&lt;-</code> <code class="nf">mice</code><code class="p">(</code><code class="n">augmented_data</code><code class="p">,</code> <code class="n">print</code> <code class="o">=</code> <code class="kc">FALSE</code><code class="p">)</code>
</pre>
<pre data-type="programlisting" data-code-language="python">
<code class="c1">## Python</code>
<code class="n">augmented_data_df</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">concat</code><code class="p">([</code><code class="n">available_data_df</code><code class="p">,</code> <code class="n">available_data_supp_df</code><code class="p">],</code> 
                              <code class="n">axis</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>
<code class="n">MI_data_aux_df</code> <code class="o">=</code> <code class="n">mice</code><code class="o">.</code><code class="n">MICEData</code><code class="p">(</code><code class="n">augmented_data_df</code><code class="p">)</code></pre>
<p>We can then run all our analyses as before. When adding auxiliary variables, it generally makes sense to use the normal method for the variables correlated with our auxiliary variables (here <em>Neuroticism</em> and <em>Extraversion</em>), especially when these variables are truncated or MNAR.</p>
<p>Apart from computation constraints, there are no limits to the number of auxiliary variables we can include. However, a potential risk is that some of our auxiliary variables may misleadingly appear correlated with a variable in our original data set just out of sheer randomness, e.g., <em>Insurance</em> appearing correlated with <em>Extraversion</em> even though it isn’t truly. Such a “false positive” correlation would then be unduly reinforced by the imputation model.</p>
<p>The solution to that potential problem is to restrict auxiliary variables to be used only for the imputation of certain variables. Unfortunately, this solution is available only in R. This is where the predictor matrix of the <code>mice()</code> function comes in. This matrix appears when printing the summary of the imputation phase, and can also be extracted directly from our <code>MIDS</code> object:</p>
<pre data-type="programlisting" data-code-language="r"><code class="c1">## R</code>
<code class="o">&gt;</code> <code class="n">pred_mat</code> <code class="o">&lt;-</code> <code class="n">MI_data_aux</code><code class="o">$</code><code class="n">predictorMatrix</code>
<code class="o">&gt;</code> <code class="n">pred_mat</code>
          <code class="n">age</code> <code class="n">open</code> <code class="n">extra</code> <code class="n">neuro</code> <code class="n">gender</code> <code class="n">state</code> <code class="n">bkg_amt</code> <code class="n">insurance</code> <code class="n">active</code>
<code class="n">age</code>         <code class="m">0</code>    <code class="m">1</code>     <code class="m">1</code>     <code class="m">1</code>      <code class="m">1</code>     <code class="m">1</code>       <code class="m">1</code>         <code class="m">1</code>      <code class="m">1</code>
<code class="n">open</code>        <code class="m">1</code>    <code class="m">0</code>     <code class="m">1</code>     <code class="m">1</code>      <code class="m">1</code>     <code class="m">1</code>       <code class="m">1</code>         <code class="m">1</code>      <code class="m">1</code>
<code class="n">extra</code>       <code class="m">1</code>    <code class="m">1</code>     <code class="m">0</code>     <code class="m">1</code>      <code class="m">1</code>     <code class="m">1</code>       <code class="m">1</code>         <code class="m">1</code>      <code class="m">1</code>
<code class="n">neuro</code>       <code class="m">1</code>    <code class="m">1</code>     <code class="m">1</code>     <code class="m">0</code>      <code class="m">1</code>     <code class="m">1</code>       <code class="m">1</code>         <code class="m">1</code>      <code class="m">1</code>
<code class="n">gender</code>      <code class="m">1</code>    <code class="m">1</code>     <code class="m">1</code>     <code class="m">1</code>      <code class="m">0</code>     <code class="m">1</code>       <code class="m">1</code>         <code class="m">1</code>      <code class="m">1</code>
<code class="n">state</code>       <code class="m">1</code>    <code class="m">1</code>     <code class="m">1</code>     <code class="m">1</code>      <code class="m">1</code>     <code class="m">0</code>       <code class="m">1</code>         <code class="m">1</code>      <code class="m">1</code>
<code class="n">bkg_amt</code>     <code class="m">1</code>    <code class="m">1</code>     <code class="m">1</code>     <code class="m">1</code>      <code class="m">1</code>     <code class="m">1</code>       <code class="m">0</code>         <code class="m">1</code>      <code class="m">1</code>
<code class="n">insurance</code>   <code class="m">1</code>    <code class="m">1</code>     <code class="m">1</code>     <code class="m">1</code>      <code class="m">1</code>     <code class="m">1</code>       <code class="m">1</code>         <code class="m">0</code>      <code class="m">1</code>
<code class="n">active</code>      <code class="m">1</code>    <code class="m">1</code>     <code class="m">1</code>     <code class="m">1</code>      <code class="m">1</code>     <code class="m">1</code>       <code class="m">1</code>         <code class="m">1</code>      <code class="m">0</code></pre>
<p>This matrix indicates which variable is used to impute which. By default, all variables are used to impute all variables except themselves. A “1” in the matrix indicates that the “column” variable is used to impute the “row” variable. We’ll therefore want to modify the last two columns, for <em>Insurance</em> and <em>Active</em>, so that they’ll be used only to impute <em>Neuroticism</em> and <em>Extraversion</em> respectively:</p>
<pre data-type="programlisting" data-code-language="r"><code class="c1">## R</code>
<code class="o">&gt;</code> <code class="n">pred_mat</code><code class="p">[,</code><code class="s">"insurance"</code><code class="p">]</code> <code class="o">&lt;-</code> <code class="m">0</code>
<code class="o">&gt;</code> <code class="n">pred_mat</code><code class="p">[,</code><code class="s">"active"</code><code class="p">]</code> <code class="o">&lt;-</code> <code class="m">0</code>
<code class="o">&gt;</code> <code class="n">pred_mat</code><code class="p">[</code><code class="s">"neuro"</code><code class="p">,</code><code class="s">"insurance"</code><code class="p">]</code> <code class="o">&lt;-</code> <code class="m">1</code>
<code class="o">&gt;</code> <code class="n">pred_mat</code><code class="p">[</code><code class="s">"extra"</code><code class="p">,</code><code class="s">"active"</code><code class="p">]</code> <code class="o">&lt;-</code> <code class="m">1</code>
<code class="o">&gt;</code> <code class="n">pred_mat</code>
          <code class="n">age</code> <code class="n">open</code> <code class="n">extra</code> <code class="n">neuro</code> <code class="n">gender</code> <code class="n">state</code> <code class="n">bkg_amt</code> <code class="n">insurance</code> <code class="n">active</code>
<code class="n">age</code>         <code class="m">0</code>    <code class="m">1</code>     <code class="m">1</code>     <code class="m">1</code>      <code class="m">1</code>     <code class="m">1</code>       <code class="m">1</code>         <code class="m">0</code>      <code class="m">0</code>
<code class="n">open</code>        <code class="m">1</code>    <code class="m">0</code>     <code class="m">1</code>     <code class="m">1</code>      <code class="m">1</code>     <code class="m">1</code>       <code class="m">1</code>         <code class="m">0</code>      <code class="m">0</code>
<code class="n">extra</code>       <code class="m">1</code>    <code class="m">1</code>     <code class="m">0</code>     <code class="m">1</code>      <code class="m">1</code>     <code class="m">1</code>       <code class="m">1</code>         <code class="m">0</code>      <code class="m">1</code>
<code class="n">neuro</code>       <code class="m">1</code>    <code class="m">1</code>     <code class="m">1</code>     <code class="m">0</code>      <code class="m">1</code>     <code class="m">1</code>       <code class="m">1</code>         <code class="m">1</code>      <code class="m">0</code>
<code class="n">gender</code>      <code class="m">1</code>    <code class="m">1</code>     <code class="m">1</code>     <code class="m">1</code>      <code class="m">0</code>     <code class="m">1</code>       <code class="m">1</code>         <code class="m">0</code>      <code class="m">0</code>
<code class="n">state</code>       <code class="m">1</code>    <code class="m">1</code>     <code class="m">1</code>     <code class="m">1</code>      <code class="m">1</code>     <code class="m">0</code>       <code class="m">1</code>         <code class="m">0</code>      <code class="m">0</code>
<code class="n">bkg_amt</code>     <code class="m">1</code>    <code class="m">1</code>     <code class="m">1</code>     <code class="m">1</code>      <code class="m">1</code>     <code class="m">1</code>       <code class="m">0</code>         <code class="m">0</code>      <code class="m">0</code>
<code class="n">insurance</code>   <code class="m">1</code>    <code class="m">1</code>     <code class="m">1</code>     <code class="m">1</code>      <code class="m">1</code>     <code class="m">1</code>       <code class="m">1</code>         <code class="m">0</code>      <code class="m">0</code>
<code class="n">active</code>      <code class="m">1</code>    <code class="m">1</code>     <code class="m">1</code>     <code class="m">1</code>      <code class="m">1</code>     <code class="m">1</code>       <code class="m">1</code>         <code class="m">0</code>      <code class="m">0</code></pre>
<p>With that modification, we’ll reduce the risk of inadvertently baking in fluke correlations into our imputation model.</p>
</div></section>
<section data-type="sect2" data-pdf-bookmark="Scaling Up the Number of Imputed Data Sets"><div class="sect2" id="scaling_up_the_number_of_imputed_datase">
<h2>Scaling Up the Number of Imputed Data Sets</h2>
<p>The default number of imputed data sets created by the<a contenteditable="false" data-type="indexterm" data-primary="multiple imputation (MI)" data-secondary="scaling up number of imputed data sets" id="idm45968157407000"/><a contenteditable="false" data-type="indexterm" data-primary="missing data" data-secondary="fixing missing data" data-tertiary="scaling up number of imputed data sets" id="idm45968157212744"/> <code>mice</code> algorithm is 5 in R and 10 in Python. These are fine defaults for exploratory analyses.</p>
<p>For your final run, you should use 20 (by passing <code>m=20</code> as a parameter setting to the <code>mice()</code> function) if you’re interested only in the estimated values of the regression coefficients. If you want more precise information such as confidence intervals or interactions between variables, you might want to aim for 50 to 100. The main constraints then become the computer’s speed and memory—if your data set is 100 Mb or even 1 Gb do you have the RAM to create a hundred copies of it?—as well as your patience.</p>
<p>The syntax to change the number of imputed data sets is straightforward. In R it is passed as a parameter to the <code>mice()</code> function, while in Python it is passed as a parameter for the <code>.fit()</code> method of the <code>MICE</code> object:</p>
<pre data-type="programlisting" data-code-language="r"><code class="c1">## R</code>
<code class="n">MI_data</code> <code class="o">&lt;-</code> <code class="nf">mice</code><code class="p">(</code><code class="n">available_data</code><code class="p">,</code> <code class="n">print</code> <code class="o">=</code> <code class="kc">FALSE</code><code class="p">,</code> <code class="n">m</code><code class="o">=</code><code class="m">20</code><code class="p">)</code>
</pre>
<pre data-type="programlisting" data-code-language="python">
<code class="c1">## Python</code>
<code class="n">fit</code> <code class="o">=</code> <code class="n">mice</code><code class="o">.</code><code class="n">MICE</code><code class="p">(</code><code class="n">model_formula</code><code class="o">=</code><code class="s1">'bkg_amt~age+open+extra+neuro+gender+state'</code><code class="p">,</code> 
                <code class="n">model_class</code><code class="o">=</code><code class="n">sm</code><code class="o">.</code><code class="n">OLS</code><code class="p">,</code> <code class="n">data</code><code class="o">=</code><code class="n">MI_data_df</code><code class="p">)</code> 
<code class="n">MI_summ</code> <code class="o">=</code> <code class="n">fit</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">n_imputations</code><code class="o">=</code><code class="mi">20</code><code class="p">)</code><code class="o">.</code><code class="n">summary</code><code class="p">()</code></pre>
</div></section>
</div></section>
<section data-type="sect1" data-pdf-bookmark="Conclusion"><div class="sect1" id="conclusion-id00011">
<h1>Conclusion</h1>
<p>Missing data can present a real problem in behavioral data analysis, but it doesn’t have to. At a minimum, using the <code>mice</code> package in R or Python with its default parameters will outperform deleting all rows with missing values. By properly diagnosing missingness based on Rubin’s classification, and leveraging all available information, you can generally do better than that. To recap the decision rules in one place, <a data-type="xref" href="#decision_tree_to_diagnose_missing_datad">Figure 6-26</a> shows the decision tree to diagnose missing data and <a data-type="xref" href="#optimal_imputation_method_based_on_type">Table 6-5</a> the optimal MI parameters based on type of missingness and information available.</p>
<figure><div id="decision_tree_to_diagnose_missing_datad" class="figure">
<img src="Images/BEDA_0626.png" alt="Decision tree to diagnose missing data" width="1451" height="1273"/>
<h6><span class="label">Figure 6-26. </span>Decision tree to diagnose missing data</h6>
</div></figure>
<table class="border" id="optimal_imputation_method_based_on_type">
<caption><span class="label">Table 6-5. </span>Optimal MI parameters based on type of missingness and information available</caption>
<thead>
<tr>
<th>Type of missingness</th>
<th>No info</th>
<th>Variable distribution is normal</th>
<th>Missingness distribution is deterministic</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>MCAR</strong></td>
<td>PMM</td>
<td><code>norm.nob</code></td>
<td> </td>
</tr>
<tr>
<td><strong>MAR</strong></td>
<td>PMM</td>
<td><code>norm.nob</code></td>
<td><code>norm.nob</code> + aux. var.</td>
</tr>
<tr>
<td><strong>MNAR</strong></td>
<td>PMM + aux. var.</td>
<td><code>norm.nob</code> + aux. var.</td>
<td><code>norm.nob</code> + aux. var.</td>
</tr>
</tbody>
</table>
</div></section>
</div></section></div></body></html>