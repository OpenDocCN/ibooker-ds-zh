["```py\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\n# Import points\ndf = pd.read_csv('https://bit.ly/3goOAnt', delimiter=\",\")\n\n# Extract input variables (all rows, all columns but last column)\nX = df.values[:, :-1]\n\n# Extract output column (all rows, last column)\nY = df.values[:, -1]\n\n# Fit a line to the points\nfit = LinearRegression().fit(X, Y)\n\n# m = 1.7867224, b = -16.51923513\nm = fit.coef_.flatten()\nb = fit.intercept_.flatten()\nprint(\"m = {0}\".format(m))\nprint(\"b = {0}\".format(b))\n\n# show in chart\nplt.plot(X, Y, 'o') # scatterplot\nplt.plot(X, m*X+b) # line\nplt.show()\n```", "```py\nimport pandas as pd\n\n# Import points\npoints = pd.read_csv('https://bit.ly/3goOAnt', delimiter=\",\").itertuples()\n\n# Test with a given line\nm = 1.93939\nb = 4.73333\n\n# Calculate the residuals\nfor p in points:\n    y_actual = p.y\n    y_predict = m*p.x + b\n    residual = y_actual - y_predict\n    print(residual)\n```", "```py\n-1.67272\n1.3878900000000005\n-0.5515000000000008\n2.5091099999999997\n-0.4302799999999998\n-1.3696699999999993f\n0.6909400000000012\n-2.2484499999999983\n2.812160000000002\n-1.1272299999999973\n```", "```py\nimport pandas as pd\n\n# Import points\npoints = pd.read_csv(\"https://bit.ly/2KF29Bd\").itertuples()\n\n# Test with a given line\nm = 1.93939\nb = 4.73333\n\nsum_of_squares = 0.0\n\n# calculate sum of squares\nfor p in points:\n    y_actual = p.y\n    y_predict = m*p.x + b\n    residual_squared = (y_predict - y_actual)**2\n    sum_of_squares += residual_squared\n\nprint(\"sum of squares = {}\".format(sum_of_squares))\n# sum of squares = 28.096969704500005\n```", "```py\nimport pandas as pd\n\n# Load the data\npoints = list(pd.read_csv('https://bit.ly/2KF29Bd', delimiter=\",\").itertuples())\n\nn = len(points)\n\nm = (n*sum(p.x*p.y for p in points) - sum(p.x for p in points) *\n    sum(p.y for p in points)) / (n*sum(p.x**2 for p in points) -\n    sum(p.x for p in points)**2)\n\nb = (sum(p.y for p in points) / n) - m * sum(p.x for p in points) / n\n\nprint(m, b)\n# 1.9393939393939394 4.7333333333333325\n```", "```py\nimport pandas as pd\nfrom numpy.linalg import inv\nimport numpy as np\n\n# Import points\ndf = pd.read_csv('https://bit.ly/3goOAnt', delimiter=\",\")\n\n# Extract input variables (all rows, all columns but last column)\nX = df.values[:, :-1].flatten()\n\n# Add placeholder \"1\" column to generate intercept\nX_1 = np.vstack([X, np.ones(len(X))]).T\n\n# Extract output column (all rows, last column)\nY = df.values[:, -1]\n\n# Calculate coefficents for slope and intercept\nb = inv(X_1.transpose() @ X_1) @ (X_1.transpose() @ Y)\nprint(b) # [1.93939394, 4.73333333]\n\n# Predict against the y-values\ny_predict = X_1.dot(b)\n```", "```py\nimport pandas as pd\nfrom numpy.linalg import qr, inv\nimport numpy as np\n\n# Import points\ndf = pd.read_csv('https://bit.ly/3goOAnt', delimiter=\",\")\n\n# Extract input variables (all rows, all columns but last column)\nX = df.values[:, :-1].flatten()\n\n# Add placeholder \"1\" column to generate intercept\nX_1 = np.vstack([X, np.ones(len(X))]).transpose()\n\n# Extract output column (all rows, last column)\nY = df.values[:, -1]\n\n# calculate coefficents for slope and intercept\n# using QR decomposition\nQ, R = qr(X_1)\nb = inv(R).dot(Q.transpose()).dot(Y)\n\nprint(b) # [1.93939394, 4.73333333]\n```", "```py\nimport random\n\ndef f(x):\n    return (x - 3) ** 2 + 4\n\ndef dx_f(x):\n    return 2*(x - 3)\n\n# The learning rate\nL = 0.001\n\n# The number of iterations to perform gradient descent\niterations = 100_000\n\n # start at a random x\nx = random.randint(-15,15)\n\nfor i in range(iterations):\n\n    # get slope\n    d_x = dx_f(x)\n\n    # update x by subtracting the (learning rate) * (slope)\n    x -= L * d_x\n\nprint(x, f(x)) # prints 2.999999999999889 4.0\n```", "```py\nimport pandas as pd\n\n# Import points from CSV\npoints = list(pd.read_csv(\"https://bit.ly/2KF29Bd\").itertuples())\n\n# Building the model\nm = 0.0\nb = 0.0\n\n# The learning Rate\nL = .001\n\n# The number of iterations\niterations = 100_000\n\nn = float(len(points))  # Number of elements in X\n\n# Perform Gradient Descent\nfor i in range(iterations):\n\n    # slope with respect to m\n    D_m = sum(2 * p.x * ((m * p.x + b) - p.y) for p in points)\n\n    # slope with respect to b\n    D_b = sum(2 * ((m * p.x + b) - p.y) for p in points)\n\n    # update m and b\n    m -= L * D_m\n    b -= L * D_b\n\nprint(\"y = {0}x + {1}\".format(m, b))\n# y = 1.9393939393939548x + 4.733333333333227\n```", "```py\nfrom sympy import *\n\nm, b, i, n = symbols('m b i n')\nx, y = symbols('x y', cls=Function)\n\nsum_of_squares = Sum((m*x(i) + b - y(i)) ** 2, (i, 0, n))\n\nd_m = diff(sum_of_squares, m)\nd_b = diff(sum_of_squares, b)\nprint(d_m)\nprint(d_b)\n\n# OUTPUTS\n# Sum(2*(b + m*x(i) - y(i))*x(i), (i, 0, n))\n# Sum(2*b + 2*m*x(i) - 2*y(i), (i, 0, n))\n```", "```py\nimport pandas as pd\nfrom sympy import *\n\n# Import points from CSV\npoints = list(pd.read_csv(\"https://bit.ly/2KF29Bd\").itertuples())\n\nm, b, i, n = symbols('m b i n')\nx, y = symbols('x y', cls=Function)\n\nsum_of_squares = Sum((m*x(i) + b - y(i)) ** 2, (i, 0, n))\n\nd_m = diff(sum_of_squares, m) \\\n    .subs(n, len(points) - 1).doit() \\\n    .replace(x, lambda i: points[i].x) \\\n    .replace(y, lambda i: points[i].y)\n\nd_b = diff(sum_of_squares, b) \\\n    .subs(n, len(points) - 1).doit() \\\n    .replace(x, lambda i: points[i].x) \\\n    .replace(y, lambda i: points[i].y)\n\n# compile using lambdify for faster computation\nd_m = lambdify([m, b], d_m)\nd_b = lambdify([m, b], d_b)\n\n# Building the model\nm = 0.0\nb = 0.0\n\n# The learning Rate\nL = .001\n\n# The number of iterations\niterations = 100_000\n\n# Perform Gradient Descent\nfor i in range(iterations):\n\n    # update m and b\n    m -= d_m(m,b) * L\n    b -= d_b(m,b) * L\n\nprint(\"y = {0}x + {1}\".format(m, b))\n# y = 1.939393939393954x + 4.733333333333231\n```", "```py\nfrom sympy import *\nfrom sympy.plotting import plot3d\nimport pandas as pd\n\npoints = list(pd.read_csv(\"https://bit.ly/2KF29Bd\").itertuples())\nm, b, i, n = symbols('m b i n')\nx, y = symbols('x y', cls=Function)\n\nsum_of_squares = Sum((m*x(i) + b - y(i)) ** 2, (i, 0, n)) \\\n    .subs(n, len(points) - 1).doit() \\\n    .replace(x, lambda i: points[i].x) \\\n    .replace(y, lambda i: points[i].y)\n\nplot3d(sum_of_squares)\n```", "```py\nimport pandas as pd\nimport numpy as np\n\n# Input data\ndata = pd.read_csv('https://bit.ly/2KF29Bd', header=0)\n\nX = data.iloc[:, 0].values\nY = data.iloc[:, 1].values\n\nn = data.shape[0]  # rows\n\n# Building the model\nm = 0.0\nb = 0.0\n\nsample_size = 1  # sample size\nL = .0001  # The learning Rate\nepochs = 1_000_000  # The number of iterations to perform gradient descent\n\n# Performing Stochastic Gradient Descent\nfor i in range(epochs):\n    idx = np.random.choice(n, sample_size, replace=False)\n    x_sample = X[idx]\n    y_sample = Y[idx]\n\n    # The current predicted value of Y\n    Y_pred = m * x_sample + b\n\n    # d/dm derivative of loss function\n    D_m = (-2 / sample_size) * sum(x_sample * (y_sample - Y_pred))\n\n    # d/db derivative of loss function\n    D_b = (-2 / sample_size) * sum(y_sample - Y_pred)\n    m = m - L * D_m  # Update m\n    b = b - L * D_b  # Update b\n\n    # print progress\n    if i % 10000 == 0:\n        print(i, m, b)\n\nprint(\"y = {0}x + {1}\".format(m, b))\n```", "```py\nimport pandas as pd\n\n# Read data into Pandas dataframe\ndf = pd.read_csv('https://bit.ly/2KF29Bd', delimiter=\",\")\n\n# Print correlations between variables\ncorrelations = df.corr(method='pearson')\nprint(correlations)\n\n# OUTPUT:\n#           x         y\n# x  1.000000  0.957586\n# y  0.957586  1.000000\n```", "```py\nfrom scipy.stats import t\n\nn = 10\nlower_cv = t(n-1).ppf(.025)\nupper_cv = t(n-1).ppf(.975)\n\nprint(lower_cv, upper_cv)\n# -2.262157162740992 2.2621571627409915\n```", "```py\nfrom scipy.stats import t\nfrom math import sqrt\n\n# sample size\nn = 10\n\nlower_cv = t(n-1).ppf(.025)\nupper_cv = t(n-1).ppf(.975)\n\n# correlation coefficient\n# derived from data https://bit.ly/2KF29Bd\nr = 0.957586\n\n# Perform the test\ntest_value = r / sqrt((1-r**2) / (n-2))\n\nprint(\"TEST VALUE: {}\".format(test_value))\nprint(\"CRITICAL RANGE: {}, {}\".format(lower_cv, upper_cv))\n\nif test_value < lower_cv or test_value > upper_cv:\n    print(\"CORRELATION PROVEN, REJECT H0\")\nelse:\n    print(\"CORRELATION NOT PROVEN, FAILED TO REJECT H0 \")\n\n# Calculate p-value\nif test_value > 0:\n    p_value = 1.0 - t(n-1).cdf(test_value)\nelse:\n    p_value = t(n-1).cdf(test_value)\n\n# Two-tailed, so multiply by 2\np_value = p_value * 2\nprint(\"P-VALUE: {}\".format(p_value))\n```", "```py\nimport pandas as pd\n\n# Read data into Pandas dataframe\ndf = pd.read_csv('https://bit.ly/2KF29Bd', delimiter=\",\")\n\n# Print correlations between variables\ncoeff_determination = df.corr(method='pearson') ** 2\nprint(coeff_determination)\n\n# OUTPUT:\n#           x         y\n# x  1.000000  0.916971\n# y  0.916971  1.000000\n```", "```py\nHere is how we calculate it in Python:\n\nimport pandas as pd\nfrom math import sqrt\n\n# Load the data\npoints = list(pd.read_csv('https://bit.ly/2KF29Bd', delimiter=\",\").itertuples())\n\nn = len(points)\n\n# Regression line\nm = 1.939\nb = 4.733\n\n# Calculate Standard Error of Estimate\nS_e = sqrt((sum((p.y - (m*p.x +b))**2 for p in points))/(n-2))\n\nprint(S_e)\n# 1.87406793500129\n```", "```py\nimport pandas as pd\nfrom scipy.stats import t\nfrom math import sqrt\n\n# Load the data\npoints = list(pd.read_csv('https://bit.ly/2KF29Bd', delimiter=\",\").itertuples())\n\nn = len(points)\n\n# Linear Regression Line\nm = 1.939\nb = 4.733\n\n# Calculate Prediction Interval for x = 8.5\nx_0 = 8.5\nx_mean = sum(p.x for p in points) / len(points)\n\nt_value = t(n - 2).ppf(.975)\n\nstandard_error = sqrt(sum((p.y - (m * p.x + b)) ** 2 for p in points) / (n - 2))\n\nmargin_of_error = t_value * standard_error * \\\n                  sqrt(1 + (1 / n) + (n * (x_0 - x_mean) ** 2) / \\\n                       (n * sum(p.x ** 2 for p in points) - \\\n                            sum(p.x for p in points) ** 2))\n\npredicted_y = m*x_0 + b\n\n# Calculate prediction interval\nprint(predicted_y - margin_of_error, predicted_y + margin_of_error)\n# 16.462516875955465 25.966483124044537\n```", "```py\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\n\n# Load the data\ndf = pd.read_csv('https://bit.ly/3cIH97A', delimiter=\",\")\n\n# Extract input variables (all rows, all columns but last column)\nX = df.values[:, :-1]\n\n# Extract output column (all rows, last column)\nY = df.values[:, -1]\n\n# Separate training and testing data\n# This leaves a third of the data out for testing\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=1/3)\n\nmodel = LinearRegression()\nmodel.fit(X_train, Y_train)\nresult = model.score(X_test, Y_test)\nprint(\"r^2: %.3f\" % result)\n```", "```py\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import KFold, cross_val_score\n\ndf = pd.read_csv('https://bit.ly/3cIH97A', delimiter=\",\")\n\n# Extract input variables (all rows, all columns but last column)\nX = df.values[:, :-1]\n\n# Extract output column (all rows, last column)\\\nY = df.values[:, -1]\n\n# Perform a simple linear regression\nkfold = KFold(n_splits=3, random_state=7, shuffle=True)\nmodel = LinearRegression()\nresults = cross_val_score(model, X, Y, cv=kfold)\nprint(results)\nprint(\"MSE: mean=%.3f (stdev-%.3f)\" % (results.mean(), results.std()))\n```", "```py\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import cross_val_score, ShuffleSplit\n\ndf = pd.read_csv('https://bit.ly/38XwbeB', delimiter=\",\")\n\n# Extract input variables (all rows, all columns but last column)\nX = df.values[:, :-1]\n\n# Extract output column (all rows, last column)\\\nY = df.values[:, -1]\n\n# Perform a simple linear regression\nkfold = ShuffleSplit(n_splits=10, test_size=.33, random_state=7)\nmodel = LinearRegression()\nresults = cross_val_score(model, X, Y, cv=kfold)\n\nprint(results)\nprint(\"mean=%.3f (stdev-%.3f)\" % (results.mean(), results.std()))\n```", "```py\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\n\n# Load the data\ndf = pd.read_csv('https://bit.ly/2X1HWH7', delimiter=\",\")\n\n# Extract input variables (all rows, all columns but last column)\nX = df.values[:, :-1]\n\n# Extract output column (all rows, last column)\\\nY = df.values[:, -1]\n\n# Training\nfit = LinearRegression().fit(X, Y)\n\n# Print coefficients\nprint(\"Coefficients = {0}\".format(fit.coef_))\nprint(\"Intercept = {0}\".format(fit.intercept_))\nprint(\"z = {0} + {1}x + {2}y\".format(fit.intercept_, fit.coef_[0], fit.coef_[1]))\n```"]