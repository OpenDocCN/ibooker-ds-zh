<html><head></head><body><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" class="pagenumrestart" data-pdf-bookmark="Chapter 1. Exploratory Data Analysis"><div class="chapter" id="EDA">
<h1><span class="label">Chapter 1. </span>Exploratory Data Analysis</h1>


<p>This chapter focuses on the<a data-type="indexterm" data-primary="exploratory data analysis" id="ix_expda"/> first step in any data science project: exploring the data.</p>

<p>Classical statistics focused almost exclusively on <em>inference</em>, a sometimes complex set of <a data-type="indexterm" data-primary="inference" id="idm46522859549416"/>procedures for drawing conclusions about large populations based on small samples.
In 1962, <a href="https://oreil.ly/LQw6q">John W. Tukey</a> (<a data-type="xref" href="#JWTukey">Figure 1-1</a>) called for a reformation of statistics in his seminal paper “The Future of Data Analysis” <a data-type="link" href="bibliography01.xhtml#Tukey-1962">[Tukey-1962]</a>.
He proposed a new scientific discipline called <em>data analysis</em> that included statistical inference as just one component.<a data-type="indexterm" data-primary="data analysis" data-seealso="exploratory data analysis" id="idm46522859013912"/>
Tukey forged links to the engineering and computer science communities (he coined the terms <em>bit</em>, short for binary digit, and <em>software</em>), and his original tenets are surprisingly durable and form part of the foundation for data science.<a data-type="indexterm" data-primary="Tukey, John Wilder" id="idm46522874564120"/><a data-type="indexterm" data-primary="Exploratory Data Analysis (Tukey)" id="idm46522874563448"/>
The field of exploratory data analysis was established with Tukey’s 1977 now-classic book <em>Exploratory Data Analysis</em> <a data-type="link" href="bibliography01.xhtml#Tukey-1977">[Tukey-1977]</a>.  Tukey presented simple plots (e.g., boxplots, scatterplots) that, along with summary statistics (mean, median, quantiles, etc.), help paint a picture of a data set.</p>

<p>With the ready availability of computing power and expressive data analysis software, exploratory data analysis has evolved well beyond its original scope.
Key drivers of this discipline have been the rapid development of new technology, access to more and bigger data, and the greater use of quantitative analysis in a variety of disciplines.<a data-type="indexterm" data-primary="Donoho, David" id="idm46522859566904"/>
David Donoho, professor of statistics at Stanford University and former undergraduate student of Tukey’s, authored an excellent article based on his presentation at the Tukey Centennial workshop in Princeton, New Jersey <a data-type="link" href="bibliography01.xhtml#Donoho-2015">[Donoho-2015]</a>. Donoho traces the genesis of data science back to Tukey’s pioneering work in data analysis.</p>

<figure><div id="JWTukey" class="figure">
<img src="Images/psd2_0101.png" alt="John Tukey, the eminent statistician, whose ideas developed over fifty years ago form the foundation of data science." width="220" height="268"/>
<h6><span class="label">Figure 1-1. </span>John Tukey, the eminent statistician whose ideas developed over 50 years ago form the foundation of data science</h6>
</div></figure>






<section data-type="sect1" data-pdf-bookmark="Elements of Structured Data"><div class="sect1" id="StructuredData">
<h1>Elements of Structured Data</h1>

<p>Data comes from many sources: sensor measurements, events, text, images, and videos.<a data-type="indexterm" data-primary="structured data" id="ix_strctda"/>
The <em>Internet of Things</em> (IoT) is spewing out streams of information.<a data-type="indexterm" data-primary="Internet of Things (IoT)" id="idm46522859589848"/>
Much of this data is unstructured: images are a collection of pixels, with each pixel containing RGB (red, green, blue) color information.
Texts are sequences of words and nonword characters, often organized by sections, subsections, and so on.
Clickstreams are sequences of actions by a user interacting with an app or a web page.
In fact, a major challenge of data science is to harness this torrent of raw data into actionable information.
To apply the statistical concepts covered in this book,
unstructured raw data must be processed and manipulated into a structured form.  One of the commonest forms of structured data is a table with rows and columns—as data might emerge from a relational database or be collected for a study.</p>

<p>There are two basic types of structured data: numeric and categorical.
Numeric data comes in two<a data-type="indexterm" data-primary="numeric data" id="idm46522859660792"/><a data-type="indexterm" data-primary="continuous data" id="idm46522859571048"/><a data-type="indexterm" data-primary="discrete data" id="idm46522859570376"/> forms: <em>continuous</em>, such as wind speed or time duration, and <em>discrete</em>, such as the count of the occurrence of an event.
<em>Categorical</em> data takes<a data-type="indexterm" data-primary="exploratory data analysis" data-secondary="categorical and binary data" id="ix_expdacabi"/> only a fixed set of values, such as a type of TV screen (plasma, LCD, LED, etc.) or a state name (Alabama, Alaska, etc.).
<em>Binary</em> data is an important special case of categorical data that takes on only one of two values, such as 0/1, yes/no, or true/false.<a data-type="indexterm" data-primary="binary data" id="idm46522874862952"/>
Another useful type of categorical data is <em>ordinal</em> data in which the categories are ordered; an example of this is a numerical rating (1, 2, 3, 4, or 5).<a data-type="indexterm" data-primary="ordinal data" id="idm46522874861736"/></p>

<p>Why do we bother with a taxonomy of data types?<a data-type="indexterm" data-primary="data types" data-secondary="key terms for" id="idm46522867254056"/>
It turns out that for the purposes of data analysis and predictive modeling,
the data type is important to help determine the type of visual display, data analysis, or statistical model.
In fact, data science <span class="keep-together">software</span>, such as <em>R</em> and <em>Python</em>, uses these data types to improve computational performance.
More important, the data type for a variable determines how software will handle computations for that variable.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46522874835512">
<h5>Key Terms for Data Types</h5><dl>
<dt class="horizontal"><strong>Numeric</strong></dt>
<dd>
<p>Data that are expressed on a numeric scale.</p>
<dl>
<dt><strong><em>Continuous</em></strong></dt>
<dd>
<p>Data that can take on any value in an interval.
(<em>Synonyms</em>: interval, float, numeric)</p>
</dd>
<dt><strong><em>Discrete</em></strong></dt>
<dd>
<p>Data that can take on only integer values, such as counts.
(<em>Synonyms</em>: integer, count)</p>
</dd>
</dl>
</dd>
<dt class="horizontal"><strong>Categorical</strong></dt>
<dd>
<p>Data that can take on only a specific set of values representing a set of possible categories.
(<em>Synonyms</em>: enums, enumerated, factors, nominal)</p>
<dl>
<dt><strong><em>Binary</em></strong></dt>
<dd>
<p>A special case of categorical data with just two categories of values, e.g., 0/1, true/false. (<em>Synonyms</em>: dichotomous, logical, indicator, boolean)</p>
</dd>
<dt><strong><em>Ordinal</em></strong></dt>
<dd>
<p>Categorical data that has an explicit ordering.
(<em>Synonym</em>: ordered factor)</p>
</dd>
</dl>
</dd>
</dl>
</div></aside>

<p>Software engineers and database programmers may wonder why we even need the notion of <em>categorical</em> and <em>ordinal</em> data for analytics.<a data-type="indexterm" data-primary="categorical data" data-secondary="importance of the concept" id="idm46522874853944"/><a data-type="indexterm" data-primary="ordinal data" data-secondary="importance of the concept" id="idm46522874853080"/>
After all, categories are merely a collection of text (or numeric) values,
and the underlying database automatically handles the internal representation.
However, explicit identification of data as categorical, as distinct from text, does offer some advantages:</p>

<ul>
<li>
<p>Knowing that data is categorical can act as a signal telling software  how statistical procedures, such as producing a chart or fitting a model,
should behave.
In particular, ordinal data can be represented as an <code>ordered.factor</code> in <em>R</em>, preserving a user-specified ordering in charts, tables, and models. In <em>Python</em>, <code>scikit-learn</code> supports ordinal data with the  <code>sklearn.preprocessing.OrdinalEncoder</code>.</p>
</li>
<li>
<p>Storage and indexing can be optimized (as in a relational database).</p>
</li>
<li>
<p>The possible values a given categorical variable can take are enforced in the software (like an enum).</p>
</li>
</ul>

<p>The third “benefit” can lead to unintended or unexpected behavior:
the default behavior of data import functions in <em>R</em> (e.g., <code>read.csv</code>) is to automatically convert a text column into a <code>factor</code>.<a data-type="indexterm" data-primary="factor variables" id="idm46522867219752"/>
Subsequent operations on that column will assume that the only allowable values for that column are the ones originally imported,
and assigning a new text value will introduce a warning and produce an <code>NA</code> (missing value). The <code>pandas</code> package in <em>Python</em> will not make such a conversion automatically. However, you can specify a column as categorical explicitly in the <code>read_csv</code> function.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46522867229336">
<h5>Key Ideas</h5>
<ul>
<li>
<p>Data is typically classified in software by type.</p>
</li>
<li>
<p>Data types include numeric (continuous, discrete) and categorical (binary,  <span class="keep-together">ordinal</span>).</p>
</li>
<li>
<p>Data typing in software acts as a signal to the software on how to process the data.</p>
</li>
</ul>
</div></aside>








<section data-type="sect2" data-pdf-bookmark="Further Reading"><div class="sect2" id="idm46522859704008">
<h2>Further Reading</h2>

<ul>
<li>
<p>The <a href="https://oreil.ly/UGX-4"><code>pandas</code> documentation</a> describes<a data-type="indexterm" data-primary="data types" data-secondary="resources for further reading" id="idm46522859643688"/> the different data types and how they can be manipulated in <em>Python</em>.</p>
</li>
<li>
<p>Data types can be confusing, since types may overlap, and the taxonomy in one software may differ from that in another.<a data-type="indexterm" data-primary="R-Tutorial website" id="idm46522867215064"/> The <a href="https://oreil.ly/2YUoA">R Tutorial website</a> covers the taxonomy for <em>R</em>. The <a href="https://oreil.ly/UGX-4"><code>pandas</code> documentation</a> describes the different data types and how they can be manipulated in <em>Python</em>.</p>
</li>
<li>
<p>Databases are more detailed in their classification of data types, incorporating considerations of precision levels, fixed- or variable-length fields, and more;<a data-type="indexterm" data-primary="databases, data types in" id="idm46522859617944"/><a data-type="indexterm" data-primary="exploratory data analysis" data-secondary="categorical and binary data" data-startref="ix_expdacabi" id="idm46522859617224"/> see the <a href="https://oreil.ly/cThTM">W3Schools guide to SQL</a>.<a data-type="indexterm" data-primary="W3Schools guide for SQL" id="idm46522867243976"/><a data-type="indexterm" data-primary="SQL (Structured Query Language)" id="idm46522867243240"/></p>
</li>
</ul>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Rectangular Data"><div class="sect1" id="RectangularData">
<h1>Rectangular Data</h1>

<p>The typical frame of reference for an <a data-type="indexterm" data-primary="structured data" data-startref="ix_strctda" id="idm46522874849992"/>analysis in data science is a <em>rectangular data</em> object,
like a spreadsheet or database table.<a data-type="indexterm" data-primary="rectangular data" id="ix_rect"/></p>

<p><em>Rectangular data</em> is the general term for a two-dimensional matrix with rows indicating records (cases) and columns indicating features (variables); <em>data frame</em> is the specific format in <em>R</em> and <em>Python</em>.<a data-type="indexterm" data-primary="data frames" id="idm46522858640424"/>
The data doesn’t always start in this form:
unstructured data (e.g., text) must be processed and manipulated so that it can be represented as a set of features in the rectangular data (see <a data-type="xref" href="#StructuredData">“Elements of Structured Data”</a>). Data in relational databases must be extracted and put into a single table for most data analysis and modeling tasks.<a data-type="indexterm" data-primary="rectangular data" data-secondary="key terms for" id="idm46522858638408"/></p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46522867468664">
<h5>Key Terms for Rectangular Data</h5><dl>
<dt class="horizontal"><strong><em>Data frame</em></strong></dt>
<dd>
<p>Rectangular data (like a spreadsheet) is the basic data structure for statistical and machine learning models.</p>
</dd>
<dt class="horizontal"><strong><em>Feature</em></strong></dt>
<dd>
<p>A column within a table is commonly referred to as a <em>feature</em>.</p>
<dl>
<dt>Synonyms</dt>
<dd>
<p>attribute, input, predictor, variable<a data-type="indexterm" data-primary="features" id="idm46522858687656"/></p>
</dd>
</dl>
</dd>
<dt class="horizontal"><strong><em>Outcome</em></strong></dt>
<dd>
<p>Many data science projects involve predicting an <em>outcome</em>—often a yes/no outcome (in <a data-type="xref" href="#dataframe">Table 1-1</a>, it is “auction was competitive or not”).
The <em>features</em> are sometimes used to predict the <em>outcome</em> in an experiment or a study.<a data-type="indexterm" data-primary="outcome" id="idm46522859607624"/></p>
<dl>
<dt>Synonyms</dt>
<dd>
<p>dependent variable, response, target, output</p>
</dd>
</dl>
</dd>
<dt class="horizontal"><strong><em>Records</em></strong></dt>
<dd>
<p>A row within a table <a data-type="indexterm" data-primary="records" id="idm46522859572680"/>is commonly referred to as a <em>record</em>.</p>
<dl>
<dt>Synonyms</dt>
<dd>
<p>case, example, instance, observation, pattern, sample</p>
</dd>
</dl>
</dd>
</dl>
</div></aside>
<table id="dataframe">
<caption><span class="label">Table 1-1. </span>A typical data frame format</caption>
<thead>
<tr>
<th>Category</th>
<th>currency</th>
<th>sellerRating</th>
<th>Duration</th>
<th>endDay</th>
<th>ClosePrice</th>
<th>OpenPrice</th>
<th>Competitive?</th>
</tr>
</thead>
<tbody>
<tr>
<td><p>Music/Movie/Game</p></td>
<td><p>US</p></td>
<td><p>3249</p></td>
<td><p>5</p></td>
<td><p>Mon</p></td>
<td><p>0.01</p></td>
<td><p>0.01</p></td>
<td><p>0</p></td>
</tr>
<tr>
<td><p>Music/Movie/Game</p></td>
<td><p>US</p></td>
<td><p>3249</p></td>
<td><p>5</p></td>
<td><p>Mon</p></td>
<td><p>0.01</p></td>
<td><p>0.01</p></td>
<td><p>0</p></td>
</tr>
<tr>
<td><p>Automotive</p></td>
<td><p>US</p></td>
<td><p>3115</p></td>
<td><p>7</p></td>
<td><p>Tue</p></td>
<td><p>0.01</p></td>
<td><p>0.01</p></td>
<td><p>0</p></td>
</tr>
<tr>
<td><p>Automotive</p></td>
<td><p>US</p></td>
<td><p>3115</p></td>
<td><p>7</p></td>
<td><p>Tue</p></td>
<td><p>0.01</p></td>
<td><p>0.01</p></td>
<td><p>0</p></td>
</tr>
<tr>
<td><p>Automotive</p></td>
<td><p>US</p></td>
<td><p>3115</p></td>
<td><p>7</p></td>
<td><p>Tue</p></td>
<td><p>0.01</p></td>
<td><p>0.01</p></td>
<td><p>0</p></td>
</tr>
<tr>
<td><p>Automotive</p></td>
<td><p>US</p></td>
<td><p>3115</p></td>
<td><p>7</p></td>
<td><p>Tue</p></td>
<td><p>0.01</p></td>
<td><p>0.01</p></td>
<td><p>0</p></td>
</tr>
<tr>
<td><p>Automotive</p></td>
<td><p>US</p></td>
<td><p>3115</p></td>
<td><p>7</p></td>
<td><p>Tue</p></td>
<td><p>0.01</p></td>
<td><p>0.01</p></td>
<td><p>1</p></td>
</tr>
<tr>
<td><p>Automotive</p></td>
<td><p>US</p></td>
<td><p>3115</p></td>
<td><p>7</p></td>
<td><p>Tue</p></td>
<td><p>0.01</p></td>
<td><p>0.01</p></td>
<td><p>1</p></td>
</tr>
</tbody>
</table>

<p>In <a data-type="xref" href="#dataframe">Table 1-1</a>, there is a mix of measured or counted data (e.g., duration and price) and categorical data (e.g., category and currency).<a data-type="indexterm" data-primary="data frames" data-secondary="typical data frame" id="idm46522859669864"/>
As mentioned earlier, a special form of categorical variable is a binary (yes/no or 0/1) variable, seen in the rightmost column in <a data-type="xref" href="#dataframe">Table 1-1</a>—an indicator variable showing whether an auction was competitive (had multiple bidders) or not.
This indicator variable also happens to be an <em>outcome</em> variable, when the scenario is to predict whether an auction is competitive or not.</p>








<section data-type="sect2" data-pdf-bookmark="Data Frames and Indexes"><div class="sect2" id="idm46522859667176">
<h2>Data Frames and Indexes</h2>

<p>Traditional database tables have one or more columns designated as an index, essentially a row number.<a data-type="indexterm" data-primary="data frames" data-secondary="and indexes" data-secondary-sortas="indexes" id="idm46522859665512"/><a data-type="indexterm" data-primary="indexes, data frames and" id="idm46522859664264"/>
This can vastly improve the efficiency of certain database queries.
In <em>Python</em>, with the <code>pandas</code> library,
the basic rectangular data structure is a <code>DataFrame</code> object.
By default, an automatic integer index is created for a <code>DataFrame</code> based on the order of the rows.
In <code>pandas</code>, it is also possible to set multilevel/hierarchical indexes to improve the efficiency of certain operations.</p>

<p>In <em>R</em>, the basic rectangular data structure is a <code>data.frame</code> object.
A <code>data.frame</code> also has an implicit integer index based on the row order.
The native <em>R</em> <code>data.frame</code> does not support user-specified or multilevel indexes, though a custom key can be created through the <code>row.names</code> attribute.
To overcome this deficiency,
two new packages are gaining widespread use: <code>data.table</code> and <code>dplyr</code>.
Both support multilevel indexes and offer significant speedups in working with a <code>data.frame</code>.<a data-type="indexterm" data-primary="rectangular data" data-secondary="terminology differences" id="idm46522859696808"/></p>
<div data-type="warning" epub:type="warning"><h1>Terminology Differences</h1>
<p>Terminology for rectangular data can be confusing.
Statisticians and data scientists use different terms for the same thing.
For a statistician, <em>predictor variables</em> are used in a model to predict a <em>response</em> or <em>dependent variable</em>.<a data-type="indexterm" data-primary="predictor variables" id="idm46522859654392"/><a data-type="indexterm" data-primary="response" id="idm46522859653656"/><a data-type="indexterm" data-primary="dependent variables" id="idm46522859652984"/>
For a data scientist, <em>features</em> are used to predict a <em>target</em>.<a data-type="indexterm" data-primary="features" data-secondary="terminology differences" id="idm46522859651320"/><a data-type="indexterm" data-primary="target" id="idm46522859650312"/>
One synonym is particularly confusing: computer scientists will use the term <em>sample</em> for a single row; a <em>sample</em> to a statistician means a collection of rows.<a data-type="indexterm" data-primary="samples" data-secondary="terminology differences" id="idm46522859648680"/></p>
</div>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Nonrectangular Data Structures"><div class="sect2" id="idm46522859647704">
<h2>Nonrectangular Data Structures</h2>

<p>There are other data structures besides rectangular data.<a data-type="indexterm" data-primary="nonrectangular data structures" id="idm46522859646200"/></p>

<p>Time series data records successive measurements of the same variable.  It is the raw material for statistical forecasting methods, and it is also a key component of the data produced by devices—the Internet of Things.</p>

<p>Spatial data structures, which are used in mapping and location analytics, are more complex and varied than rectangular data structures.  <a data-type="indexterm" data-primary="spatial data structures" id="idm46522867275656"/><a data-type="indexterm" data-primary="object representation (spatial data)" id="idm46522867274952"/>In the <em>object</em> representation, the focus of the data is an object (e.g., a house) and its spatial coordinates.<a data-type="indexterm" data-primary="field view (spatial data)" id="idm46522867273608"/>  The <em>field</em> view, by contrast, focuses on small units of space and the value of a relevant metric (pixel brightness, for example).</p>

<p class="pagebreak-before">Graph (or network) data structures are used to represent physical, social, and abstract relationships.<a data-type="indexterm" data-primary="graphs" id="idm46522867271416"/><a data-type="indexterm" data-primary="network data structures" id="idm46522867270712"/>
For example, a graph of a social network, such as Facebook or LinkedIn, may represent connections between people on the network.
Distribution hubs connected by roads are an example of a physical network.
Graph structures are useful for certain types of problems, such as network optimization and recommender systems.</p>

<p>Each of these data types has its specialized methodology in data science.
The focus of this book is on rectangular data, the fundamental building block of predictive <span class="keep-together">modeling</span>.</p>
<div data-type="warning" epub:type="warning"><h1>Graphs in Statistics</h1>
<p>In computer science and information technology, the term <em>graph</em> typically refers to a depiction of the connections among entities, and to the underlying data structure.<a data-type="indexterm" data-primary="graphs" data-secondary="in computer science versus statistics" data-secondary-sortas="computer" id="idm46522867266456"/>
In statistics, <em>graph</em> is used to refer to a variety of plots and <em>visualizations</em>, not just of connections among entities, and the term applies only to the visualization, not to the data structure.<a data-type="indexterm" data-primary="visualizations" data-seealso="graphs" id="idm46522867264104"/></p>
</div>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46522867262872">
<h5>Key Ideas</h5>
<ul>
<li>
<p>The basic data structure in data science is a rectangular matrix in which rows are records and columns are variables (features).</p>
</li>
<li>
<p>Terminology can be confusing; there are a variety of synonyms arising from the different disciplines that contribute to data science (statistics, computer science, and information technology).</p>
</li>
</ul>
</div></aside>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Further Reading"><div class="sect2" id="idm46522867259208">
<h2>Further Reading</h2>

<ul>
<li>
<p><a href="https://oreil.ly/NsONR">Documentation on data frames in <em>R</em></a></p>
</li>
<li>
<p><a href="https://oreil.ly/oxDKQ">Documentation on data frames in <em>Python</em></a></p>
</li>
</ul>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Estimates of Location"><div class="sect1" id="Location">
<h1>Estimates of Location</h1>

<p>Variables with measured or count data might <a data-type="indexterm" data-primary="rectangular data" data-startref="ix_rect" id="idm46522858621512"/>have thousands of distinct values.<a data-type="indexterm" data-primary="estimates of location" id="ix_estloc"/><a data-type="indexterm" data-primary="location, estimates of" id="ix_locest"/>
A basic step in exploring your data is getting a “typical value” for each feature (variable):
an estimate of where most<a data-type="indexterm" data-primary="exploratory data analysis" data-secondary="estimates of location" id="ix_expdaest"/><a data-type="indexterm" data-primary="central tendency" data-see="estimates of location" id="idm46522858616920"/> of the data is located (i.e., its central tendency).</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46522858615720">
<h5>Key Terms for Estimates of Location</h5><dl>
<dt class="horizontal"><strong><em>Mean</em></strong></dt>
<dd>
<p>The sum of all values divided by the number of values.</p>
<dl>
<dt>Synonym</dt>
<dd>
<p>average</p>
</dd>
</dl>
</dd>
<dt class="horizontal"><strong><em>Weighted mean</em></strong></dt>
<dd>
<p>The sum of all values times a weight divided by the sum of the weights.<a data-type="indexterm" data-primary="weighted mean" id="idm46522858608952"/></p>
<dl>
<dt>Synonym</dt>
<dd>
<p>weighted average</p>
</dd>
</dl>
</dd>
<dt class="horizontal"><strong><em>Median</em></strong></dt>
<dd>
<p>The value such that one-half of the data lies above and below.</p>
<dl>
<dt>Synonym</dt>
<dd>
<p>50th percentile<a data-type="indexterm" data-primary="median" id="idm46522859781016"/></p>
</dd>
</dl>
</dd>
<dt class="horizontal"><strong><em>Percentile</em></strong></dt>
<dd>
<p>The value such that <em>P</em> percent of the data lies below.</p>
<dl>
<dt>Synonym</dt>
<dd>
<p>quantile<a data-type="indexterm" data-primary="percentiles" id="idm46522859776264"/></p>
</dd>
</dl>
</dd>
<dt class="horizontal"><strong><em>Weighted median</em></strong></dt>
<dd>
<p>The value such that one-half of the sum of the weights lies above and below the sorted data.<a data-type="indexterm" data-primary="weighted median" id="idm46522859773352"/></p>
</dd>
<dt class="horizontal"><strong><em>Trimmed mean</em></strong></dt>
<dd>
<p>The average of all values after dropping a fixed number of extreme values.<a data-type="indexterm" data-primary="trimmed mean" id="idm46522859770840"/></p>
<dl>
<dt>Synonym</dt>
<dd>
<p>truncated mean</p>
</dd>
</dl>
</dd>
<dt class="horizontal"><strong><em>Robust</em></strong></dt>
<dd>
<p>Not sensitive to extreme values.<a data-type="indexterm" data-primary="robust" id="idm46522859766536"/></p>
<dl>
<dt>Synonym</dt>
<dd>
<p>resistant</p>
</dd>
</dl>
</dd>
<dt class="horizontal"><strong><em>Outlier</em></strong></dt>
<dd>
<p>A data value that is very different from most of the data.</p>
<dl>
<dt>Synonym</dt>
<dd>
<p>extreme value<a data-type="indexterm" data-primary="outliers" id="idm46522867207416"/></p>
</dd>
</dl>
</dd>
</dl>
</div></aside>

<p>At first glance, summarizing data might seem fairly trivial: just take the <em>mean</em> of the data.<a data-type="indexterm" data-primary="mean" id="idm46522867205496"/>
In fact, while the mean is easy to compute and expedient to use, it may not always be the best measure for a central value.
For this reason,
statisticians have developed and promoted several alternative estimates to the mean.</p>
<div data-type="note" epub:type="note"><h1>Metrics and Estimates</h1>
<p>Statisticians often use the term <em>estimate</em> for a value calculated from the data at hand, to draw a distinction between what we see from the data and the theoretical true or exact state of affairs.<a data-type="indexterm" data-primary="estimates" data-secondary="metrics and" id="idm46522867202488"/>
Data scientists and business analysts are more likely to refer to such a value as a <em>metric</em>.<a data-type="indexterm" data-primary="metrics" id="idm46522867201000"/>
The difference reflects the approach of statistics versus that of data science:  accounting for uncertainty lies at the heart of the discipline of statistics, whereas concrete business or organizational objectives are the focus of data science.
Hence, statisticians estimate, and data scientists measure.</p>
</div>








<section data-type="sect2" data-pdf-bookmark="Mean"><div class="sect2" id="Mean">
<h2>Mean</h2>

<p>The most <a data-type="indexterm" data-primary="mean" id="idm46522867197368"/>basic estimate of location is the mean, or <em>average</em> value.<a data-type="indexterm" data-primary="average value" data-see="mean" id="idm46522867196120"/>
The mean is the sum of all values divided by the number of values.
Consider the following set of numbers:  {3 5 1 2}.
The mean is  (3 + 5 + 1 + 2) / 4 = 11 / 4 = 2.75.
You will encounter the symbol
<math alttext="x overbar">
  <mover accent="true"><mi>x</mi> <mo>¯</mo></mover>
</math> (pronounced “x-bar”) being used to represent the mean of a sample from a population.
The formula to compute the mean for a set of <em>n</em> values
<math alttext="x 1 comma x 2 comma ellipsis comma x Subscript n Baseline">
  <mrow>
    <msub><mi>x</mi> <mn>1</mn> </msub>
    <mo>,</mo>
    <msub><mi>x</mi> <mn>2</mn> </msub>
    <mo>,</mo>
    <mo>...</mo>
    <mo>,</mo>
    <msub><mi>x</mi> <mi>n</mi> </msub>
  </mrow>
</math> is:</p>
<div data-type="equation">
<math display="block">
  <mrow>
    <mtext>Mean</mtext>
    <mo>=</mo>
    <mover accent="true"><mi>x</mi> <mo>¯</mo></mover>
    <mo>=</mo>
    <mfrac><mrow><msubsup><mo>∑</mo> <mi>i=1</mi> <mi>n</mi> </msubsup><msub><mi>x</mi> <mi>i</mi> </msub></mrow> <mi>n</mi></mfrac>
  </mrow>
</math>
</div>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p><em>N</em> (or <em>n</em>) refers to the total number of records or observations.<a data-type="indexterm" data-primary="N (or n) referring to total records" id="idm46522867175816"/> In statistics it is capitalized if it is referring to a population, and lowercase if it refers to a sample from a population.  In data science, that distinction is not vital, so you may see it both ways.</p>
</div>

<p id="TrimmedMean">A variation of the mean is a <em>trimmed mean</em>, which you calculate by dropping a fixed number of sorted values at each end and then taking an average of the remaining values.<a data-type="indexterm" data-primary="mean" data-secondary="trimmed mean" id="idm46522867173112"/><a data-type="indexterm" data-primary="trimmed mean" data-secondary="formula for" id="idm46522867172136"/>
Representing the sorted values by <math alttext="x Subscript left-parenthesis 1 right-parenthesis Baseline comma x Subscript left-parenthesis 2 right-parenthesis Baseline comma ellipsis comma x Subscript left-parenthesis n right-parenthesis Baseline">
  <mrow>
    <msub><mi>x</mi> <mrow><mo>(</mo><mn>1</mn><mo>)</mo></mrow> </msub>
    <mo>,</mo>
    <msub><mi>x</mi> <mrow><mo>(</mo><mn>2</mn><mo>)</mo></mrow> </msub>
    <mo>,</mo>
    <mo>...</mo>
    <mo>,</mo>
    <msub><mi>x</mi> <mrow><mo>(</mo><mi>n</mi><mo>)</mo></mrow> </msub>
  </mrow>
</math> where <math alttext="x Subscript left-parenthesis 1 right-parenthesis">
  <msub><mi>x</mi> <mrow><mo>(</mo><mn>1</mn><mo>)</mo></mrow> </msub>
</math> is the smallest value and <math alttext="x Subscript left-parenthesis n right-parenthesis">
  <msub><mi>x</mi> <mrow><mo>(</mo><mi>n</mi><mo>)</mo></mrow> </msub>
</math> the largest,
the formula to compute the trimmed mean with <math alttext="p">
  <mi>p</mi>
</math> smallest and largest values omitted is:</p>
<div data-type="equation">
<math display="block">
  <mrow>
    <mtext>Trimmed</mtext>
    <mspace width="4.pt"/>
    <mtext>mean</mtext>
    <mo>=</mo>
    <mover accent="true"><mi>x</mi> <mo>¯</mo></mover>
    <mo>=</mo>
    <mfrac><mrow><msubsup><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mi>p</mi><mo>+</mo><mn>1</mn></mrow> <mrow><mi>n</mi><mo>-</mo><mi>p</mi></mrow> </msubsup><msub><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow> </msub></mrow> <mrow><mi>n</mi><mo>-</mo><mn>2</mn><mi>p</mi></mrow></mfrac>
  </mrow>
</math>
</div>

<p>A trimmed mean eliminates the influence of extreme values.
For example, in international diving the top score and bottom score from five judges are dropped, and <a href="https://oreil.ly/uV4P0">the final score is the average of the  scores from the three remaining judges</a>.
This makes it difficult for a single judge to manipulate the score, perhaps to favor their country’s contestant.
Trimmed means are widely used, and in many cases are preferable to using the ordinary mean—see <a data-type="xref" href="#Median">“Median and Robust Estimates”</a> for further discussion.</p>

<p>Another<a data-type="indexterm" data-primary="mean" data-secondary="weighted mean" id="idm46522867139848"/><a data-type="indexterm" data-primary="weighted mean" data-secondary="formula for" id="idm46522867138840"/> type of mean is a <em>weighted mean</em>, which you calculate by multiplying each data value <math alttext="x Subscript i">
  <msub><mi>x</mi> <mi>i</mi> </msub>
</math> by a user-specified weight <math alttext="w Subscript i">
  <msub><mi>w</mi> <mi>i</mi> </msub>
</math> and dividing their sum by the sum of the weights.
The formula for a weighted mean is:</p>
<div data-type="equation">
<math alttext="Weighted mean x overbar Subscript w Baseline equals StartFraction sigma-summation Underscript i equals 1 Overscript n Endscripts w Subscript i Baseline x Subscript i Baseline Over sigma-summation Underscript i equals 1 Overscript n Endscripts w Subscript i Baseline EndFraction" display="block">
  <mrow>
    <mi> Weighted </mi>
    <mspace width="0.166667em"/>
    <mi> mean </mi>
    <mo>=</mo>
    <msub><mover accent="true"><mi>x</mi> <mo>¯</mo></mover> <mi>w</mi> </msub>
    <mo>=</mo>
    <mfrac><mrow><msubsup><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>n</mi> </msubsup><msub><mi>w</mi> <mi>i</mi> </msub><msub><mi>x</mi> <mi>i</mi> </msub></mrow> <mrow><msubsup><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>n</mi> </msubsup><msub><mi>w</mi> <mi>i</mi> </msub></mrow></mfrac>
  </mrow>
</math>
</div>

<p>There are two main motivations for using a weighted mean:</p>

<ul>
<li>
<p>Some values are intrinsically more variable than others, and highly variable observations are given a lower weight. For example, if we are taking the average from multiple sensors and one of the sensors is less accurate, then we might downweight the data from that sensor.</p>
</li>
<li>
<p>The data collected does not equally represent the different groups that we are interested in measuring. For example, because of the way an online experiment was conducted, we may not have a set of data that accurately reflects all groups in the user base. To correct that, we can give a higher weight to the values from the groups that were underrepresented.</p>
</li>
</ul>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Median and Robust Estimates"><div class="sect2" id="Median">
<h2>Median and Robust Estimates</h2>

<p>The <em>median</em> is the middle number on a sorted list of the data.<a data-type="indexterm" data-primary="robust estimates of location" id="ix_robest"/><a data-type="indexterm" data-primary="median" id="idm46522867111608"/>
If there is an even number of data values, the middle value is one that is not actually in the data set, but rather the average of the two values that divide the sorted data into upper and lower halves.<a data-type="indexterm" data-primary="robust estimates of location" data-secondary="median" id="idm46522867110584"/>
Compared to the mean, which uses all observations, the median depends only on the values in the center of the sorted data.
While this might seem to be a disadvantage, since the mean is much more sensitive to the data, there are many instances in which the median is a better metric for location.
Let’s say we want to look at typical household incomes in neighborhoods around Lake Washington in Seattle.
In comparing the Medina neighborhood to the Windermere neighborhood, using the mean would produce very different results because Bill Gates lives in Medina.
If we use the median, it won’t matter how rich Bill Gates is—the position of the middle observation will remain the same.</p>

<p>For the same reasons that one uses a weighted mean,
it is also possible to compute a <em>weighted median</em>.
As with the median, we first sort the data, although each data value has an associated weight.<a data-type="indexterm" data-primary="robust estimates of location" data-secondary="weighted median" id="idm46522867107880"/><a data-type="indexterm" data-primary="weighted median" id="idm46522867106888"/>
Instead of the middle number,
the weighted median is a value such that the sum of the weights is equal for the lower and upper halves of the sorted list.
Like the median, the weighted median is robust to outliers.</p>










<section data-type="sect3" data-pdf-bookmark="Outliers"><div class="sect3" id="idm46522867105736">
<h3>Outliers</h3>

<p>The median is referred to as a <em>robust</em> estimate of location since it is not influenced by <em>outliers</em> (extreme cases) that could skew the results.<a data-type="indexterm" data-primary="robust estimates of location" data-secondary="median" data-tertiary="outliers and" id="idm46522867103080"/><a data-type="indexterm" data-primary="outliers" id="idm46522867101816"/>
An outlier is any value that is very distant from the other values in a data set.
The exact definition of an outlier is somewhat subjective, although certain conventions are used in various data summaries and plots (see <a data-type="xref" href="#Boxplots">“Percentiles and Boxplots”</a>).
Being an outlier in itself does not make a data value invalid or erroneous (as in the previous example with Bill Gates).
Still, outliers are often the result of data errors such as mixing data of different units (kilometers versus meters) or bad readings from a sensor.
When outliers are the result of bad data,
the mean will result in a poor estimate of location, while the median will still be valid.
In any case, outliers should be identified and are usually worthy of further investigation.</p>
<div data-type="note" epub:type="note"><h1>Anomaly Detection</h1>
<p>In contrast to typical data analysis, where outliers are sometimes informative and sometimes a nuisance, in <em>anomaly detection</em> the points of interest are the outliers, and the greater mass of data serves primarily to define the “normal” against which anomalies are measured.<a data-type="indexterm" data-primary="anomaly detection" data-secondary="outliers and" id="idm46522867097480"/></p>
</div>

<p>The median is not the only robust estimate of location.
In fact, a trimmed mean is widely used to avoid the influence of outliers.
For example, trimming the bottom and top 10% (a common choice) of the data will provide protection against outliers in all but the smallest data sets.
The trimmed mean
can be thought of as a compromise between the median and the mean: it is robust to extreme values in the data, but uses more data to calculate the estimate for location.</p>
<div data-type="tip"><h1>Other Robust Metrics for Location</h1>
<p>Statisticians have developed a plethora of other estimators for location, primarily with the goal of developing an estimator more robust than the mean and also
more efficient (i.e., better able to discern small location differences between data sets). <a data-type="indexterm" data-primary="robust estimates of location" data-secondary="other robust metrics for" id="idm46522867093976"/>
While these methods are potentially useful for small data sets, they are not likely to provide added benefit for large or even moderately sized data sets.</p>
</div>
</div></section>



</div></section>













<section data-type="sect2" data-pdf-bookmark="Example: Location Estimates of Population and Murder Rates"><div class="sect2" id="idm46522867092360">
<h2>Example: Location Estimates of Population and Murder Rates</h2>

<p><a data-type="xref" href="#state_table">Table 1-2</a> shows the first few rows in the data set containing population and murder rates (in units of murders per 100,000 people per year) for each US state (2010 <span class="keep-together">Census</span>).<a data-type="indexterm" data-primary="robust estimates of location" data-secondary="example, location estimates of population and murder rates" id="idm46522867089160"/></p>
<table id="state_table">
<caption><span class="label">Table 1-2. </span>A few rows of the <code>data.frame</code> state of population and murder rate by state</caption>
<thead>
<tr>
<th/>
<th>State</th>
<th>Population</th>
<th>Murder rate</th>
<th>Abbreviation</th>
</tr>
</thead>
<tbody>
<tr>
<td><p>1</p></td>
<td><p>Alabama</p></td>
<td><p>4,779,736</p></td>
<td><p>5.7</p></td>
<td><p>AL</p></td>
</tr>
<tr>
<td><p>2</p></td>
<td><p>Alaska</p></td>
<td><p>710,231</p></td>
<td><p>5.6</p></td>
<td><p>AK</p></td>
</tr>
<tr>
<td><p>3</p></td>
<td><p>Arizona</p></td>
<td><p>6,392,017</p></td>
<td><p>4.7</p></td>
<td><p>AZ</p></td>
</tr>
<tr>
<td><p>4</p></td>
<td><p>Arkansas</p></td>
<td><p>2,915,918</p></td>
<td><p>5.6</p></td>
<td><p>AR</p></td>
</tr>
<tr>
<td><p>5</p></td>
<td><p>California</p></td>
<td><p>37,253,956</p></td>
<td><p>4.4</p></td>
<td><p>CA</p></td>
</tr>
<tr>
<td><p>6</p></td>
<td><p>Colorado</p></td>
<td><p>5,029,196</p></td>
<td><p>2.8</p></td>
<td><p>CO</p></td>
</tr>
<tr>
<td><p>7</p></td>
<td><p>Connecticut</p></td>
<td><p>3,574,097</p></td>
<td><p>2.4</p></td>
<td><p>CT</p></td>
</tr>
<tr>
<td><p>8</p></td>
<td><p>Delaware</p></td>
<td><p>897,934</p></td>
<td><p>5.8</p></td>
<td><p>DE</p></td>
</tr>
</tbody>
</table>

<p>Compute the mean, trimmed mean, and median for the population using <em>R</em>:</p>

<pre data-type="programlisting" data-code-language="r"><code class="o">&gt;</code> <code class="n">state</code> <code class="o">&lt;-</code> <code class="nf">read.csv</code><code class="p">(</code><code class="s">'state.csv'</code><code class="p">)</code>
<code class="o">&gt;</code> <code class="nf">mean</code><code class="p">(</code><code class="n">state</code><code class="p">[[</code><code class="s">'Population'</code><code class="p">]])</code>
<code class="p">[</code><code class="m">1</code><code class="p">]</code> <code class="m">6162876</code>
<code class="o">&gt;</code> <code class="nf">mean</code><code class="p">(</code><code class="n">state</code><code class="p">[[</code><code class="s">'Population'</code><code class="p">]],</code> <code class="n">trim</code><code class="o">=</code><code class="m">0.1</code><code class="p">)</code>
<code class="p">[</code><code class="m">1</code><code class="p">]</code> <code class="m">4783697</code>
<code class="o">&gt;</code> <code class="nf">median</code><code class="p">(</code><code class="n">state</code><code class="p">[[</code><code class="s">'Population'</code><code class="p">]])</code>
<code class="p">[</code><code class="m">1</code><code class="p">]</code> <code class="m">4436370</code></pre>

<p>To compute mean and median in <em>Python</em> we can use the <code>pandas</code> methods of the data frame. The trimmed mean requires the <code>trim_mean</code> function in <code>scipy.stats</code>:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">state</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">read_csv</code><code class="p">(</code><code class="s1">'state.csv'</code><code class="p">)</code>
<code class="n">state</code><code class="p">[</code><code class="s1">'Population'</code><code class="p">]</code><code class="o">.</code><code class="n">mean</code><code class="p">()</code>
<code class="n">trim_mean</code><code class="p">(</code><code class="n">state</code><code class="p">[</code><code class="s1">'Population'</code><code class="p">],</code> <code class="mf">0.1</code><code class="p">)</code>
<code class="n">state</code><code class="p">[</code><code class="s1">'Population'</code><code class="p">]</code><code class="o">.</code><code class="n">median</code><code class="p">()</code></pre>

<p>The mean is bigger than the trimmed mean, which is bigger than the median.</p>

<p>This is because the trimmed mean excludes the largest and smallest five states (<code>trim=0.1</code> drops 10% from each end).
If we want to compute the average murder rate for the country, we need to use a weighted mean or median to account for different populations in the states.
Since base <em>R</em> doesn’t have a function for weighted median,
we need to install a package such as <code>matrixStats</code>:</p>

<pre data-type="programlisting" data-code-language="r"><code class="o">&gt;</code> <code class="nf">weighted.mean</code><code class="p">(</code><code class="n">state</code><code class="p">[[</code><code class="s">'Murder.Rate'</code><code class="p">]],</code> <code class="n">w</code><code class="o">=</code><code class="n">state</code><code class="p">[[</code><code class="s">'Population'</code><code class="p">]])</code>
<code class="p">[</code><code class="m">1</code><code class="p">]</code> <code class="m">4.445834</code>
<code class="o">&gt;</code> <code class="nf">library</code><code class="p">(</code><code class="s">'matrixStats'</code><code class="p">)</code>
<code class="o">&gt;</code> <code class="nf">weightedMedian</code><code class="p">(</code><code class="n">state</code><code class="p">[[</code><code class="s">'Murder.Rate'</code><code class="p">]],</code> <code class="n">w</code><code class="o">=</code><code class="n">state</code><code class="p">[[</code><code class="s">'Population'</code><code class="p">]])</code>
<code class="p">[</code><code class="m">1</code><code class="p">]</code> <code class="m">4.4</code></pre>

<p>Weighted mean is available with <code>NumPy</code>. For weighted median, we can use the specialized package <a href="https://oreil.ly/4SIPQ">wquantiles</a>:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">np</code><code class="o">.</code><code class="n">average</code><code class="p">(</code><code class="n">state</code><code class="p">[</code><code class="s1">'Murder.Rate'</code><code class="p">],</code> <code class="n">weights</code><code class="o">=</code><code class="n">state</code><code class="p">[</code><code class="s1">'Population'</code><code class="p">])</code>
<code class="n">wquantiles</code><code class="o">.</code><code class="n">median</code><code class="p">(</code><code class="n">state</code><code class="p">[</code><code class="s1">'Murder.Rate'</code><code class="p">],</code> <code class="n">weights</code><code class="o">=</code><code class="n">state</code><code class="p">[</code><code class="s1">'Population'</code><code class="p">])</code></pre>

<p>In this case, the weighted mean and the weighted median are about the same.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46522866397096">
<h5>Key Ideas</h5>
<ul>
<li>
<p>The basic metric for location is the mean, but it can be sensitive to extreme <span class="keep-together">values</span> (outlier).</p>
</li>
<li>
<p>Other metrics (median, trimmed mean) are less sensitive to outliers and unusual distributions and hence are more robust.<a data-type="indexterm" data-primary="robust estimates of location" data-startref="ix_robest" id="idm46522866393560"/></p>
</li>
</ul>
</div></aside>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Further Reading"><div class="sect2" id="idm46522867091768">
<h2>Further Reading</h2>

<ul>
<li>
<p>The Wikipedia article on <a href="https://oreil.ly/qUW2i">central tendency</a>
contains an extensive discussion of various measures of location.</p>
</li>
<li>
<p>John Tukey’s 1977 classic <em>Exploratory Data Analysis</em> (Pearson) is still widely read.</p>
</li>
</ul>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Estimates of Variability"><div class="sect1" id="Variability">
<h1>Estimates of Variability</h1>

<p>Location <a data-type="indexterm" data-primary="exploratory data analysis" data-secondary="estimates of location" data-startref="ix_expdaest" id="idm46522866357112"/><a data-type="indexterm" data-primary="location, estimates of" data-startref="ix_locest" id="idm46522866355816"/><a data-type="indexterm" data-primary="estimates of location" data-startref="ix_estloc" id="idm46522866354872"/>is just one dimension in summarizing a feature.<a data-type="indexterm" data-primary="exploratory data analysis" data-secondary="estimates of variability" id="ix_expdaestvar"/><a data-type="indexterm" data-primary="variability, estimates of" id="ix_varest"/>
A second dimension, <em>variability</em>, also referred to as <em>dispersion</em>, measures whether the data values are tightly clustered or spread out.<a data-type="indexterm" data-primary="dispersion" data-seealso="variability" id="idm46522866350600"/>
At the heart of statistics lies variability: measuring it, reducing it, distinguishing random from real variability, identifying the various sources of real variability, and making decisions <a data-type="indexterm" data-primary="variability, estimates of" data-secondary="key terminology" id="idm46522866349288"/>in the presence of it.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46522866348200">
<h5>Key Terms for Variability Metrics</h5><dl>
<dt class="horizontal"><strong><em>Deviations</em></strong></dt>
<dd>
<p>The difference between the observed values and the estimate of location.<a data-type="indexterm" data-primary="deviations" data-secondary="defined" id="idm46522866345064"/></p>
<dl>
<dt>Synonyms</dt>
<dd>
<p>errors, residuals</p>
</dd>
</dl>
</dd>
<dt class="horizontal"><strong><em>Variance</em></strong></dt>
<dd>
<p>The sum of squared deviations from the mean divided by <em>n</em> – 1 where <em>n</em> is the number of data values.<a data-type="indexterm" data-primary="variance" id="idm46522866339624"/></p>
<dl>
<dt>Synonym</dt>
<dd>
<p>mean-squared-error</p>
</dd>
</dl>
</dd>
<dt class="horizontal"><strong><em>Standard deviation</em></strong></dt>
<dd>
<p>The square root of the variance.<a data-type="indexterm" data-primary="standard deviation" id="idm46522866316392"/></p>
</dd>
<dt class="horizontal"><strong><em>Mean absolute deviation</em></strong></dt>
<dd>
<p>The mean of the absolute values of the deviations from the mean.<a data-type="indexterm" data-primary="mean absolute deviation" id="idm46522866313880"/></p>
<dl>
<dt>Synonyms</dt>
<dd>
<p>l1-norm, Manhattan norm</p>
</dd>
</dl>
</dd>
<dt class="horizontal"><strong><em>Median absolute deviation from the median</em></strong></dt>
<dd>
<p>The median of the absolute values of the deviations from the median.<a data-type="indexterm" data-primary="median absolute deviation from the median (MAD)" id="idm46522866309608"/></p>
</dd>
<dt class="horizontal"><strong><em>Range</em></strong></dt>
<dd>
<p>The difference between the largest and the smallest value in a data set.<a data-type="indexterm" data-primary="range" id="idm46522866307064"/></p>
</dd>
<dt class="horizontal"><strong><em>Order statistics</em></strong></dt>
<dd>
<p>Metrics based on the data values sorted from smallest to biggest.<a data-type="indexterm" data-primary="order statistics" id="idm46522866304552"/></p>
<dl>
<dt>Synonym</dt>
<dd>
<p>ranks</p>
</dd>
</dl>
</dd>
<dt class="horizontal"><strong><em>Percentile</em></strong></dt>
<dd>
<p>The value such that <em>P</em> percent of the values take on this value or less and (100–P) percent take on this value or more.<a data-type="indexterm" data-primary="percentiles" id="idm46522866299688"/></p>
<dl>
<dt>Synonym</dt>
<dd>
<p>quantile</p>
</dd>
</dl>
</dd>
<dt class="horizontal"><strong><em>Interquartile range</em></strong></dt>
<dd>
<p>The difference between the 75th percentile and the 25th percentile.<a data-type="indexterm" data-primary="IQR" data-see="interquartile range" id="idm46522866295384"/><a data-type="indexterm" data-primary="interquartile range" id="idm46522866294408"/></p>
<dl>
<dt>Synonym</dt>
<dd>
<p>IQR</p>
</dd>
</dl>
</dd>
</dl>
</div></aside>

<p>Just as there are different ways to measure location (mean, median, etc.), there are also different ways to measure variability.</p>








<section data-type="sect2" data-pdf-bookmark="Standard Deviation and Related Estimates"><div class="sect2" id="idm46522866291160">
<h2>Standard Deviation and Related Estimates</h2>

<p>The most widely used <a data-type="indexterm" data-primary="variability, estimates of" data-secondary="standard deviation and related estimates" id="ix_vareststd"/>estimates of variation are based on the differences, or <em>deviations</em>,  between the estimate of location and the observed data.<a data-type="indexterm" data-primary="deviations" data-secondary="standard deviation and related estimates" id="idm46522866287576"/>
For a set of data <span class="keep-together">{1, 4, 4}</span>, the mean is 3 and the median is 4.
The deviations from the mean are the differences:
1 – 3 = –2,		4 – 3 = 1,	4 – 3 = 1.
These deviations tell us how dispersed the data is around the central value.</p>

<p>One way to measure variability is to estimate a typical value for these deviations.
Averaging the deviations themselves would not tell us much—the negative deviations offset the positive ones.
In fact, the sum of the deviations from the mean is precisely zero.
Instead, a simple approach is to take the average of the absolute values of the deviations from the mean.
In the preceding example, the absolute value of the deviations is {2 1 1}, and their average is (2 + 1 + 1) / 3 = 1.33.
This is known as the <em>mean absolute deviation</em> and <a data-type="indexterm" data-primary="mean absolute deviation" data-secondary="formula for calculating" id="idm46522866284168"/>is computed with the formula:</p>
<div data-type="equation">
<math display="block">
  <mrow>
    <mtext>Mean</mtext>
    <mspace width="4.pt"/>
    <mtext>absolute</mtext>
    <mspace width="4.pt"/>
    <mtext>deviation</mtext>
    <mo>=</mo>
    <mfrac><mrow><msubsup><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>n</mi> </msubsup><mfenced separators="" open="|" close="|"><msub><mi>x</mi> <mi>i</mi> </msub><mo>-</mo><mover accent="true"><mi>x</mi> <mo>¯</mo></mover></mfenced></mrow> <mi>n</mi></mfrac>
  </mrow>
</math>
</div>

<p>where <math alttext="x overbar">
  <mover accent="true"><mi>x</mi> <mo>¯</mo></mover>
</math> is the sample mean.</p>

<p>The best-known estimates of variability are the <em>variance</em> and the <em>standard deviation</em>, which are based on squared deviations.<a data-type="indexterm" data-primary="standard deviation" data-secondary="and related estimates" data-secondary-sortas="related" id="idm46522866267704"/><a data-type="indexterm" data-primary="variance" id="idm46522866266456"/>
The variance is an average of the squared deviations, and the standard deviation is the square root of the variance:</p>
<div data-type="equation">
<math display="block">
  <mtable displaystyle="true">
    <mtr>
      <mtd>
        <mtext>Variance</mtext>
      </mtd>
      <mtd>
        <mi/>
        <mo>=</mo>
        <msup>
          <mi>s</mi>
          <mn>2</mn>
        </msup>
        <mo>=</mo>
        <mfrac>
          <mrow>
            <munderover>
              <mo>∑<!-- ∑ --></mo>
              <mrow class="MJX-TeXAtom-ORD">
                <mi>i</mi>
                <mo>=</mo>
                <mn>1</mn>
              </mrow>
              <mi>n</mi>
            </munderover>
            <msup>
              <mrow>
                <mo>(</mo>
                <msub>
                  <mi>x</mi>
                  <mi>i</mi>
                </msub>
                <mo>−<!-- − --></mo>
                <mrow class="MJX-TeXAtom-ORD">
                  <mover>
                    <mi>x</mi>
                    <mo>¯<!-- ¯ --></mo>
                  </mover>
                </mrow>
                <mo>)</mo>
              </mrow>
              <mn>2</mn>
            </msup>
          </mrow>
          <mrow>
            <mi>n</mi>
            <mo>−<!-- − --></mo>
            <mn>1</mn>
          </mrow>
        </mfrac>
      </mtd>
    </mtr>
    <mtr>
      <mtd>
        <mtext>Standard deviation</mtext>
      </mtd>
      <mtd>
        <mi/>
        <mo>=</mo>
        <mi>s</mi>
        <mo>=</mo>
        <msqrt>
          <mtext>Variance</mtext>
        </msqrt>
      </mtd>
    </mtr>
  </mtable>
</math>
</div>

<p>The standard deviation is much easier to interpret than the variance since it is on the same scale as the original data.
Still, with its more complicated and less intuitive formula,
it might seem peculiar that the standard deviation is preferred in statistics over the mean absolute deviation.
It owes its preeminence to statistical theory:
mathematically, working with squared values is much more convenient than absolute values,
especially for statistical models.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="Nminus1">
<h5>Degrees of Freedom, and <em>n</em> or <em>n</em> – 1?</h5>
<p>In statistics books, there is always some discussion of why we have <em>n</em> – 1 in the denominator in the variance formula, instead of <em>n</em>, leading into the concept of <em>degrees of freedom</em>.<a data-type="indexterm" data-primary="n or n – 1, dividing by in variance formula" id="idm46522866236344"/><a data-type="indexterm" data-primary="degrees of freedom" id="idm46522866235512"/>
This distinction is not important since <em>n</em> is generally large enough that it won’t make much difference whether you divide by <em>n</em> or <em>n</em> – 1.
But in case you are interested, here is the story. It is based on the premise that you want to make estimates about a population, based on a sample.</p>

<p>If you use the intuitive denominator of <em>n</em> in the variance formula, you will underestimate the true value of the variance and the standard deviation in the population.
This is referred to as a <em>biased</em> estimate.<a data-type="indexterm" data-primary="biased estimates" id="idm46522866231784"/><a data-type="indexterm" data-primary="unbiased estimates" id="idm46522866231048"/>
However, if you divide by <em>n</em> – 1 instead of <em>n</em>, the variance becomes an <em>unbiased</em> estimate.</p>

<p>To fully explain why using <em>n</em> leads to a biased estimate involves the notion of degrees of freedom, which takes into account the number of constraints in computing an estimate.
In this case, there are <em>n</em> – 1 degrees of freedom since there is one constraint: the standard deviation depends on calculating the sample mean.
For most problems, data scientists do not need to worry about degrees of freedom.</p>
</div></aside>

<p>Neither the variance, the standard deviation, nor the mean absolute deviation is robust to outliers and extreme values (see <a data-type="xref" href="#Median">“Median and Robust Estimates”</a> for a discussion of robust estimates for location).
The variance and standard deviation are especially sensitive to outliers since they are based on the squared deviations.</p>

<p>A robust estimate of variability<a data-type="indexterm" data-primary="robust estimates of variability, median absolute deviation from the median" id="idm46522866225464"/><a data-type="indexterm" data-primary="median absolute deviation from the median (MAD)" id="idm46522866224792"/><a data-type="indexterm" data-primary="MAD" data-see="median absolute deviation from the median" id="idm46522866224024"/> is the <em>median absolute deviation from the median</em> or MAD:</p>
<div data-type="equation">
<math display="block">
  <mrow>
    <mtext>Median</mtext>
    <mspace width="4.pt"/>
    <mtext>absolute</mtext>
    <mspace width="4.pt"/>
    <mtext>deviation</mtext>
    <mo>=</mo>
    <mtext>Median</mtext>
    <mfenced separators="" open="(" close=")">
      <mfenced separators="" open="|" close="|">
        <msub><mi>x</mi> <mn>1</mn> </msub>
        <mo>-</mo>
        <mi>m</mi>
      </mfenced>
      <mo>,</mo>
      <mfenced separators="" open="|" close="|">
        <msub><mi>x</mi> <mn>2</mn> </msub>
        <mo>-</mo>
        <mi>m</mi>
      </mfenced>
      <mo>,</mo>
      <mo>...</mo>
      <mo>,</mo>
      <mfenced separators="" open="|" close="|">
        <msub><mi>x</mi> <mi>N</mi> </msub>
        <mo>-</mo>
        <mi>m</mi>
      </mfenced>
    </mfenced>
  </mrow>
</math>
</div>

<p>where <em>m</em> is the median.
Like the median, the MAD is not influenced by extreme values.
It is also possible to compute a trimmed standard deviation analogous to the trimmed mean (see <a data-type="xref" href="#Mean">“Mean”</a>).</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>The variance, the standard deviation, the mean absolute deviation, and the median absolute deviation from the median are not equivalent estimates,
even in the case where the data comes from a normal distribution.
In fact, the standard deviation is always greater than the mean absolute deviation, which itself is greater than the median absolute deviation.
Sometimes, the median absolute deviation is multiplied by a constant scaling factor to put the MAD on the same scale as the standard deviation in the case of a normal distribution. The commonly used factor of 1.4826 means that 50% of the normal distribution fall within the range <math alttext="plus-or-minus MAD">
  <mrow>
    <mo>±</mo>
    <mi> MAD </mi>
  </mrow>
</math> (see, e.g., <a href="https://oreil.ly/SfDk2"><em class="hyperlink">https://oreil.ly/SfDk2</em></a>).<a data-type="indexterm" data-primary="variability, estimates of" data-secondary="standard deviation and related estimates" data-startref="ix_vareststd" id="idm46522866197112"/></p>
</div>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Estimates Based on Percentiles"><div class="sect2" id="Percentiles">
<h2>Estimates Based on Percentiles</h2>

<p>A different approach to estimating dispersion is based on looking at the spread of the sorted data.<a data-type="indexterm" data-primary="variability, estimates of" data-secondary="percentiles" id="idm46522866193688"/>
Statistics based on sorted (ranked) data are referred to as <em>order statistics</em>.<a data-type="indexterm" data-primary="order statistics" id="idm46522866192136"/><a data-type="indexterm" data-primary="range" id="idm46522866191400"/>
The most basic measure is the <em>range</em>: the difference between the largest and smallest numbers.
The minimum and maximum values themselves are useful to know and are helpful in identifying outliers,  but the range is extremely sensitive to outliers and not very useful as a general measure of dispersion in the data.</p>

<p>To avoid the sensitivity to outliers, we can look at the range of the data after dropping values from each end.
Formally, these types of estimates are based on differences between <em>percentiles</em>.<a data-type="indexterm" data-primary="percentiles" data-secondary="estimates based on" id="idm46522866188680"/>
In a data set, the <em>P</em>th percentile is a value such that at least <em>P</em> percent of the values take on this value or less and at least (100 –  <em>P</em>) percent of the values take on this value or more.
For example, to find the 80th percentile, sort the data.
Then, starting with the smallest value, proceed 80 percent of the way to the largest value.
Note that the median is the same thing as the 50th percentile.
The percentile is essentially the same as a <em>quantile</em>, with quantiles indexed by fractions (so the .8 quantile is the same as the 80th percentile).<a data-type="indexterm" data-primary="quantiles" data-seealso="percentiles" id="idm46522866185448"/></p>

<p>A common measurement of variability is the difference between the 25th percentile and the 75th percentile, called the <em>interquartile range</em> (or IQR).
Here is a simple example: {3,1,5,3,6,7,2,9}.<a data-type="indexterm" data-primary="interquartile range" id="idm46522866183672"/>
We sort these to get {1,2,3,3,5,6,7,9}.
The 25th percentile is at 2.5, and the 75th percentile is at 6.5, so the interquartile range is 6.5 – 2.5 = 4.
Software can have slightly differing approaches that yield different answers (see the following tip); typically, these differences are smaller.</p>

<p>For very large data sets,
calculating exact percentiles can be computationally very expensive since it requires sorting all the data values.
Machine learning and statistical software use special algorithms, such as <a data-type="link" href="bibliography01.xhtml#Zhang-Wang-2007">[Zhang-Wang-2007]</a>, to get an approximate percentile that can be calculated very quickly and is guaranteed to have a certain accuracy.</p>
<div data-type="tip"><h1>Percentile: Precise Definition</h1>
<p>If we have an even <a data-type="indexterm" data-primary="percentiles" data-secondary="precise definition of" id="idm46522866179048"/>number of data (<em>n</em> is even),
then the percentile is ambiguous under the preceding definition.
In fact, we could take on any value between the order statistics <math alttext="x Subscript left-parenthesis j right-parenthesis">
  <msub><mi>x</mi> <mrow><mo>(</mo><mi>j</mi><mo>)</mo></mrow> </msub>
</math> and <math alttext="x Subscript left-parenthesis j plus 1 right-parenthesis">
  <msub><mi>x</mi> <mrow><mo>(</mo><mi>j</mi><mo>+</mo><mn>1</mn><mo>)</mo></mrow> </msub>
</math> where <em>j</em> satisfies:</p>
<div data-type="equation">
<math display="block">
  <mrow>
    <mspace width="1.em"/>
    <mspace width="1.em"/>
    <mspace width="1.em"/>
    <mspace width="1.em"/>
    <mspace width="1.em"/>
    <mspace width="1.em"/>
    <mspace width="1.em"/>
    <mspace width="1.em"/>
    <mn>100</mn>
    <mo>*</mo>
    <mfrac><mi>j</mi> <mi>n</mi></mfrac>
    <mo>≤</mo>
    <mi>P</mi>
    <mo>&lt;</mo>
    <mn>100</mn>
    <mo>*</mo>
    <mfrac><mrow><mi>j</mi><mo>+</mo><mn>1</mn></mrow> <mi>n</mi></mfrac>
  </mrow>
</math>
</div>

<p>Formally, the percentile is the weighted average:</p>
<div data-type="equation">
<math display="block">
  <mrow>
    <mspace width="1.em"/>
    <mspace width="1.em"/>
    <mspace width="1.em"/>
    <mspace width="1.em"/>
    <mspace width="1.em"/>
    <mspace width="1.em"/>
    <mspace width="1.em"/>
    <mspace width="1.em"/>
    <mtext>Percentile</mtext>
    <mrow>
      <mo>(</mo>
      <mi>P</mi>
      <mo>)</mo>
    </mrow>
    <mo>=</mo>
    <mfenced separators="" open="(" close=")">
      <mn>1</mn>
      <mo>-</mo>
      <mi>w</mi>
    </mfenced>
    <msub><mi>x</mi> <mrow><mo>(</mo><mi>j</mi><mo>)</mo></mrow> </msub>
    <mo>+</mo>
    <mi>w</mi>
    <msub><mi>x</mi> <mrow><mo>(</mo><mi>j</mi><mo>+</mo><mn>1</mn><mo>)</mo></mrow> </msub>
  </mrow>
</math>
</div>

<p>for some weight <em>w</em> between 0 and 1.
Statistical software has slightly differing approaches to choosing <em>w</em>.
In fact, the <em>R</em> function <code>quantile</code> offers nine different alternatives to compute the quantile.<a data-type="indexterm" data-primary="quantiles" data-secondary="functions for" id="idm46522866138648"/>
Except for small data sets,
you don’t usually need to worry about the precise way a percentile is calculated. At the time of this writing, <em>Python</em>’s <code>numpy.quantile</code> supports only one approach, linear interpolation.</p>
</div>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Example: Variability Estimates of State Population"><div class="sect2" id="idm46522866136488">
<h2>Example: Variability Estimates of State Population</h2>

<p><a data-type="xref" href="#state">Table 1-3</a> (repeated from <a data-type="xref" href="#state_table">Table 1-2</a> for convenience) shows the first few rows in the data set containing population and murder rates for each state.<a data-type="indexterm" data-primary="variability, estimates of" data-secondary="example, murder rates by state population" id="idm46522866133288"/></p>
<table id="state">
<caption><span class="label">Table 1-3. </span>A few rows of the <code>data.frame</code> state of population and murder rate by state</caption>
<thead>
<tr>
<th/>
<th>State</th>
<th>Population</th>
<th>Murder rate</th>
<th>Abbreviation</th>
</tr>
</thead>
<tbody>
<tr>
<td><p>1</p></td>
<td><p>Alabama</p></td>
<td><p>4,779,736</p></td>
<td><p>5.7</p></td>
<td><p>AL</p></td>
</tr>
<tr>
<td><p>2</p></td>
<td><p>Alaska</p></td>
<td><p>710,231</p></td>
<td><p>5.6</p></td>
<td><p>AK</p></td>
</tr>
<tr>
<td><p>3</p></td>
<td><p>Arizona</p></td>
<td><p>6,392,017</p></td>
<td><p>4.7</p></td>
<td><p>AZ</p></td>
</tr>
<tr>
<td><p>4</p></td>
<td><p>Arkansas</p></td>
<td><p>2,915,918</p></td>
<td><p>5.6</p></td>
<td><p>AR</p></td>
</tr>
<tr>
<td><p>5</p></td>
<td><p>California</p></td>
<td><p>37,253,956</p></td>
<td><p>4.4</p></td>
<td><p>CA</p></td>
</tr>
<tr>
<td><p>6</p></td>
<td><p>Colorado</p></td>
<td><p>5,029,196</p></td>
<td><p>2.8</p></td>
<td><p>CO</p></td>
</tr>
<tr>
<td><p>7</p></td>
<td><p>Connecticut</p></td>
<td><p>3,574,097</p></td>
<td><p>2.4</p></td>
<td><p>CT</p></td>
</tr>
<tr>
<td><p>8</p></td>
<td><p>Delaware</p></td>
<td><p>897,934</p></td>
<td><p>5.8</p></td>
<td><p>DE</p></td>
</tr>
</tbody>
</table>

<p>Using <em>R</em>’s  built-in functions for the<a data-type="indexterm" data-primary="interquartile range" data-secondary="calculating" id="idm46522866098856"/> standard deviation, the interquartile range (IQR), and the median absolute deviation from the median (MAD),
we can compute estimates of variability for the state population data:</p>

<pre data-type="programlisting" data-code-language="r"><code class="o">&gt;</code> <code class="nf">sd</code><code class="p">(</code><code class="n">state</code><code class="p">[[</code><code class="s">'Population'</code><code class="p">]])</code>
<code class="p">[</code><code class="m">1</code><code class="p">]</code> <code class="m">6848235</code>
<code class="o">&gt;</code> <code class="nf">IQR</code><code class="p">(</code><code class="n">state</code><code class="p">[[</code><code class="s">'Population'</code><code class="p">]])</code>
<code class="p">[</code><code class="m">1</code><code class="p">]</code> <code class="m">4847308</code>
<code class="o">&gt;</code> <code class="nf">mad</code><code class="p">(</code><code class="n">state</code><code class="p">[[</code><code class="s">'Population'</code><code class="p">]])</code>
<code class="p">[</code><code class="m">1</code><code class="p">]</code> <code class="m">3849870</code></pre>

<p>The <code>pandas</code> data frame provides methods for calculating standard deviation and quantiles. Using the quantiles, we can easily determine the IQR. For the robust MAD, we use the function <code>robust.scale.mad</code> from the <code>statsmodels</code> package:<a data-type="indexterm" data-primary="robust estimates of variance, calculating robust MAD" id="idm46522866065016"/></p>

<pre data-type="programlisting" data-code-language="python"><code class="n">state</code><code class="p">[</code><code class="s1">'Population'</code><code class="p">]</code><code class="o">.</code><code class="n">std</code><code class="p">()</code>
<code class="n">state</code><code class="p">[</code><code class="s1">'Population'</code><code class="p">]</code><code class="o">.</code><code class="n">quantile</code><code class="p">(</code><code class="mf">0.75</code><code class="p">)</code> <code class="o">-</code> <code class="n">state</code><code class="p">[</code><code class="s1">'Population'</code><code class="p">]</code><code class="o">.</code><code class="n">quantile</code><code class="p">(</code><code class="mf">0.25</code><code class="p">)</code>
<code class="n">robust</code><code class="o">.</code><code class="n">scale</code><code class="o">.</code><code class="n">mad</code><code class="p">(</code><code class="n">state</code><code class="p">[</code><code class="s1">'Population'</code><code class="p">])</code></pre>

<p>The standard deviation is almost twice as large as the MAD (in <em>R</em>, by default, the scale of the MAD is adjusted to be on the same scale as the mean).
This is not surprising since the standard deviation is sensitive to outliers.</p>
<aside data-type="sidebar" epub:type="sidebar" class="pagebreak-before less_space"><div class="sidebar" id="idm46522865993848">
<h5>Key Ideas</h5>
<ul>
<li>
<p>Variance and standard deviation are the most widespread and routinely reported statistics of variability.</p>
</li>
<li>
<p>Both are sensitive to outliers.</p>
</li>
<li>
<p>More robust metrics include mean absolute deviation, median absolute deviation from the median, and percentiles (quantiles).</p>
</li>
</ul>
</div></aside>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Further Reading"><div class="sect2" id="idm46522865989256">
<h2>Further Reading</h2>

<ul>
<li>
<p>David Lane’s online statistics resource has a <a href="https://oreil.ly/o2fBI">section on percentiles</a>.</p>
</li>
<li>
<p>Kevin Davenport has a <a href="https://oreil.ly/E7zcG">useful post on <em>R</em>-Bloggers</a> about deviations from the median and their robust properties.</p>
</li>
</ul>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Exploring the Data Distribution"><div class="sect1" id="idm46522865983960">
<h1>Exploring the Data Distribution</h1>

<p>Each of the estimates we’ve covered sums up the data in a single number to describe the location or variability of the data.<a data-type="indexterm" data-primary="variability, estimates of" data-startref="ix_varest" id="idm46522865982280"/><a data-type="indexterm" data-primary="exploratory data analysis" data-secondary="estimates of variability" data-startref="ix_expdaestvar" id="idm46522865981336"/>
It is also useful to explore how the data is distributed overall.<a data-type="indexterm" data-primary="exploratory data analysis" data-secondary="data distribution" id="ix_expdadist"/><a data-type="indexterm" data-primary="data distribution" id="ix_dadis"/></p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46522865977704">
<h5>Key Terms for Exploring the Distribution</h5><dl>
<dt class="horizontal"><strong><em>Boxplot</em></strong></dt>
<dd>
<p>A plot introduced by Tukey as a quick way to visualize the distribution of data.<a data-type="indexterm" data-primary="boxplots" id="idm46522865974760"/></p>
<dl>
<dt>Synonym</dt>
<dd>
<p>box and whiskers plot</p>
</dd>
</dl>
</dd>
<dt class="horizontal"><strong><em>Frequency table</em></strong></dt>
<dd>
<p>A tally of the count of numeric data values that fall into a set of intervals (bins).<a data-type="indexterm" data-primary="frequency tables" id="idm46522865970456"/></p>
</dd>
<dt class="horizontal"><strong><em>Histogram</em></strong></dt>
<dd>
<p>A plot of the frequency table with the bins on the x-axis and the count (or proportion) on the y-axis. While visually similar, bar charts should not be confused with histograms.<a data-type="indexterm" data-primary="histograms" id="idm46522865967752"/>
See <a data-type="xref" href="#ExploringBinaryCategoricalData">“Exploring Binary and Categorical Data”</a> for a discussion of the difference.</p>
</dd>
<dt class="horizontal"><strong><em>Density plot</em></strong></dt>
<dd>
<p>A smoothed version of the histogram, often based on a <em>kernel density estimate</em>.<a data-type="indexterm" data-primary="density plots" id="idm46522865963832"/><a data-type="indexterm" data-primary="kernel density estimates" data-seealso="density plots" id="idm46522865963096"/></p>
</dd>
</dl>
</div></aside>








<section data-type="sect2" data-pdf-bookmark="Percentiles and Boxplots"><div class="sect2" id="Boxplots">
<h2>Percentiles and Boxplots</h2>

<p>In <a data-type="xref" href="#Percentiles">“Estimates Based on Percentiles”</a>, we explored how percentiles can be used to measure the spread of the data.<a data-type="indexterm" data-primary="data distribution" data-secondary="percentiles and boxplots" id="ix_dadisper"/><a data-type="indexterm" data-primary="percentiles" data-secondary="and boxplots" id="ix_perbox"/><a data-type="indexterm" data-primary="boxplots" data-secondary="percentiles and" id="ix_boxper"/>
Percentiles are also valuable for summarizing the entire distribution.
It is common to report the quartiles (25th, 50th, and 75th percentiles) and the deciles (the 10th, 20th, …, 90th percentiles).
Percentiles are especially valuable for summarizing the <em>tails</em> (the outer range) of the distribution.<a data-type="indexterm" data-primary="tails" data-secondary="summarizing with percentiles" id="idm46522865954280"/>
Popular culture has coined the term <em>one-percenters</em> to refer to the people in the top 99th percentile of wealth.</p>

<p><a data-type="xref" href="#PercentileTable">Table 1-4</a> displays some percentiles of the murder rate by state.
In <em>R</em>, this would be produced by the <code>quantile</code> function:</p>

<pre data-type="programlisting" data-code-language="r"><code class="nf">quantile</code><code class="p">(</code><code class="n">state</code><code class="p">[[</code><code class="s">'Murder.Rate'</code><code class="p">]],</code> <code class="n">p</code><code class="o">=</code><code class="nf">c</code><code class="p">(</code><code class="m">.05</code><code class="p">,</code> <code class="m">.25</code><code class="p">,</code> <code class="m">.5</code><code class="p">,</code> <code class="m">.75</code><code class="p">,</code> <code class="m">.95</code><code class="p">))</code>
   <code class="m">5</code><code class="o">%   25%</code>   <code class="m">50</code><code class="o">%   75%</code>   <code class="m">95</code>%
<code class="m">1.600</code> <code class="m">2.425</code> <code class="m">4.000</code> <code class="m">5.550</code> <code class="m">6.510</code></pre>

<p>The <code>pandas</code> data frame method <code>quantile</code> provides it in <em>Python</em>:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">state</code><code class="p">[</code><code class="s1">'Murder.Rate'</code><code class="p">]</code><code class="o">.</code><code class="n">quantile</code><code class="p">([</code><code class="mf">0.05</code><code class="p">,</code> <code class="mf">0.25</code><code class="p">,</code> <code class="mf">0.5</code><code class="p">,</code> <code class="mf">0.75</code><code class="p">,</code> <code class="mf">0.95</code><code class="p">])</code></pre>
<table id="PercentileTable">
<caption><span class="label">Table 1-4. </span>Percentiles of murder rate by state</caption>
<thead>
<tr>
<th>5%</th>
<th>25%</th>
<th>50%</th>
<th>75%</th>
<th>95%</th>
</tr>
</thead>
<tbody>
<tr>
<td><p>1.60</p></td>
<td><p>2.42</p></td>
<td><p>4.00</p></td>
<td><p>5.55</p></td>
<td><p>6.51</p></td>
</tr>
</tbody>
</table>

<p>The median is 4 murders per 100,000 people, although there is quite a bit of variability: the 5th percentile is only 1.6 and the 95th percentile is 6.51.</p>

<p><em>Boxplots</em>, introduced by Tukey <a data-type="link" href="bibliography01.xhtml#Tukey-1977">[Tukey-1977]</a>, are based on percentiles and give a quick way to visualize the distribution of data.
<a data-type="xref" href="#BoxplotFigure">Figure 1-2</a> shows a boxplot of the population by state produced by <em>R</em>:</p>

<pre data-type="programlisting" data-code-language="r"><code class="nf">boxplot</code><code class="p">(</code><code class="n">state</code><code class="p">[[</code><code class="s">'Population'</code><code class="p">]]</code><code class="o">/</code><code class="m">1000000</code><code class="p">,</code> <code class="n">ylab</code><code class="o">=</code><code class="s">'Population (millions)'</code><code class="p">)</code></pre>

<p><code>pandas</code> provides a number of basic exploratory plots for data frame; one of them is boxplots:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">ax</code> <code class="o">=</code> <code class="p">(</code><code class="n">state</code><code class="p">[</code><code class="s1">'Population'</code><code class="p">]</code><code class="o">/</code><code class="mi">1</code><code class="n">_000_000</code><code class="p">)</code><code class="o">.</code><code class="n">plot</code><code class="o">.</code><code class="n">box</code><code class="p">()</code>
<code class="n">ax</code><code class="o">.</code><code class="n">set_ylabel</code><code class="p">(</code><code class="s1">'Population (millions)'</code><code class="p">)</code></pre>

<figure class="width-50"><div id="BoxplotFigure" class="figure">
<img src="Images/psd2_0102.png" alt="Boxplot of state populations" width="879" height="1192"/>
<h6><span class="label">Figure 1-2. </span>Boxplot of state populations</h6>
</div></figure>

<p>From this boxplot we can immediately see that the median state population is about 5 million, half the states fall between about 2 million and about 7 million, and there are some high population outliers. The top and bottom of the box are the 75th and 25th percentiles, respectively.
The median is shown by the horizontal line in the box.
The dashed lines, referred to as <em>whiskers</em>, extend from the top and bottom of the box to indicate the range for the bulk of the data.<a data-type="indexterm" data-primary="whiskers (in boxplots)" id="idm46522865785144"/>
There are many variations of a boxplot; see, for example, the documentation for the <em>R</em> function <code>boxplot</code> <a data-type="link" href="bibliography01.xhtml#R-base-2015">[R-base-2015]</a>.
By default, the <em>R</em> function extends the whiskers to the furthest point beyond the box, except that it will not go beyond 1.5 times the IQR. <em>Matplotlib</em> uses the same implementation; other software may use a different rule.</p>

<p>Any data outside of the whiskers is plotted as single points or circles (often considered outliers).<a data-type="indexterm" data-primary="boxplots" data-secondary="percentiles and" data-startref="ix_boxper" id="idm46522865757528"/><a data-type="indexterm" data-primary="percentiles" data-secondary="and boxplots" data-startref="ix_perbox" id="idm46522865756280"/><a data-type="indexterm" data-primary="data distribution" data-secondary="percentiles and boxplots" data-startref="ix_dadisper" id="idm46522865755064"/><a data-type="indexterm" data-primary="outliers" data-secondary="in boxplots" id="idm46522865753880"/></p>
</div></section>













<section data-type="sect2" class="pagebreak-before less_space" data-pdf-bookmark="Frequency Tables and Histograms"><div class="sect2" id="Histograms">
<h2>Frequency Tables and Histograms</h2>

<p>A frequency table of a variable divides up the variable range into equally spaced segments and tells us how many values fall within each segment.<a data-type="indexterm" data-primary="data distribution" data-secondary="frequency table and histogram" id="ix_dadisfth"/><a data-type="indexterm" data-primary="frequency tables" data-secondary="example, murder rates by state" id="idm46522865749336"/>
<a data-type="xref" href="#FreqTable">Table 1-5</a> shows a frequency table of the population by state computed in <em>R</em>:</p>

<pre data-type="programlisting" data-code-language="r"><code class="n">breaks</code> <code class="o">&lt;-</code> <code class="nf">seq</code><code class="p">(</code><code class="n">from</code><code class="o">=</code><code class="nf">min</code><code class="p">(</code><code class="n">state</code><code class="p">[[</code><code class="s">'Population'</code><code class="p">]]),</code>
                <code class="n">to</code><code class="o">=</code><code class="nf">max</code><code class="p">(</code><code class="n">state</code><code class="p">[[</code><code class="s">'Population'</code><code class="p">]]),</code> <code class="n">length</code><code class="o">=</code><code class="m">11</code><code class="p">)</code>
<code class="n">pop_freq</code> <code class="o">&lt;-</code> <code class="nf">cut</code><code class="p">(</code><code class="n">state</code><code class="p">[[</code><code class="s">'Population'</code><code class="p">]],</code> <code class="n">breaks</code><code class="o">=</code><code class="n">breaks</code><code class="p">,</code>
                <code class="n">right</code><code class="o">=</code><code class="kc">TRUE</code><code class="p">,</code> <code class="n">include.lowest</code><code class="o">=</code><code class="kc">TRUE</code><code class="p">)</code>
<code class="nf">table</code><code class="p">(</code><code class="n">pop_freq</code><code class="p">)</code></pre>

<p>The function <code>pandas.cut</code> creates a series that maps the values into the segments. Using the method <code>value_counts</code>, we get the frequency table:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">binnedPopulation</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">cut</code><code class="p">(</code><code class="n">state</code><code class="p">[</code><code class="s1">'Population'</code><code class="p">],</code> <code class="mi">10</code><code class="p">)</code>
<code class="n">binnedPopulation</code><code class="o">.</code><code class="n">value_counts</code><code class="p">()</code></pre>
<table id="FreqTable">
<caption><span class="label">Table 1-5. </span>A frequency table of population by state</caption>
<thead>
<tr>
<th>BinNumber</th>
<th>BinRange</th>
<th>Count</th>
<th>States</th>
</tr>
</thead>
<tbody>
<tr>
<td><p>1</p></td>
<td><p>563,626–4,232,658</p></td>
<td><p>24</p></td>
<td><p>WY,VT,ND,AK,SD,DE,MT,RI,NH,ME,HI,ID,NE,WV,NM,NV,UT,KS,AR,MS,IA,CT,OK,OR</p></td>
</tr>
<tr>
<td><p>2</p></td>
<td><p>4,232,659–7,901,691</p></td>
<td><p>14</p></td>
<td><p>KY,LA,SC,AL,CO,MN,WI,MD,MO,TN,AZ,IN,MA,WA</p></td>
</tr>
<tr>
<td><p>3</p></td>
<td><p>7,901,692–11,570,724</p></td>
<td><p>6</p></td>
<td><p>VA,NJ,NC,GA,MI,OH</p></td>
</tr>
<tr>
<td><p>4</p></td>
<td><p>11,570,725–15,239,757</p></td>
<td><p>2</p></td>
<td><p>PA,IL</p></td>
</tr>
<tr>
<td><p>5</p></td>
<td><p>15,239,758–18,908,790</p></td>
<td><p>1</p></td>
<td><p>FL</p></td>
</tr>
<tr>
<td><p>6</p></td>
<td><p>18,908,791–22,577,823</p></td>
<td><p>1</p></td>
<td><p>NY</p></td>
</tr>
<tr>
<td><p>7</p></td>
<td><p>22,577,824–26,246,856</p></td>
<td><p>1</p></td>
<td><p>TX</p></td>
</tr>
<tr>
<td><p>8</p></td>
<td><p>26,246,857–29,915,889</p></td>
<td><p>0</p></td>
<td/>
</tr>
<tr>
<td><p>9</p></td>
<td><p>29,915,890–33,584,922</p></td>
<td><p>0</p></td>
<td/>
</tr>
<tr>
<td><p>10</p></td>
<td><p>33,584,923–37,253,956</p></td>
<td><p>1</p></td>
<td><p>CA</p></td>
</tr>
</tbody>
</table>

<p>The least populous state is Wyoming, with 563,626 people, and the most populous is California, with 37,253,956 people.<a data-type="indexterm" data-primary="bins, in frequency tables and histograms" id="idm46522865572392"/>
This gives us a range of 37,253,956 – 563,626 = 36,690,330, which we must divide up into equal size bins—let’s say 10 bins.
With 10 equal size bins, each bin will have a width of 3,669,033, so the first bin will span from 563,626 to 4,232,658.
By contrast, the top bin, 33,584,923 to 37,253,956, has only one state: California.
The two bins immediately below California are empty, until we reach Texas.
It is important to include the empty bins; the fact that there are no values in those bins is useful information.
It can also be useful to experiment with different bin sizes.
If they are too large, important features of the distribution can be obscured.  If they are too small, the result is too granular, and the ability to see the bigger picture is lost.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Both frequency tables and percentiles summarize the data by creating bins.
In general, quartiles and deciles will have the same count in each bin (equal-count bins), but the bin sizes will be different.
The frequency table, by contrast, will have different counts in the bins (equal-size bins), and the bin sizes will be the same.</p>
</div>

<p>A histogram is a way to visualize a frequency table, with bins on the x-axis and the data count on the y-axis.<a data-type="indexterm" data-primary="histograms" data-secondary="visualizing frequency tables with" id="idm46522865568456"/>
In <a data-type="xref" href="#HistogramFigure">Figure 1-3</a>, for example, the bin centered at 10 million <span class="keep-together">(1e+07)</span> runs from roughly 8 million to 12 million, and there are six states in that bin.  To create a histogram corresponding to <a data-type="xref" href="#FreqTable">Table 1-5</a> in <em>R</em>, use the <code>hist</code> function with the <code>breaks</code> argument:</p>

<pre data-type="programlisting" data-code-language="r"><code class="nf">hist</code><code class="p">(</code><code class="n">state</code><code class="p">[[</code><code class="s">'Population'</code><code class="p">]],</code> <code class="n">breaks</code><code class="o">=</code><code class="n">breaks</code><code class="p">)</code></pre>

<p><code>pandas</code> supports histograms for data frames with the <code>DataFrame.plot.hist</code> method. <a data-type="indexterm" data-primary="data frames" data-secondary="histograms for" id="idm46522865557128"/>Use the keyword argument <code>bins</code> to define the number of bins. The various plot methods return an axis object that allows further fine-tuning of the visualization using <span class="keep-together"><code>Matplotlib</code></span>:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">ax</code> <code class="o">=</code> <code class="p">(</code><code class="n">state</code><code class="p">[</code><code class="s1">'Population'</code><code class="p">]</code> <code class="o">/</code> <code class="mi">1</code><code class="n">_000_000</code><code class="p">)</code><code class="o">.</code><code class="n">plot</code><code class="o">.</code><code class="n">hist</code><code class="p">(</code><code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">4</code><code class="p">,</code> <code class="mi">4</code><code class="p">))</code>
<code class="n">ax</code><code class="o">.</code><code class="n">set_xlabel</code><code class="p">(</code><code class="s1">'Population (millions)'</code><code class="p">)</code></pre>

<p>The histogram is shown in <a data-type="xref" href="#HistogramFigure">Figure 1-3</a>. In general, histograms are plotted such that:</p>

<ul>
<li>
<p>Empty bins are included in the graph.</p>
</li>
<li>
<p>Bins are of equal width.<a data-type="indexterm" data-primary="histograms" data-secondary="plotting of" id="idm46522865486328"/></p>
</li>
<li>
<p>The number of bins (or, equivalently, bin size) is up to the user.</p>
</li>
<li>
<p>Bars are contiguous—no empty space shows between bars, unless there is an empty bin.</p>
</li>
</ul>

<figure class="width-75"><div id="HistogramFigure" class="figure">
<img src="Images/psd2_0103.png" alt="Histogram of state populations" width="1200" height="1200"/>
<h6><span class="label">Figure 1-3. </span>Histogram of state populations</h6>
</div></figure>
<div data-type="tip"><h1>Statistical Moments</h1>
<p>In statistical theory, location and variability are referred to as the first and second <em>moments</em> of a distribution.<a data-type="indexterm" data-primary="moments (of a distribution)" id="idm46522865479144"/>
The third and fourth moments are called <em>skewness</em> and <em>kurtosis</em>.<a data-type="indexterm" data-primary="skewness" id="idm46522865477480"/><a data-type="indexterm" data-primary="kurtosis" id="idm46522865476744"/>
Skewness refers to whether the data is skewed to larger or smaller values, and kurtosis indicates the propensity of the data to have extreme values.
Generally, metrics are not used to measure skewness and kurtosis;
instead, these are discovered through visual displays such as Figures <a data-type="xref" data-xrefstyle="select:labelnumber" href="#BoxplotFigure">1-2</a> and <a data-type="xref" data-xrefstyle="select:labelnumber" href="#HistogramFigure">1-3</a>.</p>
</div>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Density Plots and Estimates"><div class="sect2" id="idm46522865752344">
<h2>Density Plots and Estimates</h2>

<p>Related to the histogram is a density plot, which shows the distribution of data values as a continuous line.<a data-type="indexterm" data-primary="data distribution" data-secondary="frequency table and histogram" data-startref="ix_dadisfth" id="idm46522865471160"/><a data-type="indexterm" data-primary="density plots" data-secondary="and estimates" id="idm46522865469944"/>
A density <a data-type="indexterm" data-primary="kernel density estimates" id="idm46522865468872"/><a data-type="indexterm" data-primary="data distribution" data-secondary="density plots and estimates" id="ix_dadisdpe"/>plot can be thought of as a smoothed histogram,
although it is typically computed directly from the data through a <em>kernel density estimate</em> (see <a data-type="link" href="bibliography01.xhtml#Duong-2001">[Duong-2001]</a> for a short tutorial).
<a data-type="xref" href="#DensityFigure">Figure 1-4</a> displays a density estimate superposed on a histogram.
In <em>R</em>, you can compute a density estimate using the <span class="keep-together"><code>density</code></span> function:</p>

<pre data-type="programlisting" data-code-language="r"><code class="nf">hist</code><code class="p">(</code><code class="n">state</code><code class="p">[[</code><code class="s">'Murder.Rate'</code><code class="p">]],</code> <code class="n">freq</code><code class="o">=</code><code class="kc">FALSE</code><code class="p">)</code>
<code class="nf">lines</code><code class="p">(</code><code class="nf">density</code><code class="p">(</code><code class="n">state</code><code class="p">[[</code><code class="s">'Murder.Rate'</code><code class="p">]]),</code> <code class="n">lwd</code><code class="o">=</code><code class="m">3</code><code class="p">,</code> <code class="n">col</code><code class="o">=</code><code class="s">'blue'</code><code class="p">)</code></pre>

<p><code>pandas</code> provides the <code>density</code> method to create a density plot. Use the argument <code>bw_method</code> to control the smoothness of the density curve:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">ax</code><code> </code><code class="o">=</code><code> </code><code class="n">state</code><code class="p">[</code><code class="s1">'</code><code class="s1">Murder.Rate</code><code class="s1">'</code><code class="p">]</code><code class="o">.</code><code class="n">plot</code><code class="o">.</code><code class="n">hist</code><code class="p">(</code><code class="n">density</code><code class="o">=</code><code class="bp">True</code><code class="p">,</code><code> </code><code class="n">xlim</code><code class="o">=</code><code class="p">[</code><code class="mi">0</code><code class="p">,</code><code class="mi">12</code><code class="p">]</code><code class="p">,</code><code> </code><code class="n">bins</code><code class="o">=</code><code class="nb">range</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code><code class="mi">12</code><code class="p">)</code><code class="p">)</code><code>
</code><code class="n">state</code><code class="p">[</code><code class="s1">'</code><code class="s1">Murder.Rate</code><code class="s1">'</code><code class="p">]</code><code class="o">.</code><code class="n">plot</code><code class="o">.</code><code class="n">density</code><code class="p">(</code><code class="n">ax</code><code class="o">=</code><code class="n">ax</code><code class="p">)</code><code> </code><a class="co" id="co_exploratory_data_analysis_CO1-1" href="#callout_exploratory_data_analysis_CO1-1"><img src="Images/1.png" alt="1" width="12" height="12"/></a><code>
</code><code class="n">ax</code><code class="o">.</code><code class="n">set_xlabel</code><code class="p">(</code><code class="s1">'</code><code class="s1">Murder Rate (per 100,000)</code><code class="s1">'</code><code class="p">)</code></pre>
<dl class="calloutlist">
<dt><a class="co" id="callout_exploratory_data_analysis_CO1-1" href="#co_exploratory_data_analysis_CO1-1"><img src="Images/1.png" alt="1" width="12" height="12"/></a></dt>
<dd><p>Plot functions often take an optional axis (<code>ax</code>) argument, which will cause the plot to be added to the same graph.</p></dd>
</dl>

<p>A key distinction from the histogram plotted in <a data-type="xref" href="#HistogramFigure">Figure 1-3</a> is the scale of the y-axis:
a density plot corresponds to plotting the histogram as a proportion rather than counts (you specify this in <em>R</em> using the argument <code>freq=FALSE</code>).  Note that the total area under the density curve = 1, and instead of counts in bins you calculate areas under the curve between any two points on the x-axis, which correspond to the proportion of the distribution lying between those two points.</p>

<figure class="width-75"><div id="DensityFigure" class="figure">
<img src="Images/psd2_0104.png" alt="Density of state murder rates" width="1176" height="1154"/>
<h6><span class="label">Figure 1-4. </span>Density of state murder rates</h6>
</div></figure>
<div data-type="tip"><h1>Density Estimation</h1>
<p>Density estimation is a rich topic with a long history in statistical literature.
In fact, over 20 <em>R</em> packages have been published that offer functions for density estimation.
<a data-type="link" href="bibliography01.xhtml#Deng-Wickham-2011">[Deng-Wickham-2011]</a> give a comprehensive review of <em>R</em> packages, with a particular recommendation for <code>ASH</code> or <code>KernSmooth</code>. The density estimation methods in <code>pandas</code> and <code>scikit-learn</code> also offer good implementations.
For many data science problems, there is no need to worry about the various types of density estimates; it suffices to use the base <span class="keep-together">functions</span>.</p>
</div>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46522865292120">
<h5>Key Ideas</h5>
<ul>
<li>
<p>A frequency histogram plots frequency counts on the y-axis and variable values on the x-axis; it gives a sense of the distribution of the data at a glance.</p>
</li>
<li>
<p>A frequency table is a tabular version of the frequency counts found in a <span class="keep-together">histogram</span>.</p>
</li>
<li>
<p>A boxplot—with the top and bottom of the box at the 75th and 25th percentiles, respectively—also gives a quick sense of the distribution of the data; it is often used in side-by-side displays to compare distributions.</p>
</li>
<li>
<p>A density plot is a smoothed version of a histogram; it requires a function to estimate a plot based on the data (multiple estimates are possible, of course).</p>
</li>
</ul>
</div></aside>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Further Reading"><div class="sect2" id="idm46522865472584">
<h2>Further Reading</h2>

<ul>
<li>
<p>A SUNY Oswego professor provides a <a href="https://oreil.ly/wTpnE">step-by-step guide to creating a boxplot</a>.</p>
</li>
<li>
<p>Density estimation in <em>R</em> is covered in <a href="https://oreil.ly/TbWYS">Henry Deng and Hadley Wickham’s paper of the same name</a>.</p>
</li>
<li>
<p><em>R</em>-Bloggers has a <a href="https://oreil.ly/Ynp-n">useful post on histograms in <em>R</em></a>, including customization elements, such as binning (breaks).</p>
</li>
<li>
<p><em>R</em>-Bloggers also has a <a href="https://oreil.ly/0DSb2">similar post on boxplots in <em>R</em></a>.</p>
</li>
<li>
<p>Matthew Conlen published an <a href="https://oreil.ly/bC9nu">interactive presentation</a> that demonstrates the effect of choosing different kernels and bandwidth on kernel density estimates.</p>
</li>
</ul>
</div></section>





</div></section>













<section data-type="sect1" class="pagebreak-before less_space" data-pdf-bookmark="Exploring Binary and Categorical Data"><div class="sect1" id="ExploringBinaryCategoricalData">
<h1>Exploring Binary and Categorical Data</h1>

<p>For categorical data, simple proportions or percentages tell the story of the data.<a data-type="indexterm" data-primary="data distribution" data-secondary="density plots and estimates" data-startref="ix_dadisdpe" id="idm46522865272376"/><a data-type="indexterm" data-primary="exploratory data analysis" data-secondary="data distribution" data-startref="ix_expdadist" id="idm46522865271064"/><a data-type="indexterm" data-primary="data distribution" data-startref="ix_dadis" id="idm46522865269832"/><a data-type="indexterm" data-primary="exploratory data analysis" data-secondary="categorical and binary data" id="ix_expcat"/><a data-type="indexterm" data-primary="categorical data" data-secondary="exploring" id="ix_catexp"/><a data-type="indexterm" data-primary="binary data" data-secondary="exploring" id="ix_binda"/></p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46522865265080">
<h5>Key Terms for Exploring Categorical Data</h5><dl>
<dt class="horizontal"><strong><em>Mode</em></strong></dt>
<dd>
<p>The most commonly occurring category <a data-type="indexterm" data-primary="mode" id="idm46522865262136"/>or value in a data set.</p>
</dd>
<dt class="horizontal"><strong><em>Expected value</em></strong></dt>
<dd>
<p>When the categories can be associated with a numeric value, this gives an average value based on a category’s probability of occurrence.<a data-type="indexterm" data-primary="expected value" id="idm46522865259304"/></p>
</dd>
<dt class="horizontal"><strong><em>Bar charts</em></strong></dt>
<dd>
<p>The frequency or proportion for each category plotted as bars.<a data-type="indexterm" data-primary="bar charts" id="idm46522865256792"/></p>
</dd>
<dt class="horizontal"><strong><em>Pie charts</em></strong></dt>
<dd>
<p>The frequency or proportion for each category plotted as wedges in a pie.<a data-type="indexterm" data-primary="pie charts" id="idm46522865254280"/></p>
</dd>
</dl>
</div></aside>

<p>Getting a summary of a binary variable or a categorical variable with a few categories is a fairly easy matter: we just figure out the proportion of 1s, or the proportions of the important categories.
For example, <a data-type="xref" href="#AirportDelays">Table 1-6</a> shows the percentage of delayed flights by the cause of delay at Dallas/Fort Worth Airport since 2010.
Delays are categorized as being due to factors under carrier control, air traffic control (ATC) system delays, weather, security, or a late inbound aircraft.</p>
<table id="AirportDelays">
<caption><span class="label">Table 1-6. </span>Percentage of delays by cause at Dallas/Fort Worth Airport</caption>
<thead>
<tr>
<th>Carrier</th>
<th>ATC</th>
<th>Weather</th>
<th>Security</th>
<th>Inbound</th>
</tr>
</thead>
<tbody>
<tr>
<td><p>23.02</p></td>
<td><p>30.40</p></td>
<td><p>4.03</p></td>
<td><p>0.12</p></td>
<td><p>42.43</p></td>
</tr>
</tbody>
</table>

<p>Bar charts, seen often in the popular press, are a common visual tool for displaying a single categorical variable.
Categories are listed on the x-axis, and frequencies or proportions on the y-axis.
<a data-type="xref" href="#Barchart">Figure 1-5</a> shows the airport delays per year by cause for <span class="keep-together">Dallas/Fort Worth</span> (DFW), and it is produced with the <em>R</em> function <code>barplot</code>:</p>

<pre data-type="programlisting" data-code-language="r"><code class="nf">barplot</code><code class="p">(</code><code class="nf">as.matrix</code><code class="p">(</code><code class="n">dfw</code><code class="p">)</code> <code class="o">/</code> <code class="m">6</code><code class="p">,</code> <code class="n">cex.axis</code><code class="o">=</code><code class="m">0.8</code><code class="p">,</code> <code class="n">cex.names</code><code class="o">=</code><code class="m">0.7</code><code class="p">,</code>
        <code class="n">xlab</code><code class="o">=</code><code class="s">'Cause of delay'</code><code class="p">,</code> <code class="n">ylab</code><code class="o">=</code><code class="s">'Count'</code><code class="p">)</code></pre>

<p><code>pandas</code> also supports bar charts for data frames:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">ax</code> <code class="o">=</code> <code class="n">dfw</code><code class="o">.</code><code class="n">transpose</code><code class="p">()</code><code class="o">.</code><code class="n">plot</code><code class="o">.</code><code class="n">bar</code><code class="p">(</code><code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">4</code><code class="p">,</code> <code class="mi">4</code><code class="p">),</code> <code class="n">legend</code><code class="o">=</code><code class="bp">False</code><code class="p">)</code>
<code class="n">ax</code><code class="o">.</code><code class="n">set_xlabel</code><code class="p">(</code><code class="s1">'Cause of delay'</code><code class="p">)</code>
<code class="n">ax</code><code class="o">.</code><code class="n">set_ylabel</code><code class="p">(</code><code class="s1">'Count'</code><code class="p">)</code></pre>

<figure class="width-75"><div id="Barchart" class="figure">
<img src="Images/psd2_0105.png" alt="Bar chart of airline delays at DFW by cause." width="1086" height="1189"/>
<h6><span class="label">Figure 1-5. </span>Bar chart of airline delays at DFW by cause</h6>
</div></figure>

<p>Note that a bar chart resembles a histogram; in a bar chart the x-axis represents different categories of a factor variable, while in a histogram the x-axis represents values of a single variable on a numeric scale.
In a histogram, the bars are typically shown touching each other, with gaps indicating values that did not occur in the data.  In a bar chart, the bars are shown separate from one another.</p>

<p>Pie charts are <a data-type="indexterm" data-primary="pie charts" id="idm46522865147816"/>an alternative to bar charts,
although statisticians and data visualization experts generally eschew pie charts as less visually informative (see <a data-type="link" href="bibliography01.xhtml#Few-2007">[Few-2007]</a>).</p>
<div data-type="note" epub:type="note"><h1>Numerical Data as Categorical Data</h1>
<p>In <a data-type="xref" href="#Histograms">“Frequency Tables and Histograms”</a>, we looked at frequency tables based on binning the data.
This implicitly converts the numeric data to an ordered factor.<a data-type="indexterm" data-primary="histograms" data-secondary="bar charts and" id="idm46522865143560"/><a data-type="indexterm" data-primary="bar charts" data-secondary="histograms and" id="idm46522865142616"/>
In this sense, histograms and bar charts are similar, except that the categories on the x-axis in the bar chart are not ordered.
Converting numeric data to categorical data is an important and widely used step in data analysis since it reduces the complexity (and size) of the data.
This aids in the discovery of relationships between features, particularly at the initial stages of an analysis.</p>
</div>








<section data-type="sect2" data-pdf-bookmark="Mode"><div class="sect2" id="idm46522865140872">
<h2>Mode</h2>

<p>The mode is the value—or values in case of a tie—that appears most often in the data.<a data-type="indexterm" data-primary="categorical data" data-secondary="exploring" data-tertiary="mode" id="idm46522865139464"/><a data-type="indexterm" data-primary="mode" data-secondary="examples in categorical data" id="idm46522865138216"/>
For example, the mode of the cause of delay at Dallas/Fort Worth airport is “Inbound.”
As another example, in most parts of the United States, the mode for religious preference would be Christian.
The mode is a simple summary statistic for <span class="keep-together">categorical</span> data, and it is generally not used for numeric data.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Expected Value"><div class="sect2" id="idm46522865135944">
<h2>Expected Value</h2>

<p>A special type of categorical data is data in which the categories represent or can be mapped to discrete values on the same scale.<a data-type="indexterm" data-primary="expected value" id="idm46522865134232"/><a data-type="indexterm" data-primary="categorical data" data-secondary="exploring" data-tertiary="expected value" id="idm46522865133528"/>
A marketer for a new cloud technology, for example, offers two levels of service, one priced at $300/month and another at $50/month.
The marketer offers free webinars to generate leads, and the firm figures that 5% of the attendees will sign up for the $300 service, 15% will sign up for the $50 service, and 80% will not sign up for anything.
This data can be summed up, for financial purposes, in a single “expected value,” which is a form of weighted mean, in which the weights are probabilities.</p>

<p>The expected value is calculated as follows:</p>
<ol>
<li>
<p>Multiply each outcome by its probability of occurrence.</p>
</li>
<li>
<p>Sum these values.</p>
</li>

</ol>

<p>In the cloud service example, the expected value of a webinar attendee is thus $22.50 per month, calculated as follows:</p>
<div data-type="equation">
<math display="block">
  <mtable displaystyle="true">
    <mtr>
      <mtd columnalign="right">
        <mrow>
          <mi>E</mi>
          <mi>V</mi>
        </mrow>
      </mtd>
      <mtd>
        <mo>=</mo>
      </mtd>
      <mtd columnalign="left">
        <mrow>
          <mo>(</mo>
          <mn>0</mn>
          <mo>.</mo>
          <mn>05</mn>
          <mo>)</mo>
          <mo>(</mo>
          <mn>300</mn>
          <mo>)</mo>
          <mo>+</mo>
          <mo>(</mo>
          <mn>0</mn>
          <mo>.</mo>
          <mn>15</mn>
          <mo>)</mo>
          <mo>(</mo>
          <mn>50</mn>
          <mo>)</mo>
          <mo>+</mo>
          <mo>(</mo>
          <mn>0</mn>
          <mo>.</mo>
          <mn>80</mn>
          <mo>)</mo>
          <mo>(</mo>
          <mn>0</mn>
          <mo>)</mo>
        </mrow>
      </mtd>
      <mtd>
        <mo>=</mo>
      </mtd>
      <mtd>
        <mrow>
          <mn>22</mn>
          <mo>.</mo>
          <mn>5</mn>
        </mrow>
      </mtd>
    </mtr>
  </mtable>
</math>
</div>

<p>The expected value is really a form of weighted mean: it adds the ideas of future expectations and probability weights, often based on subjective judgment.
Expected value is a fundamental concept in business valuation and capital budgeting—for example, the expected value of five years of profits from a new acquisition, or the expected cost savings from new patient management software at a clinic.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Probability"><div class="sect2" id="idm46522865084632">
<h2>Probability</h2>

<p>We referred above to the <em>probability</em> of a value occurring.<a data-type="indexterm" data-primary="categorical data" data-secondary="exploring" data-tertiary="probability" id="idm46522865082888"/><a data-type="indexterm" data-primary="probability" id="idm46522865081608"/>  Most people have an intuitive understanding of probability, encountering the concept frequently in weather forecasts (the chance of rain) or sports analysis (the probability of winning).
Sports and games are more often expressed as odds, which are readily convertible to probabilities (if the odds that a team will win are 2 to 1, its probability of winning is 2/(2+1) = 2/3).  Surprisingly, though, the concept of probability can be the source of deep philosophical discussion when it comes to defining it.  Fortunately, we do not need a formal mathematical or philosophical definition here.  For our purposes, the probability that an event will happen is the proportion of times it will occur if the situation could be repeated over and over, countless times. Most often this is an imaginary construction, but it is an adequate operational understanding of probability.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46522865079784">
<h5>Key Ideas</h5>
<ul>
<li>
<p>Categorical data is typically summed up in proportions and can be visualized in a bar chart.</p>
</li>
<li>
<p>Categories might represent distinct things (apples and oranges, male and female), levels of a factor variable (low, medium, and high), or numeric data that has been binned.</p>
</li>
<li>
<p>Expected value is the sum of values times their probability of occurrence, often used to sum up factor variable levels.</p>
</li>
</ul>
</div></aside>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Further Reading"><div class="sect2" id="idm46522865075048">
<h2>Further Reading</h2>

<p>No statistics course is complete without a <a href="https://oreil.ly/rDMuT">lesson on misleading graphs</a>, which often involves bar charts and pie charts.<a data-type="indexterm" data-primary="categorical data" data-secondary="exploring" data-startref="ix_catexp" id="idm46522865072904"/><a data-type="indexterm" data-primary="binary data" data-secondary="exploring" data-startref="ix_binda" id="idm46522865071688"/><a data-type="indexterm" data-primary="exploratory data analysis" data-secondary="categorical and binary data" data-startref="ix_expcat" id="idm46522865070472"/></p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Correlation"><div class="sect1" id="Correlations">
<h1>Correlation</h1>

<p>Exploratory data analysis in many modeling projects (whether in data science or in research) involves examining correlation among predictors, and between predictors and a target variable.<a data-type="indexterm" data-primary="exploratory data analysis" data-secondary="correlation" id="ix_expcorr"/><a data-type="indexterm" data-primary="correlation" id="ix_corr"/>
Variables X and Y (each with measured data) are said to be positively correlated if high values of X go with high values of Y, and low values of X go with low values of Y.
If high values of X go with low values of Y, and vice versa, the variables are negatively correlated.<a data-type="indexterm" data-primary="correlation" data-secondary="key terms for" id="idm46522865064088"/></p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46522865063016">
<h5>Key Terms for Correlation</h5><dl>
<dt class="horizontal"><strong><em>Correlation coefficient</em></strong></dt>
<dd>
<p>A metric that measures the extent to which numeric variables are associated with one another (ranges from –1 to +1).</p>
</dd>
<dt class="horizontal"><strong><em>Correlation matrix</em></strong></dt>
<dd>
<p>A table where the variables are shown on both rows and columns, and the cell values are the correlations between the variables.</p>
</dd>
<dt class="horizontal"><strong><em>Scatterplot</em></strong></dt>
<dd>
<p>A plot in which the x-axis is the value of one variable, and the y-axis the value of another.<a data-type="indexterm" data-primary="scatterplots" id="idm46522865056008"/></p>
</dd>
</dl>
</div></aside>

<p>Consider these two variables, perfectly correlated in the sense that each goes from low to high:</p>
<ul class="simplelist">
  <li>v1: {1, 2, 3}</li>
  <li>v2: {4, 5, 6}</li>
</ul>

<p>The vector sum of products is <math alttext="1 dot 4 plus 2 dot 5 plus 3 dot 6 equals 32">
  <mrow>
    <mn>1</mn>
    <mo>·</mo>
    <mn>4</mn>
    <mo>+</mo>
    <mn>2</mn>
    <mo>·</mo>
    <mn>5</mn>
    <mo>+</mo>
    <mn>3</mn>
    <mo>·</mo>
    <mn>6</mn>
    <mo>=</mo>
    <mn>32</mn>
  </mrow>
</math>.
Now try shuffling one of them and recalculating—the vector sum of products will never be higher than 32.
So this sum of products could be used as a metric; that is, the observed sum of 32 could be compared to lots of random shufflings (in fact, this idea relates to a resampling-based estimate; see <a data-type="xref" href="ch03.xhtml#Permutation">“Permutation Test”</a>).
Values produced by this metric, though, are not that meaningful, except by reference to the resampling distribution.</p>

<p>More useful is a standardized variant: the <em>correlation coefficient</em>, which gives an estimate of the correlation between two variables that always lies on the same scale.<a data-type="indexterm" data-primary="correlation coefficient" id="idm46522865042760"/><a data-type="indexterm" data-primary="correlation coefficient" data-secondary="calculating Pearson's correlation coefficient" id="idm46522865042056"/>
To compute <em>Pearson’s correlation coefficient</em>, we multiply deviations from the mean<a data-type="indexterm" data-primary="Pearson's correlation coefficient" id="idm46522865040472"/> for variable 1 times those for variable 2, and divide by the product of the standard <span class="keep-together">deviations</span>:</p>
<div data-type="equation">
<math alttext="r equals StartFraction sigma-summation Underscript i equals 1 Overscript n Endscripts left-parenthesis x Subscript i Baseline minus x overbar right-parenthesis left-parenthesis y Subscript i Baseline minus y overbar right-parenthesis Over left-parenthesis n minus 1 right-parenthesis s Subscript x Baseline s Subscript y Baseline EndFraction" display="block">
  <mrow>
    <mi>r</mi>
    <mo>=</mo>
    <mfrac><mrow><msubsup><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>n</mi> </msubsup><mrow><mo>(</mo><msub><mi>x</mi> <mi>i</mi> </msub><mo>-</mo><mover accent="true"><mi>x</mi> <mo>¯</mo></mover><mo>)</mo></mrow><mrow><mo>(</mo><msub><mi>y</mi> <mi>i</mi> </msub><mo>-</mo><mover accent="true"><mi>y</mi> <mo>¯</mo></mover><mo>)</mo></mrow></mrow> <mrow><mrow><mo>(</mo><mi>n</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow><msub><mi>s</mi> <mi>x</mi> </msub><msub><mi>s</mi> <mi>y</mi> </msub></mrow></mfrac>
  </mrow>
</math>
</div>

<p>Note that we divide by <em>n</em> – 1 instead of <em>n</em>; see <a data-type="xref" href="#Nminus1">“Degrees of Freedom, and <em>n</em> or <em>n</em> – 1?”</a> for more details. The correlation coefficient always lies between +1 (<span class="keep-together">perfect</span> positive correlation) and –1 (perfect negative correlation); 0 indicates no <span class="keep-together">correlation</span>.</p>

<p>Variables can have an association that is not linear, in which case the correlation coefficient may not be a useful metric.
The relationship between tax rates and revenue raised is an example: as tax rates increase from zero, the revenue raised also increases.
However, once tax rates reach a high level and approach 100%, tax avoidance increases and tax revenue actually declines.</p>

<p><a data-type="xref" href="#CorrTable">Table 1-7</a>, called a <em>correlation matrix</em>, shows the correlation between the daily returns for telecommunication stocks from July 2012 through June 2015.<a data-type="indexterm" data-primary="correlation matrix" id="idm46522865016664"/>
From the table, you can see that Verizon (VZ) and ATT (T) have the highest correlation.
Level 3 (LVLT), which is an infrastructure company,
has the lowest correlation with the others.
Note the diagonal of 1s (the correlation of a stock with itself is 1) and the redundancy of the information above and below the diagonal.</p>
<table id="CorrTable">
<caption><span class="label">Table 1-7. </span>Correlation between telecommunication stock returns</caption>
<thead>
<tr>
<th/>
<th>T</th>
<th>CTL</th>
<th>FTR</th>
<th>VZ</th>
<th>LVLT</th>
</tr>
</thead>
<tbody>
<tr>
<td><p>T</p></td>
<td><p>1.000</p></td>
<td><p>0.475</p></td>
<td><p>0.328</p></td>
<td><p>0.678</p></td>
<td><p>0.279</p></td>
</tr>
<tr>
<td><p>CTL</p></td>
<td><p>0.475</p></td>
<td><p>1.000</p></td>
<td><p>0.420</p></td>
<td><p>0.417</p></td>
<td><p>0.287</p></td>
</tr>
<tr>
<td><p>FTR</p></td>
<td><p>0.328</p></td>
<td><p>0.420</p></td>
<td><p>1.000</p></td>
<td><p>0.287</p></td>
<td><p>0.260</p></td>
</tr>
<tr>
<td><p>VZ</p></td>
<td><p>0.678</p></td>
<td><p>0.417</p></td>
<td><p>0.287</p></td>
<td><p>1.000</p></td>
<td><p>0.242</p></td>
</tr>
<tr>
<td><p>LVLT</p></td>
<td><p>0.279</p></td>
<td><p>0.287</p></td>
<td><p>0.260</p></td>
<td><p>0.242</p></td>
<td><p>1.000</p></td>
</tr>
</tbody>
</table>

<p>A table of correlations like <a data-type="xref" href="#CorrTable">Table 1-7</a>
is commonly plotted to visually display the relationship between multiple variables.
<a data-type="xref" href="#CorrPlot">Figure 1-6</a> shows the correlation between the daily returns for major exchange-traded funds (ETFs).
In <em>R</em>, we can easily create this using the package <code>corrplot</code>:</p>

<pre data-type="programlisting" data-code-language="r"><code class="n">etfs</code> <code class="o">&lt;-</code> <code class="n">sp500_px</code><code class="nf">[row.names</code><code class="p">(</code><code class="n">sp500_px</code><code class="p">)</code> <code class="o">&gt;</code> <code class="s">'2012-07-01'</code><code class="p">,</code>
                 <code class="n">sp500_sym</code><code class="p">[</code><code class="n">sp500_sym</code><code class="o">$</code><code class="n">sector</code> <code class="o">==</code> <code class="s">'etf'</code><code class="p">,</code> <code class="s">'symbol'</code><code class="p">]]</code>
<code class="nf">library</code><code class="p">(</code><code class="n">corrplot</code><code class="p">)</code>
<code class="nf">corrplot</code><code class="p">(</code><code class="nf">cor</code><code class="p">(</code><code class="n">etfs</code><code class="p">),</code> <code class="n">method</code><code class="o">=</code><code class="s">'ellipse'</code><code class="p">)</code></pre>

<p>It is possible to create the same graph in <em>Python</em>, but there is no implementation in the common packages. However, most support the visualization of correlation matrices using heatmaps. The following code demonstrates this using the <code>seaborn.heatmap</code> package. In the accompanying source code repository, we include <em>Python</em> code to generate the more comprehensive visualization:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">etfs</code> <code class="o">=</code> <code class="n">sp500_px</code><code class="o">.</code><code class="n">loc</code><code class="p">[</code><code class="n">sp500_px</code><code class="o">.</code><code class="n">index</code> <code class="o">&gt;</code> <code class="s1">'2012-07-01'</code><code class="p">,</code>
                    <code class="n">sp500_sym</code><code class="p">[</code><code class="n">sp500_sym</code><code class="p">[</code><code class="s1">'sector'</code><code class="p">]</code> <code class="o">==</code> <code class="s1">'etf'</code><code class="p">][</code><code class="s1">'symbol'</code><code class="p">]]</code>
<code class="n">sns</code><code class="o">.</code><code class="n">heatmap</code><code class="p">(</code><code class="n">etfs</code><code class="o">.</code><code class="n">corr</code><code class="p">(),</code> <code class="n">vmin</code><code class="o">=-</code><code class="mi">1</code><code class="p">,</code> <code class="n">vmax</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code>
            <code class="n">cmap</code><code class="o">=</code><code class="n">sns</code><code class="o">.</code><code class="n">diverging_palette</code><code class="p">(</code><code class="mi">20</code><code class="p">,</code> <code class="mi">220</code><code class="p">,</code> <code class="n">as_cmap</code><code class="o">=</code><code class="bp">True</code><code class="p">))</code></pre>

<p>The ETFs for the S&amp;P 500 (SPY) and the Dow Jones Index (DIA) have a  high correlation.
Similarly, the QQQ and the XLK, composed mostly of technology companies, are positively correlated.
Defensive ETFs,
such as those tracking gold prices (GLD),  oil prices (USO), or market volatility (VXX), tend to be weakly or negatively correlated with the other ETFs.<a data-type="indexterm" data-primary="correlation" data-secondary="example, correlation between ETF returns" id="idm46522864877464"/>
The orientation of the ellipse indicates whether two variables are positively correlated (ellipse is pointed to the top right) or negatively correlated (ellipse is pointed to the top left).
The shading and width of the ellipse indicate the strength of the association: thinner and darker ellipses correspond to stronger <span class="keep-together">relationships</span>.</p>

<figure class="width-75"><div id="CorrPlot" class="figure">
<img src="Images/psd2_0106.png" alt="Correlation between ETF returns." width="1126" height="1015"/>
<h6><span class="label">Figure 1-6. </span>Correlation between ETF returns</h6>
</div></figure>

<p>Like the mean and standard deviation, the correlation coefficient is sensitive to outliers in the data.<a data-type="indexterm" data-primary="outliers" data-secondary="correlation coefficient and" id="idm46522864872536"/>
Software packages offer robust alternatives to the classical correlation coefficient. For example, the <em>R</em> package <a href="https://oreil.ly/isORz"><code>robust</code></a> uses the function <code>covRob</code> to compute a robust estimate of correlation.<a data-type="indexterm" data-primary="robust estimates of correlation" id="idm46522864869656"/> The methods in the <code>scikit-learn</code> module <a href="https://oreil.ly/su7wi"><em>sklearn.covariance</em></a> implement a variety of approaches.</p>
<div data-type="tip"><h1>Other Correlation Estimates</h1>
<p>Statisticians long ago proposed other types of correlation coefficients, such as <em>Spearman’s rho</em> or <em>Kendall’s tau</em>.<a data-type="indexterm" data-primary="correlation coefficient" data-secondary="other types of" id="idm46522864865592"/><a data-type="indexterm" data-primary="Kendall's tau" id="idm46522864864584"/><a data-type="indexterm" data-primary="Spearman's rho" id="idm46522864863912"/>
These are correlation coefficients based on the rank of the data.
Since they work with ranks rather than values,
these estimates are robust to outliers and can handle certain types of nonlinearities.
However, data scientists can generally stick to Pearson’s correlation coefficient, and its robust alternatives, for exploratory analysis.
The appeal of rank-based estimates is mostly for smaller data sets and specific hypothesis tests.</p>
</div>








<section data-type="sect2" data-pdf-bookmark="Scatterplots"><div class="sect2" id="idm46522864862408">
<h2>Scatterplots</h2>

<p>The standard way to visualize the relationship between two measured data variables is with a scatterplot.<a data-type="indexterm" data-primary="scatterplots" id="idm46522864860744"/><a data-type="indexterm" data-primary="correlation" data-secondary="scatterplots" id="idm46522864860040"/>
The x-axis represents one variable and the y-axis another,
and each point on the graph is a record.
See <a data-type="xref" href="#ScatterplotImage">Figure 1-7</a> for a plot of the correlation between the daily returns for ATT and Verizon.
This is produced in <em>R</em> with the command:</p>

<pre data-type="programlisting" data-code-language="r"><code class="nf">plot</code><code class="p">(</code><code class="n">telecom</code><code class="o">$</code><code class="bp">T</code><code class="p">,</code> <code class="n">telecom</code><code class="o">$</code><code class="n">VZ</code><code class="p">,</code> <code class="n">xlab</code><code class="o">=</code><code class="s">'ATT (T)'</code><code class="p">,</code> <code class="n">ylab</code><code class="o">=</code><code class="s">'Verizon (VZ)'</code><code class="p">)</code></pre>

<p>The same graph can be generated in <em>Python</em> using the <code>pandas</code> scatter method:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">ax</code> <code class="o">=</code> <code class="n">telecom</code><code class="o">.</code><code class="n">plot</code><code class="o">.</code><code class="n">scatter</code><code class="p">(</code><code class="n">x</code><code class="o">=</code><code class="s1">'T'</code><code class="p">,</code> <code class="n">y</code><code class="o">=</code><code class="s1">'VZ'</code><code class="p">,</code> <code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">4</code><code class="p">,</code> <code class="mi">4</code><code class="p">),</code> <code class="n">marker</code><code class="o">=</code><code class="s1">'$</code><code class="se">\u25EF</code><code class="s1">$'</code><code class="p">)</code>
<code class="n">ax</code><code class="o">.</code><code class="n">set_xlabel</code><code class="p">(</code><code class="s1">'ATT (T)'</code><code class="p">)</code>
<code class="n">ax</code><code class="o">.</code><code class="n">set_ylabel</code><code class="p">(</code><code class="s1">'Verizon (VZ)'</code><code class="p">)</code>
<code class="n">ax</code><code class="o">.</code><code class="n">axhline</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="n">color</code><code class="o">=</code><code class="s1">'grey'</code><code class="p">,</code> <code class="n">lw</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>
<code class="n">ax</code><code class="o">.</code><code class="n">axvline</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="n">color</code><code class="o">=</code><code class="s1">'grey'</code><code class="p">,</code> <code class="n">lw</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code></pre>

<p>The returns have a positive relationship:  while they cluster around zero,
on most days, the stocks go up or go down in tandem (upper-right and lower-left quadrants).
There are fewer days where one stock goes down significantly while the other stock goes up, or vice versa (lower-right and upper-left quadrants).</p>

<p>While the plot <a data-type="xref" href="#ScatterplotImage">Figure 1-7</a> displays only 754 data points, it’s already obvious how
difficult it is to identify details in the middle of the plot. We will see later how adding transparency
to the points, or using hexagonal binning and density plots, can help to find additional structure in the data.</p>

<figure class="width-75"><div id="ScatterplotImage" class="figure">
<img src="Images/psd2_0107.png" alt="Scatterplot between returns for ATT and Verizon." width="1139" height="1189"/>
<h6><span class="label">Figure 1-7. </span>Scatterplot of correlation between returns for ATT and Verizon</h6>
</div></figure>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46522864683656">
<h5>Key Ideas</h5>
<ul>
<li>
<p>The correlation coefficient measures the extent to which two paired variables (e.g., height and weight for individuals) are associated with one another.<a data-type="indexterm" data-primary="correlation" data-secondary="key concepts" id="idm46522864681288"/></p>
</li>
<li>
<p>When high values of v1 go with high values of v2, v1 and v2 are positively <span class="keep-together">associated</span>.</p>
</li>
<li>
<p>When high values of v1 go with low values of v2, v1 and v2 are negatively <span class="keep-together">associated</span>.</p>
</li>
<li>
<p>The correlation coefficient is a standardized metric, so that it always ranges from –1 (perfect negative correlation) to +1 (perfect positive correlation).</p>
</li>
<li>
<p>A correlation coefficient of zero indicates no correlation, but be aware that random arrangements of data will produce both positive and negative values for the correlation coefficient just by chance.</p>
</li>
</ul>
</div></aside>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Further Reading"><div class="sect2" id="idm46522864674664">
<h2>Further Reading</h2>

<p><em>Statistics</em>, 4th ed., by David Freedman, Robert Pisani, and Roger Purves (W. W. Norton, 2007) has an excellent discussion of correlation.<a data-type="indexterm" data-primary="exploratory data analysis" data-secondary="correlation" data-startref="ix_expcorr" id="idm46522864672904"/><a data-type="indexterm" data-primary="correlation" data-startref="ix_corr" id="idm46522864671688"/></p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Exploring Two or More Variables"><div class="sect1" id="idm46522865068232">
<h1>Exploring Two or More Variables</h1>

<p>Familiar estimators like mean and variance look at variables one at a time (<em>univariate analysis</em>).<a data-type="indexterm" data-primary="exploratory data analysis" data-secondary="exploring two or more variables" id="ix_expvars"/><a data-type="indexterm" data-primary="variables" data-secondary="exploring two or more" id="ix_varexp"/><a data-type="indexterm" data-primary="univariate analysis" id="idm46522864666344"/><a data-type="indexterm" data-primary="bivariate analysis" id="idm46522864665672"/>
Correlation analysis (see <a data-type="xref" href="#Correlations">“Correlation”</a>) is an important method that compares two variables (<em>bivariate analysis</em>).  In this section we look at additional estimates and plots, and at more than two variables (<em>multivariate analysis</em>).<a data-type="indexterm" data-primary="multivariate analysis" id="ix_mltivar"/></p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46522864662152">
<h5>Key Terms for Exploring Two or More Variables</h5><dl>
<dt class="horizontal"><strong><em>Contingency table</em></strong></dt>
<dd>
<p>A tally of counts between two or more categorical variables.<a data-type="indexterm" data-primary="variables" data-secondary="exploring two or more" data-tertiary="key terms" id="idm46522864659208"/><a data-type="indexterm" data-primary="contingency tables" id="idm46522864657960"/></p>
</dd>
<dt class="horizontal"><strong><em>Hexagonal binning</em></strong></dt>
<dd>
<p>A plot of two numeric variables with the records binned into hexagons.<a data-type="indexterm" data-primary="hexagonal binning" id="idm46522864655480"/></p>
</dd>
<dt class="horizontal"><strong><em>Contour plot</em></strong></dt>
<dd>
<p>A plot showing the density of two numeric variables like a topographical map.<a data-type="indexterm" data-primary="contour plots" id="idm46522864652968"/><a data-type="indexterm" data-primary="violin plots" id="idm46522864652264"/></p>
</dd>
<dt class="horizontal"><strong><em>Violin plot</em></strong></dt>
<dd>
<p>Similar to a boxplot but showing the density estimate.</p>
</dd>
</dl>
</div></aside>

<p>Like univariate analysis,
bivariate analysis involves both computing summary statistics and producing visual displays.
The appropriate type of bivariate or multivariate analysis depends on the nature of the data: numeric versus categorical.</p>








<section data-type="sect2" data-pdf-bookmark="Hexagonal Binning and Contours (Plotting Numeric Versus Numeric Data)"><div class="sect2" id="idm46522864648664">
<h2>Hexagonal Binning and Contours <span class="keep-together">(Plotting Numeric Versus Numeric Data)</span></h2>

<p>Scatterplots are fine when there is a relatively small number of data values.
The plot of stock returns in <a data-type="xref" href="#ScatterplotImage">Figure 1-7</a> involves only about 750 points.<a data-type="indexterm" data-primary="variables" data-secondary="exploring two or more" data-tertiary="using hexagonal binning and contour plot" id="ix_varexphex"/><a data-type="indexterm" data-primary="hexagonal binning" data-secondary="and contours, plotting relationship between two numeric values" id="ix_hexcntr"/><a data-type="indexterm" data-primary="contour plots" data-secondary="using with hexagonal binning" id="ix_cntrplt"/><a data-type="indexterm" data-primary="numeric data" data-secondary="exploring relationship between two numeric variables" id="ix_numda"/>
For data sets with hundreds of thousands or millions of records, a scatterplot will be too dense, so we need a different way to visualize the relationship.
To illustrate, consider the data set <code>kc_tax</code>, which contains the tax-assessed values for residential properties in King County, Washington.
In order to focus on the main part of the data,
we strip out very expensive and very small or large residences using the <code>subset</code> function:</p>

<pre data-type="programlisting" data-code-language="r" class="pagebreak-before"><code class="n">kc_tax0</code> <code class="o">&lt;-</code> <code class="nf">subset</code><code class="p">(</code><code class="n">kc_tax</code><code class="p">,</code> <code class="n">TaxAssessedValue</code> <code class="o">&lt;</code> <code class="m">750000</code> <code class="o">&amp;</code>
                  <code class="n">SqFtTotLiving</code> <code class="o">&gt;</code> <code class="m">100</code> <code class="o">&amp;</code>
                  <code class="n">SqFtTotLiving</code> <code class="o">&lt;</code> <code class="m">3500</code><code class="p">)</code>
<code class="nf">nrow</code><code class="p">(</code><code class="n">kc_tax0</code><code class="p">)</code>
<code class="m">432693</code></pre>

<p>In <code>pandas</code>, we filter the data set as follows:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">kc_tax0</code> <code class="o">=</code> <code class="n">kc_tax</code><code class="o">.</code><code class="n">loc</code><code class="p">[(</code><code class="n">kc_tax</code><code class="o">.</code><code class="n">TaxAssessedValue</code> <code class="o">&lt;</code> <code class="mi">750000</code><code class="p">)</code> <code class="o">&amp;</code>
                     <code class="p">(</code><code class="n">kc_tax</code><code class="o">.</code><code class="n">SqFtTotLiving</code> <code class="o">&gt;</code> <code class="mi">100</code><code class="p">)</code> <code class="o">&amp;</code>
                     <code class="p">(</code><code class="n">kc_tax</code><code class="o">.</code><code class="n">SqFtTotLiving</code> <code class="o">&lt;</code> <code class="mi">3500</code><code class="p">),</code> <code class="p">:]</code>
<code class="n">kc_tax0</code><code class="o">.</code><code class="n">shape</code>
<code class="p">(</code><code class="mi">432693</code><code class="p">,</code> <code class="mi">3</code><code class="p">)</code></pre>

<p><a data-type="xref" href="#HexagonalBinning">Figure 1-8</a> is a <em>hexagonal binning</em> plot of the relationship between the finished square feet and the tax-assessed value for homes in King County.
Rather than plotting points,
which would appear as a monolithic dark cloud, we grouped the records into hexagonal bins and plotted the hexagons with a color indicating the number of records in that bin.
In this chart, the positive relationship between square feet and tax-assessed value is clear.
An interesting feature is the hint of additional bands above the main (darkest) band at the bottom, indicating homes that have the same square footage as those in the main band but a higher tax-assessed value.</p>

<p><a data-type="xref" href="#HexagonalBinning">Figure 1-8</a> was generated by the powerful <em>R</em> package <code>ggplot2</code>, developed by Hadley Wickham <a data-type="link" href="bibliography01.xhtml#ggplot2">[ggplot2]</a>.
<code>ggplot2</code> is one of several new software libraries for advanced exploratory visual analysis of data; see <a data-type="xref" href="#StatisticalGraphics">“Visualizing Multiple Variables”</a>:</p>

<pre data-type="programlisting" data-code-language="r"><code class="nf">ggplot</code><code class="p">(</code><code class="n">kc_tax0</code><code class="p">,</code> <code class="p">(</code><code class="nf">aes</code><code class="p">(</code><code class="n">x</code><code class="o">=</code><code class="n">SqFtTotLiving</code><code class="p">,</code> <code class="n">y</code><code class="o">=</code><code class="n">TaxAssessedValue</code><code class="p">)))</code> <code class="o">+</code>
  <code class="nf">stat_binhex</code><code class="p">(</code><code class="n">color</code><code class="o">=</code><code class="s">'white'</code><code class="p">)</code> <code class="o">+</code>
  <code class="nf">theme_bw</code><code class="p">()</code> <code class="o">+</code>
  <code class="nf">scale_fill_gradient</code><code class="p">(</code><code class="n">low</code><code class="o">=</code><code class="s">'white'</code><code class="p">,</code> <code class="n">high</code><code class="o">=</code><code class="s">'black'</code><code class="p">)</code> <code class="o">+</code>
  <code class="nf">labs</code><code class="p">(</code><code class="n">x</code><code class="o">=</code><code class="s">'Finished Square Feet'</code><code class="p">,</code> <code class="n">y</code><code class="o">=</code><code class="s">'Tax-Assessed Value'</code><code class="p">)</code></pre>

<p>In <em>Python</em>, hexagonal binning plots are readily available using the <code>pandas</code> data frame method <code>hexbin</code>:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">ax</code> <code class="o">=</code> <code class="n">kc_tax0</code><code class="o">.</code><code class="n">plot</code><code class="o">.</code><code class="n">hexbin</code><code class="p">(</code><code class="n">x</code><code class="o">=</code><code class="s1">'SqFtTotLiving'</code><code class="p">,</code> <code class="n">y</code><code class="o">=</code><code class="s1">'TaxAssessedValue'</code><code class="p">,</code>
                         <code class="n">gridsize</code><code class="o">=</code><code class="mi">30</code><code class="p">,</code> <code class="n">sharex</code><code class="o">=</code><code class="bp">False</code><code class="p">,</code> <code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">5</code><code class="p">,</code> <code class="mi">4</code><code class="p">))</code>
<code class="n">ax</code><code class="o">.</code><code class="n">set_xlabel</code><code class="p">(</code><code class="s1">'Finished Square Feet'</code><code class="p">)</code>
<code class="n">ax</code><code class="o">.</code><code class="n">set_ylabel</code><code class="p">(</code><code class="s1">'Tax-Assessed Value'</code><code class="p">)</code></pre>

<figure class="width-75"><div id="HexagonalBinning" class="figure">
<img src="Images/psd2_0108.png" alt="Hexagonal binning for tax-assessed value versus finished square feet." width="1130" height="1006"/>
<h6><span class="label">Figure 1-8. </span>Hexagonal binning for tax-assessed value versus finished square feet</h6>
</div></figure>

<p><a data-type="xref" href="#Contours">Figure 1-9</a> uses contours overlaid onto a scatterplot to visualize the relationship between two numeric variables.
The contours are essentially a topographical map to two variables; each contour band represents a specific density of points, increasing as one nears a “peak.”
This plot shows a similar story as <a data-type="xref" href="#HexagonalBinning">Figure 1-8</a>: there is a secondary peak “north” of the main peak.
This chart was also created using <code>ggplot2</code> with the built-in <code>geom_density2d</code> function:</p>

<pre data-type="programlisting" data-code-language="r"><code class="nf">ggplot</code><code class="p">(</code><code class="n">kc_tax0</code><code class="p">,</code> <code class="nf">aes</code><code class="p">(</code><code class="n">SqFtTotLiving</code><code class="p">,</code> <code class="n">TaxAssessedValue</code><code class="p">))</code> <code class="o">+</code>
  <code class="nf">theme_bw</code><code class="p">()</code> <code class="o">+</code>
  <code class="nf">geom_point</code><code class="p">(</code><code class="n">alpha</code><code class="o">=</code><code class="m">0.1</code><code class="p">)</code> <code class="o">+</code>
  <code class="nf">geom_density2d</code><code class="p">(</code><code class="n">color</code><code class="o">=</code><code class="s">'white'</code><code class="p">)</code> <code class="o">+</code>
  <code class="nf">labs</code><code class="p">(</code><code class="n">x</code><code class="o">=</code><code class="s">'Finished Square Feet'</code><code class="p">,</code> <code class="n">y</code><code class="o">=</code><code class="s">'Tax-Assessed Value'</code><code class="p">)</code></pre>

<p>The <code>seaborn</code> <code>kdeplot</code> function in <em>Python</em> creates a contour plot:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">ax</code> <code class="o">=</code> <code class="n">sns</code><code class="o">.</code><code class="n">kdeplot</code><code class="p">(</code><code class="n">kc_tax0</code><code class="o">.</code><code class="n">SqFtTotLiving</code><code class="p">,</code> <code class="n">kc_tax0</code><code class="o">.</code><code class="n">TaxAssessedValue</code><code class="p">,</code> <code class="n">ax</code><code class="o">=</code><code class="n">ax</code><code class="p">)</code>
<code class="n">ax</code><code class="o">.</code><code class="n">set_xlabel</code><code class="p">(</code><code class="s1">'Finished Square Feet'</code><code class="p">)</code>
<code class="n">ax</code><code class="o">.</code><code class="n">set_ylabel</code><code class="p">(</code><code class="s1">'Tax-Assessed Value'</code><code class="p">)</code></pre>

<figure class="width-75"><div id="Contours" class="figure">
<img src="Images/psd2_0109.png" alt="Contour plot of tax-assessed value versus finished square feet." width="1156" height="1156"/>
<h6><span class="label">Figure 1-9. </span>Contour plot for tax-assessed value versus finished square feet</h6>
</div></figure>

<p>Other types of charts are used to show the relationship between two numeric variables, including <em>heat maps</em>.<a data-type="indexterm" data-primary="heat maps" id="idm46522864291704"/>
Heat maps, hexagonal binning, and contour plots all give a visual representation of a two-dimensional density.
In this way, they are natural analogs to histograms and density plots.<a data-type="indexterm" data-primary="numeric data" data-secondary="exploring relationship between two numeric variables" data-startref="ix_numda" id="idm46522864290648"/><a data-type="indexterm" data-primary="variables" data-secondary="exploring two or more variables" data-tertiary="using hexagonal binning and contour plot" data-startref="ix_varexphex" id="idm46522864289400"/><a data-type="indexterm" data-primary="contour plots" data-secondary="using with hexagonal binning" data-startref="ix_cntrplt" id="idm46522864287912"/><a data-type="indexterm" data-primary="hexagonal binning" data-secondary="and contours, plotting relationship between two numeric values" data-startref="ix_hexcntr" id="idm46522864286728"/></p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Two Categorical Variables"><div class="sect2" id="idm46522864648040">
<h2>Two Categorical Variables</h2>

<p>A useful way to summarize two categorical variables is a contingency table—a table of counts by category.<a data-type="indexterm" data-primary="categorical data" data-secondary="exploring two categorical variables" id="idm46522864283912"/><a data-type="indexterm" data-primary="variables" data-secondary="exploring two or more" data-tertiary="categorical variables" id="idm46522864282968"/><a data-type="indexterm" data-primary="contingency tables" data-secondary="summarizing two categorical variables" id="idm46522864281752"/>
<a data-type="xref" href="#CrossTabs">Table 1-8</a> shows the contingency table between the grade of a personal loan and the outcome of that loan.
This is taken from data provided by Lending Club, a leader in the peer-to-peer lending business.
The grade goes from A (high) to G (low).
The outcome is either fully paid, current, late, or charged off (the balance of the loan is not expected to be collected).
This table shows the count and row percentages.
High-grade loans have a very low late/charge-off percentage as compared with lower-grade loans.</p>
<table id="CrossTabs">
<caption><span class="label">Table 1-8. </span>Contingency table of loan grade and status</caption>
<thead>
<tr>
<th>Grade</th>
<th>Charged off</th>
<th>Current</th>
<th>Fully paid</th>
<th>Late</th>
<th>Total</th>
</tr>
</thead>
<tbody>
<tr>
<td><p>A</p></td>
<td><p>1562</p></td>
<td><p>50051</p></td>
<td><p>20408</p></td>
<td><p>469</p></td>
<td><p>72490</p></td>
</tr>
<tr>
<td/>
<td><p>0.022</p></td>
<td><p>0.690</p></td>
<td><p>0.282</p></td>
<td><p>0.006</p></td>
<td><p>0.161</p></td>
</tr>
<tr>
<td><p>B</p></td>
<td><p>5302</p></td>
<td><p>93852</p></td>
<td><p>31160</p></td>
<td><p>2056</p></td>
<td><p>132370</p></td>
</tr>
<tr>
<td/>
<td><p>0.040</p></td>
<td><p>0.709</p></td>
<td><p>0.235</p></td>
<td><p>0.016</p></td>
<td><p>0.294</p></td>
</tr>
<tr>
<td><p>C</p></td>
<td><p>6023</p></td>
<td><p>88928</p></td>
<td><p>23147</p></td>
<td><p>2777</p></td>
<td><p>120875</p></td>
</tr>
<tr>
<td/>
<td><p>0.050</p></td>
<td><p>0.736</p></td>
<td><p>0.191</p></td>
<td><p>0.023</p></td>
<td><p>0.268</p></td>
</tr>
<tr>
<td><p>D</p></td>
<td><p>5007</p></td>
<td><p>53281</p></td>
<td><p>13681</p></td>
<td><p>2308</p></td>
<td><p>74277</p></td>
</tr>
<tr>
<td/>
<td><p>0.067</p></td>
<td><p>0.717</p></td>
<td><p>0.184</p></td>
<td><p>0.031</p></td>
<td><p>0.165</p></td>
</tr>
<tr>
<td><p>E</p></td>
<td><p>2842</p></td>
<td><p>24639</p></td>
<td><p>5949</p></td>
<td><p>1374</p></td>
<td><p>34804</p></td>
</tr>
<tr>
<td/>
<td><p>0.082</p></td>
<td><p>0.708</p></td>
<td><p>0.171</p></td>
<td><p>0.039</p></td>
<td><p>0.077</p></td>
</tr>
<tr>
<td><p>F</p></td>
<td><p>1526</p></td>
<td><p>8444</p></td>
<td><p>2328</p></td>
<td><p>606</p></td>
<td><p>12904</p></td>
</tr>
<tr>
<td/>
<td><p>0.118</p></td>
<td><p>0.654</p></td>
<td><p>0.180</p></td>
<td><p>0.047</p></td>
<td><p>0.029</p></td>
</tr>
<tr>
<td><p>G</p></td>
<td><p>409</p></td>
<td><p>1990</p></td>
<td><p>643</p></td>
<td><p>199</p></td>
<td><p>3241</p></td>
</tr>
<tr>
<td/>
<td><p>0.126</p></td>
<td><p>0.614</p></td>
<td><p>0.198</p></td>
<td><p>0.061</p></td>
<td><p>0.007</p></td>
</tr>
</tbody>
<tfoot>
<tr>
<td><p>Total</p></td>
<td><p>22671</p></td>
<td><p>321185</p></td>
<td><p>97316</p></td>
<td><p>9789</p></td>
<td><p>450961</p></td>
</tr>
</tfoot>
</table>

<p>Contingency tables can look only at counts, or they can also include column and total percentages.
Pivot tables in Excel are perhaps the most common tool used to create contingency tables.<a data-type="indexterm" data-primary="pivot tables" data-seealso="contingency tables" id="idm46522864141416"/>
In <em>R</em>, the <code>CrossTable</code> function in the <code>descr</code> package produces contingency tables, and the following code was used to create <a data-type="xref" href="#CrossTabs">Table 1-8</a>:</p>

<pre data-type="programlisting" data-code-language="r"><code class="nf">library</code><code class="p">(</code><code class="n">descr</code><code class="p">)</code>
<code class="n">x_tab</code> <code class="o">&lt;-</code> <code class="nf">CrossTable</code><code class="p">(</code><code class="n">lc_loans</code><code class="o">$</code><code class="n">grade</code><code class="p">,</code> <code class="n">lc_loans</code><code class="o">$</code><code class="n">status</code><code class="p">,</code>
                    <code class="n">prop.c</code><code class="o">=</code><code class="kc">FALSE</code><code class="p">,</code> <code class="n">prop.chisq</code><code class="o">=</code><code class="kc">FALSE</code><code class="p">,</code> <code class="n">prop.t</code><code class="o">=</code><code class="kc">FALSE</code><code class="p">)</code></pre>

<p>The <code>pivot_table</code> method creates the pivot table in <em>Python</em>. The <code>aggfunc</code> argument allows us to get the counts. Calculating the percentages is a bit more involved:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">crosstab</code><code> </code><code class="o">=</code><code> </code><code class="n">lc_loans</code><code class="o">.</code><code class="n">pivot_table</code><code class="p">(</code><code class="n">index</code><code class="o">=</code><code class="s1">'</code><code class="s1">grade</code><code class="s1">'</code><code class="p">,</code><code> </code><code class="n">columns</code><code class="o">=</code><code class="s1">'</code><code class="s1">status</code><code class="s1">'</code><code class="p">,</code><code>
</code><code>                                </code><code class="n">aggfunc</code><code class="o">=</code><code class="k">lambda</code><code> </code><code class="n">x</code><code class="p">:</code><code> </code><code class="nb">len</code><code class="p">(</code><code class="n">x</code><code class="p">)</code><code class="p">,</code><code> </code><code class="n">margins</code><code class="o">=</code><code class="bp">True</code><code class="p">)</code><code> </code><a class="co" id="co_exploratory_data_analysis_CO2-1" href="#callout_exploratory_data_analysis_CO2-1"><img src="Images/1.png" alt="1" width="12" height="12"/></a><code>
</code><code>
</code><code class="n">df</code><code> </code><code class="o">=</code><code> </code><code class="n">crosstab</code><code class="o">.</code><code class="n">loc</code><code class="p">[</code><code class="s1">'</code><code class="s1">A</code><code class="s1">'</code><code class="p">:</code><code class="s1">'</code><code class="s1">G</code><code class="s1">'</code><code class="p">,</code><code class="p">:</code><code class="p">]</code><code class="o">.</code><code class="n">copy</code><code class="p">(</code><code class="p">)</code><code> </code><a class="co" id="co_exploratory_data_analysis_CO2-2" href="#callout_exploratory_data_analysis_CO2-2"><img src="Images/2.png" alt="2" width="12" height="12"/></a><code>
</code><code class="n">df</code><code class="o">.</code><code class="n">loc</code><code class="p">[</code><code class="p">:</code><code class="p">,</code><code class="s1">'</code><code class="s1">Charged Off</code><code class="s1">'</code><code class="p">:</code><code class="s1">'</code><code class="s1">Late</code><code class="s1">'</code><code class="p">]</code><code> </code><code class="o">=</code><code> </code><code class="n">df</code><code class="o">.</code><code class="n">loc</code><code class="p">[</code><code class="p">:</code><code class="p">,</code><code class="s1">'</code><code class="s1">Charged Off</code><code class="s1">'</code><code class="p">:</code><code class="s1">'</code><code class="s1">Late</code><code class="s1">'</code><code class="p">]</code><code class="o">.</code><code class="n">div</code><code class="p">(</code><code class="n">df</code><code class="p">[</code><code class="s1">'</code><code class="s1">All</code><code class="s1">'</code><code class="p">]</code><code class="p">,</code><code>
</code><code>                                                                    </code><code class="n">axis</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code><code> </code><a class="co" id="co_exploratory_data_analysis_CO2-3" href="#callout_exploratory_data_analysis_CO2-3"><img src="Images/3.png" alt="3" width="12" height="12"/></a><code>
</code><code class="n">df</code><code class="p">[</code><code class="s1">'</code><code class="s1">All</code><code class="s1">'</code><code class="p">]</code><code> </code><code class="o">=</code><code> </code><code class="n">df</code><code class="p">[</code><code class="s1">'</code><code class="s1">All</code><code class="s1">'</code><code class="p">]</code><code> </code><code class="o">/</code><code> </code><code class="nb">sum</code><code class="p">(</code><code class="n">df</code><code class="p">[</code><code class="s1">'</code><code class="s1">All</code><code class="s1">'</code><code class="p">]</code><code class="p">)</code><code> </code><a class="co" id="co_exploratory_data_analysis_CO2-4" href="#callout_exploratory_data_analysis_CO2-4"><img src="Images/4.png" alt="4" width="12" height="12"/></a><code>
</code><code class="n">perc_crosstab</code><code> </code><code class="o">=</code><code> </code><code class="n">df</code></pre>
<dl class="calloutlist">
<dt><a class="co" id="callout_exploratory_data_analysis_CO2-1" href="#co_exploratory_data_analysis_CO2-1"><img src="Images/1.png" alt="1" width="12" height="12"/></a></dt>
<dd><p>The <code>margins</code> keyword argument will add the column and row sums.</p></dd>
<dt><a class="co" id="callout_exploratory_data_analysis_CO2-2" href="#co_exploratory_data_analysis_CO2-2"><img src="Images/2.png" alt="2" width="12" height="12"/></a></dt>
<dd><p>We create a copy of the pivot table, ignoring the column sums.</p></dd>
<dt><a class="co" id="callout_exploratory_data_analysis_CO2-3" href="#co_exploratory_data_analysis_CO2-3"><img src="Images/3.png" alt="3" width="12" height="12"/></a></dt>
<dd><p>We divide the rows with the row sum.</p></dd>
<dt><a class="co" id="callout_exploratory_data_analysis_CO2-4" href="#co_exploratory_data_analysis_CO2-4"><img src="Images/4.png" alt="4" width="12" height="12"/></a></dt>
<dd><p>We divide the <code>'All'</code> column by its sum.</p></dd>
</dl>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Categorical and Numeric Data"><div class="sect2" id="idm46522863944040">
<h2>Categorical and Numeric Data</h2>

<p>Boxplots (see <a data-type="xref" href="#Boxplots">“Percentiles and Boxplots”</a>) are a simple way to visually compare the distributions of a numeric variable grouped according to a categorical variable.<a data-type="indexterm" data-primary="variables" data-secondary="exploring two or more" data-tertiary="categorical and numeric data" id="idm46522863888872"/><a data-type="indexterm" data-primary="categorical data" data-secondary="exploring numeric variable grouped by categorical variable" id="idm46522863887688"/><a data-type="indexterm" data-primary="boxplots" data-secondary="comparing numeric and categorical data" id="idm46522863886776"/><a data-type="indexterm" data-primary="numeric data" data-secondary="grouped by categorical variable, exploring" id="idm46522863885864"/>
For example, we might want to compare how the percentage of flight delays varies across airlines.
<a data-type="xref" href="#SideBySideBoxplots">Figure 1-10</a> shows the percentage of flights in a month that were delayed where the delay was within the carrier’s control:</p>

<pre data-type="programlisting" data-code-language="r"><code class="nf">boxplot</code><code class="p">(</code><code class="n">pct_carrier_delay</code> <code class="o">~</code> <code class="n">airline</code><code class="p">,</code> <code class="n">data</code><code class="o">=</code><code class="n">airline_stats</code><code class="p">,</code> <code class="n">ylim</code><code class="o">=</code><code class="nf">c</code><code class="p">(</code><code class="m">0</code><code class="p">,</code> <code class="m">50</code><code class="p">))</code></pre>

<p>The <code>pandas</code> <code>boxplot</code> method takes the <code>by</code> argument that splits the data set into groups and creates the individual boxplots:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">ax</code> <code class="o">=</code> <code class="n">airline_stats</code><code class="o">.</code><code class="n">boxplot</code><code class="p">(</code><code class="n">by</code><code class="o">=</code><code class="s1">'airline'</code><code class="p">,</code> <code class="n">column</code><code class="o">=</code><code class="s1">'pct_carrier_delay'</code><code class="p">)</code>
<code class="n">ax</code><code class="o">.</code><code class="n">set_xlabel</code><code class="p">(</code><code class="s1">''</code><code class="p">)</code>
<code class="n">ax</code><code class="o">.</code><code class="n">set_ylabel</code><code class="p">(</code><code class="s1">'Daily </code><code class="si">% o</code><code class="s1">f Delayed Flights'</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">suptitle</code><code class="p">(</code><code class="s1">''</code><code class="p">)</code></pre>

<figure class="width-75"><div id="SideBySideBoxplots" class="figure">
<img src="Images/psd2_0110.png" alt="Boxplot of percent of airline delays by carrier." width="1200" height="1200"/>
<h6><span class="label">Figure 1-10. </span>Boxplot of percent of airline delays by carrier</h6>
</div></figure>

<p>Alaska stands out as having the fewest delays, while American has the most delays: the lower quartile for American is higher than the upper quartile for Alaska.</p>

<p>A <em>violin plot</em>, introduced by <a data-type="link" href="bibliography01.xhtml#Hintze-Nelson-1998">[Hintze-Nelson-1998]</a>, is an enhancement to the boxplot and plots the density estimate with the density on the y-axis.<a data-type="indexterm" data-primary="violin plots" data-secondary="boxplots versus" id="idm46522863803816"/><a data-type="indexterm" data-primary="boxplots" data-secondary="violin plots versus" id="idm46522863802840"/>
The density is mirrored and flipped over, and the resulting shape is filled in, creating an image resembling a violin.
The advantage of a violin plot is that it can show nuances in the distribution that aren’t perceptible in a boxplot.
On the other hand, the boxplot more clearly shows the outliers in the data.
In <code>ggplot2</code>, the function <code>geom_violin</code> can be used to create a violin plot as follows:</p>

<pre data-type="programlisting" data-code-language="r"><code class="nf">ggplot</code><code class="p">(</code><code class="n">data</code><code class="o">=</code><code class="n">airline_stats</code><code class="p">,</code> <code class="nf">aes</code><code class="p">(</code><code class="n">airline</code><code class="p">,</code> <code class="n">pct_carrier_delay</code><code class="p">))</code> <code class="o">+</code>
  <code class="nf">ylim</code><code class="p">(</code><code class="m">0</code><code class="p">,</code> <code class="m">50</code><code class="p">)</code> <code class="o">+</code>
  <code class="nf">geom_violin</code><code class="p">()</code> <code class="o">+</code>
  <code class="nf">labs</code><code class="p">(</code><code class="n">x</code><code class="o">=</code><code class="s">''</code><code class="p">,</code> <code class="n">y</code><code class="o">=</code><code class="s">'Daily % of Delayed Flights'</code><code class="p">)</code></pre>

<p>Violin plots are available with the <code>violinplot</code> method of the <code>seaborn</code> package:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">ax</code> <code class="o">=</code> <code class="n">sns</code><code class="o">.</code><code class="n">violinplot</code><code class="p">(</code><code class="n">airline_stats</code><code class="o">.</code><code class="n">airline</code><code class="p">,</code> <code class="n">airline_stats</code><code class="o">.</code><code class="n">pct_carrier_delay</code><code class="p">,</code>
                    <code class="n">inner</code><code class="o">=</code><code class="s1">'quartile'</code><code class="p">,</code> <code class="n">color</code><code class="o">=</code><code class="s1">'white'</code><code class="p">)</code>
<code class="n">ax</code><code class="o">.</code><code class="n">set_xlabel</code><code class="p">(</code><code class="s1">''</code><code class="p">)</code>
<code class="n">ax</code><code class="o">.</code><code class="n">set_ylabel</code><code class="p">(</code><code class="s1">'Daily </code><code class="si">% o</code><code class="s1">f Delayed Flights'</code><code class="p">)</code></pre>

<p>The corresponding plot is shown in <a data-type="xref" href="#ViolinPlot">Figure 1-11</a>.
The violin plot shows a concentration in the distribution near zero for Alaska and, to a lesser extent, Delta.
This phenomenon is not as obvious in the boxplot.
You can combine a violin plot with a boxplot by adding <code>geom_boxplot</code> to the plot (although this works best when colors are used).</p>

<figure class="width-75"><div id="ViolinPlot" class="figure">
<img src="Images/psd2_0111.png" alt="Violin plot of percent of airline delays by carrier." width="1157" height="1095"/>
<h6><span class="label">Figure 1-11. </span>Violin plot of percent of airline delays by carrier</h6>
</div></figure>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Visualizing Multiple Variables"><div class="sect2" id="StatisticalGraphics">
<h2>Visualizing Multiple Variables</h2>

<p>The types of charts used to compare two variables—scatterplots, hexagonal binning, and boxplots—are readily extended to more variables through the notion of <em>conditioning</em>.<a data-type="indexterm" data-primary="hexagonal binning" data-secondary="extending with conditional variables" id="idm46522863664568"/><a data-type="indexterm" data-primary="scatterplots" data-secondary="extending with conditional variables" id="idm46522863663592"/><a data-type="indexterm" data-primary="boxplots" data-secondary="extending with conditional variables" id="idm46522863662680"/><a data-type="indexterm" data-primary="variables" data-secondary="exploring two or more" data-tertiary="visualizing multiple variables" id="idm46522863661768"/><a data-type="indexterm" data-primary="conditioning variables" id="idm46522863660536"/>
As an example, look back at <a data-type="xref" href="#HexagonalBinning">Figure 1-8</a>, which showed the relationship between homes’ finished square feet and their tax-assessed values.
We observed that there appears to be a cluster of homes that have higher tax-assessed value per square foot.
Diving deeper, <a data-type="xref" href="#HouseByZip">Figure 1-12</a> accounts for the effect of location by plotting the data for a set of zip codes.
Now the picture is much clearer: tax-assessed value is much higher in some zip codes (98105, 98126) than in others (98108, 98188).
This disparity gives rise to the clusters observed in <a data-type="xref" href="#HexagonalBinning">Figure 1-8</a>.</p>

<p class="pagebreak-before">We created <a data-type="xref" href="#HouseByZip">Figure 1-12</a> using <code>ggplot2</code> and the idea of <em>facets</em>, or a conditioning<a data-type="indexterm" data-primary="facets" id="idm46522863654520"/> variable (in this case, zip code):</p>

<pre data-type="programlisting" data-code-language="r"><code class="nf">ggplot</code><code class="p">(</code><code class="nf">subset</code><code class="p">(</code><code class="n">kc_tax0</code><code class="p">,</code><code> </code><code class="n">ZipCode</code><code> </code><code class="o">%in%</code><code> </code><code class="nf">c</code><code class="p">(</code><code class="m">98188</code><code class="p">,</code><code> </code><code class="m">98105</code><code class="p">,</code><code> </code><code class="m">98108</code><code class="p">,</code><code> </code><code class="m">98126</code><code class="p">)</code><code class="p">)</code><code class="p">,</code><code>
         </code><code class="nf">aes</code><code class="p">(</code><code class="n">x</code><code class="o">=</code><code class="n">SqFtTotLiving</code><code class="p">,</code><code> </code><code class="n">y</code><code class="o">=</code><code class="n">TaxAssessedValue</code><code class="p">)</code><code class="p">)</code><code> </code><code class="o">+</code><code>
  </code><code class="nf">stat_binhex</code><code class="p">(</code><code class="n">color</code><code class="o">=</code><code class="s">'</code><code class="s">white'</code><code class="p">)</code><code> </code><code class="o">+</code><code>
  </code><code class="nf">theme_bw</code><code class="p">(</code><code class="p">)</code><code> </code><code class="o">+</code><code>
  </code><code class="nf">scale_fill_gradient</code><code class="p">(</code><code class="n">low</code><code class="o">=</code><code class="s">'</code><code class="s">white'</code><code class="p">,</code><code> </code><code class="n">high</code><code class="o">=</code><code class="s">'</code><code class="s">blue'</code><code class="p">)</code><code> </code><code class="o">+</code><code>
  </code><code class="nf">labs</code><code class="p">(</code><code class="n">x</code><code class="o">=</code><code class="s">'</code><code class="s">Finished Square Feet'</code><code class="p">,</code><code> </code><code class="n">y</code><code class="o">=</code><code class="s">'</code><code class="s">Tax-Assessed Value'</code><code class="p">)</code><code> </code><code class="o">+</code><code>
  </code><code class="nf">facet_wrap</code><code class="p">(</code><code class="s">'</code><code class="s">ZipCode'</code><code class="p">)</code><code> </code><a class="co" id="co_exploratory_data_analysis_CO3-1" href="#callout_exploratory_data_analysis_CO3-1"><img src="Images/1.png" alt="1" width="12" height="12"/></a></pre>
<dl class="calloutlist">
<dt><a class="co" id="callout_exploratory_data_analysis_CO3-1" href="#co_exploratory_data_analysis_CO3-1"><img src="Images/1.png" alt="1" width="12" height="12"/></a></dt>
<dd><p>Use the <code>ggplot</code> functions <code>facet_wrap</code> and <code>facet_grid</code> to specify the conditioning variable.</p></dd>
</dl>

<figure><div id="HouseByZip" class="figure">
<img src="Images/psd2_0112.png" alt="Tax assess value versus finished square feet by zip code." width="1430" height="1156"/>
<h6><span class="label">Figure 1-12. </span>Tax-assessed value versus finished square feet by zip code</h6>
</div></figure>

<p class="pagebreak-after">Most <em>Python</em> packages base their visualizations on <code>Matplotlib</code>. While it is in principle possible to create faceted graphs using <code>Matplotlib</code>, the code can get complicated. Fortunately, <code>seaborn</code> has a relatively straightforward way of creating these graphs:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">zip_codes</code><code> </code><code class="o">=</code><code> </code><code class="p">[</code><code class="mi">98188</code><code class="p">,</code><code> </code><code class="mi">98105</code><code class="p">,</code><code> </code><code class="mi">98108</code><code class="p">,</code><code> </code><code class="mi">98126</code><code class="p">]</code><code>
</code><code class="n">kc_tax_zip</code><code> </code><code class="o">=</code><code> </code><code class="n">kc_tax0</code><code class="o">.</code><code class="n">loc</code><code class="p">[</code><code class="n">kc_tax0</code><code class="o">.</code><code class="n">ZipCode</code><code class="o">.</code><code class="n">isin</code><code class="p">(</code><code class="n">zip_codes</code><code class="p">)</code><code class="p">,</code><code class="p">:</code><code class="p">]</code><code>
</code><code class="n">kc_tax_zip</code><code>
</code><code>
</code><code class="k">def</code><code> </code><code class="nf">hexbin</code><code class="p">(</code><code class="n">x</code><code class="p">,</code><code> </code><code class="n">y</code><code class="p">,</code><code> </code><code class="n">color</code><code class="p">,</code><code> </code><code class="o">*</code><code class="o">*</code><code class="n">kwargs</code><code class="p">)</code><code class="p">:</code><code>
</code><code>    </code><code class="n">cmap</code><code> </code><code class="o">=</code><code> </code><code class="n">sns</code><code class="o">.</code><code class="n">light_palette</code><code class="p">(</code><code class="n">color</code><code class="p">,</code><code> </code><code class="n">as_cmap</code><code class="o">=</code><code class="bp">True</code><code class="p">)</code><code>
</code><code>    </code><code class="n">plt</code><code class="o">.</code><code class="n">hexbin</code><code class="p">(</code><code class="n">x</code><code class="p">,</code><code> </code><code class="n">y</code><code class="p">,</code><code> </code><code class="n">gridsize</code><code class="o">=</code><code class="mi">25</code><code class="p">,</code><code> </code><code class="n">cmap</code><code class="o">=</code><code class="n">cmap</code><code class="p">,</code><code> </code><code class="o">*</code><code class="o">*</code><code class="n">kwargs</code><code class="p">)</code><code>
</code><code>
</code><code class="n">g</code><code> </code><code class="o">=</code><code> </code><code class="n">sns</code><code class="o">.</code><code class="n">FacetGrid</code><code class="p">(</code><code class="n">kc_tax_zip</code><code class="p">,</code><code> </code><code class="n">col</code><code class="o">=</code><code class="s1">'</code><code class="s1">ZipCode</code><code class="s1">'</code><code class="p">,</code><code> </code><code class="n">col_wrap</code><code class="o">=</code><code class="mi">2</code><code class="p">)</code><code> </code><a class="co" id="co_exploratory_data_analysis_CO4-1" href="#callout_exploratory_data_analysis_CO4-1"><img src="Images/1.png" alt="1" width="12" height="12"/></a><code>
</code><code class="n">g</code><code class="o">.</code><code class="n">map</code><code class="p">(</code><code class="n">hexbin</code><code class="p">,</code><code> </code><code class="s1">'</code><code class="s1">SqFtTotLiving</code><code class="s1">'</code><code class="p">,</code><code> </code><code class="s1">'</code><code class="s1">TaxAssessedValue</code><code class="s1">'</code><code class="p">,</code><code>
</code><code>      </code><code class="n">extent</code><code class="o">=</code><code class="p">[</code><code class="mi">0</code><code class="p">,</code><code> </code><code class="mi">3500</code><code class="p">,</code><code> </code><code class="mi">0</code><code class="p">,</code><code> </code><code class="mi">700000</code><code class="p">]</code><code class="p">)</code><code> </code><a class="co" id="co_exploratory_data_analysis_CO4-2" href="#callout_exploratory_data_analysis_CO4-2"><img src="Images/2.png" alt="2" width="12" height="12"/></a><code>
</code><code class="n">g</code><code class="o">.</code><code class="n">set_axis_labels</code><code class="p">(</code><code class="s1">'</code><code class="s1">Finished Square Feet</code><code class="s1">'</code><code class="p">,</code><code> </code><code class="s1">'</code><code class="s1">Tax-Assessed Value</code><code class="s1">'</code><code class="p">)</code><code>
</code><code class="n">g</code><code class="o">.</code><code class="n">set_titles</code><code class="p">(</code><code class="s1">'</code><code class="s1">Zip code {col_name:.0f}</code><code class="s1">'</code><code class="p">)</code></pre>
<dl class="calloutlist">
<dt><a class="co" id="callout_exploratory_data_analysis_CO4-1" href="#co_exploratory_data_analysis_CO4-1"><img src="Images/1.png" alt="1" width="12" height="12"/></a></dt>
<dd><p>Use the arguments <code>col</code> and <code>row</code> to specify the conditioning variables. For a single conditioning variable, use <code>col</code> together with <code>col_wrap</code> to wrap the faceted graphs into multiple rows.</p></dd>
<dt><a class="co" id="callout_exploratory_data_analysis_CO4-2" href="#co_exploratory_data_analysis_CO4-2"><img src="Images/2.png" alt="2" width="12" height="12"/></a></dt>
<dd><p>The <code>map</code> method calls the <code>hexbin</code> function with subsets of the original data set for the different zip codes. <code>extent</code> defines the limits of the x- and y-axes.</p></dd>
</dl>

<p>The concept of conditioning variables <a data-type="indexterm" data-primary="Trellis graphics" id="idm46522863413272"/>in a graphics system was pioneered with <em>Trellis graphics</em>, developed by Rick Becker, Bill Cleveland, and others at Bell Labs <a data-type="link" href="bibliography01.xhtml#Trellis-Graphics">[Trellis-Graphics]</a>.
This idea has propagated to various modern graphics systems, such as the <code>lattice</code> <a data-type="link" href="bibliography01.xhtml#lattice">[lattice]</a> and <code>ggplot2</code> packages in <em>R</em> and the  <code>seaborn</code> <a data-type="link" href="bibliography01.xhtml#seaborn">[seaborn]</a> and <code>Bokeh</code> <a data-type="link" href="bibliography01.xhtml#bokeh">[bokeh]</a> modules in <em>Python</em>.
Conditioning variables are also integral to business intelligence platforms such as Tableau and Spotfire.
With the advent of vast computing power, modern visualization platforms have moved well beyond the humble beginnings of exploratory data analysis.
However, key concepts and tools developed a half century ago (e.g., simple boxplots) still form a foundation for these systems.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46522863412984">
<h5>Key Ideas</h5>
<ul>
<li>
<p>Hexagonal binning and contour plots are useful tools that permit graphical examination of two numeric variables at a time, without being overwhelmed by huge amounts of data.</p>
</li>
<li>
<p>Contingency tables are the standard tool for looking at the counts of two categorical variables.</p>
</li>
<li>
<p>Boxplots and violin plots allow you to plot a numeric variable against a categorical variable.</p>
</li>
</ul>
</div></aside>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Further Reading"><div class="sect2" id="idm46522863666728">
<h2>Further Reading</h2>

<ul>
<li>
<p><em>Modern Data Science with R</em> by Benjamin Baumer, Daniel Kaplan, and Nicholas Horton (Chapman &amp; Hall/CRC Press, 2017) has an excellent presentation of “a grammar for graphics” (the “gg” in <code>ggplot</code>).</p>
</li>
<li>
<p><em>ggplot2: Elegant Graphics for Data Analysis</em> by Hadley Wickham (Springer, 2009) is an excellent resource from the creator of <code>ggplot2</code>.</p>
</li>
<li>
<p>Josef Fruehwald has a web-based tutorial on <a href="https://oreil.ly/zB2Dz"><code>ggplot2</code></a>.</p>
</li>
</ul>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Summary"><div class="sect1" id="Conclusion_EDA">
<h1>Summary</h1>

<p>Exploratory data analysis (EDA), pioneered by John Tukey, set a foundation for the field of data science.<a data-type="indexterm" data-primary="multivariate analysis" data-startref="ix_mltivar" id="idm46522863391992"/><a data-type="indexterm" data-primary="variables" data-secondary="exploring two or more" data-startref="ix_varexp" id="idm46522863391016"/><a data-type="indexterm" data-primary="exploratory data analysis" data-secondary="exploring two or more variables" data-startref="ix_expvars" id="idm46522863389800"/>
The key idea of EDA is that the first and most important step in any project based on data is to <em>look at the data</em>.
By summarizing and visualizing the data, you can gain valuable intuition and  understanding of the project.</p>

<p>This chapter has reviewed concepts
ranging from simple metrics, such as estimates of location and variability, to rich visual displays that explore the relationships between multiple variables, as in <a data-type="xref" href="#HouseByZip">Figure 1-12</a>.
The diverse set of tools and techniques being developed by the open source community, combined with the expressiveness of the <em>R</em> and <em>Python</em> languages, has created a plethora of ways to explore and analyze data.
Exploratory analysis should be a cornerstone of any data science project.<a data-type="indexterm" data-primary="exploratory data analysis" data-startref="ix_expda" id="idm46522863385192"/></p>
</div></section>







</div></section></div>



  </body></html>