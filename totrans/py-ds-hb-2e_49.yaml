- en: 'Chapter 44\. In Depth: Decision Trees and Random Forests'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Previously we have looked in depth at a simple generative classifier (naive
    Bayes; see [Chapter 41](ch41.xhtml#section-0505-naive-bayes)) and a powerful discriminative
    classifier (support vector machines; see [Chapter 43](ch43.xhtml#section-0507-support-vector-machines)).
    Here we’ll take a look at another powerful algorithm: a nonparametric algorithm
    called *random forests*. Random forests are an example of an *ensemble* method,
    meaning one that relies on aggregating the results of a set of simpler estimators.
    The somewhat surprising result with such ensemble methods is that the sum can
    be greater than the parts: that is, the predictive accuracy of a majority vote
    among a number of estimators can end up being better than that of any of the individual
    estimators doing the voting! We will see examples of this in the following sections.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We begin with the standard imports:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Motivating Random Forests: Decision Trees'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Random forests are an example of an ensemble learner built on decision trees.
    For this reason, we’ll start by discussing decision trees themselves.
  prefs: []
  type: TYPE_NORMAL
- en: 'Decision trees are extremely intuitive ways to classify or label objects: you
    simply ask a series of questions designed to zero in on the classification. For
    example, if you wanted to build a decision tree to classify animals you come across
    while on a hike, you might construct the one shown in [Figure 44-1](#fig_images_in_0508-decision-tree).'
  prefs: []
  type: TYPE_NORMAL
- en: '![05.08 decision tree](assets/05.08-decision-tree.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 44-1\. An example of a binary decision tree^([1](ch44.xhtml#idm45858730712544))
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The binary splitting makes this extremely efficient: in a well-constructed
    tree, each question will cut the number of options by approximately half, very
    quickly narrowing the options even among a large number of classes. The trick,
    of course, comes in deciding which questions to ask at each step. In machine learning
    implementations of decision trees, the questions generally take the form of axis-aligned
    splits in the data: that is, each node in the tree splits the data into two groups
    using a cutoff value within one of the features. Let’s now look at an example
    of this.'
  prefs: []
  type: TYPE_NORMAL
- en: Creating a Decision Tree
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Consider the following two-dimensional data, which has one of four class labels
    (see [Figure 44-2](#fig_0508-random-forests_files_in_output_8_0)).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![output 8 0](assets/output_8_0.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 44-2\. Data for the decision tree classifier
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: A simple decision tree built on this data will iteratively split the data along
    one or the other axis according to some quantitative criterion, and at each level
    assign the label of the new region according to a majority vote of points within
    it. [Figure 44-3](#fig_images_in_0508-decision-tree-levels) presents a visualization
    of the first four levels of a decision tree classifier for this data.
  prefs: []
  type: TYPE_NORMAL
- en: '![05.08 decision tree levels](assets/05.08-decision-tree-levels.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 44-3\. Visualization of how the decision tree splits the data^([2](ch44.xhtml#idm45858730648048))
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Notice that after the first split, every point in the upper branch remains unchanged,
    so there is no need to further subdivide this branch. Except for nodes that contain
    all of one color, at each level *every* region is again split along one of the
    two features.
  prefs: []
  type: TYPE_NORMAL
- en: 'This process of fitting a decision tree to our data can be done in Scikit-Learn
    with the `DecisionTreeClassifier` estimator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s write a utility function to help us visualize the output of the classifier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Now we can examine what the decision tree classification looks like (see [Figure 44-4](#fig_0508-random-forests_files_in_output_17_0)).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '![output 17 0](assets/output_17_0.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 44-4\. Visualization of a decision tree classification
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'If you’re running this notebook live, you can use the helper script included
    in the online [appendix](https://oreil.ly/etDrN) to bring up an interactive visualization
    of the decision tree building process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Notice that as the depth increases, we tend to get very strangely shaped classification
    regions; for example, at a depth of five, there is a tall and skinny purple region
    between the yellow and blue regions. It’s clear that this is less a result of
    the true, intrinsic data distribution, and more a result of the particular sampling
    or noise properties of the data. That is, this decision tree, even at only five
    levels deep, is clearly overfitting our data.
  prefs: []
  type: TYPE_NORMAL
- en: Decision Trees and Overfitting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Such overfitting turns out to be a general property of decision trees: it is
    very easy to go too deep in the tree, and thus to fit details of the particular
    data rather than the overall properties of the distributions it is drawn from.
    Another way to see this overfitting is to look at models trained on different
    subsets of the data—for example, in [Figure 44-5](#fig_images_in_0508-decision-tree-overfitting)
    we train two different trees, each on half of the original data.'
  prefs: []
  type: TYPE_NORMAL
- en: '![05.08 decision tree overfitting](assets/05.08-decision-tree-overfitting.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 44-5\. An example of two randomized decision trees^([3](ch44.xhtml#idm45858730171984))
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: It is clear that in some places the two trees produce consistent results (e.g.,
    in the four corners), while in other places the two trees give very different
    classifications (e.g., in the regions between any two clusters). The key observation
    is that the inconsistencies tend to happen where the classification is less certain,
    and thus by using information from *both* of these trees, we might come up with
    a better result!
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are running this notebook live, the following function will allow you
    to interactively display the fits of trees trained on a random subset of the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Just as using information from two trees improves our results, we might expect
    that using information from many trees would improve our results even further.
  prefs: []
  type: TYPE_NORMAL
- en: 'Ensembles of Estimators: Random Forests'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This notion—that multiple overfitting estimators can be combined to reduce the
    effect of this overfitting—is what underlies an ensemble method called *bagging*.
    Bagging makes use of an ensemble (a grab bag, perhaps) of parallel estimators,
    each of which overfits the data, and averages the results to find a better classification.
    An ensemble of randomized decision trees is known as a *random forest*.
  prefs: []
  type: TYPE_NORMAL
- en: This type of bagging classification can be done manually using Scikit-Learn’s
    `BaggingClassifier` meta-estimator, as shown here (see [Figure 44-6](#fig_0508-random-forests_files_in_output_28_0)).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'In this example, we have randomized the data by fitting each estimator with
    a random subset of 80% of the training points. In practice, decision trees are
    more effectively randomized by injecting some stochasticity in how the splits
    are chosen: this way all the data contributes to the fit each time, but the results
    of the fit still have the desired randomness. For example, when determining which
    feature to split on, the randomized tree might select from among the top several
    features. You can read more technical details about these randomization strategies
    in the [Scikit-Learn documentation](https://oreil.ly/4jrv4) and references within.'
  prefs: []
  type: TYPE_NORMAL
- en: '![output 28 0](assets/output_28_0.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 44-6\. Decision boundaries for an ensemble of random decision trees
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In Scikit-Learn, such an optimized ensemble of randomized decision trees is
    implemented in the `RandomForestClassifier` estimator, which takes care of all
    the randomization automatically. All you need to do is select a number of estimators,
    and it will very quickly—in parallel, if desired—fit the ensemble of trees (see
    [Figure 44-7](#fig_0508-random-forests_files_in_output_30_0)).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '![output 30 0](assets/output_30_0.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 44-7\. Decision boundaries for a random forest, which is an optimized
    ensemble of decision trees
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We see that by averaging over one hundred randomly perturbed models, we end
    up with an overall model that is much closer to our intuition about how the parameter
    space should be split.
  prefs: []
  type: TYPE_NORMAL
- en: Random Forest Regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section we considered random forests within the context of classification.
    Random forests can also be made to work in the case of regression (that is, with
    continuous rather than categorical variables). The estimator to use for this is
    the `RandomForestRegressor`, and the syntax is very similar to what we saw earlier.
  prefs: []
  type: TYPE_NORMAL
- en: Consider the following data, drawn from the combination of a fast and slow oscillation
    (see [Figure 44-8](#fig_0508-random-forests_files_in_output_33_0)).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '![output 33 0](assets/output_33_0.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 44-8\. Data for random forest regression
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Using the random forest regressor, we can find the best-fit curve ([Figure 44-9](#fig_0508-random-forests_files_in_output_35_0)).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '![output 35 0](assets/output_35_0.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 44-9\. Random forest model fit to the data
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Here the true model is shown in the smooth gray curve, while the random forest
    model is shown by the jagged red curve. The nonparametric random forest model
    is flexible enough to fit the multiperiod data, without us needing to specifying
    a multi-period model!
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: Random Forest for Classifying Digits'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In [Chapter 38](ch38.xhtml#section-0502-introducing-scikit-learn), we worked
    through an example using the digits dataset included with Scikit-Learn. Let’s
    use that again here to see how the random forest classifier can be applied in
    this context:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: To remind us what we’re looking at, we’ll visualize the first few data points
    (see [Figure 44-10](#fig_0508-random-forests_files_in_output_40_0)).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '![output 40 0](assets/output_40_0.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 44-10\. Representation of the digits data
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'We can classify the digits using a random forest as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s look at the classification report for this classifier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: And for good measure, plot the confusion matrix (see [Figure 44-11](#fig_0508-random-forests_files_in_output_46_0)).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: We find that a simple, untuned random forest results in a quite accurate classification
    of the digits data.
  prefs: []
  type: TYPE_NORMAL
- en: '![output 46 0](assets/output_46_0.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 44-11\. Confusion matrix for digit classification with random forests
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter provided a brief introduction to the concept of ensemble estimators,
    and in particular the random forest, an ensemble of randomized decision trees.
    Random forests are a powerful method with several advantages:'
  prefs: []
  type: TYPE_NORMAL
- en: Both training and prediction are very fast, because of the simplicity of the
    underlying decision trees. In addition, both tasks can be straightforwardly parallelized,
    because the individual trees are entirely independent entities.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The multiple trees allow for a probabilistic classification: a majority vote
    among estimators gives an estimate of the probability (accessed in Scikit-Learn
    with the `predict_proba` method).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The nonparametric model is extremely flexible and can thus perform well on tasks
    that are underfit by other estimators.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A primary disadvantage of random forests is that the results are not easily
    interpretable: that is, if you would like to draw conclusions about the *meaning*
    of the classification model, random forests may not be the best choice.'
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch44.xhtml#idm45858730712544-marker)) Code to produce this figure can
    be found in the [online appendix](https://oreil.ly/xP9ZI).
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch44.xhtml#idm45858730648048-marker)) Code to produce this figure can
    be found in the [online appendix](https://oreil.ly/H4WFg).
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](ch44.xhtml#idm45858730171984-marker)) Code to produce this figure can
    be found in the [online appendix](https://oreil.ly/PessV).
  prefs: []
  type: TYPE_NORMAL
