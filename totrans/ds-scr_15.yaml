- en: Chapter 14\. Simple Linear Regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Art, like morality, consists in drawing the line somewhere.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: G. K. Chesterton
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In [Chapter 5](ch05.html#statistics), we used the `correlation` function to
    measure the strength of the linear relationship between two variables. For most
    applications, knowing that such a linear relationship exists isn’t enough. We’ll
    want to understand the nature of the relationship. This is where we’ll use simple
    linear regression.
  prefs: []
  type: TYPE_NORMAL
- en: The Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Recall that we were investigating the relationship between a DataSciencester
    user’s number of friends and the amount of time the user spends on the site each
    day. Let’s assume that you’ve convinced yourself that having more friends *causes*
    people to spend more time on the site, rather than one of the alternative explanations
    we discussed.
  prefs: []
  type: TYPE_NORMAL
- en: The VP of Engagement asks you to build a model describing this relationship.
    Since you found a pretty strong linear relationship, a natural place to start
    is a linear model.
  prefs: []
  type: TYPE_NORMAL
- en: 'In particular, you hypothesize that there are constants *α* (alpha) and *β*
    (beta) such that:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="y Subscript i Baseline equals beta x Subscript i Baseline plus
    alpha plus epsilon Subscript i" display="block"><mrow><msub><mi>y</mi> <mi>i</mi></msub>
    <mo>=</mo> <mi>β</mi> <msub><mi>x</mi> <mi>i</mi></msub> <mo>+</mo> <mi>α</mi>
    <mo>+</mo> <msub><mi>ε</mi> <mi>i</mi></msub></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: where <math><msub><mi>y</mi> <mi>i</mi></msub></math> is the number of minutes
    user *i* spends on the site daily, <math><msub><mi>x</mi> <mi>i</mi></msub></math>
    is the number of friends user *i* has, and *ε* is a (hopefully small) error term
    representing the fact that there are other factors not accounted for by this simple
    model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Assuming we’ve determined such an `alpha` and `beta`, then we make predictions
    simply with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'How do we choose `alpha` and `beta`? Well, any choice of `alpha` and `beta`
    gives us a predicted output for each input `x_i`. Since we know the actual output
    `y_i`, we can compute the error for each pair:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: What we’d really like to know is the total error over the entire dataset. But
    we don’t want to just add the errors—if the prediction for `x_1` is too high and
    the prediction for `x_2` is too low, the errors may just cancel out.
  prefs: []
  type: TYPE_NORMAL
- en: 'So instead we add up the *squared* errors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The *least squares solution* is to choose the `alpha` and `beta` that make `sum_of_sqerrors`
    as small as possible.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using calculus (or tedious algebra), the error-minimizing `alpha` and `beta`
    are given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Without going through the exact mathematics, let’s think about why this might
    be a reasonable solution. The choice of `alpha` simply says that when we see the
    average value of the independent variable `x`, we predict the average value of
    the dependent variable `y`.
  prefs: []
  type: TYPE_NORMAL
- en: The choice of `beta` means that when the input value increases by `standard_deviation(x)`,
    the prediction then increases by `correlation(x, y) * standard_deviation(y)`.
    In the case where `x` and `y` are perfectly correlated, a one-standard-deviation
    increase in `x` results in a one-standard-deviation-of-`y` increase in the prediction.
    When they’re perfectly anticorrelated, the increase in `x` results in a *decrease*
    in the prediction. And when the correlation is 0, `beta` is 0, which means that
    changes in `x` don’t affect the prediction at all.
  prefs: []
  type: TYPE_NORMAL
- en: 'As usual, let’s write a quick test for this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Now it’s easy to apply this to the outlierless data from [Chapter 5](ch05.html#statistics):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: This gives values of `alpha` = 22.95 and `beta` = 0.903\. So our model says
    that we expect a user with *n* friends to spend 22.95 + *n* * 0.903 minutes on
    the site each day. That is, we predict that a user with no friends on DataSciencester
    would still spend about 23 minutes a day on the site. And for each additional
    friend, we expect a user to spend almost a minute more on the site each day.
  prefs: []
  type: TYPE_NORMAL
- en: In [Figure 14-1](#simple_linear_regression_image), we plot the prediction line
    to get a sense of how well the model fits the observed data.
  prefs: []
  type: TYPE_NORMAL
- en: '![Simple Linear Regression.](assets/dsf2_1401.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14-1\. Our simple linear model
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Of course, we need a better way to figure out how well we’ve fit the data than
    staring at the graph. A common measure is the *coefficient of determination* (or
    *R-squared*), which measures the fraction of the total variation in the dependent
    variable that is captured by the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Recall that we chose the `alpha` and `beta` that minimized the sum of the squared
    prediction errors. A linear model we could have chosen is “always predict `mean(y)`”
    (corresponding to `alpha` = mean(y) and `beta` = 0), whose sum of squared errors
    exactly equals its total sum of squares. This means an R-squared of 0, which indicates
    a model that (obviously, in this case) performs no better than just predicting
    the mean.
  prefs: []
  type: TYPE_NORMAL
- en: Clearly, the least squares model must be at least as good as that one, which
    means that the sum of the squared errors is *at most* the total sum of squares,
    which means that the R-squared must be at least 0\. And the sum of squared errors
    must be at least 0, which means that the R-squared can be at most 1.
  prefs: []
  type: TYPE_NORMAL
- en: The higher the number, the better our model fits the data. Here we calculate
    an R-squared of 0.329, which tells us that our model is only sort of okay at fitting
    the data, and that clearly there are other factors at play.
  prefs: []
  type: TYPE_NORMAL
- en: Using Gradient Descent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If we write `theta = [alpha, beta]`, we can also solve this using gradient
    descent:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: If you run this you’ll get the same values for `alpha` and `beta` as we did
    using the exact formula.
  prefs: []
  type: TYPE_NORMAL
- en: Maximum Likelihood Estimation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Why choose least squares? One justification involves *maximum likelihood estimation*.
    Imagine that we have a sample of data <math><mrow><msub><mi>v</mi> <mn>1</mn></msub>
    <mo>,</mo> <mo>...</mo> <mo>,</mo> <msub><mi>v</mi> <mi>n</mi></msub></mrow></math>
    that comes from a distribution that depends on some unknown parameter *θ* (theta):'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="p left-parenthesis v 1 comma ellipsis comma v Subscript n Baseline
    vertical-bar theta right-parenthesis" display="block"><mrow><mi>p</mi> <mo>(</mo>
    <msub><mi>v</mi> <mn>1</mn></msub> <mo>,</mo> <mo>...</mo> <mo>,</mo> <msub><mi>v</mi>
    <mi>n</mi></msub> <mo>|</mo> <mi>θ</mi> <mo>)</mo></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'If we didn’t know *θ*, we could turn around and think of this quantity as the
    *likelihood* of *θ* given the sample:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="upper L left-parenthesis theta vertical-bar v 1 comma ellipsis
    comma v Subscript n Baseline right-parenthesis" display="block"><mrow><mi>L</mi>
    <mo>(</mo> <mi>θ</mi> <mo>|</mo> <msub><mi>v</mi> <mn>1</mn></msub> <mo>,</mo>
    <mo>...</mo> <mo>,</mo> <msub><mi>v</mi> <mi>n</mi></msub> <mo>)</mo></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: Under this approach, the most likely *θ* is the value that maximizes this likelihood
    function—that is, the value that makes the observed data the most probable. In
    the case of a continuous distribution, in which we have a probability distribution
    function rather than a probability mass function, we can do the same thing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Back to regression. One assumption that’s often made about the simple regression
    model is that the regression errors are normally distributed with mean 0 and some
    (known) standard deviation *σ*. If that’s the case, then the likelihood based
    on seeing a pair `(x_i, y_i)` is:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="upper L left-parenthesis alpha comma beta vertical-bar x Subscript
    i Baseline comma y Subscript i Baseline comma sigma right-parenthesis equals StartFraction
    1 Over StartRoot 2 pi EndRoot sigma EndFraction exp left-parenthesis minus left-parenthesis
    y Subscript i Baseline minus alpha minus beta x Subscript i Baseline right-parenthesis
    squared slash 2 sigma squared right-parenthesis" display="block"><mrow><mi>L</mi>
    <mrow><mo>(</mo> <mi>α</mi> <mo>,</mo> <mi>β</mi> <mo>|</mo> <msub><mi>x</mi>
    <mi>i</mi></msub> <mo>,</mo> <msub><mi>y</mi> <mi>i</mi></msub> <mo>,</mo> <mi>σ</mi>
    <mo>)</mo></mrow> <mo>=</mo> <mfrac><mn>1</mn> <mrow><msqrt><mrow><mn>2</mn><mi>π</mi></mrow></msqrt><mi>σ</mi></mrow></mfrac>
    <mo form="prefix">exp</mo> <mrow><mo>(</mo> <mo>-</mo> <msup><mrow><mo>(</mo><msub><mi>y</mi>
    <mi>i</mi></msub> <mo>-</mo><mi>α</mi><mo>-</mo><mi>β</mi><msub><mi>x</mi> <mi>i</mi></msub>
    <mo>)</mo></mrow> <mn>2</mn></msup> <mo>/</mo> <mn>2</mn> <msup><mi>σ</mi> <mn>2</mn></msup>
    <mo>)</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: The likelihood based on the entire dataset is the product of the individual
    likelihoods, which is largest precisely when `alpha` and `beta` are chosen to
    minimize the sum of squared errors. That is, in this case (with these assumptions),
    minimizing the sum of squared errors is equivalent to maximizing the likelihood
    of the observed data.
  prefs: []
  type: TYPE_NORMAL
- en: For Further Exploration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Continue reading about multiple regression in [Chapter 15](ch15.html#multiple_regression)!
  prefs: []
  type: TYPE_NORMAL
