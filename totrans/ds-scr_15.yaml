- en: Chapter 14\. Simple Linear Regression
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第14章。简单线性回归
- en: Art, like morality, consists in drawing the line somewhere.
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 艺术，如道德，就在于在某处划出界限。
- en: ''
  id: totrans-2
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: G. K. Chesterton
  id: totrans-3
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: G. K. Chesterton
- en: In [Chapter 5](ch05.html#statistics), we used the `correlation` function to
    measure the strength of the linear relationship between two variables. For most
    applications, knowing that such a linear relationship exists isn’t enough. We’ll
    want to understand the nature of the relationship. This is where we’ll use simple
    linear regression.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第5章](ch05.html#statistics)中，我们使用`correlation`函数来衡量两个变量之间线性关系的强度。对于大多数应用程序，知道存在这样一个线性关系是不够的。我们需要理解关系的本质。这就是我们将使用简单线性回归的地方。
- en: The Model
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型
- en: Recall that we were investigating the relationship between a DataSciencester
    user’s number of friends and the amount of time the user spends on the site each
    day. Let’s assume that you’ve convinced yourself that having more friends *causes*
    people to spend more time on the site, rather than one of the alternative explanations
    we discussed.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，我们正在研究DataSciencester用户的朋友数量和每天在网站上花费的时间之间的关系。假设您已经确信，拥有更多朋友*导致*人们在网站上花费更多时间，而不是我们讨论过的其他解释之一。
- en: The VP of Engagement asks you to build a model describing this relationship.
    Since you found a pretty strong linear relationship, a natural place to start
    is a linear model.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 参与用户参与部长要求您建立描述这种关系的模型。由于您找到了一个相当强的线性关系，线性模型是一个自然的起点。
- en: 'In particular, you hypothesize that there are constants *α* (alpha) and *β*
    (beta) such that:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是，您假设存在常数*α*（alpha）和*β*（beta），使得：
- en: <math alttext="y Subscript i Baseline equals beta x Subscript i Baseline plus
    alpha plus epsilon Subscript i" display="block"><mrow><msub><mi>y</mi> <mi>i</mi></msub>
    <mo>=</mo> <mi>β</mi> <msub><mi>x</mi> <mi>i</mi></msub> <mo>+</mo> <mi>α</mi>
    <mo>+</mo> <msub><mi>ε</mi> <mi>i</mi></msub></mrow></math>
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="y Subscript i Baseline equals beta x Subscript i Baseline plus
    alpha plus epsilon Subscript i" display="block"><mrow><msub><mi>y</mi> <mi>i</mi></msub>
    <mo>=</mo> <mi>β</mi> <msub><mi>x</mi> <mi>i</mi></msub> <mo>+</mo> <mi>α</mi>
    <mo>+</mo> <msub><mi>ε</mi> <mi>i</mi></msub></mrow></math>
- en: where <math><msub><mi>y</mi> <mi>i</mi></msub></math> is the number of minutes
    user *i* spends on the site daily, <math><msub><mi>x</mi> <mi>i</mi></msub></math>
    is the number of friends user *i* has, and *ε* is a (hopefully small) error term
    representing the fact that there are other factors not accounted for by this simple
    model.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 其中<math><msub><mi>y</mi> <mi>i</mi></msub></math>是用户*i*每天在网站上花费的分钟数，<math><msub><mi>x</mi>
    <mi>i</mi></msub></math>是用户*i*的朋友数，*ε*是一个（希望很小的）误差项，表示这个简单模型未考虑到的其他因素。
- en: 'Assuming we’ve determined such an `alpha` and `beta`, then we make predictions
    simply with:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们已经确定了这样的`alpha`和`beta`，那么我们可以简单地进行预测：
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'How do we choose `alpha` and `beta`? Well, any choice of `alpha` and `beta`
    gives us a predicted output for each input `x_i`. Since we know the actual output
    `y_i`, we can compute the error for each pair:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 如何选择`alpha`和`beta`？嗯，任何`alpha`和`beta`的选择都会给我们每个输入`x_i`预测输出。由于我们知道实际输出`y_i`，我们可以计算每对的误差：
- en: '[PRE1]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: What we’d really like to know is the total error over the entire dataset. But
    we don’t want to just add the errors—if the prediction for `x_1` is too high and
    the prediction for `x_2` is too low, the errors may just cancel out.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们真正想知道的是整个数据集的总误差。但我们不只是想把误差加起来——如果`x_1`的预测值过高，而`x_2`的预测值过低，误差可能会抵消掉。
- en: 'So instead we add up the *squared* errors:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们将平方误差加起来：
- en: '[PRE2]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The *least squares solution* is to choose the `alpha` and `beta` that make `sum_of_sqerrors`
    as small as possible.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '*最小二乘解*是选择使`sum_of_sqerrors`尽可能小的`alpha`和`beta`。'
- en: 'Using calculus (or tedious algebra), the error-minimizing `alpha` and `beta`
    are given by:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 使用微积分（或繁琐的代数），最小化误差的`alpha`和`beta`由以下公式给出：
- en: '[PRE3]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Without going through the exact mathematics, let’s think about why this might
    be a reasonable solution. The choice of `alpha` simply says that when we see the
    average value of the independent variable `x`, we predict the average value of
    the dependent variable `y`.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 不需要详细进行数学推导，让我们思考为什么这可能是一个合理的解决方案。选择`alpha`简单地表示，当我们看到自变量`x`的平均值时，我们预测因变量`y`的平均值。
- en: The choice of `beta` means that when the input value increases by `standard_deviation(x)`,
    the prediction then increases by `correlation(x, y) * standard_deviation(y)`.
    In the case where `x` and `y` are perfectly correlated, a one-standard-deviation
    increase in `x` results in a one-standard-deviation-of-`y` increase in the prediction.
    When they’re perfectly anticorrelated, the increase in `x` results in a *decrease*
    in the prediction. And when the correlation is 0, `beta` is 0, which means that
    changes in `x` don’t affect the prediction at all.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 选择`beta`的意义在于，当输入值增加了`standard_deviation(x)`时，预测值就会增加`correlation(x, y) * standard_deviation(y)`。如果`x`和`y`完全正相关，`x`增加一个标准差会导致预测值增加一个`y`的标准差。当它们完全负相关时，`x`的增加会导致预测值的*减少*。当相关性为0时，`beta`为0，这意味着`x`的变化对预测没有任何影响。
- en: 'As usual, let’s write a quick test for this:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，我们来快速测试一下：
- en: '[PRE4]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now it’s easy to apply this to the outlierless data from [Chapter 5](ch05.html#statistics):'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 现在很容易将其应用于[第5章](ch05.html#statistics)中去除异常值的数据：
- en: '[PRE5]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This gives values of `alpha` = 22.95 and `beta` = 0.903\. So our model says
    that we expect a user with *n* friends to spend 22.95 + *n* * 0.903 minutes on
    the site each day. That is, we predict that a user with no friends on DataSciencester
    would still spend about 23 minutes a day on the site. And for each additional
    friend, we expect a user to spend almost a minute more on the site each day.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这给出了`alpha` = 22.95和`beta` = 0.903的值。因此，我们的模型表明，我们预计一个没有朋友的用户每天在DataSciencester上花费大约23分钟。对于每个额外的朋友，我们预计用户每天在网站上多花大约一分钟。
- en: In [Figure 14-1](#simple_linear_regression_image), we plot the prediction line
    to get a sense of how well the model fits the observed data.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图14-1](#simple_linear_regression_image)中，我们绘制预测线，以了解模型拟合观察数据的程度。
- en: '![Simple Linear Regression.](assets/dsf2_1401.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![简单线性回归。](assets/dsf2_1401.png)'
- en: Figure 14-1\. Our simple linear model
  id: totrans-30
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图14-1\. 我们的简单线性模型
- en: 'Of course, we need a better way to figure out how well we’ve fit the data than
    staring at the graph. A common measure is the *coefficient of determination* (or
    *R-squared*), which measures the fraction of the total variation in the dependent
    variable that is captured by the model:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我们需要一种比盯着图表更好的方法来确定我们对数据的拟合程度。一个常见的度量是*决定系数*（或*R平方*），它衡量因变量的总变异中模型所捕获的比例：
- en: '[PRE6]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Recall that we chose the `alpha` and `beta` that minimized the sum of the squared
    prediction errors. A linear model we could have chosen is “always predict `mean(y)`”
    (corresponding to `alpha` = mean(y) and `beta` = 0), whose sum of squared errors
    exactly equals its total sum of squares. This means an R-squared of 0, which indicates
    a model that (obviously, in this case) performs no better than just predicting
    the mean.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，我们选择了使预测误差平方和最小化的`alpha`和`beta`。我们可以选择一个线性模型“始终预测`mean(y)`”（对应于`alpha` =
    mean(y)和`beta` = 0），其预测误差平方和恰好等于总平方和。这意味着R平方为0，表明该模型（在这种情况下显然）的表现不比简单预测均值好。
- en: Clearly, the least squares model must be at least as good as that one, which
    means that the sum of the squared errors is *at most* the total sum of squares,
    which means that the R-squared must be at least 0\. And the sum of squared errors
    must be at least 0, which means that the R-squared can be at most 1.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，最小二乘模型至少要与此一样好，这意味着预测误差平方和最多等于总平方和，这意味着R平方至少为0。而预测误差平方和至少为0，这意味着R平方最多为1。
- en: The higher the number, the better our model fits the data. Here we calculate
    an R-squared of 0.329, which tells us that our model is only sort of okay at fitting
    the data, and that clearly there are other factors at play.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 数字越高，我们的模型拟合数据越好。这里我们计算了一个R平方为0.329，表明我们的模型在拟合数据方面只算是可以接受，显然还有其他因素在起作用。
- en: Using Gradient Descent
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用梯度下降法
- en: 'If we write `theta = [alpha, beta]`, we can also solve this using gradient
    descent:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们写`theta = [alpha, beta]`，我们也可以用梯度下降法解决这个问题：
- en: '[PRE7]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: If you run this you’ll get the same values for `alpha` and `beta` as we did
    using the exact formula.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您运行此操作，您将得到与我们使用精确公式相同的`alpha`和`beta`值。
- en: Maximum Likelihood Estimation
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最大似然估计
- en: 'Why choose least squares? One justification involves *maximum likelihood estimation*.
    Imagine that we have a sample of data <math><mrow><msub><mi>v</mi> <mn>1</mn></msub>
    <mo>,</mo> <mo>...</mo> <mo>,</mo> <msub><mi>v</mi> <mi>n</mi></msub></mrow></math>
    that comes from a distribution that depends on some unknown parameter *θ* (theta):'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么选择最小二乘法？其中一个理由涉及*最大似然估计*。想象一下，我们有来自依赖于某些未知参数*θ*（theta）的分布的数据样本<math><mrow><msub><mi>v</mi>
    <mn>1</mn></msub> <mo>,</mo> <mo>...</mo> <mo>,</mo> <msub><mi>v</mi> <mi>n</mi></msub></mrow></math>：
- en: <math alttext="p left-parenthesis v 1 comma ellipsis comma v Subscript n Baseline
    vertical-bar theta right-parenthesis" display="block"><mrow><mi>p</mi> <mo>(</mo>
    <msub><mi>v</mi> <mn>1</mn></msub> <mo>,</mo> <mo>...</mo> <mo>,</mo> <msub><mi>v</mi>
    <mi>n</mi></msub> <mo>|</mo> <mi>θ</mi> <mo>)</mo></mrow></math>
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="p left-parenthesis v 1 comma ellipsis comma v Subscript n Baseline
    vertical-bar theta right-parenthesis" display="block"><mrow><mi>p</mi> <mo>(</mo>
    <msub><mi>v</mi> <mn>1</mn></msub> <mo>,</mo> <mo>...</mo> <mo>,</mo> <msub><mi>v</mi>
    <mi>n</mi></msub> <mo>|</mo> <mi>θ</mi> <mo>)</mo></mrow></math>
- en: 'If we didn’t know *θ*, we could turn around and think of this quantity as the
    *likelihood* of *θ* given the sample:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们不知道*θ*，我们可以反过来将这个数量视为给定样本的*θ*的*似然*：
- en: <math alttext="upper L left-parenthesis theta vertical-bar v 1 comma ellipsis
    comma v Subscript n Baseline right-parenthesis" display="block"><mrow><mi>L</mi>
    <mo>(</mo> <mi>θ</mi> <mo>|</mo> <msub><mi>v</mi> <mn>1</mn></msub> <mo>,</mo>
    <mo>...</mo> <mo>,</mo> <msub><mi>v</mi> <mi>n</mi></msub> <mo>)</mo></mrow></math>
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper L left-parenthesis theta vertical-bar v 1 comma ellipsis
    comma v Subscript n Baseline right-parenthesis" display="block"><mrow><mi>L</mi>
    <mo>(</mo> <mi>θ</mi> <mo>|</mo> <msub><mi>v</mi> <mn>1</mn></msub> <mo>,</mo>
    <mo>...</mo> <mo>,</mo> <msub><mi>v</mi> <mi>n</mi></msub> <mo>)</mo></mrow></math>
- en: Under this approach, the most likely *θ* is the value that maximizes this likelihood
    function—that is, the value that makes the observed data the most probable. In
    the case of a continuous distribution, in which we have a probability distribution
    function rather than a probability mass function, we can do the same thing.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种方法下，最可能的*θ*是能够最大化这个似然函数的值——即使得观察数据最有可能出现的值。对于连续分布的情况，我们有一个概率分布函数而不是概率质量函数，我们也可以做同样的事情。
- en: 'Back to regression. One assumption that’s often made about the simple regression
    model is that the regression errors are normally distributed with mean 0 and some
    (known) standard deviation *σ*. If that’s the case, then the likelihood based
    on seeing a pair `(x_i, y_i)` is:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 回到回归。关于简单回归模型经常做的一个假设是回归误差服从均值为0、某个（已知）标准差*σ*的正态分布。如果是这种情况，那么基于观察到一对`(x_i, y_i)`的似然是：
- en: <math alttext="upper L left-parenthesis alpha comma beta vertical-bar x Subscript
    i Baseline comma y Subscript i Baseline comma sigma right-parenthesis equals StartFraction
    1 Over StartRoot 2 pi EndRoot sigma EndFraction exp left-parenthesis minus left-parenthesis
    y Subscript i Baseline minus alpha minus beta x Subscript i Baseline right-parenthesis
    squared slash 2 sigma squared right-parenthesis" display="block"><mrow><mi>L</mi>
    <mrow><mo>(</mo> <mi>α</mi> <mo>,</mo> <mi>β</mi> <mo>|</mo> <msub><mi>x</mi>
    <mi>i</mi></msub> <mo>,</mo> <msub><mi>y</mi> <mi>i</mi></msub> <mo>,</mo> <mi>σ</mi>
    <mo>)</mo></mrow> <mo>=</mo> <mfrac><mn>1</mn> <mrow><msqrt><mrow><mn>2</mn><mi>π</mi></mrow></msqrt><mi>σ</mi></mrow></mfrac>
    <mo form="prefix">exp</mo> <mrow><mo>(</mo> <mo>-</mo> <msup><mrow><mo>(</mo><msub><mi>y</mi>
    <mi>i</mi></msub> <mo>-</mo><mi>α</mi><mo>-</mo><mi>β</mi><msub><mi>x</mi> <mi>i</mi></msub>
    <mo>)</mo></mrow> <mn>2</mn></msup> <mo>/</mo> <mn>2</mn> <msup><mi>σ</mi> <mn>2</mn></msup>
    <mo>)</mo></mrow></mrow></math>
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper L left-parenthesis alpha comma beta vertical-bar x Subscript
    i Baseline comma y Subscript i Baseline comma sigma right-parenthesis equals StartFraction
    1 Over StartRoot 2 pi EndRoot sigma EndFraction exp left-parenthesis minus left-parenthesis
    y Subscript i Baseline minus alpha minus beta x Subscript i Baseline right-parenthesis
    squared slash 2 sigma squared right-parenthesis" display="block"><mrow><mi>L</mi>
    <mrow><mo>(</mo> <mi>α</mi> <mo>,</mo> <mi>β</mi> <mo>|</mo> <msub><mi>x</mi>
    <mi>i</mi></msub> <mo>,</mo> <msub><mi>y</mi> <mi>i</mi></msub> <mo>,</mo> <mi>σ</mi>
    <mo>)</mo></mrow> <mo>=</mo> <mfrac><mn>1</mn> <mrow><msqrt><mrow><mn>2</mn><mi>π</mi></mrow></msqrt><mi>σ</mi></mrow></mfrac>
    <mo form="prefix">exp</mo> <mrow><mo>(</mo> <mo>-</mo> <msup><mrow><mo>(</mo><msub><mi>y</mi>
    <mi>i</mi></msub> <mo>-</mo><mi>α</mi><mo>-</mo><mi>β</mi><msub><mi>x</mi> <mi>i</mi></msub>
    <mo>)</mo></mrow> <mn>2</mn></msup> <mo>/</mo> <mn>2</mn> <msup><mi>σ</mi> <mn>2</mn></msup>
    <mo>)</mo></mrow></mrow></math>
- en: The likelihood based on the entire dataset is the product of the individual
    likelihoods, which is largest precisely when `alpha` and `beta` are chosen to
    minimize the sum of squared errors. That is, in this case (with these assumptions),
    minimizing the sum of squared errors is equivalent to maximizing the likelihood
    of the observed data.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 基于整个数据集的似然是每个个体似然的乘积，在`alpha`和`beta`被选择以最小化平方误差时最大。也就是说，在这种情况下（在这些假设下），最小化平方误差等价于最大化观察数据的似然。
- en: For Further Exploration
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步探索
- en: Continue reading about multiple regression in [Chapter 15](ch15.html#multiple_regression)!
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 继续阅读关于多元回归的内容在[第15章](ch15.html#multiple_regression)！
