- en: Chapter 4\. Data Format Context
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Boyan Angelov
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter we’ll review tools in Python and R for importing and processing
    data in a variety of formats. We’ll cover a selection of packages, compare and
    contrast them, and highlight the properties that make them effective. At the end
    of this tour, you’ll be able to select packages with confidence. Each section
    illustrates the tool’s capabilities with a specific mini-case study, based on
    tasks that a data scientist encounters daily. If you’re transitioning your work
    from one language to another, or simply want to find out how to get started quickly
    using complete, well-maintained and context-specific packages this chapter will
    guide you.
  prefs: []
  type: TYPE_NORMAL
- en: Before we dive in, remember that the open-source ecosystem is constantly changing.
    New developments, such as [transformer models](https://www.tensorflow.org/tutorials/text/transformer)
    and [xAI](https://robotics.sciencemag.org/content/4/37/eaay7120), seem to emerge
    every other week. These often aim at lowering the learning curve and increasing
    developer productivity. This explosion of diversity also applies to related packages,
    resulting in a nearly constant flow of new and (hopefully) better tools. If you
    have a very specific problem, there’s probably a package already available for
    you, so you don’t have to reinvent the wheel. Tool selection can be overwhelming,
    but at the same time this variety of options can improve the quality and speed
    of your data science work.
  prefs: []
  type: TYPE_NORMAL
- en: The package selection in this chapter can appear limited in view, hence it is
    essential to clarify our selection criteria. So what qualities should we look
    for in a good tool?
  prefs: []
  type: TYPE_NORMAL
- en: 'It should be **open source**: there is a large number of valuable commercial
    tools available, but we firmly believe that open source tools have a great advantage.
    They tend to be easier to extend and understand what their inner workings are,
    and are more popular.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'It should be **feature-complete**: the package should include a comprehensive
    set of functions that help the user do their fundamental work without resorting
    to other tools.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'It should be **well-maintained**: one of the drawbacks of using Open Source
    Software (OSS) is that sometimes packages have a short lifecycle, and their maintenance
    is abandoned (so called “abandonware”). We want to use packages which are actively
    worked on, so we can feel confident they are up-to-date.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s begin with a definition. What is a “data format”? There are [several answers](https://en.wikipedia.org/wiki/Data_format)
    available. Possible candidates are *data type*, *recording format* and *file format*.
    *Data type* is related to data stored in databases or types in programming languages
    (for example integer, float or string). The *recording format* is how data is
    stored in a physical medium, such as CD or DVD. And finally, what we’re after,
    the *file format*, i.e. how information is *prepared for a computing purpose*.
  prefs: []
  type: TYPE_NORMAL
- en: 'With that definition in hand, one might still wonder why should we dedicate
    an entire chapter to focus just on file formats? You have probably been exposed
    to them in another context, such as saving a PowerPoint slide deck with a `.ppt`
    or `.pptx` extension (and wondering which one is better). The problem here goes
    much further beyond basic tool compatibility. The way information is stored influences
    the complete downstream data science process. For example, if our end goal is
    to perform advanced analytics and the information is stored in a text format,
    we have to pay attention to factors such as character encoding (a notorious problem,
    especially for Python^([1](ch04.xhtml#idm45127452048648))). For such data to be
    effectively processed, it also needs to go through several steps^([2](ch04.xhtml#idm45127452046808)),
    such as [tokenization](https://en.wikipedia.org/wiki/Tokenization) and [stop word](https://en.wikipedia.org/wiki/Stop_word)
    removal. Those same steps are not applicable to image data, even though we may
    have the same end goal in mind, e.g. classification. In that case other processing
    techniques are more suitable, such as resizing and scaling. These differences
    in data processing pipelines are shown on [???](#pipelines_diff). To summarize:
    the data format is the most significant factor influencing what you can, and cannot
    do with it.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We now use the word “pipeline” for the first time in this context, so let’s
    use the opportunity to define it. You have probably heard the expression that
    “data is the new oil”. This expression goes beyond a simple marketing strategy
    and represents a useful way to think about data. There are surprisingly many parallels
    between how oil and data are processed. You can imagine that the initial data
    that the business collects is the rawest form - probably of limited use initially.
    It then undergoes a sequence of steps, called data processing, before it’s used
    in some application (i.e. for training an ML model or feeding a dashboard). In
    oil processing this would be called refinement and enrichment - making the data
    usable for a business purpose. Pipelines transport the different oil types (raw,
    refined) through the system to its final state. The same term can be used in data
    science and engineering to describe the infrastructure and technology required
    to process and deliver data.
  prefs: []
  type: TYPE_NORMAL
- en: difference between common data format pipelines. The green color indicates the
    shared steps between the workflows. image::img/pipelines_diff.jpg[""]
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Infrastructure and performance also need to be taken into consideration when
    working with a specific data format. For example, with image data, you’ll need
    more storage availability. For time-series data you might need to use a particular
    database, such as [Influx DB](https://www.influxdata.com/products/influxdb-overview/).
    And finally, in terms of performance, image classification is often solved using
    deep learning methods based on Convolutional Neural Networks (CNNs) which may
    require a Graphics Processing Unit (GPU). Without it, model training can be very
    slow and become a bottleneck both for your development work and a potential production
    deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we covered the reasons to carefully consider which packages to use,
    we’ll have a look at the possible data formats. This overview is presented in
    [Table 4-1](#data-format-zoo-table) (note that those tools are mainly designed
    for small to medium size datasets). Admittedly, we are just scratching the surface
    on what’s out there, and there are a few notable omissions (such as audio and
    video). Here, we’ll focusing on the most widely used formats.
  prefs: []
  type: TYPE_NORMAL
- en: Table 4-1\. An overview of data formats and popular Python and R packages used
    to process data stored in them.
  prefs: []
  type: TYPE_NORMAL
- en: '| Data type | Python package | R package |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Tabular | `pandas` | `readr`, `rio` |'
  prefs: []
  type: TYPE_TB
- en: '| Image | `open-cv`, `scikit-image`, `PIL` | `magickr`, `imager`, `EBImage`
    |'
  prefs: []
  type: TYPE_TB
- en: '| Text | `nltk`, `spaCy` | `tidytext`, `stringr` |'
  prefs: []
  type: TYPE_TB
- en: '| Time series | `prophet`, `sktime` | `prophet`, `ts`, `zoo` |'
  prefs: []
  type: TYPE_TB
- en: '| Spatial | `gdal`, `geopandas`, `pysal` | `rgdal`, `sp`, `sf`, `raster` |'
  prefs: []
  type: TYPE_TB
- en: This table is by no means exhaustive, and we are certain new tools will appear
    soon, but those are the workhorses fulfilling our selection criteria. Let’s get
    them to work in the following sections, and see which ones are the best for the
    job!
  prefs: []
  type: TYPE_NORMAL
- en: External versus base packages
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 2](ch02.xhtml#ch03) and [Chapter 3](ch03.xhtml#ch04), we introduced
    packages very early in the learning process. In Python we used `pandas` right
    at the outset and we also transitioned to the `tidyverse` in R relatively quickly.
    This allowed us to be productive much faster than if we went down the rabbit holes
    of archaic language features that you’re unlikely to need as a beginner^([3](ch04.xhtml#idm45127452007560)).
    A programming language’s utility is defined by the availability and quality of
    it’s third-party packages, as opposed to the core features of the language itself.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This is not to say that the aren’t a lot of things that you can accomplish with
    just base R (as you’ll see in some of the upcoming examples), but taking advantage
    of the open-source ecosystem is a fundamental skill to increase your productivity
    and avoid reinventing the wheel.
  prefs: []
  type: TYPE_NORMAL
- en: Go back and learn the basics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There is a danger in overusing third-party packages, and you have to be aware
    of when the right time to go back to the basics is. Otherwise you might fall victim
    to a false sense of security, and become reliant on the training wheels provided
    by tools such as `pandas`. This might lead to difficulties when dealing with more
    specific real-world challenges.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s now see this package vs. base language concept plays out in practice
    by going into detail with a topic we’re already familiar with: tabular data^([4](ch04.xhtml#idm45127452002248)).
    There are at least two ways to do this in Python. First, using `pandas`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Second, with the built-in `csv` module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_data_format_context_CO1-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Note how you need to specify the [file mode](https://docs.python.org/3/tutorial/inputoutput.html#reading-and-writing-files),
    in this case `"r"` (standing for “read”). This is to make sure the file is not
    overwritten by accident, hinting at a more general-purpose oriented language.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_data_format_context_CO1-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Using a loop to read a file might seem strange to a beginner, but it’s making
    the process explicit.
  prefs: []
  type: TYPE_NORMAL
- en: This example tells us that `pd.read_csv()` in `pandas` provides a more concise,
    convenient and intuitive way to import data. It is also less explicit than vanilla
    Python and not essential. `pd.read_csv()` is, in essence, a *convenience wrapper*
    of existing functionality — good for us!
  prefs: []
  type: TYPE_NORMAL
- en: Here we see that packages serve two functions. First, as we have come to expect,
    they provide *new* functionality. Second, they are also convenience wrappers for
    existing standard functions, which make our lives easier.
  prefs: []
  type: TYPE_NORMAL
- en: This is brilliantly demonstrated in R’s `rio` package^([5](ch04.xhtml#idm45127451851368)).
    `rio` stands for “R input/output” and it does just was it says^([6](ch04.xhtml#idm45127451849368)).
    Here, the single function `import()` uses the file’s filename extension to select
    the best function in a collection of packages for importing. This works on Excel,
    SPSS, stata, SAS or many other formats commonly seen.
  prefs: []
  type: TYPE_NORMAL
- en: Another R tidyverse package, `vroom` allows for fast import of tabular data,
    and can read an entire directory of files in one command, with the use of `map()`
    functions or `for loops`.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the `data.table` package, which is often neglected at the expense of
    promoting the tidyverse, provides the exceptional `fread()` which can import very
    large files at a fraction of what base R or `readr` offer.
  prefs: []
  type: TYPE_NORMAL
- en: The usefulness of learning how to use a third-party packages becomes more apparent
    when we try to perform more complex tasks, as we’ll see next when processing other
    data formats.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we can appreciate the advantages of packages, we’ll demonstrate some
    of their capabilities. For this we’ll work on several different real-world use
    cases, listed in [Table 4-2](#case-study-table). We won’t focus on minute implementation
    details, but instead cover the elements that expose their benefits (and shortcomings)
    for the tasks at hand. Since the focus in this chapter is on data formats, and
    [Chapter 5](ch05.xhtml#ch06) is all about workflows, all these case studies are
    about data processing (as illustrated previously in [???](#pipelines_diff)).
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: For pedagogic purposes we have omitted parts of the code. If you’d like to follow
    along, executable code is available in the [book repository](https://github.com/moderndatadesign/PyR4MDS).
  prefs: []
  type: TYPE_NORMAL
- en: Table 4-2\. An overview of the different use-cases
  prefs: []
  type: TYPE_NORMAL
- en: '| Data format | Use case |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| image | [Swimming pool and car Detection](https://www.kaggle.com/kbhartiya83/swimming-pool-and-car-detection)
    |'
  prefs: []
  type: TYPE_TB
- en: '| text | [Amazon Music reviews processing](http://jmcauley.ucsd.edu/data/amazon/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| time series | [Daily Australian Temperatures](https://raw.githubusercontent.com/jbrownlee/Datasets/master/daily-min-temperatures.csv)
    |'
  prefs: []
  type: TYPE_TB
- en: '| spatial | [*Loxodonta africana* species distribution data](https://www.gbif.org/)
    |'
  prefs: []
  type: TYPE_TB
- en: Further information on how to download and process these data is available in
    the official [repository](https://github.com/moderndatadesign/PyR4MDS) for the
    book.
  prefs: []
  type: TYPE_NORMAL
- en: Image data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Images pose a unique set of challenges for data scientists. We’ll demonstrate
    the optimal methodology by covering the challenge of aerial image processing -
    a domain of growing importance in agriculture, biodiversity conservation, urban
    planning and climate change research. Our mini use-case uses data from Kaggle,
    collected to help the detection of swimming pools and cars. For more information
    on the dataset, you can use the URL in [Table 4-2](#case-study-table).
  prefs: []
  type: TYPE_NORMAL
- en: OpenCV and scikit-image
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we mentioned at the beginning of the chapter, downstream purpose influences
    data processing heavily. Since aerial data is often used to train machine learning
    algorithms, our focus will be on preparatory tasks.
  prefs: []
  type: TYPE_NORMAL
- en: The [OpenCV](https://opencv.org/) package is one of the most common ways to
    work with image data in Python. It contains all the necessary tools for image
    loading, manipulation and storage. The “CV” in the name stands for Computer Vision
    - the field of machine learning that focuses on image data. Another handy tool
    that we’ll use is `scikit-image`. As its naming suggests, it’s very much related
    to [scikit-learn](https://scikit-learn.org/stable/)^([7](ch04.xhtml#idm45127451817912)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the steps of our task (refer to [Table 4-2](#case-study-table)):'
  prefs: []
  type: TYPE_NORMAL
- en: Resize the image to a specific size
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Convert the image to black and white
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Augment the data by rotating the image
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For an ML algorithm to learn successfully from data, the input has to be cleaned
    (data munging), standardized (i.e., scaling) and filtered (feature engineering)^([8](ch04.xhtml#idm45127451810536)).
    You can imagine gathering a dataset of images (for example, by scraping^([9](ch04.xhtml#idm45127451809800))
    data from Google Images). They will differ in some way or another - such as size
    and/or color. Steps 1 and 2 in our task list help us deal with that. Step 3 is
    handy for ML applications. The performance (i.e., classification accuracy, or
    Area Under the Curve(AUC)) of ML algorithms depends mostly on the amount of training
    data, which is often in little supply. To get around this, without resorting to
    obtaining more data^([10](ch04.xhtml#idm45127451808600)), data scientists have
    discovered that playing around with the data already available, such as rotating
    and cropping, can introduce new data points. Those can then be used to train the
    model again and improve performance. This process is formally known as data augmentation^([11](ch04.xhtml#idm45127451807656)).
  prefs: []
  type: TYPE_NORMAL
- en: Enough talk - let’s start by importing the data! Remember, if you want to follow
    along, check the complete code at the book’s [repository](https://github.com/moderndatadesign/PyR4MDS).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_data_format_context_CO2-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Using `cv2` might seem confusing since the package is named `OpenCV.` `cv2`
    is used as a short-hand name. The same naming pattern is used for `scikit-image`,
    where the import statement is shortened to `skimage`.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/prds_0401.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-1\. Raw image plot in Python with `matplotlib`.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'So in what object type did `cv2` store the data? We can check with `type`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Here we can observe an important feature that already provides advantages to
    using Python for CV tasks as opposed to R. The image is directly stored as a `numpy`
    multidimensional array (`nd` stands for n-dimensions), making it accessible to
    a variety of other tools available in the wider Python ecosystem. Because this
    is built on the `pyData` stack, it’s well-supported. Is this true for R? Let’s
    have a look at the `magick` package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The `magick-image` class is only accessible to functions from the `magick` package,
    or other closely related tools, but not the powerful base R methods (such as the
    ones shown in [Chapter 2](ch02.xhtml#ch03), with the notable exception of `plot()`).
    Those different approaches in how various open source packages support each other
    is illustrated in [Figure 4-2](#package_design), and is a common thread throughout
    the examples in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: There is at least one exception to this rule - the `EBImage` package, a part
    of [Bioconductor](https://bioconductor.org/). By using it you can get access to
    the image in its raw array form, and then use other tools on top of that. The
    drawback here is that it’s a part of a domain-specific package, and it might not
    be easy to see how it works in a standard CV pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/prds_0402.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-2\. The two types of package design hierarchies as they are used during
    a data lifecycle (bottom to top). The left pattern shows a suboptimal structure,
    where users are forced to adopt and use purpose-specific tools at the first level
    which limits their flexibility and productivity. The pattern on the right shows
    a better structure, where there are standard tools for the initial phases of the
    data lineage, enabling a variety of tools downstream.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note that in the previous step (where we loaded the raw image in Python), we
    also used one of the most popular plotting tool - `matplotlib` (data visualization
    is covered in [Chapter 5](ch05.xhtml#ch06)), so we again took advantage of this
    better design pattern.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we know that the image data is stored as a `numpy` `ndarray`, we can
    use `numpy`’s methods. What’s the size of the image? For this we can try the `.shape`
    method of `ndarray`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'It worked indeed! The first two output values correspond to the image `height`
    and `width` respectively, and the third one to the number of channels in the image
    - three in this case ((r)ed, (g)reen and (b)lue). Now let’s continue and deliver
    on the first standardization step - image resizing. Here we’ll use `cv2` for the
    first time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If you gain experience working with such fundamental tools in both languages,
    you’ll be able to test your ideas quickly, even without knowing whether those
    methods exist. If the tools you use are designed well (as in the better design
    in [Figure 4-2](#package_design)), often they will work as expected!
  prefs: []
  type: TYPE_NORMAL
- en: 'Perfect, it worked like a charm! The next step is to convert the image to black
    and white. For this, we’ll also use `cv2`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The colors are greenish and not grey. This default option chooses a color scheme
    that makes the contrast more easily discernible for a human eye than black and
    white. When you look at the shape of the `numpy` `ndarray` you can see that the
    channel number has disappeared - there is just one now. Now let’s complete our
    task and do a simple data augmentation step and flip the image horizontally. Here
    we’re again taking advantage that the data is stored as a `numpy` array. We’ll
    use a function directly from `numpy`, without relying on the other CV libraries
    (`OpenCV` or `scikit-image`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The results are shown on [Figure 4-3](#flipped_image).
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/prds_0403.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-3\. Plot of an image flipped by using `numpy` functions.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'We can use `scikit-image` for further image manipulation tasks such as rotation,
    and even this different package will work as expected on our data format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The data standardization and augmentation steps we went through illustrate
    how the less complex package design ([Figure 4-2](#package_design)) makes us more
    productive. We can drive the point home by showing a negative example for the
    third step, this time in R. For that, we’ll have to rely on the `adimpro` package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Whenever we load yet another package, we are decreasing the quality, readability,
    and reusability of our code. This issue is primarily due to possible unknown bugs,
    a steeper learning curve, or a potential lack of consistent and thorough documentation
    for that third-party package. A quick check on the status of `adimpro` on [CRAN](https://cran.r-project.org/web/packages/adimpro/index.html)
    reveals that the last time it was updated was in November 2019^([12](ch04.xhtml#idm45127451457144)).
    This is why using tools such as `OpenCV`, which work on image data by taking advantage
    of the `PyData` stack^([13](ch04.xhtml#idm45127451455800)), such as `numpy` is
    preferred.
  prefs: []
  type: TYPE_NORMAL
- en: A less complex, modular, and abstract enough package design goes a long way
    to make data scientists productive and happy in using their tools. They are then
    free to focus on actual work and not dealing with complex documentation or a multitude
    of abandonware packages. These considerations make Python the clear winner in
    importing and processing image data, but is this the case for the other formats?
  prefs: []
  type: TYPE_NORMAL
- en: Text data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The analysis of text data is often used interchangeably with the term Natural
    Language Processing (NLP). This, in turn, is a subfield of ML. Hence it’s not
    surprising to see that Python-based tools also dominate it. The inherently compute-intensive
    nature of working with text data is one good reason to why that’s the case. Another
    one is that dealing with larger datasets can be a more significant challenge in
    R^([14](ch04.xhtml#idm45127451424392)) then in Python (this topic is covered further
    in [Chapter 5](ch05.xhtml#ch06)). And it is a Big Data problem. The amount of
    text data has proliferated in recent years with the rise of services on the internet
    and social media giants such as Twitter and Facebook. Such organizations have
    also invested heavily in the technology and related open-source, due to the fact
    that a large chunk of data available to them is in text format.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly to the image data case, we’ll start by designing a standard NLP task.
    It should contain the most fundamental elements of an NLP pipeline. For a dataset,
    we selected texts from the Amazon Product Reviews Dataset ([Table 4-2](#case-study-table)),
    and we have to prepare it for an advanced analytics use case, such as text classification,
    sentiment analysis, or topic modeling. The steps needed for completion are the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: Tokenize the data
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Remove stop-words
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Tag the Parts of Speech (PoS)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We’ll also go through more advanced methods (such as word embeddings) in `spaCy`
    to demonstrate what the Python packages are capable of, and at the same time,
    provide a few R examples for comparison.
  prefs: []
  type: TYPE_NORMAL
- en: NLTK and spaCy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So what are the most common tools in Python? The most popular one is often referred
    to as the swiss-army knife of NLP - the Natural Language Toolkit (NLTK)^([15](ch04.xhtml#idm45127451414392)).
    It contains a good selection of tools covering the whole pipeline. It also has
    excellent documentation and a relatively low learning curve for its API.
  prefs: []
  type: TYPE_NORMAL
- en: NLTK Book
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The NLTK authors have also written one of the most accessible books on working
    with text data - the NLTK Book, currently in version 3\. It’s available to read
    online for free [on the official website](https://www.nltk.org/book/). It can
    serve as an excellent reference manual, so if you want to dive deeper into some
    of the topics we cover in this section, go ahead and have a look!
  prefs: []
  type: TYPE_NORMAL
- en: 'As a data scientist, one of the first steps in a project is to look at the
    raw data. Here’s one example review, along with its data type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: This here is important - the data is stored in a fundamental data type in Python
    - `str` (string). Similar to the image data being stored as a multidimensional
    `numpy` array, many other tools can have access to it. For example, suppose we
    were to use a tool that efficiently searches and replaces parts of a string, such
    as [flashtext](https://github.com/vi3k6i5/flashtext). In that case, we’d be able
    to use it here without formatting issues, and the need to coerce^([16](ch04.xhtml#idm45127451365400))
    the data type.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we can take the first step in our mini case study - *tokenization*. It
    will split the reviews into components, such as words or sentences:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Easy enough! For illustration purposes, would it be that hard to attempt this
    relatively simple task in R, with some functions from `tidytext`?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: This is one of the most well-documented methods to use. One issue with this
    is that it relies heavily on the “tidy data” concept, and also on the pipeline
    chaining concept from `dplyr` (we covered both in [Chapter 2](ch02.xhtml#ch03)).
    These concepts are specific to R, and to use `tidytext` successfully, you would
    have to learn them first, instead of directly jumping to processing your data.
    The second issue is the output of this procedure - a new `data.frame` containing
    the data in a processed column. While this might be what we need in the end, this
    skips a few intermediate steps and is several layers of abstraction higher than
    what we did with `nltk`. Lowering this abstraction and working in a more modular
    fashion (such as processing a single text field first) adheres to software development
    best practices, such as DRY (“Do not repeat yourself”) and separation of concerns.
  prefs: []
  type: TYPE_NORMAL
- en: The second step of our small NLP data processing pipeline is removing stop words^([17](ch04.xhtml#idm45127451276088)).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'This code suffers from the same issues, along with a new confusing function
    - `anti_join`. Let’s compare to the simple list comprehension (more information
    on this in [Chapter 3](ch03.xhtml#ch04)) step in `nltk`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '`english_stop_words` is just a `list,` and then the only thing we do is loop
    through every word in another `list` (`words`) and remove it *if* it’s present
    in both. This is easier to understand. There’s no relying on advanced concepts
    or functions that are not directly related. This code is also at the right level
    of abstraction. Small code chunks can be used more flexibly as parts of a larger
    text processing pipeline function. A similar “meta” processing function in R can
    become bloated - slow to execute and hard to read.'
  prefs: []
  type: TYPE_NORMAL
- en: While `nltk` allows for such fundamental tasks, we’ll now have a look at a more
    advanced package - `spaCy`. We’ll use this for the third and final step in our
    case study - Part of Speech (PoS) tagging^([18](ch04.xhtml#idm45127451178120)).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_data_format_context_CO3-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Here we are loading all the advanced functionality we need through one function.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_data_format_context_CO3-2)'
  prefs: []
  type: TYPE_NORMAL
- en: 'We take one example review and feed it to a `spaCy` model, resulting in the
    `spacy.tokens.doc.Doc` type, not a `str`. This object can then be used for all
    kinds of other operations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The data is already tokenized on loading. Not only that - all the PoS tags are
    marked already!
  prefs: []
  type: TYPE_NORMAL
- en: 'The data processing steps that we covered are relatively basic. How about some
    newer and more advanced NLP methods? We can take word embeddings for example.
    This is one of the more advanced text vectorization^([19](ch04.xhtml#idm45127451081640))
    methods, where each vector represents the meaning of a word based on its context.
    For that, we can already use the same `nlp` object from the `spaCy` code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: It’s a welcome surprise to see that those abilities are already built-in into
    one of the most popular Python NLP packages. On this level of NLP methods, we
    can see that there’s almost no alternative available in R (or even other languages
    for that matter). Many analogous solutions in R rely on wrapper code around a
    Python backend (which can defeat the purpose of using the R language)^([20](ch04.xhtml#idm45127451027384)).
    This pattern is often seen in the book, especially in [Chapter 5](ch05.xhtml#ch06).
    The same is also true for some other advanced methods such as transformer models^([21](ch04.xhtml#idm45127451025304)).
  prefs: []
  type: TYPE_NORMAL
- en: For round two Python is again the winner. The capabilities of `nltk`, `spaCy`
    and other associated packages make it an excellent choice for NLP work!
  prefs: []
  type: TYPE_NORMAL
- en: Time series data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The time-series format is used to store any data with an associated temporal
    dimension. It could be as simple as shampoo sales from a local grocery store,
    with a timestamp, or millions of data points from a sensor network measuring humidity
    in an agricultural field.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: There are some exceptions to the domination of R for the analysis of time-series
    data. The recent developments in deep learning methods, in particular, Long Short
    Term Memory networks (LSTM) have proved to be very successful for time series
    prediction. As is the case for other deep learning methods (more on this in [Chapter 5](ch05.xhtml#ch06)),
    this is an area better supported by Python tools.
  prefs: []
  type: TYPE_NORMAL
- en: Base R
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are quite a few different packages that an R user can use to analyze time-series
    data, including `xts`, and `zoo`, but we’ll be focusing on base R functions as
    a start. After this, we’ll have a look at one more modern package to illustrate
    more advanced functionality - [Facebook’s Prophet](https://facebook.github.io/prophet/).
  prefs: []
  type: TYPE_NORMAL
- en: 'Weather data is both widely available and relatively easy to interpret, so
    for our case study, we’ll analyze the daily minimum temperature in Australia ([Table 4-2](#case-study-table)).
    To do a time series analysis, we need to go through the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Load the data into an appropriate format
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Plot the data
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Remove noise and seasonal effects and extract trend
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then we would be able to proceed with more advanced analysis. Imagine we have
    loaded the data from a `.csv` file into a `data.frame` object in R. Nothing out
    of the ordinary here. Still, differently from most Python packages, R requires
    data coercion into a specific object type. In this case, we need to transform
    the `data.frame` into a `ts` (which stands for time series).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: So why would we prefer that to `pandas`? Well, even after you manage to convert
    the raw data into a time series `pd.DataFrame`, you’ll encounter a new concept
    - `DataFrame` indexing (see [Figure 4-4](#ts_index)). To be efficient in data
    munging, you’ll need to understand how this works first!
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/prds_0404.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-4\. The time series index in `pandas`.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'This indexing concept can be confusing, so let’s now look at what the alternative
    is in R and whether that’s better. With the `df_ts` time series object, there
    are already a few useful things we can do. It’s also a good starting point when
    you are working with more advanced time series packages in R, since the coercion
    of a `ts` object into `xts` or `zoo`, should throw no errors (this once again
    is an example of the good object design we covered in [Figure 4-2](#package_design)).
    The first thing you can try to do is `plot` the object, which often yields good
    results in R:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Calling the `plot` function does not simply use a standard function that can
    plot all kinds of different objects in R (this is what you would expect). It calls
    a particular method that is associated with the data object (more on the difference
    between functions and methods is available in [Chapter 2](ch02.xhtml#ch03)). A
    lot of complexity is hidden behind this simple function call!
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/prds_0405.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-5\. Plot of a time-series (`ts`) object in base R.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The results from `plot(df_ts)` on [Figure 4-5](#ts_plot) are already useful.
    The dates on the x-axis are recognized, and a `line` plot is chosen instead of
    the default `points` Plot. The most prevalent issue in analyzing time-series data
    (and most ML data for that matter) is dealing with noise. The difference between
    this data format and others is that there are a few different noise sources, and
    different patterns can be cleaned. This is achieved by a technique called decomposition,
    for which we have the built-in and well-named function `decompose`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: The results can be seen on [Figure 4-6](#ts_decompose).
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/prds_0406.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-6\. Plot of decomposed time-series in base R.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We can see what the random noise is and also what is a seasonal and overall
    pattern. We achieved all this with just one function call in base R! In Python,
    we would need to use the `statsmodels` package to achieve the same.
  prefs: []
  type: TYPE_NORMAL
- en: prophet
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For analyzing time-series data, we also have another exciting package example.
    It’s simultaneously developed for both R and Python (similar to the `lime` explainable
    ML tool) - [Facebook Prophet](https://facebook.github.io/prophet/). This example
    can help us compare the differences in API design. `prophet` is a package whose
    main strength lies in the flexibility for a domain user to adjust to their particular
    need, ease of use of the API, and focus on production readiness. These factors
    make it a good choice for prototyping time series work and using it in a data
    product. Let’s have a look; our data is stored as a `pandas` `DataFrame` in `df`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_data_format_context_CO4-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Here we see the same `fit` API pattern again, borrowed from `scikit-learn`.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_data_format_context_CO4-2)'
  prefs: []
  type: TYPE_NORMAL
- en: This step creates a new empty `Data.Frame` that stores our predictions later.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Both are simple enough and contain the same amount of steps - this is an excellent
    example of a consistent API design (more on this in [Chapter 5](ch05.xhtml#ch06)).
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: It’s an interesting and helpful idea to offer a consistent user experience across
    languages, but we do not predict it’ll be widely implemented. Few organizations
    possess the resources to do such work, which can be limiting since compromises
    have to be made in software design choices.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, you can appreciate that knowing both languages would give you
    a significant advantage in daily work. If you were exposed only to the Python
    package ecosystem, you would probably try to find similar tools for analyzing
    time-series and missing out on the incredible opportunities that base R and related
    R packages provide.
  prefs: []
  type: TYPE_NORMAL
- en: Spatial data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The analysis of spatial data is one of the most promising areas in modern machine
    learning and has a rich history. New tools have been developed in recent years,
    but R has had the upper hand for a long time, despite some recent Python advances.
    As in the previous sections, we’ll look at a practical example to see the packages
    in action.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: There are several formats of spatial data available. In this subsection, we
    are focusing on the analysis of *raster* data. For other formats there are some
    interesting tools available in Python, such as [GeoPandas](https://geopandas.org/),
    but this is out of scope for this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our task is to process occurrence^([22](ch04.xhtml#idm45127450735128)) and
    environmental data for *Loxodonta africana* (African elephant) make it suitable
    for spatial predictions. Such data processing is typical in Species Distribution
    Modeling (SDM), where the predictions are used to construct habitat suitability
    maps used for conservation. This case study is more advanced than the previous
    ones, and a lot of the steps hide some complexity where the packages are doing
    the heavy lifting. The steps are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Obtain environmental raster data
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Cut the raster to fit the area of interest
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Deal with spatial autocorrelation with sampling methods
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: raster
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To solve this problem as a first step, we need to process raster data^([23](ch04.xhtml#idm45127450728616)).
    This is, in a way, very similar to standard image data, but still different in
    processing steps. For this R has the excellent `raster` package available (the
    alternative is Python’s `gdal` and R’s `rgdal`, which in our opinion, are trickier
    to use).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '`raster` allows us to download most of the common useful spatial environmental
    datasets, including the bioclimactic data^([24](ch04.xhtml#idm45127450683720)).'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Here we use the handy `extent` function to crop (cut) the raster data - we are
    only interested in a subsection of all those environmental layers surrounding
    the occurrence data. Here we use the longitude and latitude coordinates to draw
    this rectangle. As a next step, to have a classification problem, we are randomly
    sampling data points from the raster data (those are called “pseudo absences).
    You could imagine that those are the `0`’s in our classification task, and the
    occurrences (observations) are the `1`’s - the target variable. We then convert
    the pseudo-absences to `spatial points`, and finally extract the climate data
    for them as well. In the `SpatialPoints` function, you can also see how we specify
    the geographic projection system, one of the fundamental concepts when analyzing
    spatial data.
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the most common issues when working in ML is correlations within the
    data. The fundamental assumption for a correct dataset is that the individual
    observations in the data are *independent* of each other to get accurate statistical
    results. This issue is always present in spatial data due to its very nature.
    This issue is called *spatial autocorrelation*. There are several packages available
    for sampling from the data to mitigate this risk to deal with this. One such package
    is `ENMeval`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: The `get.checkerboard1` function samples the data in an evenly distributed manner,
    similar to taking equal points from each square from a black and white chessboard.
    We can then take this resampled data and successfully train an ML model without
    worrying about spatial autocorrelation. As a final step, we can take those predictions
    and create the habitat suitability map, shown on ([???](#sdm_map)).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Plot of a `raster` object prediction in R, resulting in a habitat suitability
    map. image::img/sdm_map.png[""]
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When you’re working with spatial raster data, the better package design is provided
    by R. The fundamental tools such as `raster` provide a consistent foundation for
    more advanced application specific ones such as `ENMeval` and `dismo`, without
    the need to worry about complex transformation or error-prone type coercion.
  prefs: []
  type: TYPE_NORMAL
- en: Final thoughts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter we went through the different common data formats, and what
    are the best packages to process them so they are ready for advanced tasks. In
    each case study, we demonstrated a good package design and how that can make a
    data scientist more productive. We have seen that for more ML-focused tasks, such
    as CV and NLP, Python is providing the better user experience and lower learning
    curve. In contrast, for more time series prediction and spatial analysis, R has
    the upper hand. Those selection choices are shown on [Figure 4-7](#decision_tree).
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/prds_0407.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-7\. Decision tree for package selection.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: What the best tools have in common is the better package design ([Figure 4-2](#package_design)).
    You should always use the optimal tool for the job and pay attention to the complexity,
    documentation, and performance of the tools you use!
  prefs: []
  type: TYPE_NORMAL
- en: '^([1](ch04.xhtml#idm45127452048648-marker)) For a more thorough explanation
    on this have a look here: [*https://realpython.com/python-encodings-guide/*](https://realpython.com/python-encodings-guide/).'
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch04.xhtml#idm45127452046808-marker)) This is commonly referred to as
    “data lineage”.
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](ch04.xhtml#idm45127452007560-marker)) Who else didn’t learn what `if __name__
    == "__main__"` does in Python?
  prefs: []
  type: TYPE_NORMAL
- en: ^([4](ch04.xhtml#idm45127452002248-marker)) One table from the data, stored
    in a single file.
  prefs: []
  type: TYPE_NORMAL
- en: ^([5](ch04.xhtml#idm45127451851368-marker)) Not to forget `tidyr`, which was
    discussed in [Chapter 2](ch02.xhtml#ch03)
  prefs: []
  type: TYPE_NORMAL
- en: ^([6](ch04.xhtml#idm45127451849368-marker)) We did mention that statisticians
    are very literal, right?
  prefs: []
  type: TYPE_NORMAL
- en: ^([7](ch04.xhtml#idm45127451817912-marker)) This consistency is a common thread
    in the chapters in [Part III](part03.xhtml#p03) and is addressed additionally
    in [Chapter 5](ch05.xhtml#ch06).
  prefs: []
  type: TYPE_NORMAL
- en: ^([8](ch04.xhtml#idm45127451810536-marker)) Remember - garbage in, garbage out.
  prefs: []
  type: TYPE_NORMAL
- en: ^([9](ch04.xhtml#idm45127451809800-marker)) Using code to go through the content
    of a web page, download and store it in a machine-readable format.
  prefs: []
  type: TYPE_NORMAL
- en: ^([10](ch04.xhtml#idm45127451808600-marker)) Which can be expensive, or even
    impossible in some cases.
  prefs: []
  type: TYPE_NORMAL
- en: ^([11](ch04.xhtml#idm45127451807656-marker)) If you want to learn more on data
    augmentation of images have a look at [this](https://www.tensorflow.org/tutorials/images/data_augmentation)
    tutorial.
  prefs: []
  type: TYPE_NORMAL
- en: ^([12](ch04.xhtml#idm45127451457144-marker)) At the time of writing.
  prefs: []
  type: TYPE_NORMAL
- en: ^([13](ch04.xhtml#idm45127451455800-marker)) Not to be confused with the conference
    of the same name, the PyData stack refers to `NumPy`, `SciPy`, `Pandas`, `IPython`
    and `matplotlib`.
  prefs: []
  type: TYPE_NORMAL
- en: ^([14](ch04.xhtml#idm45127451424392-marker)) The R community has also rallied
    to the call and improved the tooling in recent times, but it still arguably lags
    behind its Python counterparts.
  prefs: []
  type: TYPE_NORMAL
- en: ^([15](ch04.xhtml#idm45127451414392-marker)) To learn more about NLTK have a
    look at the official book available [here](https://www.nltk.org/book/).
  prefs: []
  type: TYPE_NORMAL
- en: ^([16](ch04.xhtml#idm45127451365400-marker)) Data type coercion is the conversion
    of one data type to another.
  prefs: []
  type: TYPE_NORMAL
- en: ^([17](ch04.xhtml#idm45127451276088-marker)) This is a common step in NLP. Some
    examples of stop words are “the”, “a” and “this”. These need to be removed since
    they rarely offer useful information for ML algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: ^([18](ch04.xhtml#idm45127451178120-marker)) The process of labeling words in
    with the PoS they belong to.
  prefs: []
  type: TYPE_NORMAL
- en: ^([19](ch04.xhtml#idm45127451081640-marker)) Converting text into numbers for
    ingestion by a ML algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: ^([20](ch04.xhtml#idm45127451027384-marker)) Such as trying to create custom
    embeddings. Check the RStudio blog [here](https://blogs.rstudio.com/ai/posts/2017-12-22-word-embeddings-with-keras/)
    for more information.
  prefs: []
  type: TYPE_NORMAL
- en: ^([21](ch04.xhtml#idm45127451025304-marker)) You can read more about that [here](https://blogs.rstudio.com/ai/posts/2020-07-30-state-of-the-art-nlp-models-from-r/).
  prefs: []
  type: TYPE_NORMAL
- en: ^([22](ch04.xhtml#idm45127450735128-marker)) Location-tagged observations of
    the animal in the wild.
  prefs: []
  type: TYPE_NORMAL
- en: ^([23](ch04.xhtml#idm45127450728616-marker)) Data representing cells, where
    the cell value represents some information.
  prefs: []
  type: TYPE_NORMAL
- en: ^([24](ch04.xhtml#idm45127450683720-marker)) Environmental features that have
    been determined by ecologists to be highly predictive of species distributions,
    i.e. humidity and temperature.
  prefs: []
  type: TYPE_NORMAL
