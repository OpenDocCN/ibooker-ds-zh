- en: Chapter 8\. Data Cleaning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A universal problem when working with data is understanding the completeness
    of your data. Data engineering depends on the ability to clean, process, and visualize
    data. Now that you’re familiar with the basic functionality of and integration
    of data with notebook-based code editors, either locally in a Jupyter Notebook
    or in the cloud with Google Colab, it’s time to learn how to clean your data.
    Data is frequently incomplete (missing), inconsistently formatted, or otherwise
    inaccurate—problems often called *messy data. Data cleaning* is the process of
    addressing these problems and preparing the data for analysis.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we’ll explore some publicly available datasets, finding and
    cleaning up messes with a few packages that you can load into a Colab notebook.
    You’re going to work with NYPD_Complaint_Data_Historic, a dataset from the open
    data portal for New York City, [NYC Open Data](https://oreil.ly/W8sNI), updated
    on July 7, 2021\. I filtered the data for 2020 to make it a little more manageable
    for viewing and manipulating. You can filter the data based on your data question
    and [export it as a CSV file](https://oreil.ly/7J4Kj). This chapter will show
    you how to manage, remove, update, and consolidate data and process it with a
    few useful Python packages. Data analysis is only as accurate as the quality of
    the dataset or database, and this chapter will provide tools to assess and address
    common inconsistencies.
  prefs: []
  type: TYPE_NORMAL
- en: Checking for Missing Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you’ve ever participated in a data competition, like those available on [Kaggle](https://www.kaggle.com),
    you may have noticed that the datasets are engineered to help you focus your attention
    on a specific task, such as building a visualization. Often, the data has already
    been cleaned for you. Real life is a little more complicated, and missing data
    is a persistent problem. Let’s see how you can make your data world a little tidier.
  prefs: []
  type: TYPE_NORMAL
- en: With spreadsheets and tabular data formats, evaluating the shape of your data
    is a straightforward task. A little bit of scrolling can easily reveal rows and/or
    columns that are missing data. When you review datasets for patterns of missing
    data, you are actually evaluating them for *nullity,* or the presence of null
    data (missing values). Geospatial analysis, like analysis in general, often involves
    multiple tables, so it’s important to learn how to identify patterns in the data
    residing between these tabular datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Uploading to Colab
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I am using [Google Colab](https://oreil.ly/J8wam) for this example, but feel
    free to use whatever environment you prefer. Upload the [NYPD Complaint Data Historic](https://oreil.ly/kv3Pe)
    CSV file to your notebook and click the dots next to the filename. This will give
    you the path to include in the notebook ([Figure 8-1](#uploading_a_file_to_colab)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Uploading a file to Colab](assets/pgda_0801.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-1\. Uploading a file to Colab
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '[Missingno](https://oreil.ly/ps5JI) is a Python library that detects missing
    values in a dataset and visualizes how they are distributed within the dataframe,
    which makes it easier to spot patterns.^([1](ch08.xhtml#ch01fn11)) Missingno has
    NumPy, pandas, SciPy, matplotlib, and seaborn running under the hood, bringing
    familiarity to the underlying code snippets. Install missingno:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Depending on the size of your dataset, you can adjust the sample size. Here
    I’ve set the sample to `1000`. Importing the Python library pandas will allow
    you to begin a little data cleaning and preparation along the way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Once you upload the dataset, you will need to review the data dictionary to
    see what the column headings indicate.
  prefs: []
  type: TYPE_NORMAL
- en: The columns listed in our dataframe are shown in [Table 8-1](#data_dictionary_for_the_nypd_complaint).
    CMPLNT_NUM appears to be the key identifier for each row. This is important because
    each row is a record of one incident in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Table 8-1\. Data dictionary for the NYPD Complaint dataset
  prefs: []
  type: TYPE_NORMAL
- en: '| Field name | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| CMPLNT_NUM | Randomly generated persistent ID for each complaint |'
  prefs: []
  type: TYPE_TB
- en: '| CMPLNT_FR_DT | Exact date of occurrence for the reported event (or starting
    date of occurrence, if CMPLNT_TO_DT exists) |'
  prefs: []
  type: TYPE_TB
- en: '| CMPLNT_FR_TM | Exact time of occurrence for the reported event (or starting
    time of occurrence, if CMPLNT_TO_TM exists) |'
  prefs: []
  type: TYPE_TB
- en: '| CMPLNT_TO_DT | Ending date of occurrence for the reported event, if exact
    time of occurrence is unknown |'
  prefs: []
  type: TYPE_TB
- en: '| CMPLNT_TO_TM | Ending time of occurrence for the reported event, if exact
    time of occurrence is unknown |'
  prefs: []
  type: TYPE_TB
- en: '| ADDR_PCT_CD | Precinct in which the incident occurred |'
  prefs: []
  type: TYPE_TB
- en: '| RPT_DT | Date event was reported to police |'
  prefs: []
  type: TYPE_TB
- en: '| KY_CD | Three-digit offense classification code |'
  prefs: []
  type: TYPE_TB
- en: '| OFNS_DESC | Description of offense corresponding with the key code |'
  prefs: []
  type: TYPE_TB
- en: '| PD_CD | Three-digit internal classification code (more granular than the
    key code) |'
  prefs: []
  type: TYPE_TB
- en: '| PD_DESC | Description of internal classification corresponding with the PD
    code (more granular than the offense description) |'
  prefs: []
  type: TYPE_TB
- en: '| CRM_ATPT_CPTD_CD | Indicator of whether the crime was successfully completed,
    attempted but failed, or interrupted prematurely |'
  prefs: []
  type: TYPE_TB
- en: '| LAW_CAT_CD | Level of offense: felony, misdemeanor, or violation |'
  prefs: []
  type: TYPE_TB
- en: '| BORO_NM | Name of the borough in which the incident occurred |'
  prefs: []
  type: TYPE_TB
- en: '| LOC_OF_OCCUR_DESC | Specific location of occurrence in or around the premises:
    inside, opposite of, front of, or rear of |'
  prefs: []
  type: TYPE_TB
- en: '| PREM_TYP_DESC | Specific description of the premises: grocery store, residence,
    street, etc. |'
  prefs: []
  type: TYPE_TB
- en: '| JURIS_DESC | Description of the jurisdiction code |'
  prefs: []
  type: TYPE_TB
- en: '| JURISDICTION_CODE | Jurisdiction responsible for the incident: either internal,
    like Police (0), Transit (1), and Housing (2), or external (3), like Correction,
    Port Authority, etc. |'
  prefs: []
  type: TYPE_TB
- en: '| PARKS_NM | Name of New York City park, playground, or greenspace of occurrence,
    if applicable (state parks are not included) |'
  prefs: []
  type: TYPE_TB
- en: '| HADEVELOPT | Name of New York City Housing Authority housing development
    of occurrence, if applicable |'
  prefs: []
  type: TYPE_TB
- en: '| HOUSING_PSA | Development-level code |'
  prefs: []
  type: TYPE_TB
- en: '| X_COORD_CD | *x*-coordinate for New York State Plane Coordinate System, Long
    Island Zone, NAD 83, units feet (FIPS 3104) |'
  prefs: []
  type: TYPE_TB
- en: '| Y_COORD_CD | *y*-coordinate for New York State Plane Coordinate System, Long
    Island Zone, NAD 83, units feet (FIPS 3104) |'
  prefs: []
  type: TYPE_TB
- en: '| SUSP_AGE_GROUP | Suspect’s age group |'
  prefs: []
  type: TYPE_TB
- en: '| SUSP_RACE | Suspect’s race description |'
  prefs: []
  type: TYPE_TB
- en: '| SUSP_SEX | Suspect’s sex description |'
  prefs: []
  type: TYPE_TB
- en: '| TRANSIT_DISTRICT | Transit district in which the offense occurred |'
  prefs: []
  type: TYPE_TB
- en: '| Latitude | Midblock latitude coordinate for Global Coordinate System, WGS
    1984, decimal degrees (EPSG 4326) |'
  prefs: []
  type: TYPE_TB
- en: '| Longitude | Midblock longitude coordinate for Global Coordinate System, WGS
    1984, decimal degrees (EPSG 4326) |'
  prefs: []
  type: TYPE_TB
- en: '| Lat_Lon | Geospatial location point (latitude and longitude combined) |'
  prefs: []
  type: TYPE_TB
- en: '| PATROL_BORO | The name of the patrol borough in which the incident occurred
    |'
  prefs: []
  type: TYPE_TB
- en: '| STATION_NAME | Transit station name |'
  prefs: []
  type: TYPE_TB
- en: '| VIC_AGE_GROUP | Victim’s age group |'
  prefs: []
  type: TYPE_TB
- en: '| VIC_RACE | Victim’s race description |'
  prefs: []
  type: TYPE_TB
- en: '| VIC_SEX | Victim’s sex description |'
  prefs: []
  type: TYPE_TB
- en: The variables in a dataset need to be called exactly as they are presented in
    the dataframe. If they are all capitalized or separated by underscores, you need
    to be sure to copy them (or you can rename them similarly to how you renamed the
    columns in the census data in [Chapter 7](ch07.xhtml#geopandas_and_spatial_statistics)).
  prefs: []
  type: TYPE_NORMAL
- en: Next, there are other considerations to examine in a dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Nulls and Non-Nulls
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You’ll first need to examine the dataset for missing values. Start by quickly
    reviewing the data with pandas features such as `NYPD.info()` and `NYPD.describe()`
    (see Figures [8-2](#dataframe_summary_of_nypddotinfoleft_pa) and [8-4](#the_dfdotdescribeleft_parenthesisright)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Using `.info()` can show you column names, the number of non-nulls, and data
    types (`Dtype`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The range index in this dataset is 1,115 entries; therefore, any columns with
    fewer than 1,115 values are missing data. The column non-null count allows you
    to view missing data and decide which columns have not captured enough data to
    provide insights.
  prefs: []
  type: TYPE_NORMAL
- en: This dataset’s [documentation](https://oreil.ly/nhqvO) states that the null
    values are likely attributable to changes in department forms and to data being
    collected inconsistently. In addition, if information was unavailable for 2020
    (the year I selected to filter the data) or was unknown at the point of collection,
    it was classified as Unknown/Not Available/Not Reported. It is important to read
    the supporting documentation to discover the limits of the data, which might in
    turn limit the questions you can usefully ask.
  prefs: []
  type: TYPE_NORMAL
- en: Data Types
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of your top priorities in data cleaning should be examining the data type
    of each column in your dataset. The most common data types, which you should definitely
    be familiar with, are [boolean, integer, float, object, and datetime](https://oreil.ly/WLzh0).
  prefs: []
  type: TYPE_NORMAL
- en: 'The most problematic of these data types is the object. Python objects can
    include a wide variety of data types. Most often they are strings, but you should
    know that they can also include integers, floats, lists, and dictionaries. In
    the pandas library, different data types are detected and classified as NaN (the
    default marker for “not a number”) or NaT for missing datetimes. The output of
    `NYPD.info` includes the data type (`Dtype`), or you can use the `dtypes` DataFrame
    attribute by appending it with a dot: `NYPD.dtypes`.'
  prefs: []
  type: TYPE_NORMAL
- en: Metadata
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can also return information about the shape of the data. This, along with
    the data type information, is what we call *metadata*, or data about your data.
    To see the shape of the NYPD data, run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'For simplicity in this example, I filtered the data at the source to include
    only 2020 data so that we are working with a smaller number of records: 1,115
    entries ([Figure 8-2](#dataframe_summary_of_nypddotinfoleft_pa)).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Dataframe summary of NYPD.info()](assets/pgda_0802.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-2\. Dataframe summary of `NYPD.info()`
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The exact time of each complaint’s occurrence, or CMPLNT_FR_TM, is listed as
    an object or string in [Figure 8-2](#dataframe_summary_of_nypddotinfoleft_pa).
    Pandas will often guess the type of data in a column when a datetime or other
    data type is assigned as a string or object. This is often complicated by defaults
    such as the `low_memory=True` argument. Large files are “chunked,” and the file
    often generates columns with multiple dtypes. I would resist simply switching
    arguments to `False`, however. It is much more efficient to simply change the
    `dtype` manually, as shown in the following code cell:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '`NYPD.head()` will update the columns you retain and confirm that your “dropped”
    columns no longer appear in the dataframe.'
  prefs: []
  type: TYPE_NORMAL
- en: You can eliminate columns with incomplete data if they are not relevant to what
    you are interested in exploring. However, I strongly discourage you from *deleting*
    data. Often, the reason for some data being missing is as compelling or significant
    as the data selected for inclusion.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s try this out by identifying a few sample columns to drop from the dataframe.
  prefs: []
  type: TYPE_NORMAL
- en: Summary Statistics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can examine *summary statistics*, or statistics that summarize your sample
    data, using `.describe()`.
  prefs: []
  type: TYPE_NORMAL
- en: Look at row counts for another opportunity to identify missing data of the different
    features in the dataframe. In the output of `df.describe()`, the NaN values are
    excluded so that you can see the actual distribution of the data as well as the
    count of values in each column.
  prefs: []
  type: TYPE_NORMAL
- en: Measures of Central Tendency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A normal distribution is when the mean, mode, and median are all the same value.
    A *probability* distribution, or *measure of central tendency*, describes the
    central value in a distribution when the distribution is not normal, as shown
    in [Figure 8-3](#a_review_of_measures_of_central_tendenc).
  prefs: []
  type: TYPE_NORMAL
- en: '![A review of measures of central tendency](assets/pgda_0803.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-3\. A review of measures of central tendency
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: There are also certain measures that can indicate the presence of skewed distributions
    and outliers. [Figure 8-4](#the_dfdotdescribeleft_parenthesisright) is a snapshot
    of [Energy and Water Data Disclosure for Local Law 84 2021 (Data for Calendar
    Year 2020)](https://oreil.ly/vOcv7), a large dataset that examines energy and
    water usage in private buildings in New York City. The median or second quartile
    (Q2), the 50% measure in the table, indicates the *median value*, the point at
    which half the values are above and half the values are below.
  prefs: []
  type: TYPE_NORMAL
- en: '![The df.describe() function provides descriptive statistics for columns with
    numerical data](assets/pgda_0804.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-4\. The `df.describe()` function provides descriptive statistics for
    columns with numerical data
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: A quick way to look at the shape of your data is to observe where the Q2, or
    median, is located. If you look at the second column in [Figure 8-4](#the_dfdotdescribeleft_parenthesisright),
    you can determine if the value of the median is closer to 25% or 75%. If it is
    closer to 25% or Q1, the data is right-skewed, meaning that 25% of the values
    are below the value for Q1\. Think of Q1 as values between the minimum value and
    the median. The Q3 value is between the median and the maximum values, indicating
    that 75% of the reported values will be below the Q3\. This will result in data
    with a left skew.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can have a quick view of the numeric values in your dataset with a `df.describe()`
    function.  Enter the code snippet into your notebook to view below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Plotting a histogram makes it easier to visualize the shape of the data. There
    is a right-sided tail, or positive skew, as shown in [Figure 8-5](#visualization_of_distribution_of_the_da).
    This indicates that the mean is greater than the median, which is greater than
    the mode.
  prefs: []
  type: TYPE_NORMAL
- en: '![Visualization of distribution of the data](assets/pgda_0805.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-5\. Visualization of distribution of the data
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Let’s return to the NYPD data. We can visualize the data and see its distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The visualization also indicates a rightward skew of the data in the distribution
    of complaint types ([Figure 8-5](#visualization_of_distribution_of_the_da)).
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 8-6](#nypddotisnaleft_parenthesisright_parent) is simply a tally of
    how many values are missing in each column. Knowing when records are missing before
    analyzing data provides a level of transparency, and it’s critical to communicate
    that information when you share your data visualizations.'
  prefs: []
  type: TYPE_NORMAL
- en: '![NYPD.isna().sum() lists missing values tally for columns in dataset](assets/pgda_0806.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-6\. `NYPD.isna().sum()` lists missing values tally for columns in dataset
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '`NYPD.isna()` returns the dataframe with Boolean values indicating missing
    values, as shown in [Figure 8-7](#dataframe_nypddotisnaleft_parenthesisri). In
    computation, Boolean values have only two possible values: `True` and `False`,
    often represented as `1` and `0`. `True` in this case indicates missing data.
    At a glance, you can see where the data is missing.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Dataframe NYPD.isna()](assets/pgda_0807.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-7\. Dataframe `NYPD.isna()`
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: These are just a few of the options available for exploring a dataframe and
    understanding the scope of the completeness or nullity of the data.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you’ve found your missing data, what should you do next?
  prefs: []
  type: TYPE_NORMAL
- en: Replacing Missing Values
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'An additional reason for examining the dataframe is to review missing values
    and identify how they’re characterized. Once you know how missing values are recorded
    in the table, you can replace them with a variable of your selection, for example
    `NaN`, `unknown`, or another value of interest, using the `na_values` parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Exploring the dataset often reveals inconsistent reporting of missing values
    in the data tables. The code snippet will query alternative variables, such as
    `?` and `999`.
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing Data with Missingno
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we’ll work with missingno again. Missingno can convert tabular
    data matrices into B*oolean masks*, so called because *masking* returns data based
    on certain criteria of the underlying manipulated data. The data returns a True/False
    Boolean according to the indicated criteria. In computation, Boolean values have
    only two possible values. Missingno marks cells that contain data as `True` and
    empty cells as `False`. It can visualize these in a *nullity matrix* ([Figure 8-8](#nullity_matrix_for_the_nypd_complaint_d)),
    a chart that uses white spaces to visually represent missing data and reveal patterns.
    Where data is present, it will be shaded.
  prefs: []
  type: TYPE_NORMAL
- en: '![Nullity matrix for the NYPD Complaint dataset](assets/pgda_0808.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-8\. Nullity matrix for the NYPD Complaint dataset
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In [Figure 8-8](#nullity_matrix_for_the_nypd_complaint_d), each column represents
    a feature in the data, and each row represents an observation. The sparkline at
    the far right of the matrix shows how much data is missing in each record.
  prefs: []
  type: TYPE_NORMAL
- en: 'Do you see any patterns in the missing data here? I notice that the fourth
    and fifth bars, which represent complaint ending dates and times, have the same
    pattern: when one of those two values is missing, the other one is too. You can
    use `msno.bar` to display the same information as a column instead of a simplified
    matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The height of the bar equals the nullity, or level of missing data ([Figure 8-9](#nullity_shown_by_column)).
    Taller columns indicate that the value is complete (not missing data) or almost
    complete.
  prefs: []
  type: TYPE_NORMAL
- en: '![Nullity shown by column](assets/pgda_0809.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-9\. Nullity shown by column
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Evaluating `msno.heatmap(NYPD)` examines *nullity correlation*, or the absence
    or presence of a variable, and how strongly this affects the presence of the other
    variable. Correlation is measured on a scale from –1 to 1, as shown in [Figure 8-10](#heatmap_of_nullity_data_left_parenthesi).
    *Zero correlation* means that the presence of one variable appears to be unrelated
    to whether another variable is present; a correlation score of 1 means that one
    variable appears whenever the other is present. Negative values indicate *negative
    correlation:* the presence of one variable indicates that the other variable will
    not be present.
  prefs: []
  type: TYPE_NORMAL
- en: '![Heatmap of nullity data (NYPD)](assets/pgda_0810.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-10\. Heatmap of nullity data (NYPD)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Values that are either always present or never present are not visualized in
    the heatmap. For example, there are two columns—RPT_DT (the date of the report
    to police) and JURIS_DESC (jurisdiction description)—that report no missing data,
    and they are not visualized in the heatmap. Other values that don’t appear to
    be measuring anything might be generated from erroneous data. A closer examination
    of the data might reveal something of interest. Pay attention to values that are
    below 1: the value of –0.3 in [Figure 8-10](#heatmap_of_nullity_data_left_parenthesi),
    for instance, is worth a closer look. For example, a value of less than –1 means
    the correlation is almost 100% negative.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Dendrograms*, or tree diagrams, are branching diagrams that visually represent
    relationships between different groups. The branches are called *clades*, and
    the terminal ends are called *leaves*. The arrangement of the clades tells us
    which leaves are most similar to one another based on how close together they
    are. The height of the branch points indicates how similar or different they are
    from one another: the greater the height, the greater the difference. The closer
    to 0 the groupings are in the dendrogram, the more closely the presence of nulls
    in one column is related to the presence or absence of nulls in the other columns.'
  prefs: []
  type: TYPE_NORMAL
- en: The [*scipy.cluster.hierarchy.dendrogram*](https://oreil.ly/3Pxln) composes
    clusters by drawing a U-shaped link that joins clades. The dendrogram’s orientation
    defaults to top-down if there are 50 or fewer columns and to left-right when there
    are more than 50.
  prefs: []
  type: TYPE_NORMAL
- en: In [Figure 8-11](#dendrogram_relationships_in_nypd_datase), the columns denoting
    the race and sex of the suspect (SUSP_RACE and SUSP_SEX) are more similar to each
    other than are the columns denoting suspect age group and transit district (SUSP_AGE_GROUP
    and TRANSIT_DISTRICT).
  prefs: []
  type: TYPE_NORMAL
- en: '![Dendrogram relationships in NYPD dataset](assets/pgda_0811.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-11\. Dendrogram relationships in NYPD dataset
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'To view hierarchical clustering, write the following code snippet into your
    code cell:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Dendrograms are useful when focusing on the distribution of data within the
    dataset. The hierarchical clustering quantitatively estimates the relationship
    of each data point to every other point in the dataset. Cluster distance is on
    the vertical axis, and the distinct data points are shown horizontally. The highest
    level of granularity is at the level of the data sample, and the dendrogram is
    a way to visualize this quickly.
  prefs: []
  type: TYPE_NORMAL
- en: Mapping Patterns
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: QGIS is an additional way to view data and look for patterns in the data. The
    data in [Figure 8-12](#visualizing_nypd_filtered_crime_data_on) is filtered to
    show only burglaries in order to help guide additional insights based on location.
    Let’s look at how to generate such a map.
  prefs: []
  type: TYPE_NORMAL
- en: '![Visualizing NYPD filtered crime data on a map](assets/pgda_0812.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-12\. Visualizing NYPD filtered crime data on a map
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Latitude and Longitude
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The dataset has Latitude and Longitude columns. You can convert these to point
    out features, then use GeoPandas to create a map. Let’s use the same file to import
    data and add a shapefile. First, import pandas:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, `read_csv` reads the datafile into your notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Upload the file directly to Google Colab if that’s what you’re using. You can
    modify the `df` variable to whatever you prefer. The `df.head()` function will
    return the first rows in each column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: As a best practice, make sure to capture spellings and capitalizations exactly
    as they appear in your data, and always list longitude before latitude when writing
    code.
  prefs: []
  type: TYPE_NORMAL
- en: Shapefiles
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You will use the Python library [Shapely](https://oreil.ly/KE75Y) for *computational
    geometry,* or working with geometric objects like points, curves, and surfaces.
    When uploading a shapefile, be sure to upload all of the files associated with
    the shapefile (they’ll have the extensions *.dbf*, *.shx*, and *.prj*) into the
    same directory. The shapefile in [Figure 8-13](#nyc_open_data_borough_boundaries)
    is from [NYC Open Data Borough Boundaries](https://oreil.ly/LYjNT).
  prefs: []
  type: TYPE_NORMAL
- en: '![NYC Open Data borough boundaries](assets/pgda_0813.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-13\. NYC Open Data borough boundaries
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'First, import your libraries and packages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Python libraries are often abbreviated in code. You’ve seen that pandas is abbreviated
    as `pd` and GeoPandas as `gpd`. You will notice `plt` used for matplotlib—don’t
    mistake this for part of a function or argument.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now Python simply needs to know how to apply the coordinates in a defined space—so
    you need to define it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: GeoPandas is a *Cartesian coordinate reference system,* which means that each
    point is defined by a pair of numerical coordinates, such as latitude and longitude
    in our example. It assigns geographic data to pandas objects. The GeoPandas dataframe
    has been created from defining our CRS and combining our Longitude and Latitude
    columns. The `Point` function in Shapely uses the Longitude and Latitude columns
    to create a new column labeled geometry in [Figure 8-14](#adding_a_new_columncomma_geometry).
  prefs: []
  type: TYPE_NORMAL
- en: '![Adding a new column, geometry](assets/pgda_0814.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-14\. Adding a new column, geometry
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'In the preceding code cell, we defined `geo_df`, and now we can view the newly
    created geometry column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Here I have enlarged the `figsize` for better visibility, but it will load
    more quickly at a smaller size. The default units are inches, but [conversions
    are available](https://oreil.ly/1PeVm) for centimeters and pixels as well. The
    `street_map` is assigned to the axes as that is where we assigned our shapefile:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[Figure 8-15](#geopandas_dataframe_visualized_on_a_map) is a map created from
    a GeoPandas dataframe. You can filter and edit the reported incidents to create
    a more tailored map. (Refer to the [Jupyter Notebook](https://oreil.ly/9ADWy)
    for additional options.) The `plt.xlim` and `plt.ylim` commands let you select
    a specific boundary to further edit your projection.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you would like to select a type of crime from the dataset, use the `df.loc`
    function to locate all instances. Here is an example showing burglaries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Or perhaps you want to list a few different offenses:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Or a combination of parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '![GeoPandas dataframe visualized on a map of New York City boroughs](assets/pgda_0815.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-15\. GeoPandas dataframe visualized on a map of New York City boroughs
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Experiment with asking different questions!
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You have learned a variety of methods useful for cleaning your data. This is
    important, especially when working with open source data. For example, community-level
    data is often entered manually, and misspellings, missing dates, and the absence
    of geometry variables can limit the utility of these valuable resources.
  prefs: []
  type: TYPE_NORMAL
- en: '^([1](ch08.xhtml#ch01fn11-marker)) Bilogur, Aleksey. 2018\. “Missingno: A Missing
    Data Visualization Suite.” *Journal of Open Source Software* 3 (22): 547\. [*https://doi.org/10.21105/joss.00547*](https://doi.org/10.21105/joss.00547)'
  prefs: []
  type: TYPE_NORMAL
