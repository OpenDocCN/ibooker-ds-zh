<html><head></head><body>
<div id="sbo-rt-content"><section data-pdf-bookmark="Chapter 2. Introduction to Data Analysis with PySpark" data-type="chapter" epub:type="chapter"><div class="chapter" id="introduction_to_data_anlysis_with_pyspark">
<h1><span class="label">Chapter 2. </span>Introduction to Data Analysis with PySpark</h1>
<p>Python is the most widely used language for data science tasks.<a data-primary="Python" data-secondary="about PyData ecosystem" data-type="indexterm" id="idm46507995032912"/><a data-primary="PySpark API" data-secondary="about" data-tertiary="PyData ecosystem" data-type="indexterm" id="idm46507995031936"/><a data-primary="PySpark API" data-secondary="data analysis introduction" data-tertiary="about PyData ecosystem" data-type="indexterm" id="idm46507995030720"/><a data-primary="data analysis" data-secondary="introduction to" data-tertiary="about PyData ecosystem" data-type="indexterm" id="idm46507995029536"/><a data-primary="Spark (Apache)" data-secondary="data analysis introduction" data-tertiary="about PyData ecosystem" data-type="indexterm" id="idm46507995028320"/><a data-primary="big data" data-secondary="PyData ecosystem" data-type="indexterm" id="idm46507995027088"/><a data-primary="PyData ecosystem" data-secondary="PySpark API" data-type="indexterm" id="idm46507995026144"/> The prospect of being able to do statistical computing and web programming using the same language contributed to its rise in popularity in the early 2010s. This has led to a thriving ecosystem of tools and a helpful community for data analysis, often referred to as the PyData ecosystem. This is a big reason for PySpark’s popularity. <a data-primary="big data" data-secondary="challenges data scientists face" data-tertiary="analyst productivity" data-type="indexterm" id="idm46507995025072"/><a data-primary="PySpark API" data-secondary="data challenges faced" data-tertiary="analyst productivity" data-type="indexterm" id="idm46507995023840"/>Being able to leverage distributed computing via Spark in Python helps data science practitioners be more productive because of familiarity with the programming language and presence of a wide community. For that same reason, we have opted to write our examples in PySpark.</p>
<p>It’s difficult to express how transformative it is to do all of your data munging and analysis in a single environment, regardless of where the data itself is stored and processed. It’s the sort of thing that you have to experience to understand, and we wanted to be sure that our examples captured some of that magic feeling we experienced when we first started using PySpark. For example, PySpark provides interoperability with pandas, which is one of the most popular PyData tools. We will explore this feature further in the chapter.</p>
<p>In this chapter, we will explore PySpark’s powerful DataFrame API via a data cleansing exercise. <a data-primary="PySpark API" data-secondary="data analysis introduction" data-tertiary="dataframes" data-type="indexterm" id="ch02-dframes"/><a data-primary="dataframes" data-secondary="about" data-type="indexterm" id="ch02-dframes2"/><a data-primary="Spark (Apache)" data-secondary="data analysis introduction" data-tertiary="dataframes" data-type="indexterm" id="ch02-dframes3"/><a data-primary="data analysis" data-secondary="introduction to" data-tertiary="dataframes in PySpark" data-type="indexterm" id="ch02-dframes4"/>In PySpark, the DataFrame is an abstraction for datasets that have a regular structure in which each record is a row made up of a set of columns, and each column has a well-defined data type. You can think of a dataframe as the Spark analogue of a table in a relational database. <a data-primary="pandas" data-secondary="Spark dataframes versus pandas" data-type="indexterm" id="idm46507993051456"/><a data-primary="dataframes" data-secondary="about" data-tertiary="pandas versus" data-type="indexterm" id="idm46507993050544"/>Even though the naming convention might make you think of a <code>pandas.DataFrame</code> object, Spark’s DataFrames are a different beast. This is because they represent distributed datasets on a cluster, not local data where every row in the data is stored on the same machine. Although there are similarities in how you use DataFrames and the role they play inside the Spark ecosystem, there are some things you may be used to doing when working with dataframes in pandas or R that do not apply to Spark, so it’s best to think of them as their own distinct entity and try to approach them with an open mind.</p>
<p>As for data cleansing, it is the first step<a data-primary="preprocessing data" data-secondary="data cleansing example" data-tertiary="about" data-type="indexterm" id="idm46507993048384"/><a data-primary="data cleansing example" data-secondary="about" data-type="indexterm" id="idm46507993047136"/> in any data science project, and often the most important. Many clever analyses have been undone because the data analyzed had fundamental quality problems or underlying artifacts that biased the analysis or led the data scientist to see things that weren’t really there. Hence, what better way to introduce you to working with data using PySpark and DataFrames than a data cleansing exercise?</p>
<p>First, we will introduce PySpark’s fundamentals and practice them using a sample dataset from the University of California, Irvine, Machine Learning Repository. We’ll reiterate why PySpark is a good choice for data science and introduce its programming model. Then we’ll set up PySpark on our system or cluster and analyze our dataset using PySpark’s DataFrame API. Most of your time using PySpark for data analysis will center around the DataFrame API, so get ready to become intimately familiar with it. This will prepare us for the following chapters where we delve into various machine learning algorithms.</p>
<p>You don’t need to deeply understand how Spark works under the hood for performing data science tasks. However, understanding basic concepts about Spark’s architecture will make it easier to work with PySpark and make better decisions when writing code. That is what we will cover in the next section.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46507993045136">
<h5>DataFrame and RDDs</h5>
<p>An RDD (resilient distributed dataset) is<a data-primary="dataframes" data-secondary="about" data-tertiary="RDDs versus" data-type="indexterm" id="idm46507993043744"/><a data-primary="RDDs (resilient distributed datasets)" data-type="indexterm" id="idm46507993042496"/><a data-primary="resilient distributed datasets (RDDs)" data-type="indexterm" id="idm46507993041856"/><a data-primary="big data" data-secondary="RDDs (resilient distributed datasets)" data-type="indexterm" id="idm46507993041168"/> the most basic abstraction in Spark. It is an immutable distributed collection of elements of your data, partitioned across machines in your Spark cluster. The partitions can be operated in parallel with a low-level API that offers transformations and actions. This was the primary user-facing abstraction at Spark’s inception. However, there are some problems with this model. <a data-primary="Spark Core" data-secondary="RDDs versus DataFrames" data-type="indexterm" id="idm46507993040080"/>Most importantly, any computation that you perform on top of an RDD is opaque to Spark Core. Hence, there’s no built-in optimization that can be done. The problem gets even more acute in the case of PySpark. We won’t go into the architecture-level details since that is beyond the scope of this book.</p>
<p>Enter DataFrames. Introduced in Spark 1.3, DataFrames are like distributed, in-memory tables with named columns and schemas, where each column has a specific data type: integer, string, array, map, real, date, timestamp, etc. It is like a table from our point of view. There also exist a common set of operations that allow us to perform typical computations (joins, aggregations). Because of this, Spark is able to construct an efficient execution plan resulting in better performance as compared to an RDD.</p>
<p>We will focus on DataFrames throughout the book.<a data-primary="online resources" data-secondary="DataFrames versus RDDs" data-type="indexterm" id="idm46507993038144"/> If you want to delve deeper into the differences between DataFrames and RDDs, an excellent resource is <a class="orm:hideurl" href="https://learning.oreilly.com/library/view/learning-spark-2nd/9781492050032/ch03.xhtml"><em>Learning Spark</em></a> (O’Reilly).</p>
</div></aside>
<div data-type="note" epub:type="note">
<p>When using the DataFrame API, your PySpark code should provide comparable performance with Scala. If you’re using a UDF or RDDs, you will have a performance impact.<a data-startref="ch02-dframes" data-type="indexterm" id="idm46507993034720"/><a data-startref="ch02-dframes2" data-type="indexterm" id="idm46507993034016"/><a data-startref="ch02-dframes3" data-type="indexterm" id="idm46507993033344"/><a data-startref="ch02-dframes4" data-type="indexterm" id="idm46507993032672"/></p>
</div>
<section data-pdf-bookmark="Spark Architecture" data-type="sect1"><div class="sect1" id="idm46507993031744">
<h1>Spark Architecture</h1>
<figure><div class="figure" id="spark_architecture_diagram">
<img alt="aaps 0201" height="546" src="assets/aaps_0201.png" width="1251"/>
<h6><span class="label">Figure 2-1. </span>Spark architecture diagram</h6>
</div></figure>
<p><a data-type="xref" href="#spark_architecture_diagram">Figure 2-1</a> depicts the Spark architecture <a data-primary="data analysis" data-secondary="introduction to" data-tertiary="architecture of Spark" data-type="indexterm" id="idm46507993027200"/><a data-primary="Spark (Apache)" data-secondary="architecture of" data-type="indexterm" id="idm46507993025984"/><a data-primary="PySpark API" data-secondary="data analysis introduction" data-tertiary="architecture of Spark" data-type="indexterm" id="idm46507993025040"/><a data-primary="Spark (Apache)" data-secondary="data analysis introduction" data-tertiary="architecture of Spark" data-type="indexterm" id="idm46507993023760"/><a data-primary="driver process in Spark architecture" data-type="indexterm" id="idm46507993022528"/><a data-primary="executor processes of Spark architecture" data-type="indexterm" id="idm46507993021840"/><a data-primary="cluster manager of Spark architecture" data-type="indexterm" id="idm46507993021136"/>through high-level components. Spark applications run as independent sets of processes on a cluster or locally. At a high level, a Spark application is comprised of a driver process, a cluster manager, and a set of executor processes. The driver program is the central component and responsible for distributing tasks across executor processes. There will always be just one driver process. <a data-primary="scalability" data-secondary="Spark" data-tertiary="executor numbers increased" data-type="indexterm" id="idm46507995366560"/>When we talk about scaling, we mean increasing the number of executors. The cluster manager simply manages resources.</p>
<p>Spark is a distributed, data-parallel compute engine.<a data-primary="data-parallel model" data-type="indexterm" id="idm46507995364816"/><a data-primary="Spark (Apache)" data-secondary="about" data-tertiary="data-parallel compute engine" data-type="indexterm" id="idm46507995364112"/><a data-primary="partitioning in data-parallel model" data-type="indexterm" id="idm46507995362928"/> In the data-parallel model, more data partitions equals more parallelism. Partitioning allows for efficient parallelism. A distributed scheme of breaking up data into chunks or partitions allows Spark executors to process only data that is close to them, minimizing network bandwidth. That is, each executor’s core is assigned its own data partition to work on. Remember this whenever a choice related to partitioning comes up.</p>
<p>Spark programming starts with a dataset,<a data-primary="Spark (Apache)" data-secondary="about" data-tertiary="data from many sources" data-type="indexterm" id="idm46507995361664"/><a data-primary="Hadoop (Apache)" data-secondary="Spark data source" data-tertiary="Hadoop distributed file system" data-type="indexterm" id="idm46507995360416"/><a data-primary="PySpark API" data-secondary="data analysis introduction" data-tertiary="dataset" data-type="indexterm" id="idm46507995359184"/><a data-primary="data analysis" data-secondary="introduction to" data-tertiary="dataset" data-type="indexterm" id="idm46507995357952"/><a data-primary="Spark (Apache)" data-secondary="data analysis introduction" data-tertiary="dataset" data-type="indexterm" id="idm46507995356736"/><a data-primary="preprocessing data" data-secondary="data cleansing example" data-tertiary="dataset" data-type="indexterm" id="idm46507995355504"/><a data-primary="data cleansing example" data-secondary="dataset" data-type="indexterm" id="idm46507995354288"/> usually residing in some form of distributed, persistent storage like the Hadoop distributed file system (HDFS) or a cloud-based solution such as AWS S3 and in a format like Parquet. Writing a Spark program typically consists of a few steps:</p>
<ol>
<li>
<p>Define a set of transformations on the input dataset.</p>
</li>
<li>
<p>Invoke actions that output the transformed datasets to persistent storage or return results to the driver’s local memory. These actions will ideally be performed by the worker nodes, as depicted on the right in <a data-type="xref" href="#spark_architecture_diagram">Figure 2-1</a>.</p>
</li>
<li>
<p>Run local computations that operate on the results computed in a distributed fashion. These can help you decide what transformations and actions to undertake next.</p>
</li>
</ol>
<p>It’s important to remember that all of PySpark’s higher-level abstractions still rely on the same philosophy that has been present in Spark since the very beginning: the interplay between storage and execution. Understanding these principles will help you make better use of Spark for data analysis.</p>
<p>Next, we will install and set up PySpark on our machine so that we can start performing data analysis. This is a one-time exercise that will help us run the code examples from this and following chapters.</p>
</div></section>
<section data-pdf-bookmark="Installing PySpark" data-type="sect1"><div class="sect1" id="idm46507995348208">
<h1>Installing PySpark</h1>
<p>The examples and code in this book<a data-primary="PySpark API" data-secondary="installing" data-type="indexterm" id="idm46507995345936"/><a data-primary="installing PySpark API" data-type="indexterm" id="idm46507995344960"/><a data-primary="data analysis" data-secondary="introduction to" data-tertiary="installing PySpark" data-type="indexterm" id="idm46507995344288"/><a data-primary="Spark (Apache)" data-secondary="PySpark API" data-tertiary="installing" data-type="indexterm" id="idm46507995343072"/><a data-primary="Spark (Apache)" data-secondary="about" data-tertiary="version 3 assumed by book" data-type="indexterm" id="idm46507995341856"/><a data-primary="PyPI (Python Package Index)" data-secondary="installing PySpark" data-type="indexterm" id="idm46507995340624"/><a data-primary="online resources" data-secondary="PySpark installer" data-type="indexterm" id="idm46507995339664"/> assume you have Spark 3.1.1 available. For the purpose of following the code examples, install PySpark from the <a href="https://oreil.ly/t0WBZ">PyPI repository</a>.</p>
<pre data-code-language="shell" data-type="programlisting">$ pip3 install pyspark</pre>
<p>Note that PySpark requires Java 8 or later to be installed. If you want SQL, ML, and/or MLlib as<a data-primary="Spark (Apache)" data-secondary="components" data-tertiary="installing" data-type="indexterm" id="idm46507996931904"/><a data-primary="Spark SQL module" data-secondary="installing" data-type="indexterm" id="idm46507990998064"/><a data-primary="MLlib component of Spark" data-secondary="installing" data-type="indexterm" id="idm46507992938160"/><a data-primary="ML component of Spark" data-secondary="installing" data-type="indexterm" id="idm46507993588720"/><a data-primary="installing PySpark API" data-secondary="ML, MLlib, SQL components" data-type="indexterm" id="idm46507991744928"/> extra dependencies, that’s an option too. We will need these later.</p>
<pre data-code-language="shell" data-type="programlisting">$ pip3 install pyspark<code class="o">[</code>sql,ml,mllib<code class="o">]</code></pre>
<div data-type="note" epub:type="note">
<p>Installing from PyPI skips the libraries<a data-primary="Scala" data-secondary="PySpark install from PyPI" data-type="indexterm" id="idm46507990385184"/><a data-primary="Java" data-secondary="PySpark install from PyPI" data-type="indexterm" id="idm46507990106432"/><a data-primary="R" data-secondary="PySpark install from PyPI" data-type="indexterm" id="idm46507989844432"/><a data-primary="Spark (Apache)" data-secondary="documentation online" data-type="indexterm" id="idm46507996384000"/><a data-primary="online resources" data-secondary="Spark" data-tertiary="documentation" data-type="indexterm" id="idm46507995941936"/><a data-primary="online resources" data-secondary="Spark" data-tertiary="full releases" data-type="indexterm" id="idm46507996500080"/><a data-primary="Spark (Apache)" data-secondary="download link for full releases" data-type="indexterm" id="idm46507995676336"/> required to run Scala, Java, or R. Full releases can be obtained from the <a href="https://oreil.ly/pK2Wi">Spark project site</a>. Refer to the <a href="https://oreil.ly/FLh4U">Spark documentation</a> for instructions on setting up a Spark environment, whether on a cluster or simply on your local machine.</p>
</div>
<p>Now we’re ready to launch the <code>pyspark-shell</code>, <a data-primary="PySpark API" data-secondary="installing" data-tertiary="launching shell" data-type="indexterm" id="ch02-shell"/><a data-primary="REPL (read-eval-print loop) environment" data-secondary="PySpark shell" data-type="indexterm" id="idm46507995673280"/><a data-primary="data analysis" data-secondary="introduction to" data-tertiary="launching PySpark shell" data-type="indexterm" id="ch02-shell2"/><a data-primary="Spark (Apache)" data-secondary="PySpark API" data-tertiary="launching shell" data-type="indexterm" id="ch02-shell3"/><a data-primary="installing PySpark API" data-secondary="launching shell" data-type="indexterm" id="ch02-shell4"/>which is an REPL for the Python language that also has some Spark-specific extensions. This is similar to the Python or IPython shell that you may have used. <a data-primary="clusters in Spark" data-secondary="launching local cluster" data-type="indexterm" id="idm46507989923648"/><a data-primary="Spark (Apache)" data-secondary="clusters" data-tertiary="launching local cluster" data-type="indexterm" id="idm46507988762336"/>If you’re just running these examples on your personal computer, you can launch a local Spark cluster by specifying <code>local[N]</code>, where <code>N</code> is the number of threads to run, or <code>*</code> to match the number of cores available on your machine. For example, to launch a local cluster that uses eight threads on an eight-core machine:</p>
<pre data-code-language="shell" data-type="programlisting">$ pyspark --master local<code class="o">[</code>*<code class="o">]</code></pre>
<div data-type="note" epub:type="note">
<p>A Spark application itself is often referred<a data-primary="Spark (Apache)" data-secondary="clusters" data-type="indexterm" id="idm46507991741264"/><a data-primary="clusters in Spark" data-type="indexterm" id="idm46507989945904"/> to as a Spark <em>cluster</em>. That is a logical abstraction and is different from a physical cluster (multiple machines).</p>
</div>
<p>If you have a Hadoop cluster<a data-primary="Hadoop (Apache)" data-secondary="launching Spark jobs on cluster" data-type="indexterm" id="idm46507989546144"/><a data-primary="clusters in Spark" data-secondary="master argument to spark-shell" data-type="indexterm" id="idm46507991241808"/><a data-primary="master argument to spark-shell" data-type="indexterm" id="idm46507989497904"/><a data-primary="Spark (Apache)" data-secondary="clusters" data-tertiary="master argument to spark-shell" data-type="indexterm" id="idm46507996349456"/><a data-primary="YARN running Spark" data-type="indexterm" id="idm46507996559792"/> that runs a version of Hadoop that supports YARN, you can launch the Spark jobs on the cluster by using the value of <code>yarn</code> for the Spark master:</p>
<pre data-code-language="shell" data-type="programlisting">$ pyspark --master yarn --deploy-mode client</pre>
<p>The rest of the examples in this book will not show a <code>--master</code> argument to <code>spark-shell</code>, but you will typically need to specify this argument as appropriate for your environment.</p>
<p>You may need to specify additional<a data-primary="PySpark API" data-secondary="installing" data-tertiary="pyspark arguments" data-type="indexterm" id="idm46507994480624"/> arguments to make the Spark shell fully utilize your resources. A list of arguments can be found by executing <code>pyspark --help</code>. For example, when running Spark with a local master, you can use <code>--driver-memory 2g</code> to let the single local process use 2 GB of memory. <a data-primary="YARN running Spark" data-secondary="documentation online" data-type="indexterm" id="idm46507990274992"/><a data-primary="online resources" data-secondary="Spark" data-tertiary="Spark on YARN documentation" data-type="indexterm" id="idm46507996084912"/>YARN memory configuration is more complex, and relevant options like <code>--executor-memory</code> are explained in the <a href="https://oreil.ly/3bRjy">Spark on YARN documentation</a>.</p>
<div data-type="note" epub:type="note">
<p>The Spark framework officially supports<a data-primary="Spark (Apache)" data-secondary="about" data-tertiary="deployment modes" data-type="indexterm" id="idm46507991787408"/><a data-primary="deployment modes of Spark" data-type="indexterm" id="idm46507989550800"/> four cluster deployment modes: standalone, YARN, Kubernetes, and Mesos. <a data-primary="deployment modes of Spark" data-secondary="documentation online" data-type="indexterm" id="idm46507994941136"/><a data-primary="online resources" data-secondary="Spark" data-tertiary="deployment modes documentation" data-type="indexterm" id="idm46507994009792"/>More details can be found in the <a href="https://oreil.ly/hG2a5">Deploying Spark documentation</a>.</p>
</div>
<p>After running one of these commands, you will see a lot of log messages from Spark as it initializes itself, but you should also see a bit of ASCII art, followed by some additional log messages and a prompt:</p>
<pre data-type="programlisting">Python 3.6.12 |Anaconda, Inc.| (default, Sep  8 2020, 23:10:56)
[GCC 7.3.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 3.0.1
      /_/

Using Python version 3.6.12 (default, Sep  8 2020 23:10:56)
SparkSession available as 'spark'.</pre>
<p>You can run the <code>:help</code> command in the shell.<a data-primary="PySpark API" data-secondary="installing" data-tertiary="shell help command" data-type="indexterm" id="idm46507994027488"/><a data-primary="installing PySpark API" data-secondary="launching shell" data-tertiary="help command" data-type="indexterm" id="idm46507990264784"/><a data-primary="Spark (Apache)" data-secondary="PySpark API" data-tertiary="shell help command" data-type="indexterm" id="idm46507989656080"/><a data-primary="help command in PySpark shell" data-type="indexterm" id="idm46507991950208"/> This will prompt you to either start an interactive help mode or ask for help about specific Python objects. In addition to the note about <code>:help</code>, the Spark log messages indicated<a data-primary="PySpark API" data-secondary="installing" data-tertiary="SparkSession object" data-type="indexterm" id="idm46507995605312"/><a data-primary="Spark (Apache)" data-secondary="PySpark API" data-tertiary="SparkSession object" data-type="indexterm" id="idm46507990520224"/><a data-primary="SparkSession object" data-type="indexterm" id="idm46507989677568"/><a data-primary="spark command in shell" data-type="indexterm" id="idm46507993078672"/> “SparkSession available as <em>spark</em>.” This is a reference to the <code>SparkSession</code>, which acts as an entry point to all Spark operations and data. Go ahead and type <code>spark</code> at the command line:</p>
<pre data-code-language="python" data-type="programlisting"><code class="n">spark</code>
<code class="o">...</code>
<code class="o">&lt;</code><code class="n">pyspark</code><code class="o">.</code><code class="n">sql</code><code class="o">.</code><code class="n">session</code><code class="o">.</code><code class="n">SparkSession</code> <code class="nb">object</code> <code class="n">at</code> <code class="n">DEADBEEF</code><code class="o">&gt;</code></pre>
<p>The REPL will print the string form of the object. For the <code>SparkSession</code> object, this is
simply its name plus the hexadecimal address of the object in memory. (<code>DEADBEEF</code> is a placeholder; the exact value you see here will vary from run to run.) In an interactive Spark shell, the Spark driver instantiates a SparkSession for you, while in a Spark application, you create a SparkSession object yourself.</p>
<div data-type="note" epub:type="note">
<p>In Spark 2.0, the SparkSession became a unified entry point to all Spark operations and data. <a data-primary="sparkContext" data-type="indexterm" id="idm46507988608512"/>Previously used entry points such as SparkContext, SQLContext, HiveContext, SparkConf, and StreamingContext can be accessed through it too.</p>
</div>
<p>What exactly do we do with the <code>spark</code> variable? <code>SparkSession</code> is an<a data-primary="SparkSession object" data-secondary="methods associated with" data-type="indexterm" id="idm46507988606304"/><a data-primary="spark command in shell" data-secondary="methods associated with" data-type="indexterm" id="idm46507988605296"/> object, so it has methods associated with it. We can see what those methods are in the PySpark shell by typing the name of a variable, followed by a period, followed by tab:</p>
<pre data-code-language="python" data-type="programlisting"> <code class="n">spark</code><code class="o">.</code><code class="p">[</code>\<code class="n">t</code><code class="p">]</code>
<code class="o">...</code>
<code class="n">spark</code><code class="o">.</code><code class="n">Builder</code><code class="p">(</code>           <code class="n">spark</code><code class="o">.</code><code class="n">conf</code>
<code class="n">spark</code><code class="o">.</code><code class="n">newSession</code><code class="p">(</code>        <code class="n">spark</code><code class="o">.</code><code class="n">readStream</code>
<code class="n">spark</code><code class="o">.</code><code class="n">stop</code><code class="p">(</code>              <code class="n">spark</code><code class="o">.</code><code class="n">udf</code>
<code class="n">spark</code><code class="o">.</code><code class="n">builder</code>            <code class="n">spark</code><code class="o">.</code><code class="n">createDataFrame</code><code class="p">(</code>
<code class="n">spark</code><code class="o">.</code><code class="n">range</code><code class="p">(</code>             <code class="n">spark</code><code class="o">.</code><code class="n">sparkContext</code>
<code class="n">spark</code><code class="o">.</code><code class="n">streams</code>            <code class="n">spark</code><code class="o">.</code><code class="n">version</code>
<code class="n">spark</code><code class="o">.</code><code class="n">catalog</code>            <code class="n">spark</code><code class="o">.</code><code class="n">getActiveSession</code><code class="p">(</code>
<code class="n">spark</code><code class="o">.</code><code class="n">read</code>               <code class="n">spark</code><code class="o">.</code><code class="n">sql</code><code class="p">(</code>
<code class="n">spark</code><code class="o">.</code><code class="n">table</code><code class="p">(</code></pre>
<p>Out of all the methods provided by SparkSession, the ones that we’re going to use most often allow us to create DataFrames. Now that we have set up PySpark, we can set up our dataset of interest and start using PySpark’s DataFrame API to interact with it. That’s what we will do in the next section.<a data-startref="ch02-shell" data-type="indexterm" id="idm46507988525760"/><a data-startref="ch02-shell2" data-type="indexterm" id="idm46507988525152"/><a data-startref="ch02-shell3" data-type="indexterm" id="idm46507988450480"/><a data-startref="ch02-shell4" data-type="indexterm" id="idm46507988449872"/></p>
</div></section>
<section data-pdf-bookmark="Setting Up Our Data" data-type="sect1"><div class="sect1" id="idm46507988449008">
<h1>Setting Up Our Data</h1>
<p>The UC Irvine Machine Learning Repository<a data-primary="PySpark API" data-secondary="data analysis introduction" data-tertiary="setting up the data" data-type="indexterm" id="ch02-set"/><a data-primary="data analysis" data-secondary="introduction to" data-tertiary="data preparation" data-type="indexterm" id="ch02-set2"/><a data-primary="Spark (Apache)" data-secondary="data analysis introduction" data-tertiary="data preparation" data-type="indexterm" id="ch02-set3"/><a data-primary="preprocessing data" data-secondary="data cleansing example" data-tertiary="data preparation" data-type="indexterm" id="ch02-set4"/><a data-primary="data cleansing example" data-secondary="data preparation" data-type="indexterm" id="ch02-set5"/><a data-primary="Machine Learning Repository (UC Irvine)" data-secondary="cleanup of dataset" data-type="indexterm" id="ch02-set6"/><a data-primary="datasets" data-secondary="UC Irvine Machine Learning Repository" data-tertiary="dataframe creation" data-type="indexterm" id="ch02-set7"/><a data-primary="record linkage" data-secondary="dataset cleanup" data-type="indexterm" id="ch02-set8"/> is a fantastic source for interesting (and free) datasets for research and education. The dataset we’ll analyze was curated from a record linkage study performed at a German hospital in 2010, and it contains several million pairs of patient records that were matched according to several different criteria, such as the patient’s name (first and last), address, and birthday. Each matching field was assigned a numerical score from 0.0 to 1.0 based on how similar the strings were, and the data was then hand-labeled to identify which pairs represented the same person and which did not. The underlying values of the fields that were used to create the dataset were removed to protect the privacy of the patients. Numerical identifiers, the match scores for the fields, and the label for each pair (match versus nonmatch) were published for use in record linkage research.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46507988435504">
<h5>Record Linkage</h5>
<p>The general structure of a<a data-primary="record linkage" data-type="indexterm" id="idm46507988434128"/> record linkage problem is something like this: we have a large collection of records from one or more source systems, and it is likely that multiple records refer to the same underlying entity, such as a customer, a patient, or the location of a business or an event. Each entity has a number of attributes, such as a name, an address, or a birthday, and we will need to use these attributes to find the records that refer to the same entity. Unfortunately, the values of these attributes aren’t perfect: values might have different formatting, typos, or missing information, which means that a simple equality test on the values of the attributes will cause us to miss a significant number of duplicate records. For example, let’s compare the business listings shown in <a data-type="xref" href="#table2-1">Table 2-1</a>.</p>
<table id="table2-1">
<caption><span class="label">Table 2-1. </span>The challenge of record linkage</caption>
<thead>
<tr>
<th>Name</th>
<th>Address</th>
<th>City</th>
<th>State</th>
<th>Phone</th>
</tr>
</thead>
<tbody>
<tr>
<td><p>Josh’s Coffee Shop</p></td>
<td><p>1234 Sunset Boulevard</p></td>
<td><p>West Hollywood</p></td>
<td><p>CA</p></td>
<td><p>(213)-555-1212</p></td>
</tr>
<tr>
<td><p>Josh Coffee</p></td>
<td><p>1234 Sunset Blvd West</p></td>
<td><p>Hollywood</p></td>
<td><p>CA</p></td>
<td><p>555-1212</p></td>
</tr>
<tr>
<td><p>Coffee Chain #1234</p></td>
<td><p>1400 Sunset Blvd #2</p></td>
<td><p>Hollywood</p></td>
<td><p>CA</p></td>
<td><p>206-555-1212</p></td>
</tr>
<tr>
<td><p>Coffee Chain Regional Office</p></td>
<td><p>1400 Sunset Blvd Suite 2</p></td>
<td><p>Hollywood</p></td>
<td><p>California</p></td>
<td><p>206-555-1212</p></td>
</tr>
</tbody>
</table>
<p>The first two entries in this table refer to the same small coffee shop, even though a
data entry error makes it look as if they are in two different cities (West Hollywood and Hollywood). The second two entries, on the other hand, are actually referring to different
business locations of the same chain of coffee shops that happen to share a common
address: one of the entries refers to an actual coffee shop, and the other one refers
to a local corporate office location. Both of the entries give the official phone number
of the corporate headquarters in Seattle.</p>
<p>This example illustrates everything that makes record linkage so difficult: even though
both pairs of entries look similar to each other, the criteria that we use to make the
duplicate/not-duplicate decision is different for each pair. This is the kind of distinction
that is easy for a human to understand and identify at a glance, but is difficult for a
computer to learn.</p>
<p>Record linkage goes by a lot of different names<a data-primary="entity resolution" data-see="record linkage" data-type="indexterm" id="idm46507988415168"/><a data-primary="record deduplication" data-see="record linkage" data-type="indexterm" id="idm46507988414320"/><a data-primary="merge-and-purge" data-see="record linkage" data-type="indexterm" id="idm46507988413472"/><a data-primary="list washing" data-see="record linkage" data-type="indexterm" id="idm46507988412624"/> in the literature and in practice: entity resolution, record deduplication, merge-and-purge, and list washing. For our purposes, we refer to this problem as <em>record linkage</em>.</p>
</div></aside>
<p>From the shell, let’s pull the data from the repository:</p>
<pre data-code-language="shell" data-type="programlisting">$ mkdir linkage
$ <code class="nb">cd</code> linkage/
$ curl -L -o donation.zip https://bit.ly/1Aoywaq
$ unzip donation.zip
$ unzip <code class="s1">'block_*.zip'</code></pre>
<p>If you have a Hadoop cluster handy, you can create a directory for the block data in HDFS and
copy the files from the dataset there:</p>
<pre data-code-language="shell" data-type="programlisting">$ hadoop dfs -mkdir linkage
$ hadoop dfs -put block_*.csv linkage</pre>
<p>To create a dataframe for our<a data-primary="dataframes" data-secondary="record linkage dataset example" data-type="indexterm" id="ch02-dfcreate"/><a data-primary="PySpark API" data-secondary="data analysis introduction" data-tertiary="dataframe created" data-type="indexterm" id="ch02-dfcreate2"/><a data-primary="data analysis" data-secondary="introduction to" data-tertiary="dataframe created" data-type="indexterm" id="ch02-dfcreate3"/><a data-primary="Spark (Apache)" data-secondary="data analysis introduction" data-tertiary="dataframe created" data-type="indexterm" id="ch02-dfcreate4"/><a data-primary="SparkSession object" data-secondary="dataframe created" data-type="indexterm" id="ch02-dfcreate5"/><a data-primary="csv method" data-secondary="dataframe created" data-type="indexterm" id="ch02-dfcreate6"/><a data-primary="Reader API" data-secondary="csv method" data-type="indexterm" id="ch02-dfcreate7"/><a data-primary="csv method" data-type="indexterm" id="idm46507988371920"/> record linkage dataset, we’re going to use the <code>S⁠p⁠a⁠r⁠k​S⁠e⁠s⁠s⁠i⁠o⁠n</code> object. Specifically, we will use the <code>csv</code> method on its Reader API:</p>
<pre data-code-language="python" data-type="programlisting"><code class="n">prev</code> <code class="o">=</code> <code class="n">spark</code><code class="o">.</code><code class="n">read</code><code class="o">.</code><code class="n">csv</code><code class="p">(</code><code class="s2">"linkage/block*.csv"</code><code class="p">)</code>
<code class="o">...</code>
<code class="n">prev</code>
<code class="o">...</code>
<code class="n">DataFrame</code><code class="p">[</code><code class="n">_c0</code><code class="p">:</code> <code class="n">string</code><code class="p">,</code> <code class="n">_c1</code><code class="p">:</code> <code class="n">string</code><code class="p">,</code> <code class="n">_c2</code><code class="p">:</code> <code class="n">string</code><code class="p">,</code> <code class="n">_c3</code><code class="p">:</code> <code class="n">string</code><code class="p">,</code><code class="o">...</code></pre>
<p>By default, every column in a CSV file is treated as a <code>string</code> type, and the column names default to <code>_c0</code>, <code>_c1</code>, <code>_c2</code>, and so on. <a data-primary="show method of dataframes" data-type="indexterm" id="idm46507988293136"/>We can look at the head of a dataframe in the shell by calling its <code>show</code> method:</p>
<pre data-code-language="python" data-type="programlisting"><code class="n">prev</code><code class="o">.</code><code class="n">show</code><code class="p">(</code><code class="mi">2</code><code class="p">)</code>
<code class="o">...</code>
<code class="o">+-----+-----+------------+------------+------------+------------+-------+------+</code>
<code class="o">|</code>  <code class="n">_c0</code><code class="o">|</code>  <code class="n">_c1</code><code class="o">|</code>         <code class="n">_c2</code><code class="o">|</code>         <code class="n">_c3</code><code class="o">|</code>         <code class="n">_c4</code><code class="o">|</code>         <code class="n">_c5</code><code class="o">|</code>    <code class="n">_c6</code><code class="o">|</code>   <code class="n">_c7</code><code class="o">|</code>
<code class="o">+-----+-----+------------+------------+------------+------------+-------+------+</code>
<code class="o">|</code> <code class="n">id_1</code><code class="o">|</code> <code class="n">id_2</code><code class="o">|</code><code class="n">cmp_fname_c1</code><code class="o">|</code><code class="n">cmp_fname_c2</code><code class="o">|</code><code class="n">cmp_lname_c1</code><code class="o">|</code><code class="n">cmp_lname_c2</code><code class="o">|</code><code class="n">cmp_sex</code><code class="o">|</code><code class="n">cmp_bd</code><code class="o">|</code>
<code class="o">|</code> <code class="mi">3148</code><code class="o">|</code> <code class="mi">8326</code><code class="o">|</code>           <code class="mi">1</code><code class="o">|</code>           <code class="err">?</code><code class="o">|</code>           <code class="mi">1</code><code class="o">|</code>           <code class="err">?</code><code class="o">|</code>      <code class="mi">1</code><code class="o">|</code>     <code class="mi">1</code><code class="o">|</code>
<code class="o">|</code><code class="mi">14055</code><code class="o">|</code><code class="mi">94934</code><code class="o">|</code>           <code class="mi">1</code><code class="o">|</code>           <code class="err">?</code><code class="o">|</code>           <code class="mi">1</code><code class="o">|</code>           <code class="err">?</code><code class="o">|</code>      <code class="mi">1</code><code class="o">|</code>     <code class="mi">1</code><code class="o">|</code>
<code class="o">|</code><code class="mi">33948</code><code class="o">|</code><code class="mi">34740</code><code class="o">|</code>           <code class="mi">1</code><code class="o">|</code>           <code class="err">?</code><code class="o">|</code>           <code class="mi">1</code><code class="o">|</code>           <code class="err">?</code><code class="o">|</code>      <code class="mi">1</code><code class="o">|</code>     <code class="mi">1</code><code class="o">|</code>
<code class="o">|</code>  <code class="mi">946</code><code class="o">|</code><code class="mi">71870</code><code class="o">|</code>           <code class="mi">1</code><code class="o">|</code>           <code class="err">?</code><code class="o">|</code>           <code class="mi">1</code><code class="o">|</code>           <code class="err">?</code><code class="o">|</code>      <code class="mi">1</code><code class="o">|</code>     <code class="mi">1</code><code class="o">|</code></pre>
<p>We can see that the first row of the DataFrame is the name of the header columns, as we expected, and that the CSV file has been cleanly split up into its individual columns. <a data-primary="missing data values" data-secondary="dataframe creation" data-type="indexterm" id="idm46507988287824"/><a data-primary="data values missing" data-see="missing data values" data-type="indexterm" id="idm46507988192176"/>We can also see the presence of the <code>?</code> strings in some of the columns; we will need to handle these as missing values. In addition to naming each column correctly, it would be ideal if Spark could properly infer the data type of each of the columns for us.</p>
<p>Fortunately, Spark’s CSV reader provides all of this functionality for us via options that we can set on the Reader API. You can see the full list of options that the API takes in the <a href="https://oreil.ly/xiLj1"><code>pyspark</code> documentation</a>. For now, we’ll read and parse the linkage data like this:</p>
<pre data-code-language="python" data-type="programlisting"><code class="n">parsed</code> <code class="o">=</code> <code class="n">spark</code><code class="o">.</code><code class="n">read</code><code class="o">.</code><code class="n">option</code><code class="p">(</code><code class="s2">"header"</code><code class="p">,</code> <code class="s2">"true"</code><code class="p">)</code><code class="o">.</code><code class="n">option</code><code class="p">(</code><code class="s2">"nullValue"</code><code class="p">,</code> <code class="s2">"?"</code><code class="p">)</code><code class="o">.</code>\
          <code class="n">option</code><code class="p">(</code><code class="s2">"inferSchema"</code><code class="p">,</code> <code class="s2">"true"</code><code class="p">)</code><code class="o">.</code><code class="n">csv</code><code class="p">(</code><code class="s2">"linkage/block*.csv"</code><code class="p">)</code></pre>
<p>When we call <code>show</code> on the <code>parsed</code> data, we see that the column names are set correctly and the <code>?</code> strings have been replaced by <code>null</code> values. To see the inferred type for each column, we can print the schema of the <code>parsed</code> DataFrame like this:</p>
<pre data-code-language="python" data-type="programlisting"><code class="n">parsed</code><code class="o">.</code><code class="n">printSchema</code><code class="p">()</code>
<code class="o">...</code>
<code class="n">root</code>
 <code class="o">|--</code> <code class="n">id_1</code><code class="p">:</code> <code class="n">integer</code> <code class="p">(</code><code class="n">nullable</code> <code class="o">=</code> <code class="n">true</code><code class="p">)</code>
 <code class="o">|--</code> <code class="n">id_2</code><code class="p">:</code> <code class="n">integer</code> <code class="p">(</code><code class="n">nullable</code> <code class="o">=</code> <code class="n">true</code><code class="p">)</code>
 <code class="o">|--</code> <code class="n">cmp_fname_c1</code><code class="p">:</code> <code class="n">double</code> <code class="p">(</code><code class="n">nullable</code> <code class="o">=</code> <code class="n">true</code><code class="p">)</code>
 <code class="o">|--</code> <code class="n">cmp_fname_c2</code><code class="p">:</code> <code class="n">double</code> <code class="p">(</code><code class="n">nullable</code> <code class="o">=</code> <code class="n">true</code><code class="p">)</code>
<code class="o">...</code></pre>
<p>Each <code>Column</code> instance contains the name of the column, the most specific data type that could handle the type of data contained in each record, and a boolean field that indicates whether or not the column may contain null values, which is true by default. <a data-primary="iteration in data analysis" data-secondary="schema inference in two passes" data-type="indexterm" id="idm46507987988064"/>In order to perform the schema inference, Spark must do <em>two</em> passes over the dataset: one pass to figure out the type of each column, and a second pass to do the actual parsing. The first pass can work on a sample if desired.</p>
<p>If you know the schema that you want to use for a file ahead of time,<a data-primary="Reader API" data-secondary="schema function" data-type="indexterm" id="idm46507987986320"/><a data-primary="schema function of Reader API" data-type="indexterm" id="idm46507987985344"/> you can create an instance of the <code>pyspark.sql.types.StructType</code> class and pass it to the Reader API via the <code>schema</code> function. This can have a significant performance benefit when the dataset is very large, since Spark will not need to perform an extra pass over the data to figure out the data type of each column.</p>
<p>Here is an example of defining a schema using <code>StructType</code> and <code>StructField</code>:</p>
<pre data-code-language="python" data-type="programlisting"><code class="kn">from</code> <code class="nn">pyspark.sql.types</code> <code class="kn">import</code> <code class="o">*</code>
<code class="n">schema</code> <code class="o">=</code> <code class="n">StructType</code><code class="p">([</code><code class="n">StructField</code><code class="p">(</code><code class="s2">"id_1"</code><code class="p">,</code> <code class="n">IntegerType</code><code class="p">(),</code> <code class="kc">False</code><code class="p">),</code>
  <code class="n">StructField</code><code class="p">(</code><code class="s2">"id_2"</code><code class="p">,</code> <code class="n">StringType</code><code class="p">(),</code> <code class="kc">False</code><code class="p">),</code>
  <code class="n">StructField</code><code class="p">(</code><code class="s2">"cmp_fname_c1"</code><code class="p">,</code> <code class="n">DoubleType</code><code class="p">(),</code> <code class="kc">False</code><code class="p">)])</code>

<code class="n">spark</code><code class="o">.</code><code class="n">read</code><code class="o">.</code><code class="n">schema</code><code class="p">(</code><code class="n">schema</code><code class="p">)</code><code class="o">.</code><code class="n">csv</code><code class="p">(</code><code class="s2">"..."</code><code class="p">)</code></pre>
<p class="pagebreak-before">Another way to define the schema is using DDL (data definition language) 
<span class="keep-together">statements</span>:</p>
<pre data-code-language="python" data-type="programlisting"><code class="n">schema</code> <code class="o">=</code> <code class="s2">"id_1 INT, id_2 INT, cmp_fname_c1 DOUBLE"</code></pre>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46507987875520">
<h5>Data Formats and Data Sources</h5>
<p>Spark ships with built-in support for reading and writing dataframes<a data-primary="dataframes" data-secondary="data formats supported" data-type="indexterm" id="idm46507987932960"/><a data-primary="data formats supported by Spark" data-type="indexterm" id="idm46507987932112"/><a data-primary="persistent storage" data-secondary="data formats supported by Spark" data-type="indexterm" id="idm46507987931504"/> in a variety of formats via the DataFrameReader and DataFrameWriter APIs. In addition to the CSV format discussed here, you can also read and write structured data from the following sources:</p>
<dl>
<dt><code>parquet</code></dt>
<dd>
<p>Leading columnar-oriented data storage format (default option in Spark)<a data-primary="Parquet data storage format" data-type="indexterm" id="idm46507987884400"/></p>
</dd>
<dt><code>orc</code></dt>
<dd>
<p>Another columnar-oriented data storage format<a data-primary="orc data storage format" data-type="indexterm" id="idm46507987882320"/></p>
</dd>
<dt><code>json</code></dt>
<dd>
<p>Supports many of the same schema-inference functionality that the CSV format does<a data-primary="json data storage format" data-type="indexterm" id="idm46507987880208"/></p>
</dd>
<dt><code>jdbc</code></dt>
<dd>
<p>Connects to a relational database via the JDBC data connection standard<a data-primary="jdbc data storage format" data-type="indexterm" id="idm46507987878128"/></p>
</dd>
<dt><code>avro</code></dt>
<dd>
<p>Provides efficient message serialization and deserialization when using a streaming source such as Apache Kafka<a data-primary="Avro (Apache)" data-secondary="decoupling storage from modeling" data-tertiary="data storage format" data-type="indexterm" id="idm46507987856768"/></p>
</dd>
<dt><code>text</code></dt>
<dd>
<p>Maps each line of a file to a dataframe with a single column of type <code>string</code><a data-primary="text data storage format" data-type="indexterm" id="idm46507987854016"/></p>
</dd>
<dt><code>image</code></dt>
<dd>
<p>Loads image files from a directory as a dataframe with one column, containing image data stored as image schema<a data-primary="image data storage format" data-type="indexterm" id="idm46507987851808"/></p>
</dd>
<dt><code>libsvm</code></dt>
<dd>
<p>Popular text file format for representing labeled observations with sparse features<a data-primary="libsvm data storage format" data-type="indexterm" id="idm46507987849728"/></p>
</dd>
<dt><code>binary</code></dt>
<dd>
<p>Reads binary files and converts each file into a single dataframe row (new in Spark 3.0)<a data-primary="binary data storage format" data-type="indexterm" id="idm46507987847536"/></p>
</dd>
<dt><code>xml</code></dt>
<dd>
<p>Simple text-based format for representing structured information such as documents, data, configuration, or books (available via the <code>spark-xml</code> package)<a data-primary="xml data storage format" data-type="indexterm" id="idm46507987845040"/></p>
</dd>
</dl>
<p>You access the methods<a data-primary="SparkSession object" data-secondary="read method" data-type="indexterm" id="idm46507987884992"/><a data-primary="dataframes" data-secondary="DataFrameReader API" data-type="indexterm" id="idm46507987843296"/> of the DataFrameReader API by calling the <code>read</code> method on a SparkSession instance, and you can load data from a file using either the <code>format</code> and <code>load</code> methods or one of the shortcut methods for built-in formats:</p>
<pre data-code-language="python" data-type="programlisting"><code class="n">d1</code> <code class="o">=</code> <code class="n">spark</code><code class="o">.</code><code class="n">read</code><code class="o">.</code><code class="n">format</code><code class="p">(</code><code class="s2">"json"</code><code class="p">)</code><code class="o">.</code><code class="n">load</code><code class="p">(</code><code class="s2">"file.json"</code><code class="p">)</code>
<code class="n">d2</code> <code class="o">=</code> <code class="n">spark</code><code class="o">.</code><code class="n">read</code><code class="o">.</code><code class="n">json</code><code class="p">(</code><code class="s2">"file.json"</code><code class="p">)</code></pre>
<p>In this example, <code>d1</code> and <code>d2</code> reference the same underlying JSON data and will have the same contents. Each of the different file formats has its own set of options that can be set via the same <code>option</code> method that we used for CSV files.</p>
<p>To write data out again, you access<a data-primary="dataframes" data-secondary="DataFrameWriter API" data-type="indexterm" id="idm46507987792416"/> the DataFrameWriter API via the <code>write</code> method on any DataFrame instance. The DataFrameWriter API supports the same built-in formats as the DataFrameReader API, so the following two methods are equivalent ways of writing the contents of the <code>d1</code> DataFrame as a Parquet file:</p>
<pre data-code-language="python" data-type="programlisting"><code class="n">d1</code><code class="o">.</code><code class="n">write</code><code class="o">.</code><code class="n">format</code><code class="p">(</code><code class="s2">"parquet"</code><code class="p">)</code><code class="o">.</code><code class="n">save</code><code class="p">(</code><code class="s2">"file.parquet"</code><code class="p">)</code>
<code class="n">d1</code><code class="o">.</code><code class="n">write</code><code class="o">.</code><code class="n">parquet</code><code class="p">(</code><code class="s2">"file.parquet"</code><code class="p">)</code></pre>
<p>By default, Spark will throw an error<a data-primary="dataframes" data-secondary="DataFrameWriter API" data-tertiary="file already exists" data-type="indexterm" id="idm46507987756336"/> if you try to save a dataframe to a file that already exists. You can control Spark’s behavior in this situation via the <code>mode</code> method on the DataFrameWriter API to either <code>Overwrite</code> the existing file, <code>Append</code> the data in the DataFrame to the file (if it exists), or <code>Ignore</code> the write operation if the file already exists and leave it in place:</p>
<pre data-code-language="python" data-type="programlisting"><code class="n">d2</code><code class="o">.</code><code class="n">write</code><code class="o">.</code><code class="n">format</code><code class="p">(</code><code class="s2">"parquet"</code><code class="p">)</code><code class="o">.</code><code class="n">mode</code><code class="p">(</code><code class="s2">"overwrite"</code><code class="p">)</code><code class="o">.</code><code class="n">save</code><code class="p">(</code><code class="s2">"file.parquet"</code><code class="p">)</code></pre>
<p>You can specify the <code>mode</code> as a string literal (<code>"overwrite"</code>, <code>"append"</code>, <code>"ignore"</code>).</p>
</div></aside>
<p>DataFrames have a number of methods<a data-primary="dataframes" data-secondary="first method to read first element" data-type="indexterm" id="idm46507987713056"/><a data-primary="first method of dataframes" data-type="indexterm" id="idm46507987710096"/> that enable us to read data from the cluster into the PySpark REPL on our client machine. Perhaps the simplest of these is <code>first</code>, which returns the first element of the DataFrame into the client:</p>
<pre data-code-language="python" data-type="programlisting"><code class="n">parsed</code><code class="o">.</code><code class="n">first</code><code class="p">()</code>
<code class="o">...</code>
<code class="n">Row</code><code class="p">(</code><code class="n">id_1</code><code class="o">=</code><code class="mi">3148</code><code class="p">,</code> <code class="n">id_2</code><code class="o">=</code><code class="mi">8326</code><code class="p">,</code> <code class="n">cmp_fname_c1</code><code class="o">=</code><code class="mf">1.0</code><code class="p">,</code> <code class="n">cmp_fname_c2</code><code class="o">=</code><code class="kc">None</code><code class="p">,</code><code class="o">...</code></pre>
<p>The <code>first</code> method can be useful for sanity checking a dataset, but we’re generally interested in bringing back larger samples of a DataFrame into the client for analysis. When we know that a DataFrame contains only a small number of records, we can use the <code>toPandas</code> or <code>collect</code> method to return all the contents of a DataFrame to the client as an array. For extremely large DataFrames, using these methods can be dangerous and cause an out-of-memory exception. Because we don’t know how big the linkage dataset is just yet, we’ll hold off on doing this right now.<a data-startref="ch02-dfcreate" data-type="indexterm" id="idm46507987661952"/><a data-startref="ch02-dfcreate2" data-type="indexterm" id="idm46507987661248"/><a data-startref="ch02-dfcreate3" data-type="indexterm" id="idm46507987660576"/><a data-startref="ch02-dfcreate4" data-type="indexterm" id="idm46507987659904"/><a data-startref="ch02-dfcreate5" data-type="indexterm" id="idm46507987659232"/><a data-startref="ch02-dfcreate6" data-type="indexterm" id="idm46507987658560"/><a data-startref="ch02-dfcreate7" data-type="indexterm" id="idm46507987657888"/></p>
<p>In the next several sections, we’ll use a mix of local development and testing and cluster computation to perform more munging and analysis of the record linkage data, but if you need to take a moment to drink in the new world of awesome that you have just entered, we certainly understand.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46507987656832">
<h5>Transformations and Actions</h5>
<p>The act of creating a DataFrame does not cause any distributed computation to take place on the cluster. Rather, DataFrames define logical datasets that are intermediate steps in a computation. Spark operations on distributed data can be classified into two types: transformations and actions.</p>
<p>All transformations are <a data-primary="dataframes" data-secondary="Spark operations" data-seealso="transformations" data-tertiary="transformations" data-type="indexterm" id="idm46507987654656"/><a data-primary="transformations of distributed data" data-secondary="lazy evaluation" data-type="indexterm" id="idm46507987653136"/><a data-primary="lazy evaluation of transformations" data-type="indexterm" id="idm46507987652224"/>evaluated lazily. That is, their results are not computed immediately, but they are recorded as a lineage. This allows Spark to optimize the query plan. <a data-primary="dataframes" data-secondary="Spark operations" data-seealso="actions" data-tertiary="actions" data-type="indexterm" id="idm46507987651456"/><a data-primary="actions for distributed computations" data-type="indexterm" id="idm46507987649968"/>Distributed computation occurs upon invoking an <em>action</em> on a DataFrame. <a data-primary="count action on DataFrame" data-type="indexterm" id="idm46507987648816"/>For example, the <code>count</code> action returns the number of objects in an DataFrame:</p>
<pre data-code-language="python" data-type="programlisting"><code class="n">df</code><code class="o">.</code><code class="n">count</code><code class="p">()</code>
<code class="o">...</code>
<code class="mi">15</code></pre>
<p>The <code>collect</code> action returns<a data-primary="collect action on DataFrame" data-type="indexterm" id="idm46507987619600"/> an <code>Array</code> with all the <code>Row</code> objects from the DataFrame.  This <code>Array</code> resides in local memory, not on the cluster:</p>
<pre data-code-language="python" data-type="programlisting"><code class="n">df</code><code class="o">.</code><code class="n">collect</code><code class="p">()</code>
<code class="p">[</code><code class="n">Row</code><code class="p">(</code><code class="nb">id</code><code class="o">=</code><code class="s1">'12'</code><code class="p">,</code> <code class="n">department</code><code class="o">=</code><code class="s1">'sales'</code><code class="p">),</code> <code class="o">...</code></pre>
<p>Actions need not only return results to the local process. <a data-primary="dataframes" data-secondary="DataFrameWriter API" data-tertiary="save action" data-type="indexterm" id="idm46507987600912"/><a data-primary="save action of dataframes" data-type="indexterm" id="idm46507987599824"/><a data-primary="persistent storage" data-secondary="save action of dataframes" data-type="indexterm" id="idm46507987599184"/><a data-primary="storage" data-see="persistent storage" data-type="indexterm" id="idm46507987569760"/>The <code>save</code> action saves the contents of the DataFrame to persistent storage:</p>
<pre data-code-language="python" data-type="programlisting"><code class="n">df</code><code class="o">.</code><code class="n">write</code><code class="o">.</code><code class="n">format</code><code class="p">(</code><code class="s2">"parquet"</code><code class="p">)</code><code class="o">.</code><code class="p">(</code><code class="s2">"user/ds/mynumbers"</code><code class="p">)</code></pre>
<p>Remember that <code>DataFrameReader</code> can accept<a data-primary="dataframes" data-secondary="DataFrameReader API" data-tertiary="directory of text files as input" data-type="indexterm" id="idm46507987530512"/> a directory of text files as input, meaning that a future Spark job could refer to <code>mynumbers</code> as an input directory.<a data-startref="ch02-set" data-type="indexterm" id="idm46507987528848"/><a data-startref="ch02-set2" data-type="indexterm" id="idm46507987528112"/><a data-startref="ch02-set3" data-type="indexterm" id="idm46507987527440"/><a data-startref="ch02-set4" data-type="indexterm" id="idm46507987526768"/><a data-startref="ch02-set5" data-type="indexterm" id="idm46507987526096"/><a data-startref="ch02-set6" data-type="indexterm" id="idm46507987525424"/><a data-startref="ch02-set7" data-type="indexterm" id="idm46507987508320"/><a data-startref="ch02-set8" data-type="indexterm" id="idm46507987507712"/></p>
</div></aside>
</div></section>
<section data-pdf-bookmark="Analyzing Data with the DataFrame API" data-type="sect1"><div class="sect1" id="idm46507988448096">
<h1>Analyzing Data with the DataFrame API</h1>
<p>The DataFrame API comes with a powerful <a data-primary="record linkage" data-secondary="analyzing data via DataFrame API" data-type="indexterm" id="ch02-anal"/><a data-primary="dataframes" data-secondary="analyzing data via DataFrame API" data-type="indexterm" id="ch02-anal2"/><a data-primary="data analysis" data-secondary="introduction to" data-tertiary="DataFrame API for analysis" data-type="indexterm" id="ch02-anal3"/><a data-primary="PySpark API" data-secondary="data analysis introduction" data-tertiary="DataFrame API for analysis" data-type="indexterm" id="ch02-anal4"/><a data-primary="Spark (Apache)" data-secondary="data analysis introduction" data-tertiary="DataFrame API for analysis" data-type="indexterm" id="ch02-anal5"/><a data-primary="datasets" data-secondary="UC Irvine Machine Learning Repository" data-tertiary="data analysis" data-type="indexterm" id="ch02-anal6"/><a data-primary="Machine Learning Repository (UC Irvine)" data-secondary="data analysis" data-type="indexterm" id="ch02-anal7"/><a data-primary="repositories" data-see="datasets" data-type="indexterm" id="idm46507987496304"/>set of tools that will likely be familiar to data scientists who are used to Python and SQL. In this section, we will begin to explore these tools and how to apply them to the record linkage data.</p>
<p>If we look at the schema of the <code>parsed</code> DataFrame<a data-primary="schemas of dataframes" data-secondary="Machine Learning Repository dataset" data-type="indexterm" id="idm46507987494432"/><a data-primary="dataframes" data-secondary="schemas" data-tertiary="Machine Learning Repository dataset" data-type="indexterm" id="idm46507987493360"/><a data-primary="printSchema method" data-type="indexterm" id="idm46507987492128"/> and the first few rows of data, we see this:</p>
<pre data-code-language="python" data-type="programlisting"><code class="n">parsed</code><code class="o">.</code><code class="n">printSchema</code><code class="p">()</code>
<code class="o">...</code>
<code class="n">root</code>
 <code class="o">|--</code> <code class="n">id_1</code><code class="p">:</code> <code class="n">integer</code> <code class="p">(</code><code class="n">nullable</code> <code class="o">=</code> <code class="n">true</code><code class="p">)</code>
 <code class="o">|--</code> <code class="n">id_2</code><code class="p">:</code> <code class="n">integer</code> <code class="p">(</code><code class="n">nullable</code> <code class="o">=</code> <code class="n">true</code><code class="p">)</code>
 <code class="o">|--</code> <code class="n">cmp_fname_c1</code><code class="p">:</code> <code class="n">double</code> <code class="p">(</code><code class="n">nullable</code> <code class="o">=</code> <code class="n">true</code><code class="p">)</code>
 <code class="o">|--</code> <code class="n">cmp_fname_c2</code><code class="p">:</code> <code class="n">double</code> <code class="p">(</code><code class="n">nullable</code> <code class="o">=</code> <code class="n">true</code><code class="p">)</code>
 <code class="o">|--</code> <code class="n">cmp_lname_c1</code><code class="p">:</code> <code class="n">double</code> <code class="p">(</code><code class="n">nullable</code> <code class="o">=</code> <code class="n">true</code><code class="p">)</code>
 <code class="o">|--</code> <code class="n">cmp_lname_c2</code><code class="p">:</code> <code class="n">double</code> <code class="p">(</code><code class="n">nullable</code> <code class="o">=</code> <code class="n">true</code><code class="p">)</code>
 <code class="o">|--</code> <code class="n">cmp_sex</code><code class="p">:</code> <code class="n">integer</code> <code class="p">(</code><code class="n">nullable</code> <code class="o">=</code> <code class="n">true</code><code class="p">)</code>
 <code class="o">|--</code> <code class="n">cmp_bd</code><code class="p">:</code> <code class="n">integer</code> <code class="p">(</code><code class="n">nullable</code> <code class="o">=</code> <code class="n">true</code><code class="p">)</code>
 <code class="o">|--</code> <code class="n">cmp_bm</code><code class="p">:</code> <code class="n">integer</code> <code class="p">(</code><code class="n">nullable</code> <code class="o">=</code> <code class="n">true</code><code class="p">)</code>
 <code class="o">|--</code> <code class="n">cmp_by</code><code class="p">:</code> <code class="n">integer</code> <code class="p">(</code><code class="n">nullable</code> <code class="o">=</code> <code class="n">true</code><code class="p">)</code>
 <code class="o">|--</code> <code class="n">cmp_plz</code><code class="p">:</code> <code class="n">integer</code> <code class="p">(</code><code class="n">nullable</code> <code class="o">=</code> <code class="n">true</code><code class="p">)</code>
 <code class="o">|--</code> <code class="n">is_match</code><code class="p">:</code> <code class="n">boolean</code> <code class="p">(</code><code class="n">nullable</code> <code class="o">=</code> <code class="n">true</code><code class="p">)</code>

<code class="o">...</code>

<code class="n">parsed</code><code class="o">.</code><code class="n">show</code><code class="p">(</code><code class="mi">5</code><code class="p">)</code>
<code class="o">...</code>
<code class="o">+-----+-----+------------+------------+------------+------------+.....</code>
<code class="o">|</code> <code class="n">id_1</code><code class="o">|</code> <code class="n">id_2</code><code class="o">|</code><code class="n">cmp_fname_c1</code><code class="o">|</code><code class="n">cmp_fname_c2</code><code class="o">|</code><code class="n">cmp_lname_c1</code><code class="o">|</code><code class="n">cmp_lname_c2</code><code class="o">|.....</code>
<code class="o">+-----+-----+------------+------------+------------+------------+.....</code>
<code class="o">|</code> <code class="mi">3148</code><code class="o">|</code> <code class="mi">8326</code><code class="o">|</code>         <code class="mf">1.0</code><code class="o">|</code>        <code class="n">null</code><code class="o">|</code>         <code class="mf">1.0</code><code class="o">|</code>        <code class="n">null</code><code class="o">|.....</code>
<code class="o">|</code><code class="mi">14055</code><code class="o">|</code><code class="mi">94934</code><code class="o">|</code>         <code class="mf">1.0</code><code class="o">|</code>        <code class="n">null</code><code class="o">|</code>         <code class="mf">1.0</code><code class="o">|</code>        <code class="n">null</code><code class="o">|.....</code>
<code class="o">|</code><code class="mi">33948</code><code class="o">|</code><code class="mi">34740</code><code class="o">|</code>         <code class="mf">1.0</code><code class="o">|</code>        <code class="n">null</code><code class="o">|</code>         <code class="mf">1.0</code><code class="o">|</code>        <code class="n">null</code><code class="o">|.....</code>
<code class="o">|</code>  <code class="mi">946</code><code class="o">|</code><code class="mi">71870</code><code class="o">|</code>         <code class="mf">1.0</code><code class="o">|</code>        <code class="n">null</code><code class="o">|</code>         <code class="mf">1.0</code><code class="o">|</code>        <code class="n">null</code><code class="o">|.....</code>
<code class="o">|</code><code class="mi">64880</code><code class="o">|</code><code class="mi">71676</code><code class="o">|</code>         <code class="mf">1.0</code><code class="o">|</code>        <code class="n">null</code><code class="o">|</code>         <code class="mf">1.0</code><code class="o">|</code>        <code class="n">null</code><code class="o">|.....</code></pre>
<ul>
<li>
<p>The first two fields are integer IDs that represent the patients who were matched in the record.</p>
</li>
<li>
<p>The next nine fields are (possibly missing) numeric values (either doubles or ints) that represent match scores on different fields of the patient records, such as their names, birthdays, and locations. The fields are stored as integers when the only possible values are match (1) or no-match (0), and doubles whenever partial matches are possible.</p>
</li>
<li>
<p>The last field is a boolean value (<code>true</code> or <code>false</code>) indicating whether or not the pair of patient records represented by the line was a match.</p>
</li>
</ul>
<p>Our goal is to come up with a simple classifier that allows us to predict whether a record will be a match based on the values of the match scores for the patient records. Let’s start by getting an idea of the number of records we’re dealing with via the <code>count</code> method:</p>
<pre data-code-language="python" data-type="programlisting"><code class="n">parsed</code><code class="o">.</code><code class="n">count</code><code class="p">()</code>
<code class="o">...</code>
<code class="mi">5749132</code></pre>
<p>This is a relatively small dataset—certainly small enough to fit in memory on one of the nodes in a cluster or even on your local machine if you don’t have a cluster available. Thus far, every time we’ve processed the data, Spark has reopened the file, reparsed the rows, and then performed the action requested, like showing the first few rows of the data or counting the number of records. When we ask another question, Spark will do these same operations, again and again, even if we have filtered the data down to a small number of records or are working with an aggregated version of the original dataset.</p>
<p>This isn’t an optimal use of our compute resources. After the data has been parsed once, we’d like to save the data in its parsed form on the cluster so that we don’t have to reparse it every time we want to ask a new question. <a data-primary="dataframes" data-secondary="cache method on instance" data-type="indexterm" id="idm46507987188352"/><a data-primary="cache method on dataframes" data-type="indexterm" id="idm46507987187504"/>Spark supports this use case by allowing us to signal that a given DataFrame should be cached in memory after it is generated by calling the <code>cache</code> method on the instance. Let’s do that now for the <code>parsed</code> DataFrame:</p>
<pre data-code-language="python" data-type="programlisting"><code class="n">parsed</code><code class="o">.</code><code class="n">cache</code><code class="p">()</code></pre>
<p>Once our data has been cached, the next thing we want to know is the relative fraction of records that were matches versus those that were nonmatches:</p>
<pre data-code-language="python" data-type="programlisting"><code class="kn">from</code> <code class="nn">pyspark.sql.functions</code> <code class="kn">import</code> <code class="n">col</code>

<code class="n">parsed</code><code class="o">.</code><code class="n">groupBy</code><code class="p">(</code><code class="s2">"is_match"</code><code class="p">)</code><code class="o">.</code><code class="n">count</code><code class="p">()</code><code class="o">.</code><code class="n">orderBy</code><code class="p">(</code><code class="n">col</code><code class="p">(</code><code class="s2">"count"</code><code class="p">)</code><code class="o">.</code><code class="n">desc</code><code class="p">())</code><code class="o">.</code><code class="n">show</code><code class="p">()</code>
<code class="o">...</code>
<code class="o">+--------+-------+</code>
<code class="o">|</code><code class="n">is_match</code><code class="o">|</code>  <code class="n">count</code><code class="o">|</code>
<code class="o">+--------+-------+</code>
<code class="o">|</code>   <code class="n">false</code><code class="o">|</code><code class="mi">5728201</code><code class="o">|</code>
<code class="o">|</code>    <code class="n">true</code><code class="o">|</code>  <code class="mi">20931</code><code class="o">|</code>
<code class="o">+--------+-------+</code></pre>
<p>Instead of writing a function to extract the <code>is_match</code> column, we simply pass its name to the <code>groupBy</code> method on the DataFrame, call the <code>count</code> method to, well, count the number of records inside each grouping, sort the resulting data in descending order based on the <code>count</code> column, and then cleanly render the result of the computation in the REPL with <code>show</code>. Under the covers, the Spark engine determines the most efficient way to perform the aggregation and return the results. This illustrates the clean, fast, and expressive way to do data analysis that Spark provides.</p>
<p>Note that there are two ways we can<a data-primary="names" data-secondary="dataframe column names referenced" data-type="indexterm" id="idm46507987104992"/><a data-primary="dataframes" data-secondary="column names referenced" data-type="indexterm" id="idm46507987104048"/> reference the names of the columns in the DataFrame: either as literal strings, like in <code>groupBy("is_match")</code>, or as <code>Column</code> objects by using the <code>col</code> function that we used on the <code>count</code> column. Either approach is valid in most cases, but we needed to use the <code>col</code> function to call the <code>desc</code> method on the resulting <code>count</code> column object.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46507987099888">
<h5>DataFrame Aggregation Functions</h5>
<p>In addition to <code>count</code>, we can also<a data-primary="aggregation functions of dataframes" data-type="indexterm" id="idm46507987054608"/><a data-primary="dataframes" data-secondary="aggregation functions" data-type="indexterm" id="idm46507987054000"/> compute more complex aggregations like sums, mins, maxes, means, and standard deviation using the <code>agg</code> method of the DataFrame API in conjunction with the aggregation functions defined in the <code>pyspark.sql.functions</code> collection. For example, to find the mean and standard deviation of the <code>cmp_sex</code> field in the overall <code>parsed</code> DataFrame, we could type:</p>
<pre data-code-language="python" data-type="programlisting"><code class="kn">from</code> <code class="nn">pyspark.sql.functions</code> <code class="kn">import</code> <code class="n">avg</code><code class="p">,</code> <code class="n">stddev</code>

<code class="n">parsed</code><code class="o">.</code><code class="n">agg</code><code class="p">(</code><code class="n">avg</code><code class="p">(</code><code class="s2">"cmp_sex"</code><code class="p">),</code> <code class="n">stddev</code><code class="p">(</code><code class="s2">"cmp_sex"</code><code class="p">))</code><code class="o">.</code><code class="n">show</code><code class="p">()</code>
<code class="o">+-----------------+--------------------+</code>
<code class="o">|</code>     <code class="n">avg</code><code class="p">(</code><code class="n">cmp_sex</code><code class="p">)</code><code class="o">|</code><code class="n">stddev_samp</code><code class="p">(</code><code class="n">cmp_sex</code><code class="p">)</code><code class="o">|</code>
<code class="o">+-----------------+--------------------+</code>
<code class="o">|</code><code class="mf">0.955001381078048</code><code class="o">|</code>  <code class="mf">0.2073011111689795</code><code class="o">|</code>
<code class="o">+-----------------+--------------------+</code></pre>
<p>Note that by default Spark computes the sample standard deviation; there is also a <code>stddev_pop</code> function for computing the population standard deviation.</p>
</div></aside>
<p>You may have noticed that the functions<a data-primary="SQL for data analysis" data-secondary="dataframes" data-type="indexterm" id="idm46507986986112"/><a data-primary="Spark SQL module" data-secondary="dataframe data analysis" data-type="indexterm" id="idm46507987030544"/> in the DataFrame API are similar to the components of a SQL query. This isn’t a coincidence, and in fact we have the option to treat any DataFrame we create as if it were a database table and to express our questions using familiar and powerful SQL syntax. First, we need to tell the Spark SQL execution engine the name it should associate with the <code>parsed</code> DataFrame, since the name of the variable itself (“parsed”) isn’t available to Spark:</p>
<pre data-code-language="python" data-type="programlisting"><code class="n">parsed</code><code class="o">.</code><code class="n">createOrReplaceTempView</code><code class="p">(</code><code class="s2">"linkage"</code><code class="p">)</code></pre>
<p>Because the <code>parsed</code> DataFrame is available only during the length of this PySpark REPL session, it is a <em>temporary</em> table. <a data-primary="persistent storage" data-secondary="Spark SQL queries" data-type="indexterm" id="idm46507987024304"/><a data-primary="Hive (Apache) and Spark SQL queries" data-type="indexterm" id="idm46507987023360"/><a data-primary="Apache Hive" data-see="Hive" data-type="indexterm" id="idm46507987022720"/>Spark SQL may also be used to query persistent tables in HDFS if we configure Spark to connect to an Apache Hive metastore that tracks the schemas and locations of structured datasets.</p>
<p>Once our temporary table is registered with the Spark SQL engine, we can query it like this:</p>
<pre data-code-language="python" data-type="programlisting"><code class="n">spark</code><code class="o">.</code><code class="n">sql</code><code class="p">(</code><code class="s2">"""</code>
<code class="s2">  SELECT is_match, COUNT(*) cnt</code>
<code class="s2">  FROM linkage</code>
<code class="s2">  GROUP BY is_match</code>
<code class="s2">  ORDER BY cnt DESC</code>
<code class="s2">"""</code><code class="p">)</code><code class="o">.</code><code class="n">show</code><code class="p">()</code>
<code class="o">...</code>
<code class="o">+--------+-------+</code>
<code class="o">|</code><code class="n">is_match</code><code class="o">|</code>    <code class="n">cnt</code><code class="o">|</code>
<code class="o">+--------+-------+</code>
<code class="o">|</code>   <code class="n">false</code><code class="o">|</code><code class="mi">5728201</code><code class="o">|</code>
<code class="o">|</code>    <code class="n">true</code><code class="o">|</code>  <code class="mi">20931</code><code class="o">|</code>
<code class="o">+--------+-------+</code></pre>
<p>You have the option of running Spark either by using an ANSI 2003-compliant version of Spark SQL (the default) or in HiveQL mode by calling the <code>enableHiveSupport</code> method when you create a <code>SparkSession</code> instance via its Builder API.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46507986889888">
<h5>Connecting Spark SQL to Hive</h5>
<p>You can connect to a Hive metastore<a data-primary="Hive (Apache) and Spark SQL queries" data-secondary="connecting Spark SQL to Hive" data-type="indexterm" id="idm46507986888688"/> via a <em>hive-site.xml</em> file, and you can also use HiveQL in queries by calling the <code>enableHiveSupport</code> method on the SparkSession Builder API:</p>
<pre data-code-language="python" data-type="programlisting"><code class="n">spark_session</code> <code class="o">=</code> <code class="n">SparkSession</code><code class="o">.</code><code class="n">builder</code><code class="o">.</code><code class="n">master</code><code class="p">(</code><code class="s2">"local[4]"</code><code class="p">)</code><code class="o">.</code>\
                  <code class="n">enableHiveSupport</code><code class="p">()</code><code class="o">.</code><code class="n">getOrCreate</code><code class="p">()</code></pre>
<p>You can treat any table in the Hive metastore as a dataframe, execute Spark SQL queries against tables defined in the metastore, and persist the output of those queries to the metastore so that they can be queried by other tools, including Hive itself, Apache Impala, or Presto.</p>
</div></aside>
<p>Should you use Spark SQL or the DataFrame API<a data-primary="data analysis" data-secondary="introduction to" data-tertiary="DataFrame API versus Spark SQL" data-type="indexterm" id="idm46507986872688"/><a data-primary="PySpark API" data-secondary="data analysis introduction" data-tertiary="DataFrame API versus Spark SQL" data-type="indexterm" id="idm46507986871600"/><a data-primary="Spark (Apache)" data-secondary="data analysis introduction" data-tertiary="DataFrame API versus Spark SQL" data-type="indexterm" id="idm46507986870448"/><a data-primary="SQL for data analysis" data-secondary="dataframes" data-tertiary="DataFrame API versus Spark SQL" data-type="indexterm" id="idm46507986869296"/><a data-primary="Spark SQL module" data-secondary="dataframe data analysis" data-tertiary="DataFrame API versus Spark SQL" data-type="indexterm" id="idm46507986868112"/> to do your analysis in PySpark? There are pros and cons to each: SQL has the benefit of being broadly familiar and expressive for simple queries. It also lets you query data using JDBC/ODBC connectors from databases such as PostgreSQL or tools such as Tableau. The downside of SQL is that it can be difficult to express complex, multistage analyses in a dynamic, readable, and testable way—all areas where the DataFrame API shines. Throughout the rest of the book, we use both Spark SQL and the DataFrame API, and we leave it as an exercise for the reader to examine the choices we made and translate our computations from one interface to the other.</p>
<p>We can apply functions one by one to our DataFrame to obtain statistics such as count and mean. However, PySpark offers a better way to obtain summary statistics for DataFrames, and that’s what we will cover in the next section.<a data-startref="ch02-anal" data-type="indexterm" id="idm46507986854144"/><a data-startref="ch02-anal2" data-type="indexterm" id="idm46507986853440"/><a data-startref="ch02-anal3" data-type="indexterm" id="idm46507986852768"/><a data-startref="ch02-anal4" data-type="indexterm" id="idm46507986852096"/><a data-startref="ch02-anal5" data-type="indexterm" id="idm46507986851424"/><a data-startref="ch02-anal6" data-type="indexterm" id="idm46507986850752"/><a data-startref="ch02-anal7" data-type="indexterm" id="idm46507986850080"/></p>
</div></section>
<section data-pdf-bookmark="Fast Summary Statistics for DataFrames" data-type="sect1"><div class="sect1" id="idm46507987506320">
<h1>Fast Summary Statistics for DataFrames</h1>
<p>Although there are many kinds of analyses<a data-primary="PySpark API" data-secondary="data analysis introduction" data-tertiary="summary statistics" data-type="indexterm" id="idm46507986847856"/><a data-primary="data analysis" data-secondary="introduction to" data-tertiary="summary statistics" data-type="indexterm" id="idm46507986846592"/><a data-primary="Spark (Apache)" data-secondary="data analysis introduction" data-tertiary="summary statistics" data-type="indexterm" id="idm46507986845376"/><a data-primary="summary statistics from dataframes" data-type="indexterm" id="idm46507986844144"/><a data-primary="statistics" data-secondary="summary statistics from dataframes" data-type="indexterm" id="idm46507986843456"/> that may be expressed equally well in SQL or with the DataFrame API, there are certain common things that we want to be able to do with dataframes that can be tedious to express in SQL. One such analysis that is especially helpful is computing the min, max, mean, and standard deviation of all the non-null values in the numerical columns of a dataframe. <a data-primary="describe function of PySpark" data-type="indexterm" id="idm46507986842368"/>In PySpark, this function has the same name that it does in pandas, <code>describe</code>:</p>
<pre data-code-language="python" data-type="programlisting"><code class="n">summary</code> <code class="o">=</code> <code class="n">parsed</code><code class="o">.</code><code class="n">describe</code><code class="p">()</code>
<code class="o">...</code>
<code class="n">summary</code><code class="o">.</code><code class="n">show</code><code class="p">()</code></pre>
<p>The <code>summary</code> DataFrame has one column for each variable in the <code>parsed</code> DataFrame, along with another column (also named <code>summary</code>) that indicates which metric—<code>count</code>, <code>mean</code>, <code>stddev</code>, <code>min</code>, or <code>max</code>—is present in the rest of the columns in the row. We can use the <code>select</code> method to choose a subset of the columns to make the summary statistics easier to read and compare:</p>
<pre data-code-language="python" data-type="programlisting"><code class="n">summary</code><code class="o">.</code><code class="n">select</code><code class="p">(</code><code class="s2">"summary"</code><code class="p">,</code> <code class="s2">"cmp_fname_c1"</code><code class="p">,</code> <code class="s2">"cmp_fname_c2"</code><code class="p">)</code><code class="o">.</code><code class="n">show</code><code class="p">()</code>
<code class="o">+-------+------------------+------------------+</code>
<code class="o">|</code><code class="n">summary</code><code class="o">|</code>      <code class="n">cmp_fname_c1</code><code class="o">|</code>      <code class="n">cmp_fname_c2</code><code class="o">|</code>
<code class="o">+-------+------------------+------------------+</code>
<code class="o">|</code>  <code class="n">count</code><code class="o">|</code>           <code class="mi">5748125</code><code class="o">|</code>            <code class="mi">103698</code><code class="o">|</code>
<code class="o">|</code>   <code class="n">mean</code><code class="o">|</code><code class="mf">0.7129024704436274</code><code class="o">|</code><code class="mf">0.9000176718903216</code><code class="o">|</code>
<code class="o">|</code> <code class="n">stddev</code><code class="o">|</code><code class="mf">0.3887583596162788</code><code class="o">|</code><code class="mf">0.2713176105782331</code><code class="o">|</code>
<code class="o">|</code>    <code class="nb">min</code><code class="o">|</code>               <code class="mf">0.0</code><code class="o">|</code>               <code class="mf">0.0</code><code class="o">|</code>
<code class="o">|</code>    <code class="nb">max</code><code class="o">|</code>               <code class="mf">1.0</code><code class="o">|</code>               <code class="mf">1.0</code><code class="o">|</code>
<code class="o">+-------+------------------+------------------+</code></pre>
<p>Note the difference in the value of the <code>count</code> variable between <code>cmp_fname_c1</code> and <code>cmp_fname_c2</code>. While almost every record has a non-null value for <code>cmp_fname_c1</code>, less than 2% of the records have a non-null value for <code>cmp_fname_c2</code>. <a data-primary="missing data values" data-secondary="classifiers" data-type="indexterm" id="idm46507986740736"/><a data-primary="classifiers" data-secondary="missing data values" data-type="indexterm" id="idm46507986739728"/>To create a useful classifier, we need to rely on variables that are almost always present in the data—unless the fact that they are missing indicates something meaningful about whether the record matches.</p>
<p>Once we have an overall feel for the distribution of the variables in our data, we want to understand how the values of those variables are correlated with the value of the <code>is_match</code> column. Therefore, our next step is to compute those same summary statistics for just the subsets of the <code>parsed</code> DataFrame that correspond to matches and nonmatches. We can filter DataFrames using either SQL-style <code>where</code> syntax or with <code>Column</code> objects using the DataFrame API and then use <code>describe</code> on the resulting DataFrames:</p>
<pre data-code-language="python" data-type="programlisting"><code class="n">matches</code> <code class="o">=</code> <code class="n">parsed</code><code class="o">.</code><code class="n">where</code><code class="p">(</code><code class="s2">"is_match = true"</code><code class="p">)</code>
<code class="n">match_summary</code> <code class="o">=</code> <code class="n">matches</code><code class="o">.</code><code class="n">describe</code><code class="p">()</code>

<code class="n">misses</code> <code class="o">=</code> <code class="n">parsed</code><code class="o">.</code><code class="n">filter</code><code class="p">(</code><code class="n">col</code><code class="p">(</code><code class="s2">"is_match"</code><code class="p">)</code> <code class="o">==</code> <code class="kc">False</code><code class="p">)</code>
<code class="n">miss_summary</code> <code class="o">=</code> <code class="n">misses</code><code class="o">.</code><code class="n">describe</code><code class="p">()</code></pre>
<p>The logic inside the string we pass to the <code>where</code> function can include statements that would be valid inside a <code>WHERE</code> clause in Spark SQL. For the filtering condition that uses the DataFrame API, we use the <code>==</code> operator on the <code>is_match</code> column object to check for equality with the boolean object <code>False</code>, because that is just Python, not SQL.  Note that the <code>where</code> function is an alias for the <code>filter</code> function; we could have reversed the <code>where</code> and <code>filter</code> calls in the above snippet and everything would have worked the same way.</p>
<p>We can now start to compare our <code>match_summary</code> and <code>miss_summary</code> DataFrames to see how the distribution of the variables changes depending on whether the record is a match or a miss. Although this is a relatively small dataset, doing this comparison is still somewhat tedious—what we really want is to transpose the <code>match_summary</code> and <code>miss_summary</code> DataFrames so that the rows and columns are swapped, which would allow us to join the transposed DataFrames together by variable and analyze the summary statistics, a practice that most data scientists know as “pivoting” or “reshaping” a dataset. In the next section, we’ll show you how to perform these transformations.</p>
</div></section>
<section data-pdf-bookmark="Pivoting and Reshaping DataFrames" data-type="sect1"><div class="sect1" id="idm46507986849152">
<h1>Pivoting and Reshaping DataFrames</h1>
<p>We can transpose the DataFrames entirely<a data-primary="data analysis" data-secondary="introduction to" data-tertiary="pivoting and reshaping dataframes" data-type="indexterm" id="ch02-pivot"/><a data-primary="PySpark API" data-secondary="data analysis introduction" data-tertiary="pivoting and reshaping dataframes" data-type="indexterm" id="ch02-pivot2"/><a data-primary="Spark (Apache)" data-secondary="data analysis introduction" data-tertiary="pivoting and reshaping dataframes" data-type="indexterm" id="ch02-pivot3"/><a data-primary="dataframes" data-secondary="pivoting and reshaping" data-type="indexterm" id="ch02-pivot4"/><a data-primary="pandas" data-secondary="Spark dataframes converted to and from" data-type="indexterm" id="ch02-pivot5"/><a data-primary="converting Spark dataframes to and from pandas" data-type="indexterm" id="ch02-pivot6"/> using functions provided by PySpark. However, there is another way to perform this task. PySpark allows conversion between Spark and pandas DataFrames. We will convert the DataFrames in question into pandas DataFrames, reshape them, and convert them back to Spark DataFrames. We can safely do this because of the small size of the <code>summary</code>, <code>match_summary</code>, and <code>miss_summary</code> DataFrames since pandas DataFrames reside in memory. In upcoming chapters, we will rely on Spark operations for such transformations on larger datasets.</p>
<div data-type="note" epub:type="note">
<p>Conversion to/from pandas DataFrames<a data-primary="Arrow (Apache)" data-type="indexterm" id="idm46507986594576"/><a data-primary="Apache Arrow" data-type="indexterm" id="idm46507986593872"/><a data-primary="PyArrow library" data-type="indexterm" id="idm46507986593200"/><a data-primary="Java" data-secondary="data transfer between JVM and Python processes" data-type="indexterm" id="idm46507986592528"/><a data-primary="Python" data-secondary="data transfer between JVM and Python processes" data-type="indexterm" id="idm46507986591552"/><a data-primary="JVM data transfer with Python processes" data-type="indexterm" id="idm46507986590576"/> is possible because of the Apache Arrow project, which allows efficient data transfer between JVM and Python processes. The PyArrow library was installed as a dependency of the Spark SQL module when we installed <code>pyspark[sql]</code> using pip.</p>
</div>
<p>Let’s convert <code>summary</code> into a pandas DataFrame:</p>
<pre data-code-language="python" data-type="programlisting"><code class="n">summary_p</code> <code class="o">=</code> <code class="n">summary</code><code class="o">.</code><code class="n">toPandas</code><code class="p">()</code></pre>
<p>We can now use pandas functions on the <code>summary_p</code> DataFrame:</p>
<pre data-code-language="python" data-type="programlisting"><code class="n">summary_p</code><code class="o">.</code><code class="n">head</code><code class="p">()</code>
<code class="o">...</code>
<code class="n">summary_p</code><code class="o">.</code><code class="n">shape</code>
<code class="o">...</code>
<code class="p">(</code><code class="mi">5</code><code class="p">,</code><code class="mi">12</code><code class="p">)</code></pre>
<p>We can now perform a transpose operation to swap rows and columns using familiar pandas methods on the DataFrame:</p>
<pre data-code-language="python" data-type="programlisting"><code class="n">summary_p</code> <code class="o">=</code> <code class="n">summary_p</code><code class="o">.</code><code class="n">set_index</code><code class="p">(</code><code class="s1">'summary'</code><code class="p">)</code><code class="o">.</code><code class="n">transpose</code><code class="p">()</code><code class="o">.</code><code class="n">reset_index</code><code class="p">()</code>
<code class="o">...</code>
<code class="n">summary_p</code> <code class="o">=</code> <code class="n">summary_p</code><code class="o">.</code><code class="n">rename</code><code class="p">(</code><code class="n">columns</code><code class="o">=</code><code class="p">{</code><code class="s1">'index'</code><code class="p">:</code><code class="s1">'field'</code><code class="p">})</code>
<code class="o">...</code>
<code class="n">summary_p</code> <code class="o">=</code> <code class="n">summary_p</code><code class="o">.</code><code class="n">rename_axis</code><code class="p">(</code><code class="kc">None</code><code class="p">,</code> <code class="n">axis</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>
<code class="o">...</code>
<code class="n">summary_p</code><code class="o">.</code><code class="n">shape</code>
<code class="o">...</code>
<code class="p">(</code><code class="mi">11</code><code class="p">,</code><code class="mi">6</code><code class="p">)</code></pre>
<p>We have successfully transposed the <code>summary_p</code> pandas DataFrame. Convert it into a Spark DataFrame using SparkSession’s <code>createDataFrame</code> method:</p>
<pre data-code-language="python" data-type="programlisting"><code class="n">summaryT</code> <code class="o">=</code> <code class="n">spark</code><code class="o">.</code><code class="n">createDataFrame</code><code class="p">(</code><code class="n">summary_p</code><code class="p">)</code>
<code class="o">...</code>
<code class="n">summaryT</code><code class="o">.</code><code class="n">show</code><code class="p">()</code>
<code class="o">...</code>
<code class="o">+------------+-------+-------------------+-------------------+---+------+</code>
<code class="o">|</code>       <code class="n">field</code><code class="o">|</code>  <code class="n">count</code><code class="o">|</code>               <code class="n">mean</code><code class="o">|</code>             <code class="n">stddev</code><code class="o">|</code><code class="nb">min</code><code class="o">|</code>   <code class="nb">max</code><code class="o">|</code>
<code class="o">+------------+-------+-------------------+-------------------+---+------+</code>
<code class="o">|</code>        <code class="n">id_1</code><code class="o">|</code><code class="mi">5749132</code><code class="o">|</code>  <code class="mf">33324.48559643438</code><code class="o">|</code> <code class="mf">23659.859374488064</code><code class="o">|</code>  <code class="mi">1</code><code class="o">|</code> <code class="mi">99980</code><code class="o">|</code>
<code class="o">|</code>        <code class="n">id_2</code><code class="o">|</code><code class="mi">5749132</code><code class="o">|</code>  <code class="mf">66587.43558331935</code><code class="o">|</code> <code class="mf">23620.487613269695</code><code class="o">|</code>  <code class="mi">6</code><code class="o">|</code><code class="mi">100000</code><code class="o">|</code>
<code class="o">|</code><code class="n">cmp_fname_c1</code><code class="o">|</code><code class="mi">5748125</code><code class="o">|</code> <code class="mf">0.7129024704437266</code><code class="o">|</code><code class="mf">0.38875835961628014</code><code class="o">|</code><code class="mf">0.0</code><code class="o">|</code>   <code class="mf">1.0</code><code class="o">|</code>
<code class="o">|</code><code class="n">cmp_fname_c2</code><code class="o">|</code> <code class="mi">103698</code><code class="o">|</code> <code class="mf">0.9000176718903189</code><code class="o">|</code> <code class="mf">0.2713176105782334</code><code class="o">|</code><code class="mf">0.0</code><code class="o">|</code>   <code class="mf">1.0</code><code class="o">|</code>
<code class="o">|</code><code class="n">cmp_lname_c1</code><code class="o">|</code><code class="mi">5749132</code><code class="o">|</code> <code class="mf">0.3156278193080383</code><code class="o">|</code> <code class="mf">0.3342336339615828</code><code class="o">|</code><code class="mf">0.0</code><code class="o">|</code>   <code class="mf">1.0</code><code class="o">|</code>
<code class="o">|</code><code class="n">cmp_lname_c2</code><code class="o">|</code>   <code class="mi">2464</code><code class="o">|</code> <code class="mf">0.3184128315317443</code><code class="o">|</code><code class="mf">0.36856706620066537</code><code class="o">|</code><code class="mf">0.0</code><code class="o">|</code>   <code class="mf">1.0</code><code class="o">|</code>
<code class="o">|</code>     <code class="n">cmp_sex</code><code class="o">|</code><code class="mi">5749132</code><code class="o">|</code>  <code class="mf">0.955001381078048</code><code class="o">|</code><code class="mf">0.20730111116897781</code><code class="o">|</code>  <code class="mi">0</code><code class="o">|</code>     <code class="mi">1</code><code class="o">|</code>
<code class="o">|</code>      <code class="n">cmp_bd</code><code class="o">|</code><code class="mi">5748337</code><code class="o">|</code><code class="mf">0.22446526708507172</code><code class="o">|</code><code class="mf">0.41722972238462636</code><code class="o">|</code>  <code class="mi">0</code><code class="o">|</code>     <code class="mi">1</code><code class="o">|</code>
<code class="o">|</code>      <code class="n">cmp_bm</code><code class="o">|</code><code class="mi">5748337</code><code class="o">|</code><code class="mf">0.48885529849763504</code><code class="o">|</code> <code class="mf">0.4998758236779031</code><code class="o">|</code>  <code class="mi">0</code><code class="o">|</code>     <code class="mi">1</code><code class="o">|</code>
<code class="o">|</code>      <code class="n">cmp_by</code><code class="o">|</code><code class="mi">5748337</code><code class="o">|</code> <code class="mf">0.2227485966810923</code><code class="o">|</code> <code class="mf">0.4160909629831756</code><code class="o">|</code>  <code class="mi">0</code><code class="o">|</code>     <code class="mi">1</code><code class="o">|</code>
<code class="o">|</code>     <code class="n">cmp_plz</code><code class="o">|</code><code class="mi">5736289</code><code class="o">|</code><code class="mf">0.00552866147434343</code><code class="o">|</code><code class="mf">0.07414914925420046</code><code class="o">|</code>  <code class="mi">0</code><code class="o">|</code>     <code class="mi">1</code><code class="o">|</code>
<code class="o">+------------+-------+-------------------+-------------------+---+------+</code></pre>
<p>We are not done yet. Print the schema of the <code>summaryT</code> DataFrame:<a data-primary="schemas of dataframes" data-secondary="Machine Learning Repository dataset" data-type="indexterm" id="idm46507986366416"/><a data-primary="dataframes" data-secondary="schemas" data-tertiary="Machine Learning Repository dataset" data-type="indexterm" id="idm46507986365568"/></p>
<pre data-code-language="python" data-type="programlisting"><code class="n">summaryT</code><code class="o">.</code><code class="n">printSchema</code><code class="p">()</code>
<code class="o">...</code>
<code class="n">root</code>
 <code class="o">|--</code> <code class="n">field</code><code class="p">:</code> <code class="n">string</code> <code class="p">(</code><code class="n">nullable</code> <code class="o">=</code> <code class="n">true</code><code class="p">)</code>
 <code class="o">|--</code> <code class="n">count</code><code class="p">:</code> <code class="n">string</code> <code class="p">(</code><code class="n">nullable</code> <code class="o">=</code> <code class="n">true</code><code class="p">)</code>
 <code class="o">|--</code> <code class="n">mean</code><code class="p">:</code> <code class="n">string</code> <code class="p">(</code><code class="n">nullable</code> <code class="o">=</code> <code class="n">true</code><code class="p">)</code>
 <code class="o">|--</code> <code class="n">stddev</code><code class="p">:</code> <code class="n">string</code> <code class="p">(</code><code class="n">nullable</code> <code class="o">=</code> <code class="n">true</code><code class="p">)</code>
 <code class="o">|--</code> <code class="nb">min</code><code class="p">:</code> <code class="n">string</code> <code class="p">(</code><code class="n">nullable</code> <code class="o">=</code> <code class="n">true</code><code class="p">)</code>
 <code class="o">|--</code> <code class="nb">max</code><code class="p">:</code> <code class="n">string</code> <code class="p">(</code><code class="n">nullable</code> <code class="o">=</code> <code class="n">true</code><code class="p">)</code></pre>
<p>In the summary schema, as obtained from the <code>describe</code> method, every field is treated as a string. <a data-primary="numeric data" data-secondary="converting strings to" data-type="indexterm" id="idm46507986114192"/><a data-primary="string data" data-secondary="converting to numeric" data-type="indexterm" id="idm46507986082576"/><a data-primary="converting Spark dataframes to and from pandas" data-secondary="string data to numeric" data-type="indexterm" id="idm46507986081728"/>Since we want to analyze the summary statistics as numbers, we’ll need to convert the values from strings to doubles:</p>
<pre data-code-language="python" data-type="programlisting"><code class="kn">from</code> <code class="nn">pyspark.sql.types</code> <code class="kn">import</code> <code class="n">DoubleType</code>
<code class="k">for</code> <code class="n">c</code> <code class="ow">in</code> <code class="n">summaryT</code><code class="o">.</code><code class="n">columns</code><code class="p">:</code>
  <code class="k">if</code> <code class="n">c</code> <code class="o">==</code> <code class="s1">'field'</code><code class="p">:</code>
    <code class="k">continue</code>
  <code class="n">summaryT</code> <code class="o">=</code> <code class="n">summaryT</code><code class="o">.</code><code class="n">withColumn</code><code class="p">(</code><code class="n">c</code><code class="p">,</code> <code class="n">summaryT</code><code class="p">[</code><code class="n">c</code><code class="p">]</code><code class="o">.</code><code class="n">cast</code><code class="p">(</code><code class="n">DoubleType</code><code class="p">()))</code>
<code class="o">...</code>
<code class="n">summaryT</code><code class="o">.</code><code class="n">printSchema</code><code class="p">()</code>
<code class="o">...</code>
<code class="n">root</code>
 <code class="o">|--</code> <code class="n">field</code><code class="p">:</code> <code class="n">string</code> <code class="p">(</code><code class="n">nullable</code> <code class="o">=</code> <code class="n">true</code><code class="p">)</code>
 <code class="o">|--</code> <code class="n">count</code><code class="p">:</code> <code class="n">double</code> <code class="p">(</code><code class="n">nullable</code> <code class="o">=</code> <code class="n">true</code><code class="p">)</code>
 <code class="o">|--</code> <code class="n">mean</code><code class="p">:</code> <code class="n">double</code> <code class="p">(</code><code class="n">nullable</code> <code class="o">=</code> <code class="n">true</code><code class="p">)</code>
 <code class="o">|--</code> <code class="n">stddev</code><code class="p">:</code> <code class="n">double</code> <code class="p">(</code><code class="n">nullable</code> <code class="o">=</code> <code class="n">true</code><code class="p">)</code>
 <code class="o">|--</code> <code class="nb">min</code><code class="p">:</code> <code class="n">double</code> <code class="p">(</code><code class="n">nullable</code> <code class="o">=</code> <code class="n">true</code><code class="p">)</code>
 <code class="o">|--</code> <code class="nb">max</code><code class="p">:</code> <code class="n">double</code> <code class="p">(</code><code class="n">nullable</code> <code class="o">=</code> <code class="n">true</code><code class="p">)</code></pre>
<p>Now that we have figured out how to transpose a summary DataFrame, let’s implement our logic into a function that we can reuse on the <code>match_summary</code> and <code>m⁠i⁠s⁠s⁠_​s⁠u⁠m⁠m⁠a⁠r⁠y</code> DataFrames:</p>
<pre data-code-language="python" data-type="programlisting"><code class="kn">from</code> <code class="nn">pyspark.sql</code> <code class="kn">import</code> <code class="n">DataFrame</code>
<code class="kn">from</code> <code class="nn">pyspark.sql.types</code> <code class="kn">import</code> <code class="n">DoubleType</code>

<code class="k">def</code> <code class="nf">pivot_summary</code><code class="p">(</code><code class="n">desc</code><code class="p">):</code>
  <code class="c1"># convert to pandas dataframe</code>
  <code class="n">desc_p</code> <code class="o">=</code> <code class="n">desc</code><code class="o">.</code><code class="n">toPandas</code><code class="p">()</code>
  <code class="c1"># transpose</code>
  <code class="n">desc_p</code> <code class="o">=</code> <code class="n">desc_p</code><code class="o">.</code><code class="n">set_index</code><code class="p">(</code><code class="s1">'summary'</code><code class="p">)</code><code class="o">.</code><code class="n">transpose</code><code class="p">()</code><code class="o">.</code><code class="n">reset_index</code><code class="p">()</code>
  <code class="n">desc_p</code> <code class="o">=</code> <code class="n">desc_p</code><code class="o">.</code><code class="n">rename</code><code class="p">(</code><code class="n">columns</code><code class="o">=</code><code class="p">{</code><code class="s1">'index'</code><code class="p">:</code><code class="s1">'field'</code><code class="p">})</code>
  <code class="n">desc_p</code> <code class="o">=</code> <code class="n">desc_p</code><code class="o">.</code><code class="n">rename_axis</code><code class="p">(</code><code class="kc">None</code><code class="p">,</code> <code class="n">axis</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>
  <code class="c1"># convert to Spark dataframe</code>
  <code class="n">descT</code> <code class="o">=</code> <code class="n">spark</code><code class="o">.</code><code class="n">createDataFrame</code><code class="p">(</code><code class="n">desc_p</code><code class="p">)</code>
  <code class="c1"># convert metric columns to double from string</code>
  <code class="k">for</code> <code class="n">c</code> <code class="ow">in</code> <code class="n">descT</code><code class="o">.</code><code class="n">columns</code><code class="p">:</code>
    <code class="k">if</code> <code class="n">c</code> <code class="o">==</code> <code class="s1">'field'</code><code class="p">:</code>
      <code class="k">continue</code>
    <code class="k">else</code><code class="p">:</code>
      <code class="n">descT</code> <code class="o">=</code> <code class="n">descT</code><code class="o">.</code><code class="n">withColumn</code><code class="p">(</code><code class="n">c</code><code class="p">,</code> <code class="n">descT</code><code class="p">[</code><code class="n">c</code><code class="p">]</code><code class="o">.</code><code class="n">cast</code><code class="p">(</code><code class="n">DoubleType</code><code class="p">()))</code>
  <code class="k">return</code> <code class="n">descT</code></pre>
<p>Now in your Spark shell, use the <code>pivot_summary</code> function on the <code>match_summary</code> and <code>miss_summary</code> DataFrames:</p>
<pre data-code-language="python" data-type="programlisting"><code class="n">match_summaryT</code> <code class="o">=</code> <code class="n">pivot_summary</code><code class="p">(</code><code class="n">match_summary</code><code class="p">)</code>
<code class="n">miss_summaryT</code> <code class="o">=</code> <code class="n">pivot_summary</code><code class="p">(</code><code class="n">miss_summary</code><code class="p">)</code></pre>
<p>Now that we have successfully transposed the summary DataFrames, we can join and compare them. That’s what we will do in the next section. Further, we will also select desirable features for building our model.<a data-startref="ch02-pivot" data-type="indexterm" id="idm46507985764752"/><a data-startref="ch02-pivot2" data-type="indexterm" id="idm46507985764144"/><a data-startref="ch02-pivot3" data-type="indexterm" id="idm46507985763536"/><a data-startref="ch02-pivot4" data-type="indexterm" id="idm46507985762864"/><a data-startref="ch02-pivot5" data-type="indexterm" id="idm46507985762192"/><a data-startref="ch02-pivot6" data-type="indexterm" id="idm46507985761520"/></p>
</div></section>
<section data-pdf-bookmark="Joining DataFrames and Selecting Features" data-type="sect1"><div class="sect1" id="idm46507986605728">
<h1>Joining DataFrames and Selecting Features</h1>
<p>So far, we have used Spark SQL and the DataFrame API <a data-primary="data analysis" data-secondary="introduction to" data-tertiary="joining dataframes" data-type="indexterm" id="idm46507985759472"/><a data-primary="PySpark API" data-secondary="data analysis introduction" data-tertiary="joining dataframes" data-type="indexterm" id="idm46507985758224"/><a data-primary="Spark (Apache)" data-secondary="data analysis introduction" data-tertiary="joining dataframes" data-type="indexterm" id="idm46507985757040"/><a data-primary="dataframes" data-secondary="joining" data-type="indexterm" id="idm46507985729424"/><a data-primary="join function for dataframes" data-type="indexterm" id="idm46507985728576"/><a data-primary="SQL for data analysis" data-secondary="joining dataframes" data-type="indexterm" id="idm46507985727968"/><a data-primary="Spark SQL module" data-secondary="joining dataframes" data-type="indexterm" id="idm46507985727088"/>only to filter and aggregate the records from a dataset, but we can also use these tools to perform joins (inner, left outer, right outer, or full outer) on DataFrames. Although the DataFrame API includes a <code>join</code> function, it’s often easier to express these joins using Spark SQL, especially when the tables we are joining have a large number of column names in common and we want to be able to clearly indicate which column we are referring to in our select expressions. Let’s create temporary views for the <code>match_summaryT</code> and <code>miss_summaryT</code> DataFrames, join them on the <code>field</code> column, and compute some simple summary statistics on the resulting rows:</p>
<pre data-code-language="python" data-type="programlisting"><code class="n">match_summaryT</code><code class="o">.</code><code class="n">createOrReplaceTempView</code><code class="p">(</code><code class="s2">"match_desc"</code><code class="p">)</code>
<code class="n">miss_summaryT</code><code class="o">.</code><code class="n">createOrReplaceTempView</code><code class="p">(</code><code class="s2">"miss_desc"</code><code class="p">)</code>
<code class="n">spark</code><code class="o">.</code><code class="n">sql</code><code class="p">(</code><code class="s2">"""</code>
<code class="s2">  SELECT a.field, a.count + b.count total, a.mean - b.mean delta</code>
<code class="s2">  FROM match_desc a INNER JOIN miss_desc b ON a.field = b.field</code>
<code class="s2">  WHERE a.field NOT IN ("id_1", "id_2")</code>
<code class="s2">  ORDER BY delta DESC, total DESC</code>
<code class="s2">"""</code><code class="p">)</code><code class="o">.</code><code class="n">show</code><code class="p">()</code>
<code class="o">...</code>
<code class="o">+------------+---------+--------------------+</code>
<code class="o">|</code>       <code class="n">field</code><code class="o">|</code>    <code class="n">total</code><code class="o">|</code>               <code class="n">delta</code><code class="o">|</code>
<code class="o">+------------+---------+--------------------+</code>
<code class="o">|</code>     <code class="n">cmp_plz</code><code class="o">|</code><code class="mf">5736289.0</code><code class="o">|</code>  <code class="mf">0.9563812499852176</code><code class="o">|</code>
<code class="o">|</code><code class="n">cmp_lname_c2</code><code class="o">|</code>   <code class="mf">2464.0</code><code class="o">|</code>  <code class="mf">0.8064147192926264</code><code class="o">|</code>
<code class="o">|</code>      <code class="n">cmp_by</code><code class="o">|</code><code class="mf">5748337.0</code><code class="o">|</code>  <code class="mf">0.7762059675300512</code><code class="o">|</code>
<code class="o">|</code>      <code class="n">cmp_bd</code><code class="o">|</code><code class="mf">5748337.0</code><code class="o">|</code>   <code class="mf">0.775442311783404</code><code class="o">|</code>
<code class="o">|</code><code class="n">cmp_lname_c1</code><code class="o">|</code><code class="mf">5749132.0</code><code class="o">|</code>  <code class="mf">0.6838772482590526</code><code class="o">|</code>
<code class="o">|</code>      <code class="n">cmp_bm</code><code class="o">|</code><code class="mf">5748337.0</code><code class="o">|</code>  <code class="mf">0.5109496938298685</code><code class="o">|</code>
<code class="o">|</code><code class="n">cmp_fname_c1</code><code class="o">|</code><code class="mf">5748125.0</code><code class="o">|</code>  <code class="mf">0.2854529057460786</code><code class="o">|</code>
<code class="o">|</code><code class="n">cmp_fname_c2</code><code class="o">|</code> <code class="mf">103698.0</code><code class="o">|</code> <code class="mf">0.09104268062280008</code><code class="o">|</code>
<code class="o">|</code>     <code class="n">cmp_sex</code><code class="o">|</code><code class="mf">5749132.0</code><code class="o">|</code><code class="mf">0.032408185250332844</code><code class="o">|</code>
<code class="o">+------------+---------+--------------------+</code></pre>
<p>A good feature has two properties: it tends to have significantly different values for matches and nonmatches (so the difference between the means will be large), and it occurs often enough in the data that we can rely on it to be regularly available for any pair of records. By this measure, <code>cmp_fname_c2</code> isn’t very useful because it’s missing a lot of the time, and the difference in the mean value for matches and nonmatches is relatively small—0.09, for a score that ranges from 0 to 1. The <code>cmp_sex</code> feature also isn’t particularly helpful because even though it’s available for any pair of records, the difference in means is just 0.03.</p>
<p>Features <code>cmp_plz</code> and <code>cmp_by</code>, on the other hand, are excellent. They almost always occur for any pair of records, and there is a very large difference in the mean values (more than 0.77 for both features). Features <code>cmp_bd</code>, <code>cmp_lname_c1</code>, and <code>cmp_bm</code> also seem beneficial: they are generally available in the dataset, and the difference in mean values for matches and nonmatches is <span class="keep-together">substantial.</span></p>
<p>Features <code>cmp_fname_c1</code> and <code>cmp_lname_c2</code> are more of a mixed bag: <code>cmp_fname_c1</code> doesn’t discriminate all that well (the difference in the means is only 0.28) even though it’s usually available for a pair of records, whereas <code>cmp_lname_c2</code> has a large difference in the means, but it’s almost always missing. It’s not quite obvious under what circumstances we should include these features in our model based on this data.</p>
<p>For now, we’re going to use a simple scoring model that ranks the similarity of pairs of records based on the sums of the values of the obviously good features: <code>cmp_plz</code>, <code>cmp_by</code>, <code>cmp_bd</code>, <code>cmp_lname_c1</code>, and <code>cmp_bm</code>. For the few records where the values of these features are missing, we’ll use 0 in place of the <code>null</code> value in our sum. We can get a rough feel for the performance of our simple model by creating a dataframe of the computed scores and the value of the <code>is_match</code> column and evaluating how well the score discriminates between matches and nonmatches at various thresholds.</p>
</div></section>
<section data-pdf-bookmark="Scoring and Model Evaluation" data-type="sect1"><div class="sect1" id="idm46507985564272">
<h1>Scoring and Model Evaluation</h1>
<p>For our scoring function, we are going to<a data-primary="data analysis" data-secondary="introduction to" data-tertiary="scoring and model evaluation" data-type="indexterm" id="ch02-scor"/><a data-primary="PySpark API" data-secondary="data analysis introduction" data-tertiary="scoring and model evaluation" data-type="indexterm" id="ch02-scor2"/><a data-primary="Spark (Apache)" data-secondary="data analysis introduction" data-tertiary="scoring and model evaluation" data-type="indexterm" id="ch02-scor3"/><a data-primary="scoring and model evaluation" data-type="indexterm" id="ch02-scor4"/><a data-primary="models" data-secondary="scoring and model evaluation" data-type="indexterm" id="ch02-scor5"/><a data-primary="expr function" data-type="indexterm" id="ch02-scor6"/> sum up the value of five fields (<code>cmp_lname_c1</code>, <code>cmp_plz</code>, <code>cmp_by</code>, <code>cmp_bd</code>, and <code>cmp_bm</code>). We will use <code>expr</code> from <code>pyspark.sql.functions</code> for doing this. The <code>expr</code> function parses an input expression string into the column that it represents. This string can even involve multiple columns.</p>
<p>Let’s create the required expression string:</p>
<pre data-code-language="python" data-type="programlisting"><code class="n">good_features</code> <code class="o">=</code> <code class="p">[</code><code class="s2">"cmp_lname_c1"</code><code class="p">,</code> <code class="s2">"cmp_plz"</code><code class="p">,</code> <code class="s2">"cmp_by"</code><code class="p">,</code> <code class="s2">"cmp_bd"</code><code class="p">,</code> <code class="s2">"cmp_bm"</code><code class="p">]</code>
<code class="o">...</code>
<code class="n">sum_expression</code> <code class="o">=</code> <code class="s2">" + "</code><code class="o">.</code><code class="n">join</code><code class="p">(</code><code class="n">good_features</code><code class="p">)</code>
<code class="o">...</code>
<code class="n">sum_expression</code>
<code class="o">...</code>
<code class="s1">'cmp_lname_c1 + cmp_plz + cmp_by + cmp_bd + cmp_bm'</code></pre>
<p>We can now use the <code>sum_expression</code> string for calculating the score. When summing up the values, we will account for and replace null values with 0 using DataFrame’s <code>fillna</code> method:</p>
<pre data-code-language="python" data-type="programlisting"><code class="kn">from</code> <code class="nn">pyspark.sql.functions</code> <code class="kn">import</code> <code class="n">expr</code>
<code class="n">scored</code> <code class="o">=</code> <code class="n">parsed</code><code class="o">.</code><code class="n">fillna</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="n">subset</code><code class="o">=</code><code class="n">good_features</code><code class="p">)</code><code class="o">.</code>\
                <code class="n">withColumn</code><code class="p">(</code><code class="s1">'score'</code><code class="p">,</code> <code class="n">expr</code><code class="p">(</code><code class="n">sum_expression</code><code class="p">))</code><code class="o">.</code>\
                <code class="n">select</code><code class="p">(</code><code class="s1">'score'</code><code class="p">,</code> <code class="s1">'is_match'</code><code class="p">)</code>
<code class="o">...</code>
<code class="n">scored</code><code class="o">.</code><code class="n">show</code><code class="p">()</code>
<code class="o">...</code>
<code class="o">+-----+--------+</code>
<code class="o">|</code><code class="n">score</code><code class="o">|</code><code class="n">is_match</code><code class="o">|</code>
<code class="o">+-----+--------+</code>
<code class="o">|</code>  <code class="mf">5.0</code><code class="o">|</code>    <code class="n">true</code><code class="o">|</code>
<code class="o">|</code>  <code class="mf">5.0</code><code class="o">|</code>    <code class="n">true</code><code class="o">|</code>
<code class="o">|</code>  <code class="mf">5.0</code><code class="o">|</code>    <code class="n">true</code><code class="o">|</code>
<code class="o">|</code>  <code class="mf">5.0</code><code class="o">|</code>    <code class="n">true</code><code class="o">|</code>
<code class="o">|</code>  <code class="mf">5.0</code><code class="o">|</code>    <code class="n">true</code><code class="o">|</code>
<code class="o">|</code>  <code class="mf">5.0</code><code class="o">|</code>    <code class="n">true</code><code class="o">|</code>
<code class="o">|</code>  <code class="mf">4.0</code><code class="o">|</code>    <code class="n">true</code><code class="o">|</code>
<code class="o">...</code></pre>
<p>The final step in creating our scoring function is to decide what threshold the score must exceed in order for us to predict that the two records represent a match. <a data-primary="false negatives" data-type="indexterm" id="idm46507985463792"/><a data-primary="false positives" data-type="indexterm" id="idm46507985463184"/>If we set the threshold too high, then we will incorrectly mark a matching record as a miss (called the <em>false-negative</em> rate), whereas if we set the threshold too low, we will incorrectly label misses as matches (the <em>false-positive</em> rate). For any nontrivial problem, we always have to trade some false positives for some false negatives, and the question of what the threshold value should be usually comes down to the relative cost of the two kinds of errors in the situation to which the model is being applied.</p>
<p>To help us choose a threshold, it’s helpful to<a data-primary="contingency tables" data-type="indexterm" id="idm46507985394032"/> create a <em>contingency table</em> (which is sometimes called a <em>cross tabulation</em>, or <em>crosstab</em>) that counts the number of records whose scores fall above/below the threshold value crossed with the number of records in each of those categories that were/were not matches. Since we don’t know what threshold value we’re going to use yet, let’s write a function that takes the <code>scored</code> DataFrame and the choice of threshold as parameters and computes the crosstabs using the DataFrame API:</p>
<pre data-code-language="python" data-type="programlisting"><code class="k">def</code> <code class="nf">crossTabs</code><code class="p">(</code><code class="n">scored</code><code class="p">:</code> <code class="n">DataFrame</code><code class="p">,</code> <code class="n">t</code><code class="p">:</code> <code class="n">DoubleType</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="n">DataFrame</code><code class="p">:</code>
  <code class="k">return</code>  <code class="n">scored</code><code class="o">.</code><code class="n">selectExpr</code><code class="p">(</code><code class="sa">f</code><code class="s2">"score &gt;= </code><code class="si">{</code><code class="n">t</code><code class="si">}</code><code class="s2"> as above"</code><code class="p">,</code> <code class="s2">"is_match"</code><code class="p">)</code><code class="o">.</code>\
          <code class="n">groupBy</code><code class="p">(</code><code class="s2">"above"</code><code class="p">)</code><code class="o">.</code><code class="n">pivot</code><code class="p">(</code><code class="s2">"is_match"</code><code class="p">,</code> <code class="p">(</code><code class="s2">"true"</code><code class="p">,</code> <code class="s2">"false"</code><code class="p">))</code><code class="o">.</code>\
          <code class="n">count</code><code class="p">()</code></pre>
<p>Note that we are including the <code>selectExpr</code> method of the DataFrame API to dynamically determine the value of the field named <code>above</code> based on the value of the <code>t</code> argument using Python’s f-string formatting syntax, which allows us to substitute variables by name if we preface the string literal with the letter <code>f</code> (yet another handy bit of Scala implicit magic). Once the <code>above</code> field is defined, we create the crosstab with a standard combination of the <code>groupBy</code>, <code>pivot</code>, and <code>count</code> methods that we used before.</p>
<p>By applying a high threshold value of 4.0, meaning that the average of the five features is 0.8, we can filter out almost all of the nonmatches while keeping over 90% of the matches:</p>
<pre data-code-language="python" data-type="programlisting"><code class="n">crossTabs</code><code class="p">(</code><code class="n">scored</code><code class="p">,</code> <code class="mf">4.0</code><code class="p">)</code><code class="o">.</code><code class="n">show</code><code class="p">()</code>
<code class="o">...</code>
<code class="o">+-----+-----+-------+</code>
<code class="o">|</code><code class="n">above</code><code class="o">|</code> <code class="n">true</code><code class="o">|</code>  <code class="n">false</code><code class="o">|</code>
<code class="o">+-----+-----+-------+</code>
<code class="o">|</code> <code class="n">true</code><code class="o">|</code><code class="mi">20871</code><code class="o">|</code>    <code class="mi">637</code><code class="o">|</code>
<code class="o">|</code><code class="n">false</code><code class="o">|</code>   <code class="mi">60</code><code class="o">|</code><code class="mi">5727564</code><code class="o">|</code>
<code class="o">+-----+-----+-------+</code></pre>
<p>By applying a lower threshold of 2.0, we can ensure that we capture <em>all</em> of the known matching records, but at a substantial cost in terms of false positive (top-right cell):</p>
<pre data-code-language="python" data-type="programlisting"><code class="n">crossTabs</code><code class="p">(</code><code class="n">scored</code><code class="p">,</code> <code class="mf">2.0</code><code class="p">)</code><code class="o">.</code><code class="n">show</code><code class="p">()</code>
<code class="o">...</code>
<code class="o">+-----+-----+-------+</code>
<code class="o">|</code><code class="n">above</code><code class="o">|</code> <code class="n">true</code><code class="o">|</code>  <code class="n">false</code><code class="o">|</code>
<code class="o">+-----+-----+-------+</code>
<code class="o">|</code> <code class="n">true</code><code class="o">|</code><code class="mi">20931</code><code class="o">|</code> <code class="mi">596414</code><code class="o">|</code>
<code class="o">|</code><code class="n">false</code><code class="o">|</code> <code class="n">null</code><code class="o">|</code><code class="mi">5131787</code><code class="o">|</code>
<code class="o">+-----+-----+-------+</code></pre>
<p>Even though the number of false positives is higher than we want, this more generous filter still removes 90% of the nonmatching records from our consideration while including every positive match. Even though this is pretty good, it’s possible to do even better; see if you can find a way to use some of the other values from <code>MatchData</code> (both missing and not) to come up with a scoring function that successfully identifies every <code>true</code> match at the cost of less than 100 false positives.<a data-startref="ch02-scor" data-type="indexterm" id="idm46507985164368"/><a data-startref="ch02-scor2" data-type="indexterm" id="idm46507985163760"/><a data-startref="ch02-scor3" data-type="indexterm" id="idm46507985163088"/><a data-startref="ch02-scor4" data-type="indexterm" id="idm46507985162416"/><a data-startref="ch02-scor5" data-type="indexterm" id="idm46507985161744"/><a data-startref="ch02-scor6" data-type="indexterm" id="idm46507985161072"/></p>
</div></section>
<section data-pdf-bookmark="Where to Go from Here" data-type="sect1"><div class="sect1" id="idm46507985563680">
<h1>Where to Go from Here</h1>
<p>If this chapter was your first time carrying out data preparation and analysis with PySpark, we hope that you got a feel for what a powerful foundation these tools provide. If you have been using Python and Spark for a while, we hope that you will pass this chapter along to your friends and colleagues as a way of introducing them to that power as well.</p>
<p>Our goal for this chapter was to provide you with enough knowledge to be able to understand and complete the rest of the examples in this book. If you are the kind of person who learns best through practical examples, your next step is to continue on to the next set of chapters, where we will introduce you to MLlib, the machine learning library designed for Spark.</p>
</div></section>
</div></section></div></body></html>