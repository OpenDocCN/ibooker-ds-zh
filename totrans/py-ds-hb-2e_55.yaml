- en: 'Chapter 50\. Application: A Face Detection Pipeline'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This part of the book has explored a number of the central concepts and algorithms
    of machine learning. But moving from these concepts to a real-world application
    can be a challenge. Real-world datasets are noisy and heterogeneous; they may
    have missing features, and data may be in a form that is difficult to map to a
    clean `[n_samples, n_features]` matrix. Before applying any of the methods discussed
    here, you must first extract these features from your data: there is no formula
    for how to do this that applies across all domains, and thus this is where you
    as a data scientist must exercise your own intuition and expertise.'
  prefs: []
  type: TYPE_NORMAL
- en: 'One interesting and compelling application of machine learning is to images,
    and we have already seen a few examples of this where pixel-level features are
    used for classification. Again, the real world data is rarely so uniform, and
    simple pixels will not be suitable: this has led to a large literature on *feature
    extraction* methods for image data (see [Chapter 40](ch40.xhtml#section-0504-feature-engineering)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter we will take a look at one such feature extraction technique:
    the [histogram of oriented gradients (HOG)](https://oreil.ly/eiJ4X), which transforms
    image pixels into a vector representation that is sensitive to broadly informative
    image features regardless of confounding factors like illumination. We will use
    these features to develop a simple face detection pipeline, using machine learning
    algorithms and concepts we’ve seen throughout this part of the book.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We begin with the standard imports:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: HOG Features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'HOG is a straightforward feature extraction procedure that was developed in
    the context of identifying pedestrians within images. It involves the following
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Optionally prenormalize the images. This leads to features that resist dependence
    on variations in illumination.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Convolve the image with two filters that are sensitive to horizontal and vertical
    brightness gradients. These capture edge, contour, and texture information.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Subdivide the image into cells of a predetermined size, and compute a histogram
    of the gradient orientations within each cell.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Normalize the histograms in each cell by comparing to the block of neighboring
    cells. This further suppresses the effect of illumination across the image.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Construct a one-dimensional feature vector from the information in each cell.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A fast HOG extractor is built into the Scikit-Image project, and we can try
    it out relatively quickly and visualize the oriented gradients within each cell
    (see [Figure 50-1](#fig_0514-image-features_files_in_output_4_0)).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![output 4 0](assets/output_4_0.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 50-1\. Visualization of HOG features computed from an image
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'HOG in Action: A Simple Face Detector'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Using these HOG features, we can build up a simple facial detection algorithm
    with any Scikit-Learn estimator; here we will use a linear support vector machine
    (refer back to [Chapter 43](ch43.xhtml#section-0507-support-vector-machines) if
    you need a refresher on this). The steps are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Obtain a set of image thumbnails of faces to constitute “positive” training
    samples.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Obtain a set of image thumbnails of non-faces to constitute “negative” training
    samples.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Extract HOG features from these training samples.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train a linear SVM classifier on these samples.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For an “unknown” image, pass a sliding window across the image, using the model
    to evaluate whether that window contains a face or not.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If detections overlap, combine them into a single window.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let’s go through these steps and try it out.
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Obtain a Set of Positive Training Samples
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We’ll start by finding some positive training samples that show a variety of
    faces. We have one easy set of data to work with—the Labeled Faces in the Wild
    dataset, which can be downloaded by Scikit-Learn:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This gives us a sample of 13,000 face images to use for training.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Obtain a Set of Negative Training Samples
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Next we need a set of similarly sized thumbnails that *do not* have a face
    in them. One way to obtain this is to take any corpus of input images, and extract
    thumbnails from them at a variety of scales. Here we’ll use some of the images
    shipped with Scikit-Image, along with Scikit-Learn’s `PatchExtractor`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: We now have 30,000 suitable image patches that do not contain faces. Let’s visualize
    a few of them to get an idea of what they look like (see [Figure 50-2](#fig_0514-image-features_files_in_output_14_0)).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Our hope is that these will sufficiently cover the space of “non-faces” that
    our algorithm is likely to see.
  prefs: []
  type: TYPE_NORMAL
- en: '![output 14 0](assets/output_14_0.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 50-2\. Negative image patches, which don’t include faces
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 3\. Combine Sets and Extract HOG Features
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that we have these positive samples and negative samples, we can combine
    them and compute HOG features. This step takes a little while, because it involves
    a nontrivial computation for each image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: We are left with 43,000 training samples in 1,215 dimensions, and we now have
    our data in a form that we can feed into Scikit-Learn!
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Train a Support Vector Machine
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Next we use the tools we have been exploring here to create a classifier of
    thumbnail patches. For such a high-dimensional binary classification task, a linear
    support vector machine is a good choice. We will use Scikit-Learn’s `LinearSVC`,
    because in comparison to `SVC` it often has better scaling for a large number
    of samples.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, though, let’s use a simple Gaussian naive Bayes estimator to get a quick
    baseline:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We see that on our training data, even a simple naive Bayes algorithm gets
    us upwards of 95% accuracy. Let’s try the support vector machine, with a grid
    search over a few choices of the `C` parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'This pushes us up to near 99% accuracy. Let’s take the best estimator and retrain
    it on the full dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 5\. Find Faces in a New Image
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that we have this model in place, let’s grab a new image and see how the
    model does. We will use one portion of the astronaut image shown in [Figure 50-3](#fig_0514-image-features_files_in_output_28_0)
    for simplicity (see discussion of this in the following section, and run a sliding
    window over it and evaluate each patch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '![output 28 0](assets/output_28_0.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 50-3\. An image in which we will attempt to locate a face
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Next, let’s create a window that iterates over patches of this image, and compute
    HOG features for each patch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can take these HOG-featured patches and use our model to evaluate
    whether each patch contains a face:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: We see that out of nearly 2,000 patches, we have found 48 detections. Let’s
    use the information we have about these patches to show where they lie on our
    test image, drawing them as rectangles (see [Figure 50-4](#fig_0514-image-features_files_in_output_34_0)).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '![output 34 0](assets/output_34_0.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 50-4\. Windows that were determined to contain a face
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: All of the detected patches overlap and found the face in the image! Not bad
    for a few lines of Python.
  prefs: []
  type: TYPE_NORMAL
- en: Caveats and Improvements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you dig a bit deeper into the preceding code and examples, you’ll see that
    we still have a bit of work to do before we can claim a production-ready face
    detector. There are several issues with what we’ve done, and several improvements
    that could be made. In particular:'
  prefs: []
  type: TYPE_NORMAL
- en: Our training set, especially for negative features, is not very complete
  prefs: []
  type: TYPE_NORMAL
- en: 'The central issue is that there are many face-like textures that are not in
    the training set, and so our current model is very prone to false positives. You
    can see this if you try out the algorithm on the *full* astronaut image: the current
    model leads to many false detections in other regions of the image.'
  prefs: []
  type: TYPE_NORMAL
- en: We might imagine addressing this by adding a wider variety of images to the
    negative training set, and this would probably yield some improvement. Another
    option would be to use a more directed approach, such as *hard negative mining*,
    where we take a new set of images that our classifier has not seen, find all the
    patches representing false positives, and explicitly add them as negative instances
    in the training set before retraining the classifier.
  prefs: []
  type: TYPE_NORMAL
- en: Our current pipeline searches only at one scale
  prefs: []
  type: TYPE_NORMAL
- en: As currently written, our algorithm will miss faces that are not approximately
    62 × 47 pixels. This can be straightforwardly addressed by using sliding windows
    of a variety of sizes, and resizing each patch using `skimage.transform.resize`
    before feeding it into the model. In fact, the `sliding_window` utility used here
    is already built with this in mind.
  prefs: []
  type: TYPE_NORMAL
- en: We should combine overlapped detection patches
  prefs: []
  type: TYPE_NORMAL
- en: For a production-ready pipeline, we would prefer not to have 30 detections of
    the same face, but to somehow reduce overlapping groups of detections down to
    a single detection. This could be done via an unsupervised clustering approach
    (mean shift clustering is one good candidate for this), or via a procedural approach
    such as *non-maximum suppression*, an algorithm common in machine vision.
  prefs: []
  type: TYPE_NORMAL
- en: The pipeline should be streamlined
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we address the preceding issues, it would also be nice to create a more
    streamlined pipeline for ingesting training images and predicting sliding-window
    outputs. This is where Python as a data science tool really shines: with a bit
    of work, we could take our prototype code and package it with a well-designed
    object-oriented API that gives the user the ability to use it easily. I will leave
    this as a proverbial “exercise for the reader.”'
  prefs: []
  type: TYPE_NORMAL
- en: 'More recent advances: deep learning'
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, I should add that in machine learning contexts, HOG and other procedural
    feature extraction methods are not always used. Instead, many modern object detection
    pipelines use variants of deep neural networks (often referred to as *deep learning*):
    one way to think of neural networks is as estimators that determine optimal feature
    extraction strategies from the data, rather than relying on the intuition of the
    user.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Though the field has produced fantastic results in recent years, deep learning
    is not all that conceptually different from the machine learning models explored
    in the previous chapters. The main advance is the ability to utilize modern computing
    hardware (often large clusters of powerful machines) to train much more flexible
    models on much larger corpuses of training data. But though the scale differs,
    the end goal is very much the same the same: building models from data.'
  prefs: []
  type: TYPE_NORMAL
- en: If you’re interested in going further, the list of references in the following
    section should provide a useful place to start!
  prefs: []
  type: TYPE_NORMAL
- en: Further Machine Learning Resources
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This part of the book has been a quick tour of machine learning in Python,
    primarily using the tools within the Scikit-Learn library. As long as these chapters
    are, they are still too short to cover many interesting and important algorithms,
    approaches, and discussions. Here I want to suggest some resources to learn more
    about machine learning in Python, for those who are interested:'
  prefs: []
  type: TYPE_NORMAL
- en: '[The Scikit-Learn website](http://scikit-learn.org)'
  prefs: []
  type: TYPE_NORMAL
- en: The Scikit-Learn website has an impressive breadth of documentation and examples
    covering some of the models discussed here, and much, much more. If you want a
    brief survey of the most important and often-used machine learning algorithms,
    this is a good place to start.
  prefs: []
  type: TYPE_NORMAL
- en: '*SciPy, PyCon, and PyData tutorial videos*'
  prefs: []
  type: TYPE_NORMAL
- en: Scikit-Learn and other machine learning topics are perennial favorites in the
    tutorial tracks of many Python-focused conference series, in particular the PyCon,
    SciPy, and PyData conferences. Most of these conferences publish videos of their
    keynotes, talks, and tutorials for free online, and you should be able to find
    these easily via a suitable web search (for example, “PyCon 2022 videos”).
  prefs: []
  type: TYPE_NORMAL
- en: '[*Introduction to Machine Learning with Python*](https://oreil.ly/kaQQs), by
    Andreas C. Müller and Sarah Guido (O’Reilly)'
  prefs: []
  type: TYPE_NORMAL
- en: This book covers many of the machine learning fundamentals discussed in these
    chapters, but is particularly relevant for its coverage of more advanced features
    of Scikit-Learn, including additional estimators, model validation approaches,
    and pipelining.
  prefs: []
  type: TYPE_NORMAL
- en: '[*Machine Learning with PyTorch and Scikit-Learn*](https://oreil.ly/p268i),
    by Sebastian Raschka (Packt)'
  prefs: []
  type: TYPE_NORMAL
- en: Sebastian Raschka’s most recent book starts with some of the fundamental topics
    covered in these chapters, but goes deeper and shows how those concepts apply
    to more sophisticated and computationally intensive deep learing and reinforcement
    learning models using the well-known [PyTorch library](https://pytorch.org).
  prefs: []
  type: TYPE_NORMAL
