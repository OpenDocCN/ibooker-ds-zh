- en: 'Chapter 8\. Unsupervised Methods: Topic Modeling and Clustering'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When working with a large number of documents, one of the first questions you
    want to ask without reading all of them is “What are they talking about?” You
    are interested in the general topics of the documents, i.e., which (ideally semantic)
    words are often used together.
  prefs: []
  type: TYPE_NORMAL
- en: Topic modeling tries to solve that challenge by using statistical techniques
    for finding out topics from a corpus of documents. Depending on your vectorization
    (see [Chapter 5](ch05.xhtml#ch-vectorization)), you might find different kinds
    of topics. Topics consist of a probability distribution of features (words, n-grams,
    etc.).
  prefs: []
  type: TYPE_NORMAL
- en: 'Topics normally overlap with each other; they are not clearly separated. The
    same is true for documents: it is not possible to assign a document uniquely to
    a single topic; a document always contains a mixture of different topics. The
    aim of topic modeling is not primarily to assign a topic to an arbitrary document
    but to find the global structure of the corpus.'
  prefs: []
  type: TYPE_NORMAL
- en: Often, a set of documents has an explicit structure that is given by categories,
    keywords, and so on. If we want to take a look at the organic composition of the
    corpus, then topic modeling will help a lot to uncover the latent structure.
  prefs: []
  type: TYPE_NORMAL
- en: Topic modeling has been known for a long time and has gained immense popularity
    during the last 15 years, mainly due to the invention of LDA,^([1](ch08.xhtml#idm45634189170488))
    a stochastic method for discovering topics. LDA is flexible and allows many modifications.
    However, it is not the only method for topic modeling (although you might believe
    this by looking at the literature, much of which is biased toward LDA). Conceptually
    simpler methods are non-negative matrix factorization, singular-value decomposition
    (sometimes called *LSI*), and a few others.
  prefs: []
  type: TYPE_NORMAL
- en: What You’ll Learn and What We’ll Build
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will take an in-depth look at the various methods of topic
    modeling, try to find differences and similarities between the methods, and run
    them on the same use case. Depending on your requirements, it might also be a
    good idea to not only try a single method but compare the results of a few.
  prefs: []
  type: TYPE_NORMAL
- en: After studying this chapter, you will know the different methods of topic modeling
    and their specific advantages and drawbacks. You will understand how topic modeling
    can be applied not only to find topics but also to create quick summaries of document
    corpora. You will learn about the importance of choosing the correct granularity
    of entities for calculating topic models. You have experimented with many parameters
    to find the optimal topic model. You are able to judge the quality of the resulting
    topic models by quantitative methods and numbers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our Dataset: UN General Debates'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our use case is to semantically analyze the corpus of the UN general debates.
    You might know this dataset from the earlier chapter about text statistics.
  prefs: []
  type: TYPE_NORMAL
- en: 'This time we are more interested in the meaning and in the semantic content
    of the speeches and how we can arrange them topically. We want to know what the
    speakers are talking about and answer questions like these: Is there a structure
    in the document corpus? What are the topics? Which of them is most prominent?
    Does this change over time?'
  prefs: []
  type: TYPE_NORMAL
- en: Checking Statistics of the Corpus
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before starting with topic modeling, it is always a good idea to check the statistics
    of the underlying text corpus. Depending on the results of this analysis, you
    will often choose to analyze different entities, e.g., documents, sections, or
    paragraphs of text.
  prefs: []
  type: TYPE_NORMAL
- en: 'We are not so much interested in authors and additional information, so it’s
    enough to work with one of the supplied *CSV* files:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The result looks fine. There are no null values in the text column; we might
    use years and countries later, and they also have only non-null values.
  prefs: []
  type: TYPE_NORMAL
- en: 'The speeches are quite long and cover a lot of topics as each country is allowed
    only to deliver a single speech per year. Different parts of the speeches are
    almost always separated by paragraphs. Unfortunately, the dataset has some formatting
    issues. Compare the text of two selected speeches:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, in some speeches the newline character is used to separate
    paragraphs. In the transcription of other speeches, a newline is used to separate
    lines. To recover the paragraphs, we therefore cannot just split at newlines.
    It turns out that splitting at stops, exclamation points, or question marks occurring
    at line ends works well enough. We ignore spaces after the stops:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: From the analysis in [Chapter 2](ch02.xhtml#ch-api), we already know that the
    number of speeches per year does not change much. Is this also true for the number
    of paragraphs?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/btap_08in01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The average number of paragraphs has dropped considerably over time. We would
    have expected that, as the number of speakers per year increased and the total
    time for speeches is limited.
  prefs: []
  type: TYPE_NORMAL
- en: Apart from that, the statistical analysis shows no systematic problems with
    the dataset. The corpus is still quite up-to-date; there is no missing data for
    any year. We can now safely start with uncovering the latent structure and detect
    topics.
  prefs: []
  type: TYPE_NORMAL
- en: Preparations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Topic modeling is a machine learning method and needs vectorized data. All topic
    modeling methods start with the document-term matrix. Recalling the meaning of
    this matrix (which was introduced in [Chapter 4](ch04.xhtml#ch-preparation)),
    its elements are word frequencies (or often scaled as TF-IDF weights) of the words
    (columns) in the corresponding documents (rows). The matrix is sparse, as most
    documents contain only a small fraction of the vocabulary.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s calculate the TF-IDF matrix both for the speeches and for the paragraphs
    of the speeches. First, we have to import the necessary packages from scikit-learn.
    We start with a naive approach and use the standard spaCy stop words:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Calculating the document-term matrix for the speeches is easy; we also include
    bigrams:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'For the paragraphs, it’s a bit more complicated as we have to flatten the list
    first. In the same step, we omit empty paragraphs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Of course, the paragraph matrix has many more rows. The number of columns (words)
    is also different because `min_df` and `max_df` have an effect in selecting features,
    as the number of documents has changed.
  prefs: []
  type: TYPE_NORMAL
- en: Nonnegative Matrix Factorization (NMF)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The conceptually easiest way to find a latent structure in the document corpus
    is the factorization of the document-term matrix. Fortunately, the document-term
    matrix has only positive-value elements; therefore, we can use methods from linear
    algebra that allow us to represent the [matrix as the product of two other nonnegative
    matrices](https://oreil.ly/JVpFA). Conventionally, the original matrix is called
    *V*, and the factors are *W* and *H*:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="normal upper V almost-equals normal upper W dot normal upper
    H"><mrow><mi mathvariant="normal">V</mi> <mo>≈</mo> <mi mathvariant="normal">W</mi>
    <mo>·</mo> <mi mathvariant="normal">H</mi></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: Or we can represent it graphically (visualizing the dimensions necessary for
    matrix multiplication), as in [Figure 8-1](#nmf-decomposition).
  prefs: []
  type: TYPE_NORMAL
- en: Depending on the dimensions, the factorization can be performed exactly. But
    as this is so much more computationally expensive, an approximate factorization
    is sufficient.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/btap_0801.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-1\. Schematic nonnegative matrix factorization; the original matrix
    V is decomposed into W and H.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In the context of text analytics, both *W* and *H* have an interpretation. The
    matrix *W* has the same number of rows as the document-term matrix and therefore
    maps documents to topics (document-topic matrix). *H* has the same number of columns
    as features, so it shows how the topics are constituted of features (topic-feature
    matrix). The number of topics (the columns of *W* and the rows of *H*) can be
    chosen arbitrarily. The smaller this number, the less exact the factorization.
  prefs: []
  type: TYPE_NORMAL
- en: 'Blueprint: Creating a Topic Model Using NMF for Documents'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'It’s really easy to perform this decomposition for speeches in scikit-learn.
    As (almost) all topic models need the number of topics as a parameter, we arbitrarily
    choose 10 topics (which will later turn out to be a good choice):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Similar to the `TfidfVectorizer`, NMF also has a `fit_transform` method that
    returns one of the positive factor matrices. The other factor can be accessed
    by the `components_` member variable of the NMF class.
  prefs: []
  type: TYPE_NORMAL
- en: 'Topics are word distributions. We are now going to analyze this distribution
    and see whether we can find an interpretation of the topics. Taking a look at [Figure 8-1](#nmf-decomposition),
    we need to consider the *H* matrix and find the index of the largest values in
    each row (topic) that we then use as a lookup index in the vocabulary. As this
    is helpful for all topic models, we define a function for outputting a summary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Calling this function, we get a nice summary of the topics that NMF detected
    in the speeches (the numbers are the percentage contributions of the words to
    the respective topic):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Topic 00** co (0.79)'
  prefs: []
  type: TYPE_NORMAL
- en: operation (0.65)
  prefs: []
  type: TYPE_NORMAL
- en: disarmament (0.36)
  prefs: []
  type: TYPE_NORMAL
- en: nuclear (0.34)
  prefs: []
  type: TYPE_NORMAL
- en: relations (0.25) | **Topic 01** terrorism (0.38)
  prefs: []
  type: TYPE_NORMAL
- en: challenges (0.32)
  prefs: []
  type: TYPE_NORMAL
- en: sustainable (0.30)
  prefs: []
  type: TYPE_NORMAL
- en: millennium (0.29)
  prefs: []
  type: TYPE_NORMAL
- en: reform (0.28) | **Topic 02** africa (1.15)
  prefs: []
  type: TYPE_NORMAL
- en: african (0.82)
  prefs: []
  type: TYPE_NORMAL
- en: south (0.63)
  prefs: []
  type: TYPE_NORMAL
- en: namibia (0.36)
  prefs: []
  type: TYPE_NORMAL
- en: delegation (0.30) | **Topic 03** arab (1.02)
  prefs: []
  type: TYPE_NORMAL
- en: israel (0.89)
  prefs: []
  type: TYPE_NORMAL
- en: palestinian (0.60)
  prefs: []
  type: TYPE_NORMAL
- en: lebanon (0.54)
  prefs: []
  type: TYPE_NORMAL
- en: israeli (0.54) | **Topic 04** american (0.33)
  prefs: []
  type: TYPE_NORMAL
- en: america (0.31)
  prefs: []
  type: TYPE_NORMAL
- en: latin (0.31)
  prefs: []
  type: TYPE_NORMAL
- en: panama (0.21)
  prefs: []
  type: TYPE_NORMAL
- en: bolivia (0.21) |
  prefs: []
  type: TYPE_NORMAL
- en: '| **Topic 05** pacific (1.55)'
  prefs: []
  type: TYPE_NORMAL
- en: islands (1.23)
  prefs: []
  type: TYPE_NORMAL
- en: solomon (0.86)
  prefs: []
  type: TYPE_NORMAL
- en: island (0.82)
  prefs: []
  type: TYPE_NORMAL
- en: fiji (0.71) | **Topic 06** soviet (0.81)
  prefs: []
  type: TYPE_NORMAL
- en: republic (0.78)
  prefs: []
  type: TYPE_NORMAL
- en: nuclear (0.68)
  prefs: []
  type: TYPE_NORMAL
- en: viet (0.64)
  prefs: []
  type: TYPE_NORMAL
- en: socialist (0.63) | **Topic 07** guinea (4.26)
  prefs: []
  type: TYPE_NORMAL
- en: equatorial (1.75)
  prefs: []
  type: TYPE_NORMAL
- en: bissau (1.53)
  prefs: []
  type: TYPE_NORMAL
- en: papua (1.47)
  prefs: []
  type: TYPE_NORMAL
- en: republic (0.57) | **Topic 08** european (0.61)
  prefs: []
  type: TYPE_NORMAL
- en: europe (0.44)
  prefs: []
  type: TYPE_NORMAL
- en: cooperation (0.39)
  prefs: []
  type: TYPE_NORMAL
- en: bosnia (0.34)
  prefs: []
  type: TYPE_NORMAL
- en: herzegovina (0.30) | **Topic 09** caribbean (0.98)
  prefs: []
  type: TYPE_NORMAL
- en: small (0.66)
  prefs: []
  type: TYPE_NORMAL
- en: bahamas (0.63)
  prefs: []
  type: TYPE_NORMAL
- en: saint (0.63)
  prefs: []
  type: TYPE_NORMAL
- en: barbados (0.61) |
  prefs: []
  type: TYPE_NORMAL
- en: Topic 00 and Topic 01 look really promising as people are talking about nuclear
    disarmament and terrorism. These are definitely real topics in the UN general
    debates.
  prefs: []
  type: TYPE_NORMAL
- en: The subsequent topics, however, are more or less focused on different regions
    of the world. This is due to speakers mentioning primarily their own country and
    neighboring countries. This is especially evident in Topic 03, which reflects
    the conflict in the Middle East.
  prefs: []
  type: TYPE_NORMAL
- en: It’s also interesting to take a look at the percentages with which the words
    contribute to the topics. Due to the large number of words, the individual contributions
    are quite small, except for *guinea* in Topic 07\. As we will see later, the percentages
    of the words within a topic are a good indication for the quality of the topic
    model. If the percentage within a topic is rapidly decreasing, the topic is well-defined,
    whereas slowly decreasing word probabilities indicate a less-pronounced topic.
    It’s much more difficult to intuitively find out how well the topics are separated;
    we will take a look at that later.
  prefs: []
  type: TYPE_NORMAL
- en: 'It would be interesting to find out how “big” the topics are, i.e., how many
    documents could be assigned mainly to each topic. This can be calculated using
    the document-topic matrix and summing the individual topic contributions over
    all documents. Normalizing them with the total sum and multiplying by 100 gives
    a percentage value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: We can easily see that there are smaller and larger topics but basically no
    outliers. Having an even distribution is a quality indicator. If your topic models
    have, for example, one or two large topics compared to all the others, you should
    probably adjust the number of topics.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will use the paragraphs of the speeches as entities
    for topic modeling and try to find out if that improves the topics.
  prefs: []
  type: TYPE_NORMAL
- en: 'Blueprint: Creating a Topic Model for Paragraphs Using NMF'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In UN general debates, as in many other texts, different topics are often mixed,
    and it is hard for the topic modeling algorithm to find a common topic of an individual
    speech. Especially in longer texts, it happens quite often that documents do not
    cover just one but several topics. How can we deal with that? One idea is to find
    smaller entities in the documents that are more coherent from a topic perspective.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our corpus, paragraphs are a natural subdivision of speeches, and we can
    assume that the speakers try to stick to one topic within one paragraph. In many
    documents, paragraphs are a good candidate (if they can be identified as such),
    and we have already prepared the corresponding TF-IDF vectors. Let’s try to calculate
    their topic models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Our `display_topics` function developed earlier can be used to find the content
    of the topics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Topic 00** nations (5.63)'
  prefs: []
  type: TYPE_NORMAL
- en: united (5.52)
  prefs: []
  type: TYPE_NORMAL
- en: organization (1.27)
  prefs: []
  type: TYPE_NORMAL
- en: states (1.03)
  prefs: []
  type: TYPE_NORMAL
- en: charter (0.93) | **Topic 01** general (2.87)
  prefs: []
  type: TYPE_NORMAL
- en: session (2.83)
  prefs: []
  type: TYPE_NORMAL
- en: assembly (2.81)
  prefs: []
  type: TYPE_NORMAL
- en: mr (1.98)
  prefs: []
  type: TYPE_NORMAL
- en: president (1.81) | **Topic 02** countries (4.44)
  prefs: []
  type: TYPE_NORMAL
- en: developing (2.49)
  prefs: []
  type: TYPE_NORMAL
- en: economic (1.49)
  prefs: []
  type: TYPE_NORMAL
- en: developed (1.35)
  prefs: []
  type: TYPE_NORMAL
- en: trade (0.92) | **Topic 03** people (1.36)
  prefs: []
  type: TYPE_NORMAL
- en: peace (1.34)
  prefs: []
  type: TYPE_NORMAL
- en: east (1.28)
  prefs: []
  type: TYPE_NORMAL
- en: middle (1.17)
  prefs: []
  type: TYPE_NORMAL
- en: palestinian (1.14) | **Topic 04** nuclear (4.93)
  prefs: []
  type: TYPE_NORMAL
- en: weapons (3.27)
  prefs: []
  type: TYPE_NORMAL
- en: disarmament (2.01)
  prefs: []
  type: TYPE_NORMAL
- en: treaty (1.70)
  prefs: []
  type: TYPE_NORMAL
- en: proliferation (1.46) |
  prefs: []
  type: TYPE_NORMAL
- en: '| **Topic 05** rights (6.49)'
  prefs: []
  type: TYPE_NORMAL
- en: human (6.18)
  prefs: []
  type: TYPE_NORMAL
- en: respect (1.15)
  prefs: []
  type: TYPE_NORMAL
- en: fundamental (0.86)
  prefs: []
  type: TYPE_NORMAL
- en: universal (0.82) | **Topic 06** africa (3.83)
  prefs: []
  type: TYPE_NORMAL
- en: south (3.32)
  prefs: []
  type: TYPE_NORMAL
- en: african (1.70)
  prefs: []
  type: TYPE_NORMAL
- en: namibia (1.38)
  prefs: []
  type: TYPE_NORMAL
- en: apartheid (1.19) | **Topic 07** security (6.13)
  prefs: []
  type: TYPE_NORMAL
- en: council (5.88)
  prefs: []
  type: TYPE_NORMAL
- en: permanent (1.50)
  prefs: []
  type: TYPE_NORMAL
- en: reform (1.48)
  prefs: []
  type: TYPE_NORMAL
- en: peace (1.30) | **Topic 08** international (2.05)
  prefs: []
  type: TYPE_NORMAL
- en: world (1.50)
  prefs: []
  type: TYPE_NORMAL
- en: community (0.92)
  prefs: []
  type: TYPE_NORMAL
- en: new (0.77)
  prefs: []
  type: TYPE_NORMAL
- en: peace (0.67) | **Topic 09** development (4.47)
  prefs: []
  type: TYPE_NORMAL
- en: sustainable (1.18)
  prefs: []
  type: TYPE_NORMAL
- en: economic (1.07)
  prefs: []
  type: TYPE_NORMAL
- en: social (1.00)
  prefs: []
  type: TYPE_NORMAL
- en: goals (0.93) |
  prefs: []
  type: TYPE_NORMAL
- en: Compared to the previous results for topic modeling speeches, we have almost
    lost all countries or regions except for South Africa and the Middle East. These
    are due to the regional conflicts that sparked interest in other parts of the
    world. Topics in the paragraphs like “Human rights,” “international relations,”
    “developing countries,” “nuclear weapons,” “security council,” “world peace,”
    and “sustainable development” (the last one probably occurring only lately) look
    much more reasonable compared to the topics of the speeches. Taking a look at
    the percentage values of the words, we can observe that they are dropping much
    faster, and the topics are more pronounced.
  prefs: []
  type: TYPE_NORMAL
- en: Latent Semantic Analysis/Indexing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another algorithm for performing topic modeling is based on the so-called singular
    value decomposition (SVD), another method from linear algebra.
  prefs: []
  type: TYPE_NORMAL
- en: Graphically, it is possible to conceive SVD as rearranging documents and words
    in a way to uncover a block structure in the document-term matrix. There is a
    nice visualization of that process at [topicmodels.info](https://oreil.ly/yJnWL).
    [Figure 8-2](#svd-animation) shows the start of the document-term matrix and the
    resulting block diagonal form.
  prefs: []
  type: TYPE_NORMAL
- en: 'Making use of the principal axis theorem, orthogonal *n* × *n* matrices have
    an eigenvalue decomposition. Unfortunately, we do not have orthogonal square document-term
    matrices (except for rare cases). Therefore, we need a generalization called *singular
    value decomposition*. In its most general form, the theorem states that any *m*
    × *n* matrix **V **can be decomposed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="normal upper V equals normal upper U dot normal upper Sigma dot
    normal upper V Superscript asterisk"><mrow><mi mathvariant="normal">V</mi> <mo>=</mo>
    <mi mathvariant="normal">U</mi> <mo>·</mo> <mi>Σ</mi> <mo>·</mo> <msup><mi mathvariant="normal">V</mi>
    <mo>*</mo></msup></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/btap_0802.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-2\. Visualization of topic modeling with SVD.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '*U* is a unitary *m* × *m* matrix, *V** is an *n* × *n* matrix, and *Σ* is
    an *m* × *n* diagonal matrix containing the singular values. There are exact solutions
    for this equation, but as they take a lot of time and computational effort to
    find, we are looking for approximate solutions that can be found quickly. The
    approximation works by only considering the largest singular values. This leads
    to *Σ* becoming a *t* × *t* matrix; in turn, *U* has *m* × *t* and *V** *t* ×
    *n* dimensions. Graphically, this is similar to the nonnegative matrix factorization,
    as shown in [Figure 8-3](#svd-decomposition).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/btap_0803.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-3\. Schematic singular value decomposition.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The singular values are the diagonal elements of *Σ*. The document-topic relations
    are included in *U*, whereas the word-to-topic mapping is represented by *V**.
    Note that neither the elements of *U* nor the elements of *V** are guaranteed
    to be positive. The relative sizes of the contributions will still be interpretable,
    but the probability explanation is no longer valid.
  prefs: []
  type: TYPE_NORMAL
- en: 'Blueprint: Creating a Topic Model for Paragraphs with SVD'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In scikit-learn the interface to SVD is identical to that of NMF. This time
    we start directly with the paragraphs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Our previously defined function for evaluating the topic model can also be
    used:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Topic 00** nations (0.67)'
  prefs: []
  type: TYPE_NORMAL
- en: united (0.65)
  prefs: []
  type: TYPE_NORMAL
- en: international (0.58)
  prefs: []
  type: TYPE_NORMAL
- en: peace (0.46)
  prefs: []
  type: TYPE_NORMAL
- en: world (0.46) | **Topic 01** general (14.04)
  prefs: []
  type: TYPE_NORMAL
- en: assembly (13.09)
  prefs: []
  type: TYPE_NORMAL
- en: session (12.94)
  prefs: []
  type: TYPE_NORMAL
- en: mr (10.02)
  prefs: []
  type: TYPE_NORMAL
- en: president (8.59) | **Topic 02** countries (19.15)
  prefs: []
  type: TYPE_NORMAL
- en: development (14.61)
  prefs: []
  type: TYPE_NORMAL
- en: economic (13.91)
  prefs: []
  type: TYPE_NORMAL
- en: developing (13.00)
  prefs: []
  type: TYPE_NORMAL
- en: session (10.29) | **Topic 03** nations (4.41)
  prefs: []
  type: TYPE_NORMAL
- en: united (4.06)
  prefs: []
  type: TYPE_NORMAL
- en: development (0.95)
  prefs: []
  type: TYPE_NORMAL
- en: organization (0.84)
  prefs: []
  type: TYPE_NORMAL
- en: charter (0.80) | **Topic 04** nuclear (21.13)
  prefs: []
  type: TYPE_NORMAL
- en: weapons (14.01)
  prefs: []
  type: TYPE_NORMAL
- en: disarmament (9.02)
  prefs: []
  type: TYPE_NORMAL
- en: treaty (7.23)
  prefs: []
  type: TYPE_NORMAL
- en: proliferation (6.31) |
  prefs: []
  type: TYPE_NORMAL
- en: '| **Topic 05** rights (29.50)'
  prefs: []
  type: TYPE_NORMAL
- en: human (28.81)
  prefs: []
  type: TYPE_NORMAL
- en: nuclear (9.20)
  prefs: []
  type: TYPE_NORMAL
- en: weapons (6.42)
  prefs: []
  type: TYPE_NORMAL
- en: respect (4.98) | **Topic 06** africa (8.73)
  prefs: []
  type: TYPE_NORMAL
- en: south (8.24)
  prefs: []
  type: TYPE_NORMAL
- en: united (3.91)
  prefs: []
  type: TYPE_NORMAL
- en: african (3.71)
  prefs: []
  type: TYPE_NORMAL
- en: nations (3.41) | **Topic 07** council (14.96)
  prefs: []
  type: TYPE_NORMAL
- en: security (13.38)
  prefs: []
  type: TYPE_NORMAL
- en: africa (8.50)
  prefs: []
  type: TYPE_NORMAL
- en: south (6.11)
  prefs: []
  type: TYPE_NORMAL
- en: african (3.94) | **Topic 08** world (48.49)
  prefs: []
  type: TYPE_NORMAL
- en: international (41.03)
  prefs: []
  type: TYPE_NORMAL
- en: peace (32.98)
  prefs: []
  type: TYPE_NORMAL
- en: community (23.27)
  prefs: []
  type: TYPE_NORMAL
- en: africa (22.00) | **Topic 09** development (63.98)
  prefs: []
  type: TYPE_NORMAL
- en: sustainable (20.78)
  prefs: []
  type: TYPE_NORMAL
- en: peace (20.74)
  prefs: []
  type: TYPE_NORMAL
- en: goals (15.92)
  prefs: []
  type: TYPE_NORMAL
- en: africa (15.61) |
  prefs: []
  type: TYPE_NORMAL
- en: Most of the resulting topics are surprisingly similar to those of the nonnegative
    matrix factorization. However, the Middle East conflict does not appear as a separate
    topic this time. As the topic-word mappings can also have negative values, the
    normalization varies from topic to topic. Only the relative sizes of the words
    constituting the topics are relevant.
  prefs: []
  type: TYPE_NORMAL
- en: Don’t worry about the negative percentages. These arise as SVD does not guarantee
    positive values in W, so contributions of individual words might be negative.
    This means that words appearing in documents “reject” the corresponding topic.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we want to determine the sizes of the topics, we now have to take a look
    at the singular values of the decomposition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The sizes of the topics also correspond quite nicely with the ones from the
    NMF method for the paragraphs.
  prefs: []
  type: TYPE_NORMAL
- en: Both NMF and SVF have used the document-term matrix (with TF-IDF transformations
    applied) as a basis for the topic decomposition. Also, the dimensions of the *U*
    matrix are identical to those of *W*; the same is true for *V** and *H*. It is
    therefore not surprising that both of these methods produce similar and comparable
    results. As these methods are really fast to calculate, for real-life projects
    we recommend starting with the linear algebra methods.
  prefs: []
  type: TYPE_NORMAL
- en: We will now turn away from these linear-algebra-based methods and focus on probabilistic
    topic models, which have become immensely popular in the past 20 years.
  prefs: []
  type: TYPE_NORMAL
- en: Latent Dirichlet Allocation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: LDA is arguably the most prominent method of topic modeling in use today. It
    has been popularized during the last 15 years and can be adapted in flexible ways
    to different usage scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: How does it work?
  prefs: []
  type: TYPE_NORMAL
- en: LDA views each document as consisting of different topics. In other words, each
    document is a mix of different topics. In the same way, topics are mixed from
    words. To keep the number of topics per document low and to have only a few, important
    words constituting the topics, LDA initially uses a [Dirichlet distribution](https://oreil.ly/Kkd9k),
    a so-called *Dirichlet prior*. This is applied both for assigning topics to documents
    and for finding words for the topics. The Dirichlet distribution ensures that
    documents have only a small number of topics and topics are mainly defined by
    a small number of words. Assuming that LDA generated topic distributions like
    the previous ones, a topic could be made up of words like *nuclear*, *treaty*,
    and *disarmament*, while another topic would be sampled by *sustainable*, *development*,
    etc.
  prefs: []
  type: TYPE_NORMAL
- en: After the initial assignments, the generative process starts. It uses the Dirichlet
    distributions for topics and words and tries to re-create the words from the original
    documents with stochastic sampling. This process has to be iterated many times
    and is therefore computationally intensive.^([2](ch08.xhtml#idm45634187759704))
    On the other hand, the results can be used to generate documents for any identified
    topic.
  prefs: []
  type: TYPE_NORMAL
- en: 'Blueprint: Creating a Topic Model for Paragraphs with LDA'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Scikit-learn hides all these differences and uses the same API as the other
    topic modeling methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Waiting Time
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Due to the probabilistic sampling, the process takes a lot longer than NMF and
    SVD. Expect at least minutes, if not hours, of runtime.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our utility function can again be used to visualize the latent topics of the
    paragraph corpus:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Topic 00** africa (2.38)'
  prefs: []
  type: TYPE_NORMAL
- en: people (1.86)
  prefs: []
  type: TYPE_NORMAL
- en: south (1.57)
  prefs: []
  type: TYPE_NORMAL
- en: namibia (0.88)
  prefs: []
  type: TYPE_NORMAL
- en: regime (0.75) | **Topic 01** republic (1.52)
  prefs: []
  type: TYPE_NORMAL
- en: government (1.39)
  prefs: []
  type: TYPE_NORMAL
- en: united (1.21)
  prefs: []
  type: TYPE_NORMAL
- en: peace (1.16)
  prefs: []
  type: TYPE_NORMAL
- en: people (1.02) | **Topic 02** general (4.22)
  prefs: []
  type: TYPE_NORMAL
- en: assembly (3.63)
  prefs: []
  type: TYPE_NORMAL
- en: session (3.38)
  prefs: []
  type: TYPE_NORMAL
- en: president (2.33)
  prefs: []
  type: TYPE_NORMAL
- en: mr (2.32) | **Topic 03** human (3.62)
  prefs: []
  type: TYPE_NORMAL
- en: rights (3.48)
  prefs: []
  type: TYPE_NORMAL
- en: international (1.83)
  prefs: []
  type: TYPE_NORMAL
- en: law (1.01)
  prefs: []
  type: TYPE_NORMAL
- en: terrorism (0.99) | **Topic 04** world (2.22)
  prefs: []
  type: TYPE_NORMAL
- en: people (1.14)
  prefs: []
  type: TYPE_NORMAL
- en: countries (0.94)
  prefs: []
  type: TYPE_NORMAL
- en: years (0.88)
  prefs: []
  type: TYPE_NORMAL
- en: today (0.66) |
  prefs: []
  type: TYPE_NORMAL
- en: '| **Topic 05** peace (1.76)'
  prefs: []
  type: TYPE_NORMAL
- en: security (1.63)
  prefs: []
  type: TYPE_NORMAL
- en: east (1.34)
  prefs: []
  type: TYPE_NORMAL
- en: middle (1.34)
  prefs: []
  type: TYPE_NORMAL
- en: israel (1.24) | **Topic 06** countries (3.19)
  prefs: []
  type: TYPE_NORMAL
- en: development (2.70)
  prefs: []
  type: TYPE_NORMAL
- en: economic (2.22)
  prefs: []
  type: TYPE_NORMAL
- en: developing (1.61)
  prefs: []
  type: TYPE_NORMAL
- en: international (1.45) | **Topic 07** nuclear (3.14)
  prefs: []
  type: TYPE_NORMAL
- en: weapons (2.32)
  prefs: []
  type: TYPE_NORMAL
- en: disarmament (1.82)
  prefs: []
  type: TYPE_NORMAL
- en: states (1.47)
  prefs: []
  type: TYPE_NORMAL
- en: arms (1.46) | **Topic 08** nations (5.50)
  prefs: []
  type: TYPE_NORMAL
- en: united (5.11)
  prefs: []
  type: TYPE_NORMAL
- en: international (1.46)
  prefs: []
  type: TYPE_NORMAL
- en: security (1.45)
  prefs: []
  type: TYPE_NORMAL
- en: organization (1.44) | **Topic 09** international (1.96)
  prefs: []
  type: TYPE_NORMAL
- en: world (1.91)
  prefs: []
  type: TYPE_NORMAL
- en: peace (1.60)
  prefs: []
  type: TYPE_NORMAL
- en: economic (1.00)
  prefs: []
  type: TYPE_NORMAL
- en: relations (0.99) |
  prefs: []
  type: TYPE_NORMAL
- en: It’s interesting to observe that LDA has generated a completely different topic
    structure compared to the linear algebra methods described earlier. *People* is
    the most prominent word in three quite different topics. In Topic 04, South Africa
    is related to Israel and Palestine, while in Topic 00, Cyprus, Afghanistan, and
    Iraq are related. This is not easy to explain. This is also reflected in the slowly
    decreasing word weights of the topics.
  prefs: []
  type: TYPE_NORMAL
- en: Other topics are easier to comprehend, such as climate change, nuclear weapons,
    elections, developing countries, and organizational questions.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this example, LDA does not yield much better results than either NMF or
    SVD. However, due to the sampling process, LDA is not limited to sample topics
    just consisting of words. There are several variations, such as author-topic models,
    that can also sample categorical features. Moreover, as there is so much research
    going on in LDA, other ideas are published quite frequently, which extend the
    focus of the method well beyond text analytics (see, for example, Minghui Qiu
    et al., [“It Is Not Just What We Say, But How We Say Them: LDA-based Behavior-Topic
    Model”](https://oreil.ly/dnqq5) or Rahji Abdurehman, [“Keyword-Assisted LDA: Exploring
    New Methods for Supervised Topic Modeling”](https://oreil.ly/DDClf)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Blueprint: Visualizing LDA Results'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As LDA is so popular, there is a nice package in Python to visualize the LDA
    results called pyLDAvis.^([3](ch08.xhtml#idm45634187547192)) Fortunately, it can
    directly use the results from sciki-learn for its visualization.
  prefs: []
  type: TYPE_NORMAL
- en: 'Be careful, this takes some time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/btap_08in02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'There is a multitude of information available in the visualization. Let’s start
    with the topic “bubbles” and click the topic. Now take a look at the red bars,
    which symbolize the word distribution in the currently selected topic. As the
    length of the bars is not decreasing quickly, Topic 2 is not very pronounced.
    This is the same effect you can see in the table from [“Blueprint: Creating a
    Topic Model for Paragraphs with LDA”](#ch08-topic-model-para) (look at Topic 1,
    where we have used the array indices, whereas pyLDAvis starts enumerating the
    topics with 1).'
  prefs: []
  type: TYPE_NORMAL
- en: To visualize the results, the topics are mapped from their original dimension
    (the number of words) into two dimensions using principal component analysis (PCA),
    a standard method for dimension reduction. This results in a point; the circle
    is added to see the relative sizes of the topics. It is possible to use T-SNE
    instead of PCA by passing `mds="tsne"` as a parameter in the preparation stage.
    This changes the intertopic distance map and shows fewer overlapping topic bubbles.
    This is, however, just an artifact of projecting the many word dimensions in just
    two for visualization. Therefore, it’s always a good idea to look at the word
    distribution of the topics and not exclusively trust a low-dimensional visualization.
  prefs: []
  type: TYPE_NORMAL
- en: It’s interesting to see the strong overlap of Topics 4, 6, and 10 (“international”),
    whereas Topic 3 (“general assembly”) seems to be far away from all other topics.
    By hovering over the other topic bubbles or clicking them, you can take a look
    at their respective word distributions on the right side. Although not all the
    topics are perfectly separated, there are some (like Topic 1 and Topic 7) that
    are far away from the others. Try to hover over them and you will find that their
    word content is also different from each other. For such topics, it might be useful
    to extract the most representative documents and use them as a training set for
    supervised learning.
  prefs: []
  type: TYPE_NORMAL
- en: pyLDAvis is a nice tool to play with and is well-suited for screenshots in presentations.
    Even though it looks explorative, the real exploration in the topic models takes
    place by modifying the features and the hyperparameters of the algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Using pyLDAvis gives us a good idea how the topics are arranged with respect
    to one another and which individual words are important. However, if we need a
    more qualitative understanding of the topics, we can use additional visualizations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Blueprint: Using Word Clouds to Display and Compare Topic Models'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have used lists to display the topic models. This way, we could nicely
    identify how pronounced the different topics were. However, in many cases topic
    models are used to give you a first impression about the validity of the corpus
    and better visualizations. As we have seen in [Chapter 1](ch01.xhtml#ch-exploration),
    word clouds are a qualitative and intuitive instrument to show this.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can directly use word clouds to show our topic models. The code is easily
    derived from the previously defined `display_topics` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'By using this code, we can qualitatively compare the results of the NMF model
    ([Figure 8-4](#fig-wordcloud-nmf)) with those of the LDA model([Figure 8-5](#fig-wordcloud-lda)).
    Larger words are more important in their respective topics. If many words have
    roughly the same size, the topic is not well-pronounced:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Word Clouds Use Individual Scaling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The font sizes in the word clouds use scaling within each topic separately,
    and therefore it’s important to verify with the actual numbers before drawing
    any final conclusions.
  prefs: []
  type: TYPE_NORMAL
- en: The presentation is now way more compelling. It is much easier to match topics
    between the two methods, like 0-NMF with 8-LDA. For most topics, this is quite
    obvious, but there are also differences. 1-LDA (“people republic”) has no equivalent
    in NMF, whereas 9-NMF (“sustainable development”) cannot be found in LDA.
  prefs: []
  type: TYPE_NORMAL
- en: As we have found a nice qualitative visualization of the topics, we are now
    interested in how that topic distribution has changed over time.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/btap_0804.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-4\. Word clouds representing the NMF topic model.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![](Images/btap_0805.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-5\. Word clouds representing the LDA topic model.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Blueprint: Calculating Topic Distribution of Documents and Time Evolution'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As you can see in the analysis at the beginning of the chapter, the speech metadata
    changes over time. This leads to the interesting question of how the distribution
    of the topics changes over time. It turns out that this is easy to calculate and
    insightful.
  prefs: []
  type: TYPE_NORMAL
- en: 'Like the scikit-learn vectorizers, the topic models also have a `transform`
    method, which calculates the topic distribution of existing documents keeping
    the already fitted topic model. Let’s use this to first separate speeches before
    1990 from those after 1990\. For this, we create NumPy arrays for the documents
    before and after 1990:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we can calculate the respective *W* matrices:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: The result is interesting, as some percentages have changed considerably; specifically,
    the size of the second-to-last topic is much smaller in the later years. We will
    now try to take a deeper look at the topics and their changes over time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s try to calculate the distribution for individual years and see whether
    we can find a visualization to uncover possible patterns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'To make the plots more intuitive, we first create a list of topics with their
    two most important words:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'We then combine the results in a `DataFrame` with the previous topics as column
    names, so we can easily visualize that as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/btap_08in03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In the resulting graph you can see how the topic distribution changes over the
    year.  We can recognize that the “sustainable development” topic is continuously
    increasing, while “south africa” has lost popularity after the apartheid regime
    ended.
  prefs: []
  type: TYPE_NORMAL
- en: Compared to showing the time development of single (guessed) words, topics seem
    to be a more natural entity as they arise from the text corpus itself. Note that
    this chart was generated with an unsupervised method exclusively, so there is
    no bias in it. Everything was already in the debates data; we have just uncovered
    it.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have used scikit-learn exclusively for topic modeling. In the Python
    ecosystem, there is a specialized library for topic models called Gensim, which
    we will now investigate.
  prefs: []
  type: TYPE_NORMAL
- en: Using Gensim for Topic Modeling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apart from scikit-learn, [*Gensim*](https://oreil.ly/Ybn63) is another popular
    tool for performing topic modeling in Python. Compared to scikit-learn, it offers
    more algorithms for calculating topic models and can also give estimates about
    the quality of the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Blueprint: Preparing Data for Gensim'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before we can start calculating the Gensim models, we have to prepare the data.
    Unfortunately, the API and the terminology are different from scikit-learn. In
    the first step, we have to prepare the vocabulary. Gensim has no integrated tokenizer
    and expects each line of a document corpus to be already tokenized:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'After tokenization, we can initialize the Gensim dictionary with these tokenized
    documents. Think of the dictionary as a mapping from words to columns (like the
    features we used in [Chapter 2](ch02.xhtml#ch-api)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Similar to the scikit-learn `TfidfVectorizer`, we can reduce the vocabulary
    by filtering out words that appear not often enough or too frequently. To keep
    the dimensions low, we choose a minimum of five documents in which words must
    appear, but not in more than 70% of the documents. As we saw in [Chapter 2](ch02.xhtml#ch-api),
    these parameters can be optimized and require some experimentation.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Gensim, this is implemented via a filter with the parameters `no_below`
    and `no_above` (in scikit-learn, the analog would be `min_df` and `max_df`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'With the dictionary read, we can now use Gensim to calculate the bag-of-words
    matrix (which is called a *corpus* in Gensim, but we will stick with our current
    terminology):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can perform the TF-IDF transformation. The first line fits the
    bag-of-words model, while the second line transforms the weights:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: The `vectors_gensim_para` matrix is the one that we will use for all upcoming
    topic modeling tasks with Gensim.
  prefs: []
  type: TYPE_NORMAL
- en: 'Blueprint: Performing Nonnegative Matrix Factorization with Gensim'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s check first the results of NMF and see whether we can reproduce those
    of scikit-learn:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'The evaluation can take a while. Although Gensim offers a `show_topics` method
    for directly displaying the topics, we have a different implementation to make
    it look like the scikit-learn results so it’s easier to compare them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Topic 00** nations (0.03)'
  prefs: []
  type: TYPE_NORMAL
- en: united (0.02)
  prefs: []
  type: TYPE_NORMAL
- en: human (0.02)
  prefs: []
  type: TYPE_NORMAL
- en: rights (0.02)
  prefs: []
  type: TYPE_NORMAL
- en: role (0.01) | **Topic 01** africa (0.02)
  prefs: []
  type: TYPE_NORMAL
- en: south (0.02)
  prefs: []
  type: TYPE_NORMAL
- en: people (0.02)
  prefs: []
  type: TYPE_NORMAL
- en: government (0.01)
  prefs: []
  type: TYPE_NORMAL
- en: republic (0.01) | **Topic 02** economic (0.01)
  prefs: []
  type: TYPE_NORMAL
- en: development (0.01)
  prefs: []
  type: TYPE_NORMAL
- en: countries (0.01)
  prefs: []
  type: TYPE_NORMAL
- en: social (0.01)
  prefs: []
  type: TYPE_NORMAL
- en: international (0.01) | **Topic 03** countries (0.02)
  prefs: []
  type: TYPE_NORMAL
- en: developing (0.02)
  prefs: []
  type: TYPE_NORMAL
- en: resources (0.01)
  prefs: []
  type: TYPE_NORMAL
- en: sea (0.01)
  prefs: []
  type: TYPE_NORMAL
- en: developed (0.01) | **Topic 04** israel (0.02)
  prefs: []
  type: TYPE_NORMAL
- en: arab (0.02)
  prefs: []
  type: TYPE_NORMAL
- en: palestinian (0.02)
  prefs: []
  type: TYPE_NORMAL
- en: council (0.01)
  prefs: []
  type: TYPE_NORMAL
- en: security (0.01) |
  prefs: []
  type: TYPE_NORMAL
- en: '| **Topic 05** organization (0.02)'
  prefs: []
  type: TYPE_NORMAL
- en: charter (0.02)
  prefs: []
  type: TYPE_NORMAL
- en: principles (0.02)
  prefs: []
  type: TYPE_NORMAL
- en: member (0.01)
  prefs: []
  type: TYPE_NORMAL
- en: respect (0.01) | **Topic 06** problem (0.01)
  prefs: []
  type: TYPE_NORMAL
- en: solution (0.01)
  prefs: []
  type: TYPE_NORMAL
- en: east (0.01)
  prefs: []
  type: TYPE_NORMAL
- en: situation (0.01)
  prefs: []
  type: TYPE_NORMAL
- en: problems (0.01) | **Topic 07** nuclear (0.02)
  prefs: []
  type: TYPE_NORMAL
- en: co (0.02)
  prefs: []
  type: TYPE_NORMAL
- en: operation (0.02)
  prefs: []
  type: TYPE_NORMAL
- en: disarmament (0.02)
  prefs: []
  type: TYPE_NORMAL
- en: weapons (0.02) | **Topic 08** session (0.02)
  prefs: []
  type: TYPE_NORMAL
- en: general (0.02)
  prefs: []
  type: TYPE_NORMAL
- en: assembly (0.02)
  prefs: []
  type: TYPE_NORMAL
- en: mr (0.02)
  prefs: []
  type: TYPE_NORMAL
- en: president (0.02) | **Topic 09** world (0.02)
  prefs: []
  type: TYPE_NORMAL
- en: peace (0.02)
  prefs: []
  type: TYPE_NORMAL
- en: peoples (0.02)
  prefs: []
  type: TYPE_NORMAL
- en: security (0.01)
  prefs: []
  type: TYPE_NORMAL
- en: states (0.01) |
  prefs: []
  type: TYPE_NORMAL
- en: 'NMF is also a statistical method, so the results are not supposed to be identical
    to the ones that we calculated with scikit-learn, but they are similar enough.
    Gensim has code for calculating the coherence score for topic models, a quality
    indicator. Let’s try this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: The score varies with the number of topics. If you want to find the optimal
    number of topics, a frequent approach is to run NMF for several different values,
    calculate the coherence score, and take the number of topics that maximizes the
    score.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s try the same with LDA and compare the quality indicators.
  prefs: []
  type: TYPE_NORMAL
- en: 'Blueprint: Using LDA with Gensim'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Running LDA with Gensim is as easy as using NMF if we have the data prepared.
    The `LdaModel` class has a lot of parameters for tuning the model; we use the
    recommended values here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'We are interested in the word distribution of the topics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Topic 00** climate (0.12)'
  prefs: []
  type: TYPE_NORMAL
- en: convention (0.03)
  prefs: []
  type: TYPE_NORMAL
- en: pacific (0.02)
  prefs: []
  type: TYPE_NORMAL
- en: environmental (0.02)
  prefs: []
  type: TYPE_NORMAL
- en: sea (0.02) | **Topic 01** country (0.05)
  prefs: []
  type: TYPE_NORMAL
- en: people (0.05)
  prefs: []
  type: TYPE_NORMAL
- en: government (0.03)
  prefs: []
  type: TYPE_NORMAL
- en: national (0.02)
  prefs: []
  type: TYPE_NORMAL
- en: support (0.02) | **Topic 02** nations (0.10)
  prefs: []
  type: TYPE_NORMAL
- en: united (0.10)
  prefs: []
  type: TYPE_NORMAL
- en: human (0.04)
  prefs: []
  type: TYPE_NORMAL
- en: security (0.03)
  prefs: []
  type: TYPE_NORMAL
- en: rights (0.03) | **Topic 03** international (0.03)
  prefs: []
  type: TYPE_NORMAL
- en: community (0.01)
  prefs: []
  type: TYPE_NORMAL
- en: efforts (0.01)
  prefs: []
  type: TYPE_NORMAL
- en: new (0.01)
  prefs: []
  type: TYPE_NORMAL
- en: global (0.01) | **Topic 04** africa (0.06)
  prefs: []
  type: TYPE_NORMAL
- en: african (0.06)
  prefs: []
  type: TYPE_NORMAL
- en: continent (0.02)
  prefs: []
  type: TYPE_NORMAL
- en: terrorist (0.02)
  prefs: []
  type: TYPE_NORMAL
- en: crimes (0.02) |
  prefs: []
  type: TYPE_NORMAL
- en: '| **Topic 05** world (0.05)'
  prefs: []
  type: TYPE_NORMAL
- en: years (0.02)
  prefs: []
  type: TYPE_NORMAL
- en: today (0.02)
  prefs: []
  type: TYPE_NORMAL
- en: peace (0.01)
  prefs: []
  type: TYPE_NORMAL
- en: time (0.01) | **Topic 06** peace (0.03)
  prefs: []
  type: TYPE_NORMAL
- en: conflict (0.02)
  prefs: []
  type: TYPE_NORMAL
- en: region (0.02)
  prefs: []
  type: TYPE_NORMAL
- en: people (0.02)
  prefs: []
  type: TYPE_NORMAL
- en: state (0.02) | **Topic 07** south (0.10)
  prefs: []
  type: TYPE_NORMAL
- en: sudan (0.05)
  prefs: []
  type: TYPE_NORMAL
- en: china (0.04)
  prefs: []
  type: TYPE_NORMAL
- en: asia (0.04)
  prefs: []
  type: TYPE_NORMAL
- en: somalia (0.04) | **Topic 08** general (0.10)
  prefs: []
  type: TYPE_NORMAL
- en: assembly (0.09)
  prefs: []
  type: TYPE_NORMAL
- en: session (0.05)
  prefs: []
  type: TYPE_NORMAL
- en: president (0.04)
  prefs: []
  type: TYPE_NORMAL
- en: secretary (0.04) | **Topic 09** development (0.07)
  prefs: []
  type: TYPE_NORMAL
- en: countries (0.05)
  prefs: []
  type: TYPE_NORMAL
- en: economic (0.03)
  prefs: []
  type: TYPE_NORMAL
- en: sustainable (0.02)
  prefs: []
  type: TYPE_NORMAL
- en: 2015 (0.02) |
  prefs: []
  type: TYPE_NORMAL
- en: The topics are not as easy to interpret as the ones generated by NMF. Checking
    the coherence score as shown earlier, we find a lower score of 0.45270703180962374\.
    Gensim also allows us to calculate the perplexity score of an LDA model. Perplexity
    measures how well a probability model predicts a sample. When we execute `lda_gensim_para.log_perplexity(vectors_gensim_para)`,
    we get a perplexity score of -9.70558947109483.
  prefs: []
  type: TYPE_NORMAL
- en: 'Blueprint: Calculating Coherence Scores'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Gensim can also calculate topic coherence. The method itself is a four-stage
    process consisting of segmentation, probability estimation, a confirmation measure
    calculation, and aggregation. Fortunately, Gensim has a `CoherenceModel` class
    that encapsulates all these single tasks, and we can directly use it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Substituting `nmf` for `lda`, we can calculate the same score for our NMF model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: The score is quite a bit higher, which means that the NMF model is a better
    approximation to the real topics compared to LDA.
  prefs: []
  type: TYPE_NORMAL
- en: 'Calculating the coherence score of the individual topics for LDA is even easier,
    as it is directly supported by the LDA model. Let’s take a look at the average
    first:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'We are also interested in the coherence scores of the individual topics, which
    is contained in `top_topics`. However, the output is verbose (check it!), so we
    try to condense it a bit by just printing the coherence scores together with the
    most important words of the topics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: Coherence scores for topic models can easily be calculated using Gensim. The
    absolute values are difficult to interpret, but varying the methods (NMF versus
    LDA) or the number of topics can give you ideas about which way you want to proceed
    in your topic models. Coherence scores and coherence models are a big advantage
    of Gensim, as they are not (yet) included in scikit-learn.
  prefs: []
  type: TYPE_NORMAL
- en: As it’s difficult to estimate the “correct” number of topics, we are now taking
    a look at an approach that creates hierarchical models and does not need a fixed
    number of topics as a parameter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Blueprint: Finding the Optimal Number of Topics'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous sections, we have always worked with 10 topics. So far we have
    not compared the quality of this topic model to different ones with a lower or
    higher number of topics. We want to find the optimal number of topics in a structured
    way without having to go into the interpretation of each constellation.
  prefs: []
  type: TYPE_NORMAL
- en: 'It turns out there is a way to achieve this. The “quality” of a topic model
    can be measured by the previously introduced coherence score. To find the best
    coherence score, we will now calculate it for a different number of topics with
    an LDA model. We will try to find the highest score, which should give us the
    optimal number of topics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: Coherence Calculations Take Time
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Calculating the LDA model (and the coherence) is computationally expensive,
    so in real life it would be better to optimize the algorithm to calculate only
    a minimal number of models and perplexities. Sometimes it might make sense if
    you calculate the coherence scores for only a few numbers of topics.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we can choose which number of topics produces a good coherence score. Note
    that typically the score grows with the number of topics. Taking too many topics
    makes interpretation difficult:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/btap_08in04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Overall, the graph grows with the number of topics, which is almost always
    the case. However, we can see “spikes” at 13 and 17 topics, so these numbers look
    like good choices. We will visualize the results for 17 topics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Topic 00** peace (0.02)'
  prefs: []
  type: TYPE_NORMAL
- en: international (0.02)
  prefs: []
  type: TYPE_NORMAL
- en: cooperation (0.01)
  prefs: []
  type: TYPE_NORMAL
- en: countries (0.01)
  prefs: []
  type: TYPE_NORMAL
- en: region (0.01) | **Topic 01** general (0.05)
  prefs: []
  type: TYPE_NORMAL
- en: assembly (0.04)
  prefs: []
  type: TYPE_NORMAL
- en: session (0.03)
  prefs: []
  type: TYPE_NORMAL
- en: president (0.03)
  prefs: []
  type: TYPE_NORMAL
- en: mr (0.03) | **Topic 02** united (0.04)
  prefs: []
  type: TYPE_NORMAL
- en: nations (0.04)
  prefs: []
  type: TYPE_NORMAL
- en: states (0.03)
  prefs: []
  type: TYPE_NORMAL
- en: european (0.02)
  prefs: []
  type: TYPE_NORMAL
- en: union (0.02) | **Topic 03** nations (0.07)
  prefs: []
  type: TYPE_NORMAL
- en: united (0.07)
  prefs: []
  type: TYPE_NORMAL
- en: security (0.03)
  prefs: []
  type: TYPE_NORMAL
- en: council (0.02)
  prefs: []
  type: TYPE_NORMAL
- en: international (0.02) | **Topic 04** development (0.03)
  prefs: []
  type: TYPE_NORMAL
- en: general (0.02)
  prefs: []
  type: TYPE_NORMAL
- en: conference (0.02)
  prefs: []
  type: TYPE_NORMAL
- en: assembly (0.02)
  prefs: []
  type: TYPE_NORMAL
- en: sustainable (0.01) | **Topic 05** international (0.03)
  prefs: []
  type: TYPE_NORMAL
- en: terrorism (0.03)
  prefs: []
  type: TYPE_NORMAL
- en: states (0.01)
  prefs: []
  type: TYPE_NORMAL
- en: iraq (0.01)
  prefs: []
  type: TYPE_NORMAL
- en: acts (0.01) |
  prefs: []
  type: TYPE_NORMAL
- en: '| **Topic 06** peace (0.03)'
  prefs: []
  type: TYPE_NORMAL
- en: east (0.02)
  prefs: []
  type: TYPE_NORMAL
- en: middle (0.02)
  prefs: []
  type: TYPE_NORMAL
- en: israel (0.02)
  prefs: []
  type: TYPE_NORMAL
- en: solution (0.01) | **Topic 07** africa (0.08)
  prefs: []
  type: TYPE_NORMAL
- en: south (0.05)
  prefs: []
  type: TYPE_NORMAL
- en: african (0.05)
  prefs: []
  type: TYPE_NORMAL
- en: namibia (0.02)
  prefs: []
  type: TYPE_NORMAL
- en: republic (0.01) | **Topic 08** states (0.04)
  prefs: []
  type: TYPE_NORMAL
- en: small (0.04)
  prefs: []
  type: TYPE_NORMAL
- en: island (0.03)
  prefs: []
  type: TYPE_NORMAL
- en: sea (0.02)
  prefs: []
  type: TYPE_NORMAL
- en: pacific (0.02) | **Topic 09** world (0.03)
  prefs: []
  type: TYPE_NORMAL
- en: international (0.02)
  prefs: []
  type: TYPE_NORMAL
- en: problems (0.01)
  prefs: []
  type: TYPE_NORMAL
- en: war (0.01)
  prefs: []
  type: TYPE_NORMAL
- en: peace (0.01) | **Topic 10** human (0.07)
  prefs: []
  type: TYPE_NORMAL
- en: rights (0.06)
  prefs: []
  type: TYPE_NORMAL
- en: law (0.02)
  prefs: []
  type: TYPE_NORMAL
- en: respect (0.02)
  prefs: []
  type: TYPE_NORMAL
- en: international (0.01) | **Topic 11** climate (0.03)
  prefs: []
  type: TYPE_NORMAL
- en: change (0.03)
  prefs: []
  type: TYPE_NORMAL
- en: global (0.02)
  prefs: []
  type: TYPE_NORMAL
- en: environment (0.01)
  prefs: []
  type: TYPE_NORMAL
- en: energy (0.01) |
  prefs: []
  type: TYPE_NORMAL
- en: '| **Topic 12** world (0.03)'
  prefs: []
  type: TYPE_NORMAL
- en: people (0.02)
  prefs: []
  type: TYPE_NORMAL
- en: future (0.01)
  prefs: []
  type: TYPE_NORMAL
- en: years (0.01)
  prefs: []
  type: TYPE_NORMAL
- en: today (0.01) | **Topic 13** people (0.03)
  prefs: []
  type: TYPE_NORMAL
- en: independence (0.02)
  prefs: []
  type: TYPE_NORMAL
- en: peoples (0.02)
  prefs: []
  type: TYPE_NORMAL
- en: struggle (0.01)
  prefs: []
  type: TYPE_NORMAL
- en: countries (0.01) | **Topic 14** people (0.02)
  prefs: []
  type: TYPE_NORMAL
- en: country (0.02)
  prefs: []
  type: TYPE_NORMAL
- en: government (0.02)
  prefs: []
  type: TYPE_NORMAL
- en: humanitarian (0.01)
  prefs: []
  type: TYPE_NORMAL
- en: refugees (0.01) | **Topic 15** countries (0.05)
  prefs: []
  type: TYPE_NORMAL
- en: development (0.03)
  prefs: []
  type: TYPE_NORMAL
- en: economic (0.03)
  prefs: []
  type: TYPE_NORMAL
- en: developing (0.02)
  prefs: []
  type: TYPE_NORMAL
- en: trade (0.01) | **Topic 16** nuclear (0.06)
  prefs: []
  type: TYPE_NORMAL
- en: weapons (0.04)
  prefs: []
  type: TYPE_NORMAL
- en: disarmament (0.03)
  prefs: []
  type: TYPE_NORMAL
- en: arms (0.03)
  prefs: []
  type: TYPE_NORMAL
- en: treaty (0.02) |
  prefs: []
  type: TYPE_NORMAL
- en: Most of the topics are easy to interpret, but quite a few are difficult (like
    0, 3, 8) as they contain many words with small, but not too different, sizes.
    Is the topic model with 17 topics therefore easier to explain? Not really. The
    coherence measure is higher, but that does not necessarily mean a more obvious
    interpretation. In other words, relying solely on coherence scores can be dangerous
    if the number of topics gets too large. Although in theory higher coherence should
    contribute to better interpretability, it is often a trade-off, and choosing smaller
    numbers of topics can make life easier. Taking a look back at the coherence graph,
    10 seems to be a good value as it is a *local maximum* of the coherence score.
  prefs: []
  type: TYPE_NORMAL
- en: As it’s obviously difficult to find the “correct” number of topics, we will
    now take a look at an approach that creates hierarchical models and does not need
    a fixed number of topics as a parameter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Blueprint: Creating a Hierarchical Dirichlet Process with Gensim'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Take a step back and recall the visualization of the topics in [“Blueprint:
    Using LDA with Gensim”](#ch08usingldawithgensim). The sizes of the topics vary
    quite a bit, and some topics have a large overlap. It would be nice if the results
    gave us broader topics first and some subtopics below them. This is the exact
    idea of the hierarchical Dirichlet process (HDP). The hierarchical topic model
    should give us just a few broad topics that are well separated, then go into more
    detail by adding more words and getting more differentiated topic definitions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'HDP is still quite new and has not yet been extensively analyzed. Gensim is
    also often used in research and has an experimental implementation of HDP integrated.
    As we can directly use our already existing vectorization, it’s not complicated
    to try it. Note that we are again using the bag-of-words vectorization as the
    Dirichlet processes themselves handle frequent words correctly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'HDP can estimate the number of topics and can show all that it identified:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/btap_08in05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The results are sometimes difficult to understand. It can be an option to first
    perform a “rough” topic modeling with only a few topics. If you find out that
    a topic is really big or suspect that it might have subtopics, you can create
    a subset of the original corpus where the only documents included are those that
    have a significant mixture of this topic. This needs some manual interaction but
    often yields much better results compared to HDP. At this stage of development,
    we would not recommend using HDP exclusively.
  prefs: []
  type: TYPE_NORMAL
- en: Topic models focus on uncovering the topic structure of a large corpus of documents.
    As all documents are modeled as a mixture of different topics, they are not well-suited
    for assigning documents to exactly one topic. This can be achieved using clustering.
  prefs: []
  type: TYPE_NORMAL
- en: 'Blueprint: Using Clustering to Uncover the Structure of Text Data'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apart from topic modeling, there is a multitude of other unsupervised methods.
    Not all are suitable for text data, but many clustering algorithms can be used.
    Compared to topic modeling, it is important for us to know that each document
    (or paragraph) gets assigned to exactly one cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Clustering Works Well for Mono-Typical Texts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In our case, it is a reasonable assumption that each document belongs to exactly
    one cluster, as there are probably not too many different things contained in
    one paragraph. For larger text fragments, we would rather use topic modeling to
    take possible mixtures into account.
  prefs: []
  type: TYPE_NORMAL
- en: Most clustering methods need the number of clusters as a parameter, while there
    are a few (like mean-shift) that can guess the correct number of clusters. Most
    of the latter do not work well with sparse data and therefore are not suitable
    for text analytics. In our case, we decided to use k-means clustering, but birch
    or spectral clustering should work in a similar manner. There are a few nice explanations
    of how the k-means algorithm works.^([4](ch08.xhtml#idm45634185352648))
  prefs: []
  type: TYPE_NORMAL
- en: Clustering Is Much Slower Than Topic Modeling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For most algorithms, clustering takes considerable time, much more than even
    LDA. So, be prepared to wait for roughly one hour when executing the clustering
    in the next code fragment.
  prefs: []
  type: TYPE_NORMAL
- en: 'The scikit-learn API for clustering is similar to what we have seen with topic
    models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'But now it’s much easier to find out how many paragraphs belong to which cluster.
    Everything necessary is in the `labels_` field of the `k_means_para` object. For
    each document, it contains the label that was assigned by the clustering algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'In many cases, you might already have found some conceptual problems here.
    If the data is too heterogeneous, most clusters tend to be small (containing a
    comparatively small vocabulary) and are accompanied by a large cluster that absorbs
    all the rest. Fortunately (and due to the short paragraphs), this is not the case
    here; cluster 0 is much bigger than the others, but it’s not orders of magnitude.
    Let’s visualize the distribution with the y-axis showing the size of the clusters
    ([Figure 8-6](#fig-cluster-size)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: Visualizing the clusters works in a similar way to the topic models. However,
    we have to calculate the individual feature contributions manually. For this,
    we add up the TF-IDF vectors of all documents in the cluster and keep only the
    largest values.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/btap_08in06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-6\. Visualization of the size of the clusters.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'These are the weights for their corresponding words. In fact, that’s the only
    change compared to the previous code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/btap_08in07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, the results are (fortunately) not too different from the various
    topic modeling approaches; you might recognize the topics of nuclear weapons,
    South Africa, general assembly, etc. Note, however, that the clusters are more
    pronounced. In other words, they have more specific words. Unfortunately, this
    is not true for the biggest cluster, 1, which has no clear direction but many
    words with similar, smaller sizes. This is a typical phenomenon of clustering
    algorithms compared to topic modeling.
  prefs: []
  type: TYPE_NORMAL
- en: Clustering calculations can take quite long, especially compared to NMF topic
    models. On the positive side, we are now free to choose documents in a certain
    cluster (opposed to a topic model, this is well-defined) and perform additional,
    more sophisticated operations, such as hierarchical clustering, etc.
  prefs: []
  type: TYPE_NORMAL
- en: The quality of the clustering can be calculated by using coherence or the Calinski-Harabasz
    score. These metrics are not optimized for sparse data and take a long time to
    calculate, and therefore we skip this here.
  prefs: []
  type: TYPE_NORMAL
- en: Further Ideas
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we have shown different methods for performing topic modeling.
    However, we have only scratched the surface of the possibilities:'
  prefs: []
  type: TYPE_NORMAL
- en: It’s possible to add n-grams in the vectorization process. In scikit-learn this
    is straightforward by using the `ngram_range` parameter. Gensim has a special
    `Phrases` class for that. Due to the higher TF-IDF weights of n-grams, they can
    contribute considerably to the features of a topic and add a lot of context information.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As we have used years to have time-dependent topic models, you could also use
    countries or continents and find the topics that are most relevant in the speeches
    of their ambassadors.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calculate the coherence score for an LDA topic model using the whole speeches
    instead of the paragraphs and compare the scores.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary and Recommendation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In your daily work, it might turn out that unsupervised methods such as topic
    modeling or clustering are often used as first methods to understand the content
    of unknown text corpora. It is further useful to check whether the right features
    have been chosen or this can still be optimized.
  prefs: []
  type: TYPE_NORMAL
- en: One of the most important decisions is the entity on which you will be calculating
    the topics. As shown in our blueprint example, documents don’t always have to
    be the best choice, especially when they are quite long and consist of algorithmically
    determinable subentities.
  prefs: []
  type: TYPE_NORMAL
- en: Finding the correct number of topics is always a challenge. Normally, this must
    be solved iteratively by calculating the quality indicators. A frequently used,
    more pragmatic approach is to try with a reasonable number of topics and find
    out whether the results can be interpreted.
  prefs: []
  type: TYPE_NORMAL
- en: Using a (much) higher number of topics (like a few hundred), topic models are
    often used as techniques for the dimensionality reduction of text documents. With
    the resulting vectorizations, similarity scores can then be calculated in the
    latent space and frequently yield better results compared to the naive distance
    in TF-IDF space.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Topic models are a powerful technique and are not computationally expensive.
    Therefore, they can be used widely in text analytics. The first and foremost reason
    to use them is uncovering the latent structure of a document corpus.
  prefs: []
  type: TYPE_NORMAL
- en: Topic models are also useful for getting a summarization and an idea about the
    structure of large unknown texts. For this reason, they are often used routinely
    in the beginning of an analysis.
  prefs: []
  type: TYPE_NORMAL
- en: As there is a large number of different algorithms and implementations, it makes
    sense to experiment with the different methods and see which one yields the best
    results for a given text corpus. The linear-algebra-based methods are quite fast
    and make analyses possible by changing the number of topics combined with calculating
    the respective quality indicators.
  prefs: []
  type: TYPE_NORMAL
- en: Aggregating data in different ways before performing topic modeling can lead
    to interesting variations. As we have seen in the UN general debates dataset,
    paragraphs were more suited as the speakers talked about one topic after the other.
    If you have a corpus with texts from many authors, concatenating all texts per
    author will give you persona models for different types of authors.
  prefs: []
  type: TYPE_NORMAL
- en: '^([1](ch08.xhtml#idm45634189170488-marker)) Blei, David M., et al. “Latent
    Dirichlet Allocation.” *Journal of Machine Learning Research* 3 (4–5): 993–1022\.
    doi:10.1162/jmlr.2003.3.4-5.993.'
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch08.xhtml#idm45634187759704-marker)) For a more detailed description,
    see the [Wikipedia page](https://oreil.ly/yr5yA).
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](ch08.xhtml#idm45634187547192-marker)) pyLDAvis must be installed separately
    using **`pip install pyldavis`** or **`conda install pyldavis`**.
  prefs: []
  type: TYPE_NORMAL
- en: ^([4](ch08.xhtml#idm45634185352648-marker)) See, for example, Andrey A. Shabalin’s
    [k-means clustering page](https://oreil.ly/OTGWX) or Naftali Harris’s [“Visualizing
    K-Means Clustering”](https://oreil.ly/Po3bL).
  prefs: []
  type: TYPE_NORMAL
