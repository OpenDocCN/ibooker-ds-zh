- en: Chapter 6\. Handling Missing Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Missing data is a common occurrence in data analysis. In the age of big data,
    many authors and even more practitioners treat it as a minor annoyance that is
    given scant thought: just filter out the rows with missing data—if you go from
    12 million rows to 11 million, what’s the big deal? That still leaves you with
    plenty of data to run your analyses.'
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, filtering out the rows with missing data can introduce significant
    biases in your analysis. Let’s say that older customers are more likely to have
    missing data, for example because they are less likely to set up automated payments;
    by filtering these customers out you would bias your analysis toward younger customers,
    who would be overrepresented in your filtered data. Other common methods to handle
    missing data, such as replacing them by the average value for that variable, also
    introduce biases of their own.
  prefs: []
  type: TYPE_NORMAL
- en: Statisticians and methodologists have developed methods that have much smaller
    or even no bias. These methods have not been adopted broadly by practitioners
    yet, but hopefully this chapter will help you get ahead of the curve!
  prefs: []
  type: TYPE_NORMAL
- en: 'The theory of missing values is rooted in statistics and can easily get very
    mathematical. To make our journey in this chapter more concrete, we’ll work through
    a simulated data set for AirCnC. The business context is that the marketing department,
    in an effort to better understand customer characteristics and motivations, has
    sent out by email a survey to a sample of 2,000 customers in three states and
    collected the following information:'
  prefs: []
  type: TYPE_NORMAL
- en: Demographic characteristics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Age
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Gender
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: State (selecting customers from only three states, which for convenience we’ll
    call A, B, and C)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Personality traits
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Openness
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Extraversion
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Neuroticism
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Booking amount
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To keep things simpler, we’ll assume that the demographic variables are all
    causes of booking amount and unrelated to each other ([Figure 6-1](#the_demographic_variables_cause_booking)).
  prefs: []
  type: TYPE_NORMAL
- en: '![The demographic variables cause booking amount](Images/BEDA_0601.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-1\. The demographic variables cause booking amount
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'As we discussed in [Chapter 2](ch02.xhtml#understanding_behavioral_data), when
    I say that demographic variables such as *Gender* and *Extraversion* are causes
    of *BookingAmount*, I mean two things: first that they are exogenous variables
    (i.e., variables that are primary causes for our purposes), and second that they
    are predictors of *BookingAmount* because of causal effects that are heavily mediated
    as well as moderated by social phenomena.'
  prefs: []
  type: TYPE_NORMAL
- en: For instance, the effect of *Gender* is probably mediated by the person’s income,
    occupation, and family status, among many others. In that sense, it would be more
    accurate to say that *Gender* is a cause of causes of *BookingAmount*. However,
    it is important to note that this effect is *not* confounded, and as such it is
    truly causal.
  prefs: []
  type: TYPE_NORMAL
- en: 'The flow of this chapter will follow the steps you would take when facing a
    new data set: first, we’ll visualize our missing data, to get a rough sense of
    what’s going on. Then, we’ll learn how to diagnose missing data and see the classification
    developed by the statistician Donald Rubin, which is the reference in the matter.
    The last three sections will show how to handle each one of the categories in
    that classification.'
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately for Python users, the excellent R packages that we’ll be using
    don’t have direct Python counterparts. I’ll do my best to show you alternatives
    and workarounds in Python, but the code will be significantly longer and less
    elegant. Sorry!
  prefs: []
  type: TYPE_NORMAL
- en: Data and Packages
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One of the luxuries of using simulated data is that we know the true values
    for the missing data. The [GitHub folder for this chapter](https://oreil.ly/BehavioralDataAnalysisCh6)
    contains three data sets ([Table 6-1](#variables_in_our_dat)):'
  prefs: []
  type: TYPE_NORMAL
- en: The complete data for our four variables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The “available” data where some values are missing for some of the variables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A secondary data set of auxiliary variables that we’ll use to complement our
    analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Table 6-1\. Variables in our data
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Variable description | chap6-complete_data.csv | chap6-available_data.csv
    | chap6-available_data_supp.csv |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| *Age* | Age of customer | Complete | Complete |  |'
  prefs: []
  type: TYPE_TB
- en: '| *Open* | Openness psychological trait, 0-10 | Complete | Complete |  |'
  prefs: []
  type: TYPE_TB
- en: '| *Extra* | Extraversion psychological trait, 0-10 | Complete | Partial |  |'
  prefs: []
  type: TYPE_TB
- en: '| *Neuro* | Neuroticism psychological trait, 0-10 | Complete | Partial |  |'
  prefs: []
  type: TYPE_TB
- en: '| *Gender* | Categorical variable for customer gender, F/M | Complete | Complete
    |  |'
  prefs: []
  type: TYPE_TB
- en: '| *State* | Categorical variable for customer state of residence, A/B/C | Complete
    | Partial |  |'
  prefs: []
  type: TYPE_TB
- en: '| *Bkg_amt* | Amount booked by customer | Complete | Partial |  |'
  prefs: []
  type: TYPE_TB
- en: '| *Insurance* | Amount of travel insurance purchased by customer |  |  | Complete
    |'
  prefs: []
  type: TYPE_TB
- en: '| *Active* | Numeric measure of how active the customer bookings are |  |  |
    Complete |'
  prefs: []
  type: TYPE_TB
- en: 'In this chapter, we’ll use the following packages in addition to the common
    ones:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Visualizing Missing Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: By definition, missing data is hard to visualize. Univariate methods (i.e.,
    one variable at a time) will only get us so far, so we’ll mostly rely on bivariate
    methods, plotting two variables against each other to tease out some insights.
    Used in conjunction with causal diagrams, bivariate graphs will allow us to visualize
    relationships that would otherwise be very complex to grasp.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our first step is to get a sense of “how” our data is missing. The mice package
    in R has a very convenient function `md.pattern()` to visualize missing data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The `md.pattern()` function returns a table where each row represents a pattern
    of data availability. The first row has a “1” for each variable, so it represents
    complete records. The number on the left of the table indicates the number of
    rows with that pattern, and the number on the right indicates the number of fields
    that are missing in that pattern. We have 368 complete rows in our data. The second
    row has a “0” for *Neuroticism* only, so it represents records where only *Neuroticism*
    is missing; we have 358 such rows. The numbers at the bottom of the table indicate
    the number of missing values for the corresponding variables, and the variables
    are ordered by increasing number of missing values. *Neuroticism* is the last
    variable to the right, which means it has the highest number of missing values,
    1,000\. This function also conveniently returns a visual representation of the
    table ([Figure 6-2](#patterns_of_missing_data)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Patterns of missing data](Images/BEDA_0602.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-2\. Patterns of missing data
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'As we can see in [Figure 6-2](#patterns_of_missing_data), the variables *Age*,
    *Openness,* and *Gender* don’t have any missing data, but all the other variables
    do. We can obtain the same results in Python with an ad hoc function I wrote,
    although in a less readable format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is composed of two tables:'
  prefs: []
  type: TYPE_NORMAL
- en: The first table indicates the total number of missing values for each variable
    in our data, as seen at the bottom of [Figure 6-2](#patterns_of_missing_data).
    *Extraversion* has 793 missing values, and so on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second table represents the details of each missing data pattern. The variables
    above the logical values on the left (i.e., *Extraversion, Neuroticism, State,
    BookingAmount)* are the ones with some missing values in the data. Each line of
    the table indicates the number of rows with a certain pattern of missing data.
    The first line is made of four `False`, i.e., the pattern where none of the variables
    has missing data, and there are 368 such rows in our data, as you’ve seen previously
    in the first line of [Figure 6-2](#patterns_of_missing_data). The second line
    only changes the last `False` to `True`, with the first three `False` omitted
    for readability (i.e., any blank logical value should be read up). This pattern
    `False``/``False``/``False``/``True` happens when only *BookingAmount* has a missing
    value, which happens in 33 rows of our data, and so on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Even with such a small data set, this visualization is very rich and it can
    be hard to know what to look for. We will explore two aspects:'
  prefs: []
  type: TYPE_NORMAL
- en: Amount of missing data
  prefs: []
  type: TYPE_NORMAL
- en: How much of our data is missing? For which variables do we have the highest
    percentage of missing data? Can we just discard rows with missing data?
  prefs: []
  type: TYPE_NORMAL
- en: Correlation of missingness
  prefs: []
  type: TYPE_NORMAL
- en: Is data missing at the individual or variable level?
  prefs: []
  type: TYPE_NORMAL
- en: Amount of Missing Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first order of business is to determine how much of our data is missing
    and which variables have the highest percentage of missing data. We can find the
    necessary values at the bottom of [Figure 6-2](#patterns_of_missing_data), with
    the number of missing values per variable, in increasing order of missingness,
    or at the bottom of the Python output. If the amount of missing data is very limited,
    e.g., you have a 10-million-row data set where no variable has more than 10 missing
    values, then handling them properly through multiple imputation would be overkill
    as we’ll see later. Just drop all the rows with missing data and be done with
    it. The rationale here is that even if the missing values are extremely biased,
    there are too few of them to materially influence the outcomes of your analysis
    in any way.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our example, the variable with the highest number of missing values is *Neuroticism*,
    with 1,000 missing values. Is that a lot? Where is the limit? Is it 10, 100, 1,000
    rows or more? It depends on the context. A quick-and-dirty strategy that you can
    use is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Take the variable with the highest number of missing values and create two
    new data sets: one where all the missing values are replaced by the minimum of
    that variable and one where they are replaced by the maximum of that variable.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run a regression for that variable’s most important relationship with each of
    the three data sets you now have. For example, if that variable is a predictor
    of your effect of interest, then run that regression.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'If the regression coefficient is not materially different across the three
    regressions, i.e., you would draw the same business implications or take the same
    actions based on the different values, then you’re below the limit and you can
    drop the missing data. In simpler words: would these numbers mean the same thing
    to your business partners? If so, you can drop the missing data.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This rule of thumb is easily applied to numeric variables, but what about binary
    or categorical variables?
  prefs: []
  type: TYPE_NORMAL
- en: For binary variables, the minimum will be 0 and the maximum will be 1, and that’s
    OK. The two data sets you create translate into a best-case scenario and a worst-case
    scenario.
  prefs: []
  type: TYPE_NORMAL
- en: 'For categorical variables, the minimum and maximum rule must be slightly adjusted:
    replace all the missing values with either the least frequent or the most frequent
    category.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s do that here, for example for *Neuroticism*. *Neuroticism* is a predictor
    of our effect of interest, *BookingAmount*, so we’ll use that relationship as
    indicated earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The results are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The coefficient based on the available data is −5.9.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The coefficient based on replacing the missing values with the minimum of *Neuroticism*
    is −8.0.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The coefficient based on replacing the missing values with the maximum of *Neuroticism*
    is 2.7.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These values are very different from each other, to the point of having different
    signs, therefore we’re definitely above the threshold for material significance.
    We can’t simply drop the rows that have missing data for *Neuroticism*. Applying
    the same approach to the other variables would also show that we can’t disregard
    their missing values and need to handle them adequately.
  prefs: []
  type: TYPE_NORMAL
- en: Correlation of Missingness
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once we have determined which variables we need to deal with, we’ll want to
    know how much their missingness is correlated. If you have variables whose missingness
    is highly correlated, this suggests that the missingness of one causes the missingness
    of others (e.g., if someone stops answering a survey halfway through, then all
    answers after a certain point will be missing). Alternatively, their missingness
    may have a common cause (e.g., if some subjects are more reluctant to reveal information
    about themselves). In both of these cases, identifying correlation in missingness
    will help you build a more accurate CD, which will save you time and make your
    analyses more effective.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at it through a simple illustration: imagine that we have interview
    data for two offices: Tampa and Tacoma. In both offices, candidates must go through
    the same mandatory three interview sections, but in Tampa the first interviewer
    is responsible for recording all the scores of a candidate whereas in Tacoma each
    interviewer records the score for their section. Interviewers are human beings
    and they sometimes forget to turn in the data to HR. In Tampa, when an interviewer
    forgets to turn in the data, we have no data whatsoever for the candidate, apart
    from their ID in the system ([Figure 6-3](#highly_correlated_missingness_in_tampa)
    shows the data for Tampa only).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Highly correlated missingness in Tampa data](Images/BEDA_0603.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-3\. Highly correlated missingness in Tampa data
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The telltale sign for high missingness correlation is a line with a large number
    of light-shaded squares (here 3) that represent a high number of cases (here 400
    out of 2,000 total rows). In addition, the figure has no line with only one or
    two light squares.
  prefs: []
  type: TYPE_NORMAL
- en: In such a situation, it wouldn’t make sense to analyze our missing data one
    variable at a time. If we find that data for the first section is highly likely
    to be missing when Murphy is the first interviewer, then it will also be true
    for the other sections. (You had one job, Murphy!)
  prefs: []
  type: TYPE_NORMAL
- en: In Tacoma, on the other hand, the missingness of the different sections is entirely
    uncorrelated ([Figure 6-4](#uncorrelated_missingness_in_tacomaapost)).
  prefs: []
  type: TYPE_NORMAL
- en: 'The pattern is the opposite of Tampa’s:'
  prefs: []
  type: TYPE_NORMAL
- en: We have a high number of lines with few missing variables (see all the 1s and
    2s on the right of the figure).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These lines represent the bulk of our data (we can see on the left that only
    17 rows have 3 missing variables).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lines with a high number of light squares at the bottom of the figure represent
    very few cases (the same 17 individuals) because they are the result of independent
    randomness.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Uncorrelated missingness in Tacoma’s data](Images/BEDA_0604.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-4\. Uncorrelated missingness in Tacoma’s data
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The argument for the last bullet point can be extended by looking more broadly
    at what we could call “Russian dolls” sequences of increasing missingness where
    each pattern adds a missing variable on the previous pattern, for example (I3)
    → (I3, I2) → (I3, I2, I1). The corresponding numbers of cases are 262 → 55 → 17\.
    These numbers form a decreasing sequence, which is logical because if the missingness
    of the variables is completely uncorrelated, we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Prob*(*I*3 *missing & I*2 *missing*) = *Prob*(*I*3 *missing*) * *Prob*(*I*2
    *missing*)'
  prefs: []
  type: TYPE_NORMAL
- en: '*Prob*(*I*3 *missing & I*2 *missing & I*1 *missing*) = *Prob*(*I*3 *missing*)
    * *Prob*(*I*2 *missing*) ** Prob*(*I*1 *missing*)'
  prefs: []
  type: TYPE_NORMAL
- en: 'With a small sample and/or very high levels of missingness, these equations
    may not hold exactly true in our data, but if less than 50% of any variable is
    missing we should generally have:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Prob*(*I*3 *missing* & *I*2 *missing* & *I*1 *missing*) < *Prob*(*I*3 *missing*
    & *I*2 *missing*) < *Prob*(*I*3 *missing*)'
  prefs: []
  type: TYPE_NORMAL
- en: In real-life situations, it would be rather cumbersome to test all these inequalities
    by yourself, although you could write a function to do so at scale. Instead, I
    would recommend looking at the visualization for any significant outlier (i.e.,
    a value for several variables much larger than the values for some of the same
    variables).
  prefs: []
  type: TYPE_NORMAL
- en: 'More broadly, this visualization is easy to use with only a few variables.
    As soon as you have a large number of variables, you’ll have to build and visualize
    the correlation matrix for missingness:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[Figure 6-5](#correlation_matrices_for_completely_cor) shows the resulting
    correlation matrices. In the one on the left, for Tampa, all the values are equal
    to 1: if one variable is missing, then the other two are as well. In the correlation
    matrix on the right, for Tacoma, the values are equal to 1 on the main diagonal
    but 0 everywhere else: knowing that one variable is missing tells you nothing
    about the missingness of the others.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Correlation matrices for completely correlated missingness (left) and completely
    uncorrelated missingness (right)](Images/BEDA_0605.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-5\. Correlation matrices for completely correlated missingness (left)
    and completely uncorrelated missingness (right)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Let’s get back to our AirCnC data set and see where it falls between the two
    extremes outlined in our theoretical interview example. [Figure 6-6](#patterns_of_missing_data_left_parenthes)
    repeats [Figure 6-2](#patterns_of_missing_data) for ease of access.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 6-6](#patterns_of_missing_data_left_parenthes) falls somewhere in the
    middle: all possible patterns of missingness are fairly represented, which suggests
    that we don’t have strongly clustered sources of missingness.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Patterns of missing data (repeats)](Images/BEDA_0606.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-6\. Patterns of missing data (repeats [Figure 6-2](#patterns_of_missing_data))
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '[Figure 6-7](#correlation_matrix_of_missingness_in_ou) shows the correlation
    matrix of missingness for our AirCnC data. As you can see, the missingness of
    our variables is almost entirely uncorrelated, well within the range of random
    fluctuations. If you want to familiarize yourself more with correlation patterns
    in missingness, one of the [exercises for this chapter on GitHub](https://oreil.ly/BehavioralDataAnalysisCh6)
    asks you to identify a few of them. As a reminder, looking at correlation patterns
    is never necessary in itself, but it can often be illuminating and save you time.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Correlation matrix of missingness in our AirCnC data](Images/BEDA_0607.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-7\. Correlation matrix of missingness in our AirCnC data
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Diagnosing Missing Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have visualized our missing data, it’s time to understand what causes
    it. This is where causal diagrams come in, as we’ll use them to represent the
    causal mechanisms of missing data.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start with a very simple example from [Chapter 1](ch01.xhtml#the_causal_behavioral_framework_for_da).
    When introducing causal diagrams, I mentioned that unobserved variables, such
    as a customer’s taste for vanilla ice cream, are represented in a darker shaded
    rectangle ([Figure 6-8](#unobserved_variables_are_represented_in)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Unobserved variables are represented in an oval](Images/BEDA_0608.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-8\. Unobserved variables are represented in a darker-shaded rectangle
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Unobserved variables, which are sometimes called “latent” variables in certain
    disciplines, refer to information that we don’t have in practice, even though
    it may or may not in theory be accessible. In the present case, let’s say that
    we force our customers to disclose their taste for vanilla before making a purchase.
    This would create the corresponding data in our systems, which we would then use
    for our data analyses ([Figure 6-9](#collecting_previously_unobserved_inform)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Collecting previously unobserved information](Images/BEDA_0609.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-9\. Collecting previously unobserved information
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: However, it is generally poor business practice to try and force customers to
    disclose information they don’t want to, and it’s often left optional. More generally,
    a large amount of data is collected on some customers but not others. We’ll represent
    that situation by drawing the corresponding box in the CD with dashes ([Figure 6-10](#representing_partially_observed_variabl)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Representing partially observed variables with a dashed box](Images/BEDA_0610.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-10\. Representing partially observed variables with a dashed box
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: For example, with three customers, we could have the following data, with one
    customer refusing to disclose their taste for vanilla ice cream ([Table 6-2](#the_data_underlying_our_cd)).
  prefs: []
  type: TYPE_NORMAL
- en: Table 6-2\. The data underlying our CD
  prefs: []
  type: TYPE_NORMAL
- en: '| Customer name | Taste for vanilla | Stated taste | Purchased IC at stand
    (Y/N) |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Ann | Low | Low | N |'
  prefs: []
  type: TYPE_TB
- en: '| Bob | High | High | Y |'
  prefs: []
  type: TYPE_TB
- en: '| Carolyn | High | N/A | Y |'
  prefs: []
  type: TYPE_TB
- en: In this chapter, we’re interested in understanding what causes missingness of
    a variable, and not just what causes the values of a variable. Therefore, we’ll
    create a variable to track when the stated taste variable is missing ([Table 6-3](#adding_a_missingness_variable)).
  prefs: []
  type: TYPE_NORMAL
- en: Table 6-3\. Adding a missingness variable
  prefs: []
  type: TYPE_NORMAL
- en: '| Customer name | Taste for vanilla | Stated taste for vanilla | Stated taste
    missing (Y/N) | Purchased IC at stand |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Ann | Low | Low | N | N |'
  prefs: []
  type: TYPE_TB
- en: '| Bob | High | High | N | Y |'
  prefs: []
  type: TYPE_TB
- en: '| Carolyn | High | N/A | Y | Y |'
  prefs: []
  type: TYPE_TB
- en: Let’s add that variable to our CD ([Figure 6-11](#adding_missingness_to_our_causal_diagra)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Adding missingness to our causal diagram](Images/BEDA_0611.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-11\. Adding missingness to our causal diagram
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We conventionally make missingness a cause of the corresponding partially observed
    variable. The intuition is that the information exists fully in the unobserved
    variable, and the partially observed variable is equal to the unobserved variable,
    unless information is “hidden” by the missingness variable. This convention will
    make our lives much easier, because it will allow us to express and discuss causes
    of missingness in the CD that represent our relationships of interest, instead
    of having to consider missingness separately.
  prefs: []
  type: TYPE_NORMAL
- en: Now that missingness is part of our CD, the natural next step is to ask ourselves,
    “What causes it?”
  prefs: []
  type: TYPE_NORMAL
- en: 'Causes of Missingness: Rubin’s Classification'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are three basic and mutually exclusive possibilities for what causes missingness
    of a variable, which have been categorized by statistician Donald Rubin.
  prefs: []
  type: TYPE_NORMAL
- en: First, if the missingness of a variable depends only on variables outside of
    our data, such as purely random factors, that variable is said to be *m**issing
    completely at random* (MCAR) ([Figure 6-12](#stated_taste_is_missing_completely_at_r)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Stated taste is missing completely at random](Images/BEDA_0612.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-12\. Stated taste is missing completely at random
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Then, a variable goes from MCAR to *missing at random* (MAR) if even one variable
    in our data affects its missingness. Variables outside of our data and random
    factors may also play a role, but the value of the variable in question may not
    affect its own missingness. This would be the case for instance if *Purchased*
    caused the missingness of *StatedVanillaTaste*, e.g., because we only interview
    customers who purchased instead of passersby ([Figure 6-13](#stated_taste_is_missing_at_random)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Stated taste is missing at random](Images/BEDA_0613.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-13\. Stated taste is missing at random
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Finally, any variable whose value influences its own missingness is *m**issing
    not at random* (MNAR), even if other variables inside or outside of the data also
    affect the missingness. Other variables inside or outside of our data may also
    play a role, but a variable goes from MCAR or MAR to MNAR as soon as the variable
    influences its own missingness. In our example, this would mean that *VanillaTaste*
    causes the missingness of *StatedVanillaTaste* ([Figure 6-14](#stated_taste_is_missing_not_at_randomdo)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Stated taste is missing not at random](Images/BEDA_0614.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-14\. Stated taste is missing not at random
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We represent the idea that the values of a variable influence its missingness
    by drawing an arrow from the unobserved variable, and not the partially observed
    variable. This way, we can make meaningful statements such as “all values that
    are in reality below a certain threshold are missing in our data.” If the arrow
    came from the partially observable variable, we would be stuck with uninformative
    statements such as “values that are missing cause themselves to be missing.”
  prefs: []
  type: TYPE_NORMAL
- en: In an ideal world, the rest of the section would consist of recipes to identify
    each category of missingness. Unfortunately, missing data analysis is still an
    open area of research that has not yet been fully explored. In particular, how
    missingness and causality interact is not well understood. Therefore, dealing
    with missing data remains more art than science. Trying to create systematic recipes
    would require dealing with an intractable number of exceptions, as well as introducing
    circular arguments such as “pattern X indicates that variable 1 is MAR unless
    variable 2 is MNAR; pattern Y indicates that variable 2 is MNAR unless variable
    1 is MAR.” I have done my best to cover as many cases as possible within a limited
    data set, but in the real world you might encounter situations that are “a little
    bit of this and a little bit of that” and you’ll have to make judgment calls as
    to how to proceed.
  prefs: []
  type: TYPE_NORMAL
- en: Some good news, however, is that with a few exceptions that I’ll call out, being
    cautious takes more time but doesn’t introduce bias. When you’re uncertain whether
    a variable is MCAR, MAR, or MNAR, just assume the worst of the possible scenarios
    and your analyses will be as unbiased as they can be.
  prefs: []
  type: TYPE_NORMAL
- en: 'With that caveat in mind, let’s get back to our AirCnC data and see how we
    can diagnose missingness in a realistic data set. As a quick refresher, our data
    set contains the following variables:'
  prefs: []
  type: TYPE_NORMAL
- en: Demographic characteristics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Age
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Gender
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: State (A, B, and C)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Personality traits
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Openness
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Extraversion
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Neuroticism
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Booking amount
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Diagnosing MCAR Variables
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'MCAR variables are the simplest case. A sensor went faulty, a bug prevented
    data transmission from a customer’s mobile app, or a customer just missed the
    field to enter their taste for vanilla ice cream. Regardless, missingness happens
    in a way that is intuitively “random.” We diagnose MCAR variables by default:
    if a variable doesn’t appear to be MAR, we’ll treat it as MCAR. In other words,
    you can think of MCAR as our null hypothesis in the absence of evidence to the
    contrary.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The main tool that we’ll use to diagnose missingness is a logistic regression
    of whether a variable is missing on all the other variables in our data set. Let’s
    look, for example, at the *Extraversion* variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: None of the variables has a large and strongly statistically significant coefficient.
    In the absence of any other evidence, this suggests that the source of missingness
    for *Extraversion* is purely random and we’ll treat our *Extraversion* variable
    as MCAR.
  prefs: []
  type: TYPE_NORMAL
- en: You can think of MCAR data as rolling dice or flipping a coin. Both of these
    actions are “random” from our perspective, but they still obey the laws of physics.
    Theoretically, if we had enough information and computing power, the outcome would
    be entirely predictable. The same could happen here. By saying that *Extraversion*
    is MCAR, we’re not saying “the missingness of *Extraversion* is fundamentally
    random and unpredictable,” we’re just saying “none of the variables *currently
    included* in our analysis is correlated with the missingness of *Extraversion.*”
    But maybe—and even probably—other variables (conscientiousness? trust? familiarity
    with technology?) would be correlated. Our goal is not to make a philosophical
    statement about *Extraversion*, but to determine if its missingness may bias our
    analyses, given the data currently available.
  prefs: []
  type: TYPE_NORMAL
- en: Diagnosing MAR Variables
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'MAR variables are variables whose missingness depends on the values of other
    variables in our data. If other variables in our data set are predictive of a
    variable’s missingness, then MAR becomes our default hypothesis for that variable,
    unless we have strong enough evidence that it’s MNAR. Let’s see what this looks
    like with the *State* variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '*Age* is mildly significant, with a positive coefficient. In other words, older
    customers appear less likely to provide their state. The corresponding causal
    diagram is represented in [Figure 6-15](#gender_missing_at_random).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Gender missing at random](Images/BEDA_0615.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-15\. Gender missing at random
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We can confirm that correlation by plotting the density of *State* missingness
    by recorded *Age* ([Figure 6-16](#density_of_missing_and_observed_state)). *State*
    has more observed values for younger customers than for older customers, or conversely,
    more missing values for older customers than for younger customers.
  prefs: []
  type: TYPE_NORMAL
- en: '![Density of missing and observed State data, by observed Age](Images/BEDA_0616.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-16\. Density of missing and observed State data, by observed Age
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: One limitation of this density plot is that it doesn’t show the rows where the
    X variable (here *Age*) is also missing. This could be problematic or misleading
    when that variable also has missing values. A possible trick is to replace the
    missing values for the X variable by a nonsensical value, such as −10\. *Age*
    doesn’t have any missing value, so instead we’ll use as our X variable *Extraversion*,
    which has missing values. Let’s plot the density of observed and missing *State*
    data, by values of *Extraversion* ([Figure 6-17](#density_of_missing_and_observed_state_d)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Density of missing and observed State data by level of Extraversion, including
    missing Extraversion](Images/BEDA_0617.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-17\. Density of missing and observed State data by level of Extraversion,
    including missing Extraversion
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '[Figure 6-17](#density_of_missing_and_observed_state_d) shows that among individuals
    with no data for *Extraversion*, there are disproportionately more individuals
    for which we observe *State* than individuals for which *State* is missing. Overall,
    we’re seeing strong evidence that *State* is not MCAR but indeed MAR, because
    its missingness appears correlated with other variables available in our data
    set.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You may have noticed that I used the word “correlation” earlier when talking
    about the relationship between *Age* (or *Extraversion*) and the missingness of
    *State*. Indeed, we’ve only shown correlation so far, and it is entirely possible
    that *Age* doesn’t cause missingness of *State* but that they are both caused
    by a third unobserved variable. Fortunately, when talking about missingness, the
    causal nature of a correlation (or lack thereof) doesn’t affect our analyses.
    Loosely equating the two will not introduce bias because we’ll never actually
    deal with the coefficient for that relationship.
  prefs: []
  type: TYPE_NORMAL
- en: Diagnosing MNAR Variables
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'MNAR variables are variables whose missingness depends on their own values:
    higher values are more likely to be missing than lower values, or vice versa.
    This situation is both the most problematic for data analysis and the trickiest
    to diagnose. It is trickiest to diagnose because, by definition, we don’t know
    the values that are missing. Therefore, we’ll need to do a bit more sleuthing.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at the *Neuroticism* variable, and as before, start by running a
    regression of its missingness on the other variables in the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: We can see that *BookingAmount* has a strongly significant coefficient. On the
    face of it, this would suggest that *Neuroticism* is MAR on *BookingAmount*. However,
    and this is a key clue, *BookingAmount* is a child of *Neuroticism* in our CD.
    From a behavioral perspective, it also seems more likely that *Neuroticism* is
    MNAR rather than MAR on *BookingAmount* (i.e., the missingness is driven by a
    personality trait rather than by the amount a customer spent).
  prefs: []
  type: TYPE_NORMAL
- en: A way to confirm our suspicion is to identify another child of the variable
    with missing data, ideally as correlated with it as possible and as little correlated
    as possible with the first child. In our secondary data set, we have data about
    the total amount of travel insurance that customers purchased over their lifetime
    with the company. The fee per trip depends on trip characteristics that are only
    very loosely correlated with the booking amount, so we’re good on that front.
    Adding *Insurance* to our data set, we find that it’s strongly predictive of the
    missingness of *Neuroticism*, and that the distributions of *Insurance* amount
    with observed and missing *Neuroticism* are vastly different from each other ([Figure 6-18](#density_of_missing_and_observed_neuroti)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Density of missing and observed Neuroticism data, by observed Insurance amount](Images/BEDA_0618.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-18\. Density of missing and observed Neuroticism data, by observed
    Insurance amount
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The more children variables we find correlated with the missingness of *Neuroticism*,
    the stronger our case becomes that the variable is MNAR. As we’ll see later, the
    way to handle a MNAR variable is to add auxiliary variables to our imputation
    model, and our children variables are perfect candidates for that, so finding
    several of them is not a waste of time but a head start for the next step.
  prefs: []
  type: TYPE_NORMAL
- en: 'Technically, we can never fully prove that a variable is MNAR and not just
    MAR on several of its children, but that is not an issue: auxiliary variables
    do not bias the imputation if the original variable is indeed MAR and not MNAR.'
  prefs: []
  type: TYPE_NORMAL
- en: Missingness as a Spectrum
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Rubin’s classification relies on binary tests. For example, as soon as a variable
    is more likely to be missing for higher values than for lower values (or vice
    versa), it is MNAR, regardless of any other consideration. However, the shape
    of that relationship between values and missingness matters for practical purposes:
    if *all* values of a variable are missing above or below a certain threshold,
    we’ll need to handle this variable differently from our default approach. This
    situation can also occur with MAR variables so it’s worth taking a step back and
    thinking more broadly about shapes of missingness.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can think of the missingness of a variable as falling on a spectrum from
    fully probabilistic to fully deterministic. At the “probabilistic” end of the
    spectrum, the variable is MCAR and all values are as likely to be missing. At
    the “deterministic” end of the spectrum, there is a threshold: the values are
    missing for all individuals on one side of the threshold and available for all
    individuals on the other side of the threshold. This often results from the application
    of a business rule. For example, in a hiring context, if only candidates with
    a GPA above 3.0 get interviewed, you wouldn’t have any interview score for candidates
    below that threshold. This would make *InterviewScore* MAR on *GPA* ([Figure 6-19](#interview_score_being_mar_on_gpa)).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Interview score being MAR on GPA](Images/BEDA_0619.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-19\. Interview score being MAR on GPA
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Rubin’s classification of MCAR/MAR/MNAR is based solely on what the cause of
    missingness is. It doesn’t take into account whether that causal relationship
    shows randomness or not. Here, counterintuitively, the fact that the missingness
    of *InterviewScore* is based deterministically on *GPA* makes *InterviewScore
    MAR* on *GPA* even though there’s no randomness involved.
  prefs: []
  type: TYPE_NORMAL
- en: This can also happen for variables that are MNAR, where only values above or
    below a certain threshold get recorded. For instance, only values outside of a
    normal range may be saved in a file, or only people under or above a certain threshold
    will register themselves (e.g., for tax purposes).
  prefs: []
  type: TYPE_NORMAL
- en: In between these two extremes of complete randomness and complete determinism
    (either of the MAR or MNAR type), there are situations where the probability of
    missingness increases or decreases continuously based on the values of the cause
    of missingness.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 6-20](#missingness_spectrumcomma_from_mcar_lef) shows what this looks
    like in the simplest case of two variables, X and Y, where X has missing values.
    For the sake of readability, available values are shown as full squares, whereas
    “missing” values are shown as crosses. The first row of [Figure 6-20](#missingness_spectrumcomma_from_mcar_lef)
    shows scatterplots of Y against X and the second row shows for each one of these
    a line plot of the relationship between X and the probability of missingness:'
  prefs: []
  type: TYPE_NORMAL
- en: The leftmost column shows X being MCAR. The probability of missingness is constant
    at 0.5 and independent of X. Squares and crosses are spread similarly across the
    plot.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The central columns show X being probabilistically MNAR with increasing strength.
    Squares are more common on the left of the plot and crosses more common on the
    right, but there are still crosses on the left and squares on the right.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The rightmost column shows X being deterministically MNAR. All values of X below
    5 are available (squares) and all the values above 5 are “missing” (crosses).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Missingness spectrum, from MCAR (leftmost) to deterministic MNAR (rightmost)
    through probabilistic MNAR (center)](Images/BEDA_0620.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-20\. Missingness spectrum, from MCAR (leftmost) to deterministic MNAR
    (rightmost) through probabilistic MNAR (center)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This spectrum of missingness is rarely discussed in statistical treatments of
    missing data, because it is difficult to confirm through purely mathematical methods.
    But this is a book about behavioral analytics, so we can and should use common
    sense and business knowledge. In the GPA example, the threshold in data results
    from the application of a business rule that you should be aware of. In most situations,
    you expect a variable to be in a certain range of values, and you should have
    a sense of how likely it is for a possible value to not be represented in your
    data.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our AirCnC survey data, we have three personality traits: *Openness*, *Extraversion*,
    and *Neuroticism*. In real life, these variables would result from the aggregation
    of the answers to several questions and would have a bell-shaped distribution
    over a known interval (see Funder (2016) for a good introduction to personality
    psychology). Let’s assume that the relevant interval is 0 to 10 in our data and
    let’s look at the distribution of our variables ([Figure 6-21](#histograms_of_personality_traits_in_our)).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Histograms of personality traits in our data](Images/BEDA_0621.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-21\. Histograms of personality traits in our data
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Clearly, something is going on with *Neuroticism*. Based on how the personality
    traits are constructed, we would expect the same type of curve for all three variables,
    and we would certainly not expect to have a large number of customers with a value
    of 5, and none with a value of 4\. This overwhelmingly suggests a deterministically
    MNAR variable, which we’ll have to handle accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: You should now be able to form a reasonable opinion of the pattern of missingness
    in a data set. How many missing values are there? Does their missingness appear
    related to the values of the variable itself (MNAR), another variable (MAR), or
    neither (MCAR)? Are these missingness relationships probabilistic or deterministic?
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 6-22](#decision_tree_to_diagnose_missing_data) presents a decision
    tree recapping our logic to diagnose missing data.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Decision tree to diagnose missing data](Images/BEDA_0622.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-22\. Decision tree to diagnose missing data
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In the next section, we’ll see how to deal with missing data in each of these
    cases.
  prefs: []
  type: TYPE_NORMAL
- en: Handling Missing Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The first thing to keep in mind as we get into the how-to section of the chapter
    is that we’re not trying to deal with missing data for the sake of it: our goal
    is to obtain unbiased and accurate estimates of causal relationships in our data.
    Missing data is problematic only to the extent that it interferes with that goal.'
  prefs: []
  type: TYPE_NORMAL
- en: This bears emphasizing, because your first instinct might be that the outcome
    of successfully addressing missing data is a data set with no missing data, and
    that’s just not the case. The method we’ll be using, multiple imputation (MI),
    creates multiple copies of your data, each one with its own imputed values. In
    layman’s terms, we will never be saying “the correct replacement for Bob’s missing
    age is 42” but rather “Bob might be 42, 38, 44, 42, or 38 years old.” There is
    no single best guess, but instead a distribution of possibilities. Another best-practice
    approach, maximum likelihood estimation, doesn’t even make any guess for individual
    values and only deals with higher-order coefficients such as means and covariances.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next subsection, I will give you a high-level overview of the MI approach.
    After that, we’ll dive into more detailed algorithm specifications for the model:'
  prefs: []
  type: TYPE_NORMAL
- en: First, the predictive mean matching algorithm
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, the normal algorithm
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, how to add auxiliary variables to an algorithm
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Unfortunately, there isn’t a one-to-one relationship between the type of missingness
    in Rubin’s classifications and the appropriate algorithm specification, as the
    amount of information available also matters ([Table 6-4](#optimal_mi_parameters_based_on_type_of)).
  prefs: []
  type: TYPE_NORMAL
- en: Table 6-4\. Optimal MI parameters based on type of missingness and information
    available
  prefs: []
  type: TYPE_NORMAL
- en: '| Missingness type | No info | Distribution of variable is normal | Distribution
    of missingness is deterministic |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **MCAR** | Mean matching | Normal | (not possible) |'
  prefs: []
  type: TYPE_TB
- en: '| **MAR** | Mean matching | Normal | Normal + auxiliary variables |'
  prefs: []
  type: TYPE_TB
- en: '| **MNAR** | Mean matching + auxiliary variables | Normal + auxiliary variables
    | Normal + auxiliary variables |'
  prefs: []
  type: TYPE_TB
- en: Introduction to Multiple Imputation (MI)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To understand how multiple imputation works, it helps to contrast it with the
    traditional approaches to missing data. Apart from simply dropping all rows with
    missing values, traditional approaches all rely on replacing missing values with
    a specific value. The replacement value may be the overall mean of the variable,
    or the predicted values based on the other variables available for that customer.
    Regardless of the rule used for the replacement value, these approaches are fundamentally
    flawed because they ignore the additional uncertainty introduced by the presence
    of missing data, and they may introduce bias in our analyses.
  prefs: []
  type: TYPE_NORMAL
- en: The MI solution to this problem is, as its name indicates, to build multiple
    data sets where missing values are replaced by different values, then run our
    analysis of interest with each one of them, and finally aggregate the resulting
    coefficients.
  prefs: []
  type: TYPE_NORMAL
- en: In both R and Python, this whole process is managed behind the scenes, and if
    you want to keep it simple, you can just specify the data and analyses you want
    to run.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look first at the R code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The `mice` package (Multiple Imputation by Chained Equations) has the `mice()`
    function, which generates the multiple data sets. We then apply our regression
    of interest to each one of them by using the keyword `with()`. Finally, the `pool()`
    function from `mice` aggregates the results in a format that we can read with
    the traditional `summary()` function.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Python code is almost identical, because it implements the same approach:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Here, the `mice` algorithm is imported from the `statsmodels.imputation` package.
    The `MICEData()` function generates the multiple data sets. We then indicate through
    the `MICE()` function the model formula, regression type (here, `statsmodels.OLS`),
    and data we want to use. We fit our model with the `.fit()` and `.summary()` methods
    before printing the outcome.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'One complication with the Python implementation of `mice` is that it doesn’t
    accommodate categorical variables as predictors. If you really want to use Python
    nonetheless, you’ll have to one-hot encode categorical variables first. The following
    code shows how to do it for the *Gender* variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: First, we use the `get_dummies()` function from pandas to create variables `gender_F`
    and `gender_M`. After adding these columns to our dataframe, we indicate where
    the missing values are (by default, the one-hot encoding function sets all binary
    variables to 0 when the value for the categorical variable is missing). Finally,
    we drop our original categorical variable from our data and fit our model with
    the new variables included.
  prefs: []
  type: TYPE_NORMAL
- en: However, one-hot encoding breaks some of the internal structure of the data
    by removing the logical connections between variables, so your mileage may vary
    (e.g., you can see that the coefficients for the categorical variables are different
    between R and Python because of the different structures) and I would encourage
    you to use R instead if categorical variables play an important role in your data.
  prefs: []
  type: TYPE_NORMAL
- en: Voilà! If you were to stop reading this chapter right now, you would have a
    solution to handle missing data that would be significantly better than the traditional
    approaches. However, we can do even better by taking the time to lift the hood
    and better understand the imputation algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 'Default Imputation Method: Predictive Mean Matching'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the previous subsection, we left the imputation method unspecified and relied
    on `mice`’s defaults. In Python, the only imputation method available is predictive
    mean matching, so there’s nothing to do there. Let’s check what the default imputation
    methods are in R by asking for a summary of the imputation process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'That’s a lot of information. For now, let’s look only at the `Imputation methods`
    line. Variables that don’t have missing data have an empty field `""`, which makes
    sense because they don’t get imputed. Categorical variables have the method `logreg`,
    i.e., logistic regression. Finally, numeric variables have the method `pmm`, which
    stands for predictive mean matching (PMM). The `pmm` method works by selecting
    the closest neighbors of the individual with the missing value and replacing the
    missing value with the value of one of the neighbors. Imagine, for example, a
    data set with only two variables: *Age* and *ZipCode*. If you have a customer
    from zip code 60612 with a missing age, the algorithm will pick at random the
    age of another customer in the same zip code, or as close as possible.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Because of some randomness baked in the process, each of the imputed data sets
    will end up with slightly different values, as we can visualize through the convenient
    `densityplot``()` function from the `mice` package in R:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[Figure 6-23](#distributions_of_imputed_values_for_num) shows the distributions
    of the numeric variables in the original available data (thick line) and in the
    imputed data sets (thin dashed lines). As you can see, the distributions stick
    pretty close to the original data; the exception is *BookingAmount*, which is
    overall more concentrated around the mean (i.e., “higher peaks”) in the imputed
    data set than in the original data.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Distributions of imputed values for numeric variables in our data](Images/BEDA_0623.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-23\. Distributions of imputed values for numeric variables in our data
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: PMM has some important properties that may or may not be desirable, depending
    on the context. The most important property is that it’s basically an interpolation
    method. Therefore, you can picture PMM as creating values that are between existing
    values. By doing so, it minimizes the risk of creating nonsensical situations
    such as pregnant fathers or negative amounts. This approach will also work well
    when a variable has a weird distribution, such as *Age* in our data, which has
    two peaks, because it makes no assumptions about the shape of the overall distribution;
    it just grabs a neighbor.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several downsides to PMM, however: it’s slow and doesn’t scale well
    to large data sets, because it must constantly recalculate distances between individuals.
    In addition, when you have many variables or a lot of missing values, the closest
    neighbors may be “far away,” and the quality of the imputation will deteriorate.
    This is why PMM will not be our preferred option when we have distributional information,
    as we’ll see in the next subsection.'
  prefs: []
  type: TYPE_NORMAL
- en: From PMM to Normal Imputation (R Only)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'While PMM is a decent starting point, we often have information about the distribution
    of numeric variables that we can leverage to speed up and improve our imputation
    models in R. In particular, behavioral and natural sciences often assume that
    numeric variables follow a normal distribution, because it is very common. When
    that’s the case, we can fit a normal distribution to a variable and then draw
    imputation values from that distribution instead of using PMM. This is done by
    creating a vector of imputation methods, with the value `"norm.nob"` for the variables
    for which we’ll assume normality, and then passing that vector to the `parameter`
    method of the `mice()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the syntax is very simple. The only question is to determine
    for which of the numeric variables we want to use a normal imputation. Let’s look
    at the numeric variables in our available data ([Figure 6-24](#distribution_of_numeric_variables_in_ou)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Distribution of numeric variables in our data](Images/BEDA_0624.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-24\. Distribution of numeric variables in our data
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '*Age* is obviously not normal with its two peaks, but all the other variables
    have only one peak. *Openness*, *Extraversion,* and *BookingAmount* also appear
    reasonably symmetrical (in technical terms, they’re not skewed). Statistical simulations
    show that as long as a variable is one-peaked and doesn’t have a “fat tail” in
    only one direction, assuming normality does not introduce bias. Therefore, we
    can assume normality for *Openness*, *Extraversion,* and *BookingAmount*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As we saw in the previous section, *Neuroticism* presents an unusual asymmetrical
    pattern: values are restricted to [5,10] even though the psychological scale we’re
    using goes from 0 to 10, which suggests that *Neuroticism* might be “deterministically”
    MNAR, i.e., all values of *Neuroticism* below a certain threshold are missing.
    Using PMM for imputation is problematic in such a situation: there are only a
    few or no neighbors for imputation on a significant range of values. At the extreme,
    all the missing values of X would be imputed as 5, the value of the threshold.
    This is a situation where the normal method will be better able to recover the
    true missing values. We can see that by comparing the values imputed by the two
    methods. [Figure 6-25](#pmm_imputation_left_parenthesistopright) shows the available
    values of *Neuroticism* (squares) and the values imputed with the PMM method (crosses,
    top panel) and with the normal method (crosses, bottom panel).'
  prefs: []
  type: TYPE_NORMAL
- en: '![PMM imputation (top) and normal imputation (bottom) with a deterministically
    MNAR variable](Images/BEDA_0625.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-25\. PMM imputation (top) and normal imputation (bottom) with a deterministically
    MNAR variable
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As you can see, the PMM method doesn’t impute any value for *Neuroticism* below
    5, whereas the normal method does. In addition, the PMM method imputes too many
    values close to 10, whereas the normal method more adequately captures the overall
    shape of the distribution. Still, the normal method is far from recovering the
    true distribution (which goes all the way to zero). This is a common problem for
    variables that are deterministically MAR or MNAR. We can further improve on the
    normal imputation by using auxiliary variables, as we’ll see in the next subsection.
  prefs: []
  type: TYPE_NORMAL
- en: Adding Auxiliary Variables
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Quite often, we’ll have variables that are correlated with one of our variables
    with missing data (e.g., causes or effects of that variable) but don’t belong
    in our regression model. This is a situation where the `mice` algorithm especially
    shines, because we can add these variables to our imputation model to increase
    its accuracy. They are then referred to as the “auxiliary variables” of our imputation
    model.
  prefs: []
  type: TYPE_NORMAL
- en: For our AirCnC example, the supplementary available data set contains two variables,
    *Insurance* and *Active*. The former indicates the amount of travel insurance
    bought by the customer and is strongly correlated with *Neuroticism*, while the
    latter measures the degree to which the customer has picked active vacations (e.g.,
    rock climbing) and is strongly correlated with *Extraversion*. We’ll use them
    to help impute the two personality variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'Adding auxiliary variables to the imputation model is extremely simple: we
    simply need to add them to our data set before the imputation phase:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: We can then run all our analyses as before. When adding auxiliary variables,
    it generally makes sense to use the normal method for the variables correlated
    with our auxiliary variables (here *Neuroticism* and *Extraversion*), especially
    when these variables are truncated or MNAR.
  prefs: []
  type: TYPE_NORMAL
- en: Apart from computation constraints, there are no limits to the number of auxiliary
    variables we can include. However, a potential risk is that some of our auxiliary
    variables may misleadingly appear correlated with a variable in our original data
    set just out of sheer randomness, e.g., *Insurance* appearing correlated with
    *Extraversion* even though it isn’t truly. Such a “false positive” correlation
    would then be unduly reinforced by the imputation model.
  prefs: []
  type: TYPE_NORMAL
- en: 'The solution to that potential problem is to restrict auxiliary variables to
    be used only for the imputation of certain variables. Unfortunately, this solution
    is available only in R. This is where the predictor matrix of the `mice()` function
    comes in. This matrix appears when printing the summary of the imputation phase,
    and can also be extracted directly from our `MIDS` object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'This matrix indicates which variable is used to impute which. By default, all
    variables are used to impute all variables except themselves. A “1” in the matrix
    indicates that the “column” variable is used to impute the “row” variable. We’ll
    therefore want to modify the last two columns, for *Insurance* and *Active*, so
    that they’ll be used only to impute *Neuroticism* and *Extraversion* respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: With that modification, we’ll reduce the risk of inadvertently baking in fluke
    correlations into our imputation model.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling Up the Number of Imputed Data Sets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The default number of imputed data sets created by the `mice` algorithm is 5
    in R and 10 in Python. These are fine defaults for exploratory analyses.
  prefs: []
  type: TYPE_NORMAL
- en: For your final run, you should use 20 (by passing `m=20` as a parameter setting
    to the `mice()` function) if you’re interested only in the estimated values of
    the regression coefficients. If you want more precise information such as confidence
    intervals or interactions between variables, you might want to aim for 50 to 100\.
    The main constraints then become the computer’s speed and memory—if your data
    set is 100 Mb or even 1 Gb do you have the RAM to create a hundred copies of it?—as
    well as your patience.
  prefs: []
  type: TYPE_NORMAL
- en: 'The syntax to change the number of imputed data sets is straightforward. In
    R it is passed as a parameter to the `mice()` function, while in Python it is
    passed as a parameter for the `.fit()` method of the `MICE` object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Missing data can present a real problem in behavioral data analysis, but it
    doesn’t have to. At a minimum, using the `mice` package in R or Python with its
    default parameters will outperform deleting all rows with missing values. By properly
    diagnosing missingness based on Rubin’s classification, and leveraging all available
    information, you can generally do better than that. To recap the decision rules
    in one place, [Figure 6-26](#decision_tree_to_diagnose_missing_datad) shows the
    decision tree to diagnose missing data and [Table 6-5](#optimal_imputation_method_based_on_type)
    the optimal MI parameters based on type of missingness and information available.
  prefs: []
  type: TYPE_NORMAL
- en: '![Decision tree to diagnose missing data](Images/BEDA_0626.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-26\. Decision tree to diagnose missing data
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Table 6-5\. Optimal MI parameters based on type of missingness and information
    available
  prefs: []
  type: TYPE_NORMAL
- en: '| Type of missingness | No info | Variable distribution is normal | Missingness
    distribution is deterministic |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **MCAR** | PMM | `norm.nob` |   |'
  prefs: []
  type: TYPE_TB
- en: '| **MAR** | PMM | `norm.nob` | `norm.nob` + aux. var. |'
  prefs: []
  type: TYPE_TB
- en: '| **MNAR** | PMM + aux. var. | `norm.nob` + aux. var. | `norm.nob` + aux. var.
    |'
  prefs: []
  type: TYPE_TB
