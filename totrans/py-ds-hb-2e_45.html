<html><head></head><body>
<div id="sbo-rt-content"><section data-pdf-bookmark="Chapter 40. Feature Engineering" data-type="chapter" epub:type="chapter"><div class="chapter" id="section-0504-feature-engineering">
<h1><span class="label">Chapter 40. </span>Feature Engineering</h1>
<p><a data-primary="feature engineering" data-type="indexterm" id="ix_ch40-asciidoc0"/><a data-primary="machine learning" data-secondary="feature engineering" data-type="indexterm" id="ix_ch40-asciidoc1"/>The previous chapters outlined the fundamental ideas of machine
learning, but all of the examples so far have assumed that you have numerical data
in a tidy, <code>[n_samples, n_features]</code> format. In the real world, data
rarely comes in such a form. With this in mind, one of the more
important steps in using machine learning in practice is <em>feature
engineering</em>: that is, taking whatever information you have about your
problem and turning it into numbers that you can use to build your
feature matrix.</p>
<p>In this chapter, we will cover a few common examples of feature
engineering tasks: we’ll look at features for representing
categorical data, text, and images. Additionally, we will discuss
derived features for increasing model complexity and imputation of
missing data. This process is commonly referred to as vectorization, as
it involves converting arbitrary data into well-behaved vectors.</p>
<section data-pdf-bookmark="Categorical Features" data-type="sect1"><div class="sect1" id="ch_0504-feature-engineering_categorical-features">
<h1>Categorical Features</h1>
<p><a data-primary="categorical data" data-type="indexterm" id="idm45858739807184"/><a data-primary="feature engineering" data-secondary="categorical features" data-type="indexterm" id="idm45858739806480"/>One common type of nonnumerical data is <em>categorical</em> data. For example,
imagine you are exploring some data on housing prices, and along with
numerical features like “price” and “rooms,” you also have
“neighborhood” information. For example, your data might look
something like this:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">1</code><code class="p">]:</code> <code class="n">data</code> <code class="o">=</code> <code class="p">[</code>
            <code class="p">{</code><code class="s1">'price'</code><code class="p">:</code> <code class="mi">850000</code><code class="p">,</code> <code class="s1">'rooms'</code><code class="p">:</code> <code class="mi">4</code><code class="p">,</code> <code class="s1">'neighborhood'</code><code class="p">:</code> <code class="s1">'Queen Anne'</code><code class="p">},</code>
            <code class="p">{</code><code class="s1">'price'</code><code class="p">:</code> <code class="mi">700000</code><code class="p">,</code> <code class="s1">'rooms'</code><code class="p">:</code> <code class="mi">3</code><code class="p">,</code> <code class="s1">'neighborhood'</code><code class="p">:</code> <code class="s1">'Fremont'</code><code class="p">},</code>
            <code class="p">{</code><code class="s1">'price'</code><code class="p">:</code> <code class="mi">650000</code><code class="p">,</code> <code class="s1">'rooms'</code><code class="p">:</code> <code class="mi">3</code><code class="p">,</code> <code class="s1">'neighborhood'</code><code class="p">:</code> <code class="s1">'Wallingford'</code><code class="p">},</code>
            <code class="p">{</code><code class="s1">'price'</code><code class="p">:</code> <code class="mi">600000</code><code class="p">,</code> <code class="s1">'rooms'</code><code class="p">:</code> <code class="mi">2</code><code class="p">,</code> <code class="s1">'neighborhood'</code><code class="p">:</code> <code class="s1">'Fremont'</code><code class="p">}</code>
        <code class="p">]</code></pre>
<p class="pagebreak-before less_space">You might be tempted to encode this data with a straightforward
numerical mapping:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">2</code><code class="p">]:</code> <code class="p">{</code><code class="s1">'Queen Anne'</code><code class="p">:</code> <code class="mi">1</code><code class="p">,</code> <code class="s1">'Fremont'</code><code class="p">:</code> <code class="mi">2</code><code class="p">,</code> <code class="s1">'Wallingford'</code><code class="p">:</code> <code class="mi">3</code><code class="p">};</code></pre>
<p>But it turns out that this is not generally a useful approach in
Scikit-Learn. The package’s models make the fundamental
assumption that numerical features reflect algebraic quantities, so such
a mapping would imply, for example, that <em>Queen Anne &lt; Fremont &lt;
Wallingford</em>, or even that <em>Wallingford–Queen Anne = Fremont</em>, which
(niche demographic jokes aside) does not make much sense.</p>
<p><a data-primary="one-hot encoding" data-type="indexterm" id="idm45858739683616"/>In this case, one proven technique is to use <em>one-hot encoding</em>, which
effectively creates extra columns indicating the presence or absence of
a category with a value of 1 or 0, respectively. When your data takes
the form of a list of dictionaries, Scikit-Learn’s
<code>DictVectorizer</code> will do this for you:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">3</code><code class="p">]:</code> <code class="kn">from</code> <code class="nn">sklearn.feature_extraction</code> <code class="kn">import</code> <code class="n">DictVectorizer</code>
        <code class="n">vec</code> <code class="o">=</code> <code class="n">DictVectorizer</code><code class="p">(</code><code class="n">sparse</code><code class="o">=</code><code class="kc">False</code><code class="p">,</code> <code class="n">dtype</code><code class="o">=</code><code class="nb">int</code><code class="p">)</code>
        <code class="n">vec</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">data</code><code class="p">)</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">3</code><code class="p">]:</code> <code class="n">array</code><code class="p">([[</code>     <code class="mi">0</code><code class="p">,</code>      <code class="mi">1</code><code class="p">,</code>      <code class="mi">0</code><code class="p">,</code> <code class="mi">850000</code><code class="p">,</code>      <code class="mi">4</code><code class="p">],</code>
               <code class="p">[</code>     <code class="mi">1</code><code class="p">,</code>      <code class="mi">0</code><code class="p">,</code>      <code class="mi">0</code><code class="p">,</code> <code class="mi">700000</code><code class="p">,</code>      <code class="mi">3</code><code class="p">],</code>
               <code class="p">[</code>     <code class="mi">0</code><code class="p">,</code>      <code class="mi">0</code><code class="p">,</code>      <code class="mi">1</code><code class="p">,</code> <code class="mi">650000</code><code class="p">,</code>      <code class="mi">3</code><code class="p">],</code>
               <code class="p">[</code>     <code class="mi">1</code><code class="p">,</code>      <code class="mi">0</code><code class="p">,</code>      <code class="mi">0</code><code class="p">,</code> <code class="mi">600000</code><code class="p">,</code>      <code class="mi">2</code><code class="p">]])</code></pre>
<p>Notice that the <code>neighborhood</code> column has been expanded into three
separate columns representing the three neighborhood labels, and that
each row has a 1 in the column associated with its neighborhood. With
these categorical features thus encoded, you can proceed as normal with
fitting a Scikit-Learn model.</p>
<p>To see the meaning of each column, you can inspect the feature names:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">4</code><code class="p">]:</code> <code class="n">vec</code><code class="o">.</code><code class="n">get_feature_names_out</code><code class="p">()</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">4</code><code class="p">]:</code> <code class="n">array</code><code class="p">([</code><code class="s1">'neighborhood=Fremont'</code><code class="p">,</code> <code class="s1">'neighborhood=Queen Anne'</code><code class="p">,</code>
               <code class="s1">'neighborhood=Wallingford'</code><code class="p">,</code> <code class="s1">'price'</code><code class="p">,</code> <code class="s1">'rooms'</code><code class="p">],</code> <code class="n">dtype</code><code class="o">=</code><code class="nb">object</code><code class="p">)</code></pre>
<p>There is one clear disadvantage of this approach: if your category has
many possible values, this can <em>greatly</em> increase the size of your
dataset. However, because the encoded data contains mostly zeros, a
sparse output can be a very efficient solution:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">5</code><code class="p">]:</code> <code class="n">vec</code> <code class="o">=</code> <code class="n">DictVectorizer</code><code class="p">(</code><code class="n">sparse</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code> <code class="n">dtype</code><code class="o">=</code><code class="nb">int</code><code class="p">)</code>
        <code class="n">vec</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">data</code><code class="p">)</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">5</code><code class="p">]:</code> <code class="o">&lt;</code><code class="mi">4</code><code class="n">x5</code> <code class="n">sparse</code> <code class="n">matrix</code> <code class="n">of</code> <code class="nb">type</code> <code class="s1">'&lt;class '</code><code class="n">numpy</code><code class="o">.</code><code class="n">int64</code><code class="s1">'&gt;'</code>
                <code class="k">with</code> <code class="mi">12</code> <code class="n">stored</code> <code class="n">elements</code> <code class="ow">in</code> <code class="n">Compressed</code> <code class="n">Sparse</code> <code class="n">Row</code> <code class="nb">format</code><code class="o">&gt;</code></pre>
<p>Nearly all of the Scikit-Learn estimators accept such sparse inputs when
fitting and evaluating models. Two additional tools that
Scikit-Learn includes to support this type of encoding are <code>sklearn.preprocessing.OneHotEncoder</code> and
<code>sklearn​.fea⁠ture_​extraction.FeatureHasher</code>.</p>
</div></section>
<section class="pagebreak-before less_space" data-pdf-bookmark="Text Features" data-type="sect1"><div class="sect1" id="ch_0504-feature-engineering_text-features">
<h1>Text Features</h1>
<p><a data-primary="feature engineering" data-secondary="text features" data-type="indexterm" id="ix_ch40-asciidoc2"/><a data-primary="text" data-secondary="feature engineering" data-type="indexterm" id="ix_ch40-asciidoc3"/>Another common need in feature engineering is to convert text to a set
of representative numerical values. For example, most automatic mining
of social media data relies on some form of encoding the text as
numbers. <a data-primary="word counts" data-type="indexterm" id="idm45858739433968"/>One of the simplest methods of encoding this type of data is by
<em>word counts</em>: you take each snippet of text, count the occurrences of
each word within it, and put the results in a table.</p>
<p>For example, consider the following set of three phrases:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">6</code><code class="p">]:</code> <code class="n">sample</code> <code class="o">=</code> <code class="p">[</code><code class="s1">'problem of evil'</code><code class="p">,</code>
                  <code class="s1">'evil queen'</code><code class="p">,</code>
                  <code class="s1">'horizon problem'</code><code class="p">]</code></pre>
<p>For a vectorization of this data based on word count, we could construct
individual columns representing the words “problem,” “of,” “evil,”
and so on. While doing this by hand would be possible for this simple
example, the tedium can be avoided by using Scikit-Learn’s
<code>CountVectorizer</code>:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">7</code><code class="p">]:</code> <code class="kn">from</code> <code class="nn">sklearn.feature_extraction.text</code> <code class="kn">import</code> <code class="n">CountVectorizer</code>

        <code class="n">vec</code> <code class="o">=</code> <code class="n">CountVectorizer</code><code class="p">()</code>
        <code class="n">X</code> <code class="o">=</code> <code class="n">vec</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">sample</code><code class="p">)</code>
        <code class="n">X</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">7</code><code class="p">]:</code> <code class="o">&lt;</code><code class="mi">3</code><code class="n">x5</code> <code class="n">sparse</code> <code class="n">matrix</code> <code class="n">of</code> <code class="nb">type</code> <code class="s1">'&lt;class '</code><code class="n">numpy</code><code class="o">.</code><code class="n">int64</code><code class="s1">'&gt;'</code>
                <code class="k">with</code> <code class="mi">7</code> <code class="n">stored</code> <code class="n">elements</code> <code class="ow">in</code> <code class="n">Compressed</code> <code class="n">Sparse</code> <code class="n">Row</code> <code class="nb">format</code><code class="o">&gt;</code></pre>
<p>The result is a sparse matrix recording the number of times each word
appears; it is easier to inspect if we convert this to a <code>DataFrame</code>
with labeled columns:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">8</code><code class="p">]:</code> <code class="kn">import</code> <code class="nn">pandas</code> <code class="k">as</code> <code class="nn">pd</code>
        <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">X</code><code class="o">.</code><code class="n">toarray</code><code class="p">(),</code> <code class="n">columns</code><code class="o">=</code><code class="n">vec</code><code class="o">.</code><code class="n">get_feature_names_out</code><code class="p">())</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">8</code><code class="p">]:</code>    <code class="n">evil</code>  <code class="n">horizon</code>  <code class="n">of</code>  <code class="n">problem</code>  <code class="n">queen</code>
        <code class="mi">0</code>     <code class="mi">1</code>        <code class="mi">0</code>   <code class="mi">1</code>        <code class="mi">1</code>      <code class="mi">0</code>
        <code class="mi">1</code>     <code class="mi">1</code>        <code class="mi">0</code>   <code class="mi">0</code>        <code class="mi">0</code>      <code class="mi">1</code>
        <code class="mi">2</code>     <code class="mi">0</code>        <code class="mi">1</code>   <code class="mi">0</code>        <code class="mi">1</code>      <code class="mi">0</code></pre>
<p>There are some issues with using a simple raw word count, however: it
can lead to features that put too much weight on words that appear very
frequently, and this can be suboptimal in some classification
algorithms. <a data-primary="term frequency-inverse document frequency (TF-IDF)" data-type="indexterm" id="idm45858739254544"/>One approach to fix this is known as <em>term frequency–inverse
document frequency</em> (<em>TF–IDF</em>), which weights the word counts by a
measure of how often they appear in the documents. The syntax for
computing these features is similar to the previous example:</p>
<p>The solid lines show the new results, while the fainter dashed lines
show the results on the previous smaller dataset. It is clear from the
validation curve that the larger dataset can support a much more
complicated model: the peak here is probably around a degree of 6, but
even a degree-20 model isn’t seriously overfitting the data—the
validation and training scores remain very close.</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">9</code><code class="p">]:</code> <code class="kn">from</code> <code class="nn">sklearn.feature_extraction.text</code> <code class="kn">import</code> <code class="n">TfidfVectorizer</code>
        <code class="n">vec</code> <code class="o">=</code> <code class="n">TfidfVectorizer</code><code class="p">()</code>
        <code class="n">X</code> <code class="o">=</code> <code class="n">vec</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">sample</code><code class="p">)</code>
        <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">X</code><code class="o">.</code><code class="n">toarray</code><code class="p">(),</code> <code class="n">columns</code><code class="o">=</code><code class="n">vec</code><code class="o">.</code><code class="n">get_feature_names_out</code><code class="p">())</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">9</code><code class="p">]:</code>        <code class="n">evil</code>   <code class="n">horizon</code>        <code class="n">of</code>   <code class="n">problem</code>     <code class="n">queen</code>
        <code class="mi">0</code>  <code class="mf">0.517856</code>  <code class="mf">0.000000</code>  <code class="mf">0.680919</code>  <code class="mf">0.517856</code>  <code class="mf">0.000000</code>
        <code class="mi">1</code>  <code class="mf">0.605349</code>  <code class="mf">0.000000</code>  <code class="mf">0.000000</code>  <code class="mf">0.000000</code>  <code class="mf">0.795961</code>
        <code class="mi">2</code>  <code class="mf">0.000000</code>  <code class="mf">0.795961</code>  <code class="mf">0.000000</code>  <code class="mf">0.605349</code>  <code class="mf">0.000000</code></pre>
<p>For an example of using TF-IDF in a classification problem, see
<a data-type="xref" href="ch41.xhtml#section-0505-naive-bayes">Chapter 41</a>.<a data-startref="ix_ch40-asciidoc3" data-type="indexterm" id="idm45858739172576"/><a data-startref="ix_ch40-asciidoc2" data-type="indexterm" id="idm45858739115664"/></p>
</div></section>
<section data-pdf-bookmark="Image Features" data-type="sect1"><div class="sect1" id="ch_0504-feature-engineering_image-features">
<h1>Image Features</h1>
<p><a data-primary="feature engineering" data-secondary="image features" data-type="indexterm" id="idm45858739113584"/><a data-primary="images, encoding for machine learning analysis" data-type="indexterm" id="idm45858739112608"/>Another common need is to suitably encode images for machine learning
analysis. The simplest approach is what we used for the digits data in
<a data-type="xref" href="ch38.xhtml#section-0502-introducing-scikit-learn">Chapter 38</a>:
simply using the pixel values themselves. But depending on the
application, such an approach may not be optimal.</p>
<p>A comprehensive summary of feature extraction techniques for images is
well beyond the scope of this chapter, but you can find excellent
implementations of many of the standard approaches in the
<a href="http://scikit-image.org">Scikit-Image project</a>. For one example of using
Scikit-Learn and Scikit-Image together, see
<a data-type="xref" href="ch50.xhtml#section-0514-image-features">Chapter 50</a>.</p>
</div></section>
<section data-pdf-bookmark="Derived Features" data-type="sect1"><div class="sect1" id="ch_0504-feature-engineering_derived-features">
<h1>Derived Features</h1>
<p><a data-primary="feature engineering" data-secondary="derived features" data-type="indexterm" id="ix_ch40-asciidoc4"/>Another useful type of feature is one that is mathematically derived
from some input features. We saw an example of this in
<a data-type="xref" href="ch39.xhtml#section-0503-hyperparameters-and-model-validation">Chapter 39</a> when we constructed <em>polynomial features</em> from our
input data. We saw that we could convert a linear regression into a
polynomial regression not by changing the model, but by transforming the
input!</p>
<p>For example, this data clearly cannot be well described by a straight
line (see <a data-type="xref" href="#fig_0504-feature-engineering_files_in_output_24_0">Figure 40-1</a>):</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">10</code><code class="p">]:</code> <code class="o">%</code><code class="k">matplotlib</code> inline
         <code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>
         <code class="kn">import</code> <code class="nn">matplotlib.pyplot</code> <code class="k">as</code> <code class="nn">plt</code>

         <code class="n">x</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">([</code><code class="mi">1</code><code class="p">,</code> <code class="mi">2</code><code class="p">,</code> <code class="mi">3</code><code class="p">,</code> <code class="mi">4</code><code class="p">,</code> <code class="mi">5</code><code class="p">])</code>
         <code class="n">y</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">([</code><code class="mi">4</code><code class="p">,</code> <code class="mi">2</code><code class="p">,</code> <code class="mi">1</code><code class="p">,</code> <code class="mi">3</code><code class="p">,</code> <code class="mi">7</code><code class="p">])</code>
         <code class="n">plt</code><code class="o">.</code><code class="n">scatter</code><code class="p">(</code><code class="n">x</code><code class="p">,</code> <code class="n">y</code><code class="p">);</code></pre>
<figure class="width-75"><div class="figure" id="fig_0504-feature-engineering_files_in_output_24_0">
<img alt="output 24 0" height="383" src="assets/output_24_0.png" width="600"/>
<h6><span class="label">Figure 40-1. </span>Data that is not well described by a straight line</h6>
</div></figure>
<p>We can still fit a line to the data using <code>LinearRegression</code> and get the
optimal result, as shown in <a data-type="xref" href="#fig_0504-feature-engineering_files_in_output_26_0">Figure 40-2</a>:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">11</code><code class="p">]:</code> <code class="kn">from</code> <code class="nn">sklearn.linear_model</code> <code class="kn">import</code> <code class="n">LinearRegression</code>
         <code class="n">X</code> <code class="o">=</code> <code class="n">x</code><code class="p">[:,</code> <code class="n">np</code><code class="o">.</code><code class="n">newaxis</code><code class="p">]</code>
         <code class="n">model</code> <code class="o">=</code> <code class="n">LinearRegression</code><code class="p">()</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code>
         <code class="n">yfit</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X</code><code class="p">)</code>
         <code class="n">plt</code><code class="o">.</code><code class="n">scatter</code><code class="p">(</code><code class="n">x</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code>
         <code class="n">plt</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">x</code><code class="p">,</code> <code class="n">yfit</code><code class="p">);</code></pre>
<figure class="width-75"><div class="figure" id="fig_0504-feature-engineering_files_in_output_26_0">
<img alt="output 26 0" height="377" src="assets/output_26_0.png" width="600"/>
<h6><span class="label">Figure 40-2. </span>A poor straight-line fit</h6>
</div></figure>
<p>But it’s clear that we need a more sophisticated model to
describe the relationship between <math alttext="x">
<mi>x</mi>
</math> and <math alttext="y">
<mi>y</mi>
</math>.</p>
<p>One approach to this is to transform the data, adding extra columns of
features to drive more flexibility in the model. For example, we can add
polynomial features to the data this way:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">12</code><code class="p">]:</code> <code class="kn">from</code> <code class="nn">sklearn.preprocessing</code> <code class="kn">import</code> <code class="n">PolynomialFeatures</code>
         <code class="n">poly</code> <code class="o">=</code> <code class="n">PolynomialFeatures</code><code class="p">(</code><code class="n">degree</code><code class="o">=</code><code class="mi">3</code><code class="p">,</code> <code class="n">include_bias</code><code class="o">=</code><code class="kc">False</code><code class="p">)</code>
         <code class="n">X2</code> <code class="o">=</code> <code class="n">poly</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">X</code><code class="p">)</code>
         <code class="nb">print</code><code class="p">(</code><code class="n">X2</code><code class="p">)</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">12</code><code class="p">]:</code> <code class="p">[[</code>  <code class="mf">1.</code>   <code class="mf">1.</code>   <code class="mf">1.</code><code class="p">]</code>
          <code class="p">[</code>  <code class="mf">2.</code>   <code class="mf">4.</code>   <code class="mf">8.</code><code class="p">]</code>
          <code class="p">[</code>  <code class="mf">3.</code>   <code class="mf">9.</code>  <code class="mf">27.</code><code class="p">]</code>
          <code class="p">[</code>  <code class="mf">4.</code>  <code class="mf">16.</code>  <code class="mf">64.</code><code class="p">]</code>
          <code class="p">[</code>  <code class="mf">5.</code>  <code class="mf">25.</code> <code class="mf">125.</code><code class="p">]]</code></pre>
<p>The derived feature matrix has one column representing <math alttext="x">
<mi>x</mi>
</math>,
a second column representing <math alttext="x squared">
<msup><mi>x</mi> <mn>2</mn> </msup>
</math>, and a third column
representing <math alttext="x cubed">
<msup><mi>x</mi> <mn>3</mn> </msup>
</math>. Computing a linear regression on this
expanded input gives a much closer fit to our data, as you can see in
<a data-type="xref" href="#fig_0504-feature-engineering_files_in_output_30_0">Figure 40-3</a>:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">13</code><code class="p">]:</code> <code class="n">model</code> <code class="o">=</code> <code class="n">LinearRegression</code><code class="p">()</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X2</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code>
         <code class="n">yfit</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X2</code><code class="p">)</code>
         <code class="n">plt</code><code class="o">.</code><code class="n">scatter</code><code class="p">(</code><code class="n">x</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code>
         <code class="n">plt</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">x</code><code class="p">,</code> <code class="n">yfit</code><code class="p">);</code></pre>
<figure class="width-75"><div class="figure" id="fig_0504-feature-engineering_files_in_output_30_0">
<img alt="output 30 0" height="377" src="assets/output_30_0.png" width="600"/>
<h6><span class="label">Figure 40-3. </span>A linear fit to polynomial features derived from the data</h6>
</div></figure>
<p>This idea of improving a model not by changing the model, but by
transforming the inputs, is fundamental to many of the more powerful
machine learning methods. <a data-primary="basis function regression" data-type="indexterm" id="idm45858738719824"/>We’ll explore this idea further in
<a data-type="xref" href="ch42.xhtml#section-0506-linear-regression">Chapter 42</a> in the
context of <em>basis function regression</em>. More generally, this is one
motivational path to the powerful set of techniques known as <em>kernel
methods</em>, which we will explore in
<a data-type="xref" href="ch43.xhtml#section-0507-support-vector-machines">Chapter 43</a>.<a data-startref="ix_ch40-asciidoc4" data-type="indexterm" id="idm45858738716544"/></p>
</div></section>
<section data-pdf-bookmark="Imputation of Missing Data" data-type="sect1"><div class="sect1" id="ch_0504-feature-engineering_imputation-of-missing-data">
<h1>Imputation of Missing Data</h1>
<p><a data-primary="feature engineering" data-secondary="imputation of missing data" data-type="indexterm" id="idm45858738709536"/>Another common need in feature engineering is handling of missing data.
We discussed the handling of missing data in <code>DataFrame</code> objects in
<a data-type="xref" href="ch16.xhtml#section-0304-missing-values">Chapter 16</a>, and saw that
<code>NaN</code> is often is used to mark missing values. For example, we might
have a dataset that looks like this:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">14</code><code class="p">]:</code> <code class="kn">from</code> <code class="nn">numpy</code> <code class="kn">import</code> <code class="n">nan</code>
         <code class="n">X</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">([[</code> <code class="n">nan</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code>   <code class="mi">3</code>  <code class="p">],</code>
                       <code class="p">[</code> <code class="mi">3</code><code class="p">,</code>   <code class="mi">7</code><code class="p">,</code>   <code class="mi">9</code>  <code class="p">],</code>
                       <code class="p">[</code> <code class="mi">3</code><code class="p">,</code>   <code class="mi">5</code><code class="p">,</code>   <code class="mi">2</code>  <code class="p">],</code>
                       <code class="p">[</code> <code class="mi">4</code><code class="p">,</code>   <code class="n">nan</code><code class="p">,</code> <code class="mi">6</code>  <code class="p">],</code>
                       <code class="p">[</code> <code class="mi">8</code><code class="p">,</code>   <code class="mi">8</code><code class="p">,</code>   <code class="mi">1</code>  <code class="p">]])</code>
         <code class="n">y</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">([</code><code class="mi">14</code><code class="p">,</code> <code class="mi">16</code><code class="p">,</code> <code class="o">-</code><code class="mi">1</code><code class="p">,</code>  <code class="mi">8</code><code class="p">,</code> <code class="o">-</code><code class="mi">5</code><code class="p">])</code></pre>
<p>When applying a typical machine learning model to such data, we will
need to first replace the missing values with some appropriate fill
value. This is known as <em>imputation</em> of missing values, and strategies
range from simple (e.g., replacing missing values with the mean of the
column) to sophisticated (e.g., using matrix completion or a robust
model to handle such data).</p>
<p>The sophisticated approaches tend to be very application-specific, and
we won’t dive into them here. For a baseline imputation
approach using the mean, median, or most frequent value, Scikit-Learn
provides the <code>SimpleImputer</code> class:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">15</code><code class="p">]:</code> <code class="kn">from</code> <code class="nn">sklearn.impute</code> <code class="kn">import</code> <code class="n">SimpleImputer</code>
         <code class="n">imp</code> <code class="o">=</code> <code class="n">SimpleImputer</code><code class="p">(</code><code class="n">strategy</code><code class="o">=</code><code class="s1">'mean'</code><code class="p">)</code>
         <code class="n">X2</code> <code class="o">=</code> <code class="n">imp</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">X</code><code class="p">)</code>
         <code class="n">X2</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">15</code><code class="p">]:</code> <code class="n">array</code><code class="p">([[</code><code class="mf">4.5</code><code class="p">,</code> <code class="mf">0.</code> <code class="p">,</code> <code class="mf">3.</code> <code class="p">],</code>
                <code class="p">[</code><code class="mf">3.</code> <code class="p">,</code> <code class="mf">7.</code> <code class="p">,</code> <code class="mf">9.</code> <code class="p">],</code>
                <code class="p">[</code><code class="mf">3.</code> <code class="p">,</code> <code class="mf">5.</code> <code class="p">,</code> <code class="mf">2.</code> <code class="p">],</code>
                <code class="p">[</code><code class="mf">4.</code> <code class="p">,</code> <code class="mf">5.</code> <code class="p">,</code> <code class="mf">6.</code> <code class="p">],</code>
                <code class="p">[</code><code class="mf">8.</code> <code class="p">,</code> <code class="mf">8.</code> <code class="p">,</code> <code class="mf">1.</code> <code class="p">]])</code></pre>
<p>We see that in the resulting data, the two missing values have been
replaced with the mean of the remaining values in the column. This
imputed data can then be fed directly into, for example, a
<code>LinearRegression</code> estimator:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">16</code><code class="p">]:</code> <code class="n">model</code> <code class="o">=</code> <code class="n">LinearRegression</code><code class="p">()</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X2</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code>
         <code class="n">model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X2</code><code class="p">)</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">16</code><code class="p">]:</code> <code class="n">array</code><code class="p">([</code><code class="mf">13.14869292</code><code class="p">,</code> <code class="mf">14.3784627</code> <code class="p">,</code> <code class="o">-</code><code class="mf">1.15539732</code><code class="p">,</code> <code class="mf">10.96606197</code><code class="p">,</code> <code class="o">-</code><code class="mf">5.33782027</code><code class="p">])</code></pre>
</div></section>
<section data-pdf-bookmark="Feature Pipelines" data-type="sect1"><div class="sect1" id="ch_0504-feature-engineering_feature-pipelines">
<h1>Feature Pipelines</h1>
<p><a data-primary="feature engineering" data-secondary="processing pipeline" data-type="indexterm" id="idm45858738462128"/><a data-primary="missing data" data-secondary="feature engineering and" data-type="indexterm" id="idm45858738461152"/><a data-primary="pipelines" data-type="indexterm" id="idm45858738460208"/>With any of the preceding examples, it can quickly become tedious to do
the transformations by hand, especially if you wish to string together
multiple steps. For example, we might want a processing pipeline that
looks something like this:</p>
<ol>
<li>
<p>Impute missing values using the mean.</p>
</li>
<li>
<p>Transform features to
quadratic.</p>
</li>
<li>
<p>Fit a linear regression model.</p>
</li>
</ol>
<p>To streamline this type of processing pipeline, Scikit-Learn provides a
<code>Pipeline</code> object, which can be used as follows:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">17</code><code class="p">]:</code> <code class="kn">from</code> <code class="nn">sklearn.pipeline</code> <code class="kn">import</code> <code class="n">make_pipeline</code>

         <code class="n">model</code> <code class="o">=</code> <code class="n">make_pipeline</code><code class="p">(</code><code class="n">SimpleImputer</code><code class="p">(</code><code class="n">strategy</code><code class="o">=</code><code class="s1">'mean'</code><code class="p">),</code>
                               <code class="n">PolynomialFeatures</code><code class="p">(</code><code class="n">degree</code><code class="o">=</code><code class="mi">2</code><code class="p">),</code>
                               <code class="n">LinearRegression</code><code class="p">())</code></pre>
<p>This pipeline looks and acts like a standard Scikit-Learn object, and
will apply all the specified steps to any input data:</p>
<pre data-code-language="ipython" data-type="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">18</code><code class="p">]:</code> <code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code>  <code class="c1"># X with missing values, from above</code>
         <code class="nb">print</code><code class="p">(</code><code class="n">y</code><code class="p">)</code>
         <code class="nb">print</code><code class="p">(</code><code class="n">model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X</code><code class="p">))</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">18</code><code class="p">]:</code> <code class="p">[</code><code class="mi">14</code> <code class="mi">16</code> <code class="o">-</code><code class="mi">1</code>  <code class="mi">8</code> <code class="o">-</code><code class="mi">5</code><code class="p">]</code>
         <code class="p">[</code><code class="mf">14.</code> <code class="mf">16.</code> <code class="o">-</code><code class="mf">1.</code>  <code class="mf">8.</code> <code class="o">-</code><code class="mf">5.</code><code class="p">]</code></pre>
<p>All the steps of the model are applied automatically. Notice that for
simplicity, in this demonstration we’ve applied the model to
the data it was trained on; this is why it was able to perfectly predict
the result (refer back to
<a data-type="xref" href="ch39.xhtml#section-0503-hyperparameters-and-model-validation">Chapter 39</a> for further discussion).</p>
<p>For some examples of Scikit-Learn pipelines in action, see the following
chapter on naive Bayes classification, as well as Chapters
<a href="ch42.xhtml#section-0506-linear-regression">42</a> and
<a href="ch43.xhtml#section-0507-support-vector-machines">43</a>.<a data-startref="ix_ch40-asciidoc1" data-type="indexterm" id="idm45858738311920"/><a data-startref="ix_ch40-asciidoc0" data-type="indexterm" id="idm45858738311216"/></p>
</div></section>
</div></section></div></body></html>