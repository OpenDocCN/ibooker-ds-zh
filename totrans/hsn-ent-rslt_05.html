<html><head></head><body><section data-pdf-bookmark="Chapter 5. Record Blocking" data-type="chapter" epub:type="chapter"><div class="chapter" id="chapter_5">&#13;
<h1><span class="label">Chapter 5. </span>Record Blocking</h1>&#13;
&#13;
<p>In <a data-type="xref" href="ch04.html#chapter_4">Chapter 4</a>, we introduced probabilistic matching techniques to allow us to combine exact equivalence on individual attributes into a weighted composite score. That score allowed us to calculate the overall probability that two records refer to the same entity.</p>&#13;
&#13;
<p>So far we have sought to resolve only small-scale datasets where we could exhaustively compare every record with every other to find all possible matches. However, in most entity resolution scenarios, we will be dealing with larger datasets where this approach isn’t practical or affordable.</p>&#13;
&#13;
<p>In<a contenteditable="false" data-primary="record blocking" data-secondary="purpose of" data-type="indexterm" id="id443"/> this chapter we will introduce record blocking to reduce the number of permutations we need to consider while minimizing the likelihood of missing a true positive match. We will leverage the Splink framework, introduced in the last chapter, to apply the Fellegi-Sunter model and use the expectation-maximization algorithm to estimate the model parameters.</p>&#13;
&#13;
<p>Lastly, we will consider how to measure our matching performance over this larger dataset.</p>&#13;
&#13;
<section data-pdf-bookmark="Sample Problem" data-type="sect1"><div class="sect1" id="id250">&#13;
<h1>Sample Problem</h1>&#13;
&#13;
<p>In previous chapters, we considered the challenge of resolving entities across two datasets containing information about members of the UK House of Commons. In this chapter, we extend this resolution challenge to a much larger dataset containing a list of the persons with significant control of registered UK companies.</p>&#13;
&#13;
<p>In the UK, Companies House is an executive agency sponsored by the Department for Business and Trade. It incorporates and dissolves limited companies, registering company information and making it available to the public.</p>&#13;
&#13;
<p>When registering a UK limited company, there is an obligation to declare who owns or controls a company. These entities are known as persons with significant control (PSCs); they’re sometimes called “beneficial owners.” Companies House provides a downloadable data snapshot containing the full list of PSCs.</p>&#13;
&#13;
<p>For this exercise, we will attempt to resolve the entities listed in this dataset with the list of members of Parliament we acquired from Wikipedia. This will show us which MPs may be PSC of UK companies.</p>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Data Acquisition" data-type="sect1"><div class="sect1" id="id120">&#13;
<h1>Data Acquisition</h1>&#13;
&#13;
<p>In<a contenteditable="false" data-primary="record blocking" data-secondary="data acquisition" data-type="indexterm" id="RBdata05"/> this example, we will reuse the same Wikipedia source data on MPs returned at the 2019 UK general election that we examined in previous chapters. However, to allow us to match against a much larger dataset,  without generating an unmanageable number of false positives, we need to enrich our initial data with additional attributes. Specifically, we will seek to augment our dataset with date of birth information, extracted from the individual wiki page associated with each of the MPs, to help strengthen the quality of our matches.</p>&#13;
&#13;
<p>We will also download the most recent snapshot of the PSC data published by Companies House and then normalize and filter that dataset down to the attributes we need for matching.</p>&#13;
&#13;
<section data-pdf-bookmark="Wikipedia Data" data-type="sect2"><div class="sect2" id="id45">&#13;
<h2>Wikipedia Data</h2>&#13;
&#13;
<p>To<a contenteditable="false" data-primary="data, acquiring" data-secondary="Wikipedia data" data-type="indexterm" id="id444"/><a contenteditable="false" data-primary="Wikipedia data" data-secondary="acquiring" data-type="indexterm" id="id445"/> create our enriched Wikipedia dataset, we select the MPs from the wiki page as we did in <a data-type="xref" href="ch02.html#chapter_2">Chapter 2</a>; however, this time we also extract the Wikipedia link to each individual MP and append this as an<a contenteditable="false" data-primary="pandas DataFrames" data-secondary="appending additional columns to" data-type="indexterm" id="id446"/> additional column in our DataFrame.</p>&#13;
&#13;
<pre data-type="programlisting">&#13;
url = "https://en.wikipedia.org/wiki/&#13;
       List_of_MPs_elected_in_the_2019_United_Kingdom_general_election"&#13;
&#13;
website_page = requests.get(url).textsoup = &#13;
    BeautifulSoup(website_page,'html.parser')&#13;
tables = soup.find_all('table')&#13;
&#13;
for table in tables:&#13;
   if 'Member returned' in table.text:&#13;
      headers = [header.text.strip() for header in table.find_all('th')]&#13;
      headers = headers[:5]&#13;
      dfrows = []&#13;
      table_rows = table.find_all('tr')&#13;
      for row in table_rows:&#13;
         td = row.find_all('td')&#13;
         dfrow = [row.text for row in td if row.text!='\n']&#13;
         tdlink = row.find_all("td", {"data-sort-value" : True})&#13;
         for element in tdlink:&#13;
             for link in element.select("a[title]"):&#13;
                 urltail = link['href']&#13;
                 url = f'https://en.wikipedia.org{urltail}' &#13;
      dfrow.append(url)&#13;
      dfrows.append(dfrow)&#13;
   headers.append('Wikilink')&#13;
df_w = pd.DataFrame()</pre>&#13;
&#13;
<p>We can now follow these links and extract the date of birth information, if present, from the web page Infobox. As before, we can use the<a contenteditable="false" data-primary="Beautiful Soup" data-type="indexterm" id="id447"/><a contenteditable="false" data-primary="Python" data-secondary="Beautiful Soup package" data-type="indexterm" id="id448"/> Beautiful Soup <code>html parser</code> to find and extract the attribute we need or return a default null value. The <code>apply</code> method allows us to apply this function to each row in the Wikipedia dataset, creating a new column entitled <code>Birthday</code>:</p>&#13;
&#13;
<pre data-type="programlisting">&#13;
def get_bday(url):&#13;
   wiki_page = requests.get(url).text&#13;
   soup = BeautifulSoup(wiki_page,'html.parser')&#13;
   bday = ''&#13;
   bdayelement = soup.select_one("span[class='bday']")&#13;
   if bdayelement is not None:&#13;
      bday = bdayelement.text&#13;
      return(bday)&#13;
&#13;
df_w['Birthday'] = df_w.apply(lambda x: get_bday(x.Wikilink), axis=1)</pre>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="UK Companies House Data" data-type="sect2"><div class="sect2" id="id121">&#13;
<h2>UK Companies House Data</h2>&#13;
&#13;
<p>Companies House<a contenteditable="false" data-primary="data, acquiring" data-secondary="UK Companies House data" data-type="indexterm" id="id449"/><a contenteditable="false" data-primary="UK Companies House data" data-secondary="acquiring" data-type="indexterm" id="id450"/> publishes a snapshot of PSC data in JSON format. It is made available both as a single ZIP file and as multiple ZIP files for ease of downloading. Extracting<a contenteditable="false" data-primary="pandas DataFrames" data-secondary="normalizing JSON prior to concatenation" data-type="indexterm" id="id451"/> each partial ZIP file in turn allows us to normalize the JSON structure that we concatenate into a composite DataFrame of the attributes we need for matching plus the associated unique company number:</p>&#13;
&#13;
<pre data-type="programlisting">&#13;
url = "http://download.companieshouse.gov.uk/en_pscdata.html"&#13;
&#13;
&gt;df_psctotal = pd.DataFrame()&#13;
with requests.Session() as req:&#13;
   r = req.get(url)&#13;
   soup = BeautifulSoup(r.content, 'html.parser')&#13;
   snapshots = [f"{url[:38]}{item['href']}" for item in soup.select(&#13;
      "a[href*='psc-snapshot']")]&#13;
   for snapshot in snapshots:&#13;
      print(snapshot)&#13;
      response = requests.get(snapshot).content zipsnapshot =&#13;
         zipfile.ZipFile(io.BytesIO(response))&#13;
      tempfile = zipsnapshot.extract(zipsnapshot.namelist()[0])&#13;
      df_psc = pd.json_normalize(pd.Series(open(tempfile,&#13;
         encoding="utf8").readlines()).apply(json.loads))&#13;
&#13;
      must_cols =  ['company_number',&#13;
                    'data.name_elements.surname',&#13;
                    'data.name_elements.middle_name',&#13;
                    'data.name_elements.forename',&#13;
                    'data.date_of_birth.month',&#13;
                    'data.date_of_birth.year',&#13;
                    'data.name_elements.title',&#13;
                    'data.nationality']  &#13;
       all_cols =list(set(df_psc.columns).union(must_cols))&#13;
&#13;
&gt;      df_psc=df_psc.reindex(columns=sorted(all_cols))&#13;
      df_psc = df_psc.dropna(subset=['company_number',&#13;
                    'data.name_elements.surname',&#13;
                    'data.name_elements.forename',&#13;
                    'data.date_of_birth.month',&#13;
                    'data.date_of_birth.year'])&#13;
      df_psc = df_psc[must_cols]&#13;
      df_psctotal = pd.concat([df_psctotal, df_psc],&#13;
         ignore_index=True)</pre>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Data Standardization" data-type="sect1"><div class="sect1" id="id46">&#13;
<h1>Data Standardization</h1>&#13;
&#13;
<p>Now<a contenteditable="false" data-primary="" data-startref="RBdata05" data-type="indexterm" id="id452"/><a contenteditable="false" data-primary="record blocking" data-secondary="data standardization" data-type="indexterm" id="id453"/><a contenteditable="false" data-primary="data standardization" data-secondary="attributes and column names" data-secondary-sortas="record blocking" data-type="indexterm" id="id454"/><a contenteditable="false" data-primary="attributes" data-secondary="standardizing for record blocking" data-type="indexterm" id="id455"/> that we have the raw data we need, we standardize the attributes and column names across the two datasets. As we will be using the Splink framework, we also add a unique ID column.</p>&#13;
&#13;
<section data-pdf-bookmark="Wikipedia Data" data-type="sect2"><div class="sect2" id="id122">&#13;
<h2>Wikipedia Data</h2>&#13;
&#13;
<p>To<a contenteditable="false" data-primary="Wikipedia data" data-secondary="standardizing for record blocking" data-type="indexterm" id="id456"/> standardize the date-enriched Wikipedia data, we convert the date column into month and year integers. As in <a data-type="xref" href="ch02.html#chapter_2">Chapter 2</a>, we extract <code>Firstname</code> and <code>Lastname</code> attributes. We also add a unique ID column and a blank company number column to match the equivalent field in the Companies House data. Finally, we retain only the columns we need:</p>&#13;
&#13;
<pre data-type="programlisting">&#13;
df_w = df_w.dropna()&#13;
df_w['Year'] =&#13;
   pd.to_datetime(df_w['Birthday']).dt.year.astype('int64')&#13;
df_w['Month'] =&#13;
   pd.to_datetime(df_w['Birthday']).dt.month.astype('int64')&#13;
&#13;
df_w = df_w.rename(columns={ 'Member returned' : 'Fullname'})&#13;
df_w['Fullname'] = df_w['Fullname'].str.rstrip("\n")&#13;
df_w['Fullname'] = df_w['Fullname'].str.lstrip("\n")&#13;
df_w['Firstname'] = df_w['Fullname'].str.split().str[0]&#13;
df_w['Lastname'] = df_w['Fullname'].astype(str).apply(lambda x:&#13;
   ' '.join(x.split()[1:]))&#13;
&#13;
df_w['unique_id'] = df_w.index&#13;
df_w["company_number"] = np.nan&#13;
&#13;
df_w=df_w[['Firstname','Lastname','Month','Year','unique_id',&#13;
   'company_number']]</pre>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="UK Companies House Data" data-type="sect2"><div class="sect2" id="id123">&#13;
<h2>UK Companies House Data</h2>&#13;
&#13;
<p>To<a contenteditable="false" data-primary="UK Companies House data" data-secondary="standardizing for record blocking" data-type="indexterm" id="id457"/> standardize the UK Companies House data, we first drop any rows with missing year or month date of birth columns as we won’t be able to match these records. As with the Wikipedia data, we standardize the column names, generate the unique ID, and retain the matching subset:</p>&#13;
&#13;
<pre data-type="programlisting">&#13;
df_psc = df_psc.dropna(subset=['data.date_of_birth.year',&#13;
                                'data.date_of_birth.month'])&#13;
&#13;
df_psc['Year'] = df_psc['data.date_of_birth.year'].astype('int64')&#13;
df_psc['Month'] = df_psc['data.date_of_birth.month'].astype('int64')&#13;
df_psc['Firstname']=df_psc['data.name_elements.forename']&#13;
df_psc['Lastname']=df_psc['data.name_elements.surname']&#13;
df_psc['unique_id'] = df_psc.index&#13;
&#13;
df_psc = df_psc[['Lastname','Firstname','company_number',&#13;
   'Year','Month','unique_id']]</pre>&#13;
&#13;
<p>Let’s look at a few rows (with <code>Firstname</code>s and <code>Lastname</code>s sanitized), as shown in <a data-type="xref" href="#fig-5-1">Figure 5-1</a>.</p>&#13;
&#13;
<figure><div class="figure" id="fig-5-1"><img alt="" class="iimagesch05ch05pscheadpng" src="assets/hoer_0501.png"/>&#13;
<h6><span class="label">Figure 5-1. </span>Example rows of UK Companies House persons with significant control data</h6>&#13;
</div></figure>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Record Blocking and Attribute Comparison" data-type="sect1"><div class="sect1" id="id124">&#13;
<h1>Record Blocking and Attribute Comparison</h1>&#13;
&#13;
<p>Now<a contenteditable="false" data-primary="record blocking" data-secondary="attribute comparison" data-type="indexterm" id="RBatcomp05"/> that we have consistent data, we can configure our matching process. Before we do, it’s worth taking a look at the size of the challenge. We have 650 MP records and our standardized PSC data has more than 10 million records. If we were to consider all permutations, we would have approximately 6 billion comparisons to make.</p>&#13;
&#13;
<p>Performing a simple join on records with matching <code>Month</code> and <code>Year</code> values, we can see the size of the intersection is approximately 11 million records:</p>&#13;
&#13;
<pre data-type="programlisting">&#13;
df_mp = df_w.merge(df_psc, on=['Year','Month'],&#13;
    suffixes=('_w','_psc'))&#13;
&#13;
<strong>len(df_mp)</strong>&#13;
11135080</pre>&#13;
&#13;
<p class="pagebreak-before">A simple exact match on all four attributes yields 266 potential matches:</p>&#13;
&#13;
<pre data-type="programlisting">&#13;
df_result = df_w.merge(df_psc, on= ['Lastname','Firstname','Year','Month'], &#13;
    suffixes=('_w', '_psc'))&#13;
&#13;
df_result</pre>&#13;
&#13;
<p>A sanitized sample of these simple join matches is shown in <a data-type="xref" href="#fig-5-2">Figure 5-2</a>.</p>&#13;
&#13;
<figure><div class="figure" id="fig-5-2"><img alt="" class="iimagesch05ch05simplejoinpng" src="assets/hoer_0502.png"/>&#13;
<h6><span class="label">Figure 5-2. </span>Simple join on <code>Lastname</code>, <code>Firstname</code>, <code>Year</code>, and <code>Month</code></h6>&#13;
</div></figure>&#13;
&#13;
<section data-pdf-bookmark="Record Blocking with Splink" data-type="sect2"><div class="sect2" id="id47">&#13;
<h2>Record Blocking with Splink</h2>&#13;
&#13;
<p>To<a contenteditable="false" data-primary="Splink" data-secondary="record blocking with" data-type="indexterm" id="Srecord05"/> reduce the number of record combinations we need to consider, Splink allows us to configure<a contenteditable="false" data-primary="blocking rules" data-secondary="configuring with Splink" data-type="indexterm" id="id458"/> blocking rules. These rules determine which record pairs are evaluated to determine whether they refer to the same entity. Clearly, considering only a subset of the population creates a risk of missing true matches, it’s important to select rules that minimize this while at the same time reducing the volume as much as possible.</p>&#13;
&#13;
<p class="pagebreak-after">Splink allows us to create<a contenteditable="false" data-primary="composite rules" data-type="indexterm" id="id459"/> composite rules, essentially <code>OR</code> statements, where if any of the conditions are met, then the combination is selected for further comparison. However, in this example we’ll use only a single blocking rule that selects only records with matching year and month of birth:</p>&#13;
&#13;
<pre data-type="programlisting">&#13;
from splink.duckdb.linker import DuckDBLinker&#13;
from splink.duckdb import comparison_library as cl&#13;
settings = {&#13;
   "link_type": "link_only",&#13;
   "blocking_rules_to_generate_predictions":&#13;
      ["l.Year = r.Year and l.Month = r.Month"],&#13;
   "comparisons": [&#13;
      cl.jaro_winkler_at_thresholds("Firstname", [0.9]),&#13;
      cl.jaro_winkler_at_thresholds("Lastname", [0.9]),&#13;
      cl.exact_match("Month"),&#13;
      cl.exact_match("Year", term_frequency_adjustments=True),&#13;
      ],&#13;
    "additional_columns_to_retain": ["company_number"]&#13;
}</pre>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Attribute Comparison" data-type="sect2"><div class="sect2" id="id48">&#13;
<h2>Attribute Comparison</h2>&#13;
&#13;
<p>For<a contenteditable="false" data-primary="attribute comparison" data-secondary="for record blocking" data-secondary-sortas="record blocking" data-type="indexterm" id="ACrecord05"/> the record comparisons that are produced by the blocking rules, we will determine whether they refer to the same person by using a combination of approximate matches scores on first name and last name and exact matches on month and year. Because we are comparing names, we use the Jaro-Winkler algorithm from <a data-type="xref" href="ch03.html#chapter_3">Chapter 3</a>.</p>&#13;
&#13;
<p>We can configure Splink with a set of minimum threshold values that together segment the population; Splink will add an exact match segment and a default zero match segment for those attribute pairs that score beneath the minimum value provided. In this case, we will just use a single threshold of 0.9 to illustrate the process, giving us three segments for each component of the name. Each segment is evaluated as a separate attribute for the purposes of calculating the overall match probability of the record pair.</p>&#13;
&#13;
<p>Now that we have our settings, let’s instantiate our linker and profile the matching columns:</p>&#13;
&#13;
<pre data-type="programlisting">&#13;
linker = DuckDBLinker([df_w, df_psc], settings,&#13;
   input_table_aliases = ["df_w", "df_psc"])&#13;
linker.profile_columns(["Firstname","Lastname","Month","Year"],&#13;
   top_n=10, bottom_n=5)</pre>&#13;
&#13;
<p>You can see the results in <a data-type="xref" href="#fig-5-3">Figure 5-3</a>.</p>&#13;
&#13;
<figure><div class="figure" id="fig-5-3"><img class="iimagesch05ch05splinkprofilecolumnspng" src="assets/hoer_0503.png"/>&#13;
<h6><span class="label">Figure 5-3. </span>First name, last name, month, and year distributions</h6>&#13;
</div></figure>&#13;
&#13;
<p>We can see that we have some common first names and last names with a long tail of less frequent values. For month of birth, the values are fairly regularly distributed but for year, we see some years are more likely than others. We can take this frequency distribution into account in our matching process by setting:</p>&#13;
&#13;
<pre data-type="programlisting">&#13;
term_frequency_adjustments=True</pre>&#13;
&#13;
<p>Each year value will be considered separately for the purposes of calculating match probabilities; thus a match on an unpopular year will be weighted more highly than a match on a more frequently observed value.</p>&#13;
&#13;
<p>As we did in <a data-type="xref" href="ch04.html#chapter_4">Chapter 4</a>, we could use the expectation-maximization algorithm to determine the <em>m</em> and <em>u</em> values, that is, the match and not match probabilities, for each attribute segment. By default, these calculations consider the full population prior to applying the blocking rules.</p>&#13;
&#13;
<p>To estimate the <em>u</em> values, Splink takes a slightly different approach by taking random pairwise record comparisons, assuming they do not match, and computing how often these coincidences occur. Since the probability of two random records being a match (representing the same entity) is usually very low, this approach generates good estimates of the <em>u</em> values. An additional benefit of this approach is that if the <em>u</em> probabilities are correct, it “anchors” the<a contenteditable="false" data-primary="EM (expectation-maximization) model" data-secondary="benefits of Splink for" data-type="indexterm" id="id460"/> EM estimation procedure and greatly improves the chance of it converging to a global, rather than a local, minimum. To apply this approach, we need to make sure our random population is sufficiently large to be representative of the full range of possible combinations:</p>&#13;
&#13;
<pre data-type="programlisting">&#13;
linker.estimate_u_using_random_sampling(max_pairs=1e7)</pre>&#13;
&#13;
<p>Splink allows us to set blocking rules for estimating the match probabilities. Here the attribute parameters for each segment are estimated on the subset of the population according to the first condition and then the process is repeated for the subset selected by the second condition. Since the attributes included in the blocking condition cannot themselves be estimated, it is essential that the conditions overlap, allowing each attribute to be evaluated under at least one condition.</p>&#13;
&#13;
<div data-type="warning" epub:type="warning">&#13;
<h1>Random Sample</h1>&#13;
&#13;
<p>Note that the expectation-maximization method selects records at random, so you can expect some variation from the calculated parameters in this book if you are following along.</p>&#13;
</div>&#13;
&#13;
<p class="pagebreak-before">In this example, we block on equivalent last name and month, allowing us to estimate first name and year segment probabilities, and then we repeat with the opposite combination. This way each attribute segment is evaluated at least once:</p>&#13;
&#13;
<pre data-type="programlisting">&#13;
linker.estimate_parameters_using_expectation_maximisation&#13;
   ("l.Lastname = r.Lastname and l.Month = r.Month",&#13;
      fix_u_probabilities=False)&#13;
linker.estimate_parameters_using_expectation_maximisation&#13;
   ("l.Firstname = r.Firstname and l.Year = r.Year",&#13;
      fix_u_probabilities=False)</pre>&#13;
&#13;
<p>We can examine the resulting match weights using:</p>&#13;
&#13;
<pre data-type="programlisting">&#13;
linker.match_weights_chart()</pre>&#13;
&#13;
<figure><div class="figure" id="fig-5-4"><img alt="" class="iimagesch05ch05splinkmodelparaspng" src="assets/hoer_0504.png"/>&#13;
<h6><span class="label">Figure 5-4. </span>Model parameters</h6>&#13;
</div></figure>&#13;
&#13;
<p>In <a data-type="xref" href="#fig-5-4">Figure 5-4</a>, we can see a strongly negative prior (starting) match weight with positive weights for each attribute exact match and for approximate matches on <code>Firstname</code> and <code>Lastname</code>:</p>&#13;
&#13;
<pre data-type="programlisting">&#13;
linker.m_u_parameters_chart()</pre>&#13;
&#13;
<p>In <a data-type="xref" href="#fig-5-5">Figure 5-5</a>, we can see the proportion of matching and nonmatching record comparisons that the expectation maximization algorithm calculates for<a contenteditable="false" data-primary="" data-startref="ACrecord05" data-type="indexterm" id="id461"/> each segment.</p>&#13;
&#13;
<figure><div class="figure" id="fig-5-5"><img alt="" class="iimagesch05ch05splinkrecordcomparisonpng" src="assets/hoer_0505.png"/>&#13;
<h6><span class="label">Figure 5-5. </span>Proportion of record comparisons</h6>&#13;
</div></figure>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Match Classification" data-type="sect1"><div class="sect1" id="id49">&#13;
<h1>Match Classification</h1>&#13;
&#13;
<p>Now<a contenteditable="false" data-primary="" data-startref="Srecord05" data-type="indexterm" id="id462"/><a contenteditable="false" data-primary="" data-startref="RBatcomp05" data-type="indexterm" id="id463"/><a contenteditable="false" data-primary="record blocking" data-secondary="match classification" data-type="indexterm" id="RBmclass05"/> that we have a trained model with optimized match parameters for each attribute, we can predict whether the record pairs that aren’t blocked refer to the same entity. In this example, we set an overall threshold match probability at 0.99:</p>&#13;
&#13;
<pre data-type="programlisting">&#13;
results = linker.predict(threshold_match_probability=0.99)&#13;
pres = results.as_pandas_dataframe()</pre>&#13;
&#13;
<p>We then join the prediction results to the PSC dataset by unique ID so that we can pick up the company number that the matched entity is associated with.</p>&#13;
&#13;
<p>Then we rename our output columns and retain only the ones we need:</p>&#13;
&#13;
<pre data-type="programlisting">&#13;
pres = pres.rename(columns={"Firstname_l": "Firstname_psc",&#13;
                             "Lastname_l": "Lastname_psc",&#13;
                             "Firstname_r":"Firstname_w",&#13;
                             "Lastname_r":"Lastname_w",&#13;
                             "company_number_l":"company_number"})&#13;
pres = pres[['match_weight','match_probability',&#13;
             'Firstname_psc','Firstname_w',&#13;
             'Lastname_psc','Lastname_w','company_number']]</pre>&#13;
&#13;
<p>This gives us 346 predicted matches, both exact and approximate, as shown in <a data-type="xref" href="#fig-5-6">Figure 5-6</a> (with the PSC first names and last names sanitized).</p>&#13;
&#13;
<figure><div class="figure" id="fig-5-6"><img alt="" class="iimagesch05ch05matchespng" src="assets/hoer_0506.png"/>&#13;
<h6><span class="label">Figure 5-6. </span>Exact matches on <code>Lastname</code> and <code>Firstname</code></h6>&#13;
</div></figure>&#13;
&#13;
<p>If we remove the exact matches, we can examine the additional approximate matches to see how well our probabilistic approach has performed. This is shown in <a data-type="xref" href="#fig-5-7">Figure 5-7</a> (with the PSC first names and last names sanitized):</p>&#13;
&#13;
<pre data-type="programlisting">&#13;
pres[(pres['Lastname_psc']!=pres['Lastname_w']) |&#13;
      (pres['Firstname_psc']!=pres['Firstname_w'])]</pre>&#13;
&#13;
<figure><div class="figure" id="fig-5-7"><img alt="" class="iimagesch05ch05partialmatchespng" src="assets/hoer_0507.png"/>&#13;
<h6><span class="label">Figure 5-7. </span>Approximate matches—nonexact <code>Firstname</code> or <code>Lastname</code></h6>&#13;
</div></figure>&#13;
&#13;
<p>Examining the results, shown in <a data-type="xref" href="#table-5-1">Table 5-1</a>, we can see several candidates that may be true positive matches.</p>&#13;
&#13;
<table id="table-5-1">&#13;
	<caption><span class="label">Table 5-1. </span>Approximate matches—manual comparison</caption>&#13;
	<thead>&#13;
		<tr>&#13;
			<th scope="col"><code>match_weight</code></th>&#13;
			<th scope="col"><code>match_probability</code></th>&#13;
			<th scope="col"><code>Firstname_psc</code></th>&#13;
			<th scope="col"><code>Firstname_w</code></th>&#13;
			<th scope="col"><code>Lastname_psc</code></th>&#13;
			<th scope="col"><code>Lastname_w</code></th>&#13;
			<th scope="col"><code>company_​num⁠ber</code></th>&#13;
		</tr>&#13;
	</thead>&#13;
	<tbody>&#13;
		<tr>&#13;
			<td>13.51481459</td>&#13;
			<td>0.999914572</td>&#13;
			<td>John</td>&#13;
			<td>John</td>&#13;
			<td>Mcdonnell</td>&#13;
			<td>McDonnell</td>&#13;
			<td>5350064</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td>11.66885836</td>&#13;
			<td>0.999692963</td>&#13;
			<td>Stephen</td>&#13;
			<td>Stephen</td>&#13;
			<td>Mcpartland</td>&#13;
			<td>McPartland</td>&#13;
			<td>7572556</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td>11.50728191</td>&#13;
			<td>0.999656589</td>&#13;
			<td>James</td>&#13;
			<td>James</td>&#13;
			<td>Heappey Mp</td>&#13;
			<td>Heappey</td>&#13;
			<td>5074477</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td>9.637598832</td>&#13;
			<td>0.998746141</td>&#13;
			<td>Matt</td>&#13;
			<td>Matthew</td>&#13;
			<td>Hancock</td>&#13;
			<td>Hancock</td>&#13;
			<td>14571407</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td>13.51481459</td>&#13;
			<td>0.999914572</td>&#13;
			<td>John</td>&#13;
			<td>John</td>&#13;
			<td>Mcdonnell</td>&#13;
			<td>McDonnell</td>&#13;
			<td>4662034</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td>9.320995827</td>&#13;
			<td>0.998438931</td>&#13;
			<td>Siobhan</td>&#13;
			<td>Siobhan</td>&#13;
			<td>Mcdonagh</td>&#13;
			<td>McDonagh</td>&#13;
			<td>246884</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td>11.46050878</td>&#13;
			<td>0.999645277</td>&#13;
			<td>Alison</td>&#13;
			<td>Alison</td>&#13;
			<td>Mcgovern</td>&#13;
			<td>McGovern</td>&#13;
			<td>10929919</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td>9.57364719</td>&#13;
			<td>0.998689384</td>&#13;
			<td>Jessica</td>&#13;
			<td>Jess</td>&#13;
			<td>Phillips</td>&#13;
			<td>Phillips</td>&#13;
			<td>560074</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td>12.14926274</td>&#13;
			<td>0.999779904</td>&#13;
			<td>Grahame</td>&#13;
			<td>Grahame</td>&#13;
			<td>Morris Mp</td>&#13;
			<td>Morris</td>&#13;
			<td>13523499</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td>11.66885836</td>&#13;
			<td>0.999692963</td>&#13;
			<td>Stephen</td>&#13;
			<td>Stephen</td>&#13;
			<td>Mcpartland</td>&#13;
			<td>McPartland</td>&#13;
			<td>9165947</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td>13.51481459</td>&#13;
			<td>0.999914572</td>&#13;
			<td>John</td>&#13;
			<td>John</td>&#13;
			<td>Mcdonnell</td>&#13;
			<td>McDonnell</td>&#13;
			<td>6496912</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td>11.62463457</td>&#13;
			<td>0.999683409</td>&#13;
			<td>Anna</td>&#13;
			<td>Anna</td>&#13;
			<td>Mcmorrin</td>&#13;
			<td>McMorrin</td>&#13;
			<td>9965110</td>&#13;
		</tr>&#13;
	</tbody>&#13;
</table>&#13;
&#13;
<p>Despite our initial data standardization, we can see that we have<a contenteditable="false" data-primary="case sensitivity" data-type="indexterm" id="id464"/> inconsistent capitalization on last name, and we also have a couple of PSC records where the last name is appended with “Mp.” This is frequently the case with<a contenteditable="false" data-primary="entity resolution" data-secondary="iterative nature of" data-type="indexterm" id="id465"/> entity resolution problems—we often have to iterate several times, refining our data standardization as we learn more about our dataset.<a contenteditable="false" data-primary="" data-startref="RBmclass05" data-type="indexterm" id="id466"/></p>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Measuring Performance" data-type="sect1"><div class="sect1" id="id125">&#13;
<h1>Measuring Performance</h1>&#13;
&#13;
<p>If<a contenteditable="false" data-primary="record blocking" data-secondary="measuring performance" data-type="indexterm" id="id467"/><a contenteditable="false" data-primary="performance, measuring" data-secondary="record blocking" data-type="indexterm" id="id468"/> we assume that all the exact matches and the approximate matches in <a data-type="xref" href="#table-5-1">Table 5-1</a> are true positive matches, then we can calculate our precision metrics as:</p>&#13;
&#13;
<div data-type="equation">&#13;
<p><math alttext="upper T r u e p o s i t i v e m a t c h e s left-parenthesis upper F upper P right-parenthesis equals 266 plus 12 equals 278">&#13;
  <mrow>&#13;
    <mi>T</mi>&#13;
    <mi>r</mi>&#13;
    <mi>u</mi>&#13;
    <mi>e</mi>&#13;
    <mspace width="0.166667em"/>&#13;
    <mi>p</mi>&#13;
    <mi>o</mi>&#13;
    <mi>s</mi>&#13;
    <mi>i</mi>&#13;
    <mi>t</mi>&#13;
    <mi>i</mi>&#13;
    <mi>v</mi>&#13;
    <mi>e</mi>&#13;
    <mspace width="0.166667em"/>&#13;
    <mi>m</mi>&#13;
    <mi>a</mi>&#13;
    <mi>t</mi>&#13;
    <mi>c</mi>&#13;
    <mi>h</mi>&#13;
    <mi>e</mi>&#13;
    <mi>s</mi>&#13;
    <mspace width="0.166667em"/>&#13;
    <mo>(</mo>&#13;
    <mi>F</mi>&#13;
    <mi>P</mi>&#13;
    <mo>)</mo>&#13;
    <mo>=</mo>&#13;
    <mn>266</mn>&#13;
    <mo>+</mo>&#13;
    <mn>12</mn>&#13;
    <mo>=</mo>&#13;
    <mn>278</mn>&#13;
  </mrow>&#13;
</math></p>&#13;
&#13;
<p><math alttext="upper F a l s e p o s i t i v e m a t c h e s left-parenthesis upper F upper P right-parenthesis equals 80 minus 12 equals 68">&#13;
  <mrow>&#13;
    <mi>F</mi>&#13;
    <mi>a</mi>&#13;
    <mi>l</mi>&#13;
    <mi>s</mi>&#13;
    <mi>e</mi>&#13;
    <mspace width="0.166667em"/>&#13;
    <mi>p</mi>&#13;
    <mi>o</mi>&#13;
    <mi>s</mi>&#13;
    <mi>i</mi>&#13;
    <mi>t</mi>&#13;
    <mi>i</mi>&#13;
    <mi>v</mi>&#13;
    <mi>e</mi>&#13;
    <mspace width="0.166667em"/>&#13;
    <mi>m</mi>&#13;
    <mi>a</mi>&#13;
    <mi>t</mi>&#13;
    <mi>c</mi>&#13;
    <mi>h</mi>&#13;
    <mi>e</mi>&#13;
    <mi>s</mi>&#13;
    <mspace width="0.166667em"/>&#13;
    <mo>(</mo>&#13;
    <mi>F</mi>&#13;
    <mi>P</mi>&#13;
    <mo>)</mo>&#13;
    <mo>=</mo>&#13;
    <mn>80</mn>&#13;
    <mo>-</mo>&#13;
    <mn>12</mn>&#13;
    <mo>=</mo>&#13;
    <mn>68</mn>&#13;
  </mrow>&#13;
</math></p>&#13;
&#13;
<p><math alttext="upper P r e c i s i o n equals StartFraction upper T upper P Over left-parenthesis upper T upper P plus upper F upper P right-parenthesis EndFraction equals StartFraction 278 Over left-parenthesis 278 plus 68 right-parenthesis EndFraction almost-equals 80 percent-sign">&#13;
  <mrow>&#13;
    <mi>P</mi>&#13;
    <mi>r</mi>&#13;
    <mi>e</mi>&#13;
    <mi>c</mi>&#13;
    <mi>i</mi>&#13;
    <mi>s</mi>&#13;
    <mi>i</mi>&#13;
    <mi>o</mi>&#13;
    <mi>n</mi>&#13;
    <mo>=</mo>&#13;
    <mfrac><mrow><mi>T</mi><mi>P</mi></mrow> <mrow><mo>(</mo><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>P</mi><mo>)</mo></mrow></mfrac>&#13;
    <mo>=</mo>&#13;
    <mfrac><mn>278</mn> <mrow><mo>(</mo><mn>278</mn><mo>+</mo><mn>68</mn><mo>)</mo></mrow></mfrac>&#13;
    <mo>≈</mo>&#13;
    <mn>80</mn>&#13;
    <mo>%</mo>&#13;
  </mrow>&#13;
</math></p>&#13;
</div>&#13;
&#13;
<p>Without manual verification, we don’t definitively know which of our <code>notmatch</code> population are true or false negatives, and therefore we cannot calculate recall or overall accuracy metrics.</p>&#13;
</div></section>&#13;
&#13;
<section class="pagebreak-before" data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="id251">&#13;
<h1 class="less_space">Summary</h1>&#13;
&#13;
<p>In this chapter, we used approximate matching within a probabilistic framework  to identify members of Parliament who may have significant control over UK <span class="keep-together">companies</span>.</p>&#13;
&#13;
<p>We saw how blocking can be used to reduce the number of record pairs we need to evaluate to a practical size without unacceptably increasing the risk that we miss some important potential matches.</p>&#13;
&#13;
<p>We saw how important data standardization is to optimizing performance and how getting the best performance in entity resolution is often an iterative process.</p>&#13;
</div></section>&#13;
</div></section></body></html>