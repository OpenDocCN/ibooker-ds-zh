- en: Chapter 4\. Regression and Prediction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Perhaps the most common goal in statistics is to answer the question “Is the
    variable *X* (or more likely, <math alttext="upper X 1 comma ellipsis comma upper
    X Subscript p Baseline"><mrow><msub><mi>X</mi> <mn>1</mn></msub> <mo>,</mo> <mo>...</mo>
    <mo>,</mo> <msub><mi>X</mi> <mi>p</mi></msub></mrow></math> ) associated with
    a variable *Y*, and if so, what is the relationship and can we use it to predict
    *Y*?”
  prefs: []
  type: TYPE_NORMAL
- en: Nowhere is the nexus between statistics and data science stronger than in the
    realm of prediction—specifically, the prediction of an outcome (target) variable
    based on the values of other “predictor” variables. This process of training a
    model on data where the outcome is known, for subsequent application to data where
    the outcome is not known, is termed *supervised learning*. Another important connection
    between data science and statistics is in the area of *anomaly detection*, where
    regression diagnostics originally intended for data analysis and improving the
    regression model can be used to detect unusual records.
  prefs: []
  type: TYPE_NORMAL
- en: Simple Linear Regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Simple linear regression provides a model of the relationship between the magnitude
    of one variable and that of a second—for example, as *X* increases, *Y* also increases.
    Or as *X* increases, *Y* decreases.^([1](ch04.xhtml#idm46522856635400)) Correlation
    is another way to measure how two variables are related—see the section [“Correlation”](ch01.xhtml#Correlations).
    The difference is that while correlation measures the *strength* of an association
    between two variables, regression quantifies the *nature* of the relationship.
  prefs: []
  type: TYPE_NORMAL
- en: The Regression Equation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Simple linear regression estimates how much *Y* will change when *X* changes
    by a certain amount. With the correlation coefficient, the variables *X* and *Y*
    are interchangeable. With regression, we are trying to predict the *Y* variable
    from *X* using a linear relationship (i.e., a line):'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block" alttext="upper Y equals b 0 plus b 1 upper X"><mrow><mi>Y</mi>
    <mo>=</mo> <msub><mi>b</mi> <mn>0</mn></msub> <mo>+</mo> <msub><mi>b</mi> <mn>1</mn></msub>
    <mi>X</mi></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: We read this as “Y equals b[1] times X, plus a constant b[0].” The symbol <math
    alttext="b 0"><msub><mi>b</mi> <mn>0</mn></msub></math> is known as the *intercept*
    (or constant), and the symbol <math alttext="b 1"><msub><mi>b</mi> <mn>1</mn></msub></math>
    as the *slope* for *X*. Both appear in *R* output as *coefficients*, though in
    general use the term *coefficient* is often reserved for <math alttext="b 1"><msub><mi>b</mi>
    <mn>1</mn></msub></math> . The *Y* variable is known as the *response* or *dependent*
    variable since it depends on *X*. The *X* variable is known as the *predictor*
    or *independent* variable. The machine learning community tends to use other terms,
    calling *Y* the *target* and *X* a *feature* vector. Throughout this book, we
    will use the terms *predictor* and *feature* interchangeably.
  prefs: []
  type: TYPE_NORMAL
- en: Consider the scatterplot in [Figure 4-1](#cotton) displaying the number of years
    a worker was exposed to cotton dust (`Exposure`) versus a measure of lung capacity
    (`PEFR` or “peak expiratory flow rate”). How is `PEFR` related to `Exposure`?
    It’s hard to tell based just on the picture.
  prefs: []
  type: TYPE_NORMAL
- en: '![images/lung_scatter.png](Images/psd2_0401.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-1\. Cotton exposure versus lung capacity
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Simple linear regression tries to find the “best” line to predict the response
    `PEFR` as a function of the predictor variable `Exposure`:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block" alttext="PEFR equals b 0 plus b 1 Exposure"><mrow><mtext>PEFR</mtext>
    <mo>=</mo> <msub><mi>b</mi> <mn>0</mn></msub> <mo>+</mo> <msub><mi>b</mi> <mn>1</mn></msub>
    <mtext>Exposure</mtext></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'The `lm` function in *R* can be used to fit a linear regression:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '`lm` stands for *linear model*, and the `~` symbol denotes that `PEFR` is predicted
    by `Exposure`. With this model definition, the intercept is automatically included
    and fitted. If you want to exclude the intercept from the model, you need to write
    the model definition as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Printing the `model` object produces the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The intercept, or <math alttext="b 0"><msub><mi>b</mi> <mn>0</mn></msub></math>
    , is 424.583 and can be interpreted as the predicted `PEFR` for a worker with
    zero years exposure. The regression coefficient, or <math alttext="b 1"><msub><mi>b</mi>
    <mn>1</mn></msub></math> , can be interpreted as follows: for each additional
    year that a worker is exposed to cotton dust, the worker’s `PEFR` measurement
    is reduced by –4.185.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Python*, we can use `LinearRegression` from the `scikit-learn` package.
    (the `statsmodels` package has a linear regression implementation that is more
    similar to *R* (`sm.OLS`); we will use it later in this chapter):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The regression line from this model is displayed in [Figure 4-2](#lung_model).
  prefs: []
  type: TYPE_NORMAL
- en: '![images/lung_model.png](Images/psd2_0402.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-2\. Slope and intercept for the regression fit to the lung data
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Fitted Values and Residuals
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Important concepts in regression analysis are the *fitted values* (the predictions)
    and *residuals* (prediction errors). In general, the data doesn’t fall exactly
    on a line, so the regression equation should include an explicit error term <math
    alttext="e Subscript i"><msub><mi>e</mi> <mi>i</mi></msub></math> :'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><msub><mi>Y</mi> <mi>i</mi></msub> <mo>=</mo> <msub><mi>b</mi>
    <mn>0</mn></msub> <mo>+</mo> <msub><mi>b</mi> <mn>1</mn></msub> <msub><mi>X</mi>
    <mi>i</mi></msub> <mo>+</mo> <msub><mi>e</mi> <mi>i</mi></msub></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'The fitted values, also referred to as the *predicted values*, are typically
    denoted by <math alttext="ModifyingAbove upper Y With caret Subscript i"><msub><mover
    accent="true"><mi>Y</mi> <mo>^</mo></mover> <mi>i</mi></msub></math> (Y-hat).
    These are given by:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><msub><mover accent="true"><mi>Y</mi> <mo>^</mo></mover>
    <mi>i</mi></msub> <mo>=</mo> <msub><mover accent="true"><mi>b</mi> <mo>^</mo></mover>
    <mn>0</mn></msub> <mo>+</mo> <msub><mover accent="true"><mi>b</mi> <mo>^</mo></mover>
    <mn>1</mn></msub> <msub><mi>X</mi> <mi>i</mi></msub></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: The notation <math alttext="ModifyingAbove b With caret Subscript 0"><msub><mover
    accent="true"><mi>b</mi> <mo>^</mo></mover> <mn>0</mn></msub></math> and <math
    alttext="ModifyingAbove b With caret Subscript 1"><msub><mover accent="true"><mi>b</mi>
    <mo>^</mo></mover> <mn>1</mn></msub></math> indicates that the coefficients are
    estimated versus known.
  prefs: []
  type: TYPE_NORMAL
- en: 'Hat Notation: Estimates Versus Known Values'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The “hat” notation is used to differentiate between estimates and known values.
    So the symbol <math alttext="ModifyingAbove b With caret"><mover accent="true"><mi>b</mi>
    <mo>^</mo></mover></math> (“b-hat”) is an estimate of the unknown parameter <math
    alttext="b"><mi>b</mi></math> . Why do statisticians differentiate between the
    estimate and the true value? The estimate has uncertainty, whereas the true value
    is fixed.^([2](ch04.xhtml#idm46522856258456))
  prefs: []
  type: TYPE_NORMAL
- en: 'We compute the residuals <math alttext="ModifyingAbove e With caret Subscript
    i"><msub><mover accent="true"><mi>e</mi> <mo>^</mo></mover> <mi>i</mi></msub></math>
    by subtracting the *predicted* values from the original data:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><msub><mover accent="true"><mi>e</mi> <mo>^</mo></mover>
    <mi>i</mi></msub> <mo>=</mo> <msub><mi>Y</mi> <mi>i</mi></msub> <mo>-</mo> <msub><mover
    accent="true"><mi>Y</mi> <mo>^</mo></mover> <mi>i</mi></msub></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'In *R*, we can obtain the fitted values and residuals using the functions `predict`
    and `residuals`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'With `scikit-learn`’s `LinearRegression` model, we use the `predict` method
    on the training data to get the `fitted` values and subsequently the `residuals`.
    As we will see, this is a general pattern that all models in `scikit-learn` follow:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[Figure 4-3](#residuals) illustrates the residuals from the regression line
    fit to the lung data. The residuals are the length of the vertical dashed lines
    from the data to the line.'
  prefs: []
  type: TYPE_NORMAL
- en: '![images/lung_residuals.png](Images/psd2_0403.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-3\. Residuals from a regression line (to accommodate all the data,
    the y-axis scale differs from [Figure 4-2](#lung_model), hence the apparently
    different slope)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Least Squares
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'How is the model fit to the data? When there is a clear relationship, you could
    imagine fitting the line by hand. In practice, the regression line is the estimate
    that minimizes the sum of squared residual values, also called the *residual sum
    of squares* or *RSS*:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mi>R</mi>
    <mi>S</mi> <mi>S</mi></mrow></mtd> <mtd><mo>=</mo></mtd> <mtd columnalign="left"><mrow><munderover><mo>∑</mo>
    <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>n</mi></munderover> <msup><mfenced
    separators="" open="(" close=")"><msub><mi>Y</mi> <mi>i</mi></msub> <mo>-</mo><msub><mover
    accent="true"><mi>Y</mi> <mo>^</mo></mover> <mi>i</mi></msub></mfenced> <mn>2</mn></msup></mrow></mtd></mtr>
    <mtr><mtd><mo>=</mo></mtd> <mtd columnalign="left"><mrow><munderover><mo>∑</mo>
    <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>n</mi></munderover> <msup><mfenced
    separators="" open="(" close=")"><msub><mi>Y</mi> <mi>i</mi></msub> <mo>-</mo><msub><mover
    accent="true"><mi>b</mi> <mo>^</mo></mover> <mn>0</mn></msub> <mo>-</mo><msub><mover
    accent="true"><mi>b</mi> <mo>^</mo></mover> <mn>1</mn></msub> <msub><mi>X</mi>
    <mi>i</mi></msub></mfenced> <mn>2</mn></msup></mrow></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: The estimates <math alttext="ModifyingAbove b With caret Subscript 0"><msub><mover
    accent="true"><mi>b</mi> <mo>^</mo></mover> <mn>0</mn></msub></math> and <math
    alttext="ModifyingAbove b With caret Subscript 1"><msub><mover accent="true"><mi>b</mi>
    <mo>^</mo></mover> <mn>1</mn></msub></math> are the values that minimize RSS.
  prefs: []
  type: TYPE_NORMAL
- en: The method of minimizing the sum of the squared residuals is termed *least squares*
    regression, or *ordinary least squares* (OLS) regression. It is often attributed
    to Carl Friedrich Gauss, the German mathematician, but was first published by
    the French mathematician Adrien-Marie Legendre in 1805. Least squares regression
    can be computed quickly and easily with any standard statistical software.
  prefs: []
  type: TYPE_NORMAL
- en: Historically, computational convenience is one reason for the widespread use
    of least squares in regression. With the advent of big data, computational speed
    is still an important factor. Least squares, like the mean (see [“Median and Robust
    Estimates”](ch01.xhtml#Median)), are sensitive to outliers, although this tends
    to be a significant problem only in small or moderate-sized data sets. See [“Outliers”](#regression_outliers)
    for a discussion of outliers in regression.
  prefs: []
  type: TYPE_NORMAL
- en: Regression Terminology
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When analysts and researchers use the term *regression* by itself, they are
    typically referring to linear regression; the focus is usually on developing a
    linear model to explain the relationship between predictor variables and a numeric
    outcome variable. In its formal statistical sense, regression also includes nonlinear
    models that yield a functional relationship between predictors and outcome variables.
    In the machine learning community, the term is also occasionally used loosely
    to refer to the use of any predictive model that produces a predicted numeric
    outcome (as opposed to classification methods that predict a binary or categorical
    outcome).
  prefs: []
  type: TYPE_NORMAL
- en: Prediction Versus Explanation (Profiling)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Historically, a primary use of regression was to illuminate a supposed linear
    relationship between predictor variables and an outcome variable. The goal has
    been to understand a relationship and explain it using the data that the regression
    was fit to. In this case, the primary focus is on the estimated slope of the regression
    equation, <math alttext="ModifyingAbove b With caret"><mover accent="true"><mi>b</mi>
    <mo>^</mo></mover></math> . Economists want to know the relationship between consumer
    spending and GDP growth. Public health officials might want to understand whether
    a public information campaign is effective in promoting safe sex practices. In
    such cases, the focus is not on predicting individual cases but rather on understanding
    the overall relationship among variables.
  prefs: []
  type: TYPE_NORMAL
- en: With the advent of big data, regression is widely used to form a model to predict
    individual outcomes for new data (i.e., a predictive model) rather than explain
    data in hand. In this instance, the main items of interest are the fitted values
    <math alttext="ModifyingAbove upper Y With caret"><mover accent="true"><mi>Y</mi>
    <mo>^</mo></mover></math> . In marketing, regression can be used to predict the
    change in revenue in response to the size of an ad campaign. Universities use
    regression to predict students’ GPA based on their SAT scores.
  prefs: []
  type: TYPE_NORMAL
- en: A regression model that fits the data well is set up such that changes in *X*
    lead to changes in *Y*. However, by itself, the regression equation does not prove
    the direction of causation. Conclusions about causation must come from a broader
    understanding about the relationship. For example, a regression equation might
    show a definite relationship between number of clicks on a web ad and number of
    conversions. It is our knowledge of the marketing process, not the regression
    equation, that leads us to the conclusion that clicks on the ad lead to sales,
    and not vice versa.
  prefs: []
  type: TYPE_NORMAL
- en: Further Reading
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For an in-depth treatment of prediction versus explanation, see Galit Shmueli’s
    article [“To Explain or to Predict?”](https://oreil.ly/4fVUY).
  prefs: []
  type: TYPE_NORMAL
- en: Multiple Linear Regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When there are multiple predictors, the equation is simply extended to accommodate
    them:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mi>Y</mi> <mo>=</mo> <msub><mi>b</mi> <mn>0</mn></msub>
    <mo>+</mo> <msub><mi>b</mi> <mn>1</mn></msub> <msub><mi>X</mi> <mn>1</mn></msub>
    <mo>+</mo> <msub><mi>b</mi> <mn>2</mn></msub> <msub><mi>X</mi> <mn>2</mn></msub>
    <mo>+</mo> <mo>...</mo> <mo>+</mo> <msub><mi>b</mi> <mi>p</mi></msub> <msub><mi>X</mi>
    <mi>p</mi></msub> <mo>+</mo> <mi>e</mi></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: Instead of a line, we now have a linear model—the relationship between each
    coefficient and its variable (feature) is linear.
  prefs: []
  type: TYPE_NORMAL
- en: 'All of the other concepts in simple linear regression, such as fitting by least
    squares and the definition of fitted values and residuals, extend to the multiple
    linear regression setting. For example, the fitted values are given by:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><msub><mover accent="true"><mi>Y</mi> <mo>^</mo></mover>
    <mi>i</mi></msub> <mo>=</mo> <msub><mover accent="true"><mi>b</mi> <mo>^</mo></mover>
    <mn>0</mn></msub> <mo>+</mo> <msub><mover accent="true"><mi>b</mi> <mo>^</mo></mover>
    <mn>1</mn></msub> <msub><mi>X</mi> <mrow><mn>1</mn><mo>,</mo><mi>i</mi></mrow></msub>
    <mo>+</mo> <msub><mover accent="true"><mi>b</mi> <mo>^</mo></mover> <mn>2</mn></msub>
    <msub><mi>X</mi> <mrow><mn>2</mn><mo>,</mo><mi>i</mi></mrow></msub> <mo>+</mo>
    <mo>...</mo> <mo>+</mo> <msub><mover accent="true"><mi>b</mi> <mo>^</mo></mover>
    <mi>p</mi></msub> <msub><mi>X</mi> <mrow><mi>p</mi><mo>,</mo><mi>i</mi></mrow></msub></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: King County Housing Data'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'An example of using multiple linear regression is in estimating the value of
    houses. County assessors must estimate the value of a house for the purposes of
    assessing taxes. Real estate professionals and home buyers consult popular websites
    such as [Zillow](https://zillow.com) to ascertain a fair price. Here are a few
    rows of housing data from King County (Seattle), Washington, from the `house data.frame`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The `head` method of `pandas` data frame lists the top rows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The goal is to predict the sales price from the other variables. The `lm` function
    handles the multiple regression case simply by including more terms on the righthand
    side of the equation; the argument `na.action=na.omit` causes the model to drop
    records that have missing values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '`scikit-learn`’s `LinearRegression` can be used for multiple linear regression
    as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Printing `house_lm` object produces the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'For a `LinearRegression` model, intercept and coefficients are the fields `intercept_`
    and `coef_` of the fitted model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The interpretation of the coefficients is as with simple linear regression:
    the predicted value <math alttext="ModifyingAbove upper Y With caret"><mover accent="true"><mi>Y</mi>
    <mo>^</mo></mover></math> changes by the coefficient <math alttext="b Subscript
    j"><msub><mi>b</mi> <mi>j</mi></msub></math> for each unit change in <math alttext="upper
    X Subscript j"><msub><mi>X</mi> <mi>j</mi></msub></math> assuming all the other
    variables, <math alttext="upper X Subscript k"><msub><mi>X</mi> <mi>k</mi></msub></math>
    for <math alttext="k not-equals j"><mrow><mi>k</mi> <mo>≠</mo> <mi>j</mi></mrow></math>
    , remain the same. For example, adding an extra finished square foot to a house
    increases the estimated value by roughly $229; adding 1,000 finished square feet
    implies the value will increase by $228,800.'
  prefs: []
  type: TYPE_NORMAL
- en: Assessing the Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The most important performance metric from a data science perspective is *root
    mean squared error*, or *RMSE*. RMSE is the square root of the average squared
    error in the predicted <math alttext="ModifyingAbove y With caret Subscript i"><msub><mover
    accent="true"><mi>y</mi> <mo>^</mo></mover> <mi>i</mi></msub></math> values:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mi>R</mi> <mi>M</mi> <mi>S</mi> <mi>E</mi> <mo>=</mo>
    <msqrt><mfrac><mrow><msubsup><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>n</mi></msubsup> <msup><mfenced separators="" open="(" close=")"><msub><mi>y</mi>
    <mi>i</mi></msub> <mo>-</mo><msub><mover accent="true"><mi>y</mi> <mo>^</mo></mover>
    <mi>i</mi></msub></mfenced> <mn>2</mn></msup></mrow> <mi>n</mi></mfrac></msqrt></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'This measures the overall accuracy of the model and is a basis for comparing
    it to other models (including models fit using machine learning techniques). Similar
    to RMSE is the *residual standard error*, or *RSE*. In this case we have *p* predictors,
    and the RSE is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mi>R</mi> <mi>S</mi> <mi>E</mi> <mo>=</mo> <msqrt><mfrac><mrow><msubsup><mo>∑</mo>
    <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>n</mi></msubsup> <msup><mfenced
    separators="" open="(" close=")"><msub><mi>y</mi> <mi>i</mi></msub> <mo>-</mo><msub><mover
    accent="true"><mi>y</mi> <mo>^</mo></mover> <mi>i</mi></msub></mfenced> <mn>2</mn></msup></mrow>
    <mfenced separators="" open="(" close=")"><mi>n</mi><mo>-</mo><mi>p</mi><mo>-</mo><mn>1</mn></mfenced></mfrac></msqrt></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: The only difference is that the denominator is the degrees of freedom, as opposed
    to number of records (see [“Degrees of Freedom”](ch03.xhtml#DOF)). In practice,
    for linear regression, the difference between RMSE and RSE is very small, particularly
    for big data applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `summary` function in *R* computes RSE as well as other metrics for a regression
    model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '`scikit-learn` provides a number of metrics for regression and classification.
    Here, we use `mean_squared_error` to get RMSE and `r2_score` for the coefficient
    of determination:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Use `statsmodels` to get a more detailed analysis of the regression model in
    *Python*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The `pandas` method `assign`, as used here, adds a constant column with value
    1 to the predictors. This is required to model the intercept.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another useful metric that you will see in software output is the *coefficient
    of determination*, also called the *R-squared* statistic or <math alttext="upper
    R squared"><msup><mi>R</mi> <mn>2</mn></msup></math> . R-squared ranges from 0
    to 1 and measures the proportion of variation in the data that is accounted for
    in the model. It is useful mainly in explanatory uses of regression where you
    want to assess how well the model fits the data. The formula for <math alttext="upper
    R squared"><msup><mi>R</mi> <mn>2</mn></msup></math> is:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><msup><mi>R</mi> <mn>2</mn></msup> <mo>=</mo> <mn>1</mn>
    <mo>-</mo> <mfrac><mrow><msubsup><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>n</mi></msubsup> <msup><mfenced separators="" open="(" close=")"><msub><mi>y</mi>
    <mi>i</mi></msub> <mo>-</mo><msub><mover accent="true"><mi>y</mi> <mo>^</mo></mover>
    <mi>i</mi></msub></mfenced> <mn>2</mn></msup></mrow> <mrow><msubsup><mo>∑</mo>
    <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>n</mi></msubsup> <msup><mfenced
    separators="" open="(" close=")"><msub><mi>y</mi> <mi>i</mi></msub> <mo>-</mo><mover
    accent="true"><mi>y</mi> <mo>¯</mo></mover></mfenced> <mn>2</mn></msup></mrow></mfrac></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: The denominator is proportional to the variance of *Y*. The output from *R*
    also reports an *adjusted R-squared*, which adjusts for the degrees of freedom,
    effectively penalizing the addition of more predictors to a model. Seldom is this
    significantly different from *R-squared* in multiple regression with large data
    sets.
  prefs: []
  type: TYPE_NORMAL
- en: 'Along with the estimated coefficients, *R* and `statsmodels` report the standard
    error of the coefficients (SE) and a *t-statistic*:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><msub><mi>t</mi> <mi>b</mi></msub> <mo>=</mo> <mfrac><mover
    accent="true"><mi>b</mi> <mo>^</mo></mover> <mrow><mtext>SE</mtext><mfenced separators=""
    open="(" close=")"><mover accent="true"><mi>b</mi> <mo>^</mo></mover></mfenced></mrow></mfrac></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: The t-statistic—and its mirror image, the p-value—measures the extent to which
    a coefficient is “statistically significant”—that is, outside the range of what
    a random chance arrangement of predictor and target variable might produce. The
    higher the t-statistic (and the lower the p-value), the more significant the predictor.
    Since parsimony is a valuable model feature, it is useful to have a tool like
    this to guide choice of variables to include as predictors (see [“Model Selection
    and Stepwise Regression”](#StepwiseRegression)).
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In addition to the t-statistic, *R* and other packages will often report a *p-value*
    (`Pr(>|t|)` in the *R* output) and *F-statistic*. Data scientists do not generally
    get too involved with the interpretation of these statistics, nor with the issue
    of statistical significance. Data scientists primarily focus on the t-statistic
    as a useful guide for whether to include a predictor in a model or not. High t-statistics
    (which go with p-values near 0) indicate a predictor should be retained in a model,
    while very low t-statistics indicate a predictor could be dropped. See [“p-Value”](ch03.xhtml#p-value)
    for more discussion.
  prefs: []
  type: TYPE_NORMAL
- en: Cross-Validation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Classic statistical regression metrics (*R²*, F-statistics, and p-values) are
    all “in-sample” metrics—they are applied to the same data that was used to fit
    the model. Intuitively, you can see that it would make a lot of sense to set aside
    some of the original data, not use it to fit the model, and then apply the model
    to the set-aside (holdout) data to see how well it does. Normally, you would use
    a majority of the data to fit the model and use a smaller portion to test the
    model.
  prefs: []
  type: TYPE_NORMAL
- en: This idea of “out-of-sample” validation is not new, but it did not really take
    hold until larger data sets became more prevalent; with a small data set, analysts
    typically want to use all the data and fit the best possible model.
  prefs: []
  type: TYPE_NORMAL
- en: Using a holdout sample, though, leaves you subject to some uncertainty that
    arises simply from variability in the small holdout sample. How different would
    the assessment be if you selected a different holdout sample?
  prefs: []
  type: TYPE_NORMAL
- en: 'Cross-validation extends the idea of a holdout sample to multiple sequential
    holdout samples. The algorithm for basic *k-fold cross-validation* is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Set aside *1/k* of the data as a holdout sample.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train the model on the remaining data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Apply (score) the model to the *1/k* holdout, and record needed model assessment
    metrics.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Restore the first *1/k* of the data, and set aside the next *1/k* (excluding
    any records that got picked the first time).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat steps 2 and 3.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat until each record has been used in the holdout portion.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Average or otherwise combine the model assessment metrics.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The division of the data into the training sample and the holdout sample is
    also called a *fold*.
  prefs: []
  type: TYPE_NORMAL
- en: Model Selection and Stepwise Regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In some problems, many variables could be used as predictors in a regression.
    For example, to predict house value, additional variables such as the basement
    size or year built could be used. In *R*, these are easy to add to the regression
    equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'In *Python*, we need to convert the categorical and boolean variables into
    numbers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Adding more variables, however, does not necessarily mean we have a better
    model. Statisticians use the principle of *Occam’s razor* to guide the choice
    of a model: all things being equal, a simpler model should be used in preference
    to a more complicated model.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Including additional variables always reduces RMSE and increases <math alttext="upper
    R squared"><msup><mi>R</mi> <mn>2</mn></msup></math> for the training data. Hence,
    these are not appropriate to help guide the model choice. One approach to including
    model complexity is to use the adjusted <math alttext="upper R squared"><msup><mi>R</mi>
    <mn>2</mn></msup></math> :'
  prefs: []
  type: TYPE_NORMAL
- en: <math><mstyle displaystyle="true"><mrow class="MJX-TeXAtom-ORD"><msubsup><mi>R</mi>
    <mrow class="MJX-TeXAtom-ORD"><mi>a</mi> <mi>d</mi> <mi>j</mi></mrow> <mn>2</mn></msubsup>
    <mo>=</mo> <mn>1</mn> <mo>−</mo> <mo stretchy="false">(</mo> <mn>1</mn> <mo>−</mo>
    <msup><mi>R</mi> <mn>2</mn></msup> <mo stretchy="false">)</mo> <mfrac><mrow><mi>n</mi>
    <mo>−</mo> <mn>1</mn></mrow> <mrow><mi>n</mi> <mo>−</mo> <mi>P</mi> <mo>−</mo>
    <mn>1</mn></mrow></mfrac></mrow></mstyle></math>
  prefs: []
  type: TYPE_NORMAL
- en: Here, *n* is the number of records and *P* is the number of variables in the
    model.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the 1970s, Hirotugu Akaike, the eminent Japanese statistician, developed
    a metric called *AIC* (Akaike’s Information Criteria) that penalizes adding terms
    to a model. In the case of regression, AIC has the form:'
  prefs: []
  type: TYPE_NORMAL
- en: AIC = 2*P* + *n* log(`RSS`/*n*)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: where *P* is the number of variables and *n* is the number of records. The goal
    is to find the model that minimizes AIC; models with *k* more extra variables
    are penalized by 2*k*.
  prefs: []
  type: TYPE_NORMAL
- en: AIC, BIC, and Mallows Cp
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The formula for AIC may seem a bit mysterious, but in fact it is based on asymptotic
    results in information theory. There are several variants to AIC:'
  prefs: []
  type: TYPE_NORMAL
- en: AICc
  prefs: []
  type: TYPE_NORMAL
- en: A version of AIC corrected for small sample sizes.
  prefs: []
  type: TYPE_NORMAL
- en: BIC or Bayesian information criteria
  prefs: []
  type: TYPE_NORMAL
- en: Similar to AIC, with a stronger penalty for including additional variables to
    the model.
  prefs: []
  type: TYPE_NORMAL
- en: Mallows Cp
  prefs: []
  type: TYPE_NORMAL
- en: A variant of AIC developed by Colin Mallows.
  prefs: []
  type: TYPE_NORMAL
- en: These are typically reported as in-sample metrics (i.e., on the training data),
    and data scientists using holdout data for model assessment do not need to worry
    about the differences among them or the underlying theory behind them.
  prefs: []
  type: TYPE_NORMAL
- en: 'How do we find the model that minimizes AIC or maximizes adjusted <math alttext="upper
    R squared"><msup><mi>R</mi> <mn>2</mn></msup></math> ? One way is to search through
    all possible models, an approach called *all subset regression*. This is computationally
    expensive and is not feasible for problems with large data and many variables.
    An attractive alternative is to use *stepwise regression*. It could start with
    a full model and successively drop variables that don’t contribute meaningfully.
    This is called *backward elimination*. Alternatively one could start with a constant
    model and successively add variables (*forward selection*). As a third option
    we can also successively add and drop predictors to find a model that lowers AIC
    or adjusted <math alttext="upper R squared"><msup><mi>R</mi> <mn>2</mn></msup></math>
    . The `MASS` in *R* package by Venebles and Ripley offers a stepwise regression
    function called `stepAIC`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '`scikit-learn` has no implementation for stepwise regression. We implemented
    functions `stepwise_selection`, `forward_selection`, and `backward_elimination`
    in our `dmba` package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_regression_and_prediction_CO1-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Define a function that returns a fitted model for a given set of variables.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_regression_and_prediction_CO1-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Define a function that returns a score for a given model and set of variables.
    In this case, we use the `AIC_score` implemented in the `dmba` package.
  prefs: []
  type: TYPE_NORMAL
- en: 'The function chose a model in which several variables were dropped from `house_full`:
    `SqFtLot`, `NbrLivingUnits`, `YrRenovated`, and `NewConstruction`.'
  prefs: []
  type: TYPE_NORMAL
- en: Simpler yet are *forward selection* and *backward selection*. In forward selection,
    you start with no predictors and add them one by one, at each step adding the
    predictor that has the largest contribution to <math alttext="upper R squared"><msup><mi>R</mi>
    <mn>2</mn></msup></math> , and stopping when the contribution is no longer statistically
    significant. In backward selection, or *backward elimination*, you start with
    the full model and take away predictors that are not statistically significant
    until you are left with a model in which all predictors are statistically significant.
  prefs: []
  type: TYPE_NORMAL
- en: '*Penalized regression* is similar in spirit to AIC. Instead of explicitly searching
    through a discrete set of models, the model-fitting equation incorporates a constraint
    that penalizes the model for too many variables (parameters). Rather than eliminating
    predictor variables entirely—as with stepwise, forward, and backward selection—penalized
    regression applies the penalty by reducing coefficients, in some cases to near
    zero. Common penalized regression methods are *ridge regression* and *lasso regression*.'
  prefs: []
  type: TYPE_NORMAL
- en: Stepwise regression and all subset regression are *in-sample* methods to assess
    and tune models. This means the model selection is possibly subject to overfitting
    (fitting the noise in the data) and may not perform as well when applied to new
    data. One common approach to avoid this is to use cross-validation to validate
    the models. In linear regression, overfitting is typically not a major issue,
    due to the simple (linear) global structure imposed on the data. For more sophisticated
    types of models, particularly iterative procedures that respond to local data
    structure, cross-validation is a very important tool; see [“Cross-Validation”](#CrossValidation)
    for details.
  prefs: []
  type: TYPE_NORMAL
- en: Weighted Regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Weighted regression is used by statisticians for a variety of purposes; in
    particular, it is important for analysis of complex surveys. Data scientists may
    find weighted regression useful in two cases:'
  prefs: []
  type: TYPE_NORMAL
- en: Inverse-variance weighting when different observations have been measured with
    different precision; the higher variance ones receiving lower weights.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analysis of data where rows represent multiple cases; the weight variable encodes
    how many original observations each row represents.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For example, with the housing data, older sales are less reliable than more
    recent sales. Using the `DocumentDate` to determine the year of the sale, we can
    compute a `Weight` as the number of years since 2005 (the beginning of the data):'
  prefs: []
  type: TYPE_NORMAL
- en: '*R*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '*Python*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'We can compute a weighted regression with the `lm` function using the `weight`
    argument:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The coefficients in the weighted regression are slightly different from the
    original regression.
  prefs: []
  type: TYPE_NORMAL
- en: 'Most models in `scikit-learn` accept weights as the keyword argument `sample_weight`
    in the call of the `fit` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Further Reading
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An excellent treatment of cross-validation and resampling can be found in *An
    Introduction to Statistical Learning* by Gareth James, Daniela Witten, Trevor
    Hastie, and Robert Tibshirani (Springer, 2013).
  prefs: []
  type: TYPE_NORMAL
- en: Prediction Using Regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The primary purpose of regression in data science is prediction. This is useful
    to keep in mind, since regression, being an old and established statistical method,
    comes with baggage that is more relevant to its traditional role as a tool for
    explanatory modeling than to prediction.
  prefs: []
  type: TYPE_NORMAL
- en: The Dangers of Extrapolation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Regression models should not be used to extrapolate beyond the range of the
    data (leaving aside the use of regression for time series forecasting.). The model
    is valid only for predictor values for which the data has sufficient values (even
    in the case that sufficient data is available, there could be other problems—see
    [“Regression Diagnostics”](#RegressionDiagnostics)). As an extreme case, suppose
    `model_lm` is used to predict the value of a 5,000-square-foot empty lot. In such
    a case, all the predictors related to the building would have a value of 0, and
    the regression equation would yield an absurd prediction of –521,900 + 5,000 ×
    –.0605 = –$522,202. Why did this happen? The data contains only parcels with buildings—there
    are no records corresponding to vacant land. Consequently, the model has no information
    to tell it how to predict the sales price for vacant land.
  prefs: []
  type: TYPE_NORMAL
- en: Confidence and Prediction Intervals
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Much of statistics involves understanding and measuring variability (uncertainty).
    The t-statistics and p-values reported in regression output deal with this in
    a formal way, which is sometimes useful for variable selection (see [“Assessing
    the Model”](#RMSE)). More useful metrics are confidence intervals, which are uncertainty
    intervals placed around regression coefficients and predictions. An easy way to
    understand this is via the bootstrap (see [“The Bootstrap”](ch02.xhtml#bootstrap)
    for more details about the general bootstrap procedure). The most common regression
    confidence intervals encountered in software output are those for regression parameters
    (coefficients). Here is a bootstrap algorithm for generating confidence intervals
    for regression parameters (coefficients) for a data set with *P* predictors and
    *n* records (rows):'
  prefs: []
  type: TYPE_NORMAL
- en: Consider each row (including outcome variable) as a single “ticket” and place
    all the *n* tickets in a box.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Draw a ticket at random, record the values, and replace it in the box.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat step 2 *n* times; you now have one bootstrap resample.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fit a regression to the bootstrap sample, and record the estimated coefficients.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat steps 2 through 4, say, 1,000 times.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You now have 1,000 bootstrap values for each coefficient; find the appropriate
    percentiles for each one (e.g., 5th and 95th for a 90% confidence interval).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You can use the `Boot` function in *R* to generate actual bootstrap confidence
    intervals for the coefficients, or you can simply use the formula-based intervals
    that are a routine *R* output. The conceptual meaning and interpretation are the
    same, and not of central importance to data scientists, because they concern the
    regression coefficients. Of greater interest to data scientists are intervals
    around predicted *y* values ( <math alttext="ModifyingAbove upper Y With caret
    Subscript i"><msub><mover accent="true"><mi>Y</mi> <mo>^</mo></mover> <mi>i</mi></msub></math>
    ). The uncertainty around <math alttext="ModifyingAbove upper Y With caret Subscript
    i"><msub><mover accent="true"><mi>Y</mi> <mo>^</mo></mover> <mi>i</mi></msub></math>
    comes from two sources:'
  prefs: []
  type: TYPE_NORMAL
- en: Uncertainty about what the relevant predictor variables and their coefficients
    are (see the preceding bootstrap algorithm)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Additional error inherent in individual data points
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The individual data point error can be thought of as follows: even if we knew
    for certain what the regression equation was (e.g., if we had a huge number of
    records to fit it), the *actual* outcome values for a given set of predictor values
    will vary. For example, several houses—each with 8 rooms, a 6,500-square-foot
    lot, 3 bathrooms, and a basement—might have different values. We can model this
    individual error with the residuals from the fitted values. The bootstrap algorithm
    for modeling both the regression model error and the individual data point error
    would look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Take a bootstrap sample from the data (spelled out in greater detail earlier).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fit the regression, and predict the new value.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Take a single residual at random from the original regression fit, add it to
    the predicted value, and record the result.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat steps 1 through 3, say, 1,000 times.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Find the 2.5th and the 97.5th percentiles of the results.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Prediction Interval or Confidence Interval?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A prediction interval pertains to uncertainty around a single value, while a
    confidence interval pertains to a mean or other statistic calculated from multiple
    values. Thus, a prediction interval will typically be much wider than a confidence
    interval for the same value. We model this individual value error in the bootstrap
    model by selecting an individual residual to tack on to the predicted value. Which
    should you use? That depends on the context and the purpose of the analysis, but,
    in general, data scientists are interested in specific individual predictions,
    so a prediction interval would be more appropriate. Using a confidence interval
    when you should be using a prediction interval will greatly underestimate the
    uncertainty in a given predicted value.
  prefs: []
  type: TYPE_NORMAL
- en: Factor Variables in Regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Factor* variables, also termed *categorical* variables, take on a limited
    number of discrete values. For example, a loan purpose can be “debt consolidation,”
    “wedding,” “car,” and so on. The binary (yes/no) variable, also called an *indicator*
    variable, is a special case of a factor variable. Regression requires numerical
    inputs, so factor variables need to be recoded to use in the model. The most common
    approach is to convert a variable into a set of binary *dummy* variables.'
  prefs: []
  type: TYPE_NORMAL
- en: Dummy Variables Representation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the King County housing data, there is a factor variable for the property
    type; a small subset of six records is shown below:'
  prefs: []
  type: TYPE_NORMAL
- en: '*R*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '*Python*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'There are three possible values: `Multiplex`, `Single Family`, and `Townhouse`.
    To use this factor variable, we need to convert it to a set of binary variables.
    We do this by creating a binary variable for each possible value of the factor
    variable. To do this in *R*, we use the `model.matrix` function:^([3](ch04.xhtml#idm46522853979800))'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: The function `model.matrix` converts a data frame into a matrix suitable to
    a linear model. The factor variable `PropertyType`, which has three distinct levels,
    is represented as a matrix with three columns. In the machine learning community,
    this representation is referred to as *one hot encoding* (see [“One Hot Encoder”](ch06.xhtml#OneHotEncoder)).
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Python*, we can convert categorical variables to dummies using the `pandas`
    method `get_dummies`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_regression_and_prediction_CO2-1)'
  prefs: []
  type: TYPE_NORMAL
- en: By default, returns one hot encoding of the categorical variable.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_regression_and_prediction_CO2-2)'
  prefs: []
  type: TYPE_NORMAL
- en: The keyword argument `drop_first` will return *P* – 1 columns. Use this to avoid
    the problem of multicollinearity.
  prefs: []
  type: TYPE_NORMAL
- en: In certain machine learning algorithms, such as nearest neighbors and tree models,
    one hot encoding is the standard way to represent factor variables (for example,
    see [“Tree Models”](ch06.xhtml#TreeModels)).
  prefs: []
  type: TYPE_NORMAL
- en: In the regression setting, a factor variable with *P* distinct levels is usually
    represented by a matrix with only *P* – 1 columns. This is because a regression
    model typically includes an intercept term. With an intercept, once you have defined
    the values for *P* – 1 binaries, the value for the *P*th is known and could be
    considered redundant. Adding the *P*th column will cause a multicollinearity error
    (see [“Multicollinearity”](#Multicollinearity)).
  prefs: []
  type: TYPE_NORMAL
- en: 'The default representation in *R* is to use the first factor level as a *reference*
    and interpret the remaining levels relative to that factor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The method `get_dummies` takes the optional keyword argument `drop_first` to
    exclude the first factor as *reference*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The output from the *R* regression shows two coefficients corresponding to
    `PropertyType`: `PropertyTypeSingle Family` and `PropertyTypeTownhouse`. There
    is no coefficient of `Multiplex` since it is implicitly defined when `PropertyTypeSingle
    Family == 0` and `PropertyTypeTownhouse == 0`. The coefficients are interpreted
    as relative to `Multiplex`, so a home that is `Single Family` is worth almost
    $85,000 less, and a home that is `Townhouse` is worth over $150,000 less.^([4](ch04.xhtml#idm46522853557496))'
  prefs: []
  type: TYPE_NORMAL
- en: Different Factor Codings
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are several different ways to encode factor variables, known as *contrast
    coding* systems. For example, *deviation coding*, also known as *sum contrasts*,
    compares each level against the overall mean. Another contrast is *polynomial
    coding*, which is appropriate for ordered factors; see the section [“Ordered Factor
    Variables”](#OrderedFactorsRegression). With the exception of ordered factors,
    data scientists will generally not encounter any type of coding besides reference
    coding or one hot encoder.
  prefs: []
  type: TYPE_NORMAL
- en: Factor Variables with Many Levels
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Some factor variables can produce a huge number of binary dummies—zip codes
    are a factor variable, and there are 43,000 zip codes in the US. In such cases,
    it is useful to explore the data, and the relationships between predictor variables
    and the outcome, to determine whether useful information is contained in the categories.
    If so, you must further decide whether it is useful to retain all factors, or
    whether the levels should be consolidated.
  prefs: []
  type: TYPE_NORMAL
- en: 'In King County, there are 80 zip codes with a house sale:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The `value_counts` method of `pandas` data frames returns the same information:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '`ZipCode` is an important variable, since it is a proxy for the effect of location
    on the value of a house. Including all levels requires 79 coefficients corresponding
    to 79 degrees of freedom. The original model `house_lm` has only 5 degrees of
    freedom; see [“Assessing the Model”](#RMSE). Moreover, several zip codes have
    only one sale. In some problems, you can consolidate a zip code using the first
    two or three digits, corresponding to a submetropolitan geographic region. For
    King County, almost all of the sales occur in 980xx or 981xx, so this doesn’t
    help.'
  prefs: []
  type: TYPE_NORMAL
- en: 'An alternative approach is to group the zip codes according to another variable,
    such as sale price. Even better is to form zip code groups using the residuals
    from an initial model. The following `dplyr` code in *R* consolidates the 80 zip
    codes into five groups based on the median of the residual from the `house_lm`
    regression:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: The median residual is computed for each zip, and the `ntile` function is used
    to split the zip codes, sorted by the median, into five groups. See [“Confounding
    Variables”](#ConfoundingVariables) for an example of how this is used as a term
    in a regression improving upon the original fit.
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Python* we can calculate this information as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: The concept of using the residuals to help guide the regression fitting is a
    fundamental step in the modeling process; see [“Regression Diagnostics”](#RegressionDiagnostics).
  prefs: []
  type: TYPE_NORMAL
- en: Ordered Factor Variables
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Some factor variables reflect levels of a factor; these are termed *ordered
    factor variables* or *ordered categorical variables*. For example, the loan grade
    could be A, B, C, and so on—each grade carries more risk than the prior grade.
    Often, ordered factor variables can be converted to numerical values and used
    as is. For example, the variable `BldgGrade` is an ordered factor variable. Several
    of the types of grades are shown in [Table 4-1](#BldgGrade). While the grades
    have specific meaning, the numeric value is ordered from low to high, corresponding
    to higher-grade homes. With the regression model `house_lm`, fit in [“Multiple
    Linear Regression”](#MultipleLinearRegression), `BldgGrade` was treated as a numeric
    variable.
  prefs: []
  type: TYPE_NORMAL
- en: Table 4-1\. Building grades and numeric equivalents
  prefs: []
  type: TYPE_NORMAL
- en: '| Value | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | Cabin |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | Substandard |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | Fair |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | Very good |'
  prefs: []
  type: TYPE_TB
- en: '| 12 | Luxury |'
  prefs: []
  type: TYPE_TB
- en: '| 13 | Mansion |'
  prefs: []
  type: TYPE_TB
- en: Treating ordered factors as a numeric variable preserves the information contained
    in the ordering that would be lost if it were converted to a factor.
  prefs: []
  type: TYPE_NORMAL
- en: Interpreting the Regression Equation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In data science, the most important use of regression is to predict some dependent
    (outcome) variable. In some cases, however, gaining insight from the equation
    itself to understand the nature of the relationship between the predictors and
    the outcome can be of value. This section provides guidance on examining the regression
    equation and interpreting it.
  prefs: []
  type: TYPE_NORMAL
- en: Correlated Predictors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In multiple regression, the predictor variables are often correlated with each
    other. As an example, examine the regression coefficients for the model `step_lm`,
    fit in [“Model Selection and Stepwise Regression”](#StepwiseRegression).
  prefs: []
  type: TYPE_NORMAL
- en: '*R*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '*Python*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The coefficient for `Bedrooms` is negative! This implies that adding a bedroom
    to a house will reduce its value. How can this be? This is because the predictor
    variables are correlated: larger houses tend to have more bedrooms, and it is
    the size that drives house value, not the number of bedrooms. Consider two homes
    of the exact same size: it is reasonable to expect that a home with more but smaller
    bedrooms would be considered less desirable.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Having correlated predictors can make it difficult to interpret the sign and
    value of regression coefficients (and can inflate the standard error of the estimates).
    The variables for bedrooms, house size, and number of bathrooms are all correlated.
    This is illustrated by the following example in *R*, which fits another regression
    removing the variables `SqFtTotLiving`, `SqFtFinBasement`, and `Bathrooms` from
    the equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: The `update` function can be used to add or remove variables from a model. Now
    the coefficient for bedrooms is positive—in line with what we would expect (though
    it is really acting as a proxy for house size, now that those variables have been
    removed).
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Python*, there is no equivalent to *R*’s `update` function. We need to
    refit the model with the modified predictor list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Correlated variables are only one issue with interpreting regression coefficients.
    In `house_lm`, there is no variable to account for the location of the home, and
    the model is mixing together very different types of regions. Location may be
    a *confounding* variable; see [“Confounding Variables”](#ConfoundingVariables)
    for further discussion.
  prefs: []
  type: TYPE_NORMAL
- en: Multicollinearity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'An extreme case of correlated variables produces multicollinearity—a condition
    in which there is redundance among the predictor variables. Perfect multicollinearity
    occurs when one predictor variable can be expressed as a linear combination of
    others. Multicollinearity occurs when:'
  prefs: []
  type: TYPE_NORMAL
- en: A variable is included multiple times by error.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*P* dummies, instead of *P* – 1 dummies, are created from a factor variable
    (see [“Factor Variables in Regression”](#FactorsRegression)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Two variables are nearly perfectly correlated with one another.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multicollinearity in regression must be addressed—variables should be removed
    until the multicollinearity is gone. A regression does not have a well-defined
    solution in the presence of perfect multicollinearity. Many software packages,
    including *R* and *Python*, automatically handle certain types of multicollinearity.
    For example, if `SqFtTotLiving` is included twice in the regression of the `house`
    data, the results are the same as for the `house_lm` model. In the case of nonperfect
    multicollinearity, the software may obtain a solution, but the results may be
    unstable.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Multicollinearity is not such a problem for nonlinear regression methods like
    trees, clustering, and nearest-neighbors, and in such methods it may be advisable
    to retain *P* dummies (instead of *P* – 1). That said, even in those methods,
    nonredundancy in predictor variables is still a virtue.
  prefs: []
  type: TYPE_NORMAL
- en: Confounding Variables
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'With correlated variables, the problem is one of commission: including different
    variables that have a similar predictive relationship with the response. With
    *confounding variables*, the problem is one of omission: an important variable
    is not included in the regression equation. Naive interpretation of the equation
    coefficients can lead to invalid conclusions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Take, for example, the King County regression equation `house_lm` from [“Example:
    King County Housing Data”](#KingCountyHousingData). The regression coefficients
    of `SqFtLot`, `Bathrooms`, and `Bedrooms` are all negative. The original regression
    model does not contain a variable to represent location—a very important predictor
    of house price. To model location, include a variable `ZipGroup` that categorizes
    the zip code into one of five groups, from least expensive (1) to most expensive
    (5):^([5](ch04.xhtml#idm46522852564136))'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The same model in *Python*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '`ZipGroup` is clearly an important variable: a home in the most expensive zip
    code group is estimated to have a higher sales price by almost $340,000. The coefficients
    of `SqFtLot` and `Bathrooms` are now positive, and adding a bathroom increases
    the sale price by $5,928.'
  prefs: []
  type: TYPE_NORMAL
- en: The coefficient for `Bedrooms` is still negative. While this is unintuitive,
    this is a well-known phenomenon in real estate. For homes of the same livable
    area and number of bathrooms, having more and therefore smaller bedrooms is associated
    with less valuable homes.
  prefs: []
  type: TYPE_NORMAL
- en: Interactions and Main Effects
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Statisticians like to distinguish between *main effects*, or independent variables,
    and the *interactions* between the main effects. Main effects are what are often
    referred to as the *predictor variables* in the regression equation. An implicit
    assumption when only main effects are used in a model is that the relationship
    between a predictor variable and the response is independent of the other predictor
    variables. This is often not the case.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, the model fit to the King County Housing Data in [“Confounding
    Variables”](#ConfoundingVariables) includes several variables as main effects,
    including `ZipCode`. Location in real estate is everything, and it is natural
    to presume that the relationship between, say, house size and the sale price depends
    on location. A big house built in a low-rent district is not going to retain the
    same value as a big house built in an expensive area. You include interactions
    between variables in *R* using the `*` operator. For the King County data, the
    following fits an interaction between `SqFtTotLiving` and `ZipGroup`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting model has four new terms: `SqFtTotLiving:ZipGroup2`, `SqFtTotLiving:ZipGroup3`,
    and so on.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Python*, we need to use the `statsmodels` package to train linear regression
    models with interactions. This package was designed similar to *R* and allows
    defining models using a formula interface:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: The `statsmodels` package takes care of categorical variables (e.g., `ZipGroup[T.1]`,
    `PropertyType[T.Single Family]`) and interaction terms (e.g., `SqFtTotLiving:ZipGroup[T.1]`).
  prefs: []
  type: TYPE_NORMAL
- en: Location and house size appear to have a strong interaction. For a home in the
    lowest `ZipGroup`, the slope is the same as the slope for the main effect `SqFtTotLiving`,
    which is $118 per square foot (this is because *R* uses *reference* coding for
    factor variables; see [“Factor Variables in Regression”](#FactorsRegression)).
    For a home in the highest `ZipGroup`, the slope is the sum of the main effect
    plus `SqFtTotLiving:ZipGroup5`, or $115 + $227 = $342 per square foot. In other
    words, adding a square foot in the most expensive zip code group boosts the predicted
    sale price by a factor of almost three, compared to the average boost from adding
    a square foot.
  prefs: []
  type: TYPE_NORMAL
- en: Model Selection with Interaction Terms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In problems involving many variables, it can be challenging to decide which
    interaction terms should be included in the model. Several different approaches
    are commonly taken:'
  prefs: []
  type: TYPE_NORMAL
- en: In some problems, prior knowledge and intuition can guide the choice of which
    interaction terms to include in the model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stepwise selection (see [“Model Selection and Stepwise Regression”](#StepwiseRegression))
    can be used to sift through the various models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Penalized regression can automatically fit to a large set of possible interaction
    terms.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perhaps the most common approach is to use *tree models*, as well as their descendants,
    *random forest* and *gradient boosted trees*. This class of models automatically
    searches for optimal interaction terms; see [“Tree Models”](ch06.xhtml#TreeModels).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regression Diagnostics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In explanatory modeling (i.e., in a research context), various steps, in addition
    to the metrics mentioned previously (see [“Assessing the Model”](#RMSE)), are
    taken to assess how well the model fits the data; most are based on analysis of
    the residuals. These steps do not directly address predictive accuracy, but they
    can provide useful insight in a predictive setting.
  prefs: []
  type: TYPE_NORMAL
- en: Outliers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Generally speaking, an extreme value, also called an *outlier*, is one that
    is distant from most of the other observations. Just as outliers need to be handled
    for estimates of location and variability (see [“Estimates of Location”](ch01.xhtml#Location)
    and [“Estimates of Variability”](ch01.xhtml#Variability)), outliers can cause
    problems with regression models. In regression, an outlier is a record whose actual
    *y* value is distant from the predicted value. You can detect outliers by examining
    the *standardized residual*, which is the residual divided by the standard error
    of the residuals.
  prefs: []
  type: TYPE_NORMAL
- en: There is no statistical theory that separates outliers from nonoutliers. Rather,
    there are (arbitrary) rules of thumb for how distant from the bulk of the data
    an observation needs to be in order to be called an outlier. For example, with
    the boxplot, outliers are those data points that are too far above or below the
    box boundaries (see [“Percentiles and Boxplots”](ch01.xhtml#Boxplots)), where
    “too far” = “more than 1.5 times the interquartile range.” In regression, the
    standardized residual is the metric that is typically used to determine whether
    a record is classified as an outlier. Standardized residuals can be interpreted
    as “the number of standard errors away from the regression line.”
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s fit a regression to the King County house sales data for all sales in
    zip code 98105 in *R*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'In *Python*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'We extract the standardized residuals in *R* using the `rstandard` function
    and obtain the index of the smallest residual using the `order` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'In `statsmodels`, use `OLSInfluence` to analyze the residuals:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'The biggest overestimate from the model is more than four standard errors above
    the regression line, corresponding to an overestimate of $757,754. The original
    data record corresponding to this outlier is as follows in *R*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'In *Python*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'In this case, it appears that there is something wrong with the record: a house
    of that size typically sells for much more than $119,748 in that zip code. [Figure 4-4](#StatutoryDeed)
    shows an excerpt from the statutory deed from this sale: it is clear that the
    sale involved only partial interest in the property. In this case, the outlier
    corresponds to a sale that is anomalous and should not be included in the regression.
    Outliers could also be the result of other problems, such as a “fat-finger” data
    entry or a mismatch of units (e.g., reporting a sale in thousands of dollars rather
    than simply in dollars).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Statutory warranty deed for the largest negative residual](Images/psd2_0404.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-4\. Statutory warrany deed for the largest negative residual
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: For big data problems, outliers are generally not a problem in fitting the regression
    to be used in predicting new data. However, outliers are central to anomaly detection,
    where finding outliers is the whole point. The outlier could also correspond to
    a case of fraud or an accidental action. In any case, detecting outliers can be
    a critical business need.
  prefs: []
  type: TYPE_NORMAL
- en: Influential Values
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A value whose absence would significantly change the regression equation is
    termed an *influential observation*. In regression, such a value need not be associated
    with a large residual. As an example, consider the regression lines in [Figure 4-5](#InfluenceExample).
    The solid line corresponds to the regression with all the data, while the dashed
    line corresponds to the regression with the point in the upper-right corner removed.
    Clearly, that data value has a huge influence on the regression even though it
    is not associated with a large outlier (from the full regression). This data value
    is considered to have high *leverage* on the regression.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to standardized residuals (see [“Outliers”](#regression_outliers)),
    statisticians have developed several metrics to determine the influence of a single
    record on a regression. A common measure of leverage is the *hat-value*; values
    above <math alttext="2 left-parenthesis upper P plus 1 right-parenthesis slash
    n"><mrow><mn>2</mn> <mo>(</mo> <mi>P</mi> <mo>+</mo> <mn>1</mn> <mo>)</mo> <mo>/</mo>
    <mi>n</mi></mrow></math> indicate a high-leverage data value.^([6](ch04.xhtml#idm46522851692056))
  prefs: []
  type: TYPE_NORMAL
- en: '![An example of an influential data point in regression](Images/psd2_0405.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-5\. An example of an influential data point in regression
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Another metric is *Cook’s distance*, which defines influence as a combination
    of leverage and residual size. A rule of thumb is that an observation has high
    influence if Cook’s distance exceeds <math alttext="4 slash left-parenthesis n
    minus upper P minus 1 right-parenthesis"><mrow><mn>4</mn> <mo>/</mo> <mo>(</mo>
    <mi>n</mi> <mo>-</mo> <mi>P</mi> <mo>-</mo> <mn>1</mn> <mo>)</mo></mrow></math>
    .
  prefs: []
  type: TYPE_NORMAL
- en: 'An *influence plot* or *bubble plot* combines standardized residuals, the hat-value,
    and Cook’s distance in a single plot. [Figure 4-6](#InfluencePlot) shows the influence
    plot for the King County house data and can be created by the following *R* code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the *Python* code to create a similar figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: There are apparently several data points that exhibit large influence in the
    regression. Cook’s distance can be computed using the function `cooks.distance`,
    and you can use `hatvalues` to compute the diagnostics. The hat values are plotted
    on the x-axis, the residuals are plotted on the y-axis, and the size of the points
    is related to the value of Cook’s distance.
  prefs: []
  type: TYPE_NORMAL
- en: '![A plot to determine which observations have high influence](Images/psd2_0406.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-6\. A plot to determine which observations have high influence; points
    with Cook’s distance greater than 0.08 are highlighted in grey
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '[Table 4-2](#InfluenceTable) compares the regression with the full data set
    and with highly influential data points removed (Cook’s distance > 0.08).'
  prefs: []
  type: TYPE_NORMAL
- en: The regression coefficient for `Bathrooms` changes quite dramatically.^([7](ch04.xhtml#idm46522851379304))
  prefs: []
  type: TYPE_NORMAL
- en: Table 4-2\. Comparison of regression coefficients with the full data and with
    influential data removed
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Original | Influential removed |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| (Intercept) | –772,550 | –647,137 |'
  prefs: []
  type: TYPE_TB
- en: '| SqFtTotLiving | 210 | 230 |'
  prefs: []
  type: TYPE_TB
- en: '| SqFtLot | 39 | 33 |'
  prefs: []
  type: TYPE_TB
- en: '| Bathrooms | 2282 | –16,132 |'
  prefs: []
  type: TYPE_TB
- en: '| Bedrooms | –26,320 | –22,888 |'
  prefs: []
  type: TYPE_TB
- en: '| BldgGrade | 130,000 | 114,871 |'
  prefs: []
  type: TYPE_TB
- en: For purposes of fitting a regression that reliably predicts future data, identifying
    influential observations is useful only in smaller data sets. For regressions
    involving many records, it is unlikely that any one observation will carry sufficient
    weight to cause extreme influence on the fitted equation (although the regression
    may still have big outliers). For purposes of anomaly detection, though, identifying
    influential observations can be very useful.
  prefs: []
  type: TYPE_NORMAL
- en: Heteroskedasticity, Non-Normality, and Correlated Errors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Statisticians pay considerable attention to the distribution of the residuals.
    It turns out that ordinary least squares (see [“Least Squares”](#OLS)) are unbiased,
    and in some cases are the “optimal” estimator, under a wide range of distributional
    assumptions. This means that in most problems, data scientists do not need to
    be too concerned with the distribution of the residuals.
  prefs: []
  type: TYPE_NORMAL
- en: The distribution of the residuals is relevant mainly for the validity of formal
    statistical inference (hypothesis tests and p-values), which is of minimal importance
    to data scientists concerned mainly with predictive accuracy. Normally distributed
    errors are a sign that the model is complete; errors that are not normally distributed
    indicate the model may be missing something. For formal inference to be fully
    valid, the residuals are assumed to be normally distributed, have the same variance,
    and be independent. One area where this may be of concern to data scientists is
    the standard calculation of confidence intervals for predicted values, which are
    based upon the assumptions about the residuals (see [“Confidence and Prediction
    Intervals”](#RegressionCIs)).
  prefs: []
  type: TYPE_NORMAL
- en: '*Heteroskedasticity* is the lack of constant residual variance across the range
    of the predicted values. In other words, errors are greater for some portions
    of the range than for others. Visualizing the data is a convenient way to analyze
    residuals.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code in *R* plots the absolute residuals versus the predicted
    values for the `lm_98105` regression fit in [“Outliers”](#regression_outliers):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: '[Figure 4-7](#HouseHetero) shows the resulting plot. Using `geom_smooth`, it
    is easy to superpose a smooth of the absolute residuals. The function calls the
    `loess` method (locally estimated scatterplot smoothing) to produce a smoothed
    estimate of the relationship between the variables on the x-axis and y-axis in
    a scatterplot (see [“Scatterplot Smoothers”](#ScatterplotSmoothers)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Python*, the `seaborn` package has the `regplot` function to create a similar
    figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: '![A plot of the absolute value of the residuals versus the predicted values](Images/psd2_0407.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-7\. A plot of the absolute value of the residuals versus the predicted
    values
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Evidently, the variance of the residuals tends to increase for higher-valued
    homes but is also large for lower-valued homes. This plot indicates that `lm_98105`
    has *heteroskedastic* errors.
  prefs: []
  type: TYPE_NORMAL
- en: Why Would a Data Scientist Care About Heteroskedasticity?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Heteroskedasticity indicates that prediction errors differ for different ranges
    of the predicted value, and may suggest an incomplete model. For example, the
    heteroskedasticity in `lm_98105` may indicate that the regression has left something
    unaccounted for in high- and low-range homes.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 4-8](#HistRegression) is a histogram of the standardized residuals
    for the `lm_98105` regression. The distribution has decidedly longer tails than
    the normal distribution and exhibits mild skewness toward larger residuals.'
  prefs: []
  type: TYPE_NORMAL
- en: '![A histogram of the residuals from the regression of the housing data](Images/psd2_0408.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-8\. A histogram of the residuals from the regression of the housing
    data
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Statisticians may also check the assumption that the errors are independent.
    This is particularly true for data that is collected over time or space. The *Durbin-Watson*
    statistic can be used to detect if there is significant autocorrelation in a regression
    involving time series data. If the errors from a regression model are correlated,
    then this information can be useful in making short-term forecasts and should
    be built into the model. See *Practical Time Series Forecasting with R*, 2nd ed.,
    by Galit Shmueli and Kenneth Lichtendahl (Axelrod Schnall, 2018) to learn more
    about how to build autocorrelation information into regression models for time
    series data. If longer-term forecasts or explanatory models are the goal, excess
    autocorrelated data at the microlevel may distract. In that case, smoothing, or
    less granular collection of data in the first place, may be in order.
  prefs: []
  type: TYPE_NORMAL
- en: Even though a regression may violate one of the distributional assumptions,
    should we care? Most often in data science, the interest is primarily in predictive
    accuracy, so some review of heteroskedasticity may be in order. You may discover
    that there is some signal in the data that your model has not captured. However,
    satisfying distributional assumptions simply for the sake of validating formal
    statistical inference (p-values, F-statistics, etc.) is not that important for
    the data scientist.
  prefs: []
  type: TYPE_NORMAL
- en: Scatterplot Smoothers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Regression is about modeling the relationship between the response and predictor
    variables. In evaluating a regression model, it is useful to use a *scatterplot
    smoother* to visually highlight relationships between two variables.
  prefs: []
  type: TYPE_NORMAL
- en: For example, in [Figure 4-7](#HouseHetero), a smooth of the relationship between
    the absolute residuals and the predicted value shows that the variance of the
    residuals depends on the value of the residual. In this case, the `loess` function
    was used; `loess` works by repeatedly fitting a series of local regressions to
    contiguous subsets to come up with a smooth. While `loess` is probably the most
    commonly used smoother, other scatterplot smoothers are available in *R*, such
    as super smooth (`supsmu`) and kernel smoothing (`ksmooth`). In *Python*, we can
    find additional smoothers in `scipy` (`wiener` or `sav`) and `statsmodels` (`kernel_regression`).
    For the purposes of evaluating a regression model, there is typically no need
    to worry about the details of these scatterplot smooths.
  prefs: []
  type: TYPE_NORMAL
- en: Partial Residual Plots and Nonlinearity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Partial residual plots* are a way to visualize how well the estimated fit
    explains the relationship between a predictor and the outcome. The basic idea
    of a partial residual plot is to isolate the relationship between a predictor
    variable and the response, *taking into account all of the other predictor variables*.
    A partial residual might be thought of as a “synthetic outcome” value, combining
    the prediction based on a single predictor with the actual residual from the full
    regression equation. A partial residual for predictor <math alttext="upper X Subscript
    i"><msub><mi>X</mi> <mi>i</mi></msub></math> is the ordinary residual plus the
    regression term associated with <math alttext="upper X Subscript i"><msub><mi>X</mi>
    <mi>i</mi></msub></math> :'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block" alttext="Partial residual equals Residual plus ModifyingAbove
    b With caret Subscript i Baseline upper X Subscript i Baseline"><mrow><mtext>Partial</mtext>
    <mtext>residual</mtext> <mo>=</mo> <mtext>Residual</mtext> <mo>+</mo> <msub><mover
    accent="true"><mi>b</mi> <mo>^</mo></mover> <mi>i</mi></msub> <msub><mi>X</mi>
    <mi>i</mi></msub></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'where <math alttext="ModifyingAbove b With caret Subscript i"><msub><mover
    accent="true"><mi>b</mi> <mo>^</mo></mover> <mi>i</mi></msub></math> is the estimated
    regression coefficient. The `predict` function in *R* has an option to return
    the individual regression terms <math alttext="ModifyingAbove b With caret Subscript
    i Baseline upper X Subscript i"><mrow><msub><mover accent="true"><mi>b</mi> <mo>^</mo></mover>
    <mi>i</mi></msub> <msub><mi>X</mi> <mi>i</mi></msub></mrow></math> :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'The partial residual plot displays the <math alttext="upper X Subscript i"><msub><mi>X</mi>
    <mi>i</mi></msub></math> predictor on the x-axis and the partial residuals on
    the y-axis. Using `ggplot2` makes it easy to superpose a smooth of the partial
    residuals:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'The `statsmodels` package has the method `sm.graphics.plot_ccpr` that creates
    a similar partial residual plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: The *R* and *Python* graphs differ by a constant shift. In *R*, a constant is
    added so that the mean of the terms is zero.
  prefs: []
  type: TYPE_NORMAL
- en: The resulting plot is shown in [Figure 4-9](#HousePartialResid). The partial
    residual is an estimate of the contribution that `SqFtTotLiving` adds to the sales
    price. The relationship between `SqFtTotLiving` and the sales price is evidently
    nonlinear (dashed line). The regression line (solid line) underestimates the sales
    price for homes less than 1,000 square feet and overestimates the price for homes
    between 2,000 and 3,000 square feet. There are too few data points above 4,000
    square feet to draw conclusions for those homes.
  prefs: []
  type: TYPE_NORMAL
- en: '![A partial residual plot of for the variable SqFtTotLiving](Images/psd2_0409.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-9\. A partial residual plot for the variable `SqFtTotLiving`
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'This nonlinearity makes sense in this case: adding 500 feet in a small home
    makes a much bigger difference than adding 500 feet in a large home. This suggests
    that, instead of a simple linear term for `SqFtTotLiving`, a nonlinear term should
    be considered (see [“Polynomial and Spline Regression”](#NonlinearTerms)).'
  prefs: []
  type: TYPE_NORMAL
- en: Polynomial and Spline Regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The relationship between the response and a predictor variable isn’t necessarily
    linear. The response to the dose of a drug is often nonlinear: doubling the dosage
    generally doesn’t lead to a doubled response. The demand for a product isn’t a
    linear function of marketing dollars spent; at some point, demand is likely to
    be saturated. There are many ways that regression can be extended to capture these
    nonlinear effects.'
  prefs: []
  type: TYPE_NORMAL
- en: Nonlinear Regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When statisticians talk about *nonlinear regression*, they are referring to
    models that can’t be fit using least squares. What kind of models are nonlinear?
    Essentially all models where the response cannot be expressed as a linear combination
    of the predictors or some transform of the predictors. Nonlinear regression models
    are harder and computationally more intensive to fit, since they require numerical
    optimization. For this reason, it is generally preferred to use a linear model
    if possible.
  prefs: []
  type: TYPE_NORMAL
- en: Polynomial
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Polynomial regression* involves including polynomial terms in a regression
    equation. The use of polynomial regression dates back almost to the development
    of regression itself with a paper by Gergonne in 1815. For example, a quadratic
    regression between the response *Y* and the predictor *X* would take the form:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mi>Y</mi> <mo>=</mo> <msub><mi>b</mi> <mn>0</mn></msub>
    <mo>+</mo> <msub><mi>b</mi> <mn>1</mn></msub> <mi>X</mi> <mo>+</mo> <msub><mi>b</mi>
    <mn>2</mn></msub> <msup><mi>X</mi> <mn>2</mn></msup> <mo>+</mo> <mi>e</mi></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Polynomial regression can be fit in *R* through the `poly` function. For example,
    the following fits a quadratic polynomial for `SqFtTotLiving` with the King County
    housing data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'In `statsmodels`, we add the squared term to the model definition using `I(SqFtTotLiving**2)`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_regression_and_prediction_CO3-1)'
  prefs: []
  type: TYPE_NORMAL
- en: The intercept and the polynomial coefficients are different compared to *R*.
    This is due to different implementations. The remaining coefficients and the predictions
    are equivalent.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are now two coefficients associated with `SqFtTotLiving`: one for the
    linear term and one for the quadratic term.'
  prefs: []
  type: TYPE_NORMAL
- en: The partial residual plot (see [“Partial Residual Plots and Nonlinearity”](#PartialResidualPlots))
    indicates some curvature in the regression equation associated with `SqFtTotLiving`.
    The fitted line more closely matches the smooth (see [“Splines”](#Splines)) of
    the partial residuals as compared to a linear fit (see [Figure 4-10](#PolynomialRegressionPlot)).
  prefs: []
  type: TYPE_NORMAL
- en: The `statsmodels` implementation works only for linear terms. The accompanying
    source code gives an implementation that will work for polynomial regression as
    well.
  prefs: []
  type: TYPE_NORMAL
- en: '![A polynomial regression fit for the variable SqFtTotLiving (solid line) versus
    a smooth (dashed line)](Images/psd2_0410.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-10\. A polynomial regression fit for the variable `SqFtTotLiving` (solid
    line) versus a smooth (dashed line; see the following section about splines)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Splines
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Polynomial regression captures only a certain amount of curvature in a nonlinear
    relationship. Adding in higher-order terms, such as a cubic quartic polynomial,
    often leads to undesirable “wiggliness” in the regression equation. An alternative,
    and often superior, approach to modeling nonlinear relationships is to use *splines*.
    *Splines* provide a way to smoothly interpolate between fixed points. Splines
    were originally used by draftsmen to draw a smooth curve, particularly in ship
    and aircraft building.
  prefs: []
  type: TYPE_NORMAL
- en: The splines were created by bending a thin piece of wood using weights, referred
    to as “ducks”; see [Figure 4-11](#SplineDucks).
  prefs: []
  type: TYPE_NORMAL
- en: '![Splines were originally created using bendable wood and ``ducks,'''' and
    were used as a draftsman''s tool to fit curves. Photo courtesy of Bob Perry.](Images/psd2_0411.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-11\. Splines were originally created using bendable wood and “ducks”
    and were used as a draftsman’s tool to fit curves (photo courtesy of Bob Perry)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The technical definition of a spline is a series of piecewise continuous polynomials.
    They were first developed during World War II at the US Aberdeen Proving Grounds
    by I. J. Schoenberg, a Romanian mathematician. The polynomial pieces are smoothly
    connected at a series of fixed points in a predictor variable, referred to as
    *knots*. Formulation of splines is much more complicated than polynomial regression;
    statistical software usually handles the details of fitting a spline. The *R*
    package `splines` includes the function `bs` to create a *b-spline* (basis spline)
    term in a regression model. For example, the following adds a b-spline term to
    the house regression model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'Two parameters need to be specified: the degree of the polynomial and the location
    of the knots. In this case, the predictor `SqFtTotLiving` is included in the model
    using a cubic spline (`degree=3`). By default, `bs` places knots at the boundaries;
    in addition, knots were also placed at the lower quartile, the median quartile,
    and the upper quartile.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `statsmodels` formula interface supports the use of splines in a similar
    way to *R*. Here, we specify the *b-spline* using `df`, the degrees of freedom.
    This will create `df` – `degree` = 6 – 3 = 3 internal knots with positions calculated
    in the same way as in the *R* code above:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'In contrast to a linear term, for which the coefficient has a direct meaning,
    the coefficients for a spline term are not interpretable. Instead, it is more
    useful to use the visual display to reveal the nature of the spline fit. [Figure 4-12](#SplineRegressionPlot)
    displays the partial residual plot from the regression. In contrast to the polynomial
    model, the spline model more closely matches the smooth, demonstrating the greater
    flexibility of splines. In this case, the line more closely fits the data. Does
    this mean the spline regression is a better model? Not necessarily: it doesn’t
    make economic sense that very small homes (less than 1,000 square feet) would
    have higher value than slightly larger homes. This is possibly an artifact of
    a confounding variable; see [“Confounding Variables”](#ConfoundingVariables).'
  prefs: []
  type: TYPE_NORMAL
- en: '![A spline regression fit for the variable SqFtTotLiving (solid line) compared
    to a smooth (dashed line)](Images/psd2_0412.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-12\. A spline regression fit for the variable `SqFtTotLiving` (solid
    line) compared to a smooth (dashed line)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Generalized Additive Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Suppose you suspect a nonlinear relationship between the response and a predictor
    variable, either by a priori knowledge or by examining the regression diagnostics.
    Polynomial terms may not be flexible enough to capture the relationship, and spline
    terms require specifying the knots. *Generalized additive models*, or *GAM*, are
    a flexible modeling technique that can be used to automatically fit a spline regression.
    The `mgcv` package in *R* can be used to fit a GAM model to the housing data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: The term `s(SqFtTotLiving)` tells the `gam` function to find the “best” knots
    for a spline term (see [Figure 4-13](#GAMPlot)).
  prefs: []
  type: TYPE_NORMAL
- en: '![A GAM regression fit for the variable SqFtTotLiving (solid line) compared
    to a smooth (dashed line)](Images/psd2_0413.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-13\. A GAM regression fit for the variable `SqFtTotLiving` (solid line)
    compared to a smooth (dashed line)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'In *Python*, we can use the `pyGAM` package. It provides methods for regression
    and classification. Here, we use `LinearGAM` to create a regression model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_regression_and_prediction_CO4-1)'
  prefs: []
  type: TYPE_NORMAL
- en: The default value for `n_splines` is 20\. This leads to overfitting for larger
    `SqFtTotLiving` values. A value of 12 leads to a more reasonable fit.
  prefs: []
  type: TYPE_NORMAL
- en: Further Reading
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For more on spline models and GAMs, see *The Elements of Statistical Learning*,
    2nd ed., by Trevor Hastie, Robert Tibshirani, and Jerome Friedman (2009), and
    its shorter cousin based on *R*, *An Introduction to Statistical Learning* by
    Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani (2013); both
    are Springer books.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To learn more about using regression models for time series forecasting, see
    *Practical Time Series Forecasting with R* by Galit Shmueli and Kenneth Lichtendahl
    (Axelrod Schnall, 2018).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Perhaps no other statistical method has seen greater use over the years than
    regression—the process of establishing a relationship between multiple predictor
    variables and an outcome variable. The fundamental form is linear: each predictor
    variable has a coefficient that describes a linear relationship between the predictor
    and the outcome. More advanced forms of regression, such as polynomial and spline
    regression, permit the relationship to be nonlinear. In classical statistics,
    the emphasis is on finding a good fit to the observed data to explain or describe
    some phenomenon, and the strength of this fit is how traditional *in-sample* metrics
    are used to assess the model. In data science, by contrast, the goal is typically
    to predict values for new data, so metrics based on predictive accuracy for out-of-sample
    data are used. Variable selection methods are used to reduce dimensionality and
    create more compact models.'
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch04.xhtml#idm46522856635400-marker)) This and subsequent sections in
    this chapter © 2020 Datastats, LLC, Peter Bruce, Andrew Bruce, and Peter Gedeck;
    used by permission.
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch04.xhtml#idm46522856258456-marker)) In Bayesian statistics, the true
    value is assumed to be a random variable with a specified distribution. In the
    Bayesian context, instead of estimates of unknown parameters, there are posterior
    and prior distributions.
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](ch04.xhtml#idm46522853979800-marker)) The `-1` argument in the `model.matrix`
    produces one hot encoding representation (by removing the intercept, hence the
    “-”). Otherwise, the default in *R* is to produce a matrix with *P* – 1 columns
    with the first factor level as a reference.
  prefs: []
  type: TYPE_NORMAL
- en: ^([4](ch04.xhtml#idm46522853557496-marker)) This is unintuitive, but can be
    explained by the impact of location as a confounding variable; see [“Confounding
    Variables”](#ConfoundingVariables).
  prefs: []
  type: TYPE_NORMAL
- en: ^([5](ch04.xhtml#idm46522852564136-marker)) There are 80 zip codes in King County,
    several with just a handful of sales. An alternative to directly using zip code
    as a factor variable, `ZipGroup` clusters similar zip codes into a single group.
    See [“Factor Variables with Many Levels”](#FactorVariablesManyLevels) for details.
  prefs: []
  type: TYPE_NORMAL
- en: ^([6](ch04.xhtml#idm46522851692056-marker)) The term *hat-value* comes from
    the notion of the hat matrix in regression. Multiple linear regression can be
    expressed by the formula <math alttext="ModifyingAbove upper Y With caret equals
    upper H upper Y"><mrow><mover accent="true"><mi>Y</mi> <mo>^</mo></mover> <mo>=</mo>
    <mi>H</mi> <mi>Y</mi></mrow></math> where <math alttext="upper H"><mi>H</mi></math>
    is the hat matrix. The hat-values correspond to the diagonal of <math alttext="upper
    H"><mi>H</mi></math> .
  prefs: []
  type: TYPE_NORMAL
- en: ^([7](ch04.xhtml#idm46522851379304-marker)) The coefficient for `Bathrooms`
    becomes negative, which is unintuitive. Location has not been taken into account,
    and the zip code 98105 contains areas of disparate types of homes. See [“Confounding
    Variables”](#ConfoundingVariables) for a discussion of confounding variables.
  prefs: []
  type: TYPE_NORMAL
