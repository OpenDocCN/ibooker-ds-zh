- en: Appendix A. Supplemental Topics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Using LaTeX Rendering with SymPy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As you get more comfortable with mathematical notation, it can be helpful to
    take your SymPy expressions and display them in mathematical notation.
  prefs: []
  type: TYPE_NORMAL
- en: The quickest way to do this is to use the `latex()` function in SymPy on your
    expression and then copy the result to a LaTeX math viewer.
  prefs: []
  type: TYPE_NORMAL
- en: '[Example A-1](#pqebhBvrfC) is an example that takes a simple expression and
    turns it into a LaTeX string. Of course we can take the results of derivatives,
    integrals, and other SymPy operations and render those as LaTeX too. But let’s
    keep the example straightforward.'
  prefs: []
  type: TYPE_NORMAL
- en: Example A-1\. Using SymPy to convert an expression into LaTeX
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'This `\frac{x^{2}}{\sqrt{2 y^{3} - 1}}` string is formatted mathlatex, and
    there are a variety of tools and document formats that can be adapted to support
    it. But to simply render the mathlatex, go to a LaTeX equation editor. Here are
    two different ones I use online:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Lagrida LaTeX Equation Editor](https://latexeditor.lagrida.com)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[CodeCogs Equation Editor](https://latex.codecogs.com)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In [Figure A-1](#daiTFCUMVR) I use Lagrida’s LaTeX editor to render the mathematical
    expression.
  prefs: []
  type: TYPE_NORMAL
- en: '![emds aa01](Images/emds_aa01.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure A-1\. Using a math editor to view the SymPy LaTeX output
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If you want to save the copy/paste step, you can append the LaTeX directly as
    an argument to the CodeCogs LaTeX editor URL as shown in [Example A-2](#ECwErAvaVL),
    and it will show the rendered math equation in your browser.
  prefs: []
  type: TYPE_NORMAL
- en: Example A-2\. Open a mathlatex rendering using CodeCogs
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: If you use Jupyter, you can also use [plugins to render mathlatex](https://oreil.ly/mWYf7).
  prefs: []
  type: TYPE_NORMAL
- en: Binomial Distribution from Scratch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you want to implement a binomial distribution from scratch, here are all
    the parts you need in [Example A-3](#HvgtibdvEk).
  prefs: []
  type: TYPE_NORMAL
- en: Example A-3\. Building a binomial distribution from scratch
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Using the `factorial()` and the `binomial_coefficient()`, we can build a binomial
    distribution function from scratch. The factorial function multiplies a consecutive
    range of integers from 1 to `n`. For example, a factorial of 5! would be <math
    alttext="1 asterisk 2 asterisk 3 asterisk 4 asterisk 5 equals 120"><mrow><mn>1</mn>
    <mo>*</mo> <mn>2</mn> <mo>*</mo> <mn>3</mn> <mo>*</mo> <mn>4</mn> <mo>*</mo> <mn>5</mn>
    <mo>=</mo> <mn>120</mn></mrow></math> .
  prefs: []
  type: TYPE_NORMAL
- en: The binomial coefficient function allows us to select *k* outcomes from *n*
    possibilities with no regard for ordering. If you have *k* = 2 and *n* = 3, that
    would yield sets (1,2) and (1,2,3), respectively. Between those two sets, the
    possible distinct combinations would be (1,3), (1,2), and (2,3). That is three
    combinations so that would be a binomial coefficient of 3\. Of course, using the
    `binomial_coefficient()` function we can avoid all that permutation work by using
    factorials and multiplication instead.
  prefs: []
  type: TYPE_NORMAL
- en: 'When implementing `binomial_distribution()`, notice how we take the binomial
    coefficient and multiply it by the probability of success `p` occurring `k` times
    (hence the exponent). We then multiply it by the opposite case: the probability
    of failure `1.0 – p` occurring `n – k` times. This allows us to account for the
    probability `p` of an event occurring versus not occurring across several trials.'
  prefs: []
  type: TYPE_NORMAL
- en: Beta Distribution from Scratch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you are curious how to build a beta distribution from scratch, you will need
    to reuse the `factorial()` function we used for the binomial distribution as well
    as the `approximate_integral()` function we built in [Chapter 2](ch02.xhtml#ch02).
  prefs: []
  type: TYPE_NORMAL
- en: Just like we did in [Chapter 1](ch01.xhtml#ch01), we pack rectangles under the
    curve for the range we are interested in as shown in [Figure A-2](#ljBKFFcbOi).
  prefs: []
  type: TYPE_NORMAL
- en: '![emds aa02](Images/emds_aa02.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure A-2\. Packing rectangles under the curve to find the area/probability
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This is using just six rectangles; we will get better accuracy if we were to
    use more rectangles. Let’s implement the `beta_distribution()` from scratch and
    integrate 1,000 rectangles between 0.9 and 1.0 as shown in [Example A-4](#QATSjURCDG).
  prefs: []
  type: TYPE_NORMAL
- en: Example A-4\. Beta distribution from scratch
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: You will notice with the `beta_distribution()` function, we provide a given
    probability `x`, an `alpha` value quantifying successes, and a `beta` value quantifying
    failures. The function will return how likely we are to observe a given likelihood
    `x`. But again, to get a probability of observing probability `x` we need to find
    an area within a range of `x` values.
  prefs: []
  type: TYPE_NORMAL
- en: Thankfully, we have our `approximate_integral()` function defined and ready
    to go from [Chapter 2](ch02.xhtml#ch02). We can calculate the probability that
    the success rate is greater than 90% as well as less than 90%, as shown in the
    last few lines.
  prefs: []
  type: TYPE_NORMAL
- en: Deriving Bayes’ Theorem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you want to understand why Bayes’ Theorem works rather than take my word
    for it, let’s do a thought experiment. Let’s say I have a population of 100,000
    people. Multiply it with our given probabilities to get the count of people who
    drink coffee and the count of people who have cancer:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mi>N</mi>
    <mo>=</mo> <mn>100,000</mn></mrow></mtd></mtr></mtable></math> <math display="block"><mtable
    displaystyle="true"><mtr><mtd columnalign="right"><mrow><mi>P</mi> <mo>(</mo>
    <mtext>Coffee</mtext> <mtext>Drinker</mtext> <mo>)</mo> <mo>=</mo> <mn>.65</mn></mrow></mtd></mtr></mtable></math>
    <math display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mi>P</mi>
    <mo>(</mo> <mtext>Cancer</mtext> <mo>)</mo> <mo>=</mo> <mn>.005</mn></mrow></mtd></mtr></mtable></math>
    <math display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mtext>Coffee</mtext>
    <mtext>Drinkers</mtext> <mo>=</mo> <mn>65,000</mn></mrow></mtd></mtr></mtable></math>
    <math display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mtext>Cancer</mtext>
    <mtext>Patients</mtext> <mo>=</mo> <mn>500</mn></mrow></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'We have 65,000 coffee drinkers and 500 cancer patients. Now of those 500 cancer
    patients, how many are coffee drinkers? We were provided with a conditional probability
    <math alttext="upper P left-parenthesis Coffee vertical-bar Cancer right-parenthesis"><mrow><mi>P</mi>
    <mo>(</mo> <mtext>Coffee|Cancer</mtext> <mo>)</mo></mrow></math> we can multiply
    against those 500 people, which should give us 425 cancer patients who drink coffee:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mi>P</mi>
    <mo>(</mo> <mtext>Coffee</mtext> <mtext>Drinker|Cancer</mtext> <mo>)</mo> <mo>=</mo>
    <mn>.85</mn></mrow></mtd></mtr></mtable></math> <math display="block"><mtable
    displaystyle="true"><mtr><mtd columnalign="right"><mrow><mtext>Coffee</mtext>
    <mtext>Drinkers</mtext> <mtext>with</mtext> <mtext>Cancer</mtext> <mo>=</mo> <mn>500</mn>
    <mo>×</mo> <mn>.85</mn> <mo>=</mo> <mn>425</mn></mrow></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Now what is the percentage of coffee drinkers who have cancer? What two numbers
    do we divide? We already have the number of people who drink coffee *and* have
    cancer. Therefore, we proportion that against the total number of coffee drinkers:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mi>P</mi> <mrow><mo>(</mo> <mtext>Cancer|Coffee</mtext>
    <mtext>Drinker</mtext> <mo>)</mo></mrow> <mo>=</mo> <mfrac><mrow><mtext>Coffee</mtext><mtext>Drinkers</mtext><mtext>with</mtext><mtext>Cancer</mtext></mrow>
    <mrow><mtext>Coffee</mtext><mtext>Drinkers</mtext></mrow></mfrac></mrow></math><math
    display="block"><mrow><mi>P</mi> <mrow><mo>(</mo> <mtext>Cancer|Coffee</mtext>
    <mtext>Drinker</mtext> <mo>)</mo></mrow> <mo>=</mo> <mfrac><mn>425</mn> <mrow><mn>65,000</mn></mrow></mfrac></mrow></math><math
    display="block"><mrow><mi>P</mi> <mo>(</mo> <mtext>Cancer|Coffee</mtext> <mtext>Drinker</mtext>
    <mo>)</mo> <mo>=</mo> <mn>0.006538</mn></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: Hold on a minute, did we just flip our conditional probability? Yes we did!
    We started with <math alttext="upper P left-parenthesis Coffee Drinker vertical-bar
    Cancer right-parenthesis"><mrow><mi>P</mi> <mo>(</mo> <mtext>Coffee</mtext> <mtext>Drinker|Cancer</mtext>
    <mo>)</mo></mrow></math> and ended up with <math alttext="upper P left-parenthesis
    Cancer vertical-bar Coffee Drinker right-parenthesis"><mrow><mi>P</mi> <mo>(</mo>
    <mtext>Cancer|Coffee</mtext> <mtext>Drinker</mtext> <mo>)</mo></mrow></math> .
    By taking two subsets of the population (65,000 coffee drinkers and 500 cancer
    patients), and then applying a joint probability using the conditional probability
    we had, we ended up with 425 people in our population who both drink coffee and
    have cancer. We then divide that by the number of coffee drinkers to get the probability
    of cancer given one’s a coffee drinker.
  prefs: []
  type: TYPE_NORMAL
- en: 'But where is Bayes’ Theorem in this? Let’s focus on the <math alttext="upper
    P left-parenthesis Cancer vertical-bar Coffee Drinker right-parenthesis"><mrow><mi>P</mi>
    <mo>(</mo> <mtext>Cancer|Coffee</mtext> <mtext>Drinker</mtext> <mo>)</mo></mrow></math>
    expression and expand it with all the expressions we previously calculated:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mi>P</mi> <mrow><mo>(</mo> <mtext>Cancer|Coffee</mtext>
    <mtext>Drinker</mtext> <mo>)</mo></mrow> <mo>=</mo> <mfrac><mrow><mn>100,000</mn><mo>×</mo><mi>P</mi><mo>(</mo><mtext>Cancer</mtext><mo>)</mo><mo>×</mo><mi>P</mi><mo>(</mo><mtext>Coffee</mtext><mtext>Drinker|Cancer</mtext><mo>)</mo></mrow>
    <mrow><mn>100,000</mn><mo>×</mo><mi>P</mi><mo>(</mo><mtext>Coffee</mtext><mtext>Drinker</mtext><mo>)</mo></mrow></mfrac></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: Notice the population <math alttext="upper N"><mi>N</mi></math> of 100,000 exists
    in both the numerator and denominator so it cancels out. Does this look familiar
    now?
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="upper P left-parenthesis Cancer vertical-bar Coffee Drinker right-parenthesis
    equals StartFraction upper P left-parenthesis Cancer right-parenthesis times upper
    P left-parenthesis Coffee Drinker vertical-bar Cancer right-parenthesis Over upper
    P left-parenthesis Coffee Drinker right-parenthesis EndFraction" display="block"><mrow><mi>P</mi>
    <mrow><mo>(</mo> <mtext>Cancer|Coffee</mtext> <mtext>Drinker</mtext> <mo>)</mo></mrow>
    <mo>=</mo> <mfrac><mrow><mi>P</mi><mo>(</mo><mtext>Cancer</mtext><mo>)</mo><mo>×</mo><mi>P</mi><mo>(</mo><mtext>Coffee</mtext><mtext>Drinker|Cancer</mtext><mo>)</mo></mrow>
    <mrow><mi>P</mi><mo>(</mo><mtext>Coffee</mtext><mtext>Drinker</mtext><mo>)</mo></mrow></mfrac></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: Sure enough, this should match Bayes’ Theorem!
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mi>P</mi> <mrow><mo>(</mo> <mtext>A|B</mtext> <mo>)</mo></mrow>
    <mo>=</mo> <mfrac><mrow><mi>P</mi><mo>(</mo><mtext>B|A</mtext><mo>)</mo><mo>*</mo><mi>P</mi><mo>(</mo><mi>B</mi><mo>)</mo></mrow>
    <mrow><mi>P</mi><mo>(</mo><mi>A</mi><mo>)</mo></mrow></mfrac></mrow></math> <math
    display="block"><mrow><mi>P</mi> <mrow><mo>(</mo> <mtext>Cancer|Coffee</mtext>
    <mtext>Drinker</mtext> <mo>)</mo></mrow> <mo>=</mo> <mfrac><mrow><mi>P</mi><mo>(</mo><mtext>Cancer</mtext><mo>)</mo><mo>×</mo><mi>P</mi><mo>(</mo><mtext>Coffee</mtext><mtext>Drinker|Cancer</mtext><mo>)</mo></mrow>
    <mrow><mi>P</mi><mo>(</mo><mtext>Coffee</mtext><mtext>Drinker</mtext><mo>)</mo></mrow></mfrac></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: So if you get confused by Bayes’ Theorem or struggle with the intuition behind
    it, try taking subsets of a fixed population based on the provided probabilities.
    You can then trace your way to flip a conditional probability.
  prefs: []
  type: TYPE_NORMAL
- en: CDF and Inverse CDF from Scratch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To calculate areas for the normal distribution, we can of course use the rectangle-packing
    method we learned in [Chapter 1](ch01.xhtml#ch01) and applied to the beta distribution
    earlier in the appendix. It doesn’t require the cumulative density function (CDF)
    but simply packed rectangles under the probability density function (PDF). Using
    this method, we can find the probability a golden retriever weighs between 61
    and 62 pounds as shown in [Example A-5](#CjcMWPpGJu), using 1,000 packed rectangles
    against the normal PDF.
  prefs: []
  type: TYPE_NORMAL
- en: Example A-5\. The normal distribution function in Python
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: That will give us about 8.25% probability a golden retriever weighs between
    61 and 62 pounds. If we wanted to leverage a CDF that is already integrated for
    us and does not require any rectangle packing, we can declare it from scratch
    as shown in [Example A-6](#KuqFAIPtpi).
  prefs: []
  type: TYPE_NORMAL
- en: Example A-6\. Using the inverse CDF (called `ppf()`) in Python
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The `math.erf()` is known as the error function and is often used to compute
    cumulative distributions. Finally, to do the inverse CDF from scratch you will
    need to use the inverse of the `erf()` function called `erfinv()`. [Example A-7](#JORdwCMIRL)
    calculates one thousand randomly generated golden retriever weights using an inverse
    CDF coded from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: Example A-7\. Generating random golden retriever weights
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Use e to Predict Event Probability Over Time
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s look at one more use case for <math alttext="e"><mi>e</mi></math> that
    you might find useful. Let’s say you are a manufacturer of propane tanks. Obviously,
    you do not want the tank to leak or else that could create hazards, particularly
    around open flames and sparks. Testing a new tank design, your engineer reports
    that there is a 5% chance in a given year that it will leak.
  prefs: []
  type: TYPE_NORMAL
- en: You know this is already an unacceptably high number, but you want to know how
    this probability compounds over time. You now ask yourself, “What is the probability
    of a leak happening within 2 years? 5 years? 10 years?” The more time that is
    exposed, would not the probability of seeing the tank leak only get higher? Euler’s
    number can come to the rescue again!
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><msub><mi>P</mi> <mrow><mi>l</mi><mi>e</mi><mi>a</mi><mi>k</mi></mrow></msub>
    <mo>=</mo> <mn>1.0</mn> <mo>-</mo> <msup><mi>e</mi> <mrow><mo>-</mo><mi>λ</mi><mi>T</mi></mrow></msup></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: This function models the probability of an event over time, or in this case
    the tank leaking after *T* time. <math alttext="e"><mi>e</mi></math> again is
    Euler’s number, lambda <math alttext="lamda"><mi>λ</mi></math> is the failure
    rate across each unit of time (each year), and *T* is the amount of time gone
    by (number of years).
  prefs: []
  type: TYPE_NORMAL
- en: If we graph this function where *T* is our x-axis, the probability of a leak
    is our y-axis, and λ = .05, [Figure A-3](#HKCiJNGOam) shows what we get.
  prefs: []
  type: TYPE_NORMAL
- en: '![emds aa03](Images/emds_aa03.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure A-3\. Predicting the probability of a leak over time
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Here is how we model this function in Python for <math alttext="lamda equals
    .05"><mrow><mi>λ</mi> <mo>=</mo> <mo>.</mo> <mn>05</mn></mrow></math> and <math
    alttext="upper T equals 5"><mrow><mi>T</mi> <mo>=</mo> <mn>5</mn></mrow></math>
    years in [Example A-8](#RJmkBwKJPf).
  prefs: []
  type: TYPE_NORMAL
- en: Example A-8\. Code for predicting the probability of a leak over time
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The probability of a tank failure after 2 years is about 9.5%, 5 years is about
    22.1%, and 10 years 39.3%. The more time that passes, the more likely the tank
    will leak. We can generalize this formula to predict any event with a probability
    in a given period and see how that probability shifts over different periods of
    time.
  prefs: []
  type: TYPE_NORMAL
- en: Hill Climbing and Linear Regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you find the calculus overwhelming in building machine learning from scratch,
    you can try a more brute-force method. Let’s try a *hill climbing* algorithm,
    where we randomly adjust *m* and *b* by adding random values for a number of iterations.
    These random values will be positive or negative (which will make the addition
    operation effectively subtraction), and we will only keep adjustments that improve
    our sum of squares.
  prefs: []
  type: TYPE_NORMAL
- en: But do we just generate any random number as the adjustment? We will want to
    prefer smaller moves but occasionally we might allow larger moves. This way, we
    have mostly fine adjustments, but occasionally we will make big jumps if they
    are needed. The best tool to do this is a standard normal distribution, with a
    mean of 0 and a standard deviation of 1\. Recall from [Chapter 3](ch03.xhtml#ch03)
    that a standard normal distribution will have a high density of values near 0,
    and the farther the value is away from 0 (in both the negative and positive direction),
    the less likely the value becomes as shown in [Figure A-4](#dvHeFXxfYw).
  prefs: []
  type: TYPE_NORMAL
- en: '![emds aa04](Images/emds_aa04.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure A-4\. Most values in a standard normal distribution are small and near
    0, while larger values are less frequent on the tails
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Circling back to the linear regression, we will start `m` and `b` at 0 or some
    other starting values. Then for 150,000 iterations in a `for` loop, we will randomly
    adjust `m` and `b` by adding values sampled from the standard normal distribution.
    If a random adjustment improves/lessens the sum of squares, we keep it. But if
    the sum of squares increases, we undo that random adjustment. In other words,
    we only keep adjustments that improve the sum of squares. Let’s take a look in
    [Example A-9](#MLMseKbSoJ).
  prefs: []
  type: TYPE_NORMAL
- en: Example A-9\. Using hill climbing for linear regression
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: You will see the progress of the algorithm, but ultimately you should get a
    fitted function of approximately `y = 1.9395722046562853x + 4.731834051245578`,
    give or take. Let’s validate this answer. When I used Excel or Desmos to perform
    a linear regression, Desmos gave me `y = 1.93939x + 4.73333`. Not bad! I got pretty
    close!
  prefs: []
  type: TYPE_NORMAL
- en: Why did we need one million iterations? Through experimentation, I found this
    with enough iterations where the solution was not really improving much anymore
    and converged closely to the optimal values for `m` and `b` to minimize the sum
    of squares. You will find many machine learning libraries and algorithms have
    a parameter for the number of iterations to perform, and it does exactly this.
    You need to have enough so it converges on the right answer approximately, but
    not so much that it wastes computation time when it has already found an acceptable
    solution.
  prefs: []
  type: TYPE_NORMAL
- en: One other question you may have is why I started the `best_loss` at an extremely
    large number. I did this to initialize the best loss with a value I know will
    be overwritten once the search starts, and it will then be compared to the new
    loss of each iteration to see if it results in an improvement. I also could have
    used positive infinity `float('inf')` instead of a very large number.
  prefs: []
  type: TYPE_NORMAL
- en: Hill Climbing and Logistic Regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Just like in the previous example with linear regression, we can also apply
    hill climbing to logistic regression. Again, use this technique if you find the
    calculus and partial derivatives to be too much at once.
  prefs: []
  type: TYPE_NORMAL
- en: 'The hill climbing methodology is identical: adjust `m` and `b` with random
    values from a normal distribution. However we do have a different objective function,
    the maximum likelihood estimation, as discussed in [Chapter 6](ch06.xhtml#ch06).
    Therefore, we only take random adjustments that increase the likelihood estimation,
    and after enough iterations we should converge on a fitted logistic regression.'
  prefs: []
  type: TYPE_NORMAL
- en: This is all demonstrated in [Example A-10](#WjhOVwMJCO).
  prefs: []
  type: TYPE_NORMAL
- en: Example A-10\. Using hill climbing for a simple logistic regression
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Refer to [Chapter 6](ch06.xhtml#ch06) for more details on the maximum likelihood
    estimation, the logistic function, and the reason we use the `log()` function.
  prefs: []
  type: TYPE_NORMAL
- en: A Brief Intro to Linear Programming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A technique that every data science professional should be familiar with is
    *linear programming*, which solves a system of inequalities by adapting systems
    of equations with “slack variables.” When you have variables that are discrete
    integers or binaries (0 or 1) in a linear programming system, it is known as *integer
    programming*. When linear continuous and integer variables are used, it is known
    as *mixed integer programming*.
  prefs: []
  type: TYPE_NORMAL
- en: While it is much more algorithm-driven than data-driven, linear programming
    and its variants can be used to solve a wide array of classic AI problems. If
    it sounds dubious to brand linear programming systems as AI, it is common practice
    by many vendors and companies as it increases the perceived value.
  prefs: []
  type: TYPE_NORMAL
- en: In practice, it is best to use the many available solver libraries to do linear
    programming for you, but resources at the end of this section will be provided
    on how to do it from scratch. For these examples we will use [PuLP](https://pypi.org/project/PuLP),
    although [Pyomo](https://www.pyomo.org) is an option as well. We will also use
    graphical intuition, although problems with more than three dimensions cannot
    be visualized easily.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s our example. You have two lines of products: the iPac and iPac Ultra.
    The iPac makes $200 profit while the iPac Ultra makes $300 profit.'
  prefs: []
  type: TYPE_NORMAL
- en: However, the assembly line can work for only 20 hours, and it takes 1 hour to
    produce the iPac and 3 hours to produce an iPac Ultra.
  prefs: []
  type: TYPE_NORMAL
- en: Only 45 kits can be provided in a day, and an iPac requires 6 kits while iPac
    Ultra requires 2 kits.
  prefs: []
  type: TYPE_NORMAL
- en: Assuming all supply will be sold, how many of the iPac and iPac Ultra should
    we sell to maximize profit?
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s first note that first constraint and break it down:'
  prefs: []
  type: TYPE_NORMAL
- en: …the assembly line can work for only 20 hours, and it takes 1 hour to produce
    the iPac and 3 hours to produce an iPac Ultra.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: We can express that as an inequality where *x* is the number of iPac units and
    *y* is the number of iPac Ultra units. Both must be positive and [Figure A-5](#kCkjAwhqqP)
    shows we can graph accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="x plus 3 y less-than-or-equal-to 20 left-parenthesis x greater-than-or-equal-to
    0 comma y greater-than-or-equal-to 0 right-parenthesis" display="block"><mrow><mi>x</mi>
    <mo>+</mo> <mn>3</mn> <mi>y</mi> <mo>≤</mo> <mn>20</mn> <mo>(</mo> <mi>x</mi>
    <mo>≥</mo> <mn>0</mn> <mo>,</mo> <mi>y</mi> <mo>≥</mo> <mn>0</mn> <mo>)</mo></mrow></math>![emds
    aa05](Images/emds_aa05.png)
  prefs: []
  type: TYPE_NORMAL
- en: Figure A-5\. Graphing the first constraint
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Now let us look at the second constraint:'
  prefs: []
  type: TYPE_NORMAL
- en: Only 45 kits can be provided in a day, and an iPac requires 6 kits while iPac
    Ultra requires 2 kits.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: We can also model and graph in [Figure A-6](#VnhgsinHGv) accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="6 x plus 2 y less-than-or-equal-to 45 left-parenthesis x greater-than-or-equal-to
    0 comma y greater-than-or-equal-to 0 right-parenthesis" display="block"><mrow><mn>6</mn>
    <mi>x</mi> <mo>+</mo> <mn>2</mn> <mi>y</mi> <mo>≤</mo> <mn>45</mn> <mo>(</mo>
    <mi>x</mi> <mo>≥</mo> <mn>0</mn> <mo>,</mo> <mi>y</mi> <mo>≥</mo> <mn>0</mn> <mo>)</mo></mrow></math>![emds
    aa06](Images/emds_aa06.png)
  prefs: []
  type: TYPE_NORMAL
- en: Figure A-6\. Graphing the second constraint
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Notice in [Figure A-6](#VnhgsinHGv) that we now have an overlap between these
    two constraints. Our solution is somewhere in that overlap and we will call it
    the *feasible region*. Finally, we are maximizing our profit *Z*, which is expressed
    next, given the profit amounts for the iPac and iPac Ultra, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="upper Z equals 200 x plus 300 y" display="block"><mrow><mi>Z</mi>
    <mo>=</mo> <mn>200</mn> <mi>x</mi> <mo>+</mo> <mn>300</mn> <mi>y</mi></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: If we express this function as a line, we can increase *Z* as much as possible
    until the line is just no longer in the feasible region. We then note the x- and
    y-values as visualized in [Figure A-7](#OPAqHuJwSt).
  prefs: []
  type: TYPE_NORMAL
- en: Desmos Graph of the Objective Function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you need to see this visualized in a more interactive and animated fashion,
    check out [this graph on Desmos](https://oreil.ly/RQMBT).
  prefs: []
  type: TYPE_NORMAL
- en: '![emds aa07](Images/emds_aa07.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure A-7\. Increasing our objective line until it no longer is in the feasible
    region
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: When that line “just touches” the feasible region as you increase profit *Z*
    as much as possible, you will land on a vertex, or a corner, of the feasible region.
    That vertex provides the x- and y-values that will maximize profit, as shown in
    [Figure A-8](#bdonOArOSi).
  prefs: []
  type: TYPE_NORMAL
- en: While we could use NumPy and a bunch of matrix operations to solve this numerically,
    it will be easier to use PuLP as shown in [Example A-11](#MJjcOPJjvs). Note that
    `LpVariable` defines the variables to solve for. `LpProblem` is the linear programming
    system that adds constraints and objective functions using Python operators. Then
    the variables are solved by calling `solve()` on the `LpProblem`.
  prefs: []
  type: TYPE_NORMAL
- en: '![emds aa08](Images/emds_aa08.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure A-8\. Maximized objective for our linear programming system
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Example A-11\. Using Python PuLP to solve a linear programming system
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: You might be wondering if it makes sense to build 5.9375 and 4.6875 units. Linear
    programming systems are much more efficient if you can tolerate continuous values
    in your variables, and perhaps you can just round them afterward. But certain
    types of problems absolutely require integers and binary variables to be handled
    discretely.
  prefs: []
  type: TYPE_NORMAL
- en: To force the `x` and `y` variables to be treated as integers, provide a category
    argument `cat=LpInteger` as shown in [Example A-12](#pqntuvKNRo).
  prefs: []
  type: TYPE_NORMAL
- en: Example A-12\. Forcing variables to be solved as integers
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Graphically, this means we fill our feasible region with discrete points rather
    than a continuous region. Our solution will not land on a vertex necessarily but
    rather the point that is closest to the vertex as shown in [Figure A-9](#TWnQjpTKGu).
  prefs: []
  type: TYPE_NORMAL
- en: There are a couple of special cases in linear programming, as shown in [Figure A-10](#FnSsEsjOEf).
    Sometimes there can be many solutions. At times, there may be no solution at all.
  prefs: []
  type: TYPE_NORMAL
- en: This is just a quick introductory example to linear programming, and unfortunately
    there is not enough room in this book to do the topic justice. It can be used
    for surprising problems, including scheduling constrained resources (like workers,
    server jobs, or rooms), solving Sudokus, and optimizing financial portfolios.
  prefs: []
  type: TYPE_NORMAL
- en: '![emds aa09](Images/emds_aa09.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure A-9\. A discrete linear programming system
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![emds aa10](Images/emds_aa10.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure A-10\. Special cases of linear programming
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If you want to learn more, there are some good YouTube videos out there including
    [PatrickJMT](https://oreil.ly/lqeeR) and [Josh Emmanuel](https://oreil.ly/jAHWc).
    If you want to deep dive into discrete optimization, Professor Pascal Van Hentenryck
    has done a tremendous service [putting a course together on Coursera](https://oreil.ly/aVGxY).
  prefs: []
  type: TYPE_NORMAL
- en: MNIST Classifier Using scikit-learn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Example A-13](#rqjdqTwRJL) shows how to use scikit-learn’s neural network
    for handwritten digit classification.'
  prefs: []
  type: TYPE_NORMAL
- en: Example A-13\. A handwritten digit classifier neural network in scikit-learn
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
