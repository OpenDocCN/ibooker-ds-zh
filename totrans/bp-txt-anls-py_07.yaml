- en: Chapter 7\. How to Explain a Text Classifier
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第七章。如何解释文本分类器
- en: In the previous chapters, we have learned a lot about advanced analytical methods
    for unstructured text data. Starting with statistics and using NLP, we have found
    interesting insights from text.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，我们已经学习了许多关于针对非结构化文本数据的高级分析方法。从统计学开始，使用自然语言处理，我们从文本中找到了有趣的见解。
- en: 'Using supervised methods for classification, we have assigned text documents
    to already-given categories by training algorithms. Although we have checked the
    quality of the classification process, we have skipped an important aspect: we
    have no idea *why* the model has decided to assign a category to a text.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 使用监督方法进行分类，我们通过训练算法将文本文档分配到已知类别。虽然我们已经检查了分类过程的质量，但我们忽略了一个重要的方面：我们不知道模型为什么决定将一个类别分配给一个文本。
- en: This might sound unimportant if the category was correct. However, in daily
    life you often have to *explain* your own decisions and make them *transparent*
    to others. The same is true for machine learning algorithms.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 如果类别是正确的，这可能听起来不重要。然而，在日常生活中，您经常必须*解释*您自己的决定，并使它们对他人*透明*。对于机器学习算法也是如此。
- en: In real-life projects, you will more often than not hear the question “Why has
    the algorithm assigned this category/sentiment?” Even before that, understanding
    how the algorithm has learned something will help you to improve the classification
    by using different algorithms, adding features, changing weights, and so on. Compared
    to structured data, the question is much more important with text as humans can
    interpret the text itself. Moreover, text has many artifacts such as signatures
    in emails that you better avoid and make sure that they are not the dominant features
    in your classification.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实项目中，您很可能经常听到“为什么算法分配了这个类别/情绪？”的问题。甚至在此之前，了解算法是如何学习的将帮助您通过使用不同的算法、添加特征、更改权重等来改进分类。与结构化数据相比，对于文本来说，这个问题更为重要，因为人类可以解释文本本身。此外，文本有许多人为因素，比如电子邮件中的签名，最好避免这些因素，并确保它们不是分类中的主要特征。
- en: In addition to the technological perspective, there are also some legal aspects
    to keep in mind. You might be responsible for proving that your algorithm is not
    biased or does not discriminate. The GDPR in the European Union even demands that
    for algorithms that make decisions (like allowing only certain kinds of payment)
    on public websites.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 除了技术视角之外，还有一些法律方面需要注意。您可能需要证明您的算法没有偏见或不歧视。欧盟的GDPR甚至要求对公共网站上做出决策（比如只允许某种支付方式）的算法进行证明。
- en: Last but not least, trust needs information. If you make your results as transparent
    as possible, you will enormously increase the confidence and trust that somebody
    has in your method.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 最后但同样重要的是，信任需要信息。如果您尽可能地公开您的结果，您将大大增加某人对您的方法的信心和信任。
- en: What You’ll Learn and What We’ll Build
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 你将学到什么，我们将构建什么
- en: In this chapter, we will take a look at several methods for explaining the results
    of a supervised machine learning model. Wherever possible, we will build on classification
    examples that have been part of the previous chapters.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍几种解释监督机器学习模型结果的方法。在可能的情况下，我们将建立在先前章节中的分类示例之上。
- en: We will start by revisiting the classification of the bug reports from [Chapter 6](ch06.xhtml#ch-classification).
    Some reports were classified correctly, some not. We will take a step back and
    analyze whether classification is always a binary decision. For some models, it
    is not, and we will calculate the probabilities of bug reports belonging to a
    certain class and check with the correct values (the so-called *ground* *truth*).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从重新审视[第六章](ch06.xhtml#ch-classification)中的错误报告的分类开始。一些报告被正确分类，一些没有。我们将退后一步，分析分类是否总是二进制决策。对于某些模型来说，它不是，我们将计算错误报告属于某个类别的概率，并与正确值（所谓的*地*
    *实*）进行核对。
- en: In the next section, we will analyze which features were responsible for the
    decision of the model. We can calculate this using support vector machines. We
    will try to interpret the results and see if we can use that knowledge to improve
    the method.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分中，我们将分析哪些特征决定了模型的决策。我们可以使用支持向量机来计算这一点。我们将尝试解释结果，并看看我们是否可以利用这些知识来改进方法。
- en: Afterward, we will take a more general approach and introduce *local interpretable
    model-agnostic explanations* (LIME). LIME is (almost) agnostic to the specific
    machine learning model and can explain the results of many algorithms.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们将采取更一般的方法，并介绍*本地可解释的模型无关解释*（LIME）。LIME（几乎）不依赖于特定的机器学习模型，可以解释许多算法的结果。
- en: People have been researching explainable AI a lot in recent years and came up
    with a more sophisticated model called *Anchor*, which we will present in the
    last part of this chapter.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来人们在研究可解释人工智能方面投入了大量工作，并提出了一种更复杂的模型称为*Anchor*，我们将在本章的最后部分介绍它。
- en: After studying this chapter, you will know different methods for explaining
    the results of supervised machine learning models. You will be able to use this
    for your own projects and decide which of the methods is best suited for your
    specific requirements. You will be able to interpret the results and create intuitive
    visualizations to make them easily understandable for nonexperts.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在学习了本章之后，您将了解到解释监督学习模型结果的不同方法。您将能够将这些方法应用于您自己的项目，并决定哪种方法最适合您的特定需求。您将能够解释结果并创建直观的可视化，以便非专家也能轻松理解。
- en: 'Blueprint: Determining Classification Confidence Using Prediction Probability'
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 蓝图：使用预测概率确定分类置信度
- en: 'You might remember the example from [Chapter 6](ch06.xhtml#ch-classification)
    where we tried to classify the bug reports according to their component. We will
    now train a support vector machine with the optimal parameters found in that chapter.
    The rest of the notation stays the same:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能还记得[第6章](ch06.xhtml#ch-classification)中的例子，我们尝试根据其组件对缺陷报告进行分类。现在我们将使用在该章节中找到的最佳参数来训练支持向量机。其余的符号表示保持不变：
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: If you recall the classification report, we had a good average precision and
    recall of 75%, so the classification worked rather well. But there were some cases
    where the prediction differed from the actual value. We will try to look at the
    results of these predictions in more detail now to understand if there is a pattern
    that we can use to distinguish between “good” and “bad” prediction without taking
    a look at the actual results as those will be unknown in real classification scenarios.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您还记得分类报告，我们的平均精确度和召回率为75%，因此分类效果相当不错。但也有一些情况下预测与实际值不同。现在我们将更详细地查看这些预测结果，以了解是否有可以用来区分“好”和“坏”预测的模式，而不查看实际结果，因为在真实的分类场景中这些将是未知的。
- en: 'For this, we will use the function `predict_proba` of the support vector machine
    model, which tells us about the internals of the SVM, namely, the probabilities
    it calculated for the respective classes (obviously the prediction itself has
    the highest probability).^([1](ch07.xhtml#idm45634192517144)) As a parameter,
    it expects a matrix consisting of document vectors. The result is the probability
    for the different classes. As a first step, we are going to construct a `DataFrame`
    from the prediction results:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们将使用支持向量机模型的`predict_proba`函数，该函数告诉我们SVM的内部情况，即它对各个类别计算的概率（显然，预测本身的概率最高）。^([1](ch07.xhtml#idm45634192517144))
    作为参数，它期望一个由文档向量组成的矩阵。结果是不同类别的概率。作为第一步，我们将从预测结果构建一个`DataFrame`：
- en: '[PRE1]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Let’s try it with one document of the test dataset and assume that we want
    to optimize our classification and are mainly interested in cases where the predictions
    are wrong:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试使用测试数据集的一个文档，并假设我们想优化我们的分类，主要关注预测错误的情况：
- en: '[PRE2]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '`Out:`'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '|   | text | actual | predicted |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '|   | 文本 | 实际 | 预测 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| 2 | NPE in Delta processor while executing JDT/UI ... | Core | UI |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 在执行JDT/UI时Delta处理器中的NPE... | Core | UI |'
- en: '| 15 | Inserting a block of text in editor badly alig... | UI | Text |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| 15 | 在编辑器中插入文本块时排版不佳... | UI | 文本 |'
- en: '| 16 | Differences when debugging identical objects W... | Debug | Core |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| 16 | 在调试相同对象时的差异... | Debug | Core |'
- en: '| 20 | Foreach template doesnt work for class members... | Core | UI |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| 20 | 模板中对类成员使用Foreach不起作用... | Core | UI |'
- en: '| 21 | exchange left and right operands for compariso... | UI | Core |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| 21 | 交换比较运算符的左右操作数... | UI | Core |'
- en: 'Document 21 looks like a good candidate. The predicted class “Core” is wrong,
    but “left” and “right” also sound like UI (which would be correct). Let’s take
    a deeper look at that:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 文档21看起来是一个很好的候选。预测的类“Core”是错误的，但“left”和“right”听起来也像是UI（这将是正确的）。让我们深入研究一下：
- en: '[PRE3]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '`Out:`'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE4]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This looks like a good candidate for a more detailed analysis as it contains
    words that would naively speak for both Core and for UI. Maybe we can understand
    that in more detail if we look at the probabilities. Calculating this is quite
    easy:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '`Out:`'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Remembering that the classes had the order APT, Core, Debug,  Doc, Text, and
    UI, the algorithm was a bit more convinced of Core compared to UI, which would
    have been its second choice.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: 'Is this always the case? We will try to find out and calculate the decision
    probability for all documents in the test dataset and add it to a `DataFrame`:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Let’s take a look at some samples of the data frame and find out whether the
    predictions are better if the algorithm was quite convinced about its decision
    (i.e., the probability for the chosen category was much higher than the others):'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '`Out:`'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: '|   | actual | predicted | APT | Core | Debug | Doc | Text | UI |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
- en: '| 266 | UI | UI | 0.000598 | 0.000929 | 0.000476 | 0.001377 | 0.224473 | 0.772148
    |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
- en: '| 835 | Text | Text | 0.002083 | 0.032109 | 0.001481 | 0.002085 | 0.696666
    | 0.265577 |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
- en: '| 998 | Text | Text | 0.000356 | 0.026525 | 0.003425 | 0.000673 | 0.942136
    | 0.026884 |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
- en: '| 754 | Core | Text | 0.003862 | 0.334308 | 0.011312 | 0.015478 | 0.492112
    | 0.142927 |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
- en: '| 686 | UI | UI | 0.019319 | 0.099088 | 0.143744 | 0.082969 | 0.053174 | 0.601705
    |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
- en: Looking at the table, there is only one wrong prediction (754). In this case,
    the algorithm was quite “unsure” and decided for the category with a probability
    of less than 50%.  Can we find a pattern for this?
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s try to build two `DataFrame`s, one with correct and another with wrong
    predictions. Afterward, we will analyze the distribution of the highest probability
    and see whether we can find any differences:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We will now plot this as a histogram:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '`Out:`'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/btap_07in01.jpg)![](Images/btap_07in02.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
- en: We can see that in the case of correct predictions, the model often decided
    with high probabilities, whereas the probabilities were considerably lower when
    the decision was wrong. As we will see later, the small peak in the wrong category
    with high probability is due to short texts or missing words.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we will take a look at whether we can improve the results if we only
    consider decisions that have been made with a probability of more than 80%:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '`Out:`'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Compare this to the original result, shown here:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '`Out:`'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: We can see that we have considerably improved the precision for predicting the
    components Core, Debug, Text, and UI while at the same time increasing the recall.
    This is great, as the explanation of the SVM has led us to a smaller subset of
    data in which the classifier works better. However, in the components with few
    samples (Apt, Doc), the result has actually only improved the recall. It seems
    that there are just too few samples in these categories, and the algorithm has
    too little information to decide based on the text. In the case of Doc, we just
    removed most of the documents belonging to this class and so increased the recall.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，在预测核心、调试、文本和用户界面组件的精度方面，我们有了显著的改进，同时也提高了召回率。这很棒，因为支持向量机（SVM）的解释使我们进入了一个数据子集，分类器在这里工作得更好。然而，在样本较少的组件（Apt、Doc）中，实际上只改善了召回率。看来这些类别中的样本太少了，算法基于文本的信息也太少，难以作出决策。在
    Doc 的情况下，我们刚刚移除了大部分属于此类的文档，从而提高了召回率。
- en: The improvement came with a price, though. We have excluded more than 900 documents,
    roughly half of the dataset. So, overall, we have actually found fewer documents
    in the smaller dataset! In some projects, it might be useful to let the model
    only decide in cases where it is quite “sure” and discard ambiguous cases (or
    classify them by hand). This often depends on the business requirements.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，改进是有代价的。我们排除了超过 900 份文件，大约是数据集的一半。因此，总体而言，我们在较小的数据集中实际上找到了更少的文档！在某些项目中，让模型仅在“确定”情况下做决策，并且丢弃模棱两可的情况（或手动分类），可能是有用的。这通常取决于业务需求。
- en: In this section, we have found a correlation between the predicting probability
    and the quality of results. However, we have not yet understood how the model
    predicts (i.e., which words are used). We will analyze this in the next section.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分中，我们发现了预测概率与结果质量之间的相关性。但我们尚未理解模型如何预测（即使用哪些单词）。我们将在下一节中进行分析。
- en: 'Blueprint: Measuring Feature Importance of Predictive Models'
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 蓝图：测量预测模型的特征重要性
- en: 'In this section, we want to find out which features were relevant for the model
    to find the correct class. Fortunately, our SVM class can tell us the necessary
    parameters (called *coefficients*):'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们希望找出哪些特征对模型找到正确类别是相关的。幸运的是，我们的 SVM 类可以告诉我们必要的参数（称为*系数*）：
- en: '[PRE15]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '`Out:`'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出：`'
- en: '[PRE16]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 6403 is the size of the vocabulary (check with `len(tfidf.get_feature_names()`),
    but where does the 15 originate from? That is a bit more complicated. Technically,
    the coefficients are organized in a matrix as each class competes against each
    other in a one-to-one way. As we have six classes and classes do not have to compete
    against themselves, there are 15 combinations (the binomial coefficient 6 over
    2). The 15 coefficients are organized as described in [Table 7-1](#tab-coefficients).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 6403 是词汇表的大小（检查`len(tfidf.get_feature_names()`），但是 15 是从哪里来的呢？这有点复杂。从技术上讲，系数组织成一个矩阵，每个类与其他类以一对一的方式竞争。由于我们有六个类别，并且类别不必与自身竞争，因此有
    15 个组合（组合数 6 选 2）。这 15 个系数如表 7-1 所述组织。
- en: Table 7-1\. Coefficient layout for a multiclass SVC classifier
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7-1\. 多类支持向量分类器的系数布局
- en: '|   | APT | Core | Debug | Doc | Text | UI |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '|   | APT | 核心 | 调试 | Doc | 文本 | 用户界面 |'
- en: '| APT |   | 0 | 1 | 2 | 3 | 4 |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| APT |   | 0 | 1 | 2 | 3 | 4 |'
- en: '| Core |   |   | 5 | 6 | 7 | 8 |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| 核心 |   |   | 5 | 6 | 7 | 8 |'
- en: '| Debug |   |   |   | 9 | 10 | 11 |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| 调试 |   |   |   | 9 | 10 | 11 |'
- en: '| Doc |   |   |   |   | 12 | 13 |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| Doc |   |   |   |   | 12 | 13 |'
- en: '| Text |   |   |   |   |   | 14 |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| 文本 |   |   |   |   |   | 14 |'
- en: '| UI |   |   |   |   |   |   |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| 用户界面 |   |   |   |   |   |   |'
- en: Coefficient Structure Depends on Machine Learning Model
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 系数结构取决于机器学习模型
- en: The coefficients might have a completely different organization if you use other
    classifiers. Even for SVM, using a nonlinear model (created by SGDClassifier)
    creates only one coefficient set per class. We will see some examples of this
    when we talk about ELI5.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您使用其他分类器，则系数可能具有完全不同的组织结构。即使对于 SVM，使用 SGDClassifier 创建的非线性模型也每类只有一个系数集。当我们讨论
    ELI5 时，我们将看到一些示例。
- en: 'The rows should be read first, so if we want to find out how the model distinguishes
    APT from Core, we should take index 0 of the coefficients. However, we are more
    interested in the difference of Core and UI, so we take index 8\. In the first
    step, we sort the coefficients by their values and keep the indices, which are
    the vocabulary positions:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Afterward, we now take the top positive and negative contributions:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Then we will aggregate this to a `DataFrame` to make it easier to display the
    results:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We would like to visualize the contributions of the coefficients to make it
    easy to understand. Positive values favor the Core component, and negative values
    prefer UI, as shown in [Figure 7-1](#fig-coefficients-core-ui). To obtain this,
    we use the following:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: These results are quite easy to interpret. The SVM model has nicely learned
    that the words *compiler* and *ast* are specific to the Core component, whereas
    *wizard*, *ui*, and *dialog* are used to identify bugs in the UI component. It
    seems a quick fix is more popular in the UI, which emphasizes the long-term stability
    of the core.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: 'We have just found the features that are important for the whole SVM model
    to choose between Core and UI. But this does not indicate which features are important
    to identify a bug that can be categorized as Core given any bug report. If we
    want to get these features for the Core component and consider the previous matrix,
    we need indices 5, 6, 7, and 8\. With this strategy, we have ignored the difference
    between APT and Core. To take this into account, we need to subtract index 0:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '![](Images/btap_07in03.jpg)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
- en: Figure 7-1\. Word contributions to UI (negative) and Core (positive).
  id: totrans-98
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The rest of the code is almost identical to the previous code. We now extend
    the diagram to 20 words ([Figure 7-2](#fig-coefficients-core)):'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: In the diagram, you can see a lot of words that the model uses to identify the
    Core component and in the lower part those that are used to primarily identify
    other components.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: You can use the methods described in this blueprint to make the results of the
    SVM model transparent and explainable. In many projects, this has proved to be
    valuable as it takes away the “magic” and the subjectivity of machine learning.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: This works quite well, but we do not yet know how sensitive the model is to
    changes in certain words. This is a more complicated question that we will try
    to answer in the next section.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/btap_07in04.jpg)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
- en: Figure 7-2\. Coefficients favoring or opposing the Core component.
  id: totrans-105
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Blueprint: Using LIME to Explain the Classification Results'
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: LIME is an acronym for [“Local Interpretable Model-Agnostic Explanations”](https://oreil.ly/D8cIN)
    and is a popular framework for explainable machine learning. It was conceived
    at the [University of Washington](https://oreil.ly/Q8zly) and is publicly available
    [on GitHub](https://oreil.ly/bErrv).
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take a look at the defining features of LIME. It works *locally* by taking
    a look at each prediction separately. This is achieved by modifying the input
    vector to find the local components that the predictions are sensitive to.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看LIME的定义特征。它通过单独查看每个预测*局部地*工作。通过修改输入向量以找到预测敏感的局部组件来实现这一点。
- en: Explainability Needs Computation Time
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可解释性需要计算时间
- en: Running the explainer code can take considerable time. We tried to tailor the
    examples in a way that you don’t have to wait for more than 10 minutes on normal
    computers. However, by increasing the sample size, this can easily take hours.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 运行解释器代码可能需要相当长的时间。我们尝试通过调整示例的方式，让您在普通计算机上等待时间不超过10分钟。但是，通过增加样本大小，这可能很容易需要几个小时。
- en: From the behavior in the vicinity of the vector, it will draw conclusions about
    which components are more or less important. LIME will visualize the contributions
    and explain the decision mechanism of the algorithm *for individual documents*.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 从向量周围的行为来看，它将得出哪些组件更重要或不重要的结论。LIME将可视化贡献，并解释算法*对个别文档*的决策机制。
- en: LIME does not depend on a specific machine learning model and can be applied
    to a multitude of problems. Not every model qualifies; the model needs to predict
    the probabilities of the categories. Not all support vector machine models can
    do that. In addition, using complicated models where predictions take considerable
    time is not very practical in high-dimensional feature spaces like those common
    in text analytics. As LIME attempts to locally modify the feature vectors, it
    needs to perform a lot of predictions and in this case takes a long time to finish.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: LIME不依赖于特定的机器学习模型，可以应用于多种问题。并非所有模型都符合条件；模型需要预测类别的概率。并非所有支持向量机模型都能做到这一点。此外，在像文本分析中常见的高维特征空间中使用复杂模型进行预测时间较长并不是很实际。由于LIME试图局部修改特征向量，因此需要执行大量预测，在这种情况下需要很长时间才能完成。
- en: Finally, LIME will generate an explanation for the model on a per-sample basis
    and allow you to understand the model. You can use this to improve your model
    but also to explain how a classification works. Although the model will still
    be a black box, you will gain some knowledge of what might be going on in the
    box.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，LIME将根据每个样本生成模型解释，并帮助您理解模型。您可以用它来改进模型，也可以用来解释分类的工作原理。虽然模型仍然是黑箱，但您将获得一些可能发生在箱子里的知识。
- en: 'Let’s get back to the classification problem of the previous section and try
    to find a LIME explanation for a few samples. As LIME wants text as input and
    classification probabilities as output, we arrange the vectorizer and classifier
    in a *pipeline*:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到前一节的分类问题，并尝试为几个样本找到LIME解释。由于LIME需要文本作为输入和分类概率作为输出，我们将向量化器和分类器安排在*管道*中：
- en: '[PRE23]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The pipeline should be able to make predictions if we give it some text, as
    done here:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们给它一些文本，流水线应该能够进行预测，就像这里做的那样：
- en: '[PRE24]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '`Out:`'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出:`'
- en: '[PRE25]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The classifier suggests with very high probability to put this in class 2,
    which is Core. So, our pipeline works exactly in the way we want it: we can give
    it text documents as parameters, and it returns the probabilities for the documents
    belonging to each category. Now it’s time to turn on LIME by first importing the
    package (you might have to install the package first with `pip` or `conda`). Afterward,
    we will create an explainer, which is one of the central elements of LIME and
    is responsible for explaining individual predictions:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 分类器建议将此置于类别2中的概率非常高，即核心。因此，我们的流水线正按照我们希望的方式运行：我们可以将文本文档作为参数传递给它，并返回文档属于每个类别的概率。现在是打开LIME的时候了，首先导入该包（您可能需要使用`pip`或`conda`先安装该包）。之后，我们将创建一个解释器，这是LIME的核心元素之一，负责解释单个预测：
- en: '[PRE26]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We check the `DataFrame` for classes that have been wrongly predicted in the
    following:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们检查`DataFrame`中错误预测的类别如下：
- en: '[PRE27]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '`Out:`'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出:`'
- en: '|   | index | text | actual | predicted | APT | Core | Debug | Doc | Text |
    UI |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '|   | 索引 | 文本 | 实际 | 预测 | APT | 核心 | 调试 | 文档 | 文本 | UI |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| 2 | 2 | NPE in Delta processor while executing JDT/UI ... | Core | UI | 0.003357
    | 0.309548 | 0.046491 | 0.002031 | 0.012309 | 0.626265 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 2 | Delta处理器中的NPE执行JDT/UI ... | 核心 | UI | 0.003357 | 0.309548 | 0.046491
    | 0.002031 | 0.012309 | 0.626265 |'
- en: '| 15 | 15 | Inserting a block of text in editor badly alig... | UI | Text |
    0.001576 | 0.063076 | 0.034610 | 0.003907 | 0.614473 | 0.282356 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| 15 | 15 | 在编辑器中插入文本块严重对齐不良... | UI | 文本 | 0.001576 | 0.063076 | 0.034610
    | 0.003907 | 0.614473 | 0.282356 |'
- en: '| 16 | 16 | Differences when debugging identical objects W... | Debug | Core
    | 0.002677 | 0.430862 | 0.313465 | 0.004193 | 0.055838 | 0.192965 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| 16 | 16 | 调试相同对象时的差异 W... | 调试 | 核心 | 0.002677 | 0.430862 | 0.313465 | 0.004193
    | 0.055838 | 0.192965 |'
- en: '| 20 | 20 | Foreach template doesnt work for class members... | Core | UI |
    0.000880 | 0.044018 | 0.001019 | 0.000783 | 0.130766 | 0.822535 |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| 20 | 20 | 模板的 foreach 对类成员不起作用... | 核心 | UI | 0.000880 | 0.044018 | 0.001019
    | 0.000783 | 0.130766 | 0.822535 |'
- en: '| 21 | 21 | exchange left and right operands for compariso... | UI | Core |
    0.002669 | 0.467366 | 0.077252 | 0.003194 | 0.068749 | 0.380770 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| 21 | 21 | 交换比较中左右操作数... | UI | 核心 | 0.002669 | 0.467366 | 0.077252 | 0.003194
    | 0.068749 | 0.380770 |'
- en: 'Take a look at the corresponding record (row 21 in our case):'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 看看相应记录（我们的情况下是第 21 行）：
- en: '[PRE28]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '`Out:`'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE29]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Now it’s time for LIME to explain this to us!
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是 LIME 向我们解释的时候了！
- en: '[PRE30]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '`Out:`'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE31]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: LIME shows us which words it thinks are in favor (positive) or against (negative)
    a certain class. This is quite easy and similar to what we have achieved in the
    SVM example. Even better, now it’s independent of the model itself; it just needs
    to support `predict_proba` (which is also true for Random Forest and so on).
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: LIME 展示了哪些词语它认为对某个类别有利（正面）或不利（负面）。这与我们在 SVM 示例中实现的情况非常相似。更好的是，现在它独立于模型本身；它只需要支持`predict_proba`（这也适用于随机森林等）。
- en: 'With LIME, you can extend the analysis to more classes and create a graphics
    representation of their specific words:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 LIME，您可以将分析扩展到更多类别，并创建它们特定词语的图形表示：
- en: '[PRE32]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '`Out:`'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '![](Images/btap_07_svgcombo_1.jpg)![](Images/btap_07_svgcombo_2.jpg)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/btap_07_svgcombo_1.jpg)![](Images/btap_07_svgcombo_2.jpg)'
- en: This looks intuitive and much more suitable for interpretation and even inclusion
    in a presentation. We can clearly see that *fix* and *right* are crucial for assigning
    the UI class and at the same time against Core. Bug, however, speaks for Core,
    as do *comparison* and *semantics*. Unfortunately, this is not what a human would
    accept as rules for classification; they seem too specific, and there is no abstraction.
    In other words, our model looks *overfitted*.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 这看起来很直观，更适合解释甚至包含在演示中。我们可以清楚地看到*fix* 和 *right* 对于分配 UI 类别至关重要，同时反对核心。然而，*Bug*
    表示核心，正如*comparison* 和 *semantics* 一样。不幸的是，这不是人类接受作为分类规则的样子；它们似乎过于具体，没有抽象化。换句话说，我们的模型看起来*过拟合*。
- en: Improving Models
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 改进模型
- en: With this knowledge and the expertise of people familiar with the tickets, you
    could improve the model. We could, for example, ask if *Bug* is really specific
    to Core or if we’d better make it a stop word. It might also prove useful to convert
    everything to lowercase.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些知识和熟悉票务的专家的经验，您可以改进模型。例如，我们可以询问*Bug* 是否真的特定于核心，或者我们最好将其作为停用词。把所有内容转换为小写可能也会证明有用。
- en: 'LIME can even support you in finding representative samples that help you interpret
    the model performance as a whole. The feature is called *submodular picks* and
    works like this:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: LIME 甚至可以帮助您找到有助于全面解释模型性能的代表性样本。这个功能被称为*子模块挑选*，工作原理如下：
- en: '[PRE33]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The individual “picks” can be visualized as shown previously in the notebook
    and are even more complete now with highlighting. We show only the first of the
    picks here:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 个别“挑选”可以像之前笔记本中显示的那样进行可视化，并且现在更加完整，带有高亮显示。我们在这里只展示了第一个挑选：
- en: '[PRE34]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '`Out:`'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '![](Images/btap_07_svgcombo_3.jpg)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/btap_07_svgcombo_3.jpg)'
- en: In the following case, we can interpret the results, but it does not look like
    the model learned the abstraction, which is again a sign of *overfitting*.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下情况下，我们可以解释结果，但它似乎并没有学习抽象化，这又是*过拟合*的迹象。
- en: '![](Images/btap_07_highlightwords.jpg)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/btap_07_highlightwords.jpg)'
- en: The LIME software module works for linear support vector machines in scikit-learn
    but not for those with more complex kernels. The graphical presentation is nice
    but is not directly suitable for presentations. Therefore, we will take a look
    at ELI5, which is an alternative implementation and tries to overcome these problems.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: LIME 软件模块适用于 scikit-learn 中的线性支持向量机，但不适用于具有更复杂内核的支持向量机。图形化展示很好，但不直接适合演示。因此，我们将看看
    ELI5，这是一种替代实现，试图克服这些问题。
- en: 'Blueprint: Using ELI5 to Explain the Classification Results'
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 蓝图：使用 ELI5 解释分类结果
- en: ELI5 (“Explain it to me like I’m 5”) is another popular software library for
    machine learning explanation also using the LIME algorithm. As it can be used
    for nonlinear SVMs and has a different API, we will take a short look at it and
    show how to use it in our case.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: ELI5（“Explain it to me like I’m 5”）是另一个流行的机器学习解释软件库，也使用LIME算法。由于它可用于非线性SVM，并且具有不同的API，我们将简要介绍它，并展示如何在我们的案例中使用它。
- en: 'ELI5 needs a model that has been trained with `libsvm`, which our SVC model
    from before unfortunately is not. Luckily, training an SVM is really fast, so
    we can create a new classifier with the same data, but with a `libsvm`-based model,
    and check its performance. You might remember the classification report from [Chapter 6](ch06.xhtml#ch-classification),
    which gives a good summary about the quality of the model:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: ELI5需要一个使用`libsvm`训练过的模型，而我们之前的SVC模型不幸不是这样的。幸运的是，训练SVM非常快速，因此我们可以用相同的数据创建一个新的分类器，但使用基于`libsvm`的模型，并检查其性能。你可能还记得[第6章](ch06.xhtml#ch-classification)中的分类报告，它提供了模型质量的良好总结：
- en: '[PRE35]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '`Out:`'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE36]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Taking a look at the last line, this is roughly as good as what we have achieved
    with SVC. Thus, it makes sense to explain it! Using ELI5, finding explanations
    for this model is easy:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 看看最后一行，这大致与我们使用SVC取得的效果一样好。因此，解释它是有意义的！使用ELI5，找到这个模型的解释是很容易的：
- en: '[PRE37]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '![](Images/btap_07_eli5_1.jpg)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/btap_07_eli5_1.jpg)'
- en: 'The positive features (i.e., words) are shown in green. More intense shades
    of green mean a larger contribution of the word to the corresponding class. The
    red colors work exactly opposite: words appearing in red “repel” the classes (for
    example, “refactoring” in the lower part of the second row strongly rejects class
    `Core`). `<BIAS>` is a special case and contains the so-called *intercept*, i.e.,
    systematic failures of the model.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 正面特征（即词汇）显示为绿色。更浓烈的绿色意味着该词对应类别的贡献更大。红色则完全相反：出现在红色中的词汇会“排斥”类别（例如，第二行下部的“refactoring”强烈排斥`Core`类）。`<BIAS>`则是一个特例，包含所谓的*截距*，即模型的系统性失败。
- en: As you can see, we now get weights for the individual classes. This is due to
    the non-linear SVM model working differently in multiclass scenarios compared
    to SVC. Each class is “scored” on its own, and there is no competition. At first
    sight, the words look very plausible.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，我们现在为各个类别获得了权重。这是由于非线性SVM模型在多类场景下与SVC不同的工作方式。每个类别都“打分”，没有竞争。乍一看，这些词看起来非常合理。
- en: 'ELI5 can also explain individual observations:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: ELI5还可以解释单个观察结果：
- en: '[PRE38]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '![](Images/btap_07_eli5_2.jpg)![](Images/btap_07_eli5_3.jpg)![](Images/btap_07_eli5_4.jpg)![](Images/btap_07_eli5_5.jpg)![](Images/btap_07_eli5_6.jpg)![](Images/btap_07_eli5_7.jpg)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/btap_07_eli5_2.jpg)![](Images/btap_07_eli5_3.jpg)![](Images/btap_07_eli5_4.jpg)![](Images/btap_07_eli5_5.jpg)![](Images/btap_07_eli5_6.jpg)![](Images/btap_07_eli5_7.jpg)'
- en: This is a nice visualization for understanding which words contribute to the
    algorithm deciding the categories. Compared to the original LIME package, with
    ELI5 you need considerably less code, and you can use ELI5 for nonlinear SVM models.
    Depending on your classifier and use case, you might decide on LIME or ELI5\.
    Due to the same method, the results should be comparable (if not identical).
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个很好的可视化工具，用于理解哪些词汇对算法决定类别具有贡献。与原始的LIME软件包相比，使用ELI5需要的代码要少得多，你可以将ELI5用于非线性SVM模型。根据你的分类器和使用情况，你可能会选择LIME或ELI5。由于使用了相同的方法，结果应该是可比较的（如果不是相同的）。
- en: Work in Progress
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作正在进行中
- en: ELI5 is still under heavy development, and you might experience difficulties
    with new scikit-learn versions. We have used ELI5 version 0.10.1 in this chapter.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: ELI5仍在积极开发中，您可能会在新版本的scikit-learn中遇到困难。在本章中，我们使用了ELI5版本0.10.1。
- en: ELI5 is an easy-to-use software library for understanding and visualizing the
    decision logic of classifiers, but it also suffers from the shortcomings of the
    underlying LIME algorithm, such as explainability by example only. To make the
    black-box classification more transparent, it would be insightful to gain access
    to the “rules” that a model uses. That was the motivation for the group at Washington
    University to create a follow-up project called Anchor.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: ELI5是一个易于使用的软件库，用于理解和可视化分类器的决策逻辑，但它也受到底层LIME算法的缺点的影响，例如只能通过示例来解释的可解释性。为了使黑盒分类更透明，获得模型使用的“规则”将是有见地的。这是华盛顿大学团队创建后续项目Anchor的动机。
- en: 'Blueprint: Using Anchor to Explain the Classification Results'
  id: totrans-175
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 蓝图：使用Anchor解释分类结果
- en: Like LIME, [Anchor](https://oreil.ly/qSDMl) is model agnostic and works for
    any black-box model. As a tool for explanations, it creates rules, the so-called
    *anchors*, which explain the behavior of the model. Reading these rules, you will
    not only be able to explain a prediction of the model but also predict in the
    same way as the model has learned to.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于LIME，[Anchor](https://oreil.ly/qSDMl)与任何黑盒模型都兼容。作为解释工具，它创建了规则，即所谓的*锚点*，用于解释模型的行为。阅读这些规则，你不仅能够解释模型的预测，还能以与模型学习相同的方式进行预测。
- en: Compared to LIME, Anchor has considerable advantages for better explaining the
    models with the rules. However, the software itself is quite new and still a work
    in progress. Not all examples were working for us, so we chose a selection of
    methods that help in interpreting the classification model.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 相较于LIME，Anchor在通过规则更好地解释模型方面具有显著优势。然而，软件本身还是比较新的，仍在不断完善中。并非所有示例对我们都适用，因此我们选择了一些有助于解释分类模型的方法。
- en: Using the Distribution with Masked Words
  id: totrans-178
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用带有屏蔽词的分布
- en: There are different ways Anchor can be used. We start with the so-called *unknown*
    distribution. Anchor will explain how a model makes a decision by replacing existing
    tokens that are supposed to be unimportant for the prediction with the word *unknown*.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: Anchor有多种使用方式。我们从所谓的*未知*分布开始。Anchor将通过用词汇*unknown*替换预测中被认为不重要的现有标记，解释模型的决策方式。
- en: Again, we will use the document with an ID of 21\. In this case, the classifier
    has the difficult task of choosing between two categories that have roughly the
    same probability. This should make it an interesting example for studying.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们将使用ID为21的文档。在这种情况下，分类器需要在两个概率大致相同的类别之间进行选择，这对研究是一个有趣的示例。
- en: To create (semantic) variance in the text, Anchor uses spaCy’s word vectors
    and needs a spaCy model that includes these vectors, like `en_core_web_lg`.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在文本中创建（语义）差异，Anchor使用spaCy的词向量，并需要包含这些向量的spaCy模型，例如`en_core_web_lg`。
- en: 'As a prerequisite, you should therefore install `anchor-exp` and `spacy` (using
    either `conda` or `pip`) and load the model with the following:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，作为先决条件，您应该安装`anchor-exp`和`spacy`（使用`conda`或`pip`），并加载以下模型：
- en: '[PRE39]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'In the first step, we can then instantiate our explainer. The explainer has
    some probabilistic elements, so it’s better to restart the random-number generator
    at the same time:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一步中，我们可以实例化我们的解释器。解释器具有一些概率元素，因此最好同时重新启动随机数生成器：
- en: '[PRE40]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Let’s check the predicted results and alternatives and compare them to the
    ground truth. `predicted_class_ids` contains the indices of the predicted classes
    with decreasing probability, so the element 0 is the prediction, and element 1
    is its closest competitor:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查预测结果及其替代方案，并将其与真实情况进行比较。`predicted_class_ids`包含预测类的索引，按概率降序排列，因此元素0是预测值，元素1是其最接近的竞争者：
- en: '[PRE41]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '`Out:`'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE42]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'In the next step, we will let the algorithm find the rules for the predictions.
    The parameters are the same as for LIME earlier:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一步中，我们将让算法找出预测的规则。参数与之前的LIME相同：
- en: '[PRE43]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: The calculation can take up to 60 minutes depending on the speed of your CPU.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 计算时间取决于CPU的速度，可能需要高达60分钟。
- en: 'Everything is now contained in the explainer, so we can query the explainer
    to find out about the inner workings of the model:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 现在一切都包含在解释器中，因此我们可以查询解释器，了解模型的内部工作情况：
- en: '[PRE44]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '`Out:`'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE45]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'So, the rule tells us that an occurrence of the words *following* and *comparison*
    combined with *Bug* and *semantic* leads to a prediction of “Core” with more than
    98% precision, which is unfortunately wrong. We can now also find typical examples
    that the model would classify as Core:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，规则告诉我们，单词*following*和*comparison*与*Bug*和*semantic*的组合会导致“Core”预测，精度超过98%，但不幸的是这是错误的。现在，我们还可以找到模型将其分类为Core的典型示例：
- en: '[PRE46]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'The UNK token shown next stands for “unknown” and means that the word at the
    corresponding position is not important:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 下面显示的UNK标记代表“未知”，意味着对应位置的词汇不重要：
- en: '[PRE47]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'We can also ask for examples where the rule matches but the model predicts
    the wrong class:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以要求提供符合规则但模型预测错误类的示例：
- en: '[PRE48]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '`Out:`'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE49]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: To be honest, this is not a good result for the model. We would have expected
    that the underlying rules learned by the models would be sensitive to words specific
    to the different components. However, there is no obvious reason why *following*
    and *Bug* would be specific to Core. More or less these are generic words that
    are not very characteristic of either of the categories.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 老实说，这对模型来说并不是一个好结果。我们本来期望模型学习的底层规则会对不同组件特定的单词比较敏感。然而，并没有明显的理由可以解释为什么*following*和*Bug*会特定于核心。这些都是一些通用词汇，不太具有任何类别的特征。
- en: The UNK tokens are a bit misleading. Even if they are not important in this
    sample, they might be replaced by other, realistic words that would influence
    the decision of the algorithm. Anchor can also help us illustrate that.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: UNK令牌有点误导。即使它们在这个样本中并不重要，它们可能被其他真实的单词替换，这些单词会影响算法的决策。Anchor也可以帮助我们说明这一点。
- en: Working with Real Words
  id: totrans-207
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用真实词语进行工作
- en: 'By substituting `use_unk_distribution=False` in the original constructor of
    the explainer, we can tell Anchor to use real words (similar to the one it is
    substituting by using the word vectors from spaCy) and observe the behavior of
    the model:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在解释器的原始构造函数中替换`use_unk_distribution=False`，我们可以告诉Anchor使用真实词语（类似于使用spaCy的词向量替换）并观察模型的行为：
- en: '[PRE50]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '`Out:`'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出：`'
- en: '[PRE51]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: The rules are a bit different from the earlier unknown distribution. It seems
    that some of the words have become a bit more specific for the Core, like *left*
    and *right*, whereas other words like *for* have vanished.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 这些规则与之前未知的分布有些不同。似乎有些单词变得更加特定于核心，如*left*和*right*，而其他词语如*for*则消失了。
- en: 'Let’s also ask Anchor to generate alternative texts that would also be (wrongly)
    classified as Core as the previous rule applies:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们还让Anchor生成一些替代文本，这些文本也会（错误地）被分类为核心，因为前面的规则也适用：
- en: '[PRE52]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Some words have changed and have not affected the result of the classification.
    In some cases, it is only prepositions, and normally this should not have an effect
    on the results. However, *operators* can also be replaced by *dispatchers* without
    affecting the results. Anchor shows you that it is stable against these modifications.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 一些单词已经改变，并且并没有影响分类结果。在某些情况下，只是介词，通常情况下这不会影响结果。然而，*operators*也可以被*dispatchers*替换而不会影响结果。Anchor向您展示它对这些修改是稳定的。
- en: 'Compare the previous results to those where the model would (correctly) predict
    “UI.” Again, the difference affects single words like *changes*, *metaphors*,
    and so on, which definitely carry more meaning than the smaller modifications
    in the previous example, but it is highly unlikely that you as a human would interpret
    these words as signals for a different category:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 将先前的结果与模型正确预测“UI”的结果进行比较。同样，这种差异影响单词如*changes*、*metaphors*等，这些单词显然比前一个例子中的较小修改更具意义，但很难想象你作为一个人类会将这些词解释为不同类别的信号：
- en: '[PRE53]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: Anchor also has an intuitive way of showing the results with the important words
    highlighted in the notebook and also includes the rules it has calculated:^([2](ch07.xhtml#idm45634189444808))
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: Anchor还有一种直观的方式在笔记本中显示结果，重要的单词会被突出显示，同时还包括它计算出的规则：^([2](ch07.xhtml#idm45634189444808))
- en: '[PRE54]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: '`Out:`'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出：`'
- en: '![](Images/btap_07_highlightwords2.jpg)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/btap_07_highlightwords2.jpg)'
- en: As it’s quite likely that you are also familiar with software development, it
    would be hard to determine the correct category from the rules alone. In other
    words, this means the model seems to be quite fragile when trained with the corpus.
    The “correct” category can probably be determined only by a project contributor
    who has a lot of context knowledge (which we will revisit later in [Chapter 11](ch11.xhtml#ch-sentiment)).
    So, finding that a classifier works does not necessarily mean that it has really
    learned in a way that is transparent for us.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 由于你很可能也熟悉软件开发，单靠规则很难确定正确的类别。换句话说，这意味着当模型使用语料库训练时，模型似乎是相当脆弱的。只有那些具有大量背景知识的项目贡献者才能可能真正确定“正确”的类别（我们稍后将在[第11章](ch11.xhtml#ch-sentiment)回顾）。因此，发现分类器有效并不一定意味着它真正学习的方式对我们是透明的。
- en: To summarize this section, Anchor is quite interesting. The authors of Anchor
    did not choose version number 0.0.1 by chance; the program is still in its infancy.
    During our experiments, we have seen quite a few quirks, and to make it work in
    production, a lot of things have to be improved. Conceptually, however, it is
    already really convincing for explaining single predictions and making models
    transparent. The calculated rules especially are almost unique and cannot be created
    by any other solution.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 总结本节，Anchor 很有趣。Anchor 的作者选择版本号 0.0.1 并非偶然；该程序仍处于起步阶段。在我们的实验中，我们看到了一些小问题，要使其在生产环境中运行，还需要改进许多事情。但从概念上来说，它已经非常令人信服，可以用于解释单个预测并使模型透明化。特别是计算出的规则几乎是独一无二的，任何其他解决方案都无法创建。
- en: Closing Remarks
  id: totrans-224
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结语
- en: Using the techniques presented in this chapter will help make your model predictions
    more transparent.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 使用本章介绍的技术将有助于使您的模型预测更加透明。
- en: From a technical perspective, this transparency can be a great help as it supports
    you in choosing among competing models or improving your feature models. The techniques
    presented in this chapter give you insights into the “inner workings” of a model
    and help to detect and improve untrustworthy models.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 从技术角度来看，这种透明性可以极大地帮助您在选择竞争模型或改进特征模型时提供支持。本章介绍的技术能够深入了解模型的“内部运作”，有助于检测和改进不可信的模型。
- en: Considering the business perspective, explainability is a great selling proposition
    for projects. It is much easier to talk about models and present them if you don’t
    exclusively pursue the black-box model but rather make your models transparent.
    Recent articles in [Forbes](https://oreil.ly/Xcfjx) and [VentureBeat](https://oreil.ly/SIa-R)
    have focused on this interesting development. Being able to “trust” a model will
    be more and more important when you want to build trustable machine learning solutions.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 从商业角度来看，可解释性对项目来说是一个很好的销售主张。如果不只是追求黑盒模型，而是使模型透明化，那么在谈论模型并展示它们时会更容易。最近的文章在[福布斯](https://oreil.ly/Xcfjx)和[VentureBeat](https://oreil.ly/SIa-R)上已经专注于这一有趣的发展。当您想要构建可信的机器学习解决方案时，“信任”模型将变得越来越重要。
- en: Explainable AI is a young field. We can expect to see tremendous progress, better
    algorithms, and improved tooling in the future.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 可解释人工智能是一个年轻的领域。我们可以预期未来会看到巨大的进步，更好的算法和改进的工具。
- en: For most of the book, machine learning methods have worked nicely as black-box
    models. This is fine, as long as the results are consistent and we don’t have
    to justify the models. If either is challenged, as is becoming more common, then
    the time for explainable AI has arrived.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数情况下，机器学习方法都很好地作为黑盒模型运行。只要结果一致，我们就不需要为模型辩护，这样也挺好。但如果其中任何一个受到质疑（这种情况变得越来越普遍），那么可解释人工智能的时代就已经到来了。
- en: ^([1](ch07.xhtml#idm45634192517144-marker)) Graphically, you can think of the
    probabilities as the distance of the samples to the hyperplane defined by the
    SVM.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch07.xhtml#idm45634192517144-marker)) 从图形上来看，您可以将这些概率视为样本到由 SVM 定义的超平面的距离。
- en: ^([2](ch07.xhtml#idm45634189444808-marker)) We had a hard time getting this
    to work, as it was suited only for numerical categories. We plan to make some
    pull requests to get the upstream working for textual categories as well.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch07.xhtml#idm45634189444808-marker)) 我们在让此工作起来时遇到了一些困难，因为它只适用于数值类别。我们计划提交一些拉取请求，以使上游也适用于文本类别。
