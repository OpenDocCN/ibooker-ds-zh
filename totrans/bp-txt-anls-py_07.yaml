- en: Chapter 7\. How to Explain a Text Classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapters, we have learned a lot about advanced analytical methods
    for unstructured text data. Starting with statistics and using NLP, we have found
    interesting insights from text.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using supervised methods for classification, we have assigned text documents
    to already-given categories by training algorithms. Although we have checked the
    quality of the classification process, we have skipped an important aspect: we
    have no idea *why* the model has decided to assign a category to a text.'
  prefs: []
  type: TYPE_NORMAL
- en: This might sound unimportant if the category was correct. However, in daily
    life you often have to *explain* your own decisions and make them *transparent*
    to others. The same is true for machine learning algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: In real-life projects, you will more often than not hear the question “Why has
    the algorithm assigned this category/sentiment?” Even before that, understanding
    how the algorithm has learned something will help you to improve the classification
    by using different algorithms, adding features, changing weights, and so on. Compared
    to structured data, the question is much more important with text as humans can
    interpret the text itself. Moreover, text has many artifacts such as signatures
    in emails that you better avoid and make sure that they are not the dominant features
    in your classification.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to the technological perspective, there are also some legal aspects
    to keep in mind. You might be responsible for proving that your algorithm is not
    biased or does not discriminate. The GDPR in the European Union even demands that
    for algorithms that make decisions (like allowing only certain kinds of payment)
    on public websites.
  prefs: []
  type: TYPE_NORMAL
- en: Last but not least, trust needs information. If you make your results as transparent
    as possible, you will enormously increase the confidence and trust that somebody
    has in your method.
  prefs: []
  type: TYPE_NORMAL
- en: What You’ll Learn and What We’ll Build
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will take a look at several methods for explaining the results
    of a supervised machine learning model. Wherever possible, we will build on classification
    examples that have been part of the previous chapters.
  prefs: []
  type: TYPE_NORMAL
- en: We will start by revisiting the classification of the bug reports from [Chapter 6](ch06.xhtml#ch-classification).
    Some reports were classified correctly, some not. We will take a step back and
    analyze whether classification is always a binary decision. For some models, it
    is not, and we will calculate the probabilities of bug reports belonging to a
    certain class and check with the correct values (the so-called *ground* *truth*).
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will analyze which features were responsible for the
    decision of the model. We can calculate this using support vector machines. We
    will try to interpret the results and see if we can use that knowledge to improve
    the method.
  prefs: []
  type: TYPE_NORMAL
- en: Afterward, we will take a more general approach and introduce *local interpretable
    model-agnostic explanations* (LIME). LIME is (almost) agnostic to the specific
    machine learning model and can explain the results of many algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: People have been researching explainable AI a lot in recent years and came up
    with a more sophisticated model called *Anchor*, which we will present in the
    last part of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: After studying this chapter, you will know different methods for explaining
    the results of supervised machine learning models. You will be able to use this
    for your own projects and decide which of the methods is best suited for your
    specific requirements. You will be able to interpret the results and create intuitive
    visualizations to make them easily understandable for nonexperts.
  prefs: []
  type: TYPE_NORMAL
- en: 'Blueprint: Determining Classification Confidence Using Prediction Probability'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You might remember the example from [Chapter 6](ch06.xhtml#ch-classification)
    where we tried to classify the bug reports according to their component. We will
    now train a support vector machine with the optimal parameters found in that chapter.
    The rest of the notation stays the same:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: If you recall the classification report, we had a good average precision and
    recall of 75%, so the classification worked rather well. But there were some cases
    where the prediction differed from the actual value. We will try to look at the
    results of these predictions in more detail now to understand if there is a pattern
    that we can use to distinguish between “good” and “bad” prediction without taking
    a look at the actual results as those will be unknown in real classification scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this, we will use the function `predict_proba` of the support vector machine
    model, which tells us about the internals of the SVM, namely, the probabilities
    it calculated for the respective classes (obviously the prediction itself has
    the highest probability).^([1](ch07.xhtml#idm45634192517144)) As a parameter,
    it expects a matrix consisting of document vectors. The result is the probability
    for the different classes. As a first step, we are going to construct a `DataFrame`
    from the prediction results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s try it with one document of the test dataset and assume that we want
    to optimize our classification and are mainly interested in cases where the predictions
    are wrong:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '|   | text | actual | predicted |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | NPE in Delta processor while executing JDT/UI ... | Core | UI |'
  prefs: []
  type: TYPE_TB
- en: '| 15 | Inserting a block of text in editor badly alig... | UI | Text |'
  prefs: []
  type: TYPE_TB
- en: '| 16 | Differences when debugging identical objects W... | Debug | Core |'
  prefs: []
  type: TYPE_TB
- en: '| 20 | Foreach template doesnt work for class members... | Core | UI |'
  prefs: []
  type: TYPE_TB
- en: '| 21 | exchange left and right operands for compariso... | UI | Core |'
  prefs: []
  type: TYPE_TB
- en: 'Document 21 looks like a good candidate. The predicted class “Core” is wrong,
    but “left” and “right” also sound like UI (which would be correct). Let’s take
    a deeper look at that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'This looks like a good candidate for a more detailed analysis as it contains
    words that would naively speak for both Core and for UI. Maybe we can understand
    that in more detail if we look at the probabilities. Calculating this is quite
    easy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Remembering that the classes had the order APT, Core, Debug,  Doc, Text, and
    UI, the algorithm was a bit more convinced of Core compared to UI, which would
    have been its second choice.
  prefs: []
  type: TYPE_NORMAL
- en: 'Is this always the case? We will try to find out and calculate the decision
    probability for all documents in the test dataset and add it to a `DataFrame`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s take a look at some samples of the data frame and find out whether the
    predictions are better if the algorithm was quite convinced about its decision
    (i.e., the probability for the chosen category was much higher than the others):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '|   | actual | predicted | APT | Core | Debug | Doc | Text | UI |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 266 | UI | UI | 0.000598 | 0.000929 | 0.000476 | 0.001377 | 0.224473 | 0.772148
    |'
  prefs: []
  type: TYPE_TB
- en: '| 835 | Text | Text | 0.002083 | 0.032109 | 0.001481 | 0.002085 | 0.696666
    | 0.265577 |'
  prefs: []
  type: TYPE_TB
- en: '| 998 | Text | Text | 0.000356 | 0.026525 | 0.003425 | 0.000673 | 0.942136
    | 0.026884 |'
  prefs: []
  type: TYPE_TB
- en: '| 754 | Core | Text | 0.003862 | 0.334308 | 0.011312 | 0.015478 | 0.492112
    | 0.142927 |'
  prefs: []
  type: TYPE_TB
- en: '| 686 | UI | UI | 0.019319 | 0.099088 | 0.143744 | 0.082969 | 0.053174 | 0.601705
    |'
  prefs: []
  type: TYPE_TB
- en: Looking at the table, there is only one wrong prediction (754). In this case,
    the algorithm was quite “unsure” and decided for the category with a probability
    of less than 50%.  Can we find a pattern for this?
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s try to build two `DataFrame`s, one with correct and another with wrong
    predictions. Afterward, we will analyze the distribution of the highest probability
    and see whether we can find any differences:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We will now plot this as a histogram:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/btap_07in01.jpg)![](Images/btap_07in02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We can see that in the case of correct predictions, the model often decided
    with high probabilities, whereas the probabilities were considerably lower when
    the decision was wrong. As we will see later, the small peak in the wrong category
    with high probability is due to short texts or missing words.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we will take a look at whether we can improve the results if we only
    consider decisions that have been made with a probability of more than 80%:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Compare this to the original result, shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: We can see that we have considerably improved the precision for predicting the
    components Core, Debug, Text, and UI while at the same time increasing the recall.
    This is great, as the explanation of the SVM has led us to a smaller subset of
    data in which the classifier works better. However, in the components with few
    samples (Apt, Doc), the result has actually only improved the recall. It seems
    that there are just too few samples in these categories, and the algorithm has
    too little information to decide based on the text. In the case of Doc, we just
    removed most of the documents belonging to this class and so increased the recall.
  prefs: []
  type: TYPE_NORMAL
- en: The improvement came with a price, though. We have excluded more than 900 documents,
    roughly half of the dataset. So, overall, we have actually found fewer documents
    in the smaller dataset! In some projects, it might be useful to let the model
    only decide in cases where it is quite “sure” and discard ambiguous cases (or
    classify them by hand). This often depends on the business requirements.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we have found a correlation between the predicting probability
    and the quality of results. However, we have not yet understood how the model
    predicts (i.e., which words are used). We will analyze this in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: 'Blueprint: Measuring Feature Importance of Predictive Models'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we want to find out which features were relevant for the model
    to find the correct class. Fortunately, our SVM class can tell us the necessary
    parameters (called *coefficients*):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 6403 is the size of the vocabulary (check with `len(tfidf.get_feature_names()`),
    but where does the 15 originate from? That is a bit more complicated. Technically,
    the coefficients are organized in a matrix as each class competes against each
    other in a one-to-one way. As we have six classes and classes do not have to compete
    against themselves, there are 15 combinations (the binomial coefficient 6 over
    2). The 15 coefficients are organized as described in [Table 7-1](#tab-coefficients).
  prefs: []
  type: TYPE_NORMAL
- en: Table 7-1\. Coefficient layout for a multiclass SVC classifier
  prefs: []
  type: TYPE_NORMAL
- en: '|   | APT | Core | Debug | Doc | Text | UI |'
  prefs: []
  type: TYPE_TB
- en: '| APT |   | 0 | 1 | 2 | 3 | 4 |'
  prefs: []
  type: TYPE_TB
- en: '| Core |   |   | 5 | 6 | 7 | 8 |'
  prefs: []
  type: TYPE_TB
- en: '| Debug |   |   |   | 9 | 10 | 11 |'
  prefs: []
  type: TYPE_TB
- en: '| Doc |   |   |   |   | 12 | 13 |'
  prefs: []
  type: TYPE_TB
- en: '| Text |   |   |   |   |   | 14 |'
  prefs: []
  type: TYPE_TB
- en: '| UI |   |   |   |   |   |   |'
  prefs: []
  type: TYPE_TB
- en: Coefficient Structure Depends on Machine Learning Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The coefficients might have a completely different organization if you use other
    classifiers. Even for SVM, using a nonlinear model (created by SGDClassifier)
    creates only one coefficient set per class. We will see some examples of this
    when we talk about ELI5.
  prefs: []
  type: TYPE_NORMAL
- en: 'The rows should be read first, so if we want to find out how the model distinguishes
    APT from Core, we should take index 0 of the coefficients. However, we are more
    interested in the difference of Core and UI, so we take index 8\. In the first
    step, we sort the coefficients by their values and keep the indices, which are
    the vocabulary positions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Afterward, we now take the top positive and negative contributions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we will aggregate this to a `DataFrame` to make it easier to display the
    results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'We would like to visualize the contributions of the coefficients to make it
    easy to understand. Positive values favor the Core component, and negative values
    prefer UI, as shown in [Figure 7-1](#fig-coefficients-core-ui). To obtain this,
    we use the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: These results are quite easy to interpret. The SVM model has nicely learned
    that the words *compiler* and *ast* are specific to the Core component, whereas
    *wizard*, *ui*, and *dialog* are used to identify bugs in the UI component. It
    seems a quick fix is more popular in the UI, which emphasizes the long-term stability
    of the core.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have just found the features that are important for the whole SVM model
    to choose between Core and UI. But this does not indicate which features are important
    to identify a bug that can be categorized as Core given any bug report. If we
    want to get these features for the Core component and consider the previous matrix,
    we need indices 5, 6, 7, and 8\. With this strategy, we have ignored the difference
    between APT and Core. To take this into account, we need to subtract index 0:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '![](Images/btap_07in03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-1\. Word contributions to UI (negative) and Core (positive).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The rest of the code is almost identical to the previous code. We now extend
    the diagram to 20 words ([Figure 7-2](#fig-coefficients-core)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: In the diagram, you can see a lot of words that the model uses to identify the
    Core component and in the lower part those that are used to primarily identify
    other components.
  prefs: []
  type: TYPE_NORMAL
- en: You can use the methods described in this blueprint to make the results of the
    SVM model transparent and explainable. In many projects, this has proved to be
    valuable as it takes away the “magic” and the subjectivity of machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: This works quite well, but we do not yet know how sensitive the model is to
    changes in certain words. This is a more complicated question that we will try
    to answer in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/btap_07in04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-2\. Coefficients favoring or opposing the Core component.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Blueprint: Using LIME to Explain the Classification Results'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: LIME is an acronym for [“Local Interpretable Model-Agnostic Explanations”](https://oreil.ly/D8cIN)
    and is a popular framework for explainable machine learning. It was conceived
    at the [University of Washington](https://oreil.ly/Q8zly) and is publicly available
    [on GitHub](https://oreil.ly/bErrv).
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take a look at the defining features of LIME. It works *locally* by taking
    a look at each prediction separately. This is achieved by modifying the input
    vector to find the local components that the predictions are sensitive to.
  prefs: []
  type: TYPE_NORMAL
- en: Explainability Needs Computation Time
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Running the explainer code can take considerable time. We tried to tailor the
    examples in a way that you don’t have to wait for more than 10 minutes on normal
    computers. However, by increasing the sample size, this can easily take hours.
  prefs: []
  type: TYPE_NORMAL
- en: From the behavior in the vicinity of the vector, it will draw conclusions about
    which components are more or less important. LIME will visualize the contributions
    and explain the decision mechanism of the algorithm *for individual documents*.
  prefs: []
  type: TYPE_NORMAL
- en: LIME does not depend on a specific machine learning model and can be applied
    to a multitude of problems. Not every model qualifies; the model needs to predict
    the probabilities of the categories. Not all support vector machine models can
    do that. In addition, using complicated models where predictions take considerable
    time is not very practical in high-dimensional feature spaces like those common
    in text analytics. As LIME attempts to locally modify the feature vectors, it
    needs to perform a lot of predictions and in this case takes a long time to finish.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, LIME will generate an explanation for the model on a per-sample basis
    and allow you to understand the model. You can use this to improve your model
    but also to explain how a classification works. Although the model will still
    be a black box, you will gain some knowledge of what might be going on in the
    box.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s get back to the classification problem of the previous section and try
    to find a LIME explanation for a few samples. As LIME wants text as input and
    classification probabilities as output, we arrange the vectorizer and classifier
    in a *pipeline*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The pipeline should be able to make predictions if we give it some text, as
    done here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The classifier suggests with very high probability to put this in class 2,
    which is Core. So, our pipeline works exactly in the way we want it: we can give
    it text documents as parameters, and it returns the probabilities for the documents
    belonging to each category. Now it’s time to turn on LIME by first importing the
    package (you might have to install the package first with `pip` or `conda`). Afterward,
    we will create an explainer, which is one of the central elements of LIME and
    is responsible for explaining individual predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'We check the `DataFrame` for classes that have been wrongly predicted in the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '|   | index | text | actual | predicted | APT | Core | Debug | Doc | Text |
    UI |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 2 | NPE in Delta processor while executing JDT/UI ... | Core | UI | 0.003357
    | 0.309548 | 0.046491 | 0.002031 | 0.012309 | 0.626265 |'
  prefs: []
  type: TYPE_TB
- en: '| 15 | 15 | Inserting a block of text in editor badly alig... | UI | Text |
    0.001576 | 0.063076 | 0.034610 | 0.003907 | 0.614473 | 0.282356 |'
  prefs: []
  type: TYPE_TB
- en: '| 16 | 16 | Differences when debugging identical objects W... | Debug | Core
    | 0.002677 | 0.430862 | 0.313465 | 0.004193 | 0.055838 | 0.192965 |'
  prefs: []
  type: TYPE_TB
- en: '| 20 | 20 | Foreach template doesnt work for class members... | Core | UI |
    0.000880 | 0.044018 | 0.001019 | 0.000783 | 0.130766 | 0.822535 |'
  prefs: []
  type: TYPE_TB
- en: '| 21 | 21 | exchange left and right operands for compariso... | UI | Core |
    0.002669 | 0.467366 | 0.077252 | 0.003194 | 0.068749 | 0.380770 |'
  prefs: []
  type: TYPE_TB
- en: 'Take a look at the corresponding record (row 21 in our case):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Now it’s time for LIME to explain this to us!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: LIME shows us which words it thinks are in favor (positive) or against (negative)
    a certain class. This is quite easy and similar to what we have achieved in the
    SVM example. Even better, now it’s independent of the model itself; it just needs
    to support `predict_proba` (which is also true for Random Forest and so on).
  prefs: []
  type: TYPE_NORMAL
- en: 'With LIME, you can extend the analysis to more classes and create a graphics
    representation of their specific words:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/btap_07_svgcombo_1.jpg)![](Images/btap_07_svgcombo_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This looks intuitive and much more suitable for interpretation and even inclusion
    in a presentation. We can clearly see that *fix* and *right* are crucial for assigning
    the UI class and at the same time against Core. Bug, however, speaks for Core,
    as do *comparison* and *semantics*. Unfortunately, this is not what a human would
    accept as rules for classification; they seem too specific, and there is no abstraction.
    In other words, our model looks *overfitted*.
  prefs: []
  type: TYPE_NORMAL
- en: Improving Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With this knowledge and the expertise of people familiar with the tickets, you
    could improve the model. We could, for example, ask if *Bug* is really specific
    to Core or if we’d better make it a stop word. It might also prove useful to convert
    everything to lowercase.
  prefs: []
  type: TYPE_NORMAL
- en: 'LIME can even support you in finding representative samples that help you interpret
    the model performance as a whole. The feature is called *submodular picks* and
    works like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The individual “picks” can be visualized as shown previously in the notebook
    and are even more complete now with highlighting. We show only the first of the
    picks here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/btap_07_svgcombo_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In the following case, we can interpret the results, but it does not look like
    the model learned the abstraction, which is again a sign of *overfitting*.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/btap_07_highlightwords.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The LIME software module works for linear support vector machines in scikit-learn
    but not for those with more complex kernels. The graphical presentation is nice
    but is not directly suitable for presentations. Therefore, we will take a look
    at ELI5, which is an alternative implementation and tries to overcome these problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'Blueprint: Using ELI5 to Explain the Classification Results'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ELI5 (“Explain it to me like I’m 5”) is another popular software library for
    machine learning explanation also using the LIME algorithm. As it can be used
    for nonlinear SVMs and has a different API, we will take a short look at it and
    show how to use it in our case.
  prefs: []
  type: TYPE_NORMAL
- en: 'ELI5 needs a model that has been trained with `libsvm`, which our SVC model
    from before unfortunately is not. Luckily, training an SVM is really fast, so
    we can create a new classifier with the same data, but with a `libsvm`-based model,
    and check its performance. You might remember the classification report from [Chapter 6](ch06.xhtml#ch-classification),
    which gives a good summary about the quality of the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Taking a look at the last line, this is roughly as good as what we have achieved
    with SVC. Thus, it makes sense to explain it! Using ELI5, finding explanations
    for this model is easy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '![](Images/btap_07_eli5_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The positive features (i.e., words) are shown in green. More intense shades
    of green mean a larger contribution of the word to the corresponding class. The
    red colors work exactly opposite: words appearing in red “repel” the classes (for
    example, “refactoring” in the lower part of the second row strongly rejects class
    `Core`). `<BIAS>` is a special case and contains the so-called *intercept*, i.e.,
    systematic failures of the model.'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, we now get weights for the individual classes. This is due to
    the non-linear SVM model working differently in multiclass scenarios compared
    to SVC. Each class is “scored” on its own, and there is no competition. At first
    sight, the words look very plausible.
  prefs: []
  type: TYPE_NORMAL
- en: 'ELI5 can also explain individual observations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '![](Images/btap_07_eli5_2.jpg)![](Images/btap_07_eli5_3.jpg)![](Images/btap_07_eli5_4.jpg)![](Images/btap_07_eli5_5.jpg)![](Images/btap_07_eli5_6.jpg)![](Images/btap_07_eli5_7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This is a nice visualization for understanding which words contribute to the
    algorithm deciding the categories. Compared to the original LIME package, with
    ELI5 you need considerably less code, and you can use ELI5 for nonlinear SVM models.
    Depending on your classifier and use case, you might decide on LIME or ELI5\.
    Due to the same method, the results should be comparable (if not identical).
  prefs: []
  type: TYPE_NORMAL
- en: Work in Progress
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ELI5 is still under heavy development, and you might experience difficulties
    with new scikit-learn versions. We have used ELI5 version 0.10.1 in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: ELI5 is an easy-to-use software library for understanding and visualizing the
    decision logic of classifiers, but it also suffers from the shortcomings of the
    underlying LIME algorithm, such as explainability by example only. To make the
    black-box classification more transparent, it would be insightful to gain access
    to the “rules” that a model uses. That was the motivation for the group at Washington
    University to create a follow-up project called Anchor.
  prefs: []
  type: TYPE_NORMAL
- en: 'Blueprint: Using Anchor to Explain the Classification Results'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Like LIME, [Anchor](https://oreil.ly/qSDMl) is model agnostic and works for
    any black-box model. As a tool for explanations, it creates rules, the so-called
    *anchors*, which explain the behavior of the model. Reading these rules, you will
    not only be able to explain a prediction of the model but also predict in the
    same way as the model has learned to.
  prefs: []
  type: TYPE_NORMAL
- en: Compared to LIME, Anchor has considerable advantages for better explaining the
    models with the rules. However, the software itself is quite new and still a work
    in progress. Not all examples were working for us, so we chose a selection of
    methods that help in interpreting the classification model.
  prefs: []
  type: TYPE_NORMAL
- en: Using the Distribution with Masked Words
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are different ways Anchor can be used. We start with the so-called *unknown*
    distribution. Anchor will explain how a model makes a decision by replacing existing
    tokens that are supposed to be unimportant for the prediction with the word *unknown*.
  prefs: []
  type: TYPE_NORMAL
- en: Again, we will use the document with an ID of 21\. In this case, the classifier
    has the difficult task of choosing between two categories that have roughly the
    same probability. This should make it an interesting example for studying.
  prefs: []
  type: TYPE_NORMAL
- en: To create (semantic) variance in the text, Anchor uses spaCy’s word vectors
    and needs a spaCy model that includes these vectors, like `en_core_web_lg`.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a prerequisite, you should therefore install `anchor-exp` and `spacy` (using
    either `conda` or `pip`) and load the model with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'In the first step, we can then instantiate our explainer. The explainer has
    some probabilistic elements, so it’s better to restart the random-number generator
    at the same time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s check the predicted results and alternatives and compare them to the
    ground truth. `predicted_class_ids` contains the indices of the predicted classes
    with decreasing probability, so the element 0 is the prediction, and element 1
    is its closest competitor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'In the next step, we will let the algorithm find the rules for the predictions.
    The parameters are the same as for LIME earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: The calculation can take up to 60 minutes depending on the speed of your CPU.
  prefs: []
  type: TYPE_NORMAL
- en: 'Everything is now contained in the explainer, so we can query the explainer
    to find out about the inner workings of the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'So, the rule tells us that an occurrence of the words *following* and *comparison*
    combined with *Bug* and *semantic* leads to a prediction of “Core” with more than
    98% precision, which is unfortunately wrong. We can now also find typical examples
    that the model would classify as Core:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'The UNK token shown next stands for “unknown” and means that the word at the
    corresponding position is not important:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also ask for examples where the rule matches but the model predicts
    the wrong class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: To be honest, this is not a good result for the model. We would have expected
    that the underlying rules learned by the models would be sensitive to words specific
    to the different components. However, there is no obvious reason why *following*
    and *Bug* would be specific to Core. More or less these are generic words that
    are not very characteristic of either of the categories.
  prefs: []
  type: TYPE_NORMAL
- en: The UNK tokens are a bit misleading. Even if they are not important in this
    sample, they might be replaced by other, realistic words that would influence
    the decision of the algorithm. Anchor can also help us illustrate that.
  prefs: []
  type: TYPE_NORMAL
- en: Working with Real Words
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'By substituting `use_unk_distribution=False` in the original constructor of
    the explainer, we can tell Anchor to use real words (similar to the one it is
    substituting by using the word vectors from spaCy) and observe the behavior of
    the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: The rules are a bit different from the earlier unknown distribution. It seems
    that some of the words have become a bit more specific for the Core, like *left*
    and *right*, whereas other words like *for* have vanished.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s also ask Anchor to generate alternative texts that would also be (wrongly)
    classified as Core as the previous rule applies:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: Some words have changed and have not affected the result of the classification.
    In some cases, it is only prepositions, and normally this should not have an effect
    on the results. However, *operators* can also be replaced by *dispatchers* without
    affecting the results. Anchor shows you that it is stable against these modifications.
  prefs: []
  type: TYPE_NORMAL
- en: 'Compare the previous results to those where the model would (correctly) predict
    “UI.” Again, the difference affects single words like *changes*, *metaphors*,
    and so on, which definitely carry more meaning than the smaller modifications
    in the previous example, but it is highly unlikely that you as a human would interpret
    these words as signals for a different category:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: Anchor also has an intuitive way of showing the results with the important words
    highlighted in the notebook and also includes the rules it has calculated:^([2](ch07.xhtml#idm45634189444808))
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: '`Out:`'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/btap_07_highlightwords2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As it’s quite likely that you are also familiar with software development, it
    would be hard to determine the correct category from the rules alone. In other
    words, this means the model seems to be quite fragile when trained with the corpus.
    The “correct” category can probably be determined only by a project contributor
    who has a lot of context knowledge (which we will revisit later in [Chapter 11](ch11.xhtml#ch-sentiment)).
    So, finding that a classifier works does not necessarily mean that it has really
    learned in a way that is transparent for us.
  prefs: []
  type: TYPE_NORMAL
- en: To summarize this section, Anchor is quite interesting. The authors of Anchor
    did not choose version number 0.0.1 by chance; the program is still in its infancy.
    During our experiments, we have seen quite a few quirks, and to make it work in
    production, a lot of things have to be improved. Conceptually, however, it is
    already really convincing for explaining single predictions and making models
    transparent. The calculated rules especially are almost unique and cannot be created
    by any other solution.
  prefs: []
  type: TYPE_NORMAL
- en: Closing Remarks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Using the techniques presented in this chapter will help make your model predictions
    more transparent.
  prefs: []
  type: TYPE_NORMAL
- en: From a technical perspective, this transparency can be a great help as it supports
    you in choosing among competing models or improving your feature models. The techniques
    presented in this chapter give you insights into the “inner workings” of a model
    and help to detect and improve untrustworthy models.
  prefs: []
  type: TYPE_NORMAL
- en: Considering the business perspective, explainability is a great selling proposition
    for projects. It is much easier to talk about models and present them if you don’t
    exclusively pursue the black-box model but rather make your models transparent.
    Recent articles in [Forbes](https://oreil.ly/Xcfjx) and [VentureBeat](https://oreil.ly/SIa-R)
    have focused on this interesting development. Being able to “trust” a model will
    be more and more important when you want to build trustable machine learning solutions.
  prefs: []
  type: TYPE_NORMAL
- en: Explainable AI is a young field. We can expect to see tremendous progress, better
    algorithms, and improved tooling in the future.
  prefs: []
  type: TYPE_NORMAL
- en: For most of the book, machine learning methods have worked nicely as black-box
    models. This is fine, as long as the results are consistent and we don’t have
    to justify the models. If either is challenged, as is becoming more common, then
    the time for explainable AI has arrived.
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch07.xhtml#idm45634192517144-marker)) Graphically, you can think of the
    probabilities as the distance of the samples to the hyperplane defined by the
    SVM.
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch07.xhtml#idm45634189444808-marker)) We had a hard time getting this
    to work, as it was suited only for numerical categories. We plan to make some
    pull requests to get the upstream working for textual categories as well.
  prefs: []
  type: TYPE_NORMAL
