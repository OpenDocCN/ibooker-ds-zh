- en: Chapter 25\. MapReduce
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The future has already arrived. It’s just not evenly distributed yet.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: William Gibson
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: MapReduce is a programming model for performing parallel processing on large
    datasets. Although it is a powerful technique, its basics are relatively simple.
  prefs: []
  type: TYPE_NORMAL
- en: 'Imagine we have a collection of items we’d like to process somehow. For instance,
    the items might be website logs, the texts of various books, image files, or anything
    else. A basic version of the MapReduce algorithm consists of the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Use a `mapper` function to turn each item into zero or more key/value pairs.
    (Often this is called the `map` function, but there is already a Python function
    called `map` and we don’t need to confuse the two.)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Collect together all the pairs with identical keys.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use a `reducer` function on each collection of grouped values to produce output
    values for the corresponding key.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: MapReduce is sort of passé, so much so that I considered removing this chapter
    from the second edition. But I decided it’s still an interesting topic, so I ended
    up leaving it in (obviously).
  prefs: []
  type: TYPE_NORMAL
- en: This is all sort of abstract, so let’s look at a specific example. There are
    few absolute rules of data science, but one of them is that your first MapReduce
    example has to involve counting words.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: Word Count'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: DataSciencester has grown to millions of users! This is great for your job security,
    but it makes routine analyses slightly more difficult.
  prefs: []
  type: TYPE_NORMAL
- en: For example, your VP of Content wants to know what sorts of things people are
    talking about in their status updates. As a first attempt, you decide to count
    the words that appear, so that you can prepare a report on the most frequent ones.
  prefs: []
  type: TYPE_NORMAL
- en: 'When you had a few hundred users, this was simple to do:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: With millions of users the set of `documents` (status updates) is suddenly too
    big to fit on your computer. If you can just fit this into the MapReduce model,
    you can use some “big data” infrastructure that your engineers have implemented.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need a function that turns a document into a sequence of key/value
    pairs. We’ll want our output to be grouped by word, which means that the keys
    should be words. And for each word, we’ll just emit the value `1` to indicate
    that this pair corresponds to one occurrence of the word:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Skipping the “plumbing” step 2 for the moment, imagine that for some word we’ve
    collected a list of the corresponding counts we emitted. To produce the overall
    count for that word, then, we just need:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Returning to step 2, we now need to collect the results from `wc_mapper` and
    feed them to `wc_reducer`. Let’s think about how we would do this on just one
    computer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Imagine that we have three documents `["data science", "big data", "science
    fiction"]`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then `wc_mapper` applied to the first document yields the two pairs `("data",
    1)` and `("science", 1)`. After we’ve gone through all three documents, the `collector`
    contains:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Then `wc_reducer` produces the counts for each word:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Why MapReduce?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As mentioned earlier, the primary benefit of MapReduce is that it allows us
    to distribute computations by moving the processing to the data. Imagine we want
    to word-count across billions of documents.
  prefs: []
  type: TYPE_NORMAL
- en: Our original (non-MapReduce) approach requires the machine doing the processing
    to have access to every document. This means that the documents all need to either
    live on that machine or else be transferred to it during processing. More important,
    it means that the machine can process only one document at a time.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Possibly it can process up to a few at a time if it has multiple cores and if
    the code is rewritten to take advantage of them. But even so, all the documents
    still have to *get to* that machine.
  prefs: []
  type: TYPE_NORMAL
- en: 'Imagine now that our billions of documents are scattered across 100 machines.
    With the right infrastructure (and glossing over some of the details), we can
    do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Have each machine run the mapper on its documents, producing lots of key/value
    pairs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distribute those key/value pairs to a number of “reducing” machines, making
    sure that the pairs corresponding to any given key all end up on the same machine.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Have each reducing machine group the pairs by key and then run the reducer on
    each set of values.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Return each (key, output) pair.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is amazing about this is that it scales horizontally. If we double the
    number of machines, then (ignoring certain fixed costs of running a MapReduce
    system) our computation should run approximately twice as fast. Each mapper machine
    will only need to do half as much work, and (assuming there are enough distinct
    keys to further distribute the reducer work) the same is true for the reducer
    machines.
  prefs: []
  type: TYPE_NORMAL
- en: MapReduce More Generally
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you think about it for a minute, all of the word count–specific code in the
    previous example is contained in the `wc_mapper` and `wc_reducer` functions. This
    means that with a couple of changes we have a much more general framework (that
    still runs on a single machine).
  prefs: []
  type: TYPE_NORMAL
- en: 'We could use generic types to fully type-annotate our `map_reduce` function,
    but it would end up being kind of a mess pedagogically, so in this chapter we’ll
    be much more casual about our type annotations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can write a general `map_reduce` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we can count words simply by using:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: This gives us the flexibility to solve a wide variety of problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we proceed, notice that `wc_reducer` is just summing the values corresponding
    to each key. This kind of aggregation is common enough that it’s worth abstracting
    it out:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'After which we can easily create:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: Analyzing Status Updates'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The content VP was impressed with the word counts and asks what else you can
    learn from people’s status updates. You manage to extract a dataset of status
    updates that look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s say we need to figure out which day of the week people talk the most
    about data science. In order to find this, we’ll just count how many data science
    updates there are on each day of the week. This means we’ll need to group by the
    day of week, so that’s our key. And if we emit a value of `1` for each update
    that contains “data science,” we can simply get the total number using `sum`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'As a slightly more complicated example, imagine we need to find out for each
    user the most common word that she puts in her status updates. There are three
    possible approaches that spring to mind for the `mapper`:'
  prefs: []
  type: TYPE_NORMAL
- en: Put the username in the key; put the words and counts in the values.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Put the word in the key; put the usernames and counts in the values.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Put the username and word in the key; put the counts in the values.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If you think about it a bit more, we definitely want to group by `username`,
    because we want to consider each person’s words separately. And we don’t want
    to group by `word`, since our reducer will need to see all the words for each
    person to find out which is the most popular. This means that the first option
    is the right choice:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Or we could find out the number of distinct status-likers for each user:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Example: Matrix Multiplication'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Recall from [“Matrix Multiplication”](ch22.html#matrix_multiplication) that
    given an `[n, m]` matrix `A` and an `[m, k]` matrix `B`, we can multiply them
    to form an `[n, k]` matrix `C`, where the element of `C` in row `i` and column
    `j` is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: This works if we represent our matrices as lists of lists, as we’ve been doing.
  prefs: []
  type: TYPE_NORMAL
- en: 'But large matrices are sometimes *sparse*, which means that most of their elements
    equal 0\. For large sparse matrices, a list of lists can be a very wasteful representation.
    A more compact representation stores only the locations with nonzero values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: For example, a 1 billion × 1 billion matrix has 1 *quintillion* entries, which
    would not be easy to store on a computer. But if there are only a few nonzero
    entries in each row, this alternative representation is many orders of magnitude
    smaller.
  prefs: []
  type: TYPE_NORMAL
- en: Given this sort of representation, it turns out that we can use MapReduce to
    perform matrix multiplication in a distributed manner.
  prefs: []
  type: TYPE_NORMAL
- en: 'To motivate our algorithm, notice that each element `A[i][j]` is only used
    to compute the elements of `C` in row `i`, and each element `B[i][j]` is only
    used to compute the elements of `C` in column `j`. Our goal will be for each output
    of our `reducer` to be a single entry of `C`, which means we’ll need our mapper
    to emit keys identifying a single entry of `C`. This suggests the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'And then:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'For example, if you had these two matrices:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'you could rewrite them as tuples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: This isn’t terribly interesting on such small matrices, but if you had millions
    of rows and millions of columns, it could help you a lot.
  prefs: []
  type: TYPE_NORMAL
- en: 'An Aside: Combiners'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One thing you have probably noticed is that many of our mappers seem to include
    a bunch of extra information. For example, when counting words, rather than emitting
    `(word, 1)` and summing over the values, we could have emitted `(word, None)`
    and just taken the length.
  prefs: []
  type: TYPE_NORMAL
- en: One reason we didn’t do this is that, in the distributed setting, we sometimes
    want to use *combiners* to reduce the amount of data that has to be transferred
    around from machine to machine. If one of our mapper machines sees the word *data*
    500 times, we can tell it to combine the 500 instances of `("data", 1)` into a
    single `("data", 500)` before handing off to the reducing machine. This results
    in a lot less data getting moved around, which can make our algorithm substantially
    faster still.
  prefs: []
  type: TYPE_NORMAL
- en: Because of the way we wrote our reducer, it would handle this combined data
    correctly. (If we’d written it using `len`, it would not have.)
  prefs: []
  type: TYPE_NORMAL
- en: For Further Exploration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Like I said, MapReduce feels a lot less popular now than it did when I wrote
    the first edition. It’s probably not worth investing a ton of your time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: That said, the most widely used MapReduce system is [Hadoop](http://hadoop.apache.org).
    There are various commercial and noncommercial distributions and a huge ecosystem
    of Hadoop-related tools.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Amazon.com offers an [Elastic MapReduce](http://aws.amazon.com/elasticmapreduce/)
    service that’s probably easier than setting up your own cluster.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hadoop jobs are typically high-latency, which makes them a poor choice for “real-time”
    analytics. A popular choice for these workloads is [Spark](http://spark.apache.org/),
    which can be MapReduce-y.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
