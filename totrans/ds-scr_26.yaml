- en: Chapter 25\. MapReduce
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第25章。MapReduce
- en: The future has already arrived. It’s just not evenly distributed yet.
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 未来已经到来，只是尚未均匀分布。
- en: ''
  id: totrans-2
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: William Gibson
  id: totrans-3
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 威廉·吉布森
- en: MapReduce is a programming model for performing parallel processing on large
    datasets. Although it is a powerful technique, its basics are relatively simple.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce是一种在大型数据集上执行并行处理的编程模型。虽然它是一种强大的技术，但其基础相对简单。
- en: 'Imagine we have a collection of items we’d like to process somehow. For instance,
    the items might be website logs, the texts of various books, image files, or anything
    else. A basic version of the MapReduce algorithm consists of the following steps:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 想象我们有一系列希望以某种方式处理的项目。例如，这些项目可能是网站日志、各种书籍的文本、图像文件或其他任何内容。MapReduce算法的基本版本包括以下步骤：
- en: Use a `mapper` function to turn each item into zero or more key/value pairs.
    (Often this is called the `map` function, but there is already a Python function
    called `map` and we don’t need to confuse the two.)
  id: totrans-6
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`mapper`函数将每个项转换为零个或多个键/值对。（通常称为`map`函数，但Python已经有一个名为`map`的函数，我们不需要混淆这两者。）
- en: Collect together all the pairs with identical keys.
  id: totrans-7
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 收集所有具有相同键的对。
- en: Use a `reducer` function on each collection of grouped values to produce output
    values for the corresponding key.
  id: totrans-8
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对每个分组值集合使用`reducer`函数，以生成相应键的输出值。
- en: Note
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: MapReduce is sort of passé, so much so that I considered removing this chapter
    from the second edition. But I decided it’s still an interesting topic, so I ended
    up leaving it in (obviously).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce已经有些过时了，以至于我考虑从第二版中删除这一章节。但我决定它仍然是一个有趣的主题，所以最终我还是留了下来（显然）。
- en: This is all sort of abstract, so let’s look at a specific example. There are
    few absolute rules of data science, but one of them is that your first MapReduce
    example has to involve counting words.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这些都有点抽象，让我们看一个具体的例子。数据科学中几乎没有绝对规则，但其中一个规则是，您的第一个MapReduce示例必须涉及单词计数。
- en: 'Example: Word Count'
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 示例：单词计数
- en: DataSciencester has grown to millions of users! This is great for your job security,
    but it makes routine analyses slightly more difficult.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: DataSciencester已经发展到数百万用户！这对于您的工作安全来说是好事，但也使得例行分析略微更加困难。
- en: For example, your VP of Content wants to know what sorts of things people are
    talking about in their status updates. As a first attempt, you decide to count
    the words that appear, so that you can prepare a report on the most frequent ones.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，您的内容副总裁想知道人们在其状态更新中谈论的内容。作为第一次尝试，您决定计算出现的单词，以便可以准备一份最频繁出现单词的报告。
- en: 'When you had a few hundred users, this was simple to do:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 当您只有几百个用户时，这样做很简单：
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: With millions of users the set of `documents` (status updates) is suddenly too
    big to fit on your computer. If you can just fit this into the MapReduce model,
    you can use some “big data” infrastructure that your engineers have implemented.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 当您有数百万用户时，`documents`（状态更新）的集合突然变得太大，无法放入您的计算机中。如果您能将其适应MapReduce模型，您可以使用工程师们实施的一些“大数据”基础设施。
- en: 'First, we need a function that turns a document into a sequence of key/value
    pairs. We’ll want our output to be grouped by word, which means that the keys
    should be words. And for each word, we’ll just emit the value `1` to indicate
    that this pair corresponds to one occurrence of the word:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要一个将文档转换为键/值对序列的函数。我们希望输出按单词分组，这意味着键应该是单词。对于每个单词，我们只需发出值`1`来表示该对应单词的出现次数为一次：
- en: '[PRE1]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Skipping the “plumbing” step 2 for the moment, imagine that for some word we’ve
    collected a list of the corresponding counts we emitted. To produce the overall
    count for that word, then, we just need:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 暂时跳过“管道”步骤2，想象一下对于某个单词，我们已经收集到了我们发出的相应计数列表。为了生成该单词的总计数，我们只需：
- en: '[PRE2]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Returning to step 2, we now need to collect the results from `wc_mapper` and
    feed them to `wc_reducer`. Let’s think about how we would do this on just one
    computer:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 回到步骤2，现在我们需要收集`wc_mapper`的结果并将其提供给`wc_reducer`。让我们考虑如何在一台计算机上完成这项工作：
- en: '[PRE3]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Imagine that we have three documents `["data science", "big data", "science
    fiction"]`.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有三个文档`["data science", "big data", "science fiction"]`。
- en: 'Then `wc_mapper` applied to the first document yields the two pairs `("data",
    1)` and `("science", 1)`. After we’ve gone through all three documents, the `collector`
    contains:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，将`wc_mapper`应用于第一个文档，产生两对`("data", 1)`和`("science", 1)`。在我们处理完所有三个文档之后，`collector`包含：
- en: '[PRE4]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Then `wc_reducer` produces the counts for each word:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，`wc_reducer`生成每个单词的计数：
- en: '[PRE5]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Why MapReduce?
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么要使用MapReduce？
- en: As mentioned earlier, the primary benefit of MapReduce is that it allows us
    to distribute computations by moving the processing to the data. Imagine we want
    to word-count across billions of documents.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 正如前面提到的，MapReduce的主要优势在于它允许我们通过将处理移到数据上来分布计算。假设我们想要跨数十亿文档进行单词计数。
- en: Our original (non-MapReduce) approach requires the machine doing the processing
    to have access to every document. This means that the documents all need to either
    live on that machine or else be transferred to it during processing. More important,
    it means that the machine can process only one document at a time.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最初的（非MapReduce）方法要求处理文档的机器能够访问每个文档。这意味着所有文档都需要在该机器上存储，或者在处理过程中将其传输到该机器上。更重要的是，这意味着机器一次只能处理一个文档。
- en: Note
  id: totrans-32
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Possibly it can process up to a few at a time if it has multiple cores and if
    the code is rewritten to take advantage of them. But even so, all the documents
    still have to *get to* that machine.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 如果具有多个核心并且代码已重写以利用它们，可能可以同时处理几个。但即便如此，所有文档仍然必须*到达*该机器。
- en: 'Imagine now that our billions of documents are scattered across 100 machines.
    With the right infrastructure (and glossing over some of the details), we can
    do the following:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 现在假设我们的数十亿文档分散在100台机器上。有了正确的基础设施（并且忽略某些细节），我们可以执行以下操作：
- en: Have each machine run the mapper on its documents, producing lots of key/value
    pairs.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 让每台机器在其文档上运行映射器，生成大量的键/值对。
- en: Distribute those key/value pairs to a number of “reducing” machines, making
    sure that the pairs corresponding to any given key all end up on the same machine.
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将这些键/值对分发到多个“减少”机器，确保与任何给定键对应的所有对最终都在同一台机器上。
- en: Have each reducing machine group the pairs by key and then run the reducer on
    each set of values.
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 让每个减少机器按键分组然后对每组值运行减少器。
- en: Return each (key, output) pair.
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 返回每个（键，输出）对。
- en: What is amazing about this is that it scales horizontally. If we double the
    number of machines, then (ignoring certain fixed costs of running a MapReduce
    system) our computation should run approximately twice as fast. Each mapper machine
    will only need to do half as much work, and (assuming there are enough distinct
    keys to further distribute the reducer work) the same is true for the reducer
    machines.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这其中令人惊奇的是它的水平扩展能力。如果我们将机器数量翻倍，那么（忽略运行MapReduce系统的某些固定成本），我们的计算速度应该大约加快一倍。每台映射器机器只需完成一半的工作量，并且（假设有足够多的不同键来进一步分发减少器的工作）减少器机器也是如此。
- en: MapReduce More Generally
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更普遍的MapReduce
- en: If you think about it for a minute, all of the word count–specific code in the
    previous example is contained in the `wc_mapper` and `wc_reducer` functions. This
    means that with a couple of changes we have a much more general framework (that
    still runs on a single machine).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你仔细想一想，你会发现前面示例中所有与计数特定单词有关的代码都包含在`wc_mapper`和`wc_reducer`函数中。这意味着只需做出一些更改，我们就可以得到一个更通用的框架（仍然在单台机器上运行）。
- en: 'We could use generic types to fully type-annotate our `map_reduce` function,
    but it would end up being kind of a mess pedagogically, so in this chapter we’ll
    be much more casual about our type annotations:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用通用类型完全类型注释我们的`map_reduce`函数，但这在教学上可能会有些混乱，因此在本章中，我们对类型注释要更加随意：
- en: '[PRE6]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now we can write a general `map_reduce` function:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以编写一个通用的`map_reduce`函数：
- en: '[PRE7]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Then we can count words simply by using:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以简单地通过以下方式计算单词数：
- en: '[PRE8]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This gives us the flexibility to solve a wide variety of problems.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这使我们能够灵活地解决各种问题。
- en: 'Before we proceed, notice that `wc_reducer` is just summing the values corresponding
    to each key. This kind of aggregation is common enough that it’s worth abstracting
    it out:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，请注意`wc_reducer`仅仅是对每个键对应的值求和。这种聚合是如此普遍，以至于值得将其抽象出来：
- en: '[PRE9]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'After which we can easily create:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以轻松创建：
- en: '[PRE10]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: and so on.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 等等。
- en: 'Example: Analyzing Status Updates'
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 示例：分析状态更新
- en: 'The content VP was impressed with the word counts and asks what else you can
    learn from people’s status updates. You manage to extract a dataset of status
    updates that look like:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 内容VP对单词计数印象深刻，并询问您可以从人们的状态更新中学到什么其他内容。您设法提取出一个看起来像这样的状态更新数据集：
- en: '[PRE11]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Let’s say we need to figure out which day of the week people talk the most
    about data science. In order to find this, we’ll just count how many data science
    updates there are on each day of the week. This means we’ll need to group by the
    day of week, so that’s our key. And if we emit a value of `1` for each update
    that contains “data science,” we can simply get the total number using `sum`:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'As a slightly more complicated example, imagine we need to find out for each
    user the most common word that she puts in her status updates. There are three
    possible approaches that spring to mind for the `mapper`:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: Put the username in the key; put the words and counts in the values.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Put the word in the key; put the usernames and counts in the values.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Put the username and word in the key; put the counts in the values.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If you think about it a bit more, we definitely want to group by `username`,
    because we want to consider each person’s words separately. And we don’t want
    to group by `word`, since our reducer will need to see all the words for each
    person to find out which is the most popular. This means that the first option
    is the right choice:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Or we could find out the number of distinct status-likers for each user:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Example: Matrix Multiplication'
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Recall from [“Matrix Multiplication”](ch22.html#matrix_multiplication) that
    given an `[n, m]` matrix `A` and an `[m, k]` matrix `B`, we can multiply them
    to form an `[n, k]` matrix `C`, where the element of `C` in row `i` and column
    `j` is given by:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: This works if we represent our matrices as lists of lists, as we’ve been doing.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: 'But large matrices are sometimes *sparse*, which means that most of their elements
    equal 0\. For large sparse matrices, a list of lists can be a very wasteful representation.
    A more compact representation stores only the locations with nonzero values:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: For example, a 1 billion × 1 billion matrix has 1 *quintillion* entries, which
    would not be easy to store on a computer. But if there are only a few nonzero
    entries in each row, this alternative representation is many orders of magnitude
    smaller.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: Given this sort of representation, it turns out that we can use MapReduce to
    perform matrix multiplication in a distributed manner.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: 'To motivate our algorithm, notice that each element `A[i][j]` is only used
    to compute the elements of `C` in row `i`, and each element `B[i][j]` is only
    used to compute the elements of `C` in column `j`. Our goal will be for each output
    of our `reducer` to be a single entry of `C`, which means we’ll need our mapper
    to emit keys identifying a single entry of `C`. This suggests the following:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'And then:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'For example, if you had these two matrices:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'you could rewrite them as tuples:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: This isn’t terribly interesting on such small matrices, but if you had millions
    of rows and millions of columns, it could help you a lot.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: 'An Aside: Combiners'
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One thing you have probably noticed is that many of our mappers seem to include
    a bunch of extra information. For example, when counting words, rather than emitting
    `(word, 1)` and summing over the values, we could have emitted `(word, None)`
    and just taken the length.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到，我们的许多 mapper 看起来包含了大量额外的信息。例如，在计算单词数量时，我们可以发射 `(word, None)` 并且只计算长度，而不是发射
    `(word, 1)` 并对值求和。
- en: One reason we didn’t do this is that, in the distributed setting, we sometimes
    want to use *combiners* to reduce the amount of data that has to be transferred
    around from machine to machine. If one of our mapper machines sees the word *data*
    500 times, we can tell it to combine the 500 instances of `("data", 1)` into a
    single `("data", 500)` before handing off to the reducing machine. This results
    in a lot less data getting moved around, which can make our algorithm substantially
    faster still.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们没有这样做的一个原因是，在分布式设置中，有时我们想要使用*组合器*来减少必须从一台机器传输到另一台机器的数据量。如果我们的某个 mapper 机器看到单词
    *data* 出现了 500 次，我们可以告诉它将 `("data", 1)` 的 500 个实例合并成一个 `("data", 500)`，然后再交给 reducer
    处理。这样可以减少传输的数据量，从而使我们的算法速度显著提高。
- en: Because of the way we wrote our reducer, it would handle this combined data
    correctly. (If we’d written it using `len`, it would not have.)
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们写的 reducer 的方式，它将正确处理这些合并的数据。（如果我们使用 `len` 写的话，就不会。）
- en: For Further Exploration
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深入探讨
- en: Like I said, MapReduce feels a lot less popular now than it did when I wrote
    the first edition. It’s probably not worth investing a ton of your time.
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正如我所说的，与我写第一版时相比，MapReduce 现在似乎不那么流行了。也许不值得投入大量时间。
- en: That said, the most widely used MapReduce system is [Hadoop](http://hadoop.apache.org).
    There are various commercial and noncommercial distributions and a huge ecosystem
    of Hadoop-related tools.
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尽管如此，最广泛使用的 MapReduce 系统是[Hadoop](http://hadoop.apache.org)。有各种商业和非商业的发行版，以及一个庞大的与
    Hadoop 相关的工具生态系统。
- en: Amazon.com offers an [Elastic MapReduce](http://aws.amazon.com/elasticmapreduce/)
    service that’s probably easier than setting up your own cluster.
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Amazon.com 提供了一个[Elastic MapReduce](http://aws.amazon.com/elasticmapreduce/)服务，可能比自己搭建集群更容易。
- en: Hadoop jobs are typically high-latency, which makes them a poor choice for “real-time”
    analytics. A popular choice for these workloads is [Spark](http://spark.apache.org/),
    which can be MapReduce-y.
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hadoop 作业通常具有较高的延迟，这使得它们在“实时”分析方面表现不佳。这些工作负载的流行选择是[Spark](http://spark.apache.org/)，它可以像
    MapReduce 一样运行。
