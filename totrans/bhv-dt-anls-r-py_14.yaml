- en: Chapter 10\. Cluster Randomization and Hierarchical Modeling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Our last experiment, while conceptually simple, will illustrate some of the
    logistical and statistical difficulties of experimenting in business. AirCnC has
    10 customer call centers spread across the country, where representatives handle
    any issue that might come up in the course of a booking (e.g., the payment did
    not go through, the property doesn’t look like the pictures, etc.). Having read
    an article in the *Harvard Business Review* (HBR) about customer service,^([1](ch10.xhtml#ch01fn22))
    the VP of customer service has decided to implement a change in standard operating
    procedures (SOP): instead of apologizing repeatedly when something went wrong,
    the call center reps should apologize at the beginning of the interaction, then
    get into “problem-solving mode,” then end up offering several options to the customer.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This experiment presents multiple challenges: due to logistical constraints,
    we’ll be able to randomize treatment only at the level of call centers and not
    reps, and we’ll have difficulties enforcing and measuring compliance. This certainly
    doesn’t mean that we can’t or shouldn’t run an experiment!'
  prefs: []
  type: TYPE_NORMAL
- en: Regarding the randomization constraint, we’ll see that this makes the standard
    linear regression algorithm inappropriate and that we should use hierarchical
    linear modeling (HLM) instead.
  prefs: []
  type: TYPE_NORMAL
- en: 'As before, our approach will be:'
  prefs: []
  type: TYPE_NORMAL
- en: Planning the experiment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Determining random assignment and sample size/power
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analyzing the experiment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Planning the Experiment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, I’ll go briskly through our theory of change to provide you
    with some necessary context and behavioral grounding:'
  prefs: []
  type: TYPE_NORMAL
- en: First, the business goal and target metric
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, the definition of our intervention
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, the behavioral logic that connects them
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Business Goal and Target Metric
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Based on the HBR article, our criterion for success or target metric appears
    straightforward: customer satisfaction as measured by a one-question survey administered
    by email after the phone call. However, we’ll see in a minute that there are complications,
    so we’ll need to revisit it after discussing what we’re testing.'
  prefs: []
  type: TYPE_NORMAL
- en: Definition of the Intervention
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The treatment we’re testing will be whether the reps have been trained in the
    new SOP and instructed to implement it.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first difficulty is in the implementation of the treatment. We know from
    past experience that asking the reps to apply different SOPs to different customers
    is very challenging: asking reps to switch processes at random between calls increases
    their cognitive load and the risk of noncompliance. Therefore, we’ll have to train
    some reps and instruct them to use the new SOP for all of their calls, while keeping
    other reps on the old SOP.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Even with that correction, compliance remains at risk: reps in the treatment
    group may implement the new SOP inconsistently or even not at all, while reps
    in the control group may also apply the old SOP inconsistently. Obviously, this
    would muddle our analysis and make the treatment appear less different from the
    control group than it is. One way to mitigate this issue is to first observe current
    compliance with the SOP in place by listening to calls, then run a pilot study,
    where we select a few reps, train them, and observe compliance with the new SOP.
    Debriefing the reps in the pilot study after the fact can help identify misunderstandings
    and obstacles to compliance. Unfortunately, it is generally impossible to have
    100% compliance in an experiment where human beings are delivering or choosing
    the treatment. The best we can do is to try to measure compliance and take it
    into account when drawing conclusions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, there is a risk of “leakage” between our control and our treatment
    groups. Reps are human beings, and reps in a given call center interact and chat.
    Given that reps are incentivized on the average monthly customer satisfaction
    (CSAT) for their calls, if reps in the treatment group started seeing significantly
    better results, there is a risk that reps in the control group of the same call
    center would start changing their procedure. Having some people in the control
    group apply the treatment would muddle the comparison for the two groups and make
    the difference appear smaller than it really is. Therefore, we’ll apply the treatment
    at the call center level: all reps in a given call center will either be in the
    treatment group or in the control group.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Applying the treatment at the call center level instead of at the call level
    has implications for our criterion for success. If our unit of randomization is
    the call center, should we measure the CSAT at the call center level? This would
    seem logical, but it would mean that we can’t use any information about individual
    reps or individual calls. On the other hand, measuring average CSAT at the rep
    level or even CSAT at the call level would allow us to use more information, but
    it is problematic for two reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: First, if we were to disregard the fact that randomization was not done at the
    call level and use standard power analysis, our results would be biased because
    randomization is unavoidably correlated with the call center variable; adding
    more calls in our sample would not change the fact that we have only 10 call centers
    and therefore only 10 randomization units.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Second, in our data analysis, we would run into trouble due to the nested nature
    of the data: assuming that each rep belongs to one and only one call center, there
    will be multicollinearity between our call center variable and our rep variable
    (e.g., we can add 1 to the coefficient for the first call center and subtract
    1 from the coefficients for all the reps in that call center without changing
    the results of the regression; therefore the coefficients for the regression are
    essentially undetermined).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fortunately, there is a simple solution to this problem: we’ll use a hierarchical
    model, which recognizes the nested structure of our data and handles it appropriately,
    while allowing us to use explanatory variables down to the call level.^([2](ch10.xhtml#ch01fn23))
    For our purposes, we won’t get into statistical details and we’ll only see how
    to run the corresponding code and interpret the results. A hierarchical model
    is a general framework that can be applied to linear and logistic regression,
    so we’ll still be in known territory.'
  prefs: []
  type: TYPE_NORMAL
- en: Behavioral Logic
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Finally, the logic for success for this experiment is simple: the new SOP will
    make customers feel better during the interaction, which will translate into a
    higher measured CSAT ([Figure 10-1](#causal_logic_for_our_experiment)).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Causal logic for our experiment](Images/BEDA_1001.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-1\. Causal logic for our experiment
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Data and Packages
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The [GitHub folder for this chapter](https://oreil.ly/BehavioralDataAnalysisCh10)
    contains two CSV files with the variables listed in [Table 10-1](#variables_in_our_data-id00083).
    The check mark (✓) indicates the variables present in that file, while the cross
    (☓) indicates the variables that are not present.
  prefs: []
  type: TYPE_NORMAL
- en: Table 10-1\. Variables in our data
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Variable description | chap10-historical_data.csv | chap10-experimental_data.csv
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| *Center_ID* | Categorical variable for the 10 call centers | ✓ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| *Rep_ID* | Categorical variable for the 193 call center reps | ✓ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| *Age* | Age of customer calling, 20-60 | ✓ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| *Reason* | Reason for call, “payment”/“property” | ✓ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| *Call_CSAT* | Customer satisfaction with call, 0-10 | ✓ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| *Group* | Experimental assignment, “ctrl”/“treat” | ☓ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: Note that the two data sets also contain the binary variable *M6Spend*, the
    amount spent on subsequent bookings within six months of a given booking. This
    variable will be used in [Chapter 11](ch11.xhtml#introduction_to_moderation) only.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’ll use the following packages in addition to the common
    ones:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Introduction to Hierarchical Modeling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Hierarchical models (HMs) can be used when you have categorical variables in
    your data:'
  prefs: []
  type: TYPE_NORMAL
- en: Customer transactions across multiple stores
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rental properties across multiple states
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Etc.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some situations call for HMs because you can’t use traditional categorical variables.
    The main one is if you have a categorical variable that is dependent on another
    categorical variable (e.g., Vegetarian = {“yes,” “no”} and Flavor = {“ham,” “turkey,”
    “tofu,” “cheese”}), a.k.a. “nested” categorical variables. Then, multicollinearity
    issues make HMs the way to go. That’s also why they are called “hierarchical”
    models, even though they can be applied to non-nested categories as well.
  prefs: []
  type: TYPE_NORMAL
- en: Beyond that, HMs also offer a more robust alternative if you have a categorical
    variable with a large number of categories, such as the call center rep ID in
    our example, and especially if some of the categories have very few rows in your
    data. Without getting into too much detail, this robustness comes from the way
    coefficients in HMs incorporate some information from other rows, which brings
    them closer to the overall average. Let’s imagine that we had in our data a call
    center rep having answered only one call, with an exceptionally bad CSAT that
    is clearly an outlier. With only one call for that rep, we don’t know whether
    the rep or the call is the outlier. A categorical variable would assign 100% of
    the “outlier-ness” to the rep, whereas an HM would split it between the rep and
    the call, i.e., we would expect the rep to have lower-than-average CSAT with other
    calls, but not as extreme as the observed call.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, in situations where both categorical variables and HMs could be applied
    (which is basically any situation where you have a categorical variable with a
    few, non-nested, categories!), there are some nuances in interpretation that may
    make you prefer one or the other. Conceptually, a categorical variable is a partition
    of your data into groups with intrinsic differences between them that we want
    to understand, whereas an HM treats groups as random draws from a potentially
    infinite distribution of groups. AirCnC has 30 call centers, but it could have
    been 10 or 50 instead, and we’re not interested in the differences between call
    center number 3 and call center number 28\. On the other hand, we’d like to know
    whether calls for payment reasons have a higher or lower average CSAT than calls
    related to property issues, and we wouldn’t be satisfied with just knowing that
    the standard deviation between groups is 0.3\. But again, these are nuances of
    interpretation, so don’t think too much about it.
  prefs: []
  type: TYPE_NORMAL
- en: R Code
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s review the syntax for hierarchical modeling in a simple context, by looking
    at the determinants of call CSAT in our historical data, leaving the *Rep_ID*
    variable aside for now. The R code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The `lmer``()` function has a similar syntax to the traditional `lm``()` function,
    with one exception: we need to enter the clustering variable, here `center_ID`,
    between parentheses and preceded by `1|`. This allows the intercept of our regression
    to vary from one call center to another. Therefore, we have one coefficient for
    each call center; you can think of these coefficients as similar to the coefficients
    we would get in a standard linear regression with a dummy for each call center.^([3](ch10.xhtml#ch01fn24))'
  prefs: []
  type: TYPE_NORMAL
- en: 'The “Random effects” section of the results refers to the clustering variable(s).
    The coefficients for each call center ID are not displayed in the summary results
    (they can be accessed with the command `coef(hlm_mod)`). Instead, we get measures
    of the variability of our data within call centers and between call centers, in
    the form of variance and standard deviation. Here, the standard deviation of our
    data between call centers is 1.185; in other words, if we were to calculate the
    mean CSAT for each call center and then calculate the standard deviation of the
    means, we would get the same value as you can check for yourself:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The standard deviation of the residuals, here 1.059, indicates how much variability
    there is left in our data after accounting for the effect of call centers. Comparing
    the two standard deviations, we can see that the call center effects represent
    more than half of the variability in our data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The “Fixed effects” section of the results should look familiar: it indicates
    the coefficients for the call level variables. Here, we can see that customers
    calling for a “property” issue have on average a CSAT 0.199 higher than customers
    calling for a “payment” issue, and that each year of additional age for our customers
    adds on average 0.020 to the call CSAT.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s then include the `r``ep_ID` variable as a clustering variable nested
    under the `center_ID` variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, this is done by adding `rep_ID` as a clustering variable after
    `center_ID`, separating them with `/`. Also note that I was getting a warning
    that the model had failed to converge, so I changed the optimizer algorithm to
    "`Nelder_Mead"`.^([4](ch10.xhtml#ch01fn25)) The coefficients for the fixed effects
    are slightly different, but not that much.
  prefs: []
  type: TYPE_NORMAL
- en: Python Code
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Though it’s more concise, Python code works similarly. The main difference
    is that the groups are expressed with `groups = hist_data_df["center_ID"]`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The coefficients for the fixed effects (i.e., the intercept, the reason for
    the call and age) are identical to the R code. The coefficient for the variance
    of the random effect is expressed at the bottom of the fixed effects. At 1.122,
    it’s slightly different from the R value, due to differences in algorithms, but
    it won’t affect the coefficients we care about.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using nested clustering variables also has a different syntax in Python. We
    need to express the lower-level, nested, variable in a separate formula (the “variance
    components formula,” which I abbreviated as `vcf`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The syntax for the variance components formula is a bit esoteric but the intuition
    is straightforward. The formula itself is a dictionary with each of the nested
    variables as key. The value attached to each key indicates whether we want that
    variable to have a random intercept or a random slope (random here means “varying
    by category”). A random intercept is the HM equivalent of a *categorical* variable
    and is expressed as `"0+C(var)"`, where `var` is the name of the nested variable,
    i.e., the same as the key. Random slopes are beyond the scope of this book, but
    for example if you wanted the relationship between age and call satisfaction to
    have a different slope for each rep, the variance component formula would be `vcf
    = {"rep_ID": "0+C(rep_ID)", "age":"0+age"}`, without a `C()` in the second case.'
  prefs: []
  type: TYPE_NORMAL
- en: Determining Random Assignment and Sample Size/Power
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have planned the qualitative aspects of our experiment, we need
    to determine the random assignment we’ll use as well as our sample size and power.
    In our two previous experiments ([Chapter 8](ch08.xhtml#experimental_design_the_basics)
    and [Chapter 9](ch09.xhtml#stratified_randomizatio)), we had some target effect
    size and statistical power, and we chose our sample size accordingly. Here, we’ll
    add a wrinkle by assuming that our business partners are willing to run the experiment
    only for a month,^([5](ch10.xhtml#ch01fn26)) and the minimum detectable effect
    they’re interested in capturing is 0.6 (i.e., they want to make sure that you
    have sufficient power to capture an effect of that size, but they’re willing to
    take the risk that the effect size will be lower).
  prefs: []
  type: TYPE_NORMAL
- en: 'Under these constraints, the question becomes: how much power do we have to
    capture a difference of that amount with that sample? In other words, assuming
    that the difference is indeed equal to 0.6, what is the probability that our decision
    rule will conclude that the treatment is indeed better than the control?'
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned earlier, we’ll be using a hierarchical regression to analyze our
    data and that will complicate our power analysis a bit, but let’s first briefly
    review the process for random assignment.
  prefs: []
  type: TYPE_NORMAL
- en: Random Assignment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Even though we don’t know ahead of time which customers are going to call, it
    doesn’t matter for the random assignment because we’ll do it at the call center
    level. Therefore, we can do it in advance, assigning control and treatment groups
    all at once. With clustered experiments like this one, stratification is especially
    useful because we have so few actual units to randomize. Here, we’re randomizing
    at the level of call centers, so we would want to stratify based on the centers’
    characteristics, such as number of reps and average values of the call metrics.
    The code to do so is a straightforward version of the code in [Chapter 9](ch09.xhtml#stratified_randomizatio),
    split between a data prep function and a wrapper for the blocking function ([Example 10-1](#stratified_random_assignment_of_call_centers)).
  prefs: []
  type: TYPE_NORMAL
- en: Example 10-1\. Stratified random assignment of call centers
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#comarker101)'
  prefs: []
  type: TYPE_NORMAL
- en: 'We group by `center_ID` and summarize our clustering variable: we take the
    number of reps by center, calculate the average call CSAT and customer age, and
    determine the percentage of calls whose reason is ''`payment''`.'
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#comarker102)'
  prefs: []
  type: TYPE_NORMAL
- en: We rescale all clustering variables to between 0 and 1.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](Images/3.png)](#comarker103)'
  prefs: []
  type: TYPE_NORMAL
- en: We use the `block()` function from `blockTools`, using the '`optimal'` algorithm
    from the `nbpMatching` package (we can afford the extra computation with so few
    call centers).
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](Images/4.png)](#comarker104)'
  prefs: []
  type: TYPE_NORMAL
- en: We extract the pairing from the output of `block()`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The resulting pairing is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'As mentioned in the previous chapter, there’s no equivalent to the `block`
    package in Python, so we’ll use the two functions for that purpose I have described
    in the previous chapter, with minor adjustments (e.g., we don’t have categorical
    variables at the center level, so we don’t need to one-hot encode them):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Power Analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Using a standard statistical formula for power analysis (in this case it would
    be the formula for the T-test) would be highly misleading because it would not
    take into account the correlation that exists in the data. Gelman and Hill (2006)
    provide some specific statistical formulas for hierarchical models, but I don’t
    want to go down the rabbit hole of accumulating increasingly complex and narrow
    formulas. As usual, we’ll be running simulations as our foolproof approach to
    power analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s first define our metric function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: This function returns the coefficient for the treatment group from our hierarchical
    model. As we did in the previous chapters, let’s now run the simulations for our
    power analysis, which you should hopefully be familiar with by now. The only additional
    thing we need to take into account here is that our data is stratified, a.k.a.
    clustered. This has two implications.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we can’t just draw calls from historical data at random. In our experiment,
    we expect reps to have almost exactly the same number of calls each; on the other
    hand, a truly random draw would generate some significant variation in the number
    of calls per rep. We expect reps to handle around 1,200 calls a month; having
    one rep handle 1,000 calls and another handle 1,400 is much more likely with a
    truly random draw than in reality. Fortunately, from a programming standpoint,
    this can easily be resolved by grouping our historical data at the call center
    and rep level before making a random draw:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Using permutations when randomness is “limited”
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The second implication is at the statistical level and is more profound. We’re
    using stratification to pair similar call centers and assign one from each pair
    to the control group and the other to the treatment group. This is good, because
    we reduce the risk that some call center characteristics will bias our analysis.
    But at the same time, this introduces a fixed effect in our simulations: let’s
    say that call centers 1 and 5 are paired together because they’re very similar.
    Then, however many simulations we run, one of them will be in the control group
    and the other one will be in the treatment group; we’ve reduced the total number
    of possible combinations. With a completely free randomization, there are 10!/(5!
    * 5!) ≈ 252 different assignments of 10 call centers in equally sized experimental
    groups, which is already not that many.^([6](ch10.xhtml#ch01fn27)) With stratification,
    there are only 2^5 ≈ 32 different assignments, because there are two possible
    assignments for each of the five pairs: (control, treatment) and (treatment, control).
    This means that even if you were to run 32,000 simulations, you would only see
    32 different random allocations at the call center level. Moreover, with only
    three months worth of historical data, we can only generate three completely different
    (i.e., mutually exclusive) samples per rep, for a total of 32 * 3 = 96 different
    simulations.'
  prefs: []
  type: TYPE_NORMAL
- en: This doesn’t mean that we should not use stratification; on the contrary, stratification
    is even more crucial the smaller our experimental population gets! This does imply
    however that it is pretty much pointless and potentially misleading to run many
    more simulations than you have truly different assignments.
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand why, let’s use a metaphor: imagine a student who decides to increase
    their vocabulary ahead of a test (e.g., the LSAT). They buy a learner’s dictionary
    and plan to read the definition of a random word in it ten times a day until they’ve
    done it a thousand times, to learn a thousand words. But here’s the catch: their
    dictionary has only 96 words in it! This means that however many times the student
    looks up a random word, their vocabulary cannot increase by more than 96 words.
    There’s certainly value in reading a word’s definition more than once, to better
    understand and memorize it, but that’s different from reading the definition of
    more words. This also means that looking at definitions at random is a very inefficient
    way to proceed. It is much better to simply go through the 96 words in order.'
  prefs: []
  type: TYPE_NORMAL
- en: 'That logic applies in the same way to simulations: we usually draw at random
    from our historical data to build a simulated experimental data set, and we (correctly)
    treat as negligible the probability of several simulations being identical. In
    the present case, if we had a hundred call centers, each with a thousand reps
    and ten years of data, we could confidently simulate hundreds, or even thousands,
    of experiments without worrying. With our limited number of call centers and reps,
    we’re better off going systematically through the limited number of possibilities.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see how to do this in code. We have the call-center pairings (see [Figure 10-2](#ninezeropercent_confidence_intervals_wi)
    in the previous subsection) and we need to go through the 32 possible permutations
    of those pairs. The first pair is made up of call centers #7 and #2, so half of
    the simulations will have #7 in control group and #2 in treatment group, while
    the other half will have #2 in control group and #7 in the treatment group, and
    so on. So the first simulation might have as control group the call centers (7,
    9, 3, 10, 4) while the second simulation has as control group (2, 9, 3, 10, 4).'
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll use a trick to help us go through the permutations easily. It is not
    really complex, but it relies on properties of binary numbers that are not intuitive,
    so brace yourself and bear with me. Any integer can be expressed in binary base
    as a sequence of zeros and ones. 0 is 0, 1 is 1, 2 is 10, 3 is 11, and so on.
    These can be left-padded with zeros to have a constant number of digits. We want
    the number of digits to be equal to the number of pairs, here 5\. This means that
    0 is 00000, 1 is 00001, 2 is 00010, and 3 is 00011\. The largest integer we can
    express with 5 digits is 31\. Note that, and this is not a coincidence, including
    0 as 00000, we can express 32 different integers with 5 binary digits, and that
    32 is the number of permutations we want to implement. Therefore, we can decide
    that the first simulation, which we’ll call “simulation 00000,” has as control
    group (7, 9, 3, 10, 4) from [Figure 10-2](#ninezeropercent_confidence_intervals_wi).
    From there, we’ll swap a pair between the control and the treatment groups whenever
    the digit corresponding to the pair in the binary form of the simulation number
    is a 1\. So for example, for simulation 10000, we would swap call centers #7 and
    #2, giving us the control group (2, 9, 3, 10, 4). Here’s where the magic happens:
    by going from 00000 to 11111, we’ll see all the possible permutations of the 5
    pairs!'
  prefs: []
  type: TYPE_NORMAL
- en: Code for permutations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Because of the differences in indexing between Python and R (the former starting
    at 0 and the latter at 1), the code is a bit simpler in Python, so let’s start
    with the corresponding snippet of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#comarker1001)'
  prefs: []
  type: TYPE_NORMAL
- en: We convert the permutation counter `perm` to a binary string. In Python, there
    are several ways to do it. I did it here with an F-string. The syntax of an F-string
    is `f'{exp}'`, where the expression `exp` is evaluated before getting formatted
    as a string. Within the expression, `Npairs` is also between curly braces, so
    it’s evaluated first before being passed to the expression; after that first evaluation,
    `exp` is equal to `perm:05b`. The first term on the left of the colon is the number
    to format; the letter after the colon indicates the format to use, here `b` for
    binary; the number immediately to the left of the letter indicates the total number
    of digits to use (here 5); and finally, any character to the left of that number
    is to be used for padding (here 0).
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#comarker1002)'
  prefs: []
  type: TYPE_NORMAL
- en: We match the digits of the binary string with a counter for the pairs within
    the `idx` matrix. So “00000” becomes <math><mrow><mrow><mo>(</mo><mtable><mtr><mtd><mn>0</mn></mtd><mtd><mn>0</mn></mtd></mtr><mtr><mtd><mn>1</mn></mtd><mtd><mn>0</mn></mtd></mtr><mtr><mtd><mn>2</mn></mtd><mtd><mn>0</mn></mtd></mtr><mtr><mtd><mn>3</mn></mtd><mtd><mn>0</mn></mtd></mtr><mtr><mtd><mn>4</mn></mtd><mtd><mn>0</mn></mtd></mtr></mtable><mo>)</mo></mrow></mrow></math>
    in Python after transposing.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](Images/3.png)](#comarker1003)'
  prefs: []
  type: TYPE_NORMAL
- en: We pass the rows of `idx` as indices to indicate which element of each pair
    goes into the treatment group. That is, to indicate that the first element of
    the first pair should go into the treatment group, we pass [0, 0]. With 00000,
    we always put the first element of each pair in the treatment group. With the
    last permutation, 11111, we put the second element of each pair in the treatment
    group, mirroring the allocation for 00000\. Taking a more complicated example,
    for permutation number 7, whose binary format is 00111, we would put in the control
    group the first element for the first two pairs, and the second element for the
    last three pairs.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](Images/4.png)](#comarker1004)'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we update our simulated experimental data set, assigning each row to
    either the control or treatment group based on its center ID.
  prefs: []
  type: TYPE_NORMAL
- en: 'The process is identical in R with a few differences in syntax:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#comarker10001)'
  prefs: []
  type: TYPE_NORMAL
- en: Converting `perm` to a binary format is done in R with the `as.binary()` function,
    which takes as first argument the number to convert and as second argument the
    total number of digits we want (i.e., the number of pairs, here 5).
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#comarker10002)'
  prefs: []
  type: TYPE_NORMAL
- en: Because the indexing starts with 1 and not 0 in R, we need to add 1 to all the
    elements of the second column in the `idx` matrix. Thus, for the first permutation,
    00000, where the first element of each pair goes into the control group, the `idx`
    matrix is <math><mrow><mrow><mo>(</mo><mtable><mtr><mtd><mn>1</mn></mtd><mtd><mn>1</mn></mtd></mtr><mtr><mtd><mn>2</mn></mtd><mtd><mn>1</mn></mtd></mtr><mtr><mtd><mn>3</mn></mtd><mtd><mn>1</mn></mtd></mtr><mtr><mtd><mn>4</mn></mtd><mtd><mn>1</mn></mtd></mtr><mtr><mtd><mn>5</mn></mtd><mtd><mn>1</mn></mtd></mtr></mtable><mo>)</mo></mrow></mrow></math>.
    For permutation 11111, the second column would be made up of 2s, and for 00111
    it would be 11222.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](Images/3.png)](#comarker10003)'
  prefs: []
  type: TYPE_NORMAL
- en: We pass the rows of `idx` as indices to indicate which element of each pair
    goes into the treatment group.
  prefs: []
  type: TYPE_NORMAL
- en: The `permutation_gen_fun()` function returns a list of the center IDs for the
    treatment group, which can then be used in the random assignment function.
  prefs: []
  type: TYPE_NORMAL
- en: Power curve
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that we have a solution to the problem of limited possible samples, we can
    get back to our power analysis. Remember that business partners want to run the
    experiment for no longer than a month, meaning a sample size of about 230,000
    calls. Instead of calculating the required sample size for the threshold value
    of 0.6 points of CSAT and the desired power, we need to take the sample size as
    given and calculate what power we have for this threshold value.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s first look at statistical significance. Remember that in the previous
    chapter, our estimator was “underconfident”: the 90%-CI included zero more than
    90% of the time. Even using a 40%-CI led only to a small number of false positives.
    Here, we have the opposite problem: our estimator is “overconfident” as the 90%-CI
    includes zero much less than 90% of the time, and indeed it never includes it:
    our coverage is null. [Figure 10-2](#ninezeropercent_confidence_intervals_wi)
    shows the 96 confidence intervals ranked from lowest to highest.'
  prefs: []
  type: TYPE_NORMAL
- en: '![90% confidence intervals with no effect](Images/BEDA_1002.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-2\. 90% confidence intervals with no effect
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The situation we can see in [Figure 10-3](#power_curve_with_decision_threshold_of)
    is similar to what we saw in [Chapter 7](ch07.xhtml#measuring_uncertainty_with_the_bootstra),
    where having very limited data led to discontinuities in our graphs. Here, the
    random errors never quite line up in the way that would result in a CI including
    zero. Instead, we have four tight clusters of CIs, even though the distribution
    of our CIs is symmetric around zero (i.e., our estimator is unbiased) and half
    of them are very close to it. From a practical perspective, that means that if
    we run our experiment, we shouldn’t expect the true value to be included in our
    CI.
  prefs: []
  type: TYPE_NORMAL
- en: 'This doesn’t mean that our experiment is doomed, but that we shouldn’t trust
    our CI bounds and we should rely on our decision rule instead. With the default
    decision rule of accepting any CI that is strictly positive, our significance
    is 50%: because half of our CIs are below zero and half above, in half of the
    cases we would observe a negative coefficient and rightly conclude that the treatment
    group is no better than the control group. [Figure 10-3](#power_curve_with_decision_threshold_of)
    plots the power curve with this decision rule for different effect sizes.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Power curve with decision threshold of 0 for different effect sizes](Images/BEDA_1003.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-3\. Power curve with decision threshold of 0 for different effect
    sizes
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As you can see, our power reaches 75% very quickly, basically as soon as the
    cluster of CIs that was just below zero gets shifted just above it. After that,
    our power remains constant for a range of values including our threshold effect
    size of 0.6, until the cluster of strongly negative CIs gets shifted above zero
    in turn. Then our power is close to 100% for effect sizes of 1 or above. That
    is, if the true effect is 1 or higher, we’re extremely unlikely to see a negative
    CI.
  prefs: []
  type: TYPE_NORMAL
- en: We could go back to our business partners and tell them that our CIs are unreliable
    and therefore our risk of false positives is large, but our risk of false negatives
    is very low. In the present case, we can do better by setting a more stringent
    decision rule and implementing the intervention only if we observe an effect size
    of 0.25 or above. [Figure 10-4](#power_curve_for_different_effect_sizes) shows
    the power curve for that decision rule.
  prefs: []
  type: TYPE_NORMAL
- en: '![Power curve for different effect sizes with decision threshold of 0.25](Images/BEDA_1004.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-4\. Power curve for different effect sizes with decision threshold
    of 0.25
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As we can see in [Figure 10-4](#power_curve_for_different_effect_sizes), by
    increasing our decision threshold, we’ve lowered the left side of our power curve.
    This implies a lower significance (i.e., lower risk of false positives) at the
    cost of a lower power (i.e., higher risk of false negatives) for small effect
    sizes. However, the right side of our power curve remains mostly unchanged, meaning
    that our power to detect an effect of 0.6 remains at 75%.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s recap what our power analysis told us. Because we plan to use a stratified
    random assignment with a limited number of effective experimental units (i.e.,
    call centers), our experiment has a rigid structure that constrains possible outcomes.
    This makes our CIs unreliable by themselves. However, we can adjust our decision
    rule to a higher threshold (i.e., we’ll implement our intervention only if we
    observe an effect of 0.25 or higher). By doing so, we can reduce the risk of false
    positives for a null effect size while keeping our power for the target effect
    size high enough. This remains an underpowered experiment but this is the best
    we can offer as experimentalists, and our business partners will have to decide
    how they feel about these odds.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note the difference between our decision threshold, 0.25, and our target effect,
    0.6\. By definition, the power at the decision threshold is always 0.5 and we
    set out to get as much power as possible for an effect size of 0.6.
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing the Experiment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Once we have run our experiment, we can collect and analyze the data. Having
    defined our metric function previously, the analysis is now as simple as applying
    it to our experimental data, then obtaining a Bootstrap 90%-CI of its value for
    our experimental data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Our confidence interval is very narrow and squarely above 0.25\. Based on our
    power analysis, the true effect size is unlikely to be actually within that CI,
    but it is as likely to be lower as it is to be higher, so our expected effect
    size is equal to 0.48\. Because that is above our decision threshold, we would
    implement the intervention, even though the expected effect size is less than
    our target. Interestingly, that confidence interval is much smaller than the one
    we would obtain based on the normal approximation (i.e., coefficient +/− 1.96
    * coefficient standard error), in part because of the stratified randomization.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This concludes our tour of experimental design. In the last part of the book,
    we’ll see advanced tools that will allow us to dig deeper in the analysis of experimental
    data, but the call center experiment we’ve just seen is about as complex as experiments
    get in real life. Being unable to randomize at the lowest level and having a predetermined
    amount of time to run an experiment are unpleasant but not infrequent circumstances.
    Randomizing at the level of an office or a store instead of customers or employees
    is common, to avoid logistical complications and “leakage” between experimental
    groups. Leveraging simulations for power analysis and stratification for random
    assignment becomes pretty much unavoidable if you want to get useful results out
    of your experiment; hopefully you should now be fully equipped to do so.
  prefs: []
  type: TYPE_NORMAL
- en: Designing and running experiments is in my opinion one of the most fun parts
    of behavioral science. When everything goes well, you get to measure with clarity
    the impact of a business initiative or a behavioral science intervention. But
    getting everything to go well is no small feat in itself. Popular media and business
    vendors often feed the impression that experimentation can be as simple as “plug
    and play, check for 5% significance and you’re done!” but this is misleading,
    and I’ve tried to address several misconceptions that come out of this.
  prefs: []
  type: TYPE_NORMAL
- en: First of all, statistical significance and power are often misunderstood, which
    can lead to wasted experiments and suboptimal decisions. I believe that eschewing
    p-values in favor of Bootstrap confidence intervals leads to results and interpretations
    that are both more correct and more relevant to applied settings.
  prefs: []
  type: TYPE_NORMAL
- en: Second, treating experiments as a pure technology and data analysis problem
    is easier but less fruitful than adopting a causal-behavioral approach. Using
    causal diagrams allows you to articulate more clearly what would be a success
    and what makes you believe your treatment would be successful.
  prefs: []
  type: TYPE_NORMAL
- en: 'Implementing an experiment in the field is fraught with difficulties (see [Bibliography](bibliography01.xhtml#bibliography)
    for further resources), and unfortunately each experiment is different, therefore
    I can only give you some generic advice:'
  prefs: []
  type: TYPE_NORMAL
- en: Running field experiments is an art and science, and nothing can replace experience
    with a specific context. Start with smaller and simpler experiments at first.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Start by implementing the treatment on a small pilot group that you then observe
    for a little while and extensively debrief. This will allow you to ensure as much
    as possible that people understand the treatment and apply it somewhat correctly
    and consistently.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Try to imagine all the ways things could go wrong and to prevent them from happening.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recognize that things will go wrong nonetheless, and build flexibility into
    your experiment (e.g., plan for “buffers” of time, because things will take longer
    than you think—people might take a week to settle into implementing the treatment
    correctly, data might come in late, etc.).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ^([1](ch10.xhtml#ch01fn22-marker)) “‘Sorry’ Is Not Enough,” *Harvard Business
    Review*, Jan.–Feb. 2018.
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch10.xhtml#ch01fn23-marker)) If you want to learn more about this type
    of models, Gelman and Hill (2006) is the classic reference on the topic.
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](ch10.xhtml#ch01fn24-marker)) If you really want to know, these coefficients
    are calculated as a weighted average of the mean CSAT in a call center and the
    mean CSAT across our whole data.
  prefs: []
  type: TYPE_NORMAL
- en: '^([4](ch10.xhtml#ch01fn25-marker)) As always with numerical simulations, your
    mileage may vary. Thanks to Jessica Jakubowski for suggesting an alternative specification:
    `lmerControl(optimizer ="bobyqa", optCtrl=list(maxfun=2e5))`.'
  prefs: []
  type: TYPE_NORMAL
- en: ^([5](ch10.xhtml#ch01fn26-marker)) Does that suck for your experimental design?
    Totally. Is that unrealistic? Absolutely not, unfortunately. As we used to say
    when I was a consultant, the client is always the client.
  prefs: []
  type: TYPE_NORMAL
- en: ^([6](ch10.xhtml#ch01fn27-marker)) The exclamation mark indicates the mathematical
    operator factorial. See [this Wikipedia page](https://oreil.ly/I5PTW) if you want
    to better understand the underlying math.
  prefs: []
  type: TYPE_NORMAL
