- en: Chapter 10\. Cluster Randomization and Hierarchical Modeling
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Our last experiment, while conceptually simple, will illustrate some of the
    logistical and statistical difficulties of experimenting in business. AirCnC has
    10 customer call centers spread across the country, where representatives handle
    any issue that might come up in the course of a booking (e.g., the payment did
    not go through, the property doesn’t look like the pictures, etc.). Having read
    an article in the *Harvard Business Review* (HBR) about customer service,^([1](ch10.xhtml#ch01fn22))
    the VP of customer service has decided to implement a change in standard operating
    procedures (SOP): instead of apologizing repeatedly when something went wrong,
    the call center reps should apologize at the beginning of the interaction, then
    get into “problem-solving mode,” then end up offering several options to the customer.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: 'This experiment presents multiple challenges: due to logistical constraints,
    we’ll be able to randomize treatment only at the level of call centers and not
    reps, and we’ll have difficulties enforcing and measuring compliance. This certainly
    doesn’t mean that we can’t or shouldn’t run an experiment!'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Regarding the randomization constraint, we’ll see that this makes the standard
    linear regression algorithm inappropriate and that we should use hierarchical
    linear modeling (HLM) instead.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: 'As before, our approach will be:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: Planning the experiment
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Determining random assignment and sample size/power
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analyzing the experiment
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Planning the Experiment
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, I’ll go briskly through our theory of change to provide you
    with some necessary context and behavioral grounding:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: First, the business goal and target metric
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, the definition of our intervention
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, the behavioral logic that connects them
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Business Goal and Target Metric
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Based on the HBR article, our criterion for success or target metric appears
    straightforward: customer satisfaction as measured by a one-question survey administered
    by email after the phone call. However, we’ll see in a minute that there are complications,
    so we’ll need to revisit it after discussing what we’re testing.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: Definition of the Intervention
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The treatment we’re testing will be whether the reps have been trained in the
    new SOP and instructed to implement it.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: 'The first difficulty is in the implementation of the treatment. We know from
    past experience that asking the reps to apply different SOPs to different customers
    is very challenging: asking reps to switch processes at random between calls increases
    their cognitive load and the risk of noncompliance. Therefore, we’ll have to train
    some reps and instruct them to use the new SOP for all of their calls, while keeping
    other reps on the old SOP.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: 'Even with that correction, compliance remains at risk: reps in the treatment
    group may implement the new SOP inconsistently or even not at all, while reps
    in the control group may also apply the old SOP inconsistently. Obviously, this
    would muddle our analysis and make the treatment appear less different from the
    control group than it is. One way to mitigate this issue is to first observe current
    compliance with the SOP in place by listening to calls, then run a pilot study,
    where we select a few reps, train them, and observe compliance with the new SOP.
    Debriefing the reps in the pilot study after the fact can help identify misunderstandings
    and obstacles to compliance. Unfortunately, it is generally impossible to have
    100% compliance in an experiment where human beings are delivering or choosing
    the treatment. The best we can do is to try to measure compliance and take it
    into account when drawing conclusions.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 即使做出这种修正，遵从仍然面临风险：治疗组的代表可能不一致地实施新的SOP，甚至根本不实施，而控制组的代表也可能不一致地应用旧的SOP。显然，这将混淆我们的分析，并使治疗方法看起来与控制组的不同性不那么明显。减轻这个问题的一种方法是首先观察当前SOP的遵从情况，通过听取呼叫来运行试点研究，选择几个代表，对他们进行培训，并观察对新SOP的遵从情况。事后与试点研究中的代表进行总结讨论可以帮助识别误解和遵从的障碍。不幸的是，在人类进行治疗交付或选择的实验中，通常无法实现100%的遵从。我们能做的最好努力是尽量测量遵从情况，并在得出结论时加以考虑。
- en: 'Finally, there is a risk of “leakage” between our control and our treatment
    groups. Reps are human beings, and reps in a given call center interact and chat.
    Given that reps are incentivized on the average monthly customer satisfaction
    (CSAT) for their calls, if reps in the treatment group started seeing significantly
    better results, there is a risk that reps in the control group of the same call
    center would start changing their procedure. Having some people in the control
    group apply the treatment would muddle the comparison for the two groups and make
    the difference appear smaller than it really is. Therefore, we’ll apply the treatment
    at the call center level: all reps in a given call center will either be in the
    treatment group or in the control group.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们存在“泄漏”的风险，即我们的控制组和治疗组之间的“泄漏”。代表是人类，同一呼叫中心的代表之间会互动和交流。鉴于代表受到月平均客户满意度（CSAT）的激励，如果治疗组的代表开始看到显著更好的结果，那么同一呼叫中心的控制组的代表可能会开始改变他们的程序。让控制组的一些人应用治疗方法将混淆两组的比较，并使差异看起来比实际小。因此，我们将在呼叫中心级别应用治疗方法：同一呼叫中心的所有代表将分为治疗组或控制组。
- en: 'Applying the treatment at the call center level instead of at the call level
    has implications for our criterion for success. If our unit of randomization is
    the call center, should we measure the CSAT at the call center level? This would
    seem logical, but it would mean that we can’t use any information about individual
    reps or individual calls. On the other hand, measuring average CSAT at the rep
    level or even CSAT at the call level would allow us to use more information, but
    it is problematic for two reasons:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 应用在呼叫中心级别而不是在呼叫级别上的治疗对我们的成功标准有影响。如果我们的随机化单位是呼叫中心，那么我们应该在呼叫中心级别衡量CSAT吗？这看起来似乎是合理的，但这意味着我们不能使用任何关于个别代表或个别呼叫的信息。另一方面，衡量代表级别的平均CSAT甚至呼叫级别的CSAT将允许我们使用更多信息，但有两个问题：
- en: First, if we were to disregard the fact that randomization was not done at the
    call level and use standard power analysis, our results would be biased because
    randomization is unavoidably correlated with the call center variable; adding
    more calls in our sample would not change the fact that we have only 10 call centers
    and therefore only 10 randomization units.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，如果我们忽略了随机化不是在呼叫级别进行的事实，并使用标准功效分析，我们的结果将存在偏差，因为随机化与呼叫中心变量不可避免地相关；增加样本中的更多呼叫不会改变我们只有10个呼叫中心，因此只有10个随机化单位的事实。
- en: 'Second, in our data analysis, we would run into trouble due to the nested nature
    of the data: assuming that each rep belongs to one and only one call center, there
    will be multicollinearity between our call center variable and our rep variable
    (e.g., we can add 1 to the coefficient for the first call center and subtract
    1 from the coefficients for all the reps in that call center without changing
    the results of the regression; therefore the coefficients for the regression are
    essentially undetermined).'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其次，在我们的数据分析中，由于数据的嵌套结构，我们会遇到麻烦：假设每个代表只属于一个呼叫中心，我们的呼叫中心变量和代表变量之间会存在多重共线性（例如，我们可以为第一个呼叫中心的系数加1，并在所有该呼叫中心的代表的系数中减去1，而不改变回归的结果；因此回归的系数基本上是不确定的）。
- en: 'Fortunately, there is a simple solution to this problem: we’ll use a hierarchical
    model, which recognizes the nested structure of our data and handles it appropriately,
    while allowing us to use explanatory variables down to the call level.^([2](ch10.xhtml#ch01fn23))
    For our purposes, we won’t get into statistical details and we’ll only see how
    to run the corresponding code and interpret the results. A hierarchical model
    is a general framework that can be applied to linear and logistic regression,
    so we’ll still be in known territory.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，这个问题有一个简单的解决方案：我们将使用分层模型，它识别我们数据的嵌套结构并适当处理，同时允许我们使用直到呼叫级别的解释变量。^([2](ch10.xhtml#ch01fn23))
    对于我们的目的，我们不会深入统计细节，只会看如何运行相应的代码和解释结果。分层模型是一个通用框架，可应用于线性和逻辑回归，因此我们仍然处于已知领域内。
- en: Behavioral Logic
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 行为逻辑
- en: 'Finally, the logic for success for this experiment is simple: the new SOP will
    make customers feel better during the interaction, which will translate into a
    higher measured CSAT ([Figure 10-1](#causal_logic_for_our_experiment)).'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，这次实验成功的逻辑很简单：新的SOP将使客户在互动过程中感觉更好，这将转化为更高的测量CSAT（[图10-1](#causal_logic_for_our_experiment)）。
- en: '![Causal logic for our experiment](Images/BEDA_1001.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![我们实验的因果逻辑](Images/BEDA_1001.png)'
- en: Figure 10-1\. Causal logic for our experiment
  id: totrans-27
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-1. 我们实验的因果逻辑
- en: Data and Packages
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据和包
- en: The [GitHub folder for this chapter](https://oreil.ly/BehavioralDataAnalysisCh10)
    contains two CSV files with the variables listed in [Table 10-1](#variables_in_our_data-id00083).
    The check mark (✓) indicates the variables present in that file, while the cross
    (☓) indicates the variables that are not present.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '[本章的GitHub文件夹](https://oreil.ly/BehavioralDataAnalysisCh10)包含两个CSV文件，列出了[表格10-1](#variables_in_our_data-id00083)中的变量。勾号（✓）表示该文件中存在的变量，而叉号（☓）表示不存在的变量。'
- en: Table 10-1\. Variables in our data
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 表格10-1. 我们数据中的变量
- en: '|  | Variable description | chap10-historical_data.csv | chap10-experimental_data.csv
    |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '|  | 变量描述 | chap10-historical_data.csv | chap10-experimental_data.csv |'
- en: '| --- | --- | --- | --- |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| *Center_ID* | Categorical variable for the 10 call centers | ✓ | ✓ |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| *Center_ID* | 10个呼叫中心的分类变量 | ✓ | ✓ |'
- en: '| *Rep_ID* | Categorical variable for the 193 call center reps | ✓ | ✓ |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| *Rep_ID* | 193名呼叫中心代表的分类变量 | ✓ | ✓ |'
- en: '| *Age* | Age of customer calling, 20-60 | ✓ | ✓ |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| *Age* | 客户呼叫时的年龄，20-60 | ✓ | ✓ |'
- en: '| *Reason* | Reason for call, “payment”/“property” | ✓ | ✓ |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| *Reason* | 呼叫原因，“支付”/“物业” | ✓ | ✓ |'
- en: '| *Call_CSAT* | Customer satisfaction with call, 0-10 | ✓ | ✓ |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| *Call_CSAT* | 客户对通话的满意度，0-10 | ✓ | ✓ |'
- en: '| *Group* | Experimental assignment, “ctrl”/“treat” | ☓ | ✓ |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| *Group* | 实验分配，“对照组”/“处理组” | ☓ | ✓ |'
- en: Note that the two data sets also contain the binary variable *M6Spend*, the
    amount spent on subsequent bookings within six months of a given booking. This
    variable will be used in [Chapter 11](ch11.xhtml#introduction_to_moderation) only.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这两个数据集还包含二元变量*M6Spend*，即在给定预订后六个月内用于后续预订的金额。这个变量仅在[第11章](ch11.xhtml#introduction_to_moderation)中使用。
- en: 'In this chapter, we’ll use the following packages in addition to the common
    ones:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，除了常见的包外，我们还将使用以下包：
- en: '[PRE0]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Introduction to Hierarchical Modeling
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分层建模介绍
- en: 'Hierarchical models (HMs) can be used when you have categorical variables in
    your data:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 当您的数据中包含分类变量时，可以使用分层模型（HMs）：
- en: Customer transactions across multiple stores
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 跨多个商店的客户交易
- en: Rental properties across multiple states
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 跨多个州的租赁物业
- en: Etc.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 等等。
- en: Some situations call for HMs because you can’t use traditional categorical variables.
    The main one is if you have a categorical variable that is dependent on another
    categorical variable (e.g., Vegetarian = {“yes,” “no”} and Flavor = {“ham,” “turkey,”
    “tofu,” “cheese”}), a.k.a. “nested” categorical variables. Then, multicollinearity
    issues make HMs the way to go. That’s also why they are called “hierarchical”
    models, even though they can be applied to non-nested categories as well.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 有些情况需要使用 HMs，因为你无法使用传统的分类变量。其中一个主要情况是，如果你有一个分类变量依赖于另一个分类变量（例如，素食者 = {“是”，“否”}
    和 口味 = {“火腿”，“火鸡”，“豆腐”，“奶酪”}），即“嵌套”分类变量。然后，多重共线性问题使得使用 HMs 成为必要的方法。这也是为什么它们被称为“分层”模型，即使它们也可以应用于非嵌套的分类的原因之一。
- en: Beyond that, HMs also offer a more robust alternative if you have a categorical
    variable with a large number of categories, such as the call center rep ID in
    our example, and especially if some of the categories have very few rows in your
    data. Without getting into too much detail, this robustness comes from the way
    coefficients in HMs incorporate some information from other rows, which brings
    them closer to the overall average. Let’s imagine that we had in our data a call
    center rep having answered only one call, with an exceptionally bad CSAT that
    is clearly an outlier. With only one call for that rep, we don’t know whether
    the rep or the call is the outlier. A categorical variable would assign 100% of
    the “outlier-ness” to the rep, whereas an HM would split it between the rep and
    the call, i.e., we would expect the rep to have lower-than-average CSAT with other
    calls, but not as extreme as the observed call.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 除此之外，如果你有一个分类变量具有大量类别，比如我们示例中的呼叫中心代表 ID，特别是如果其中一些类别在你的数据中只有很少的行数，那么 HMs 也提供了一种更稳健的选择。不详细展开，这种稳健性来自
    HMs 中系数的方式，它们包含了来自其他行的一些信息，使它们更接近总体平均值。让我们想象一下，在我们的数据中，一个呼叫中心代表只回答了一个呼叫，其客户满意度（CSAT）异常糟糕。由于该代表只有一个呼叫，我们无法确定该代表还是呼叫才是异常值。分类变量会将“异常性”100%
    分配给代表，而 HM 会将其分配给代表和呼叫，即，我们期望该代表的 CSAT 与其他呼叫相比较低，但不像观察到的呼叫那样极端。
- en: Finally, in situations where both categorical variables and HMs could be applied
    (which is basically any situation where you have a categorical variable with a
    few, non-nested, categories!), there are some nuances in interpretation that may
    make you prefer one or the other. Conceptually, a categorical variable is a partition
    of your data into groups with intrinsic differences between them that we want
    to understand, whereas an HM treats groups as random draws from a potentially
    infinite distribution of groups. AirCnC has 30 call centers, but it could have
    been 10 or 50 instead, and we’re not interested in the differences between call
    center number 3 and call center number 28\. On the other hand, we’d like to know
    whether calls for payment reasons have a higher or lower average CSAT than calls
    related to property issues, and we wouldn’t be satisfied with just knowing that
    the standard deviation between groups is 0.3\. But again, these are nuances of
    interpretation, so don’t think too much about it.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在既可以应用分类变量又可以应用 HMs 的情况下（这基本上是任何你有几个非嵌套类别的分类变量的情况！），在解释上有一些细微差别可能会使你更喜欢其中之一。在概念上，分类变量是将数据分为具有内在差异的组，我们希望理解这些差异，而
    HM 将组视为从潜在的无限组分布中随机抽取的样本。AirCnC 有 30 个呼叫中心，但它也可以有 10 个或 50 个，我们并不关心第 3 个呼叫中心和第
    28 个呼叫中心之间的差异。另一方面，我们想知道与与物业问题相关的呼叫相比，与付款原因相关的呼叫的平均客户满意度是否更高或更低，我们不满意只知道组之间的标准差为
    0.3。但再说一遍，这些是解释上的细微差别，所以不要想得太多。
- en: R Code
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: R 代码
- en: 'Let’s review the syntax for hierarchical modeling in a simple context, by looking
    at the determinants of call CSAT in our historical data, leaving the *Rep_ID*
    variable aside for now. The R code is as follows:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们简单地回顾一下分层建模的语法，在一个简单的情境下，通过查看我们的历史数据中呼叫客户满意度的决定因素，暂时不考虑 *Rep_ID* 变量。R 代码如下：
- en: '[PRE2]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The `lmer``()` function has a similar syntax to the traditional `lm``()` function,
    with one exception: we need to enter the clustering variable, here `center_ID`,
    between parentheses and preceded by `1|`. This allows the intercept of our regression
    to vary from one call center to another. Therefore, we have one coefficient for
    each call center; you can think of these coefficients as similar to the coefficients
    we would get in a standard linear regression with a dummy for each call center.^([3](ch10.xhtml#ch01fn24))'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '`lmer()` 函数的语法与传统的 `lm()` 函数类似，唯一的例外是我们需要在括号中输入聚类变量，这里是 `center_ID`，并在其前加上
    `1|`。这允许我们的回归截距因呼叫中心而异。因此，我们有每个呼叫中心的一个系数；您可以将这些系数视为带有每个呼叫中心虚拟变量的标准线性回归中会获得的系数类似。'
- en: 'The “Random effects” section of the results refers to the clustering variable(s).
    The coefficients for each call center ID are not displayed in the summary results
    (they can be accessed with the command `coef(hlm_mod)`). Instead, we get measures
    of the variability of our data within call centers and between call centers, in
    the form of variance and standard deviation. Here, the standard deviation of our
    data between call centers is 1.185; in other words, if we were to calculate the
    mean CSAT for each call center and then calculate the standard deviation of the
    means, we would get the same value as you can check for yourself:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 结果的“随机效应”部分涉及聚类变量。每个呼叫中心 ID 的系数未在摘要结果中显示（可以通过命令 `coef(hlm_mod)` 访问）。相反，我们得到数据在呼叫中心内部和呼叫中心之间变异性的度量，以方差和标准偏差的形式。在这里，呼叫中心之间的数据标准偏差为
    1.185；换句话说，如果我们计算每个呼叫中心的 CSAT 均值，然后计算均值的标准偏差，我们将得到与您自己验证的相同值：
- en: '[PRE3]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The standard deviation of the residuals, here 1.059, indicates how much variability
    there is left in our data after accounting for the effect of call centers. Comparing
    the two standard deviations, we can see that the call center effects represent
    more than half of the variability in our data.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 残差的标准偏差，这里为 1.059，表示在考虑呼叫中心效应后，我们的数据中剩余的变异性有多大。比较两个标准偏差，我们可以看到呼叫中心效应占数据变异性的一半以上。
- en: 'The “Fixed effects” section of the results should look familiar: it indicates
    the coefficients for the call level variables. Here, we can see that customers
    calling for a “property” issue have on average a CSAT 0.199 higher than customers
    calling for a “payment” issue, and that each year of additional age for our customers
    adds on average 0.020 to the call CSAT.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 结果的“固定效应”部分应该看起来很熟悉：它指示了呼叫级别变量的系数。在这里，我们可以看到，打电话因“财产”问题的客户的平均 CSAT 比打电话因“支付”问题的客户高
    0.199，并且我们的客户每增加一岁，平均电话 CSAT 就增加 0.020。
- en: 'Let’s then include the `r``ep_ID` variable as a clustering variable nested
    under the `center_ID` variable:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，将 `rep_ID` 变量作为 `center_ID` 变量下嵌套的聚类变量加入：
- en: '[PRE4]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: As you can see, this is done by adding `rep_ID` as a clustering variable after
    `center_ID`, separating them with `/`. Also note that I was getting a warning
    that the model had failed to converge, so I changed the optimizer algorithm to
    "`Nelder_Mead"`.^([4](ch10.xhtml#ch01fn25)) The coefficients for the fixed effects
    are slightly different, but not that much.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，通过在 `center_ID` 后添加 `rep_ID` 作为聚类变量，用 `/` 分隔它们来实现。还请注意，我收到一个警告，即模型未能收敛，因此我将优化算法更改为
    "`Nelder_Mead`"。^([4](ch10.xhtml#ch01fn25)) 固定效应的系数略有不同，但差别不大。
- en: Python Code
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Python 代码
- en: 'Though it’s more concise, Python code works similarly. The main difference
    is that the groups are expressed with `groups = hist_data_df["center_ID"]`:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然更为简洁，Python 代码的工作方式类似。主要区别在于组别用 `groups = hist_data_df["center_ID"]` 表示：
- en: '[PRE5]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The coefficients for the fixed effects (i.e., the intercept, the reason for
    the call and age) are identical to the R code. The coefficient for the variance
    of the random effect is expressed at the bottom of the fixed effects. At 1.122,
    it’s slightly different from the R value, due to differences in algorithms, but
    it won’t affect the coefficients we care about.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 固定效应的系数（即截距、呼叫原因和年龄）与 R 代码中相同。随机效应方差的系数在固定效应的底部表达。在 1.122 处，与 R 值略有不同，这是由于算法的差异，但不会影响我们关心的系数。
- en: 'Using nested clustering variables also has a different syntax in Python. We
    need to express the lower-level, nested, variable in a separate formula (the “variance
    components formula,” which I abbreviated as `vcf`):'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python中，使用嵌套聚类变量的语法也有所不同。我们需要在单独的公式中表示较低级别、嵌套的变量（“方差分量公式”，我缩写为`vcf`）：
- en: '[PRE6]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The syntax for the variance components formula is a bit esoteric but the intuition
    is straightforward. The formula itself is a dictionary with each of the nested
    variables as key. The value attached to each key indicates whether we want that
    variable to have a random intercept or a random slope (random here means “varying
    by category”). A random intercept is the HM equivalent of a *categorical* variable
    and is expressed as `"0+C(var)"`, where `var` is the name of the nested variable,
    i.e., the same as the key. Random slopes are beyond the scope of this book, but
    for example if you wanted the relationship between age and call satisfaction to
    have a different slope for each rep, the variance component formula would be `vcf
    = {"rep_ID": "0+C(rep_ID)", "age":"0+age"}`, without a `C()` in the second case.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '方差分量公式的语法有些神秘，但直觉很简单。公式本身是一个字典，每个嵌套变量都是一个键。附加到每个键的值表示我们希望该变量具有随机截距或随机斜率（这里的随机意味着“按类别变化”）。随机截距是*分类*变量的HM等效，表示为`"0+C(var)"`，其中`var`是嵌套变量的名称，即与键相同。随机斜率超出了本书的范围，但例如，如果您希望年龄与通话满意度之间的关系对每个代表都有不同的斜率，方差分量公式将是`vcf
    = {"rep_ID": "0+C(rep_ID)", "age":"0+age"}`，第二种情况中不需要`C()`。'
- en: Determining Random Assignment and Sample Size/Power
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 确定随机分配和样本大小/功效
- en: Now that we have planned the qualitative aspects of our experiment, we need
    to determine the random assignment we’ll use as well as our sample size and power.
    In our two previous experiments ([Chapter 8](ch08.xhtml#experimental_design_the_basics)
    and [Chapter 9](ch09.xhtml#stratified_randomizatio)), we had some target effect
    size and statistical power, and we chose our sample size accordingly. Here, we’ll
    add a wrinkle by assuming that our business partners are willing to run the experiment
    only for a month,^([5](ch10.xhtml#ch01fn26)) and the minimum detectable effect
    they’re interested in capturing is 0.6 (i.e., they want to make sure that you
    have sufficient power to capture an effect of that size, but they’re willing to
    take the risk that the effect size will be lower).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经计划了实验的定性方面，我们需要确定我们将使用的随机分配以及我们的样本大小和功效。在我们之前的两个实验中（[第8章](ch08.xhtml#experimental_design_the_basics)
    和 [第9章](ch09.xhtml#stratified_randomizatio)），我们有一些目标效应大小和统计功效，并根据此选择了样本大小。在这里，我们将增加一个细节，假设我们的业务伙伴只愿意进行一个月的实验^([5](ch10.xhtml#ch01fn26))，他们感兴趣捕捉的最小可检测效应是0.6（即，他们希望确保你有足够的能力捕捉到这个大小的效应，但他们愿意承担效应大小可能较低的风险）。
- en: 'Under these constraints, the question becomes: how much power do we have to
    capture a difference of that amount with that sample? In other words, assuming
    that the difference is indeed equal to 0.6, what is the probability that our decision
    rule will conclude that the treatment is indeed better than the control?'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些约束条件下，问题是：我们有多大的能力捕捉这个样本量的差异？换句话说，假设这个差异确实等于0.6，我们的决策规则将得出治疗组确实比对照组更好的结论的概率是多少？
- en: As mentioned earlier, we’ll be using a hierarchical regression to analyze our
    data and that will complicate our power analysis a bit, but let’s first briefly
    review the process for random assignment.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 正如前文所述，我们将使用分层回归分析我们的数据，这将稍微复杂化我们的功效分析，但让我们首先简要回顾一下随机分配的过程。
- en: Random Assignment
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 随机分配
- en: Even though we don’t know ahead of time which customers are going to call, it
    doesn’t matter for the random assignment because we’ll do it at the call center
    level. Therefore, we can do it in advance, assigning control and treatment groups
    all at once. With clustered experiments like this one, stratification is especially
    useful because we have so few actual units to randomize. Here, we’re randomizing
    at the level of call centers, so we would want to stratify based on the centers’
    characteristics, such as number of reps and average values of the call metrics.
    The code to do so is a straightforward version of the code in [Chapter 9](ch09.xhtml#stratified_randomizatio),
    split between a data prep function and a wrapper for the blocking function ([Example 10-1](#stratified_random_assignment_of_call_centers)).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 即使我们事先不知道哪些客户会打电话，但对于随机分配来说并不重要，因为我们将在呼叫中心层面进行。 因此，我们可以预先进行，一次性分配控制组和治疗组。 像这样的分组实验中，分层尤其有用，因为我们要随机化的实际单位很少。
    在这里，我们是在呼叫中心级别进行随机化的，因此我们希望根据中心的特征进行分层，例如代表数和呼叫指标的平均值。 这样做的代码是[第9章](ch09.xhtml#stratified_randomizatio)中的代码的简化版本，分为数据准备函数和用于阻塞函数的包装器（[示例10-1](#stratified_random_assignment_of_call_centers)）。
- en: Example 10-1\. Stratified random assignment of call centers
  id: totrans-75
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 10-1\. 呼叫中心分层随机分配
- en: '[PRE7]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[![1](Images/1.png)](#comarker101)'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#comarker101)'
- en: 'We group by `center_ID` and summarize our clustering variable: we take the
    number of reps by center, calculate the average call CSAT and customer age, and
    determine the percentage of calls whose reason is ''`payment''`.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们按`center_ID`分组并总结我们的聚类变量：我们按中心计算代表人数，计算平均呼叫CSAT和客户年龄，并确定其原因是"`payment`"的呼叫的百分比。
- en: '[![2](Images/2.png)](#comarker102)'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#comarker102)'
- en: We rescale all clustering variables to between 0 and 1.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将所有聚类变量重新缩放为0到1之间。
- en: '[![3](Images/3.png)](#comarker103)'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#comarker103)'
- en: We use the `block()` function from `blockTools`, using the '`optimal'` algorithm
    from the `nbpMatching` package (we can afford the extra computation with so few
    call centers).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`blockTools`中的`block()`函数，使用`nbpMatching`包中的"`optimal'`算法（对于这么少的呼叫中心，我们可以承担额外的计算）。
- en: '[![4](Images/4.png)](#comarker104)'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](Images/4.png)](#comarker104)'
- en: We extract the pairing from the output of `block()`.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从`block()`的输出中提取配对信息。
- en: 'The resulting pairing is:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 得到的配对信息是：
- en: '[PRE8]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'As mentioned in the previous chapter, there’s no equivalent to the `block`
    package in Python, so we’ll use the two functions for that purpose I have described
    in the previous chapter, with minor adjustments (e.g., we don’t have categorical
    variables at the center level, so we don’t need to one-hot encode them):'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 如前一章所述，Python中没有`block`包的等效物，因此我们将使用我在前一章中描述的两个函数来实现这个目的，进行轻微调整（例如，我们在中心层级没有分类变量，因此不需要对它们进行独热编码）：
- en: '[PRE9]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Power Analysis
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 功效分析
- en: Using a standard statistical formula for power analysis (in this case it would
    be the formula for the T-test) would be highly misleading because it would not
    take into account the correlation that exists in the data. Gelman and Hill (2006)
    provide some specific statistical formulas for hierarchical models, but I don’t
    want to go down the rabbit hole of accumulating increasingly complex and narrow
    formulas. As usual, we’ll be running simulations as our foolproof approach to
    power analysis.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 使用标准统计公式进行功效分析（在这种情况下，它将是T检验的公式）会非常误导，因为它不会考虑到数据中存在的相关性。 Gelman和Hill（2006）为层次模型提供了一些具体的统计公式，但我不想沉迷于积累越来越复杂和狭窄的公式。
    像往常一样，我们将运行模拟作为我们的万全之策来进行功效分析。
- en: 'Let’s first define our metric function:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先定义我们的度量函数：
- en: '[PRE10]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: This function returns the coefficient for the treatment group from our hierarchical
    model. As we did in the previous chapters, let’s now run the simulations for our
    power analysis, which you should hopefully be familiar with by now. The only additional
    thing we need to take into account here is that our data is stratified, a.k.a.
    clustered. This has two implications.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数返回我们的分层模型中治疗组的系数。 就像我们在前几章中所做的那样，现在让我们为我们的功效分析运行模拟，希望你现在对此很熟悉。 这里唯一需要考虑的额外事项是我们的数据是分层的，即聚类的。
    这有两个含义。
- en: 'First, we can’t just draw calls from historical data at random. In our experiment,
    we expect reps to have almost exactly the same number of calls each; on the other
    hand, a truly random draw would generate some significant variation in the number
    of calls per rep. We expect reps to handle around 1,200 calls a month; having
    one rep handle 1,000 calls and another handle 1,400 is much more likely with a
    truly random draw than in reality. Fortunately, from a programming standpoint,
    this can easily be resolved by grouping our historical data at the call center
    and rep level before making a random draw:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们不能随意从历史数据中随机抽取电话。在我们的实验中，我们预期代表们几乎每个人每次都会接到几乎相同数量的电话；而在真正的随机抽取中，每个代表接到电话数量的变化会显著。我们预期代表们每月会处理大约1,200通电话；在真正的随机抽取中，一个代表处理1,000通电话，另一个处理1,400通电话的情况比在实际中更有可能发生。幸运的是，从编程的角度来看，可以在做随机抽取之前将我们的历史数据按照呼叫中心和代表级别进行分组，这样可以很容易地解决这个问题：
- en: '[PRE12]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Using permutations when randomness is “limited”
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 当随机性“受限”时使用排列组合
- en: 'The second implication is at the statistical level and is more profound. We’re
    using stratification to pair similar call centers and assign one from each pair
    to the control group and the other to the treatment group. This is good, because
    we reduce the risk that some call center characteristics will bias our analysis.
    But at the same time, this introduces a fixed effect in our simulations: let’s
    say that call centers 1 and 5 are paired together because they’re very similar.
    Then, however many simulations we run, one of them will be in the control group
    and the other one will be in the treatment group; we’ve reduced the total number
    of possible combinations. With a completely free randomization, there are 10!/(5!
    * 5!) ≈ 252 different assignments of 10 call centers in equally sized experimental
    groups, which is already not that many.^([6](ch10.xhtml#ch01fn27)) With stratification,
    there are only 2^5 ≈ 32 different assignments, because there are two possible
    assignments for each of the five pairs: (control, treatment) and (treatment, control).
    This means that even if you were to run 32,000 simulations, you would only see
    32 different random allocations at the call center level. Moreover, with only
    three months worth of historical data, we can only generate three completely different
    (i.e., mutually exclusive) samples per rep, for a total of 32 * 3 = 96 different
    simulations.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个含义在统计水平上更为深刻。我们正在使用分层抽样来配对相似的呼叫中心，并将每一对中的一个分配给对照组，另一个分配给处理组。这很好，因为我们减少了某些呼叫中心特征会偏倚我们分析的风险。但与此同时，这在我们的模拟中引入了一个固定效应：比如说呼叫中心1和5因为非常相似而被配对在一起。无论我们运行多少次模拟，其中一个将在对照组，另一个将在处理组；我们减少了可能的组合总数。在完全自由的随机化下，有10!/(5!
    * 5!) ≈ 252种不同的方式将10个呼叫中心均匀地分配到实验组中，这已经不算多了。而使用分层抽样，只有2^5 ≈ 32种不同的分配方式，因为每个五对中有两种可能的分配方式：（对照组，处理组）和（处理组，对照组）。这意味着即使你运行了32,000次模拟，你只会看到32种不同的随机分配方式。此外，只有三个月的历史数据，我们每个代表只能生成三个完全不同的（即互斥的）样本，总共有32
    * 3 = 96种不同的模拟。
- en: This doesn’t mean that we should not use stratification; on the contrary, stratification
    is even more crucial the smaller our experimental population gets! This does imply
    however that it is pretty much pointless and potentially misleading to run many
    more simulations than you have truly different assignments.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这并不意味着我们不应该使用分层抽样；相反，分层抽样在我们实验人口较小的情况下更为关键！然而，这确实意味着如果你运行的模拟比你真正有的不同分配要多得多，那么这样做可能毫无意义，甚至可能具有误导性。
- en: 'To understand why, let’s use a metaphor: imagine a student who decides to increase
    their vocabulary ahead of a test (e.g., the LSAT). They buy a learner’s dictionary
    and plan to read the definition of a random word in it ten times a day until they’ve
    done it a thousand times, to learn a thousand words. But here’s the catch: their
    dictionary has only 96 words in it! This means that however many times the student
    looks up a random word, their vocabulary cannot increase by more than 96 words.
    There’s certainly value in reading a word’s definition more than once, to better
    understand and memorize it, but that’s different from reading the definition of
    more words. This also means that looking at definitions at random is a very inefficient
    way to proceed. It is much better to simply go through the 96 words in order.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解其中的原因，让我们使用一个比喻：想象一个学生决定在考试前（例如 LSAT）扩展他们的词汇量。他们购买了一本学习词典，并计划每天随机查阅其中一个词的定义十次，直到累计一千次，以学习一千个单词。但问题在于：他们的词典里只有96个单词！这意味着无论学生多少次查阅随机单词，他们的词汇量都不能增加超过96个单词。多次阅读一个单词的定义确实有助于更好地理解和记忆它，但这与查看更多单词的定义是不同的。这也意味着随机查阅定义是一种非常低效的方法。简单地按顺序逐个查阅这96个单词要好得多。
- en: 'That logic applies in the same way to simulations: we usually draw at random
    from our historical data to build a simulated experimental data set, and we (correctly)
    treat as negligible the probability of several simulations being identical. In
    the present case, if we had a hundred call centers, each with a thousand reps
    and ten years of data, we could confidently simulate hundreds, or even thousands,
    of experiments without worrying. With our limited number of call centers and reps,
    we’re better off going systematically through the limited number of possibilities.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这种逻辑同样适用于模拟：通常我们从历史数据中随机抽取来构建模拟实验数据集，而且我们（正确地）将多个模拟重复的概率视为可以忽略不计。在当前情况下，如果我们有一百个呼叫中心，每个中心有一千名代表和十年的数据，我们可以自信地进行数百甚至数千次实验模拟而不用担心。由于我们呼叫中心和代表人数有限，我们最好系统地研究有限的可能性。
- en: 'Let’s see how to do this in code. We have the call-center pairings (see [Figure 10-2](#ninezeropercent_confidence_intervals_wi)
    in the previous subsection) and we need to go through the 32 possible permutations
    of those pairs. The first pair is made up of call centers #7 and #2, so half of
    the simulations will have #7 in control group and #2 in treatment group, while
    the other half will have #2 in control group and #7 in the treatment group, and
    so on. So the first simulation might have as control group the call centers (7,
    9, 3, 10, 4) while the second simulation has as control group (2, 9, 3, 10, 4).'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '让我们看看如何用代码实现这一点。我们有呼叫中心的配对（参见前一小节的 [Figure 10-2](#ninezeropercent_confidence_intervals_wi)），我们需要遍历这些配对的32种可能的排列组合。第一对由呼叫中心
    #7 和 #2 组成，所以一半的模拟将使 #7 成为对照组， #2 成为治疗组，而另一半将使 #2 成为对照组， #7 成为治疗组，依此类推。因此，第一次模拟可能将呼叫中心（7,
    9, 3, 10, 4）作为对照组，而第二次模拟将呼叫中心（2, 9, 3, 10, 4）作为对照组。'
- en: 'We’ll use a trick to help us go through the permutations easily. It is not
    really complex, but it relies on properties of binary numbers that are not intuitive,
    so brace yourself and bear with me. Any integer can be expressed in binary base
    as a sequence of zeros and ones. 0 is 0, 1 is 1, 2 is 10, 3 is 11, and so on.
    These can be left-padded with zeros to have a constant number of digits. We want
    the number of digits to be equal to the number of pairs, here 5\. This means that
    0 is 00000, 1 is 00001, 2 is 00010, and 3 is 00011\. The largest integer we can
    express with 5 digits is 31\. Note that, and this is not a coincidence, including
    0 as 00000, we can express 32 different integers with 5 binary digits, and that
    32 is the number of permutations we want to implement. Therefore, we can decide
    that the first simulation, which we’ll call “simulation 00000,” has as control
    group (7, 9, 3, 10, 4) from [Figure 10-2](#ninezeropercent_confidence_intervals_wi).
    From there, we’ll swap a pair between the control and the treatment groups whenever
    the digit corresponding to the pair in the binary form of the simulation number
    is a 1\. So for example, for simulation 10000, we would swap call centers #7 and
    #2, giving us the control group (2, 9, 3, 10, 4). Here’s where the magic happens:
    by going from 00000 to 11111, we’ll see all the possible permutations of the 5
    pairs!'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用一个技巧来帮助我们轻松地穿过排列。这并不是很复杂，但它依赖于二进制数的性质，这些性质并不直观，因此请准备好并忍耐。任何整数都可以用二进制基数表示为一串零和一。0是0，1是1，2是10，3是11，依此类推。这些可以左填充为零，以具有恒定数量的数字。我们希望数字的位数等于对数对的数量，在这里是5。这意味着0是00000，1是00001，2是00010，3是00011。我们可以用5位二进制数表示的最大整数是31。请注意，这不是巧合，包括0作为00000，我们可以用5位二进制数字表示32个不同的整数，而32就是我们要实现的排列数。因此，我们可以决定第一个模拟，我们将其称为“模拟00000”，在[图10-2](#ninezeropercent_confidence_intervals_wi)中的对照组是（7,
    9, 3, 10, 4）。从那里开始，每当模拟号码的二进制形式中与对中对应的数字是1时，我们就会在对照组和治疗组之间交换一对。例如，对于模拟10000，我们将交换呼叫中心#7和#2，得到控制组（2,
    9, 3, 10, 4）。这里的魔法发生了：通过从00000到11111，我们将看到所有可能的五对排列！
- en: Code for permutations
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 排列的代码
- en: 'Because of the differences in indexing between Python and R (the former starting
    at 0 and the latter at 1), the code is a bit simpler in Python, so let’s start
    with the corresponding snippet of code:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Python和R之间的索引差异（前者从0开始，后者从1开始），因此Python中的代码要简单一些，因此让我们从相应的代码片段开始：
- en: '[PRE14]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[![1](Images/1.png)](#comarker1001)'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#comarker1001)'
- en: We convert the permutation counter `perm` to a binary string. In Python, there
    are several ways to do it. I did it here with an F-string. The syntax of an F-string
    is `f'{exp}'`, where the expression `exp` is evaluated before getting formatted
    as a string. Within the expression, `Npairs` is also between curly braces, so
    it’s evaluated first before being passed to the expression; after that first evaluation,
    `exp` is equal to `perm:05b`. The first term on the left of the colon is the number
    to format; the letter after the colon indicates the format to use, here `b` for
    binary; the number immediately to the left of the letter indicates the total number
    of digits to use (here 5); and finally, any character to the left of that number
    is to be used for padding (here 0).
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将排列计数器`perm`转换为二进制字符串。在Python中，有几种方法可以实现这一点。我在这里使用了F-string。F-string的语法是`f'{exp}'`，其中表达式`exp`在格式化为字符串之前进行评估。在表达式内部，`Npairs`也位于大括号之间，因此它在传递给表达式之前首先进行评估；在第一次评估之后，`exp`等于`perm:05b`。冒号左边的第一个术语是要格式化的数字；冒号后面的字母表示要使用的格式，在这里是`b`表示二进制；紧挨着数字左边的数字表示要使用的总位数（这里是5）；最后，数字左边的任何字符都是用于填充的（这里是0）。
- en: '[![2](Images/2.png)](#comarker1002)'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#comarker1002)'
- en: We match the digits of the binary string with a counter for the pairs within
    the `idx` matrix. So “00000” becomes <math><mrow><mrow><mo>(</mo><mtable><mtr><mtd><mn>0</mn></mtd><mtd><mn>0</mn></mtd></mtr><mtr><mtd><mn>1</mn></mtd><mtd><mn>0</mn></mtd></mtr><mtr><mtd><mn>2</mn></mtd><mtd><mn>0</mn></mtd></mtr><mtr><mtd><mn>3</mn></mtd><mtd><mn>0</mn></mtd></mtr><mtr><mtd><mn>4</mn></mtd><mtd><mn>0</mn></mtd></mtr></mtable><mo>)</mo></mrow></mrow></math>
    in Python after transposing.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将二进制字符串的数字与`idx`矩阵中的对数计数器匹配。因此，“00000”在Python中变为<math><mrow><mrow><mo>(</mo><mtable><mtr><mtd><mn>0</mn></mtd><mtd><mn>0</mn></mtd></mtr><mtr><mtd><mn>1</mn></mtd><mtd><mn>0</mn></mtd></mtr><mtr><mtd><mn>2</mn></mtd><mtd><mn>0</mn></mtd></mtr><mtr><mtd><mn>3</mn></mtd><mtd><mn>0</mn></mtd></mtr><mtr><mtd><mn>4</mn></mtd><mtd><mn>0</mn></mtd></mtr></mtable><mo>)</mo></mrow></mrow></math>在转置后。
- en: '[![3](Images/3.png)](#comarker1003)'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#comarker1003)'
- en: We pass the rows of `idx` as indices to indicate which element of each pair
    goes into the treatment group. That is, to indicate that the first element of
    the first pair should go into the treatment group, we pass [0, 0]. With 00000,
    we always put the first element of each pair in the treatment group. With the
    last permutation, 11111, we put the second element of each pair in the treatment
    group, mirroring the allocation for 00000\. Taking a more complicated example,
    for permutation number 7, whose binary format is 00111, we would put in the control
    group the first element for the first two pairs, and the second element for the
    last three pairs.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将`idx`的行作为索引传递，以指示每对中的哪个元素进入治疗组。也就是说，为了指示第一对的第一个元素应该进入治疗组，我们传递 [0, 0]。对于 00000，我们总是将每对的第一个元素放入治疗组。对于最后一个排列
    11111，我们将每对的第二个元素放入治疗组，与 00000 的分配相反。以更复杂的例子来说，对于排列编号为 7 的排列，其二进制格式为 00111，在前两对中将第一个元素放入对照组，最后三对中将第二个元素放入对照组。
- en: '[![4](Images/4.png)](#comarker1004)'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](Images/4.png)](#comarker1004)'
- en: Finally, we update our simulated experimental data set, assigning each row to
    either the control or treatment group based on its center ID.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们更新了模拟实验数据集，根据其中心 ID 将每行分配到对照组或治疗组。
- en: 'The process is identical in R with a few differences in syntax:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程在 R 中基本相同，语法上有一些差异：
- en: '[PRE15]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[![1](Images/1.png)](#comarker10001)'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#comarker10001)'
- en: Converting `perm` to a binary format is done in R with the `as.binary()` function,
    which takes as first argument the number to convert and as second argument the
    total number of digits we want (i.e., the number of pairs, here 5).
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 将`perm`转换为二进制格式在 R 中使用 `as.binary()` 函数完成，该函数以第一个参数为要转换的数字，第二个参数为我们想要的总位数（即对数，这里是
    5）。
- en: '[![2](Images/2.png)](#comarker10002)'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#comarker10002)'
- en: Because the indexing starts with 1 and not 0 in R, we need to add 1 to all the
    elements of the second column in the `idx` matrix. Thus, for the first permutation,
    00000, where the first element of each pair goes into the control group, the `idx`
    matrix is <math><mrow><mrow><mo>(</mo><mtable><mtr><mtd><mn>1</mn></mtd><mtd><mn>1</mn></mtd></mtr><mtr><mtd><mn>2</mn></mtd><mtd><mn>1</mn></mtd></mtr><mtr><mtd><mn>3</mn></mtd><mtd><mn>1</mn></mtd></mtr><mtr><mtd><mn>4</mn></mtd><mtd><mn>1</mn></mtd></mtr><mtr><mtd><mn>5</mn></mtd><mtd><mn>1</mn></mtd></mtr></mtable><mo>)</mo></mrow></mrow></math>.
    For permutation 11111, the second column would be made up of 2s, and for 00111
    it would be 11222.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 因为 R 中的索引从 1 开始而不是从 0 开始，我们需要将`idx`矩阵第二列的所有元素加 1。因此，对于第一个排列 00000，其中每对的第一个元素进入对照组，`idx`矩阵为
    <math><mrow><mrow><mo>(</mo><mtable><mtr><mtd><mn>1</mn></mtd><mtd><mn>1</mn></mtd></mtr><mtr><mtd><mn>2</mn></mtd><mtd><mn>1</mn></mtd></mtr><mtr><mtd><mn>3</mn></mtd><mtd><mn>1</mn></mtd></mtr><mtr><mtd><mn>4</mn></mtd><mtd><mn>1</mn></mtd></mtr><mtr><mtd><mn>5</mn></mtd><mtd><mn>1</mn></mtd></mtr></mtable><mo>)</mo></mrow></mrow></math>。对于排列
    11111，第二列将由 2 组成；对于 00111，它将由 11222 组成。
- en: '[![3](Images/3.png)](#comarker10003)'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#comarker10003)'
- en: We pass the rows of `idx` as indices to indicate which element of each pair
    goes into the treatment group.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将`idx`的行作为索引传递，以指示每对中的哪个元素进入治疗组。
- en: The `permutation_gen_fun()` function returns a list of the center IDs for the
    treatment group, which can then be used in the random assignment function.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '`permutation_gen_fun()` 函数返回治疗组的中心 ID 列表，然后可以在随机分配函数中使用。'
- en: Power curve
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 功率曲线
- en: Now that we have a solution to the problem of limited possible samples, we can
    get back to our power analysis. Remember that business partners want to run the
    experiment for no longer than a month, meaning a sample size of about 230,000
    calls. Instead of calculating the required sample size for the threshold value
    of 0.6 points of CSAT and the desired power, we need to take the sample size as
    given and calculate what power we have for this threshold value.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们解决了可能样本量有限的问题，可以继续进行功率分析。请记住，业务伙伴希望在一个月内完成实验，这意味着大约需要 230,000 通话的样本量。我们不再计算
    CSAT 0.6 分的阈值所需的样本量和所需功率，而是采用给定的样本量，计算我们在这个阈值下的功率。
- en: 'Let’s first look at statistical significance. Remember that in the previous
    chapter, our estimator was “underconfident”: the 90%-CI included zero more than
    90% of the time. Even using a 40%-CI led only to a small number of false positives.
    Here, we have the opposite problem: our estimator is “overconfident” as the 90%-CI
    includes zero much less than 90% of the time, and indeed it never includes it:
    our coverage is null. [Figure 10-2](#ninezeropercent_confidence_intervals_wi)
    shows the 96 confidence intervals ranked from lowest to highest.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先看统计显著性。请记住，在上一章中，我们的估计器“不自信”：90% 的置信区间超过了 90% 的时间都包括零。即使使用 40% 的置信区间，也只有少量的误报。在这里，我们面临相反的问题：我们的估计器“过于自信”，因为
    90% 的置信区间几乎从不包括零，实际上从未包括过：我们的覆盖率为零。[图 10-2](#ninezeropercent_confidence_intervals_wi)
    展示了从最低到最高排名的 96 个置信区间。
- en: '![90% confidence intervals with no effect](Images/BEDA_1002.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![90% 置信区间无效果](Images/BEDA_1002.png)'
- en: Figure 10-2\. 90% confidence intervals with no effect
  id: totrans-129
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-2\. 90% 置信区间无效果
- en: The situation we can see in [Figure 10-3](#power_curve_with_decision_threshold_of)
    is similar to what we saw in [Chapter 7](ch07.xhtml#measuring_uncertainty_with_the_bootstra),
    where having very limited data led to discontinuities in our graphs. Here, the
    random errors never quite line up in the way that would result in a CI including
    zero. Instead, we have four tight clusters of CIs, even though the distribution
    of our CIs is symmetric around zero (i.e., our estimator is unbiased) and half
    of them are very close to it. From a practical perspective, that means that if
    we run our experiment, we shouldn’t expect the true value to be included in our
    CI.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 [图 10-3](#power_curve_with_decision_threshold_of) 中看到的情况类似于我们在 [第 7 章](ch07.xhtml#measuring_uncertainty_with_the_bootstra)
    中看到的情况，即数据非常有限导致图表中出现不连续性。在这里，随机误差永远不会完全对齐，导致置信区间永远不包括零。相反，我们有四个紧密聚集的置信区间群，尽管我们的置信区间分布在零周围是对称的（即，我们的估计器是无偏的），并且其中一半置信区间非常接近零。从实际的角度来看，这意味着如果我们进行实验，我们不应期望真实值包含在我们的置信区间内。
- en: 'This doesn’t mean that our experiment is doomed, but that we shouldn’t trust
    our CI bounds and we should rely on our decision rule instead. With the default
    decision rule of accepting any CI that is strictly positive, our significance
    is 50%: because half of our CIs are below zero and half above, in half of the
    cases we would observe a negative coefficient and rightly conclude that the treatment
    group is no better than the control group. [Figure 10-3](#power_curve_with_decision_threshold_of)
    plots the power curve with this decision rule for different effect sizes.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 这并不意味着我们的实验注定要失败，而是我们不应信任我们的置信区间边界，而应依赖我们的决策规则。使用接受任何严格正值置信区间的默认决策规则，我们的显著性为
    50%：因为一半的置信区间低于零，另一半高于零，在一半的情况下，我们会观察到一个负系数，并正确地得出结论，即治疗组不比对照组好。[图 10-3](#power_curve_with_decision_threshold_of)
    绘制了这种决策规则下不同效应大小的功效曲线。
- en: '![Power curve with decision threshold of 0 for different effect sizes](Images/BEDA_1003.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![决策阈值为 0 的不同效应大小的功效曲线](Images/BEDA_1003.png)'
- en: Figure 10-3\. Power curve with decision threshold of 0 for different effect
    sizes
  id: totrans-133
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-3\. 决策阈值为 0 的不同效应大小的功效曲线
- en: As you can see, our power reaches 75% very quickly, basically as soon as the
    cluster of CIs that was just below zero gets shifted just above it. After that,
    our power remains constant for a range of values including our threshold effect
    size of 0.6, until the cluster of strongly negative CIs gets shifted above zero
    in turn. Then our power is close to 100% for effect sizes of 1 or above. That
    is, if the true effect is 1 or higher, we’re extremely unlikely to see a negative
    CI.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所见，我们的功效非常快地达到了 75%，基本上就在刚刚低于零的置信区间群被移动到稍高于零的位置时。之后，我们的功效在一定范围内保持稳定，包括我们的阈值效应大小为
    0.6，直到强烈负置信区间群依次移动到高于零的位置。然后，我们的功效接近于 100%，对于效应大小为 1 或更高的情况，我们几乎不太可能看到一个负置信区间。
- en: We could go back to our business partners and tell them that our CIs are unreliable
    and therefore our risk of false positives is large, but our risk of false negatives
    is very low. In the present case, we can do better by setting a more stringent
    decision rule and implementing the intervention only if we observe an effect size
    of 0.25 or above. [Figure 10-4](#power_curve_for_different_effect_sizes) shows
    the power curve for that decision rule.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以向业务伙伴反馈，我们的置信区间不可靠，因此误报的风险很大，但误检的风险非常低。在当前情况下，我们可以通过设定更严格的决策规则，并且只在观察到效应大小为
    0.25 或以上时实施干预来做得更好。[图 10-4](#power_curve_for_different_effect_sizes) 展示了该决策规则的功效曲线。
- en: '![Power curve for different effect sizes with decision threshold of 0.25](Images/BEDA_1004.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![决策阈值为0.25的不同效应大小的功效曲线](Images/BEDA_1004.png)'
- en: Figure 10-4\. Power curve for different effect sizes with decision threshold
    of 0.25
  id: totrans-137
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-4. 决策阈值为0.25的不同效应大小的功效曲线
- en: As we can see in [Figure 10-4](#power_curve_for_different_effect_sizes), by
    increasing our decision threshold, we’ve lowered the left side of our power curve.
    This implies a lower significance (i.e., lower risk of false positives) at the
    cost of a lower power (i.e., higher risk of false negatives) for small effect
    sizes. However, the right side of our power curve remains mostly unchanged, meaning
    that our power to detect an effect of 0.6 remains at 75%.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[图10-4](#power_curve_for_different_effect_sizes)中所见，通过增加我们的决策阈值，我们降低了功效曲线的左侧。这意味着在小效应大小时有更低的显著性（即较低的假阳性风险），但以牺牲较高的错误否定率（即更高的假阴性风险）为代价。然而，功效曲线的右侧大部分保持不变，这意味着我们对于检测0.6效应的能力仍然为75%。
- en: Let’s recap what our power analysis told us. Because we plan to use a stratified
    random assignment with a limited number of effective experimental units (i.e.,
    call centers), our experiment has a rigid structure that constrains possible outcomes.
    This makes our CIs unreliable by themselves. However, we can adjust our decision
    rule to a higher threshold (i.e., we’ll implement our intervention only if we
    observe an effect of 0.25 or higher). By doing so, we can reduce the risk of false
    positives for a null effect size while keeping our power for the target effect
    size high enough. This remains an underpowered experiment but this is the best
    we can offer as experimentalists, and our business partners will have to decide
    how they feel about these odds.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下我们的功效分析告诉我们的内容。由于我们计划使用分层随机分配有限数量的有效实验单元（即呼叫中心），我们的实验具有严格的结构，限制了可能的结果。这使得单独的置信区间本身不可靠。然而，我们可以调整我们的决策规则以更高的阈值（即只有观察到0.25或更高的效应时，我们才会实施我们的干预）。通过这样做，我们可以在零效应大小的情况下减少假阳性风险，同时保持对目标效应大小的功效足够高。这仍然是一个功效不足的实验，但这是我们作为实验者能提供的最好方案，而我们的业务伙伴将不得不决定他们对这些机会的看法。
- en: Warning
  id: totrans-140
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Note the difference between our decision threshold, 0.25, and our target effect,
    0.6\. By definition, the power at the decision threshold is always 0.5 and we
    set out to get as much power as possible for an effect size of 0.6.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 注意我们决策阈值0.25和目标效应0.6之间的差异。根据定义，决策阈值的功效始终为0.5，我们的目标是在0.6效应大小上获得尽可能多的功效。
- en: Analyzing the Experiment
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分析实验结果
- en: 'Once we have run our experiment, we can collect and analyze the data. Having
    defined our metric function previously, the analysis is now as simple as applying
    it to our experimental data, then obtaining a Bootstrap 90%-CI of its value for
    our experimental data:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们完成实验，我们就可以收集和分析数据。在事先定义了度量函数的基础上，现在分析就像简单地将其应用于我们的实验数据，然后获取其值的Bootstrap
    90%置信区间一样：
- en: '[PRE16]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Our confidence interval is very narrow and squarely above 0.25\. Based on our
    power analysis, the true effect size is unlikely to be actually within that CI,
    but it is as likely to be lower as it is to be higher, so our expected effect
    size is equal to 0.48\. Because that is above our decision threshold, we would
    implement the intervention, even though the expected effect size is less than
    our target. Interestingly, that confidence interval is much smaller than the one
    we would obtain based on the normal approximation (i.e., coefficient +/− 1.96
    * coefficient standard error), in part because of the stratified randomization.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的置信区间非常窄，且完全高于0.25。根据我们的功效分析，真实效应大小不太可能落在该置信区间内，但它可能比该值更低或更高，因此我们预期的效应大小等于0.48。因为这高于我们的决策阈值，所以我们会实施这项干预，尽管预期效应大小低于我们的目标。有趣的是，该置信区间远小于基于正态近似得到的那个置信区间（即系数±1.96
    * 系数标准误），部分原因是分层随机化。
- en: Conclusion
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: This concludes our tour of experimental design. In the last part of the book,
    we’ll see advanced tools that will allow us to dig deeper in the analysis of experimental
    data, but the call center experiment we’ve just seen is about as complex as experiments
    get in real life. Being unable to randomize at the lowest level and having a predetermined
    amount of time to run an experiment are unpleasant but not infrequent circumstances.
    Randomizing at the level of an office or a store instead of customers or employees
    is common, to avoid logistical complications and “leakage” between experimental
    groups. Leveraging simulations for power analysis and stratification for random
    assignment becomes pretty much unavoidable if you want to get useful results out
    of your experiment; hopefully you should now be fully equipped to do so.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们实验设计之旅的结束。在本书的最后部分，我们将看到一些高级工具，这些工具将允许我们深入分析实验数据，但是我们刚刚看到的呼叫中心实验已经是现实生活中实验复杂性的极限了。无法在最低层级进行随机化，并且有一个预先确定的实验运行时间是不愉快但并不罕见的情况。在办公室或商店而不是顾客或员工级别进行随机化是常见的，以避免后勤复杂性和实验组之间的“泄漏”。如果你希望从实验中得到有用的结果，那么利用仿真进行功效分析和分层随机分配几乎是不可避免的；希望现在你已经完全具备了这样做的能力。
- en: Designing and running experiments is in my opinion one of the most fun parts
    of behavioral science. When everything goes well, you get to measure with clarity
    the impact of a business initiative or a behavioral science intervention. But
    getting everything to go well is no small feat in itself. Popular media and business
    vendors often feed the impression that experimentation can be as simple as “plug
    and play, check for 5% significance and you’re done!” but this is misleading,
    and I’ve tried to address several misconceptions that come out of this.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 设计和运行实验在我看来是行为科学中最有趣的部分之一。当一切顺利时，您可以清晰地衡量业务举措或行为科学干预的影响。但要使一切顺利本身就不是一件小事。流行媒体和商业供应商经常传达这样一种印象，即实验可以像“插入并播放，检查5%显著性，然后完成！”那么简单，但这是误导的，我试图解决了几个由此产生的误解。
- en: First of all, statistical significance and power are often misunderstood, which
    can lead to wasted experiments and suboptimal decisions. I believe that eschewing
    p-values in favor of Bootstrap confidence intervals leads to results and interpretations
    that are both more correct and more relevant to applied settings.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，统计显著性和功效经常被误解，这可能导致实验浪费和次优决策。我相信，在应用设置中，放弃p值而选择Bootstrap置信区间会导致更正确和更相关的结果和解释。
- en: Second, treating experiments as a pure technology and data analysis problem
    is easier but less fruitful than adopting a causal-behavioral approach. Using
    causal diagrams allows you to articulate more clearly what would be a success
    and what makes you believe your treatment would be successful.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，将实验视为纯技术和数据分析问题比采用因果行为方法更容易但成果较少。使用因果图允许您更清楚地表达什么是成功，以及是什么让您相信您的治疗会成功。
- en: 'Implementing an experiment in the field is fraught with difficulties (see [Bibliography](bibliography01.xhtml#bibliography)
    for further resources), and unfortunately each experiment is different, therefore
    I can only give you some generic advice:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在现场实施实验充满困难（请参阅[Bibliography](bibliography01.xhtml#bibliography)获取更多资源），不幸的是，每个实验都是不同的，因此我只能给您一些通用建议：
- en: Running field experiments is an art and science, and nothing can replace experience
    with a specific context. Start with smaller and simpler experiments at first.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行现场实验是一门艺术和科学，没有什么可以取代对特定环境的经验。起初从更小更简单的实验开始。
- en: Start by implementing the treatment on a small pilot group that you then observe
    for a little while and extensively debrief. This will allow you to ensure as much
    as possible that people understand the treatment and apply it somewhat correctly
    and consistently.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，在一个小的试点组上实施治疗，然后观察一段时间并进行广泛的反馈。这将使您尽可能地确保人们理解治疗并相对正确和一致地应用它。
- en: Try to imagine all the ways things could go wrong and to prevent them from happening.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 努力想象所有事情可能出错的方式，并防止它们发生。
- en: Recognize that things will go wrong nonetheless, and build flexibility into
    your experiment (e.g., plan for “buffers” of time, because things will take longer
    than you think—people might take a week to settle into implementing the treatment
    correctly, data might come in late, etc.).
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 认识到事情可能出错，尽管如此，还是要在实验中增加灵活性（例如，计划“缓冲”时间，因为事情会比你想象中花费更长时间——人们可能需要一周时间来正确实施治疗，数据可能会延迟等）。
- en: ^([1](ch10.xhtml#ch01fn22-marker)) “‘Sorry’ Is Not Enough,” *Harvard Business
    Review*, Jan.–Feb. 2018.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch10.xhtml#ch01fn22-marker)) “‘对不起’ 不足以”，《哈佛商业评论》，2018年1月至2月。
- en: ^([2](ch10.xhtml#ch01fn23-marker)) If you want to learn more about this type
    of models, Gelman and Hill (2006) is the classic reference on the topic.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch10.xhtml#ch01fn23-marker)) 如果你想了解更多关于这类模型的信息，Gelman 和 Hill（2006）是该主题的经典参考文献。
- en: ^([3](ch10.xhtml#ch01fn24-marker)) If you really want to know, these coefficients
    are calculated as a weighted average of the mean CSAT in a call center and the
    mean CSAT across our whole data.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch10.xhtml#ch01fn24-marker)) 如果你真的想知道，这些系数是呼叫中心平均客户满意度和整体数据平均客户满意度的加权平均值。
- en: '^([4](ch10.xhtml#ch01fn25-marker)) As always with numerical simulations, your
    mileage may vary. Thanks to Jessica Jakubowski for suggesting an alternative specification:
    `lmerControl(optimizer ="bobyqa", optCtrl=list(maxfun=2e5))`.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch10.xhtml#ch01fn25-marker)) 对于数值模拟，你的情况可能有所不同。感谢 Jessica Jakubowski 建议另一种规格：`lmerControl(optimizer
    ="bobyqa", optCtrl=list(maxfun=2e5))`。
- en: ^([5](ch10.xhtml#ch01fn26-marker)) Does that suck for your experimental design?
    Totally. Is that unrealistic? Absolutely not, unfortunately. As we used to say
    when I was a consultant, the client is always the client.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch10.xhtml#ch01fn26-marker)) 这对你的实验设计不利吗？当然。这不现实吗？非常不幸，并非如此。就像我在做顾问时常说的那样，客户始终是客户。
- en: ^([6](ch10.xhtml#ch01fn27-marker)) The exclamation mark indicates the mathematical
    operator factorial. See [this Wikipedia page](https://oreil.ly/I5PTW) if you want
    to better understand the underlying math.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: ^([6](ch10.xhtml#ch01fn27-marker)) 感叹号表示数学运算符阶乘。如果你想更好地理解基础数学，请参阅[此 Wikipedia
    页面](https://oreil.ly/I5PTW)。
