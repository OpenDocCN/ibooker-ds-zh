<html><head></head><body><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 8. Unsupervised Methods: Topic Modeling and Clustering"><div class="chapter" id="ch-topicmodels">
<h1><span class="label">Chapter 8. </span>Unsupervised Methods: Topic Modeling and Clustering</h1>

<p>When working with a large number of documents, one of the first questions you want to ask without reading all of them is “What are they talking about?” You are interested in the general topics of the documents, i.e., which (ideally semantic) words are often used together.</p>

<p>Topic <a contenteditable="false" data-type="indexterm" data-primary="topic modeling" data-secondary="about" id="idm45634189178968"/>modeling tries to solve that challenge by using statistical techniques for finding out topics from a corpus of documents. <a contenteditable="false" data-type="indexterm" data-primary="vectorizers/vectorization" data-secondary="with topic modeling" data-secondary-sortas="topic modeling" id="idm45634189177288"/>Depending on your vectorization (see <a data-type="xref" href="ch05.xhtml#ch-vectorization">Chapter 5</a>), you might find different kinds of topics. Topics consist of a probability distribution of <span class="keep-together">features</span> (words, n-grams, etc.).</p>

<p>Topics normally overlap with each other; they are not clearly separated. The same is true for documents: it is not possible to assign a document uniquely to a single topic; a document always contains a mixture of different topics. The aim of topic modeling is not primarily to assign a topic to an arbitrary document but to find the global structure of the corpus.</p>

<p>Often, a set of documents has an explicit structure that is given by categories, keywords, and so on. If we want to take a look at the organic composition of the corpus, then topic modeling will help a lot to uncover the latent structure.</p>

<p>Topic modeling has been known for a long time and has gained immense popularity during the last 15 years, mainly due to the <a contenteditable="false" data-type="indexterm" data-primary="LDA (Latent Dirichlet Allocation) method" id="idm45634189171752"/>invention of LDA,<sup><a data-type="noteref" id="idm45634189170488-marker" href="ch08.xhtml#idm45634189170488">1</a></sup> a stochastic method for discovering topics. LDA is flexible and allows many modifications. However, it is not the only method for topic modeling (although you might believe this by looking at the literature, much of which is biased toward LDA). Conceptually simpler methods are non-negative matrix factorization, singular-value decomposition (sometimes called <em>LSI</em>), and a few others.</p>

<section data-type="sect1" data-pdf-bookmark="What You’ll Learn and What We’ll Build"><div class="sect1" id="idm45634189168136">
<h1>What You’ll Learn and What We’ll Build</h1>

<p>In this chapter, we will take an in-depth look at the various methods of topic modeling, try to find differences and similarities between the methods, and run them on the same use case. Depending on your requirements, it might also be a good idea to not only try a single method but compare the results of a few.</p>

<p>After studying this chapter, you will know the different methods of topic modeling and their specific advantages and drawbacks. You will understand how topic modeling can be applied not only to find topics but also to create quick summaries of document corpora. You will learn about the importance of choosing the correct granularity of entities for calculating topic models. You have experimented with many parameters to find the optimal topic model. You are able to judge the quality of the resulting topic models by quantitative methods and numbers.</p>
</div></section>

<section data-type="sect1" data-pdf-bookmark="Our Dataset: UN General Debates"><div class="sect1" id="idm45634189165256">
<h1>Our Dataset: UN General Debates</h1>

<p>Our use <a contenteditable="false" data-type="indexterm" data-primary="topic modeling" data-secondary="use case for" id="idm45634189163672"/><a contenteditable="false" data-type="indexterm" data-primary="use cases" data-secondary="for topic modeling" data-secondary-sortas="topic modeling" id="idm45634189162264"/>case is to semantically analyze the <a contenteditable="false" data-type="indexterm" data-primary="datasets, examples of" data-secondary="UN General Debates" id="ch8_term2"/><a contenteditable="false" data-type="indexterm" data-primary="topic modeling" data-secondary="with UN general debates" data-secondary-sortas="UN general debates" id="ch8_term4"/><a contenteditable="false" data-type="indexterm" data-primary="UN General Debates dataset" id="ch8_term5"/>corpus of the UN general debates. You might know this dataset from the earlier chapter about text statistics.</p>

<p>This time we are more interested in the meaning and in the semantic content of the speeches and how we can arrange them topically. We want to know what the speakers are talking about and answer questions like these: Is there a structure in the document corpus? What are the topics? Which of them is most prominent? Does this change over time?</p>

<section data-type="sect2" data-pdf-bookmark="Checking Statistics of the Corpus"><div class="sect2" id="idm45634189154280">
<h2>Checking Statistics of the Corpus</h2>

<p>Before starting with topic modeling, it is always a good idea to <a contenteditable="false" data-type="indexterm" data-primary="statistical analysis of datasets" id="ch8_term3"/>check the statistics of the underlying text corpus. Depending on the results of this analysis, you will often choose to analyze different entities, e.g., documents, sections, or paragraphs of text.</p>

<p>We are not so much interested in authors and additional information, so it’s enough to work with one of the supplied <em>CSV</em> files:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="kn">import</code> <code class="nn">pandas</code> <code class="kn">as</code> <code class="nn">pd</code>
<code class="n">debates</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">read_csv</code><code class="p">(</code><code class="s2">"un-general-debates.csv"</code><code class="p">)</code>
<code class="n">debates</code><code class="o">.</code><code class="n">info</code><code class="p">()</code>
</pre>

<p class="pagebreak-before"><code>Out:</code></p>

<pre data-code-language="python" data-type="programlisting">
<code class="o">&lt;</code><code class="k">class</code> <code class="err">'</code><code class="nc">pandas</code><code class="o">.</code><code class="n">core</code><code class="o">.</code><code class="n">frame</code><code class="o">.</code><code class="n">DataFrame</code><code class="s1">'&gt;</code>
<code class="n">RangeIndex</code><code class="p">:</code> <code class="mi">7507</code> <code class="n">entries</code><code class="p">,</code> <code class="mi">0</code> <code class="n">to</code> <code class="mi">7506</code>
<code class="n">Data</code> <code class="n">columns</code> <code class="p">(</code><code class="n">total</code> <code class="mi">4</code> <code class="n">columns</code><code class="p">):</code>
<code class="n">session</code><code class="err">   </code> <code class="mi">7507</code> <code class="n">non</code><code class="o">-</code><code class="n">null</code> <code class="n">int64</code>
<code class="n">year</code><code class="err">      </code> <code class="mi">7507</code> <code class="n">non</code><code class="o">-</code><code class="n">null</code> <code class="n">int64</code>
<code class="n">country</code><code class="err">   </code> <code class="mi">7507</code> <code class="n">non</code><code class="o">-</code><code class="n">null</code> <code class="nb">object</code>
<code class="n">text</code><code class="err">      </code> <code class="mi">7507</code> <code class="n">non</code><code class="o">-</code><code class="n">null</code> <code class="nb">object</code>
<code class="n">dtypes</code><code class="p">:</code> <code class="n">int64</code><code class="p">(</code><code class="mi">2</code><code class="p">),</code> <code class="nb">object</code><code class="p">(</code><code class="mi">2</code><code class="p">)</code>
<code class="n">memory</code> <code class="n">usage</code><code class="p">:</code> <code class="mf">234.7</code><code class="o">+</code> <code class="n">KB</code>
</pre>

<p>The result looks fine. There are no null values in the text column; we might use years and countries later, and they also have only non-null values.</p>

<p>The speeches are <a contenteditable="false" data-type="indexterm" data-primary="speeches, UN" data-secondary="statistical analysis of" id="idm45634189060616"/>quite long and cover a lot of topics as each country is allowed only to deliver a single speech per year. Different parts of the speeches are almost always separated by paragraphs. Unfortunately, the dataset has some formatting issues. Compare the text of two selected speeches:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="k">print</code><code class="p">(</code><code class="nb">repr</code><code class="p">(</code><code class="n">df</code><code class="o">.</code><code class="n">iloc</code><code class="p">[</code><code class="mi">2666</code><code class="p">][</code><code class="s2">"text"</code><code class="p">][</code><code class="mi">0</code><code class="p">:</code><code class="mi">200</code><code class="p">]))</code>
<code class="k">print</code><code class="p">(</code><code class="nb">repr</code><code class="p">(</code><code class="n">df</code><code class="o">.</code><code class="n">iloc</code><code class="p">[</code><code class="mi">4729</code><code class="p">][</code><code class="s2">"text"</code><code class="p">][</code><code class="mi">0</code><code class="p">:</code><code class="mi">200</code><code class="p">]))</code>
</pre>

<p><code>Out:</code></p>

<pre data-type="programlisting">
'\ufeffIt is indeed a pleasure for me and the members of my delegation to
extend to Ambassador Garba our sincere congratulations on his election to the
presidency of the forty-fourth session of the General '
'\ufeffOn behalf of the State of Kuwait, it\ngives me pleasure to congratulate
Mr. Han Seung-soo,\nand his friendly country, the Republic of Korea, on
his\nelection as President of the fifty-sixth session of t'
</pre>

<p>As you can see, in some speeches the newline character is used to separate paragraphs. In the transcription of other speeches, a newline is used to separate lines. To recover the paragraphs, we therefore cannot just split at newlines. It turns out that splitting at stops, exclamation points, or question marks occurring at line ends works well enough. We ignore spaces after the stops:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="kn">import</code> <code class="nn">re</code>
<code class="n">df</code><code class="p">[</code><code class="s2">"paragraphs"</code><code class="p">]</code> <code class="o">=</code> <code class="n">df</code><code class="p">[</code><code class="s2">"text"</code><code class="p">]</code><code class="o">.</code><code class="n">map</code><code class="p">(</code><code class="k">lambda</code> <code class="n">text</code><code class="p">:</code> <code class="n">re</code><code class="o">.</code><code class="n">split</code><code class="p">(</code><code class="s1">'[.?!]\s*</code><code class="se">\n</code><code class="s1">'</code><code class="p">,</code> <code class="n">text</code><code class="p">))</code>
<code class="n">df</code><code class="p">[</code><code class="s2">"number_of_paragraphs"</code><code class="p">]</code> <code class="o">=</code> <code class="n">df</code><code class="p">[</code><code class="s2">"paragraphs"</code><code class="p">]</code><code class="o">.</code><code class="n">map</code><code class="p">(</code><code class="nb">len</code><code class="p">)</code>
</pre>

<p>From the analysis in <a data-type="xref" href="ch02.xhtml#ch-api">Chapter 2</a>, we already know that the number of speeches per year does not change much. Is this also true for the number of paragraphs?</p>

<pre data-code-language="python" data-type="programlisting">
<code class="o">%</code><code class="n">matplotlib</code> <code class="n">inline</code>
<code class="n">debates</code><code class="o">.</code><code class="n">groupby</code><code class="p">(</code><code class="s1">'year'</code><code class="p">)</code><code class="o">.</code><code class="n">agg</code><code class="p">({</code><code class="s1">'number_of_paragraphs'</code><code class="p">:</code> <code class="s1">'mean'</code><code class="p">})</code><code class="o">.</code><code class="n">plot</code><code class="o">.</code><code class="n">bar</code><code class="p">()</code>
</pre>

<p class="pagebreak-before"><code>Out:</code></p>

<figure><div class="figure"><img src="Images/btap_08in01.jpg" width="1359" height="699"/><h6/></div></figure>

<p>The average number of paragraphs has dropped considerably over time. We would have expected that, as the number of speakers per year increased and the total time for speeches is limited.</p>

<p>Apart from that, the statistical analysis shows no systematic problems with the dataset. The corpus is still quite up-to-date; there is no missing data for any year. We can <a contenteditable="false" data-type="indexterm" data-primary="speeches, UN" data-secondary="topic modeling with" id="idm45634188762264"/>now safely start with uncovering the latent structure and <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch8_term2" id="idm45634188760760"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch8_term3" id="idm45634188759384"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch8_term4" id="idm45634188758008"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch8_term5" id="idm45634188807128"/>detect topics.</p>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Preparations"><div class="sect2" id="idm45634189153656">
<h2>Preparations</h2>

<p>Topic modeling is a machine learning method and needs <a contenteditable="false" data-type="indexterm" data-primary="vectorizers/vectorization" data-secondary="with topic modeling" data-secondary-sortas="topic modeling" id="ch8_term8"/>vectorized data. All topic modeling methods start with the <a contenteditable="false" data-type="indexterm" data-primary="document-term matrix" id="ch8_term6"/>document-term matrix. Recalling the meaning of this matrix (which was introduced in <a data-type="xref" href="ch04.xhtml#ch-preparation">Chapter 4</a>), its elements are word frequencies (or often scaled as TF-IDF weights) of the words (columns) in the corresponding documents (rows). The matrix is sparse, as most documents contain only a small fraction of the vocabulary.</p>

<p>Let’s calculate the <a contenteditable="false" data-type="indexterm" data-primary="TF-IDF (Term-Frequency Inverse Document Frequency) weighting" data-secondary="vectorization with" id="idm45634188798536"/><a contenteditable="false" data-type="indexterm" data-primary="vectorizers/vectorization" data-secondary="with TF-IDF weighting" data-secondary-sortas="TF-IDF weighting" id="idm45634188797160"/>TF-IDF matrix both for the speeches and for the <a contenteditable="false" data-type="indexterm" data-primary="paragraphs, topic modeling for" id="idm45634188795352"/>paragraphs of the speeches. First, we have to <a contenteditable="false" data-type="indexterm" data-primary="scikit-learn" data-secondary="TfidfVectorizer of" id="idm45634188794088"/><a contenteditable="false" data-type="indexterm" data-primary="TfidfVectorizer" id="idm45634188732776"/><a contenteditable="false" data-type="indexterm" data-primary="scikit-learn" data-secondary="topic modeling with" id="idm45634188731672"/>import the necessary packages from scikit-learn. We start with a naive approach and use the <a contenteditable="false" data-type="indexterm" data-primary="spaCy, linguistic processing with" data-secondary="for stop words" data-secondary-sortas="stop words" id="idm45634188730056"/><a contenteditable="false" data-type="indexterm" data-primary="stop words" id="idm45634188728440"/><a contenteditable="false" data-type="indexterm" data-primary="words" data-secondary="stop words" id="idm45634188727336"/>standard spaCy stop words:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="kn">from</code> <code class="nn">sklearn.feature_extraction.text</code> <code class="kn">import</code> <code class="n">TfidfVectorizer</code>
<code class="kn">from</code> <code class="nn">spacy.lang.en.stop_words</code> <code class="kn">import</code> <code class="n">STOP_WORDS</code> <code class="k">as</code> <code class="n">stopwords</code>
</pre>

<p>Calculating the document-term matrix for the speeches is easy; we also include bigrams:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">tfidf_text</code> <code class="o">=</code> <code class="n">TfidfVectorizer</code><code class="p">(</code><code class="n">stop_words</code><code class="o">=</code><code class="n">stopwords</code><code class="p">,</code> <code class="n">min_df</code><code class="o">=</code><code class="mi">5</code><code class="p">,</code> <code class="n">max_df</code><code class="o">=</code><code class="mf">0.7</code><code class="p">)</code>
<code class="n">vectors_text</code> <code class="o">=</code> <code class="n">tfidf_text</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">debates</code><code class="p">[</code><code class="s1">'text'</code><code class="p">])</code>
<code class="n">vectors_text</code><code class="o">.</code><code class="n">shape</code>
</pre>

<p><code>Out:</code></p>

<pre data-type="programlisting">
(7507, 24611)
</pre>

<p>For the paragraphs, it’s a bit more complicated as we have to flatten the list first. In the same step, we omit empty paragraphs:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="c1"># flatten the paragraphs keeping the years</code>
<code class="n">paragraph_df</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">([{</code> <code class="s2">"text"</code><code class="p">:</code> <code class="n">paragraph</code><code class="p">,</code> <code class="s2">"year"</code><code class="p">:</code> <code class="n">year</code> <code class="p">}</code><code class="err"> </code>
<code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code><code class="k">for</code> <code class="n">paragraphs</code><code class="p">,</code> <code class="n">year</code> <code class="ow">in</code> \
                               <code class="nb">zip</code><code class="p">(</code><code class="n">df</code><code class="p">[</code><code class="s2">"paragraphs"</code><code class="p">],</code> <code class="n">df</code><code class="p">[</code><code class="s2">"year"</code><code class="p">])</code><code class="err"> </code>
<code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="k">for</code> <code class="n">paragraph</code> <code class="ow">in</code> <code class="n">paragraphs</code> <code class="k">if</code> <code class="n">paragraph</code><code class="p">])</code>

<code class="n">tfidf_para_vectorizer</code> <code class="o">=</code> <code class="n">TfidfVectorizer</code><code class="p">(</code><code class="n">stop_words</code><code class="o">=</code><code class="n">stopwords</code><code class="p">,</code> <code class="n">min_df</code><code class="o">=</code><code class="mi">5</code><code class="p">,</code>
                                        <code class="n">max_df</code><code class="o">=</code><code class="mf">0.7</code><code class="p">)</code>
<code class="n">tfidf_para_vectors</code> <code class="o">=</code> <code class="n">tfidf_para_vectorizer</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">paragraph_df</code><code class="p">[</code><code class="s2">"text"</code><code class="p">])</code>
<code class="n">tfidf_para_vectors</code><code class="o">.</code><code class="n">shape</code>
</pre>

<p><code>Out:</code></p>

<pre data-type="programlisting">
(282210, 25165)
</pre>

<p>Of course, the paragraph matrix has many more rows. The number of columns (words) is also different <a contenteditable="false" data-type="indexterm" data-primary="scikit-learn" data-secondary="max_df and min_df parameters of" id="idm45634188654536"/><a contenteditable="false" data-type="indexterm" data-primary="max_df parameter" id="idm45634188529352"/><a contenteditable="false" data-type="indexterm" data-primary="min_df parameter" id="idm45634188528248"/>because <code>min_df</code> and <code>max_df</code> have an effect in selecting features, as the number of documents has changed.</p>
</div></section>
</div></section>

<section data-type="sect1" data-pdf-bookmark="Nonnegative Matrix Factorization (NMF)"><div class="sect1" id="idm45634188805032">
<h1>Nonnegative Matrix Factorization (NMF)</h1>

<p>The conceptually <a contenteditable="false" data-type="indexterm" data-primary="NMF (Nonnegative Matrix Factorization) for topic modeling" id="ch8_term9"/><a contenteditable="false" data-type="indexterm" data-primary="topic modeling" data-secondary="NMF for" id="ch8_term10"/>easiest way to find a latent structure in the document corpus is the factorization of the document-term matrix. Fortunately, the document-term matrix has only positive-value elements; therefore, we can use <a contenteditable="false" data-type="indexterm" data-primary="linear algebra methods for topic modeling" id="ch8_term11"/>methods from linear algebra that allow us to represent the <a href="https://oreil.ly/JVpFA">matrix as the product of two other nonnegative matrices</a>. Conventionally, the original matrix is called <em>V</em>, and the factors are <em>W</em> and <em>H</em>:</p>

<div data-type="equation">
  <p><math alttext="normal upper V almost-equals normal upper W dot normal upper H">
  <mrow>
    <mi mathvariant="normal">V</mi>
    <mo>≈</mo>
    <mi mathvariant="normal">W</mi>
    <mo>·</mo>
    <mi mathvariant="normal">H</mi>
  </mrow>
</math></p>
</div>

<p>Or we can represent it graphically (visualizing the dimensions necessary for matrix multiplication), as in <a data-type="xref" href="#nmf-decomposition">Figure 8-1</a>.</p>



<p>Depending on the dimensions, the factorization can be performed exactly. But as this is so much more computationally expensive, an approximate factorization is <span class="keep-together">sufficient</span>.</p>

<figure><div id="nmf-decomposition" class="figure"><img src="Images/btap_0801.jpg" width="1138" height="486"/>
	<h6><span class="label">Figure 8-1. </span>Schematic nonnegative matrix factorization; the original matrix V is decomposed into W and H.</h6>
	</div></figure>

<p>In the context of text analytics, both <em>W</em> and <em>H</em> have an interpretation. The matrix <em>W</em> has the same number of rows as the document-term matrix and therefore maps documents to topics (document-topic matrix). <em>H</em> has the same number of columns as features, so it shows how the topics are constituted of features (topic-feature matrix). The number of topics (the columns of <em>W</em> and the rows of <em>H</em>) can be chosen arbitrarily. The smaller this number, the less <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch8_term6" id="idm45634188504328"/>exact the factorization.</p>

<section data-type="sect2" class="blueprint" data-pdf-bookmark="Blueprint: Creating a Topic Model Using NMF for Documents"><div class="sect2" id="idm45634188502824">
<h2>Blueprint: Creating a Topic Model Using NMF for Documents</h2>

<p>It’s really easy to <a contenteditable="false" data-type="indexterm" data-primary="documents" data-secondary="topic modeling for" id="ch8_term12"/><a contenteditable="false" data-type="indexterm" data-primary="topic modeling" data-secondary="for documents using NMF" data-secondary-sortas="documents using NMF" id="ch8_term13"/><a contenteditable="false" data-type="indexterm" data-primary="speeches, UN" data-secondary="topic modeling with" id="ch8_term14"/>perform this decomposition for speeches in <a contenteditable="false" data-type="indexterm" data-primary="scikit-learn" data-secondary="topic modeling with" id="idm45634188495160"/>scikit-learn. As (almost) all topic models need the number of topics as a parameter, we arbitrarily choose 10 topics (which will later turn out to be a good choice):</p>

<pre data-code-language="python" data-type="programlisting">
<code class="kn">from</code> <code class="nn">sklearn.decomposition</code> <code class="kn">import</code> <code class="n">NMF</code>

<code class="n">nmf_text_model</code> <code class="o">=</code> <code class="n">NMF</code><code class="p">(</code><code class="n">n_components</code><code class="o">=</code><code class="mi">10</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">)</code>
<code class="n">W_text_matrix</code> <code class="o">=</code> <code class="n">nmf_text_model</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">tfidf_text_vectors</code><code class="p">)</code>
<code class="n">H_text_matrix</code> <code class="o">=</code> <code class="n">nmf_text_model</code><code class="o">.</code><code class="n">components_</code>
</pre>

<p>Similar to the <code>TfidfVectorizer</code>, NMF <a contenteditable="false" data-type="indexterm" data-primary="fit_transform method" id="idm45634188408216"/>also has a <code>fit_transform</code> method that returns one of the positive factor matrices. The other factor can be accessed by the <code>components_</code> member variable of the NMF class.</p>

<p>Topics are word distributions. We are now going to analyze this distribution and see whether we can find an interpretation of the topics. Taking a look at <a data-type="xref" href="#nmf-decomposition">Figure 8-1</a>, we need to consider the <em>H</em> matrix and find the index of the largest values in each row (topic) that we then use as a lookup index in the vocabulary. As this is helpful for all topic models, we define a function for outputting a summary:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="k">def</code> <code class="nf">display_topics</code><code class="p">(</code><code class="n">model</code><code class="p">,</code> <code class="n">features</code><code class="p">,</code> <code class="n">no_top_words</code><code class="o">=</code><code class="mi">5</code><code class="p">):</code>
<code class="err">   </code> <code class="k">for</code> <code class="n">topic</code><code class="p">,</code> <code class="n">word_vector</code> <code class="ow">in</code> <code class="nb">enumerate</code><code class="p">(</code><code class="n">model</code><code class="o">.</code><code class="n">components_</code><code class="p">):</code>
<code class="err">       </code> <code class="n">total</code> <code class="o">=</code> <code class="n">word_vector</code><code class="o">.</code><code class="n">sum</code><code class="p">()</code>
<code class="err">      </code> <code class="err"> </code><code class="n">largest</code> <code class="o">=</code> <code class="n">word_vector</code><code class="o">.</code><code class="n">argsort</code><code class="p">()[::</code><code class="o">-</code><code class="mi">1</code><code class="p">]</code> <code class="c1"># invert sort order</code>
<code class="err">       </code> <code class="k">print</code><code class="p">(</code><code class="s2">"</code><code class="se">\n</code><code class="s2">Topic </code><code class="si">%02d</code><code class="s2">"</code> <code class="o">%</code> <code class="n">topic</code><code class="p">)</code>
<code class="err">       </code> <code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="n">no_top_words</code><code class="p">):</code>
<code class="err">           </code> <code class="k">print</code><code class="p">(</code><code class="s2">"  </code><code class="si">%s</code><code class="s2"> (</code><code class="si">%2.2f</code><code class="s2">)"</code> <code class="o">%</code> <code class="p">(</code><code class="n">features</code><code class="p">[</code><code class="n">largest</code><code class="p">[</code><code class="n">i</code><code class="p">]],</code>
                  <code class="n">word_vector</code><code class="p">[</code><code class="n">largest</code><code class="p">[</code><code class="n">i</code><code class="p">]]</code><code class="o">*</code><code class="mf">100.0</code><code class="o">/</code><code class="n">total</code><code class="p">))</code>
</pre>

<p>Calling this function, we get a nice summary of the topics that NMF detected in the speeches (the numbers are the percentage contributions of the words to the respective topic):</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">display_topics</code><code class="p">(</code><code class="n">nmf_text_model</code><code class="p">,</code> <code class="n">tfidf_text_vectorizer</code><code class="o">.</code><code class="n">get_feature_names</code><code class="p">())</code></pre>

<p><code>Out:</code></p>

<table>
	<tbody>
		<tr>
			<td><strong>Topic 00</strong><br/>
			co (0.79)<br/>
			operation (0.65)<br/>
			disarmament (0.36)<br/>
			nuclear (0.34)<br/>
			relations (0.25)</td>
			<td><strong>Topic 01</strong><br/>
			terrorism (0.38)<br/>
			challenges (0.32)<br/>
			sustainable (0.30)<br/>
			millennium (0.29)<br/>
			reform (0.28)</td>
			<td><strong>Topic 02</strong><br/>
			africa (1.15)<br/>
			african (0.82)<br/>
			south (0.63)<br/>
			namibia (0.36)<br/>
			delegation (0.30)</td>
			<td><strong>Topic 03</strong><br/>
			arab (1.02)<br/>
			israel (0.89)<br/>
			palestinian (0.60)<br/>
			lebanon (0.54)<br/>
			israeli (0.54)</td>
			<td><strong>Topic 04</strong><br/>
			american (0.33)<br/>
			america (0.31)<br/>
			latin (0.31)<br/>
			panama (0.21)<br/>
			bolivia (0.21)</td>
		</tr>
		<tr>
			<td><strong>Topic 05</strong><br/>
			pacific (1.55)<br/>
			islands (1.23)<br/>
			solomon (0.86)<br/>
			island (0.82)<br/>
			fiji (0.71)</td>
			<td><strong>Topic 06</strong><br/>
			soviet (0.81)<br/>
			republic (0.78)<br/>
			nuclear (0.68)<br/>
			viet (0.64)<br/>
			socialist (0.63)</td>
			<td><strong>Topic 07</strong><br/>
			guinea (4.26)<br/>
			equatorial (1.75)<br/>
			bissau (1.53)<br/>
			papua (1.47)<br/>
			republic (0.57)</td>
			<td><strong>Topic 08</strong><br/>
			european (0.61)<br/>
			europe (0.44)<br/>
			cooperation (0.39)<br/>
			bosnia (0.34)<br/>
			herzegovina (0.30)</td>
			<td><strong>Topic 09</strong><br/>
			caribbean (0.98)<br/>
			small (0.66)<br/>
			bahamas (0.63)<br/>
			saint (0.63)<br/>
			barbados (0.61)</td>
		</tr>
	</tbody>
</table>

<p>Topic 00 and Topic 01 look really promising as people are talking about nuclear disarmament and terrorism. These are definitely real topics in the UN general debates.</p>

<p>The subsequent topics, however, are more or less focused on different regions of the world. This is due to speakers mentioning primarily their own country and neighboring countries. This is especially evident in Topic 03, which reflects the conflict in the Middle East.</p>

<p>It’s also interesting to take a look at the percentages with which the words contribute to the topics. Due to the large number of words, the individual contributions are quite small, except for <em>guinea</em> in Topic 07. As we will see later, the percentages of the words within a topic are a good indication for the quality of the topic model. If the percentage within a topic is rapidly decreasing, the topic is well-defined, whereas slowly decreasing word probabilities indicate a less-pronounced topic. It’s much more difficult to intuitively find out how well the topics are separated; we will take a look at that later.</p>

<p>It would be interesting to find out how “big” the topics are, i.e., how many documents could be assigned mainly to each topic. This can be calculated using the document-topic matrix and summing the individual topic contributions over all documents. Normalizing them with the total sum and multiplying by 100 gives a percentage value:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">W_text_matrix</code><code class="o">.</code><code class="n">sum</code><code class="p">(</code><code class="n">axis</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code><code class="o">/</code><code class="n">W_text_matrix</code><code class="o">.</code><code class="n">sum</code><code class="p">()</code><code class="o">*</code><code class="mf">100.0</code></pre>

<p><code>Out:</code></p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">array</code><code class="p">([</code><code class="mf">11.13926287</code><code class="p">,</code> <code class="mf">17.07197914</code><code class="p">,</code> <code class="mf">13.64509781</code><code class="p">,</code> <code class="mf">10.18184685</code><code class="p">,</code> <code class="mf">11.43081404</code><code class="p">,</code>
<code class="err">       </code> <code class="mf">5.94072639</code><code class="p">,</code><code class="err"> </code> <code class="mf">7.89602474</code><code class="p">,</code><code class="err"> </code> <code class="mf">4.17282682</code><code class="p">,</code> <code class="mf">11.83871081</code><code class="p">,</code><code class="err"> </code> <code class="mf">6.68271054</code><code class="p">])</code>
</pre>

<p>We can easily see that there are smaller and larger topics but basically no outliers. Having an even distribution is a quality indicator. If your topic models have, for example, one or two large topics compared to all the others, you should probably adjust the <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch8_term8" id="idm45634188149176"/>number of topics.</p>

<p>In the next section, we will use the paragraphs of the speeches as entities for topic modeling and try to find out if that <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch8_term12" id="idm45634188126856"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch8_term13" id="idm45634188125480"/>improves the topics.</p>
</div></section>

<section data-type="sect2" class="blueprint" data-pdf-bookmark="Blueprint: Creating a Topic Model for Paragraphs Using NMF"><div class="sect2" id="idm45634188502200">
<h2>Blueprint: Creating a Topic Model for Paragraphs Using NMF</h2>

<p>In UN general <a contenteditable="false" data-type="indexterm" data-primary="paragraphs, topic modeling for" id="idm45634188122376"/>debates, as in many other texts, different topics are often mixed, and it is hard for the topic modeling algorithm to find a common topic of an individual speech. Especially in longer texts, it happens quite often that documents do not cover just one but several topics. How can we deal with that? One idea is to find smaller entities in the documents that are more coherent from a topic perspective.</p>

<p>In our corpus, paragraphs are a natural subdivision of speeches, and we can assume that the speakers try to stick to one topic within one paragraph. In many documents, paragraphs are a good candidate (if they can be identified as such), and we have already prepared the corresponding TF-IDF vectors. Let’s try to calculate their topic models:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">nmf_para_model</code> <code class="o">=</code> <code class="n">NMF</code><code class="p">(</code><code class="n">n_components</code><code class="o">=</code><code class="mi">10</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">)</code>
<code class="n">W_para_matrix</code> <code class="o">=</code> <code class="n">nmf_para_model</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">tfidf_para_vectors</code><code class="p">)</code>
<code class="n">H_para_matrix</code> <code class="o">=</code> <code class="n">nmf_para_model</code><code class="o">.</code><code class="n">components_</code>
</pre>

<p>Our <code>display_topics</code> function developed earlier can be used to find the content of the topics:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">display_topics</code><code class="p">(</code><code class="n">nmf_para_model</code><code class="p">,</code> <code class="n">tfidf_para_vectorizer</code><code class="o">.</code><code class="n">get_feature_names</code><code class="p">())</code></pre>

<p class="pagebreak-before"><code>Out:</code></p>


<table>
	<tbody>
		<tr>
			<td><strong>Topic 00</strong><br/>
			nations (5.63)<br/>
			united (5.52)<br/>
			organization (1.27)<br/>
			states (1.03)<br/>
			charter (0.93)</td>
			<td><strong>Topic 01</strong><br/>
			general (2.87)<br/>
			session (2.83)<br/>
			assembly (2.81)<br/>
			mr (1.98)<br/>
			president (1.81)</td>
			<td><strong>Topic 02</strong><br/>
			countries (4.44)<br/>
			developing (2.49)<br/>
			economic (1.49)<br/>
			developed (1.35)<br/>
			trade (0.92)</td>
			<td><strong>Topic 03</strong><br/>
			people (1.36)<br/>
			peace (1.34)<br/>
			east (1.28)<br/>
			middle (1.17)<br/>
			palestinian (1.14)</td>
			<td><strong>Topic 04</strong><br/>
			nuclear (4.93)<br/>
			weapons (3.27)<br/>
			disarmament (2.01)<br/>
			treaty (1.70)<br/>
			proliferation (1.46)</td>
		</tr>
		<tr>
			<td><strong>Topic 05</strong><br/>
			rights (6.49)<br/>
			human (6.18)<br/>
			respect (1.15)<br/>
			fundamental (0.86)<br/>
			universal (0.82)</td>
			<td><strong>Topic 06</strong><br/>
			africa (3.83)<br/>
			south (3.32)<br/>
			african (1.70)<br/>
			namibia (1.38)<br/>
			apartheid (1.19)</td>
			<td><strong>Topic 07</strong><br/>
			security (6.13)<br/>
			council (5.88)<br/>
			permanent (1.50)<br/>
			reform (1.48)<br/>
			peace (1.30)</td>
			<td><strong>Topic 08</strong><br/>
			international (2.05)<br/>
			world (1.50)<br/>
			community (0.92)<br/>
			new (0.77)<br/>
			peace (0.67)</td>
			<td><strong>Topic 09</strong><br/>
			development (4.47)<br/>
			sustainable (1.18)<br/>
			economic (1.07)<br/>
			social (1.00)<br/>
			goals (0.93)</td>
		</tr>
	</tbody>
</table>

<p>Compared to the previous results for topic modeling speeches, we have almost lost all countries or regions except for South Africa and the Middle East. These are due to the regional conflicts that sparked interest in other parts of the world. Topics in the paragraphs like “Human rights,” “international relations,” “developing countries,” “nuclear weapons,” “security council,” “world peace,” and “sustainable development” (the last one probably occurring only lately) look much more reasonable compared to the topics of the speeches. Taking a look at the percentage values of the words, we can observe that they are dropping much faster, and the topics <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch8_term9" id="idm45634187986648"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch8_term10" id="idm45634187985544"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch8_term14" id="idm45634187984136"/>are more pronounced.</p>
</div></section>
</div></section>

<section data-type="sect1" data-pdf-bookmark="Latent Semantic Analysis/Indexing"><div class="sect1" id="idm45634188123720">
<h1>Latent Semantic Analysis/Indexing</h1>

<p>Another algorithm for performing topic modeling is <a contenteditable="false" data-type="indexterm" data-primary="singular value decomposition (SVD) for topic modeling" id="ch8_term15"/><a contenteditable="false" data-type="indexterm" data-primary="SVD (singular value decomposition) for topic modeling" id="ch8_term16"/><a contenteditable="false" data-type="indexterm" data-primary="topic modeling" data-secondary="SVD for" id="ch8_term17"/>based on the so-called singular value decomposition (SVD), another method from linear algebra.</p>

<p>Graphically, it is possible to conceive SVD as rearranging documents and words in a way to uncover a block structure in the <a contenteditable="false" data-type="indexterm" data-primary="document-term matrix" id="idm45634187975672"/>document-term matrix. There is a <a contenteditable="false" data-type="indexterm" data-primary="topic modeling" data-secondary="visualizations for" id="idm45634187974440"/><a contenteditable="false" data-type="indexterm" data-primary="visualization of data" data-secondary="with topic modeling" data-secondary-sortas="topic modeling" id="idm45634187973064"/>nice visualization of that process at <a href="https://oreil.ly/yJnWL">topicmodels.info</a>. <a data-type="xref" href="#svd-animation">Figure 8-2</a> shows the start of the document-term matrix and the resulting block diagonal form.</p>



<p>Making use of <a contenteditable="false" data-type="indexterm" data-primary="principal axis theorem" id="idm45634187968968"/>the principal axis theorem, orthogonal <em>n</em> × <em>n</em> matrices have an eigenvalue decomposition. Unfortunately, we do not have orthogonal square document-term matrices (except for rare cases). Therefore, we need a generalization called <em>singular value decomposition</em>. In its most general form, the theorem states that any <em>m</em> × <em>n</em> matrix <strong>V </strong>can be decomposed as follows:</p>

<div data-type="equation">
  <p><math alttext="normal upper V equals normal upper U dot normal upper Sigma dot normal upper V Superscript asterisk">
  <mrow>
    <mi mathvariant="normal">V</mi>
    <mo>=</mo>
    <mi mathvariant="normal">U</mi>
    <mo>·</mo>
    <mi>Σ</mi>
    <mo>·</mo>
    <msup><mi mathvariant="normal">V</mi> <mo>*</mo> </msup>
  </mrow>
</math></p>
</div>

<figure><div id="svd-animation" class="figure"><img src="Images/btap_0802.jpg" width="1143" height="2078"/>
	<h6><span class="label">Figure 8-2. </span>Visualization of topic modeling with SVD.</h6>
	</div></figure>

<p><em>U</em> is a unitary <em>m</em> × <em>m</em> matrix, <em>V*</em> is an <em>n</em> × <em>n</em> matrix, and <em>Σ</em> is an <em>m</em> × <em>n</em> diagonal matrix containing the singular values. There are exact solutions for this equation, but as they take a lot of time and computational effort to find, we are looking for approximate solutions that can be found quickly. The approximation works by only considering the largest singular values. This leads to <em>Σ</em> becoming a <em>t</em> × <em>t</em> matrix; in turn, <em>U </em>has <em>m</em> × <em>t</em> and <em>V*</em> <em>t</em> × <em>n</em> dimensions. Graphically, this is similar to the nonnegative matrix factorization, as shown in <a data-type="xref" href="#svd-decomposition">Figure 8-3</a>.</p>

<figure><div id="svd-decomposition" class="figure"><img src="Images/btap_0803.jpg" width="1424" height="486"/>
<h6><span class="label">Figure 8-3. </span>Schematic singular value decomposition.</h6>
</div></figure>

<p>The singular values are the diagonal elements of <em>Σ</em>. The document-topic relations are included in <em>U</em>, whereas the word-to-topic mapping is represented by <em>V*</em>. Note that neither the elements of <em>U</em> nor the elements of <em>V*</em> are guaranteed to be positive. The relative sizes of the contributions will still be interpretable, but the probability explanation is no longer valid.</p>

<section data-type="sect2" class="blueprint" data-pdf-bookmark="Blueprint: Creating a Topic Model for Paragraphs with SVD"><div class="sect2" id="idm45634187941960">
<h2>Blueprint: Creating a Topic Model for Paragraphs with SVD</h2>

<p>In scikit-learn <a contenteditable="false" data-type="indexterm" data-primary="paragraphs, topic modeling for" id="idm45634187940056"/><a contenteditable="false" data-type="indexterm" data-primary="scikit-learn" data-secondary="topic modeling with" id="idm45634187938952"/>the interface to SVD is identical to that of NMF. This time we start directly with the paragraphs:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="kn">from</code> <code class="nn">sklearn.decomposition</code> <code class="kn">import</code> <code class="n">TruncatedSVD</code>

<code class="n">svd_para_model</code> <code class="o">=</code> <code class="n">TruncatedSVD</code><code class="p">(</code><code class="n">n_components</code> <code class="o">=</code> <code class="mi">10</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">)</code>
<code class="n">W_svd_para_matrix</code> <code class="o">=</code> <code class="n">svd_para_model</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">tfidf_para_vectors</code><code class="p">)</code>
<code class="n">H_svd_para_matrix</code> <code class="o">=</code> <code class="n">svd_para_model</code><code class="o">.</code><code class="n">components_</code>
</pre>

<p>Our previously defined function for evaluating the topic model can also be used:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">display_topics</code><code class="p">(</code><code class="n">svd_para_model</code><code class="p">,</code> <code class="n">tfidf_para_vectorizer</code><code class="o">.</code><code class="n">get_feature_names</code><code class="p">())</code></pre>

<p class="pagebreak-before"><code>Out:</code></p>

<table>
	<tbody>
		<tr>
			<td><strong>Topic 00</strong><br/>
			nations (0.67)<br/>
			united (0.65)<br/>
			international (0.58)<br/>
			peace (0.46)<br/>
			world (0.46)</td>
			<td><strong>Topic 01</strong><br/>
			general (14.04)<br/>
			assembly (13.09)<br/>
			session (12.94)<br/>
			mr (10.02)<br/>
			president (8.59)</td>
			<td><strong>Topic 02</strong><br/>
			countries (19.15)<br/>
			development (14.61)<br/>
			economic (13.91)<br/>
			developing (13.00)<br/>
			session (10.29)</td>
			<td><strong>Topic 03</strong><br/>
			nations (4.41)<br/>
			united (4.06)<br/>
			development (0.95)<br/>
			organization (0.84)<br/>
			charter (0.80)</td>
			<td><strong>Topic 04</strong><br/>
			nuclear (21.13)<br/>
			weapons (14.01)<br/>
			disarmament (9.02)<br/>
			treaty (7.23)<br/>
			proliferation (6.31)</td>
		</tr>
		<tr>
			<td><strong>Topic 05</strong><br/>
			rights (29.50)<br/>
			human (28.81)<br/>
			nuclear (9.20)<br/>
			weapons (6.42)<br/>
			respect (4.98)</td>
			<td><strong>Topic 06</strong><br/>
			africa (8.73)<br/>
			south (8.24)<br/>
			united (3.91)<br/>
			african (3.71)<br/>
			nations (3.41)</td>
			<td><strong>Topic 07</strong><br/>
			council (14.96)<br/>
			security (13.38)<br/>
			africa (8.50)<br/>
			south (6.11)<br/>
			african (3.94)</td>
			<td><strong>Topic 08</strong><br/>
			world (48.49)<br/>
			international (41.03)<br/>
			peace (32.98)<br/>
			community (23.27)<br/>
			africa (22.00)</td>
			<td><strong>Topic 09</strong><br/>
			development (63.98)<br/>
			sustainable (20.78)<br/>
			peace (20.74)<br/>
			goals (15.92)<br/>
			africa (15.61)</td>
		</tr>
	</tbody>
</table>

<p>Most of the resulting topics are <a contenteditable="false" data-type="indexterm" data-primary="NMF (Nonnegative Matrix Factorization) for topic modeling" id="idm45634187827208"/><a contenteditable="false" data-type="indexterm" data-primary="topic modeling" data-secondary="NMF for" id="idm45634187826344"/>surprisingly similar to those of the nonnegative matrix factorization. However, the Middle East conflict does not appear as a separate topic this time. As the topic-word mappings can also have negative values, the normalization varies from topic to topic. Only the relative sizes of the words constituting the topics are relevant.</p>

<p>Don’t worry about the <a contenteditable="false" data-type="indexterm" data-primary="negative percentages in SVD" id="idm45634187824264"/>negative percentages. These arise as SVD does not guarantee positive values in W, so contributions of individual words might be negative. This means that words appearing in documents “reject” the corresponding topic.</p>

<p>If we want to determine the sizes of the topics, we now have to take a look at the singular values of the decomposition:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">svd_para</code><code class="o">.</code><code class="n">singular_values_</code>
</pre>

<p><code>Out:</code></p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">array</code><code class="p">([</code><code class="mf">68.21400653</code><code class="p">,</code> <code class="mf">39.20120165</code><code class="p">,</code> <code class="mf">36.36831431</code><code class="p">,</code> <code class="mf">33.44682727</code><code class="p">,</code> <code class="mf">31.76183677</code><code class="p">,</code>
<code class="err">      </code> <code class="mf">30.59557993</code><code class="p">,</code> <code class="mf">29.14061799</code><code class="p">,</code> <code class="mf">27.40264054</code><code class="p">,</code> <code class="mf">26.85684195</code><code class="p">,</code> <code class="mf">25.90408013</code><code class="p">])</code>
</pre>

<p>The sizes of the topics also correspond quite nicely with the ones from the NMF method for the paragraphs.</p>

<p>Both NMF and SVF have used the <a contenteditable="false" data-type="indexterm" data-primary="document-term matrix" id="idm45634187810824"/>document-term matrix (with TF-IDF transformations applied) as a basis for the topic decomposition. Also, the dimensions of the <em>U</em> matrix are identical to those of <em>W</em>; the same is true for <em>V*</em> and <em>H</em>. It is therefore not surprising that both of these methods produce similar and comparable results. As these methods are really fast to calculate, for real-life projects we recommend starting with the linear algebra methods.</p>

<p>We will now turn away from these linear-algebra-based methods <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch8_term11" id="idm45634187775832"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch8_term15" id="idm45634187774456"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch8_term16" id="idm45634187773080"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch8_term17" id="idm45634187771704"/>and focus on probabilistic topic models, which have become immensely popular in the past 20 years.</p>
</div></section>
</div></section>

<section data-type="sect1" data-pdf-bookmark="Latent Dirichlet Allocation"><div class="sect1" id="idm45634187982168">
<h1>Latent Dirichlet Allocation</h1>

<p>LDA is arguably the <a contenteditable="false" data-type="indexterm" data-primary="LDA (Latent Dirichlet Allocation) method" id="ch8_term18"/><a contenteditable="false" data-type="indexterm" data-primary="topic modeling" data-secondary="LDA for" id="ch8_term19"/>most prominent method of topic modeling in use today. It has been popularized during the last 15 years and can be adapted in flexible ways to different usage scenarios.</p>

<p>How does it work?</p>

<p>LDA views each document as consisting of different topics. In other words, each document is a mix of different topics. In the same way, topics are mixed from words. To keep the number of topics per document low and to have only a few, important words constituting the topics, LDA initially uses a <a href="https://oreil.ly/Kkd9k">Dirichlet distribution</a>, a so-called <em>Dirichlet prior</em>. This is applied both for assigning topics to documents and for finding words for the topics. The Dirichlet distribution ensures that documents have only a small number of topics and topics are mainly defined by a small number of words. Assuming that LDA generated topic distributions like the previous ones, a topic could be made up of words like <em>nuclear</em>, <em>treaty</em>, and <em>disarmament</em>, while another topic would be sampled by <em>sustainable</em>, <em>development</em>, etc.</p>

<p>After the initial assignments, the generative process starts. It uses the Dirichlet distributions for topics and words and tries to re-create the words from the original documents with stochastic sampling. This process has to be iterated many times and is therefore computationally intensive.<sup><a data-type="noteref" id="idm45634187759704-marker" href="ch08.xhtml#idm45634187759704">2</a></sup> On the other hand, the results can be used to generate documents for any identified topic.</p>

<section data-type="sect2" class="blueprint" data-pdf-bookmark="Blueprint: Creating a Topic Model for Paragraphs with LDA"><div class="sect2" id="ch08-topic-model-para">
<h2>Blueprint: Creating a Topic Model for Paragraphs with LDA</h2>

<p>Scikit-learn hides all these differences and uses the same API as the other topic modeling methods:</p>


<pre data-code-language="python" data-type="programlisting">
<code class="kn">from</code> <code class="nn">sklearn.feature_extraction.text</code> <code class="kn">import</code> <code class="n">CountVectorizer</code>

<code class="n">count_para_vectorizer</code> <code class="o">=</code> <code class="n">CountVectorizer</code><code class="p">(</code><code class="n">stop_words</code><code class="o">=</code><code class="n">stopwords</code><code class="p">,</code> <code class="n">min_df</code><code class="o">=</code><code class="mi">5</code><code class="p">,</code>
                        <code class="n">max_df</code><code class="o">=</code><code class="mf">0.7</code><code class="p">)</code>
<code class="n">count_para_vectors</code> <code class="o">=</code> <code class="n">count_para_vectorizer</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">paragraph_df</code><code class="p">[</code><code class="s2">"text"</code><code class="p">])</code>
</pre>

<pre data-code-language="python" data-type="programlisting">
<code class="kn">from</code> <code class="nn">sklearn.decomposition</code> <code class="kn">import</code> <code class="n">LatentDirichletAllocation</code>

<code class="n">lda_para_model</code> <code class="o">=</code> <code class="n">LatentDirichletAllocation</code><code class="p">(</code><code class="n">n_components</code> <code class="o">=</code> <code class="mi">10</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">)</code>
<code class="n">W_lda_para_matrix</code> <code class="o">=</code> <code class="n">lda_para_model</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">count_para_vectors</code><code class="p">)</code>
<code class="n">H_lda_para_matrix</code> <code class="o">=</code> <code class="n">lda_para_model</code><code class="o">.</code><code class="n">components_</code>

</pre><div data-type="warning" epub:type="warning">
	<h1>Waiting Time</h1>
	<p>Due to the probabilistic sampling, the process takes a lot longer than NMF and SVD. Expect at least minutes, if not hours, of runtime.</p>
</div>

<p>Our utility function can again be used to visualize the latent topics of the paragraph corpus:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">display_topics</code><code class="p">(</code><code class="n">lda_para_model</code><code class="p">,</code> <code class="n">tfidf_para</code><code class="o">.</code><code class="n">get_feature_names</code><code class="p">())</code>
</pre>

<p><code>Out:</code></p>

<table>
	<tbody>
		<tr>
			<td><strong>Topic 00</strong><br/>
			africa (2.38)<br/>
			people (1.86)<br/>
			south (1.57)<br/>
			namibia (0.88)<br/>
			regime (0.75)</td>
			<td><strong>Topic 01</strong><br/>
			republic (1.52)<br/>
			government (1.39)<br/>
			united (1.21)<br/>
			peace (1.16)<br/>
			people (1.02)</td>
			<td><strong>Topic 02</strong><br/>
			general (4.22)<br/>
			assembly (3.63)<br/>
			session (3.38)<br/>
			president (2.33)<br/>
			mr (2.32)</td>
			<td><strong>Topic 03</strong><br/>
			human (3.62)<br/>
			rights (3.48)<br/>
			international (1.83)<br/>
			law (1.01)<br/>
			terrorism (0.99)</td>
			<td><strong>Topic 04</strong><br/>
			world (2.22)<br/>
			people (1.14)<br/>
			countries (0.94)<br/>
			years (0.88)<br/>
			today (0.66)</td>
		</tr>
		<tr>
			<td><strong>Topic 05</strong><br/>
			peace (1.76)<br/>
			security (1.63)<br/>
			east (1.34)<br/>
			middle (1.34)<br/>
			israel (1.24)</td>
			<td><strong>Topic 06</strong><br/>
			countries (3.19)<br/>
			development (2.70)<br/>
			economic (2.22)<br/>
			developing (1.61)<br/>
			international (1.45)</td>
			<td><strong>Topic 07</strong><br/>
			nuclear (3.14)<br/>
			weapons (2.32)<br/>
			disarmament (1.82)<br/>
			states (1.47)<br/>
			arms (1.46)</td>
			<td><strong>Topic 08</strong><br/>
			nations (5.50)<br/>
			united (5.11)<br/>
			international (1.46)<br/>
			security (1.45)<br/>
			organization (1.44)</td>
			<td><strong>Topic 09</strong><br/>
			international (1.96)<br/>
			world (1.91)<br/>
			peace (1.60)<br/>
			economic (1.00)<br/>
			relations (0.99)</td>
		</tr>
	</tbody>
</table>

<p>It’s interesting to observe that LDA has generated a completely different topic structure compared to the linear algebra methods described earlier. <em>People</em> is the most prominent word in three quite different topics. In Topic 04, South Africa is related to Israel and Palestine, while in Topic 00, Cyprus, Afghanistan, and Iraq are related. This is not easy to explain. This is also reflected in the slowly decreasing word weights of the topics.</p>

<p>Other topics are easier to comprehend, such as climate change, nuclear weapons, elections, developing countries, and organizational questions.</p>

<p>In this example, LDA does not yield much better results than either NMF or SVD. However, due to the sampling process, LDA is not limited to sample topics just consisting of words. There are several variations, such as author-topic models, that can also sample categorical features. Moreover, as there is so much research going on in LDA, other ideas are published quite frequently, which extend the focus of the method well beyond text analytics (see, for example, Minghui Qiu et al., <a href="https://oreil.ly/dnqq5">“It Is Not Just What We Say, But How We Say Them: LDA-based Behavior-Topic Model”</a> or Rahji Abdurehman, <a href="https://oreil.ly/DDClf">“Keyword-Assisted LDA: Exploring New Methods for Supervised Topic Modeling”</a>).</p>
</div></section>

<section data-type="sect2" class="blueprint" data-pdf-bookmark="Blueprint: Visualizing LDA Results"><div class="sect2" id="ch08visualizing_lda_results">
<h2>Blueprint: Visualizing LDA Results</h2>

<p>As LDA is so popular, there is a <a contenteditable="false" data-type="indexterm" data-primary="topic modeling" data-secondary="visualizations for" id="ch8_term20"/><a contenteditable="false" data-type="indexterm" data-primary="visualization of data" data-secondary="with topic modeling" data-secondary-sortas="topic modeling" id="ch8_term21"/>nice package in Python to visualize the LDA results <a contenteditable="false" data-type="indexterm" data-primary="pyLDAvis package" id="idm45634187548424"/>called pyLDAvis.<sup><a data-type="noteref" id="idm45634187547192-marker" href="ch08.xhtml#idm45634187547192">3</a></sup> Fortunately, it can directly <a contenteditable="false" data-type="indexterm" data-primary="scikit-learn" data-secondary="with visualization of results" data-secondary-sortas="visualization of results" id="idm45634187544984"/>use the results from sciki-learn for its visualization.</p>

<p>Be careful, this <a contenteditable="false" data-type="indexterm" data-primary="execution time" id="idm45634187542824"/><a contenteditable="false" data-type="indexterm" data-primary="time, execution" id="idm45634187541688"/>takes some time:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="kn">import</code> <code class="nn">pyLDAvis.sklearn</code>

<code class="n">lda_display</code> <code class="o">=</code> <code class="n">pyLDAvis</code><code class="o">.</code><code class="n">sklearn</code><code class="o">.</code><code class="n">prepare</code><code class="p">(</code><code class="n">lda_para_model</code><code class="p">,</code> <code class="n">count_para_vectors</code><code class="p">,</code>
<code class="err"> </code>                           <code class="n">count_para_vectorizer</code><code class="p">,</code> <code class="n">sort_topics</code><code class="o">=</code><code class="bp">False</code><code class="p">)</code>
<code class="n">pyLDAvis</code><code class="o">.</code><code class="n">display</code><code class="p">(</code><code class="n">lda_display</code><code class="p">)</code>
</pre>

<p><code>Out:</code></p>

<figure><div id="pyldavis" class="figure"><img src="Images/btap_08in02.jpg" width="1180" height="736"/>
	<h6/>
	<!-- <figcaption>Output of pyLDAvis for the paragraph topic model.</figcaption> -->
</div></figure>

<p>There is a multitude of information available in the visualization. Let’s start with the topic “bubbles” and click the topic. Now take a look at the red bars, which symbolize the word distribution in the currently selected topic. As the length of the bars is not decreasing quickly, Topic 2 is not very pronounced. This is the same effect you can see in the table from <a data-type="xref" href="#ch08-topic-model-para">“Blueprint: Creating a Topic Model for Paragraphs with LDA”</a> (look at Topic 1, where we have used the array indices, whereas pyLDAvis starts enumerating the topics with 1).</p>

<p>To visualize the results, the topics are mapped from their original dimension (the number of words) into two dimensions using <a contenteditable="false" data-type="indexterm" data-primary="principal component analysis (PCA)" id="idm45634187514072"/><a contenteditable="false" data-type="indexterm" data-primary="PCA (principal component analysis)" id="idm45634187513000"/>principal component analysis (PCA), a standard method for dimension reduction. This results in a point; the circle is added to see the relative sizes of the topics. It is possible to use <a contenteditable="false" data-type="indexterm" data-primary="t-SNE algorithm" id="idm45634187511592"/>T-SNE instead of PCA by passing <code>mds="tsne"</code> as a parameter in the preparation stage. This changes the intertopic distance map and shows fewer overlapping topic bubbles. This is, however, just an artifact of projecting the many word dimensions in just two for visualization. Therefore, it’s always a good idea to look at the word distribution of the topics and not exclusively trust a low-dimensional visualization.</p>

<p>It’s interesting to see the strong overlap of Topics 4, 6, and 10 (“international”), whereas Topic 3 (“general assembly”) seems to be far away from all other topics. By hovering over the other topic bubbles or clicking them, you can take a look at their respective word distributions on the right side. Although not all the topics are perfectly separated, there are some (like Topic 1 and Topic 7) that are far away from the others. Try to hover over them and you will find that their word content is also different from each other. For such topics, it might be useful to extract the most representative documents and use them as a training set for supervised learning.</p>

<p>pyLDAvis is a nice tool to play with and is well-suited for screenshots in presentations. Even though it looks explorative, the real exploration in the topic models takes place by modifying the features and the hyperparameters of the algorithms.</p>

<p>Using pyLDAvis gives us a good idea how the topics are arranged with respect to one another and which individual words are important. However, if we need a more qualitative understanding of the topics, we can use <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch8_term18" id="idm45634187507128"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch8_term19" id="idm45634187505752"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch8_term20" id="idm45634187504376"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch8_term21" id="idm45634187503000"/>additional visualizations.</p>
</div></section>
</div></section>

<section data-type="sect1" class="blueprint pagebreak-after" data-pdf-bookmark="Blueprint: Using Word Clouds to Display and Compare Topic Models"><div class="sect1" id="idm45634187553608">
<h1>Blueprint: Using Word Clouds to Display and Compare Topic Models</h1>

<p>So far, we have used lists to display the topic models. This way, we could nicely identify how pronounced the different topics were. However, in many cases <a contenteditable="false" data-type="indexterm" data-primary="topic modeling" data-secondary="word clouds for display and comparing of" id="ch8_term22"/><a contenteditable="false" data-type="indexterm" data-primary="visualization of data" data-secondary="with word clouds" data-secondary-sortas="word clouds" id="ch8_term23"/><a contenteditable="false" data-type="indexterm" data-primary="word clouds " id="ch8_term24"/><a contenteditable="false" data-type="indexterm" data-primary="word frequency, analysis of" data-secondary="word clouds for" id="ch8_term25"/>topic models are used to give you a first impression about the validity of the corpus and better visualizations. As we have seen in <a data-type="xref" href="ch01.xhtml#ch-exploration">Chapter 1</a>, word clouds are a qualitative and intuitive instrument to show this.</p>

<p>We can directly use word clouds to show our topic models. The code is easily derived from the previously defined <code>display_topics</code> function:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="kn">import</code> <code class="nn">matplotlib.pyplot</code> <code class="kn">as</code> <code class="nn">plt</code>
<code class="kn">from</code> <code class="nn">wordcloud</code> <code class="kn">import</code> <code class="n">WordCloud</code>

<code class="k">def</code> <code class="nf">wordcloud_topics</code><code class="p">(</code><code class="n">model</code><code class="p">,</code> <code class="n">features</code><code class="p">,</code> <code class="n">no_top_words</code><code class="o">=</code><code class="mi">40</code><code class="p">):</code>
<code class="err"> </code> <code class="err"> </code> <code class="k">for</code> <code class="n">topic</code><code class="p">,</code> <code class="n">words</code> <code class="ow">in</code> <code class="nb">enumerate</code><code class="p">(</code><code class="n">model</code><code class="o">.</code><code class="n">components_</code><code class="p">):</code>
<code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="n">size</code> <code class="o">=</code> <code class="p">{}</code>
<code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="n">largest</code> <code class="o">=</code> <code class="n">words</code><code class="o">.</code><code class="n">argsort</code><code class="p">()[::</code><code class="o">-</code><code class="mi">1</code><code class="p">]</code> <code class="c1"># invert sort order</code>
<code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="n">no_top_words</code><code class="p">):</code>
<code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="n">size</code><code class="p">[</code><code class="n">features</code><code class="p">[</code><code class="n">largest</code><code class="p">[</code><code class="n">i</code><code class="p">]]]</code> <code class="o">=</code> <code class="nb">abs</code><code class="p">(</code><code class="n">words</code><code class="p">[</code><code class="n">largest</code><code class="p">[</code><code class="n">i</code><code class="p">]])</code>
<code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="n">wc</code> <code class="o">=</code> <code class="n">WordCloud</code><code class="p">(</code><code class="n">background_color</code><code class="o">=</code><code class="s2">"white"</code><code class="p">,</code> <code class="n">max_words</code><code class="o">=</code><code class="mi">100</code><code class="p">,</code>
                       <code class="n">width</code><code class="o">=</code><code class="mi">960</code><code class="p">,</code> <code class="n">height</code><code class="o">=</code><code class="mi">540</code><code class="p">)</code>
<code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="n">wc</code><code class="o">.</code><code class="n">generate_from_frequencies</code><code class="p">(</code><code class="n">size</code><code class="p">)</code>
<code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="n">plt</code><code class="o">.</code><code class="n">figure</code><code class="p">(</code><code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">12</code><code class="p">,</code><code class="mi">12</code><code class="p">))</code>
<code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="n">plt</code><code class="o">.</code><code class="n">imshow</code><code class="p">(</code><code class="n">wc</code><code class="p">,</code> <code class="n">interpolation</code><code class="o">=</code><code class="s1">'bilinear'</code><code class="p">)</code>
<code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="n">plt</code><code class="o">.</code><code class="n">axis</code><code class="p">(</code><code class="s2">"off"</code><code class="p">)</code>
<code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="c1"># if you don't want to save the topic model, comment the next line</code>
<code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="err"> </code> <code class="n">plt</code><code class="o">.</code><code class="n">savefig</code><code class="p">(</code><code class="n">f</code><code class="s1">'topic{topic}.png'</code><code class="p">)</code>
</pre>

<p>By using this code, we can qualitatively <a contenteditable="false" data-type="indexterm" data-primary="LDA (Latent Dirichlet Allocation) method" id="ch8_term26"/><a contenteditable="false" data-type="indexterm" data-primary="topic modeling" data-secondary="LDA for" id="ch8_term27"/><a contenteditable="false" data-type="indexterm" data-primary="NMF (Nonnegative Matrix Factorization) for topic modeling" id="ch8_term28"/><a contenteditable="false" data-type="indexterm" data-primary="topic modeling" data-secondary="NMF for" id="ch8_term29"/>compare the results of the NMF model (<a data-type="xref" href="#fig-wordcloud-nmf">Figure 8-4</a>) with those of the LDA model(<a data-type="xref" href="#fig-wordcloud-lda">Figure 8-5</a>). Larger words are more important in their respective topics. If many words have roughly the same size, the topic is not well-pronounced:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">wordcloud_topics</code><code class="p">(</code><code class="n">nmf_para_model</code><code class="p">,</code> <code class="n">tfidf_para_vectorizer</code><code class="o">.</code><code class="n">get_feature_names</code><code class="p">())</code>
<code class="n">wordcloud_topics</code><code class="p">(</code><code class="n">lda_para_model</code><code class="p">,</code> <code class="n">count_para_vectorizer</code><code class="o">.</code><code class="n">get_feature_names</code><code class="p">())</code>
</pre>


<div data-type="warning" epub:type="warning">
	<h1>Word Clouds Use Individual Scaling</h1>
	
	<p>The font sizes in the word clouds use scaling within each topic separately, and therefore it’s important to verify with the actual numbers before drawing any final conclusions.</p>
	</div>
	
	<p>The presentation is now way more compelling. It is much easier to match topics between the two methods, like 0-NMF with 8-LDA. For most topics, this is quite obvious, but there are also differences. 1-LDA (“people republic”) has no equivalent in NMF, whereas 9-NMF (“sustainable development”) cannot <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch8_term26" id="idm45634187258008"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch8_term27" id="idm45634187256632"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch8_term28" id="idm45634187255256"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch8_term29" id="idm45634187253880"/>be found in LDA.</p>
	
	<p>As we have found a nice qualitative visualization of the topics, we are now interested in how that topic distribution has <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch8_term22" id="idm45634187251784"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch8_term23" id="idm45634187250408"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch8_term24" id="idm45634187249032"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch8_term25" id="idm45634187247656"/>changed over time.</p>

<figure class="width-90"><div id="fig-wordcloud-nmf" class="figure"><img src="Images/btap_0804.jpg" width="1428" height="2000"/>
<h6><span class="label">Figure 8-4. </span>Word clouds representing the NMF topic model.</h6>
</div></figure>

<figure class="width-90"><div id="fig-wordcloud-lda" class="figure"><img src="Images/btap_0805.jpg" width="1428" height="2000"/>
<h6><span class="label">Figure 8-5. </span>Word clouds representing the LDA topic model.</h6>
</div></figure>

</div></section>

<section data-type="sect1" class="blueprint" data-pdf-bookmark="Blueprint: Calculating Topic Distribution of Documents and Time Evolution"><div class="sect1" id="idm45634187500904">
<h1>Blueprint: Calculating Topic Distribution of Documents and Time Evolution</h1>

<p>As you can see in the analysis at the beginning of the chapter, the <a contenteditable="false" data-type="indexterm" data-primary="speeches, UN" data-secondary="topic modeling with" id="idm45634187240136"/>speech metadata changes over time. This leads to the interesting question of <a contenteditable="false" data-type="indexterm" data-primary="time evolution and topic distribution of documents" id="ch8_term30"/><a contenteditable="false" data-type="indexterm" data-primary="topic modeling" data-secondary="changes over time in" id="ch8_term31"/>how the distribution of the topics changes over time. It turns out that this is easy to calculate and insightful.</p>

<p>Like the scikit-learn vectorizers, the <a contenteditable="false" data-type="indexterm" data-primary="transform method" id="idm45634187215448"/>topic models also have a <code>transform</code> method, which calculates the topic distribution of existing documents keeping the already fitted topic model. Let’s use this to first separate speeches before 1990 from those after 1990. For this, we <a contenteditable="false" data-type="indexterm" data-primary="NumPy library" id="idm45634187213576"/>create NumPy arrays for the documents before and after 1990:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="kn">import</code> <code class="nn">numpy</code> <code class="kn">as</code> <code class="nn">np</code>
<code class="n">before_1990</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">(</code><code class="n">paragraph_df</code><code class="p">[</code><code class="s2">"year"</code><code class="p">]</code> <code class="o">&lt;</code> <code class="mi">1990</code><code class="p">)</code>
<code class="n">after_1990</code> <code class="o">=</code> <code class="o">~</code> <code class="n">before_1990</code>
</pre>

<p>Then we can calculate the respective <em>W</em> matrices:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">W_para_matrix_early</code> <code class="o">=</code> <code class="n">nmf_para_model</code><code class="o">.</code><code class="n">transform</code><code class="p">(</code><code class="n">tfidf_para_vectors</code><code class="p">[</code><code class="n">before_1990</code><code class="p">])</code>
<code class="n">W_para_matrix_late</code><code class="err"> </code> <code class="o">=</code> <code class="n">nmf_para_model</code><code class="o">.</code><code class="n">transform</code><code class="p">(</code><code class="n">tfidf_para_vectors</code><code class="p">[</code><code class="n">after_1990</code><code class="p">])</code>
<code class="k">print</code><code class="p">(</code><code class="n">W_para_matrix_early</code><code class="o">.</code><code class="n">sum</code><code class="p">(</code><code class="n">axis</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code><code class="o">/</code><code class="n">W_para_matrix_early</code><code class="o">.</code><code class="n">sum</code><code class="p">()</code><code class="o">*</code><code class="mf">100.0</code><code class="p">)</code>
<code class="k">print</code><code class="p">(</code><code class="n">W_para_matrix_late</code><code class="o">.</code><code class="n">sum</code><code class="p">(</code><code class="n">axis</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code><code class="o">/</code><code class="n">W_para_matrix_late</code><code class="o">.</code><code class="n">sum</code><code class="p">()</code><code class="o">*</code><code class="mf">100.0</code><code class="p">)</code></pre>

<p><code>Out:</code></p>

<pre data-code-language="python" data-type="programlisting">
<code class="p">[</code><code class="s1">'9.34'</code><code class="p">,</code> <code class="s1">'10.43'</code><code class="p">,</code> <code class="s1">'12.18'</code><code class="p">,</code> <code class="s1">'12.18'</code><code class="p">,</code> <code class="s1">'7.82'</code><code class="p">,</code> <code class="s1">'6.05'</code><code class="p">,</code> <code class="s1">'12.10'</code><code class="p">,</code> <code class="s1">'5.85'</code><code class="p">,</code> <code class="s1">'17.36'</code><code class="p">,</code>
 <code class="s1">'6.69'</code><code class="p">]</code>
<code class="p">[</code><code class="s1">'7.48'</code><code class="p">,</code> <code class="s1">'8.34'</code><code class="p">,</code> <code class="s1">'9.75'</code><code class="p">,</code> <code class="s1">'9.75'</code><code class="p">,</code> <code class="s1">'6.26'</code><code class="p">,</code> <code class="s1">'4.84'</code><code class="p">,</code> <code class="s1">'9.68'</code><code class="p">,</code> <code class="s1">'4.68'</code><code class="p">,</code> <code class="s1">'13.90'</code><code class="p">,</code>
 <code class="s1">'5.36'</code><code class="p">]</code>
</pre>

<p>The result is interesting, as some percentages have changed considerably; specifically, the size of the second-to-last topic is much smaller in the later years. We will now try to take a deeper look at the topics and their changes over time.</p>

<p>Let’s try to calculate the distribution for individual years and see whether we can find a <a contenteditable="false" data-type="indexterm" data-primary="visualization of data" data-secondary="with topic modeling" data-secondary-sortas="topic modeling" id="idm45634186932008"/><a contenteditable="false" data-type="indexterm" data-primary="topic modeling" data-secondary="visualizations for" id="idm45634186930488"/>visualization to uncover possible patterns:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">year_data</code> <code class="o">=</code> <code class="p">[]</code>
<code class="n">years</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">unique</code><code class="p">(</code><code class="n">paragraph_years</code><code class="p">)</code>
<code class="k">for</code> <code class="n">year</code> <code class="ow">in</code> <code class="n">tqdm</code><code class="p">(</code><code class="n">years</code><code class="p">):</code>
<code class="err">   </code> <code class="n">W_year</code> <code class="o">=</code> <code class="n">nmf_para_model</code><code class="o">.</code><code class="n">transform</code><code class="p">(</code><code class="n">tfidf_para_vectors</code><code class="p">[</code><code class="n">paragraph_years</code> \
                                      <code class="o">==</code> <code class="n">year</code><code class="p">])</code>
<code class="err">   </code> <code class="n">year_data</code><code class="o">.</code><code class="n">append</code><code class="p">([</code><code class="n">year</code><code class="p">]</code> <code class="o">+</code> <code class="nb">list</code><code class="p">(</code><code class="n">W_year</code><code class="o">.</code><code class="n">sum</code><code class="p">(</code><code class="n">axis</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code><code class="o">/</code><code class="n">W_year</code><code class="o">.</code><code class="n">sum</code><code class="p">()</code><code class="o">*</code><code class="mf">100.0</code><code class="p">))</code>
</pre>

<p>To make the plots more intuitive, we first create a list of topics with their two most important words:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">topic_names</code> <code class="o">=</code> <code class="p">[]</code>
<code class="n">voc</code> <code class="o">=</code> <code class="n">tfidf_para_vectorizer</code><code class="o">.</code><code class="n">get_feature_names</code><code class="p">()</code>
<code class="k">for</code> <code class="n">topic</code> <code class="ow">in</code> <code class="n">nmf_para_model</code><code class="o">.</code><code class="n">components_</code><code class="p">:</code>
<code class="err">   </code> <code class="n">important</code> <code class="o">=</code> <code class="n">topic</code><code class="o">.</code><code class="n">argsort</code><code class="p">()</code>
<code class="err">   </code> <code class="n">top_word</code> <code class="o">=</code> <code class="n">voc</code><code class="p">[</code><code class="n">important</code><code class="p">[</code><code class="o">-</code><code class="mi">1</code><code class="p">]]</code> <code class="o">+</code> <code class="s2">" "</code> <code class="o">+</code> <code class="n">voc</code><code class="p">[</code><code class="n">important</code><code class="p">[</code><code class="o">-</code><code class="mi">2</code><code class="p">]]</code>
<code class="err">   </code> <code class="n">topic_names</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="s2">"Topic "</code> <code class="o">+</code> <code class="n">top_word</code><code class="p">)</code>
</pre>

<p>We then combine the results in a <code>DataFrame</code> with the previous topics as column names, so we can easily visualize that as follows:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">df_year</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">year_data</code><code class="p">,</code>
<code class="err">              </code> <code class="n">columns</code><code class="o">=</code><code class="p">[</code><code class="s2">"year"</code><code class="p">]</code> <code class="o">+</code> <code class="n">topic_names</code><code class="p">)</code><code class="o">.</code><code class="n">set_index</code><code class="p">(</code><code class="s2">"year"</code><code class="p">)</code>
<code class="n">df_year</code><code class="o">.</code><code class="n">plot</code><code class="o">.</code><code class="n">area</code><code class="p">()</code>
</pre>

<p><code>Out:</code></p>

<figure><div id="fig-topic-time" class="figure"><img src="Images/btap_08in03.jpg" width="1462" height="828"/><h6/></div></figure>

<p>In the resulting graph you can see how the topic distribution changes over the year.  We can recognize that the “sustainable development” topic is continuously increasing, while “south africa” has lost popularity after the apartheid regime ended.</p>

<p>Compared to showing the time development of single (guessed) words, topics seem to be a more natural entity as they arise from the text corpus itself. Note that this <a contenteditable="false" data-type="indexterm" data-primary="unsupervised methods" id="idm45634186699992"/>chart was generated with an unsupervised method exclusively, so there is no bias in it. Everything was already in the debates data; we have just <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch8_term30" id="idm45634186698600"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch8_term31" id="idm45634186697224"/>uncovered it.</p>

<p>So far, we have used <a contenteditable="false" data-type="indexterm" data-primary="scikit-learn" data-secondary="topic modeling with" id="idm45634186695272"/>scikit-learn exclusively for topic modeling. In the Python ecosystem, there is a specialized library for topic models called Gensim, which we will now investigate.</p>
</div></section>

<section data-type="sect1" data-pdf-bookmark="Using Gensim for Topic Modeling"><div class="sect1" id="idm45634187241528">
<h1>Using Gensim for Topic Modeling</h1>

<p>Apart from scikit-learn, <a href="https://oreil.ly/Ybn63"><em>Gensim</em></a> is another <a contenteditable="false" data-type="indexterm" data-primary="Gensim" data-secondary="for topic modeling" data-secondary-sortas="topic modeling" id="ch8_term32"/><a contenteditable="false" data-type="indexterm" data-primary="topic modeling" data-secondary="Gensim for" id="ch8_term33"/>popular tool for performing topic modeling in Python. Compared to scikit-learn, it offers more algorithms for calculating topic models and can also give estimates about the quality of the model.</p>

<section data-type="sect2" class="blueprint" data-pdf-bookmark="Blueprint: Preparing Data for Gensim"><div class="sect2" id="idm45634186687528">
<h2>Blueprint: Preparing Data for Gensim</h2>

<p>Before we can start calculating the Gensim models, we have to <a contenteditable="false" data-type="indexterm" data-primary="data preprocessing" data-secondary="for topic modeling" data-secondary-sortas="topic modeling" id="idm45634186685560"/><a contenteditable="false" data-type="indexterm" data-primary="topic modeling" data-secondary="data preparation with Gensim for" id="idm45634186683912"/>prepare the data. Unfortunately, the API and the terminology are different from scikit-learn. In the first step, we have to prepare the vocabulary. <a contenteditable="false" data-type="indexterm" data-primary="vocabulary" data-secondary="filtering of" id="idm45634186682184"/><a contenteditable="false" data-type="indexterm" data-primary="tokenization/tokens" data-secondary="in data preparation" data-secondary-sortas="data preparation" id="idm45634186680808"/><a contenteditable="false" data-type="indexterm" data-primary="text data preparation" data-secondary="tokenizers for" id="idm45634186657976"/>Gensim has no integrated tokenizer and expects each line of a document corpus to be already tokenized:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="c1"># create tokenized documents</code>
<code class="n">gensim_paragraphs</code> <code class="o">=</code> <code class="p">[[</code><code class="n">w</code> <code class="k">for</code> <code class="n">w</code> <code class="ow">in</code> <code class="n">re</code><code class="o">.</code><code class="n">findall</code><code class="p">(</code><code class="s-Affix">r</code><code class="s1">'\b\w\w+\b'</code> <code class="p">,</code> <code class="n">paragraph</code><code class="o">.</code><code class="n">lower</code><code class="p">())</code>
<code class="err"> </code>                         <code class="k">if</code> <code class="n">w</code> <code class="ow">not</code> <code class="ow">in</code> <code class="n">stopwords</code><code class="p">]</code>
<code class="err">                          </code>   <code class="k">for</code> <code class="n">paragraph</code> <code class="ow">in</code> <code class="n">paragraph_df</code><code class="p">[</code><code class="s2">"text"</code><code class="p">]]</code></pre>

<p>After tokenization, we can initialize the Gensim dictionary with these tokenized documents. Think of the dictionary as a mapping from words to columns (like the features we used in <a data-type="xref" href="ch02.xhtml#ch-api">Chapter 2</a>):</p>

<pre data-code-language="python" data-type="programlisting">
<code class="kn">from</code> <code class="nn">gensim.corpora</code> <code class="kn">import</code> <code class="n">Dictionary</code>
<code class="n">dict_gensim_para</code> <code class="o">=</code> <code class="n">Dictionary</code><code class="p">(</code><code class="n">gensim_paragraphs</code><code class="p">)</code>
</pre>

<p>Similar to the scikit-learn <code>TfidfVectorizer</code>, we can reduce the vocabulary by <a contenteditable="false" data-type="indexterm" data-primary="words" data-secondary="filtering" id="idm45634186607208"/>filtering out words that <a contenteditable="false" data-type="indexterm" data-primary="minimum frequency of words" id="idm45634186605832"/>appear not often enough or too frequently. To keep the dimensions low, we choose a minimum of five documents in which words must appear, but not in more than 70% of the documents. As we saw in <a data-type="xref" href="ch02.xhtml#ch-api">Chapter 2</a>, these parameters can be optimized and require some experimentation.</p>

<p>In Gensim, this is implemented via a filter with the <a contenteditable="false" data-type="indexterm" data-primary="no_below parameter (Gensim)" id="idm45634186597032"/><a contenteditable="false" data-type="indexterm" data-primary="no_above parameter (Gensim)" id="idm45634186595960"/><a contenteditable="false" data-type="indexterm" data-primary="max_df parameter" id="idm45634186594888"/><a contenteditable="false" data-type="indexterm" data-primary="min_df parameter" id="idm45634186593784"/><a contenteditable="false" data-type="indexterm" data-primary="scikit-learn" data-secondary="max_df and min_df parameters of" id="idm45634186592680"/>parameters <code>no_below</code> and <code>no_above</code> (in scikit-learn, the analog would be <code>min_df</code> and <code>max_df</code>):</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">dict_gensim_para</code><code class="o">.</code><code class="n">filter_extremes</code><code class="p">(</code><code class="n">no_below</code><code class="o">=</code><code class="mi">5</code><code class="p">,</code> <code class="n">no_above</code><code class="o">=</code><code class="mf">0.7</code><code class="p">)</code>
</pre>

<p>With the dictionary read, we can now use Gensim to <a contenteditable="false" data-type="indexterm" data-primary="bag-of-words models" id="idm45634186586104"/>calculate the bag-of-words matrix (which is called a <em>corpus</em> in Gensim, but we will stick with our current terminology):</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">bow_gensim_para</code> <code class="o">=</code> <code class="p">[</code><code class="n">dict_gensim_para</code><code class="o">.</code><code class="n">doc2bow</code><code class="p">(</code><code class="n">paragraph</code><code class="p">)</code> \
                    <code class="k">for</code> <code class="n">paragraph</code> <code class="ow">in</code> <code class="n">gensim_paragraphs</code><code class="p">]</code>
</pre>

<p>Finally, we can <a contenteditable="false" data-type="indexterm" data-primary="TF-IDF (Term-Frequency Inverse Document Frequency) weighting" data-secondary="for data preparation in Gensim" data-secondary-sortas="data preparation in Gensim" id="idm45634186537800"/><a contenteditable="false" data-type="indexterm" data-primary="TF-IDF (Term-Frequency Inverse Document Frequency) weighting" data-secondary="vectorization with" id="idm45634186536344"/><a contenteditable="false" data-type="indexterm" data-primary="vectorizers/vectorization" data-secondary="with TF-IDF weighting" data-secondary-sortas="TF-IDF weighting" id="idm45634186535032"/>perform the TF-IDF transformation. The first line fits the bag-of-words model, while the second line transforms the weights:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="kn">from</code> <code class="nn">gensim.models</code> <code class="kn">import</code> <code class="n">TfidfModel</code>
<code class="n">tfidf_gensim_para</code> <code class="o">=</code> <code class="n">TfidfModel</code><code class="p">(</code><code class="n">bow_gensim_para</code><code class="p">)</code>
<code class="n">vectors_gensim_para</code> <code class="o">=</code> <code class="n">tfidf_gensim_para</code><code class="p">[</code><code class="n">bow_gensim_para</code><code class="p">]</code>
</pre>

<p>The <code>vectors_gensim_para</code> matrix <a contenteditable="false" data-type="indexterm" data-primary="vectors_gensim_para matrix" id="idm45634186487384"/><a contenteditable="false" data-type="indexterm" data-primary="vectorizers/vectorization" data-secondary="with topic modeling" data-secondary-sortas="topic modeling" id="idm45634186486488"/>is the one that we will use for all upcoming topic modeling tasks with Gensim.</p>
</div></section>

<section data-type="sect2" class="blueprint" data-pdf-bookmark="Blueprint: Performing Nonnegative Matrix Factorization with Gensim"><div class="sect2" id="idm45634186686904">
<h2>Blueprint: Performing Nonnegative Matrix Factorization with Gensim</h2>

<p>Let’s check first <a contenteditable="false" data-type="indexterm" data-primary="NMF (Nonnegative Matrix Factorization) for topic modeling" id="idm45634186482808"/><a contenteditable="false" data-type="indexterm" data-primary="topic modeling" data-secondary="NMF for" id="idm45634186481704"/><a contenteditable="false" data-type="indexterm" data-primary="topic modeling" data-secondary="NMF with Gensim for" id="idm45634186418360"/><a contenteditable="false" data-type="indexterm" data-primary="speeches, UN" data-secondary="topic modeling with" id="ch8_term38"/>the results of NMF and see whether we can reproduce those of scikit-learn:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="kn">from</code> <code class="nn">gensim.models.nmf</code> <code class="kn">import</code> <code class="n">Nmf</code>
<code class="n">nmf_gensim_para</code> <code class="o">=</code> <code class="n">Nmf</code><code class="p">(</code><code class="n">vectors_gensim_para</code><code class="p">,</code> <code class="n">num_topics</code><code class="o">=</code><code class="mi">10</code><code class="p">,</code>
                      <code class="n">id2word</code><code class="o">=</code><code class="n">dict_gensim_para</code><code class="p">,</code> <code class="n">kappa</code><code class="o">=</code><code class="mf">0.1</code><code class="p">,</code> <code class="n">eval_every</code><code class="o">=</code><code class="mi">5</code><code class="p">)</code>
</pre>

<p>The evaluation can take a while. Although Gensim offers a <code>show_topics</code> method for directly displaying the topics, we have a different implementation to make it look like the scikit-learn results so it’s easier to compare them:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">display_topics_gensim</code><code class="p">(</code><code class="n">nmf_gensim_para</code><code class="p">)</code>
</pre>

<p><code>Out:</code></p>

<table>
	<tbody>
		<tr>
			<td><strong>Topic 00</strong><br/>
			nations (0.03)<br/>
			united (0.02)<br/>
			human (0.02)<br/>
			rights (0.02)<br/>
			role (0.01)</td>
			<td><strong>Topic 01</strong><br/>
			africa (0.02)<br/>
			south (0.02)<br/>
			people (0.02)<br/>
			government (0.01)<br/>
			republic (0.01)</td>
			<td><strong>Topic 02</strong><br/>
			economic (0.01)<br/>
			development (0.01)<br/>
			countries (0.01)<br/>
			social (0.01)<br/>
			international (0.01)</td>
			<td><strong>Topic 03</strong><br/>
			countries (0.02)<br/>
			developing (0.02)<br/>
			resources (0.01)<br/>
			sea (0.01)<br/>
			developed (0.01)</td>
			<td><strong>Topic 04</strong><br/>
			israel (0.02)<br/>
			arab (0.02)<br/>
			palestinian (0.02)<br/>
			council (0.01)<br/>
			security (0.01)</td>
		</tr>
		<tr>
			<td><strong>Topic 05</strong><br/>
			organization (0.02)<br/>
			charter (0.02)<br/>
			principles (0.02)<br/>
			member (0.01)<br/>
			respect (0.01)</td>
			<td><strong>Topic 06</strong><br/>
			problem (0.01)<br/>
			solution (0.01)<br/>
			east (0.01)<br/>
			situation (0.01)<br/>
			problems (0.01)</td>
			<td><strong>Topic 07</strong><br/>
			nuclear (0.02)<br/>
			co (0.02)<br/>
			operation (0.02)<br/>
			disarmament (0.02)<br/>
			weapons (0.02)</td>
			<td><strong>Topic 08</strong><br/>
			session (0.02)<br/>
			general (0.02)<br/>
			assembly (0.02)<br/>
			mr (0.02)<br/>
			president (0.02)</td>
			<td><strong>Topic 09</strong><br/>
			world (0.02)<br/>
			peace (0.02)<br/>
			peoples (0.02)<br/>
			security (0.01)<br/>
			states (0.01)</td>
		</tr>
	</tbody>
</table>

<p>NMF is also a statistical method, so the results are not supposed to be identical to the ones that we calculated with scikit-learn, but they are similar enough. Gensim has <a contenteditable="false" data-type="indexterm" data-primary="coherence scores for topic models" id="idm45634186289896"/><a contenteditable="false" data-type="indexterm" data-primary="topic modeling" data-secondary="coherence scores, calculating and using" id="idm45634186289032"/>code for calculating the coherence score for topic models, a quality indicator. Let’s try this:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="kn">from</code> <code class="nn">gensim.models.coherencemodel</code> <code class="kn">import</code> <code class="n">CoherenceModel</code>

<code class="n">nmf_gensim_para_coherence</code> <code class="o">=</code> <code class="n">CoherenceModel</code><code class="p">(</code><code class="n">model</code><code class="o">=</code><code class="n">nmf_gensim_para</code><code class="p">,</code>
                                           <code class="n">texts</code><code class="o">=</code><code class="n">gensim_paragraphs</code><code class="p">,</code>
                                           <code class="n">dictionary</code><code class="o">=</code><code class="n">dict_gensim_para</code><code class="p">,</code>
                                           <code class="n">coherence</code><code class="o">=</code><code class="s1">'c_v'</code><code class="p">)</code>
<code class="n">nmf_gensim_para_coherence_score</code> <code class="o">=</code> <code class="n">nmf_gensim_para_coherence</code><code class="o">.</code><code class="n">get_coherence</code><code class="p">()</code>
<code class="k">print</code><code class="p">(</code><code class="n">nmf_gensim_para_coherence_score</code><code class="p">)</code>
</pre>

<p><code>Out:</code></p>

<pre data-type="programlisting">
0.6500661701098243
</pre>

<p>The score varies with the number of topics. If you want to <a contenteditable="false" data-type="indexterm" data-primary="topic modeling" data-secondary="with optimal number of topics" data-secondary-sortas="optimal number of topics" id="idm45634186249208"/>find the optimal number of topics, a frequent approach is to run NMF for several different values, calculate the coherence score, and take the number of topics that maximizes the score.</p>

<p>Let’s try the same with LDA and compare the quality indicators.</p>
</div></section>

<section data-type="sect2" class="blueprint" data-pdf-bookmark="Blueprint: Using LDA with Gensim"><div class="sect2" id="ch08usingldawithgensim">
<h2>Blueprint: Using LDA with Gensim</h2>

<p>Running LDA <a contenteditable="false" data-type="indexterm" data-primary="LdaModel with Gensim" id="idm45634186244824"/><a contenteditable="false" data-type="indexterm" data-primary="topic modeling" data-secondary="LDA with Gensim for" id="idm45634186243688"/><a contenteditable="false" data-type="indexterm" data-primary="LDA (Latent Dirichlet Allocation) method" id="ch8_term34"/><a contenteditable="false" data-type="indexterm" data-primary="topic modeling" data-secondary="LDA for" id="ch8_term35"/>with Gensim is as easy as using NMF if we have the data prepared. The <code>LdaModel</code> class has a lot of parameters for tuning the model; we use the recommended values here:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="kn">from</code> <code class="nn">gensim.models</code> <code class="kn">import</code> <code class="n">LdaModel</code>
<code class="n">lda_gensim_para</code> <code class="o">=</code> <code class="n">LdaModel</code><code class="p">(</code><code class="n">corpus</code><code class="o">=</code><code class="n">bow_gensim_para</code><code class="p">,</code> <code class="n">id2word</code><code class="o">=</code><code class="n">dict_gensim_para</code><code class="p">,</code>
    <code class="n">chunksize</code><code class="o">=</code><code class="mi">2000</code><code class="p">,</code> <code class="n">alpha</code><code class="o">=</code><code class="s1">'auto'</code><code class="p">,</code> <code class="n">eta</code><code class="o">=</code><code class="s1">'auto'</code><code class="p">,</code> <code class="n">iterations</code><code class="o">=</code><code class="mi">400</code><code class="p">,</code> <code class="n">num_topics</code><code class="o">=</code><code class="mi">10</code><code class="p">,</code> 
    <code class="n">passes</code><code class="o">=</code><code class="mi">20</code><code class="p">,</code> <code class="n">eval_every</code><code class="o">=</code><code class="bp">None</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">)</code></pre>

<p>We are interested in the word distribution of the topics:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">display_topics_gensim</code><code class="p">(</code><code class="n">lda_gensim_para</code><code class="p">)</code>
</pre>

<p><code>Out:</code></p>

<table>
	<tbody>
		<tr>
			<td><strong>Topic 00</strong><br/>
			climate (0.12)<br/>
			convention (0.03)<br/>
			pacific (0.02)<br/>
			environmental (0.02)<br/>
			sea (0.02)</td>
			<td><strong>Topic 01</strong><br/>
			country (0.05)<br/>
			people (0.05)<br/>
			government (0.03)<br/>
			national (0.02)<br/>
			support (0.02)</td>
			<td><strong>Topic 02</strong><br/>
			nations (0.10)<br/>
			united (0.10)<br/>
			human (0.04)<br/>
			security (0.03)<br/>
			rights (0.03)</td>
			<td><strong>Topic 03</strong><br/>
			international (0.03)<br/>
			community (0.01)<br/>
			efforts (0.01)<br/>
			new (0.01)<br/>
			global (0.01)</td>
			<td><strong>Topic 04</strong><br/>
			africa (0.06)<br/>
			african (0.06)<br/>
			continent (0.02)<br/>
			terrorist (0.02)<br/>
			crimes (0.02)</td>
		</tr>
		<tr>
			<td><strong>Topic 05</strong><br/>
			world (0.05)<br/>
			years (0.02)<br/>
			today (0.02)<br/>
			peace (0.01)<br/>
			time (0.01)</td>
			<td><strong>Topic 06</strong><br/>
			peace (0.03)<br/>
			conflict (0.02)<br/>
			region (0.02)<br/>
			people (0.02)<br/>
			state (0.02)</td>
			<td><strong>Topic 07</strong><br/>
			south (0.10)<br/>
			sudan (0.05)<br/>
			china (0.04)<br/>
			asia (0.04)<br/>
			somalia (0.04)</td>
			<td><strong>Topic 08</strong><br/>
			general (0.10)<br/>
			assembly (0.09)<br/>
			session (0.05)<br/>
			president (0.04)<br/>
			secretary (0.04)</td>
			<td><strong>Topic 09</strong><br/>
			development (0.07)<br/>
			countries (0.05)<br/>
			economic (0.03)<br/>
			sustainable (0.02)<br/>
			2015 (0.02)</td>
		</tr>
	</tbody>
</table>

<p>The topics are not as easy to interpret as the ones generated by NMF. Checking the coherence score as shown earlier, we find a lower score of 0.45270703180962374. Gensim also allows us to calculate the perplexity score of an LDA model. Perplexity measures how well a probability model predicts a sample. When we execute  <span class="keep-together"><code>lda_gensim_para.log_perplexity(vectors_gensim_para)</code></span>, we get a perplexity score of -9.70558947109483.</p>
</div></section>

<section data-type="sect2" class="blueprint" data-pdf-bookmark="Blueprint: Calculating Coherence Scores"><div class="sect2" id="ch08-calc-coherence-score">
<h2>Blueprint: Calculating Coherence Scores</h2>

<p>Gensim can also <a contenteditable="false" data-type="indexterm" data-primary="coherence scores for topic models" id="ch8_term36"/><a contenteditable="false" data-type="indexterm" data-primary="topic modeling" data-secondary="coherence scores, calculating and using" id="ch8_term37"/>calculate topic coherence. The method itself is a four-stage process consisting of segmentation, probability estimation, a confirmation measure calculation, and aggregation. Fortunately, Gensim has <a contenteditable="false" data-type="indexterm" data-primary="CoherenceModel (Gensim)" id="idm45634186079496"/><span class="keep-together">a <code>CoherenceModel</code></span> class that encapsulates all these single tasks, and we can directly <span class="keep-together">use it</span>:</p>

<pre class="pre drop-element-attached-top drop-element-attached-center drop-target-attached-bottom drop-target-attached-center" data-code-language="python" data-type="programlisting">
<code class="kn">from</code> <code class="nn">gensim.models.coherencemodel</code> <code class="kn">import</code> <code class="n">CoherenceModel</code>

<code class="n">lda_gensim_para_coherence</code> <code class="o">=</code> <code class="n">CoherenceModel</code><code class="p">(</code><code class="n">model</code><code class="o">=</code><code class="n">lda_gensim_para</code><code class="p">,</code>
    <code class="n">texts</code><code class="o">=</code><code class="n">gensim_paragraphs</code><code class="p">,</code> <code class="n">dictionary</code><code class="o">=</code><code class="n">dict_gensim_para</code><code class="p">,</code> <code class="n">coherence</code><code class="o">=</code><code class="s1">'c_v'</code><code class="p">)</code>
<code class="n">lda_gensim_para_coherence_score</code> <code class="o">=</code> <code class="n">lda_gensim_para_coherence</code><code class="o">.</code><code class="n">get_coherence</code><code class="p">()</code>
<code class="k">print</code><code class="p">(</code><code class="n">lda_gensim_para_coherence_score</code><code class="p">)</code>
</pre>

<p><code>Out:</code></p>

<pre data-type="programlisting">
0.5444930496493174
</pre>

<p>Substituting <code>nmf</code> for <code>lda</code>, we can <a contenteditable="false" data-type="indexterm" data-primary="NMF (Nonnegative Matrix Factorization) for topic modeling" id="idm45634186038856"/><a contenteditable="false" data-type="indexterm" data-primary="topic modeling" data-secondary="NMF for" id="idm45634186037752"/>calculate the same score for our NMF model:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">nmf_gensim_para_coherence</code> <code class="o">=</code> <code class="n">CoherenceModel</code><code class="p">(</code><code class="n">model</code><code class="o">=</code><code class="n">nmf_gensim_para</code><code class="p">,</code>
    <code class="n">texts</code><code class="o">=</code><code class="n">gensim_paragraphs</code><code class="p">,</code> <code class="n">dictionary</code><code class="o">=</code><code class="n">dict_gensim_para</code><code class="p">,</code> <code class="n">coherence</code><code class="o">=</code><code class="s1">'c_v'</code><code class="p">)</code>
<code class="n">nmf_gensim_para_coherence_score</code> <code class="o">=</code> <code class="n">nmf_gensim_para_coherence</code><code class="o">.</code><code class="n">get_coherence</code><code class="p">()</code>
<code class="k">print</code><code class="p">(</code><code class="n">nmf_gensim_para_coherence_score</code><code class="p">)</code>
</pre>

<p><code>Out:</code></p>

<pre data-type="programlisting">
0.6505110480127619
</pre>

<p>The score is quite a bit higher, which means that the NMF model is a better approximation to the real topics compared to LDA.</p>

<p>Calculating the coherence score of the individual topics for LDA is even easier, as it is directly supported by the LDA model. Let’s take a look at the average first:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">top_topics</code> <code class="o">=</code> <code class="n">lda_gensim_para</code><code class="o">.</code><code class="n">top_topics</code><code class="p">(</code><code class="n">vectors_gensim_para</code><code class="p">,</code> <code class="n">topn</code><code class="o">=</code><code class="mi">5</code><code class="p">)</code>
<code class="n">avg_topic_coherence</code> <code class="o">=</code> <code class="nb">sum</code><code class="p">([</code><code class="n">t</code><code class="p">[</code><code class="mi">1</code><code class="p">]</code> <code class="k">for</code> <code class="n">t</code> <code class="ow">in</code> <code class="n">top_topics</code><code class="p">])</code> <code class="o">/</code> <code class="nb">len</code><code class="p">(</code><code class="n">top_topics</code><code class="p">)</code>
<code class="k">print</code><code class="p">(</code><code class="s1">'Average topic coherence: </code><code class="si">%.4f</code><code class="s1">.'</code> <code class="o">%</code> <code class="n">avg_topic_coherence</code><code class="p">)</code>
</pre>

<p class="pagebreak-before"><code>Out:</code></p>

<pre data-type="programlisting">
Average topic coherence: -2.4709.
</pre>

<p>We are also interested in the coherence scores of the individual topics, which is contained in <code>top_topics</code>. However, the output is verbose (check it!), so we try to condense it a bit by just printing the coherence scores together with the most important words of the topics:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="p">[(</code><code class="n">t</code><code class="p">[</code><code class="mi">1</code><code class="p">],</code> <code class="s2">" "</code><code class="o">.</code><code class="n">join</code><code class="p">([</code><code class="n">w</code><code class="p">[</code><code class="mi">1</code><code class="p">]</code> <code class="k">for</code> <code class="n">w</code> <code class="ow">in</code> <code class="n">t</code><code class="p">[</code><code class="mi">0</code><code class="p">]]))</code> <code class="k">for</code> <code class="n">t</code> <code class="ow">in</code> <code class="n">top_topics</code><code class="p">]</code>
</pre>

<p><code>Out:</code></p>

<pre data-code-language="python" data-type="programlisting">
<code class="p">[(</code><code class="o">-</code><code class="mf">1.5361194241843663</code><code class="p">,</code> <code class="s1">'general assembly session president secretary'</code><code class="p">),</code>
<code class="err"> </code><code class="p">(</code><code class="o">-</code><code class="mf">1.7014902754187737</code><code class="p">,</code> <code class="s1">'nations united human security rights'</code><code class="p">),</code>
<code class="err"> </code><code class="p">(</code><code class="o">-</code><code class="mf">1.8485895463251694</code><code class="p">,</code> <code class="s1">'country people government national support'</code><code class="p">),</code>
<code class="err"> </code><code class="p">(</code><code class="o">-</code><code class="mf">1.9729985026779555</code><code class="p">,</code> <code class="s1">'peace conflict region people state'</code><code class="p">),</code>
<code class="err"> </code><code class="p">(</code><code class="o">-</code><code class="mf">1.9743434414778658</code><code class="p">,</code> <code class="s1">'world years today peace time'</code><code class="p">),</code>
<code class="err"> </code><code class="p">(</code><code class="o">-</code><code class="mf">2.0202823396586433</code><code class="p">,</code> <code class="s1">'international community efforts new global'</code><code class="p">),</code>
<code class="err"> </code><code class="p">(</code><code class="o">-</code><code class="mf">2.7269347656599225</code><code class="p">,</code> <code class="s1">'development countries economic sustainable 2015'</code><code class="p">),</code>
<code class="err"> </code><code class="p">(</code><code class="o">-</code><code class="mf">2.9089975883502706</code><code class="p">,</code> <code class="s1">'climate convention pacific environmental sea'</code><code class="p">),</code>
<code class="err"> </code><code class="p">(</code><code class="o">-</code><code class="mf">3.8680684770508753</code><code class="p">,</code> <code class="s1">'africa african continent terrorist crimes'</code><code class="p">),</code>
<code class="err"> </code><code class="p">(</code><code class="o">-</code><code class="mf">4.1515707817343195</code><code class="p">,</code> <code class="s1">'south sudan china asia somalia'</code><code class="p">)]</code>
</pre>

<p>Coherence scores for topic models can easily be calculated using Gensim. The absolute values are difficult to interpret, but varying the methods (NMF versus LDA) or the number of topics can give you ideas about which way you want to proceed in your topic models. Coherence scores and coherence models are a big advantage of Gensim, as <a contenteditable="false" data-type="indexterm" data-primary="scikit-learn" data-secondary="topic modeling with" id="idm45634185848792"/>they are not (yet) included in scikit-learn.</p>

<p>As it’s difficult to estimate the “correct” number of topics, we are now taking a look at an approach that creates hierarchical models and does not need a fixed number of topics as a parameter.</p>
</div></section>

<section data-type="sect2" class="blueprint pagebreak-after" data-pdf-bookmark="Blueprint: Finding the Optimal Number of Topics"><div class="sect2" id="idm45634186083928">
<h2>Blueprint: Finding the Optimal Number of Topics</h2>

<p>In the previous sections, we have always worked with 10 topics. So far we have not compared the quality of this topic model to different ones with a lower or higher number of topics. We <a contenteditable="false" data-type="indexterm" data-primary="topic modeling" data-secondary="with optimal number of topics" data-secondary-sortas="optimal number of topics" id="ch8_term39"/>want to find the optimal number <span class="keep-together">of topics</span> in a structured way without having to go into the interpretation of each <span class="keep-together">constellation</span>.</p>

<p>It turns out there is a way to achieve this. The “quality” of a topic model can be measured by the previously introduced coherence score. To find the best coherence score, we will now calculate it for a different number of topics with an LDA model. We will try to find the highest score, which should give us the optimal number of topics:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="kn">from</code> <code class="nn">gensim.models.ldamulticore</code> <code class="kn">import</code> <code class="n">LdaMulticore</code>
<code class="n">lda_para_model_n</code> <code class="o">=</code> <code class="p">[]</code>
<code class="k">for</code> <code class="n">n</code> <code class="ow">in</code> <code class="n">tqdm</code><code class="p">(</code><code class="nb">range</code><code class="p">(</code><code class="mi">5</code><code class="p">,</code> <code class="mi">21</code><code class="p">)):</code>
<code class="err">   </code> <code class="n">lda_model</code> <code class="o">=</code> <code class="n">LdaMulticore</code><code class="p">(</code><code class="n">corpus</code><code class="o">=</code><code class="n">bow_gensim_para</code><code class="p">,</code> <code class="n">id2word</code><code class="o">=</code><code class="n">dict_gensim_para</code><code class="p">,</code>
                             <code class="n">chunksize</code><code class="o">=</code><code class="mi">2000</code><code class="p">,</code> <code class="n">eta</code><code class="o">=</code><code class="s1">'auto'</code><code class="p">,</code> <code class="n">iterations</code><code class="o">=</code><code class="mi">400</code><code class="p">,</code>
                             <code class="n">num_topics</code><code class="o">=</code><code class="n">n</code><code class="p">,</code> <code class="n">passes</code><code class="o">=</code><code class="mi">20</code><code class="p">,</code> <code class="n">eval_every</code><code class="o">=</code><code class="bp">None</code><code class="p">,</code>
                             <code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">)</code>
<code class="err">   </code> <code class="n">lda_coherence</code> <code class="o">=</code> <code class="n">CoherenceModel</code><code class="p">(</code><code class="n">model</code><code class="o">=</code><code class="n">lda_model</code><code class="p">,</code> <code class="n">texts</code><code class="o">=</code><code class="n">gensim_paragraphs</code><code class="p">,</code>
<code class="err">                                  </code> <code class="n">dictionary</code><code class="o">=</code><code class="n">dict_gensim_para</code><code class="p">,</code> <code class="n">coherence</code><code class="o">=</code><code class="s1">'c_v'</code><code class="p">)</code>
<code class="err">   </code> <code class="n">lda_para_model_n</code><code class="o">.</code><code class="n">append</code><code class="p">((</code><code class="n">n</code><code class="p">,</code> <code class="n">lda_model</code><code class="p">,</code> <code class="n">lda_coherence</code><code class="o">.</code><code class="n">get_coherence</code><code class="p">()))</code>
</pre>

<div data-type="warning" epub:type="warning">
<h1>Coherence Calculations Take Time</h1>

<p>Calculating the <a contenteditable="false" data-type="indexterm" data-primary="execution time" id="idm45634185747576"/><a contenteditable="false" data-type="indexterm" data-primary="time, execution" id="idm45634185610344"/>LDA model (and the coherence) is computationally expensive, so in real life it would be better to optimize the algorithm to calculate only a minimal number of models and perplexities. Sometimes it might make sense if you calculate the coherence scores for only a few <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch8_term34" id="idm45634185608824"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch8_term35" id="idm45634185607448"/>numbers of topics.</p>
</div>

<p>Now we can choose which number of topics produces a good coherence score. Note that typically the score grows with the number of topics. Taking too many topics makes interpretation difficult:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">lda_para_model_n</code><code class="p">,</code> <code class="n">columns</code><code class="o">=</code><code class="p">[</code><code class="s2">"n"</code><code class="p">,</code> <code class="s2">"model"</code><code class="p">,</code> \
    <code class="s2">"coherence"</code><code class="p">])</code><code class="o">.</code><code class="n">set_index</code><code class="p">(</code><code class="s2">"n"</code><code class="p">)[[</code><code class="s2">"coherence"</code><code class="p">]]</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">16</code><code class="p">,</code><code class="mi">9</code><code class="p">))</code></pre>

<p><code>Out:</code></p>

<figure><div id="fig-coherence-topics" class="figure"><img src="Images/btap_08in04.jpg" width="1420" height="795"/><h6/></div></figure>

<p>Overall, the graph grows with the number of topics, which is almost always the case. However, we can see “spikes” at 13 and 17 topics, so these numbers look like good choices. We will visualize the results for 17 topics:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">display_topics_gensim</code><code class="p">(</code><code class="n">lda_para_model_n</code><code class="p">[</code><code class="mi">12</code><code class="p">][</code><code class="mi">1</code><code class="p">])</code></pre>

<p><code>Out:</code></p>

<table>
	<tbody>
		<tr>
			<td><strong>Topic 00</strong><br/>
			peace (0.02)<br/>
			international (0.02)<br/>
			cooperation (0.01)<br/>
			countries (0.01)<br/>
			region (0.01)</td>
			<td><strong>Topic 01</strong><br/>
			general (0.05)<br/>
			assembly (0.04)<br/>
			session (0.03)<br/>
			president (0.03)<br/>
			mr (0.03)</td>
			<td><strong>Topic 02</strong><br/>
			united (0.04)<br/>
			nations (0.04)<br/>
			states (0.03)<br/>
			european (0.02)<br/>
			union (0.02)</td>
			<td><strong>Topic 03</strong><br/>
			nations (0.07)<br/>
			united (0.07)<br/>
			security (0.03)<br/>
			council (0.02)<br/>
			international (0.02)</td>
			<td><strong>Topic 04</strong><br/>
			development (0.03)<br/>
			general (0.02)<br/>
			conference (0.02)<br/>
			assembly (0.02)<br/>
			sustainable (0.01)</td>
			<td><strong>Topic 05</strong><br/>
			international (0.03)<br/>
			terrorism (0.03)<br/>
			states (0.01)<br/>
			iraq (0.01)<br/>
			acts (0.01)</td>
		</tr>
		<tr>
			<td><strong>Topic 06</strong><br/>
			peace (0.03)<br/>
			east (0.02)<br/>
			middle (0.02)<br/>
			israel (0.02)<br/>
			solution (0.01)</td>
			<td><strong>Topic 07</strong><br/>
			africa (0.08)<br/>
			south (0.05)<br/>
			african (0.05)<br/>
			namibia (0.02)<br/>
			republic (0.01)</td>
			<td><strong>Topic 08</strong><br/>
			states (0.04)<br/>
			small (0.04)<br/>
			island (0.03)<br/>
			sea (0.02)<br/>
			pacific (0.02)</td>
			<td><strong>Topic 09</strong><br/>
			world (0.03)<br/>
			international (0.02)<br/>
			problems (0.01)<br/>
			war (0.01)<br/>
			peace (0.01)</td>
			<td><strong>Topic 10</strong><br/>
			human (0.07)<br/>
			rights (0.06)<br/>
			law (0.02)<br/>
			respect (0.02)<br/>
			international (0.01)</td>
			<td><strong>Topic 11</strong><br/>
			climate (0.03)<br/>
			change (0.03)<br/>
			global (0.02)<br/>
			environment (0.01)<br/>
			energy (0.01)</td>
		</tr>
		<tr>
			<td><strong>Topic 12</strong><br/>
			world (0.03)<br/>
			people (0.02)<br/>
			future (0.01)<br/>
			years (0.01)<br/>
			today (0.01)</td>
			<td><strong>Topic 13</strong><br/>
			people (0.03)<br/>
			independence (0.02)<br/>
			peoples (0.02)<br/>
			struggle (0.01)<br/>
			countries (0.01)</td>
			<td><strong>Topic 14</strong><br/>
			people (0.02)<br/>
			country (0.02)<br/>
			government (0.02)<br/>
			humanitarian (0.01)<br/>
			refugees (0.01)</td>
			<td><strong>Topic 15</strong><br/>
			countries (0.05)<br/>
			development (0.03)<br/>
			economic (0.03)<br/>
			developing (0.02)<br/>
			trade (0.01)</td>
			<td><strong>Topic 16</strong><br/>
			nuclear (0.06)<br/>
			weapons (0.04)<br/>
			disarmament (0.03)<br/>
			arms (0.03)<br/>
			treaty (0.02)</td>
		</tr>
	</tbody>
</table>

<p>Most of the topics are easy to interpret, but quite a few are difficult (like 0, 3, 8) as they contain many words with small, but not too different, sizes. Is the topic model with 17 topics therefore easier to explain? Not really. The coherence measure is higher, but that does not necessarily mean a more obvious interpretation. In other words, relying solely on coherence scores can be dangerous if the number of topics gets too large. Although in theory higher coherence should contribute to better interpretability, it is often a trade-off, and choosing smaller numbers of topics can make life easier. Taking a look back at the coherence graph, 10 seems to be a good value as it is a <em>local maximum</em> of the <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch8_term36" id="idm45634185438088"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch8_term37" id="idm45634185436680"/>coherence score.</p>

<p>As it’s obviously difficult to find the “correct” number of topics, we will now take a look at an approach that creates hierarchical models and does not need a fixed number of topics <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch8_term39" id="idm45634185434520"/>as a parameter.</p>
</div></section>

<section data-type="sect2" class="blueprint" data-pdf-bookmark="Blueprint: Creating a Hierarchical Dirichlet Process with Gensim"><div class="sect2" id="idm45634185756216">
<h2>Blueprint: Creating a Hierarchical Dirichlet Process with Gensim</h2>

<p>Take a step back <a contenteditable="false" data-type="indexterm" data-primary="HDP (hierarchical Dirichlet process)" id="ch8_term40"/><a contenteditable="false" data-type="indexterm" data-primary="topic modeling" data-secondary="hierarchical Dirichlet process for" id="ch8_term41"/>and recall the visualization of the topics in <a data-type="xref" href="#ch08usingldawithgensim">“Blueprint: Using LDA with Gensim”</a>. The sizes of the topics vary quite a bit, and some topics have a large overlap. It would be nice if the results gave us broader topics first and some subtopics below them. This is the exact idea of the hierarchical Dirichlet process (HDP). The hierarchical topic model should give us just a few broad topics that are well separated, then go into more detail by adding more words and getting more differentiated topic definitions.</p>

<p>HDP is still quite new and has not yet been extensively analyzed. Gensim is also often used in research and has an experimental implementation of HDP integrated. As we <a contenteditable="false" data-type="indexterm" data-primary="vectorizers/vectorization" data-secondary="with topic modeling" data-secondary-sortas="topic modeling" id="idm45634185425976"/>can directly use our already existing vectorization, it’s not complicated to try it. Note that we are again using the bag-of-words vectorization as the Dirichlet processes themselves handle frequent words correctly:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="kn">from</code> <code class="nn">gensim.models</code> <code class="kn">import</code> <code class="n">HdpModel</code>
<code class="n">hdp_gensim_para</code> <code class="o">=</code> <code class="n">HdpModel</code><code class="p">(</code><code class="n">corpus</code><code class="o">=</code><code class="n">bow_gensim_para</code><code class="p">,</code> <code class="n">id2word</code><code class="o">=</code><code class="n">dict_gensim_para</code><code class="p">)</code>
</pre>

<p>HDP can estimate the number of topics and can show all that it identified:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">hdp_gensim_para</code><code class="o">.</code><code class="n">print_topics</code><code class="p">(</code><code class="n">num_words</code><code class="o">=</code><code class="mi">10</code><code class="p">)</code>
</pre>

<p><code>Out:</code></p>

<figure><div class="figure"><img src="Images/btap_08in05.jpg" width="979" height="532"/><h6/></div></figure>

<p>The results are sometimes difficult to understand. It can be an option to first perform a “rough” topic modeling with only a few topics. If you find out that a topic is really big or suspect that it might have subtopics, you can create a subset of the original corpus where the only documents included are those that have a significant mixture of this topic. This needs some manual interaction but often yields much better results compared to HDP. At this stage of development, we would not recommend <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch8_term32" id="idm45634185402024"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch8_term33" id="idm45634185400680"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch8_term40" id="idm45634185399304"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch8_term41" id="idm45634185397928"/>using HDP exclusively.</p>

<p>Topic models focus on uncovering the topic structure of a large corpus of documents. As all documents are modeled as a mixture of different topics, they are not well-suited for assigning documents to exactly one topic. This can be achieved using <span class="keep-together">clustering</span>.</p>
</div></section>
</div></section>

<section data-type="sect1" class="blueprint" data-pdf-bookmark="Blueprint: Using Clustering to Uncover the Structure of Text Data"><div class="sect1" id="idm45634185432424">
<h1>Blueprint: Using Clustering to Uncover the Structure of Text Data</h1>

<p>Apart from topic modeling, there is a <a contenteditable="false" data-type="indexterm" data-primary="unsupervised methods" id="idm45634185361160"/>multitude of other unsupervised methods. Not all are suitable for text data, but <a contenteditable="false" data-type="indexterm" data-primary="clustering of text data" id="ch8_term42"/><a contenteditable="false" data-type="indexterm" data-primary="topic modeling" data-secondary="clustering as alternative to" id="ch8_term43"/>many clustering algorithms can be used. Compared to topic modeling, it is important for us to know that each document (or paragraph) gets assigned to exactly one cluster.</p>

<div data-type="note" epub:type="note">
<h1>Clustering Works Well for Mono-Typical Texts</h1>

<p>In our case, it is a reasonable assumption that each document belongs to exactly one cluster, as there are probably not too many different things contained in one paragraph. For larger text fragments, we would rather use topic modeling to take possible mixtures into account.</p>
</div>

<p>Most clustering methods need the number of clusters as a parameter, while there are a few (like mean-shift) that can guess the correct number of clusters. Most of the latter do not work well with sparse data and therefore are not suitable for text analytics. In our case, <a contenteditable="false" data-type="indexterm" data-primary="k-means clustering" id="idm45634185354072"/>we decided to use k-means clustering, but birch or spectral clustering should work in a similar manner. There are a few nice explanations of how the k-means algorithm works.<sup><a data-type="noteref" id="idm45634185352648-marker" href="ch08.xhtml#idm45634185352648">4</a></sup></p>

<div data-type="warning" epub:type="warning">
<h1>Clustering Is Much Slower Than Topic Modeling</h1>

<p>For most <a contenteditable="false" data-type="indexterm" data-primary="execution time" id="idm45634185332824"/><a contenteditable="false" data-type="indexterm" data-primary="time, execution" id="idm45634185331688"/>algorithms, clustering takes considerable time, much more than even LDA. So, be prepared to wait for roughly one hour when executing the clustering in the next code fragment.</p>
</div>

<p>The scikit-learn API <a contenteditable="false" data-type="indexterm" data-primary="scikit-learn" data-secondary="clustering with" id="idm45634185329720"/>for clustering is similar to what we have seen with topic models:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="kn">from</code> <code class="nn">sklearn.cluster</code> <code class="kn">import</code> <code class="n">KMeans</code>
<code class="n">k_means_text</code> <code class="o">=</code> <code class="n">KMeans</code><code class="p">(</code><code class="n">n_clusters</code><code class="o">=</code><code class="mi">10</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">)</code>
<code class="n">k_means_text</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">tfidf_para_vectors</code><code class="p">)</code>
</pre>

<pre data-code-language="python" data-type="programlisting">
<code class="n">KMeans</code><code class="p">(</code><code class="n">n_clusters</code><code class="o">=</code><code class="mi">10</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">)</code>
</pre>

<p>But now it’s much easier to find out how many paragraphs belong to which cluster. Everything necessary is in the <code>labels_</code> field of the <code>k_means_para</code> object. For each document, it contains the label that was assigned by the clustering algorithm:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">np</code><code class="o">.</code><code class="n">unique</code><code class="p">(</code><code class="n">k_means_para</code><code class="o">.</code><code class="n">labels_</code><code class="p">,</code> <code class="n">return_counts</code><code class="o">=</code><code class="bp">True</code><code class="p">)</code></pre>

<p><code>Out:</code></p>

<pre data-code-language="python" data-type="programlisting">
<code class="p">(</code><code class="n">array</code><code class="p">([</code><code class="mi">0</code><code class="p">,</code> <code class="mi">1</code><code class="p">,</code> <code class="mi">2</code><code class="p">,</code> <code class="mi">3</code><code class="p">,</code> <code class="mi">4</code><code class="p">,</code> <code class="mi">5</code><code class="p">,</code> <code class="mi">6</code><code class="p">,</code> <code class="mi">7</code><code class="p">,</code> <code class="mi">8</code><code class="p">,</code> <code class="mi">9</code><code class="p">],</code> <code class="n">dtype</code><code class="o">=</code><code class="n">int32</code><code class="p">),</code>
<code class="n">array</code><code class="p">([</code><code class="mi">133370</code><code class="p">,</code>  <code class="mi">41705</code><code class="p">,</code>  <code class="mi">12396</code><code class="p">,</code>   <code class="mi">9142</code><code class="p">,</code>  <code class="mi">12674</code><code class="p">,</code>  <code class="mi">21080</code><code class="p">,</code>  <code class="mi">19727</code><code class="p">,</code>  <code class="mi">10563</code><code class="p">,</code>
         <code class="mi">10437</code><code class="p">,</code>  <code class="mi">11116</code><code class="p">]))</code></pre>

<p>In many cases, you might already have found some conceptual problems here. If the data is too heterogeneous, most clusters tend to be small (containing a comparatively small vocabulary) and are accompanied by a large cluster that absorbs all the rest. Fortunately (and due to the short paragraphs), this is not the case here; cluster 0 is much bigger than the others, but it’s not orders of magnitude. Let’s visualize the distribution with the y-axis showing the size of the clusters (<a data-type="xref" href="#fig-cluster-size">Figure 8-6</a>):</p>

<pre data-code-language="python" data-type="programlisting">
<code class="n">sizes</code> <code class="o">=</code> <code class="p">[]</code>
<code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">10</code><code class="p">):</code>
<code class="err">   </code> <code class="n">sizes</code><code class="o">.</code><code class="n">append</code><code class="p">({</code><code class="s2">"cluster"</code><code class="p">:</code> <code class="n">i</code><code class="p">,</code> <code class="s2">"size"</code><code class="p">:</code> <code class="n">np</code><code class="o">.</code><code class="n">sum</code><code class="p">(</code><code class="n">k_means_para</code><code class="o">.</code><code class="n">labels_</code><code class="o">==</code><code class="n">i</code><code class="p">)})</code>
<code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">sizes</code><code class="p">)</code><code class="o">.</code><code class="n">set_index</code><code class="p">(</code><code class="s2">"cluster"</code><code class="p">)</code><code class="o">.</code><code class="n">plot</code><code class="o">.</code><code class="n">bar</code><code class="p">(</code><code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">16</code><code class="p">,</code><code class="mi">9</code><code class="p">))</code>
</pre>


<p>Visualizing <a contenteditable="false" data-type="indexterm" data-primary="topic modeling" data-secondary="word clouds for display and comparing of" id="ch8_term44"/><a contenteditable="false" data-type="indexterm" data-primary="visualization of data" data-secondary="with word clouds" data-secondary-sortas="word clouds" id="ch8_term45"/><a contenteditable="false" data-type="indexterm" data-primary="word clouds " id="ch8_term46"/>the clusters works in a similar way to the topic models. However, we have to calculate the individual feature contributions manually. For this, we <a contenteditable="false" data-type="indexterm" data-primary="TF-IDF (Term-Frequency Inverse Document Frequency) weighting" data-secondary="vectorization with" id="idm45634185077096"/><a contenteditable="false" data-type="indexterm" data-primary="vectorizers/vectorization" data-secondary="with topic modeling" data-secondary-sortas="topic modeling" id="idm45634185075752"/>add up the TF-IDF vectors of all documents in the cluster and keep only the largest values.</p>

<figure><div id="fig-cluster-size" class="figure"><img src="Images/btap_08in06.jpg" width="1428" height="1116"/><h6><span class="label">Figure 8-6. </span>Visualization of the size of the clusters.</h6></div></figure>

<p>These are the weights for their corresponding words. In fact, that’s the only change compared to the previous code:</p>

<pre data-code-language="python" data-type="programlisting">
<code class="k">def</code> <code class="nf">wordcloud_clusters</code><code class="p">(</code><code class="n">model</code><code class="p">,</code> <code class="n">vectors</code><code class="p">,</code> <code class="n">features</code><code class="p">,</code> <code class="n">no_top_words</code><code class="o">=</code><code class="mi">40</code><code class="p">):</code>
<code class="err">   </code> <code class="k">for</code> <code class="n">cluster</code> <code class="ow">in</code> <code class="n">np</code><code class="o">.</code><code class="n">unique</code><code class="p">(</code><code class="n">model</code><code class="o">.</code><code class="n">labels_</code><code class="p">):</code>
<code class="err">       </code> <code class="n">size</code> <code class="o">=</code> <code class="p">{}</code>
<code class="err">       </code> <code class="n">words</code> <code class="o">=</code> <code class="n">vectors</code><code class="p">[</code><code class="n">model</code><code class="o">.</code><code class="n">labels_</code> <code class="o">==</code> <code class="n">cluster</code><code class="p">]</code><code class="o">.</code><code class="n">sum</code><code class="p">(</code><code class="n">axis</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code><code class="o">.</code><code class="n">A</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code>
<code class="err">       </code> <code class="n">largest</code> <code class="o">=</code> <code class="n">words</code><code class="o">.</code><code class="n">argsort</code><code class="p">()[::</code><code class="o">-</code><code class="mi">1</code><code class="p">]</code> <code class="c1"># invert sort order</code>
<code class="err">       </code> <code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="n">no_top_words</code><code class="p">):</code>
<code class="err">           </code> <code class="n">size</code><code class="p">[</code><code class="n">features</code><code class="p">[</code><code class="n">largest</code><code class="p">[</code><code class="n">i</code><code class="p">]]]</code> <code class="o">=</code> <code class="nb">abs</code><code class="p">(</code><code class="n">words</code><code class="p">[</code><code class="n">largest</code><code class="p">[</code><code class="n">i</code><code class="p">]])</code>
<code class="err">       </code> <code class="n">wc</code> <code class="o">=</code> <code class="n">WordCloud</code><code class="p">(</code><code class="n">background_color</code><code class="o">=</code><code class="s2">"white"</code><code class="p">,</code> <code class="n">max_words</code><code class="o">=</code><code class="mi">100</code><code class="p">,</code>
                       <code class="n">width</code><code class="o">=</code><code class="mi">960</code><code class="p">,</code> <code class="n">height</code><code class="o">=</code><code class="mi">540</code><code class="p">)</code>
<code class="err">       </code> <code class="n">wc</code><code class="o">.</code><code class="n">generate_from_frequencies</code><code class="p">(</code><code class="n">size</code><code class="p">)</code>
<code class="err">       </code> <code class="n">plt</code><code class="o">.</code><code class="n">figure</code><code class="p">(</code><code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">12</code><code class="p">,</code><code class="mi">12</code><code class="p">))</code>
<code class="err">       </code> <code class="n">plt</code><code class="o">.</code><code class="n">imshow</code><code class="p">(</code><code class="n">wc</code><code class="p">,</code> <code class="n">interpolation</code><code class="o">=</code><code class="s1">'bilinear'</code><code class="p">)</code>
<code class="err">       </code> <code class="n">plt</code><code class="o">.</code><code class="n">axis</code><code class="p">(</code><code class="s2">"off"</code><code class="p">)</code>
<code class="err">       </code> <code class="c1"># if you don't want to save the topic model, comment the next line</code>
<code class="err">       </code> <code class="n">plt</code><code class="o">.</code><code class="n">savefig</code><code class="p">(</code><code class="n">f</code><code class="s1">'cluster{cluster}.png'</code><code class="p">)</code>

<code class="n">wordcloud_clusters</code><code class="p">(</code><code class="n">k_means_para</code><code class="p">,</code> <code class="n">tfidf_para_vectors</code><code class="p">,</code>
<code class="err"> </code>                  <code class="n">tfidf_para_vectorizer</code><code class="o">.</code><code class="n">get_feature_names</code><code class="p">())</code></pre>

<p><code>Out:</code></p>

<figure><div id="fig-wordcloud-cluster" class="figure"><img src="Images/btap_08in07.jpg" width="1428" height="2000"/><h6/></div></figure>

<p>As you can see, the results are (fortunately) not too different from the various topic modeling <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch8_term44" id="idm45634185065432"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch8_term45" id="idm45634184828712"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch8_term46" id="idm45634184827336"/>approaches; you might recognize the topics of nuclear weapons, South Africa, general assembly, etc. Note, however, that the clusters are more pronounced. In other words, they have more specific words. Unfortunately, this is not true for the biggest cluster, 1, which has no clear direction but many words with similar, smaller sizes. This is a typical phenomenon of clustering algorithms compared to <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch8_term38" id="idm45634184825416"/>topic modeling.</p>

<p>Clustering calculations can take quite long, especially compared to NMF topic models. On the positive side, we are now free to choose documents in a certain cluster (opposed to a topic model, this is well-defined) and perform additional, more sophisticated operations, such as hierarchical clustering, etc.</p>

<p>The quality of the clustering <a contenteditable="false" data-type="indexterm" data-primary="coherence scores for topic models" id="idm45634184822728"/><a contenteditable="false" data-type="indexterm" data-primary="topic modeling" data-secondary="coherence scores, calculating and using" id="idm45634184821656"/>can be calculated by using coherence or the Calinski-Harabasz score. These metrics are not optimized for sparse data and take a long time to calculate, and therefore we <a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch8_term42" id="idm45634184819944"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="ch8_term43" id="idm45634184818568"/>skip this here.</p>
</div></section>

<section data-type="sect1" data-pdf-bookmark="Further Ideas"><div class="sect1" id="idm45634185362552">
<h1>Further Ideas</h1>

<p>In this chapter, we have shown different methods for performing topic modeling. However, we have only scratched the surface of the possibilities:</p>

<ul>
	<li>It’s possible to add <a contenteditable="false" data-type="indexterm" data-primary="n-grams" id="idm45634184814920"/>n-grams in the <a contenteditable="false" data-type="indexterm" data-primary="vectorizers/vectorization" data-secondary="with topic modeling" data-secondary-sortas="topic modeling" id="idm45634184813656"/>vectorization process. <a contenteditable="false" data-type="indexterm" data-primary="scikit-learn" data-secondary="vectorization with" id="idm45634184811880"/>In scikit-learn this is straightforward by using the <code>ngram_range</code> parameter. <a contenteditable="false" data-type="indexterm" data-primary="Gensim" data-secondary="for topic modeling" data-secondary-sortas="topic modeling" id="idm45634184809928"/><a contenteditable="false" data-type="indexterm" data-primary="topic modeling" data-secondary="Gensim for" id="idm45634184808248"/>Gensim has a special <code>Phrases</code> class for that. Due to the higher TF-IDF weights of n-grams, they can contribute considerably to the features of a topic and add a lot of context information.</li>
	<li>As we have used years to have time-dependent topic models, you could also use countries or continents and find the topics that are most relevant in the speeches of their ambassadors.</li>
	<li>Calculate the coherence score for <a contenteditable="false" data-type="indexterm" data-primary="LDA (Latent Dirichlet Allocation) method" id="idm45634184805224"/><a contenteditable="false" data-type="indexterm" data-primary="topic modeling" data-secondary="LDA for" id="idm45634184804088"/>an LDA topic model using the whole speeches instead of the paragraphs and compare the scores.</li>
</ul>
</div></section>

<section data-type="sect1" data-pdf-bookmark="Summary and Recommendation"><div class="sect1" id="idm45634184802088">
<h1>Summary and Recommendation</h1>

<p>In your <a contenteditable="false" data-type="indexterm" data-primary="topic modeling" data-secondary="about" id="idm45634184800552"/>daily work, it might turn out that <a contenteditable="false" data-type="indexterm" data-primary="unsupervised methods" id="idm45634184798968"/>unsupervised methods such as topic modeling or clustering are often used as first methods to understand the content of unknown text corpora. It is further useful to check whether the right features have been chosen or this can still be optimized.</p>

<p>One of the most important decisions is the entity on which you will be calculating the topics. As shown in our blueprint example, documents don’t always have to be the best choice, especially when they are quite long and consist of algorithmically determinable subentities.</p>

<p>Finding the correct number of topics is always a challenge. Normally, this must be solved iteratively by calculating the quality indicators. A frequently used, more pragmatic approach is to try with a reasonable number of topics and find out whether the results can be interpreted.</p>

<p>Using a (much) higher number of topics (like a few hundred), topic models are often used as techniques for the <a contenteditable="false" data-type="indexterm" data-primary="dimensionality reduction" id="idm45634184795512"/>dimensionality reduction of text documents. With the resulting vectorizations, similarity scores can then be calculated in the latent space and frequently yield better results compared to the naive distance in TF-IDF space.</p>
</div></section>

<section data-type="sect1" data-pdf-bookmark="Conclusion"><div class="sect1" id="idm45634184793848">
<h1>Conclusion</h1>

<p>Topic models are a powerful technique and are not computationally expensive. Therefore, they can be used widely in text analytics. The first and foremost reason to use them is uncovering the latent structure of a document corpus.</p>

<p>Topic models are also useful for getting a summarization and an idea about the structure of large unknown texts. For this reason, they are often used routinely in the beginning of an analysis.</p>

<p>As there is a large number of different algorithms and implementations, it makes sense to experiment with the different methods and see which one yields the best results for a given text corpus. The linear-algebra-based methods are quite fast and make analyses possible by changing the number of topics combined with calculating the respective quality indicators.</p>

<p>Aggregating data in different ways before performing topic modeling can lead to interesting variations. As we have seen in the UN general debates dataset, paragraphs were more suited as the speakers talked about one topic after the other. If you have a corpus with texts from many authors, concatenating all texts per author will give you persona models for different types of authors.</p>
</div></section>

<div data-type="footnotes"><p data-type="footnote" id="idm45634189170488"><sup><a href="ch08.xhtml#idm45634189170488-marker">1</a></sup> Blei, David M., et al. “Latent Dirichlet Allocation.” <em>Journal of Machine Learning Research</em> 3 (4–5): 993–1022. doi:10.1162/jmlr.2003.3.4-5.993.</p><p data-type="footnote" id="idm45634187759704"><sup><a href="ch08.xhtml#idm45634187759704-marker">2</a></sup> For a more detailed description, see the <a href="https://oreil.ly/yr5yA">Wikipedia page</a>.</p><p data-type="footnote" id="idm45634187547192"><sup><a href="ch08.xhtml#idm45634187547192-marker">3</a></sup> pyLDAvis must be installed separately using <strong><code>pip install pyldavis</code></strong> or <strong><code>conda install pyldavis</code></strong>.</p><p data-type="footnote" id="idm45634185352648"><sup><a href="ch08.xhtml#idm45634185352648-marker">4</a></sup>  See, for example, Andrey A. Shabalin’s <a href="https://oreil.ly/OTGWX">k-means clustering page</a> or Naftali Harris’s <a href="https://oreil.ly/Po3bL">“Visualizing K-Means Clustering”</a>.</p></div></div></section></div>



  </body></html>