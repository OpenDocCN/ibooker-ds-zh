- en: Chapter 5\. Workflow Context
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Boyan Angelov
  prefs: []
  type: TYPE_NORMAL
- en: A common source of frustration for data scientists is discussing their work
    with colleagues from adjacent fields. Let’s take the example of someone who has
    been working primarily in developing machine learning (ML) models, having a chat
    about their work with a colleague from the Business Intelligence (BI) team, more
    focused on reporting. More often than not, such a discussion can make both parties
    uncomfortable due to a perceived lack of knowledge about each other’s work domain
    (and associated workflows) - despite sharing the same job title. The ML person
    might wonder, what D3.js is, the grammar of graphics, and all that? On the other
    hand, the BI data scientist might feel insecure about not knowing how to build
    a deployable API. The feelings that might arise from such a situation have been
    termed “impostor syndrome,” where doubts about your competency arise. Such a situation
    is a by-product of the sheer volume of possible applications of data science.
    A single person is rarely familiar to the same extent with more than several sub-fields.
    Flexibility is still often required in this fast-evolving field.
  prefs: []
  type: TYPE_NORMAL
- en: This complexity sets the foundation for the workflow focus in this chapter.
    We’ll cover the primary data science workflows and how the languages’ different
    ecosystems support them. Much like [Chapter 4](ch04.xhtml#ch05), at the end of
    this chapter, you’ll have everything needed for making educated decisions regarding
    your workflows.
  prefs: []
  type: TYPE_NORMAL
- en: Defining workflows
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let’s take a step back, and define a workflow:'
  prefs: []
  type: TYPE_NORMAL
- en: '**A workflow is a complete collection of tools and frameworks to perform all
    tasks required from a specific job function.**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: For this example, let’s say you’re an ML engineer. Your daily tasks might include
    tools to obtain data, process it, train a model on it, and deployment frameworks.
    Those, collectively, represent the ML engineer workflow. An overview of the data
    workflows for this and other titles and their supporting tools, is presented in
    [Table 5-1](#workflows-table).
  prefs: []
  type: TYPE_NORMAL
- en: Table 5-1\. Common data science workflows and their enabling tools.
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Python package | R package |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Data Munging^([a](ch05.xhtml#idm45127450466184)) | `pandas` | `dplyr` |'
  prefs: []
  type: TYPE_TB
- en: '| EDA | `matplotlib`, `seaborn`, `pandas` | `ggplot2`, `base-r`, `leaflet`
    |'
  prefs: []
  type: TYPE_TB
- en: '| Machine Learning | `scikit-learn` | `mlr`, `tidymodels`, `caret` |'
  prefs: []
  type: TYPE_TB
- en: '| Deep Learning | `keras`, `tensorflow`, `pytorch` | `keras`, `tensorflow`,
    `torch` |'
  prefs: []
  type: TYPE_TB
- en: '| Data Engineering^([b](ch05.xhtml#idm45127450450408)) | `flask`, `bentoML`,
    `fastapi` | `plumber` |'
  prefs: []
  type: TYPE_TB
- en: '| Reporting | `jupyter`, `streamlit` | `rmarkdown`, `shiny` |'
  prefs: []
  type: TYPE_TB
- en: '| ^([a](ch05.xhtml#idm45127450466184-marker)) Data munging (or wrangling) is
    such a fundamental topic in data science that it was already covered in [Chapter 2](ch02.xhtml#ch03).^([b](ch05.xhtml#idm45127450450408-marker))
    There is much more to data engineering than model deployment, but we decided to
    focus on this subset to illustrate Python’s ability. |'
  prefs: []
  type: TYPE_TB
- en: We omitted some areas in the hope that the listed ones are the most common and
    critical. Those selected workflows are related to each other, as presented on
    [Figure 5-1](#meta_workflow). This diagram borrows heavily from the [CRISP-DM](https://en.wikipedia.org/wiki/Cross-industry_standard_process_for_data_mining)
    framework, which shows all significant steps in a typical data science project.
    Each of the diagram’s steps has a separate workflow associated with it, generally
    assigned to an individual or a team.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/prds_0501.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-1\. Meta-workflow in data science and engineering.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Now that we have defined a workflow, what are the defining properties of a
    “good” one? We can compile a checklist with three main factors to consider:'
  prefs: []
  type: TYPE_NORMAL
- en: It’s well established. It’s widely adopted by the community (also across different
    application domains, such as Computer Vision or Natural Language Processing).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It’s supported by a well-maintained, open-source ecosystem and community. A
    workflow that relies heavily on closed-source and commercial applications (such
    as Matlab) is not considered acceptable.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It’s suitable for overlapping job functions. The best workflows are similar
    to lego bricks - their modular design and extensibility can support diverse tech
    stacks.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: With the big picture and definitions out of the way, let’s dive deeper into
    the different workflows and how they are supported by R and Python!
  prefs: []
  type: TYPE_NORMAL
- en: Exploratory data analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Looking at numbers is *hard*. Looking at rows of data containing millions upon
    millions of them is even more challenging. Any person dealing with data faces
    this challenge daily. This need has led to considerable developments in data visualization
    (DV) tools. A recent trend in the area is the explosion of self-serving analytics
    tools, such as [Tableau](https://www.tableau.com/), [Alteryx](https://www.alteryx.com/),
    and [Microsoft PowerBI](https://powerbi.microsoft.com/en-us/). These are very
    useful, but the open-source world has many alternatives available, often rivaling
    or even exceeding their commercial counterparts’ capabilities (except, in some
    cases, ease of use). Such tools collectively represent the EDA workflow.
  prefs: []
  type: TYPE_NORMAL
- en: When to use a GUI for EDA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Many data scientists frown at the notion of using a graphical user interface
    (GUI) for their daily work. They would much rather prefer the flexibility and
    utility of command-line tools instead. Nevertheless, one area where using a GUI
    makes more sense (for productivity reasons) is EDA. It can be quite time-consuming
    to generate multiple plots, especially at the beginning of a data science project.
    Usually, one would need to create tens, if not hundreds of them. Imagine writing
    the code for each one (even if you improve your code’s organization by refactoring
    into functions). For some larger datasets, it’s sometimes much easier to use some
    GUI, such as AWS Quicksight or Google Data Studio. By using a GUI the data scientist
    can quickly generate a lot of plots first and only then write the code for the
    ones that make the cut after screening. There are a few good open-source GUI tools,
    for example [Orange](https://orange.biolab.si/).
  prefs: []
  type: TYPE_NORMAL
- en: EDA is a fundamental step at the beginning of the analysis of any data source.
    It is typically performed directly after data loading, at the stage where there’s
    a significant need for business understanding. This explains why it’s an essential
    step. You are probably familiar with the *garbage in*, *garbage out* paradigm
    - the quality of any data project depends on the quality of the input data and
    the domain knowledge behind it. EDA enables the success of the downstream workflows
    (such as ML), ensuring both the data and the assumptions behind it are correct
    and of sufficient quality.
  prefs: []
  type: TYPE_NORMAL
- en: In EDA, R has far better tools available than Python. As we discussed in [Chapter 1](ch01.xhtml#ch01)
    and [Chapter 2](ch02.xhtml#ch03), R is a language made *by* statisticians and
    *for* statisticians (remember FUBU from [Chapter 2](ch02.xhtml#ch03)?), and data
    visualization (plotting) has been of great importance in statistics for decades.
    Python has made some forward strides in recent years but is still seen as lagging
    (you need just to look at example `matplotlib` plot to realize this fact^([1](ch05.xhtml#idm45127450420728))).
    Enough praise for R; let’s have a look at why it’s great for EDA!
  prefs: []
  type: TYPE_NORMAL
- en: Static visualizations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You should already be acquainted with base R’s powers in terms of DV from [Chapter 4](ch04.xhtml#ch05),
    especially regarding time series plotting. Here we’ll take a step further and
    discuss one of the most famous R packages - `ggplot2`. It’s one of the main reasons
    why Pythonistas want to switch to R^([2](ch05.xhtml#idm45127450414536)). What
    makes `ggplot2` so successful in EDA work is that it’s based on a well thought-through
    methodology - the Grammar of Graphics (GoG). It was developed by L. Wilkinson,
    and the package by Hadley Wickham^([3](ch05.xhtml#idm45127450412248)).
  prefs: []
  type: TYPE_NORMAL
- en: What *is* the GoG? The [original paper](https://vita.had.co.nz/papers/layered-grammar.html)
    behind it has the title “A layered grammar of graphics,” and the word “layered”
    holds the key. Everything you see on a plot contributes to a larger stack or system.
    For example, the axes and grids form a separate layer compared to the lines, bars,
    and points. Those latter elements constitute the “data” layer. The complete stack
    of layers forms the result - a complete `ggplot`. Such a modular design pattern
    allows for great flexibility and provides a new way of thinking about data visualization.
    The logic behind GoG is illustrated in [Figure 5-2](#gog).
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/prds_0502.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-2\. The layered Grammar of Graphics.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: To illustrate the different procedures for a regular EDA workflow we’ll use
    the `starwars` dataset (available from the `dplyr` package^([4](ch05.xhtml#idm45127450403976))).
    This dataset contains information on characters in the Star Wars movies, such
    as their gender, height and species. Let’s have a look!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_workflow_context_CO1-1)'
  prefs: []
  type: TYPE_NORMAL
- en: This will make the dataset visible in your RStudio environment, but it’s not
    strictly necessary.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a first step, let’s do a basic plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'This plots the counts of the hair color variable. Here, we see a familiar operator,
    `+`, used unconventionally. We use `+` in ggplot2 to *add* layers on top of each
    other in `ggplot2`. Let’s build on this with a more involved case. Note that we
    omitted a filtering step from the code here (there’s an outlier - Jabba the Hut):
    ^([5](ch05.xhtml#idm45127450350536)).'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_workflow_context_CO2-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Specify which data and features to use.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_workflow_context_CO2-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Select a points plot (the most suitable for continuous data).
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](Images/3.png)](#co_workflow_context_CO2-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Use a built-in `theme` - a collection of specific layer styles.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](Images/4.png)](#co_workflow_context_CO2-4)'
  prefs: []
  type: TYPE_NORMAL
- en: Fit a linear model and show the results as a layer on the plot.
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](Images/5.png)](#co_workflow_context_CO2-5)'
  prefs: []
  type: TYPE_NORMAL
- en: Add title and axes labels.
  prefs: []
  type: TYPE_NORMAL
- en: The results of this plotting operation are shown on [Figure 5-3](#adv_ggplot_1).
    With just several lines of code, we created a beautiful plot, which can be extended
    even further.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/prds_0503.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-3\. An advanced ggplot2 plot.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Now that we covered static visualizations let’s see how to make them more interesting
    by adding interactivity!
  prefs: []
  type: TYPE_NORMAL
- en: Interactive visualizations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Interactivity can be a great aid to exploratory plots. Two excellent R packages
    stand out: [`leaflet`](https://rstudio.github.io/leaflet/) and [`plotly`](https://plotly.com/).'
  prefs: []
  type: TYPE_NORMAL
- en: Beware of JavaScript
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Interactivity in Python and R is often based on an underlying JavaScript codebase.
    Packages like `leaflet` and `plotly` take care of this for us, but keen to learn
    pure JavaScript. Low-level packages for interactive graphics, like [D3.js](https://d3js.org/),
    can be overwhelming to learn for the novice. Thus, we’d encourage learning a high-level
    framework, such as [Dimple.js](http://dimplejs.org/) instead.
  prefs: []
  type: TYPE_NORMAL
- en: Different datasets require different visualization methods. We covered the case
    of a standard tabular dataset (`starwars`), but how about something different?
    We’ll have a go at visualizing data with a spatial dimension and use it to show
    R’s excellent capabilities in producing interactive plots. For this, we selected
    the [Shared Cars Locations dataset](https://www.kaggle.com/gidutz/autotel-shared-car-locations).
    It provides the locations of car-sharing vehicles in Tel-Aviv, Israel. Can we
    show those on a map?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: In this case, we subset the data using the first 20 rows only (to make the visualization
    less cluttered). The `addTiles` function provides the map background, with the
    street and city names^([6](ch05.xhtml#idm45127450129960)). The next step is to
    add the markers which specify the car locations by using `addMarkers`. The result
    of this relatively simple operation is shown in [Figure 5-4](#leaflet).
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/prds_0504.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-4\. An interactive map plot with leaflet
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As with the best data science tools, packages like `leaflet` hide a lot of complexity
    under the hood. They do much of the heavy lifting necessary for advanced visualization
    and enable the data scientist to do what they do best - focus on the data. There
    are many more advanced features available in `leaflet`, and we encourage the motivated
    user to explore them.
  prefs: []
  type: TYPE_NORMAL
- en: Make ggplot2 interactive
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As our book’s subtitle suggests, we are always attempting to take the best of
    both worlds. So one easy way to do it is to use the `ggplotly` command from the
    `plotly` package and pass it a `ggplot2` plot. This will make the plot interactive!
  prefs: []
  type: TYPE_NORMAL
- en: Hopefully, this section has made clear why the EDA workflow makes using R and
    tools such as `ggplot2` and `leaflet` the best options. We’ve just scratched the
    surface on what’s possible, and if one decides to go deeper into the data visualization
    aspects, there are a ton of great resources available.
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Nowadays, data science is used almost synonymously with machine learning (ML).
    While there are many different workflows necessary for a data science project
    ([Figure 5-1](#meta_workflow)), ML often steals the focus of aspiring data scientists.
    This is partly due to an increasing growth surge in recent years due to the availability
    of large amounts of data, better computing resources (such as better CPUs and
    GPUs), and the need for predictions and automation in modern business. In the
    early days of the field, it was known under a different name - statistical learning.
    As previously mentioned, statistics has been historically the primary domain of
    R. Thus there were good tools available early on for doing ML in it. However,
    this has changed in recent years, and Python’s tools have mostly overtaken its
    statistical competitor.
  prefs: []
  type: TYPE_NORMAL
- en: One can trace Python’s ML ecosystem’s success to one specific package - [`scikit-learn`](https://scikit-learn.org/stable/).
    Since its early versions, the core development team has focused on designing an
    accessible and easy-to-use API. They supported this with some of the most complete
    and accessible documentation available in the open-source world. It’s not only
    a reference documentation but contains excellent tutorials on various specific
    modern ML applications, such as [working with text data](https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html).
    `scikit-learn` provides access to almost all common ML algorithms out of the box^([7](ch05.xhtml#idm45127450113112)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s have a look at some proof of why `scikit-learn` is so great for ML. First,
    we can demonstrate the model imports:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Here we can already see how consistently those models are designed - similar
    to books in a well-organized library; everything is at the right place. ML algorithms
    in `scikit-learn` are grouped based on their similarities. In this example, tree-based
    methods such as Decision Tree belong to the `tree` module. In contrast, linear
    algorithms can be found in the `linear_model` one (i.e., if you want to perform
    a Lasso model, you can predictably find it in `linear_model.Lasso`). Such hierarchical
    design makes it easier to focus on writing code and not to search for documentation
    since any good autocomplete engine will find the relevant model for you.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We discussed modules in [Chapter 3](ch03.xhtml#ch04), but it’s a concept that
    bears repeating since it might be confusing for some R users. Modules in Python
    are nothing more than collections of organized scripts (based on some similarities,
    such as “data_processing” for example), which allows them to be imported into
    your applications, improving readability and making the codebase more organized.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we need to prepare the data for modeling. An essential element of any
    ML project is splitting the data into train and test sets. While newer R packages
    such as `mlr` improve on this as well, `scikit-learn` has better (in terms of
    both consistency and syntax) functions available:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Suppose we have been consistent in the steps before and have followed traditional
    ML convention. In that case, we have the `X` object to store our features and
    ``y'' - the labels (in the case of a supervised learning problemfootnote:[For
    those readers new to ML, supervised learning is concerned with prediction tasks
    where a target is available (label), as compared to unsupervised learning where
    it''s missing, and the prediction task is on uncovering groups in the data.]).
    In this case, the data will be randomly split. The official way to do this in
    R''s `mlr`` is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'This can be harder to understand, and if one needs documentation on how to
    perform a more advanced split, such as by stratification, there’s little available,
    and another package might be required, increasing the learning curve and cognitive
    load on the data scientist. `scikit-learn`, on the other hand, provides a handy
    function in `StratifiedShuffleSplit`. The capabilities only increase further when
    we start to perform the actual modeling:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'These three code lines are all we need to initialize the model with default
    parameters, fit (train) it on the training dataset, and predict on the test one.
    This pattern is consistent across projects (except for the model initialization,
    where one selects their algorithm of choice and its parameters - those do differ,
    of course). A visual comparison between several different packages (from other
    developers and purposes) is shown in [Figure 5-6](#consistent_api_ml). Finally,
    let’s compute some performance metrics; many of them are handily available:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The `metrics` module contains everything needed to check our model’s performance,
    with a simple and predictable Application Programming Interface (API). The pattern
    of `fit` and `predict` we saw earlier has been so influential in the open-source
    world that it has been widely adopted by other packages, such as `yellowbrick`
    (a package for model performance visualization):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: There are many other visualizations available in `yellowbrick`, all obtained
    with a similar procedure. Some are presented in [Figure 5-5](#yellowbrick).
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/prds_0505.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-5\. Different possible `yellowbrick` regression plots.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The consistency and ease of use are among the significant reasons users want
    to use Python for ML. It enables the user to focus on the task at hand and not
    on writing code and sifting through tedious documentation pages. There were changes
    in R packages in recent years aiming at reducing those deficiencies. Such packages
    most notably include `mlr` and `tidymodels`. Still, they are not widely used,
    but perhaps this pattern can change in the future. There is an additional factor
    to consider here, which is similar to the ecosystem interoperability we saw in
    [Chapter 4](ch04.xhtml#ch05). `scikit-learn` works very well with other tools
    Python, which are necessary for the development and deployment of ML models. Such
    tools include database connections, high-performance computing packages, testing
    frameworks, and deployment frameworks. Writing the ML code in `scikit-learn` will
    enable the data scientists to be a more productive part of a data team (just imagine
    the expression of your data engineering colleagues’ faces when you deliver an
    `mlr` model to them for deployment).
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/prds_0506.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-6\. API consistency overview in the Python ML ecosystem.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'To wrap up this section, we can summarize the main points about the ML workflow
    and why Python tools better support it:'
  prefs: []
  type: TYPE_NORMAL
- en: Focus has moved to real-time predictions and automation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Python ML workflow provides a more consistent and easy-to-use API.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Python is more of a glue language ^([8](ch05.xhtml#idm45127449767160)), ideal
    for combining different software components (i.e.,frontend/backend and databases).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the next section, we’ll go deeper into the third part of this list and demonstrate
    the recommended Data Engineering workflow.
  prefs: []
  type: TYPE_NORMAL
- en: Data engineering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Despite the ML tools’ advancements in recent years, the completion rate of such
    projects in companies remains low. One reason which is often credited for this
    is the lack of data engineering (DE) support. To apply ML and advanced analytics,
    companies need the infrastructural foundation provided by data engineers, including
    databases, data processing pipelines, testing, and deployment tools. Of course,
    this forms a separate job title - data engineer. Still, data scientists need to
    interface (and sometimes implement themselves) with those technologies to ensure
    data science projects are completed successfully.
  prefs: []
  type: TYPE_NORMAL
- en: While DE is a massive field, we’ll focus on a subset for this section. We selected
    model deployment for this since it’s the most common DE workflow that a data scientist
    might need to participate in. So what is ML deployment? Most of the time, this
    means creating an application programming interface (API) and making it available
    to other applications, either internally or externally (to customers, this is
    called “exposing” an API, to be “consumed”). Commonly ML models are deployed via
    a REST interface^([9](ch05.xhtml#idm45127449761224)).
  prefs: []
  type: TYPE_NORMAL
- en: ML model deployment, compared to the other topics in this chapter, requires
    interfacing with many different technologies, not directly related to data science.
    These include web frameworks, CSS, HTML, JavaScript, cloud servers, load balancers,
    and others. Thus it’s not surprising that Python tools dominate here^([10](ch05.xhtml#idm45127449733704))
    - as we covered before, it’s a fantastic glue language.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The model deployment workflow requires code to be executed on other machines
    rather than the local one where the data scientist performs their daily work.
    This hits “it works on my machine” problem right on the head. There are different
    ways to deal with managing different environments consistently, ranging from simple
    to complex. A simple way to do this is to use a `requirements.txt` file, where
    all dependencies are specified. A more complex option, which is often used in
    large-scale, critical deployments, uses container solutions such as [Docker](https://www.docker.com/).
    This dependency management is much easier to achieve in Python than in R.
  prefs: []
  type: TYPE_NORMAL
- en: One of the most popular tools to create an API is Python’s [Flask](https://flask.palletsprojects.com/en/1.1.x/)
    - a [micro-framework](https://en.wikipedia.org/wiki/Microframework#:~:text=A%20microframework%20is%20a%20term,Accounts%2C%20authentication%2C%20authorization%2C%20roles).
    It provides a minimalist interface that is easy to extend with other tools, such
    as ones providing user authentication or better design. To get started, we’ll
    go through a small example. We would need a typical Python installation with some
    other additional configurations such as a virtual environment^([11](ch05.xhtml#idm45127449727448))
    and a GUI to query the API. Let’s get started!
  prefs: []
  type: TYPE_NORMAL
- en: ML-focused API frameworks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Recently competitors to Flask have sprung up. They serve the same purpose but
    with an increased focus on ML. Two popular examples include [BentoML](https://www.bentoml.ai/)
    and [FastAPI](https://fastapi.tiangolo.com/). Those frameworks provide you with
    some additional options that make ML deployment easier. Remember that Flask was
    initially built for web development APIs, and the needs of an ML project can be
    different.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll be building an API that predicts housing prices^([12](ch05.xhtml#idm45127449720408)).
    It’s always prudent to start with the end goal in mind and how we’d like such
    a predictive model to be used by an external application or an end-user. In this
    case, we can imagine our API to be integrated into an online house rental portal.
  prefs: []
  type: TYPE_NORMAL
- en: 'For brevity, we’ll omit the model training part. Imagine that you have followed
    a traditional `scikit-learn` model development. The results of the predictive
    model are stored in a `.pkl` (`Pickle` object, the standard Python way to store
    objects on disk). This process is called serialization, and we need to do it to
    use the model in the API later:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We can save this code in a script called `train_model.py`. By running it: `python
    train_model.py`, the pickled model will be produced and saved. [Figure 5-7](#ml_api_diagram)
    provides an overview of how the different components fit.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/prds_0507.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-7\. Example architecture for an ML API.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_workflow_context_CO3-1)'
  prefs: []
  type: TYPE_NORMAL
- en: We use this function to specify that the payload string object is actually a
    dictionary.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_workflow_context_CO3-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Here we create an object that holds the app.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](Images/3.png)](#co_workflow_context_CO3-3)'
  prefs: []
  type: TYPE_NORMAL
- en: In those several lines we load the serialized model.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](Images/4.png)](#co_workflow_context_CO3-4)'
  prefs: []
  type: TYPE_NORMAL
- en: This Python decorator creates an “end-point” (see info box below).
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](Images/5.png)](#co_workflow_context_CO3-5)'
  prefs: []
  type: TYPE_NORMAL
- en: At this step, the serialised model is used for inference.
  prefs: []
  type: TYPE_NORMAL
- en: '[![6](Images/6.png)](#co_workflow_context_CO3-6)'
  prefs: []
  type: TYPE_NORMAL
- en: The inference results are returned in a JSON format.
  prefs: []
  type: TYPE_NORMAL
- en: This code is added to a file `app.py`. Once you execute this script, the command
    line will output a local URL. We can then use a tool such as Postman to query
    it^([13](ch05.xhtml#idm45127449396760)). Have a look at [Figure 5-8](#postman)
    to see how such a query works. Voilà - we built an ML API!
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In our example, the API provides just one functionality - the ability to predict
    a housing price on a dataset. Often in the real world, the same application might
    need to do different things. This is organized by creating different end-points.
    For example, there might be an end-point for triggering a data preparation script
    and a separate inference one.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/prds_0508.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-8\. Querying the ML API with Postman.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Cloud deployment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After you’re done with writing and testing the ML API code, the next phase would
    be to deploy it. Of course, you could use your computer as a server and expose
    it to the internet, but you can imagine that doesn’t scale very well (you have
    to keep your machine running, and it might run out of resources). One of the significant
    changes seen in recent years in terms of DE tools is the advent of cloud computing.
    Cloud platforms such as Amazon Web Services (AWS), or Google Cloud Provider (GCP)
    provide you with excellent opportunities and deploy your apps. Your Flask API
    can be deployed via a cloud service such as [Elastic Beanstalk](https://aws.amazon.com/elasticbeanstalk/)
    or [Google App Engine](https://cloud.google.com/appengine).
  prefs: []
  type: TYPE_NORMAL
- en: Due to the “glue-like” nature of Python packages, they dominate the DE workflow.
    If a data scientist can write such applications on their own in Python, the success
    of the complete data project is ensured.
  prefs: []
  type: TYPE_NORMAL
- en: Reporting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Every data scientist is aware (perhaps painfully so) of how vital communication
    is for their daily work. It’s also an often underrated skill, so this mantra bears
    repeating. So, what is more important than one of the essential deliverables of
    a data science project - reporting your results?
  prefs: []
  type: TYPE_NORMAL
- en: There are different reporting methods available. The most typical use case for
    a data scientist is to create a document, or a slide deck, containing the results
    of the analysis they have performed on a dataset. This is usually a collection
    of visualizations with an associated text and a consistent storyline (i.e., going
    through the different stages of a project lifecycle - data importing, cleaning,
    and visualization). There are other situations where the report has to be referred
    to often and updated in real-time - called dashboards. And finally, some reports
    allow the end-user to explore them more interactively. We’ll go through those
    three report types in the following subsections.
  prefs: []
  type: TYPE_NORMAL
- en: Static reporting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The popularization of the markdown (MD) language helps data scientists focus
    on writing code and associated thoughts instead of the tool itself. A flavor of
    this language - R Markdown (RMD) is widely used in the R community. This allows
    for the concept of “literate programming”, where the code is mixed with the analysis.
    The RStudio IDE provides even further functionality with tools such as [R notebooks](https://rmarkdown.rstudio.com/lesson-10.html).
    This is how easy writing an RMD report is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]{r}'
  prefs: []
  type: TYPE_NORMAL
- en: library(dplyr)
  prefs: []
  type: TYPE_NORMAL
- en: data(starwars)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: This `.rmd` file can then be `knit` (compiled) into a `.pdf` or an `.html` (best
    for interactive plots), creating a beautiful report. There are additional templates
    to create even slides, dashboards and websites from RMD files. Have a look at
    [Figure 5-9](#example_rmd) to check it out in action.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/prds_0509.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-9\. RMarkdown editing within RStudio
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As with everything in the open-source world, data scientists worldwide have
    contributed to the further development of RMD. There are many templates available
    for RMD, enabling users to create everything from a custom-styled report to a
    dynamically generated blogging website.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The widely adopted alternative to RMD in the Python world is the [Jupyter](https://jupyter.org/)
    Notebook (along with its newer version - [Jupyter Lab](https://jupyterlab.readthedocs.io/en/stable/)).
    The “r” in Jupyter stands for R, and it is certainly possible to use that, but
    we argue that the RMD notebooks in RStudio provide a better interface, at least
    for R work.
  prefs: []
  type: TYPE_NORMAL
- en: Interactive reporting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: What if we want to be able to let the recipients of our report do some work
    as well? If we allow for some interactivity, our end-users would answer questions
    for themselves without relying on us to go back, change the code and regenerate
    the graphs. There are several tools available^([14](ch05.xhtml#idm45127449354904)),
    but most of them pale in comparison to the ease of use and capabilities of R’s
    `shiny` package^([15](ch05.xhtml#idm45127449353176)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Using this package requires a bit of a different way of writing R code, but
    you will create fantastic applications once you get used to it. Let’s go through
    a basic yet practical example. `shiny` apps consist of two fundamental elements:
    the user interface (UI) and the server logic. Those are often even separated into
    two files. For simplicity we’ll use the single file layout and use two functions
    for the app.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_workflow_context_CO4-1)'
  prefs: []
  type: TYPE_NORMAL
- en: This function specifies that we want to have a “fluid” layout - that makes the
    app “responsive” - easy to read on a variety of devices, such as smartphones.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_workflow_context_CO4-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Add the dynamic input for the user.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](Images/3.png)](#co_workflow_context_CO4-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Add a dedicated area for the output.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `ui` object contains all the “frontend” parts of the application. The actual
    computation happens in the following function; we’ll be adding the `ggplot` from
    the DV section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_workflow_context_CO5-1)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The server needs two things: input and output.'
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_workflow_context_CO5-2)'
  prefs: []
  type: TYPE_NORMAL
- en: There is just one output in our case.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](Images/3.png)](#co_workflow_context_CO5-3)'
  prefs: []
  type: TYPE_NORMAL
- en: We can add all kinds of R computations here, as in a regular R script.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](Images/4.png)](#co_workflow_context_CO5-4)'
  prefs: []
  type: TYPE_NORMAL
- en: The most recent item (in this case, a plot) is returned for display in the frontend.
  prefs: []
  type: TYPE_NORMAL
- en: The computation happens in this function. In the end, we need to pass those
    two functions here to start the app. The results of this are shown on [Figure 5-10](#shiny_example).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '![](Images/prds_0510.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-10\. An interactive report with Shiny.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: One difference for our Shiny app that might make it tricky to use than our markdown
    files is that you would need to host the application on a remote machine. For
    a normal `.rmd` on the files, you need to knit the file into a PDF and then share
    it. How such applications are deployed is beyond this book’s scope.
  prefs: []
  type: TYPE_NORMAL
- en: Creating reports is a small but vital component of data science work. This is
    how your work is shown to the outside world, be it your manager or another department.
    Even if you have done a great job in your analysis, it will often be judged by
    how well you communicate the process and results. Tools of literate programming
    such as RMD and more advanced interactive reports in `shiny` can go a long way
    to creating a state of the art reports. In the final chapter of this book, [Chapter 7](ch07.xhtml#ch08),
    we’ll provide a great example of this in action.
  prefs: []
  type: TYPE_NORMAL
- en: Final thoughts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter,, we went through the most essential workflows in a data science
    project and discovered the best tools in R and Python. In terms of EDA and reporting,
    R can be crowned the king. Packages such as `ggplot2` are peerless in the data
    science community, and `shiny` can allow for fascinating new ways to present data
    science results to stakeholders and colleagues. In the ML and DE worlds, Python’s
    glue-like nature provides fantastic options, enabling modern data scientists to
    focus on the work rather than the tools.
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch05.xhtml#idm45127450420728-marker)) It’s a bit unfair to present `matplotlib`
    as the only viable alternative from Python. The [seaborn](https://seaborn.pydata.org/)
    package also enables the creation of beautiful plots quickly but still lags behind
    the `ggplot` features. It’s worth mentioning that newer versions of `pandas` have
    plotting capabilities as well, so we should watch this space.
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch05.xhtml#idm45127450414536-marker)) There have been attempts to recreate
    this package in Python, such as [ggplot](https://pypi.org/project/ggplot/) but
    they have not caught on in the community so far.
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](ch05.xhtml#idm45127450412248-marker)) He wrote many other packages, and
    in some ways almost single-handedly changed the way people use R in a modern context.
    Have a look at [Chapter 2](ch02.xhtml#ch03) for more information on his packages.
  prefs: []
  type: TYPE_NORMAL
- en: ^([4](ch05.xhtml#idm45127450403976-marker)) More information on the dataset
    is available [here](https://rdrr.io/cran/dplyr/man/starwars.html).
  prefs: []
  type: TYPE_NORMAL
- en: ^([5](ch05.xhtml#idm45127450350536-marker)) Did you know that his real name
    is Jabba Desilijic Tiure?
  prefs: []
  type: TYPE_NORMAL
- en: ^([6](ch05.xhtml#idm45127450129960-marker)) Explore the official documentation
    [here](https://rstudio.github.io/leaflet/) for different map styles.
  prefs: []
  type: TYPE_NORMAL
- en: ^([7](ch05.xhtml#idm45127450113112-marker)) An overview of those is available
    [here](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html).
  prefs: []
  type: TYPE_NORMAL
- en: ^([8](ch05.xhtml#idm45127449767160-marker)) For a visual appreciation of the
    complexity of ML architectures, have a look at [this](https://cloud.google.com/solutions/machine-learning/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning)
    MLOps document from Google.
  prefs: []
  type: TYPE_NORMAL
- en: ^([9](ch05.xhtml#idm45127449761224-marker)) To learn more about what is REST,
    have a look at [this](https://en.wikipedia.org/wiki/Representational_state_transfer)
    resource.
  prefs: []
  type: TYPE_NORMAL
- en: ^([10](ch05.xhtml#idm45127449733704-marker)) The R alternative to Flask is `plumber`.
    The RStudio IDE provides a friendly interface to use this tool, but still, it
    is lagging in options and adoption in the ML community.
  prefs: []
  type: TYPE_NORMAL
- en: ^([11](ch05.xhtml#idm45127449727448-marker)) For brevity, we will not go deeper
    into setting up virtual environments here. We urge the dedicated reader to read
    up upon the [`virtualenv`](https://virtualenv.pypa.io/en/latest/) and [`renv`](https://rstudio.github.io/renv/articles/renv.html)
    tools, covered in [Chapter 3](ch03.xhtml#ch04).
  prefs: []
  type: TYPE_NORMAL
- en: ^([12](ch05.xhtml#idm45127449720408-marker)) The dataset is “Boston Housing”,
    available [here](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html).
  prefs: []
  type: TYPE_NORMAL
- en: ^([13](ch05.xhtml#idm45127449396760-marker)) If you are more of a command-line
    person, have a look at `curl`.
  prefs: []
  type: TYPE_NORMAL
- en: ^([14](ch05.xhtml#idm45127449354904-marker)) There’s an advanced new tool in
    Python, called [streamlit](https://www.streamlit.io/), but it is yet to gain in
    popularity and adoption.
  prefs: []
  type: TYPE_NORMAL
- en: ^([15](ch05.xhtml#idm45127449353176-marker)) To get inspired with what’s possible
    in Shiny, look at the gallery of use cases at the [RStudio website](https://shiny.rstudio.com/gallery/).
  prefs: []
  type: TYPE_NORMAL
