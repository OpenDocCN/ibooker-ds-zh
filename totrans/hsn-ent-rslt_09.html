<html><head></head><body><section data-pdf-bookmark="Chapter 9. Cloud Entity Resolution Services" data-type="chapter" epub:type="chapter"><div class="chapter" id="chapter_9">&#13;
<h1><span class="label">Chapter 9. </span>Cloud Entity Resolution Services</h1>&#13;
&#13;
<p>In the last chapter, we saw how to scale up our entity resolution process to run on a Google Cloud–managed Spark cluster. This approach allowed us to match larger datasets in a reasonable time but it required us to do quite a bit of setup and management ourselves.</p>&#13;
&#13;
<p>An alternative approach is to use entity resolution API provided by a cloud provider to perform the hard work for us. Google, Amazon, and Microsoft all offer these services.</p>&#13;
&#13;
<p>In this chapter, we will use the entity reconciliation service, provided as part of Google’s Enterprise Knowledge Graph API, to resolve the MCA and Companies House datasets we examined in Chapters <a data-type="xref" data-xrefstyle="select:labelnumber" href="ch06.html#chapter_6">6</a> and <a data-type="xref" data-xrefstyle="select:labelnumber" href="ch08.html#chapter_8">8</a>. We will:</p>&#13;
&#13;
<ul>&#13;
	<li>Upload our standardized datasets to Google’s data warehouse, BigQuery.</li>&#13;
	<li>Provide a mapping of our data schema to a standard ontology.</li>&#13;
	<li>Invoke the API from the console (we will also invoke the API using a Python script).</li>&#13;
	<li>Use some basic SQL to process the results.</li>&#13;
</ul>&#13;
&#13;
<p>​​To complete the chapter we will examine how well the service performs.</p>&#13;
&#13;
<section class="pagebreak-before" data-pdf-bookmark="Introduction to BigQuery" data-type="sect1"><div class="sect1" id="id70">&#13;
<h1 class="less_space">Introduction to BigQuery</h1>&#13;
&#13;
<p>BigQuery<a contenteditable="false" data-primary="cloud entity resolution services" data-secondary="BigQuery" data-type="indexterm" id="CERSbigq09"/><a contenteditable="false" data-primary="BigQuery" data-type="indexterm" id="bigQ09"/> is Google’s fully managed, serverless data warehouse that enables scalable analysis over petabytes of data. It is a platform as a service that supports data querying and analysis using a dialect of SQL.</p>&#13;
&#13;
<p>To begin, we select the BigQuery product from the Google Cloud console. Under ANALYSIS we select “SQL workspace.”</p>&#13;
&#13;
<p>Our first step is to select “Create dataset” from the ellipsis menu alongside your project name, as shown in <a data-type="xref" href="#fig-9-1">Figure 9-1</a>.</p>&#13;
&#13;
<figure><div class="figure" id="fig-9-1"><img class="iimagesch09ch09createdatasetpng" src="assets/hoer_0901.png"/>&#13;
<h6><span class="label">Figure 9-1. </span>BigQuery Create dataset</h6>&#13;
</div></figure>&#13;
&#13;
<p>In the pop-up window, as shown in <a data-type="xref" href="#fig-9-2">Figure 9-2</a>, we need to name the Dataset ID as Chapter9, and then select a Location Type. You can then select a specific Region if you prefer or simply accept the Multi-region default. Optionally, you can add a number of days after which the table expires automatically.</p>&#13;
&#13;
<p>Once we have an empty dataset created, our next task is to upload our MCA and Companies House tables. We can upload these tables from the data we saved in the Google Cloud Storage bucket in <a data-type="xref" href="ch08.html#chapter_8">Chapter 8</a>.</p>&#13;
&#13;
<figure><div class="figure" id="fig-9-2"><img class="iimagesch09ch09createdataset2png" src="assets/hoer_0902.png"/>&#13;
<h6><span class="label">Figure 9-2. </span>BigQuery Create dataset config</h6>&#13;
</div></figure>&#13;
&#13;
<p>With the dataset selected, we can click “+ Add,” or Add Data and then select Google Cloud Storage as the source (as shown in <a data-type="xref" href="#fig-9-3">Figure 9-3</a>). You can then browse to your Cloud Storage bucket and select the <em>mari_clean.csv</em> file. Select the Chapter9 dataset as the destination and name the table <em>mari</em>. Under Schema, click the “Auto detect” checkbox. You can accept the remainder of the default settings.</p>&#13;
&#13;
<figure><div class="figure" id="fig-9-3"><img alt="" class="iimagesch09ch09createtablepng" src="assets/hoer_0903.png"/>&#13;
<h6><span class="label">Figure 9-3. </span>BigQuery Create table</h6>&#13;
</div></figure>&#13;
&#13;
<p>Repeat this procedure for the <em>basic_clean.csv</em> file, naming it <em>basic</em>. You can then select the table from the dataset to examine the schema. Selecting Preview will give you a view of the first few rows, as shown in <a data-type="xref" href="#fig-9-4">Figure 9-4</a>.</p>&#13;
&#13;
<figure><div class="figure" id="fig-9-4"><img alt="" class="iimagesch09ch09tableschemapng" src="assets/hoer_0904.png"/>&#13;
<h6><span class="label">Figure 9-4. </span>BigQuery table schema </h6>&#13;
</div></figure>&#13;
&#13;
<p>Now that we have successfully loaded our data, we need to tell the Enterprise Knowledge Graph API how to map our schema and then run a reconciliation job.<a contenteditable="false" data-primary="" data-startref="CERSbigq09" data-type="indexterm" id="id577"/><a contenteditable="false" data-primary="" data-startref="bigQ09" data-type="indexterm" id="id578"/></p>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Enterprise Knowledge Graph API" data-type="sect1"><div class="sect1" id="enterprise-knowledge">&#13;
<h1>Enterprise Knowledge Graph API</h1>&#13;
&#13;
<p>The Google Enterprise Knowledge Graph API<a contenteditable="false" data-primary="cloud entity resolution services" data-secondary="Google Enterprise Knowledge Graph API" data-type="indexterm" id="CERSgentknow09"/><a contenteditable="false" data-primary="Google Enterprise Knowledge Graph API" data-secondary="basics of" data-type="indexterm" id="id579"/><a contenteditable="false" data-primary="graph visualizations" data-type="indexterm" id="graphvis09"/> provides a lightweight entity resolution service that they call Entity Reconciliation. The service uses an AI model trained on Google data. It uses a parallel version of<a contenteditable="false" data-primary="agglomerative hierarchical clustering" data-type="indexterm" id="id580"/><a contenteditable="false" data-primary="clustering" data-secondary="hierarchical agglomerative clustering" data-type="indexterm" id="id581"/><a contenteditable="false" data-primary="hierarchical agglomerative clustering" data-type="indexterm" id="id582"/> <em>hierarchical agglomerative clustering</em>.</p>&#13;
&#13;
<div data-type="note" epub:type="note">&#13;
<h1>Hierarchical Agglomerative Clustering</h1>&#13;
&#13;
<p>This is a “bottom-up” approach to clustering entities. Each entity starts in its own cluster and then they are aggregated depending upon their similarity.</p>&#13;
</div>&#13;
&#13;
<p>At the time of writing, the<a contenteditable="false" data-primary="Google Cloud Entity Reconciliation service" data-type="indexterm" id="id583"/> Entity Reconciliation service is at Preview status and is made available on Pre-GA terms, details of which are available on the <a href="https://oreil.ly/dThBk">Google Cloud website</a>.</p>&#13;
&#13;
<p>To enable the API, select Enterprise KG under Artificial Intelligence from the console navigation menu. From here you can click “Enable the Enterprise Knowledge Graph API” for your project.</p>&#13;
&#13;
<section data-pdf-bookmark="Schema Mapping" data-type="sect2"><div class="sect2" id="schema-mappingto-setu">&#13;
<h2>Schema Mapping</h2>&#13;
&#13;
<p>To<a contenteditable="false" data-primary="Google Enterprise Knowledge Graph API" data-secondary="schema mapping" data-type="indexterm" id="GEKschema09"/><a contenteditable="false" data-primary="schema mapping" data-type="indexterm" id="schmap09"/> set up our entity resolution job, we first need to map our data schema onto the schema that the Google Entity Reconciliation API understands. We do this by creating a mapping file for each data source we are going to use. The API uses a human-readable simple format language called<a contenteditable="false" data-primary="YARRRML format language" data-type="indexterm" id="id584"/> YARRRML to define the mappings between source schema and a target ontology from <a class="orm:hideurl" href="https://schema.org">schema.org</a>. It supports three different entity types: Organization, Person, and Local Business. For our example, we will use the Organization schema.</p>&#13;
&#13;
<p>To begin, we click on Schema Mapping and then select “Create a Mapping” in the Organization box. This brings us to an editor where we can modify and save a template mapping file. The mapping file is divided into a prefix section that tells the API which model and schema reference we are going to use. The mapping section then lists each entity type contained in the dataset. For each entity type, we specify the sources, a subject key (<code>s</code>) that uniquely refers to an entity in the dataset, and then the predicate list (<code>po</code>) which specifies the attributes of the entity we wish to match on.</p>&#13;
&#13;
<p>The default template is as follows:</p>&#13;
&#13;
<pre data-type="programlisting">&#13;
prefixes:&#13;
   ekg: http://cloud.google.com/ekg/0.0.1#&#13;
   schema: https://schema.org/&#13;
&#13;
mappings:&#13;
   organization:&#13;
      sources:&#13;
         - [<strong>example_project:example_dataset.example_table~bigquery</strong>]        &#13;
      s: ekg:<strong>company</strong>_$(<strong>record_id</strong>)&#13;
      po:&#13;
         - [a, schema:Organization]&#13;
         - [<strong>schema:name, $(company_name_in_source)</strong>]&#13;
         - [schema:streetAddress, $(street)]&#13;
         - [schema:postalCode, $(<strong>postal_code</strong>)]&#13;
         - [schema:addressCountry, $(country)]&#13;
         - [schema:addressLocality, $(city)]&#13;
         - [schema:addressRegion, $(state)]&#13;
         - [ekg:recon.source_name, $(<strong>source_system</strong>)]&#13;
         - [ekg:recon.source_key, $(<strong>source_key</strong>)]</pre>&#13;
&#13;
<p>Starting with the mapping file for the MCA dataset, edit the default template as follows, remembering to insert your project name in the source line. This file is also available in the repository as <em>Chapter9SchemaMari</em>:</p>&#13;
&#13;
<pre data-type="programlisting">&#13;
prefixes:&#13;
  ekg: http://cloud.google.com/ekg/0.0.1#&#13;
  schema: https://schema.org/&#13;
&#13;
mappings:&#13;
  organization:&#13;
    sources:&#13;
      - [&lt;<em>your_project_name</em>&gt;:Chapter9.mari~bigquery]&#13;
    s: ekg:company1_$(unique_id)&#13;
    po:&#13;
      - [a, schema:Organization]&#13;
      - [schema:postalCode, $(Postcode)]&#13;
      - [schema:name, $(CompanyName)]&#13;
      - [ekg:recon.source_name, (mari)]&#13;
      - [ekg:recon.source_key, $(unique_id)]</pre>&#13;
&#13;
<p>Note here that we are pointing the API to the <em>mari</em> BigQuery table we created earlier in the Chapter9 dataset. We are using the <code>unique_id</code> column as our subject key, and we are mapping our <code>Postcode</code> field to the <code>postalCode</code> property in the schema and our <code>CompanyName</code> field to the <code>name</code> property.</p>&#13;
&#13;
<p>Save this edited file into your Google Storage bucket under the <em>handsonentityresolution</em> directory as:</p>&#13;
&#13;
<pre data-type="programlisting">&#13;
gs://&lt;<em>your bucket</em>&gt;/handsonentityresolution/Chapter9SchemaMari</pre>&#13;
&#13;
<p>Repeat this process to create a mapping file for the Companies House dataset, saving in the same location as <em>Chapter9SchemaBasic</em>. Remember to substitute <em>basic</em> for <em>mari</em> in the relevant lines and reference these entities as <em>company2</em>:</p>&#13;
&#13;
<pre data-type="programlisting">&#13;
    - [&lt;your_bucket&gt;:Chapter9.<strong>basic</strong>~bigquery]&#13;
   s: ekg:<strong>company2</strong>_$(unique_id)&#13;
   po:&#13;
       - [a, schema:Organization]&#13;
       - [ekg:recon.source_name, (<strong>basic</strong>)]</pre>&#13;
&#13;
<p>We now have our datasets and our mapping files, so we can run an entity resolution (or reconciliation) job.<a contenteditable="false" data-primary="" data-startref="GEKschema09" data-type="indexterm" id="id585"/><a contenteditable="false" data-primary="" data-startref="schmap09" data-type="indexterm" id="id586"/></p>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Reconciliation Job" data-type="sect2"><div class="sect2" id="reconciliation-jobto">&#13;
<h2>Reconciliation Job</h2>&#13;
&#13;
<p>To<a contenteditable="false" data-primary="Google Enterprise Knowledge Graph API" data-secondary="reconciliation jobs" data-type="indexterm" id="GEKGrecjob09"/><a contenteditable="false" data-primary="reconciliation jobs" data-type="indexterm" id="reconciljobs09"/><a contenteditable="false" data-primary="jobs, running reconciliations" data-type="indexterm" id="jobsrecon09"/> start a reconciliation job, select Jobs from the Enterprise KG section in the console navigation menu, as shown in <a data-type="xref" href="#fig-9-5">Figure 9-5</a>.</p>&#13;
&#13;
<figure><div class="figure" id="fig-9-5"><img alt="" class="iimagesch09ch09selectjobspng" src="assets/hoer_0905.png"/>&#13;
<h6><span class="label">Figure 9-5. </span>Start a reconciliation job</h6>&#13;
</div></figure>&#13;
&#13;
<p class="pagebreak-before">Select the RUN A JOB tab, as shown in <a data-type="xref" href="#fig-9-6">Figure 9-6</a>.</p>&#13;
&#13;
<figure><div class="figure" id="fig-9-6"><img alt="" class="iimagesch09apireconciliationpng" src="assets/hoer_0906.png"/>&#13;
<h6><span class="label">Figure 9-6. </span>Run an API Job for Entity Reconciliation</h6>&#13;
</div></figure>&#13;
&#13;
<p>From the pop-up menu:</p>&#13;
&#13;
<dl>&#13;
	<dt>Step 1: Click “Select entity type”</dt>&#13;
	<dd>&#13;
	<p>Select Organization.</p>&#13;
	</dd>&#13;
	<dt>Step 2: Add BigQuery data sources</dt>&#13;
	<dd>&#13;
	<p>Browse to the BigQuery path and select the <em>mari</em> table. Then select the matching mapping table by browsing to the <em>handsonentityresolution</em> directory in your bucket and selecting the <em>Chapter9SchemaMari</em> file we created earlier.</p>&#13;
&#13;
	<p>Click Add Another BigQuery Datasource and repeat the process for the <em>basic</em> table and mapping file.</p>&#13;
	</dd>&#13;
	<dt>Step 3: Set BigQuery data destination</dt>&#13;
	<dd>&#13;
	<p>Browse and select the Chapter9 BigQuery dataset to tell the API where to write its results.</p>&#13;
	</dd>&#13;
	<dt>Step 4: Advanced settings (optional)</dt>&#13;
	<dd>&#13;
	<p>For the final step, we can specify a previous result table so that the entity reconciliation service assigns consistent IDs to entities across different jobs, as shown in <a data-type="xref" href="#fig-9-7">Figure 9-7</a>. This can be particularly useful to update existing entity records as new data is added.</p>&#13;
	</dd>&#13;
</dl>&#13;
&#13;
<figure><div class="figure" id="fig-9-7"><img alt="" class="iimagesch09advancedpng" src="assets/hoer_0907.png"/>&#13;
<h6><span class="label">Figure 9-7. </span>Entity Reconciliation API advanced settings</h6>&#13;
</div></figure>&#13;
&#13;
<p>The number of clustering rounds (iterations of the entity resolution model) can be specified; the higher the number, the more loosely entities are merged into the same cluster. The default is fine for our use case.</p>&#13;
&#13;
<p>Finally, we can click Done and start our job. Assuming all is well, we should then see a new job created under Job History, as shown in <a data-type="xref" href="#fig-9-8">Figure 9-8</a>.</p>&#13;
&#13;
<figure><div class="figure" id="fig-9-8"><img alt="" class="iimagesch09jobhistorystatuspng" src="assets/hoer_0908.png"/>&#13;
<h6><span class="label">Figure 9-8. </span>Entity Reconciliation Job History</h6>&#13;
</div></figure>&#13;
&#13;
<p>We can watch the Job Display Status column to monitor the progress of our job as it moves sequentially through the display states shown in <a data-type="xref" href="#table-9-1">Table 9-1</a>, and then finally displays Finished when complete.</p>&#13;
&#13;
<table id="table-9-1">&#13;
	<caption><span class="label">Table 9-1. </span>Job display state</caption>&#13;
	<thead>&#13;
		<tr>&#13;
			<th>Job display state</th>&#13;
			<th>Code state</th>&#13;
			<th>Description</th>&#13;
		</tr>&#13;
	</thead>&#13;
	<tbody>&#13;
		<tr>&#13;
			<td>Running</td>&#13;
			<td><code>JOB_<wbr/>STATE_<wbr/>RUNNING</code></td>&#13;
			<td>The job is in progress.</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td>Knowledge extraction</td>&#13;
			<td><code>JOB_<wbr/>STATE_<wbr/>KNOWLEDGE_<wbr/>EXTRACTION</code></td>&#13;
			<td>Enterprise Knowledge Graph is pulling data out from BigQuery and creating features.</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td>Reconciliation preprocessing</td>&#13;
			<td><code>JOB_<wbr/>STATE_<wbr/>RECON_<wbr/>​PRE⁠PROCESSING</code></td>&#13;
			<td>The job is at the reconciliation preprocessing step.</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td>Clustering</td>&#13;
			<td><code>JOB_<wbr/>STATE_<wbr/>CLUSTERING</code></td>&#13;
			<td>The job is at the clustering step.</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td>Exporting clusters</td>&#13;
			<td><code>JOB_<wbr/>STATE_<wbr/>EXPORTING_<wbr/>CLUSTERS</code></td>&#13;
			<td>The job is writing output into the BigQuery destination dataset.</td>&#13;
		</tr>&#13;
	</tbody>&#13;
</table>&#13;
&#13;
<p>This job should take approximately 1 hour 20 minutes but the duration varies widely at this Preview stage of the product.</p>&#13;
&#13;
<p>When the job is finished, if we look in the BigQuery SQL workspace we should see a new table in our Chapter9 dataset called something like clusters_15719257497877843494, as shown in <a data-type="xref" href="#fig-9-9">Figure 9-9</a>.</p>&#13;
&#13;
<figure><div class="figure" id="fig-9-9"><img class="iimagesch09ch09clustertablepng" src="assets/hoer_0909.png"/>&#13;
<h6><span class="label">Figure 9-9. </span>BigQuery clusters results table</h6>&#13;
</div></figure>&#13;
&#13;
<p>Selecting the clusters_15719257497877843494 table and then selecting the Preview tab gives us a view of the results. <a data-type="xref" href="#fig-9-10">Figure 9-10</a> shows the first few rows.</p>&#13;
&#13;
<figure><div class="figure" id="fig-9-10"><img alt="" class="iimagesch09ch09clusterresultsheadpng" src="assets/hoer_0910.png"/>&#13;
<h6><span class="label">Figure 9-10. </span>BigQuery cluster results preview</h6>&#13;
</div></figure>&#13;
&#13;
<p>Let’s consider the columns in the output:</p>&#13;
&#13;
<ul>&#13;
	<li>The <em>cluster_id</em> gives the unique reference of the cluster to which the Entity Reconciliation API has assigned the source entity.</li>&#13;
	<li>The <em>source_name</em> column gives us the name of the source table, in our case either <em>mari</em> or <em>basic</em>.</li>&#13;
	<li>The <em>source_key</em> column contains the <code>unique_id</code> of the row in the source table. </li>&#13;
	<li>The <em>confidence</em> score, between 0 and 1, indicates how strongly a record is associated with a given cluster.</li>&#13;
	<li>The <em>assignment_age</em> column is an internal API reference.</li>&#13;
	<li>The <em>cloud_kg_mid</em> column contains an MID value link to the entity in the Google Cloud Enterprise Knowledge Graph if the API can resolve a match. This can be used to look up additional details that Google has on the entity using the Cloud Enterprise Knowledge Graph API.</li>&#13;
</ul>&#13;
&#13;
<p>As every entity in both the <em>mari</em> and <em>basic</em> tables is assigned to a cluster, the row count for this table is the sum of the row counts for the source tables. In our case, this is over 5 million rows. At a glance, it’s not easy to identify which entities the API has matched, so we need to refine this data a little.<a contenteditable="false" data-primary="" data-startref="GEKGrecjob09" data-type="indexterm" id="id587"/><a contenteditable="false" data-primary="" data-startref="reconciljobs09" data-type="indexterm" id="id588"/><a contenteditable="false" data-primary="" data-startref="jobsrecon09" data-type="indexterm" id="id589"/></p>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Result Processing" data-type="sect2"><div class="sect2" id="result-processingnow">&#13;
<h2>Result Processing</h2>&#13;
&#13;
<p>With<a contenteditable="false" data-primary="Google Enterprise Knowledge Graph API" data-secondary="result processing" data-type="indexterm" id="GEKGresults09"/><a contenteditable="false" data-primary="result processing (BigQuery SQL)" data-type="indexterm" id="reslproc09"/><a contenteditable="false" data-primary="BigQuery" data-type="indexterm" id="bigquery09"/> our entity reconciliation results we can then use BigQuery SQL to process this raw information into an easier form for us to examine the resolved entities.</p>&#13;
&#13;
<p>To start, we click “Compose a New Query”, which takes us to a SQL editor. You can cut and paste the SQL template from the <em>Chapter9.sql</em> file.</p>&#13;
&#13;
<p>First we need to create a temporary table containing only rows whose <code>cluster_id</code> has at least one MCA match. We do this by building a subset of the cluster table whose rows have “mari” as the <code>source_name</code>. Then we find the intersection between the rows of this subset and the rows of the full cluster table using an <code>INNER JOIN</code> on matching <code>cluster_id</code>s.</p>&#13;
&#13;
<p>Make sure to replace the cluster table name with the name of your results table, which will be in the format <code>clusters_<em>&lt;job reference&gt;</em></code>:</p>&#13;
&#13;
<pre data-type="programlisting">&#13;
CREATE TEMP TABLE temp AS SELECT&#13;
   src.* FROM Chapter9.clusters_15719257497877843494 AS src&#13;
      INNER JOIN (SELECT cluster_id from&#13;
         Chapter9.clusters_15719257497877843494 WHERE &#13;
             source_name = "mari") AS mari&#13;
         ON src.cluster_id = mari.cluster_id;</pre>&#13;
&#13;
<p>The resulting temporary table now has only 151 rows. Next we create a second temporary table, this time with the subset of clusters that have both an MCA match and at least one Companies House match; i.e., we remove clusters with only an MCA match.</p>&#13;
&#13;
<p>To do this we select those <code>cluster_id</code>s with a count of greater than 1 and again find the intersection of this subset with the first temporary table using an <code>INNER JOIN</code> on the matching <code>cluster_id</code>s.</p>&#13;
&#13;
<p>Now we have a table of clusters containing only rows where the entity is found in both the Companies House and MCA datasets:</p>&#13;
&#13;
<pre data-type="programlisting">&#13;
CREATE TEMP TABLE match AS SELECT&#13;
   src.* FROM temp AS src&#13;
      INNER JOIN (SELECT cluster_id FROM temp GROUP BY cluster_id &#13;
          HAVING COUNT(*) &gt; 1) AS matches&#13;
      ON matches.cluster_id = src.cluster_id;</pre>&#13;
&#13;
<p>This table now has 106 rows. We have the population we are looking for, so we can create a persistent results table picking up the <code>CompanyName</code> and <code>Postcode</code> from the source tables so that we can examine the results.</p>&#13;
&#13;
<p>We need to build this table in two parts. First, for the rows that refer to the Companies House data we need to look up the identifier in the <code>source_key</code> column and use that to retrieve the corresponding name and postcode. Then we need to do the same for rows that refer to the MCA data. We use the <code>UNION ALL</code> statement to join these two datasets and then <code>ORDER BY</code> <code>confidence</code> first and then <code>cluster_id</code>. This means that entities assigned to the same cluster are adjacent in the table for easy viewing:</p>&#13;
&#13;
<pre data-type="programlisting">&#13;
CREATE TABLE Chapter9.results AS&#13;
&#13;
   SELECT * FROM Chapter9.basic AS bas&#13;
   INNER JOIN (SELECT * FROM match WHERE match.source_name = "basic") AS res1&#13;
   ON res1.source_key = CAST(bas.unique_id AS STRING)&#13;
&#13;
   UNION ALL&#13;
&#13;
   SELECT * FROM Chapter9.mari AS mari&#13;
      INNER JOIN (SELECT * FROM match WHERE match.source_name = "mari") AS res2&#13;
      ON res2.source_key = CAST(mari.unique_id AS STRING)&#13;
&#13;
ORDER BY confidence, cluster_id</pre>&#13;
&#13;
<p>This gives us our results table, which looks like that given in <a data-type="xref" href="#fig-9-11">Figure 9-11</a>.</p>&#13;
&#13;
<figure><div class="figure" id="fig-9-11"><img class="iimagesch09ch09resultspng" src="assets/hoer_0911.png"/>&#13;
<h6><span class="label">Figure 9-11. </span>Processed results table</h6>&#13;
</div></figure>&#13;
&#13;
<p>In the first row, we can see that both the MCA entity with <code>CompanyName</code> CREW AND CONCIERGE, <code>Postcode</code> BS31 1TP, and <code>unique_id</code> 18 has been assigned to cluster r-03fxqun0t2rjxn. In the second row, the Companies House entity with <code>CompanyName</code> CREW and CONCIERGE, the same <code>Postcode</code>, and <code>unique_id</code> 1182534 has been assigned to the same cluster.</p>&#13;
&#13;
<p>This means the Google Entity Reconciliation API has grouped these records into the same cluster, i.e., resolved these rows as referring to the same real-world entity, with a confidence rating of 0.7.</p>&#13;
&#13;
<p>Before we examine these results in detail, we’ll take a quick detour to see how to invoke the API from Python instead of the cloud console.<a contenteditable="false" data-primary="" data-startref="GEKGresults09" data-type="indexterm" id="id590"/><a contenteditable="false" data-primary="" data-startref="reslproc09" data-type="indexterm" id="id591"/><a contenteditable="false" data-primary="" data-startref="bigquery09" data-type="indexterm" id="id592"/></p>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Entity Reconciliation Python Client" data-type="sect2"><div class="sect2" id="python-clientthe-goog">&#13;
<h2>Entity Reconciliation Python Client</h2>&#13;
&#13;
<p>The<a contenteditable="false" data-primary="Google Enterprise Knowledge Graph API" data-secondary="versus Python client" data-secondary-sortas="Python client" data-type="indexterm" id="GEKGpython09"/><a contenteditable="false" data-primary="Python" data-secondary="client to handle reconciliation jobs" data-type="indexterm" id="Pclient09"/> Google Enterprise Knowledge Graph API also supports a Python client to create, cancel, and delete entity reconciliation jobs. We can use the Cloud Shell virtual machine to run these Python scripts and launch these jobs.</p>&#13;
&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id593">&#13;
<h1>Google Cloud Shell</h1>&#13;
&#13;
<p>Cloud Shell<a contenteditable="false" data-primary=" Google Cloud Shell" data-type="indexterm" id="id594"/> provisions a Compute Engine virtual machine running a Debian-based Linux OS for temporary use. The instance persists while your Cloud Shell session is active; after an hour of inactivity, your session terminates and its virtual machine is discarded. Cloud Shell provisions 5 GB of free persistent disk storage.</p>&#13;
</div></aside>&#13;
&#13;
<p>To activate Google Cloud Shell, click on the terminal symbol in the top right of the console. This will open a window with a command-line prompt.</p>&#13;
&#13;
<p>A Python script to invoke the entity reconciliation job is included in the repository. To transfer a copy to your Cloud Shell machine we can use:</p>&#13;
&#13;
<pre data-type="programlisting">&#13;
&gt;&gt;&gt;<strong>gsutil cp gs://&lt;your_bucket&gt;/handsonentityresolution/&#13;
   Chapter9.py .</strong></pre>&#13;
&#13;
<p>A pop-up window will ask you to authorize the Cloud Shell to connect to your bucket.</p>&#13;
&#13;
<p>The script, <em>Chapter9.py</em>, is reproduced here. You can use the Cloud Shell editor to edit this file to reference your project and bucket:</p>&#13;
&#13;
<pre data-type="programlisting">&#13;
#!/usr/bin/env python&#13;
# coding: utf-8&#13;
&#13;
from google.cloud import enterpriseknowledgegraph as ekg&#13;
&#13;
project_id = '<strong>&lt;your_project&gt;</strong>'&#13;
dataset_id = 'Chapter9'&#13;
&#13;
import google.cloud.enterpriseknowledgegraph as ekg&#13;
&#13;
client = ekg.EnterpriseKnowledgeGraphServiceClient()&#13;
parent = client.common_location_path(project=project_id, location='global')&#13;
&#13;
input_config = ekg.InputConfig(&#13;
        bigquery_input_configs=[&#13;
            ekg.BigQueryInputConfig(&#13;
                bigquery_table=client.table_path(&#13;
                    project=project_id, dataset=dataset_id, table='mari'&#13;
                ),&#13;
                gcs_uri='gs://<strong>&lt;your bucket&gt;</strong>/&#13;
                  handsonentityresolution/Chapter9SchemaMari',&#13;
            ),&#13;
             ekg.BigQueryInputConfig(&#13;
                bigquery_table=client.table_path(&#13;
                    project=project_id, dataset=dataset_id, table='basic'&#13;
                ),&#13;
                gcs_uri='gs://<strong>&lt;your bucket&gt;</strong>/&#13;
                  handsonentityresolution/Chapter9SchemaBasic',&#13;
            )   &#13;
        ],&#13;
        entity_type=ekg.InputConfig.EntityType.ORGANIZATION,&#13;
    )&#13;
&#13;
output_config = ekg.OutputConfig(&#13;
        bigquery_dataset=client.dataset_path(project=project_id, &#13;
            dataset=dataset_id)&#13;
    )&#13;
&#13;
entity_reconciliation_job = ekg.EntityReconciliationJob(&#13;
        input_config=input_config, output_config=output_config&#13;
)&#13;
&#13;
request = ekg.CreateEntityReconciliationJobRequest(&#13;
        parent=parent, entity_reconciliation_job=entity_reconciliation_job&#13;
)&#13;
&#13;
response = client.create_entity_reconciliation_job(request=request)&#13;
&#13;
print(f"Job: {response.name}")</pre>&#13;
&#13;
<p>The Cloud Shell has Python installed so we can simply run this script from the command prompt with the following:</p>&#13;
&#13;
<pre data-type="programlisting">&#13;
<strong>&gt;&gt;&gt;python Chapter9.py</strong>&#13;
</pre>&#13;
&#13;
<p>To process the results, we can use the SQL script we examined previously. To copy this from your Cloud Storage bucket:</p>&#13;
&#13;
<pre data-type="programlisting">&#13;
&gt;&gt;&gt;<strong>gsutil cp gs://&lt;your_bucket&gt;/handsonentityresolution/&#13;
    Chapter9.sql</strong></pre>&#13;
&#13;
<p>Then we run this BigQuery script using:</p>&#13;
&#13;
<pre data-type="programlisting">&#13;
<strong>&gt;&gt;&gt;bq query --use_legacy_sql=false &lt; Chapter9.sql</strong></pre>&#13;
&#13;
<p>Note that if the results table has already been created by running this query from the SQL workspace, this command will fail because the table already exists. You can delete the table using:</p>&#13;
&#13;
<pre data-type="programlisting">&#13;
<strong>&gt;&gt;&gt;bq rm -f -t Chapter9.results</strong>&#13;
</pre>&#13;
&#13;
<p>Now we can examine how well the API performed on our example.<a contenteditable="false" data-primary="" data-startref="CERSgentknow09" data-type="indexterm" id="id595"/><a contenteditable="false" data-primary="" data-startref="GEKGpython09" data-type="indexterm" id="id596"/><a contenteditable="false" data-primary="" data-startref="Pclient09" data-type="indexterm" id="id597"/><a contenteditable="false" data-primary="" data-startref="graphvis09" data-type="indexterm" id="id598"/></p>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Measuring Performance" data-type="sect1"><div class="sect1" id="id134">&#13;
<h1>Measuring Performance</h1>&#13;
&#13;
<p>Recall<a contenteditable="false" data-primary="Google Enterprise Knowledge Graph API" data-secondary="measuring performance" data-type="indexterm" id="id599"/><a contenteditable="false" data-primary="performance, measuring" data-secondary="cloud entity resolution services" data-type="indexterm" id="id600"/> from our Preview of the BigQuery results table that we have 106 rows. The distribution of match confidence is shown in <a data-type="xref" href="#table-9-2">Table 9-2</a>.</p>&#13;
&#13;
<table id="table-9-2">&#13;
	<caption><span class="label">Table 9-2. </span>Match confidence</caption>&#13;
	<thead>&#13;
		<tr>&#13;
			<th scope="col">Number of matches</th>&#13;
			<th scope="col">Confidence</th>&#13;
		</tr>&#13;
	</thead>&#13;
	<tbody>&#13;
		<tr>&#13;
			<td>6</td>&#13;
			<td>0.7</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td>1</td>&#13;
			<td>0.8</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td>45</td>&#13;
			<td>0.99</td>&#13;
		</tr>&#13;
		<tr>&#13;
			<td>44</td>&#13;
			<td>No match found</td>&#13;
		</tr>&#13;
	</tbody>&#13;
</table>&#13;
&#13;
<p>Two of the MCA entities matched to two of the Companies House entities.</p>&#13;
&#13;
<p>Looking back to <a data-type="xref" href="#fig-9-11">Figure 9-11</a>, we can see the first seven matches in ascending order of confidence. You can see that the entity reconciliation service has been able to match these entities in spite of minor spelling differences or postcode variations. The remainder are exact matches on both <code>CompanyName</code> and <code>Postcode</code> with the exception of a mismatched hyphen between INDIE PEARL and INDIE-PEARL, which has not affected the confidence score.</p>&#13;
&#13;
<p>If we assume that the unique matches are true positive matches and that the two additional matches are false positives, then we can evaluate our performance as:</p>&#13;
&#13;
<div data-type="equation">&#13;
<p><math alttext="upper T r u e p o s i t i v e m a t c h e s left-parenthesis upper T upper P right-parenthesis equals 52">&#13;
  <mrow>&#13;
    <mi>T</mi>&#13;
    <mi>r</mi>&#13;
    <mi>u</mi>&#13;
    <mi>e</mi>&#13;
    <mspace width="0.166667em"/>&#13;
    <mi>p</mi>&#13;
    <mi>o</mi>&#13;
    <mi>s</mi>&#13;
    <mi>i</mi>&#13;
    <mi>t</mi>&#13;
    <mi>i</mi>&#13;
    <mi>v</mi>&#13;
    <mi>e</mi>&#13;
    <mspace width="0.166667em"/>&#13;
    <mi>m</mi>&#13;
    <mi>a</mi>&#13;
    <mi>t</mi>&#13;
    <mi>c</mi>&#13;
    <mi>h</mi>&#13;
    <mi>e</mi>&#13;
    <mi>s</mi>&#13;
    <mspace width="0.166667em"/>&#13;
    <mo>(</mo>&#13;
    <mi>T</mi>&#13;
    <mi>P</mi>&#13;
    <mo>)</mo>&#13;
    <mo>=</mo>&#13;
    <mn>52</mn>&#13;
  </mrow>&#13;
</math></p>&#13;
&#13;
<p><math alttext="upper F a l s e p o s i t i v e m a t c h e s left-parenthesis upper F upper P right-parenthesis equals 2">&#13;
  <mrow>&#13;
    <mi>F</mi>&#13;
    <mi>a</mi>&#13;
    <mi>l</mi>&#13;
    <mi>s</mi>&#13;
    <mi>e</mi>&#13;
    <mspace width="0.166667em"/>&#13;
    <mi>p</mi>&#13;
    <mi>o</mi>&#13;
    <mi>s</mi>&#13;
    <mi>i</mi>&#13;
    <mi>t</mi>&#13;
    <mi>i</mi>&#13;
    <mi>v</mi>&#13;
    <mi>e</mi>&#13;
    <mspace width="0.166667em"/>&#13;
    <mi>m</mi>&#13;
    <mi>a</mi>&#13;
    <mi>t</mi>&#13;
    <mi>c</mi>&#13;
    <mi>h</mi>&#13;
    <mi>e</mi>&#13;
    <mi>s</mi>&#13;
    <mspace width="0.166667em"/>&#13;
    <mo>(</mo>&#13;
    <mi>F</mi>&#13;
    <mi>P</mi>&#13;
    <mo>)</mo>&#13;
    <mo>=</mo>&#13;
    <mn>2</mn>&#13;
  </mrow>&#13;
</math></p>&#13;
&#13;
<p><math alttext="upper F a l s e n e g a t i v e m a t c h e s left-parenthesis upper F upper N right-parenthesis equals 44">&#13;
  <mrow>&#13;
    <mi>F</mi>&#13;
    <mi>a</mi>&#13;
    <mi>l</mi>&#13;
    <mi>s</mi>&#13;
    <mi>e</mi>&#13;
    <mspace width="0.166667em"/>&#13;
    <mi>n</mi>&#13;
    <mi>e</mi>&#13;
    <mi>g</mi>&#13;
    <mi>a</mi>&#13;
    <mi>t</mi>&#13;
    <mi>i</mi>&#13;
    <mi>v</mi>&#13;
    <mi>e</mi>&#13;
    <mspace width="0.166667em"/>&#13;
    <mi>m</mi>&#13;
    <mi>a</mi>&#13;
    <mi>t</mi>&#13;
    <mi>c</mi>&#13;
    <mi>h</mi>&#13;
    <mi>e</mi>&#13;
    <mi>s</mi>&#13;
    <mspace width="0.166667em"/>&#13;
    <mo>(</mo>&#13;
    <mi>F</mi>&#13;
    <mi>N</mi>&#13;
    <mo>)</mo>&#13;
    <mo>=</mo>&#13;
    <mn>44</mn>&#13;
  </mrow>&#13;
</math></p>&#13;
&#13;
<p><math alttext="upper P r e c i s i o n equals StartFraction upper T upper P Over left-parenthesis upper T upper P plus upper F upper P right-parenthesis EndFraction equals StartFraction 52 Over left-parenthesis 52 plus 2 right-parenthesis EndFraction almost-equals 96 percent-sign">&#13;
  <mrow>&#13;
    <mi>P</mi>&#13;
    <mi>r</mi>&#13;
    <mi>e</mi>&#13;
    <mi>c</mi>&#13;
    <mi>i</mi>&#13;
    <mi>s</mi>&#13;
    <mi>i</mi>&#13;
    <mi>o</mi>&#13;
    <mi>n</mi>&#13;
    <mo>=</mo>&#13;
    <mfrac><mrow><mi>T</mi><mi>P</mi></mrow> <mrow><mo>(</mo><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>P</mi><mo>)</mo></mrow></mfrac>&#13;
    <mo>=</mo>&#13;
    <mfrac><mn>52</mn> <mrow><mo>(</mo><mn>52</mn><mo>+</mo><mn>2</mn><mo>)</mo></mrow></mfrac>&#13;
    <mo>≈</mo>&#13;
    <mn>96</mn>&#13;
    <mo>%</mo>&#13;
  </mrow>&#13;
</math></p>&#13;
&#13;
<p><math alttext="upper R e c a l l equals StartFraction upper T upper P Over left-parenthesis upper T upper P plus upper F upper N right-parenthesis EndFraction equals StartFraction 52 Over left-parenthesis 52 plus 44 right-parenthesis EndFraction almost-equals 54.2 percent-sign">&#13;
  <mrow>&#13;
    <mi>R</mi>&#13;
    <mi>e</mi>&#13;
    <mi>c</mi>&#13;
    <mi>a</mi>&#13;
    <mi>l</mi>&#13;
    <mi>l</mi>&#13;
    <mo>=</mo>&#13;
    <mfrac><mrow><mi>T</mi><mi>P</mi></mrow> <mrow><mo>(</mo><mi>T</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>N</mi><mo>)</mo></mrow></mfrac>&#13;
    <mo>=</mo>&#13;
    <mfrac><mn>52</mn> <mrow><mo>(</mo><mn>52</mn><mo>+</mo><mn>44</mn><mo>)</mo></mrow></mfrac>&#13;
    <mo>≈</mo>&#13;
    <mn>54</mn>&#13;
    <mo>.</mo>&#13;
    <mn>2</mn>&#13;
    <mo>%</mo>&#13;
  </mrow>&#13;
</math></p>&#13;
</div>&#13;
&#13;
<p>So the entity reconciliation gives us excellent precision but with relatively poor recall.</p>&#13;
</div></section>&#13;
&#13;
<section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="id257">&#13;
<h1>Summary</h1>&#13;
&#13;
<p>In this chapter we have seen how to use the Google Cloud Entity Reconciliation API to resolve our organization entities. We have seen how to configure and run matching jobs from both the cloud console and via the Python client.</p>&#13;
&#13;
<p>Using the API abstracts us away from much of the complexity of configuring our own matching process. It is also inherently scalable to very large datasets (hundreds of millions of rows). However, we are constrained to using a set of predefined schemas and we don’t have the freedom to tune the matching algorithm to optimize the recall/precision trade-off for our use case.</p>&#13;
</div></section>&#13;
</div></section></body></html>