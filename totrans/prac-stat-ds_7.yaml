- en: Chapter 7\. Unsupervised Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第7章 无监督学习
- en: The term *unsupervised learning* refers to statistical methods that extract
    meaning from data without training a model on labeled data (data where an outcome
    of interest is known). In Chapters [4](ch04.xhtml#Regression) to [6](ch06.xhtml#StatisticalML),
    the goal is to build a model (set of rules) to predict a response variable from
    a set of predictor variables. This is supervised learning. In contrast, unsupervised
    learning also constructs a model of the data, but it does not distinguish between
    a response variable and predictor variables.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '*无监督学习*一词指的是从数据中提取意义而不是在标记数据（已知感兴趣结果的数据）上训练模型的统计方法。在第[4](ch04.xhtml#Regression)到[6](ch06.xhtml#StatisticalML)章中，目标是构建一个模型（一组规则）来从一组预测变量中预测响应变量。这是监督学习。相比之下，无监督学习也构建数据模型，但不区分响应变量和预测变量。'
- en: Unsupervised learning can be used to achieve different goals. In some cases,
    it can be used to create a predictive rule in the absence of a labeled response.
    *Clustering* methods can be used to identify meaningful groups of data. For example,
    using the web clicks and demographic data of a user on a website, we may be able
    to group together different types of users. The website could then be personalized
    to these different types.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习可以用于实现不同的目标。在某些情况下，它可以在没有标记响应的情况下创建预测规则。*聚类*方法可用于识别有意义的数据组。例如，利用用户在网站上的点击和人口统计数据，我们可能能够将不同类型的用户分组。然后网站可以根据这些不同类型进行个性化设置。
- en: In other cases, the goal may be to *reduce the dimension* of the data to a more
    manageable set of variables. This reduced set could then be used as input into
    a predictive model, such as regression or classification. For example, we may
    have thousands of sensors to monitor an industrial process. By reducing the data
    to a smaller set of features, we may be able to build a more powerful and interpretable
    model to predict process failure than could be built by including data streams
    from thousands of sensors.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在其他情况下，目标可能是将数据的维度*减少*到一组更易管理的变量。然后可以将这个减少的集合用作预测模型的输入，例如回归或分类模型。例如，我们可能有数千个传感器来监控一个工业过程。通过将数据减少到更小的一组特征，我们可能能够构建比包含来自数千个传感器的数据流更强大且可解释的模型，以预测过程失败。
- en: Finally, unsupervised learning can be viewed as an extension of the exploratory
    data analysis (see [Chapter 1](ch01.xhtml#EDA)) to situations in which you are
    confronted with a large number of variables and records. The aim is to gain insight
    into a set of data and how the different variables relate to each other. Unsupervised
    techniques allow you to sift through and analyze these variables and discover
    relationships.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，无监督学习可以被看作是探索性数据分析（参见[第1章](ch01.xhtml#EDA)）的延伸，用于处理大量变量和记录的情况。其目的是深入了解数据集以及不同变量之间的关系。无监督技术允许您筛选和分析这些变量，并发现它们之间的关系。
- en: Unsupervised Learning and Prediction
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无监督学习与预测
- en: Unsupervised learning can play an important role in prediction, both for regression
    and classification problems. In some cases, we want to predict a category in the
    absence of any labeled data. For example, we might want to predict the type of
    vegetation in an area from a set of satellite sensory data. Since we don’t have
    a response variable to train a model, clustering gives us a way to identify common
    patterns and categorize the regions.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习在预测中可以发挥重要作用，无论是回归问题还是分类问题。在某些情况下，我们希望在没有标记数据的情况下预测一个类别。例如，我们可能希望根据一组卫星传感器数据预测一个区域的植被类型。由于没有响应变量来训练模型，聚类为我们提供了一种识别共同模式和对区域进行分类的方法。
- en: Clustering is an especially important tool for the “cold-start problem.” In
    this type of problem, such as launching a new marketing campaign or identifying
    potential new types of fraud or spam, we initially may not have any response to
    train a model. Over time, as data is collected, we can learn more about the system
    and build a traditional predictive model. But clustering helps us start the learning
    process more quickly by identifying population segments.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类对于“冷启动问题”尤为重要。在这种类型的问题中，比如推出新的营销活动或识别潜在的新型欺诈或垃圾邮件，我们最初可能没有任何响应来训练模型。随着时间的推移，随着数据的收集，我们可以更多地了解系统并构建传统的预测模型。但是聚类通过识别人群段落帮助我们更快地启动学习过程。
- en: Unsupervised learning is also important as a building block for regression and
    classification techniques. With big data, if a small subpopulation is not well
    represented in the overall population, the trained model may not perform well
    for that subpopulation. With clustering, it is possible to identify and label
    subpopulations. Separate models can then be fit to the different subpopulations.
    Alternatively, the subpopulation can be represented with its own feature, forcing
    the overall model to explicitly consider subpopulation identity as a predictor.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习对于回归和分类技术也很重要。在大数据中，如果小的子群体在整体群体中代表不足，那么训练模型可能在该子群体上表现不佳。通过聚类，可以识别和标记子群体。然后可以为不同的子群体拟合单独的模型。或者，可以用自己的特征表示子群体，迫使整体模型明确考虑子群体身份作为预测因子。
- en: Principal Components Analysis
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 主成分分析
- en: Often, variables will vary together (covary), and some of the variation in one
    is actually duplicated by variation in another (e.g., restaurant checks and tips).
    Principal components analysis (PCA) is a technique to discover the way in which
    numeric variables covary.^([1](ch07.xhtml#idm46522838490648))
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，变量会一起变化（共变），某些变量的变化实际上是由另一变量的变化重复的（例如，餐厅账单和小费）。主成分分析（PCA）是一种发现数值变量共变方式的技术。^([1](ch07.xhtml#idm46522838490648))
- en: The idea in PCA is to combine multiple numeric predictor variables into a smaller
    set of variables, which are weighted linear combinations of the original set.
    The smaller set of variables, the *principal components*, “explains” most of the
    variability of the full set of variables, reducing the dimension of the data.
    The weights used to form the principal components reveal the relative contributions
    of the original variables to the new principal components.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: PCA 的理念是将多个数值预测变量组合成较小的一组变量，这些变量是原始集合的加权线性组合。这组较小的变量，即*主成分*，“解释”了整个变量集合的大部分变异性，从而降低了数据的维度。用于形成主成分的权重显示了原始变量对新主成分的相对贡献。
- en: PCA was first [proposed by Karl Pearson](https://oreil.ly/o4EeC). In what was
    perhaps the first paper on unsupervised learning, Pearson recognized that in many
    problems there is variability in the predictor variables, so he developed PCA
    as a technique to model this variability. PCA can be viewed as the unsupervised
    version of linear discriminant analysis; see[“Discriminant Analysis”](ch05.xhtml#DiscriminantAnalysis).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: PCA 最初由 [卡尔·皮尔逊提出](https://oreil.ly/o4EeC)。在或许是第一篇无监督学习论文中，皮尔逊认识到在许多问题中，预测变量存在变异性，因此他开发了PCA作为一种模拟这种变异性的技术。PCA
    可以看作是线性判别分析的无监督版本；参见 [“判别分析”](ch05.xhtml#DiscriminantAnalysis)。
- en: A Simple Example
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 简单示例
- en: 'For two variables, <math alttext="upper X 1"><msub><mi>X</mi> <mn>1</mn></msub></math>
    and <math alttext="upper X 2"><msub><mi>X</mi> <mn>2</mn></msub></math> , there
    are two principal components <math alttext="upper Z Subscript i"><msub><mi>Z</mi>
    <mi>i</mi></msub></math> ( <math alttext="i equals 1"><mrow><mi>i</mi> <mo>=</mo>
    <mn>1</mn></mrow></math> or 2):'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 对于两个变量，<math alttext="upper X 1"><msub><mi>X</mi> <mn>1</mn></msub></math> 和
    <math alttext="upper X 2"><msub><mi>X</mi> <mn>2</mn></msub></math> ，有两个主成分 <math
    alttext="upper Z Subscript i"><msub><mi>Z</mi> <mi>i</mi></msub></math>（ <math
    alttext="i equals 1"><mrow><mi>i</mi> <mo>=</mo> <mn>1</mn></mrow></math> 或 2）：
- en: <math display="block"><mrow><msub><mi>Z</mi> <mi>i</mi></msub> <mo>=</mo> <msub><mi>w</mi>
    <mrow><mi>i</mi><mo>,</mo><mn>1</mn></mrow></msub> <msub><mi>X</mi> <mn>1</mn></msub>
    <mo>+</mo> <msub><mi>w</mi> <mrow><mi>i</mi><mo>,</mo><mn>2</mn></mrow></msub>
    <msub><mi>X</mi> <mn>2</mn></msub></mrow></math>
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><msub><mi>Z</mi> <mi>i</mi></msub> <mo>=</mo> <msub><mi>w</mi>
    <mrow><mi>i</mi><mo>,</mo><mn>1</mn></mrow></msub> <msub><mi>X</mi> <mn>1</mn></msub>
    <mo>+</mo> <msub><mi>w</mi> <mrow><mi>i</mi><mo>,</mo><mn>2</mn></mrow></msub>
    <msub><mi>X</mi> <mn>2</mn></msub></mrow></math>
- en: The weights <math alttext="left-parenthesis w Subscript i comma 1 Baseline comma
    w Subscript i comma 2 Baseline right-parenthesis"><mrow><mo>(</mo> <msub><mi>w</mi>
    <mrow><mi>i</mi><mo>,</mo><mn>1</mn></mrow></msub> <mo>,</mo> <msub><mi>w</mi>
    <mrow><mi>i</mi><mo>,</mo><mn>2</mn></mrow></msub> <mo>)</mo></mrow></math> are
    known as the component *loadings*. These transform the original variables into
    the principal components. The first principal component, <math alttext="upper
    Z 1"><msub><mi>Z</mi> <mn>1</mn></msub></math> , is the linear combination that
    best explains the total variation. The second principal component, <math alttext="upper
    Z 2"><msub><mi>Z</mi> <mn>2</mn></msub></math> , is orthogonal to the first and
    explains as much of the remaining variation as it can. (If there were additional
    components, each additional one would be orthogonal to the others.)
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 权重 <math alttext="left-parenthesis w Subscript i comma 1 Baseline comma w Subscript
    i comma 2 Baseline right-parenthesis"><mrow><mo>(</mo> <msub><mi>w</mi> <mrow><mi>i</mi><mo>,</mo><mn>1</mn></mrow></msub>
    <mo>,</mo> <msub><mi>w</mi> <mrow><mi>i</mi><mo>,</mo><mn>2</mn></mrow></msub>
    <mo>)</mo></mrow></math> 被称为成分*载荷*。这些将原始变量转换为主成分。第一个主成分，<math alttext="upper Z
    1"><msub><mi>Z</mi> <mn>1</mn></msub></math> ，是最能解释总变化的线性组合。第二个主成分，<math alttext="upper
    Z 2"><msub><mi>Z</mi> <mn>2</mn></msub></math> ，与第一个成分正交，并尽可能多地解释剩余的变化。（如果有额外的成分，每一个都会与其他成分正交。）
- en: Note
  id: totrans-17
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: It is also common to compute principal components on deviations from the means
    of the predictor variables, rather than on the values themselves.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 通常也会计算基于预测变量偏差的主成分，而不是基于值本身。
- en: 'You can compute principal components in *R* using the `princomp` function.
    The following performs a PCA on the stock price returns for Chevron (CVX) and
    ExxonMobil (XOM):'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用`princomp`函数在*R*中计算主成分。以下是对雪佛龙（CVX）和埃克森美孚（XOM）股票价格回报进行主成分分析的示例：
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'In *Python*, we can use the `scikit-learn` implementation `sklearn.decomposition.PCA`:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在*Python*中，我们可以使用`scikit-learn`中的`sklearn.decomposition.PCA`实现：
- en: '[PRE1]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The weights for CVX and XOM for the first principal component are –0.747 and
    –0.665, and for the second principal component they are 0.665 and –0.747. How
    to interpret this? The first principal component is essentially an average of
    CVX and XOM, reflecting the correlation between the two energy companies. The
    second principal component measures when the stock prices of CVX and XOM diverge.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 雪佛龙和埃克森美孚的第一个主成分权重为-0.747和-0.665，第二个主成分的权重为0.665和-0.747。如何解释这一点？第一个主成分基本上是CVX和XOM的平均值，反映了这两家能源公司之间的相关性。第二个主成分衡量了CVX和XOM股票价格分歧的时候。
- en: 'It is instructive to plot the principal components with the data. Here we create
    a visualization in *R*:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 将主成分与数据一起绘制非常有教育意义。在这里我们使用*R*创建了一个可视化效果：
- en: '[PRE2]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The following code creates a similar visualization in *Python*:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码使用*Python*创建类似的可视化效果：
- en: '[PRE3]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The result is shown in [Figure 7-1](#StockPCA).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 结果显示在[图 7-1](#StockPCA)中。
- en: '![The principal components for the stock returns for Chevron and ExxonMobil](Images/psd2_0701.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![雪佛龙和埃克森美孚股票回报的主成分](Images/psd2_0701.png)'
- en: Figure 7-1\. The principal components for the stock returns for Chevron (CVX)
    and ExxonMobil (XOM)
  id: totrans-30
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-1\. 雪佛龙（CVX）和埃克森美孚（XOM）股票回报的主成分
- en: 'The dashed lines show the direction of the two principal components: the first
    one is along the long axis of the ellipse, and the second one is along the short
    axis. You can see that a majority of the variability in the two stock returns
    is explained by the first principal component. This makes sense since energy stock
    prices tend to move as a group.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 虚线显示了两个主成分的方向：第一个主成分沿着椭圆的长轴，第二个主成分沿着短轴。您可以看到，雪佛龙和埃克森美孚的股票回报中的大部分变异性都由第一个主成分解释。这是有道理的，因为能源股票价格往往会作为一组移动。
- en: Note
  id: totrans-32
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The weights for the first principal component are both negative, but reversing
    the sign of all the weights does not change the principal component. For example,
    using weights of 0.747 and 0.665 for the first principal component is equivalent
    to the negative weights, just as an infinite line defined by the origin and 1,1
    is the same as one defined by the origin and –1, –1.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个主成分的权重都为负，但反转所有权重的符号并不会改变主成分。例如，使用第一个主成分的权重0.747和0.665等同于负权重，就像由原点和1,1定义的无限线条等同于由原点和-1,-1定义的线条一样。
- en: Computing the Principal Components
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算主成分
- en: 'Going from two variables to more variables is straightforward. For the first
    component, simply include the additional predictor variables in the linear combination,
    assigning weights that optimize the collection of the covariation from all the
    predictor variables into this first principal component (*covariance* is the statistical
    term; see [“Covariance Matrix”](ch05.xhtml#Covariance)). Calculation of principal
    components is a classic statistical method, relying on either the correlation
    matrix of the data or the covariance matrix, and it executes rapidly, not relying
    on iteration. As noted earlier, principal components analysis works only with
    numeric variables, not categorical ones. The full process can be described as
    follows:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 从两个变量到更多变量的过程很简单。对于第一个成分，只需将额外的预测变量包括在线性组合中，并分配权重以优化所有预测变量的协变化进入这第一个主成分（*协方差*是统计术语；见[“协方差矩阵”](ch05.xhtml#Covariance)）。主成分的计算是一种经典的统计方法，依赖于数据的相关矩阵或协方差矩阵，并且执行迅速，不依赖迭代。正如前面所述，主成分分析仅适用于数值变量，而不适用于分类变量。整个过程可以描述如下：
- en: In creating the first principal component, PCA arrives at the linear combination
    of predictor variables that maximizes the percent of total variance explained.
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在创建第一个主成分时，PCA得出了最大化解释总方差百分比的预测变量的线性组合。
- en: This linear combination then becomes the first “new” predictor, *Z*[1].
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，这个线性组合就成为第一个“新”预测变量*Z*[1]。
- en: PCA repeats this process, using the same variables with different weights, to
    create a second new predictor, *Z*[2]. The weighting is done such that *Z*[1]
    and *Z*[2] are uncorrelated.
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: PCA重复此过程，使用不同的权重与相同的变量，以创建第二个新的预测变量*Z*[2]。权重的设置使得*Z*[1]和*Z*[2]不相关。
- en: The process continues until you have as many new variables, or components, *Z*[i]
    as original variables *X*[i].
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 该过程持续进行，直到您获得与原始变量*X*[i]一样多的新变量或组件*Z*[i]。
- en: Choose to retain as many components as are needed to account for most of the
    variance.
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择保留尽可能多的组件，以解释大部分的方差。
- en: The result so far is a set of weights for each component. The final step is
    to convert the original data into new principal component scores by applying the
    weights to the original values. These new scores can then be used as the reduced
    set of predictor variables.
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 到目前为止，每个组件都有一组权重。最后一步是通过将这些权重应用于原始值来将原始数据转换为新的主成分分数。然后可以使用这些新分数作为减少的预测变量集。
- en: Interpreting Principal Components
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解释主成分
- en: 'The nature of the principal components often reveals information about the
    structure of the data. There are a couple of standard visualization displays to
    help you glean insight about the principal components. One such method is a *screeplot*
    to visualize the relative importance of principal components (the name derives
    from the resemblance of the plot to a scree slope; here, the y-axis is the eigenvalue).
    The following *R* code shows an example for a few top companies in the S&P 500:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 主成分的性质通常揭示了关于数据结构的信息。有几种标准的可视化显示方法可帮助您获取有关主成分的见解。其中一种方法是*屏幕图*，用于可视化主成分的相对重要性（该名称源于图表与屏坡的相似性；这里，y轴是特征值）。以下*R*代码显示了S&P
    500中几家顶级公司的示例：
- en: '[PRE4]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The information to create a loading plot from the `scikit-learn` result is
    available in `explained_variance_`. Here, we convert it into a `pandas` data frame
    and use it to make a bar chart:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 从`scikit-learn`结果创建加载图的信息可在`explained_variance_`中找到。在这里，我们将其转换为`pandas`数据框架，并用它制作条形图：
- en: '[PRE5]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: As seen in [Figure 7-2](#Screeplot), the variance of the first principal component
    is quite large (as is often the case), but the other top principal components
    are significant.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 如图[7-2](#Screeplot)所示，第一个主成分的方差非常大（通常情况下如此），但其他顶级主成分也很显著。
- en: '![A screeplot for a PCA of top stocks from the SP 500.](Images/psd2_0702.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![S&P 500中热门股票的PCA的屏幕图。](Images/psd2_0702.png)'
- en: Figure 7-2\. A screeplot for a PCA of top stocks from the S&P 500
  id: totrans-49
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-2\. S&P 500中热门股票的PCA的屏幕图
- en: 'It can be especially revealing to plot the weights of the top principal components.
    One way to do this in *R* is to use the `gather` function from the `tidyr` package
    in conjunction with `ggplot`:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 绘制顶级主成分的权重可能特别有启发性。在*R*中，一种方法是使用`tidyr`包中的`gather`函数与`ggplot`结合使用：
- en: '[PRE6]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Here is the code to create the same visualization in *Python*:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这是在*Python*中创建相同可视化的代码：
- en: '[PRE7]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The loadings for the top five components are shown in [Figure 7-3](#Loadings).
    The loadings for the first principal component have the same sign: this is typical
    for data in which all the columns share a common factor (in this case, the overall
    stock market trend). The second component captures the price changes of energy
    stocks as compared to the other stocks. The third component is primarily a contrast
    in the movements of Apple and CostCo. The fourth component contrasts the movements
    of Schlumberger (SLB) to the other energy stocks. Finally, the fifth component
    is mostly dominated by financial companies.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 前五个成分的负载显示在图 7-3中。第一个主成分的负载具有相同的符号：这对于所有列共享一个公共因子的数据是典型的（在这种情况下，是整体股市趋势）。第二个成分捕捉了能源股票的价格变化相对于其他股票的情况。第三个成分主要是对比了苹果和CostCo的动态。第四个成分对比了斯伦贝谢（SLB）与其他能源股票的动态。最后，第五个成分主要受到金融公司的影响。
- en: '![The loadings for the top five principal components of stock price returns.](Images/psd2_0703.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![股价回报的前五个主成分的负载。](Images/psd2_0703.png)'
- en: Figure 7-3\. The loadings for the top five principal components of stock price
    returns
  id: totrans-56
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-3\. 股价回报的前五个主成分的负载
- en: How Many Components to Choose?
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何选择成分数量？
- en: If your goal is to reduce the dimension of the data, you must decide how many
    principal components to select. The most common approach is to use an ad hoc rule
    to select the components that explain “most” of the variance. You can do this
    visually through the screeplot, as, for example, in [Figure 7-2](#Screeplot).
    Alternatively, you could select the top components such that the cumulative variance
    exceeds a threshold, such as 80%. Also, you can inspect the loadings to determine
    if the component has an intuitive interpretation. Cross-validation provides a
    more formal method to select the number of significant components (see [“Cross-Validation”](ch04.xhtml#CrossValidation)
    for more).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的目标是降低数据的维度，你必须决定选择多少个主成分。最常见的方法是使用一种临时规则来选择解释“大部分”方差的成分。你可以通过研究屏斜图来直观地做到这一点，例如[图 7-2](#Screeplot)。或者，你可以选择前几个成分，使累积方差超过一个阈值，比如80%。此外，你还可以检查负载以确定成分是否具有直观的解释。交叉验证提供了一种更正式的方法来选择显著成分的数量（参见[“交叉验证”](ch04.xhtml#CrossValidation)了解更多）。
- en: Correspondence Analysis
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对应分析
- en: PCA cannot be used for categorical data; however, a somewhat related technique
    is *correspondence analysis*. The goal is to recognize associations between categories,
    or between categorical features. The similarities between correspondence analysis
    and principal components analysis are mainly under the hood—the matrix algebra
    for dimension scaling. Correspondence analysis is used mainly for graphical analysis
    of low-dimensional categorical data and is not used in the same way that PCA is
    for dimension reduction as a preparatory step with big data.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: PCA 不能用于分类数据；然而，一个有点相关的技术是*对应分析*。其目标是识别类别之间的关联，或者分类特征之间的关联。对应分析与主成分分析的相似之处主要在于底层——用于尺度化维度的矩阵代数。对应分析主要用于低维分类数据的图形分析，并不像PCA那样用于大数据的维度减少作为预处理步骤。
- en: The input can be seen as a table, with rows representing one variable and columns
    another, and the cells representing record counts. The output (after some matrix
    algebra) is a *biplot*—a scatterplot with axes scaled (and with percentages indicating
    how much variance is explained by that dimension). The meaning of the units on
    the axes is not intuitively connected to the original data, and the main value
    of the scatterplot is to illustrate graphically variables that are associated
    with one another (by proximity on the plot). See for example, [Figure 7-4](#Correspondence_Analysis),
    in which household tasks are arrayed according to whether they are done jointly
    or solo (vertical axis), and whether wife or husband has primary responsibility
    (horizontal axis). Correspondence analysis is many decades old, as is the spirit
    of this example, judging by the assignment of tasks.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 输入可以看作是一个表格，其中行代表一个变量，列代表另一个变量，单元格表示记录计数。输出（经过一些矩阵代数运算后）是一个*双标图* —— 一个散点图，其轴经过缩放（并且通过百分比显示该维度解释的方差量）。轴上的单位含义与原始数据的直觉连接并不大，散点图的主要价值在于以图形方式说明彼此相关的变量（通过图中的接近度）。例如，参见[图 7-4](#Correspondence_Analysis)，在该图中，家务任务按照是否共同完成（垂直轴）和妻子或丈夫是否有主要责任（水平轴）进行排列。对应分析已经存在了几十年，就像这个示例的精神一样，根据任务的分配。
- en: 'There are a variety of packages for correspondence analysis in *R*. Here, we
    use the package `ca`:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在*R*中，有多种用于对应分析的软件包。这里我们使用`ca`软件包：
- en: '[PRE8]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'In *Python*, we can use the `prince` package, which implements correspondence
    analysis using the `scikit-learn` API:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在*Python*中，我们可以使用`prince`软件包，它使用`scikit-learn` API实现了对应分析：
- en: '[PRE9]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![Correspondence analysis of house task data.](Images/psd2_0704.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![房屋任务数据的对应分析](Images/psd2_0704.png)'
- en: Figure 7-4\. Graphical representation of a correspondence analysis of house
    task data
  id: totrans-67
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-4\. 房屋任务数据的对应分析的图形表示
- en: Further Reading
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'For a detailed look at the use of cross-validation in principal components,
    see Rasmus Bro, K. Kjeldahl, A.K. Smilde, and Henk A. L. Kiers, [“Cross-Validation
    of Component Models: A Critical Look at Current Methods”](https://oreil.ly/yVryf),
    *Analytical and Bioanalytical Chemistry* 390, no. 5 (2008).'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 想要详细了解主成分分析中交叉验证的使用方法，请参阅Rasmus Bro, K. Kjeldahl, A.K. Smilde, 和 Henk A. L.
    Kiers的文章，[“Component Models的交叉验证：对当前方法的批判性审视”](https://oreil.ly/yVryf)，发表于*分析与生物分析化学*390卷，5期（2008年）。
- en: K-Means Clustering
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: K-Means聚类
- en: Clustering is a technique to divide data into different groups, where the records
    in each group are similar to one another. A goal of clustering is to identify
    significant and meaningful groups of data. The groups can be used directly, analyzed
    in more depth, or passed as a feature or an outcome to a predictive regression
    or classification model. *K-means* was the first clustering method to be developed;
    it is still widely used, owing its popularity to the relative simplicity of the
    algorithm and its ability to scale to large data sets.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类是一种将数据分成不同组的技术，其中每组内的记录彼此相似。聚类的目标是识别重要且有意义的数据组。这些组可以直接使用，深入分析，或者作为预测回归或分类模型的特征或结果。*K-means*是最早开发的聚类方法之一；它仍然被广泛使用，因为算法相对简单且能够扩展到大数据集。
- en: '*K*-means divides the data into *K* clusters by minimizing the sum of the squared
    distances of each record to the *mean* of its assigned cluster. This is referred
    to as the *within-cluster sum of squares* or *within-cluster SS*. *K*-means does
    not ensure the clusters will have the same size but finds the clusters that are
    the best separated.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '*K*-means通过最小化每个记录到其分配的群集的均值的平方距离来将数据分成*K*个群集。这被称为*群内平方和*或*群内SS*。*K*-means不能确保群集大小相同，但能找到最佳分离的群集。'
- en: Normalization
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 标准化
- en: It is typical to normalize (standardize) continuous variables by subtracting
    the mean and dividing by the standard deviation. Otherwise, variables with large
    scale will dominate the clustering process (see [“Standardization (Normalization,
    z-Scores)”](ch06.xhtml#Standardization)).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 通常会通过减去均值并除以标准差来对连续变量进行标准化。否则，具有大量数据的变量会在聚类过程中占主导地位（参见[“标准化（归一化，z-分数）”](ch06.xhtml#Standardization)）。
- en: A Simple Example
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一个简单的例子
- en: 'Start by considering a data set with *n* records and just two variables, <math
    alttext="x"><mi>x</mi></math> and <math alttext="y"><mi>y</mi></math> . Suppose
    we want to split the data into <math alttext="upper K equals 4"><mrow><mi>K</mi>
    <mo>=</mo> <mn>4</mn></mrow></math> clusters. This means assigning each record
    <math alttext="left-parenthesis x Subscript i Baseline comma y Subscript i Baseline
    right-parenthesis"><mrow><mo>(</mo> <msub><mi>x</mi> <mi>i</mi></msub> <mo>,</mo>
    <msub><mi>y</mi> <mi>i</mi></msub> <mo>)</mo></mrow></math> to a cluster *k*.
    Given an assignment of <math alttext="n Subscript k"><msub><mi>n</mi> <mi>k</mi></msub></math>
    records to cluster *k*, the center of the cluster <math alttext="left-parenthesis
    x overbar Subscript k Baseline comma y overbar Subscript k Baseline right-parenthesis"><mrow><mo>(</mo>
    <msub><mover accent="true"><mi>x</mi> <mo>¯</mo></mover> <mi>k</mi></msub> <mo>,</mo>
    <msub><mover accent="true"><mi>y</mi> <mo>¯</mo></mover> <mi>k</mi></msub> <mo>)</mo></mrow></math>
    is the mean of the points in the cluster:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 首先考虑一个数据集，包含*n*个记录和两个变量，<math alttext="x"><mi>x</mi></math> 和 <math alttext="y"><mi>y</mi></math>
    。假设我们想将数据分成<math alttext="upper K equals 4"><mrow><mi>K</mi> <mo>=</mo> <mn>4</mn></mrow></math>个群集。这意味着将每个记录<math
    alttext="left-parenthesis x Subscript i Baseline comma y Subscript i Baseline
    right-parenthesis"><mrow><mo>(</mo> <msub><mi>x</mi> <mi>i</mi></msub> <mo>,</mo>
    <msub><mi>y</mi> <mi>i</mi></msub> <mo>)</mo></mrow></math> 分配给一个群集*k*。考虑到将<math
    alttext="n Subscript k"><msub><mi>n</mi> <mi>k</mi></msub></math>个记录分配给群集*k*，群集的中心<math
    alttext="left-parenthesis x overbar Subscript k Baseline comma y overbar Subscript
    k Baseline right-parenthesis"><mrow><mo>(</mo> <msub><mover accent="true"><mi>x</mi>
    <mo>¯</mo></mover> <mi>k</mi></msub> <mo>,</mo> <msub><mover accent="true"><mi>y</mi>
    <mo>¯</mo></mover> <mi>k</mi></msub> <mo>)</mo></mrow></math> 是群集中点的均值：
- en: <math display="block"><mtable displaystyle="true"><mtr><mtd><msub><mrow class="MJX-TeXAtom-ORD"><mover><mi>x</mi><mo
    stretchy="false">¯</mo></mover></mrow> <mi>k</mi></msub></mtd> <mtd><mo>=</mo>
    <mfrac><mn>1</mn><msub><mi>n</mi><mi>k</mi></msub></mfrac> <munder><mo>∑</mo>
    <mrow class="MJX-TeXAtom-ORD"><mtable rowspacing="0.1em" columnspacing="0em 0em
    0em 0em" displaystyle="false"><mtr><mtd><mi>i</mi><mo>∈</mo></mtd></mtr> <mtr><mtd><mrow
    class="MJX-TeXAtom-ORD"><mtext>Cluster</mtext></mrow> <mi>k</mi></mtd></mtr></mtable></mrow></munder>
    <msub><mi>x</mi> <mi>i</mi></msub></mtd></mtr> <mtr><mtd><msub><mrow class="MJX-TeXAtom-ORD"><mover><mi>y</mi><mo
    stretchy="false">¯</mo></mover></mrow> <mi>k</mi></msub></mtd> <mtd><mo>=</mo>
    <mfrac><mn>1</mn><msub><mi>n</mi><mi>k</mi></msub></mfrac> <munder><mo>∑</mo>
    <mrow class="MJX-TeXAtom-ORD"><mtable rowspacing="0.1em" columnspacing="0em 0em
    0em 0em" displaystyle="false"><mtr><mtd><mi>i</mi><mo>∈</mo></mtd></mtr> <mtr><mtd><mrow
    class="MJX-TeXAtom-ORD"><mtext>Cluster</mtext></mrow> <mi>k</mi></mtd></mtr></mtable></mrow></munder>
    <msub><mi>y</mi><mi>i</mi></msub></mtd></mtr></mtable></math>
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mtable displaystyle="true"><mtr><mtd><msub><mrow class="MJX-TeXAtom-ORD"><mover><mi>x</mi><mo
    stretchy="false">¯</mo></mover></mrow> <mi>k</mi></msub></mtd> <mtd><mo>=</mo>
    <mfrac><mn>1</mn><msub><mi>n</mi><mi>k</mi></msub></mfrac> <munder><mo>∑</mo>
    <mrow class="MJX-TeXAtom-ORD"><mtable rowspacing="0.1em" columnspacing="0em 0em
    0em 0em" displaystyle="false"><mtr><mtd><mi>i</mi><mo>∈</mo></mtd></mtr> <mtr><mtd><mrow
    class="MJX-TeXAtom-ORD"><mtext>Cluster</mtext></mrow> <mi>k</mi></mtd></mtr></mtable></mrow></munder>
    <msub><mi>x</mi> <mi>i</mi></msub></mtd></mtr> <mtr><mtd><msub><mrow class="MJX-TeXAtom-ORD"><mover><mi>y</mi><mo
    stretchy="false">¯</mo></mover></mrow> <mi>k</mi></msub></mtd> <mtd><mo>=</mo>
    <mfrac><mn>1</mn><msub><mi>n</mi><mi>k</mi></msub></mfrac> <munder><mo>∑</mo>
    <mrow class="MJX-TeXAtom-ORD"><mtable rowspacing="0.1em" columnspacing="0em 0em
    0em 0em" displaystyle="false"><mtr><mtd><mi>i</mi><mo>∈</mo></mtd></mtr> <mtr><mtd><mrow
    class="MJX-TeXAtom-ORD"><mtext>Cluster</mtext></mrow> <mi>k</mi></mtd></mtr></mtable></mrow></munder>
    <msub><mi>y</mi><mi>i</mi></msub></mtd></mtr></mtable></math>
- en: Cluster Mean
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚类平均值
- en: In clustering records with multiple variables (the typical case), the term *cluster
    mean* refers not to a single number but to the vector of means of the variables.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在具有多个变量的记录聚类中（典型情况），术语*簇均值*不是指单一数字，而是指变量均值向量。
- en: 'The sum of squares within a cluster is given by:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 簇内平方和由以下给出：
- en: <math display="block"><mrow><msub><mtext>SS</mtext> <mi>k</mi></msub> <mo>=</mo>
    <munder><mo>∑</mo> <mrow><mi>i</mi><mo>∈</mo><mtext>Cluster</mtext><mi>k</mi></mrow></munder>
    <msup><mfenced separators="" open="(" close=")"><msub><mi>x</mi> <mi>i</mi></msub>
    <mo>-</mo><msub><mover accent="true"><mi>x</mi> <mo>¯</mo></mover> <mi>k</mi></msub></mfenced>
    <mn>2</mn></msup> <mo>+</mo> <msup><mfenced separators="" open="(" close=")"><msub><mi>y</mi>
    <mi>i</mi></msub> <mo>-</mo><msub><mover accent="true"><mi>y</mi> <mo>¯</mo></mover>
    <mi>k</mi></msub></mfenced> <mn>2</mn></msup></mrow></math>
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><msub><mtext>SS</mtext> <mi>k</mi></msub> <mo>=</mo>
    <munder><mo>∑</mo> <mrow><mi>i</mi><mo>∈</mo><mtext>Cluster</mtext><mi>k</mi></mrow></munder>
    <msup><mfenced separators="" open="(" close=")"><msub><mi>x</mi> <mi>i</mi></msub>
    <mo>-</mo><msub><mover accent="true"><mi>x</mi> <mo>¯</mo></mover> <mi>k</mi></msub></mfenced>
    <mn>2</mn></msup> <mo>+</mo> <msup><mfenced separators="" open="(" close=")"><msub><mi>y</mi>
    <mi>i</mi></msub> <mo>-</mo><msub><mover accent="true"><mi>y</mi> <mo>¯</mo></mover>
    <mi>k</mi></msub></mfenced> <mn>2</mn></msup></mrow></math>
- en: '*K*-means finds the assignment of records that minimizes within-cluster sum
    of squares across all four clusters <math alttext="SS Subscript 1 Baseline plus
    SS Subscript 2 Baseline plus SS Subscript 3 Baseline plus SS Subscript 4"><mrow><msub><mtext>SS</mtext>
    <mn>1</mn></msub> <mo>+</mo> <msub><mtext>SS</mtext> <mn>2</mn></msub> <mo>+</mo>
    <msub><mtext>SS</mtext> <mn>3</mn></msub> <mo>+</mo> <msub><mtext>SS</mtext> <mn>4</mn></msub></mrow></math>
    :'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '*K*-means找到了记录分配方式，以最小化所有四个聚类的簇内平方和<math alttext="SS Subscript 1 Baseline plus
    SS Subscript 2 Baseline plus SS Subscript 3 Baseline plus SS Subscript 4"><mrow><msub><mtext>SS</mtext>
    <mn>1</mn></msub> <mo>+</mo> <msub><mtext>SS</mtext> <mn>2</mn></msub> <mo>+</mo>
    <msub><mtext>SS</mtext> <mn>3</mn></msub> <mo>+</mo> <msub><mtext>SS</mtext> <mn>4</mn></msub></mrow></math>：'
- en: <math display="block"><mrow><munderover><mo>∑</mo> <mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow>
    <mn>4</mn></munderover> <msub><mtext>SS</mtext> <mi>k</mi></msub></mrow></math>
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><munderover><mo>∑</mo> <mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow>
    <mn>4</mn></munderover> <msub><mtext>SS</mtext> <mi>k</mi></msub></mrow></math>
- en: A typical use of clustering is to locate natural, separate clusters in the data.
    Another application is to divide the data into a predetermined number of separate
    groups, where clustering is used to ensure the groups are as different as possible
    from one another.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类的典型用途是在数据中找到自然的、分离的聚类。另一个应用是将数据分为预定数量的单独组，聚类用于确保这些组尽可能彼此不同。
- en: 'For example, suppose we want to divide daily stock returns into four groups.
    *K*-means clustering can be used to separate the data into the best groupings.
    Note that daily stock returns are reported in a fashion that is, in effect, standardized,
    so we do not need to normalize the data. In *R*, *K*-means clustering can be performed
    using the `kmeans` function. For example, the following finds four clusters based
    on two variables—the daily stock returns for ExxonMobil (`XOM`) and Chevron (`CVX`):'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们想将每日股票收益分为四组。可以使用*K*-means聚类将数据分隔为最佳分组。请注意，每日股票收益以一种实际上是标准化的方式报告，因此我们不需要对数据进行标准化。在*R*中，可以使用`kmeans`函数执行*K*-means聚类。例如，以下基于两个变量——埃克森美孚（`XOM`）和雪佛龙（`CVX`）的每日股票收益来找到四个簇：
- en: '[PRE10]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We use the `sklearn.cluster.KMeans` method from `scikit-learn` in *Python*:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用*Python*中`scikit-learn`的`sklearn.cluster.KMeans`方法：
- en: '[PRE11]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The cluster assignment for each record is returned as the `cluster` component
    (*R*):'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 每条记录的簇分配作为`cluster`组件（*R*）返回：
- en: '[PRE12]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'In `scikit-learn`, the cluster labels are available in the `labels_` field:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在`scikit-learn`中，聚类标签可在`labels_`字段中找到：
- en: '[PRE13]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The first six records are assigned to either cluster 1 or cluster 2. The means
    of the clusters are also returned (*R*):'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 前六条记录分配到簇1或簇2。聚类的均值也返回（*R*）：
- en: '[PRE14]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'In `scikit-learn`, the cluster centers are available in the `cluster_centers_`
    field:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在`scikit-learn`中，聚类中心可在`cluster_centers_`字段中找到：
- en: '[PRE15]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Clusters 1 and 3 represent “down” markets, while clusters 2 and 4 represent
    “up markets.”
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 簇1和簇3代表“下跌”市场，而簇2和簇4代表“上涨”市场。
- en: As the *K*-means algorithm uses randomized starting points, the results may
    differ between subsequent runs and different implementations of the method. In
    general, you should check that the fluctuations aren’t too large.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 由于*K*-means算法使用随机起始点，结果可能在后续运行和不同实现方法之间有所不同。一般而言，应检查波动是否过大。
- en: 'In this example, with just two variables, it is straightforward to visualize
    the clusters and their means:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，只有两个变量，可以直观地展示聚类及其均值：
- en: '[PRE16]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The `seaborn` `scatterplot` function makes it easy to color (`hue`) and style
    (`style`) the points by a property:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '`seaborn`的`scatterplot`函数使得可以通过属性（`hue`）和样式（`style`）轻松着色点：'
- en: '[PRE17]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The resulting plot, shown in [Figure 7-5](#KmeansStockData), shows the cluster
    assignments and the cluster means. Note that *K*-means will assign records to
    clusters, even if those clusters are not well separated (which can be useful if
    you need to optimally divide records into groups).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 所得图中显示了[图7-5](#KmeansStockData)中的聚类分配和聚类均值。请注意，即使这些聚类没有很好地分离，*K*-means也会将记录分配到聚类中（这在需要将记录最优地分成组时非常有用）。
- en: '![The clusters of k-means applied to stock price data for ExxonMobil and Chevron
    (the cluster centers are highlighted with black symbols).](Images/psd2_0705.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![应用于埃克森美孚和雪佛龙的股价数据的*k*-means聚类（聚类中心用黑色符号突出显示）。](Images/psd2_0705.png)'
- en: Figure 7-5\. The clusters of K-means applied to daily stock returns for ExxonMobil
    and Chevron (the cluster centers are highlighted with black symbols)
  id: totrans-105
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-5\. 应用于埃克森美孚和雪佛龙每日股票收益的K均值聚类（聚类中心用黑色符号突出显示）
- en: K-Means Algorithm
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: K均值算法
- en: In general, *K*-means can be applied to a data set with *p* variables <math
    alttext="upper X 1 comma ellipsis comma upper X Subscript p Baseline"><mrow><msub><mi>X</mi>
    <mn>1</mn></msub> <mo>,</mo> <mo>...</mo> <mo>,</mo> <msub><mi>X</mi> <mi>p</mi></msub></mrow></math>
    . While the exact solution to *K*-means is computationally very difficult, heuristic
    algorithms provide an efficient way to compute a locally optimal solution.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，*K*均值可以应用于具有*p*个变量的数据集<math alttext="upper X 1 comma ellipsis comma upper
    X Subscript p Baseline"><mrow><msub><mi>X</mi> <mn>1</mn></msub> <mo>,</mo> <mo>...</mo>
    <mo>,</mo> <msub><mi>X</mi> <mi>p</mi></msub></mrow></math> 。虽然*K*均值的确切解决方案在计算上非常困难，但启发式算法提供了计算局部最优解的有效方法。
- en: 'The algorithm starts with a user-specified *K* and an initial set of cluster
    means and then iterates the following steps:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 算法从用户指定的*K*和初始聚类均值开始，然后迭代以下步骤：
- en: Assign each record to the nearest cluster mean as measured by squared distance.
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将每条记录分配到最近的聚类均值，方法是通过平方距离来衡量。
- en: Compute the new cluster means based on the assignment of records.
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据记录的分配计算新的聚类均值。
- en: The algorithm converges when the assignment of records to clusters does not
    change.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 当记录分配到聚类时不再改变时，算法收敛。
- en: For the first iteration, you need to specify an initial set of cluster means.
    Usually you do this by randomly assigning each record to one of the *K* clusters
    and then finding the means of those clusters.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第一次迭代，您需要指定一个初始的聚类均值集。通常情况下，您可以通过随机将每个记录分配给*K*个聚类之一，然后找到这些聚类的均值来完成此操作。
- en: Since this algorithm isn’t guaranteed to find the best possible solution, it
    is recommended to run the algorithm several times using different random samples
    to initialize the algorithm. When more than one set of iterations is used, the
    *K*-means result is given by the iteration that has the lowest within-cluster
    sum of squares.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 由于此算法不能保证找到最佳可能的解决方案，建议使用不同的随机样本多次运行算法以初始化算法。当使用多组迭代时，*K*均值结果由具有最低聚类内平方和的迭代给出。
- en: 'The `nstart` parameter to the *R* function `kmeans` allows you to specify the
    number of random starts to try. For example, the following code runs *K*-means
    to find 5 clusters using 10 different starting cluster means:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '*R*函数`kmeans`的`nstart`参数允许您指定尝试的随机起始次数。例如，以下代码使用10个不同的起始聚类均值运行*K*均值以找到5个聚类：'
- en: '[PRE18]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The function automatically returns the best solution out of the 10 different
    starting points. You can use the argument `iter.max` to set the maximum number
    of iterations the algorithm is allowed for each random start.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 函数会自动从10个不同的起始点中返回最佳解决方案。您可以使用参数`iter.max`来设置算法允许的每个随机起始的最大迭代次数。
- en: 'The `scikit-learn` algorithm is repeated 10 times by default (`n_init`). The
    argument `max_iter` (default 300) can be used to control the number of iterations:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，`scikit-learn`算法会重复10次（`n_init`）。参数`max_iter`（默认为300）可用于控制迭代次数：
- en: '[PRE19]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Interpreting the Clusters
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解释聚类
- en: 'An important part of cluster analysis can involve the interpretation of the
    clusters. The two most important outputs from `kmeans` are the sizes of the clusters
    and the cluster means. For the example in the previous subsection, the sizes of
    resulting clusters are given by this *R* command:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类分析的一个重要部分可能涉及对聚类的解释。`kmeans`的两个最重要的输出是聚类的大小和聚类均值。对于上一小节的示例，生成的聚类大小由以下*R*命令给出：
- en: '[PRE20]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'In *Python*, we can use the `collections.Counter` class from the standard library
    to get this information. Due to differences in the implementation and the inherent
    randomness of the algorithm, results will vary:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在*Python*中，我们可以使用标准库中的`collections.Counter`类来获取这些信息。由于实现的差异和算法固有的随机性，结果会有所不同：
- en: '[PRE21]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The cluster sizes are relatively balanced. Imbalanced clusters can result from
    distant outliers, or from groups of records very distinct from the rest of the
    data—both may warrant further inspection.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类大小相对平衡。不平衡的聚类可能是由于远离异常值或非常不同于数据其余部分的记录组成，这两者都可能需要进一步检查。
- en: 'You can plot the centers of the clusters using the `gather` function in conjunction
    with `ggplot`:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用`gather`函数与`ggplot`结合来绘制聚类的中心：
- en: '[PRE22]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The code to create this visualization in *Python* is similar to what we used
    for PCA:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 创建此可视化的*Python*代码与我们用于PCA的代码类似：
- en: '[PRE23]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The resulting plot is shown in [Figure 7-6](#ClusterMeans) and reveals the nature
    of each cluster. For example, clusters 4 and 5 correspond to days on which the
    market is down and up, respectively. Clusters 2 and 3 are characterized by up-market
    days for consumer stocks and down-market days for energy stocks, respectively.
    Finally, cluster 1 captures the days in which energy stocks were up and consumer
    stocks were down.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的图表显示在[图7-6](#ClusterMeans)中，显示了每个集群的性质。例如，集群4和5对应于市场下跌和上涨的日子。集群2和3分别以消费者股票上涨日和能源股票下跌日为特征。最后，集群1捕捉到了能源股票上涨和消费者股票下跌的日子。
- en: '![The means of the clusters.](Images/psd2_0706.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![集群均值。](Images/psd2_0706.png)'
- en: Figure 7-6\. The means of the variables in each cluster (“cluster means”)
  id: totrans-131
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-6\. 每个集群中变量的均值（“集群均值”）
- en: Cluster Analysis Versus PCA
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚类分析与主成分分析
- en: The plot of cluster means is similar in spirit to looking at the loadings for
    principal components analysis (PCA); see [“Interpreting Principal Components”](#InterpretPCA).
    A major distinction is that unlike with PCA, the sign of the cluster means is
    meaningful. PCA identifies principal directions of variation, whereas cluster
    analysis finds groups of records located near one another.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 集群均值图表的精神类似于主成分分析（PCA）的载荷。主要区别在于，与PCA不同，集群均值的符号是有意义的。PCA识别变化的主要方向，而聚类分析找到彼此附近的记录组。
- en: Selecting the Number of Clusters
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择集群数量
- en: The *K*-means algorithm requires that you specify the number of clusters *K*.
    Sometimes the number of clusters is driven by the application. For example, a
    company managing a sales force might want to cluster customers into “personas”
    to focus and guide sales calls. In such a case, managerial considerations would
    dictate the number of desired customer segments—for example, two might not yield
    useful differentiation of customers, while eight might be too many to manage.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '*K*-means算法要求您指定集群数量*K*。有时，集群数量由应用程序驱动。例如，管理销售团队的公司可能希望将客户聚类成“人物角色”以便于专注和引导销售电话。在这种情况下，管理考虑因素将决定所需客户段数——例如，两个可能不会产生有用的客户差异化，而八个可能太多难以管理。'
- en: In the absence of a cluster number dictated by practical or managerial considerations,
    a statistical approach could be used. There is no single standard method to find
    the “best” number of clusters.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在缺乏实际或管理考虑因素决定的集群数量的情况下，可以使用统计方法。没有单一标准方法来找到“最佳”集群数量。
- en: A common approach, called the *elbow method*, is to identify when the set of
    clusters explains “most” of the variance in the data. Adding new clusters beyond
    this set contributes relatively little in the variance explained. The elbow is
    the point where the cumulative variance explained flattens out after rising steeply,
    hence the name of the method.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 一个常见的方法是*肘方法*，其目标是确定集群的集合何时解释了数据的“大部分”方差。在此集合之外添加新的集群对解释的方差贡献相对较少。肘部是在累积解释的方差在陡峭上升后变得平缓的点，因此得名此方法。
- en: '[Figure 7-7](#KmeansElbowMethod) shows the cumulative percent of variance explained
    for the default data for the number of clusters ranging from 2 to 15. Where is
    the elbow in this example? There is no obvious candidate, since the incremental
    increase in variance explained drops gradually. This is fairly typical in data
    that does not have well-defined clusters. This is perhaps a drawback of the elbow
    method, but it does reveal the nature of the data.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '[图7-7](#KmeansElbowMethod)显示了默认数据在集群数量从2到15范围内解释的累积百分比方差。在此示例中，肘部在哪里？在这个例子中没有明显的候选者，因为方差解释的增量逐渐下降。这在没有明确定义集群的数据中非常典型。这可能是肘方法的一个缺点，但它确实揭示了数据的本质。'
- en: '![The elbow method applied to the stock data.](Images/psd2_0707.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![股票数据应用肘方法。](Images/psd2_0707.png)'
- en: Figure 7-7\. The elbow method applied to the stock data
  id: totrans-140
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-7\. 股票数据应用肘方法
- en: 'In *R*, the `kmeans` function doesn’t provide a single command for applying
    the elbow method, but it can be readily applied from the output of `kmeans` as
    shown here:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在*R*中，`kmeans`函数并没有提供一个单一的命令来应用肘方法，但可以从`kmeans`的输出中很容易地应用，如下所示：
- en: '[PRE24]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'For the `KMeans` result, we get this information from the property `inertia_`.
    After conversion into a `pandas` data frame, we can use its `plot` method to create
    the graph:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 对于`KMeans`的结果，我们从属性`inertia_`中获取这些信息。在转换为`pandas`数据框后，我们可以使用其`plot`方法创建图表：
- en: '[PRE25]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'In evaluating how many clusters to retain, perhaps the most important test
    is this: how likely are the clusters to be replicated on new data? Are the clusters
    interpretable, and do they relate to a general characteristic of the data, or
    do they just reflect a specific instance? You can assess this, in part, using
    cross-validation; see [“Cross-Validation”](ch04.xhtml#CrossValidation).'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在评估要保留多少个聚类时，也许最重要的测试是：这些聚类在新数据上能否被复制？这些聚类是否可解释，并且它们是否与数据的一般特征相关联，还是仅反映特定实例？部分可以通过交叉验证来评估；参见[“交叉验证”](ch04.xhtml#CrossValidation)。
- en: In general, there is no single rule that will reliably guide how many clusters
    to produce.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，没有一条单一的规则能够可靠地指导产生多少个聚类。
- en: Note
  id: totrans-147
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: There are several more formal ways to determine the number of clusters based
    on statistical or information theory. For example, [Robert Tibshirani, Guenther
    Walther, and Trevor Hastie propose a “gap” statistic](https://oreil.ly/d-N3_)
    based on statistical theory to identify the elbow. For most applications, a theoretical
    approach is probably not necessary, or even appropriate.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 根据统计学或信息理论有几种更正式的方法来确定聚类数。例如，[Robert Tibshirani、Guenther Walther 和 Trevor Hastie
    提出了基于统计理论的“间隙”统计量](https://oreil.ly/d-N3_) 来识别拐点。对于大多数应用程序来说，理论方法可能是不必要的，甚至不合适。
- en: Hierarchical Clustering
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 层次聚类
- en: '*Hierarchical clustering* is an alternative to *K*-means that can yield very
    different clusters. Hierarchical clustering allows the user to visualize the effect
    of specifying different numbers of clusters. It is more sensitive in discovering
    outlying or aberrant groups or records. Hierarchical clustering also lends itself
    to an intuitive graphical display, leading to easier interpretation of the clusters.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '*层次聚类*是一种替代*K*-均值的方法，可以产生非常不同的聚类。层次聚类允许用户可视化指定不同聚类数的效果。在发现异常或畸变组或记录方面更为敏感。层次聚类还适合直观的图形显示，有助于更容易地解释聚类。'
- en: Hierarchical clustering’s flexibility comes with a cost, and hierarchical clustering
    does not scale well to large data sets with millions of records. For even modest-sized
    data with just tens of thousands of records, hierarchical clustering can require
    intensive computing resources. Indeed, most of the applications of hierarchical
    clustering are focused on relatively small data sets.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 层次聚类的灵活性伴随着成本，且层次聚类不适用于具有数百万条记录的大数据集。即使是具有几万条记录的中等规模数据，层次聚类也可能需要大量的计算资源。实际上，大多数层次聚类的应用集中在相对小型的数据集上。
- en: A Simple Example
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 简单示例
- en: 'Hierarchical clustering works on a data set with *n* records and *p* variables
    and is based on two basic building blocks:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 层次聚类适用于一个具有*n*条记录和*p*个变量的数据集，并基于两个基本构建块：
- en: A distance metric <math alttext="d Subscript i comma j"><msub><mi>d</mi> <mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub></math>
    to measure the distance between two records *i* and *j*.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个距离度量 <math alttext="d Subscript i comma j"><msub><mi>d</mi> <mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub></math>
    用于测量两个记录*i*和*j*之间的距离。
- en: A dissimilarity metric <math alttext="upper D Subscript upper A comma upper
    B"><msub><mi>D</mi> <mrow><mi>A</mi><mo>,</mo><mi>B</mi></mrow></msub></math>
    to measure the difference between two clusters *A* and *B* based on the distances
    <math alttext="d Subscript i comma j"><msub><mi>d</mi> <mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub></math>
    between the members of each cluster.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个不相似度度量 <math alttext="upper D Subscript upper A comma upper B"><msub><mi>D</mi>
    <mrow><mi>A</mi><mo>,</mo><mi>B</mi></mrow></msub></math> 用于基于成员之间的距离 <math alttext="d
    Subscript i comma j"><msub><mi>d</mi> <mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub></math>
    来测量两个聚类*A*和*B*之间的差异。
- en: For applications involving numeric data, the most importance choice is the dissimilarity
    metric. Hierarchical clustering starts by setting each record as its own cluster
    and iterates to combine the least dissimilar clusters.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 对于涉及数值数据的应用程序，最重要的选择是不相似度度量。层次聚类通过将每条记录设置为其自己的集群，并迭代以合并最不相似的集群来开始。
- en: 'In *R*, the `hclust` function can be used to perform hierarchical clustering.
    One big difference with `hclust` versus `kmeans` is that it operates on the pairwise
    distances <math alttext="d Subscript i comma j"><msub><mi>d</mi> <mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub></math>
    rather than the data itself. You can compute these using the `dist` function.
    For example, the following applies hierarchical clustering to the stock returns
    for a set of companies:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *R* 中，可以使用 `hclust` 函数执行层次聚类。与 `kmeans` 不同之处在于，它基于成对距离 <math alttext="d Subscript
    i comma j"><msub><mi>d</mi> <mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub></math>
    而不是数据本身。可以使用 `dist` 函数计算这些距离。例如，以下代码对一组公司的股票回报应用了层次聚类：
- en: '[PRE26]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Clustering algorithms will cluster the records (rows) of a data frame. Since
    we want to cluster the companies, we need to *transpose* (`t`) the data frame
    and put the stocks along the rows and the dates along the columns.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类算法将数据框的记录（行）进行聚类。由于我们想要对公司进行聚类，因此需要*转置*（`t`）数据框，使得股票沿着行，日期沿着列。
- en: 'The `scipy` package offers a number of different methods for hierarchical clustering
    in the `scipy.cluster.hierarchy` module. Here we use the `linkage` function with
    the “complete” method:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '`scipy` 包在 `scipy.cluster.hierarchy` 模块中提供了多种不同的层次聚类方法。这里我们使用 `linkage` 函数以“complete”方法：'
- en: '[PRE27]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The Dendrogram
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 树状图
- en: 'Hierarchical clustering lends itself to a natural graphical display as a tree,
    referred to as a *dendrogram*. The name comes from the Greek words *dendro* (tree)
    and *gramma* (drawing). In *R*, you can easily produce this using the `plot` command:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 层次聚类自然适合以树状图形式展示，称为*树状图*。该名称源自希腊语单词*dendro*（树）和*gramma*（绘制）。在 *R* 中，你可以轻松地使用
    `plot` 命令生成这个图：
- en: '[PRE28]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'We can use the `dendrogram` method to plot the result of the `linkage` function
    in *Python*:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 `dendrogram` 方法绘制 *Python* 中 `linkage` 函数的结果：
- en: '[PRE29]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The result is shown in [Figure 7-8](#DendogramStocks) (note that we are now
    plotting companies that are similar to one another, not days). The leaves of the
    tree correspond to the records. The length of the branch in the tree indicates
    the degree of dissimilarity between corresponding clusters. The returns for Google
    and Amazon are quite dissimilar to one another and to the returns for the other
    stocks. The oil stocks (SLB, CVX, XOM, COP) are in their own cluster, Apple (AAPL)
    is by itself, and the rest are similar to one another.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 结果显示在 [Figure 7-8](#DendogramStocks) 中（注意，我们现在绘制的是相互相似的公司，而不是日期）。树的叶子对应记录。树中分支的长度表示相应聚类之间的差异程度。谷歌和亚马逊的回报彼此及其他股票的回报非常不同。石油股票（SLB,
    CVX, XOM, COP）位于它们自己的聚类中，苹果（AAPL）独立成一类，其余的股票彼此相似。
- en: '![A dendrogram of stocks.](Images/psd2_0708.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![股票的树状图。](Images/psd2_0708.png)'
- en: Figure 7-8\. A dendrogram of stocks
  id: totrans-169
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-8\. 股票的树状图
- en: 'In contrast to *K*-means, it is not necessary to prespecify the number of clusters.
    Graphically, you can identify different numbers of clusters with a horizontal
    line that slides up or down; a cluster is defined wherever the horizontal line
    intersects the vertical lines. To extract a specific number of clusters, you can
    use the `cutree` function:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 与*K*-means相比，不需要预先指定聚类的数量。在图形上，你可以通过一个水平线上下移动来识别不同数量的聚类；聚类在水平线与垂直线交点处定义。要提取特定数量的聚类，可以使用
    `cutree` 函数：
- en: '[PRE30]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'In *Python*, you achieve the same with the `fcluster` method:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *Python* 中，你可以使用 `fcluster` 方法实现同样的效果：
- en: '[PRE31]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The number of clusters to extract is set to 4, and you can see that Google and
    Amazon each belong to their own cluster. The oil stocks all belong to another
    cluster. The remaining stocks are in the fourth cluster.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 需要提取的聚类数量设置为 4，你可以看到谷歌和亚马逊各自属于自己的一个聚类。所有的石油股票属于另一个聚类。剩下的股票在第四个聚类中。
- en: The Agglomerative Algorithm
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 凝聚算法
- en: The main algorithm for hierarchical clustering is the *agglomerative* algorithm,
    which iteratively merges similar clusters. The agglomerative algorithm begins
    with each record constituting its own single-record cluster and then builds up
    larger and larger clusters. The first step is to calculate distances between all
    pairs of records.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 层次聚类的主要算法是*凝聚*算法，它是通过迭代地合并相似的聚类来实现的。凝聚算法首先将每个记录视为自己单独的聚类，然后逐步构建越来越大的聚类。第一步是计算所有记录对之间的距离。
- en: 'For each pair of records <math alttext="left-parenthesis x 1 comma x 2 comma
    ellipsis comma x Subscript p Baseline right-parenthesis"><mrow><mo>(</mo> <msub><mi>x</mi>
    <mn>1</mn></msub> <mo>,</mo> <msub><mi>x</mi> <mn>2</mn></msub> <mo>,</mo> <mo>...</mo>
    <mo>,</mo> <msub><mi>x</mi> <mi>p</mi></msub> <mo>)</mo></mrow></math> and <math
    alttext="left-parenthesis y 1 comma y 2 comma ellipsis comma y Subscript p Baseline
    right-parenthesis"><mrow><mo>(</mo> <msub><mi>y</mi> <mn>1</mn></msub> <mo>,</mo>
    <msub><mi>y</mi> <mn>2</mn></msub> <mo>,</mo> <mo>...</mo> <mo>,</mo> <msub><mi>y</mi>
    <mi>p</mi></msub> <mo>)</mo></mrow></math> , we measure the distance between the
    two records, <math alttext="d Subscript x comma y"><msub><mi>d</mi> <mrow><mi>x</mi><mo>,</mo><mi>y</mi></mrow></msub></math>
    , using a distance metric (see [“Distance Metrics”](ch06.xhtml#DistanceMetrics)).
    For example, we can use Euclidian distance:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每对记录 <math alttext="left-parenthesis x 1 comma x 2 comma ellipsis comma x
    Subscript p Baseline right-parenthesis"><mrow><mo>(</mo> <msub><mi>x</mi> <mn>1</mn></msub>
    <mo>,</mo> <msub><mi>x</mi> <mn>2</mn></msub> <mo>,</mo> <mo>...</mo> <mo>,</mo>
    <msub><mi>x</mi> <mi>p</mi></msub> <mo>)</mo></mrow></math> 和 <math alttext="left-parenthesis
    y 1 comma y 2 comma ellipsis comma y Subscript p Baseline right-parenthesis"><mrow><mo>(</mo>
    <msub><mi>y</mi> <mn>1</mn></msub> <mo>,</mo> <msub><mi>y</mi> <mn>2</mn></msub>
    <mo>,</mo> <mo>...</mo> <mo>,</mo> <msub><mi>y</mi> <mi>p</mi></msub> <mo>)</mo></mrow></math>
    ，我们使用距离度量 <math alttext="d Subscript x comma y"><msub><mi>d</mi> <mrow><mi>x</mi><mo>,</mo><mi>y</mi></mrow></msub></math>
    来衡量这两个记录之间的距离（见 [“距离度量”](ch06.xhtml#DistanceMetrics)）。例如，我们可以使用欧氏距离：
- en: <math display="block"><mrow><mi>d</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>,</mo>
    <mi>y</mi> <mo>)</mo></mrow> <mo>=</mo> <msqrt><mrow><msup><mrow><mo>(</mo><msub><mi>x</mi>
    <mn>1</mn></msub> <mo>-</mo><msub><mi>y</mi> <mn>1</mn></msub> <mo>)</mo></mrow>
    <mn>2</mn></msup> <mo>+</mo> <msup><mrow><mo>(</mo><msub><mi>x</mi> <mn>2</mn></msub>
    <mo>-</mo><msub><mi>y</mi> <mn>2</mn></msub> <mo>)</mo></mrow> <mn>2</mn></msup>
    <mo>+</mo> <mo>⋯</mo> <mo>+</mo> <msup><mrow><mo>(</mo><msub><mi>x</mi> <mi>p</mi></msub>
    <mo>-</mo><msub><mi>y</mi> <mi>p</mi></msub> <mo>)</mo></mrow> <mn>2</mn></msup></mrow></msqrt></mrow></math>
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mi>d</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>,</mo>
    <mi>y</mi> <mo>)</mo></mrow> <mo>=</mo> <msqrt><mrow><msup><mrow><mo>(</mo><msub><mi>x</mi>
    <mn>1</mn></msub> <mo>-</mo><msub><mi>y</mi> <mn>1</mn></msub> <mo>)</mo></mrow>
    <mn>2</mn></msup> <mo>+</mo> <msup><mrow><mo>(</mo><msub><mi>x</mi> <mn>2</mn></msub>
    <mo>-</mo><msub><mi>y</mi> <mn>2</mn></msub> <mo>)</mo></mrow> <mn>2</mn></msup>
    <mo>+</mo> <mo>⋯</mo> <mo>+</mo> <msup><mrow><mo>(</mo><msub><mi>x</mi> <mi>p</mi></msub>
    <mo>-</mo><msub><mi>y</mi> <mi>p</mi></msub> <mo>)</mo></mrow> <mn>2</mn></msup></mrow></msqrt></mrow></math>
- en: We now turn to inter-cluster distance. Consider two clusters *A* and *B*, each
    with a distinctive set of records, <math alttext="upper A equals left-parenthesis
    a 1 comma a 2 comma ellipsis comma a Subscript m Baseline right-parenthesis"><mrow><mi>A</mi>
    <mo>=</mo> <mo>(</mo> <msub><mi>a</mi> <mn>1</mn></msub> <mo>,</mo> <msub><mi>a</mi>
    <mn>2</mn></msub> <mo>,</mo> <mo>...</mo> <mo>,</mo> <msub><mi>a</mi> <mi>m</mi></msub>
    <mo>)</mo></mrow></math> and <math alttext="upper B equals left-parenthesis b
    1 comma b 2 comma ellipsis comma b Subscript q Baseline right-parenthesis"><mrow><mi>B</mi>
    <mo>=</mo> <mo>(</mo> <msub><mi>b</mi> <mn>1</mn></msub> <mo>,</mo> <msub><mi>b</mi>
    <mn>2</mn></msub> <mo>,</mo> <mo>...</mo> <mo>,</mo> <msub><mi>b</mi> <mi>q</mi></msub>
    <mo>)</mo></mrow></math> . We can measure the dissimilarity between the clusters
    <math alttext="upper D left-parenthesis upper A comma upper B right-parenthesis"><mrow><mi>D</mi>
    <mo>(</mo> <mi>A</mi> <mo>,</mo> <mi>B</mi> <mo>)</mo></mrow></math> by using
    the distances between the members of *A* and the members of *B*.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们转向簇间距离。考虑两个簇 *A* 和 *B*，每个簇都有一组不同的记录，<math alttext="upper A equals left-parenthesis
    a 1 comma a 2 comma ellipsis comma a Subscript m Baseline right-parenthesis"><mrow><mi>A</mi>
    <mo>=</mo> <mo>(</mo> <msub><mi>a</mi> <mn>1</mn></msub> <mo>,</mo> <msub><mi>a</mi>
    <mn>2</mn></msub> <mo>,</mo> <mo>...</mo> <mo>,</mo> <msub><mi>a</mi> <mi>m</mi></msub>
    <mo>)</mo></mrow></math> 和 <math alttext="upper B equals left-parenthesis b 1
    comma b 2 comma ellipsis comma b Subscript q Baseline right-parenthesis"><mrow><mi>B</mi>
    <mo>=</mo> <mo>(</mo> <msub><mi>b</mi> <mn>1</mn></msub> <mo>,</mo> <msub><mi>b</mi>
    <mn>2</mn></msub> <mo>,</mo> <mo>...</mo> <mo>,</mo> <msub><mi>b</mi> <mi>q</mi></msub>
    <mo>)</mo></mrow></math> 。我们可以通过使用 *A* 的成员与 *B* 的成员之间的距离来衡量簇间的不相似性 <math alttext="upper
    D left-parenthesis upper A comma upper B right-parenthesis"><mrow><mi>D</mi> <mo>(</mo>
    <mi>A</mi> <mo>,</mo> <mi>B</mi> <mo>)</mo></mrow></math> 。
- en: 'One measure of dissimilarity is the *complete-linkage* method, which is the
    maximum distance across all pairs of records between *A* and *B*:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 一种衡量不相似性的方法是 *complete-linkage* 方法，它是 *A* 和 *B* 之间所有记录对之间的最大距离。
- en: <math display="block"><mrow><mi>D</mi> <mrow><mo>(</mo> <mi>A</mi> <mo>,</mo>
    <mi>B</mi> <mo>)</mo></mrow> <mo>=</mo> <mo movablelimits="true" form="prefix">max</mo>
    <mi>d</mi> <mrow><mo>(</mo> <msub><mi>a</mi> <mi>i</mi></msub> <mo>,</mo> <msub><mi>b</mi>
    <mi>j</mi></msub> <mo>)</mo></mrow> <mtext>for</mtext> <mtext>all</mtext> <mtext>pairs</mtext>
    <mi>i</mi> <mo>,</mo> <mi>j</mi></mrow></math>
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mi>D</mi> <mrow><mo>(</mo> <mi>A</mi> <mo>,</mo>
    <mi>B</mi> <mo>)</mo></mrow> <mo>=</mo> <mo movablelimits="true" form="prefix">max</mo>
    <mi>d</mi> <mrow><mo>(</mo> <msub><mi>a</mi> <mi>i</mi></msub> <mo>,</mo> <msub><mi>b</mi>
    <mi>j</mi></msub> <mo>)</mo></mrow> <mtext>for</mtext> <mtext>all</mtext> <mtext>pairs</mtext>
    <mi>i</mi> <mo>,</mo> <mi>j</mi></mrow></math>
- en: This defines the dissimilarity as the biggest difference between all pairs.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 这里定义了两两之间的最大差异作为不相似性的度量。
- en: 'The main steps of the agglomerative algorithm are:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 聚合算法的主要步骤包括：
- en: Create an initial set of clusters with each cluster consisting of a single record
    for all records in the data.
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个初始的簇集，其中每个簇由数据中的单个记录组成。
- en: Compute the dissimilarity <math alttext="upper D left-parenthesis upper C Subscript
    k Baseline comma upper C Subscript script l Baseline right-parenthesis"><mrow><mi>D</mi>
    <mo>(</mo> <msub><mi>C</mi> <mi>k</mi></msub> <mo>,</mo> <msub><mi>C</mi> <mi>ℓ</mi></msub>
    <mo>)</mo></mrow></math> between all pairs of clusters <math alttext="k comma
    script l"><mrow><mi>k</mi> <mo>,</mo> <mi>ℓ</mi></mrow></math> .
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算所有簇对 <math alttext="upper D left-parenthesis upper C Subscript k Baseline
    comma upper C Subscript script l Baseline right-parenthesis"><mrow><mi>D</mi>
    <mo>(</mo> <msub><mi>C</mi> <mi>k</mi></msub> <mo>,</mo> <msub><mi>C</mi> <mi>ℓ</mi></msub>
    <mo>)</mo></mrow></math> 之间的不相似性。
- en: Merge the two clusters <math alttext="upper C Subscript k"><msub><mi>C</mi>
    <mi>k</mi></msub></math> and <math alttext="upper C Subscript script l"><msub><mi>C</mi>
    <mi>ℓ</mi></msub></math> that are least dissimilar as measured by <math alttext="upper
    D left-parenthesis upper C Subscript k Baseline comma upper C Subscript script
    l Baseline right-parenthesis"><mrow><mi>D</mi> <mo>(</mo> <msub><mi>C</mi> <mi>k</mi></msub>
    <mo>,</mo> <msub><mi>C</mi> <mi>ℓ</mi></msub> <mo>)</mo></mrow></math> .
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 合并两个最不相似的群集 <math alttext="upper C Subscript k"><msub><mi>C</mi> <mi>k</mi></msub></math>
    和 <math alttext="upper C Subscript script l"><msub><mi>C</mi> <mi>ℓ</mi></msub></math>，其度量为
    <math alttext="upper D left-parenthesis upper C Subscript k Baseline comma upper
    C Subscript script l Baseline right-parenthesis"><mrow><mi>D</mi> <mo>(</mo> <msub><mi>C</mi>
    <mi>k</mi></msub> <mo>,</mo> <msub><mi>C</mi> <mi>ℓ</mi></msub> <mo>)</mo></mrow></math>
    。
- en: If we have more than one cluster remaining, return to step 2\. Otherwise, we
    are done.
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果有多个群集保留，请返回步骤2。否则，我们完成了。
- en: Measures of Dissimilarity
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 不相似度量
- en: 'There are four common measures of dissimilarity: *complete linkage*, *single
    linkage*, *average linkage*, and *minimum variance*. These (plus other measures)
    are all supported by most hierarchical clustering software, including `hclust`
    and `linkage`. The complete linkage method defined earlier tends to produce clusters
    with members that are similar. The single linkage method is the minimum distance
    between the records in two clusters:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 有四种常见的不相似度量：*完全链接*，*单链接*，*平均链接*和*最小方差*。这些（以及其他度量）都由大多数层次聚类软件支持，包括`hclust`和`linkage`。前文定义的完全链接方法倾向于产生成员相似的群集。单链接方法是两个群集中记录之间的最小距离：
- en: <math display="block"><mrow><mi>D</mi> <mrow><mo>(</mo> <mi>A</mi> <mo>,</mo>
    <mi>B</mi> <mo>)</mo></mrow> <mo>=</mo> <mo movablelimits="true" form="prefix">min</mo>
    <mi>d</mi> <mrow><mo>(</mo> <msub><mi>a</mi> <mi>i</mi></msub> <mo>,</mo> <msub><mi>b</mi>
    <mi>j</mi></msub> <mo>)</mo></mrow> <mtext>for</mtext> <mtext>all</mtext> <mtext>pairs</mtext>
    <mi>i</mi> <mo>,</mo> <mi>j</mi></mrow></math>
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mi>D</mi> <mrow><mo>(</mo> <mi>A</mi> <mo>,</mo>
    <mi>B</mi> <mo>)</mo></mrow> <mo>=</mo> <mo movablelimits="true" form="prefix">min</mo>
    <mi>d</mi> <mrow><mo>(</mo> <msub><mi>a</mi> <mi>i</mi></msub> <mo>,</mo> <msub><mi>b</mi>
    <mi>j</mi></msub> <mo>)</mo></mrow> <mtext>for</mtext> <mtext>all</mtext> <mtext>pairs</mtext>
    <mi>i</mi> <mo>,</mo> <mi>j</mi></mrow></math>
- en: This is a “greedy” method and produces clusters that can contain quite disparate
    elements. The average linkage method is the average of all distance pairs and
    represents a compromise between the single and complete linkage methods. Finally,
    the minimum variance method, also referred to as *Ward’s* method, is similar to
    *K*-means since it minimizes the within-cluster sum of squares (see [“K-Means
    Clustering”](#Kmeans)).
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种“贪婪”方法，生成的群集可能包含非常不同的元素。平均链接方法是所有距离对的平均值，代表了单链接方法和完全链接方法之间的折衷。最后，最小方差方法，也称为*Ward*方法，类似于*K*-均值，因为它最小化了群内平方和（见[“K均值聚类”](#Kmeans)）。
- en: '[Figure 7-9](#DissimilarityMeasures) applies hierarchical clustering using
    the four measures to the ExxonMobil and Chevron stock returns. For each measure,
    four clusters are retained.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '[图7-9](#DissimilarityMeasures)利用四种方法对埃克森美孚和雪佛龙的股票收益进行层次聚类。每种方法都保留了四个群集。'
- en: '![A comparison of measures of dissimilarity applied to stock returns; ExxonMobil
    on the x-axis and Chevron on the y-axis.](Images/psd2_0709.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![应用于股票收益的差异度量比较；x轴上是埃克森美孚，y轴上是雪佛龙。](Images/psd2_0709.png)'
- en: Figure 7-9\. A comparison of measures of dissimilarity applied to stock data
  id: totrans-194
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-9。应用于股票数据的差异度量的比较
- en: 'The results are strikingly different: the single linkage measure assigns almost
    all of the points to a single cluster. Except for the minimum variance method
    (*R*: `Ward.D`; *Python*: `ward`), all measures end up with at least one cluster
    with just a few outlying points. The minimum variance method is most similar to
    the *K*-means cluster; compare with [Figure 7-5](#KmeansStockData).'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 结果大不相同：单链接度量将几乎所有点分配到一个单一群集中。除了最小方差方法（*R*：`Ward.D`；*Python*：`ward`）之外，所有度量方法最终都至少有一个包含少数异常点的群集。最小方差方法与*K*-均值群集最为相似；与[图7-5](#KmeansStockData)比较。
- en: Model-Based Clustering
  id: totrans-196
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Model-Based Clustering
- en: Clustering methods such as hierarchical clustering and *K*-means are based on
    heuristics and rely primarily on finding clusters whose members are close to one
    another, as measured directly with the data (no probability model involved). In
    the past 20 years, significant effort has been devoted to developing *model-based
    clustering* methods. Adrian Raftery and other researchers at the University of
    Washington made critical contributions to model-based clustering, including both
    theory and software. The techniques are grounded in statistical theory and provide
    more rigorous ways to determine the nature and number of clusters. They could
    be used, for example, in cases where there might be one group of records that
    are similar to one another but not necessarily close to one another (e.g., tech
    stocks with high variance of returns), and another group of records that are similar
    and also close (e.g., utility stocks with low variance).
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 层次聚类和*K*-means等聚类方法是基于启发式方法的，主要依赖于找到彼此接近的簇，直接使用数据进行测量（不涉及概率模型）。在过去的20年里，人们已经投入了大量精力来开发基于模型的聚类方法。华盛顿大学的Adrian
    Raftery和其他研究人员在模型化聚类方面做出了重要贡献，包括理论和软件两方面。这些技术基于统计理论，并提供了更严谨的方法来确定簇的性质和数量。例如，在可能存在一组记录彼此相似但不一定彼此接近的情况下（例如，具有高收益方差的科技股），以及另一组记录既相似又接近的情况下（例如，波动性低的公用事业股），这些技术可以被使用。
- en: Multivariate Normal Distribution
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多元正态分布
- en: 'The most widely used model-based clustering methods rest on the *multivariate
    normal* distribution. The multivariate normal distribution is a generalization
    of the normal distribution to a set of *p* variables <math alttext="upper X 1
    comma upper X 2 comma ellipsis comma upper X Subscript p Baseline"><mrow><msub><mi>X</mi>
    <mn>1</mn></msub> <mo>,</mo> <msub><mi>X</mi> <mn>2</mn></msub> <mo>,</mo> <mo>...</mo>
    <mo>,</mo> <msub><mi>X</mi> <mi>p</mi></msub></mrow></math> . The distribution
    is defined by a set of means <math alttext="mu bold equals mu bold 1 bold comma
    mu bold 2 bold comma ellipsis bold comma mu Subscript bold p Baseline"><mrow><mi>μ</mi>
    <mo>=</mo> <msub><mi>μ</mi> <mn mathvariant="bold">1</mn></msub> <mo>,</mo> <msub><mi>μ</mi>
    <mn mathvariant="bold">2</mn></msub> <mo>,</mo> <mo>...</mo> <mo>,</mo> <msub><mi>μ</mi>
    <mi>𝐩</mi></msub></mrow></math> and a covariance matrix <math alttext="normal
    upper Sigma"><mi>Σ</mi></math> . The covariance matrix is a measure of how the
    variables correlate with each other (see [“Covariance Matrix”](ch05.xhtml#Covariance)
    for details on the covariance). The covariance matrix <math alttext="normal upper
    Sigma"><mi>Σ</mi></math> consists of *p* variances <math alttext="sigma 1 squared
    comma sigma 2 squared comma ellipsis comma sigma Subscript p Superscript 2"><mrow><msubsup><mi>σ</mi>
    <mn>1</mn> <mn>2</mn></msubsup> <mo>,</mo> <msubsup><mi>σ</mi> <mn>2</mn> <mn>2</mn></msubsup>
    <mo>,</mo> <mo>...</mo> <mo>,</mo> <msubsup><mi>σ</mi> <mi>p</mi> <mn>2</mn></msubsup></mrow></math>
    and covariances <math alttext="sigma Subscript i comma j"><msub><mi>σ</mi> <mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub></math>
    for all pairs of variables <math alttext="i not-equals j"><mrow><mi>i</mi> <mo>≠</mo>
    <mi>j</mi></mrow></math> . With the variables put along the rows and duplicated
    along the columns, the matrix looks like this:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 最广泛使用的基于模型的聚类方法基于*多元正态*分布。多元正态分布是正态分布对一组*p*个变量<math alttext="upper X 1 comma
    upper X 2 comma ellipsis comma upper X Subscript p Baseline"><mrow><msub><mi>X</mi>
    <mn>1</mn></msub> <mo>,</mo> <msub><mi>X</mi> <mn>2</mn></msub> <mo>,</mo> <mo>...</mo>
    <mo>,</mo> <msub><mi>X</mi> <mi>p</mi></msub></mrow></math>的泛化。该分布由一组均值<math alttext="mu
    bold equals mu bold 1 bold comma mu bold 2 bold comma ellipsis bold comma mu Subscript
    bold p Baseline"><mrow><mi>μ</mi> <mo>=</mo> <msub><mi>μ</mi> <mn mathvariant="bold">1</mn></msub>
    <mo>,</mo> <msub><mi>μ</mi> <mn mathvariant="bold">2</mn></msub> <mo>,</mo> <mo>...</mo>
    <mo>,</mo> <msub><mi>μ</mi> <mi>𝐩</mi></msub></mrow></math>和一个协方差矩阵<math alttext="normal
    upper Sigma"><mi>Σ</mi></math>定义。协方差矩阵是变量之间相关程度的度量（有关协方差的详细信息，请参阅[“协方差矩阵”](ch05.xhtml#Covariance)）。协方差矩阵<math
    alttext="normal upper Sigma"><mi>Σ</mi></math>由*p*个方差<math alttext="sigma 1 squared
    comma sigma 2 squared comma ellipsis comma sigma Subscript p Superscript 2"><mrow><msubsup><mi>σ</mi>
    <mn>1</mn> <mn>2</mn></msubsup> <mo>,</mo> <msubsup><mi>σ</mi> <mn>2</mn> <mn>2</mn></msubsup>
    <mo>,</mo> <mo>...</mo> <mo>,</mo> <msubsup><mi>σ</mi> <mi>p</mi> <mn>2</mn></msubsup></mrow></math>和所有变量对<math
    alttext="sigma Subscript i comma j"><msub><mi>σ</mi> <mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub></math>的协方差组成。将变量放在行上并复制到列上，矩阵看起来像这样：
- en: <math display="block"><mrow><mi>Σ</mi> <mo>=</mo> <mfenced open="[" close="]"><mtable><mtr><mtd><msubsup><mi>σ</mi>
    <mn>1</mn> <mn>2</mn></msubsup></mtd> <mtd><msub><mi>σ</mi> <mrow><mn>1</mn><mo>,</mo><mn>2</mn></mrow></msub></mtd>
    <mtd><mo>⋯</mo></mtd> <mtd><msub><mi>σ</mi> <mrow><mn>1</mn><mo>,</mo><mi>p</mi></mrow></msub></mtd></mtr>
    <mtr><mtd><msub><mi>σ</mi> <mrow><mn>2</mn><mo>,</mo><mn>1</mn></mrow></msub></mtd>
    <mtd><msubsup><mi>σ</mi> <mrow><mn>2</mn></mrow> <mn>2</mn></msubsup></mtd> <mtd><mo>⋯</mo></mtd>
    <mtd><msub><mi>σ</mi> <mrow><mn>2</mn><mo>,</mo><mi>p</mi></mrow></msub></mtd></mtr>
    <mtr><mtd><mo>⋮</mo></mtd> <mtd><mo>⋮</mo></mtd> <mtd><mo>⋱</mo></mtd> <mtd><mo>⋮</mo></mtd></mtr>
    <mtr><mtd><msub><mi>σ</mi> <mrow><mi>p</mi><mo>,</mo><mn>1</mn></mrow></msub></mtd>
    <mtd><msubsup><mi>σ</mi> <mrow><mi>p</mi><mo>,</mo><mn>2</mn></mrow> <mn>2</mn></msubsup></mtd>
    <mtd><mo>⋯</mo></mtd> <mtd><msubsup><mi>σ</mi> <mrow><mi>p</mi></mrow> <mn>2</mn></msubsup></mtd></mtr></mtable></mfenced></mrow></math>
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mi>Σ</mi> <mo>=</mo> <mfenced open="[" close="]"><mtable><mtr><mtd><msubsup><mi>σ</mi>
    <mn>1</mn> <mn>2</mn></msubsup></mtd> <mtd><msub><mi>σ</mi> <mrow><mn>1</mn><mo>,</mo><mn>2</mn></mrow></msub></mtd>
    <mtd><mo>⋯</mo></mtd> <mtd><msub><mi>σ</mi> <mrow><mn>1</mn><mo>,</mo><mi>p</mi></mrow></msub></mtd></mtr>
    <mtr><mtd><msub><mi>σ</mi> <mrow><mn>2</mn><mo>,</mo><mn>1</mn></mrow></msub></mtd>
    <mtd><msubsup><mi>σ</mi> <mrow><mn>2</mn></mrow> <mn>2</mn></msubsup></mtd> <mtd><mo>⋯</mo></mtd>
    <mtd><msub><mi>σ</mi> <mrow><mn>2</mn><mo>,</mo><mi>p</mi></mrow></msub></mtd></mtr>
    <mtr><mtd><mo>⋮</mo></mtd> <mtd><mo>⋮</mo></mtd> <mtd><mo>⋱</mo></mtd> <mtd><mo>⋮</mo></mtd></mtr>
    <mtr><mtd><msub><mi>σ</mi> <mrow><mi>p</mi><mo>,</mo><mn>1</mn></mrow></msub></mtd>
    <mtd><msubsup><mi>σ</mi> <mrow><mi>p</mi><mo>,</mo><mn>2</mn></mrow> <mn>2</mn></msubsup></mtd>
    <mtd><mo>⋯</mo></mtd> <mtd><msubsup><mi>σ</mi> <mrow><mi>p</mi></mrow> <mn>2</mn></msubsup></mtd></mtr></mtable></mfenced></mrow></math>
- en: 'Note that the covariance matrix is symmetric around the diagonal from upper
    left to lower right. Since <math alttext="sigma Subscript i comma j Baseline equals
    sigma Subscript j comma i"><mrow><msub><mi>σ</mi> <mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub>
    <mo>=</mo> <msub><mi>σ</mi> <mrow><mi>j</mi><mo>,</mo><mi>i</mi></mrow></msub></mrow></math>
    , there are only <math alttext="left-parenthesis p times left-parenthesis p minus
    1 right-parenthesis right-parenthesis slash 2"><mrow><mo>(</mo> <mi>p</mi> <mo>×</mo>
    <mo>(</mo> <mi>p</mi> <mo>-</mo> <mn>1</mn> <mo>)</mo> <mo>)</mo> <mo>/</mo> <mn>2</mn></mrow></math>
    covariance terms. In total, the covariance matrix has <math alttext="left-parenthesis
    p times left-parenthesis p minus 1 right-parenthesis right-parenthesis slash 2
    plus p"><mrow><mo>(</mo> <mi>p</mi> <mo>×</mo> <mo>(</mo> <mi>p</mi> <mo>-</mo>
    <mn>1</mn> <mo>)</mo> <mo>)</mo> <mo>/</mo> <mn>2</mn> <mo>+</mo> <mi>p</mi></mrow></math>
    parameters. The distribution is denoted by:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 注意协方差矩阵在从左上到右下的对角线周围是对称的。由于 <math alttext="sigma Subscript i comma j Baseline
    equals sigma Subscript j comma i"><mrow><msub><mi>σ</mi> <mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub>
    <mo>=</mo> <msub><mi>σ</mi> <mrow><mi>j</mi><mo>,</mo><mi>i</mi></mrow></msub></mrow></math>
    ，只有 <math alttext="left-parenthesis p times left-parenthesis p minus 1 right-parenthesis
    right-parenthesis slash 2"><mrow><mo>(</mo> <mi>p</mi> <mo>×</mo> <mo>(</mo> <mi>p</mi>
    <mo>-</mo> <mn>1</mn> <mo>)</mo> <mo>)</mo> <mo>/</mo> <mn>2</mn></mrow></math>
    个协方差项。总之，协方差矩阵有 <math alttext="left-parenthesis p times left-parenthesis p minus
    1 right-parenthesis right-parenthesis slash 2 plus p"><mrow><mo>(</mo> <mi>p</mi>
    <mo>×</mo> <mo>(</mo> <mi>p</mi> <mo>-</mo> <mn>1</mn> <mo>)</mo> <mo>)</mo> <mo>/</mo>
    <mn>2</mn> <mo>+</mo> <mi>p</mi></mrow></math> 个参数。分布表示为：
- en: <math display="block"><mrow><mrow><mo>(</mo> <msub><mi>X</mi> <mn>1</mn></msub>
    <mo>,</mo> <msub><mi>X</mi> <mn>2</mn></msub> <mo>,</mo> <mo>...</mo> <mo>,</mo>
    <msub><mi>X</mi> <mi>p</mi></msub> <mo>)</mo></mrow> <mo>∼</mo> <msub><mi>N</mi>
    <mi>p</mi></msub> <mrow><mo>(</mo> <mi>μ</mi> <mo>,</mo> <mi>Σ</mi> <mo>)</mo></mrow></mrow></math>
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mrow><mo>(</mo> <msub><mi>X</mi> <mn>1</mn></msub>
    <mo>,</mo> <msub><mi>X</mi> <mn>2</mn></msub> <mo>,</mo> <mo>...</mo> <mo>,</mo>
    <msub><mi>X</mi> <mi>p</mi></msub> <mo>)</mo></mrow> <mo>∼</mo> <msub><mi>N</mi>
    <mi>p</mi></msub> <mrow><mo>(</mo> <mi>μ</mi> <mo>,</mo> <mi>Σ</mi> <mo>)</mo></mrow></mrow></math>
- en: This is a symbolic way of saying that the variables are all normally distributed,
    and the overall distribution is fully described by the vector of variable means
    and the covariance matrix.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种符号化的表达方式，表明所有变量都服从正态分布，整体分布由变量均值向量和协方差矩阵完全描述。
- en: '[Figure 7-10](#Normal2d) shows the probability contours for a multivariate
    normal distribution for two variables *X* and *Y* (the 0.5 probability contour,
    for example, contains 50% of the distribution).'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 7-10](#Normal2d) 显示了两个变量 *X* 和 *Y* 的多变量正态分布的概率轮廓（例如，0.5 概率轮廓包含分布的 50%）。'
- en: 'The means are <math><mrow><msub><mi>μ</mi> <mi>x</mi></msub> <mo>=</mo> <mn>0</mn>
    <mo>.</mo> <mn>5</mn></mrow></math> and <math><mrow><msub><mi>μ</mi> <mi>y</mi></msub>
    <mo>=</mo> <mo>-</mo> <mn>0</mn> <mo>.</mo> <mn>5</mn></mrow></math> , and the
    covariance matrix is:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 均值分别为 <math><mrow><msub><mi>μ</mi> <mi>x</mi></msub> <mo>=</mo> <mn>0</mn> <mo>.</mo>
    <mn>5</mn></mrow></math> 和 <math><mrow><msub><mi>μ</mi> <mi>y</mi></msub> <mo>=</mo>
    <mo>-</mo> <mn>0</mn> <mo>.</mo> <mn>5</mn></mrow></math> ，协方差矩阵为：
- en: <math display="block"><mrow><mi>Σ</mi> <mo>=</mo> <mfenced separators="" open="["
    close="]"><mtable><mtr><mtd><mn>1</mn></mtd> <mtd><mn>1</mn></mtd></mtr> <mtr><mtd><mn>1</mn></mtd>
    <mtd><mn>2</mn></mtd></mtr></mtable></mfenced></mrow></math>
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mi>Σ</mi> <mo>=</mo> <mfenced separators="" open="["
    close="]"><mtable><mtr><mtd><mn>1</mn></mtd> <mtd><mn>1</mn></mtd></mtr> <mtr><mtd><mn>1</mn></mtd>
    <mtd><mn>2</mn></mtd></mtr></mtable></mfenced></mrow></math>
- en: Since the covariance <math alttext="sigma Subscript x y"><msub><mi>σ</mi> <mrow><mi>x</mi><mi>y</mi></mrow></msub></math>
    is positive, *X* and *Y* are positively correlated.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 因为协方差 <math alttext="sigma Subscript x y"><msub><mi>σ</mi> <mrow><mi>x</mi><mi>y</mi></mrow></msub></math>
    是正的，*X* 和 *Y* 是正相关的。
- en: '![images/2d_normal.png](Images/psd2_0710.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![images/2d_normal.png](Images/psd2_0710.png)'
- en: Figure 7-10\. Probability contours for a two-dimensional normal distribution
  id: totrans-209
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-10\. 两维正态分布的概率轮廓
- en: Mixtures of Normals
  id: totrans-210
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 混合正态分布
- en: The key idea behind model-based clustering is that each record is assumed to
    be distributed as one of *K* multivariate normal distributions, where *K* is the
    number of clusters. Each distribution has a different mean <math alttext="mu"><mi>μ</mi></math>
    and covariance matrix <math alttext="normal upper Sigma"><mi>Σ</mi></math> . For
    example, if you have two variables, *X* and *Y*, then each row <math alttext="left-parenthesis
    upper X Subscript i Baseline comma upper Y Subscript i Baseline right-parenthesis"><mrow><mo>(</mo>
    <msub><mi>X</mi> <mi>i</mi></msub> <mo>,</mo> <msub><mi>Y</mi> <mi>i</mi></msub>
    <mo>)</mo></mrow></math> is modeled as having been sampled from one of *K* multivariate
    normal distributions <math alttext="upper N left-parenthesis mu 1 comma normal
    upper Sigma 1 right-parenthesis comma upper N left-parenthesis mu 2 comma normal
    upper Sigma 2 right-parenthesis comma ellipsis comma upper N left-parenthesis
    mu Subscript upper K Baseline comma normal upper Sigma Subscript upper K Baseline
    right-parenthesis"><mrow><mi>N</mi> <mrow><mo>(</mo> <msub><mi>μ</mi> <mn>1</mn></msub>
    <mo>,</mo> <msub><mi>Σ</mi> <mn>1</mn></msub> <mo>)</mo></mrow> <mo>,</mo> <mi>N</mi>
    <mrow><mo>(</mo> <msub><mi>μ</mi> <mn>2</mn></msub> <mo>,</mo> <msub><mi>Σ</mi>
    <mn>2</mn></msub> <mo>)</mo></mrow> <mo>,</mo> <mo>...</mo> <mo>,</mo> <mi>N</mi>
    <mrow><mo>(</mo> <msub><mi>μ</mi> <mi>K</mi></msub> <mo>,</mo> <msub><mi>Σ</mi>
    <mi>K</mi></msub> <mo>)</mo></mrow></mrow></math> .
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 基于模型的聚类背后的关键思想是，假设每个记录都分布在*K*个多元正态分布之一中，其中*K*是聚类的数量。每个分布都有不同的均值<math alttext="mu"><mi>μ</mi></math>和协方差矩阵<math
    alttext="normal upper Sigma"><mi>Σ</mi></math>。例如，如果您有两个变量*X*和*Y*，那么每一行<math alttext="left-parenthesis
    upper X Subscript i Baseline comma upper Y Subscript i Baseline right-parenthesis"><mrow><mo>(</mo>
    <msub><mi>X</mi> <mi>i</mi></msub> <mo>,</mo> <msub><mi>Y</mi> <mi>i</mi></msub>
    <mo>)</mo></mrow></math>被建模为从*K*个多元正态分布<math alttext="upper N left-parenthesis
    mu 1 comma normal upper Sigma 1 right-parenthesis comma upper N left-parenthesis
    mu 2 comma normal upper Sigma 2 right-parenthesis comma ellipsis comma upper N
    left-parenthesis mu Subscript upper K Baseline comma normal upper Sigma Subscript
    upper K Baseline right-parenthesis"><mrow><mi>N</mi> <mrow><mo>(</mo> <msub><mi>μ</mi>
    <mn>1</mn></msub> <mo>,</mo> <msub><mi>Σ</mi> <mn>1</mn></msub> <mo>)</mo></mrow>
    <mo>,</mo> <mi>N</mi> <mrow><mo>(</mo> <msub><mi>μ</mi> <mn>2</mn></msub> <mo>,</mo>
    <msub><mi>Σ</mi> <mn>2</mn></msub> <mo>)</mo></mrow> <mo>,</mo> <mo>...</mo> <mo>,</mo>
    <mi>N</mi> <mrow><mo>(</mo> <msub><mi>μ</mi> <mi>K</mi></msub> <mo>,</mo> <msub><mi>Σ</mi>
    <mi>K</mi></msub> <mo>)</mo></mrow></mrow></math>中的一个中抽样。
- en: '*R* has a very rich package for model-based clustering called `mclust`, originally
    developed by Chris Fraley and Adrian Raftery. With this package, we can apply
    model-based clustering to the stock return data we previously analyzed using *K*-means
    and hierarchical clustering:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '*R*有一个非常丰富的基于模型的聚类包`mclust`，最初由Chris Fraley和Adrian Raftery开发。使用这个包，我们可以将模型-based聚类应用到之前使用*K*-means和层次聚类分析过的股票收益数据中：'
- en: '[PRE32]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '`scikit-learn` has the `sklearn.mixture.GaussianMixture` class for model-based
    clustering:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '`scikit-learn`有`sklearn.mixture.GaussianMixture`类来进行基于模型的聚类：'
- en: '[PRE33]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'If you execute this code, you will notice that the computation takes significantly
    longer than other procedures. Extracting the cluster assignments using the `predict`
    function, we can visualize the clusters:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您执行此代码，您会注意到计算时间明显长于其他程序。使用`predict`函数提取集群分配，我们可以可视化这些聚类：
- en: '[PRE34]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Here is the *Python* code to create a similar figure:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 这是创建类似图形的*Python*代码：
- en: '[PRE35]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The resulting plot is shown in [Figure 7-11](#StockMclust). There are two clusters:
    one cluster in the middle of the data, and a second cluster in the outer edge
    of the data. This is very different from the clusters obtained using *K*-means
    ([Figure 7-5](#KmeansStockData)) and hierarchical clustering ([Figure 7-9](#DissimilarityMeasures)),
    which find clusters that are compact.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 结果图显示在[图7-11](#StockMclust)中。有两个聚类：一个在数据中间，另一个在数据外围。这与使用*K*-means（[图7-5](#KmeansStockData)）和层次聚类（[图7-9](#DissimilarityMeasures)）获得的紧凑聚类非常不同。
- en: '![Two clusters are obtained for stock return data using +Mclust+.](Images/psd2_0711.png)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![使用+Mclust+获得股票收益数据的两个聚类](Images/psd2_0711.png)'
- en: Figure 7-11\. Two clusters are obtained for stock return data using `mclust`
  id: totrans-222
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-11\. 使用`mclust`获得股票收益数据的两个聚类
- en: 'You can extract the parameters to the normal distributions using the `summary`
    function:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用`summary`函数提取正态分布的参数：
- en: '[PRE36]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'In *Python*, you get this information from the `means_` and `covariances_`
    properties of the result:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在*Python*中，您可以从结果的`means_`和`covariances_`属性中获取此信息：
- en: '[PRE37]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: The distributions have similar means and correlations, but the second distribution
    has much larger variances and covariances. Due to the randomness of the algorithm,
    results can vary slightly between different runs.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 这些分布具有类似的均值和相关性，但第二个分布具有更大的方差和协方差。由于算法的随机性，结果在不同运行之间可能略有不同。
- en: 'The clusters from `mclust` may seem surprising, but in fact, they illustrate
    the statistical nature of the method. The goal of model-based clustering is to
    find the best-fitting set of multivariate normal distributions. The stock data
    appears to have a normal-looking shape: see the contours of [Figure 7-10](#Normal2d).
    In fact, though, stock returns have a longer-tailed distribution than a normal
    distribution. To handle this, `mclust` fits a distribution to the bulk of the
    data but then fits a second distribution with a bigger variance.'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`mclust`生成的聚类可能看起来令人惊讶，但实际上，它展示了该方法的统计特性。基于模型的聚类的目标是找到最佳的多元正态分布集合。股票数据看起来具有正态分布的形状：请参见[图 7-10](#Normal2d)的轮廓。然而，事实上，股票回报比正态分布具有更长的尾部分布。为了处理这一点，`mclust`对大部分数据拟合一个分布，然后再拟合一个方差较大的第二个分布。
- en: Selecting the Number of Clusters
  id: totrans-229
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择聚类数
- en: Unlike *K*-means and hierarchical clustering, `mclust` automatically selects
    the number of clusters in *R* (in this case, two). It does this by choosing the
    number of clusters for which the *Bayesian Information Criteria* (*BIC*) has the
    largest value (BIC is similar to AIC; see [“Model Selection and Stepwise Regression”](ch04.xhtml#StepwiseRegression)).
    BIC works by selecting the best-fitting model with a penalty for the number of
    parameters in the model. In the case of model-based clustering, adding more clusters
    will always improve the fit at the expense of introducing additional parameters
    in the model.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 与*K*-means和层次聚类不同，`mclust`在*R*中（本例中为两个）自动选择聚类数。它通过选择BIC值最大的聚类数来完成此操作（BIC类似于AIC；请参见[“模型选择和逐步回归”](ch04.xhtml#StepwiseRegression)）。BIC通过选择最适合的模型来平衡模型中参数数量的惩罚。在基于模型的聚类中，增加更多的聚类始终会改善拟合，但会引入更多的模型参数。
- en: Warning
  id: totrans-231
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Note that in most cases BIC is usually minimized. The authors of the `mclust`
    package decided to define BIC to have the opposite sign to make interpretation
    of plots easier.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在大多数情况下，BIC通常被最小化。`mclust`包的作者决定将BIC定义为相反的符号，以便更容易地解释图表。
- en: '`mclust` fits 14 different models with increasing number of components and
    chooses an optimal model automatically. You can plot the BIC values of these models
    using a function in `mclust`:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '`mclust`拟合了14种不同的模型，并随着成分数量的增加自动选择了一个最优模型。您可以使用`mclust`中的一个函数绘制这些模型的BIC值：'
- en: '[PRE38]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: The number of clusters—or number of different multivariate normal models (components)—is
    shown on the x-axis (see [Figure 7-12](#MclustBIC)).
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类数或不同多元正态模型（成分）的数量显示在x轴上（请参见[图 7-12](#MclustBIC)）。
- en: '![BIC values for 14 models of the stock return data with increasing numbers
    of components.](Images/psd2_0712.png)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![股票回报数据的14种模型的BIC值随成分数量增加而变化。](Images/psd2_0712.png)'
- en: Figure 7-12\. BIC values for 14 models of the stock return data with increasing
    numbers of components
  id: totrans-237
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-12. 股票回报数据的14种模型的BIC值随成分数量增加而变化
- en: The `GaussianMixture` implementation on the other hand will not try out various
    combinations. As shown here, it is straightforward to run multiple combinations
    using *Python*. This implementation defines BIC as usual. Therefore, the calculated
    BIC value will be positive, and we need to minimize it.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，`GaussianMixture`的实现不会尝试各种组合。如所示，使用*Python*可以轻松运行多种组合。该实现按照通常的方式定义BIC。因此，计算出的BIC值将为正数，我们需要将其最小化。
- en: '[PRE39]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '[![1](Images/1.png)](#co_unsupervised_learning_CO1-1)'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_unsupervised_learning_CO1-1)'
- en: With the `warm_start` argument, the calculation will reuse information from
    the previous fit. This will speed up the convergence of subsequent calculations.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`warm_start`参数，计算将重用上一次拟合的信息。这将加快后续计算的收敛速度。
- en: This plot is similar to the elbow plot used to identify the number of clusters
    to choose for *K*-means, except the value being plotted is BIC instead of percent
    of variance explained (see [Figure 7-7](#KmeansElbowMethod)). One big difference
    is that instead of one line, `mclust` shows 14 different lines! This is because
    `mclust` is actually fitting 14 different models for each cluster size, and ultimately
    it chooses the best-fitting model. `GaussianMixture` implements fewer approaches,
    so the number of lines will be only four.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 该图类似于用于确定选择*K*-means中的聚类数的弯曲图，但所绘制的值是BIC而不是解释方差的百分比（参见[图 7-7](#KmeansElbowMethod)）。一个显著的差异是，`mclust`
    显示了14条不同的线！这是因为 `mclust` 实际上为每个聚类大小拟合了14种不同的模型，并最终选择最适合的模型。`GaussianMixture` 实现的方法较少，因此线的数量只有四条。
- en: Why does `mclust` fit so many models to determine the best set of multivariate
    normals? It’s because there are different ways to parameterize the covariance
    matrix <math alttext="normal upper Sigma"><mi>Σ</mi></math> for fitting a model.
    For the most part, you do not need to worry about the details of the models and
    can simply use the model chosen by `mclust`. In this example, according to BIC,
    three different models (called VEE, VEV, and VVE) give the best fit using two
    components.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '`mclust` 为什么要适配这么多模型来确定最佳的多变量正态分布集？这是因为有多种方法来为拟合模型参数化协方差矩阵 <math alttext="normal
    upper Sigma"><mi>Σ</mi></math>。在大多数情况下，您无需担心模型的细节，可以简单地使用`mclust`选择的模型。在本例中，根据BIC，三种不同的模型（称为VEE、VEV和VVE）使用两个分量给出最佳拟合。'
- en: Note
  id: totrans-244
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Model-based clustering is a rich and rapidly developing area of study, and the
    coverage in this text spans only a small part of the field. Indeed, the `mclust`
    help file is currently 154 pages long. Navigating the nuances of model-based clustering
    is probably more effort than is needed for most problems encountered by data scientists.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 基于模型的聚类是一个丰富且快速发展的研究领域，而本文中的覆盖范围仅涉及该领域的一小部分。事实上，`mclust` 的帮助文件目前长达154页。理解模型基础聚类的微妙之处可能比大多数数据科学家遇到的问题所需的工作还要多。
- en: Model-based clustering techniques do have some limitations. The methods require
    an underlying assumption of a model for the data, and the cluster results are
    very dependent on that assumption. The computations requirements are higher than
    even hierarchical clustering, making it difficult to scale to large data. Finally,
    the algorithm is more sophisticated and less accessible than that of other methods.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 基于模型的聚类技术确实具有一些局限性。这些方法需要对数据的模型假设，而聚类结果非常依赖于该假设。计算要求甚至比层次聚类还要高，使其难以扩展到大数据。最后，该算法比其他方法更复杂，不易访问。
- en: Further Reading
  id: totrans-247
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: For more detail on model-based clustering, see the [`mclust`](https://oreil.ly/bHDvR)
    and [`GaussianMixture`](https://oreil.ly/GaVVv) documentation.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 欲了解更多关于基于模型聚类的详情，请参阅[`mclust`](https://oreil.ly/bHDvR) 和 [`GaussianMixture`](https://oreil.ly/GaVVv)
    文档。
- en: Scaling and Categorical Variables
  id: totrans-249
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 缩放和分类变量
- en: Unsupervised learning techniques generally require that the data be appropriately
    scaled. This is different from many of the techniques for regression and classification
    in which scaling is not important (an exception is *K*-Nearest Neighbors; see
    [“K-Nearest Neighbors”](ch06.xhtml#KNN)).
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习技术通常要求数据适当缩放。这与许多回归和分类技术不同，这些技术中缩放并不重要（一个例外是*K*-最近邻算法；参见[“K-最近邻”](ch06.xhtml#KNN)）。
- en: For example, with the personal loan data, the variables have widely different
    units and magnitude. Some variables have relatively small values (e.g., number
    of years employed), while others have very large values (e.g., loan amount in
    dollars). If the data is not scaled, then the PCA, *K*-means, and other clustering
    methods will be dominated by the variables with large values and ignore the variables
    with small values.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，对于个人贷款数据，变量具有非常不同的单位和数量级。一些变量具有相对较小的值（例如，就业年限），而其他变量具有非常大的值（例如，以美元计的贷款金额）。如果数据未经缩放，则PCA、*K*-means和其他聚类方法将由具有大值的变量主导，并忽略具有小值的变量。
- en: Categorical data can pose a special problem for some clustering procedures.
    As with *K*-Nearest Neighbors, unordered factor variables are generally converted
    to a set of binary (0/1) variables using one hot encoding (see [“One Hot Encoder”](ch06.xhtml#OneHotEncoder)).
    Not only are the binary variables likely on a different scale from other data,
    but the fact that binary variables have only two values can prove problematic
    with techniques such as PCA and *K*-means.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 对于某些聚类过程，分类数据可能会带来特殊问题。与*K*最近邻算法一样，无序因子变量通常会使用独热编码转换为一组二进制（0/1）变量（有关“一热编码器”的更多信息，请参见[“一热编码器”](ch06.xhtml#OneHotEncoder)）。不仅二进制变量可能与其他数据不同尺度，而且二进制变量仅有两个值的事实可能会在PCA和*K*-means等技术中带来问题。
- en: Scaling the Variables
  id: totrans-253
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 缩放变量
- en: 'Variables with very different scale and units need to be normalized appropriately
    before you apply a clustering procedure. For example, let’s apply `kmeans` to
    a set of data of loan defaults without normalizing:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 变量的尺度和单位差异很大，在应用聚类过程之前需要适当进行标准化。例如，我们来看一下没有进行标准化的贷款违约数据的`kmeans`应用情况：
- en: '[PRE40]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Here is the corresponding *Python* code:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是相应的*Python*代码：
- en: '[PRE41]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: The variables `annual_inc` and `revol_bal` dominate the clusters, and the clusters
    have very different sizes. Cluster 1 has only 52 members with comparatively high
    income and revolving credit balance.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 变量`annual_inc`和`revol_bal`主导了聚类，而且聚类大小差异很大。聚类1只有52名成员，收入相对较高且循环信贷余额也较高。
- en: 'A common approach to scaling the variables is to convert them to *z*-scores
    by subtracting the mean and dividing by the standard deviation. This is termed
    *standardization* or *normalization* (see [“Standardization (Normalization, z-Scores)”](ch06.xhtml#Standardization)
    for more discussion about using *z*-scores):'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 缩放变量的常见方法是通过减去均值并除以标准差来转换它们为*z*-分数。这称为*标准化*或*归一化*（有关使用*z*-分数的更多讨论，请参见[“标准化（归一化，z-分数）”](ch06.xhtml#Standardization)）：
- en: <math display="block"><mrow><mi>z</mi> <mo>=</mo> <mfrac><mrow><mi>x</mi><mo>-</mo><mover
    accent="true"><mi>x</mi> <mo>¯</mo></mover></mrow> <mi>s</mi></mfrac></mrow></math>
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mi>z</mi> <mo>=</mo> <mfrac><mrow><mi>x</mi><mo>-</mo><mover
    accent="true"><mi>x</mi> <mo>¯</mo></mover></mrow> <mi>s</mi></mfrac></mrow></math>
- en: 'See what happens to the clusters when `kmeans` is applied to the normalized
    data:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 查看当`kmeans`应用于标准化数据时，聚类发生了什么变化：
- en: '[PRE42]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'In *Python*, we can use `scikit-learn`’s `StandardScaler`. The `inverse_transform`
    method allows converting the cluster centers back to the original scale:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 在*Python*中，我们可以使用`scikit-learn`的`StandardScaler`。`inverse_transform`方法允许将聚类中心转换回原始尺度：
- en: '[PRE43]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: The cluster sizes are more balanced, and the clusters are not dominated by `annual_inc`
    and `revol_bal`, revealing more interesting structure in the data. Note that the
    centers are rescaled to the original units in the preceding code. If we had left
    them unscaled, the resulting values would be in terms of *z*-scores and would
    therefore be less interpretable.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类大小更加平衡，聚类不再由`annual_inc`和`revol_bal`主导，数据中显示出更多有趣的结构。请注意，在前面的代码中，中心被重新缩放到原始单位。如果我们没有进行缩放，结果值将以*z*-分数的形式呈现，因此解释性会降低。
- en: Note
  id: totrans-266
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Scaling is also important for PCA. Using the *z*-scores is equivalent to using
    the correlation matrix (see [“Correlation”](ch01.xhtml#Correlations)) instead
    of the covariance matrix in computing the principal components. Software to compute
    PCA usually has an option to use the correlation matrix (in *R*, the `princomp`
    function has the argument `cor`).
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: PCA也需要缩放。使用*z*-分数相当于在计算主成分时使用相关矩阵（有关“相关性”请参见[“相关性”](ch01.xhtml#Correlations)），而不是协方差矩阵。通常，用于计算PCA的软件通常有使用相关矩阵的选项（在*R*中，`princomp`函数具有`cor`参数）。
- en: Dominant Variables
  id: totrans-268
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 主导变量
- en: Even in cases where the variables are measured on the same scale and accurately
    reflect relative importance (e.g., movement to stock prices), it can sometimes
    be useful to rescale the variables.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 即使变量在同一尺度上测量并准确反映了相对重要性（例如股票价格的变动），有时重新缩放变量也可能很有用。
- en: 'Suppose we add Google (GOOGL) and Amazon (AMZN) to the analysis in [“Interpreting
    Principal Components”](#InterpretPCA). We see how this is done in *R* below:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们在[“解释主成分”](#InterpretPCA)中增加了Google（GOOGL）和Amazon（AMZN）的分析。我们来看一下下面*R*中是如何实现的：
- en: '[PRE44]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'In *Python*, we get the screeplot as follows:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 在*Python*中，我们得到的screeplot如下：
- en: '[PRE45]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'The screeplot displays the variances for the top principal components. In this
    case, the screeplot in [Figure 7-13](#Screeplot1) reveals that the variances of
    the first and second components are much larger than the others. This often indicates
    that one or two variables dominate the loadings. This is, indeed, the case in
    this example:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: screeplot显示了顶级主成分的方差。在这种情况下，图7-13中的screeplot显示，第一和第二主成分的方差远大于其他成分。这通常表明一个或两个变量主导了载荷。这确实是这个例子的情况：
- en: '[PRE46]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'In *Python*, we use the following:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *Python* 中，我们使用以下方法：
- en: '[PRE47]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: The first two principal components are almost completely dominated by GOOGL
    and AMZN. This is because the stock price movements of GOOGL and AMZN dominate
    the variability.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 前两个主成分几乎完全由GOOGL和AMZN主导。这是因为GOOGL和AMZN的股价波动主导了变异性。
- en: To handle this situation, you can either include them as is, rescale the variables
    (see [“Scaling the Variables”](#ScalingPCA)), or exclude the dominant variables
    from the analysis and handle them separately. There is no “correct” approach,
    and the treatment depends on the application.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 处理这种情况时，可以选择将它们保留原样，重新缩放变量（参见[“缩放变量”](#ScalingPCA)），或者将主导变量从分析中排除并单独处理。没有“正确”的方法，处理方法取决于具体应用。
- en: '![A screeplot for a PCA of top stocks from the SP 500, including GOOGL and
    AMZN.](Images/psd2_0713.png)'
  id: totrans-280
  prefs: []
  type: TYPE_IMG
  zh: '![来自标准普尔500指数中排名前列股票的PCA的screeplot，包括GOOGL和AMZN。](Images/psd2_0713.png)'
- en: Figure 7-13\. A screeplot for a PCA of top stocks from the S&P 500, including
    GOOGL and AMZN
  id: totrans-281
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-13\. 来自标准普尔500指数中排名前列股票PCA的screeplot，包括GOOGL和AMZN
- en: Categorical Data and Gower’s Distance
  id: totrans-282
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分类数据和Gower距离
- en: In the case of categorical data, you must convert it to numeric data, either
    by ranking (for an ordered factor) or by encoding as a set of binary (dummy) variables.
    If the data consists of mixed continuous and binary variables, you will usually
    want to scale the variables so that the ranges are similar; see [“Scaling the
    Variables”](#ScalingPCA). One popular method is to use *Gower’s distance*.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 对于分类数据，必须将其转换为数值数据，可以通过排名（有序因子）或编码为一组二进制（虚拟）变量来实现。如果数据包含混合连续和二进制变量，通常需要对变量进行缩放，以使范围相似；参见[“缩放变量”](#ScalingPCA)。一种流行的方法是使用*Gower距离*。
- en: 'The basic idea behind Gower’s distance is to apply a different distance metric
    to each variable depending on the type of data:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: Gower距离背后的基本思想是根据数据类型对每个变量应用不同的距离度量：
- en: For numeric variables and ordered factors, distance is calculated as the absolute
    value of the difference between two records (*Manhattan distance*).
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于数值变量和有序因子，距离计算为两个记录之间差值的绝对值（*曼哈顿距离*）。
- en: For categorical variables, the distance is 1 if the categories between two records
    are different, and the distance is 0 if the categories are the same.
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于分类变量，如果两个记录之间的类别不同，则距离为1；如果类别相同，则距离为0。
- en: 'Gower’s distance is computed as follows:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: Gower距离的计算方法如下：
- en: Compute the distance <math alttext="d Subscript i comma j"><msub><mi>d</mi>
    <mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub></math> for all pairs of variables
    *i* and *j* for each record.
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算每个记录的所有变量对 *i* 和 *j* 的距离 <math alttext="d Subscript i comma j"><msub><mi>d</mi>
    <mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub></math>。
- en: Scale each pair <math alttext="d Subscript i comma j"><msub><mi>d</mi> <mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub></math>
    so the minimum is 0 and the maximum is 1.
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 缩放每对 <math alttext="d Subscript i comma j"><msub><mi>d</mi> <mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub></math>，使最小值为0，最大值为1。
- en: Add the pairwise scaled distances between variables together, using either a
    simple or a weighted mean, to create the distance matrix.
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将变量之间的成对缩放距离相加，使用简单或加权平均，创建距离矩阵。
- en: 'To illustrate Gower’s distance, take a few rows from the loan data in *R*:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明Gower距离，从 *R* 中的贷款数据中取几行：
- en: '[PRE48]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'The function `daisy` in the `cluster` package in *R* can be used to compute
    Gower’s distance:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *R* 中的 `cluster` 包中的函数 `daisy` 可用于计算Gower距离：
- en: '[PRE49]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: At the moment of this writing, Gower’s distance is not available in any of the
    popular Python packages. However, activities are ongoing to include it in `scikit-learn`.
    We will update the accompanying source code once the implementation is released.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，Gower距离尚未包含在任何流行的Python包中。然而，正在进行的工作包括将其包含在 `scikit-learn` 中。一旦实施完成，我们将更新相应的源代码。
- en: 'All distances are between 0 and 1. The pair of records with the biggest distance
    is 2 and 3: neither has the same values for `home` and `purpose`, and they have
    very different levels of `dti` (debt-to-income) and `payment_inc_ratio`. Records
    3 and 5 have the smallest distance because they share the same values for `home`
    and `purpose`.'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 所有距离介于0和1之间。距离最大的记录对是2和3：它们的`home`和`purpose`值不同，并且它们的`dti`（负债收入比）和`payment_inc_ratio`（支付收入比）水平非常不同。记录3和5的距离最小，因为它们的`home`和`purpose`值相同。
- en: 'You can pass the Gower’s distance matrix calculated from `daisy` to `hclust`
    for hierarchical clustering (see [“Hierarchical Clustering”](#HierarchicalClustering)):'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 可以将从 `daisy` 计算出的 Gower 距离矩阵传递给 `hclust` 进行层次聚类（参见 [“Hierarchical Clustering”](#HierarchicalClustering)）：
- en: '[PRE50]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'The resulting dendrogram is shown in [Figure 7-14](#DendroLoan). The individual
    records are not distinguishable on the x-axis, but we can cut the dendrogram horizontally
    at 0.5 and examine the records in one of the subtrees with this code:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 结果显示的树状图如 [图 7-14](#DendroLoan) 所示。个体记录在 x 轴上无法区分，但我们可以在 0.5 处水平切割树状图，并使用以下代码检查某个子树中的记录：
- en: '[PRE51]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: This subtree consists entirely of owners with a loan purpose labeled as “debt_consolidation.”
    While strict separation is not true of all subtrees, this illustrates that the
    categorical variables tend to be grouped together in the clusters.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 此子树完全由贷款目的标记为“债务合并”的所有者组成。尽管严格分离并非所有子树的特点，但这说明了分类变量倾向于在聚类中被组合在一起。
- en: '![A dendrogram of hclust applied to a sample of loan default data with mixed
    variable types.](Images/psd2_0714.png)'
  id: totrans-302
  prefs: []
  type: TYPE_IMG
  zh: '![应用于混合变量类型贷款违约数据样本的 `hclust` 的树状图。](Images/psd2_0714.png)'
- en: Figure 7-14\. A dendrogram of `hclust` applied to a sample of loan default data
    with mixed variable types
  id: totrans-303
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-14\. 应用于混合变量类型贷款违约数据样本的 `hclust` 的树状图
- en: Problems with Clustering Mixed Data
  id: totrans-304
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 混合数据的聚类问题
- en: '*K*-means and PCA are most appropriate for continuous variables. For smaller
    data sets, it is better to use hierarchical clustering with Gower’s distance.
    In principle, there is no reason why *K*-means can’t be applied to binary or categorical
    data. You would usually use the “one hot encoder” representation (see [“One Hot
    Encoder”](ch06.xhtml#OneHotEncoder)) to convert the categorical data to numeric
    values. In practice, however, using *K*-means and PCA with binary data can be
    difficult.'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: '*K*-means 和 PCA 最适合连续变量。对于较小的数据集，最好使用带有 Gower 距离的层次聚类。原则上，*K*-means 也可以应用于二进制或分类数据。通常会使用“独热编码器”表示法（参见
    [“One Hot Encoder”](ch06.xhtml#OneHotEncoder)）将分类数据转换为数值。然而，在实践中，使用 *K*-means
    和 PCA 处理二进制数据可能会比较困难。'
- en: 'If the standard *z*-scores are used, the binary variables will dominate the
    definition of the clusters. This is because 0/1 variables take on only two values,
    and *K*-means can obtain a small within-cluster sum-of-squares by assigning all
    the records with a 0 or 1 to a single cluster. For example, apply `kmeans` to
    loan default data including factor variables `home` and `pub_rec_zero`, shown
    here in *R*:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 如果使用标准的 *z*-分数，二进制变量将主导聚类的定义。这是因为 0/1 变量仅取两个值，*K*-means 可以通过将所有取值为 0 或 1 的记录分配到单个聚类中获得较小的簇内平方和。例如，在包括因子变量
    `home` 和 `pub_rec_zero` 的贷款违约数据中应用 `kmeans`，如下所示的 *R* 代码：
- en: '[PRE52]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'In *Python*:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *Python* 中：
- en: '[PRE53]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: The top four clusters are essentially proxies for the different levels of the
    factor variables. To avoid this behavior, you could scale the binary variables
    to have a smaller variance than other variables. Alternatively, for very large
    data sets, you could apply clustering to different subsets of data taking on specific
    categorical values. For example, you could apply clustering separately to those
    loans made to someone who has a mortgage, owns a home outright, or rents.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 前四个聚类实质上是因子变量不同水平的代理。为了避免这种行为，可以将二进制变量缩放到比其他变量具有更小的方差。或者，对于非常大的数据集，可以将聚类应用于具有特定分类值的数据子集。例如，可以单独将放贷给有抵押贷款、全额拥有房屋或租房的人士的贷款进行聚类。
- en: Summary
  id: totrans-311
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: For dimension reduction of numeric data, the main tools are either principal
    components analysis or *K*-means clustering. Both require attention to proper
    scaling of the data to ensure meaningful data reduction.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 对于数值数据的降维，主要工具是主成分分析或 *K*-means 聚类。两者都需要注意数据的适当缩放，以确保有意义的数据降维。
- en: For clustering with highly structured data in which the clusters are well separated,
    all methods will likely produce a similar result. Each method offers its own advantage.
    *K*-means scales to very large data and is easily understood. Hierarchical clustering
    can be applied to mixed data types—numeric and categorical—and lends itself to
    an intuitive display (the dendrogram). Model-based clustering is founded on statistical
    theory and provides a more rigorous approach, as opposed to the heuristic methods.
    For very large data, however, *K*-means is the main method used.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 对于具有高度结构化数据且簇之间分离良好的聚类，所有方法可能会产生类似的结果。每种方法都有其优势。*K*-means 可扩展到非常大的数据并且易于理解。层次聚类可以应用于混合数据类型——数值和分类数据，并且适合直观显示（树状图）。基于模型的聚类建立在统计理论之上，提供了更严格的方法，与启发式方法相对。然而，对于非常大的数据集，*K*-means
    是主要的方法。
- en: With noisy data, such as the loan and stock data (and much of the data that
    a data scientist will face), the choice is more stark. *K*-means, hierarchical
    clustering, and especially model-based clustering all produce very different solutions.
    How should a data scientist proceed? Unfortunately, there is no simple rule of
    thumb to guide the choice. Ultimately, the method used will depend on the data
    size and the goal of the application.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 在存在噪声数据的情况下，如贷款和股票数据（以及数据科学家将面对的大部分数据），选择更加明显。*K*-means、层次聚类，特别是基于模型的聚类都会产生非常不同的解决方案。数据科学家该如何操作？不幸的是，没有简单的经验法则可以指导选择。最终使用的方法将取决于数据规模和应用的目标。
- en: ^([1](ch07.xhtml#idm46522838490648-marker)) This and subsequent sections in
    this chapter © 2020 Datastats, LLC, Peter Bruce, Andrew Bruce, and Peter Gedeck;
    used with permission.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch07.xhtml#idm46522838490648-marker)) 本章及后续章节内容 © 2020 Datastats, LLC,
    Peter Bruce, Andrew Bruce, and Peter Gedeck；已获得授权使用。
