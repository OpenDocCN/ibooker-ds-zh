- en: Chapter 7\. Unsupervised Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The term *unsupervised learning* refers to statistical methods that extract
    meaning from data without training a model on labeled data (data where an outcome
    of interest is known). In Chapters [4](ch04.xhtml#Regression) to [6](ch06.xhtml#StatisticalML),
    the goal is to build a model (set of rules) to predict a response variable from
    a set of predictor variables. This is supervised learning. In contrast, unsupervised
    learning also constructs a model of the data, but it does not distinguish between
    a response variable and predictor variables.
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised learning can be used to achieve different goals. In some cases,
    it can be used to create a predictive rule in the absence of a labeled response.
    *Clustering* methods can be used to identify meaningful groups of data. For example,
    using the web clicks and demographic data of a user on a website, we may be able
    to group together different types of users. The website could then be personalized
    to these different types.
  prefs: []
  type: TYPE_NORMAL
- en: In other cases, the goal may be to *reduce the dimension* of the data to a more
    manageable set of variables. This reduced set could then be used as input into
    a predictive model, such as regression or classification. For example, we may
    have thousands of sensors to monitor an industrial process. By reducing the data
    to a smaller set of features, we may be able to build a more powerful and interpretable
    model to predict process failure than could be built by including data streams
    from thousands of sensors.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, unsupervised learning can be viewed as an extension of the exploratory
    data analysis (see [Chapter 1](ch01.xhtml#EDA)) to situations in which you are
    confronted with a large number of variables and records. The aim is to gain insight
    into a set of data and how the different variables relate to each other. Unsupervised
    techniques allow you to sift through and analyze these variables and discover
    relationships.
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised Learning and Prediction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Unsupervised learning can play an important role in prediction, both for regression
    and classification problems. In some cases, we want to predict a category in the
    absence of any labeled data. For example, we might want to predict the type of
    vegetation in an area from a set of satellite sensory data. Since we don’t have
    a response variable to train a model, clustering gives us a way to identify common
    patterns and categorize the regions.
  prefs: []
  type: TYPE_NORMAL
- en: Clustering is an especially important tool for the “cold-start problem.” In
    this type of problem, such as launching a new marketing campaign or identifying
    potential new types of fraud or spam, we initially may not have any response to
    train a model. Over time, as data is collected, we can learn more about the system
    and build a traditional predictive model. But clustering helps us start the learning
    process more quickly by identifying population segments.
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised learning is also important as a building block for regression and
    classification techniques. With big data, if a small subpopulation is not well
    represented in the overall population, the trained model may not perform well
    for that subpopulation. With clustering, it is possible to identify and label
    subpopulations. Separate models can then be fit to the different subpopulations.
    Alternatively, the subpopulation can be represented with its own feature, forcing
    the overall model to explicitly consider subpopulation identity as a predictor.
  prefs: []
  type: TYPE_NORMAL
- en: Principal Components Analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Often, variables will vary together (covary), and some of the variation in one
    is actually duplicated by variation in another (e.g., restaurant checks and tips).
    Principal components analysis (PCA) is a technique to discover the way in which
    numeric variables covary.^([1](ch07.xhtml#idm46522838490648))
  prefs: []
  type: TYPE_NORMAL
- en: The idea in PCA is to combine multiple numeric predictor variables into a smaller
    set of variables, which are weighted linear combinations of the original set.
    The smaller set of variables, the *principal components*, “explains” most of the
    variability of the full set of variables, reducing the dimension of the data.
    The weights used to form the principal components reveal the relative contributions
    of the original variables to the new principal components.
  prefs: []
  type: TYPE_NORMAL
- en: PCA was first [proposed by Karl Pearson](https://oreil.ly/o4EeC). In what was
    perhaps the first paper on unsupervised learning, Pearson recognized that in many
    problems there is variability in the predictor variables, so he developed PCA
    as a technique to model this variability. PCA can be viewed as the unsupervised
    version of linear discriminant analysis; see[“Discriminant Analysis”](ch05.xhtml#DiscriminantAnalysis).
  prefs: []
  type: TYPE_NORMAL
- en: A Simple Example
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For two variables, <math alttext="upper X 1"><msub><mi>X</mi> <mn>1</mn></msub></math>
    and <math alttext="upper X 2"><msub><mi>X</mi> <mn>2</mn></msub></math> , there
    are two principal components <math alttext="upper Z Subscript i"><msub><mi>Z</mi>
    <mi>i</mi></msub></math> ( <math alttext="i equals 1"><mrow><mi>i</mi> <mo>=</mo>
    <mn>1</mn></mrow></math> or 2):'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><msub><mi>Z</mi> <mi>i</mi></msub> <mo>=</mo> <msub><mi>w</mi>
    <mrow><mi>i</mi><mo>,</mo><mn>1</mn></mrow></msub> <msub><mi>X</mi> <mn>1</mn></msub>
    <mo>+</mo> <msub><mi>w</mi> <mrow><mi>i</mi><mo>,</mo><mn>2</mn></mrow></msub>
    <msub><mi>X</mi> <mn>2</mn></msub></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: The weights <math alttext="left-parenthesis w Subscript i comma 1 Baseline comma
    w Subscript i comma 2 Baseline right-parenthesis"><mrow><mo>(</mo> <msub><mi>w</mi>
    <mrow><mi>i</mi><mo>,</mo><mn>1</mn></mrow></msub> <mo>,</mo> <msub><mi>w</mi>
    <mrow><mi>i</mi><mo>,</mo><mn>2</mn></mrow></msub> <mo>)</mo></mrow></math> are
    known as the component *loadings*. These transform the original variables into
    the principal components. The first principal component, <math alttext="upper
    Z 1"><msub><mi>Z</mi> <mn>1</mn></msub></math> , is the linear combination that
    best explains the total variation. The second principal component, <math alttext="upper
    Z 2"><msub><mi>Z</mi> <mn>2</mn></msub></math> , is orthogonal to the first and
    explains as much of the remaining variation as it can. (If there were additional
    components, each additional one would be orthogonal to the others.)
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: It is also common to compute principal components on deviations from the means
    of the predictor variables, rather than on the values themselves.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can compute principal components in *R* using the `princomp` function.
    The following performs a PCA on the stock price returns for Chevron (CVX) and
    ExxonMobil (XOM):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'In *Python*, we can use the `scikit-learn` implementation `sklearn.decomposition.PCA`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The weights for CVX and XOM for the first principal component are –0.747 and
    –0.665, and for the second principal component they are 0.665 and –0.747. How
    to interpret this? The first principal component is essentially an average of
    CVX and XOM, reflecting the correlation between the two energy companies. The
    second principal component measures when the stock prices of CVX and XOM diverge.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is instructive to plot the principal components with the data. Here we create
    a visualization in *R*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code creates a similar visualization in *Python*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The result is shown in [Figure 7-1](#StockPCA).
  prefs: []
  type: TYPE_NORMAL
- en: '![The principal components for the stock returns for Chevron and ExxonMobil](Images/psd2_0701.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-1\. The principal components for the stock returns for Chevron (CVX)
    and ExxonMobil (XOM)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The dashed lines show the direction of the two principal components: the first
    one is along the long axis of the ellipse, and the second one is along the short
    axis. You can see that a majority of the variability in the two stock returns
    is explained by the first principal component. This makes sense since energy stock
    prices tend to move as a group.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The weights for the first principal component are both negative, but reversing
    the sign of all the weights does not change the principal component. For example,
    using weights of 0.747 and 0.665 for the first principal component is equivalent
    to the negative weights, just as an infinite line defined by the origin and 1,1
    is the same as one defined by the origin and –1, –1.
  prefs: []
  type: TYPE_NORMAL
- en: Computing the Principal Components
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Going from two variables to more variables is straightforward. For the first
    component, simply include the additional predictor variables in the linear combination,
    assigning weights that optimize the collection of the covariation from all the
    predictor variables into this first principal component (*covariance* is the statistical
    term; see [“Covariance Matrix”](ch05.xhtml#Covariance)). Calculation of principal
    components is a classic statistical method, relying on either the correlation
    matrix of the data or the covariance matrix, and it executes rapidly, not relying
    on iteration. As noted earlier, principal components analysis works only with
    numeric variables, not categorical ones. The full process can be described as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: In creating the first principal component, PCA arrives at the linear combination
    of predictor variables that maximizes the percent of total variance explained.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This linear combination then becomes the first “new” predictor, *Z*[1].
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: PCA repeats this process, using the same variables with different weights, to
    create a second new predictor, *Z*[2]. The weighting is done such that *Z*[1]
    and *Z*[2] are uncorrelated.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The process continues until you have as many new variables, or components, *Z*[i]
    as original variables *X*[i].
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Choose to retain as many components as are needed to account for most of the
    variance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The result so far is a set of weights for each component. The final step is
    to convert the original data into new principal component scores by applying the
    weights to the original values. These new scores can then be used as the reduced
    set of predictor variables.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Interpreting Principal Components
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The nature of the principal components often reveals information about the
    structure of the data. There are a couple of standard visualization displays to
    help you glean insight about the principal components. One such method is a *screeplot*
    to visualize the relative importance of principal components (the name derives
    from the resemblance of the plot to a scree slope; here, the y-axis is the eigenvalue).
    The following *R* code shows an example for a few top companies in the S&P 500:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The information to create a loading plot from the `scikit-learn` result is
    available in `explained_variance_`. Here, we convert it into a `pandas` data frame
    and use it to make a bar chart:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: As seen in [Figure 7-2](#Screeplot), the variance of the first principal component
    is quite large (as is often the case), but the other top principal components
    are significant.
  prefs: []
  type: TYPE_NORMAL
- en: '![A screeplot for a PCA of top stocks from the SP 500.](Images/psd2_0702.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-2\. A screeplot for a PCA of top stocks from the S&P 500
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'It can be especially revealing to plot the weights of the top principal components.
    One way to do this in *R* is to use the `gather` function from the `tidyr` package
    in conjunction with `ggplot`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the code to create the same visualization in *Python*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The loadings for the top five components are shown in [Figure 7-3](#Loadings).
    The loadings for the first principal component have the same sign: this is typical
    for data in which all the columns share a common factor (in this case, the overall
    stock market trend). The second component captures the price changes of energy
    stocks as compared to the other stocks. The third component is primarily a contrast
    in the movements of Apple and CostCo. The fourth component contrasts the movements
    of Schlumberger (SLB) to the other energy stocks. Finally, the fifth component
    is mostly dominated by financial companies.'
  prefs: []
  type: TYPE_NORMAL
- en: '![The loadings for the top five principal components of stock price returns.](Images/psd2_0703.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-3\. The loadings for the top five principal components of stock price
    returns
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: How Many Components to Choose?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If your goal is to reduce the dimension of the data, you must decide how many
    principal components to select. The most common approach is to use an ad hoc rule
    to select the components that explain “most” of the variance. You can do this
    visually through the screeplot, as, for example, in [Figure 7-2](#Screeplot).
    Alternatively, you could select the top components such that the cumulative variance
    exceeds a threshold, such as 80%. Also, you can inspect the loadings to determine
    if the component has an intuitive interpretation. Cross-validation provides a
    more formal method to select the number of significant components (see [“Cross-Validation”](ch04.xhtml#CrossValidation)
    for more).
  prefs: []
  type: TYPE_NORMAL
- en: Correspondence Analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: PCA cannot be used for categorical data; however, a somewhat related technique
    is *correspondence analysis*. The goal is to recognize associations between categories,
    or between categorical features. The similarities between correspondence analysis
    and principal components analysis are mainly under the hood—the matrix algebra
    for dimension scaling. Correspondence analysis is used mainly for graphical analysis
    of low-dimensional categorical data and is not used in the same way that PCA is
    for dimension reduction as a preparatory step with big data.
  prefs: []
  type: TYPE_NORMAL
- en: The input can be seen as a table, with rows representing one variable and columns
    another, and the cells representing record counts. The output (after some matrix
    algebra) is a *biplot*—a scatterplot with axes scaled (and with percentages indicating
    how much variance is explained by that dimension). The meaning of the units on
    the axes is not intuitively connected to the original data, and the main value
    of the scatterplot is to illustrate graphically variables that are associated
    with one another (by proximity on the plot). See for example, [Figure 7-4](#Correspondence_Analysis),
    in which household tasks are arrayed according to whether they are done jointly
    or solo (vertical axis), and whether wife or husband has primary responsibility
    (horizontal axis). Correspondence analysis is many decades old, as is the spirit
    of this example, judging by the assignment of tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a variety of packages for correspondence analysis in *R*. Here, we
    use the package `ca`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'In *Python*, we can use the `prince` package, which implements correspondence
    analysis using the `scikit-learn` API:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '![Correspondence analysis of house task data.](Images/psd2_0704.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-4\. Graphical representation of a correspondence analysis of house
    task data
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Further Reading
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For a detailed look at the use of cross-validation in principal components,
    see Rasmus Bro, K. Kjeldahl, A.K. Smilde, and Henk A. L. Kiers, [“Cross-Validation
    of Component Models: A Critical Look at Current Methods”](https://oreil.ly/yVryf),
    *Analytical and Bioanalytical Chemistry* 390, no. 5 (2008).'
  prefs: []
  type: TYPE_NORMAL
- en: K-Means Clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Clustering is a technique to divide data into different groups, where the records
    in each group are similar to one another. A goal of clustering is to identify
    significant and meaningful groups of data. The groups can be used directly, analyzed
    in more depth, or passed as a feature or an outcome to a predictive regression
    or classification model. *K-means* was the first clustering method to be developed;
    it is still widely used, owing its popularity to the relative simplicity of the
    algorithm and its ability to scale to large data sets.
  prefs: []
  type: TYPE_NORMAL
- en: '*K*-means divides the data into *K* clusters by minimizing the sum of the squared
    distances of each record to the *mean* of its assigned cluster. This is referred
    to as the *within-cluster sum of squares* or *within-cluster SS*. *K*-means does
    not ensure the clusters will have the same size but finds the clusters that are
    the best separated.'
  prefs: []
  type: TYPE_NORMAL
- en: Normalization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is typical to normalize (standardize) continuous variables by subtracting
    the mean and dividing by the standard deviation. Otherwise, variables with large
    scale will dominate the clustering process (see [“Standardization (Normalization,
    z-Scores)”](ch06.xhtml#Standardization)).
  prefs: []
  type: TYPE_NORMAL
- en: A Simple Example
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Start by considering a data set with *n* records and just two variables, <math
    alttext="x"><mi>x</mi></math> and <math alttext="y"><mi>y</mi></math> . Suppose
    we want to split the data into <math alttext="upper K equals 4"><mrow><mi>K</mi>
    <mo>=</mo> <mn>4</mn></mrow></math> clusters. This means assigning each record
    <math alttext="left-parenthesis x Subscript i Baseline comma y Subscript i Baseline
    right-parenthesis"><mrow><mo>(</mo> <msub><mi>x</mi> <mi>i</mi></msub> <mo>,</mo>
    <msub><mi>y</mi> <mi>i</mi></msub> <mo>)</mo></mrow></math> to a cluster *k*.
    Given an assignment of <math alttext="n Subscript k"><msub><mi>n</mi> <mi>k</mi></msub></math>
    records to cluster *k*, the center of the cluster <math alttext="left-parenthesis
    x overbar Subscript k Baseline comma y overbar Subscript k Baseline right-parenthesis"><mrow><mo>(</mo>
    <msub><mover accent="true"><mi>x</mi> <mo>¯</mo></mover> <mi>k</mi></msub> <mo>,</mo>
    <msub><mover accent="true"><mi>y</mi> <mo>¯</mo></mover> <mi>k</mi></msub> <mo>)</mo></mrow></math>
    is the mean of the points in the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mtable displaystyle="true"><mtr><mtd><msub><mrow class="MJX-TeXAtom-ORD"><mover><mi>x</mi><mo
    stretchy="false">¯</mo></mover></mrow> <mi>k</mi></msub></mtd> <mtd><mo>=</mo>
    <mfrac><mn>1</mn><msub><mi>n</mi><mi>k</mi></msub></mfrac> <munder><mo>∑</mo>
    <mrow class="MJX-TeXAtom-ORD"><mtable rowspacing="0.1em" columnspacing="0em 0em
    0em 0em" displaystyle="false"><mtr><mtd><mi>i</mi><mo>∈</mo></mtd></mtr> <mtr><mtd><mrow
    class="MJX-TeXAtom-ORD"><mtext>Cluster</mtext></mrow> <mi>k</mi></mtd></mtr></mtable></mrow></munder>
    <msub><mi>x</mi> <mi>i</mi></msub></mtd></mtr> <mtr><mtd><msub><mrow class="MJX-TeXAtom-ORD"><mover><mi>y</mi><mo
    stretchy="false">¯</mo></mover></mrow> <mi>k</mi></msub></mtd> <mtd><mo>=</mo>
    <mfrac><mn>1</mn><msub><mi>n</mi><mi>k</mi></msub></mfrac> <munder><mo>∑</mo>
    <mrow class="MJX-TeXAtom-ORD"><mtable rowspacing="0.1em" columnspacing="0em 0em
    0em 0em" displaystyle="false"><mtr><mtd><mi>i</mi><mo>∈</mo></mtd></mtr> <mtr><mtd><mrow
    class="MJX-TeXAtom-ORD"><mtext>Cluster</mtext></mrow> <mi>k</mi></mtd></mtr></mtable></mrow></munder>
    <msub><mi>y</mi><mi>i</mi></msub></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: Cluster Mean
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In clustering records with multiple variables (the typical case), the term *cluster
    mean* refers not to a single number but to the vector of means of the variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'The sum of squares within a cluster is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><msub><mtext>SS</mtext> <mi>k</mi></msub> <mo>=</mo>
    <munder><mo>∑</mo> <mrow><mi>i</mi><mo>∈</mo><mtext>Cluster</mtext><mi>k</mi></mrow></munder>
    <msup><mfenced separators="" open="(" close=")"><msub><mi>x</mi> <mi>i</mi></msub>
    <mo>-</mo><msub><mover accent="true"><mi>x</mi> <mo>¯</mo></mover> <mi>k</mi></msub></mfenced>
    <mn>2</mn></msup> <mo>+</mo> <msup><mfenced separators="" open="(" close=")"><msub><mi>y</mi>
    <mi>i</mi></msub> <mo>-</mo><msub><mover accent="true"><mi>y</mi> <mo>¯</mo></mover>
    <mi>k</mi></msub></mfenced> <mn>2</mn></msup></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: '*K*-means finds the assignment of records that minimizes within-cluster sum
    of squares across all four clusters <math alttext="SS Subscript 1 Baseline plus
    SS Subscript 2 Baseline plus SS Subscript 3 Baseline plus SS Subscript 4"><mrow><msub><mtext>SS</mtext>
    <mn>1</mn></msub> <mo>+</mo> <msub><mtext>SS</mtext> <mn>2</mn></msub> <mo>+</mo>
    <msub><mtext>SS</mtext> <mn>3</mn></msub> <mo>+</mo> <msub><mtext>SS</mtext> <mn>4</mn></msub></mrow></math>
    :'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><munderover><mo>∑</mo> <mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow>
    <mn>4</mn></munderover> <msub><mtext>SS</mtext> <mi>k</mi></msub></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: A typical use of clustering is to locate natural, separate clusters in the data.
    Another application is to divide the data into a predetermined number of separate
    groups, where clustering is used to ensure the groups are as different as possible
    from one another.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, suppose we want to divide daily stock returns into four groups.
    *K*-means clustering can be used to separate the data into the best groupings.
    Note that daily stock returns are reported in a fashion that is, in effect, standardized,
    so we do not need to normalize the data. In *R*, *K*-means clustering can be performed
    using the `kmeans` function. For example, the following finds four clusters based
    on two variables—the daily stock returns for ExxonMobil (`XOM`) and Chevron (`CVX`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We use the `sklearn.cluster.KMeans` method from `scikit-learn` in *Python*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The cluster assignment for each record is returned as the `cluster` component
    (*R*):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'In `scikit-learn`, the cluster labels are available in the `labels_` field:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The first six records are assigned to either cluster 1 or cluster 2. The means
    of the clusters are also returned (*R*):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'In `scikit-learn`, the cluster centers are available in the `cluster_centers_`
    field:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Clusters 1 and 3 represent “down” markets, while clusters 2 and 4 represent
    “up markets.”
  prefs: []
  type: TYPE_NORMAL
- en: As the *K*-means algorithm uses randomized starting points, the results may
    differ between subsequent runs and different implementations of the method. In
    general, you should check that the fluctuations aren’t too large.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this example, with just two variables, it is straightforward to visualize
    the clusters and their means:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The `seaborn` `scatterplot` function makes it easy to color (`hue`) and style
    (`style`) the points by a property:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The resulting plot, shown in [Figure 7-5](#KmeansStockData), shows the cluster
    assignments and the cluster means. Note that *K*-means will assign records to
    clusters, even if those clusters are not well separated (which can be useful if
    you need to optimally divide records into groups).
  prefs: []
  type: TYPE_NORMAL
- en: '![The clusters of k-means applied to stock price data for ExxonMobil and Chevron
    (the cluster centers are highlighted with black symbols).](Images/psd2_0705.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-5\. The clusters of K-means applied to daily stock returns for ExxonMobil
    and Chevron (the cluster centers are highlighted with black symbols)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: K-Means Algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In general, *K*-means can be applied to a data set with *p* variables <math
    alttext="upper X 1 comma ellipsis comma upper X Subscript p Baseline"><mrow><msub><mi>X</mi>
    <mn>1</mn></msub> <mo>,</mo> <mo>...</mo> <mo>,</mo> <msub><mi>X</mi> <mi>p</mi></msub></mrow></math>
    . While the exact solution to *K*-means is computationally very difficult, heuristic
    algorithms provide an efficient way to compute a locally optimal solution.
  prefs: []
  type: TYPE_NORMAL
- en: 'The algorithm starts with a user-specified *K* and an initial set of cluster
    means and then iterates the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Assign each record to the nearest cluster mean as measured by squared distance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the new cluster means based on the assignment of records.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The algorithm converges when the assignment of records to clusters does not
    change.
  prefs: []
  type: TYPE_NORMAL
- en: For the first iteration, you need to specify an initial set of cluster means.
    Usually you do this by randomly assigning each record to one of the *K* clusters
    and then finding the means of those clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Since this algorithm isn’t guaranteed to find the best possible solution, it
    is recommended to run the algorithm several times using different random samples
    to initialize the algorithm. When more than one set of iterations is used, the
    *K*-means result is given by the iteration that has the lowest within-cluster
    sum of squares.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `nstart` parameter to the *R* function `kmeans` allows you to specify the
    number of random starts to try. For example, the following code runs *K*-means
    to find 5 clusters using 10 different starting cluster means:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The function automatically returns the best solution out of the 10 different
    starting points. You can use the argument `iter.max` to set the maximum number
    of iterations the algorithm is allowed for each random start.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `scikit-learn` algorithm is repeated 10 times by default (`n_init`). The
    argument `max_iter` (default 300) can be used to control the number of iterations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Interpreting the Clusters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'An important part of cluster analysis can involve the interpretation of the
    clusters. The two most important outputs from `kmeans` are the sizes of the clusters
    and the cluster means. For the example in the previous subsection, the sizes of
    resulting clusters are given by this *R* command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'In *Python*, we can use the `collections.Counter` class from the standard library
    to get this information. Due to differences in the implementation and the inherent
    randomness of the algorithm, results will vary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The cluster sizes are relatively balanced. Imbalanced clusters can result from
    distant outliers, or from groups of records very distinct from the rest of the
    data—both may warrant further inspection.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can plot the centers of the clusters using the `gather` function in conjunction
    with `ggplot`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The code to create this visualization in *Python* is similar to what we used
    for PCA:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: The resulting plot is shown in [Figure 7-6](#ClusterMeans) and reveals the nature
    of each cluster. For example, clusters 4 and 5 correspond to days on which the
    market is down and up, respectively. Clusters 2 and 3 are characterized by up-market
    days for consumer stocks and down-market days for energy stocks, respectively.
    Finally, cluster 1 captures the days in which energy stocks were up and consumer
    stocks were down.
  prefs: []
  type: TYPE_NORMAL
- en: '![The means of the clusters.](Images/psd2_0706.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-6\. The means of the variables in each cluster (“cluster means”)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Cluster Analysis Versus PCA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The plot of cluster means is similar in spirit to looking at the loadings for
    principal components analysis (PCA); see [“Interpreting Principal Components”](#InterpretPCA).
    A major distinction is that unlike with PCA, the sign of the cluster means is
    meaningful. PCA identifies principal directions of variation, whereas cluster
    analysis finds groups of records located near one another.
  prefs: []
  type: TYPE_NORMAL
- en: Selecting the Number of Clusters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The *K*-means algorithm requires that you specify the number of clusters *K*.
    Sometimes the number of clusters is driven by the application. For example, a
    company managing a sales force might want to cluster customers into “personas”
    to focus and guide sales calls. In such a case, managerial considerations would
    dictate the number of desired customer segments—for example, two might not yield
    useful differentiation of customers, while eight might be too many to manage.
  prefs: []
  type: TYPE_NORMAL
- en: In the absence of a cluster number dictated by practical or managerial considerations,
    a statistical approach could be used. There is no single standard method to find
    the “best” number of clusters.
  prefs: []
  type: TYPE_NORMAL
- en: A common approach, called the *elbow method*, is to identify when the set of
    clusters explains “most” of the variance in the data. Adding new clusters beyond
    this set contributes relatively little in the variance explained. The elbow is
    the point where the cumulative variance explained flattens out after rising steeply,
    hence the name of the method.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 7-7](#KmeansElbowMethod) shows the cumulative percent of variance explained
    for the default data for the number of clusters ranging from 2 to 15. Where is
    the elbow in this example? There is no obvious candidate, since the incremental
    increase in variance explained drops gradually. This is fairly typical in data
    that does not have well-defined clusters. This is perhaps a drawback of the elbow
    method, but it does reveal the nature of the data.'
  prefs: []
  type: TYPE_NORMAL
- en: '![The elbow method applied to the stock data.](Images/psd2_0707.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-7\. The elbow method applied to the stock data
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'In *R*, the `kmeans` function doesn’t provide a single command for applying
    the elbow method, but it can be readily applied from the output of `kmeans` as
    shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'For the `KMeans` result, we get this information from the property `inertia_`.
    After conversion into a `pandas` data frame, we can use its `plot` method to create
    the graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'In evaluating how many clusters to retain, perhaps the most important test
    is this: how likely are the clusters to be replicated on new data? Are the clusters
    interpretable, and do they relate to a general characteristic of the data, or
    do they just reflect a specific instance? You can assess this, in part, using
    cross-validation; see [“Cross-Validation”](ch04.xhtml#CrossValidation).'
  prefs: []
  type: TYPE_NORMAL
- en: In general, there is no single rule that will reliably guide how many clusters
    to produce.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: There are several more formal ways to determine the number of clusters based
    on statistical or information theory. For example, [Robert Tibshirani, Guenther
    Walther, and Trevor Hastie propose a “gap” statistic](https://oreil.ly/d-N3_)
    based on statistical theory to identify the elbow. For most applications, a theoretical
    approach is probably not necessary, or even appropriate.
  prefs: []
  type: TYPE_NORMAL
- en: Hierarchical Clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Hierarchical clustering* is an alternative to *K*-means that can yield very
    different clusters. Hierarchical clustering allows the user to visualize the effect
    of specifying different numbers of clusters. It is more sensitive in discovering
    outlying or aberrant groups or records. Hierarchical clustering also lends itself
    to an intuitive graphical display, leading to easier interpretation of the clusters.'
  prefs: []
  type: TYPE_NORMAL
- en: Hierarchical clustering’s flexibility comes with a cost, and hierarchical clustering
    does not scale well to large data sets with millions of records. For even modest-sized
    data with just tens of thousands of records, hierarchical clustering can require
    intensive computing resources. Indeed, most of the applications of hierarchical
    clustering are focused on relatively small data sets.
  prefs: []
  type: TYPE_NORMAL
- en: A Simple Example
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Hierarchical clustering works on a data set with *n* records and *p* variables
    and is based on two basic building blocks:'
  prefs: []
  type: TYPE_NORMAL
- en: A distance metric <math alttext="d Subscript i comma j"><msub><mi>d</mi> <mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub></math>
    to measure the distance between two records *i* and *j*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A dissimilarity metric <math alttext="upper D Subscript upper A comma upper
    B"><msub><mi>D</mi> <mrow><mi>A</mi><mo>,</mo><mi>B</mi></mrow></msub></math>
    to measure the difference between two clusters *A* and *B* based on the distances
    <math alttext="d Subscript i comma j"><msub><mi>d</mi> <mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub></math>
    between the members of each cluster.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For applications involving numeric data, the most importance choice is the dissimilarity
    metric. Hierarchical clustering starts by setting each record as its own cluster
    and iterates to combine the least dissimilar clusters.
  prefs: []
  type: TYPE_NORMAL
- en: 'In *R*, the `hclust` function can be used to perform hierarchical clustering.
    One big difference with `hclust` versus `kmeans` is that it operates on the pairwise
    distances <math alttext="d Subscript i comma j"><msub><mi>d</mi> <mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub></math>
    rather than the data itself. You can compute these using the `dist` function.
    For example, the following applies hierarchical clustering to the stock returns
    for a set of companies:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Clustering algorithms will cluster the records (rows) of a data frame. Since
    we want to cluster the companies, we need to *transpose* (`t`) the data frame
    and put the stocks along the rows and the dates along the columns.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `scipy` package offers a number of different methods for hierarchical clustering
    in the `scipy.cluster.hierarchy` module. Here we use the `linkage` function with
    the “complete” method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: The Dendrogram
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Hierarchical clustering lends itself to a natural graphical display as a tree,
    referred to as a *dendrogram*. The name comes from the Greek words *dendro* (tree)
    and *gramma* (drawing). In *R*, you can easily produce this using the `plot` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'We can use the `dendrogram` method to plot the result of the `linkage` function
    in *Python*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: The result is shown in [Figure 7-8](#DendogramStocks) (note that we are now
    plotting companies that are similar to one another, not days). The leaves of the
    tree correspond to the records. The length of the branch in the tree indicates
    the degree of dissimilarity between corresponding clusters. The returns for Google
    and Amazon are quite dissimilar to one another and to the returns for the other
    stocks. The oil stocks (SLB, CVX, XOM, COP) are in their own cluster, Apple (AAPL)
    is by itself, and the rest are similar to one another.
  prefs: []
  type: TYPE_NORMAL
- en: '![A dendrogram of stocks.](Images/psd2_0708.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-8\. A dendrogram of stocks
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'In contrast to *K*-means, it is not necessary to prespecify the number of clusters.
    Graphically, you can identify different numbers of clusters with a horizontal
    line that slides up or down; a cluster is defined wherever the horizontal line
    intersects the vertical lines. To extract a specific number of clusters, you can
    use the `cutree` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'In *Python*, you achieve the same with the `fcluster` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: The number of clusters to extract is set to 4, and you can see that Google and
    Amazon each belong to their own cluster. The oil stocks all belong to another
    cluster. The remaining stocks are in the fourth cluster.
  prefs: []
  type: TYPE_NORMAL
- en: The Agglomerative Algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The main algorithm for hierarchical clustering is the *agglomerative* algorithm,
    which iteratively merges similar clusters. The agglomerative algorithm begins
    with each record constituting its own single-record cluster and then builds up
    larger and larger clusters. The first step is to calculate distances between all
    pairs of records.
  prefs: []
  type: TYPE_NORMAL
- en: 'For each pair of records <math alttext="left-parenthesis x 1 comma x 2 comma
    ellipsis comma x Subscript p Baseline right-parenthesis"><mrow><mo>(</mo> <msub><mi>x</mi>
    <mn>1</mn></msub> <mo>,</mo> <msub><mi>x</mi> <mn>2</mn></msub> <mo>,</mo> <mo>...</mo>
    <mo>,</mo> <msub><mi>x</mi> <mi>p</mi></msub> <mo>)</mo></mrow></math> and <math
    alttext="left-parenthesis y 1 comma y 2 comma ellipsis comma y Subscript p Baseline
    right-parenthesis"><mrow><mo>(</mo> <msub><mi>y</mi> <mn>1</mn></msub> <mo>,</mo>
    <msub><mi>y</mi> <mn>2</mn></msub> <mo>,</mo> <mo>...</mo> <mo>,</mo> <msub><mi>y</mi>
    <mi>p</mi></msub> <mo>)</mo></mrow></math> , we measure the distance between the
    two records, <math alttext="d Subscript x comma y"><msub><mi>d</mi> <mrow><mi>x</mi><mo>,</mo><mi>y</mi></mrow></msub></math>
    , using a distance metric (see [“Distance Metrics”](ch06.xhtml#DistanceMetrics)).
    For example, we can use Euclidian distance:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mi>d</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>,</mo>
    <mi>y</mi> <mo>)</mo></mrow> <mo>=</mo> <msqrt><mrow><msup><mrow><mo>(</mo><msub><mi>x</mi>
    <mn>1</mn></msub> <mo>-</mo><msub><mi>y</mi> <mn>1</mn></msub> <mo>)</mo></mrow>
    <mn>2</mn></msup> <mo>+</mo> <msup><mrow><mo>(</mo><msub><mi>x</mi> <mn>2</mn></msub>
    <mo>-</mo><msub><mi>y</mi> <mn>2</mn></msub> <mo>)</mo></mrow> <mn>2</mn></msup>
    <mo>+</mo> <mo>⋯</mo> <mo>+</mo> <msup><mrow><mo>(</mo><msub><mi>x</mi> <mi>p</mi></msub>
    <mo>-</mo><msub><mi>y</mi> <mi>p</mi></msub> <mo>)</mo></mrow> <mn>2</mn></msup></mrow></msqrt></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: We now turn to inter-cluster distance. Consider two clusters *A* and *B*, each
    with a distinctive set of records, <math alttext="upper A equals left-parenthesis
    a 1 comma a 2 comma ellipsis comma a Subscript m Baseline right-parenthesis"><mrow><mi>A</mi>
    <mo>=</mo> <mo>(</mo> <msub><mi>a</mi> <mn>1</mn></msub> <mo>,</mo> <msub><mi>a</mi>
    <mn>2</mn></msub> <mo>,</mo> <mo>...</mo> <mo>,</mo> <msub><mi>a</mi> <mi>m</mi></msub>
    <mo>)</mo></mrow></math> and <math alttext="upper B equals left-parenthesis b
    1 comma b 2 comma ellipsis comma b Subscript q Baseline right-parenthesis"><mrow><mi>B</mi>
    <mo>=</mo> <mo>(</mo> <msub><mi>b</mi> <mn>1</mn></msub> <mo>,</mo> <msub><mi>b</mi>
    <mn>2</mn></msub> <mo>,</mo> <mo>...</mo> <mo>,</mo> <msub><mi>b</mi> <mi>q</mi></msub>
    <mo>)</mo></mrow></math> . We can measure the dissimilarity between the clusters
    <math alttext="upper D left-parenthesis upper A comma upper B right-parenthesis"><mrow><mi>D</mi>
    <mo>(</mo> <mi>A</mi> <mo>,</mo> <mi>B</mi> <mo>)</mo></mrow></math> by using
    the distances between the members of *A* and the members of *B*.
  prefs: []
  type: TYPE_NORMAL
- en: 'One measure of dissimilarity is the *complete-linkage* method, which is the
    maximum distance across all pairs of records between *A* and *B*:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mi>D</mi> <mrow><mo>(</mo> <mi>A</mi> <mo>,</mo>
    <mi>B</mi> <mo>)</mo></mrow> <mo>=</mo> <mo movablelimits="true" form="prefix">max</mo>
    <mi>d</mi> <mrow><mo>(</mo> <msub><mi>a</mi> <mi>i</mi></msub> <mo>,</mo> <msub><mi>b</mi>
    <mi>j</mi></msub> <mo>)</mo></mrow> <mtext>for</mtext> <mtext>all</mtext> <mtext>pairs</mtext>
    <mi>i</mi> <mo>,</mo> <mi>j</mi></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: This defines the dissimilarity as the biggest difference between all pairs.
  prefs: []
  type: TYPE_NORMAL
- en: 'The main steps of the agglomerative algorithm are:'
  prefs: []
  type: TYPE_NORMAL
- en: Create an initial set of clusters with each cluster consisting of a single record
    for all records in the data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the dissimilarity <math alttext="upper D left-parenthesis upper C Subscript
    k Baseline comma upper C Subscript script l Baseline right-parenthesis"><mrow><mi>D</mi>
    <mo>(</mo> <msub><mi>C</mi> <mi>k</mi></msub> <mo>,</mo> <msub><mi>C</mi> <mi>ℓ</mi></msub>
    <mo>)</mo></mrow></math> between all pairs of clusters <math alttext="k comma
    script l"><mrow><mi>k</mi> <mo>,</mo> <mi>ℓ</mi></mrow></math> .
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Merge the two clusters <math alttext="upper C Subscript k"><msub><mi>C</mi>
    <mi>k</mi></msub></math> and <math alttext="upper C Subscript script l"><msub><mi>C</mi>
    <mi>ℓ</mi></msub></math> that are least dissimilar as measured by <math alttext="upper
    D left-parenthesis upper C Subscript k Baseline comma upper C Subscript script
    l Baseline right-parenthesis"><mrow><mi>D</mi> <mo>(</mo> <msub><mi>C</mi> <mi>k</mi></msub>
    <mo>,</mo> <msub><mi>C</mi> <mi>ℓ</mi></msub> <mo>)</mo></mrow></math> .
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If we have more than one cluster remaining, return to step 2\. Otherwise, we
    are done.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Measures of Dissimilarity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are four common measures of dissimilarity: *complete linkage*, *single
    linkage*, *average linkage*, and *minimum variance*. These (plus other measures)
    are all supported by most hierarchical clustering software, including `hclust`
    and `linkage`. The complete linkage method defined earlier tends to produce clusters
    with members that are similar. The single linkage method is the minimum distance
    between the records in two clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mi>D</mi> <mrow><mo>(</mo> <mi>A</mi> <mo>,</mo>
    <mi>B</mi> <mo>)</mo></mrow> <mo>=</mo> <mo movablelimits="true" form="prefix">min</mo>
    <mi>d</mi> <mrow><mo>(</mo> <msub><mi>a</mi> <mi>i</mi></msub> <mo>,</mo> <msub><mi>b</mi>
    <mi>j</mi></msub> <mo>)</mo></mrow> <mtext>for</mtext> <mtext>all</mtext> <mtext>pairs</mtext>
    <mi>i</mi> <mo>,</mo> <mi>j</mi></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: This is a “greedy” method and produces clusters that can contain quite disparate
    elements. The average linkage method is the average of all distance pairs and
    represents a compromise between the single and complete linkage methods. Finally,
    the minimum variance method, also referred to as *Ward’s* method, is similar to
    *K*-means since it minimizes the within-cluster sum of squares (see [“K-Means
    Clustering”](#Kmeans)).
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 7-9](#DissimilarityMeasures) applies hierarchical clustering using
    the four measures to the ExxonMobil and Chevron stock returns. For each measure,
    four clusters are retained.'
  prefs: []
  type: TYPE_NORMAL
- en: '![A comparison of measures of dissimilarity applied to stock returns; ExxonMobil
    on the x-axis and Chevron on the y-axis.](Images/psd2_0709.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-9\. A comparison of measures of dissimilarity applied to stock data
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The results are strikingly different: the single linkage measure assigns almost
    all of the points to a single cluster. Except for the minimum variance method
    (*R*: `Ward.D`; *Python*: `ward`), all measures end up with at least one cluster
    with just a few outlying points. The minimum variance method is most similar to
    the *K*-means cluster; compare with [Figure 7-5](#KmeansStockData).'
  prefs: []
  type: TYPE_NORMAL
- en: Model-Based Clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Clustering methods such as hierarchical clustering and *K*-means are based on
    heuristics and rely primarily on finding clusters whose members are close to one
    another, as measured directly with the data (no probability model involved). In
    the past 20 years, significant effort has been devoted to developing *model-based
    clustering* methods. Adrian Raftery and other researchers at the University of
    Washington made critical contributions to model-based clustering, including both
    theory and software. The techniques are grounded in statistical theory and provide
    more rigorous ways to determine the nature and number of clusters. They could
    be used, for example, in cases where there might be one group of records that
    are similar to one another but not necessarily close to one another (e.g., tech
    stocks with high variance of returns), and another group of records that are similar
    and also close (e.g., utility stocks with low variance).
  prefs: []
  type: TYPE_NORMAL
- en: Multivariate Normal Distribution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The most widely used model-based clustering methods rest on the *multivariate
    normal* distribution. The multivariate normal distribution is a generalization
    of the normal distribution to a set of *p* variables <math alttext="upper X 1
    comma upper X 2 comma ellipsis comma upper X Subscript p Baseline"><mrow><msub><mi>X</mi>
    <mn>1</mn></msub> <mo>,</mo> <msub><mi>X</mi> <mn>2</mn></msub> <mo>,</mo> <mo>...</mo>
    <mo>,</mo> <msub><mi>X</mi> <mi>p</mi></msub></mrow></math> . The distribution
    is defined by a set of means <math alttext="mu bold equals mu bold 1 bold comma
    mu bold 2 bold comma ellipsis bold comma mu Subscript bold p Baseline"><mrow><mi>μ</mi>
    <mo>=</mo> <msub><mi>μ</mi> <mn mathvariant="bold">1</mn></msub> <mo>,</mo> <msub><mi>μ</mi>
    <mn mathvariant="bold">2</mn></msub> <mo>,</mo> <mo>...</mo> <mo>,</mo> <msub><mi>μ</mi>
    <mi>𝐩</mi></msub></mrow></math> and a covariance matrix <math alttext="normal
    upper Sigma"><mi>Σ</mi></math> . The covariance matrix is a measure of how the
    variables correlate with each other (see [“Covariance Matrix”](ch05.xhtml#Covariance)
    for details on the covariance). The covariance matrix <math alttext="normal upper
    Sigma"><mi>Σ</mi></math> consists of *p* variances <math alttext="sigma 1 squared
    comma sigma 2 squared comma ellipsis comma sigma Subscript p Superscript 2"><mrow><msubsup><mi>σ</mi>
    <mn>1</mn> <mn>2</mn></msubsup> <mo>,</mo> <msubsup><mi>σ</mi> <mn>2</mn> <mn>2</mn></msubsup>
    <mo>,</mo> <mo>...</mo> <mo>,</mo> <msubsup><mi>σ</mi> <mi>p</mi> <mn>2</mn></msubsup></mrow></math>
    and covariances <math alttext="sigma Subscript i comma j"><msub><mi>σ</mi> <mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub></math>
    for all pairs of variables <math alttext="i not-equals j"><mrow><mi>i</mi> <mo>≠</mo>
    <mi>j</mi></mrow></math> . With the variables put along the rows and duplicated
    along the columns, the matrix looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mi>Σ</mi> <mo>=</mo> <mfenced open="[" close="]"><mtable><mtr><mtd><msubsup><mi>σ</mi>
    <mn>1</mn> <mn>2</mn></msubsup></mtd> <mtd><msub><mi>σ</mi> <mrow><mn>1</mn><mo>,</mo><mn>2</mn></mrow></msub></mtd>
    <mtd><mo>⋯</mo></mtd> <mtd><msub><mi>σ</mi> <mrow><mn>1</mn><mo>,</mo><mi>p</mi></mrow></msub></mtd></mtr>
    <mtr><mtd><msub><mi>σ</mi> <mrow><mn>2</mn><mo>,</mo><mn>1</mn></mrow></msub></mtd>
    <mtd><msubsup><mi>σ</mi> <mrow><mn>2</mn></mrow> <mn>2</mn></msubsup></mtd> <mtd><mo>⋯</mo></mtd>
    <mtd><msub><mi>σ</mi> <mrow><mn>2</mn><mo>,</mo><mi>p</mi></mrow></msub></mtd></mtr>
    <mtr><mtd><mo>⋮</mo></mtd> <mtd><mo>⋮</mo></mtd> <mtd><mo>⋱</mo></mtd> <mtd><mo>⋮</mo></mtd></mtr>
    <mtr><mtd><msub><mi>σ</mi> <mrow><mi>p</mi><mo>,</mo><mn>1</mn></mrow></msub></mtd>
    <mtd><msubsup><mi>σ</mi> <mrow><mi>p</mi><mo>,</mo><mn>2</mn></mrow> <mn>2</mn></msubsup></mtd>
    <mtd><mo>⋯</mo></mtd> <mtd><msubsup><mi>σ</mi> <mrow><mi>p</mi></mrow> <mn>2</mn></msubsup></mtd></mtr></mtable></mfenced></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that the covariance matrix is symmetric around the diagonal from upper
    left to lower right. Since <math alttext="sigma Subscript i comma j Baseline equals
    sigma Subscript j comma i"><mrow><msub><mi>σ</mi> <mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub>
    <mo>=</mo> <msub><mi>σ</mi> <mrow><mi>j</mi><mo>,</mo><mi>i</mi></mrow></msub></mrow></math>
    , there are only <math alttext="left-parenthesis p times left-parenthesis p minus
    1 right-parenthesis right-parenthesis slash 2"><mrow><mo>(</mo> <mi>p</mi> <mo>×</mo>
    <mo>(</mo> <mi>p</mi> <mo>-</mo> <mn>1</mn> <mo>)</mo> <mo>)</mo> <mo>/</mo> <mn>2</mn></mrow></math>
    covariance terms. In total, the covariance matrix has <math alttext="left-parenthesis
    p times left-parenthesis p minus 1 right-parenthesis right-parenthesis slash 2
    plus p"><mrow><mo>(</mo> <mi>p</mi> <mo>×</mo> <mo>(</mo> <mi>p</mi> <mo>-</mo>
    <mn>1</mn> <mo>)</mo> <mo>)</mo> <mo>/</mo> <mn>2</mn> <mo>+</mo> <mi>p</mi></mrow></math>
    parameters. The distribution is denoted by:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mrow><mo>(</mo> <msub><mi>X</mi> <mn>1</mn></msub>
    <mo>,</mo> <msub><mi>X</mi> <mn>2</mn></msub> <mo>,</mo> <mo>...</mo> <mo>,</mo>
    <msub><mi>X</mi> <mi>p</mi></msub> <mo>)</mo></mrow> <mo>∼</mo> <msub><mi>N</mi>
    <mi>p</mi></msub> <mrow><mo>(</mo> <mi>μ</mi> <mo>,</mo> <mi>Σ</mi> <mo>)</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: This is a symbolic way of saying that the variables are all normally distributed,
    and the overall distribution is fully described by the vector of variable means
    and the covariance matrix.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 7-10](#Normal2d) shows the probability contours for a multivariate
    normal distribution for two variables *X* and *Y* (the 0.5 probability contour,
    for example, contains 50% of the distribution).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The means are <math><mrow><msub><mi>μ</mi> <mi>x</mi></msub> <mo>=</mo> <mn>0</mn>
    <mo>.</mo> <mn>5</mn></mrow></math> and <math><mrow><msub><mi>μ</mi> <mi>y</mi></msub>
    <mo>=</mo> <mo>-</mo> <mn>0</mn> <mo>.</mo> <mn>5</mn></mrow></math> , and the
    covariance matrix is:'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mi>Σ</mi> <mo>=</mo> <mfenced separators="" open="["
    close="]"><mtable><mtr><mtd><mn>1</mn></mtd> <mtd><mn>1</mn></mtd></mtr> <mtr><mtd><mn>1</mn></mtd>
    <mtd><mn>2</mn></mtd></mtr></mtable></mfenced></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: Since the covariance <math alttext="sigma Subscript x y"><msub><mi>σ</mi> <mrow><mi>x</mi><mi>y</mi></mrow></msub></math>
    is positive, *X* and *Y* are positively correlated.
  prefs: []
  type: TYPE_NORMAL
- en: '![images/2d_normal.png](Images/psd2_0710.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-10\. Probability contours for a two-dimensional normal distribution
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Mixtures of Normals
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The key idea behind model-based clustering is that each record is assumed to
    be distributed as one of *K* multivariate normal distributions, where *K* is the
    number of clusters. Each distribution has a different mean <math alttext="mu"><mi>μ</mi></math>
    and covariance matrix <math alttext="normal upper Sigma"><mi>Σ</mi></math> . For
    example, if you have two variables, *X* and *Y*, then each row <math alttext="left-parenthesis
    upper X Subscript i Baseline comma upper Y Subscript i Baseline right-parenthesis"><mrow><mo>(</mo>
    <msub><mi>X</mi> <mi>i</mi></msub> <mo>,</mo> <msub><mi>Y</mi> <mi>i</mi></msub>
    <mo>)</mo></mrow></math> is modeled as having been sampled from one of *K* multivariate
    normal distributions <math alttext="upper N left-parenthesis mu 1 comma normal
    upper Sigma 1 right-parenthesis comma upper N left-parenthesis mu 2 comma normal
    upper Sigma 2 right-parenthesis comma ellipsis comma upper N left-parenthesis
    mu Subscript upper K Baseline comma normal upper Sigma Subscript upper K Baseline
    right-parenthesis"><mrow><mi>N</mi> <mrow><mo>(</mo> <msub><mi>μ</mi> <mn>1</mn></msub>
    <mo>,</mo> <msub><mi>Σ</mi> <mn>1</mn></msub> <mo>)</mo></mrow> <mo>,</mo> <mi>N</mi>
    <mrow><mo>(</mo> <msub><mi>μ</mi> <mn>2</mn></msub> <mo>,</mo> <msub><mi>Σ</mi>
    <mn>2</mn></msub> <mo>)</mo></mrow> <mo>,</mo> <mo>...</mo> <mo>,</mo> <mi>N</mi>
    <mrow><mo>(</mo> <msub><mi>μ</mi> <mi>K</mi></msub> <mo>,</mo> <msub><mi>Σ</mi>
    <mi>K</mi></msub> <mo>)</mo></mrow></mrow></math> .
  prefs: []
  type: TYPE_NORMAL
- en: '*R* has a very rich package for model-based clustering called `mclust`, originally
    developed by Chris Fraley and Adrian Raftery. With this package, we can apply
    model-based clustering to the stock return data we previously analyzed using *K*-means
    and hierarchical clustering:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '`scikit-learn` has the `sklearn.mixture.GaussianMixture` class for model-based
    clustering:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'If you execute this code, you will notice that the computation takes significantly
    longer than other procedures. Extracting the cluster assignments using the `predict`
    function, we can visualize the clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the *Python* code to create a similar figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting plot is shown in [Figure 7-11](#StockMclust). There are two clusters:
    one cluster in the middle of the data, and a second cluster in the outer edge
    of the data. This is very different from the clusters obtained using *K*-means
    ([Figure 7-5](#KmeansStockData)) and hierarchical clustering ([Figure 7-9](#DissimilarityMeasures)),
    which find clusters that are compact.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Two clusters are obtained for stock return data using +Mclust+.](Images/psd2_0711.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-11\. Two clusters are obtained for stock return data using `mclust`
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'You can extract the parameters to the normal distributions using the `summary`
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'In *Python*, you get this information from the `means_` and `covariances_`
    properties of the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: The distributions have similar means and correlations, but the second distribution
    has much larger variances and covariances. Due to the randomness of the algorithm,
    results can vary slightly between different runs.
  prefs: []
  type: TYPE_NORMAL
- en: 'The clusters from `mclust` may seem surprising, but in fact, they illustrate
    the statistical nature of the method. The goal of model-based clustering is to
    find the best-fitting set of multivariate normal distributions. The stock data
    appears to have a normal-looking shape: see the contours of [Figure 7-10](#Normal2d).
    In fact, though, stock returns have a longer-tailed distribution than a normal
    distribution. To handle this, `mclust` fits a distribution to the bulk of the
    data but then fits a second distribution with a bigger variance.'
  prefs: []
  type: TYPE_NORMAL
- en: Selecting the Number of Clusters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Unlike *K*-means and hierarchical clustering, `mclust` automatically selects
    the number of clusters in *R* (in this case, two). It does this by choosing the
    number of clusters for which the *Bayesian Information Criteria* (*BIC*) has the
    largest value (BIC is similar to AIC; see [“Model Selection and Stepwise Regression”](ch04.xhtml#StepwiseRegression)).
    BIC works by selecting the best-fitting model with a penalty for the number of
    parameters in the model. In the case of model-based clustering, adding more clusters
    will always improve the fit at the expense of introducing additional parameters
    in the model.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note that in most cases BIC is usually minimized. The authors of the `mclust`
    package decided to define BIC to have the opposite sign to make interpretation
    of plots easier.
  prefs: []
  type: TYPE_NORMAL
- en: '`mclust` fits 14 different models with increasing number of components and
    chooses an optimal model automatically. You can plot the BIC values of these models
    using a function in `mclust`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: The number of clusters—or number of different multivariate normal models (components)—is
    shown on the x-axis (see [Figure 7-12](#MclustBIC)).
  prefs: []
  type: TYPE_NORMAL
- en: '![BIC values for 14 models of the stock return data with increasing numbers
    of components.](Images/psd2_0712.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-12\. BIC values for 14 models of the stock return data with increasing
    numbers of components
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The `GaussianMixture` implementation on the other hand will not try out various
    combinations. As shown here, it is straightforward to run multiple combinations
    using *Python*. This implementation defines BIC as usual. Therefore, the calculated
    BIC value will be positive, and we need to minimize it.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_unsupervised_learning_CO1-1)'
  prefs: []
  type: TYPE_NORMAL
- en: With the `warm_start` argument, the calculation will reuse information from
    the previous fit. This will speed up the convergence of subsequent calculations.
  prefs: []
  type: TYPE_NORMAL
- en: This plot is similar to the elbow plot used to identify the number of clusters
    to choose for *K*-means, except the value being plotted is BIC instead of percent
    of variance explained (see [Figure 7-7](#KmeansElbowMethod)). One big difference
    is that instead of one line, `mclust` shows 14 different lines! This is because
    `mclust` is actually fitting 14 different models for each cluster size, and ultimately
    it chooses the best-fitting model. `GaussianMixture` implements fewer approaches,
    so the number of lines will be only four.
  prefs: []
  type: TYPE_NORMAL
- en: Why does `mclust` fit so many models to determine the best set of multivariate
    normals? It’s because there are different ways to parameterize the covariance
    matrix <math alttext="normal upper Sigma"><mi>Σ</mi></math> for fitting a model.
    For the most part, you do not need to worry about the details of the models and
    can simply use the model chosen by `mclust`. In this example, according to BIC,
    three different models (called VEE, VEV, and VVE) give the best fit using two
    components.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Model-based clustering is a rich and rapidly developing area of study, and the
    coverage in this text spans only a small part of the field. Indeed, the `mclust`
    help file is currently 154 pages long. Navigating the nuances of model-based clustering
    is probably more effort than is needed for most problems encountered by data scientists.
  prefs: []
  type: TYPE_NORMAL
- en: Model-based clustering techniques do have some limitations. The methods require
    an underlying assumption of a model for the data, and the cluster results are
    very dependent on that assumption. The computations requirements are higher than
    even hierarchical clustering, making it difficult to scale to large data. Finally,
    the algorithm is more sophisticated and less accessible than that of other methods.
  prefs: []
  type: TYPE_NORMAL
- en: Further Reading
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For more detail on model-based clustering, see the [`mclust`](https://oreil.ly/bHDvR)
    and [`GaussianMixture`](https://oreil.ly/GaVVv) documentation.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling and Categorical Variables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Unsupervised learning techniques generally require that the data be appropriately
    scaled. This is different from many of the techniques for regression and classification
    in which scaling is not important (an exception is *K*-Nearest Neighbors; see
    [“K-Nearest Neighbors”](ch06.xhtml#KNN)).
  prefs: []
  type: TYPE_NORMAL
- en: For example, with the personal loan data, the variables have widely different
    units and magnitude. Some variables have relatively small values (e.g., number
    of years employed), while others have very large values (e.g., loan amount in
    dollars). If the data is not scaled, then the PCA, *K*-means, and other clustering
    methods will be dominated by the variables with large values and ignore the variables
    with small values.
  prefs: []
  type: TYPE_NORMAL
- en: Categorical data can pose a special problem for some clustering procedures.
    As with *K*-Nearest Neighbors, unordered factor variables are generally converted
    to a set of binary (0/1) variables using one hot encoding (see [“One Hot Encoder”](ch06.xhtml#OneHotEncoder)).
    Not only are the binary variables likely on a different scale from other data,
    but the fact that binary variables have only two values can prove problematic
    with techniques such as PCA and *K*-means.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling the Variables
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Variables with very different scale and units need to be normalized appropriately
    before you apply a clustering procedure. For example, let’s apply `kmeans` to
    a set of data of loan defaults without normalizing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the corresponding *Python* code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: The variables `annual_inc` and `revol_bal` dominate the clusters, and the clusters
    have very different sizes. Cluster 1 has only 52 members with comparatively high
    income and revolving credit balance.
  prefs: []
  type: TYPE_NORMAL
- en: 'A common approach to scaling the variables is to convert them to *z*-scores
    by subtracting the mean and dividing by the standard deviation. This is termed
    *standardization* or *normalization* (see [“Standardization (Normalization, z-Scores)”](ch06.xhtml#Standardization)
    for more discussion about using *z*-scores):'
  prefs: []
  type: TYPE_NORMAL
- en: <math display="block"><mrow><mi>z</mi> <mo>=</mo> <mfrac><mrow><mi>x</mi><mo>-</mo><mover
    accent="true"><mi>x</mi> <mo>¯</mo></mover></mrow> <mi>s</mi></mfrac></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'See what happens to the clusters when `kmeans` is applied to the normalized
    data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'In *Python*, we can use `scikit-learn`’s `StandardScaler`. The `inverse_transform`
    method allows converting the cluster centers back to the original scale:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: The cluster sizes are more balanced, and the clusters are not dominated by `annual_inc`
    and `revol_bal`, revealing more interesting structure in the data. Note that the
    centers are rescaled to the original units in the preceding code. If we had left
    them unscaled, the resulting values would be in terms of *z*-scores and would
    therefore be less interpretable.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Scaling is also important for PCA. Using the *z*-scores is equivalent to using
    the correlation matrix (see [“Correlation”](ch01.xhtml#Correlations)) instead
    of the covariance matrix in computing the principal components. Software to compute
    PCA usually has an option to use the correlation matrix (in *R*, the `princomp`
    function has the argument `cor`).
  prefs: []
  type: TYPE_NORMAL
- en: Dominant Variables
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Even in cases where the variables are measured on the same scale and accurately
    reflect relative importance (e.g., movement to stock prices), it can sometimes
    be useful to rescale the variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose we add Google (GOOGL) and Amazon (AMZN) to the analysis in [“Interpreting
    Principal Components”](#InterpretPCA). We see how this is done in *R* below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'In *Python*, we get the screeplot as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'The screeplot displays the variances for the top principal components. In this
    case, the screeplot in [Figure 7-13](#Screeplot1) reveals that the variances of
    the first and second components are much larger than the others. This often indicates
    that one or two variables dominate the loadings. This is, indeed, the case in
    this example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'In *Python*, we use the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: The first two principal components are almost completely dominated by GOOGL
    and AMZN. This is because the stock price movements of GOOGL and AMZN dominate
    the variability.
  prefs: []
  type: TYPE_NORMAL
- en: To handle this situation, you can either include them as is, rescale the variables
    (see [“Scaling the Variables”](#ScalingPCA)), or exclude the dominant variables
    from the analysis and handle them separately. There is no “correct” approach,
    and the treatment depends on the application.
  prefs: []
  type: TYPE_NORMAL
- en: '![A screeplot for a PCA of top stocks from the SP 500, including GOOGL and
    AMZN.](Images/psd2_0713.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-13\. A screeplot for a PCA of top stocks from the S&P 500, including
    GOOGL and AMZN
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Categorical Data and Gower’s Distance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the case of categorical data, you must convert it to numeric data, either
    by ranking (for an ordered factor) or by encoding as a set of binary (dummy) variables.
    If the data consists of mixed continuous and binary variables, you will usually
    want to scale the variables so that the ranges are similar; see [“Scaling the
    Variables”](#ScalingPCA). One popular method is to use *Gower’s distance*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The basic idea behind Gower’s distance is to apply a different distance metric
    to each variable depending on the type of data:'
  prefs: []
  type: TYPE_NORMAL
- en: For numeric variables and ordered factors, distance is calculated as the absolute
    value of the difference between two records (*Manhattan distance*).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For categorical variables, the distance is 1 if the categories between two records
    are different, and the distance is 0 if the categories are the same.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gower’s distance is computed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Compute the distance <math alttext="d Subscript i comma j"><msub><mi>d</mi>
    <mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub></math> for all pairs of variables
    *i* and *j* for each record.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Scale each pair <math alttext="d Subscript i comma j"><msub><mi>d</mi> <mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub></math>
    so the minimum is 0 and the maximum is 1.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add the pairwise scaled distances between variables together, using either a
    simple or a weighted mean, to create the distance matrix.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To illustrate Gower’s distance, take a few rows from the loan data in *R*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'The function `daisy` in the `cluster` package in *R* can be used to compute
    Gower’s distance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: At the moment of this writing, Gower’s distance is not available in any of the
    popular Python packages. However, activities are ongoing to include it in `scikit-learn`.
    We will update the accompanying source code once the implementation is released.
  prefs: []
  type: TYPE_NORMAL
- en: 'All distances are between 0 and 1. The pair of records with the biggest distance
    is 2 and 3: neither has the same values for `home` and `purpose`, and they have
    very different levels of `dti` (debt-to-income) and `payment_inc_ratio`. Records
    3 and 5 have the smallest distance because they share the same values for `home`
    and `purpose`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can pass the Gower’s distance matrix calculated from `daisy` to `hclust`
    for hierarchical clustering (see [“Hierarchical Clustering”](#HierarchicalClustering)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting dendrogram is shown in [Figure 7-14](#DendroLoan). The individual
    records are not distinguishable on the x-axis, but we can cut the dendrogram horizontally
    at 0.5 and examine the records in one of the subtrees with this code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: This subtree consists entirely of owners with a loan purpose labeled as “debt_consolidation.”
    While strict separation is not true of all subtrees, this illustrates that the
    categorical variables tend to be grouped together in the clusters.
  prefs: []
  type: TYPE_NORMAL
- en: '![A dendrogram of hclust applied to a sample of loan default data with mixed
    variable types.](Images/psd2_0714.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-14\. A dendrogram of `hclust` applied to a sample of loan default data
    with mixed variable types
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Problems with Clustering Mixed Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*K*-means and PCA are most appropriate for continuous variables. For smaller
    data sets, it is better to use hierarchical clustering with Gower’s distance.
    In principle, there is no reason why *K*-means can’t be applied to binary or categorical
    data. You would usually use the “one hot encoder” representation (see [“One Hot
    Encoder”](ch06.xhtml#OneHotEncoder)) to convert the categorical data to numeric
    values. In practice, however, using *K*-means and PCA with binary data can be
    difficult.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If the standard *z*-scores are used, the binary variables will dominate the
    definition of the clusters. This is because 0/1 variables take on only two values,
    and *K*-means can obtain a small within-cluster sum-of-squares by assigning all
    the records with a 0 or 1 to a single cluster. For example, apply `kmeans` to
    loan default data including factor variables `home` and `pub_rec_zero`, shown
    here in *R*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'In *Python*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: The top four clusters are essentially proxies for the different levels of the
    factor variables. To avoid this behavior, you could scale the binary variables
    to have a smaller variance than other variables. Alternatively, for very large
    data sets, you could apply clustering to different subsets of data taking on specific
    categorical values. For example, you could apply clustering separately to those
    loans made to someone who has a mortgage, owns a home outright, or rents.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For dimension reduction of numeric data, the main tools are either principal
    components analysis or *K*-means clustering. Both require attention to proper
    scaling of the data to ensure meaningful data reduction.
  prefs: []
  type: TYPE_NORMAL
- en: For clustering with highly structured data in which the clusters are well separated,
    all methods will likely produce a similar result. Each method offers its own advantage.
    *K*-means scales to very large data and is easily understood. Hierarchical clustering
    can be applied to mixed data types—numeric and categorical—and lends itself to
    an intuitive display (the dendrogram). Model-based clustering is founded on statistical
    theory and provides a more rigorous approach, as opposed to the heuristic methods.
    For very large data, however, *K*-means is the main method used.
  prefs: []
  type: TYPE_NORMAL
- en: With noisy data, such as the loan and stock data (and much of the data that
    a data scientist will face), the choice is more stark. *K*-means, hierarchical
    clustering, and especially model-based clustering all produce very different solutions.
    How should a data scientist proceed? Unfortunately, there is no simple rule of
    thumb to guide the choice. Ultimately, the method used will depend on the data
    size and the goal of the application.
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch07.xhtml#idm46522838490648-marker)) This and subsequent sections in
    this chapter © 2020 Datastats, LLC, Peter Bruce, Andrew Bruce, and Peter Gedeck;
    used with permission.
  prefs: []
  type: TYPE_NORMAL
