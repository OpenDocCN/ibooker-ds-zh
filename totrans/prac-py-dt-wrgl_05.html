<html><head></head><body><section data-pdf-bookmark="Chapter 5. Accessing Web-Based Data" data-type="chapter" epub:type="chapter"><div class="chapter" id="chapter5">&#13;
<h1><span class="label">Chapter 5. </span>Accessing Web-Based Data</h1>&#13;
&#13;
&#13;
<p>The internet <a data-primary="data sources" data-secondary="file-based" data-seealso="API" data-type="indexterm" id="idm45143408874832"/><a data-primary="data sources" data-secondary="feed-based" data-seealso="API" data-type="indexterm" id="idm45143408873552"/><a data-primary="file-based data sources" data-seealso="API" data-type="indexterm" id="idm45143408872336"/><a data-primary="feed-based data sources" data-seealso="API" data-type="indexterm" id="idm45143408871392"/><a data-primary="web-based data sources" data-see="API; feed-based data sources; file-based data sources" data-type="indexterm" id="idm45143408870448"/>is an incredible source of data; it is, arguably, the reason that data has become such a dominant part of our social, economic, political, and even creative lives. In <a data-type="xref" href="ch04.html#chapter4">Chapter 4</a>, we focused our data wrangling efforts on the process of accessing and reformatting file-based data that had already been saved to our devices or to the cloud. At the same time, much of it came from the internet originally—whether it was downloaded from a website, like the unemployment data, or retrieved from a URL, like the Citi Bike data. Now that we have a handle on how to use Python to parse and transform a variety of file-based data formats, however, it’s time to look at what’s involved in collecting those files in the first place—especially when the data they contain is of the real-time, feed-based variety. To do this, we’re going to spend the bulk of this chapter learning how to get ahold of data made available through APIs—those <em>a</em>pplication <em>p</em>rogramming <em>i</em>nterfaces I mentioned early in <a data-type="xref" href="ch04.html#chapter4">Chapter 4</a>. APIs are the primary (and sometimes only) way that we can access the data generated by real-time or on-demand services like social media platforms, streaming music, and search services—as well as many other private and public (e.g., government-generated) data sources.</p>&#13;
&#13;
<p>While the many benefits of APIs (see <a data-type="xref" href="#why_apis">“Why APIs?”</a> for a refresher) make them a popular resource for data-collecting companies to offer, there are significant costs and risks<a data-primary="API (application programming interface)" data-secondary="risks of" data-type="indexterm" id="idm45143408863792"/> to doing so. For advertising-driven businesses like social media platforms, an outside product or project that is too comprehensive in its data collection is a profit risk. The ready availability of so much data about individuals has also significantly <a href="https://dataprivacylab.org/projects/kanonymity/kanonymity.pdf">increased privacy risks</a>. As a result, accessing data via many APIs requires registering with the data collector in advance, and even completing a code-based login or <em>authentication</em> process anytime you request data. At the same time, the data accessibility that APIs offer is a powerful tool for improving the transparency of government systems<sup><a data-type="noteref" href="ch05.html#idm45143408861008" id="idm45143408861008-marker">1</a></sup> and accountability for private companies,<sup><a data-type="noteref" href="ch05.html#idm45143408859632" id="idm45143408859632-marker">2</a></sup> so the up-front work of creating an account and protecting any Python scripts you make that access API-based data is well worth the effort.</p>&#13;
&#13;
<p>Over the course of this chapter, we’ll cover how to access a range of web-based, feed-type datasets via APIs, addressing everything from basic, no-login-required resources all the way to the multistep, highly protected APIs of social media platforms like Twitter. As we’ll see in <a data-type="xref" href="#online_xml_and_json">“Accessing Online XML and JSON”</a>, the simpler end of this spectrum just involves using the Python <em>requests</em> library to download a web page already formatted as JSON or XML—all we need is the URL. In <a data-type="xref" href="#basic_authentication">“Specialized APIs: Adding Basic Authentication”</a>, we’ll move on to the process of accessing data made available through the <a href="https://fred.stlouisfed.org/docs/api/fred">Federal Reserve Economic Database (FRED)</a> API. This is the same data we looked at in Examples <a data-type="xref" data-xrefstyle="select:labelnumber" href="ch04.html#xml_parsing">4-12</a> and <a data-type="xref" data-xrefstyle="select:labelnumber" href="ch04.html#json_parsing">4-15</a>, but rather than working with example files that I’ve provided, you’ll be programmatically downloading whatever data is most recent <em>whenever you run the script</em>.</p>&#13;
&#13;
<p>This will require both creating a login on the FRED website as well as creating—and protecting—your own basic API “key” in order to retrieve data. Finally, in <a data-type="xref" href="#oauth_apis">“Specialized APIs: Working With OAuth”</a> we’ll cover the more complex API authentication process required for social media platforms like Twitter. Despite the degree of up-front work involved, learning how to programmatically interact with APIs like this has big payoffs—for the most part, you’ll be able to rerun these scripts at any time to retrieve the most up-to-date data these services offer.<sup><a data-type="noteref" href="ch05.html#idm45143408848400" id="idm45143408848400-marker">3</a></sup> Of course, since not every data source we need offers an API, we’ll wrap up the chapter with <a data-type="xref" href="#web_scraping">“Web Scraping: The Data Source of Last Resort”</a> by explaining how we can use code to <em>responsibly</em> “scrape” data from websites with the <em>Beautiful Soup</em> Python library. Though in many cases these data-access tasks <em>could</em> be accomplished with a browser and mouse, you’ll quickly see how using Python helps lets us scale our data-retrieval efforts by making the process faster and more repeatable.</p>&#13;
<aside class="pagebreak-before less_space" data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="why_apis">&#13;
<h5>Why APIs?</h5>&#13;
<p>In <a data-type="xref" href="ch04.html#finding_feed_data">“Where to find feed-type data”</a> we <a data-primary="API (application programming interface)" data-secondary="benefits of" data-type="indexterm" id="idm45143408842672"/>touched on some of the reasons that companies and organizations provide data through APIs: when people use freely available data from a company’s API to make some other product or service, they are effectively providing the company with free advertising and brand awareness. These API-based projects also act as a kind of free research and development (R&amp;D) for the source company: apps that do well indicate a potentially profitable new business direction.</p>&#13;
&#13;
<p>Even for government and nonprofit organizations, however, offering APIs for their data has appeal. Those organizations, too, benefit from the public exposure and unique insights that others’ use of their data can provide. From a data wrangling perspective, these APIs help support transparency work, making it easier for civic technologists (volunteer or otherwise) to create more usable and accessible interfaces for public interest information. By separating the <em>data</em> from the <em>interface</em> that people use to access it, APIs provide a way to build public data tools that are also more resilient and future compatible.</p>&#13;
</div></aside>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Accessing Online XML and JSON" data-type="sect1"><div class="sect1" id="online_xml_and_json">&#13;
<h1>Accessing Online XML and JSON</h1>&#13;
&#13;
<p>In <a data-type="xref" href="ch04.html#wrangling_feed_data">“Wrangling Feed-Type Data with Python”</a>, we<a data-primary=".xml files (Extensible Markup Language)" data-primary-sortas="xml files" data-secondary="downloading" data-type="indexterm" id="xml-download"/><a data-primary=".json files (JavaScript Object Notation)" data-primary-sortas="json files" data-secondary="downloading" data-type="indexterm" id="json-download"/><a data-primary="downloading" data-secondary=".json and .xml files" data-type="indexterm" id="download-json-xml"/><a data-primary="data sources" data-secondary="feed-based" data-tertiary="downloading" data-type="indexterm" id="data-source-feed-download"/><a data-primary="feed-based data sources" data-secondary="downloading" data-type="indexterm" id="feed-based-download"/> explored the process of accessing and transforming two common forms of web-based data: XML and JSON. What we didn’t address, however, was how to actually get those data files from the internet onto your computer. With the help of the versatile Python <em>requests</em> library, however, it only take a few lines of code to access and download that data without ever having to open a web browser.</p>&#13;
&#13;
<p>For the sake of comparison, let’s start by “manually” downloading two of the files we’ve used in previous examples: the BBC’s RSS feed of articles from <a data-type="xref" href="ch04.html#bbc_example">Example 4-13</a> and the Citi Bike JSON data mentioned in <a data-type="xref" href="ch04.html#one_data_two_ways">“One Data Source, Two Ways”</a>.</p>&#13;
&#13;
<p>For both of these data sources, the process is basically the same:</p>&#13;
<ol>&#13;
<li>&#13;
<p>Visit the target URL; in this case, one of the following:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p><a href="http://feeds.bbci.co.uk/news/science_and_environment/rss.xml"><em class="hyperlink">http://feeds.bbci.co.uk/news/science_and_environment/rss.xml</em></a></p>&#13;
</li>&#13;
<li>&#13;
<p><a href="https://gbfs.citibikenyc.com/gbfs/en/station_status.json"><em class="hyperlink">https://gbfs.citibikenyc.com/gbfs/en/station_status.json</em></a></p>&#13;
</li>&#13;
</ul>&#13;
</li>&#13;
<li>&#13;
<p>Context-click (also known as “right-click” or sometime “Ctrl+click,” depending on your system). From the menu that appears, simply choose “Save As” and save the file to the same folder where your Jupyter notebook or Python script is &#13;
<span class="keep-together">located</span>.</p>&#13;
</li>&#13;
&#13;
</ol>&#13;
&#13;
<p>That’s it! Now you can run the scripts from <a data-type="xref" href="ch04.html#bbc_example">Example 4-13</a> on that updated XML file or paste the Citi Bike JSON data into <a href="https://jsonlint.com"><em class="hyperlink">https://jsonlint.com</em></a> to see what it looks like when it’s properly formatted. Note that even though the BBC page looks almost like a “normal” website in your browser, true to its <em>.xml</em> file extension, it downloads as well-formatted XML.</p>&#13;
&#13;
<p>Now that we’ve seen how to do this part of the process by hand, let’s see what it takes to do the same thing in Python. To keep this short, the code in <a data-type="xref" href="#data_download">Example 5-1</a> will download and save <em>both</em> files, one after the other.</p>&#13;
<div data-type="example" id="data_download">&#13;
<h5><span class="label">Example 5-1. </span>data_download.py</h5>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="c1"># a basic example of downloading data from the web with Python,</code>&#13;
<code class="c1"># using the requests library</code>&#13;
<code class="c1">#</code>&#13;
<code class="c1"># the source data we are downloading will come from the following URLs:</code>&#13;
<code class="c1"># http://feeds.bbci.co.uk/news/science_and_environment/rss.xml</code>&#13;
<code class="c1"># https://gbfs.citibikenyc.com/gbfs/en/station_status.json</code>&#13;
&#13;
&#13;
<code class="c1"># the `requests` library lets us write Python code that acts like</code>&#13;
<code class="c1"># a web browser</code>&#13;
<code class="kn">import</code> <code class="nn">requests</code>&#13;
&#13;
<code class="c1"># our chosen XML filename</code>&#13;
<code class="n">XMLfilename</code> <code class="o">=</code> <code class="s2">"BBC_RSS.xml"</code>&#13;
&#13;
<code class="c1"># open a new, writable file for our XML output</code>&#13;
<code class="n">xml_output_file</code> <code class="o">=</code> <code class="nb">open</code><code class="p">(</code><code class="n">XMLfilename</code><code class="p">,</code><code class="s2">"w"</code><code class="p">)</code>&#13;
&#13;
<code class="c1"># use the requests library's "get" recipe to access the contents of our</code>&#13;
<code class="c1"># target URL and store it in our `xml_data` variable</code>&#13;
<code class="n">xml_data</code><code class="o">=</code><code class="n">requests</code><code class="o">.</code><code class="n">get</code><code class="p">(</code><code class="s1">'http://feeds.bbci.co.uk/news/science_and_environment/rss.xml'</code><code class="p">)</code>&#13;
&#13;
<code class="c1"># the requests library's `get()` function puts contents of the web page</code>&#13;
<code class="c1"># in a property `text`</code>&#13;
<code class="c1"># we'll `write` that directly to our `xml_output_file`</code>&#13;
<code class="n">xml_output_file</code><code class="o">.</code><code class="n">write</code><code class="p">(</code><code class="n">xml_data</code><code class="o">.</code><code class="n">text</code><code class="p">)</code>&#13;
&#13;
<code class="c1"># close our xml_output_file</code>&#13;
<code class="n">xml_output_file</code><code class="o">.</code><code class="n">close</code><code class="p">()</code>&#13;
&#13;
<code class="c1"># our chosen JSON filename</code>&#13;
<code class="n">JSONfilename</code> <code class="o">=</code> <code class="s2">"citibikenyc_station_status.json"</code>&#13;
&#13;
<code class="c1"># open a new, writable file for our JSON output</code>&#13;
<code class="n">json_output_file</code> <code class="o">=</code> <code class="nb">open</code><code class="p">(</code><code class="n">JSONfilename</code><code class="p">,</code><code class="s2">"w"</code><code class="p">)</code>&#13;
&#13;
<code class="c1"># use the `requests` library's `get()` recipe to access the contents of our</code>&#13;
<code class="c1"># target URL and store it in our `json_data` variable</code>&#13;
<code class="n">json_data</code> <code class="o">=</code> <code class="n">requests</code><code class="o">.</code><code class="n">get</code><code class="p">(</code><code class="s1">'https://gbfs.citibikenyc.com/gbfs/en/station_status.json'</code><code class="p">)</code>&#13;
&#13;
<code class="c1"># `get()` the contents of the web page and write its `text`</code>&#13;
<code class="c1"># directly to `json_output_file`</code>&#13;
<code class="n">json_output_file</code><code class="o">.</code><code class="n">write</code><code class="p">(</code><code class="n">json_data</code><code class="o">.</code><code class="n">text</code><code class="p">)</code>&#13;
&#13;
<code class="c1"># close our json_output_file</code>&#13;
<code class="n">json_output_file</code><code class="o">.</code><code class="n">close</code><code class="p">()</code></pre></div>&#13;
&#13;
<p>Pretty<a data-primary="Python" data-secondary="downloading web-based data" data-type="indexterm" id="idm45143408957152"/> simple, right? Apart from different filenames, the <em>.xml</em> and <em>.json</em> files produced by <a data-type="xref" href="#data_download">Example 5-1</a> are exactly the same as the ones we saved manually from the web. And once we have this script set up, of course, all we have to do to get the latest data is run it again and the new data will overwrite the earlier <a data-primary=".xml files (Extensible Markup Language)" data-primary-sortas="xml files" data-secondary="downloading" data-startref="xml-download" data-type="indexterm" id="idm45143408392816"/><a data-primary=".json files (JavaScript Object Notation)" data-primary-sortas="json files" data-secondary="downloading" data-startref="json-download" data-type="indexterm" id="idm45143408391328"/><a data-primary="downloading" data-secondary=".json and .xml files" data-startref="download-json-xml" data-type="indexterm" id="idm45143408389744"/><a data-primary="data sources" data-secondary="feed-based" data-startref="data-source-feed-download" data-tertiary="downloading" data-type="indexterm" id="idm45143408388528"/><a data-primary="feed-based data sources" data-secondary="downloading" data-startref="feed-based-download" data-type="indexterm" id="idm45143408387024"/>files.</p>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45143408385680">&#13;
<h5>Download by Hand, or Write a Program?</h5>&#13;
<p>In almost <a data-primary="downloading" data-secondary="manually versus programmatically" data-type="indexterm" id="idm45143408384384"/>every phase of our data wrangling work, there will be moments when we have to decide between doing part of a task “by hand” and writing some kind of program. In <a data-type="xref" href="ch04.html#json_parsing">Example 4-15</a>, for example, we didn’t try to have our program “detect” too much about our JSON file structure; we simply <em>looked</em> at the data and then wrote our code to match. Now that we can <em>download</em> data using Python, we have another choice: save down the data “by hand” or do so programmatically.</p>&#13;
&#13;
<p>For the most part, I will always recommend that you download a sample copy of your data “by hand” when you’re first starting to work with it, simply because doing so is (usually) less work than writing a script to do it. Especially if your data source is XML or JSON, you’ll probably have found it through a web search of some kind, so you’ll already have the data open in a web browser right in front of you. At that point, you may as well just context-click and save a copy of the data to your device then and there, rather than opening up a Jupyter notebook or code editor in order to write some code. Since you’ll <em>also</em> need to look through the data carefully to assess its quality, doing a first download this way almost always makes sense. Once you know it’s worth looking at more closely, of course, you’ll probably want to automate that download process by writing a Python script to do it.</p>&#13;
</div></aside>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Introducing APIs" data-type="sect1"><div class="sect1" id="idm45143408983568">&#13;
<h1>Introducing APIs</h1>&#13;
&#13;
<p>Up until <a data-primary="API (application programming interface)" data-type="indexterm" id="api"/>this point, most of our data wrangling work has focused on data sources whose contents are almost entirely controlled by the data provider. In fact, while the contents of spreadsheet files, and documents—and even the web pages containing XML and JSON that we accessed just now in <a data-type="xref" href="#data_download">Example 5-1</a>—may change based on <em>when</em> we access them, we don’t really have any influence on <em>what</em> data they contain.</p>&#13;
&#13;
<p>At the same time, most of us are used to using the internet to get information that’s much more tailored to our needs. Often our first step when looking for information is to enter keywords or phrases into a search engine, and we expect to receive a list of highly customized “results” based (at least in part) on our chosen combination of terms. Sure, we can’t control what web pages are actually out there for our search to retrieve, but this process is so common—and so useful—for most of us that we rarely stop to think about what is happening behind the scenes.</p>&#13;
&#13;
<p>Despite their visually oriented interfaces, search engines are actually just a special instance of APIs. They are essentially just web pages that let you <em>interface</em> with a database containing information about websites on the internet, such as their URLs, titles, text, images, videos, and more. When you enter your search terms and hit Enter or Return, the search engine <em>queries</em> its database for web content that “matches” your search in some respect and then updates the web page you’re looking at to display those results in a list. Though the specialized APIs made available by social media platforms and other online services require us to authenticate <em>and</em> structure our searches in a very particular way, there are enough features shared between search engines and more specialized APIs that we can learn something useful about APIs by deconstructing a basic <a data-primary="API (application programming interface)" data-startref="api" data-type="indexterm" id="idm45143408371344"/>Google search.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Basic APIs: A Search Engine Example" data-type="sect1"><div class="sect1" id="idm45143408369968">&#13;
<h1>Basic APIs: A Search Engine Example</h1>&#13;
&#13;
<p>Though <a data-primary="API (application programming interface)" data-secondary="search engines as" data-type="indexterm" id="api-search-engine"/><a data-primary="search engines, as APIs" data-type="indexterm" id="search-engine-api"/><a data-primary="query strings" data-secondary="in URLs" data-type="indexterm" id="query-string-url"/><a data-primary="URLs, query strings in" data-type="indexterm" id="url-query-string"/><a data-primary="key/value pairs" data-secondary="in URLs" data-type="indexterm" id="key-value-url"/>an internet search engine is probably the most straightforward form of API around, that’s not always obvious from the way we see them behave onscreen. For example, if you were to visit Google and search for “weather sebastopol,” you would probably see a page that looks something like <a data-type="xref" href="#google_search_results">Figure 5-1</a>.</p>&#13;
&#13;
<figure><div class="figure" id="google_search_results">&#13;
<img alt="Sebastopol weather search results" src="assets/ppdw_0501.png"/>&#13;
<h6><span class="label">Figure 5-1. </span>Sample search results</h6>&#13;
</div></figure>&#13;
&#13;
<p>While the format of the search results is probably pretty familiar, right now let’s take a closer look at what’s happening in the URL bar. What you see will definitely be different from the <a data-type="xref" href="#google_search_results">Figure 5-1</a> screenshot, but it should contain at least some of the same information. Specifically, look through the text that now appears in <em>your</em> URL bar to find the following:</p>&#13;
<pre>q=weather+sebastopol</pre>&#13;
&#13;
<p>Found it? Great. Now without refreshing the page, change the text in the search box to “weather san francisco” and hit Enter. Once again look through the text in the URL to find:</p>&#13;
<pre>q=weather+san+francisco</pre>&#13;
&#13;
<p>Finally, copy and paste the following into your URL bar and hit Enter:</p>&#13;
<pre>https://www.google.com/search?q=weather+san+francisco</pre>&#13;
&#13;
<p>Notice anything? Hopefully, you’re seeing the same (or <em>almost</em> the same) search results when you type “weather san francisco” into Google’s search bar and hit Enter as when you directly visit the Google search URL with the key/value pair of <code>q=weather+san+francisco</code> appended (e.g., <code>https://www.google.com/search?q=weather+san+francisco</code>). That’s because <code>q=weather+san+francisco</code> is the part of the <em>query string</em> that delivers your actual search terms to Google’s database; everything else is just additional information Google tacks on for customization or tracking purposes.</p>&#13;
&#13;
<p>While Google can (and will!) add whatever it wants to our search URL, <em>we</em> can also add other useful key/value pairs. For example, in <a data-type="xref" href="ch04.html#smart_searching">“Smart Searching for Specific Data Types”</a>, we looked at searching for specific file types, such as <em>.xml</em>, by adding <code>filetype: .xml</code> to our search box query; we can do the same thing directly in the URL bar by adding the corresponding key/value pair of <code>as_filetype=xml</code> to our query string:</p>&#13;
<pre>https://www.google.com/search?q=weather+san+francisco&amp;as_filetype=xml</pre>&#13;
&#13;
<p>Not only will this return results in the correct format, but notice that it updates the contents of the search box as well!</p>&#13;
&#13;
<p>The behavior of the Google search engine in this situation is almost identical to what we’ll see with more specialized APIs in the remainder of this chapter. Most APIs follow the general structure we’re seeing in this search example, where an <em>endpoint</em> (in this case <code>https://www.google.com/search</code>) is combined with one or more <em>query parameters</em> or <em>key/value pairs</em> (such as <code>as_filetype=xml</code> or <code>q=weather+san+francisco</code>), which comprise the <em>query string</em> that is appended after the question mark (<code>?</code>). A general overview of API endpoint and query string structure is shown in <a data-type="xref" href="#query_string_structure">Figure 5-2</a>.</p>&#13;
&#13;
<figure><div class="figure" id="query_string_structure">&#13;
<img alt="Sebastopol weather search query, limited to 5 results" src="assets/ppdw_0502.png"/>&#13;
<h6><span class="label">Figure 5-2. </span>Basic query string structure</h6>&#13;
</div></figure>&#13;
&#13;
<p>While this structure is pretty universal, here are a couple of other useful tips about query string-based APIs:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Key/value pairs (such as <code>as_filetype=xml</code>, <code>num=5</code>, or even <code>q=weather+san+francisco</code>) can appear <em>in any order</em>, as long as they are added after the question mark (<code>?</code>) that indicates the start of the query string.</p>&#13;
</li>&#13;
<li>&#13;
<p>The particular keys and values that are meaningful for a given API are determined by the API provider and can only be identified by reading the API documentation, or through experimentation (though this can present problems of its own). Anything appended to the query string that is not a recognized key or valid parameter value will probably be ignored.</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>While these characteristics are common to almost all APIs, the vast majority of them will not allow you to access any data at all without first identifying (or <em>authenticating</em>) yourself by creating a login and providing unique, specialized “keys” to the API along with your queries. This part of the API process is what we’ll turn to <a data-primary="API (application programming interface)" data-secondary="search engines as" data-startref="api-search-engine" data-type="indexterm" id="idm45143408333968"/><a data-primary="search engines, as APIs" data-startref="search-engine-api" data-type="indexterm" id="idm45143408332752"/><a data-primary="query strings" data-secondary="in URLs" data-startref="query-string-url" data-type="indexterm" id="idm45143408331808"/><a data-primary="URLs, query strings in" data-startref="url-query-string" data-type="indexterm" id="idm45143408330592"/><a data-primary="key/value pairs" data-secondary="in URLs" data-startref="key-value-url" data-type="indexterm" id="idm45143408329648"/>next.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Specialized APIs: Adding Basic Authentication" data-type="sect1"><div class="sect1" id="basic_authentication">&#13;
<h1>Specialized APIs: Adding Basic Authentication</h1>&#13;
&#13;
<p>The <a data-primary="API (application programming interface)" data-secondary="authentication" data-type="indexterm" id="idm45143408326560"/><a data-primary="authentication for APIs" data-type="indexterm" id="idm45143408325584"/>first step in using most APIs is creating some kind of account with the API provider. Although many APIs allow <em>you</em> to use them for free, the process of compiling, storing, searching for, and returning data to you over the internet still presents risks and costs money, so providers want to track who is using their APIs and be able to cut off your access if they want to.<sup><a data-type="noteref" href="ch05.html#idm45143408324000" id="idm45143408324000-marker">4</a></sup> This first part of the <em>authentication</em> process usually consists of creating an account and requesting an API “key” for yourself and/or each project, program, or “app” that you plan to have interact with the API. In a “basic” API authentication process, like the one we’ll go through now, once you’ve created your API key on the service provider’s website, all you need to do to retrieve data successfully is append your key to your data request just like any other query parameter, and you’re all set.</p>&#13;
&#13;
<p>As an example, let’s get set up to programmatically access the unemployment data we worked with in <a data-type="xref" href="ch04.html#json_parsing">Example 4-15</a>. We’ll start by making an account on the FRED website and requesting an API key. Once we have that, we can just append it to our query string and start downloading data!</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Getting a FRED API Key" data-type="sect2"><div class="sect2" id="idm45143408320512">&#13;
<h2>Getting a FRED API Key</h2>&#13;
&#13;
<p>To<a data-primary="API (application programming interface)" data-secondary="authentication" data-tertiary="creating account" data-type="indexterm" id="idm45143408318816"/><a data-primary="authentication for APIs" data-secondary="creating account" data-type="indexterm" id="idm45143408317552"/><a data-primary="accounts" data-secondary="FRED, creating" data-type="indexterm" id="idm45143408316576"/><a data-primary="FRED (Federal Reserve Economic Database)" data-secondary="developer account, creating" data-type="indexterm" id="idm45143408315632"/><a data-primary="developer accounts" data-secondary="FRED, creating" data-type="indexterm" id="idm45143408314624"/> create an account with the Federal Reserve Economic Database (FRED), visit <a href="https://fred.stlouisfed.org"><em class="hyperlink">https://fred.stlouisfed.org</em></a> and click on My Account in the upper-righthand corner, as shown in <a data-type="xref" href="#fred_login">Figure 5-3</a>.</p>&#13;
&#13;
<figure><div class="figure" id="fred_login">&#13;
<img alt="Federal Reserve Economic Database (FRED) homepage" src="assets/ppdw_0503.png"/>&#13;
<h6><span class="label">Figure 5-3. </span>FRED login link</h6>&#13;
</div></figure>&#13;
&#13;
<p>Follow the directions in the pop-up, either creating an account with a new username and password or using your Google account to log in. Once your registration/login process is complete, clicking on the My Account link will open a drop-down menu that includes an option titled API Keys, as shown in <a data-type="xref" href="#fred_account_option">Figure 5-4</a>.</p>&#13;
&#13;
<figure><div class="figure" id="fred_account_option">&#13;
<img alt="FRED account actions" src="assets/ppdw_0504.png"/>&#13;
<h6><span class="label">Figure 5-4. </span>FRED account actions</h6>&#13;
</div></figure>&#13;
&#13;
<p>Clicking <a data-primary="API (application programming interface)" data-secondary="authentication" data-tertiary="API key requests" data-type="indexterm" id="idm45143408305712"/><a data-primary="authentication for APIs" data-secondary="API key requests" data-type="indexterm" id="idm45143408304416"/><a data-primary="FRED (Federal Reserve Economic Database)" data-secondary="API keys" data-tertiary="requesting" data-type="indexterm" id="idm45143408303472"/><a data-primary="API keys" data-secondary="requesting" data-type="indexterm" id="idm45143408302368"/><a data-primary="requesting" data-secondary="API keys" data-type="indexterm" id="idm45143408301520"/>that link will take you to a page where you can request one or more API keys using the Request API Key button. On the next page, you’ll be asked to provide a brief description of the application with which the API key will be used; this can just be a sentence or two. You’ll also need to read and agree to the Terms of Service by checking the provided box. Complete the process by clicking the Request API Key button.</p>&#13;
&#13;
<p>If your request is successful (and it should be), you’ll be taken to an interim page that will display the key that’s been generated. If you leave that page, you can always just log in and visit <a href="https://research.stlouisfed.org/useraccount/apikeys"><em class="hyperlink">https://research.stlouisfed.org/useraccount/apikeys</em></a> to see all of your available API keys.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Using Your API key to Request Data" data-type="sect2"><div class="sect2" id="idm45143408297888">&#13;
<h2>Using Your API key to Request Data</h2>&#13;
&#13;
<p>Now that<a data-primary="API (application programming interface)" data-secondary="authentication" data-tertiary="data requests with API key" data-type="indexterm" id="idm45143408296256"/><a data-primary="authentication for APIs" data-secondary="data requests with API key" data-type="indexterm" id="idm45143408294944"/><a data-primary="FRED (Federal Reserve Economic Database)" data-secondary="API keys" data-tertiary="data requests with" data-type="indexterm" id="idm45143408293984"/><a data-primary="API keys" data-secondary="data requests with" data-type="indexterm" id="idm45143408292672"/><a data-primary="requesting" data-secondary="data with API keys" data-type="indexterm" id="idm45143408291728"/><a data-primary="query strings" data-secondary="in APIs" data-type="indexterm" id="query-string-api"/> you have an API key, let’s explore how to request the data we used in <a data-type="xref" href="ch04.html#json_parsing">Example 4-15</a>. Start by trying to load the following URL in a browser:</p>&#13;
&#13;
<pre class="small-and-no-indent" data-type="programlisting">https://api.stlouisfed.org/fred/series/observations?series_id=U6RATE&amp;file_type=json</pre>&#13;
&#13;
<p>Even if you’re already logged in to FRED on that browser, you’ll see something like this:</p>&#13;
&#13;
<pre data-type="programlisting">{"error_code":400,"error_message":"Bad Request.  Variable api_key is not set.&#13;
Read https:\/\/research.stlouisfed.org\/docs\/api\/api_key.html for more&#13;
information."}</pre>&#13;
&#13;
<p>This is a pretty descriptive error message: it not only tells you that something went wrong, but it gives you some idea of how to fix it. Since you just created an API key, all you have to do is add it to your request as an additional parameter:</p>&#13;
<pre class="small-and-no-indent" data-type="programlisting">&#13;
https://api.stlouisfed.org/fred/series/observations?series_id=U6RATE&amp;file_type=json&amp;&#13;
api_key=<em>YOUR_API_KEY_HERE</em>&#13;
</pre>&#13;
&#13;
<p>replacing <code>YOUR_API_KEY_HERE</code> with, of course, your API key. Loading that page in a browser should return something that looks something like this:</p>&#13;
&#13;
<pre data-type="programlisting">{"realtime_start":"2021-02-03","realtime_end":"2021-02-03","observation_start":&#13;
"1600-01-01","observation_end":"9999-12-31","units":"lin","output_type":1,&#13;
"file_type":"json","order_by":"observation_date","sort_order":"asc","count":324,&#13;
"offset":0,"limit":100000,"observations":[{"realtime_start":"2021-02-03",&#13;
"realtime_end":"2021-02-03","date":"1994-01-01","value":"11.7"},&#13;
...&#13;
{"realtime_start":"2021-02-03","realtime_end":"2021-02-03","date":"2020-12-01",&#13;
"value":"11.7"}]}</pre>&#13;
&#13;
<p>Pretty nifty, right? Now that you know how to use your API key to make data requests, it’s time to review how to both customize those requests <em>and</em> protect your API key when you use it in Python scripts.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Reading API Documentation" data-type="sect1"><div class="sect1" id="reading_api_docs">&#13;
<h1>Reading API Documentation</h1>&#13;
&#13;
<p>As you <a data-primary="API (application programming interface)" data-secondary="documentation, reading" data-type="indexterm" id="api-documentation-reading"/><a data-primary="documentation" data-secondary="for APIs, reading" data-secondary-sortas="APIs" data-type="indexterm" id="documentation-api-read"/><a data-primary="reading API documentation" data-type="indexterm" id="read-api-documentation"/><a data-primary="FRED (Federal Reserve Economic Database)" data-secondary="API documentation, reading" data-type="indexterm" id="fred-api-documentation"/>can see from the preceding example, once we have an API key, we can load the latest data from the FRED database whenever we want. All we need to do is construct our query string and add our API key.</p>&#13;
&#13;
<p>But how do we know what key/value pairs the FRED API will accept and what type of information they’ll return? The only really reliable way to do this is to read the API <em>documentation</em>, which should offer guidance and (hopefully) examples of how the API can be used.</p>&#13;
&#13;
<p>Unfortunately, there’s no widely adopted standard for API documentation, which means that using a new API is almost always something of a trial-and-error process, especially if the documentation quality is poor or the provided examples don’t include the information you’re looking for. In fact, even <em>finding</em> the documentation for a particular API isn’t always straightforward, and often a web search is the simplest route.</p>&#13;
&#13;
<p>For example, getting to the FRED API documentation from the <a href="https://fred.stlouisfed.org">FRED homepage</a> requires clicking on the Tools tab about halfway down the page, then selecting the Developer API link at the bottom right, which takes you to <a href="https://fred.stlouisfed.org/docs/api/fred"><em class="hyperlink">https://fred.stlouisfed.org/docs/api/fred</em></a>. By contrast, a web search for “fred api documentation” will take you to the same page, shown in <a data-type="xref" href="#fred_api_docs_main">Figure 5-5</a>, directly.</p>&#13;
&#13;
<figure><div class="figure" id="fred_api_docs_main">&#13;
<img alt="FRED API documentation homepage" src="assets/ppdw_0505.png"/>&#13;
<h6><span class="label">Figure 5-5. </span>FRED API documentation page</h6>&#13;
</div></figure>&#13;
&#13;
<p class="pagebreak-before less_space">Unfortunately, the list of links on this page is actually a<a data-primary="API (application programming interface)" data-secondary="endpoints" data-type="indexterm" id="idm45143408264720"/><a data-primary="endpoints (APIs)" data-type="indexterm" id="idm45143408263648"/> list of <em>endpoints</em>—different base URLs that you can use to request more specific information (recall that the <em>endpoint</em> is everything before the question mark (<code>?</code>), which separates it from the <em>query string</em>). In the preceding example, you used the endpoint <code>https://api.stlouisfed.org/fred/series/observations</code> and then paired it with the key/value pairs of <code>series_id=U6RATE</code>, <code>file_type=json</code>, and, of course, your API key in order to generate a response.</p>&#13;
&#13;
<p>Scrolling down the page in <a data-type="xref" href="#fred_api_docs_main">Figure 5-5</a> and clicking on the documentation link labeled “fred/series/observations” will take you <a href="https://fred.stlouisfed.org/docs/api/fred/series_observations.html"><em class="hyperlink">https://fred.stlouisfed.org/docs/api/fred/series_observations.html</em></a>, which outlines all of the valid query keys (or <em>parameters</em>) for that particular endpoint, as well as the valid values those keys can have and some sample query URLs, as shown in <a data-type="xref" href="#fred_api_observations_docs">Figure 5-6</a>.</p>&#13;
&#13;
<figure><div class="figure" id="fred_api_observations_docs">&#13;
<img alt="FRED API 'observations' endpoint documentation" src="assets/ppdw_0506.png"/>&#13;
<h6><span class="label">Figure 5-6. </span>FRED API <em>observations</em> endpoint documentation</h6>&#13;
</div></figure>&#13;
&#13;
<p>For example, you could limit the number of observations returned by using the <code>limit</code> parameter, or reverse the sort order of the returned results by adding <code>sort_order=desc</code>. You can also specify particular data formats (such as <code>file_type=xml</code> for XML output) or units (such as <code>units=pc1</code> to see the output as percent change from a <a data-primary="API (application programming interface)" data-secondary="documentation, reading" data-startref="api-documentation-reading" data-type="indexterm" id="idm45143408250640"/><a data-primary="documentation" data-secondary="for APIs, reading" data-secondary-sortas="APIs" data-startref="documentation-api-read" data-type="indexterm" id="idm45143408249360"/><a data-primary="reading API documentation" data-startref="read-api-documentation" data-type="indexterm" id="idm45143408247872"/><a data-primary="FRED (Federal Reserve Economic Database)" data-secondary="API documentation, reading" data-startref="fred-api-documentation" data-type="indexterm" id="idm45143408246912"/><a data-primary="query strings" data-secondary="in APIs" data-startref="query-string-api" data-type="indexterm" id="idm45143408245584"/>year ago).</p>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45143408244240">&#13;
<h5>History, Revisited</h5>&#13;
<p>At about<a data-primary="FRED (Federal Reserve Economic Database)" data-secondary="data revisions" data-type="indexterm" id="fred-revision"/> the same time I discovered that the U3 unemployment number excluded many people who probably thought of themselves (and almost certainly <em>felt</em>) unemployed, I also learned something else about economic indicator reporting: the numbers often change after the fact. A few months into my work on the economic indicator data, a colleague casually mentioned that the United States’ gross domestic product (GDP) growth rate for the previous quarter had been officially revised. “What?!” I thought. Given that GDP is often referenced in national economic policy, the idea that the measure could simply be changed months later seemed shocking. A recession, for example, is officially defined as multiple consecutive quarters of “negative growth”—that is, when GDP is less than the previous quarter at least twice in a row. But if those numbers can be revised after the fact, what are the consequences for real people—especially the ones who start feeling the effects of the recession long before even the first version of this data comes out?</p>&#13;
&#13;
<p>While I don’t have a solution for the fact that these revisions happen, the good news is that FRED is at least keeping track of them—and you can see what’s happening using the API. That is what’s behind the <code>vintage_dates</code> parameter for the <a href="https://fred.stlouisfed.org/docs/api/fred/series_observations.html"><code>observations</code> endpoint</a>. For example, we can see what the monthly 2020 U6 unemployment figures were believed to be as of May 31, 2020, by making the following API call:</p>&#13;
&#13;
<pre data-type="programlisting">https://api.stlouisfed.org/fred/series/observations?series_id=U6RATE&amp;&#13;
file_type=json&amp;sort_order=desc&amp;limit=3&amp;vintage_dates=2020-05-31&#13;
&amp;api_key=YOUR_API_KEY_HERE</pre>&#13;
&#13;
<p>In late May, then, U6 unemployment for March 2020 was reported at 8.7% but was later revised to 8.8%. The same measure for April was reported at 22.8% but was later revised to 22.9%. While these adjustments may seem small, keep in mind that often the unemployment rate doesn’t change by more than a few tenths of a percent up or down over the course of an entire year.</p>&#13;
&#13;
<p>Most importantly, these changes illustrate one of the more subtle concerns around data integrity: is the information we have <em>now</em> the same as what was available during the period we’re investigating? If not, it’s essential to adjust our claims—if not our data—accordingly.</p>&#13;
</div></aside>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Protecting Your API Key When Using Python" data-type="sect1"><div class="sect1" id="idm45143408280112">&#13;
<h1>Protecting Your API Key When Using Python</h1>&#13;
&#13;
<p>As you<a data-primary="API (application programming interface)" data-secondary="authentication" data-tertiary="API key protection" data-type="indexterm" id="api-authentication-protect"/><a data-primary="authentication for APIs" data-secondary="API key protection" data-type="indexterm" id="authenticate-api-protect"/><a data-primary="API keys" data-secondary="protection in Python" data-type="indexterm" id="api-key-protect"/><a data-primary="Python" data-secondary="API key protection" data-type="indexterm" id="python-api-protect"/> may have already guessed, downloading data from FRED (or other, similar APIs) is as simple as replacing one of the URLs in <a data-type="xref" href="#data_download">Example 5-1</a> with your complete query, because the web page it generates is just another JSON file on the internet.</p>&#13;
&#13;
<p>At the same time, that query contains some especially sensitive information: your API key. Remember that as far as FRED (or any other API owner) is concerned, you are responsible for any activity on their platform that uses your API key. This means that while you <em>always</em> want to be documenting, saving, and versioning your code with something like Git, you <em>never</em> want your API keys or other credentials to end up in a file that others can access.</p>&#13;
<div data-type="warning" epub:type="warning"><h6>Warning</h6>&#13;
<p>Properly protecting your API key takes some effort, and if you’re new to working with <a data-primary="API keys" data-secondary="protection in Python" data-tertiary="importance of" data-type="indexterm" id="idm45143408223952"/><a data-primary="Python" data-secondary="API key protection" data-tertiary="importance of" data-type="indexterm" id="idm45143408222704"/>data, Python, or APIs (or all three), you may be tempted to skip the next couple of sections and just leave your API credentials inside files that could get uploaded to the internet.<sup><a data-type="noteref" href="ch05.html#idm45143408221168" id="idm45143408221168-marker">5</a></sup> Don’t! While right now you may be thinking, “Who’s ever going to bother looking at <em>my</em> work?”  or “I’m just playing around anyway—what difference does it make?” there are two things you should know.</p>&#13;
&#13;
<p>First, as with documentation, if you don’t deal with protecting your credentials correctly now, it will be <em>much</em> more difficult and time-consuming to do so later, in part because by then you’ll have forgotten what exactly is involved, and in part because <em>it may already be too late</em>. Second, while few of us feel that what we’re doing is “important” or visible enough that anyone <em>else</em> would bother looking at it, the reality is that bad actors don’t mind who their scapegoat is—and if you make it easy, they might choose you. The fallout, moreover, might not be limited to getting you kicked off of a data platform. In 2021, the former SolarWinds CEO claimed that the massive breach of thousands of high-security systems through compromises to the company’s software was made possible, in part, because of a weak password that was uploaded to a file on an intern’s personal GitHub account.<sup><a data-type="noteref" href="ch05.html#idm45143408216624" id="idm45143408216624-marker">6</a></sup>&#13;
In other words, even if you’re “just practicing,” you’re better off practicing good security in the first place.</p>&#13;
</div>&#13;
&#13;
<p>Protecting your API credentials is a two-part process:</p>&#13;
<ol>&#13;
<li>&#13;
<p>You need to separate your API key or other sensitive information from the rest of your code. We’ll do this by storing these credentials in a separate file that our main code only loads when the script is actually run.</p>&#13;
</li>&#13;
<li>&#13;
<p>You need a reliable way to ensure that as you are backing up your code using Git, for example, those credential files are <em>never</em> backed up to any online location. We’ll accomplish this by putting the word <code>credentials</code> in the name of any file that includes them and then using a <code>gitignore</code> file to make sure they don’t get uploaded to GitHub.</p>&#13;
</li>&#13;
&#13;
</ol>&#13;
&#13;
<p>The simplest way to achieve both of these things consistently is to define a naming convention for any file that contains API keys or other sensitive login-related information. In this case, we’ll make sure that any such file has the word <code>credentials</code> somewhere in the filename. We’ll then make sure to create or update a special type of Git file known as a <em>.gitignore</em>, which stores rules for telling Git which files in our repo folder should <em>never</em> be committed to our repository and/or uploaded to GitHub. By including a rule for our “credentials” file to <em>.gitignore</em>, we guarantee that no files containing sensitive login information get uploaded to GitHub by accident.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Creating Your “Credentials” File" data-type="sect2"><div class="sect2" id="idm45143408205088">&#13;
<h2>Creating Your “Credentials” File</h2>&#13;
&#13;
<p>Up until <a data-primary="API keys" data-secondary="protection in Python" data-tertiary="creating credentials file" data-type="indexterm" id="idm45143408203568"/><a data-primary="Python" data-secondary="API key protection" data-tertiary="creating credentials file" data-type="indexterm" id="idm45143408202224"/><a data-primary="credentials file" data-secondary="creating" data-type="indexterm" id="idm45143408200992"/>now, we’ve been putting all our code for a particular task—such as downloading or transforming a data file—into a single Python file or notebook. For the purposes of both security and reuse, however, when we’re working with APIs, it makes much more sense to separate our functional code from our credentials. Luckily, this process is very straightforward.</p>&#13;
&#13;
<p>First, create and save a new, empty Python file called <em>FRED_credentials.py</em>. For simplicity’s sake, go ahead and put this file in the same folder where you plan to put the Python code you’ll use to download data from FRED.</p>&#13;
&#13;
<p>Then, simply create a new variable and set its value to your own API key, as shown in <a data-type="xref" href="#FRED_credentials_example">Example 5-2</a>.</p>&#13;
<div data-type="example" id="FRED_credentials_example">&#13;
<h5><span class="label">Example 5-2. </span>Example FRED credentials file</h5>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="n">my_api_key</code><code> </code><code class="o">=</code><code> </code><code class="s2">"</code><em><code class="s2">your_api_key_surrounded_by_double_quotes</code></em><code class="s2">"</code></pre></div>&#13;
&#13;
<p>Now just save your file!</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Using Your Credentials in a Separate Script" data-type="sect2"><div class="sect2" id="idm45143408188032">&#13;
<h2>Using Your Credentials in a Separate Script</h2>&#13;
&#13;
<p>Now that<a data-primary="API keys" data-secondary="protection in Python" data-tertiary="importing credentials file" data-type="indexterm" id="api-key-protect-import"/><a data-primary="Python" data-secondary="API key protection" data-tertiary="importing credentials file" data-type="indexterm" id="python-api-protect-import"/><a data-primary="credentials file" data-secondary="importing" data-type="indexterm" id="credfile-import"/><a data-primary="importing credentials file" data-type="indexterm" id="import-credfile"/> your API key exists as a variable in another file, you can import it into any file where you want to use it, using the same method we’ve used previously to import libraries created by others. <a data-type="xref" href="#FRED_download_api_key_import">Example 5-3</a> is a sample script for downloading the U6 unemployment data from FRED using the API key stored in my <em>FRED_credentials.py</em> file.</p>&#13;
<div data-type="example" id="FRED_download_api_key_import">&#13;
<h5><span class="label">Example 5-3. </span>FRED_API_example.py</h5>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="c1"># import the requests library</code><code>&#13;
</code><code class="kn">import</code><code> </code><code class="nn">requests</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># import our API key</code><code>&#13;
</code><code class="kn">from</code><code> </code><code class="nn">FRED_credentials</code><code> </code><code class="kn">import</code><code> </code><code class="n">my_api_key</code><code> </code><a class="co" href="#callout_accessing_web_based_data_CO1-1" id="co_accessing_web_based_data_CO1-1"><img alt="1" src="assets/1.png"/></a><code>&#13;
</code><code>&#13;
</code><code class="c1"># specify the FRED endpoint we want to use</code><code>&#13;
</code><code class="n">FRED_endpoint</code><code> </code><code class="o">=</code><code> </code><code class="s2">"</code><code class="s2">https://api.stlouisfed.org/fred/series/observations?</code><code class="s2">"</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># also specify the query parameters and their values</code><code>&#13;
</code><code class="n">FRED_parameters</code><code> </code><code class="o">=</code><code> </code><code class="s2">"</code><code class="s2">series_id=U6RATE&amp;file_type=json</code><code class="s2">"</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># construct the complete URL for our API request, adding our API key to the end</code><code>&#13;
</code><code class="n">complete_data_URL</code><code> </code><code class="o">=</code><code> </code><code class="n">FRED_endpoint</code><code> </code><code class="o">+</code><code> </code><code class="n">FRED_parameters</code><code> </code><code class="o">+</code><code class="s2">"</code><code class="s2">&amp;api_key=</code><code class="s2">"</code><code class="o">+</code><code class="n">my_api_key</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># open a new, writable file with our chosen filename</code><code>&#13;
</code><code class="n">FRED_output_file</code><code> </code><code class="o">=</code><code> </code><code class="nb">open</code><code class="p">(</code><code class="s2">"</code><code class="s2">FRED_API_data.json</code><code class="s2">"</code><code class="p">,</code><code class="s2">"</code><code class="s2">w</code><code class="s2">"</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># use the `requests` library's `get()` recipe to access the contents of our</code><code>&#13;
</code><code class="c1"># target URL and store it in our `FRED_data` variable</code><code>&#13;
</code><code class="n">FRED_data</code><code> </code><code class="o">=</code><code> </code><code class="n">requests</code><code class="o">.</code><code class="n">get</code><code class="p">(</code><code class="n">complete_data_URL</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># `get()` the contents of the web page and write its `text`</code><code>&#13;
</code><code class="c1"># directly to `FRED_output_file`</code><code>&#13;
</code><code class="n">FRED_output_file</code><code class="o">.</code><code class="n">write</code><code class="p">(</code><code class="n">FRED_data</code><code class="o">.</code><code class="n">text</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># close our FRED_output_file</code><code>&#13;
</code><code class="n">FRED_output_file</code><code class="o">.</code><code class="n">close</code><code class="p">(</code><code class="p">)</code></pre></div>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_accessing_web_based_data_CO1-1" id="callout_accessing_web_based_data_CO1-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>We can make our API available to this script by using the <code>from</code> keyword with the name of our credentials file (notice that we <em>don’t</em> include the <em>.py</em> extension here) and then telling it to <code>import</code> the variable that contains our API key.</p></dd>&#13;
</dl>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45143408094192">&#13;
<h5>What’s the pycache?</h5>&#13;
<p>When <a data-primary="Python" data-secondary="pycache" data-type="indexterm" id="python-pycache"/><a data-primary="pycache" data-type="indexterm" id="pycache"/>you run the script in <a data-type="xref" href="#FRED_download_api_key_import">Example 5-3</a>, you’ll probably notice that it does more than just successfully download the FRED data—it also creates a new folder called <em>__pycache__</em>, which in turn contains a single file called <em>FRED_credentials.cpython38.pyc</em> or something similar. Where did this file come from, and what is it?</p>&#13;
&#13;
<p>When importing code from another Python file, your device first converts its contents to something <a data-primary="bytecode" data-type="indexterm" id="idm45143408103072"/><a data-primary="Python" data-secondary="bytecode" data-type="indexterm" id="idm45143408102368"/>called <em>bytecode</em>, which is what the device <em>actually</em> uses to run your Python script.<sup><a data-type="noteref" href="ch05.html#idm45143408103984" id="idm45143408103984-marker">7</a></sup> These Python bytecode files have the <a data-primary=".pyc extension" data-primary-sortas="pyc extension" data-type="indexterm" id="idm45143408106656"/>extension <em>.pyc</em>. Although we haven’t dealt with them directly before, there are lots of <em>.pyc</em> files already on your device associated with the libraries we have been importing—they’re just stored in a tucked-away part of the computer system, so we haven’t actually seen them before. In both instances, however, your system is saving resources by translating those imported files to bytecode only once and storing that translation in a <em>.pyc</em> file, rather than translating <em>every time</em> your code runs. When you import those libraries (and now, your credentials), your system relies on the pre-translated version to make the rest of your code run a little bit faster.</p>&#13;
&#13;
<p>Fortunately, you don’t have to do anything with—or about—either the <em>__pycache__</em> folder or the <em>.pyc</em> file inside it; you can even delete them if you like (though they will reappear the next time you run a Python script that imports your credentials). Since our credentials file is very small, it’s not going to slow things up very much if it’s regenerated each time. On the other hand, if it doesn’t bother you to see it in your folder, you can just leave it<a data-primary="Python" data-secondary="pycache" data-startref="python-pycache" data-type="indexterm" id="idm45143408115232"/><a data-primary="pycache" data-startref="pycache" data-type="indexterm" id="idm45143408113696"/> alone.</p>&#13;
</div></aside>&#13;
&#13;
<p>Now <a data-primary="API keys" data-secondary="protection in Python" data-startref="api-key-protect-import" data-tertiary="importing credentials file" data-type="indexterm" id="idm45143408120368"/><a data-primary="Python" data-secondary="API key protection" data-startref="python-api-protect-import" data-tertiary="importing credentials file" data-type="indexterm" id="idm45143408124944"/><a data-primary="credentials file" data-secondary="importing" data-startref="credfile-import" data-type="indexterm" id="idm45143408118544"/><a data-primary="importing credentials file" data-startref="import-credfile" data-type="indexterm" id="idm45143408117328"/>that we’ve succeeded in separating our API credentials from the main part of our code, we need to make sure that our credentials file doesn’t accidentally get backed up when we <code>git commit</code> our work and/or <code>git push</code> it to the internet. To do this simply and systematically, we’ll make use of a special type of file known as <em>.gitignore</em>.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Getting Started with .gitignore" data-type="sect2"><div class="sect2" id="using_gitignore">&#13;
<h2>Getting Started with .gitignore</h2>&#13;
&#13;
<p>As the <a data-primary="API keys" data-secondary="protection in Python" data-tertiary=".gitignore file" data-tertiary-sortas="gitignore" data-type="indexterm" id="api-key-protect-gitignore"/><a data-primary="Python" data-secondary="API key protection" data-tertiary=".gitignore file" data-tertiary-sortas="gitignore" data-type="indexterm" id="python-api-protect-gitignore"/><a data-primary="credentials file" data-secondary="ignoring in Git" data-type="indexterm" id="credfile-ignore"/><a data-primary=".gitignore file" data-primary-sortas="gitignore file" data-type="indexterm" id="gitignore-file"/><a data-primary="Git" data-secondary=".gitignore file" data-secondary-sortas="gitignore file" data-type="indexterm" id="git-gitignore"/><a data-primary="ignoring files in Git" data-type="indexterm" id="ignore-git-file"/>name suggests, a <em>.gitignore</em> file lets you specify certain types of files that—surprise, surprise!—you want Git to “ignore,” rather than track or back up. By creating (or modifying) the pattern-matching rules in the <em>.gitignore</em> file for a repository, we can predefine which types of files our repo will track or upload. While we could <em>theoretically</em> accomplish the same thing manually—by never using <code>git add</code> on files we don’t want to track—using a <em>.gitignore</em> file enforces this behavior<sup><a data-type="noteref" href="ch05.html#idm45143408008560" id="idm45143408008560-marker">8</a></sup> <em>and</em> prevents Git from “warning” us that we have untracked files every time we run <code>git status</code>. <em>Without</em> a <em>.gitignore</em> file, we would have to confirm which files we want to ignore every time we commit—which would quickly get tedious and easily lead to mistakes. All it would take is one hasty <code>git add -A</code> command to accidentally begin tracking our sensitive credentials file(s)—and getting things <em>out</em> of your Git history is much trickier than getting them in. Much better to avoid the whole problem with a little preparation.</p>&#13;
&#13;
<p>In other words, <em>.gitignore</em> files are our friend, letting us create general rules that prevent us from accidentally tracking files we don’t want to, and by making sure that Git only reports the status of files that we genuinely care about.</p>&#13;
&#13;
<p>For the time being, we’ll create a new <em>.gitignore</em> file in the same folder/repository where our <em>FRED_credentials.py</em> file is, just to get a feel for how they work.<sup><a data-type="noteref" href="ch05.html#idm45143407997552" id="idm45143407997552-marker">9</a></sup> To do this, we’ll start by opening up a new file in Atom (or you can add a new file directly in your GitHub repo) and saving it in the same folder as your <em>FRED_credentials.py</em> with the name <em>.gitignore</em> (be sure to start the filename with a dot (<code>.</code>)—that’s important!).</p>&#13;
&#13;
<p>Next, add the following lines to your file:</p>&#13;
&#13;
<pre data-type="programlisting"># ignoring all credentials files&#13;
**credentials*</pre>&#13;
&#13;
<p>As in Python, comments in <em>.gitignore</em> files are started with a hash (<code>#</code>) symbol, so the first line of this file is just descriptive. The contents of the second line (<code>**credentials*</code>) is <a data-primary="regular expressions" data-type="indexterm" id="idm45143407978800"/>a sort of <em>regular expression</em>—a special kind of pattern-matching system that lets us describe strings (including filenames) in the sort of generic way we might explain them to another person.<sup><a data-type="noteref" href="ch05.html#idm45143407977504" id="idm45143407977504-marker">10</a></sup> In this case, the expression <code>**credentials*</code> translates to “a file anywhere in this repository that contains the word <em>credentials</em>.” By adding this line to our <em>.gitignore</em> file, we ensure that any file in this repository whose filename includes the word <em>credentials</em> will never be tracked or uploaded to GitHub.</p>&#13;
&#13;
<p>To see your <em>.gitignore</em> in action, save the file, and then in the command line, run:</p>&#13;
<pre>git status</pre>&#13;
&#13;
<p>While you should see the new file you created for the code in <a data-type="xref" href="#FRED_download_api_key_import">Example 5-3</a>, you should <em>not</em> see your <em>FRED_credentials.py</em> file listed as “untracked.” If you want to be really sure that the files you intend to be ignored are, in fact, being ignored, you can also run:</p>&#13;
<pre>git status --ignored</pre>&#13;
&#13;
<p>which will show you <em>only</em> the files in your repository that are currently being ignored. Among them you’ll probably also see the <em>__pycache__</em> folder, which we also don’t need to back up.</p>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45143407967600">&#13;
<h5>Where Did My .gitignore File Go?</h5>&#13;
<p>Once<a data-primary="hidden files" data-secondary="finding" data-type="indexterm" id="hidden-find"/><a data-primary="finding" data-secondary="hidden files" data-type="indexterm" id="find-hidden"/> you’ve saved and closed your <em>.gitignore</em> file, you might be surprised to see that, well, you don’t see it anymore! Whether you’re looking for it in a folder on your device or via the command line, sometimes finding your <em>.gitignore</em> file can be a challenge in itself.</p>&#13;
&#13;
<p>The reason why these files can be hard to find is that, by default, most device operating systems hide any files that begin with a dot (<code>.</code>). While your <em>.gitignore</em> files will always be easy to find (and edit!) on the GitHub website, if you want to change them on your device, there are a few tricks that can make finding and working with them easier.</p>&#13;
&#13;
<p>Since your <em>.gitignore</em> file is one of the few “dot files” on your device that you’ll probably ever want to see and edit, I find the easiest thing to do is look for it with the command line, by navigating to the relevant folder/repository and using the “list all” command:</p>&#13;
<pre>ls -a</pre>&#13;
&#13;
<p>If you don’t see an existing <em>.gitignore</em> file, you can, of course, create one. If it <em>is</em> there, though, how do you edit it? While you’ve probably been opening the Atom code editor from an icon on your device, you can actually use it to open files right from the command line by using the <code>atom</code> keyword (similar to the way we use <code>python</code>). So to open your <em>.gitignore</em> file using Atom, you can just type:</p>&#13;
<pre>atom .gitignore</pre>&#13;
&#13;
<p>While it may take a few seconds to open (even if you have other Atom files open already), using the command line to start a program is an easy way to find and modify files—hidden or <a data-primary="API (application programming interface)" data-secondary="authentication" data-startref="api-authentication-protect" data-tertiary="API key protection" data-type="indexterm" id="idm45143407954864"/><a data-primary="authentication for APIs" data-secondary="API key protection" data-startref="authenticate-api-protect" data-type="indexterm" id="idm45143407953360"/><a data-primary="API keys" data-secondary="protection in Python" data-startref="api-key-protect" data-type="indexterm" id="idm45143407952128"/><a data-primary="Python" data-secondary="API key protection" data-startref="python-api-protect" data-type="indexterm" id="idm45143407950912"/><a data-primary="API keys" data-secondary="protection in Python" data-startref="api-key-protect-gitignore" data-tertiary=".gitignore file" data-tertiary-sortas="gitignore" data-type="indexterm" id="idm45143407949696"/><a data-primary="Python" data-secondary="API key protection" data-startref="python-api-protect-gitignore" data-tertiary=".gitignore file" data-tertiary-sortas="gitignore" data-type="indexterm" id="idm45143407947920"/><a data-primary="credentials file" data-secondary="ignoring in Git" data-startref="credfile-ignore" data-type="indexterm" id="idm45143407946144"/><a data-primary=".gitignore file" data-primary-sortas="gitignore file" data-startref="gitignore-file" data-type="indexterm" id="idm45143407944928"/><a data-primary="Git" data-secondary=".gitignore file" data-secondary-sortas="gitignore file" data-startref="git-gitignore" data-type="indexterm" id="idm45143407943712"/><a data-primary="ignoring files in Git" data-startref="ignore-git-file" data-type="indexterm" id="idm45143407942224"/><a data-primary="hidden files" data-secondary="finding" data-startref="hidden-find" data-type="indexterm" id="idm45143407941280"/><a data-primary="finding" data-secondary="hidden files" data-startref="find-hidden" data-type="indexterm" id="idm45143407940064"/>otherwise!</p>&#13;
</div></aside>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Specialized APIs: Working With OAuth" data-type="sect1"><div class="sect1" id="oauth_apis">&#13;
<h1>Specialized APIs: Working With OAuth</h1>&#13;
&#13;
<p>So far, <a data-primary="API (application programming interface)" data-secondary="authentication" data-tertiary="OAuth" data-type="indexterm" id="api-authentication-OAuth"/><a data-primary="authentication for APIs" data-secondary="OAuth" data-type="indexterm" id="authenticate-OAuth"/><a data-primary="OAuth authentication with Twitter" data-type="indexterm" id="oauth"/>we have everything we need to work with APIs that have what’s often described as a “basic” authentication process: we create an account with the API provider and are given a key that we append to our data request, just as we did in <a data-type="xref" href="#FRED_download_api_key_import">Example 5-3</a>.</p>&#13;
&#13;
<p>While this process is very straightforward, it has some drawbacks. These days, APIs can do much more than just return data: they are also the way that apps post updates to a social media account or add items to your online calendar. To make that possible, they obviously need some type of access to your account—but of course you don’t want to be sharing your login credentials with apps and programs willy-nilly. If you did just give apps that information, the only way to later <em>stop</em> an app from accessing your account would be to change your username and password, and then you’d have to give your updated credentials to all the apps you still <em>want</em> to use in order for them to continue working…it gets messy and complicated, fast.</p>&#13;
&#13;
<p>The OAuth authentication workflow was designed to address these problems by providing a way to provide API access without passing around a bunch of usernames and passwords. In general, this is achieved by scripting<a data-primary="authorization loops (APIs)" data-type="indexterm" id="idm45143407929728"/> a so-called <em>authorization loop</em>, which includes three basic steps:</p>&#13;
<ol>&#13;
<li>&#13;
<p>Obtaining and encoding your API key and “secret” (each of which is just a string that you get from the API provider—just as we did with the FRED API key).</p>&#13;
</li>&#13;
<li>&#13;
<p>Sending an encoded combination of those two strings as (yet another) “key” to a special “authorization” endpoint/URL.</p>&#13;
</li>&#13;
<li>&#13;
<p>Receiving <a data-primary="access tokens (APIs)" data-type="indexterm" id="idm45143407925168"/>an <em>access token</em> (yet another string) from the authorization endpoint. The access token is what you actually then send along to the API’s data endpoint along with your query information.</p>&#13;
</li>&#13;
&#13;
</ol>&#13;
&#13;
<p>While this probably sounds convoluted, rest assured that, in practice, even this complex-sounding process is mostly about passing strings back and forth to and from certain URLs in a certain order. Yes, in the process we’ll need to do some “encoding” on them, but as you may have guessed, that part will be handled for us by a handy Python library.</p>&#13;
&#13;
<p>Despite needing to complete an authorization loop and retrieve an access token, the process of interacting with even these more specialized APIs via Python is essentially the same as what we saw in <a data-type="xref" href="#basic_authentication">“Specialized APIs: Adding Basic Authentication”</a>. We’ll create an account, request API credentials, and then create a file that both contains those credentials and does a little bit of preparatory work to them so we can use them in our main script. Then our main script will pull in those credentials and use them to request data from our target platform and write it to an output file.</p>&#13;
&#13;
<p>For this example, we’ll be working with the Twitter API, but you’ll be able to use roughly the same process for other platforms (like Facebook) that use an OAuth approach. One thing we <em>won’t</em> do here is spend much time discussing how to structure specific queries, since the ins and outs of any given API could easily fill a book in and of itself! That said, once you have this authentication process down, you’ll have what you need to start experimenting with a whole range of APIs and can start practicing with them in order to access the data you want. Let’s get started!</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Applying for a Twitter Developer Account" data-type="sect2"><div class="sect2" id="idm45143407919216">&#13;
<h2>Applying for a Twitter Developer Account</h2>&#13;
&#13;
<p>As it<a data-primary="OAuth authentication with Twitter" data-secondary="development account application" data-type="indexterm" id="oauth-twitter-devaccount"/><a data-primary="Twitter" data-secondary="development account, applying for" data-type="indexterm" id="twitter-devaccount-apply"/><a data-primary="accounts" data-secondary="Twitter development, applying for" data-type="indexterm" id="account-twitter-apply"/><a data-primary="developer accounts" data-secondary="Twitter, applying for" data-type="indexterm" id="devaccount-twitter-apply"/> will be with (almost) every new API we want to use, our first step will be to request an API key from Twitter. Even if you already have a Twitter account, you’ll need to apply for “developer access,” which will take about 15 minutes (not counting the time for Twitter to review and/or approve), all told. Start by visiting <a href="https://developer.twitter.com/en/apply-for-access">the Twitter Developer API “Apply for Access” page</a>, and click the “Apply for a developer account” button. Once you’ve logged in, you’ll be prompted for more information about how you plan to use the API. For this exercise, you can select “Hobbyist” and “Exploring the API,” as shown in <a data-type="xref" href="#twitter_api_use_case">Figure 5-7</a>.</p>&#13;
&#13;
<figure><div class="figure" id="twitter_api_use_case">&#13;
<img alt="Use case selection for the Twitter developer API" src="assets/ppdw_0507.png"/>&#13;
<h6><span class="label">Figure 5-7. </span>Twitter developer API use case selection</h6>&#13;
</div></figure>&#13;
&#13;
<p>In the next step, you’ll be asked to provide a 200+ character explanation of what you plan to use the API for; here you can enter something like this:</p>&#13;
<blockquote>&#13;
<p>Using the Twitter API to learn more about doing data wrangling with Python.&#13;
Interested in experimenting with OAuth loops and pulling different kinds of&#13;
information from public Twitter feeds and conversations.</p></blockquote>&#13;
&#13;
<p>Since our goal here is just to practice downloading data from Twitter using a Python script and an OAuth loop, you can toggle the answer to the four subsequent questions to “No,” as shown in <a data-type="xref" href="#twitter_api_explanation">Figure 5-8</a>, though if you begin using the API in some other way, you will need to update these answers.</p>&#13;
&#13;
<figure><div class="figure" id="twitter_api_explanation">&#13;
<img alt="Intended use selection for data from the Twitter developer API" src="assets/ppdw_0508.png"/>&#13;
<h6><span class="label">Figure 5-8. </span>Twitter developer API intended uses</h6>&#13;
</div></figure>&#13;
&#13;
<p>On the next two screens, you’ll review your previous selections and click a checkbox to acknowledge the Developer Agreement. You then click “Submit application,” which will trigger a verification email. If the email doesn’t arrive in your inbox within a few minutes, be sure to check your Spam and Trash. Once you locate it, click on the link in the email for your access to be<a data-primary="OAuth authentication with Twitter" data-secondary="development account application" data-startref="oauth-twitter-devaccount" data-type="indexterm" id="idm45143407902320"/><a data-primary="Twitter" data-secondary="development account, applying for" data-startref="twitter-devaccount-apply" data-type="indexterm" id="idm45143407900976"/><a data-primary="accounts" data-secondary="Twitter development, applying for" data-startref="account-twitter-apply" data-type="indexterm" id="idm45143407899728"/><a data-primary="developer accounts" data-secondary="Twitter, applying for" data-startref="devaccount-twitter-apply" data-type="indexterm" id="idm45143407898496"/> confirmed!</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Creating Your Twitter “App” and Credentials" data-type="sect2"><div class="sect2" id="idm45143407897008">&#13;
<h2>Creating Your Twitter “App” and Credentials</h2>&#13;
&#13;
<p>Once your<a data-primary="OAuth authentication with Twitter" data-secondary="app creation" data-type="indexterm" id="oauth-twitter-appcreate"/><a data-primary="Twitter" data-secondary="apps, creating" data-type="indexterm" id="twitter-appcreate"/><a data-primary="apps (Twitter), creating" data-type="indexterm" id="apps-twitter-create"/> developer access has been approved by Twitter, you can create a new “app” by logging in to your Twitter account and visiting <a href="https://developer.twitter.com/en/portal/projects-and-apps"><em class="hyperlink">https://developer.twitter.com/en/portal/projects-and-apps</em></a>. In the center of the page, click the Create Project button.</p>&#13;
&#13;
<p>Here you’ll be taken through a mini version of the process to apply for developer access: you’ll need to provide a name for your project, indicate how you intend to use the Twitter API, describe that purpose in words, and provide a name for the first app associated with that project, as shown in Figures <a data-type="xref" data-xrefstyle="select:labelnumber" href="#twitter_project_name">5-9</a> through <a data-type="xref" data-xrefstyle="select:labelnumber" href="#twitter_app_name">5-12</a>.</p>&#13;
&#13;
<figure class="width-90"><div class="figure" id="twitter_project_name">&#13;
<img alt="Twitter project name screen" src="assets/ppdw_0509.png"/>&#13;
<h6><span class="label">Figure 5-9. </span>Twitter project creation: project name</h6>&#13;
</div></figure>&#13;
&#13;
<figure class="width-90"><div class="figure" id="twitter_project_purpose">&#13;
<img alt="Twitter project purpose screen" src="assets/ppdw_0510.png"/>&#13;
<h6><span class="label">Figure 5-10. </span>Twitter project creation: project purpose</h6>&#13;
</div></figure>&#13;
&#13;
<figure class="width-90"><div class="figure" id="twitter_project_description">&#13;
<img alt="Twitter project description screen" src="assets/ppdw_0511.png"/>&#13;
<h6><span class="label">Figure 5-11. </span>Twitter project creation: project description</h6>&#13;
</div></figure>&#13;
&#13;
<figure class="width-90"><div class="figure" id="twitter_app_name">&#13;
<img alt="Twitter app name" src="assets/ppdw_0512.png"/>&#13;
<h6><span class="label">Figure 5-12. </span>Twitter project creation: app name</h6>&#13;
</div></figure>&#13;
&#13;
<p>Once you’ve <a data-primary="OAuth authentication with Twitter" data-secondary="creating API key" data-type="indexterm" id="oauth-twitter-apikey"/><a data-primary="Twitter" data-secondary="API keys" data-tertiary="creating" data-type="indexterm" id="twitter-apikey"/><a data-primary="API keys" data-secondary="in OAuth" data-secondary-sortas="OAuth" data-tertiary="creating" data-type="indexterm" id="apikey-oauth-create"/><a data-primary="secret keys in OAuth" data-secondary="creating" data-type="indexterm" id="secrets-oauth-create"/>added your app name, you’ll see a screen that shows your API key, API secret key, and Bearer token, as shown in <a data-type="xref" href="#twitter_keys_and_tokens">Figure 5-13</a>.<sup><a data-type="noteref" href="ch05.html#idm45143407870720" id="idm45143407870720-marker">11</a></sup></p>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45143407869968">&#13;
<h5>What’s an “App” After All?</h5>&#13;
<p>For <a data-primary="OAuth authentication with Twitter" data-secondary="app creation" data-startref="oauth-twitter-appcreate" data-type="indexterm" id="idm45143407868704"/><a data-primary="Twitter" data-secondary="apps, creating" data-startref="twitter-appcreate" data-type="indexterm" id="idm45143407867360"/><a data-primary="apps (Twitter), creating" data-startref="apps-twitter-create" data-type="indexterm" id="idm45143407866144"/>most of us, the word <em>app</em> brings to mind phone games and services, not using Python to pull data from an API. For most API providers, however, an “app” is anything that programmatically interacts with their services—whether that’s a Python script downloading tweets or a full-blown mobile app that users can install on their devices. That said, once you’ve got your developer account and are downloading tweets, you can honestly say that you’re an “app developer.” Congratulations!</p>&#13;
</div></aside>&#13;
&#13;
<figure class="width-90"><div class="figure" id="twitter_keys_and_tokens">&#13;
<img alt="API Keys and tokens screen" src="assets/ppdw_0513.png"/>&#13;
<h6><span class="label">Figure 5-13. </span>Twitter API keys and tokens screen</h6>&#13;
</div></figure>&#13;
&#13;
<p>For security reasons, you’ll only be able to view your API key and API secret key  <em>on this screen</em>, so we’re going to put them into a file for our Twitter credentials right away (note that in other places in the developer dashboard these are referred to as the “API Key” and “API Key Secret”—even big tech companies can have trouble with consistency!). Don’t worry, though! If you accidentally click away from this screen too soon, miscopy a value, or anything else, you can always go back to <a href="https://developer.twitter.com/en/portal/dashboard">your dashboard</a> and click on the key icon next to your app, as shown in <a data-type="xref" href="#twitter_project_app_list">Figure 5-14</a>.</p>&#13;
&#13;
<figure><div class="figure" id="twitter_project_app_list">&#13;
<img alt="Twitter Dashboard with Apps" src="assets/ppdw_0514.png"/>&#13;
<h6><span class="label">Figure 5-14. </span>Twitter dashboard with apps</h6>&#13;
</div></figure>&#13;
&#13;
<p>Then, just click “Regenerate” under “Consumer Keys” to get a new API Key and API Key Secret, as shown in <a data-type="xref" href="#twitter_regenerate_keys">Figure 5-15</a>.</p>&#13;
&#13;
<p>Now that we know how<a data-primary="credentials file" data-secondary="creating" data-type="indexterm" id="idm45143407854880"/> to access the API Key and API Key Secret for our app, we need to put these into a new “credentials” file, similar to the one we created for our FRED API key. To do this, create a new file called <em>Twitter_credentials.py</em> and save it in the folder where you want to put your Python script for accessing Twitter data, as shown in <a data-type="xref" href="#Twitter_credentials_example">Example 5-4</a>.</p>&#13;
<div data-type="example" id="Twitter_credentials_example">&#13;
<h5><span class="label">Example 5-4. </span>Twitter_credentials.py</h5>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="n">my_Twitter_key</code><code> </code><code class="o">=</code><code> </code><code class="s2">"</code><em><code class="s2">your_api_key_surrounded_by_double_quotes</code></em><code class="s2">"</code><code>&#13;
</code><code class="n">my_Twitter_secret</code><code> </code><code class="o">=</code><code> </code><code class="s2">"</code><em><code class="s2">your_api_key_secret_surrounded_by_double_quotes</code></em><code class="s2">"</code></pre></div>&#13;
&#13;
<figure><div class="figure" id="twitter_regenerate_keys">&#13;
<img alt="Twitter Regenerate Keys" src="assets/ppdw_0515.png"/>&#13;
<h6><span class="label">Figure 5-15. </span>Twitter regenerate keys</h6>&#13;
</div></figure>&#13;
<div data-type="warning" epub:type="warning"><h6>Warning</h6>&#13;
<p>Be sure to include the word <code>credentials</code> in the name of the file where you store your Twitter API Key and API Key Secret! Recall that in <a data-type="xref" href="#using_gitignore">“Getting Started with .gitignore”</a>, we created a rule that ignores any file whose name includes the word <code>credentials</code> to make sure our API keys never get uploaded to GitHub by accident. So make sure to double-check the spelling in your filename! Just to be extra sure, you can always run:</p>&#13;
<pre>git status --ignored</pre>&#13;
&#13;
<p>in the command line to confirm that all of your credentials files are indeed being<a data-primary="OAuth authentication with Twitter" data-secondary="creating API keys" data-startref="oauth-twitter-apikey" data-type="indexterm" id="idm45143407823984"/><a data-primary="Twitter" data-secondary="API keys" data-startref="twitter-apikey" data-tertiary="creating" data-type="indexterm" id="idm45143407822768"/><a data-primary="API keys" data-secondary="in OAuth" data-secondary-sortas="OAuth" data-startref="apikey-oauth-create" data-tertiary="creating" data-type="indexterm" id="idm45143407821280"/><a data-primary="secret keys in OAuth" data-secondary="creating" data-startref="secrets-oauth-create" data-type="indexterm" id="idm45143407807216"/> ignored.</p>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Encoding Your API Key and Secret" data-type="sect2"><div class="sect2" id="idm45143407896416">&#13;
<h2>Encoding Your API Key and Secret</h2>&#13;
&#13;
<p>So far, <a data-primary="OAuth authentication with Twitter" data-secondary="encoding API keys" data-type="indexterm" id="oauth-twitter-apikey-encode"/><a data-primary="Twitter" data-secondary="API keys" data-tertiary="encoding" data-type="indexterm" id="twitter-apikey-encode"/><a data-primary="API keys" data-secondary="in OAuth" data-secondary-sortas="OAuth" data-tertiary="encoding" data-type="indexterm" id="apikey-oauth-encode"/><a data-primary="secret keys in OAuth" data-secondary="encoding" data-type="indexterm" id="secrets-oauth-encode"/><a data-primary="encoding API keys" data-type="indexterm" id="encode-apikey"/><a data-primary="Python" data-secondary="API key protection" data-tertiary="encoding keys" data-type="indexterm" id="python-api-protect-encode"/>we haven’t needed to do things <em>too</em> differently than we did for the FRED API; we’ve just had to create two variables in our credentials file (<code>my_Twitter_key</code> and <code>my_Twitter_secret</code>) instead of one.</p>&#13;
&#13;
<p>Now, however, we need to do a little bit of work on these values to get them in the right format for the next step of the authentication process. While I won’t go too far into the details of what’s happening “under the hood” in these next few steps, just know that these coding and decoding steps are necessary for protecting the raw string values of your API Key and API Key Secret so that they can be safely sent over the internet.</p>&#13;
&#13;
<p>So, to our <em>Twitter_credentials.py</em> file we’re now going to add several lines of code so that the completed file looks like <a data-type="xref" href="#complete_Twitter_credentials">Example 5-5</a>.</p>&#13;
<div data-type="example" id="complete_Twitter_credentials">&#13;
<h5><span class="label">Example 5-5. </span>Twitter_credentials.py</h5>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="n">my_Twitter_key</code><code> </code><code class="o">=</code><code> </code><code class="s2">"</code><code class="s2">your_api_key_surrounded_by_double_quotes</code><code class="s2">"</code><code>&#13;
</code><code class="n">my_Twitter_secret</code><code> </code><code class="o">=</code><code> </code><code class="s2">"</code><code class="s2">your_api_key_secret_surrounded_by_double_quotes</code><code class="s2">"</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># import the base64 encoding library</code><code>&#13;
</code><code class="kn">import</code><code> </code><code class="nn">base64</code><code> </code><a class="co" href="#callout_accessing_web_based_data_CO2-1" id="co_accessing_web_based_data_CO2-1"><img alt="1" src="assets/1.png"/></a><code>&#13;
</code><code>&#13;
</code><code class="c1"># first, combine the API Key and API Key Secret into a single string</code><code>&#13;
</code><code class="c1"># adding a colon between them</code><code>&#13;
</code><code class="n">combined_key_string</code><code> </code><code class="o">=</code><code> </code><code class="n">my_Twitter_key</code><code class="o">+</code><code class="s1">'</code><code class="s1">:</code><code class="s1">'</code><code class="o">+</code><code class="n">my_Twitter_secret</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># use the `encode()` method on that combined string,</code><code>&#13;
</code><code class="c1"># specifying the ASCII format (see: https://en.wikipedia.org/wiki/ASCII)</code><code>&#13;
</code><code class="n">encoded_combined_key</code><code> </code><code class="o">=</code><code> </code><code class="n">combined_key_string</code><code class="o">.</code><code class="n">encode</code><code class="p">(</code><code class="s1">'</code><code class="s1">ascii</code><code class="s1">'</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># encode the ASCII-formatted string to base64</code><code>&#13;
</code><code class="n">b64_encoded_combined_key</code><code> </code><code class="o">=</code><code> </code><code class="n">base64</code><code class="o">.</code><code class="n">b64encode</code><code class="p">(</code><code class="n">encoded_combined_key</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># _decode_ the encoded string back to ASCII,</code><code>&#13;
</code><code class="c1"># so that it's ready to send over the internet</code><code>&#13;
</code><code class="n">auth_ready_key</code><code> </code><code class="o">=</code><code> </code><code class="n">b64_encoded_combined_key</code><code class="o">.</code><code class="n">decode</code><code class="p">(</code><code class="s1">'</code><code class="s1">ascii</code><code class="s1">'</code><code class="p">)</code></pre></div>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_accessing_web_based_data_CO2-1" id="callout_accessing_web_based_data_CO2-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>This library will let us transform our raw API Key and API Key Secret into the correct format for sending to the Twitter authorization endpoint.</p></dd>&#13;
</dl>&#13;
&#13;
<p>Now that we’ve got our API Key and API Key Secret properly encoded, we can import <em>just</em> the <code>auth_ready_key</code> into the script we’ll use to actually specify and pull our data. After doing one request to the authorization endpoint to get our <em>access token</em>, we’ll finally(!) be ready retrieve <a data-primary="OAuth authentication with Twitter" data-secondary="encoding API keys" data-startref="oauth-twitter-apikey-encode" data-type="indexterm" id="idm45143407705504"/><a data-primary="Twitter" data-secondary="API keys" data-startref="twitter-apikey-encode" data-tertiary="encoding" data-type="indexterm" id="idm45143407704320"/><a data-primary="API keys" data-secondary="in OAuth" data-secondary-sortas="OAuth" data-startref="apikey-oauth-encode" data-tertiary="encoding" data-type="indexterm" id="idm45143407702832"/><a data-primary="secret keys in OAuth" data-secondary="encoding" data-startref="secrets-oauth-encode" data-type="indexterm" id="idm45143407701072"/><a data-primary="encoding API keys" data-startref="encode-apikey" data-type="indexterm" id="idm45143407699856"/><a data-primary="Python" data-secondary="API key protection" data-startref="python-api-protect-encode" data-tertiary="encoding keys" data-type="indexterm" id="idm45143407676608"/>some tweets!</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Requesting an Access Token and Data from the Twitter API" data-type="sect2"><div class="sect2" id="idm45143407675040">&#13;
<h2>Requesting an Access Token and Data from the Twitter API</h2>&#13;
&#13;
<p>As we <a data-primary="OAuth authentication with Twitter" data-secondary="requesting access token" data-type="indexterm" id="oauth-twitter-request"/><a data-primary="Twitter" data-secondary="access tokens, requesting" data-type="indexterm" id="twitter-access-request"/><a data-primary="access tokens (APIs)" data-type="indexterm" id="access-token-request"/><a data-primary="requesting" data-secondary="access tokens" data-type="indexterm" id="request-access-token"/><a data-primary="Python" data-secondary="requesting access tokens" data-type="indexterm" id="python-access-token"/>did in <a data-type="xref" href="#FRED_download_api_key_import">Example 5-3</a>, we’ll now create a Python file (or notebook) where we’ll do the next two steps of our Twitter data loading process:</p>&#13;
<ol>&#13;
<li>&#13;
<p>Requesting (and receiving) an access token <a data-primary="bearer tokens (APIs)" data-see="access tokens" data-type="indexterm" id="idm45143407665776"/>or <em>bearer token</em> from Twitter</p>&#13;
</li>&#13;
<li>&#13;
<p>Including that bearer token in a data request to Twitter and receiving the results</p>&#13;
</li>&#13;
&#13;
</ol>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Requesting an access token: get versus post" data-type="sect3"><div class="sect3" id="idm45143407662880">&#13;
<h3>Requesting an access token: get versus post</h3>&#13;
&#13;
<p>Requesting an access or bearer token from Twitter is really just a matter of sending a well-formatted request to <a data-primary="authorization endpoints (APIs)" data-type="indexterm" id="idm45143407660832"/>the <em>authorization endpoint</em>, which is <a href="https://api.twitter.com/oauth2/token"><em class="hyperlink">https://api.twitter.com/oauth2/token</em></a>. Instead of appending our <code>auth_ready_key</code> to the endpoint URL, however, we’ll use something called a <em>post</em> request (recall that in Examples <a data-type="xref" data-xrefstyle="select:labelnumber" href="#data_download">5-1</a> and <a data-type="xref" data-xrefstyle="select:labelnumber" href="#FRED_download_api_key_import">5-3</a>, the <code>requests</code> recipe we used was called <code>get</code>).</p>&#13;
&#13;
<p>A <code>post</code> request<a data-primary="post requests" data-type="indexterm" id="idm45143407653024"/><a data-primary="get requests" data-type="indexterm" id="idm45143407652288"/> is important here in part because it offers some additional security over <code>get</code> requests<sup><a data-type="noteref" href="ch05.html#idm45143407651104" id="idm45143407651104-marker">12</a></sup> but mostly because <code>post</code> requests are effectively the standard when we’re asking an API to do something <em>beyond</em> simply returning data. So while in <a data-type="xref" href="#FRED_download_api_key_import">Example 5-3</a> the data we got back was unique to our query, the API would return that same data to <em>anyone</em> who submitted that same request. By contrast, when we use <code>post</code> to submit our <code>auth_ready_key</code>, the Twitter API will process our unique key and return a unique bearer token—so we use a <code>post</code> request.</p>&#13;
&#13;
<p>When building our <code>post</code> request in Python, we’ll need to create two <code>dict</code> objects: one that contains the request’s <em>headers</em>, which will contain both our <code>auth_ready_key</code> and some other information, and another that contains the request’s <em>data</em>, which in this case will specify that we’re asking for credentials. Then we’ll just pass these as parameters to the <em>requests</em> library’s <code>post</code> recipe, rather than sticking them on the end of the URL string, as shown in <a data-type="xref" href="#twitter_api_access_token_request">Example 5-6</a>.</p>&#13;
<div data-type="example" id="twitter_api_access_token_request">&#13;
<h5><span class="label">Example 5-6. </span>Twitter_data_download.py</h5>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="c1"># import the encoded key from our credentials file</code><code>&#13;
</code><code class="kn">from</code><code> </code><code class="nn">Twitter_credentials</code><code> </code><code class="kn">import</code><code> </code><code class="n">auth_ready_key</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># include the requests library in order to get data from the web</code><code>&#13;
</code><code class="kn">import</code><code> </code><code class="nn">requests</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># specify the Twitter endpoint that we'll use to retrieve</code><code>&#13;
</code><code class="c1"># our access token or "bearer" token</code><code>&#13;
</code><code class="n">auth_url</code><code> </code><code class="o">=</code><code> </code><code class="s1">'</code><code class="s1">https://api.twitter.com/oauth2/token</code><code class="s1">'</code><code>&#13;
</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># add our `auth_ready_key` to a template `dict` object provided</code><code>&#13;
</code><code class="c1"># in the Twitter API documentation</code><code>&#13;
</code><code class="n">auth_headers</code><code> </code><code class="o">=</code><code> </code><code class="p">{</code><code>&#13;
</code><code>    </code><code class="s1">'</code><code class="s1">Authorization</code><code class="s1">'</code><code class="p">:</code><code> </code><code class="s1">'</code><code class="s1">Basic </code><code class="s1">'</code><code class="o">+</code><code class="n">auth_ready_key</code><code class="p">,</code><code>&#13;
</code><code>    </code><code class="s1">'</code><code class="s1">Content-Type</code><code class="s1">'</code><code class="p">:</code><code> </code><code class="s1">'</code><code class="s1">application/x-www-form-urlencoded;charset=UTF-8</code><code class="s1">'</code><code>&#13;
</code><code class="p">}</code><code> </code><a class="co" href="#callout_accessing_web_based_data_CO3-1" id="co_accessing_web_based_data_CO3-1"><img alt="1" src="assets/1.png"/></a><code>&#13;
</code><code>&#13;
</code><code class="c1"># another `dict` describes what we're asking for</code><code>&#13;
</code><code class="n">auth_data</code><code> </code><code class="o">=</code><code> </code><code class="p">{</code><code>&#13;
</code><code>    </code><code class="s1">'</code><code class="s1">grant_type</code><code class="s1">'</code><code class="p">:</code><code> </code><code class="s1">'</code><code class="s1">client_credentials</code><code class="s1">'</code><code>&#13;
</code><code class="p">}</code><code> </code><a class="co" href="#callout_accessing_web_based_data_CO3-2" id="co_accessing_web_based_data_CO3-2"><img alt="2" src="assets/2.png"/></a><code>&#13;
</code><code>&#13;
</code><code class="c1"># make our complete request to the authorization endpoint, and store</code><code>&#13;
</code><code class="c1"># the results in the `auth_resp` variable</code><code>&#13;
</code><code class="n">auth_resp</code><code> </code><code class="o">=</code><code> </code><code class="n">requests</code><code class="o">.</code><code class="n">post</code><code class="p">(</code><code class="n">auth_url</code><code class="p">,</code><code> </code><code class="n">headers</code><code class="o">=</code><code class="n">auth_headers</code><code class="p">,</code><code> </code><code class="n">data</code><code class="o">=</code><code class="n">auth_data</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># pull the access token out of the json-formatted data</code><code>&#13;
</code><code class="c1"># that the authorization endpoint sent back to us</code><code>&#13;
</code><code class="n">access_token</code><code> </code><code class="o">=</code><code> </code><code class="n">auth_resp</code><code class="o">.</code><code class="n">json</code><code class="p">(</code><code class="p">)</code><code class="p">[</code><code class="s1">'</code><code class="s1">access_token</code><code class="s1">'</code><code class="p">]</code></pre></div>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_accessing_web_based_data_CO3-1" id="callout_accessing_web_based_data_CO3-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>This <code>dict</code> contains the information the authorization endpoint wants in order to return an access token to us. This includes our encoded key and its data format.</p></dd>&#13;
<dt><a class="co" href="#co_accessing_web_based_data_CO3-2" id="callout_accessing_web_based_data_CO3-2"><img alt="2" src="assets/2.png"/></a></dt>&#13;
<dd><p>The format of both the <code>auth_headers</code> and the <code>auth_data</code> objects was defined by the API provider.</p></dd>&#13;
</dl>&#13;
&#13;
<p>Actually pretty straightforward, right? If everything went well (which we’ll confirm momentarily), we’ll just need to add a few lines of code to this script in order to use our access token to request some actual data about what’s happening on Twitter.</p>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45143407518896">&#13;
<h5>What About the Bearer Token in the Developer Dashboard?</h5>&#13;
<p>When you were copying your API Key and API Key Secret out of the developer dashboard screen shown in <a data-type="xref" href="#twitter_keys_and_tokens">Figure 5-13</a>, you may have noticed that there was <em>also</em> a “Bearer Token” listed. Why aren’t we just using that instead of writing code to get one?</p>&#13;
&#13;
<p>The short answer is that while the Twitter API provides access tokens in the dashboard, many similar APIs (such as Facebook’s) do not. Since the goal here is to illustrate a more general process for interacting with APIs that use OAuth, we’re walking through the access/bearer token request loop so you can more easily modify and reuse this code with other APIs.</p>&#13;
</div></aside>&#13;
&#13;
<p>Now that we have an access token (or <em>bearer token</em>) handy, we can go ahead and make a request for some tweets. For demonstration purposes, we’re going to keep our request simple: we’ll do a basic search request for recent tweets that contain the word <em>Python</em>, and ask to have a maximum of four tweets returned. In <a data-type="xref" href="#twitter_api_data_request">Example 5-7</a>, we’re building on the script we started in <a data-type="xref" href="#twitter_api_access_token_request">Example 5-6</a>, and we’ll structure and make this request, including our newly acquired bearer token in the header. Once the response arrives, we’ll write the data to a file. Since there is a <em>lot</em> in the JSON data we get back besides the text of the tweets, however, we’re also going to print out <em>just</em> the text of each tweet, just to give us confidence that we got correct (if sometimes unexpected) results ;-)</p>&#13;
<div data-type="example" id="twitter_api_data_request">&#13;
<h5><span class="label">Example 5-7. </span>Twitter_data_download.py, continued</h5>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="c1"># now that we have an access/bearer token, we're ready to request some data!</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># we'll create a new dict that includes this token</code><code>&#13;
</code><code class="n">search_headers</code><code> </code><code class="o">=</code><code> </code><code class="p">{</code><code>&#13;
</code><code>    </code><code class="s1">'</code><code class="s1">Authorization</code><code class="s1">'</code><code class="p">:</code><code> </code><code class="s1">'</code><code class="s1">Bearer </code><code class="s1">'</code><code> </code><code class="o">+</code><code> </code><code class="n">access_token</code><code>&#13;
</code><code class="p">}</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># this is the Twitter search API endpoint for version 1.1 of the API</code><code>&#13;
</code><code class="n">search_url</code><code>  </code><code class="o">=</code><code> </code><code class="s1">'</code><code class="s1">https://api.twitter.com/1.1/search/tweets.json</code><code class="s1">'</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># create a new dict that includes our search query parameters</code><code>&#13;
</code><code class="n">search_params</code><code> </code><code class="o">=</code><code> </code><code class="p">{</code><code>&#13;
</code><code>    </code><code class="s1">'</code><code class="s1">q</code><code class="s1">'</code><code class="p">:</code><code> </code><code class="s1">'</code><code class="s1">Python</code><code class="s1">'</code><code class="p">,</code><code>&#13;
</code><code>    </code><code class="s1">'</code><code class="s1">result_type</code><code class="s1">'</code><code class="p">:</code><code> </code><code class="s1">'</code><code class="s1">recent</code><code class="s1">'</code><code class="p">,</code><code>&#13;
</code><code>    </code><code class="s1">'</code><code class="s1">count</code><code class="s1">'</code><code class="p">:</code><code> </code><code class="mi">4</code><code>&#13;
</code><code class="p">}</code><code> </code><a class="co" href="#callout_accessing_web_based_data_CO4-1" id="co_accessing_web_based_data_CO4-1"><img alt="1" src="assets/1.png"/></a><code>&#13;
</code><code>&#13;
</code><code class="c1"># send our data request and store the results in `search_resp`</code><code>&#13;
</code><code class="n">search_resp</code><code> </code><code class="o">=</code><code> </code><code class="n">requests</code><code class="o">.</code><code class="n">get</code><code class="p">(</code><code class="n">search_url</code><code class="p">,</code><code> </code><code class="n">headers</code><code class="o">=</code><code class="n">search_headers</code><code class="p">,</code><code> </code><code class="n">params</code><code class="o">=</code><code class="n">search_params</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># parse the response into a JSON object</code><code>&#13;
</code><code class="n">Twitter_data</code><code> </code><code class="o">=</code><code> </code><code class="n">search_resp</code><code class="o">.</code><code class="n">json</code><code class="p">(</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># open an output file where we can save the results</code><code>&#13;
</code><code class="n">Twitter_output_file</code><code> </code><code class="o">=</code><code> </code><code class="nb">open</code><code class="p">(</code><code class="s2">"</code><code class="s2">Twitter_search_results.json</code><code class="s2">"</code><code class="p">,</code><code> </code><code class="s2">"</code><code class="s2">w</code><code class="s2">"</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># write the returned Twitter data to our output file</code><code>&#13;
</code><code class="n">Twitter_output_file</code><code class="o">.</code><code class="n">write</code><code class="p">(</code><code class="nb">str</code><code class="p">(</code><code class="n">Twitter_data</code><code class="p">)</code><code class="p">)</code><code> </code><a class="co" href="#callout_accessing_web_based_data_CO4-2" id="co_accessing_web_based_data_CO4-2"><img alt="2" src="assets/2.png"/></a><code>&#13;
</code><code>&#13;
</code><code class="c1"># close the output file</code><code>&#13;
</code><code class="n">Twitter_output_file</code><code class="o">.</code><code class="n">close</code><code class="p">(</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># loop through our results and print the text of the Twitter status</code><code>&#13;
</code><code class="k">for</code><code> </code><code class="n">a_Tweet</code><code> </code><code class="ow">in</code><code> </code><code class="n">Twitter_data</code><code class="p">[</code><code class="s1">'</code><code class="s1">statuses</code><code class="s1">'</code><code class="p">]</code><code class="p">:</code><code> </code><a class="co" href="#callout_accessing_web_based_data_CO4-3" id="co_accessing_web_based_data_CO4-3"><img alt="3" src="assets/3.png"/></a><code>&#13;
</code><code>    </code><code class="k">print</code><code class="p">(</code><code class="n">a_Tweet</code><code class="p">[</code><code class="s1">'</code><code class="s1">text</code><code class="s1">'</code><code class="p">]</code><code> </code><code class="o">+</code><code> </code><code class="s1">'</code><code class="se">\n</code><code class="s1">'</code><code class="p">)</code></pre></div>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_accessing_web_based_data_CO4-1" id="callout_accessing_web_based_data_CO4-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>In this case, our query (<code>q</code>) is <code>Python</code>, we’re looking for <code>recent</code> results, and we want a maximum of <code>4</code> tweets back. Remember that the keys and values we can include in this object are defined by the data provider.</p></dd>&#13;
<dt><a class="co" href="#co_accessing_web_based_data_CO4-2" id="callout_accessing_web_based_data_CO4-2"><img alt="2" src="assets/2.png"/></a></dt>&#13;
<dd><p>Because the response from Twitter is a JSON object, we have to use the built-in Python <code>str()</code> function to convert it to a string before we can write it to our file.</p></dd>&#13;
<dt><a class="co" href="#co_accessing_web_based_data_CO4-3" id="callout_accessing_web_based_data_CO4-3"><img alt="3" src="assets/3.png"/></a></dt>&#13;
<dd><p>Because there is a <em>lot</em> of information in each result, we’re going to print out the text of each tweet returned, just to get a sense of what’s there. The <code>statuses</code> is the list of tweets in the JSON object, and the actual text of the tweets can be accessed with the key <code>text</code>.</p></dd>&#13;
</dl>&#13;
&#13;
<p>Depending on how actively Twitter users have been posting about Python recently, you may see different results even if you run this script again in only a few minutes.<sup><a data-type="noteref" href="ch05.html#idm45143407307360" id="idm45143407307360-marker">13</a></sup> Of course you can change this search to contain any query term you want; just modify the value of the <code>search_params</code> variable as you like. To see all the possible parameters and their valid values, you can look through <a href="https://developer.twitter.com/en/docs/twitter-api/v1/tweets/search/api-reference/get-search-tweets">the API documentation for this particular Twitter API endpoint</a>.</p>&#13;
&#13;
<p>And that’s it! While there are a number of different APIs that Twitter makes available (others allow you to actually post to your own timeline or even someone else’s), for the purposes of accessing and wrangling data, what we’ve covered here should be enough to get you started with this and other <a data-primary="API (application programming interface)" data-secondary="authentication" data-startref="api-authentication-OAuth" data-tertiary="OAuth" data-type="indexterm" id="idm45143407262336"/><a data-primary="authentication for APIs" data-secondary="OAuth" data-startref="authenticate-OAuth" data-type="indexterm" id="idm45143407261120"/><a data-primary="OAuth authentication with Twitter" data-startref="oauth" data-type="indexterm" id="idm45143407260032"/><a data-primary="OAuth authentication with Twitter" data-secondary="requesting access token" data-startref="oauth-twitter-request" data-type="indexterm" id="idm45143407259184"/><a data-primary="Twitter" data-secondary="access tokens, requesting" data-startref="twitter-access-request" data-type="indexterm" id="idm45143407258096"/><a data-primary="access tokens (APIs)" data-startref="access-token-request" data-type="indexterm" id="idm45143407257008"/><a data-primary="requesting" data-secondary="access tokens" data-startref="request-access-token" data-type="indexterm" id="idm45143407256160"/><a data-primary="Python" data-secondary="requesting access tokens" data-startref="python-access-token" data-type="indexterm" id="idm45143407255072"/>similar APIs.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="API Ethics" data-type="sect1"><div class="sect1" id="idm45143407938256">&#13;
<h1>API Ethics</h1>&#13;
&#13;
<p>Now that<a data-primary="API (application programming interface)" data-secondary="ethical usage" data-type="indexterm" id="api-ethics"/><a data-primary="ethics" data-secondary="API usage" data-type="indexterm" id="ethics-apis"/><a data-primary="Twitter" data-secondary="ethical usage" data-type="indexterm" id="twitter-ethics"/> you know how to make API requests from services like Twitter (and others that use an OAuth process), you may be imagining all the cool things you can do with the data you can collect. Before you start writing dozens of scripts to track the conversations happening around your favorite topics online, however, it’s time to do both some practical and ethical reflection.</p>&#13;
&#13;
<p>First, know that almost every API <a data-primary="rate-limiting (APIs)" data-type="indexterm" id="idm45143407248320"/>uses <em>rate-limiting</em> to restrict how many data requests you can make within a given time interval. On the particular API endpoint we used in <a data-type="xref" href="#twitter_api_data_request">Example 5-7</a>, for example, you can make a maximum of 450 requests in a 15-minute time period, and each request can return a maximum of 100 tweets. If you exceed this, your data requests will probably fail to return data until Twitter determines that the next 15-minute window has begun.</p>&#13;
&#13;
<p>Second, while you probably didn’t read the Developer Agreement in detail (don’t worry, you’re not alone;<sup><a data-type="noteref" href="ch05.html#idm45143407245520" id="idm45143407245520-marker">14</a></sup> you can always find a <a href="https://developer.twitter.com/en/developer-terms/agreement-and-policy">reference copy online</a>), it includes provisions that have important practical and ethical implications. For example, Twitter’s Developer Agreement <em>specifically</em> prohibits the practice of “Off-Twitter matching”—that is, combining Twitter data with other information about a user, unless that user has provided you the information directly or expressly provided their consent. It also contains rules about how you can store and display Twitter content that you may get from the API, and a whole host of other rules and restrictions.</p>&#13;
&#13;
<p>Whether or not those terms of service are legally binding,<sup><a data-type="noteref" href="ch05.html#idm45143407239440" id="idm45143407239440-marker">15</a></sup> or truly ethical in and of themselves, remember that it is ultimately <em>your</em> responsibility to make sure that you are gathering, analyzing, storing, and sharing <em>any</em> data you use in an ethical way. That means taking into account the privacy and security of the people you may be using data about, as well as thinking about the implications of aggregating and sharing it.</p>&#13;
&#13;
<p>Of course, if ethical issues were easy to identify and agree upon, we would live in a very different world. Because they aren’t, many organizations have well-defined review processes and oversight committees designed to help explore and (if possible) resolve ethical issues before, for example, research and data collection impacting humans ever begins. For my own part, I still find that a good place to start when trying to think through ethical issues is <a href="https://spj.org/ethicscode.asp">the “Society of Professional Journalists’ Code of Ethics”</a>. While that document doesn’t cover every possible ethics situation in detail, it articulates some core principles that I think all data users would do well to consider when they are collecting, analyzing, and sharing information.</p>&#13;
&#13;
<p>In the end, however, the most important thing is that <em>whatever</em> choices you make, you’re ready to stand behind them. One of the great possibilities of data access and wrangling is the ability to uncover new information and generate new insights. Just as the skills for doing that are now entirely in your control, so is the responsibility for how you use <a data-primary="API (application programming interface)" data-secondary="ethical usage" data-startref="api-ethics" data-type="indexterm" id="idm45143407232784"/><a data-primary="ethics" data-secondary="API usage" data-startref="ethics-apis" data-type="indexterm" id="idm45143407231520"/><a data-primary="Twitter" data-secondary="ethical usage" data-startref="twitter-ethics" data-type="indexterm" id="idm45143407230304"/>them.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Web Scraping: The Data Source of Last Resort" data-type="sect1"><div class="sect1" id="web_scraping">&#13;
<h1>Web Scraping: The Data Source of Last Resort</h1>&#13;
&#13;
<p>While <a data-primary="scraping" data-see="web scraping" data-type="indexterm" id="idm45143407226928"/><a data-primary="data sources" data-secondary="web scraping" data-tertiary="ethical usage" data-type="indexterm" id="data-source-scrape-ethics"/><a data-primary="web scraping" data-secondary="ethical usage" data-type="indexterm" id="web-scrape-ethics"/><a data-primary="ethics" data-secondary="of web scraping" data-secondary-sortas="web scraping" data-type="indexterm" id="ethics-scrape"/>APIs are designed by companies and organizations with the specific intent of providing access to often rich, diverse datasets via the internet, there is still a whole lot of information online that only exists on form-based or highly formatted web pages as opposed to an API or even as a CSV or PDF. For situations like these, the only real solution is <em>web scraping</em>, which is the process of using code to programmatically retrieve the contents of a web page and systematically extract some amount of (usually) structured data from it.</p>&#13;
&#13;
<p>The reason why I refer to web scraping as “the data source of last resort” is that it’s both a technically and ethically complex process. Creating web scraping Python scripts almost always requires manually wading through a jumble of HTML code to locate the data you’re looking for, typically followed by a significant amount of trial and error to successfully separate the information you want from everything you don’t. It’s time-consuming, fiddly, and often frustrating. And if the web page changes even a little, you may have to start from scratch in order to make your script work with the updated page.</p>&#13;
&#13;
<p>Web scraping is also ethically complicated because, for a variety of reasons, many website owners don’t <em>want</em> you to scrape their pages. Poorly coded scraping programs can overwhelm the website, making it inaccessible to other users. Making lots of scripted data requests in quick succession can also drive up the website owner’s costs, because they have to pay their own service provider to return all that data in a brief period. As a result, many websites explicitly prohibit web scraping in their Terms of Service.</p>&#13;
&#13;
<p>At the same time, if important information—especially about powerful organizations or government agencies—is <em>only</em> available via a web page, then scraping may be your <em>only</em> option even if it goes against the Terms of Service. While it is <em>far</em> beyond the scope of this book to provide even pseudolegal advice on the subject, keep in mind that even if your scripts are written responsibly and there is a good, public-interest reason for your scraping activity, you may face sanction from the website owner (such as a “cease and desist” letter) or even legal action.</p>&#13;
&#13;
<p>Because of this, I strongly recommend that before you start down the road of writing <em>any</em> web scraping script you work through the excellent decision tree by Sophie Chou shown in <a data-type="xref" href="#scraping_decision_tree">Figure 5-16</a>.<sup><a data-type="noteref" href="ch05.html#idm45143407214288" id="idm45143407214288-marker">16</a></sup></p>&#13;
&#13;
<figure><div class="figure" id="scraping_decision_tree">&#13;
<img alt="Web scraping decision tree by Sophie Chou, for Storybench.org, 2016" src="assets/ppdw_0516.png"/>&#13;
<h6><span class="label">Figure 5-16. </span>Web scraping decision tree (Sophie Chou for Storybench.org, 2016)</h6>&#13;
</div></figure>&#13;
&#13;
<p>Once you’ve determined that scraping is your only/best option, it’s time to get to<a data-primary="data sources" data-secondary="web scraping" data-startref="data-source-scrape-ethics" data-tertiary="ethical usage" data-type="indexterm" id="idm45143407209680"/><a data-primary="web scraping" data-secondary="ethical usage" data-startref="web-scrape-ethics" data-type="indexterm" id="idm45143407208144"/><a data-primary="ethics" data-secondary="of web scraping" data-secondary-sortas="web scraping" data-startref="ethics-scrape" data-type="indexterm" id="idm45143407206928"/> work.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Carefully Scraping the MTA" data-type="sect2"><div class="sect2" id="idm45143407205184">&#13;
<h2>Carefully Scraping the MTA</h2>&#13;
&#13;
<p>For this <a data-primary="data sources" data-secondary="web scraping" data-tertiary="MTA example" data-type="indexterm" id="data-source-scrape-mta"/><a data-primary="web scraping" data-secondary="MTA example" data-type="indexterm" id="web-scrape-mta"/><a data-primary="MTA (Metropolitan Transit Authority) example" data-secondary="web scraping" data-type="indexterm" id="mta-scrape"/>example, we’re going to use web scraping to download and extract data from a web page provided by New York City’s Metropolitan Transit Authority (MTA), which provides links to all of the turnstile data for the city’s subway stations, going back to 2010. To ensure that we’re doing this as responsibly as possible, we’re going to make sure that <em>any</em> Python script we write:</p>&#13;
<ol>&#13;
<li>&#13;
<p>Identifies who we are and how to get in touch with us (in the example, we’ll include an email address).</p>&#13;
</li>&#13;
<li>&#13;
<p>Pauses between web page requests to make sure that we don’t overwhelm the server.</p>&#13;
</li>&#13;
&#13;
</ol>&#13;
&#13;
<p>In addition, we’ll structure and separate the parts of our script to make sure that we never download a particular web page more than absolutely necessary. To accomplish this, we’ll start by downloading and saving a copy of the web page that contains the links to the individual turnstile data files. Then we’ll write a separate script that goes through the <em>saved</em> version of the initial page and extracts the data we need. That way, any trial and error we go through in extracting data from the web page happens on our saved version, meaning it adds no additional load to the MTA’s server. Finally, we’ll write a third script that parses our file of extracted links and downloads the last four weeks of data.</p>&#13;
&#13;
<p>Before we begin writing any code, let’s take a look at <a href="http://web.mta.info/developers/turnstile.html">the page we’re planning to scrape</a>. As you can see, the page is little more than a header and a long list of links, each of which will take you to a comma-separated <em>.txt</em> file. To load the last several weeks of these data files, our first step is to download a copy of this index-style page (<a data-type="xref" href="#downloading_turnstile_index">Example 5-8</a>).</p>&#13;
<div data-type="example" id="downloading_turnstile_index">&#13;
<h5><span class="label">Example 5-8. </span>MTA_turnstiles_index.py</h5>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="c1"># include the requests library in order to get data from the web</code><code>&#13;
</code><code class="kn">import</code><code> </code><code class="nn">requests</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># specify the URL of the web page we're downloading</code><code>&#13;
</code><code class="c1"># this one contains a linked list of all the NYC MTA turnstile data files</code><code>&#13;
</code><code class="c1"># going back to 2010</code><code>&#13;
</code><code class="n">mta_turnstiles_index_url</code><code> </code><code class="o">=</code><code> </code><code class="s2">"</code><code class="s2">http://web.mta.info/developers/turnstile.html</code><code class="s2">"</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># create some header information for our web page request</code><code>&#13;
</code><code class="n">headers</code><code> </code><code class="o">=</code><code> </code><code class="p">{</code><code>&#13;
</code><code>    </code><code class="s1">'</code><code class="s1">User-Agent</code><code class="s1">'</code><code class="p">:</code><code> </code><code class="s1">'</code><code class="s1">Mozilla/5.0 (X11; CrOS x86_64 13597.66.0) </code><code class="s1">'</code><code> </code><code class="o">+</code><code> </code><code>\&#13;
</code><code>                  </code><code class="s1">'</code><code class="s1">AppleWebKit/537.36 (KHTML, like Gecko) </code><code class="s1">'</code><code> </code><code class="o">+</code><code> </code><code>\&#13;
</code><code>                  </code><code class="s1">'</code><code class="s1">Chrome/88.0.4324.109 Safari/537.36</code><code class="s1">'</code><code class="p">,</code><code>&#13;
</code><code>    </code><code class="s1">'</code><code class="s1">From</code><code class="s1">'</code><code class="p">:</code><code> </code><code class="s1">'</code><code class="s1">YOUR NAME HERE - youremailaddress@emailprovider.som</code><code class="s1">'</code><code>&#13;
</code><code class="p">}</code><code> </code><a class="co" href="#callout_accessing_web_based_data_CO5-1" id="co_accessing_web_based_data_CO5-1"><img alt="1" src="assets/1.png"/></a><code>&#13;
</code><code>&#13;
</code><code class="c1"># send a `get()` request for the URL, along with our informational headers</code><code>&#13;
</code><code class="n">mta_web_page</code><code> </code><code class="o">=</code><code> </code><code class="n">requests</code><code class="o">.</code><code class="n">get</code><code class="p">(</code><code class="n">mta_turnstiles_index_url</code><code class="p">,</code><code> </code><code class="n">headers</code><code class="o">=</code><code class="n">headers</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># open up a writable local file where we can save the contents of the web page</code><code>&#13;
</code><code class="n">mta_turnstiles_output_file</code><code> </code><code class="o">=</code><code> </code><code class="nb">open</code><code class="p">(</code><code class="s2">"</code><code class="s2">MTA_turnstiles_index.html</code><code class="s2">"</code><code class="p">,</code><code class="s2">"</code><code class="s2">w</code><code class="s2">"</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># write the `text` web page to our output file</code><code>&#13;
</code><code class="n">mta_turnstiles_output_file</code><code class="o">.</code><code class="n">write</code><code class="p">(</code><code class="n">mta_web_page</code><code class="o">.</code><code class="n">text</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># close our output file!</code><code>&#13;
</code><code class="n">mta_turnstiles_output_file</code><code class="o">.</code><code class="n">close</code><code class="p">(</code><code class="p">)</code></pre></div>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_accessing_web_based_data_CO5-1" id="callout_accessing_web_based_data_CO5-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>Since we’re <em>not</em> using an API here, we want to proactively provide the website owner with information about who we are and how to contact us. In this case, we’re describing the browser it should treat our traffic as being from, along with our name and contact information. This is data that the website owner will be able to see in their server logs.</p></dd>&#13;
</dl>&#13;
&#13;
<p>Now you’ll have a file called <em>MTA_turnstiles_index.html</em> in the same folder where your Python script was located. To see what it contains, you can just double-click on it, and it should open in your default web browser. Of course, because we only downloaded the raw code on the page and none of the extra files, images, and other materials that it would normally have access to on the web, it’s going to look a little wonky, probably something like what’s shown in <a data-type="xref" href="#local_copy_mta_turnstile_index">Figure 5-17</a>.</p>&#13;
&#13;
<p>Fortunately, that doesn’t matter at all, since what we’re after here is the list of links that’s stored with the page’s HTML. Before we worry about how to pull that data programmatically, however, we first need to find it within the page’s HTML code. To do this, we’re going to use our<a data-primary="data sources" data-secondary="web scraping" data-startref="data-source-scrape-mta" data-tertiary="MTA example" data-type="indexterm" id="idm45143407076464"/><a data-primary="web scraping" data-secondary="MTA example" data-startref="web-scrape-mta" data-type="indexterm" id="idm45143407074944"/><a data-primary="MTA (Metropolitan Transit Authority) example" data-secondary="web scraping" data-startref="mta-scrape" data-type="indexterm" id="idm45143407073728"/> web browser’s <em>inspection tools</em>.</p>&#13;
&#13;
<figure><div class="figure" id="local_copy_mta_turnstile_index">&#13;
<img alt="Viewing our local copy of the MTA web page in a browser" src="assets/ppdw_0517.png"/>&#13;
<h6><span class="label">Figure 5-17. </span>Viewing our local copy of the MTA web page in a browser</h6>&#13;
</div></figure>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Using Browser Inspection Tools" data-type="sect2"><div class="sect2" id="idm45143407204560">&#13;
<h2>Using Browser Inspection Tools</h2>&#13;
&#13;
<p>With the<a data-primary="data sources" data-secondary="web scraping" data-tertiary="browser inspection tools" data-type="indexterm" id="data-source-scrape-inspect"/><a data-primary="web scraping" data-secondary="browser inspection tools" data-type="indexterm" id="web-scrape-inspect"/><a data-primary="browser inspection tools" data-type="indexterm" id="browser-inspect"/><a data-primary="MTA (Metropolitan Transit Authority) example" data-secondary="browser inspection tools" data-type="indexterm" id="mta-inspect"/> local copy of the MTA turnstile data page open in a browser in front of you, scroll down until you can see the “Data Files” header, as shown in <a data-type="xref" href="#local_copy_mta_turnstile_index">Figure 5-17</a>. To better target <em>just</em> the information we want on this web page with our Python script, we need to try to identify something unique about HTML code that surrounds it—this will make it easier for us to have our script zero in quickly on just the content we want. The easiest way to do this is by “inspecting” the code alongside the regular browser interface.</p>&#13;
&#13;
<p>To get started, put your mouse cursor over the “Data Files” text and context-click (also known as “right-click” or sometimes “Ctrl+click,” depending on your system). At the bottom of the menu that pops up, shown in <a data-type="xref" href="#MTA_context_click">Figure 5-18</a>, choose “Inspect.”</p>&#13;
&#13;
<figure><div class="figure" id="MTA_context_click">&#13;
<img alt="The context menu on our local copy of the MTA web page" src="assets/ppdw_0518.png"/>&#13;
<h6><span class="label">Figure 5-18. </span>The context menu on our local copy of the MTA web page</h6>&#13;
</div></figure>&#13;
&#13;
<p>While the precise location and shape of your particular browser’s inspection tools window will vary (this screenshot is from the Chrome browser), its contents will hopefully look at least somewhat similar to the image in <a data-type="xref" href="#inspection_window_highlights">Figure 5-19</a>.</p>&#13;
&#13;
<figure><div class="figure" id="inspection_window_highlights">&#13;
<img alt="The inspection tools window with the Data Files header highlighted." src="assets/ppdw_0519.png"/>&#13;
<h6><span class="label">Figure 5-19. </span>Inspection tools example</h6>&#13;
</div></figure>&#13;
&#13;
<p>Wherever your own window appears (sometimes it is anchored to the side or bottom of your browser window, and, as in <a data-type="xref" href="#inspection_window_highlights">Figure 5-19</a>, there are often multiple panes of information), the main thing we want to locate in the inspection tools window are the words “Data Files.” If you lost them (or never saw them in the first place!) once the window appeared, just move your mouse over those words on the web page and context-click to open the inspection tools window again.</p>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45143407024912">&#13;
<h5>Making the Most of Inspection Tools</h5>&#13;
<p>Browser inspection tools can be useful in a lot of ways, but they are especially important in web scraping, when we want to find unique ways to identify specific chunks of information on a web page. Once you have them open, hovering over a piece of code will visibly highlight the part of the web page it corresponds to. By moving around the code in the inspection tools window with your mouse, you can highlight corresponding parts of the web page’s visual interface, allowing you to write your script to more precisely grab the parts of the page you need.</p>&#13;
</div></aside>&#13;
&#13;
<p>In this case, if you use your mouse to hover over the code in the inspection tools window that says:</p>&#13;
&#13;
<pre data-code-language="html" data-type="programlisting"><code class="nt">&lt;div</code> <code class="na">class=</code><code class="s">"span-84 last"</code><code class="nt">&gt;</code></pre>&#13;
&#13;
<p>you should see the “Data Files” section of the web page highlighted in your browser window. Based on the area highlighted, it <em>appears</em> that this bit of code includes the entire list of links we’re interested in, which we can confirm by scrolling down in the inspection tools window. There we’ll see that all of the data links we want (which end in <em>.txt</em>) are indeed inside this <code>div</code> (notice how they are indented beneath it? That’s another instance of nesting at work!). Now, if we can confirm that class <code>span-84 last</code> only exists in one place on the web page, we have a good starting point for writing our Python script to extract the list of<a data-primary="data sources" data-secondary="web scraping" data-startref="data-source-scrape-inspect" data-tertiary="browser inspection tools" data-type="indexterm" id="idm45143407017552"/><a data-primary="web scraping" data-secondary="browser inspection tools" data-startref="web-scrape-inspect" data-type="indexterm" id="idm45143407015200"/><a data-primary="browser inspection tools" data-startref="browser-inspect" data-type="indexterm" id="idm45143407013968"/><a data-primary="MTA (Metropolitan Transit Authority) example" data-secondary="browser inspection tools" data-startref="mta-inspect" data-type="indexterm" id="idm45143407013008"/> links.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="The Python Web Scraping Solution: Beautiful Soup" data-type="sect2"><div class="sect2" id="idm45143407041392">&#13;
<h2>The Python Web Scraping Solution: Beautiful Soup</h2>&#13;
&#13;
<p>Before<a data-primary="data sources" data-secondary="web scraping" data-tertiary="Beautiful Soup library" data-type="indexterm" id="data-source-scrape-soup"/><a data-primary="web scraping" data-secondary="Beautiful Soup library" data-type="indexterm" id="web-scrape-soup"/><a data-primary="Python" data-secondary="Beautiful Soup library" data-type="indexterm" id="python-soup"/><a data-primary="Beautiful Soup library" data-type="indexterm" id="beautiful-soup"/> we begin writing our next Python script, let’s confirm that the <code>span-84 last</code> class really is unique on the page we downloaded. The simplest way to do this is to first open the page in Atom (context-click on the filename instead of double-clicking, and choose Atom from the “Open with” menu option), which will show us the page’s code. Then do a regular “find” command (Ctrl+F or command+F) and search for <code>span-84 last</code>. As it turns out, even the <code>span-84</code> part only appears once in our file, so we can confine our Python script to looking for link information nested within that HTML tag.</p>&#13;
&#13;
<p>Now we’re ready to start writing the Python script that will extract the links from the web page. For this we’ll install and use the <em>Beautiful Soup</em> library, which is widely used for parsing the often messy markup we find on web pages. While <em>Beautiful Soup</em> has some functionality overlap with the <em>lxml</em> library that we used in <a data-type="xref" href="ch04.html#xml_parsing">Example 4-12</a>, the main difference between them is that <em>Beautiful Soup</em> can handle parsing even less-than-perfectly-structured HTML and XML—which is what we invariably have to deal with on the web. In addition, <em>Beautiful Soup</em> lets us “grab” bits of markup by almost any feature we want—class name, tag type, or even attribute value—so it’s pretty much the go-to library for pulling data out of the “soup” of markup we often find online. You can read the <a href="https://crummy.com/software/BeautifulSoup/bs4/doc">full documentation for the library</a>, but the easiest way to install it will be the same process we’ve used for other libraries, by using <code>pip</code> on the command line:</p>&#13;
<pre>pip install beautifulsoup4</pre>&#13;
&#13;
<p>Now, we’re ready to use Python to open the local copy of our web page and use the <em>Beautiful Soup</em> library to quickly grab all the links we need and write them to a simple <em>.csv</em> file, as shown in <a data-type="xref" href="#parsing_the_soup">Example 5-9</a>.</p>&#13;
<div data-type="example" id="parsing_the_soup">&#13;
<h5><span class="label">Example 5-9. </span>MTA_turnstiles_parsing.py</h5>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="c1"># import the Beautiful Soup recipe from the bs4 library</code><code>&#13;
</code><code class="kn">from</code><code> </code><code class="nn">bs4</code><code> </code><code class="kn">import</code><code> </code><code class="n">BeautifulSoup</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># open the saved copy of our MTA turnstiles web page</code><code>&#13;
</code><code class="c1"># (original here: http://web.mta.info/developers/turnstile.html)</code><code>&#13;
</code><code class="n">mta_web_page</code><code> </code><code class="o">=</code><code> </code><code class="nb">open</code><code class="p">(</code><code class="s2">"</code><code class="s2">MTA_turnstiles_index.html</code><code class="s2">"</code><code class="p">,</code><code> </code><code class="s2">"</code><code class="s2">r</code><code class="s2">"</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># define the base URL for the data files</code><code>&#13;
</code><code class="n">base_url</code><code> </code><code class="o">=</code><code> </code><code class="s2">"</code><code class="s2">http://web.mta.info/developers/</code><code class="s2">"</code><code> </code><a class="co" href="#callout_accessing_web_based_data_CO6-1" id="co_accessing_web_based_data_CO6-1"><img alt="1" src="assets/1.png"/></a><code>&#13;
</code><code>&#13;
</code><code class="c1"># the `BeautifulSoup` recipe takes the contents of our web page and another</code><code>&#13;
</code><code class="c1"># "ingredient", which tells it what kind of code it is working with</code><code>&#13;
</code><code class="c1"># in this case, it's HTML</code><code>&#13;
</code><code class="n">soup</code><code> </code><code class="o">=</code><code> </code><code class="n">BeautifulSoup</code><code class="p">(</code><code class="n">mta_web_page</code><code class="p">,</code><code> </code><code class="s2">"</code><code class="s2">html.parser</code><code class="s2">"</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># using the "find" recipe, we can pass a tag type and class name as</code><code>&#13;
</code><code class="c1"># "ingredients" to zero in on the content we want.</code><code>&#13;
</code><code class="n">data_files_section</code><code> </code><code class="o">=</code><code> </code><code class="n">soup</code><code class="o">.</code><code class="n">find</code><code class="p">(</code><code class="s2">"</code><code class="s2">div</code><code class="s2">"</code><code class="p">,</code><code> </code><code class="n">class_</code><code class="o">=</code><code class="s2">"</code><code class="s2">span-84 last</code><code class="s2">"</code><code class="p">)</code><code> </code><a class="co" href="#callout_accessing_web_based_data_CO6-2" id="co_accessing_web_based_data_CO6-2"><img alt="2" src="assets/2.png"/></a><code>&#13;
</code><code>&#13;
</code><code class="c1"># within that div, we can now just look for all the "anchor" (`a`) tags</code><code>&#13;
</code><code class="n">all_data_links</code><code> </code><code class="o">=</code><code> </code><code class="n">data_files_section</code><code class="o">.</code><code class="n">find_all</code><code class="p">(</code><code class="s2">"</code><code class="s2">a</code><code class="s2">"</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># need to open a file to write our extracted links to</code><code>&#13;
</code><code class="n">mta_data_list</code><code> </code><code class="o">=</code><code> </code><code class="nb">open</code><code class="p">(</code><code class="s2">"</code><code class="s2">MTA_data_index.csv</code><code class="s2">"</code><code class="p">,</code><code class="s2">"</code><code class="s2">w</code><code class="s2">"</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># the `find_all()` recipe returns a list of everything it matches</code><code>&#13;
</code><code class="k">for</code><code> </code><code class="n">a_link</code><code> </code><code class="ow">in</code><code> </code><code class="n">all_data_links</code><code class="p">:</code><code>&#13;
</code><code>&#13;
</code><code>    </code><code class="c1"># combine our base URL with the contents of each "href" (link) property,</code><code>&#13;
</code><code>    </code><code class="c1"># and store it in `complete_link`</code><code>&#13;
</code><code>    </code><code class="n">complete_link</code><code> </code><code class="o">=</code><code> </code><code class="n">base_url</code><code class="o">+</code><code class="n">a_link</code><code class="p">[</code><code class="s2">"</code><code class="s2">href</code><code class="s2">"</code><code class="p">]</code><code>&#13;
</code><code>&#13;
</code><code>    </code><code class="c1"># write this completed link to our output file, manually adding a</code><code>&#13;
</code><code>    </code><code class="c1"># newline `\n` character to the end, so each link will be on its own row</code><code>&#13;
</code><code>    </code><code class="n">mta_data_list</code><code class="o">.</code><code class="n">write</code><code class="p">(</code><code class="n">complete_link</code><code class="o">+</code><code class="s2">"</code><code class="se">\n</code><code class="s2">"</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># once we've written all the links to our file, close it!</code><code>&#13;
</code><code class="n">mta_data_list</code><code class="o">.</code><code class="n">close</code><code class="p">(</code><code class="p">)</code></pre></div>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_accessing_web_based_data_CO6-1" id="callout_accessing_web_based_data_CO6-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>If we click one of the data links on the live copy of the web page, we see that the first part of the URL where the actual data file lives is <code>http://web.mta.info/developers/</code>, but each link only contains the latter half of the URL (in the format <code>data/nyct/turnstile/turnstile_YYMMDD.txt</code>). So that our download script has working links, we need to specify the “base” URL.</p></dd>&#13;
<dt><a class="co" href="#co_accessing_web_based_data_CO6-2" id="callout_accessing_web_based_data_CO6-2"><img alt="2" src="assets/2.png"/></a></dt>&#13;
<dd><p>Thanks to our work with the inspection tools, we can go straight to a <code>div</code> with the class <code>span-84 last</code> to start looking for the links we want. Note that because the word <code>class</code> has a special meaning in Python, Beautiful Soup appends an underscore to the end when we’re using it here (e.g., <code>class_</code>).</p></dd>&#13;
</dl>&#13;
&#13;
<p>OK! What we should have now is a new file that contains a list of all the data links we’re interested in. Next we need to read that list in using a new script and download the files at those URLs. However, because we want to be careful not to overload the MTA website by downloading the files too quickly, we’re going to use the built-in Python <em>time</em> library to space out our requests by a second or two each. Also, we’ll be sure to only download the four files that we really want, rather than downloading everything just for the sake of it. To see how this second script is organized, take a look at <a data-type="xref" href="#downloading_the_data">Example 5-10</a>.</p>&#13;
<div data-type="example" id="downloading_the_data">&#13;
<h5><span class="label">Example 5-10. </span>MTA_turnstiles_data_download.py</h5>&#13;
&#13;
<pre data-code-language="python" data-type="programlisting"><code class="c1"># include the requests library in order to get data from the web</code><code>&#13;
</code><code class="kn">import</code><code> </code><code class="nn">requests</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># import the `os` Python library so we can create a new folder</code><code>&#13;
</code><code class="c1"># in which to store our downloaded data files</code><code>&#13;
</code><code class="kn">import</code><code> </code><code class="nn">os</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># import the `time` library</code><code>&#13;
</code><code class="kn">import</code><code> </code><code class="nn">time</code><code> </code><a class="co" href="#callout_accessing_web_based_data_CO7-1" id="co_accessing_web_based_data_CO7-1"><img alt="1" src="assets/1.png"/></a><code>&#13;
</code><code>&#13;
</code><code class="c1"># open the file where we stored our list of links</code><code>&#13;
</code><code class="n">mta_data_links</code><code> </code><code class="o">=</code><code> </code><code class="nb">open</code><code class="p">(</code><code class="s2">"</code><code class="s2">MTA_data_index.csv</code><code class="s2">"</code><code class="p">,</code><code class="s2">"</code><code class="s2">r</code><code class="s2">"</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># create a folder name so that we can keep the data organized</code><code>&#13;
</code><code class="n">folder_name</code><code> </code><code class="o">=</code><code> </code><code class="s2">"</code><code class="s2">turnstile_data</code><code class="s2">"</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># add our header information</code><code>&#13;
</code><code class="n">headers</code><code> </code><code class="o">=</code><code> </code><code class="p">{</code><code>&#13;
</code><code>    </code><code class="s1">'</code><code class="s1">User-Agent</code><code class="s1">'</code><code class="p">:</code><code> </code><code class="s1">'</code><code class="s1">Mozilla/5.0 (X11; CrOS x86_64 13597.66.0) </code><code class="s1">'</code><code> </code><code class="o">+</code><code> </code><code>\&#13;
</code><code>                  </code><code class="s1">'</code><code class="s1">AppleWebKit/537.36 (KHTML, like Gecko) </code><code class="s1">'</code><code> </code><code class="o">+</code><code> </code><code>\&#13;
</code><code>                  </code><code class="s1">'</code><code class="s1">Chrome/88.0.4324.109 Safari/537.36</code><code class="s1">'</code><code class="p">,</code><code>&#13;
</code><code>    </code><code class="s1">'</code><code class="s1">From</code><code class="s1">'</code><code class="p">:</code><code> </code><code class="s1">'</code><code class="s1">YOUR NAME HERE - youremailaddress@emailprovider.som</code><code class="s1">'</code><code>&#13;
</code><code class="p">}</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># the built-in `readlines()` function converts our data file to a</code><code>&#13;
</code><code class="c1"># list, where each line is an item</code><code>&#13;
</code><code class="n">mta_links_list</code><code> </code><code class="o">=</code><code> </code><code class="n">mta_data_links</code><code class="o">.</code><code class="n">readlines</code><code class="p">(</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># confirm there isn't already a folder with our chosen name</code><code>&#13;
</code><code class="k">if</code><code> </code><code class="n">os</code><code class="o">.</code><code class="n">path</code><code class="o">.</code><code class="n">isdir</code><code class="p">(</code><code class="n">folder_name</code><code class="p">)</code><code> </code><code class="o">==</code><code> </code><code class="bp">False</code><code class="p">:</code><code>&#13;
</code><code>&#13;
</code><code>    </code><code class="c1"># create a new folder with that name</code><code>&#13;
</code><code>    </code><code class="n">target_folder</code><code> </code><code class="o">=</code><code> </code><code class="n">os</code><code class="o">.</code><code class="n">mkdir</code><code class="p">(</code><code class="n">folder_name</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code class="c1"># only download the precise number of files we need</code><code>&#13;
</code><code class="k">for</code><code> </code><code class="n">i</code><code> </code><code class="ow">in</code><code> </code><code class="nb">range</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code><code class="mi">4</code><code class="p">)</code><code class="p">:</code><code>&#13;
</code><code>&#13;
</code><code>    </code><code class="c1"># use the built-in `strip()` method to remove the newline (`\n`)</code><code>&#13;
</code><code>    </code><code class="c1"># character at the end of each row/link</code><code>&#13;
</code><code>    </code><code class="n">data_url</code><code> </code><code class="o">=</code><code> </code><code class="p">(</code><code class="n">mta_links_list</code><code class="p">[</code><code class="n">i</code><code class="p">]</code><code class="p">)</code><code class="o">.</code><code class="n">strip</code><code class="p">(</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code>    </code><code class="c1"># create a unique output filename based on the url</code><code>&#13;
</code><code>    </code><code class="n">data_filename</code><code> </code><code class="o">=</code><code> </code><code class="n">data_url</code><code class="o">.</code><code class="n">split</code><code class="p">(</code><code class="s2">"</code><code class="s2">/</code><code class="s2">"</code><code class="p">)</code><code class="p">[</code><code class="o">-</code><code class="mi">1</code><code class="p">]</code><code> </code><a class="co" href="#callout_accessing_web_based_data_CO7-2" id="co_accessing_web_based_data_CO7-2"><img alt="2" src="assets/2.png"/></a><code>&#13;
</code><code>&#13;
</code><code>    </code><code class="c1"># make our request for the data</code><code>&#13;
</code><code>    </code><code class="n">turnstile_data_file</code><code> </code><code class="o">=</code><code> </code><code class="n">requests</code><code class="o">.</code><code class="n">get</code><code class="p">(</code><code class="n">data_url</code><code class="p">,</code><code> </code><code class="n">headers</code><code class="o">=</code><code class="n">headers</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code>    </code><code class="c1"># open a new, writable file inside our target folder</code><code>&#13;
</code><code>    </code><code class="c1"># using the appropriate filename</code><code>&#13;
</code><code>    </code><code class="n">local_data_file</code><code> </code><code class="o">=</code><code> </code><code class="nb">open</code><code class="p">(</code><code class="n">os</code><code class="o">.</code><code class="n">path</code><code class="o">.</code><code class="n">join</code><code class="p">(</code><code class="n">folder_name</code><code class="p">,</code><code class="n">data_filename</code><code class="p">)</code><code class="p">,</code><code> </code><code class="s2">"</code><code class="s2">w</code><code class="s2">"</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code>    </code><code class="c1"># save the contents of the downloaded file to that new file</code><code>&#13;
</code><code>    </code><code class="n">local_data_file</code><code class="o">.</code><code class="n">write</code><code class="p">(</code><code class="n">turnstile_data_file</code><code class="o">.</code><code class="n">text</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code>    </code><code class="c1"># close the local file</code><code>&#13;
</code><code>    </code><code class="n">local_data_file</code><code class="o">.</code><code class="n">close</code><code class="p">(</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code>    </code><code class="c1"># `sleep()` for two seconds before moving on to the next item in the loop</code><code>&#13;
</code><code>    </code><code class="n">time</code><code class="o">.</code><code class="n">sleep</code><code class="p">(</code><code class="mi">2</code><code class="p">)</code></pre></div>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_accessing_web_based_data_CO7-1" id="callout_accessing_web_based_data_CO7-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>This library will let us “pause” our downloading script between data requests so that we don’t overload the MTA server with too many requests in too short a time period (and possibly get in trouble).</p></dd>&#13;
<dt><a class="co" href="#co_accessing_web_based_data_CO7-2" id="callout_accessing_web_based_data_CO7-2"><img alt="2" src="assets/2.png"/></a></dt>&#13;
<dd><p>Here, we’re splitting the link URL on slashes, then taking the last item from the resulting list using <a href="https://w3schools.com/python/gloss_python_string_negative_indexing.asp">negative indexing</a>, which counts backward from the end of the string. This means that the item at position <code>-1</code> is the last item, which here is the <em>.txt</em> filename. This is the filename we’ll use for the local copy of the data that we save.</p></dd>&#13;
</dl>&#13;
&#13;
<p>If everything has gone well, you should now have a new folder called <em>turnstile_data</em>, with the four most recent turnstile data files<a data-primary="data sources" data-secondary="web scraping" data-startref="data-source-scrape-soup" data-tertiary="Beautiful Soup library" data-type="indexterm" id="idm45143406232176"/><a data-primary="web scraping" data-secondary="Beautiful Soup library" data-startref="web-scrape-soup" data-type="indexterm" id="idm45143406230656"/><a data-primary="Python" data-secondary="Beautiful Soup library" data-startref="python-soup" data-type="indexterm" id="idm45143406229440"/><a data-primary="Beautiful Soup library" data-startref="beautiful-soup" data-type="indexterm" id="idm45143406228224"/> saved inside it. Pretty neat, right?</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Conclusion" data-type="sect1"><div class="sect1" id="idm45143407228368">&#13;
<h1>Conclusion</h1>&#13;
&#13;
<p>Now that we have explored the many ways to actually <em>get</em> the data we’re after and convert it into formats we can use, the next question is: what do we <em>do</em> with it all? Since the goal of all this data wrangling is to be able to answer questions and generate some insight about the world, we now need to move on from the process of <em>acquiring</em> data and start the process of assessing, improving, and analyzing it. To this end, in the next chapter we’ll work through a data quality evaluation of a public dataset, with an eye toward understanding both its possibilities and limitations and how our data wrangling work can help us make the most of it.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<div data-type="footnotes"><p data-type="footnote" id="idm45143408861008"><sup><a href="ch05.html#idm45143408861008-marker">1</a></sup> Like the <a href="https://fiscaldata.treasury.gov/api-documentation">US Treasurey, for example</a>.</p><p data-type="footnote" id="idm45143408859632"><sup><a href="ch05.html#idm45143408859632-marker">2</a></sup> For two examples, see articles in <a href="https://themarkup.org/google-the-giant/2021/04/09/how-we-discovered-googles-social-justice-blocklist-for-youtube-ad-placements"><em>The Markup</em></a> and <a href="https://npr.org/2021/08/04/1024791053/facebook-boots-nyu-disinformation-researchers-off-its-platform-and-critics-cry-f"><em>NPR</em></a>.</p><p data-type="footnote" id="idm45143408848400"><sup><a href="ch05.html#idm45143408848400-marker">3</a></sup> This is also the first step to building your own “apps”!</p><p data-type="footnote" id="idm45143408324000"><sup><a href="ch05.html#idm45143408324000-marker">4</a></sup> While this is probably most likely to happen if you make too many data requests in too short a time frame, most API providers can terminate your access to their API pretty much whenever they want, and for whatever reason they want.</p><p data-type="footnote" id="idm45143408221168"><sup><a href="ch05.html#idm45143408221168-marker">5</a></sup> For example, when you <code>git push</code> your code.</p><p data-type="footnote" id="idm45143408216624"><sup><a href="ch05.html#idm45143408216624-marker">6</a></sup> “Hackers Used SolarWinds’ Dominance against It in Sprawling Spy Campaign” by Raphael Satter, Christopher Bing, and Joseph Menn,  <a href="https://reuters.com/article/global-cyber-solarwinds/hackers-at-center-of-sprawling-spy-campaign-turned-solarwinds-dominance-against-it-idUSKBN28P2N8"><em class="hyperlink">https://reuters.com/article/global-cyber-solarwinds/hackers-at-center-of-sprawling-spy-campaign-turned-solarwinds-dominance-against-it-idUSKBN28P2N8</em></a>; “Former SolarWinds CEO Blames Intern for <em>solarwinds123</em> Password Leak” by Brian Fung and Geneva Sands, <a href="https://www.cnn.com/2021/02/26/politics/solarwinds123-password-intern"><em class="hyperlink">https://www.cnn.com/2021/02/26/politics/solarwinds123-password-intern</em></a>.</p><p data-type="footnote" id="idm45143408103984"><sup><a href="ch05.html#idm45143408103984-marker">7</a></sup> That’s why the program that makes Python run on your device is often called a Python <em>interpreter</em>—because it translates the code we humans write into bytecode that your device can actually understand.</p><p data-type="footnote" id="idm45143408008560"><sup><a href="ch05.html#idm45143408008560-marker">8</a></sup> Even if we run, say, <code>git add -A</code> or <code>git commit -a</code>.</p><p data-type="footnote" id="idm45143407997552"><sup><a href="ch05.html#idm45143407997552-marker">9</a></sup> A repository can have different <em>.gitignore</em> files in different folders; for complete details, you can take a look at the <a href="https://git-scm.com/docs/gitignore">documentation</a>.</p><p data-type="footnote" id="idm45143407977504"><sup><a href="ch05.html#idm45143407977504-marker">10</a></sup> We used this approach with the <em>glob</em> library in <a data-type="xref" href="ch04.html#pdf_parsing">Example 4-16</a> and will examine it in more detail in <a data-type="xref" href="ch07.html#regex_example">“Regular Expressions: Supercharged String Matching”</a>.</p><p data-type="footnote" id="idm45143407870720"><sup><a href="ch05.html#idm45143407870720-marker">11</a></sup> These keys have since been replaced and will not work!</p><p data-type="footnote" id="idm45143407651104"><sup><a href="ch05.html#idm45143407651104-marker">12</a></sup> For example, the contents of a <code>post</code> request are not saved in the browser history the way that <code>get</code> requests are.</p><p data-type="footnote" id="idm45143407307360"><sup><a href="ch05.html#idm45143407307360-marker">13</a></sup> Remember that each time you run the script, you will also overwrite your output file, so it will only ever contain the most recent results.</p><p data-type="footnote" id="idm45143407245520"><sup><a href="ch05.html#idm45143407245520-marker">14</a></sup> Aleecia M. McDonald and Lorrie Faith Cranor, “The Cost of Reading Privacy Policies,”  <em>I/S: A Journal of Law and Policy for the Information Society</em> 4 (2008): 543, <a href="https://kb.osu.edu/bitstream/handle/1811/72839/ISJLP_V4N3_543.pdf"><em class="hyperlink">https://kb.osu.edu/bitstream/handle/1811/72839/ISJLP_V4N3_543.pdf</em></a>, and Jonathan A. Obar and Anne Oeldorf-Hirsch, “The Biggest Lie on the Internet: Ignoring the Privacy Policies and Terms of Service Policies of Social Networking Services,” <em>Information, Communication &amp; Society</em> 23 no. 1 (2020): 128–147, <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2757465"><em class="hyperlink">https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2757465</em></a>.</p><p data-type="footnote" id="idm45143407239440"><sup><a href="ch05.html#idm45143407239440-marker">15</a></sup> Victoria D. Baranetsky, “Data Journalism and the Law,” <em>Tow Center for Digital Journalism</em> (2018) <a href="https://doi.org/10.7916/d8-15sw-fy51"><em class="hyperlink">https://doi.org/10.7916/d8-15sw-fy51</em></a>.</p><p data-type="footnote" id="idm45143407214288"><sup><a href="ch05.html#idm45143407214288-marker">16</a></sup> The accompanying blog post is also excellent: <a href="https://storybench.org/to-scrape-or-not-to-scrape-the-technical-and-ethical-challenges-of-collecting-data-off-the-web"><em class="hyperlink">https://storybench.org/to-scrape-or-not-to-scrape-the-technical-and-ethical-challenges-of-collecting-data-off-the-web</em></a>.</p></div></div></section></body></html>