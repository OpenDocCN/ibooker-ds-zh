<html><head></head><body>
<div id="sbo-rt-content"><section class="pagenumrestart" data-pdf-bookmark="Chapter 1. Analyzing Big Data" data-type="chapter" epub:type="chapter"><div class="chapter" id="Introduction">
<h1><span class="label">Chapter 1. </span>Analyzing Big Data</h1>
<p>When people say that we live in an<a data-primary="big data" data-secondary="about age of big data" data-type="indexterm" id="idm46507993172576"/> age of big data they mean that we have tools for collecting, storing, and processing information at a scale previously unheard of. The following tasks simply could not have been accomplished 10 or 15 years ago:</p>
<ul>
<li>
<p>Build a model to detect credit card fraud using thousands of features and billions of transactions</p>
</li>
<li>
<p>Intelligently recommend millions of products to millions of users</p>
</li>
<li>
<p>Estimate financial risk through simulations of portfolios that include millions of instruments</p>
</li>
<li>
<p>Easily manipulate genomic data from thousands of people to detect genetic associations with disease</p>
</li>
<li>
<p>Assess agricultural land use and crop yield for improved policymaking by periodically processing millions of satellite images</p>
</li>
</ul>
<p>Sitting behind these capabilities is an ecosystem of open source software that can leverage clusters of servers to process massive amounts of data. <a data-primary="Hadoop (Apache)" data-secondary="distributed computing adoption" data-type="indexterm" id="idm46507992403840"/><a data-primary="Apache Hadoop" data-see="Hadoop" data-type="indexterm" id="idm46507992403008"/>The introduction/release of Apache Hadoop in 2006 has led to widespread adoption of distributed computing. The big data ecosystem and tooling have evolved at a rapid pace since then. The past five years have also seen the introduction and adoption of many open source machine learning (ML) and deep learning libraries. These tools aim to leverage vast amounts of data that we now collect and store.</p>
<p>But just as a chisel and a block of stone do not make a statue, there is a gap between having access to these tools and all this data and doing something useful with it. <a data-primary="SQL for data analysis" data-type="indexterm" id="idm46507991919968"/>Often, “doing something useful” means placing a schema over tabular data and using SQL to answer questions like “Of the gazillion users who made it to the third page in our registration process, how many are over 25?” The field of how to architect data storage and organize information (data warehouses, data lakes, etc.) to make answering such questions easy is a rich one, but we will mostly avoid its intricacies in this book.</p>
<p>Sometimes, “doing something useful” takes a little extra work. SQL still may be core to the approach, but to work around idiosyncrasies in the data or perform complex analysis, we need a programming paradigm that’s more flexible and with richer functionality in areas like machine learning and statistics. This is where data science comes in and that’s what we are going to talk about in this book.</p>
<p>In this chapter, we’ll start by introducing big data as a concept and discuss some of the challenges that arise when working with large datasets. We will then introduce Apache Spark, an open source framework for distributed computing, and its key components. Our focus will be on PySpark, Spark’s Python API, and how it fits within a wider ecosystem. This will be followed by a discussion of the changes brought by Spark 3.0, the framework’s first major release in four years. We will finish with a brief note about how PySpark addresses challenges of data science and why it is a great addition to your skillset.</p>
<p>Previous editions of this book used<a data-primary="PySpark API" data-secondary="Scala API versus" data-type="indexterm" id="idm46507993409904"/><a data-primary="Scala" data-secondary="API versus PySpark API" data-type="indexterm" id="idm46507993408960"/> Spark’s Scala API for code examples. We decided to use PySpark instead because of Python’s popularity in the data science community and an increased focus by the core Spark team to better support the language. By the end of this chapter, you will ideally appreciate this decision.</p>
<section data-pdf-bookmark="Working with Big Data" data-type="sect1"><div class="sect1" id="idm46507993412640">
<h1>Working with Big Data</h1>
<p>Many of our favorite small data tools<a data-primary="big data" data-secondary="working with" data-type="indexterm" id="ch01-work"/><a data-primary="data analysis" data-secondary="big data" data-type="indexterm" id="ch01-work2"/><a data-primary="datasets" data-secondary="working with big data" data-type="indexterm" id="ch01-work3"/><a data-primary="pandas" data-secondary="big data and" data-type="indexterm" id="idm46507992295536"/> hit a wall when working with big data. Libraries like pandas are not equipped to deal with data that can’t fit in our RAM. Then, what should an equivalent process look like that can leverage clusters of computers to achieve the same outcomes on large datasets? <a data-primary="big data" data-secondary="challenges data scientists face" data-type="indexterm" id="idm46507992690272"/><a data-primary="distributed computing" data-type="indexterm" id="idm46507992689360"/><a data-primary="big data" data-secondary="about distributed computing" data-type="indexterm" id="idm46507992350976"/>Challenges of distributed computing require us to rethink many of the basic assumptions that we rely on in single-node systems. For example, because data must be partitioned across many nodes on a cluster, algorithms that have wide data dependencies will suffer from the fact that network transfer rates are orders of magnitude slower than memory accesses. As the number of machines working on a problem increases, the probability of a failure increases. These facts require a programming paradigm that is sensitive to the characteristics of the underlying system: one that discourages poor choices and makes it easy to write code that will execute in a highly parallel manner.</p>
<aside class="pagebreak-before less_space" data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46507992349808">
<h5>How Big Is Big Data?</h5>
<p>Without a reference point, <a data-primary="big data" data-secondary="how big is big data" data-type="indexterm" id="idm46507993804912"/><a data-primary="scalability" data-secondary="how big is big data" data-type="indexterm" id="idm46507996913536"/><a data-primary="data analysis" data-secondary="big data" data-tertiary="how big is big data" data-type="indexterm" id="idm46507991898176"/>the term <em>big data</em> is ambiguous. Moreover, the age-old two-tier definition of small and big data can be confusing. When it comes to data size, a three-tiered definition is more helpful (see <a data-type="xref" href="#tiered-data">Table 1-1</a>).</p>
<table id="tiered-data">
<caption><span class="label">Table 1-1. </span>A tiered definition of data sizes</caption>
<thead>
<tr>
<th>Dataset type</th>
<th>Fits in RAM?</th>
<th>Fits on local disk?</th>
</tr>
</thead>
<tbody>
<tr>
<td><p>Small dataset</p></td>
<td><p>Yes</p></td>
<td><p>Yes</p></td>
</tr>
<tr>
<td><p>Medium dataset</p></td>
<td><p>No</p></td>
<td><p>Yes</p></td>
</tr>
<tr>
<td><p>Big dataset</p></td>
<td><p>No</p></td>
<td><p>No</p></td>
</tr>
</tbody>
</table>
<p>As per the table, if the dataset can fit in memory or disk on a single system, it cannot be termed big data. This definition is not perfect, but it does act as a good rule of thumb in context of an average machine.</p>
<p>The focus of this book is to enable you to work efficiently with big data. If your dataset is small and can fit in memory, stay away from distributed systems.
To analyze medium-sized datasets, a database or parallelism may be good enough at times. At other times, you may have to set up a cluster and use big data tools. Hopefully, the experience that you will gain in the following chapters will help you make such judgment calls.</p>
</div></aside>
<p>Single-machine tools that have come to recent prominence in the software community are not the only tools used for data analysis. Scientific fields like genomics that deal with large datasets have been leveraging parallel-computing frameworks for decades. Most people processing data in these fields today are familiar with a cluster-computing environment <a data-primary="HPC (high-performance computing)" data-type="indexterm" id="idm46507992711312"/><a data-primary="Python" data-secondary="scalability" data-type="indexterm" id="idm46507992710752"/><a data-primary="R" data-secondary="scalability" data-type="indexterm" id="idm46507996133792"/><a data-primary="scalability" data-secondary="Python and R" data-type="indexterm" id="idm46507996132848"/>called HPC (high-performance computing). Where the difficulties with Python and R lie in their inability to scale, the difficulties with HPC lie in its relatively low level of abstraction and difficulty of use. For example, to process a large file full of DNA-sequencing reads in parallel, we must manually split it up into smaller files and submit a job for each of those files to the cluster scheduler. If some of these fail, the user must detect the failure and manually resubmit them. If the analysis requires all-to-all operations like sorting the entire dataset, the large dataset must be streamed through a single node, or the scientist must resort to lower-level distributed frameworks like MPI, which are difficult to program without extensive knowledge of C and distributed/networked systems.</p>
<p>Tools written for HPC environments <a data-primary="parallelizing" data-secondary="distributed computing versus HPC" data-type="indexterm" id="idm46507991935984"/>often fail to decouple the in-memory data models from the lower-level storage models. For example, many tools only know how to read data from a POSIX filesystem in a single stream, making it difficult to make tools naturally parallelize or to use other storage backends, like databases. <a data-primary="big data" data-secondary="about distributed computing" data-type="indexterm" id="idm46507991934816"/><a data-primary="distributed computing" data-seealso="big data" data-type="indexterm" id="idm46507996152192"/>Modern distributed computing frameworks provide abstractions that allow users to treat a cluster of computers more like a single computer—to automatically split up files and distribute storage over many machines, divide work into smaller tasks and execute them in a distributed manner, and recover from failures. They can automate a lot of the hassle of working with large datasets and are far cheaper than HPC.</p>
<div data-type="note" epub:type="note">
<p>A simple way to think about <em>distributed systems</em> is that they are a group<a data-primary="scalability" data-secondary="distributed computing" data-type="indexterm" id="idm46507993775536"/> of independent computers that appear to the end user as a single computer.
They allow for horizontal scaling. That means adding more computers rather than upgrading a single system (vertical scaling). The latter is relatively expensive and often insufficient for large workloads.
Distributed systems are great for scaling and reliability but also introduce complexity when it comes to design, construction, and debugging. One should understand this trade-off before opting for such a tool.<a data-startref="ch01-work" data-type="indexterm" id="idm46507992610080"/><a data-startref="ch01-work2" data-type="indexterm" id="idm46507992609408"/><a data-startref="ch01-work3" data-type="indexterm" id="idm46507992851760"/></p>
</div>
</div></section>
<section data-pdf-bookmark="Introducing Apache Spark and PySpark" data-type="sect1"><div class="sect1" id="idm46507992850832">
<h1>Introducing Apache Spark and PySpark</h1>
<p>Enter Apache Spark, <a data-primary="Spark (Apache)" data-secondary="about" data-type="indexterm" id="idm46507992007872"/><a data-primary="big data" data-secondary="Spark framework" data-seealso="Spark" data-type="indexterm" id="idm46507992006864"/>an open source framework that combines an engine for distributing programs across clusters of machines with an elegant model for writing programs atop it. Spark originated at the University of California, Berkeley, AMPLab and has since been contributed to the Apache Software Foundation. When released, it was arguably the first open source software that made distributed programming truly accessible to data scientists.</p>
<section data-pdf-bookmark="Components" data-type="sect2"><div class="sect2" id="idm46507993751024">
<h2>Components</h2>
<p>Apart from the core computation engine<a data-primary="Spark (Apache)" data-secondary="components" data-type="indexterm" id="idm46507993018752"/><a data-primary="Spark (Apache)" data-secondary="components" data-tertiary="Spark Core" data-type="indexterm" id="idm46507993017776"/><a data-primary="Spark Core" data-type="indexterm" id="idm46507996915648"/> (Spark Core), Spark is comprised of four main components. Spark code written by a user, using either of its APIs, is executed in the workers’ JVMs (Java Virtual Machines) across the cluster (see <a data-type="xref" href="ch02.xhtml#introduction_to_data_anlysis_with_pyspark">Chapter 2</a>). These components are available as distinct libraries as shown in <a data-type="xref" href="#fig0101">Figure 1-1</a>:</p>
<dl>
<dt>Spark SQL and DataFrames + Datasets</dt>
<dd>
<p>A module<a data-primary="SQL for data analysis" data-secondary="Spark SQL module" data-type="indexterm" id="idm46507991860512"/><a data-primary="Spark (Apache)" data-secondary="components" data-tertiary="Spark SQL and DataFrames + Datasets" data-type="indexterm" id="idm46507991918720"/><a data-primary="dataframes" data-secondary="Spark SQL component" data-type="indexterm" id="idm46507991917472"/><a data-primary="Spark SQL module" data-secondary="about" data-type="indexterm" id="idm46507993381296"/> for working with structured data.</p>
</dd>
<dt>MLlib</dt>
<dd>
<p>A scalable machine learning library.<a data-primary="MLlib component of Spark" data-type="indexterm" id="idm46507992717424"/><a data-primary="Spark (Apache)" data-secondary="components" data-tertiary="MLlib" data-type="indexterm" id="idm46507992716656"/></p>
</dd>
<dt>Structured Streaming</dt>
<dd>
<p>This makes<a data-primary="structured streaming component of Spark" data-type="indexterm" id="idm46507991988800"/><a data-primary="Spark (Apache)" data-secondary="components" data-tertiary="structured streaming" data-type="indexterm" id="idm46507992732400"/> it easy to build scalable fault-tolerant streaming applications.</p>
</dd>
</dl>
<dl class="pagebreak-before">
<dt>GraphX (legacy)</dt>
<dd>
<p>GraphX is Apache Spark’s <a data-primary="GraphX legacy library of Spark" data-type="indexterm" id="idm46507992117264"/><a data-primary="GraphFrames graph processing library" data-type="indexterm" id="idm46507991986064"/><a data-primary="dataframes" data-secondary="GraphFrames library" data-type="indexterm" id="idm46507991985376"/><a data-primary="Spark (Apache)" data-secondary="components" data-tertiary="GraphFrames instead of GraphX" data-type="indexterm" id="idm46507991984432"/>library for graphs and graph-parallel computation. However, for graph analytics, GraphFrames is recommended instead of GraphX, which isn’t being actively developed as much and lacks Python bindings. <a href="https://oreil.ly/p6TYQ">GraphFrames</a> is an open source general graph processing library that is similar to Apache Spark’s GraphX but uses DataFrame-based APIs.</p>
</dd>
</dl>
<figure><div class="figure" id="fig0101">
<img alt="aaps 0101" height="349" src="assets/aaps_0101.png" width="1370"/>
<h6><span class="label">Figure 1-1. </span>Apache Spark components</h6>
</div></figure>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46507994376544">
<h5>Comparison with MapReduce</h5>
<p>One illuminating way to understand Spark<a data-primary="Spark (Apache)" data-secondary="MapReduce compared with" data-type="indexterm" id="idm46507996935904"/><a data-primary="Hadoop (Apache)" data-secondary="MapReduce versus Spark" data-type="indexterm" id="idm46507996934928"/><a data-primary="MapReduce versus Spark" data-type="indexterm" id="idm46507991840064"/><a data-primary="big data" data-secondary="MapReduce versus Spark" data-type="indexterm" id="idm46507991839392"/> is in terms of its advances over its predecessor, Apache Hadoop’s MapReduce. MapReduce revolutionized computation over huge datasets by offering a simple and resilient model for writing programs that could execute in parallel across hundreds to thousands of machines. It broke up work into small tasks and could gracefully accommodate task failures without compromising the job to which they belonged.</p>
<p>Spark maintains MapReduce’s<a data-primary="scalability" data-secondary="Hadoop’s MapReduce" data-type="indexterm" id="idm46507992424368"/><a data-primary="scalability" data-secondary="Spark" data-type="indexterm" id="idm46507992423392"/><a data-primary="Spark (Apache)" data-secondary="about" data-tertiary="scalability" data-type="indexterm" id="idm46507992422448"/> linear scalability and fault tolerance, but extends it in three important ways:</p>
<ul>
<li>
<p>First, rather than relying on a rigid map-then-reduce format, its engine can execute a more general directed acyclic graph of operators. This means that in situations where MapReduce must write out intermediate results to the distributed filesystem, Spark can pass them directly to the next step in the pipeline.</p>
</li>
<li>
<p>Second, it complements its computational capability with a rich set of transformations that enable users to express computation more naturally. Out-of-the-box functions are provided for various tasks, including numerical computation, datetime processing, and string manipulation.</p>
</li>
<li>
<p>Third, Spark extends its predecessors with in-memory processing. This means that future steps that want to deal with the same dataset need not recompute it or reload it from disk. Spark is well-suited for highly iterative algorithms as well as ad hoc queries.</p>
</li>
</ul>
</div></aside>
</div></section>
<section class="pagebreak-before less_space" data-pdf-bookmark="PySpark" data-type="sect2"><div class="sect2" id="idm46507993226080">
<h2>PySpark</h2>
<p>PySpark is Spark’s Python API.<a data-primary="PySpark API" data-secondary="about" data-type="indexterm" id="idm46507992399216"/><a data-primary="Scala" data-secondary="about PySpark" data-type="indexterm" id="idm46507992398240"/><a data-primary="Python" data-secondary="PySpark API" data-type="indexterm" id="idm46507992357984"/><a data-primary="Spark (Apache)" data-secondary="PySpark API" data-seealso="PySpark API" data-type="indexterm" id="idm46507992357040"/> In simpler words, PySpark is a Python-based wrapper over the core Spark framework, which is written primarily in Scala. PySpark provides an intuitive programming environment for data science practitioners and offers the flexibility of Python with the distributed processing capabilities of Spark.</p>
<p>PySpark allows us to work across programming models. For example, a common pattern<a data-primary="extract, transform, and load (ETL) workloads" data-type="indexterm" id="idm46507992473120"/><a data-primary="ETL (extract, transform, and load) workloads" data-type="indexterm" id="idm46507992472448"/><a data-primary="pandas" data-secondary="ETL data manipulation" data-type="indexterm" id="idm46507991915056"/> is to perform large-scale extract, transform, and load (ETL) workloads with Spark and then collect the results to a local machine followed by manipulation using pandas. We’ll explore such programming models as we write PySpark code in the upcoming chapters. <a data-primary="PySpark API" data-secondary="example ML classification" data-type="indexterm" id="idm46507991913984"/><a data-primary="Spark (Apache)" data-secondary="PySpark API" data-tertiary="example ML classification" data-type="indexterm" id="idm46507991913072"/><a data-primary="MLlib component of Spark" data-secondary="example classification" data-type="indexterm" id="idm46507996257904"/><a data-primary="linear regression" data-secondary="example ML classification" data-type="indexterm" id="idm46507996256944"/>Here is a code example from the official documentation to give you a glimpse of what’s to come:</p>
<pre data-code-language="python" data-type="programlisting"><code class="kn">from</code> <code class="nn">pyspark.ml.classification</code> <code class="kn">import</code> <code class="n">LogisticRegression</code>

<code class="c1"># Load training data</code>
<code class="n">training</code> <code class="o">=</code> <code class="n">spark</code><code class="o">.</code><code class="n">read</code><code class="o">.</code><code class="n">format</code><code class="p">(</code><code class="s2">"libsvm"</code><code class="p">)</code><code class="o">.</code><code class="n">load</code><code class="p">(</code><code class="s2">"data/mllib/sample_libsvm_data.txt"</code><code class="p">)</code>

<code class="n">lr</code> <code class="o">=</code> <code class="n">LogisticRegression</code><code class="p">(</code><code class="n">maxIter</code><code class="o">=</code><code class="mi">10</code><code class="p">,</code> <code class="n">regParam</code><code class="o">=</code><code class="mf">0.3</code><code class="p">,</code> <code class="n">elasticNetParam</code><code class="o">=</code><code class="mf">0.8</code><code class="p">)</code>

<code class="c1"># Fit the model</code>
<code class="n">lrModel</code> <code class="o">=</code> <code class="n">lr</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">training</code><code class="p">)</code>

<code class="c1"># Print the coefficients and intercept for logistic regression</code>
<code class="nb">print</code><code class="p">(</code><code class="s2">"Coefficients: "</code> <code class="o">+</code> <code class="nb">str</code><code class="p">(</code><code class="n">lrModel</code><code class="o">.</code><code class="n">coefficients</code><code class="p">))</code>
<code class="nb">print</code><code class="p">(</code><code class="s2">"Intercept: "</code> <code class="o">+</code> <code class="nb">str</code><code class="p">(</code><code class="n">lrModel</code><code class="o">.</code><code class="n">intercept</code><code class="p">))</code>

<code class="c1"># We can also use the multinomial family for binary classification</code>
<code class="n">mlr</code> <code class="o">=</code> <code class="n">LogisticRegression</code><code class="p">(</code><code class="n">maxIter</code><code class="o">=</code><code class="mi">10</code><code class="p">,</code> <code class="n">regParam</code><code class="o">=</code><code class="mf">0.3</code><code class="p">,</code> <code class="n">elasticNetParam</code><code class="o">=</code><code class="mf">0.8</code><code class="p">,</code>
                         <code class="n">family</code><code class="o">=</code><code class="s2">"multinomial"</code><code class="p">)</code>

<code class="c1"># Fit the model</code>
<code class="n">mlrModel</code> <code class="o">=</code> <code class="n">mlr</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">training</code><code class="p">)</code>

<code class="c1"># Print the coefficients and intercepts for logistic regression</code>
<code class="c1"># with multinomial family</code>
<code class="nb">print</code><code class="p">(</code><code class="s2">"Multinomial coefficients: "</code> <code class="o">+</code> <code class="nb">str</code><code class="p">(</code><code class="n">mlrModel</code><code class="o">.</code><code class="n">coefficientMatrix</code><code class="p">))</code>
<code class="nb">print</code><code class="p">(</code><code class="s2">"Multinomial intercepts: "</code> <code class="o">+</code> <code class="nb">str</code><code class="p">(</code><code class="n">mlrModel</code><code class="o">.</code><code class="n">interceptVector</code><code class="p">))</code></pre>
<aside class="pagebreak-before less_space" data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46507992536880">
<h5>Spark Versus PySpark Versus SparkSQL</h5>
<p>The distinction between Spark, PySpark, and SparkSQL can<a data-primary="Spark (Apache)" data-secondary="PySpark API" data-tertiary="Spark versus" data-type="indexterm" id="idm46507995577872"/><a data-primary="PySpark API" data-secondary="Spark and Spark SQL versus" data-type="indexterm" id="idm46507995576656"/><a data-primary="SQL for data analysis" data-secondary="Spark SQL module" data-tertiary="PySpark and Spark versus" data-type="indexterm" id="idm46507995575744"/><a data-primary="Spark (Apache)" data-secondary="about" data-type="indexterm" id="idm46507995574560"/><a data-primary="PySpark API" data-secondary="about" data-type="indexterm" id="idm46507995573616"/><a data-primary="Spark SQL module" data-secondary="PySpark and Spark versus" data-type="indexterm" id="idm46507995572672"/><a data-primary="Spark (Apache)" data-secondary="components" data-tertiary="Spark SQL and DataFrames + Datasets" data-type="indexterm" id="idm46507995571712"/> confuse beginners. We have introduced the three terms individually. Let’s summarize the differences to avoid any confusion going ahead.</p>
<dl>
<dt>Spark</dt>
<dd>
<p>A distributed processing framework written primarily in the Scala programming language. The framework offers different language APIs on top of the core Scala-based framework.</p>
</dd>
<dt>PySpark</dt>
<dd>
<p>Spark’s Python API. Think of it as a Python-based wrapper on top of core Spark.</p>
</dd>
<dt>SparkSQL</dt>
<dd>
<p>A Spark module for structured data processing. It is part of the core Spark framework and accessible through all of its language APIs, including PySpark.</p>
</dd>
</dl>
</div></aside>
</div></section>
<section data-pdf-bookmark="Ecosystem" data-type="sect2"><div class="sect2" id="idm46507995565520">
<h2>Ecosystem</h2>
<p>Spark is the closest thing to a Swiss Army knife<a data-primary="big data" data-secondary="Spark framework" data-type="indexterm" id="idm46507995563968"/><a data-primary="Spark (Apache)" data-secondary="about" data-tertiary="data from many sources" data-type="indexterm" id="idm46507995562992"/><a data-primary="HPC (high-performance computing)" data-type="indexterm" id="idm46507995401456"/><a data-primary="Hadoop (Apache)" data-secondary="Spark data source" data-type="indexterm" id="idm46507995400816"/> that we have in the big data ecosystem. To top it off, it integrates well with rest of the ecosystem and is extensible. Spark decouples storage and compute unlike Apache Hadoop and HPC systems described previously. That means we can use Spark to read data stored in many sources—Apache Hadoop, Apache Cassandra, Apache HBase, MongoDB, Apache Hive, RDBMSs, and more—and process it all in memory. <a data-primary="dataframes" data-secondary="Spark data sources" data-type="indexterm" id="idm46507995399744"/>Spark’s DataFrameReader and DataFrameWriter APIs can also be extended to read data from other sources, such as Apache Kafka, Amazon Kinesis, Azure Storage, and Amazon S3, on which it can operate. <a data-primary="Spark (Apache)" data-secondary="about" data-tertiary="deployment modes" data-type="indexterm" id="idm46507995398672"/><a data-primary="deployment modes of Spark" data-type="indexterm" id="idm46507995397456"/>It also supports multiple  deployment modes, ranging from local environments to Apache YARN and Kubernetes clusters.</p>
<p>There also exists a wide community around it.<a data-primary="online resources" data-secondary="Spark" data-tertiary="third-party packages" data-type="indexterm" id="idm46507995396080"/><a data-primary="Spark (Apache)" data-secondary="about" data-tertiary="third-party packages" data-type="indexterm" id="idm46507995394832"/> This has led to creation of many third-party packages. A community-created list of such packages can be found <a href="https://oreil.ly/N8ZDf">here</a>. <a data-primary="online resources" data-secondary="cloud provider–managed Spark workloads" data-type="indexterm" id="idm46507995392704"/><a data-primary="cloud provider–managed Spark workloads" data-type="indexterm" id="idm46507995391728"/><a data-primary="AWS EMR–managed Spark workloads" data-type="indexterm" id="idm46507995391088"/><a data-primary="Azure Databricks–managed Spark workloads" data-type="indexterm" id="idm46507995390448"/><a data-primary="GCP Dataproc–managed Spark workloads" data-type="indexterm" id="idm46507995389808"/>Major cloud providers (<a href="https://oreil.ly/29yh1">AWS EMR</a>, <a href="https://oreil.ly/RAShf">Azure Databricks</a>, <a href="https://oreil.ly/5i5MT">GCP Dataproc</a>) also provide third-party vendor options for running managed Spark workloads. In addition, there are dedicated conferences and local meetup groups that can be of interest for learning about interesting applications and best practices.</p>
</div></section>
</div></section>
<section class="pagebreak-before less_space" data-pdf-bookmark="Spark 3.0" data-type="sect1"><div class="sect1" id="idm46507995386784">
<h1>Spark 3.0</h1>
<p>In 2020, Apache Spark made<a data-primary="Spark (Apache)" data-secondary="about" data-tertiary="version 3 features" data-type="indexterm" id="idm46507995384976"/><a data-primary="Spark (Apache)" data-secondary="components" data-tertiary="version 3 features" data-type="indexterm" id="idm46507995383728"/> its first major release since 2016 when Spark 2.0 was released—Spark 3.0. This series’ last edition, released in 2017, covered changes brought about by Spark 2.0. Spark 3.0 does not introduce as many major API changes as the last major release. This release focuses on performance and usability improvements without introducing significant backward incompatibility.</p>
<p>The Spark SQL module has seen<a data-primary="Spark SQL module" data-secondary="version 3 features" data-type="indexterm" id="idm46507995382000"/><a data-primary="SQL for data analysis" data-secondary="Spark SQL module" data-tertiary="version 3 features" data-type="indexterm" id="idm46507995381024"/><a data-primary="adaptive execution in Spark" data-type="indexterm" id="idm46507995379808"/> major performance enhancements in the form of adaptive query execution and dynamic partition pruning. In simpler terms, they allow Spark to adapt a physical execution plan during runtime and skip over data that’s not required in a query’s results, respectively. These optimizations address significant effort that users had to previously put into manual tuning and optimization. Spark 3.0 is almost two times faster than Spark 2.4 on TPC-DS, an industry-standard analytical processing benchmark. <a data-primary="structured streaming component of Spark" data-secondary="version 3 features" data-type="indexterm" id="idm46507995378992"/><a data-primary="MLlib component of Spark" data-secondary="version 3 features" data-type="indexterm" id="idm46507995378032"/><a data-primary="dataframes" data-secondary="Spark version 3 features" data-type="indexterm" id="idm46507995377072"/>Since most Spark applications are backed by the SQL engine, all the higher-level libraries, including MLlib and structured streaming, and higher-level APIs, including SQL and DataFrames, have benefited. Compliance with the ANSI SQL standard makes the SQL API more usable.</p>
<p>Python has emerged as the leader<a data-primary="big data" data-secondary="PyData ecosystem" data-type="indexterm" id="idm46507995375376"/><a data-primary="PySpark API" data-secondary="about" data-tertiary="PyData ecosystem" data-type="indexterm" id="idm46507995374400"/><a data-primary="Python" data-secondary="about PyData ecosystem" data-type="indexterm" id="idm46507995373184"/><a data-primary="Python" data-secondary="PySpark API" data-type="indexterm" id="idm46507995372240"/><a data-primary="PyData ecosystem" data-secondary="PySpark API" data-type="indexterm" id="idm46507995371296"/> in terms of adoption in the data science ecosystem. Consequently, Python is now the most widely used language on Spark. <a data-primary="PyPI (Python Package Index)" data-secondary="PySpark monthly downloads" data-type="indexterm" id="idm46507995370224"/>PySpark has more than five million monthly downloads on PyPI, the Python Package Index. Spark 3.0 improves its functionalities and usability. <a data-primary="pandas" data-secondary="user-defined functions" data-type="indexterm" id="idm46507995369120"/><a data-primary="UDFs (user-defined functions) in pandas" data-type="indexterm" id="idm46507995058864"/>pandas user-defined functions (UDFs) have been redesigned to support Python type hints and iterators as arguments. New pandas UDF types have been included, and the error handling is now more pythonic. Python versions below 3.6 have been deprecated. From Spark 3.2 onward, Python 3.6 support has been deprecated too.</p>
<p>Over the last four years, the data science ecosystem has also changed at a rapid pace. There is an increased focus on putting machine learning models in production. Deep learning has provided remarkable results and the Spark team is currently experimenting to allow the project’s scheduler to leverage accelerators such as GPUs.</p>
</div></section>
<section data-pdf-bookmark="PySpark Addresses Challenges of Data Science" data-type="sect1"><div class="sect1" id="idm46507995057712">
<h1>PySpark Addresses Challenges of Data Science</h1>
<p>For a system that seeks to enable<a data-primary="PySpark API" data-secondary="data challenges faced" data-type="indexterm" id="idm46507995056336"/><a data-primary="big data" data-secondary="PySpark addressing challenges of" data-seealso="PySpark API" data-type="indexterm" id="idm46507995055360"/><a data-primary="big data" data-secondary="challenges data scientists face" data-type="indexterm" id="idm46507995054128"/><a data-primary="data analysis" data-secondary="preprocessing, iteration, communication" data-type="indexterm" id="idm46507995053168"/> complex analytics on huge data to be successful, it needs to be informed by—or at least not conflict with—some fundamental challenges faced by data scientists.</p>
<ul>
<li>
<p>First, the vast majority of work<a data-primary="preprocessing data" data-type="indexterm" id="idm46507995051056"/> that goes into conducting successful analyses lies in
preprocessing data. Data is messy, and cleansing, munging, fusing, mushing, and many other verbs are prerequisites to doing anything useful with it.</p>
</li>
<li>
<p>Second, <em>iteration</em> is a fundamental part of data science.<a data-primary="iteration in data analysis" data-type="indexterm" id="idm46507995048896"/> Modeling and analysis typically require multiple passes over the same data. Popular optimization 
<span class="keep-together">procedures</span> like stochastic gradient descent involve repeated scans over their inputs to reach convergence. Iteration also matters within the data scientist’s own workflow. Choosing the right features, picking the right algorithms, running the right significance tests, and finding the right hyperparameters all require 
<span class="keep-together">experimentation</span>.</p>
</li>
<li>
<p>Third, the task isn’t over when a<a data-primary="models" data-secondary="about applying" data-type="indexterm" id="idm46507995045648"/><a data-primary="communication by data scientists to non–data scientists" data-type="indexterm" id="idm46507995044672"/><a data-primary="big data" data-secondary="data science into practice" data-type="indexterm" id="idm46507995044032"/> well-performing model has been built. The point of
data science is to make data useful to non–data scientists. Uses of data recommendation engines and real-time fraud detection systems culminate in data applications. In such systems, models become part of a production service and may need to be rebuilt periodically or even in real time.</p>
</li>
</ul>
<p>PySpark deals well with the aforementioned challenges of data science,<a data-primary="PySpark API" data-secondary="data challenges faced" data-tertiary="analyst productivity" data-type="indexterm" id="idm46507995042256"/><a data-primary="big data" data-secondary="challenges data scientists face" data-tertiary="analyst productivity" data-type="indexterm" id="idm46507995041008"/> acknowledging that the biggest bottleneck in building data applications is not CPU, disk, or network, but analyst productivity. Collapsing the full pipeline, from preprocessing to model evaluation, into a single programming environment can speed up development. <a data-primary="REPL (read-eval-print loop) environment" data-type="indexterm" id="idm46507995039648"/>By packaging an expressive programming model with a set of analytic libraries under an REPL (read-eval-print loop) environment, PySpark avoids the round trips to IDEs. The more quickly analysts can experiment with their data, the higher likelihood they have of doing something useful with it.</p>
<div data-type="note" epub:type="note">
<p>A read-eval-print loop, or REPL, is a computer environment where user inputs are read and evaluated, and then the results are returned to the user.</p>
</div>
<p>PySpark’s core APIs provide a strong foundation for data transformation independent of any functionality in statistics, machine learning, or matrix algebra. When exploring and getting a feel for a dataset, data scientists can keep data in memory while they run queries, and easily cache transformed versions of the data as well, without suffering a trip to disk. As a framework that makes modeling easy but is also a good fit for production systems, it is a huge win for the data science ecosystem.</p>
</div></section>
<section data-pdf-bookmark="Where to Go from Here" data-type="sect1"><div class="sect1" id="idm46507995037056">
<h1>Where to Go from Here</h1>
<p>Spark spans the gap between systems designed for exploratory analytics and systems designed for operational analytics. It is often said that a data scientist is someone who is better at engineering than most statisticians and better at statistics than most engineers. At the very least, Spark is better at being an operational system than most exploratory systems and better for data exploration than the technologies commonly used in operational systems. We hope that this chapter was helpful and you are now excited about getting hands-on with PySpark. That’s what we will do from the next chapter onward!</p>
</div></section>
</div></section></div></body></html>