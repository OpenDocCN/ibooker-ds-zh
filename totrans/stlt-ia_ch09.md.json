["```py\npip install langchain-community langchain-core langchain-openai langchain-pinecone langgraph pinecone toml\n```", "```py\nfrom langgraph.graph import START, END, StateGraph\nfrom langchain_core.messages import AnyMessage, HumanMessage\nfrom langchain_openai import ChatOpenAI\nfrom typing import TypedDict\n\nllm = ChatOpenAI(model_name=\"gpt-4o-mini\", openai_api_key=\"sk-proj-...\")\n\nclass MyGraphState(TypedDict):\n  messages: list[AnyMessage]\n\nbuilder = StateGraph(MyGraphState)\n\ndef assistant_node(state):\n  messages = state[\"messages\"]\n  ai_response_message = llm.invoke(messages)\n  return {\"messages\": messages + [ai_response_message]}\n\nbuilder.add_node(\"assistant\", assistant_node)\nbuilder.add_edge(START, \"assistant\")\nbuilder.add_edge(\"assistant\", END)\ngraph = builder.compile()\n\ninput_message = input(\"Talk to the bot: \")\ninitial_state = {\"messages\": [HumanMessage(content=input_message)]}\nfinal_state = graph.invoke(initial_state)\n\nprint(\"Bot:\\n\" + final_state[\"messages\"][-1].content)\n```", "```py\nllm = ChatOpenAI(model_name=\"gpt-4o-mini\", openai_api_key=\"sk-proj-...\")\n```", "```py\nclass MyGraphState(TypedDict):\n  messages: list[AnyMessage]\n```", "```py\ndef assistant_node(state):\n  messages = state[\"messages\"]\n  ai_response_message = llm.invoke(messages)\n  return {\"messages\": messages + [ai_response_message]}\n```", "```py\nbuilder.add_node(\"assistant\", assistant_node)\nbuilder.add_edge(START, \"assistant\")\nbuilder.add_edge(\"assistant\", END)\n```", "```py\ninput_message = input(\"Talk to the bot: \")\ninitial_state = {\"messages\": [HumanMessage(content=input_message)]}\nfinal_state = graph.invoke(initial_state)\n\nprint(\"The LLM responded with:\\n\" + final_state[\"messages\"][-1].content)\n```", "```py\npython graph_example.py\n```", "```py\n$ python graph_example.py\nTalk to the bot: Howdy! Could you write a haiku about Note n' Nib for me?\nBot:\nInk and paper dance,\nWhispers of thoughts intertwineâ€”\nNote n' Nib's embrace.\n```", "```py\nfrom langgraph.graph import START, END, StateGraph, MessagesState\nfrom langchain_core.messages import HumanMessage\n\nclass SupportAgentGraph:\n  def __init__(self, llm):\n self.llm = llm\n self.graph = self.build_graph()\n\n  def get_assistant_node(self):\n    def assistant_node(state):\n      ai_response_message = self.llm.invoke(state[\"messages\"])\n      return {\"messages\": [ai_response_message]}\n    return assistant_node\n\n  def build_graph(self):\n    builder = StateGraph(MessagesState)\n    builder.add_node(\"assistant\", self.get_assistant_node())\n    builder.add_edge(START, \"assistant\")\n    builder.add_edge(\"assistant\", END)\n    return builder.compile()\n\n  def invoke(self, human_message_text):\n    human_msg = HumanMessage(content=human_message_text)\n    state = {\"messages\": [human_msg]}\n    return self.graph.invoke(state)\n```", "```py\ndef get_assistant_node(self):\n  def assistant_node(state):\n    ai_response_message = self.llm.invoke(state[\"messages\"])\n    return {\"messages\": [ai_response_message]}\n  return assistant_node\n```", "```py\nbuilder.add_node(\"assistant\", self.get_assistant_node())\n```", "```py\nreturn {\"messages\": messages + [ai_response_message]}\n```", "```py\nreturn {\"messages\": [ai_response_message]}\n```", "```py\nmessages: Annotated[list[AnyMessage], add_messages]\n```", "```py\nfrom langchain_openai import ChatOpenAI\nfrom graph import SupportAgentGraph\n\nclass Bot:\n  def __init__(self, api_keys):\n    self.api_keys = api_keys\n    self.llm = self.get_llm()\n    self.graph = SupportAgentGraph(llm=self.llm)\n\n  def get_llm(self):\n    return ChatOpenAI(\n      model_name=\"gpt-4o-mini\",\n      openai_api_key=self.api_keys[\"OPENAI_API_KEY\"],\n      max_tokens=2000\n    )\n\n  def chat(self, human_message_text):\n    final_state = self.graph.invoke(human_message_text)\n    return final_state[\"messages\"][-1].content\n```", "```py\nimport streamlit as st\nfrom bot import Bot\n\nif \"bot\" not in st.session_state:\n  api_keys = st.secrets[\"api_keys\"]\n  st.session_state.bot = Bot(api_keys)\nbot = st.session_state.bot\n\nif human_message_text := st.chat_input(\"Chat with me!\"):\n  st.chat_message(\"human\").markdown(human_message_text)\n  ai_message_text = bot.chat(human_message_text)\n  st.chat_message(\"ai\").markdown(ai_message_text)\n```", "```py\nif human_message_text := st.chat_input(\"Chat with me!\"):\n  ...\n```", "```py\nhuman_message_text = st.chat_input(\"Chat with Nibby!\")\nif human_message_text:\n  ...\n\n```", "```py\nst.chat_message(\"human\").markdown(human_message_text)\nai_message_text = bot.chat(human_message_text)\nst.chat_message(\"ai\").markdown(ai_message_text)\n```", "```py\nwith st.chat_message(\"human\"):\n  st.markdown(human_message_text)\n```", "```py\n[api_keys]\nOPENAI_API_KEY = 'sk-proj-...'    #A\n```", "```py\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom langgraph.graph import START, END, StateGraph, MessagesState\n...\n\nclass SupportAgentGraph:\n  def __init__(self, llm):\n    self.llm = llm\n\n    self.config = {\"configurable\": {\"thread_id\": \"1\"}}\n    ...\n...\n  def build_graph(self):\n    memory = MemorySaver()\n    builder = StateGraph(MessagesState)\n    ...\n    return builder.compile(checkpointer=memory)\n\n  def invoke(self, human_message_text):\n    ...\n    return self.graph.invoke(state, self.config)\n```", "```py\nmemory = MemorySaver()\n```", "```py\nreturn builder.compile(checkpointer=memory)\n```", "```py\nself.config = {\"configurable\": {\"thread_id\": \"1\"}}\n```", "```py\nreturn self.graph.invoke(state, self.config)\n```", "```py\n...\nclass SupportAgentGraph:\n\n  ...\n  def get_conversation(self):\n state = self.graph.get_state(self.config)\n if \"messages\" not in state.values:\n return []\n return state.values[\"messages\"]\n```", "```py\n...\nclass Bot:\n\n  ...\n  def get_history(self):\n return self.graph.get_conversation()\n```", "```py\nWe can now make the changes required in frontend.py:\n...\nbot = st.session_state.bot\n\nfor message in bot.get_history():\n st.chat_message(message.type).markdown(message.content)\n\nif human_message_text := st.chat_input(\"Chat with Nibby!\"):\n    ...\n```", "```py\nBASE_SYS_MSG = \"\"\"\n  You are a customer support agent for Note n' Nib, an online stationery\n  retailer. You are tasked with providing customer support to customers who\n  have questions or concerns about the products or services offered by the\n  company.\n\n  You must refuse to answer any questions or entertain any requests that\n  are not related to Note n' Nib or its products and services.\n\"\"\"\n```", "```py\n...\nfrom langchain_core.messages import HumanMessage, SystemMessage\nfrom prompts import *\n\nclass AgentState(MessagesState):\n sys_msg_text: str\n\nclass SupportAgentGraph:\n  def __init__(self, llm):\n    ...\n\n  @staticmethod\n def base_context_node(state):\n return {\"sys_msg_text\": BASE_SYS_MSG}\n\n  def get_assistant_node(self):\n    def assistant_node(state):\n      sys_msg = SystemMessage(content=state[\"sys_msg_text\"])\n messages_to_send = [sys_msg] + state[\"messages\"]\n      ai_response_message = self.llm.invoke(messages_to_send)\n      return {\"messages\": [ai_response_message]}\n    return assistant_node\n\n  def build_graph(self):\n    memory = MemorySaver()\n    builder = StateGraph(AgentState)\n\n    builder.add_node(\"base_context\", self.base_context_node)\n    builder.add_node(\"assistant\", self.get_assistant_node())\n\n    builder.add_edge(START, \"base_context\")\n    builder.add_edge(\"base_context\", \"assistant\")\n    builder.add_edge(\"assistant\", END)\n\n    return builder.compile(checkpointer=memory)\n\n  def invoke(self, human_message_text):\n    ...\n  ...\n```", "```py\nclass AgentState(MessagesState):\n  sys_msg_text: str\n```", "```py\n@staticmethod\ndef base_context_node(state):\n  return {\"sys_msg_text\": BASE_SYS_MSG}\n```", "```py\nsys_msg = SystemMessage(content=state[\"sys_msg_text\"])\nmessages_to_send = [sys_msg] + state[\"messages\"]\nai_response_message = self.llm.invoke(messages_to_send)\n```", "```py\nbuilder = StateGraph(AgentState)\n```", "```py\nbuilder.add_node(\"base_context\", self.base_context_node)\n```", "```py\nbuilder.add_edge(START, \"base_context\")\nbuilder.add_edge(\"base_context\", \"assistant\")\n```", "```py\nProper care ensures your InkStream and RoyalQuill fountain pens write smoothly for years.\n\n- Cleaning: Flush the nib with warm water every few weeks.\n- Refilling: Use high-quality ink to prevent clogging.\n- Storage: Store pens upright to avoid leaks and ensure ink flow.\n```", "```py\n[api_keys]\nOPENAI_API_KEY = 'sk-proj-...'    #A\nVECTOR_STORE_API_KEY = 'pcsk_...'    #B\n\n[config]\nVECTOR_STORE_INDEX_NAME = 'index_name_you_chose'    #C\n```", "```py\nfrom pinecone import Pinecone\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain_pinecone import PineconeVectorStore\nfrom langchain_community.document_loaders import DirectoryLoader, TextLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\nclass VectorStore:\n  def __init__(self, api_keys, index_name):\n    pc = Pinecone(api_key=api_keys[\"VECTOR_STORE_API_KEY\"])\n    embeddings = OpenAIEmbeddings(api_key=api_keys[\"OPENAI_API_KEY\"])\n    index = pc.Index(index_name)\n    self.store = PineconeVectorStore(index=index, embedding=embeddings)\n\n  def ingest_folder(self, folder_path):\n    loader = DirectoryLoader(\n      folder_path,\n      glob=\"**/*.txt\",\n      loader_cls=TextLoader\n    )\n    documents = loader.load()\n    splitter = RecursiveCharacterTextSplitter(\n      chunk_size=1000,\n      chunk_overlap=200\n    )\n    texts = splitter.split_documents(documents)\n    self.store.add_documents(texts)\n\n  def retrieve(self, query):\n    return self.store.similarity_search(query)\n```", "```py\nloader = DirectoryLoader(\n  folder_path,\n  glob=\"**/*.txt\",\n  loader_cls=TextLoader\n)\n```", "```py\ndocuments = loader.load()\n```", "```py\nsplitter = RecursiveCharacterTextSplitter(\n  chunk_size=500,\n  chunk_overlap=200\n)\ntexts = splitter.split_documents(documents)\n```", "```py\nself.store.add_documents(texts)\n```", "```py\ndef retrieve(self, query):\n  return self.store.similarity_search(query)\n```", "```py\nimport toml\nfrom vector_store import VectorStore\n\nsecrets = toml.load(\".streamlit/secrets.toml\")\napi_keys = secrets[\"api_keys\"]\nindex_name = secrets[\"config\"][\"VECTOR_STORE_INDEX_NAME\"]\nvector_store = VectorStore(api_keys, index_name)\nvector_store.ingest_folder(\"articles/\")\n```", "```py\nsecrets = toml.load(\".streamlit/secrets.toml\")\n```", "```py\napi_keys = secrets[\"api_keys\"]\nindex_name = secrets[\"config\"][\"VECTOR_STORE_INDEX_NAME\"]\n```", "```py\nvector_store = VectorStore(api_keys, index_name)\n```", "```py\nvector_store.ingest_folder(\"articles/\")\n```", "```py\npython ingest_to_vector_store.py\n```", "```py\nSYS_MSG_AUGMENTATION = \"\"\"\n  You have the following excerpts from Note n' Nib's\n  customer service manual:\n  ```", "```py\n  If you're unable to answer the customer's question confidently with the\n  given information, please redirect the user to call a human customer\n  service representative at 1-800-NOTENIB.\n\"\"\"\n```", "```py\n...\nfrom langchain_core.messages import HumanMessage, SystemMessage\nfrom langchain_core.documents import Document\nfrom prompts import *\n\nclass AgentState(MessagesState):\n  sys_msg_text: str\n  retrieved_docs: list[Document]\n\nclass SupportAgentGraph:\n  def __init__(self, llm, vector_store):\n    self.llm = llm\n    self.vector_store = vector_store\n\n    self.config = {\"configurable\": {\"thread_id\": \"1\"}}\n    self.graph = self.build_graph()\n\n  ...\n  def get_retrieve_node(self):\n def retrieve_node(state: AgentState):\n messages = state[\"messages\"]\n message_contents = [message.content for message in messages]\n retrieval_query = \"\\n\".join(message_contents)\n docs = self.vector_store.retrieve(retrieval_query)\n return {\"retrieved_docs\": docs}\n return retrieve_node\n\n @staticmethod\n def augment_node(state: AgentState):\n docs = state[\"retrieved_docs\"]\n docs_content_list = [doc.page_content for doc in docs]\n content = \"\\n\".join(docs_content_list)\n new_text = SYS_MSG_AUGMENTATION.replace(\"{docs_content}\", content)\n return {\"sys_msg_text\": BASE_SYS_MSG + \"\\n\\n\" + new_text}\n\n  ...\n  def build_graph(self):\n    ...\n    builder.add_node(\"base_context\", self.base_context_node)\n    builder.add_node(\"retrieve\", self.get_retrieve_node())\n builder.add_node(\"augment\", self.augment_node)\n    builder.add_node(\"assistant\", self.get_assistant_node())\n\n    builder.add_edge(START, \"base_context\")\n    builder.add_edge(\"base_context\", \"retrieve\")\n builder.add_edge(\"retrieve\", \"augment\")\n builder.add_edge(\"augment\", \"assistant\")\n    builder.add_edge(\"assistant\", END)\n\n    return builder.compile(checkpointer=memory)\n\n  ...\n```", "```py\nclass AgentState(MessagesState):\n  sys_msg_text: str\n  retrieved_docs: list[Document]\n```", "```py\ndef get_retrieve_node(self):\n  def retrieve_node(state: AgentState):\n    messages = state[\"messages\"]\n    message_contents = [message.content for message in messages]\n    retrieval_query = \"\\n\".join(message_contents)\n    docs = self.vector_store.retrieve(retrieval_query)\n    return {\"retrieved_docs\": docs}\n  return retrieve_node\n```", "```py\n@staticmethod\ndef augment_node(state: AgentState):\n  docs = state[\"retrieved_docs\"]\n  docs_content_list = [doc.page_content for doc in docs]\n  content = \"\\n\".join(docs_content_list)\n  new_text = SYS_MSG_AUGMENTATION.replace(\"{docs_content}\", content)\n  return {\"sys_msg_text\": BASE_SYS_MSG + \"\\n\\n\" + new_text}\n```", "```py\n...\nfrom vector_store import VectorStore\n\nclass Bot:\n  def __init__(self, api_keys, config):\n    self.api_keys = api_keys\n    self.config = config\n\n    self.llm = self.get_llm()\n    self.vector_store = self.get_vector_store()\n\n    self.graph = SupportAgentGraph(\n      llm=self.llm, vector_store=self.vector_store)\n\n  def get_vector_store(self):\n index_name = self.config[\"VECTOR_STORE_INDEX_NAME\"]\n return VectorStore(api_keys=self.api_keys, index_name=index_name)\n  ...\n```", "```py\n...\nif \"bot\" not in st.session_state:\n  api_keys = st.secrets[\"api_keys\"]\n  config = st.secrets[\"config\"]\n  st.session_state.bot = Bot(api_keys, config)\nbot = st.session_state.bot\n...\n```", "```py\nusers = {\n    1: {\n        \"first_name\": \"Alice\",\n        \"last_name\": \"Johnson\",\n        \"date_of_birth\": \"1990-05-14\",\n        \"email_address\": \"alice.johnson@example.com\"\n    },\n    ...\n}\n\norders = {\n    101: {\n        \"user_id\": 1,\n        \"order_placed_date\": \"2025-02-10\",\n        \"order_status\": \"Shipped\",\n        \"tracking_number\": \"TRK123456789\",\n        \"items_purchased\": [\"RoyalQuill\", \"RedPinner\"],\n        \"quantity\": [1, 1],\n        \"shipping_address\": \"123 Main St, Springfield, IL, 62701\",\n        \"expected_delivery_date\": \"2025-02-18\"\n    },\n    ...\n}\n```", "```py\ndef retrieve_user_id(email: str, dob: str) -> str:\n  \"\"\"\n  Look up a user's user ID, given their email address and date of birth.\n\n  If the user is not found, return None.\n\n  Args:\n    email (str): The email address of the user.\n    dob (str): The date of birth of the user in the format \"YYYY-MM-DD\".\n\n  Returns:\n    int: The user ID of the user, or None if the user is not found.\n  \"\"\"\n  for user_id, user_info in users.items():\n    if (user_info[\"email_address\"] == email and\n        user_info[\"date_of_birth\"] == dob):\n      return user_id\n```", "```py\ntools = [retrieve_user_id, get_order_id, get_order_status, cancel_order]\n```", "```py\n...\nfrom vector_store import VectorStore\nfrom tools import tools\n\nclass Bot:\n  def __init__(self, api_keys, config):\n    ...\n    self.llm = self.get_llm().bind_tools(tools)\n    ...\n  ...\n```", "```py\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom langgraph.graph import START, StateGraph, MessagesState\nfrom langgraph.prebuilt import tools_condition, ToolNode\nfrom langchain_core.documents import Document\nfrom langchain_core.messages import HumanMessage, SystemMessage\nfrom prompts import *\nfrom tools import tools\n\n...\nclass SupportAgentGraph:\n  ...\n  def build_graph(self):\n    ...\n    builder.add_node(\"tools\", ToolNode(tools))\n    ...\n    builder.add_edge(\"augment\", \"assistant\")\n    builder.add_conditional_edges(\"assistant\", tools_condition)\n builder.add_edge(\"tools\", \"assistant\")\n\n    return builder.compile(checkpointer=memory)\n  ...\n```", "```py\nbuilder.add_node(\"tools\", ToolNode(tools))\n```", "```py\ndef some_condition(state):\n  # Branching\n  if <something is true>:\n    return \"name_of_node_1\"\n  elif <something else is true>:\n    return \"name_of_node_2\"\n  ...\n```", "```py\nbuilder.add_conditional_edges(\"assistant\", tools_condition)\n```", "```py\nbuilder.add_edge(\"tools\", \"assistant\")\n```", "```py\n...\n@staticmethod\ndef is_internal_message(msg):\n return msg.type == \"tool\" or \"tool_calls\" in msg.additional_kwargs\n\ndef get_conversation(self):\n  state = self.graph.get_state(self.config)\n  if \"messages\" not in state.values:\n    return []\n messages = state.values[\"messages\"]\n return [msg for msg in messages if not self.is_internal_message(msg)]\n```"]